[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"intended audience includes little experience statistics, econometrics, data science, well individuals budding interest fields eager deepen knowledge. primary domain interest marketing, principles methods discussed book universally applicable discipline employs scientific methods data analysis.hope book provides valuable starting point aspiring statisticians, econometricians, data scientists, empowering navigate fascinating world causal inference data analysis confidence.","code":""},{"path":"index.html","id":"how-to-cite-this-book","chapter":"Preface","heading":"How to cite this book","text":"1. APA (7th edition):Nguyen, M. (2020). Guide Data Analysis. Bookdown.https://bookdown.org/mike/data_analysis/2. MLA (8th edition):Nguyen, Mike. Guide Data Analysis. Bookdown, 2020. https://bookdown.org/mike/data_analysis/3. Chicago (17th edition):Nguyen, Mike. 2020. Guide Data Analysis. Bookdown. https://bookdown.org/mike/data_analysis/4. Harvard:Nguyen, M. (2020) Guide Data Analysis. Bookdown. Available : https://bookdown.org/mike/data_analysis/","code":"@book{nguyen2020guide,\n  title={A Guide on Data Analysis},\n  author={Nguyen, Mike},\n  year={2020},\n  publisher={Bookdown},\n  url={https://bookdown.org/mike/data_analysis/}\n}"},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Since turn century, witnessed remarkable advancements innovations, particularly statistics, information technology, computer science, rapidly emerging field data science. However, one challenge developments overuse buzzwords like big data, machine learning, deep learning. terms powerful context, can sometimes obscure foundational principles underlying application.Every substantive field often specialized metric subfield, :Econometrics economicsPsychometrics psychologyChemometrics chemistrySabermetrics sports analyticsBiostatistics public health medicineTo layperson, disciplines often grouped broader terms like:Data ScienceApplied StatisticsComputational Social ScienceAs exciting explore new tools techniques, must admit retaining concepts can challenging. , effective way internalize apply ideas document data analysis process start finish.mind, let’s dive explore fascinating world data analysis together.","code":""},{"path":"introduction.html","id":"general-recommendations","chapter":"1 Introduction","heading":"1.1 General Recommendations","text":"journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.Readers can approach book several ways:\nFocused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.\nSequential Learning: follow traditional path data analysis, start Linear Regression section.\nExperimental Approach: interested designing experiments testing hypotheses, explore [Analysis Variance (ANOVA)] section.\nReaders can approach book several ways:Focused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.Sequential Learning: follow traditional path data analysis, start Linear Regression section.Experimental Approach: interested designing experiments testing hypotheses, explore [Analysis Variance (ANOVA)] section.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.customize code examples provided book, use R’s built-help functions. instance:\nlearn specific function, type help(function_name) ?function_name R console.\nexample, find details hist function, type ?hist help(hist) console.\ncustomize code examples provided book, use R’s built-help functions. instance:learn specific function, type help(function_name) ?function_name R console.example, find details hist function, type ?hist help(hist) console.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.adopting strategies, can tailor learning experience maximize value book.Tools statisticsProbability TheoryMathematical AnalysisComputer ScienceNumerical AnalysisDatabase ManagementCode ReplicationThis book built R version 4.4.3 (2025-02-28 ucrt) following packages:","code":"#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.4.3 (2025-02-28 ucrt)\n#>  os       Windows 11 x64 (build 26100)\n#>  system   x86_64, mingw32\n#>  ui       RTerm\n#>  language (EN)\n#>  collate  English_United States.utf8\n#>  ctype    English_United States.utf8\n#>  tz       America/Los_Angeles\n#>  date     2025-05-14\n#>  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#>  quarto   NA @ C:\\\\PROGRA~1\\\\RStudio\\\\resources\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version date (UTC) lib source\n#>  bookdown       0.43    2025-04-15 [1] CRAN (R 4.4.3)\n#>  bslib          0.9.0   2025-01-30 [1] CRAN (R 4.4.2)\n#>  cachem         1.1.0   2024-05-16 [1] CRAN (R 4.4.2)\n#>  cli            3.6.5   2025-04-23 [1] CRAN (R 4.4.3)\n#>  codetools      0.2-20  2024-03-31 [1] CRAN (R 4.4.3)\n#>  desc           1.4.3   2023-12-10 [1] CRAN (R 4.4.2)\n#>  devtools       2.4.5   2022-10-11 [1] CRAN (R 4.4.2)\n#>  dichromat      2.0-0.1 2022-05-02 [1] CRAN (R 4.4.0)\n#>  digest         0.6.37  2024-08-19 [1] CRAN (R 4.4.2)\n#>  downlit        0.4.4   2024-06-10 [1] CRAN (R 4.4.2)\n#>  dplyr        * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n#>  ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.4.2)\n#>  evaluate       1.0.3   2025-01-10 [1] CRAN (R 4.4.2)\n#>  farver         2.1.2   2024-05-13 [1] CRAN (R 4.4.2)\n#>  fastmap        1.2.0   2024-05-15 [1] CRAN (R 4.4.2)\n#>  forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.4.2)\n#>  fs             1.6.6   2025-04-12 [1] CRAN (R 4.4.3)\n#>  generics       0.1.4   2025-05-09 [1] CRAN (R 4.4.3)\n#>  ggplot2      * 3.5.2   2025-04-09 [1] CRAN (R 4.4.3)\n#>  glue           1.8.0   2024-09-30 [1] CRAN (R 4.4.2)\n#>  gtable         0.3.6   2024-10-25 [1] CRAN (R 4.4.2)\n#>  hms            1.1.3   2023-03-21 [1] CRAN (R 4.4.2)\n#>  htmltools      0.5.8.1 2024-04-04 [1] CRAN (R 4.4.2)\n#>  htmlwidgets    1.6.4   2023-12-06 [1] CRAN (R 4.4.2)\n#>  httpuv         1.6.16  2025-04-16 [1] CRAN (R 4.4.3)\n#>  jpeg         * 0.1-11  2025-03-21 [1] CRAN (R 4.4.3)\n#>  jquerylib      0.1.4   2021-04-26 [1] CRAN (R 4.4.2)\n#>  jsonlite       2.0.0   2025-03-27 [1] CRAN (R 4.4.3)\n#>  knitr          1.50    2025-03-16 [1] CRAN (R 4.4.3)\n#>  later          1.4.2   2025-04-08 [1] CRAN (R 4.4.3)\n#>  lifecycle      1.0.4   2023-11-07 [1] CRAN (R 4.4.2)\n#>  lubridate    * 1.9.4   2024-12-08 [1] CRAN (R 4.4.2)\n#>  magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.4.2)\n#>  memoise        2.0.1   2021-11-26 [1] CRAN (R 4.4.2)\n#>  mime           0.13    2025-03-17 [1] CRAN (R 4.4.3)\n#>  miniUI         0.1.2   2025-04-17 [1] CRAN (R 4.4.3)\n#>  pillar         1.10.2  2025-04-05 [1] CRAN (R 4.4.3)\n#>  pkgbuild       1.4.7   2025-03-24 [1] CRAN (R 4.4.3)\n#>  pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.4.2)\n#>  pkgload        1.4.0   2024-06-28 [1] CRAN (R 4.4.2)\n#>  profvis        0.4.0   2024-09-20 [1] CRAN (R 4.4.2)\n#>  promises       1.3.2   2024-11-28 [1] CRAN (R 4.4.2)\n#>  purrr        * 1.0.4   2025-02-05 [1] CRAN (R 4.4.3)\n#>  R6             2.6.1   2025-02-15 [1] CRAN (R 4.4.3)\n#>  RColorBrewer   1.1-3   2022-04-03 [1] CRAN (R 4.4.0)\n#>  Rcpp           1.0.14  2025-01-12 [1] CRAN (R 4.4.2)\n#>  readr        * 2.1.5   2024-01-10 [1] CRAN (R 4.4.2)\n#>  remotes        2.5.0   2024-03-17 [1] CRAN (R 4.4.2)\n#>  rlang          1.1.6   2025-04-11 [1] CRAN (R 4.4.3)\n#>  rmarkdown      2.29    2024-11-04 [1] CRAN (R 4.4.2)\n#>  rstudioapi     0.17.1  2024-10-22 [1] CRAN (R 4.4.2)\n#>  sass           0.4.10  2025-04-11 [1] CRAN (R 4.4.3)\n#>  scales       * 1.4.0   2025-04-24 [1] CRAN (R 4.4.3)\n#>  sessioninfo    1.2.3   2025-02-05 [1] CRAN (R 4.4.3)\n#>  shiny          1.10.0  2024-12-14 [1] CRAN (R 4.4.2)\n#>  stringi        1.8.7   2025-03-27 [1] CRAN (R 4.4.3)\n#>  stringr      * 1.5.1   2023-11-14 [1] CRAN (R 4.4.2)\n#>  tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.4.2)\n#>  tidyr        * 1.3.1   2024-01-24 [1] CRAN (R 4.4.2)\n#>  tidyselect     1.2.1   2024-03-11 [1] CRAN (R 4.4.2)\n#>  tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.4.2)\n#>  timechange     0.3.0   2024-01-18 [1] CRAN (R 4.4.2)\n#>  tzdb           0.5.0   2025-03-15 [1] CRAN (R 4.4.3)\n#>  urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.4.2)\n#>  usethis        3.1.0   2024-11-26 [1] CRAN (R 4.4.2)\n#>  vctrs          0.6.5   2023-12-01 [1] CRAN (R 4.4.2)\n#>  withr          3.0.2   2024-10-28 [1] CRAN (R 4.4.2)\n#>  xfun           0.52    2025-04-02 [1] CRAN (R 4.4.3)\n#>  xml2           1.3.8   2025-03-14 [1] CRAN (R 4.4.3)\n#>  xtable         1.8-4   2019-04-21 [1] CRAN (R 4.4.2)\n#>  yaml           2.3.10  2024-07-26 [1] CRAN (R 4.4.2)\n#> \n#>  [1] C:/Program Files/R/R-4.4.3/library\n#>  * ── Packages attached to the search path.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"},{"path":"prerequisites.html","id":"prerequisites","chapter":"2 Prerequisites","heading":"2 Prerequisites","text":"chapter serves concise review fundamental concepts Matrix Theory Probability Theory.confident understanding topics, can proceed directly Descriptive Statistics section begin exploring applied data analysis.","code":""},{"path":"prerequisites.html","id":"matrix-theory","chapter":"2 Prerequisites","heading":"2.1 Matrix Theory","text":"Matrix \\(\\) represents original matrix. ’s 2x2 matrix elements \\(a_{ij}\\), \\(\\) represents row \\(j\\) represents column.\\[\n=\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\] \\('\\) transpose \\(\\). transpose matrix flips rows columns.\\[\n' =\n\\begin{bmatrix}\na_{11} & a_{21} \\\\\na_{12} & a_{22}\n\\end{bmatrix}\n\\]Fundamental properties rules matrices, essential understanding operations linear algebra:\\[\n\\begin{aligned}\n\\mathbf{(ABC)'}   & = \\mathbf{C'B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(B+C)}   & = \\mathbf{AB + AC} \\quad &\\text{(Distributive property)} \\\\\n\\mathbf{AB}       & \\neq \\mathbf{BA} \\quad &\\text{(Multiplication commutative)} \\\\\n\\mathbf{(')'}    & = \\mathbf{} \\quad &\\text{(Double transpose original matrix)} \\\\\n\\mathbf{(+B)'}   & = \\mathbf{' + B'} \\quad &\\text{(Transpose sum sum transposes)} \\\\\n\\mathbf{(AB)'}    & = \\mathbf{B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(AB)^{-1}} & = \\mathbf{B^{-1}^{-1}} \\quad &\\text{(Inverse reverses order product)} \\\\\n\\mathbf{+B}      & = \\mathbf{B + } \\quad &\\text{(Addition commutative)} \\\\\n\\mathbf{AA^{-1}}  & = \\mathbf{} \\quad &\\text{(Matrix times inverse identity)}\n\\end{aligned}\n\\] properties critical solving systems equations, optimizing models, performing data transformations.matrix \\(\\mathbf{}\\) inverse, called invertible. \\(\\mathbf{}\\) inverse, referred singular.product two matrices \\(\\mathbf{}\\) \\(\\mathbf{B}\\) computed :\\[\n\\begin{aligned}\n\\mathbf{} &=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \\sum_{=1}^{3}a_{1i}b_{i2} & \\sum_{=1}^{3}a_{1i}b_{i3} \\\\\n\\sum_{=1}^{3}a_{2i}b_{i1} & \\sum_{=1}^{3}a_{2i}b_{i2} & \\sum_{=1}^{3}a_{2i}b_{i3}\n\\end{bmatrix}\n\\end{aligned}\n\\]Quadratic FormLet \\(\\mathbf{}\\) \\(3 \\times 1\\) vector. quadratic form involving matrix \\(\\mathbf{B}\\) given :\\[\n\\mathbf{'Ba} = \\sum_{=1}^{3}\\sum_{j=1}^{3}a_i b_{ij} a_{j}\n\\]Length VectorThe length (2-norm) vector \\(\\mathbf{}\\), denoted \\(||\\mathbf{}||\\), defined square root inner product vector :\\[\n||\\mathbf{}|| = \\sqrt{\\mathbf{'}}\n\\]","code":""},{"path":"prerequisites.html","id":"rank-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.1 Rank of a Matrix","text":"rank matrix refers :dimension space spanned columns (rows).number linearly independent columns rows.\\(n \\times k\\) matrix \\(\\mathbf{}\\) \\(k \\times k\\) matrix \\(\\mathbf{B}\\), following properties hold:\\(\\text{rank}(\\mathbf{}) \\leq \\min(n, k)\\)\\(\\text{rank}(\\mathbf{}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{AA'})\\)\\(\\text{rank}(\\mathbf{AB}) = \\min(\\text{rank}(\\mathbf{}), \\text{rank}(\\mathbf{B}))\\)\\(\\mathbf{B}\\) invertible (non-singular) \\(\\text{rank}(\\mathbf{B}) = k\\).","code":""},{"path":"prerequisites.html","id":"inverse-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.2 Inverse of a Matrix","text":"scalar algebra, \\(= 0\\), \\(1/\\) exist.matrix algebra, matrix invertible non-singular, meaning non-zero determinant inverse exists. square matrix \\(\\mathbf{}\\) invertible exists another square matrix \\(\\mathbf{B}\\) :\\[\n\\mathbf{AB} = \\mathbf{} \\quad \\text{(Identity Matrix)}.\n\\]case, \\(\\mathbf{}^{-1} = \\mathbf{B}\\).\\(2 \\times 2\\) matrix:\\[\n\\mathbf{} =\n\\begin{bmatrix}\n& b \\\\\nc & d\n\\end{bmatrix}\n\\]inverse :\\[\n\\mathbf{}^{-1} =\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & \n\\end{bmatrix}\n\\]inverse exists \\(ad - bc \\neq 0\\), \\(ad - bc\\) determinant \\(\\mathbf{}\\).partitioned block matrix:\\[\n\\begin{bmatrix}\n& B \\\\\nC & D\n\\end{bmatrix}^{-1}\n=\n\\begin{bmatrix}\n\\mathbf{(-BD^{-1}C)^{-1}} & \\mathbf{-(-BD^{-1}C)^{-1}BD^{-1}} \\\\\n\\mathbf{-D^{-1}C(-BD^{-1}C)^{-1}} & \\mathbf{D^{-1}+D^{-1}C(-BD^{-1}C)^{-1}BD^{-1}}\n\\end{bmatrix}\n\\]formula assumes \\(\\mathbf{D}\\) \\(\\mathbf{- BD^{-1}C}\\) invertible.Properties Inverse Non-Singular Matrices\\(\\mathbf{(^{-1})^{-1}} = \\mathbf{}\\)non-zero scalar \\(b\\), \\(\\mathbf{(bA)^{-1} = b^{-1}^{-1}}\\)matrix \\(\\mathbf{B}\\), \\(\\mathbf{(BA)^{-1} = B^{-1}^{-1}}\\) (\\(\\mathbf{B}\\) non-singular).\\(\\mathbf{(^{-1})' = (')^{-1}}\\) (transpose inverse equals inverse transpose).Never notate \\(\\mathbf{1/}\\); use \\(\\mathbf{^{-1}}\\) instead.Notes: - determinant matrix determines whether invertible. square matrices, determinant \\(0\\) means matrix singular inverse.\n- Always verify conditions invertibility, particularly dealing partitioned block matrices.","code":""},{"path":"prerequisites.html","id":"definiteness-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.3 Definiteness of a Matrix","text":"symmetric square \\(k \\times k\\) matrix \\(\\mathbf{}\\) classified based following conditions:Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.identity matrix always positive definite (PD).ExampleLet \\(\\mathbf{x} = (x_1, x_2)'\\), consider \\(2 \\times 2\\) identity matrix \\(\\mathbf{}\\):\\[\n\\begin{aligned}\n\\mathbf{x'Ix}\n&= (x_1, x_2)\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\n(x_1, x_2)\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\nx_1^2 + x_2^2 \\geq 0.\n\\end{aligned}\n\\]Thus, \\(\\mathbf{}\\) PD \\(\\mathbf{x'Ix} > 0\\) non-zero \\(\\mathbf{x}\\).Properties DefinitenessAny variance-covariance matrix PSD.matrix \\(\\mathbf{}\\) PSD exists matrix \\(\\mathbf{B}\\) : \\[\n\\mathbf{= B'B}.\n\\]\\(\\mathbf{}\\) PSD, \\(\\mathbf{B'AB}\\) also PSD conformable matrix \\(\\mathbf{B}\\).\\(\\mathbf{}\\) \\(\\mathbf{C}\\) non-singular, \\(\\mathbf{- C}\\) PSD \\(\\mathbf{C^{-1} - ^{-1}}\\) PSD.\\(\\mathbf{}\\) PD (ND), \\(\\mathbf{^{-1}}\\) also PD (ND).NotesAn indefinite matrix \\(\\mathbf{}\\) neither PSD NSD. concept direct counterpart scalar algebra.square matrix PSD invertible, PD.Examples DefinitenessInvertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Invertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"matrix-calculus","chapter":"2 Prerequisites","heading":"2.1.4 Matrix Calculus","text":"Consider scalar function \\(y = f(x_1, x_2, \\dots, x_k) = f(x)\\), \\(x\\) \\(1 \\times k\\) row vector.","code":""},{"path":"prerequisites.html","id":"gradient-first-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.1 Gradient (First-Order Derivative)","text":"gradient, first-order derivative \\(f(x)\\) respect vector \\(x\\), given :\\[\n\\frac{\\partial f(x)}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f(x)}{\\partial x_1} \\\\\n\\frac{\\partial f(x)}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f(x)}{\\partial x_k}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"hessian-second-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.2 Hessian (Second-Order Derivative)","text":"Hessian, second-order derivative \\(f(x)\\) respect \\(x\\), symmetric matrix defined :\\[\n\\frac{\\partial^2 f(x)}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n\\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_k} \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_k^2}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"derivative-of-a-scalar-function-with-respect-to-a-matrix","chapter":"2 Prerequisites","heading":"2.1.4.3 Derivative of a Scalar Function with Respect to a Matrix","text":"Let \\(f(\\mathbf{X})\\) scalar function, \\(\\mathbf{X}\\) \\(n \\times p\\) matrix. derivative :\\[\n\\frac{\\partial f(\\mathbf{X})}{\\partial \\mathbf{X}} =\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{11}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{1p}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{n1}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{np}}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"common-matrix-derivatives","chapter":"2 Prerequisites","heading":"2.1.4.4 Common Matrix Derivatives","text":"\\(\\mathbf{}\\) vector \\(\\mathbf{}\\) matrix independent \\(\\mathbf{y}\\):\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\n\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\\(\\mathbf{X}\\) symmetric:\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\\(\\mathbf{X}\\) symmetric \\(\\mathbf{}\\) matrix independent \\(\\mathbf{X}\\):\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\\(\\mathbf{X}\\) symmetric, let \\(\\mathbf{J}_{ij}\\) matrix 1 \\((,j)\\)-th position 0 elsewhere:\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)","code":""},{"path":"prerequisites.html","id":"optimization-in-scalar-and-vector-spaces","chapter":"2 Prerequisites","heading":"2.1.5 Optimization in Scalar and Vector Spaces","text":"Optimization process finding minimum maximum function. conditions optimization differ depending whether function involves scalar vector. comparison scalar vector optimization:Second-Order ConditionFor convex functions, implies minimum.Key ConceptsFirst-Order Condition:\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\n\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.vector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.Second-Order Condition:\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\n\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.vector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\nPositive Definite: minimum (convex function).Negative Definite: maximum (concave function).Indefinite: saddle point (neither minimum maximum).Convex Concave Functions:\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\n\nConvexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.Concave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.Convexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.Hessian Matrix:\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\n\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\nPositive definite Hessian: eigenvalues positive.Negative definite Hessian: eigenvalues negative.Indefinite Hessian: Eigenvalues mixed signs.","code":""},{"path":"prerequisites.html","id":"cholesky-decomposition","chapter":"2 Prerequisites","heading":"2.1.6 Cholesky Decomposition","text":"statistical analysis numerical linear algebra, decomposing matrices tractable forms crucial efficient computation. One important factorization Cholesky Decomposition. applies Hermitian (complex case) symmetric (real case), positive-definite matrices.Given \\(n \\times n\\) positive-definite matrix \\(\\), Cholesky Decomposition states:\\[\n= L L^{*},\n\\]:\\(L\\) lower-triangular matrix strictly positive diagonal entries.\\(L^{*}\\) denotes conjugate transpose \\(L\\) (simply transpose \\(L^{T}\\) real matrices).Cholesky Decomposition computationally efficient numerically stable, making go-technique many applications—particularly statistics deal extensively covariance matrices, linear systems, probability distributions.diving compute Cholesky Decomposition, need clarify means matrix positive-definite. real symmetric matrix \\(\\):\\(\\) positive-definite every nonzero vector \\(x\\), \\[\nx^T \\, x > 0.\n\\]Alternatively, can characterize positive-definiteness noting eigenvalues \\(\\) strictly positive.Many important matrices statistics—particularly covariance precision matrices—symmetric positive-definite.","code":""},{"path":"prerequisites.html","id":"existence","chapter":"2 Prerequisites","heading":"2.1.6.1 Existence","text":"real \\(n \\times n\\) matrix \\(\\) symmetric positive-definite always admits Cholesky Decomposition \\(= L L^T\\). theorem guarantees covariance matrix statistics—assuming valid (.e., positive-definite)—can decompose via Cholesky.","code":""},{"path":"prerequisites.html","id":"uniqueness","chapter":"2 Prerequisites","heading":"2.1.6.2 Uniqueness","text":"additionally require diagonal entries \\(L\\) strictly positive, \\(L\\) unique. , lower-triangular matrix strictly positive diagonal entries produce factorization. uniqueness helpful ensuring consistent numerical outputs software implementations.","code":""},{"path":"prerequisites.html","id":"constructing-the-cholesky-factor-l","chapter":"2 Prerequisites","heading":"2.1.6.3 Constructing the Cholesky Factor \\(L\\)","text":"Given real, symmetric, positive-definite matrix \\(\\\\mathbb{R}^{n \\times n}\\), want find lower-triangular matrix \\(L\\) \\(= LL^T\\). One way using simple step--step procedure (often part standard linear algebra libraries):Initialize \\(L\\) \\(n \\times n\\) zero matrix.Iterate rows \\(= 1, 2, \\dots, n\\):\nrow \\(\\), compute \\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{-1} L_{ik}^2}.\n\\]\ncolumn \\(j = +1, +2, \\dots, n\\): \\[\nL_{ji} = \\frac{1}{L_{ii}}\n          \\left(A_{ji} - \\sum_{k=1}^{-1} L_{jk} L_{ik}\\right).\n\\]\nentries \\(L\\) remain zero computed subsequent steps.\nrow \\(\\), compute \\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{-1} L_{ik}^2}.\n\\]column \\(j = +1, +2, \\dots, n\\): \\[\nL_{ji} = \\frac{1}{L_{ii}}\n          \\left(A_{ji} - \\sum_{k=1}^{-1} L_{jk} L_{ik}\\right).\n\\]entries \\(L\\) remain zero computed subsequent steps.Result: \\(L\\) lower-triangular, \\(L^T\\) transpose.Cholesky Decomposition roughly half computational cost general LU Decomposition. Specifically, requires order \\(\\frac{1}{3} n^3\\) floating-point operations (flops), making significantly efficient practice decompositions positive-definite systems.","code":""},{"path":"prerequisites.html","id":"illustrative-example","chapter":"2 Prerequisites","heading":"2.1.6.4 Illustrative Example","text":"Consider small \\(3 \\times 3\\) positive-definite matrix:\\[\n=\n\\begin{pmatrix}\n4 & 2 & 4 \\\\\n2 & 5 & 6 \\\\\n4 & 6 & 20\n\\end{pmatrix}.\n\\]claim \\(\\) positive-definite (one check calculating principal minors verifying \\(x^T x > 0\\) \\(x \\neq 0\\)). find \\(L\\) step--step:Compute \\(L_{11}\\):\\[\nL_{11} = \\sqrt{A_{11}} = \\sqrt{4} = 2.\n\\]Compute \\(L_{21}\\) \\(L_{31}\\):\n\\(L_{21} = \\frac{A_{21}}{L_{11}} = \\frac{2}{2} = 1.\\)\n\\(L_{31} = \\frac{A_{31}}{L_{11}} = \\frac{4}{2} = 2.\\)\n\\(L_{21} = \\frac{A_{21}}{L_{11}} = \\frac{2}{2} = 1.\\)\\(L_{31} = \\frac{A_{31}}{L_{11}} = \\frac{4}{2} = 2.\\)Compute \\(L_{22}\\):\\[\nL_{22} = \\sqrt{A_{22} - L_{21}^2}\n        = \\sqrt{5 - 1^2}\n        = \\sqrt{4} = 2.\n\\]Compute \\(L_{32}\\):\\[\nL_{32} = \\frac{A_{32} - L_{31} L_{21}}{L_{22}}\n        = \\frac{6 - (2)(1)}{2}\n        = \\frac{4}{2} = 2.\n\\]Compute \\(L_{33}\\):\\[\nL_{33} = \\sqrt{A_{33} - (L_{31}^2 + L_{32}^2)}\n        = \\sqrt{20 - (2^2 + 2^2)}\n        = \\sqrt{20 - 8}\n        = \\sqrt{12}\n        = 2\\sqrt{3}.\n\\]Thus,\\[\nL =\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 2 & 2\\sqrt{3}\n\\end{pmatrix}.\n\\]One can verify \\(L L^T = \\).","code":""},{"path":"prerequisites.html","id":"applications-in-statistics","chapter":"2 Prerequisites","heading":"2.1.6.5 Applications in Statistics","text":"","code":""},{"path":"prerequisites.html","id":"solving-linear-systems","chapter":"2 Prerequisites","heading":"2.1.6.5.1 Solving Linear Systems","text":"common statistical problem solving \\(x = b\\) \\(x\\). instance, regression computing Bayesian posterior modes, often need solve linear equations covariance precision matrices. \\(= LL^T\\):Forward Substitution: Solve \\(L \\, y = b\\).Backward Substitution: Solve \\(L^T x = y\\).two-step process stable efficient directly inverting \\(\\) (typically discouraged due numerical issues).","code":""},{"path":"prerequisites.html","id":"generating-correlated-random-vectors","chapter":"2 Prerequisites","heading":"2.1.6.5.2 Generating Correlated Random Vectors","text":"simulation-based statistics (e.g., Monte Carlo methods), often need generate random draws multivariate normal distribution \\(\\mathcal{N}(\\mu, \\Sigma)\\), \\(\\Sigma\\) covariance matrix. steps :Generate vector \\(z \\sim \\mathcal{N}(0, )\\) independent standard normal variables.Compute \\(x = \\mu + Lz\\), \\(\\Sigma = LL^T\\).\\(x\\) desired covariance structure \\(\\Sigma\\). technique widely used Bayesian statistics (e.g., MCMC sampling) financial modeling (e.g., portfolio simulations).","code":""},{"path":"prerequisites.html","id":"gaussian-processes-and-kriging","chapter":"2 Prerequisites","heading":"2.1.6.5.3 Gaussian Processes and Kriging","text":"Gaussian Process modeling (common spatial statistics, machine learning, geostatistics), frequently work large covariance matrices describe correlations observed data points:\\[\n\\Sigma =\n\\begin{pmatrix}\nk(x_1, x_1) & k(x_1, x_2) & \\cdots & k(x_1, x_n) \\\\\nk(x_2, x_1) & k(x_2, x_2) & \\cdots & k(x_2, x_n) \\\\\n\\vdots      & \\vdots      & \\ddots & \\vdots      \\\\\nk(x_n, x_1) & k(x_n, x_2) & \\cdots & k(x_n, x_n)\n\\end{pmatrix},\n\\]\\(k(\\cdot, \\cdot)\\) covariance (kernel) function. may need invert factorize \\(\\Sigma\\) repeatedly evaluate log-likelihood:\\[\n\\log \\mathcal{L}(\\theta) \\sim\n- \\tfrac{1}{2} \\left( y - m(\\theta) \\right)^T \\Sigma^{-1} \\left( y - m(\\theta) \\right)\n- \\tfrac{1}{2} \\log \\left| \\Sigma \\right|,\n\\]\\(m(\\theta)\\) mean function \\(\\theta\\) parameters. Using Cholesky factor \\(L\\) \\(\\Sigma\\) helps:\\(\\Sigma^{-1}\\) can implied solving systems \\(L\\) instead explicitly computing inverse.\\(\\log|\\Sigma|\\) can computed \\(2 \\sum_{=1}^n \\log L_{ii}\\).Hence, Cholesky Decomposition becomes backbone Gaussian Process computations.","code":""},{"path":"prerequisites.html","id":"bayesian-inference-with-covariance-matrices","chapter":"2 Prerequisites","heading":"2.1.6.5.4 Bayesian Inference with Covariance Matrices","text":"Many Bayesian models—especially hierarchical models—assume multivariate normal prior parameters. Cholesky Decomposition used :Sample priors posterior distributions.Regularize large covariance matrices.Speed Markov Chain Monte Carlo (MCMC) computations factorizing covariance structures.","code":""},{"path":"prerequisites.html","id":"other-notes","chapter":"2 Prerequisites","heading":"2.1.6.6 Other Notes","text":"Numerical Stability ConsiderationsCholesky Decomposition considered stable general LU Decomposition applied positive-definite matrices. Since row column pivots required, rounding errors can smaller. course, practice, software implementations can vary, extremely ill-conditioned matrices can still pose numerical challenges.Don’t Usually Compute \\(\\mathbf{}^{-1}\\)common statistics (especially older texts) see formulas involving \\(\\Sigma^{-1}\\). However, computing inverse explicitly often discouraged :numerically less stable.requires computations.Many tasks appear need \\(\\Sigma^{-1}\\) can done efficiently solving systems via Cholesky factor \\(L\\).Hence, “solve, don’t invert” common mantra. see expression like \\(\\Sigma^{-1} b\\), can use Cholesky factors \\(L\\) \\(L^T\\) solve \\(\\Sigma x = b\\) forward backward substitution, bypassing direct inverse calculation.Variants ExtensionsIncomplete Cholesky: Sometimes used iterative solvers full Cholesky factorization expensive, especially large sparse systems.LDL^T Decomposition: variant avoids taking square roots; used positive semi-definite indefinite systems, caution pivoting strategies.","code":""},{"path":"prerequisites.html","id":"probability-theory","chapter":"2 Prerequisites","heading":"2.2 Probability Theory","text":"","code":""},{"path":"prerequisites.html","id":"axioms-and-theorems-of-probability","chapter":"2 Prerequisites","heading":"2.2.1 Axioms and Theorems of Probability","text":"Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)probability empty set : \\[\nP[\\emptyset] = 0\n\\]probability empty set : \\[\nP[\\emptyset] = 0\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]","code":""},{"path":"prerequisites.html","id":"conditional-probability","chapter":"2 Prerequisites","heading":"2.2.1.1 Conditional Probability","text":"conditional probability \\(\\) given \\(B\\) defined :\\[\nP[|B] = \\frac{P[\\cap B]}{P[B]}, \\quad \\text{provided } P[B] \\neq 0.\n\\]","code":""},{"path":"prerequisites.html","id":"independent-events","chapter":"2 Prerequisites","heading":"2.2.1.2 Independent Events","text":"Two events \\(\\) \\(B\\) independent :\\(P[\\cap B] = P[]P[B]\\)\\(P[|B] = P[]\\)\\(P[B|] = P[B]\\)collection events \\(A_1, A_2, \\dots, A_n\\) independent every subcollection independent.","code":""},{"path":"prerequisites.html","id":"multiplication-rule","chapter":"2 Prerequisites","heading":"2.2.1.3 Multiplication Rule","text":"probability intersection two events can calculated : \\[\nP[\\cap B] = P[|B]P[B] = P[B|]P[].\n\\]","code":""},{"path":"prerequisites.html","id":"bayes-theorem","chapter":"2 Prerequisites","heading":"2.2.1.4 Bayes’ Theorem","text":"Let \\(A_1, A_2, \\dots, A_n\\) collection mutually exclusive events whose union \\(S\\), let \\(B\\) event \\(P[B] \\neq 0\\). , event \\(A_j\\) (\\(j = 1, 2, \\dots, n\\)): \\[\nP[A_j|B] = \\frac{P[B|A_j]P[A_j]}{\\sum_{=1}^n P[B|A_i]P[A_i]}.\n\\]","code":""},{"path":"prerequisites.html","id":"jensens-inequality","chapter":"2 Prerequisites","heading":"2.2.1.5 Jensen’s Inequality","text":"\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]Jensen’s inequality provides useful way demonstrate standard error calculated using sample standard deviation (\\(s\\)) proxy population standard deviation (\\(\\sigma\\)) biased estimator.population standard deviation \\(\\sigma\\) defined : \\[\n\\sigma = \\sqrt{\\mathbb{E}[(X - \\mu)^2]},\n\\] \\(\\mu = \\mathbb{E}[X]\\) population mean.population standard deviation \\(\\sigma\\) defined : \\[\n\\sigma = \\sqrt{\\mathbb{E}[(X - \\mu)^2]},\n\\] \\(\\mu = \\mathbb{E}[X]\\) population mean.sample standard deviation \\(s\\) given : \\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n (X_i - \\bar{X})^2},\n\\] \\(\\bar{X}\\) sample mean.sample standard deviation \\(s\\) given : \\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n (X_i - \\bar{X})^2},\n\\] \\(\\bar{X}\\) sample mean.\\(s\\) used estimator \\(\\sigma\\), expectation involves square root function, concave.\\(s\\) used estimator \\(\\sigma\\), expectation involves square root function, concave.Applying Jensen’s InequalityThe standard error formula involves square root: \\[\n\\sqrt{\\mathbb{E}[s^2]}.\n\\]However, square root function concave, Jensen’s inequality implies: \\[\n\\sqrt{\\mathbb{E}[s^2]} \\leq \\mathbb{E}[\\sqrt{s^2}] = \\mathbb{E}[s].\n\\]inequality shows expected value \\(s\\) (sample standard deviation) systematically underestimates population standard deviation \\(\\sigma\\).Quantifying BiasThe bias arises : \\[\n\\mathbb{E}[s] \\neq \\sigma.\n\\]correct bias, note sample standard deviation related population standard deviation : \\[\n\\mathbb{E}[s] = \\sigma \\cdot \\sqrt{\\frac{n-1}{n}},\n\\] \\(n\\) sample size. bias decreases \\(n\\) increases, estimator becomes asymptotically unbiased.leveraging Jensen’s inequality, observe concavity square root function ensures \\(s\\) biased estimator \\(\\sigma\\), systematically underestimating population standard deviation.","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectation","chapter":"2 Prerequisites","heading":"2.2.1.6 Law of Iterated Expectation","text":"Law Iterated Expectation states random variables \\(X\\) \\(Y\\):\\[\nE(X) = E(E(X|Y)).\n\\]means expected value \\(X\\) can obtained first calculating conditional expectation \\(E(X|Y)\\) taking expectation quantity distribution \\(Y\\).","code":""},{"path":"prerequisites.html","id":"correlation-and-independence","chapter":"2 Prerequisites","heading":"2.2.1.7 Correlation and Independence","text":"strength relationship random variables can ranked strongest weakest :Independence:\n\\(f(x, y) = f_X(x)f_Y(y)\\)\n\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\n\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)\n\\(f(x, y) = f_X(x)f_Y(y)\\)\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)Mean Independence (implied independence):\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\n\\(E[Xg(Y)] = E[X]E[g(Y)]\\)\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\\(E[Xg(Y)] = E[X]E[g(Y)]\\)Uncorrelatedness (implied independence mean independence):\n\\(\\text{Cov}(X, Y) = 0\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\\(E[XY] = E[X]E[Y]\\)\n\\(\\text{Cov}(X, Y) = 0\\)\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\\(E[XY] = E[X]E[Y]\\)","code":""},{"path":"prerequisites.html","id":"central-limit-theorem","chapter":"2 Prerequisites","heading":"2.2.2 Central Limit Theorem","text":"Central Limit Theorem states sufficiently large sample size (\\(n \\geq 25\\)), sampling distribution sample mean proportion approaches normal distribution, regardless population’s original distribution.Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) distribution \\(X\\) mean \\(\\mu\\) variance \\(\\sigma^2\\). , large \\(n\\):sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]following random variables approximately standard normal:\n\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\n\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\n\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\nfollowing random variables approximately standard normal:\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)","code":""},{"path":"prerequisites.html","id":"limiting-distribution-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.2.2.1 Limiting Distribution of the Sample Mean","text":"\\(\\{X_i\\}_{=1}^{n}\\) iid random sample distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), sample mean \\(\\bar{X}\\) scaled \\(\\sqrt{n}\\) following limiting distribution:\\[\n\\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2).\n\\]Standardizing sample mean gives: \\[\n\\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\xrightarrow{d} N(0, 1).\n\\]Notes:CLT holds random samples distribution (continuous, discrete, unknown).extends multivariate case: random sample random vector converges multivariate normal distribution.","code":""},{"path":"prerequisites.html","id":"asymptotic-variance-and-limiting-variance","chapter":"2 Prerequisites","heading":"2.2.2.2 Asymptotic Variance and Limiting Variance","text":"Asymptotic Variance (Avar): \\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\sigma^2.\n\\]Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).Limiting Variance (\\(\\lim_{n \\\\infty} Var\\))\\[\n\\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2\n\\]Represents value actual variance \\(\\sqrt{n}(\\bar{x} - \\mu)\\) converges \\(n \\\\infty\\).well-behaved estimator,\\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2.\n\\]However, asymptotic variance necessarily equal limiting value variance asymptotic variance derived limiting distribution, limiting variance convergence result sequence variances.\\[\nAvar(.) \\neq lim_{n \\\\infty} Var(.)\n\\]asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\n\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\n\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\n\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.Cases two match:Sample Quantiles: Consider sample quantile order \\(p\\), \\(0 < p < 1\\). regularity conditions, asymptotic distribution sample quantile normal, variance depends \\(p\\) density distribution \\(p\\)-th quantile. However, variance sample quantile necessarily converge limit sample size grows.Bootstrap Methods: using bootstrapping techniques estimate distribution statistic, bootstrap distribution might converge different limiting distribution original statistic. cases, variance bootstrap distribution (bootstrap variance) might differ limiting variance original statistic.Statistics Randomly Varying Asymptotic Behavior: cases, asymptotic behavior statistic can vary randomly depending sample path. statistics, asymptotic variance might provide consistent estimate limiting variance.M-estimators Varying Asymptotic Behavior: M-estimators can sometimes different asymptotic behaviors depending tail behavior underlying distribution. heavy-tailed distributions, variance estimator might stabilize even sample size grows large, making asymptotic variance different variance limiting distribution.","code":""},{"path":"prerequisites.html","id":"random-variable","chapter":"2 Prerequisites","heading":"2.2.3 Random Variable","text":"Random variables can categorized either discrete continuous, distinct properties functions defining type.Expected Value Properties\\(E[c] = c\\) constant \\(c\\).\\(E[cX] = cE[X]\\) constant \\(c\\).\\(E[X + Y] = E[X] + E[Y]\\).\\(E[XY] = E[X]E[Y]\\) (\\(X\\) \\(Y\\) independent).Variance Properties\\(\\text{Var}(c) = 0\\) constant \\(c\\).\\(\\text{Var}(cX) = c^2 \\text{Var}(X)\\) constant \\(c\\).\\(\\text{Var}(X) \\geq 0\\).\\(\\text{Var}(X) = E[X^2] - (E[X])^2\\).\\(\\text{Var}(X + c) = \\text{Var}(X)\\).\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (\\(X\\) \\(Y\\) independent).standard deviation \\(\\sigma\\) given : \\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\text{Var}(X)}.\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-random-variables","chapter":"2 Prerequisites","heading":"2.2.3.1 Multivariate Random Variables","text":"Suppose \\(y_1, \\dots, y_p\\) random variables means \\(\\mu_1, \\dots, \\mu_p\\). :\\[\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_p\n\\end{bmatrix}, \\quad E[\\mathbf{y}] = \\begin{bmatrix}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix} = \\boldsymbol{\\mu}.\n\\]covariance \\(y_i\\) \\(y_j\\) \\(\\sigma_{ij} = \\text{Cov}(y_i, y_j)\\). variance-covariance (dispersion) matrix :\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij})= \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n\\end{bmatrix}.\n\\]\\(\\mathbf{\\Sigma}\\) symmetric \\((p+1)p/2\\) unique parameters.Alternatively, let \\(u_{p \\times 1}\\) \\(v_{v \\times 1}\\) random vectors means \\(\\mathbf{\\mu_u}\\) \\(\\mathbf{\\mu_v}\\). \\[ \\mathbf{\\Sigma_{uv}} = cov(\\mathbf{u,v}) = E[\\mathbf{(u-\\mu_u)(v-\\mu_v)'}] \\]\\(\\Sigma_{uv} \\neq \\Sigma_{vu}\\) (\\(\\Sigma_{uv} = \\Sigma_{vu}'\\))Properties Covariance MatricesSymmetry: \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\).Eigen-Decomposition (spectral decomposition,symmetric decomposition): \\(\\mathbf{\\Sigma = \\Phi \\Lambda \\Phi}\\), \\(\\mathbf{\\Phi}\\) matrix eigenvectors \\(\\mathbf{\\Phi \\Phi' = }\\) (orthonormal), \\(\\mathbf{\\Lambda}\\) diagonal matrix eigenvalues \\((\\lambda_1,...,\\lambda_p)\\) diagonal.Non-Negative Definiteness: \\(\\mathbf{\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\). Equivalently, eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge ... \\ge \\lambda_p \\ge 0\\)Generalized Variance: \\(|\\mathbf{\\Sigma}| = \\lambda_1 \\dots \\lambda_p \\geq 0\\).Trace: \\(\\text{tr}(\\mathbf{\\Sigma}) = \\lambda_1 + \\dots + \\lambda_p = \\sigma_{11} + \\dots+ \\sigma_{pp} = \\sum \\sigma_{ii}\\) = sum variances (total variance).Note: \\(\\mathbf{\\Sigma}\\) required positive definite. implies eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma}= \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\)","code":""},{"path":"prerequisites.html","id":"correlation-matrices","chapter":"2 Prerequisites","heading":"2.2.3.2 Correlation Matrices","text":"correlation coefficient \\(\\rho_{ij}\\) correlation matrix \\(\\mathbf{R}\\) defined :\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}, \\quad \\mathbf{R} = \\begin{bmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\dots & 1\n\\end{bmatrix}.\n\\]\\(\\rho_{ii} = 1 \\forall \\)","code":""},{"path":"prerequisites.html","id":"linear-transformations","chapter":"2 Prerequisites","heading":"2.2.3.3 Linear Transformations","text":"Let \\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants, \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants. :\\(E[\\mathbf{Ay + c}] = \\mathbf{\\mu_y + c}\\).\\(\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{\\Sigma_y '}\\).\\(\\text{Cov}(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B'}\\).","code":""},{"path":"prerequisites.html","id":"moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4 Moment Generating Function","text":"","code":""},{"path":"prerequisites.html","id":"properties-of-the-moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4.1 Properties of the Moment Generating Function","text":"\\(\\frac{d^k(m_X(t))}{dt^k} \\bigg|_{t=0} = E[X^k]\\) (\\(k\\)-th derivative \\(t=0\\) gives \\(k\\)-th moment \\(X\\)).\\(\\mu = E[X] = m_X'(0)\\) (first derivative \\(t=0\\) gives mean).\\(E[X^2] = m_X''(0)\\) (second derivative \\(t=0\\) gives second moment).","code":""},{"path":"prerequisites.html","id":"theorems-involving-mgfs","chapter":"2 Prerequisites","heading":"2.2.4.2 Theorems Involving MGFs","text":"Let \\(X_1, X_2, \\dots, X_n, Y\\) random variables MGFs \\(m_{X_1}(t), m_{X_2}(t), \\dots, m_{X_n}(t), m_Y(t)\\):\\(m_{X_1}(t) = m_{X_2}(t)\\) \\(t\\) open interval 0, \\(X_1\\) \\(X_2\\) distribution.\\(Y = \\alpha + \\beta X_1\\), : \\[\nm_Y(t) = e^{\\alpha t}m_{X_1}(\\beta t).\n\\]\\(X_1, X_2, \\dots, X_n\\) independent \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), \\(\\alpha_0, \\alpha_1, \\dots, \\alpha_n\\) constants, : \\[\nm_Y(t) = e^{\\alpha_0 t} m_{X_1}(\\alpha_1 t) m_{X_2}(\\alpha_2 t) \\dots m_{X_n}(\\alpha_n t).\n\\]Suppose \\(X_1, X_2, \\dots, X_n\\) independent normal random variables means \\(\\mu_1, \\mu_2, \\dots, \\mu_n\\) variances \\(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2\\). \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), :\n\\(Y\\) normally distributed.\nMean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).\nVariance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).\n\\(Y\\) normally distributed.Mean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).Variance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).","code":""},{"path":"prerequisites.html","id":"moments","chapter":"2 Prerequisites","heading":"2.2.5 Moments","text":"Skewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\)\nDefinition: Skewness measures asymmetry probability distribution around mean.\nInterpretation:\nPositive skewness: right tail (higher values) longer heavier left tail.\nNegative skewness: left tail (lower values) longer heavier right tail.\nZero skewness: data symmetric.\n\nSkewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\)Definition: Skewness measures asymmetry probability distribution around mean.Interpretation:\nPositive skewness: right tail (higher values) longer heavier left tail.\nNegative skewness: left tail (lower values) longer heavier right tail.\nZero skewness: data symmetric.\nPositive skewness: right tail (higher values) longer heavier left tail.Positive skewness: right tail (higher values) longer heavier left tail.Negative skewness: left tail (lower values) longer heavier right tail.Negative skewness: left tail (lower values) longer heavier right tail.Zero skewness: data symmetric.Zero skewness: data symmetric.Kurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\)\nDefinition: Kurtosis measures “tailedness” heaviness tails probability distribution.\nExcess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).\nInterpretation:\nHigh kurtosis (>3): Heavy tails, extreme outliers.\nLow kurtosis (<3): Light tails, fewer outliers.\nNormal distribution kurtosis = 3: Benchmark comparison.\n\nKurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\)Definition: Kurtosis measures “tailedness” heaviness tails probability distribution.Definition: Kurtosis measures “tailedness” heaviness tails probability distribution.Excess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).Excess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).Interpretation:\nHigh kurtosis (>3): Heavy tails, extreme outliers.\nLow kurtosis (<3): Light tails, fewer outliers.\nNormal distribution kurtosis = 3: Benchmark comparison.\nInterpretation:High kurtosis (>3): Heavy tails, extreme outliers.High kurtosis (>3): Heavy tails, extreme outliers.Low kurtosis (<3): Light tails, fewer outliers.Low kurtosis (<3): Light tails, fewer outliers.Normal distribution kurtosis = 3: Benchmark comparison.Normal distribution kurtosis = 3: Benchmark comparison.","code":""},{"path":"prerequisites.html","id":"skewness","chapter":"2 Prerequisites","heading":"2.2.6 Skewness","text":"Skewness measures asymmetry distribution:Positive skew: right side (high values) stretched .\nPositive skew occurs right tail (higher values) distribution longer heavier.\nExamples:\nIncome Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.\nHousing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.\n\nPositive skew: right side (high values) stretched .Positive skew occurs right tail (higher values) distribution longer heavier.Positive skew occurs right tail (higher values) distribution longer heavier.Examples:\nIncome Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.\nHousing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.\nExamples:Income Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.Income Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.Housing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.Housing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.Income Distribution example, people earn moderate incomes, high earners stretch right tail.Housing Prices example, homes reasonably priced, mansions create long, expensive right tail.Negative Skew (Left Skew)Negative skew occurs left tail (lower values) distribution longer heavier.Negative skew occurs left tail (lower values) distribution longer heavier.Examples:\nScores Easy Test: exam easy, students score quite high, students score low, creating left tail.\nAge Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).\nExamples:Scores Easy Test: exam easy, students score quite high, students score low, creating left tail.Scores Easy Test: exam easy, students score quite high, students score low, creating left tail.Age Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).Age Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).Easy Test Scores example, students perform well, low scores stretch left tail.Easy Test Scores example, students perform well, low scores stretch left tail.Retirement Age example, people retire around age, small number individuals retire early, stretching left tail.Retirement Age example, people retire around age, small number individuals retire early, stretching left tail.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Simulate data for positive skew\nset.seed(123)\npositive_skew_income <-\n    rbeta(1000, 5, 2) * 100  # Income distribution example\npositive_skew_housing <-\n    rbeta(1000, 5, 2) * 1000  # Housing prices example\n\n# Combine data\ndata_positive_skew <- data.frame(\n    value = c(positive_skew_income, positive_skew_housing),\n    example = c(rep(\"Income Distribution\", 1000), rep(\"Housing Prices\", 1000))\n)\n\n# Plot positive skew\nggplot(data_positive_skew, aes(x = value, fill = example)) +\n    geom_histogram(bins = 30,\n                   alpha = 0.7,\n                   position = \"identity\") +\n    facet_wrap( ~ example, scales = \"free\") +\n    labs(title = \"Visualization of Positive Skew\",\n         x = \"Value\",\n         y = \"Frequency\") +\n    theme_minimal()\n# Simulate data for negative skew\nset.seed(123)\nnegative_skew_test <-\n    10 - rbeta(1000, 5, 2) * 10  # Easy test scores example\nnegative_skew_retirement <-\n    80 - rbeta(1000, 5, 2) * 20  # Retirement age example\n\n# Combine data\ndata_negative_skew <- data.frame(\n    value = c(negative_skew_test, negative_skew_retirement),\n    example = c(rep(\"Easy Test Scores\", 1000), rep(\"Retirement Age\", 1000))\n)\n\n# Plot negative skew\nggplot(data_negative_skew, aes(x = value, fill = example)) +\n    geom_histogram(bins = 30,\n                   alpha = 0.7,\n                   position = \"identity\") +\n    facet_wrap( ~ example, scales = \"free\") +\n    labs(title = \"Visualization of Negative Skew\",\n         x = \"Value\",\n         y = \"Frequency\") +\n    theme_minimal()"},{"path":"prerequisites.html","id":"kurtosis","chapter":"2 Prerequisites","heading":"2.2.7 Kurtosis","text":"Kurtosis measures “peakedness” heaviness tails:High kurtosis: Tall, sharp peak heavy tails.\nExample: Financial market returns crisis (extreme losses gains).\nHigh kurtosis: Tall, sharp peak heavy tails.Example: Financial market returns crisis (extreme losses gains).Low kurtosis: Flatter peak thinner tails.\nExample: Human height distribution (fewer extreme deviations mean).\nLow kurtosis: Flatter peak thinner tails.Example: Human height distribution (fewer extreme deviations mean).left panel shows low kurtosis, similar distribution human height, flatter peak thinner tails.left panel shows low kurtosis, similar distribution human height, flatter peak thinner tails.right panel shows high kurtosis, reflecting financial market returns, extreme outliers gains losses.right panel shows high kurtosis, reflecting financial market returns, extreme outliers gains losses.","code":"\n# Simulate data for kurtosis\nlow_kurtosis <- runif(1000, 0, 10)  # Low kurtosis\nhigh_kurtosis <- c(rnorm(900, 5, 1), rnorm(100, 5, 5))  # High kurtosis\n\n# Combine data\ndata_kurtosis <- data.frame(\n  value = c(low_kurtosis, high_kurtosis),\n  kurtosis_type = c(rep(\"Low Kurtosis (Height Distribution)\", 1000), \n                    rep(\"High Kurtosis (Market Returns)\", 1000))\n)\n\n# Plot kurtosis\nggplot(data_kurtosis, aes(x = value, fill = kurtosis_type)) +\n  geom_histogram(bins = 30, alpha = 0.7, position = \"identity\") +\n  facet_wrap(~kurtosis_type) +\n  labs(\n    title = \"Visualization of Kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()"},{"path":"prerequisites.html","id":"conditional-moments","chapter":"2 Prerequisites","heading":"2.2.7.1 Conditional Moments","text":"random variable \\(Y\\) given \\(X=x\\):Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-moments","chapter":"2 Prerequisites","heading":"2.2.7.2 Multivariate Moments","text":"Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}\n\\]Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}\n\\]Variance-Covariance Matrix: \\[\n\\begin{aligned}\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}\n\\end{aligned}\n\\]Variance-Covariance Matrix: \\[\n\\begin{aligned}\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"prerequisites.html","id":"properties-of-moments","chapter":"2 Prerequisites","heading":"2.2.7.3 Properties of Moments","text":"\\(E[aX + + c] = aE[X] + [Y] + c\\)\\(\\text{Var}(aX + + c) = ^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\\)\\(\\text{Cov}(aX + , cX + dY) = ac \\text{Var}(X) + bd \\text{Var}(Y) + (ad + bc) \\text{Cov}(X, Y)\\)Correlation: \\(\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)","code":""},{"path":"prerequisites.html","id":"distributions","chapter":"2 Prerequisites","heading":"2.2.8 Distributions","text":"","code":""},{"path":"prerequisites.html","id":"conditional-distributions","chapter":"2 Prerequisites","heading":"2.2.8.1 Conditional Distributions","text":"\\[\nf_{X|Y}(x|y) = \\frac{f(x, y)}{f_Y(y)}\n\\]\\(X\\) \\(Y\\) independent:\\[\nf_{X|Y}(x|y) = f_X(x).\n\\]","code":""},{"path":"prerequisites.html","id":"discrete-distributions","chapter":"2 Prerequisites","heading":"2.2.8.2 Discrete Distributions","text":"","code":""},{"path":"prerequisites.html","id":"bernoulli-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.1 Bernoulli Distribution","text":"random variable \\(X\\) follows Bernoulli distribution, denoted \\(X \\sim \\text{Bernoulli}(p)\\), represents single trial :Success probability \\(p\\)Success probability \\(p\\)Failure probability \\(q = 1-p\\).Failure probability \\(q = 1-p\\).Density Function\\[\nf(x) = p^x (1-p)^{1-x}, \\quad x \\\\{0, 1\\}\n\\]CDF: Use table manual computation.PDFMean\\[\n\\mu = E[X] = p\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = p(1-p)\n\\]","code":"\nhist(\n    mc2d::rbern(1000, prob = 0.5),\n    main = \"Histogram of Bernoulli Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"binomial-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.2 Binomial Distribution","text":"\\(X \\sim B(n, p)\\) number successes \\(n\\) independent Bernoulli trials, :\\(n\\) number trials\\(n\\) number trials\\(p\\) success probability.\\(p\\) success probability.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.Density Function\\[\nf(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n\n\\]PDFMGF\\[\nm_X(t) = (1 - p + p e^t)^n\n\\]Mean\\[\n\\mu = np\n\\]Variance\\[\n\\sigma^2 = np(1-p)\n\\]","code":"\nhist(\n    rbinom(1000, size = 100, prob = 0.5),\n    main = \"Histogram of Binomial Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"poisson-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.3 Poisson Distribution","text":"\\(X \\sim \\text{Poisson}(\\lambda)\\) models number occurrences event fixed interval, average rate \\(\\lambda\\).Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.random variable \\(X\\) number occurrences event within interval \\(s\\) units.random variable \\(X\\) number occurrences event within interval \\(s\\) units.parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).Density Function\\[\nf(x) = \\frac{e^{-k} k^x}{x!}, \\quad x = 0, 1, 2, \\dots\n\\]CDFPDFMGF\\[\nm_X(t) = e^{k (e^t - 1)}\n\\]Mean\\[\n\\mu = E(X) = k\n\\]Variance\\[\n\\sigma^2 = Var(X) = k\n\\]","code":"\nhist(rpois(1000, lambda = 5),\n     main = \"Histogram of Poisson Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"geometric-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.4 Geometric Distribution","text":"\\(X \\sim \\text{G}(p)\\) models number trials needed obtain first success, :\\(p\\): probability success\\(p\\): probability success\\(q = 1-p\\): probability failure.\\(q = 1-p\\): probability failure.experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.Density Function\\[\nf(x) = p(1-p)^{x-1}, \\quad x = 1, 2, \\dots\n\\]CDF\\[\nF(x) = 1 - (1-p)^x\n\\]PDFMGF\\[\nm_X(t) = \\frac{p e^t}{1 - (1-p)e^t}, \\quad t < -\\ln(1-p)\n\\]Mean\\[\n\\mu = \\frac{1}{p}\n\\]Variance\\[\n\\sigma^2 = \\frac{1-p}{p^2}\n\\]","code":"\nhist(rgeom(1000, prob = 0.5),\n     main = \"Histogram of Geometric Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"hypergeometric-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.5 Hypergeometric Distribution","text":"\\(X \\sim \\text{H}(N, r, n)\\) models number successes sample size \\(n\\) drawn without replacement population size \\(N\\), :\\(r\\) objects trait interest\\(r\\) objects trait interest\\(N-r\\) trait.\\(N-r\\) trait.Density Function\\[\nf(x) = \\frac{\\binom{r}{x} \\binom{N-r}{n-x}}{\\binom{N}{n}}, \\quad \\max(0, n-(N-r)) \\leq x \\leq \\min(n, r)\n\\]PDFMean\\[\n\\mu = E[X] = \\frac{n r}{N}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = n \\frac{r}{N} \\frac{N-r}{N} \\frac{N-n}{N-1}\n\\]Note: large \\(N\\) (\\(\\frac{n}{N} \\leq 0.05\\)), hypergeometric distribution can approximated binomial distribution \\(p = \\frac{r}{N}\\).","code":"\nhist(\n    rhyper(1000, m = 50, n = 20, k = 30),\n    main = \"Histogram of Hypergeometric Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"continuous-distributions","chapter":"2 Prerequisites","heading":"2.2.8.3 Continuous Distributions","text":"","code":""},{"path":"prerequisites.html","id":"uniform-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.1 Uniform Distribution","text":"Defined interval \\((, b)\\), probabilities “equally likely” subintervals equal length.Density Function: \\[\nf(x) = \\frac{1}{b-}, \\quad < x < b\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x < \\\\\n\\frac{x-}{b-} & \\le x \\le b \\\\\n1 & \\text{} x > b\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) =\n\\begin{cases}\n\\frac{e^{tb} - e^{ta}}{t(b-)} & \\text{} t \\neq 0 \\\\\n1 & \\text{} t = 0\n\\end{cases}\n\\]Mean\\[\n\\mu = E[X] = \\frac{+ b}{2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{(b-)^2}{12}\n\\]","code":"\nhist(\n    runif(1000, min = 0, max = 1),\n    main = \"Histogram of Uniform Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"gamma-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.2 Gamma Distribution","text":"gamma distribution used define exponential \\(\\chi^2\\) distributions.gamma function defined : \\[\n\\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz, \\quad \\alpha > 0\n\\]Properties Gamma Function:\\(\\Gamma(1) = 1\\)\\(\\Gamma(1) = 1\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)Density Function:\\[\nf(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n\\]CDF (\\(\\alpha = n\\), \\(x>0\\) positive integer):\\[\nF(x, n, \\beta) = 1 - \\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!}\n\\]PDF:MGF\\[\nm_X(t) = (1 - \\beta t)^{-\\alpha}, \\quad t < \\frac{1}{\\beta}\n\\]Mean\\[\n\\mu = E[X] = \\alpha \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\alpha \\beta^2\n\\]","code":"\nhist(\n    rgamma(n = 1000, shape = 5, rate = 1),\n    main = \"Histogram of Gamma Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.3 Normal Distribution","text":"normal distribution, denoted \\(N(\\mu, \\sigma^2)\\), symmetric bell-shaped parameters \\(\\mu\\) (mean) \\(\\sigma^2\\) (variance). also known Gaussian distribution.Density Function:\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\quad -\\infty < x < \\infty, \\; \\sigma > 0\n\\]CDF: Use table numerical methods.PDFMGF\\[\nm_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}\n\\]Mean\\[\n\\mu = E[X]\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X)\n\\]Standard Normal Random Variable:normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]Normal Approximation Binomial Distribution:Let \\(X\\) binomial parameters \\(n\\) \\(p\\). large \\(n\\):\\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p > 0.5\\) \\(n(1-p) > 5\\),\\(p > 0.5\\) \\(n(1-p) > 5\\),\\(X\\) approximately normally distributed mean \\(\\mu = np\\) standard deviation \\(\\sigma = \\sqrt{np(1-p)}\\).using normal approximation, add subtract 0.5 needed continuity correction.Discrete Approximate Normal (Corrected):Normal Probability RuleIf X normally distributed parameters \\(\\mu\\) \\(\\sigma\\), \\(P(-\\sigma < X - \\mu < \\sigma) \\approx .68\\)\\(P(-2\\sigma < X - \\mu < 2\\sigma) \\approx .95\\)\\(P(-3\\sigma < X - \\mu < 3\\sigma) \\approx .997\\)","code":"\nhist(\n    rnorm(1000, mean = 0, sd = 1),\n    main = \"Histogram of Normal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"logistic-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.4 Logistic Distribution","text":"logistic distribution continuous probability distribution commonly used logistic regression types statistical modeling. resembles normal distribution heavier tails, allowing extreme values. - logistic distribution symmetric around \\(\\mu\\). - heavier tails make useful modeling outcomes occasional extreme values.Density Function\\[\nf(x; \\mu, s) = \\frac{e^{-(x-\\mu)/s}}{s \\left(1 + e^{-(x-\\mu)/s}\\right)^2}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(s > 0\\) scale parameter.CDF\\[\nF(x; \\mu, s) = \\frac{1}{1 + e^{-(x-\\mu)/s}}, \\quad -\\infty < x < \\infty\n\\]PDFMGFThe MGF logistic distribution exist expected value diverges \\(t\\).Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{\\pi^2 s^2}{3}\n\\]","code":"\nhist(\n    rlogis(1000, location = 0, scale = 1),\n    main = \"Histogram of Logistic Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"laplace-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.5 Laplace Distribution","text":"Laplace distribution, also known double exponential distribution, continuous probability distribution often used economics, finance, engineering. characterized peak mean heavier tails compared normal distribution.Laplace distribution symmetric around \\(\\mu\\).heavier tails normal distribution, making suitable modeling data extreme outliers.Density Function\\[\nf(x; \\mu, b) = \\frac{1}{2b} e^{-|x-\\mu|/b}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(b > 0\\) scale parameter.CDF\\[\nF(x; \\mu, b) =\n\\begin{cases}\n    \\frac{1}{2} e^{(x-\\mu)/b} & \\text{} x < \\mu \\\\\n    1 - \\frac{1}{2} e^{-(x-\\mu)/b} & \\text{} x \\ge \\mu\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = \\frac{e^{\\mu t}}{1 - b^2 t^2}, \\quad |t| < \\frac{1}{b}\n\\]Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2b^2\n\\]","code":"\nhist(\n    VGAM::rlaplace(1000, location = 0, scale = 1),\n    main = \"Histogram of Laplace Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"log-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.6 Log-normal Distribution","text":"log-normal distribution denoted \\(\\text{Lognormal}(\\mu, \\sigma^2)\\).PDF","code":"\nhist(rlnorm(n = 1000, meanlog = 0, sdlog = 1), main=\"Histogram of Log-normal Distribution\", xlab=\"Value\", ylab=\"Frequency\")"},{"path":"prerequisites.html","id":"lognormal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.7 Lognormal Distribution","text":"lognormal distribution continuous probability distribution random variable whose logarithm normally distributed. often used model variables positively skewed, income biological measurements.lognormal distribution positively skewed.useful modeling data take negative values often used finance environmental studies.Density Function\\[\nf(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-(\\ln(x) - \\mu)^2 / (2\\sigma^2)}, \\quad x > 0\n\\]\\(\\mu\\) mean underlying normal distribution \\(\\sigma > 0\\) standard deviation.CDFThe cumulative distribution function lognormal distribution given :\\[\nF(x; \\mu, \\sigma) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{\\ln(x) - \\mu}{\\sigma \\sqrt{2}} \\right) \\right], \\quad x > 0\n\\]PDFMGFThe moment generating function (MGF) lognormal distribution exist simple closed form.Mean\\[\nE[X] = e^{\\mu + \\sigma^2 / 2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\left( e^{\\sigma^2} - 1 \\right) e^{2\\mu + \\sigma^2}\n\\]","code":"\nhist(\n    rlnorm(1000, meanlog = 0, sdlog = 1),\n    main = \"Histogram of Lognormal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"exponential-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.8 Exponential Distribution","text":"exponential distribution, denoted \\(\\text{Exp}(\\lambda)\\), special case gamma distribution \\(\\alpha = 1\\).commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.exponential distribution memoryless, meaning probability event occurring future independent past.exponential distribution memoryless, meaning probability event occurring future independent past.commonly used model waiting times, time next customer arrives time radioactive particle decays.commonly used model waiting times, time next customer arrives time radioactive particle decays.Density Function\\[\nf(x) = \\frac{1}{\\beta} e^{-x/\\beta}, \\quad x, \\beta > 0\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x \\le 0 \\\\\n1 - e^{-x/\\beta} & \\text{} x > 0\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = (1-\\beta t)^{-1}, \\quad t < 1/\\beta\n\\]Mean\\[\n\\mu = E[X] = \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\beta^2\n\\]","code":"\nhist(rexp(n = 1000, rate = 1),\n     main = \"Histogram of Exponential Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"chi-squared-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.9 Chi-Squared Distribution","text":"chi-squared distribution continuous probability distribution commonly used statistical inference, particularly hypothesis testing construction confidence intervals variance. also used goodness--fit tests.chi-squared distribution defined positive values.often used model distribution sum squares \\(k\\) independent standard normal random variables.Density Function\\[\nf(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x \\ge 0\n\\]\\(k\\) degrees freedom \\(\\Gamma\\) gamma function.CDFThe cumulative distribution function chi-squared distribution given :\\[\nF(x; k) = \\frac{\\gamma(k/2, x/2)}{\\Gamma(k/2)}, \\quad x \\ge 0\n\\]\\(\\gamma\\) lower incomplete gamma function.PDFMGF\\[\nm_X(t) = (1 - 2t)^{-k/2}, \\quad t < \\frac{1}{2}\n\\]Mean\\[\nE[X] = k\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2k\n\\]","code":"\nhist(\n    rchisq(1000, df = 5),\n    main = \"Histogram of Chi-Squared Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"students-t-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.10 Student’s T Distribution","text":"Student’s t-distribution named William Sealy Gosset, statistician Guinness Brewery early 20th century. Gosset developed t-distribution address small-sample problems quality control. Since Guinness prohibited employees publishing names, Gosset used pseudonym “Student” published work 1908 (Student 1908). name stuck ever since, honoring contribution statistics.Student’s t-distribution, denoted \\(T(v)\\), defined : \\[\nT = \\frac{Z}{\\sqrt{\\chi^2_v / v}},\n\\] \\(Z\\) standard normal random variable \\(\\chi^2_v\\) follows chi-squared distribution \\(v\\) degrees freedom.Student’s T distribution continuous probability distribution used statistical inference, particularly estimating population parameters sample size small /population variance unknown. similar normal distribution heavier tails, makes robust small sample sizes.Student’s T distribution symmetric around 0.heavier tails normal distribution, making useful dealing outliers small sample sizes.Density Function\\[\nf(x;v) = \\frac{\\Gamma((v + 1)/2)}{\\sqrt{v \\pi} \\Gamma(v/2)} \\left( 1 + \\frac{x^2}{v} \\right)^{-(v + 1)/2}\n\\]\\(v\\) degrees freedom \\(\\Gamma(x)\\) Gamma function.CDFThe cumulative distribution function Student’s T distribution complex typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) Student’s T distribution exist simple closed form.MeanFor \\(v > 1\\):\\[\nE[X] = 0\n\\]VarianceFor \\(v > 2\\):\\[\n\\sigma^2 =  \\text{Var}(X) = \\frac{v}{v - 2}\n\\]","code":"\nhist(\n    rt(1000, df = 5),\n    main = \"Histogram of Student's T Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"non-central-t-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.11 Non-central T Distribution","text":"non-central t-distribution, denoted \\(T(v, \\lambda)\\), generalization Student’s t-distribution. defined : \\[\nT = \\frac{Z + \\lambda}{\\sqrt{\\chi^2_v / v}},\n\\] \\(Z\\) standard normal random variable, \\(\\chi^2_v\\) follows chi-squared distribution \\(v\\) degrees freedom, \\(\\lambda\\) non-centrality parameter. additional parameter introduces asymmetry distribution.non-central t-distribution arises scenarios null hypothesis hold, alternative hypothesis hypothesis testing. non-centrality parameter \\(\\lambda\\) represents degree mean deviates zero.\\(\\lambda = 0\\), non-central t-distribution reduces Student’s t-distribution.distribution skewed \\(\\lambda \\neq 0\\), skewness increasing \\(\\lambda\\) grows.Density FunctionThe density function non-central t-distribution complex depends \\(v\\) \\(\\lambda\\). can expressed terms infinite sum:\\[\nf(x; v, \\lambda) = \\sum_{k=0}^\\infty \\frac{e^{-\\lambda^2/2}(\\lambda^2/2)^k}{k!} \\cdot \\frac{\\Gamma((v + k + 1)/2)}{\\sqrt{v \\pi} \\Gamma((v + k)/2)} \\left( 1 + \\frac{x^2}{v} \\right)^{-(v + k + 1)/2}.\n\\]PDFCDFThe cumulative distribution function non-central t-distribution typically computed using numerical methods due complexity.MeanFor \\(v > 1\\):\\[\nE[T] = \\lambda \\sqrt{\\frac{v}{2}} \\cdot \\frac{\\Gamma((v - 1)/2)}{\\Gamma(v/2)}.\n\\]VarianceFor \\(v > 2\\):\\[\n\\text{Var}(T) = \\frac{v}{v - 2} + \\lambda^2.\n\\]Comparison: Student’s T vs. Non-central TShape \\(v \\\\infty\\)(df \\(\\\\infty\\))Student’s t-distribution used standard hypothesis testing confidence intervals, non-central t-distribution finds applications scenarios involving non-null hypotheses, power sample size calculations.","code":"\nn <- 100  # Number of samples\ndf <- 5    # Degrees of freedom\nlambda <- 2  # Non-centrality parameter\n\n\nhist(\n  rt(n, df = df, ncp = lambda),\n  main = \"Histogram of Non-central T Distribution\",\n  xlab = \"Value\",\n  ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"f-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.12 F Distribution","text":"F-distribution, denoted \\(F(d_1, d_2)\\), strictly positive used compare variances.Definition: \\[\nF = \\frac{\\chi^2_{d_1} / d_1}{\\chi^2_{d_2} / d_2},\n\\] \\(\\chi^2_{d_1}\\) \\(\\chi^2_{d_2}\\) independent chi-squared random variables degrees freedom \\(d_1\\) \\(d_2\\), respectively.distribution asymmetric never negative.F distribution arises frequently null distribution test statistic, especially context comparing variances, analysis variance (ANOVA).Density Function\\[\nf(x; d_1, d_2) = \\frac{\\sqrt{\\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\\left( \\frac{d_1}{2}, \\frac{d_2}{2} \\right)}, \\quad x > 0\n\\]\\(d_1\\) \\(d_2\\) degrees freedom \\(B\\) beta function.CDFThe cumulative distribution function F distribution typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) F distribution exist simple closed form.MeanFor \\(d_2 > 2\\):\\[\nE[X] = \\frac{d_2}{d_2 - 2}\n\\]VarianceFor \\(d_2 > 4\\):\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}\n\\]","code":"\nhist(\n    rf(1000, df1 = 5, df2 = 2),\n    main = \"Histogram of F Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"cauchy-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.13 Cauchy Distribution","text":"Cauchy distribution continuous probability distribution often used physics heavier tails normal distribution. notable finite mean variance.Cauchy distribution finite mean variance.Central Limit Theorem Weak Law Large Numbers apply Cauchy distribution.Density Function\\[\nf(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\left[ 1 + \\left( \\frac{x - x_0}{\\gamma}\n\\right)^2 \\right]}\n\\]\\(x_0\\) location parameter \\(\\gamma > 0\\) scale parameter.CDFThe cumulative distribution function Cauchy distribution given :\\[\nF(x; x_0, \\gamma) = \\frac{1}{\\pi} \\arctan \\left( \\frac{x - x_0}{\\gamma}  \\right) + \\frac{1}{2}\n\\]PDFMGFThe MGF Cauchy distribution exist.MeanThe mean Cauchy distribution undefined.VarianceThe variance Cauchy distribution undefined.","code":"\nhist(\n    rcauchy(1000, location = 0, scale = 1),\n    main = \"Histogram of Cauchy Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"multivariate-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.14 Multivariate Normal Distribution","text":"Let \\(y\\) \\(p\\)-dimensional multivariate normal (MVN) random variable mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\). density function \\(y\\) given :\\[ f(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mu)' \\Sigma^{-1} (\\mathbf{y}-\\mu)\\right) \\]\\(|\\mathbf{\\Sigma}|\\) represents determinant variance-covariance matrix \\(\\Sigma\\), \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\).Properties:Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{y} \\sim N_r(\\mathbf{\\mu}, \\mathbf{\\Sigma '})\\). Note \\(r \\le p\\), rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma '}\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma^{-1} = G G'}\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G'\\mu}, \\mathbf{})\\) \\(\\mathbf{G'(y - \\mu)} \\sim N_p(\\mathbf{0}, \\mathbf{})\\).fixed linear combination \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c'y}\\), follows \\(\\mathbf{c'y} \\sim N_1(\\mathbf{c'\\mu}, \\mathbf{c'\\Sigma c})\\).Large Sample PropertiesSuppose \\(y_1, \\dots, y_n\\) random sample population mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\):\\[ \\mathbf{Y} \\sim MVN(\\mathbf{\\mu}, \\mathbf{\\Sigma}) \\]:\\(\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{=1}^n \\mathbf{y}_i\\) consistent estimator \\(\\mathbf{\\mu}\\).\\(\\mathbf{S} = \\frac{1}{n-1} \\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})'\\) consistent estimator \\(\\mathbf{\\Sigma}\\).Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\sim N_p(\\mathbf{0}, \\mathbf{\\Sigma})\\) \\(n\\) large relative \\(p\\) (e.g., \\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma/n})\\).Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S^{-1}} (\\bar{\\mathbf{y}} - \\mu) \\sim \\chi^2_{(p)}\\) \\(n\\) large relative \\(p\\).Density Function\\[\nf(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} | \\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\n\\right)\n\\]\\(\\boldsymbol{\\mu}\\) mean vector, \\(\\boldsymbol{\\Sigma}\\) covariance matrix, \\(\\mathbf{x} \\\\mathbb{R}^k\\) \\(k\\) number variables.CDFThe cumulative distribution function multivariate normal distribution simple closed form typically evaluated using numerical methods.PDFMGF\\[\nm_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left(\\boldsymbol{\\mu}^T \\mathbf{t} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t}\n\\right)\n\\]Mean\\[\nE[\\mathbf{X}] = \\boldsymbol{\\mu}\n\\]Variance\\[\n\\text{Var}(\\mathbf{X}) = \\boldsymbol{\\Sigma}\n\\]","code":"\nk <- 2\nn <- 1000\nmu <- c(0, 0)\nsigma <- matrix(c(1, 0.5, 0.5, 1), nrow = k)\nlibrary(MASS)\nhist(\n    mvrnorm(n, mu = mu, Sigma = sigma)[,1],\n    main = \"Histogram of MVN Distribution (1st Var)\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"general-math","chapter":"2 Prerequisites","heading":"2.3 General Math","text":"","code":""},{"path":"prerequisites.html","id":"number-sets","chapter":"2 Prerequisites","heading":"2.3.1 Number Sets","text":"","code":""},{"path":"prerequisites.html","id":"summation-notation-and-series","chapter":"2 Prerequisites","heading":"2.3.2 Summation Notation and Series","text":"","code":""},{"path":"prerequisites.html","id":"chebyshevs-inequality","chapter":"2 Prerequisites","heading":"2.3.2.1 Chebyshev’s Inequality","text":"Let \\(X\\) random variable mean \\(\\mu\\) standard deviation \\(\\sigma\\). positive number \\(k\\), Chebyshev’s Inequality states:\\[\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\n\\]provides probabilistic bound deviation \\(X\\) mean require \\(X\\) follow normal distribution.","code":""},{"path":"prerequisites.html","id":"geometric-sum","chapter":"2 Prerequisites","heading":"2.3.2.2 Geometric Sum","text":"geometric series form \\(\\sum_{k=0}^{n-1} ar^k\\), sum given :\\[\n\\sum_{k=0}^{n-1} ar^k = \\frac{1-r^n}{1-r} \\quad \\text{} r \\neq 1\n\\]","code":""},{"path":"prerequisites.html","id":"infinite-geometric-series","chapter":"2 Prerequisites","heading":"2.3.2.3 Infinite Geometric Series","text":"\\(|r| < 1\\), geometric series converges :\\[\n\\sum_{k=0}^\\infty ar^k = \\frac{}{1-r}\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-theorem","chapter":"2 Prerequisites","heading":"2.3.2.4 Binomial Theorem","text":"binomial expansion \\((x + y)^n\\) :\\[\n(x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} y^k \\quad \\text{} n \\geq 0\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-series","chapter":"2 Prerequisites","heading":"2.3.2.5 Binomial Series","text":"non-integer exponents \\(\\alpha\\):\\[\n\\sum_{k=0}^\\infty \\binom{\\alpha}{k} x^k = (1 + x)^\\alpha \\quad \\text{} |x| < 1\n\\]","code":""},{"path":"prerequisites.html","id":"telescoping-sum","chapter":"2 Prerequisites","heading":"2.3.2.6 Telescoping Sum","text":"telescoping sum simplifies intermediate terms cancel, leaving:\\[\n\\sum_{\\leq k < b} \\Delta F(k) = F(b) - F() \\quad \\text{} , b \\\\mathbb{Z}, \\leq b\n\\]","code":""},{"path":"prerequisites.html","id":"vandermonde-convolution","chapter":"2 Prerequisites","heading":"2.3.2.7 Vandermonde Convolution","text":"Vandermonde convolution identity :\\[\n\\sum_{k=0}^n \\binom{r}{k} \\binom{s}{n-k} = \\binom{r+s}{n} \\quad \\text{} n \\\\mathbb{Z}\n\\]","code":""},{"path":"prerequisites.html","id":"exponential-series","chapter":"2 Prerequisites","heading":"2.3.2.8 Exponential Series","text":"exponential function \\(e^x\\) can represented :\\[\n\\sum_{k=0}^\\infty \\frac{x^k}{k!} = e^x \\quad \\text{} x \\\\mathbb{C}\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-series","chapter":"2 Prerequisites","heading":"2.3.2.9 Taylor Series","text":"Taylor series expansion function \\(f(x)\\) \\(x=\\) :\\[\n\\sum_{k=0}^\\infty \\frac{f^{(k)}()}{k!} (x-)^k = f(x)\n\\]\\(= 0\\), becomes Maclaurin series.","code":""},{"path":"prerequisites.html","id":"maclaurin-series-for-ez","chapter":"2 Prerequisites","heading":"2.3.2.10 Maclaurin Series for \\(e^z\\)","text":"special case Taylor series, Maclaurin expansion \\(e^z\\) :\\[\ne^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots\n\\]","code":""},{"path":"prerequisites.html","id":"eulers-summation-formula","chapter":"2 Prerequisites","heading":"2.3.2.11 Euler’s Summation Formula","text":"Euler’s summation formula connects sums integrals:\\[\n\\sum_{\\leq k < b} f(k) = \\int_a^b f(x) \\, dx + \\sum_{k=1}^m \\frac{B_k}{k!} \\left[f^{(k-1)}(x)\\right]_a^b\n+ (-1)^{m+1} \\int_a^b \\frac{B_m(x-\\lfloor x \\rfloor)}{m!} f^{(m)}(x) \\, dx\n\\], \\(B_k\\) Bernoulli numbers.\\(m=1\\) (Trapezoidal Rule):\\[\n\\sum_{\\leq k < b} f(k) \\approx \\int_a^b f(x) \\, dx - \\frac{1}{2}(f(b) - f())\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-expansion","chapter":"2 Prerequisites","heading":"2.3.3 Taylor Expansion","text":"differentiable function, \\(G(x)\\), can written infinite sum derivatives. specifically, \\(G(x)\\) infinitely differentiable evaluated \\(\\), Taylor expansion :\\[\nG(x) = G() + \\frac{G'()}{1!} (x-) + \\frac{G''()}{2!}(x-)^2 + \\frac{G'''()}{3!}(x-)^3 + \\dots\n\\]expansion valid within radius convergence.","code":""},{"path":"prerequisites.html","id":"law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4 Law of Large Numbers","text":"Let \\(X_1, X_2, \\ldots\\) infinite sequence independent identically distributed (..d.) random variables finite mean \\(\\mu\\) variance \\(\\sigma^2\\). Law Large Numbers (LLN) states sample average:\\[\n\\bar{X}_n = \\frac{1}{n} \\sum_{=1}^n X_i\n\\]converges expected value \\(\\mu\\) \\(n \\rightarrow \\infty\\). can expressed :\\[\n\\bar{X}_n \\rightarrow \\mu \\quad \\text{($n \\rightarrow \\infty$)}.\n\\]","code":""},{"path":"prerequisites.html","id":"variance-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.3.4.1 Variance of the Sample Mean","text":"variance sample mean decreases sample size increases:\\[\nVar(\\bar{X}_n) = Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) = \\frac{\\sigma^2}{n}.\n\\]\\[\n\\begin{aligned}\nVar(\\bar{X}_n) &= Var(\\frac{1}{n}(X_1 + ... + X_n)) =Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2}Var(X_1 + ... + X_n) \\\\\n&=\\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}\n\\end{aligned}\n\\]Note: connection Law Large Numbers Normal Distribution lies Central Limit Theorem. CLT states , regardless original distribution dataset, distribution sample means tend follow normal distribution sample size becomes larger.difference [Weak Law] [Strong Law] regards mode convergence.","code":""},{"path":"prerequisites.html","id":"weak-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.2 Weak Law of Large Numbers","text":"Weak Law Large Numbers states sample average converges probability expected value:\\[\n\\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Formally, \\(\\epsilon > 0\\):\\[\n\\lim_{n \\\\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0.\n\\]Additionally, sample mean ..d. random sample (\\(\\{ X_i \\}_{=1}^n\\)) population finite mean variance consistent estimator population mean \\(\\mu\\):\\[\nplim(\\bar{X}_n) = plim\\left(\\frac{1}{n}\\sum_{=1}^{n} X_i\\right) = \\mu.\n\\]","code":""},{"path":"prerequisites.html","id":"strong-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.3 Strong Law of Large Numbers","text":"Strong Law Large Numbers states sample average converges almost surely expected value:\\[\n\\bar{X}_n \\xrightarrow{.s.} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Equivalently, can expressed :\\[\nP\\left(\\lim_{n \\\\infty} \\bar{X}_n = \\mu\\right) = 1.\n\\]","code":""},{"path":"prerequisites.html","id":"convergence","chapter":"2 Prerequisites","heading":"2.3.5 Convergence","text":"","code":""},{"path":"prerequisites.html","id":"convergence-in-probability","chapter":"2 Prerequisites","heading":"2.3.5.1 Convergence in Probability","text":"\\(n \\rightarrow \\infty\\), estimator (random variable) \\(\\theta_n\\) said converge probability constant \\(c\\) :\\[\n\\lim_{n \\\\infty} P(|\\theta_n - c| \\geq \\epsilon) = 0 \\quad \\text{} \\epsilon > 0.\n\\]denoted :\\[\nplim(\\theta_n) = c \\quad \\text{equivalently, } \\theta_n \\xrightarrow{p} c.\n\\]Properties Convergence Probability:Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\n\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\n\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\n\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\n\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).\n\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).properties extend random vectors matrices.properties extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"convergence-in-distribution","chapter":"2 Prerequisites","heading":"2.3.5.2 Convergence in Distribution","text":"\\(n \\rightarrow \\infty\\), distribution random variable \\(X_n\\) may converge another (“fixed”) distribution. Formally, \\(X_n\\) CDF \\(F_n(x)\\) converges distribution \\(X\\) CDF \\(F(x)\\) :\\[\n\\lim_{n \\\\infty} |F_n(x) - F(x)| = 0\n\\]points continuity \\(F(x)\\). denoted :\\[\nX_n \\xrightarrow{d} X \\quad \\text{equivalently, } F(x) \\text{ limiting distribution } X_n.\n\\]Asymptotic Properties:\\(E(X)\\): Limiting mean (asymptotic mean).\\(Var(X)\\): Limiting variance (asymptotic variance).Note: Limiting expectations variances necessarily match expectations variances \\(X_n\\):\\[\n\\begin{aligned}\nE(X) &\\neq \\lim_{n \\\\infty} E(X_n), \\\\\nAvar(X_n) &\\neq \\lim_{n \\\\infty} Var(X_n).\n\\end{aligned}\n\\]Properties Convergence Distribution:Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\n\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]\\(Y_n \\xrightarrow{d} c\\) (constant), :\n\\(X_n + Y_n \\xrightarrow{d} X + c\\),\n\\(Y_n X_n \\xrightarrow{d} c X\\),\n\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).\n\\(Y_n \\xrightarrow{d} c\\) (constant), :\\(X_n + Y_n \\xrightarrow{d} X + c\\),\\(Y_n X_n \\xrightarrow{d} c X\\),\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).properties also extend random vectors matrices.properties also extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"summary-properties-of-convergence","chapter":"2 Prerequisites","heading":"2.3.5.3 Summary: Properties of Convergence","text":"Relationship Convergence Types:Convergence Probability stronger Convergence Distribution. Therefore:Convergence Distribution guarantee Convergence Probability.","code":""},{"path":"prerequisites.html","id":"sufficient-statistics-and-likelihood","chapter":"2 Prerequisites","heading":"2.3.6 Sufficient Statistics and Likelihood","text":"","code":""},{"path":"prerequisites.html","id":"likelihood","chapter":"2 Prerequisites","heading":"2.3.6.1 Likelihood","text":"likelihood describes degree observed data supports particular value parameter \\(\\theta\\).exact value likelihood meaningful; relative comparisons matter.Likelihood informative comparing parameter values, helping identify values \\(\\theta\\) plausible given data.single observation \\(Y = y\\), likelihood function defined :\\[\nL(\\theta_0; y) = P(Y = y \\mid \\theta = \\theta_0) = f_Y(y; \\theta_0),\n\\]\\(f_Y(y; \\theta_0)\\) probability density (mass) function \\(Y\\) parameter \\(\\theta_0\\).Key Insight: likelihood tells us plausible \\(\\theta\\) , given data observed. probability, proportional probability observing data given parameter value.Example: Suppose \\(Y\\) follows binomial distribution \\(n=10\\) trials probability success \\(p\\):\\[\nP(Y = y \\mid p) = \\binom{10}{y} p^y (1-p)^{10-y}.\n\\]\\(y=7\\) observed successes, likelihood function becomes:\\[\nL(p; y=7) = \\binom{10}{7} p^7 (1-p)^3.\n\\]can use compare well different values \\(p\\) explain observed data.","code":""},{"path":"prerequisites.html","id":"likelihood-ratio","chapter":"2 Prerequisites","heading":"2.3.6.2 Likelihood Ratio","text":"likelihood ratio compares relative likelihood two parameter values \\(\\theta_0\\) \\(\\theta_1\\) given observed data:\\[\n\\text{Likelihood Ratio} = \\frac{L(\\theta_0; y)}{L(\\theta_1; y)}.\n\\]likelihood ratio greater 1 implies \\(\\theta_0\\) likely \\(\\theta_1\\), given observed data.Likelihood ratios widely used hypothesis testing model comparison evaluate evidence null hypothesis.Example: binomial example , consider \\(p_0 = 0.7\\) \\(p_1 = 0.5\\). likelihood ratio :\\[\n\\frac{L(p_0; y=7)}{L(p_1; y=7)} = \\frac{\\binom{10}{7} (0.7)^7 (0.3)^3}{\\binom{10}{7} (0.5)^7 (0.5)^3}.\n\\]simplifies :\\[\n\\frac{(0.7)^7 (0.3)^3}{(0.5)^7 (0.5)^3}.\n\\]likelihood ratio quantifies much likely \\(p_0\\) compared \\(p_1\\) given observed data.","code":""},{"path":"prerequisites.html","id":"likelihood-function","chapter":"2 Prerequisites","heading":"2.3.6.3 Likelihood Function","text":"given sample, likelihood possible values \\(\\theta\\) forms likelihood function:\\[\nL(\\theta) = L(\\theta; y) = f_Y(y; \\theta).\n\\]sample size \\(n\\), assuming independence among observations:\\[\nL(\\theta) = \\prod_{=1}^{n} f_Y(y_i; \\theta).\n\\]Taking natural logarithm likelihood gives log-likelihood function:\\[\nl(\\theta) = \\sum_{=1}^{n} \\log f_Y(y_i; \\theta).\n\\]Log-Likelihood?log-likelihood simplifies computation turning products sums.particularly useful optimization, many numerical methods (e.g., gradient-based algorithms) perform better sums products.Example: \\(Y_1, Y_2, \\dots, Y_n\\) ..d. observations normal distribution \\(N(\\mu, \\sigma^2)\\), likelihood :\\[\nL(\\mu, \\sigma^2) = \\prod_{=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right).\n\\]log-likelihood :\\[\nl(\\mu, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{=1}^n (y_i - \\mu)^2.\n\\]","code":""},{"path":"prerequisites.html","id":"sufficient-statistics","chapter":"2 Prerequisites","heading":"2.3.6.4 Sufficient Statistics","text":"sufficient statistic \\(T(y)\\) summary data retains information parameter \\(\\theta\\). allows us focus condensed statistic without losing inferential power regarding \\(\\theta\\).Formal Definition:statistic \\(T(y)\\) sufficient parameter \\(\\theta\\) conditional probability distribution data \\(y\\), given \\(T(y)\\) \\(\\theta\\), depend \\(\\theta\\). Mathematically:\\[ P(Y = y \\mid T(y), \\theta) = P(Y = y \\mid T(y)). \\]Alternatively, Factorization Theorem, \\(T(y)\\) sufficient likelihood can written :\\[ L(\\theta; y) = c(y) L^*(\\theta; T(y)), \\]:\\(c(y)\\) function data independent \\(\\theta\\).\\(L^*(\\theta; T(y))\\) function depends \\(\\theta\\) \\(T(y)\\).words, likelihood function can rewritten terms \\(T(y)\\) alone, without loss information \\(\\theta\\).Sufficient Statistics Matter:allow us simplify analysis reducing data without losing inferential power.Many inferential procedures (e.g., Maximum Likelihood Estimation, Bayesian methods) simplified working sufficient statistics.Example:Consider sample ..d. observations \\(Y_1, Y_2, \\dots, Y_n\\) normal distribution \\(N(\\mu, \\sigma^2)\\). :sample mean \\(\\bar{Y} = \\frac{1}{n} \\sum_{=1}^n Y_i\\) sufficient \\(\\mu\\).sample variance \\(S^2 = \\frac{1}{n-1} \\sum_{=1}^n (Y_i - \\bar{Y})^2\\) sufficient \\(\\sigma^2\\).Verification: joint density \\(y_1, y_2, \\dots, y_n\\) can factored :\\[\nf(y_1, \\dots, y_n; \\mu, \\sigma^2) = \\underbrace{\\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{=1}^n (y_i - \\bar{y})^2\\right)}_{L^*(\\mu, \\sigma^2; \\bar{y}, s^2)}\n\\cdot \\underbrace{\\text{[independent $\\mu$, $\\sigma^2$]}}_{c(y)}.\n\\]shows \\(\\bar{Y}\\) \\(S^2\\) sufficient.Usage Sufficient StatisticsMaximum Likelihood Estimation (MLE): MLE, sufficient statistics simplify optimization problem reducing data without losing information.\nExample: normal distribution case, \\(\\mu\\) can estimated using sufficient statistic \\(\\bar{Y}\\): \\[\n\\hat{\\mu}_{MLE} = \\bar{Y}.\n\\]Maximum Likelihood Estimation (MLE): MLE, sufficient statistics simplify optimization problem reducing data without losing information.Example: normal distribution case, \\(\\mu\\) can estimated using sufficient statistic \\(\\bar{Y}\\): \\[\n\\hat{\\mu}_{MLE} = \\bar{Y}.\n\\]Bayesian Inference: Bayesian analysis, posterior distribution depends sufficient statistic rather entire data set. normal case: \\[\nP(\\mu \\mid \\bar{Y}) \\propto P(\\mu) L(\\mu; \\bar{Y}).\n\\]Bayesian Inference: Bayesian analysis, posterior distribution depends sufficient statistic rather entire data set. normal case: \\[\nP(\\mu \\mid \\bar{Y}) \\propto P(\\mu) L(\\mu; \\bar{Y}).\n\\]Data Compression: practice, sufficient statistics reduce complexity data storage analysis condensing relevant information smaller representation.Data Compression: practice, sufficient statistics reduce complexity data storage analysis condensing relevant information smaller representation.","code":""},{"path":"prerequisites.html","id":"nuisance-parameters","chapter":"2 Prerequisites","heading":"2.3.6.5 Nuisance Parameters","text":"Parameters direct interest analysis necessary model data called nuisance parameters.Profile Likelihood: handle nuisance parameters, replace maximum likelihood estimates (MLEs) likelihood function, creating profile likelihood parameter interest.Example Profile Likelihood:regression model parameters \\(\\beta\\) (coefficients) \\(\\sigma^2\\) (error variance), \\(\\sigma^2\\) often nuisance parameter. profile likelihood \\(\\beta\\) obtained substituting MLE \\(\\sigma^2\\) likelihood:\\[\nL_p(\\beta) = L(\\beta, \\hat{\\sigma}^2),\n\\]\\(\\hat{\\sigma}^2\\) MLE \\(\\sigma^2\\) given \\(\\beta\\).simplifies problem focus parameter interest, \\(\\beta\\).","code":""},{"path":"prerequisites.html","id":"parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7 Parameter Transformations","text":"Transformations parameters often used improve interpretability statistical properties models.","code":""},{"path":"prerequisites.html","id":"log-odds-transformation","chapter":"2 Prerequisites","heading":"2.3.7.1 Log-Odds Transformation","text":"log-odds transformation commonly used logistic regression binary classification problems. transforms probabilities (bounded 0 1) real line:\\[\n\\text{Log odds} = g(\\theta) = \\ln\\left(\\frac{\\theta}{1-\\theta}\\right),\n\\]\\(\\theta\\) represents probability (e.g., success probability Bernoulli trial).","code":""},{"path":"prerequisites.html","id":"general-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7.2 General Parameter Transformations","text":"parameter \\(\\theta\\) transformation \\(g(\\cdot)\\):\\(\\theta \\(, b)\\), \\(g(\\theta)\\) may map \\(\\theta\\) different range (e.g., \\(\\mathbb{R}\\)).Useful transformations include:\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).\nExponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).\nSquare root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).Exponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).Square root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).Jacobian Adjustment Transformations: transforming parameter Bayesian inference, Jacobian transformation must included ensure proper posterior scaling.","code":""},{"path":"prerequisites.html","id":"applications-of-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7.3 Applications of Parameter Transformations","text":"Improving Interpretability:\nProbabilities can transformed odds log-odds logistic models.\nRates can transformed logarithmically multiplicative effects.\nProbabilities can transformed odds log-odds logistic models.Rates can transformed logarithmically multiplicative effects.Statistical Modeling:\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).\nRegularization simplification complex relationships.\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).Regularization simplification complex relationships.Optimization:\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.","code":""},{"path":"prerequisites.html","id":"data-importexport","chapter":"2 Prerequisites","heading":"2.4 Data Import/Export","text":"R powerful flexible tool data analysis, originally designed -memory statistical computing. imposes several practical limitations, especially handling large datasets.","code":""},{"path":"prerequisites.html","id":"key-limitations-of-r","chapter":"2 Prerequisites","heading":"2.4.1 Key Limitations of R","text":"Single-Core Default Execution\ndefault, R utilizes one CPU core, limiting performance compute-intensive tasks unless parallelization explicitly implemented.Single-Core Default Execution\ndefault, R utilizes one CPU core, limiting performance compute-intensive tasks unless parallelization explicitly implemented.Memory-Based Data Handling\nR loads data RAM. approach becomes problematic working datasets exceed available memory.\nMedium-Sized Files: Fits within typical RAM (1–2 GB). Processing straightforward.\nLarge Files: 2–10 GB. May require memory-efficient coding special packages.\nLarge Files: Exceed 10 GB. Necessitates distributed parallel computing solutions.\nMemory-Based Data Handling\nR loads data RAM. approach becomes problematic working datasets exceed available memory.Medium-Sized Files: Fits within typical RAM (1–2 GB). Processing straightforward.Large Files: 2–10 GB. May require memory-efficient coding special packages.Large Files: Exceed 10 GB. Necessitates distributed parallel computing solutions.","code":""},{"path":"prerequisites.html","id":"solutions-and-workarounds","chapter":"2 Prerequisites","heading":"2.4.2 Solutions and Workarounds","text":"Upgrade HardwareIncrease RAM\nsimple often effective solution moderately large datasets.Leverage High-Performance Computing (HPC) RThere several HPC strategies packages R facilitate working large computationally intensive tasks:Explicit Parallelism\nUse packages like parallel, foreach, doParallel, future, snow define code runs across multiple cores nodes.Explicit Parallelism\nUse packages like parallel, foreach, doParallel, future, snow define code runs across multiple cores nodes.Implicit Parallelism\nCertain functions packages data.table, dplyr, caret internally optimize performance across cores available.Implicit Parallelism\nCertain functions packages data.table, dplyr, caret internally optimize performance across cores available.Large-Memory Computation\nUse memory-efficient structures external memory algorithms.Large-Memory Computation\nUse memory-efficient structures external memory algorithms.MapReduce Paradigm\nUseful distributed computing environments, especially big data infrastructure like Hadoop.MapReduce Paradigm\nUseful distributed computing environments, especially big data infrastructure like Hadoop.Efficient Data LoadingLimit Rows Columns\nUse arguments nrows = functions like read.csv() fread() load subsets large datasets.Use Specialized Packages Large DataIn-Memory Matrix Packages (Single Class Type Support)\npackages interface C++ handle large matrices efficiently:\nbigmemory, biganalytics, bigtabulate, synchronicity, bigalgebra, bigvideo\n-Memory Matrix Packages (Single Class Type Support)\npackages interface C++ handle large matrices efficiently:bigmemory, biganalytics, bigtabulate, synchronicity, bigalgebra, bigvideoOut--Memory Storage (Multiple Class Types)\ndatasets various data types:\nff package: Stores data disk accesses needed, suitable mixed-type columns.\n--Memory Storage (Multiple Class Types)\ndatasets various data types:ff package: Stores data disk accesses needed, suitable mixed-type columns.Handling Large Datasets (>10 GB)data size exceeds capacity single machine, distributed computing becomes necessary:Hadoop Ecosystem Integration\nRHadoop: suite R packages integrate Hadoop framework.\nHadoopStreaming: Enables R scripts used Hadoop mappers reducers.\nRhipe: Provides R-like interface Hadoop, using Google’s Protocol Buffers serialization.\nRHadoop: suite R packages integrate Hadoop framework.HadoopStreaming: Enables R scripts used Hadoop mappers reducers.Rhipe: Provides R-like interface Hadoop, using Google’s Protocol Buffers serialization.","code":""},{"path":"prerequisites.html","id":"medium-size","chapter":"2 Prerequisites","heading":"2.4.3 Medium size","text":"import multiple files directoryTo export single data fileTo export multiple data filesTo convert data file types","code":"\nlibrary(\"rio\")\nstr(import_list(dir()), which = 1)\nexport(data, \"data.csv\")\nexport(data,\"data.dta\")\nexport(data,\"data.txt\")\nexport(data,\"data_cyl.rds\")\nexport(data,\"data.rdata\")\nexport(data,\"data.R\")\nexport(data,\"data.csv.zip\")\nexport(data,\"list.json\")\nexport(list(mtcars = mtcars, iris = iris), \"data_file_type\") \n# where data_file_type should substituted with the extension listed above\n# convert Stata to SPSS\nconvert(\"data.dta\", \"data.sav\")"},{"path":"prerequisites.html","id":"large-size","chapter":"2 Prerequisites","heading":"2.4.4 Large size","text":"","code":""},{"path":"prerequisites.html","id":"cloud-computing-using-aws-for-big-data","chapter":"2 Prerequisites","heading":"2.4.4.1 Cloud Computing: Using AWS for Big Data","text":"Amazon Web Service (AWS): Compute resources can rented approximately $1/hr. Use AWS process large datasets without overwhelming local machine.","code":""},{"path":"prerequisites.html","id":"importing-large-files-as-chunks","chapter":"2 Prerequisites","heading":"2.4.4.2 Importing Large Files as Chunks","text":"","code":""},{"path":"prerequisites.html","id":"using-base-r","chapter":"2 Prerequisites","heading":"2.4.4.2.1 Using Base R","text":"","code":"\nfile_in <- file(\"in.csv\", \"r\")  # Open a connection to the file\nchunk_size <- 100000            # Define chunk size\nx <- readLines(file_in, n = chunk_size)  # Read data in chunks\nclose(file_in)                  # Close the file connection"},{"path":"prerequisites.html","id":"using-the-data.table-package","chapter":"2 Prerequisites","heading":"2.4.4.2.2 Using the data.table Package","text":"","code":"\nlibrary(data.table)\nmydata <- fread(\"in.csv\", header = TRUE)  # Fast and memory-efficient"},{"path":"prerequisites.html","id":"using-the-ff-package","chapter":"2 Prerequisites","heading":"2.4.4.2.3 Using the ff Package","text":"","code":"\nlibrary(ff)\nx <- read.csv.ffdf(\n  file = \"file.csv\",\n  nrow = 10,          # Total rows\n  header = TRUE,      # Include headers\n  VERBOSE = TRUE,     # Display progress\n  first.rows = 10000, # Initial chunk\n  next.rows = 50000,  # Subsequent chunks\n  colClasses = NA\n)"},{"path":"prerequisites.html","id":"using-the-bigmemory-package","chapter":"2 Prerequisites","heading":"2.4.4.2.4 Using the bigmemory Package","text":"","code":"\nlibrary(bigmemory)\nmy_data <- read.big.matrix('in.csv', header = TRUE)"},{"path":"prerequisites.html","id":"using-the-sqldf-package","chapter":"2 Prerequisites","heading":"2.4.4.2.5 Using the sqldf Package","text":"","code":"\nlibrary(sqldf)\nmy_data <- read.csv.sql('in.csv')\n\n# Example: Filtering during import\niris2 <- read.csv.sql(\"iris.csv\", \n    sql = \"SELECT * FROM file WHERE Species = 'setosa'\")"},{"path":"prerequisites.html","id":"using-the-rmysql-package","chapter":"2 Prerequisites","heading":"2.4.4.2.6 Using the RMySQL Package","text":"RQLite packageDownload SQLite, pick “bundle command-line tools managing SQLite database files” Window 10Unzip file, open sqlite3.exe.Type prompt\nsqlite> .cd 'C:\\Users\\data' specify path desired directory\nsqlite> .open database_name.db open database\nimport CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\n\nsqlite> .exit ’re done, exit sqlite program\nsqlite> .cd 'C:\\Users\\data' specify path desired directorysqlite> .open database_name.db open databaseTo import CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\nsqlite> .mode csv specify SQLite next file .csv filesqlite> .import file_name.csv datbase_name import csv file databasesqlite> .exit ’re done, exit sqlite program","code":"\nlibrary(RMySQL)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(\"RSQLite\")\nsetwd(\"\")\ncon <- dbConnect(RSQLite::SQLite(), \"data_base.db\")\ntbl <- tbl(con, \"data_table\")\ntbl %>% \n    filter() %>%\n    select() %>%\n    collect() # to actually pull the data into the workspace\ndbDisconnect(con)"},{"path":"prerequisites.html","id":"using-the-arrow-package","chapter":"2 Prerequisites","heading":"2.4.4.2.7 Using the arrow Package","text":"","code":"\nlibrary(arrow)\ndata <- read_csv_arrow(\"file.csv\")"},{"path":"prerequisites.html","id":"using-the-vroom-package","chapter":"2 Prerequisites","heading":"2.4.4.2.8 Using the vroom Package","text":"","code":"\nlibrary(vroom)\n\n# Import a compressed CSV file\ncompressed <- vroom_example(\"mtcars.csv.zip\")\ndata <- vroom(compressed)"},{"path":"prerequisites.html","id":"using-the-data.table-package-1","chapter":"2 Prerequisites","heading":"2.4.4.2.9 Using the data.table Package","text":"","code":"\ns = fread(\"sample.csv\")"},{"path":"prerequisites.html","id":"comparisons-regarding-storage-space","chapter":"2 Prerequisites","heading":"2.4.4.2.10 Comparisons Regarding Storage Space","text":"dealing large datasets—especially exceeding 10 GB—standard data-loading strategies R can become impractical due memory constraints. One common approach store datasets compressed format .csv.gz, saves disk space preserving compatibility many tools.Compressed files (e.g., csv.gz) especially useful archiving transferring large datasets. However, R typically loads entire dataset memory writing processing, inefficient even infeasible large files.cases, sequential processing becomes essential. Rather reading entire dataset , can process chunks rows, minimizing memory usage.Comparison: read.csv() vs readr::read_csv()readr::read_csv() faster efficient smaller files, limitation large files used skip. skip argument avoid reading skipped rows—simply discards reading. leads redundant /O operations significantly slows performance large files.approach inefficient processing data chunks.read.csv() can read directly file connection, unlike readr::read_csv(), maintains connection state, allowing loop chunks without re-reading prior rows.Example: Sequential Reading ConnectionOccasionally, reading compressed encoded files, might encounter following error:Error (function (con, , n = 1L, size = NA_integer_, signed = TRUE):can read binary connectionThis can occur file correctly interpreted text. workaround explicitly set connection mode binary using \"rb\":Although file() gzfile() generally detect formats automatically, setting mode explicitly can resolve issues behavior inconsistent.","code":"\ntest = ff::read.csv.ffdf(file = \"\")\nobject.size(test) # Highest memory usage\n\ntest1 = data.table::fread(file = \"\")\nobject.size(test1) # Lowest memory usage\n\ntest2 = readr::read_csv(file = \"\")\nobject.size(test2) # Second lowest memory usage\n\ntest3 = vroom::vroom(file = \"\")\nobject.size(test3) # Similar to read_csv\nreadr::read_csv(file, n_max = 100, skip = 0)        # Reads rows 1–100\nreadr::read_csv(file, n_max = 200, skip = 100)      # Re-reads rows 1–100, then reads 101–300\ncon <- gzfile(\"large_file.csv.gz\", open = \"rt\")  # Text mode\nheaders <- read.csv(con, nrows = 1)              # Read column names\nrepeat {\n  chunk <- tryCatch(read.csv(con, nrows = 1000), error = function(e) NULL)\n  if (is.null(chunk) || nrow(chunk) == 0) break\n  # Process 'chunk' here\n}\nclose(con)\ncon <- gzfile(\"large_file.csv.gz\", open = \"rb\")  # Binary mode"},{"path":"prerequisites.html","id":"sequential-processing-for-large-data","chapter":"2 Prerequisites","heading":"2.4.4.3 Sequential Processing for Large Data","text":"","code":"\n# Open file for sequential reading\nfile_conn <- file(\"file.csv\", open = \"r\")\nwhile (TRUE) {\n  # Read a chunk of data\n  data_chunk <- read.csv(file_conn, nrows = 1000)\n  if (nrow(data_chunk) == 0) break  # Stop if no more rows\n  # Process the chunk here\n}\nclose(file_conn)  # Close connection"},{"path":"prerequisites.html","id":"data-manipulation","chapter":"2 Prerequisites","heading":"2.5 Data Manipulation","text":"verbs data manipulationselect: selecting (selecting) columns based names (eg: select columns Q1 Q25)slice: selecting (selecting) rows based position (eg: select rows 1:10)mutate: add derive new columns (variables) based existing columns (eg: create new column expresses measurement cm based existing measure inches)rename: rename variables change column names (eg: change “GraduationRate100” “grad100”)filter: selecting rows based condition (eg: rows gender = Male)arrange: ordering rows based variable(s) numeric alphabetical order (eg: sort descending order Income)sample: take random samples data (eg: sample 80% data create “training” set)summarize: condense aggregate multiple values single summary values (eg: calculate median income age group)group_by: convert tbl grouped tbl operations performed “group”; allows us summarize data apply verbs data groups (eg, gender treatment)pipe: %>%\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudio\npipe takes output function “pipes” first argument next function.\nnew pipe |> identical old one, except certain special cases.\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioThe pipe takes output function “pipes” first argument next function.pipe takes output function “pipes” first argument next function.new pipe |> identical old one, except certain special cases.new pipe |> identical old one, except certain special cases.:= (Walrus operator): similar = , cases want use glue package (.e., dynamic changes variable name left-hand side)Writing function RTunneling{{ (called curly-curly) allows tunnel data-variables arg-variables (.e., function arguments)","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# -----------------------------\n# Data Structures in R\n# -----------------------------\n\n# Create vectors\nx <- c(1, 4, 23, 4, 45)\nn <- c(1, 3, 5)\ng <- c(\"M\", \"M\", \"F\")\n\n# Create a data frame\ndf <- data.frame(n, g)\ndf  # View the data frame\n#>   n g\n#> 1 1 M\n#> 2 3 M\n#> 3 5 F\nstr(df)  # Check its structure\n#> 'data.frame':    3 obs. of  2 variables:\n#>  $ n: num  1 3 5\n#>  $ g: chr  \"M\" \"M\" \"F\"\n\n# Using tibble for cleaner outputs\ndf <- tibble(n, g)\ndf  # View the tibble\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\nstr(df)\n#> tibble [3 × 2] (S3: tbl_df/tbl/data.frame)\n#>  $ n: num [1:3] 1 3 5\n#>  $ g: chr [1:3] \"M\" \"M\" \"F\"\n\n# Create a list\nlst <- list(x, n, g, df)\nlst  # Display the list\n#> [[1]]\n#> [1]  1  4 23  4 45\n#> \n#> [[2]]\n#> [1] 1 3 5\n#> \n#> [[3]]\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> [[4]]\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Name list elements\nlst2 <- list(num = x, size = n, sex = g, data = df)\nlst2  # Named list elements are easier to reference\n#> $num\n#> [1]  1  4 23  4 45\n#> \n#> $size\n#> [1] 1 3 5\n#> \n#> $sex\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> $data\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Another list example with numeric vectors\nlst3 <- list(\n  x = c(1, 3, 5, 7),\n  y = c(2, 2, 2, 4, 5, 5, 5, 6),\n  z = c(22, 3, 3, 3, 5, 10)\n)\nlst3\n#> $x\n#> [1] 1 3 5 7\n#> \n#> $y\n#> [1] 2 2 2 4 5 5 5 6\n#> \n#> $z\n#> [1] 22  3  3  3  5 10\n\n# Find means of list elements\n# One at a time\nmean(lst3$x)\n#> [1] 4\nmean(lst3$y)\n#> [1] 3.875\nmean(lst3$z)\n#> [1] 7.666667\n\n# Using lapply to calculate means\nlapply(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Simplified output with sapply\nsapply(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# Tidyverse alternative: map() function\nmap(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Tidyverse with numeric output: map_dbl()\nmap_dbl(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# -----------------------------\n# Binding Data Frames\n# -----------------------------\n\n# Create tibbles for demonstration\ndat01 <- tibble(x = 1:5, y = 5:1)\ndat02 <- tibble(x = 10:16, y = x / 2)\ndat03 <- tibble(z = runif(5))  # 5 random numbers from (0, 1)\n\n# Row binding\nbind_rows(dat01, dat02, dat01)\n#> # A tibble: 17 × 2\n#>        x     y\n#>    <int> <dbl>\n#>  1     1   5  \n#>  2     2   4  \n#>  3     3   3  \n#>  4     4   2  \n#>  5     5   1  \n#>  6    10   5  \n#>  7    11   5.5\n#>  8    12   6  \n#>  9    13   6.5\n#> 10    14   7  \n#> 11    15   7.5\n#> 12    16   8  \n#> 13     1   5  \n#> 14     2   4  \n#> 15     3   3  \n#> 16     4   2  \n#> 17     5   1\n\n# Add a new identifier column with .id\nbind_rows(dat01, dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 1         1   5  \n#>  2 1         2   4  \n#>  3 1         3   3  \n#>  4 1         4   2  \n#>  5 1         5   1  \n#>  6 2        10   5  \n#>  7 2        11   5.5\n#>  8 2        12   6  \n#>  9 2        13   6.5\n#> 10 2        14   7  \n#> 11 2        15   7.5\n#> 12 2        16   8\n\n# Use named inputs for better identification\nbind_rows(\"dat01\" = dat01, \"dat02\" = dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 dat01     1   5  \n#>  2 dat01     2   4  \n#>  3 dat01     3   3  \n#>  4 dat01     4   2  \n#>  5 dat01     5   1  \n#>  6 dat02    10   5  \n#>  7 dat02    11   5.5\n#>  8 dat02    12   6  \n#>  9 dat02    13   6.5\n#> 10 dat02    14   7  \n#> 11 dat02    15   7.5\n#> 12 dat02    16   8\n\n# Bind a list of data frames\nlist01 <- list(\"dat01\" = dat01, \"dat02\" = dat02)\nbind_rows(list01, .id = \"source\")\n#> # A tibble: 12 × 3\n#>    source     x     y\n#>    <chr>  <int> <dbl>\n#>  1 dat01      1   5  \n#>  2 dat01      2   4  \n#>  3 dat01      3   3  \n#>  4 dat01      4   2  \n#>  5 dat01      5   1  \n#>  6 dat02     10   5  \n#>  7 dat02     11   5.5\n#>  8 dat02     12   6  \n#>  9 dat02     13   6.5\n#> 10 dat02     14   7  \n#> 11 dat02     15   7.5\n#> 12 dat02     16   8\n\n# Column binding\nbind_cols(dat01, dat03)\n#> # A tibble: 5 × 3\n#>       x     y      z\n#>   <int> <int>  <dbl>\n#> 1     1     5 0.0165\n#> 2     2     4 0.295 \n#> 3     3     3 0.211 \n#> 4     4     2 0.656 \n#> 5     5     1 0.457\n\n# -----------------------------\n# String Manipulation\n# -----------------------------\n\nnames <- c(\"Ford, MS\", \"Jones, PhD\", \"Martin, Phd\", \"Huck, MA, MLS\")\n\n# Remove everything after the first comma\nstr_remove(names, pattern = \", [[:print:]]+\")\n#> [1] \"Ford\"   \"Jones\"  \"Martin\" \"Huck\"\n\n# Explanation: [[:print:]]+ matches one or more printable characters\n\n# -----------------------------\n# Reshaping Data\n# -----------------------------\n\n# Wide format data\nwide <- data.frame(\n  name = c(\"Clay\", \"Garrett\", \"Addison\"),\n  test1 = c(78, 93, 90),\n  test2 = c(87, 91, 97),\n  test3 = c(88, 99, 91)\n)\n\n# Long format data\nlong <- data.frame(\n  name = rep(c(\"Clay\", \"Garrett\", \"Addison\"), each = 3),\n  test = rep(1:3, 3),\n  score = c(78, 87, 88, 93, 91, 99, 90, 97, 91)\n)\n\n# Summary statistics\naggregate(score ~ name, data = long, mean)  # Mean score per student\n#>      name    score\n#> 1 Addison 92.66667\n#> 2    Clay 84.33333\n#> 3 Garrett 94.33333\naggregate(score ~ test, data = long, mean)  # Mean score per test\n#>   test    score\n#> 1    1 87.00000\n#> 2    2 91.66667\n#> 3    3 92.66667\n\n# Line plot of scores over tests\nggplot(long,\n       aes(\n           x = factor(test),\n           y = score,\n           color = name,\n           group = name\n       )) +\n    geom_point() +\n    geom_line() +\n    xlab(\"Test\") +\n    ggtitle(\"Test Scores by Student\")\n\n# Reshape wide to long\npivot_longer(wide, test1:test3, names_to = \"test\", values_to = \"score\")\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    test1    78\n#> 2 Clay    test2    87\n#> 3 Clay    test3    88\n#> 4 Garrett test1    93\n#> 5 Garrett test2    91\n#> 6 Garrett test3    99\n#> 7 Addison test1    90\n#> 8 Addison test2    97\n#> 9 Addison test3    91\n\n# Use names_prefix to clean column names\npivot_longer(\n    wide,\n    -name,\n    names_to = \"test\",\n    values_to = \"score\",\n    names_prefix = \"test\"\n)\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    1        78\n#> 2 Clay    2        87\n#> 3 Clay    3        88\n#> 4 Garrett 1        93\n#> 5 Garrett 2        91\n#> 6 Garrett 3        99\n#> 7 Addison 1        90\n#> 8 Addison 2        97\n#> 9 Addison 3        91\n\n# Reshape long to wide with explicit id_cols argument\npivot_wider(\n  long,\n  id_cols = name, \n  names_from = test,\n  values_from = score\n)\n#> # A tibble: 3 × 4\n#>   name      `1`   `2`   `3`\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\n\n# Add a prefix to the resulting columns\npivot_wider(\n  long,\n  id_cols = name,  \n  names_from = test,\n  values_from = score,\n  names_prefix = \"test\"\n)\n#> # A tibble: 3 × 4\n#>   name    test1 test2 test3\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\nlibrary(tidyverse)\n# -----------------------------\n# Writing Functions with {{ }}\n# -----------------------------\n\n# Define a custom function using {{ }}\nget_mean <- function(data, group_var, var_to_mean) {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(mean = mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the function\ndata(\"mtcars\")\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl  mean\n#>   <dbl> <dbl>\n#> 1     4  26.7\n#> 2     6  19.7\n#> 3     8  15.1\n\n# Dynamically name the resulting variable\nget_mean <- function(data, group_var, var_to_mean, prefix = \"mean_of\") {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(\"{prefix}_{{var_to_mean}}\" := mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the modified function\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl mean_of_mpg\n#>   <dbl>       <dbl>\n#> 1     4        26.7\n#> 2     6        19.7\n#> 3     8        15.1"},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"3 Descriptive Statistics","heading":"3 Descriptive Statistics","text":"area interest research, problem solve, relationship investigate, theoretical empirical processes help .Estimand: Defined “quantity scientific interest can calculated population change value depending data collection design used measure (.e., vary sample size, survey design, number non-respondents, follow-efforts).” (Rubin 1996)Examples estimands include:Population meansPopulation variancesCorrelationsFactor loadingsRegression coefficients","code":""},{"path":"descriptive-statistics.html","id":"numerical-measures","chapter":"3 Descriptive Statistics","heading":"3.1 Numerical Measures","text":"differences population sample:SkewnessStandardized 3rd central moment (unitless)\\(m_2 = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\overline{y})^2\\)\\(m_3 = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\overline{y})^3\\)Kurtosis(peakedness tail thickness) Standardized 4th central momentNotes:Order Statistics: \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), \\(y_{(1)} < y_{(2)} < \\ldots < y_{(n)}\\).Order Statistics: \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), \\(y_{(1)} < y_{(2)} < \\ldots < y_{(n)}\\).Coefficient Variation:\nDefined standard deviation divided mean.\nstable, unitless statistic useful comparison.\nCoefficient Variation:Defined standard deviation divided mean.stable, unitless statistic useful comparison.Symmetry:\nSymmetric distributions: Mean = Median; Skewness = 0.\nSkewed Right: Mean > Median; Skewness > 0.\nSkewed Left: Mean < Median; Skewness < 0.\nSymmetry:Symmetric distributions: Mean = Median; Skewness = 0.Skewed Right: Mean > Median; Skewness > 0.Skewed Left: Mean < Median; Skewness < 0.Central Moments:\n\\(\\mu = E(Y)\\)\n\\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\)\n\\(\\mu_3 = E[(Y-\\mu)^3]\\)\n\\(\\mu_4 = E[(Y-\\mu)^4]\\)\nCentral Moments:\\(\\mu = E(Y)\\)\\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\)\\(\\mu_3 = E[(Y-\\mu)^3]\\)\\(\\mu_4 = E[(Y-\\mu)^4]\\)Skewness (\\(\\hat{g_1}\\))Sampling Distribution:\nsamples drawn normal population:\n\\(\\hat{g_1}\\) approximately distributed \\(N(0, \\frac{6}{n})\\) \\(n > 150\\).\n\\(\\hat{g_1}\\) approximately distributed \\(N(0, \\frac{6}{n})\\) \\(n > 150\\).Inference:\nLarge Samples: Inference skewness can based standard normal distribution.\n95% confidence interval \\(g_1\\) given : \\[\n\\hat{g_1} \\pm 1.96 \\sqrt{\\frac{6}{n}}\n\\]\nSmall Samples: small samples, consult special tables :\nSnedecor Cochran (1989), Table 19()\nMonte Carlo test results\n\nLarge Samples: Inference skewness can based standard normal distribution.\n95% confidence interval \\(g_1\\) given : \\[\n\\hat{g_1} \\pm 1.96 \\sqrt{\\frac{6}{n}}\n\\]Small Samples: small samples, consult special tables :\nSnedecor Cochran (1989), Table 19()\nMonte Carlo test results\nSnedecor Cochran (1989), Table 19()Monte Carlo test resultsKurtosis (\\(\\hat{g_2}\\))Definitions Relationships:\nnormal distribution kurtosis \\(g_2^* = 3\\).\nKurtosis often redefined : \\[\ng_2 = \\frac{E[(Y - \\mu)^4]}{\\sigma^4} - 3\n\\] 4th central moment estimated : \\[\nm_4 = \\frac{\\sum_{=1}^n (y_i - \\overline{y})^4}{n}\n\\]\nnormal distribution kurtosis \\(g_2^* = 3\\).\nKurtosis often redefined : \\[\ng_2 = \\frac{E[(Y - \\mu)^4]}{\\sigma^4} - 3\n\\] 4th central moment estimated : \\[\nm_4 = \\frac{\\sum_{=1}^n (y_i - \\overline{y})^4}{n}\n\\]Sampling Distribution:\nlarge samples (\\(n > 1000\\)):\n\\(\\hat{g_2}\\) approximately distributed \\(N(0, \\frac{24}{n})\\).\n\\(\\hat{g_2}\\) approximately distributed \\(N(0, \\frac{24}{n})\\).Inference:\nLarge Samples: Inference kurtosis can use standard normal tables.\nSmall Samples: Refer specialized tables :\nSnedecor Cochran (1989), Table 19(ii)\nGeary (1936)\n\nLarge Samples: Inference kurtosis can use standard normal tables.Small Samples: Refer specialized tables :\nSnedecor Cochran (1989), Table 19(ii)\nGeary (1936)\nSnedecor Cochran (1989), Table 19(ii)Geary (1936)","code":"\n# Generate random data from a normal distribution\ndata <- rnorm(100)\n\n# Load the e1071 package for skewness and kurtosis functions\nlibrary(e1071)\n\n# Calculate skewness\nskewness_value <- skewness(data)\ncat(\"Skewness:\", skewness_value, \"\\n\")\n#> Skewness: 0.3744006\n\n# Calculate kurtosis\nkurtosis_value <- kurtosis(data)\ncat(\"Kurtosis:\", kurtosis_value, \"\\n\")\n#> Kurtosis: 1.759225"},{"path":"descriptive-statistics.html","id":"graphical-measures","chapter":"3 Descriptive Statistics","heading":"3.2 Graphical Measures","text":"following table summarizes key graphical measures along guidance use . detailed explanations, visual examples, sample code discussed table.Tips Selecting Right Plot:Focus Question: comparing groups, investigating correlations, just exploring overall shape data?Focus Question: comparing groups, investigating correlations, just exploring overall shape data?Match Plot Data Type: Continuous vs. categorical data often dictates choice chart.Match Plot Data Type: Continuous vs. categorical data often dictates choice chart.Mind Data Size: plots become cluttered lose clarity large datasets (e.g., stem--leaf), others may less informative data points.Mind Data Size: plots become cluttered lose clarity large datasets (e.g., stem--leaf), others may less informative data points.","code":""},{"path":"descriptive-statistics.html","id":"shape","chapter":"3 Descriptive Statistics","heading":"3.2.1 Shape","text":"Properly labeling graphs essential ensure viewers can easily understand data presented. several examples graphical measures used assess shape dataset.advanced plot types can provide deeper insights data:","code":"\n# Generate random data for demonstration purposes\ndata <- rnorm(100)\n\n# Histogram: A graphical representation of the distribution of a dataset.\nhist(\n    data,\n    labels = TRUE,\n    col = \"grey\",\n    breaks = 12,\n    main = \"Histogram of Random Data\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)\n\n# Interactive Histogram: Using 'highcharter' for a more interactive visualization.\n# pacman::p_load(\"highcharter\")\n# hchart(data, type = \"column\", name = \"Random Data Distribution\")\n\n# Box-and-Whisker Plot: Useful for visualizing the distribution and identifying outliers.\nboxplot(\n    count ~ spray,\n    data = InsectSprays,\n    col = \"lightgray\",\n    main = \"Boxplot of Insect Sprays\",\n    xlab = \"Spray Type\",\n    ylab = \"Count\"\n)\n\n# Notched Boxplot: The notches indicate a confidence interval around the median.\nboxplot(\n    len ~ supp * dose,\n    data = ToothGrowth,\n    notch = TRUE,\n    col = c(\"gold\", \"darkgreen\"),\n    main = \"Tooth Growth by Supplement and Dose\",\n    xlab = \"Supplement and Dose\",\n    ylab = \"Length\"\n)\n# If the notches of two boxes do not overlap, this suggests that the medians differ significantly.\n\n# Stem-and-Leaf Plot: Provides a quick way to visualize the distribution of data.\nstem(data)\n#> \n#>   The decimal point is at the |\n#> \n#>   -1 | 97555\n#>   -1 | 4433221100\n#>   -0 | 9998888877776655555\n#>   -0 | 443333222222221100\n#>    0 | 011111222222233333334444\n#>    0 | 555577777899\n#>    1 | 00111222\n#>    1 | 67\n#>    2 | 01\n\n# Bagplot - A 2D Boxplot Extension: Visualizes the spread and identifies outliers in two-dimensional data.\npacman::p_load(aplpack)\nattach(mtcars)\nbagplot(wt,\n        mpg,\n        xlab = \"Car Weight\",\n        ylab = \"Miles Per Gallon\",\n        main = \"Bagplot of Car Weight vs. Miles Per Gallon\")\ndetach(mtcars)\n# boxplot.matrix(): Creates boxplots for each column in a matrix. Useful for comparing multiple variables.\ngraphics::boxplot.matrix(\n    cbind(\n        Uni05 = (1:100) / 21,\n        Norm = rnorm(100),\n        T5 = rt(100, df = 5),\n        Gam2 = rgamma(100, shape = 2)\n    ),\n    main = \"Boxplot Marix\",\n    notch = TRUE,\n    col = 1:4\n)\n\n# Violin Plot (vioplot()): Combines a boxplot with a density plot, providing more information about the distribution.\nlibrary(\"vioplot\")\nvioplot(data, col = \"lightblue\", main = \"Violin Plot Example\")"},{"path":"descriptive-statistics.html","id":"scatterplot","chapter":"3 Descriptive Statistics","heading":"3.2.2 Scatterplot","text":"Scatterplots useful visualizing relationships two continuous variables. help identify patterns, correlations, outliers.Pairwise Scatterplots: Visualizes relationships pairs variables dataset. especially useful exploring potential correlations.","code":"\npairs(mtcars,\n      main = \"Pairwise Scatterplots\",\n      pch = 19,\n      col = \"blue\")"},{"path":"descriptive-statistics.html","id":"normality-assessment","chapter":"3 Descriptive Statistics","heading":"3.3 Normality Assessment","text":"Normal (Gaussian) distribution plays critical role statistical analyses due theoretical practical applications. Many statistical methods assume normality data, making essential assess whether variable interest follows normal distribution. achieve , utilize Numerical Measures Graphical Assessment.","code":""},{"path":"descriptive-statistics.html","id":"graphical-assessment","chapter":"3 Descriptive Statistics","heading":"3.3.1 Graphical Assessment","text":"Graphical methods provide intuitive way visually inspect normality dataset. One common methods Q-Q plot (quantile-quantile plot). Q-Q plot compares quantiles sample data quantiles theoretical normal distribution. Deviations line indicate departures normality.example using qqnorm qqline functions R assess normality precip dataset, contains precipitation data (inches per year) 70 U.S. cities:InterpretationTheoretical Line: red line represents expected relationship data perfectly normally distributed.Theoretical Line: red line represents expected relationship data perfectly normally distributed.Data Points: dots represent actual empirical data.Data Points: dots represent actual empirical data.points closely align theoretical line, can conclude data likely follow normal distribution. However, noticeable deviations line, particularly systematic patterns (e.g., curves s-shaped patterns), indicate potential departures normality.TipsSmall Deviations: Minor deviations line small datasets uncommon may significantly impact analyses assume normality.Small Deviations: Minor deviations line small datasets uncommon may significantly impact analyses assume normality.Systematic Patterns: Look clear trends, clusters s-shaped curves, suggest skewness heavy tails.Systematic Patterns: Look clear trends, clusters s-shaped curves, suggest skewness heavy tails.Complementary Tests: Always pair graphical methods numerical measures (e.g., Shapiro-Wilk test) make robust conclusion.Complementary Tests: Always pair graphical methods numerical measures (e.g., Shapiro-Wilk test) make robust conclusion.interpreting Q-Q plot, helpful see ideal non-ideal scenarios. illustrative example:Normal Data: Points fall closely along line.Normal Data: Points fall closely along line.Skewed Data: Points systematically deviate line, curving upward downward.Skewed Data: Points systematically deviate line, curving upward downward.Heavy Tails: Points deviate extremes (ends) distribution.Heavy Tails: Points deviate extremes (ends) distribution.combining visual inspection numerical measures, can better understand nature data alignment assumption normality.","code":"\n# Load the required package\npacman::p_load(\"car\")\n\n# Generate a Q-Q plot\nqqnorm(precip,\n       ylab = \"Precipitation [in/yr] for 70 US cities\",\n       main = \"Q-Q Plot of Precipitation Data\")\nqqline(precip, col = \"red\")"},{"path":"descriptive-statistics.html","id":"summary-statistics","chapter":"3 Descriptive Statistics","heading":"3.3.2 Summary Statistics","text":"graphical assessments, Q-Q plots, provide visual indication normality, may always offer definitive conclusion. supplement graphical methods, statistical tests often employed. tests provide quantitative evidence support refute assumption normality. common methods can classified two categories:Methods Based Normal Probability Plot\nCorrelation Coefficient Normal Probability Plots\nShapiro-Wilk Test\nMethods Based Normal Probability PlotCorrelation Coefficient Normal Probability PlotsShapiro-Wilk TestMethods based empirical cumulative distribution function\nAnderson-Darling Test\nKolmogorov-Smirnov Test\nCramer-von Mises Test\nJarque–Bera Test\nMethods based empirical cumulative distribution functionAnderson-Darling TestKolmogorov-Smirnov TestCramer-von Mises TestJarque–Bera Test","code":""},{"path":"descriptive-statistics.html","id":"methods-based-on-normal-probability-plot","chapter":"3 Descriptive Statistics","heading":"3.3.2.1 Methods Based on Normal Probability Plot","text":"","code":""},{"path":"descriptive-statistics.html","id":"correlation-coefficient-with-normal-probability-plots","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.1 Correlation Coefficient with Normal Probability Plots","text":"described Looney Gulledge Jr (1985) Samuel S. Shapiro Francia (1972), method evaluates linearity normal probability plot calculating correlation coefficient ordered sample values \\(y_{()}\\) theoretical normal quantiles \\(m_i^*\\). perfectly linear relationship suggests data follow normal distribution.correlation coefficient, denoted \\(W^*\\), given :\\[\nW^* = \\frac{\\sum_{=1}^{n}(y_{()}-\\bar{y})(m_i^* - 0)}{\\sqrt{\\sum_{=1}^{n}(y_{()}-\\bar{y})^2 \\cdot \\sum_{=1}^{n}(m_i^* - 0)^2}}\n\\]:\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(\\bar{m^*} = 0\\) null hypothesis normality.\\(\\bar{m^*} = 0\\) null hypothesis normality.Pearson product-moment correlation formula can also used evaluate relationship:\\[\n\\hat{\\rho} = \\frac{\\sum_{=1}^{n}(y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum_{=1}^{n}(y_i - \\bar{y})^2 \\cdot \\sum_{=1}^{n}(x_i - \\bar{x})^2}}\n\\]Interpretation:\ncorrelation 1, plot exactly linear, normality assumed.\ncloser correlation 0, stronger evidence reject normality.\nInference \\(W^*\\) requires reference special tables (Looney Gulledge Jr 1985).\ncorrelation 1, plot exactly linear, normality assumed.closer correlation 0, stronger evidence reject normality.Inference \\(W^*\\) requires reference special tables (Looney Gulledge Jr 1985).","code":"\nlibrary(\"EnvStats\")\n\n# Perform Probability Plot Correlation Coefficient (PPCC) Test\ngofTest(data, test = \"ppcc\")$p.value # Probability Plot Correlation Coefficient\n#> [1] 0.8447804"},{"path":"descriptive-statistics.html","id":"shapiro-wilk-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.2 Shapiro-Wilk Test","text":"Shapiro-Wilk test (Samuel Sanford Shapiro Wilk 1965) one widely used tests assessing normality, especially sample sizes \\(n < 2000\\). test evaluates well data’s order statistics match theoretical normal distribution. test statistic, \\(W\\), computed :\\[\nW=\\frac{\\sum_{=1}^{n}a_i x_{()}}{\\sum_{=1}^{n}(x_{()}-\\bar{x})^2}\n\\]\\(n\\): sample size.\\(n\\): sample size.\\(x_{()}\\): \\(\\)-th smallest value sample (ordered data).\\(x_{()}\\): \\(\\)-th smallest value sample (ordered data).\\(\\bar{x}\\): sample mean.\\(\\bar{x}\\): sample mean.\\(a_i\\): Weights derived expected values variances order statistics normal distribution, precomputed based sample size \\(n\\).\\(a_i\\): Weights derived expected values variances order statistics normal distribution, precomputed based sample size \\(n\\).Sensitive :Symmetry\nShapiro-Wilk test assesses whether sample drawn normal distribution, assumes symmetry around mean.\ndata exhibit skewness (lack symmetry), test likely reject null hypothesis normality.\nShapiro-Wilk test assesses whether sample drawn normal distribution, assumes symmetry around mean.data exhibit skewness (lack symmetry), test likely reject null hypothesis normality.Heavy Tails\nHeavy tails refer distributions extreme values (outliers) likely compared normal distribution.\nShapiro-Wilk test also sensitive departures normality heavy tails affect spread variance, central calculation test statistic \\(W\\).\nHeavy tails refer distributions extreme values (outliers) likely compared normal distribution.Shapiro-Wilk test also sensitive departures normality heavy tails affect spread variance, central calculation test statistic \\(W\\).Hence, Shapiro-Wilk test’s sensitivity deviations makes powerful tool detecting non-normality small moderate-sized samples. However:generally sensitive symmetry (skewness) tail behavior (kurtosis).generally sensitive symmetry (skewness) tail behavior (kurtosis).large samples, even small deviations symmetry tail behavior may cause test reject null hypothesis, even data practically “normal” intended analysis.\nSmall sample sizes may lack power detect deviations normality.\nLarge sample sizes may detect minor deviations practically significant.\nlarge samples, even small deviations symmetry tail behavior may cause test reject null hypothesis, even data practically “normal” intended analysis.Small sample sizes may lack power detect deviations normality.Small sample sizes may lack power detect deviations normality.Large sample sizes may detect minor deviations practically significant.Large sample sizes may detect minor deviations practically significant.Key Steps:Sort Data: Arrange sample data ascending order, yielding \\(x_{(1)}, x_{(2)}, \\dots, x_{(n)}\\).Sort Data: Arrange sample data ascending order, yielding \\(x_{(1)}, x_{(2)}, \\dots, x_{(n)}\\).Compute Weights: weights \\(a_i\\) determined using covariance matrix normal order statistics. optimized maximize power test.Compute Weights: weights \\(a_i\\) determined using covariance matrix normal order statistics. optimized maximize power test.Calculate \\(W\\): Use formula determine \\(W\\), ranges 0 1.Calculate \\(W\\): Use formula determine \\(W\\), ranges 0 1.Decision Rule:Null Hypothesis (\\(H_0\\)): data follows normal distribution.Null Hypothesis (\\(H_0\\)): data follows normal distribution.Alternative Hypothesis (\\(H_1\\)): data follow normal distribution.Alternative Hypothesis (\\(H_1\\)): data follow normal distribution.small \\(W\\) value, along \\(p\\)-value chosen significance level (e.g., 0.05), leads rejection \\(H_0\\).\nnormality, \\(W\\) approaches 1.\nSmaller values \\(W\\) indicate deviations normality.\nsmall \\(W\\) value, along \\(p\\)-value chosen significance level (e.g., 0.05), leads rejection \\(H_0\\).normality, \\(W\\) approaches 1.normality, \\(W\\) approaches 1.Smaller values \\(W\\) indicate deviations normality.Smaller values \\(W\\) indicate deviations normality.","code":"\n# Perform Shapiro-Wilk Test (Default for gofTest)\nEnvStats::gofTest(mtcars$mpg, test = \"sw\")\n#> $distribution\n#> [1] \"Normal\"\n#> \n#> $dist.abb\n#> [1] \"norm\"\n#> \n#> $distribution.parameters\n#>      mean        sd \n#> 20.090625  6.026948 \n#> \n#> $n.param.est\n#> [1] 2\n#> \n#> $estimation.method\n#> [1] \"mvue\"\n#> \n#> $statistic\n#>         W \n#> 0.9475647 \n#> \n#> $sample.size\n#> [1] 32\n#> \n#> $parameters\n#>  n \n#> 32 \n#> \n#> $z.value\n#> [1] 1.160703\n#> \n#> $p.value\n#> [1] 0.1228814\n#> \n#> $alternative\n#> [1] \"True cdf does not equal the\\n                                 Normal Distribution.\"\n#> \n#> $method\n#> [1] \"Shapiro-Wilk GOF\"\n#> \n#> $data\n#>  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n#> [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n#> [31] 15.0 21.4\n#> \n#> $data.name\n#> [1] \"mtcars$mpg\"\n#> \n#> $bad.obs\n#> [1] 0\n#> \n#> attr(,\"class\")\n#> [1] \"gof\""},{"path":"descriptive-statistics.html","id":"methods-based-on-empirical-cumulative-distribution-function","chapter":"3 Descriptive Statistics","heading":"3.3.2.2 Methods Based on Empirical Cumulative Distribution Function","text":"Empirical Cumulative Distribution Function (ECDF) way represent distribution sample dataset cumulative terms. answers question:“fraction observations dataset less equal given value \\(x\\)?”ECDF defined :\\[\nF_n(x) = \\frac{1}{n} \\sum_{=1}^{n} \\mathbb{}(X_i \\leq x)\n\\]:\\(F_(x)\\): ECDF value \\(x\\).\\(F_(x)\\): ECDF value \\(x\\).\\(n\\): Total number data points.\\(n\\): Total number data points.\\(\\mathbb{}(X_i \\leq x)\\): Indicator function, equal 1 \\(X_i \\leq x\\), otherwise 0.\\(\\mathbb{}(X_i \\leq x)\\): Indicator function, equal 1 \\(X_i \\leq x\\), otherwise 0.method especially useful large sample sizes can applied distributions beyond normal (Gaussian) distribution.Properties ECDFStep Function: ECDF step function increases \\(1/n\\) data point.Non-decreasing: \\(x\\) increases, \\(F_n(x)\\) never decreases.Range: ECDF starts 0 ends 1:\n\\(F_n(x) = 0\\) \\(x < \\min(X)\\).\n\\(F_n(x) = 1\\) \\(x \\geq \\max(X)\\).\n\\(F_n(x) = 0\\) \\(x < \\min(X)\\).\\(F_n(x) = 1\\) \\(x \\geq \\max(X)\\).Convergence: \\(n \\\\infty\\), ECDF approaches true cumulative distribution function (CDF) population.Let’s consider sample dataset \\(\\{3, 7, 7, 10, 15\\}\\). ECDF different values \\(x\\) calculated :Applications ECDFGoodness--fit Tests: Compare ECDF theoretical CDF (e.g., using Kolmogorov-Smirnov test).Goodness--fit Tests: Compare ECDF theoretical CDF (e.g., using Kolmogorov-Smirnov test).Outlier Detection: Analyze cumulative trends spot unusual data points.Outlier Detection: Analyze cumulative trends spot unusual data points.Visual Data Exploration: Use ECDF understand spread, skewness, distribution data.Visual Data Exploration: Use ECDF understand spread, skewness, distribution data.Comparing Distributions: Compare ECDFs two datasets assess differences distributions.Comparing Distributions: Compare ECDFs two datasets assess differences distributions.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Sample dataset\ndata <- c(3, 7, 7, 10, 15)\n\n# ECDF calculation\necdf_function <- ecdf(data)\n\n# Generate a data frame for plotting\necdf_data <- data.frame(x = sort(unique(data)),\n                        ecdf = sapply(sort(unique(data)), function(x)\n                          mean(data <= x)))\n\n# Display ECDF values\nprint(ecdf_data)\n#>    x ecdf\n#> 1  3  0.2\n#> 2  7  0.6\n#> 3 10  0.8\n#> 4 15  1.0\n\n# Plot the ECDF\nggplot(ecdf_data, aes(x = x, y = ecdf)) +\n  geom_step() +\n  labs(\n    title = \"Empirical Cumulative Distribution Function\",\n    x = \"Data Values\",\n    y = \"Cumulative Proportion\"\n  ) +\n  theme_minimal()\n# Alternatively\nplot.ecdf(as.numeric(mtcars[1, ]),\n          verticals = TRUE,\n          do.points = FALSE)"},{"path":"descriptive-statistics.html","id":"anderson-darling-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.1 Anderson-Darling Test","text":"Anderson-Darling test statistic (T. W. Anderson Darling 1952) given :\\[\n^2 = \\int_{-\\infty}^{\\infty} \\frac{\\left(F_n(t) - F(t)\\right)^2}{F(t)(1 - F(t))} dF(t)\n\\]test calculates weighted average squared deviations empirical cumulative distribution function (CDF), \\(F_n(t)\\), theoretical CDF, \\(F(t)\\). weight given deviations tails distribution, makes test particularly sensitive regions.sample size \\(n\\), ordered observations \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\), Anderson-Darling test statistic can also written :\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\ln(F(y_{()})) + (2n + 1 - 2i) \\ln(1 - F(y_{()})) \\right]\n\\]normal distribution, test statistic simplified. Using transformation:\\[\np_i = \\Phi\\left(\\frac{y_{()} - \\bar{y}}{s}\\right),\n\\]:\\(p_i\\) cumulative probability standard normal distribution,\\(p_i\\) cumulative probability standard normal distribution,\\(y_{()}\\) ordered sample values,\\(y_{()}\\) ordered sample values,\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(s\\) sample standard deviation,\\(s\\) sample standard deviation,formula becomes:\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\ln(p_i) + (2n + 1 - 2i) \\ln(1 - p_i) \\right].\n\\]Key Features TestCDF-Based Weighting: Anderson-Darling test gives weight deviations tails, makes particularly sensitive detecting non-normality regions.CDF-Based Weighting: Anderson-Darling test gives weight deviations tails, makes particularly sensitive detecting non-normality regions.Sensitivity: Compared goodness--fit tests, Kolmogorov-Smirnov Test, Anderson-Darling test better identifying differences tails distribution.Sensitivity: Compared goodness--fit tests, Kolmogorov-Smirnov Test, Anderson-Darling test better identifying differences tails distribution.Integral Form: test statistic can also expressed integral theoretical CDF: \\[\n^2 = n \\int_{-\\infty}^\\infty \\frac{\\left[F_n(t) - F(t)\\right]^2}{F(t)(1 - F(t))} dF(t),\n\\] \\(F_n(t)\\) empirical CDF, \\(F(t)\\) specified theoretical CDF.Integral Form: test statistic can also expressed integral theoretical CDF: \\[\n^2 = n \\int_{-\\infty}^\\infty \\frac{\\left[F_n(t) - F(t)\\right]^2}{F(t)(1 - F(t))} dF(t),\n\\] \\(F_n(t)\\) empirical CDF, \\(F(t)\\) specified theoretical CDF.Applications:\nTesting normality distributions (e.g., exponential, Weibull).\nValidating assumptions statistical models.\nComparing data theoretical distributions.\nApplications:Testing normality distributions (e.g., exponential, Weibull).Validating assumptions statistical models.Comparing data theoretical distributions.Hypothesis TestingNull Hypothesis (\\(H_0\\)): data follows specified distribution (e.g., normal distribution).Alternative Hypothesis (\\(H_1\\)): data follow specified distribution.null hypothesis rejected \\(^2\\) large, indicating poor fit specified distribution.Critical values test statistic provided (Marsaglia Marsaglia 2004) (Stephens 1974).Applications DistributionsThe Anderson-Darling test can applied various distributions using specific transformation methods. Examples include:ExponentialExponentialLogisticLogisticGumbelGumbelExtreme-valueExtreme-valueWeibull (logarithmic transformation: \\(\\log(\\text{Weibull}) = \\text{Gumbel}\\))Weibull (logarithmic transformation: \\(\\log(\\text{Weibull}) = \\text{Gumbel}\\))GammaGammaCauchyCauchyvon Misesvon MisesLog-normal (two-parameter)Log-normal (two-parameter)details transformations critical values, consult (Stephens 1974).Alternatively, broader range distributions, use gofTest function gof package:","code":"\n# Perform Anderson-Darling Test\nlibrary(nortest)\nad_test_result <- ad.test(mtcars$mpg)\n\n# Output the test statistic and p-value\nad_test_result\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  mtcars$mpg\n#> A = 0.57968, p-value = 0.1207\n# General goodness-of-fit test with Anderson-Darling\nlibrary(EnvStats)\ngof_test_result <- EnvStats::gofTest(mtcars$mpg, test = \"ad\")\n\n# Extract the p-value\ngof_test_result$p.value\n#> [1] 0.1207371"},{"path":"descriptive-statistics.html","id":"kolmogorov-smirnov-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.2 Kolmogorov-Smirnov Test","text":"Kolmogorov-Smirnov (K-S) test nonparametric test compares empirical cumulative distribution function (ECDF) sample theoretical cumulative distribution function (CDF), compares ECDFs two samples. used assess whether sample comes specific distribution (one-sample test) compare two samples (two-sample test).test statistic \\(D_n\\) one-sample test defined :\\[\nD_n = \\sup_x \\left| F_n(x) - F(x) \\right|,\n\\]:\\(F_n(x)\\) empirical CDF sample,\\(F_n(x)\\) empirical CDF sample,\\(F(x)\\) theoretical CDF null hypothesis,\\(F(x)\\) theoretical CDF null hypothesis,\\(\\sup_x\\) denotes supremum (largest value) possible values \\(x\\).\\(\\sup_x\\) denotes supremum (largest value) possible values \\(x\\).two-sample K-S test, statistic :\\[\nD_{n,m} = \\sup_x \\left| F_{n,1}(x) - F_{m,2}(x) \\right|,\n\\]\\(F_{n,1}(x)\\) \\(F_{m,2}(x)\\) empirical CDFs two samples, sizes \\(n\\) \\(m\\), respectively.HypothesesNull hypothesis (\\(H_0\\)): sample comes specified distribution (one-sample) two samples drawn distribution (two-sample).Alternative hypothesis (\\(H_1\\)): sample come specified distribution (one-sample) two samples drawn different distributions (two-sample).PropertiesBased Largest Deviation: K-S test sensitive largest absolute difference empirical expected CDFs, making effective detecting shifts location scale.Based Largest Deviation: K-S test sensitive largest absolute difference empirical expected CDFs, making effective detecting shifts location scale.Distribution-Free: test assume specific distribution data null hypothesis. significance level determined distribution test statistic null hypothesis.Distribution-Free: test assume specific distribution data null hypothesis. significance level determined distribution test statistic null hypothesis.Limitations:\ntest sensitive near center distribution tails.\nmay perform well discrete data small sample sizes.\nLimitations:test sensitive near center distribution tails.may perform well discrete data small sample sizes.Related Tests:\nKuiper’s Test: variation K-S test sensitive deviations center tails distribution. Kuiper test statistic : \\[\nV_n = D^+ + D^-,\n\\] \\(D^+\\) \\(D^-\\) maximum positive negative deviations empirical CDF theoretical CDF.\nRelated Tests:Kuiper’s Test: variation K-S test sensitive deviations center tails distribution. Kuiper test statistic : \\[\nV_n = D^+ + D^-,\n\\] \\(D^+\\) \\(D^-\\) maximum positive negative deviations empirical CDF theoretical CDF.ApplicationsTesting normality specified distributions.Comparing two datasets determine drawn distribution.perform one-sample K-S test R, use ks.test() function. check goodness fit specific distribution, gofTest() function package like DescTools can also used.Advantages:\nSimple widely applicable.\nDistribution-free null hypothesis.\nAdvantages:Simple widely applicable.Simple widely applicable.Distribution-free null hypothesis.Distribution-free null hypothesis.Limitations:\nSensitive sample size: small deviations may lead significance large samples.\nReduced sensitivity differences tails compared Anderson-Darling test.\nLimitations:Sensitive sample size: small deviations may lead significance large samples.Sensitive sample size: small deviations may lead significance large samples.Reduced sensitivity differences tails compared Anderson-Darling test.Reduced sensitivity differences tails compared Anderson-Darling test.Kolmogorov-Smirnov test provides general-purpose method goodness--fit testing sample comparison, particular utility detecting central deviations.","code":"\n# One-sample Kolmogorov-Smirnov test for normality\ndata <- rnorm(50)  # Generate random normal data\nks.test(data, \"pnorm\", mean(data), sd(data))\n#> \n#>  Exact one-sample Kolmogorov-Smirnov test\n#> \n#> data:  data\n#> D = 0.085345, p-value = 0.8294\n#> alternative hypothesis: two-sided\n\n# Goodness-of-fit test using gofTest\nlibrary(DescTools)\ngofTest(data, test = \"ks\")$p.value  # Kolmogorov-Smirnov test p-value\n#> [1] 0.8294042"},{"path":"descriptive-statistics.html","id":"cramer-von-mises-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.3 Cramer-von Mises Test","text":"Cramer-von Mises (CVM) test nonparametric goodness--fit test evaluates agreement empirical cumulative distribution function (ECDF) sample specified theoretical cumulative distribution function (CDF). Unlike Kolmogorov-Smirnov test, focuses largest discrepancy, Cramer-von Mises test considers average squared discrepancy across entire distribution. Unlike Anderson-Darling test, weights parts distribution equally.test statistic \\(W^2\\) one-sample Cramer-von Mises test defined :\\[\nW^2 = n \\int_{-\\infty}^\\infty \\left[ F_n(t) - F(t) \\right]^2 dF(t),\n\\]:\\(F_n(t)\\) empirical CDF,\\(F_n(t)\\) empirical CDF,\\(F(t)\\) specified theoretical CDF null hypothesis,\\(F(t)\\) specified theoretical CDF null hypothesis,\\(n\\) sample size.\\(n\\) sample size.practice, \\(W^2\\) computed using ordered sample values \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\) :\\[\nW^2 = \\sum_{=1}^n \\left( F(y_{()}) - \\frac{2i - 1}{2n} \\right)^2 + \\frac{1}{12n},\n\\]:\\(F(y_{()})\\) theoretical CDF evaluated ordered sample values \\(y_{()}\\).HypothesesNull hypothesis (H0): sample data follow specified distribution.Alternative hypothesis (H1): sample data follow specified distribution.PropertiesFocus Average Discrepancy: Cramer-von Mises test measures overall goodness--fit considering squared deviations across points distribution, ensuring equal weighting discrepancies.Focus Average Discrepancy: Cramer-von Mises test measures overall goodness--fit considering squared deviations across points distribution, ensuring equal weighting discrepancies.Comparison Anderson-Darling: Unlike Anderson-Darling test, gives weight deviations tails, CVM test weights parts distribution equally.Comparison Anderson-Darling: Unlike Anderson-Darling test, gives weight deviations tails, CVM test weights parts distribution equally.Integral Representation: statistic expressed integral squared differences empirical theoretical CDFs.Integral Representation: statistic expressed integral squared differences empirical theoretical CDFs.Two-Sample Test: Cramer-von Mises framework can also extended compare two empirical CDFs. two-sample statistic based pooled empirical CDF.Two-Sample Test: Cramer-von Mises framework can also extended compare two empirical CDFs. two-sample statistic based pooled empirical CDF.ApplicationsAssessing goodness--fit theoretical distribution (e.g., normal, exponential, Weibull).Comparing two datasets determine drawn similar distributions.Validating model assumptions.perform Cramer-von Mises test R, gofTest() function DescTools package can used. example:Advantages:\nConsiders discrepancies across entire distribution.\nRobust outliers due equal weighting.\nSimple compute interpret.\nAdvantages:Considers discrepancies across entire distribution.Considers discrepancies across entire distribution.Robust outliers due equal weighting.Robust outliers due equal weighting.Simple compute interpret.Simple compute interpret.Limitations:\nLess sensitive deviations tails compared Anderson-Darling test.\nMay less powerful Kolmogorov-Smirnov test detecting central shifts.\nLimitations:Less sensitive deviations tails compared Anderson-Darling test.Less sensitive deviations tails compared Anderson-Darling test.May less powerful Kolmogorov-Smirnov test detecting central shifts.May less powerful Kolmogorov-Smirnov test detecting central shifts.","code":"\n# Generate random normal data\ndata <- rnorm(50)\n\n# Perform the Cramer-von Mises test\nlibrary(DescTools)\ngofTest(data, test = \"cvm\")$p.value  # Cramer-von Mises test p-value\n#> [1] 0.5938969"},{"path":"descriptive-statistics.html","id":"jarquebera-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.4 Jarque-Bera Test","text":"Jarque-Bera (JB) test (Bera Jarque 1981) goodness--fit test used check whether dataset follows normal distribution. based skewness kurtosis data, measure asymmetry “tailedness” distribution, respectively.Jarque-Bera test statistic defined :\\[\nJB = \\frac{n}{6}\\left(S^2 + \\frac{(K - 3)^2}{4}\\right),\n\\]:\\(n\\) sample size,\\(n\\) sample size,\\(S\\) sample skewness,\\(S\\) sample skewness,\\(K\\) sample kurtosis.\\(K\\) sample kurtosis.Skewness (\\(S\\)) calculated :\\[\nS = \\frac{\\hat{\\mu}_3}{\\hat{\\sigma}^3} = \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^3}{\\left(\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\right)^{3/2}},\n\\]:\\(\\hat{\\mu}_3\\) third central moment,\\(\\hat{\\mu}_3\\) third central moment,\\(\\hat{\\sigma}\\) standard deviation,\\(\\hat{\\sigma}\\) standard deviation,\\(\\bar{x}\\) sample mean.\\(\\bar{x}\\) sample mean.Kurtosis (\\(K\\)) calculated :\\[\nK = \\frac{\\hat{\\mu}_4}{\\hat{\\sigma}^4} = \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^4}{\\left(\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\right)^2},\n\\]:\\(\\hat{\\mu}_4\\) fourth central moment.HypothesisNull hypothesis (\\(H_0\\)): data follow normal distribution, implying:\nSkewness \\(S = 0\\),\nExcess kurtosis \\(K - 3 = 0\\).\nSkewness \\(S = 0\\),Excess kurtosis \\(K - 3 = 0\\).Alternative hypothesis (\\(H_1\\)): data follow normal distribution.Distribution JB StatisticUnder null hypothesis, Jarque-Bera statistic asymptotically follows chi-squared distribution 2 degrees freedom:\\[\nJB \\sim \\chi^2_2.\n\\]PropertiesSensitivity:\nSkewness (\\(S\\)) captures asymmetry data.\nKurtosis (\\(K\\)) measures heavy-tailed light-tailed distribution compared normal distribution.\nSkewness (\\(S\\)) captures asymmetry data.Kurtosis (\\(K\\)) measures heavy-tailed light-tailed distribution compared normal distribution.Limitations:\ntest sensitive large sample sizes; even small deviations normality may result rejection \\(H_0\\).\nAssumes data independently identically distributed.\ntest sensitive large sample sizes; even small deviations normality may result rejection \\(H_0\\).Assumes data independently identically distributed.ApplicationsTesting normality regression residuals.Validating distributional assumptions econometrics time series analysis.Jarque-Bera test can performed R using tseries package:","code":"\nlibrary(tseries)\n\n# Generate a sample dataset\ndata <- rnorm(100)  # Normally distributed data\n\n# Perform the Jarque-Bera test\njarque.bera.test(data)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  data\n#> X-squared = 1.8829, df = 2, p-value = 0.3901"},{"path":"descriptive-statistics.html","id":"bivariate-statistics","chapter":"3 Descriptive Statistics","heading":"3.4 Bivariate Statistics","text":"Bivariate statistics involve analysis relationships two variables. Understanding relationships can provide insights patterns, associations, (suggestive ) causal connections. , explore correlation different types variables:Two Continuous VariablesTwo Discrete VariablesCategorical Continuous VariablesBefore delving analysis, critical consider following:relationship linear non-linear?\nLinear relationships can modeled simpler statistical methods Pearson’s correlation, non-linear relationships may require alternative approaches, Spearman’s rank correlation regression transformations.\nLinear relationships can modeled simpler statistical methods Pearson’s correlation, non-linear relationships may require alternative approaches, Spearman’s rank correlation regression transformations.variable continuous, normal homoskedastic?\nparametric methods like Pearson’s correlation, assumptions normality homoskedasticity (equal variance) must met. assumptions fail, non-parametric methods like Spearman’s correlation robust alternatives preferred.\nparametric methods like Pearson’s correlation, assumptions normality homoskedasticity (equal variance) must met. assumptions fail, non-parametric methods like Spearman’s correlation robust alternatives preferred.big dataset?\nLarge datasets can reveal subtle patterns may lead statistically significant results practically meaningful. smaller datasets, careful selection statistical methods essential ensure reliability validity.\nLarge datasets can reveal subtle patterns may lead statistically significant results practically meaningful. smaller datasets, careful selection statistical methods essential ensure reliability validity.Chi-squared TestPhi CoefficientCramer’s VTschuprow’s TSpearman’s Rank CorrelationKendall’s TauGamma StatisticFreeman’s ThetaEpsilon-squaredGoodman Kruskal’s GammaSomers’ DKendall’s Tau-bYule’s Q YTetrachoric CorrelationPolychoric CorrelationPoint-Biserial CorrelationLogistic RegressionPearson CorrelationSpearman Correlation","code":""},{"path":"descriptive-statistics.html","id":"two-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.1 Two Continuous","text":"","code":"\nset.seed(1)\nn = 100 # (sample size)\n\ndata = data.frame(A = sample(1:20, replace = TRUE, size = n),\n                  B = sample(1:30, replace = TRUE, size = n))"},{"path":"descriptive-statistics.html","id":"pearson-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.1 Pearson Correlation","text":"Pearson correlation quantifies strength direction linear relationship two continuous variables.Formula:\\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\cdot \\sum (y_i - \\bar{y})^2}}\n\\] \\(x_i, y_i\\): Individual data points variables \\(X\\) \\(Y\\).\\(x_i, y_i\\): Individual data points variables \\(X\\) \\(Y\\).\\(\\bar{x}, \\bar{y}\\): Means \\(X\\) \\(Y\\).\\(\\bar{x}, \\bar{y}\\): Means \\(X\\) \\(Y\\).Assumptions:relationship variables linear.Variables normally distributed.Data exhibits homoscedasticity (equal variance \\(Y\\) values \\(X\\)).Use Case:Use relationship expected linear, assumptions normality homoscedasticity met.Interpretation:\\(r = +1\\): Perfect positive linear relationship.\\(r = -1\\): Perfect negative linear relationship.\\(r = 0\\): linear relationship.","code":"\n# Pearson correlation\npearson_corr <- stats::cor(data$A, data$B, method = \"pearson\")\ncat(\"Pearson Correlation (r):\", pearson_corr, \"\\n\")\n#> Pearson Correlation (r): 0.02394939"},{"path":"descriptive-statistics.html","id":"spearman-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.2 Spearman Correlation","text":"Spearman correlation measures strength monotonic relationship two variables. ranks data calculates correlation based ranks.Formula:\\[\n\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 -1)}\n\\]\\(d_i\\): Difference ranks \\(x_i\\) \\(y_i\\).\\(n\\): Number paired observations.Assumptions:Relationship must monotonic, necessarily linear.Relationship must monotonic, necessarily linear.assumptions distribution variables.assumptions distribution variables.Use Case:Use data ordinal normality linearity assumptions violated.Interpretation:\\(\\rho = +1\\): Perfect positive monotonic relationship.\\(\\rho = +1\\): Perfect positive monotonic relationship.\\(\\rho = -1\\): Perfect negative monotonic relationship.\\(\\rho = -1\\): Perfect negative monotonic relationship.\\(\\rho = 0\\): monotonic relationship.\\(\\rho = 0\\): monotonic relationship.","code":"\n# Spearman correlation\nspearman_corr <- stats::cor(data$A, data$B, method = \"spearman\")\ncat(\"Spearman Correlation (rho):\", spearman_corr, \"\\n\")\n#> Spearman Correlation (rho): 0.02304636"},{"path":"descriptive-statistics.html","id":"kendalls-tau-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.3 Kendall’s Tau Correlation","text":"Kendall’s Tau measures strength monotonic relationship comparing concordant discordant pairs.Formula:\\[\n\\tau = \\frac{(C- D)}{\\binom{n}{2}}\n\\]​\\(C\\): Number concordant pairs (ranks \\(X\\) \\(Y\\) increase decrease together).\\(C\\): Number concordant pairs (ranks \\(X\\) \\(Y\\) increase decrease together).\\(D\\): Number discordant pairs (one rank increases decreases).\\(D\\): Number discordant pairs (one rank increases decreases).\\(\\binom{n}{2}\\): Total number possible pairs.\\(\\binom{n}{2}\\): Total number possible pairs.Assumptions:specific assumptions data distribution.specific assumptions data distribution.Measures monotonic relationships.Measures monotonic relationships.Use Case:Preferred small datasets data contains outliers.Interpretation:\\(\\tau = +1\\): Perfect positive monotonic relationship.\\(\\tau = +1\\): Perfect positive monotonic relationship.\\(\\tau = -1\\): Perfect negative monotonic relationship.\\(\\tau = -1\\): Perfect negative monotonic relationship.\\(\\tau = 0\\): monotonic relationship.\\(\\tau = 0\\): monotonic relationship.","code":"\n# Kendall's Tau correlation\nkendall_corr <- stats::cor(data$A, data$B, method = \"kendall\")\ncat(\"Kendall's Tau Correlation (tau):\", kendall_corr, \"\\n\")\n#> Kendall's Tau Correlation (tau): 0.02171284"},{"path":"descriptive-statistics.html","id":"distance-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.4 Distance Correlation","text":"Distance Correlation measures linear non-linear relationships variables. require monotonicity linearity.Formula:\\[\nd Cor = \\frac{d Cov(X,Y)}{\\sqrt{d Var (X) \\cdot d Var (Y)}}\n\\]​\\(dCov\\): Distance covariance \\(X\\) \\(Y\\).\\(dCov\\): Distance covariance \\(X\\) \\(Y\\).\\(dVar\\): Distance variances \\(X\\) \\(Y\\).\\(dVar\\): Distance variances \\(X\\) \\(Y\\).Assumptions:specific assumptions relationship (linear, monotonic, otherwise).Use Case:Use complex relationships, including non-linear patterns.Interpretation:\\(dCor = 0\\): association.\\(dCor = 0\\): association.\\(dCor = 1\\): Perfect association.\\(dCor = 1\\): Perfect association.","code":"\n# Distance correlation\ndistance_corr <- energy::dcor(data$A, data$B)\ncat(\"Distance Correlation (dCor):\", distance_corr, \"\\n\")\n#> Distance Correlation (dCor): 0.1008934"},{"path":"descriptive-statistics.html","id":"summary-table-of-correlation-methods","chapter":"3 Descriptive Statistics","heading":"3.4.1.5 Summary Table of Correlation Methods","text":"","code":""},{"path":"descriptive-statistics.html","id":"categorical-and-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.2 Categorical and Continuous","text":"Analyzing relationship categorical variable (binary multi-class) continuous variable requires specialized techniques. methods assess whether categorical variable significantly influences continuous variable vice versa.focus following methods:Point-Biserial CorrelationLogistic Regression[Analysis Variance (ANOVA)]T-test","code":""},{"path":"descriptive-statistics.html","id":"point-biserial-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.2.1 Point-Biserial Correlation","text":"Point-Biserial Correlation special case Pearson correlation used assess relationship binary categorical variable (coded 0 1) continuous variable. measures strength direction linear relationship.Formula:\\[\nr_{pb} = \\frac{\\bar{Y_1} - \\bar{Y_0}}{s_Y} \\sqrt{\\frac{n_1 n_0}{n^2}}\n\\]\\(\\bar{Y_1}\\), \\(\\bar{Y_0}\\): Mean continuous variable groups coded 1 0, respectively.\\(\\bar{Y_1}\\), \\(\\bar{Y_0}\\): Mean continuous variable groups coded 1 0, respectively.\\(s_Y\\): Standard deviation continuous variable.\\(s_Y\\): Standard deviation continuous variable.\\(n_1, n_0\\): Number observations group (1 0).\\(n_1, n_0\\): Number observations group (1 0).\\(n\\): Total number observations.\\(n\\): Total number observations.Key Properties:Range: \\(-1\\) \\(1\\).\n\\(r_{pb} = +1\\): Perfect positive correlation.\n\\(r_{pb} = -1\\): Perfect negative correlation.\n\\(r_{pb} = 0\\): linear relationship.\n\\(r_{pb} = +1\\): Perfect positive correlation.\\(r_{pb} = -1\\): Perfect negative correlation.\\(r_{pb} = 0\\): linear relationship.positive \\(r_{pb}\\) indicates higher values continuous variable associated 1 group, negative \\(r_{pb}\\) indicates opposite.Assumptions:binary variable truly dichotomous (e.g., male/female, success/failure).continuous variable approximately normally distributed.Homogeneity variance across two groups (strictly required recommended).Use Case:evaluate linear relationship binary categorical variable continuous variable.","code":"\nlibrary(ltm)\n# Point-Biserial Correlation\nbiserial_corr <- ltm::biserial.cor(\n  c(12.5, 15.3, 10.7, 18.1, 11.2, 16.8, 13.4, 14.9), \n  c(0, 1, 0, 1, 0, 1, 0, 1), \n  use = \"all.obs\", \n  level = 2\n)\ncat(\"Point-Biserial Correlation:\", biserial_corr, \"\\n\")\n#> Point-Biserial Correlation: 0.8792835"},{"path":"descriptive-statistics.html","id":"logistic-regression","chapter":"3 Descriptive Statistics","heading":"3.4.2.2 Logistic Regression","text":"Logistic Regression models relationship binary categorical variable (dependent variable) one independent variables (may include continuous variables). predicts probability binary outcome (e.g., success/failure, yes/).Refer 3.4.2.2 detail.Formula:logistic regression model represented :\\[\n\\text{logit}(p) = \\ln \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 X\n\\]\\(p\\): Probability outcome 1.\\(p\\): Probability outcome 1.\\(\\beta_0\\): Intercept.\\(\\beta_0\\): Intercept.\\(\\beta_1\\): Coefficient continuous variable \\(X\\).\\(\\beta_1\\): Coefficient continuous variable \\(X\\).\\(\\text{logit}(p)\\): Log-odds probability.\\(\\text{logit}(p)\\): Log-odds probability.Key Features:Output: Odds ratio probability binary outcome.Output: Odds ratio probability binary outcome.Can include multiple predictors (continuous categorical).Can include multiple predictors (continuous categorical).Non-linear transformation ensures predictions probabilities 0 1.Non-linear transformation ensures predictions probabilities 0 1.Assumptions:dependent variable binary.dependent variable binary.Observations independent.Observations independent.linear relationship logit dependent variable independent variable.linear relationship logit dependent variable independent variable.multicollinearity predictors.multicollinearity predictors.Use Case:predict likelihood binary outcome based continuous predictor (e.g., probability success given test scores).","code":"\n# Simulated data\nset.seed(123)\nx <- rnorm(100, mean = 50, sd = 10)  # Continuous predictor\ny <- ifelse(x > 55, 1, 0)  # Binary outcome based on threshold\n\n# Logistic Regression\nlogistic_model <- glm(y ~ x, family = binomial)\nsummary(logistic_model)\n#> \n#> Call:\n#> glm(formula = y ~ x, family = binomial)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  -3749.9   495083.0  -0.008    0.994\n#> x               67.9     8966.6   0.008    0.994\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1.2217e+02  on 99  degrees of freedom\n#> Residual deviance: 1.4317e-07  on 98  degrees of freedom\n#> AIC: 4\n#> \n#> Number of Fisher Scoring iterations: 25\n\n# Predicted probabilities\npredicted_probs <- predict(logistic_model, type = \"response\")\nprint(head(predicted_probs))\n#>         1         2         3         4         5         6 \n#> -735.6466 -511.3844  703.2134 -307.2281 -267.3187  809.3747\n# Visualize logistic regression curve\nlibrary(ggplot2)\ndata <- data.frame(x = x, y = y, predicted = predicted_probs)\n\nggplot(data, aes(x = x, y = predicted)) +\n    geom_point(aes(y = y), color = \"red\", alpha = 0.5) +\n    geom_line(color = \"blue\") +\n    labs(title = \"Logistic Regression: Continuous vs Binary\",\n         x = \"Continuous Predictor\", y = \"Predicted Probability\")"},{"path":"descriptive-statistics.html","id":"summary-table-of-methods-between-categorical-and-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.2.3 Summary Table of Methods (Between Categorical and Continuous)","text":"","code":""},{"path":"descriptive-statistics.html","id":"two-discrete","chapter":"3 Descriptive Statistics","heading":"3.4.3 Two Discrete","text":"analyzing relationship two discrete variables (categorical ordinal), various methods available quantify degree association similarity. methods can broadly classified :Distance MetricsDistance MetricsStatistical MetricsStatistical Metrics","code":""},{"path":"descriptive-statistics.html","id":"distance-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.1 Distance Metrics","text":"Distance metrics measure dissimilarity two discrete variables often used proxy correlation specific applications like clustering machine learning.","code":""},{"path":"descriptive-statistics.html","id":"euclidean-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.1 Euclidean Distance","text":"\\[\nd(x, y) = \\sqrt{\\sum_{=1}^n (x_i - y_i)^2}\n\\]Measures straight-line distance two variables Euclidean space.Measures straight-line distance two variables Euclidean space.Sensitive scaling; variables normalized meaningful comparisons.Sensitive scaling; variables normalized meaningful comparisons.","code":""},{"path":"descriptive-statistics.html","id":"manhattan-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.2 Manhattan Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n |x_i - y_i|\n\\]Measures distance summing absolute differences along dimension.Measures distance summing absolute differences along dimension.Also called L1 norm; often used grid-based problems.Also called L1 norm; often used grid-based problems.","code":""},{"path":"descriptive-statistics.html","id":"chebyshev-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.3 Chebyshev Distance","text":"\\[\nd(x, y) = \\max_{=1}^n |x_i - y_i|\n\\]Measures maximum single-step distance along dimension.Measures maximum single-step distance along dimension.Useful discrete, grid-based problems (e.g., chess moves).Useful discrete, grid-based problems (e.g., chess moves).","code":""},{"path":"descriptive-statistics.html","id":"minkowski-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.4 Minkowski Distance","text":"\\[\nd(x, y) = \\left( \\sum_{=1}^n |x_i - y_i|^p \\right)^{1/p}\n\\]Generalized distance metric. Special cases include:\n\\(p = 1\\): Manhattan Distance.\n\\(p = 2\\): Euclidean Distance.\n\\(p \\\\infty\\): Chebyshev Distance.\nGeneralized distance metric. Special cases include:\\(p = 1\\): Manhattan Distance.\\(p = 1\\): Manhattan Distance.\\(p = 2\\): Euclidean Distance.\\(p = 2\\): Euclidean Distance.\\(p \\\\infty\\): Chebyshev Distance.\\(p \\\\infty\\): Chebyshev Distance.","code":""},{"path":"descriptive-statistics.html","id":"canberra-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.5 Canberra Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n \\frac{|x_i - y_i|}{|x_i| + |y_i|}\n\\]Emphasizes proportional differences, making sensitive smaller values.","code":""},{"path":"descriptive-statistics.html","id":"hamming-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.6 Hamming Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n (x_i \\neq y_i)\n\\]Counts number differing positions two sequences.Counts number differing positions two sequences.Widely used text similarity binary data.Widely used text similarity binary data.","code":""},{"path":"descriptive-statistics.html","id":"cosine-similarity-and-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.7 Cosine Similarity and Distance","text":"\\[\n\\text{Cosine Similarity} = \\frac{\\sum_{=1}^n x_i y_i}{\\sqrt{\\sum_{=1}^n x_i^2} \\cdot \\sqrt{\\sum_{=1}^n y_i^2}}\n\\]\\[\n\\text{Cosine Distance} = 1 - \\text{Cosine Similarity}\n\\]Measures angle two vectors high-dimensional space.Measures angle two vectors high-dimensional space.Often used text document similarity.Often used text document similarity.","code":""},{"path":"descriptive-statistics.html","id":"sum-of-absolute-differences","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.8 Sum of Absolute Differences","text":"\\[\nd(x, y) = \\sum_{=1}^n |x_i - y_i|\n\\]Equivalent Manhattan Distance without coordinate context.","code":""},{"path":"descriptive-statistics.html","id":"sum-of-squared-differences","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.9 Sum of Squared Differences","text":"\\[\nd(x, y) = \\sum_{=1}^n (x_i - y_i)^2\n\\]Equivalent squared Euclidean Distance.","code":""},{"path":"descriptive-statistics.html","id":"mean-absolute-error","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.10 Mean Absolute Error","text":"\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^n |x_i - y_i|\n\\]Measures average absolute differences.","code":"\n# Example data\nx <- c(1, 2, 3, 4, 5)\ny <- c(2, 3, 4, 5, 6)\n\n# Compute distances\neuclidean <- sqrt(sum((x - y)^2))\nmanhattan <- sum(abs(x - y))\nchebyshev <- max(abs(x - y))\nhamming <- sum(x != y)\ncosine_similarity <- sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)))\ncosine_distance <- 1 - cosine_similarity\n\n# Display results\ncat(\"Euclidean Distance:\", euclidean, \"\\n\")\n#> Euclidean Distance: 2.236068\ncat(\"Manhattan Distance:\", manhattan, \"\\n\")\n#> Manhattan Distance: 5\ncat(\"Chebyshev Distance:\", chebyshev, \"\\n\")\n#> Chebyshev Distance: 1\ncat(\"Hamming Distance:\", hamming, \"\\n\")\n#> Hamming Distance: 5\ncat(\"Cosine Distance:\", cosine_distance, \"\\n\")\n#> Cosine Distance: 0.005063324"},{"path":"descriptive-statistics.html","id":"statistical-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.2 Statistical Metrics","text":"","code":""},{"path":"descriptive-statistics.html","id":"chi-squared-test","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.1 Chi-squared Test","text":"Chi-Squared Test evaluates whether two categorical variables independent comparing observed expected frequencies contingency table.Formula:\\[\n\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\\(O_i\\): Observed frequency cell table.\\(O_i\\): Observed frequency cell table.\\(E_i\\): Expected frequency assumption independence.\\(E_i\\): Expected frequency assumption independence.Steps:Construct contingency table observed counts.Construct contingency table observed counts.Compute expected frequencies: \\(E_{ij} = \\frac{\\text{Row Total}_i \\cdot \\text{Column Total}_j}{\\text{Grand Total}}\\)Compute expected frequencies: \\(E_{ij} = \\frac{\\text{Row Total}_i \\cdot \\text{Column Total}_j}{\\text{Grand Total}}\\)Apply Chi-squared formula.Apply Chi-squared formula.Compare \\(\\chi^2\\) critical value Chi-squared distribution.Compare \\(\\chi^2\\) critical value Chi-squared distribution.Assumptions:Observations independent.Observations independent.Expected frequencies \\(\\geq 5\\) least 80% cells.Expected frequencies \\(\\geq 5\\) least 80% cells.Use Case:Tests independence two nominal variables.","code":"\n# Example data\ndt <- matrix(c(15, 25, 20, 40), nrow = 2)\nrownames(dt) <- c(\"Group A\", \"Group B\")\ncolnames(dt) <- c(\"Category 1\", \"Category 2\")\n\n# Perform Chi-Squared Test\nchi_sq_test <- chisq.test(dt)\nprint(chi_sq_test)\n#> \n#> Results of Hypothesis Test\n#> --------------------------\n#> \n#> Alternative Hypothesis:          \n#> \n#> Test Name:                       Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> Data:                            dt\n#> \n#> Test Statistic:                  X-squared = 0.04578755\n#> \n#> Test Statistic Parameter:        df = 1\n#> \n#> P-value:                         0.8305625"},{"path":"descriptive-statistics.html","id":"phi-coefficient","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.2 Phi Coefficient","text":"Phi Coefficient measure association two binary variables, derived Chi-squared statistic.Formula:\\[\n\\phi = \\frac{\\chi^2}{n}\n\\]\\(n\\): Total sample size.Interpretation:\\(\\phi = 0\\): association.\\(\\phi = 0\\): association.\\(\\phi = +1\\): Perfect positive association.\\(\\phi = +1\\): Perfect positive association.\\(\\phi = -1\\): Perfect negative association.\\(\\phi = -1\\): Perfect negative association.Use Case:Suitable 2x2 contingency tables.2 binary","code":"\nlibrary(psych)\n\n# Compute Phi Coefficient\nphi_coeff <- phi(dt)\ncat(\"Phi Coefficient:\", phi_coeff, \"\\n\")\n#> Phi Coefficient: 0.04"},{"path":"descriptive-statistics.html","id":"cramers-v","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.3 Cramer’s V","text":"Cramer’s V generalizes Phi coefficient handle contingency tables two rows columns.Formula:\\[\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(r-1, c-1)}}\n\\]​​\\(r\\): Number rows.\\(r\\): Number rows.\\(c\\): Number columns.\\(c\\): Number columns.Assumptions:Variables nominal.Variables nominal.Suitable larger contingency tables.Suitable larger contingency tables.Use Case:Measures strength association nominal variables natural order.Alternatively,ncchisq noncentral Chi-squarencchisq noncentral Chi-squarenchisqadj Adjusted noncentral Chi-squarenchisqadj Adjusted noncentral Chi-squarefisher Fisher Z transformationfisher Fisher Z transformationfisheradj bias correction Fisher z transformationfisheradj bias correction Fisher z transformation","code":"\nlibrary(lsr)\n\n# Simulate data\nset.seed(1)\ndata <- data.frame(\n  A = sample(1:5, replace = TRUE, size = 100),  # Nominal variable\n  B = sample(1:6, replace = TRUE, size = 100)  # Nominal variable\n)\n\n# Compute Cramer's V\ncramers_v <- cramersV(data$A, data$B)\ncat(\"Cramer's V:\", cramers_v, \"\\n\")\n#> Cramer's V: 0.1944616\nDescTools::CramerV(data, conf.level = 0.95,method = \"ncchisqadj\")\n#>  Cramer V    lwr.ci    upr.ci \n#> 0.3472325 0.3929964 0.4033053"},{"path":"descriptive-statistics.html","id":"adjusted-cramers-v","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.4 Adjusted Cramer’s V","text":"Adjusted versions Cramer’s V correct bias, especially small samples.Adjusted formulas account non-central Chi-squared bias correction. Examples include:Non-central Chi-squared: \\(V_{adj} = \\sqrt{\\frac{\\chi^2_{nc} / n}{\\min(r-1, c-1)}}\\)​Non-central Chi-squared: \\(V_{adj} = \\sqrt{\\frac{\\chi^2_{nc} / n}{\\min(r-1, c-1)}}\\)​Bias Correction: \\(V_{adj} = V - \\text{Bias Term}\\)Bias Correction: \\(V_{adj} = V - \\text{Bias Term}\\)","code":"\nlibrary(DescTools)\n\n# Compute Adjusted Cramer's V\ncramers_v_adj <- CramerV(data, conf.level = 0.95, method = \"ncchisqadj\")\ncramers_v_adj\n#>  Cramer V    lwr.ci    upr.ci \n#> 0.3472325 0.3929964 0.4033053"},{"path":"descriptive-statistics.html","id":"tschuprows-t","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.5 Tschuprow’s T","text":"Tschuprow’s T symmetric measure association nominal variables. differs Cramer’s V considering product rows columns, making less sensitive asymmetrical tables.Formula:\\[\nT = \\sqrt{\\frac{\\chi^2/n}{\\sqrt{(r-1)(c-1)}}}\n\\]Assumptions:Applicable nominal variables.Applicable nominal variables.Suitable contingency tables unequal dimensions.Suitable contingency tables unequal dimensions.Use Case:Preferred table dimensions highly unequal.","code":"\n# Compute Tschuprow's T\ntschuprow_t <- DescTools::TschuprowT(data$A, data$B)\ncat(\"Tschuprow's T:\", tschuprow_t, \"\\n\")\n#> Tschuprow's T: 0.1839104"},{"path":"descriptive-statistics.html","id":"ordinal-association-rank-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.6 Ordinal Association (Rank correlation)","text":"least one variable ordinal, rank-based methods appropriate respect order categories. methods often used relationships monotonic (increasing decreasing consistently) necessarily linear.","code":""},{},{},{},{},{},{},{},{},{},{},{},{"path":"descriptive-statistics.html","id":"general-approach-to-bivariate-statistics","chapter":"3 Descriptive Statistics","heading":"3.4.4 General Approach to Bivariate Statistics","text":"Get correlation table continuous variables onlyAlternatively, can also theComparing correlations different types variables (e.g., continuous vs. categorical) poses unique challenges. One key issue ensuring methods appropriate nature variables analyzed. Another challenge lies detecting non-linear relationships, traditional correlation measures, Pearson’s correlation coefficient, designed assess linear associations.address challenges, potential solution utilize mutual information information theory. Mutual information quantifies much knowing one variable reduces uncertainty another, providing general measure association accommodates linear non-linear relationships.","code":"\nlibrary(tidyverse)\n\ndata(\"mtcars\")\ndf = mtcars %>%\n    dplyr::select(cyl, vs, carb)\n\n\ndf_factor = df %>%\n    dplyr::mutate(\n        cyl = factor(cyl),\n        vs = factor(vs),\n        carb = factor(carb)\n    )\n# summary(df)\nstr(df)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#>  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#>  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nstr(df_factor)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n#>  $ vs  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 1 2 1 2 2 2 ...\n#>  $ carb: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 4 1 1 2 1 4 2 2 4 ...\ncor(df)\n#>             cyl         vs       carb\n#> cyl   1.0000000 -0.8108118  0.5269883\n#> vs   -0.8108118  1.0000000 -0.5696071\n#> carb  0.5269883 -0.5696071  1.0000000\n\n# only complete obs\n# cor(df, use = \"complete.obs\")\nHmisc::rcorr(as.matrix(df), type = \"pearson\")\n#>        cyl    vs  carb\n#> cyl   1.00 -0.81  0.53\n#> vs   -0.81  1.00 -0.57\n#> carb  0.53 -0.57  1.00\n#> \n#> n= 32 \n#> \n#> \n#> P\n#>      cyl    vs     carb  \n#> cyl         0.0000 0.0019\n#> vs   0.0000        0.0007\n#> carb 0.0019 0.0007\nmodelsummary::datasummary_correlation(df)\nggcorrplot::ggcorrplot(cor(df))"},{"path":"descriptive-statistics.html","id":"approximating-mutual-information","chapter":"3 Descriptive Statistics","heading":"3.4.4.1 Approximating Mutual Information","text":"can approximate mutual information using following relationship:\\[\n\\downarrow \\text{Prediction Error} \\approx \\downarrow \\text{Uncertainty} \\approx \\uparrow \\text{Association Strength}\n\\]principle underpins X2Y metric, implemented following steps:Predict \\(y\\) without \\(x\\) (baseline model):\n\\(y\\) continuous, predict mean \\(y\\).\n\\(y\\) categorical, predict mode \\(y\\).\nPredict \\(y\\) without \\(x\\) (baseline model):\\(y\\) continuous, predict mean \\(y\\).\\(y\\) categorical, predict mode \\(y\\).Predict \\(y\\) \\(x\\) using model (e.g., linear regression, random forest, etc.).Predict \\(y\\) \\(x\\) using model (e.g., linear regression, random forest, etc.).Calculate difference prediction error steps 1 2. difference reflects reduction uncertainty \\(y\\) \\(x\\) included, serving measure association strength.Calculate difference prediction error steps 1 2. difference reflects reduction uncertainty \\(y\\) \\(x\\) included, serving measure association strength.","code":""},{"path":"descriptive-statistics.html","id":"generalizing-across-variable-types","chapter":"3 Descriptive Statistics","heading":"3.4.4.2 Generalizing Across Variable Types","text":"construct comprehensive framework handles different variable combinations, :Continuous vs. continuousCategorical vs. continuousContinuous vs. categoricalCategorical vs. categoricala flexible modeling approach required. Classification Regression Trees (CART) particularly well-suited purpose, can accommodate continuous categorical predictors outcomes. However, models, random forests generalized additive models (GAMs), may also employed.","code":""},{"path":"descriptive-statistics.html","id":"limitations-of-the-approach","chapter":"3 Descriptive Statistics","heading":"3.4.4.3 Limitations of the Approach","text":"Despite strengths, approach limitations:Asymmetry:\nmeasure symmetric, meaning \\((x, y) \\neq (y, x)\\).Asymmetry:\nmeasure symmetric, meaning \\((x, y) \\neq (y, x)\\).Comparability:\nDifferent variable pairs may yield metrics directly comparable. instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), categorical outcomes use measures like misclassification error.Comparability:\nDifferent variable pairs may yield metrics directly comparable. instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), categorical outcomes use measures like misclassification error.limitations considered interpreting results, especially multi-variable mixed-data contexts.Alternatively,general form,heat map correlation timeMore elaboration ggplot2","code":"\nlibrary(ppsr)\nlibrary(tidyverse)\n\niris <- iris %>% \n  dplyr::select(1:3)\n\n# ppsr::score_df(iris) # if you want a dataframe\nppsr::score_matrix(iris,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>              Sepal.Length Sepal.Width Petal.Length\n#> Sepal.Length   1.00000000  0.04632352    0.5491398\n#> Sepal.Width    0.06790301  1.00000000    0.2376991\n#> Petal.Length   0.61608360  0.24263851    1.0000000\n\n# if you want a similar correlation matrix\nppsr::score_matrix(df,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>             cyl        vs      carb\n#> cyl  1.00000000 0.3982789 0.2092533\n#> vs   0.02514286 1.0000000 0.2000000\n#> carb 0.30798148 0.2537309 1.0000000\ncorrplot::corrplot(cor(df))\nPerformanceAnalytics::chart.Correlation(df, histogram = T, pch = 19)\nheatmap(as.matrix(df))\nppsr::visualize_pps(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_correlations(\n    df = iris\n)\nppsr::visualize_both(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_pps(\n    df = iris,\n    color_value_high = 'red',\n    color_value_low = 'yellow',\n    color_text = 'black'\n) +\n    ggplot2::theme_classic() +\n    ggplot2::theme(plot.background = \n                       ggplot2::element_rect(fill = \"lightgrey\")) +\n    ggplot2::theme(title = ggplot2::element_text(size = 15)) +\n    ggplot2::labs(\n        title = 'Correlation aand Heatmap',\n        subtitle = 'Subtitle',\n        caption = 'Caption',\n        x = 'More info'\n    )"},{"path":"basic-statistical-inference.html","id":"basic-statistical-inference","chapter":"4 Basic Statistical Inference","heading":"4 Basic Statistical Inference","text":"Statistical inference involves drawing conclusions population parameters based sample data. two primary goals inference :Making inferences true parameter value (\\(\\beta\\)) based estimator estimate:\ninvolves interpreting sample-derived estimate understand population parameter.\nExamples include estimating population means, variances, proportions.\ninvolves interpreting sample-derived estimate understand population parameter.Examples include estimating population means, variances, proportions.Testing whether underlying assumptions hold true, including:\nAssumptions true population parameters (e.g., \\(\\mu\\), \\(\\sigma^2\\)).\nAssumptions random variables (e.g., independence, normality).\nAssumptions model specification (e.g., linearity regression).\nAssumptions true population parameters (e.g., \\(\\mu\\), \\(\\sigma^2\\)).Assumptions random variables (e.g., independence, normality).Assumptions model specification (e.g., linearity regression).Note: Statistical testing :Confirm absolute certainty hypothesis true false.Confirm absolute certainty hypothesis true false.Interpret magnitude estimated value economic, practical, business contexts without additional analysis.\nStatistical significance: Refers whether observed effect unlikely due chance.\nPractical significance: Focuses real-world importance effect.\nInterpret magnitude estimated value economic, practical, business contexts without additional analysis.Statistical significance: Refers whether observed effect unlikely due chance.Statistical significance: Refers whether observed effect unlikely due chance.Practical significance: Focuses real-world importance effect.Practical significance: Focuses real-world importance effect.Example:marketing campaign increases sales \\(0.5\\%\\), statistically significant (\\(p < 0.05\\)). However, small market, may lack practical significance.Instead, inference provides framework making probabilistic statements population parameters, given sample data.","code":""},{"path":"basic-statistical-inference.html","id":"hypothesis-testing-framework","chapter":"4 Basic Statistical Inference","heading":"4.1 Hypothesis Testing Framework","text":"Hypothesis testing one fundamental tools statistics. provides formal procedure test claims assumptions (hypotheses) population parameters using sample data. process essential various fields, including business, medicine, social sciences, helps answer questions like “new marketing strategy improve sales?” “significant difference test scores two teaching methods?”goal hypothesis testing make decisions draw conclusions population based sample data. necessary rarely access entire population. example, company wants determine whether new advertising campaign increases sales, might analyze data sample stores rather every store globally.Key Steps Hypothesis TestingFormulate Hypotheses: Define null alternative hypotheses.Choose Significance Level (\\(\\alpha\\)): Determine acceptable probability making Type error.Select Test Statistic: Identify appropriate statistical test based data hypotheses.Define Rejection Region: Specify range values null hypothesis rejected.Compute Test Statistic: Use sample data calculate test statistic.Make Decision: Compare test statistic critical value use p-value decide whether reject fail reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"null-and-alternative-hypotheses","chapter":"4 Basic Statistical Inference","heading":"4.1.1 Null and Alternative Hypotheses","text":"heart hypothesis testing lies formulation two competing hypotheses:Null Hypothesis (\\(H_0\\)):\nRepresents current state knowledge, status quo, effect.\nassumed true unless strong evidence .\nExamples:\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\n\\(H_0: \\beta = 0\\) (predictor variable effect regression model).\n\nThink \\(H_0\\) “default assumption.”\nRepresents current state knowledge, status quo, effect.assumed true unless strong evidence .Examples:\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\n\\(H_0: \\beta = 0\\) (predictor variable effect regression model).\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\\(H_0: \\beta = 0\\) (predictor variable effect regression model).Think \\(H_0\\) “default assumption.”Alternative Hypothesis (\\(H_a\\) \\(H_1\\)):\nRepresents claim contradicts null hypothesis.\ntrying prove find evidence .\nExamples:\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\n\\(H_a: \\beta \\neq 0\\) (predictor variable effect).\n\nRepresents claim contradicts null hypothesis.trying prove find evidence .Examples:\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\n\\(H_a: \\beta \\neq 0\\) (predictor variable effect).\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\\(H_a: \\beta \\neq 0\\) (predictor variable effect).","code":""},{"path":"basic-statistical-inference.html","id":"errors-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.2 Errors in Hypothesis Testing","text":"Hypothesis testing involves decision-making uncertainty, meaning always risk making errors. errors classified two types:Type Error (\\(\\alpha\\)):\nOccurs null hypothesis rejected, even though true.\nExample: Concluding medication effective actually effect.\nprobability making Type error denoted \\(\\alpha\\), called significance level (commonly set 0.05 5%).\nOccurs null hypothesis rejected, even though true.Example: Concluding medication effective actually effect.probability making Type error denoted \\(\\alpha\\), called significance level (commonly set 0.05 5%).Type II Error (\\(\\beta\\)):\nOccurs null hypothesis rejected, alternative hypothesis true.\nExample: Failing detect medication effective actually works.\ncomplement \\(\\beta\\) called power test (\\(1 - \\beta\\)), representing probability correctly rejecting null hypothesis.\nOccurs null hypothesis rejected, alternative hypothesis true.Example: Failing detect medication effective actually works.complement \\(\\beta\\) called power test (\\(1 - \\beta\\)), representing probability correctly rejecting null hypothesis.Analogy: Legal SystemTo make concept intuitive, consider analogy courtroom:Null Hypothesis (\\(H_0\\)): defendant innocent.Alternative Hypothesis (\\(H_a\\)): defendant guilty.Type Error: Convicting innocent person (false positive).Type II Error: Letting guilty person go free (false negative).Balancing \\(\\alpha\\) \\(\\beta\\) critical hypothesis testing, reducing one often increases . example, make harder reject \\(H_0\\) (reducing \\(\\alpha\\)), increase chance failing detect true effect (increasing \\(\\beta\\)).","code":""},{"path":"basic-statistical-inference.html","id":"the-role-of-distributions-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.3 The Role of Distributions in Hypothesis Testing","text":"Distributions play fundamental role hypothesis testing provide mathematical model understanding test statistic behaves null hypothesis (\\(H_0\\)). Without distributions, impossible determine whether observed results due random chance provide evidence reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"expected-outcomes","chapter":"4 Basic Statistical Inference","heading":"4.1.3.1 Expected Outcomes","text":"One key reasons distributions crucial describe range values test statistic likely take \\(H_0\\) true. helps us understand considered “normal” variation data due random chance. example:Imagine conducting study test whether new marketing strategy increases average monthly sales. null hypothesis, assume new strategy effect, average sales remain unchanged.collect sample calculate test statistic, compare expected distribution (e.g., normal distribution \\(z\\)-test). distribution shows range test statistic values likely occur purely due random fluctuations data, assuming \\(H_0\\) true.providing baseline “normal,” distributions allow us identify unusual results may indicate null hypothesis false.","code":""},{"path":"basic-statistical-inference.html","id":"critical-values-and-rejection-regions","chapter":"4 Basic Statistical Inference","heading":"4.1.3.2 Critical Values and Rejection Regions","text":"Distributions also help define critical values rejection regions hypothesis testing. Critical values specific points distribution mark boundaries rejection region. rejection region range values test statistic lead us reject \\(H_0\\).location critical values depends :level significance (\\(\\alpha\\)), probability rejecting \\(H_0\\) true (Type error).level significance (\\(\\alpha\\)), probability rejecting \\(H_0\\) true (Type error).shape test statistic’s distribution \\(H_0\\).shape test statistic’s distribution \\(H_0\\).example:one-tailed \\(z\\)-test \\(\\alpha = 0.05\\), critical value approximately \\(1.645\\) standard normal distribution. calculated test statistic exceeds value, reject \\(H_0\\) result unlikely \\(H_0\\).Distributions help us visually mathematically determine critical points. examining distribution, can see rejection region lies probability observing value region random chance alone.","code":""},{"path":"basic-statistical-inference.html","id":"p-values","chapter":"4 Basic Statistical Inference","heading":"4.1.3.3 P-values","text":"p-value, central concept hypothesis testing, directly derived distribution test statistic \\(H_0\\). p-value represents probability observing test statistic extreme (extreme ) one calculated, assuming \\(H_0\\) true.p-value quantifies strength evidence \\(H_0\\). represents probability observing test statistic extreme (extreme ) one calculated, assuming \\(H_0\\) true.Small p-value (< \\(\\alpha\\)): Strong evidence \\(H_0\\); reject \\(H_0\\).Large p-value (> \\(\\alpha\\)): Weak evidence \\(H_0\\); fail reject \\(H_0\\).example:Suppose calculate \\(z\\)-test statistic \\(2.1\\) one-tailed test. Using standard normal distribution, p-value area curve right \\(z = 2.1\\). area represents likelihood observing result extreme \\(z = 2.1\\) \\(H_0\\) true.Suppose calculate \\(z\\)-test statistic \\(2.1\\) one-tailed test. Using standard normal distribution, p-value area curve right \\(z = 2.1\\). area represents likelihood observing result extreme \\(z = 2.1\\) \\(H_0\\) true.case, p-value approximately \\(0.0179\\). small p-value (typically less \\(\\alpha = 0.05\\)) suggests observed result unlikely \\(H_0\\) provides evidence reject null hypothesis.case, p-value approximately \\(0.0179\\). small p-value (typically less \\(\\alpha = 0.05\\)) suggests observed result unlikely \\(H_0\\) provides evidence reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"why-does-all-this-matter","chapter":"4 Basic Statistical Inference","heading":"4.1.3.4 Why Does All This Matter?","text":"summarize, distributions backbone hypothesis testing allow us :Define expected \\(H_0\\) modeling behavior test statistic.Define expected \\(H_0\\) modeling behavior test statistic.Identify results unlikely occur random chance, leads rejection \\(H_0\\).Identify results unlikely occur random chance, leads rejection \\(H_0\\).Calculate p-values quantify strength evidence \\(H_0\\).Calculate p-values quantify strength evidence \\(H_0\\).Distributions provide framework understanding role chance statistical analysis. essential determining expected outcomes, setting thresholds decision-making (critical values rejection regions), calculating p-values. solid grasp distributions greatly enhance ability interpret conduct hypothesis tests, making easier draw meaningful conclusions data.","code":""},{"path":"basic-statistical-inference.html","id":"the-test-statistic","chapter":"4 Basic Statistical Inference","heading":"4.1.4 The Test Statistic","text":"test statistic crucial component hypothesis testing, quantifies far observed data deviates expect null hypothesis (\\(H_0\\)) true. Essentially, provides standardized way compare observed outcomes expectations set \\(H_0\\), enabling us assess whether observed results likely due random chance indicative significant effect.general formula test statistic :\\[\n\\text{Test Statistic} = \\frac{\\text{Observed Value} - \\text{Expected Value } H_0}{\\text{Standard Error}}\n\\]component formula important role:Numerator:\nnumerator represents difference actual data (observed value) hypothetical value (expected value) assumed \\(H_0\\).\ndifference quantifies extent deviation. larger deviation suggests stronger evidence \\(H_0\\).\nnumerator represents difference actual data (observed value) hypothetical value (expected value) assumed \\(H_0\\).difference quantifies extent deviation. larger deviation suggests stronger evidence \\(H_0\\).Denominator:\ndenominator standard error, measures variability spread data. accounts factors sample size inherent randomness data.\ndividing numerator standard error, test statistic standardized, allowing comparisons across different studies, sample sizes, distributions.\ndenominator standard error, measures variability spread data. accounts factors sample size inherent randomness data.dividing numerator standard error, test statistic standardized, allowing comparisons across different studies, sample sizes, distributions.test statistic plays central role determining whether reject \\(H_0\\). calculated, compared known distribution (e.g., standard normal distribution \\(z\\)-tests \\(t\\)-distribution \\(t\\)-tests). comparison allows us evaluate likelihood observing test statistic \\(H_0\\):test statistic close 0: indicates observed data close expected \\(H_0\\). little evidence suggest rejecting \\(H_0\\).test statistic far 0 (tails distribution): suggests observed data deviates significantly expectations \\(H_0\\). deviations may provide strong evidence \\(H_0\\).","code":""},{"path":"basic-statistical-inference.html","id":"why-standardizing-matters","chapter":"4 Basic Statistical Inference","heading":"4.1.4.1 Why Standardizing Matters","text":"Standardizing difference observed expected values ensures test statistic biased factors scale measurement size sample. instance:raw difference 5 might highly significant one context negligible another, depending variability (standard error).raw difference 5 might highly significant one context negligible another, depending variability (standard error).Standardizing ensures magnitude test statistic reflects size difference reliability sample data.Standardizing ensures magnitude test statistic reflects size difference reliability sample data.","code":""},{"path":"basic-statistical-inference.html","id":"interpreting-the-test-statistic","chapter":"4 Basic Statistical Inference","heading":"4.1.4.2 Interpreting the Test Statistic","text":"calculating test statistic, used :Compare critical value: example, \\(z\\)-test \\(\\alpha = 0.05\\), critical values \\(-1.96\\) \\(1.96\\) two-tailed test. test statistic falls beyond values, \\(H_0\\) rejected.Calculate p-value: p-value derived distribution reflects probability observing test statistic extreme one calculated \\(H_0\\) true.","code":""},{"path":"basic-statistical-inference.html","id":"critical-values-and-rejection-regions-1","chapter":"4 Basic Statistical Inference","heading":"4.1.5 Critical Values and Rejection Regions","text":"critical value point distribution separates rejection region non-rejection region:Rejection Region: test statistic falls region, reject \\(H_0\\).Non-Rejection Region: test statistic falls , fail reject \\(H_0\\).rejection region depends significance level (\\(\\alpha\\)). two-tailed test \\(\\alpha = 0.05\\), critical values correspond top 2.5% bottom 2.5% distribution.","code":""},{"path":"basic-statistical-inference.html","id":"visualizing-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.6 Visualizing Hypothesis Testing","text":"Let’s create visualization tie concepts together:","code":"\n# Parameters\nalpha <- 0.05  # Significance level\ndf <- 29       # Degrees of freedom (for t-distribution)\nt_critical <-\n    qt(1 - alpha / 2, df)  # Critical value for two-tailed test\n\n# Generate t-distribution values\nt_values <- seq(-4, 4, length.out = 1000)\ndensity <- dt(t_values, df)\n\n# Observed test statistic\nt_obs <- 2.5  # Example observed test statistic\n\n# Plot the t-distribution\nplot(\n    t_values,\n    density,\n    type = \"l\",\n    lwd = 2,\n    col = \"blue\",\n    main = \"Hypothesis Testing with Distribution\",\n    xlab = \"Test Statistic (t-value)\",\n    ylab = \"Density\",\n    ylim = c(0, 0.4)\n)\n\n# Shade the rejection regions\npolygon(c(t_values[t_values <= -t_critical], -t_critical),\n        c(density[t_values <= -t_critical], 0),\n        col = \"red\",\n        border = NA)\npolygon(c(t_values[t_values >= t_critical], t_critical),\n        c(density[t_values >= t_critical], 0),\n        col = \"red\",\n        border = NA)\n\n# Add observed test statistic\npoints(\n    t_obs,\n    dt(t_obs, df),\n    col = \"green\",\n    pch = 19,\n    cex = 1.5\n)\ntext(\n    t_obs,\n    dt(t_obs, df) + 0.02,\n    paste(\"Observed t:\", round(t_obs, 2)),\n    col = \"green\",\n    pos = 3\n)\n\n# Highlight the critical values\nabline(\n    v = c(-t_critical, t_critical),\n    col = \"black\",\n    lty = 2\n)\ntext(\n    -t_critical,\n    0.05,\n    paste(\"Critical Value:\", round(-t_critical, 2)),\n    pos = 4,\n    col = \"black\"\n)\ntext(\n    t_critical,\n    0.05,\n    paste(\"Critical Value:\", round(t_critical, 2)),\n    pos = 4,\n    col = \"black\"\n)\n\n# Calculate p-value\np_value <- 2 * (1 - pt(abs(t_obs), df))  # Two-tailed p-value\ntext(0,\n     0.35,\n     paste(\"P-value:\", round(p_value, 4)),\n     col = \"blue\",\n     pos = 3)\n\n# Annotate regions\ntext(-3,\n     0.15,\n     \"Rejection Region\",\n     col = \"red\",\n     pos = 3)\ntext(3, 0.15, \"Rejection Region\", col = \"red\", pos = 3)\ntext(0,\n     0.05,\n     \"Non-Rejection Region\",\n     col = \"blue\",\n     pos = 3)\n\n# Add legend\nlegend(\n    \"topright\",\n    legend = c(\"Rejection Region\", \"Critical Value\", \"Observed Test Statistic\"),\n    col = c(\"red\", \"black\", \"green\"),\n    lty = c(NA, 2, NA),\n    pch = c(15, NA, 19),\n    bty = \"n\"\n)"},{"path":"basic-statistical-inference.html","id":"key-concepts-and-definitions","chapter":"4 Basic Statistical Inference","heading":"4.2 Key Concepts and Definitions","text":"","code":""},{"path":"basic-statistical-inference.html","id":"random-sample","chapter":"4 Basic Statistical Inference","heading":"4.2.1 Random Sample","text":"random sample size \\(n\\) consists \\(n\\) independent observations, drawn underlying population distribution. Independence ensures observation influences another, identical distribution guarantees observations governed probability rules.","code":""},{"path":"basic-statistical-inference.html","id":"sample-statistics","chapter":"4 Basic Statistical Inference","heading":"4.2.2 Sample Statistics","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sample-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.2.1 Sample Mean","text":"sample mean measure central tendency:\\[\n\\bar{X} = \\frac{\\sum_{=1}^{n} X_i}{n}\n\\]Example: Suppose measure heights 5 individuals (cm): \\(170, 165, 180, 175, 172\\). sample mean :\\[\n\\bar{X} = \\frac{170 + 165 + 180 + 175 + 172}{5} = 172.4 \\, \\text{cm}.\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-median","chapter":"4 Basic Statistical Inference","heading":"4.2.2.2 Sample Median","text":"sample median middle value ordered data:\\[\n\\tilde{x} = \\begin{cases}\n\\text{Middle observation,} & \\text{} n \\text{ odd}, \\\\\n\\text{Average two middle observations,} & \\text{} n \\text{ even}.\n\\end{cases}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.2.3 Sample Variance","text":"sample variance measures data spread:\\[\nS^2 = \\frac{\\sum_{=1}^{n}(X_i - \\bar{X})^2}{n-1}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-standard-deviation","chapter":"4 Basic Statistical Inference","heading":"4.2.2.4 Sample Standard Deviation","text":"sample standard deviation square root variance:\\[\nS = \\sqrt{S^2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-proportions","chapter":"4 Basic Statistical Inference","heading":"4.2.2.5 Sample Proportions","text":"Used categorical data:\\[\n\\hat{p} = \\frac{X}{n} = \\frac{\\text{Number successes}}{\\text{Sample size}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"estimators","chapter":"4 Basic Statistical Inference","heading":"4.2.2.6 Estimators","text":"Point Estimator: statistic (\\(\\hat{\\theta}\\)) used estimate population parameter (\\(\\theta\\)).Point Estimate:numerical value assumed \\(\\hat{\\theta}\\) evaluated given sample.Unbiased Estimator: point estimator \\(\\hat{\\theta}\\) unbiased \\(E(\\hat{\\theta}) = \\theta\\).Examples unbiased estimators:\\(\\bar{X}\\) \\(\\mu\\) (population mean).\\(\\bar{X}\\) \\(\\mu\\) (population mean).\\(S^2\\) \\(\\sigma^2\\) (population variance).\\(S^2\\) \\(\\sigma^2\\) (population variance).\\(\\hat{p}\\) \\(p\\) (population proportion).\\(\\hat{p}\\) \\(p\\) (population proportion).\\(\\widehat{p_1-p_2}\\) \\(p_1- p_2\\) (population proportion difference)\\(\\widehat{p_1-p_2}\\) \\(p_1- p_2\\) (population proportion difference)\\(\\bar{X_1} - \\bar{X_2}\\) \\(\\mu_1 - \\mu_2\\) (population mean difference)\\(\\bar{X_1} - \\bar{X_2}\\) \\(\\mu_1 - \\mu_2\\) (population mean difference)Note: \\(S^2\\) unbiased \\(\\sigma^2\\), \\(S\\) biased estimator \\(\\sigma\\).","code":""},{"path":"basic-statistical-inference.html","id":"distribution-of-the-sample-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.3 Distribution of the Sample Mean","text":"sampling distribution mean \\(\\bar{X}\\) depends :Population Distribution:\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\).\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\).Central Limit Theorem:\nlarge \\(n\\), \\(\\bar{X}\\) approximately follows normal distribution, regardless population’s shape.\nlarge \\(n\\), \\(\\bar{X}\\) approximately follows normal distribution, regardless population’s shape.","code":""},{"path":"basic-statistical-inference.html","id":"standard-error-of-the-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.3.1 Standard Error of the Mean","text":"standard error quantifies variability \\(\\bar{X}\\):\\[\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]Example: - Suppose \\(\\sigma = 10\\) \\(n = 25\\). : \\[\n  \\sigma_{\\bar{X}} = \\frac{10}{\\sqrt{25}} = 2.\n  \\]smaller standard error, precise estimate population mean.","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.3 One-Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-single-mean","chapter":"4 Basic Statistical Inference","heading":"4.3.1 For Single Mean","text":"Consider scenario \\[\nY_i \\sim \\text{..d. } N(\\mu, \\sigma^2),\n\\]..d. stands “independent identically distributed.” model can expressed :\\[\nY_i = \\mu + \\epsilon_i,\n\\]:\\(\\epsilon_i \\sim^{\\text{..d.}} N(0, \\sigma^2)\\),\\(E(Y_i) = \\mu\\),\\(\\text{Var}(Y_i) = \\sigma^2\\),\\(\\bar{y} \\sim N(\\mu, \\sigma^2 / n)\\).\\(\\sigma^2\\) estimated \\(s^2\\), standardized test statistic follows \\(t\\)-distribution:\\[\n\\frac{\\bar{y} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}.\n\\]\\(100(1-\\alpha)\\%\\) confidence interval \\(\\mu\\) obtained :\\[\n1 - \\alpha = P\\left(-t_{\\alpha/2;n-1} \\leq \\frac{\\bar{y} - \\mu}{s / \\sqrt{n}} \\leq t_{\\alpha/2;n-1}\\right),\n\\]equivalently,\\[\nP\\left(\\bar{y} - t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{y} + t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}}\\right).\n\\]confidence interval expressed :\\[\n\\bar{y} \\pm t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}},\n\\]\\(s / \\sqrt{n}\\) standard error \\(\\bar{y}\\).experiment repeated many times, \\(100(1-\\alpha)\\%\\) intervals contain \\(\\mu\\).","code":""},{"path":"basic-statistical-inference.html","id":"power-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1 Power in Hypothesis Testing","text":"Power (\\(\\pi(\\mu)\\)) hypothesis test represents probability correctly rejecting null hypothesis (\\(H_0\\)) false (.e., alternative hypothesis \\(H_A\\) true). Formally, expressed :\\[ \\begin{aligned} \\text{Power} &= \\pi(\\mu) = 1 - \\beta \\\\ &= P(\\text{test rejects } H_0|\\mu) \\\\ &= P(\\text{test rejects } H_0| H_A \\text{ true}), \\end{aligned} \\]\\(\\beta\\) probability Type II error (failing reject \\(H_0\\) false).calculate probability:\\(H_0\\): distribution test statistic centered around null parameter (e.g., \\(\\mu_0\\)).\\(H_0\\): distribution test statistic centered around null parameter (e.g., \\(\\mu_0\\)).\\(H_A\\): test statistic distributed differently, shifted according true value \\(H_A\\) (e.g., \\(\\mu_1\\)).\\(H_A\\): test statistic distributed differently, shifted according true value \\(H_A\\) (e.g., \\(\\mu_1\\)).Hence, evaluate power, crucial determine distribution test statistic alternative hypothesis, \\(H_A\\)., derive power one-sided two-sided z-tests.","code":""},{"path":"basic-statistical-inference.html","id":"one-sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.1 One-Sided z-Test","text":"Consider hypotheses:\\[ H_0: \\mu \\leq \\mu_0 \\quad \\text{vs.} \\quad H_A: \\mu > \\mu_0 \\]power one-sided z-test derived follows:test rejects \\(H_0\\) \\(\\bar{y} > \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}\\), \\(z_{\\alpha}\\) critical value test significance level \\(\\alpha\\).alternative hypothesis, distribution \\(\\bar{y}\\) centered \\(\\mu\\), standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).power :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P\\left(\\bar{y} > \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) \\\\\n&= P\\left(Z > z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\middle| \\mu \\right), \\quad \\text{} Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} \\\\\n&= 1 - \\Phi\\left(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}\\right) \\\\\n&= \\Phi\\left(-z_{\\alpha} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right).\n\\end{aligned}\n\\], use symmetry standard normal distribution: \\(1 - \\Phi(x) = \\Phi(-x)\\).Suppose wish show mean response \\(\\mu\\) treatment higher mean response \\(\\mu_0\\) without treatment (.e., treatment effect \\(\\delta = \\mu - \\mu_0\\) large).Since power increasing function \\(\\mu - \\mu_0\\), suffices find sample size \\(n\\) achieves desired power \\(1 - \\beta\\) \\(\\mu = \\mu_0 + \\delta\\). power \\(\\mu = \\mu_0 + \\delta\\) :\\[\n\\pi(\\mu_0 + \\delta) = \\Phi\\left(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}\\right) = 1 - \\beta\n\\]Given \\(\\Phi(z_{\\beta}) = 1 - \\beta\\), :\\[\n-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta}\n\\]Solving \\(n\\), obtain:\\[\nn = \\left(\\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta}\\right)^2\n\\]Larger sample sizes required :sample variability large (\\(\\sigma\\) large).significance level \\(\\alpha\\) small (\\(z_{\\alpha}\\) large).desired power \\(1 - \\beta\\) large (\\(z_{\\beta}\\) large).magnitude effect small (\\(\\delta\\) small).practice, \\(\\delta\\) \\(\\sigma\\) often unknown. estimate \\(\\sigma\\), can:Use prior studies pilot studies.Approximate \\(\\sigma\\) based anticipated range observations (excluding outliers). normally distributed data, dividing range 4 provides reasonable estimate \\(\\sigma\\).considerations ensure test adequately powered detect meaningful effects balancing practical constraints sample size.","code":""},{"path":"basic-statistical-inference.html","id":"two-sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.2 Two-Sided z-Test","text":"two-sided test, hypotheses :\\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs.} \\quad H_A: \\mu \\neq \\mu_0\n\\]test rejects \\(H_0\\) \\(\\bar{y}\\) lies outside interval \\(\\mu_0 \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\). power test :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P\\left(\\bar{y} < \\mu_0 - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) + P\\left(\\bar{y} > \\mu_0 + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) \\\\\n&= \\Phi\\left(-z_{\\alpha/2} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) + \\Phi\\left(-z_{\\alpha/2} - \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right).\n\\end{aligned}\n\\]ensure power \\(1-\\beta\\) treatment effect \\(\\delta = |\\mu - \\mu_0|\\) least certain value, solve \\(n\\). Since power function two-sided test increasing symmetric \\(|\\mu - \\mu_0|\\), suffices find \\(n\\) power equals \\(1-\\beta\\) \\(\\mu = \\mu_0 + \\delta\\). gives:\\[\nn = \\left(\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta}\\right)^2\n\\]Alternatively, required sample size can determined using confidence interval approach. two-sided \\(\\alpha\\)-level confidence interval form:\\[\n\\bar{y} \\pm D\n\\]\\(D = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\), solving \\(n\\) gives:\\[\nn = \\left(\\frac{z_{\\alpha/2} \\sigma}{D}\\right)^2\n\\]value rounded nearest integer ensure required precision.one-sided hypothesis test, testing \\(H_0: \\mu \\geq 30\\) versus \\(H_a: \\mu < 30\\):\\(\\sigma\\) unknown, can estimate using:Prior studies pilot studies.Prior studies pilot studies.range observations (excluding outliers) divided 4, provides reasonable approximation normally distributed data.range observations (excluding outliers) divided 4, provides reasonable approximation normally distributed data.","code":"\n# Generate random data and compute a 95% confidence interval\ndata <- rnorm(100) # Generate 100 random values\nt.test(data, conf.level = 0.95) # Perform t-test with 95% confidence interval\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = 1.8613, df = 99, p-value = 0.06567\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.01261374  0.39466291\n#> sample estimates:\n#> mean of x \n#> 0.1910246\n# Perform one-sided t-test\nt.test(data, mu = 30, alternative = \"less\")\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = -290.45, df = 99, p-value < 2.2e-16\n#> alternative hypothesis: true mean is less than 30\n#> 95 percent confidence interval:\n#>      -Inf 0.361429\n#> sample estimates:\n#> mean of x \n#> 0.1910246"},{"path":"basic-statistical-inference.html","id":"z-test-summary","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.3 z-Test Summary","text":"one-sided tests:\\[ \\pi(\\mu) = \\Phi\\left(-z_{\\alpha} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) \\]two-sided tests:\\[ \\pi(\\mu) = \\Phi\\left(-z_{\\alpha/2} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) + \\Phi\\left(-z_{\\alpha/2} - \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) \\]Factors Affecting PowerEffect Size (\\(\\mu - \\mu_0\\)): Larger differences \\(\\mu\\) \\(\\mu_0\\) increase power.Sample Size (\\(n\\)): Larger \\(n\\) reduces standard error, increasing power.Variance (\\(\\sigma^2\\)): Smaller variance increases power.Significance Level (\\(\\alpha\\)): Increasing \\(\\alpha\\) (making test liberal) increases power \\(z_{\\alpha}\\).","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-t-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.4 One-Sample t-test","text":"hypothesis testing, calculating power determining required sample size t-tests complex z-tests. complexity arises involvement Student’s t-distribution generalized form, non-central t-distribution.power function one-sample t-test can expressed :\\[\n\\pi(\\mu) = P\\left(\\frac{\\bar{y} - \\mu_0}{s / \\sqrt{n}} > t_{n-1; \\alpha} \\mid \\mu \\right)\n\\]:\\(\\mu_0\\) hypothesized population mean null hypothesis,\\(\\mu_0\\) hypothesized population mean null hypothesis,\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(s\\) sample standard deviation,\\(s\\) sample standard deviation,\\(n\\) sample size,\\(n\\) sample size,\\(t_{n-1; \\alpha}\\) critical t-value Student’s t-distribution \\(n-1\\) degrees freedom significance level \\(\\alpha\\).\\(t_{n-1; \\alpha}\\) critical t-value Student’s t-distribution \\(n-1\\) degrees freedom significance level \\(\\alpha\\).\\(\\mu > \\mu_0\\) (.e., \\(\\mu - \\mu_0 = \\delta\\)), random variable\\[\nT = \\frac{\\bar{y} - \\mu_0}{s / \\sqrt{n}}\n\\]follow Student’s t-distribution. Instead, follows non-central t-distribution :non-centrality parameter \\(\\lambda = \\delta \\sqrt{n} / \\sigma\\), \\(\\sigma\\) population standard deviation,non-centrality parameter \\(\\lambda = \\delta \\sqrt{n} / \\sigma\\), \\(\\sigma\\) population standard deviation,degrees freedom \\(n-1\\).degrees freedom \\(n-1\\).Key Properties Power FunctionThe power \\(\\pi(\\mu)\\) increasing function non-centrality parameter \\(\\lambda\\).\\(\\delta = 0\\) (.e., null hypothesis true), non-central t-distribution simplifies regular Student’s t-distribution.calculate power practice, numerical procedures (see ) precomputed charts typically required.Approximate Sample Size Adjustment t-testsWhen planning study, researchers often start approximation based z-tests adjust specifics t-test. ’s process:1. Start Sample Size z-testFor two-sided test: \\[\nn_z = \\frac{\\left(z_{\\alpha/2} + z_\\beta\\right)^2 \\sigma^2}{\\delta^2}\n\\] :\\(z_{\\alpha/2}\\) critical value standard normal distribution two-tailed test,\\(z_{\\alpha/2}\\) critical value standard normal distribution two-tailed test,\\(z_\\beta\\) corresponds desired power \\(1 - \\beta\\),\\(z_\\beta\\) corresponds desired power \\(1 - \\beta\\),\\(\\delta\\) effect size \\(\\mu - \\mu_0\\),\\(\\delta\\) effect size \\(\\mu - \\mu_0\\),\\(\\sigma\\) population standard deviation.\\(\\sigma\\) population standard deviation.2. Adjust t-distributionLet \\(v = n - 1\\), \\(n\\) sample size derived z-test. two-sided t-test, approximate sample size :\\[\nn^* = \\frac{\\left(t_{v; \\alpha/2} + t_{v; \\beta}\\right)^2 \\sigma^2}{\\delta^2}\n\\]:\\(t_{v; \\alpha/2}\\) \\(t_{v; \\beta}\\) critical values Student’s t-distribution significance level \\(\\alpha\\) desired power, respectively.\\(t_{v; \\alpha/2}\\) \\(t_{v; \\beta}\\) critical values Student’s t-distribution significance level \\(\\alpha\\) desired power, respectively.Since \\(v\\) depends \\(n^*\\), process may require iterative refinement.Since \\(v\\) depends \\(n^*\\), process may require iterative refinement.Notes:Approximations: formulas provide intuitive starting point may require adjustments based exact numerical solutions.Insights: Power increasing function :\neffect size \\(\\delta\\),\nsample size \\(n\\),\ndecreasing function population variability \\(\\sigma\\).\neffect size \\(\\delta\\),sample size \\(n\\),decreasing function population variability \\(\\sigma\\).","code":"\n# Example: Power calculation for a one-sample t-test\nlibrary(pwr)\n\n# Parameters\neffect_size <- 0.5  # Cohen's d\nalpha <- 0.05       # Significance level\npower <- 0.8        # Desired power\n\n# Compute sample size\nsample_size <-\n    pwr.t.test(\n        d = effect_size,\n        sig.level = alpha,\n        power = power,\n        type = \"one.sample\"\n    )$n\n\n# Print result\ncat(\"Required sample size for one-sample t-test:\",\n    ceiling(sample_size),\n    \"\\n\")\n#> Required sample size for one-sample t-test: 34\n\n# Power calculation for a given sample size\ncalculated_power <-\n    pwr.t.test(\n        n = ceiling(sample_size),\n        d = effect_size,\n        sig.level = alpha,\n        type = \"one.sample\"\n    )$power\ncat(\"Achieved power with computed sample size:\",\n    calculated_power,\n    \"\\n\")\n#> Achieved power with computed sample size: 0.8077775"},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-independent-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.2 For Difference of Means, Independent Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-paired-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.3 For Difference of Means, Paired Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-two-proportions","chapter":"4 Basic Statistical Inference","heading":"4.3.4 For Difference of Two Proportions","text":"mean difference two sample proportions given :\\[\n\\hat{p_1} - \\hat{p_2}\n\\]variance difference proportions :\\[\n\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}\n\\]\\(100(1-\\alpha)\\%\\) confidence interval difference proportions calculated :\\[\n\\hat{p_1} - \\hat{p_2} \\pm z_{\\alpha/2} \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]\\(z_{\\alpha/2}\\): critical value standard normal distribution.\\(z_{\\alpha/2}\\): critical value standard normal distribution.\\(\\hat{p_1}\\), \\(\\hat{p_2}\\): Sample proportions.\\(\\hat{p_1}\\), \\(\\hat{p_2}\\): Sample proportions.\\(n_1\\), \\(n_2\\): Sample sizes.\\(n_1\\), \\(n_2\\): Sample sizes.Sample Size Desired Confidence Level Margin ErrorTo achieve margin error \\(d\\) given confidence level, required sample size can estimated follows:Prior Estimates \\(\\hat{p_1}\\) \\(\\hat{p_2}\\): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\left[p_1(1-p_1) + p_2(1-p_2)\\right]}{d^2}\n\\]Prior Estimates \\(\\hat{p_1}\\) \\(\\hat{p_2}\\): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\left[p_1(1-p_1) + p_2(1-p_2)\\right]}{d^2}\n\\]Without Prior Estimates (assuming maximum variability, \\(\\hat{p} = 0.5\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Without Prior Estimates (assuming maximum variability, \\(\\hat{p} = 0.5\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Hypothesis Testing Difference ProportionsThe test statistic hypothesis testing depends null hypothesis:\\((p_1 - p_2) \\neq 0\\): \\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2}) - (p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]\\((p_1 - p_2) \\neq 0\\): \\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2}) - (p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]\\((p_1 - p_2)_0 = 0\\) (testing equality proportions): \\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p}) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\]\\((p_1 - p_2)_0 = 0\\) (testing equality proportions): \\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p}) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\]\\(\\hat{p}\\) pooled sample proportion:\\[\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1\\hat{p_1} + n_2\\hat{p_2}}{n_1 + n_2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"for-single-proportion","chapter":"4 Basic Statistical Inference","heading":"4.3.5 For Single Proportion","text":"\\(100(1-\\alpha)\\%\\) confidence interval population proportion \\(p\\) :\\[\n\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]Sample Size DeterminationWith Prior Estimate (\\(\\hat{p}\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\n\\]Prior Estimate (\\(\\hat{p}\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\n\\]Without Prior Estimate: \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\n\\]Without Prior Estimate: \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\n\\]test statistic \\(H_0: p = p_0\\) :\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"for-single-variance","chapter":"4 Basic Statistical Inference","heading":"4.3.6 For Single Variance","text":"sample variance \\(s^2\\) \\(n\\) observations, \\(100(1-\\alpha)\\%\\) confidence interval population variance \\(\\sigma^2\\) :\\[\n\\begin{aligned}\n1 - \\alpha &= P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2)\\\\\n&=P\\left(\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2; n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2; n-1}}\\right)\n\\end{aligned}\n\\]Equivalently, confidence interval can written :\\[\n\\left(\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\\right)\n\\]find confidence limits \\(\\sigma\\), compute square root interval bounds:\\[\n\\text{Confidence Interval } \\sigma: \\quad \\left(\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}}\\right)\n\\]Hypothesis Testing VarianceThe test statistic testing null hypothesis population variance (\\(\\sigma^2_0\\)) :\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0}\n\\]test statistic follows chi-squared distribution \\(n-1\\) degrees freedom null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"non-parametric-tests","chapter":"4 Basic Statistical Inference","heading":"4.3.7 Non-parametric Tests","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sign-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.1 Sign Test","text":"Sign Test used test hypotheses median population, \\(\\mu_{(0.5)}\\), without assuming specific distribution data. test ideal small sample sizes normality assumptions met.test population median, consider hypotheses:Null Hypothesis: \\(H_0: \\mu_{(0.5)} = 0\\)Alternative Hypothesis: \\(H_a: \\mu_{(0.5)} > 0\\) (one-sided test)Steps:Count Positive Negative Deviations:\nCount observations (\\(y_i\\)) greater 0: \\(s_+\\) (number positive signs).\nCount observations less 0: \\(s_-\\) (number negative signs).\n\\(s_- = n - s_+\\).\nCount Positive Negative Deviations:Count observations (\\(y_i\\)) greater 0: \\(s_+\\) (number positive signs).Count observations less 0: \\(s_-\\) (number negative signs).\\(s_- = n - s_+\\).Decision Rule:\nReject \\(H_0\\) \\(s_+\\) large (equivalently, \\(s_-\\) small).\ndetermine large \\(s_+\\) must , use distribution \\(S_+\\) \\(H_0\\), Binomial \\(p = 0.5\\).\nDecision Rule:Reject \\(H_0\\) \\(s_+\\) large (equivalently, \\(s_-\\) small).determine large \\(s_+\\) must , use distribution \\(S_+\\) \\(H_0\\), Binomial \\(p = 0.5\\).Null Distribution:\n\\(H_0\\), \\(S_+\\) follows: \\[\nS_+ \\sim Binomial(n, p = 0.5)\n\\]Null Distribution:\n\\(H_0\\), \\(S_+\\) follows: \\[\nS_+ \\sim Binomial(n, p = 0.5)\n\\]Critical Value:\nReject \\(H_0\\) : \\[\ns_+ \\ge b_{n,\\alpha}\n\\] \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical value binomial distribution.Critical Value:\nReject \\(H_0\\) : \\[\ns_+ \\ge b_{n,\\alpha}\n\\] \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical value binomial distribution.p-value Calculation:\nCompute p-value observed (one-tailed) \\(s_+\\) : \\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{=s_+}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]\nAlternatively: \\[\nP(S \\le s_-) = \\sum_{=0}^{s_-} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]p-value Calculation:\nCompute p-value observed (one-tailed) \\(s_+\\) : \\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{=s_+}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]Alternatively: \\[\nP(S \\le s_-) = \\sum_{=0}^{s_-} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]Large Sample Normal ApproximationFor large \\(n\\), use normal approximation binomial test. Reject \\(H_0\\) : \\[\ns_+ \\ge \\frac{n}{2} + \\frac{1}{2} + z_{\\alpha} \\sqrt{\\frac{n}{4}}\n\\] \\(z_\\alpha\\) critical value one-sided test.two-sided tests, use maximum minimum \\(s_+\\) \\(s_-\\):Test statistic: \\(s_{\\text{max}} = \\max(s_+, s_-)\\) \\(s_{\\text{min}} = \\min(s_+, s_-)\\)Test statistic: \\(s_{\\text{max}} = \\max(s_+, s_-)\\) \\(s_{\\text{min}} = \\min(s_+, s_-)\\)Reject \\(H_0\\) \\(p\\)-value less \\(\\alpha\\), : \\[\np\\text{-value} = 2 \\sum_{=s_{\\text{max}}}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n = 2 \\sum_{= 0}^{s_{min}} \\binom{n}{} \\left( \\frac{1}{2} \\right)^n\n\\]Reject \\(H_0\\) \\(p\\)-value less \\(\\alpha\\), : \\[\np\\text{-value} = 2 \\sum_{=s_{\\text{max}}}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n = 2 \\sum_{= 0}^{s_{min}} \\binom{n}{} \\left( \\frac{1}{2} \\right)^n\n\\]Equivalently, rejecting \\(H_0\\) \\(s_{max} \\ge b_{n,\\alpha/2}\\).large \\(n\\), normal approximation uses: \\[\nz = \\frac{s_{\\text{max}} - \\frac{n}{2} - \\frac{1}{2}}{\\sqrt{\\frac{n}{4}}}\n\\]\nReject \\(H_0\\) \\(\\alpha\\) \\(z \\ge z_{\\alpha/2}\\).Handling zeros data common issue Sign Test:Random Assignment: Assign zeros randomly either \\(s_+\\) \\(s_-\\) (2 researchers might get different results).Fractional Assignment: Count zero \\(0.5\\) toward \\(s_+\\) \\(s_-\\) (apply Binomial Distribution afterward).Ignore Zeros: Ignore zeros, note reduces sample size power.","code":"\n# Example Data\ndata <- c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33)\n\n# Count positive signs\ns_plus <- sum(data > 0)\n\n# Sample size excluding zeros\nn <- length(data)\n\n# Perform a one-sided binomial test\nbinom.test(s_plus, n, p = 0.5, alternative = \"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  s_plus and n\n#> number of successes = 8, number of trials = 10, p-value = 0.05469\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.4930987 1.0000000\n#> sample estimates:\n#> probability of success \n#>                    0.8"},{"path":"basic-statistical-inference.html","id":"wilcoxon-signed-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.2 Wilcoxon Signed Rank Test","text":"Wilcoxon Signed Rank Test improvement Sign Test considers magnitude direction deviations null hypothesis value (e.g., 0). However, test assumes data symmetrically distributed around median, unlike Sign Test.test following hypotheses:\\[\n\\begin{aligned}\nH_0: \\mu_{(0.5)} &= 0\\\\\nH_a: \\mu_{(0.5)} &> 0\n\\end{aligned}\n\\]example assumes ties duplicate observations data.Procedure Signed Rank TestRank Absolute Values:\nRank observations \\(y_i\\) based absolute values.\nLet \\(r_i\\) denote rank \\(y_i\\).\nSince ties, ranks \\(r_i\\) uniquely determined form permutation integers \\(1, 2, \\dots, n\\).\nRank observations \\(y_i\\) based absolute values.Let \\(r_i\\) denote rank \\(y_i\\).Since ties, ranks \\(r_i\\) uniquely determined form permutation integers \\(1, 2, \\dots, n\\).Calculate \\(w_+\\) \\(w_-\\):\n\\(w_+\\) sum ranks corresponding positive values \\(y_i\\).\n\\(w_-\\) sum ranks corresponding negative values \\(y_i\\).\ndefinition: \\[\nw_+ + w_- = \\sum_{=1}^n r_i = \\frac{n(n+1)}{2}\n\\]\n\\(w_+\\) sum ranks corresponding positive values \\(y_i\\).\\(w_-\\) sum ranks corresponding negative values \\(y_i\\).definition: \\[\nw_+ + w_- = \\sum_{=1}^n r_i = \\frac{n(n+1)}{2}\n\\]Decision Rule:\nReject \\(H_0\\) \\(w_+\\) large (equivalently, \\(w_-\\) small).\nReject \\(H_0\\) \\(w_+\\) large (equivalently, \\(w_-\\) small).Null Distribution \\(W_+\\)null hypothesis, distributions \\(W_+\\) \\(W_-\\) identical symmetric. p-value one-sided test :\\[\n\\text{p-value} = P(W \\ge w_+) = P(W \\le w_-)\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(w_+ \\ge w_{n,\\alpha}\\), \\(w_{n,\\alpha}\\) critical value table null distribution \\(W_+\\).two-sided tests, use:\\[\np\\text{-value} = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\]Normal Approximation Large SamplesFor large \\(n\\), null distribution \\(W_+\\) can approximated normal distribution:\\[\nz = \\frac{w_+ - \\frac{n(n+1)}{4} - \\frac{1}{2}}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) :\\[\nw_+ \\ge \\frac{n(n+1)}{4} + \\frac{1}{2} + z_{\\alpha} \\sqrt{\\frac{n(n+1)(2n+1)}{24}} \\approx w_{n,\\alpha}\n\\]two-sided test, decision rule uses maximum minimum \\(w_+\\) \\(w_-\\):\\(w_{max} = \\max(w_+, w_-)\\)\\(w_{max} = \\max(w_+, w_-)\\)\\(w_{min} = \\min(w_+, w_-)\\)\\(w_{min} = \\min(w_+, w_-)\\)p-value computed :\\[\np\\text{-value} = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\]Handling Tied RanksIf observations \\(|y_i|\\) tied absolute values, assign average rank (“midrank”) tied values. example:Suppose \\(y_1 = -1\\), \\(y_2 = 3\\), \\(y_3 = -3\\), \\(y_4 = 5\\).ranks \\(|y_i|\\) :\n\\(|y_1| = 1\\): \\(r_1 = 1\\)\n\\(|y_2| = |y_3| = 3\\): \\(r_2 = r_3 = \\frac{2+3}{2} = 2.5\\)\n\\(|y_4| = 5\\): \\(r_4 = 4\\)\n\\(|y_1| = 1\\): \\(r_1 = 1\\)\\(|y_2| = |y_3| = 3\\): \\(r_2 = r_3 = \\frac{2+3}{2} = 2.5\\)\\(|y_4| = 5\\): \\(r_4 = 4\\)large samples, can use normal approximation setting exact = FALSE:","code":"\n# Example Data\ndata <- c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33)\n\n# Perform Wilcoxon Signed Rank Test (exact test)\nwilcox_exact <- wilcox.test(data, exact = TRUE)\n\n# Display results\nwilcox_exact\n#> \n#>  Wilcoxon signed rank exact test\n#> \n#> data:  data\n#> V = 52, p-value = 0.009766\n#> alternative hypothesis: true location is not equal to 0\n# Perform Wilcoxon Signed Rank Test (normal approximation)\nwilcox_normal <- wilcox.test(data, exact = FALSE)\n\n# Display results\nwilcox_normal\n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  data\n#> V = 52, p-value = 0.01443\n#> alternative hypothesis: true location is not equal to 0"},{"path":"basic-statistical-inference.html","id":"wald-wolfowitz-runs-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.3 Wald-Wolfowitz Runs Test","text":"Runs Test non-parametric test used examine randomness sequence. Specifically, tests whether order observations sequence random. test useful detecting non-random patterns, trends, clustering, periodicity.hypotheses Runs Test :Null Hypothesis: \\(H_0\\): sequence random.Alternative Hypothesis: \\(H_a\\): sequence random.run sequence consecutive observations type. example: - binary sequence + + - - + - + +, 5 runs: ++, --, +, -, ++.Runs can formed based classification criteria, :Positive vs. Negative valuesPositive vs. Negative valuesAbove vs. medianAbove vs. medianSuccess vs. Failure binary outcomesSuccess vs. Failure binary outcomesTest StatisticNumber Runs (\\(R\\)):\nobserved number runs sequence.Number Runs (\\(R\\)):\nobserved number runs sequence.Expected Number Runs (\\(E[R]\\)):\nnull hypothesis randomness, expected number runs : \\[\nE[R] = \\frac{2 n_1 n_2}{n_1 + n_2} + 1\n\\] :\n\\(n_1\\): Number observations first category (e.g., positives).\n\\(n_2\\): Number observations second category (e.g., negatives).\n\\(n = n_1 + n_2\\): Total number observations.\nExpected Number Runs (\\(E[R]\\)):\nnull hypothesis randomness, expected number runs : \\[\nE[R] = \\frac{2 n_1 n_2}{n_1 + n_2} + 1\n\\] :\\(n_1\\): Number observations first category (e.g., positives).\\(n_2\\): Number observations second category (e.g., negatives).\\(n = n_1 + n_2\\): Total number observations.Variance Runs (\\(\\text{Var}[R]\\)):\nvariance number runs given : \\[\n\\text{Var}[R] = \\frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}\n\\]Variance Runs (\\(\\text{Var}[R]\\)):\nvariance number runs given : \\[\n\\text{Var}[R] = \\frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge samples (\\(n \\geq 20\\)), test statistic approximately normally distributed: \\[\nz = \\frac{R - E[R]}{\\sqrt{\\text{Var}[R]}}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge samples (\\(n \\geq 20\\)), test statistic approximately normally distributed: \\[\nz = \\frac{R - E[R]}{\\sqrt{\\text{Var}[R]}}\n\\]Decision RuleCompute \\(z\\)-value compare critical value standard normal distribution.significance level \\(\\alpha\\):\nReject \\(H_0\\) \\(|z| \\ge z_{\\alpha/2}\\) (two-sided test).\nReject \\(H_0\\) \\(z \\ge z_\\alpha\\) \\(z \\le -z_\\alpha\\) one-sided tests.\nReject \\(H_0\\) \\(|z| \\ge z_{\\alpha/2}\\) (two-sided test).Reject \\(H_0\\) \\(z \\ge z_\\alpha\\) \\(z \\le -z_\\alpha\\) one-sided tests.Steps Conducting Runs Test:Classify data two groups (e.g., /median, positive/negative).Count total number runs (\\(R\\)).Compute \\(E[R]\\) \\(\\text{Var}[R]\\) based \\(n_1\\) \\(n_2\\).Compute \\(z\\)-value observed number runs.Compare \\(z\\)-value critical value decide whether reject \\(H_0\\).numerical dataset test based values median:output runs.test function includes:Observed Runs: actual number runs sequence.Observed Runs: actual number runs sequence.Expected Runs: expected number runs \\(H_0\\).Expected Runs: expected number runs \\(H_0\\).p-value: probability observing number runs extreme observed one \\(H_0\\).p-value: probability observing number runs extreme observed one \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude sequence random.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude sequence random.Limitations Runs TestThe test assumes observations independent.test assumes observations independent.small sample sizes, test may limited power.small sample sizes, test may limited power.Ties data must resolved predefined rule (e.g., treating ties belonging one group excluding ).Ties data must resolved predefined rule (e.g., treating ties belonging one group excluding ).","code":"\n# Example dataset\ndata <- c(1.2, -0.5, 3.4, -1.1, 2.8, -0.8, 4.5, 0.7)\n\nlibrary(randtests)\n# Perform Runs Test (above/below median)\nruns.test(data)\n#> \n#>  Runs Test\n#> \n#> data:  data\n#> statistic = 2.2913, runs = 8, n1 = 4, n2 = 4, n = 8, p-value = 0.02195\n#> alternative hypothesis: nonrandomness"},{"path":"basic-statistical-inference.html","id":"quantile-or-percentile-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.4 Quantile (or Percentile) Test","text":"Quantile Test (also called Percentile Test) non-parametric test used evaluate whether proportion observations falling within specific quantile matches expected proportion null hypothesis. test useful assessing distribution data specific quantiles (e.g., medians percentiles) interest.Suppose want test whether true proportion data specified quantile \\(q\\) matches given probability \\(p\\). hypotheses :Null Hypothesis: \\(H_0\\): true proportion equal \\(p\\).Alternative Hypothesis: \\(H_a\\): true proportion equal \\(p\\) (two-sided), greater \\(p\\) (right-tailed), less \\(p\\) (left-tailed).Test StatisticThe test statistic based observed count data points specified quantile.Observed Count (\\(k\\)):\nnumber data points \\(y_i\\) \\(y_i \\leq q\\).Observed Count (\\(k\\)):\nnumber data points \\(y_i\\) \\(y_i \\leq q\\).Expected Count (\\(E[k]\\)):\nexpected number observations quantile \\(q\\) \\(H_0\\) : \\[\nE[k] = n \\cdot p\n\\]Expected Count (\\(E[k]\\)):\nexpected number observations quantile \\(q\\) \\(H_0\\) : \\[\nE[k] = n \\cdot p\n\\]Variance:\nbinomial distribution, variance : \\[\n\\text{Var}[k] = n \\cdot p \\cdot (1 - p)\n\\]Variance:\nbinomial distribution, variance : \\[\n\\text{Var}[k] = n \\cdot p \\cdot (1 - p)\n\\]Standardized Test Statistic (\\(z\\)):\nlarge \\(n\\), test statistic approximately normally distributed: \\[\nz = \\frac{k - E[k]}{\\sqrt{\\text{Var}[k]}} = \\frac{k - n \\cdot p}{\\sqrt{n \\cdot p \\cdot (1 - p)}}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge \\(n\\), test statistic approximately normally distributed: \\[\nz = \\frac{k - E[k]}{\\sqrt{\\text{Var}[k]}} = \\frac{k - n \\cdot p}{\\sqrt{n \\cdot p \\cdot (1 - p)}}\n\\]Decision RuleCompute \\(z\\)-value observed count.Compare \\(z\\)-value critical value standard normal distribution:\ntwo-sided test, reject \\(H_0\\) \\(|z| \\geq z_{\\alpha/2}\\).\none-sided test, reject \\(H_0\\) \\(z \\geq z_\\alpha\\) (right-tailed) \\(z \\leq -z_\\alpha\\) (left-tailed).\ntwo-sided test, reject \\(H_0\\) \\(|z| \\geq z_{\\alpha/2}\\).one-sided test, reject \\(H_0\\) \\(z \\geq z_\\alpha\\) (right-tailed) \\(z \\leq -z_\\alpha\\) (left-tailed).Alternatively, calculate p-value reject \\(H_0\\) p-value \\(\\leq \\alpha\\).Suppose dataset want test whether proportion observations 50th percentile (median) matches expected value \\(p = 0.5\\).one-sided test (e.g., testing whether proportion greater \\(p\\)):Interpretation Resultsp-value: p-value less \\(\\alpha\\), reject \\(H_0\\) conclude proportion observations quantile deviates significantly \\(p\\).p-value: p-value less \\(\\alpha\\), reject \\(H_0\\) conclude proportion observations quantile deviates significantly \\(p\\).Quantile Test Statistic (\\(z\\)): \\(z\\)-value indicates many standard deviations observed count expected count null hypothesis. Large positive negative \\(z\\) values suggest non-random deviations.Quantile Test Statistic (\\(z\\)): \\(z\\)-value indicates many standard deviations observed count expected count null hypothesis. Large positive negative \\(z\\) values suggest non-random deviations.Assumptions TestObservations independent.Observations independent.sample size large enough normal approximation binomial distribution valid (\\(n \\cdot p \\geq 5\\) \\(n \\cdot (1 - p) \\geq 5\\)).sample size large enough normal approximation binomial distribution valid (\\(n \\cdot p \\geq 5\\) \\(n \\cdot (1 - p) \\geq 5\\)).Limitations TestFor small sample sizes, normal approximation may hold. cases, exact binomial tests appropriate.small sample sizes, normal approximation may hold. cases, exact binomial tests appropriate.test assumes quantile used (e.g., median) well-defined correctly calculated data.test assumes quantile used (e.g., median) well-defined correctly calculated data.","code":"\n# Example data\ndata <- c(12, 15, 14, 10, 13, 11, 14, 16, 15, 13)\n\n# Define the quantile to test\nquantile_value <- quantile(data, 0.5) # Median\np <- 0.5                             # Proportion under H0\n\n# Count observed values below or equal to the quantile\nk <- sum(data <= quantile_value)\n\n# Sample size\nn <- length(data)\n\n# Expected count under H0\nexpected_count <- n * p\n\n# Variance\nvariance <- n * p * (1 - p)\n\n# Test statistic (z-value)\nz <- (k - expected_count) / sqrt(variance)\n\n# Calculate p-value for two-sided test\np_value <- 2 * (1 - pnorm(abs(z)))\n\n# Output results\nlist(\n  quantile_value = quantile_value,\n  observed_count = k,\n  expected_count = expected_count,\n  z_value = z,\n  p_value = p_value\n)\n#> $quantile_value\n#>  50% \n#> 13.5 \n#> \n#> $observed_count\n#> [1] 5\n#> \n#> $expected_count\n#> [1] 5\n#> \n#> $z_value\n#> [1] 0\n#> \n#> $p_value\n#> [1] 1\n# Calculate one-sided p-value\np_value_one_sided <- 1 - pnorm(z)\n\n# Output one-sided p-value\np_value_one_sided\n#> [1] 0.5"},{"path":"basic-statistical-inference.html","id":"two-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.4 Two-Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-means","chapter":"4 Basic Statistical Inference","heading":"4.4.1 For Means","text":"Suppose two sets observations:\\(y_1, \\dots, y_{n_y}\\)\\(x_1, \\dots, x_{n_x}\\)random samples two independent populations means \\(\\mu_y\\) \\(\\mu_x\\) variances \\(\\sigma_y^2\\) \\(\\sigma_x^2\\). goal compare \\(\\mu_y\\) \\(\\mu_x\\) test whether \\(\\sigma_y^2 = \\sigma_x^2\\).","code":""},{"path":"basic-statistical-inference.html","id":"large-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.4.1.1 Large Sample Tests","text":"\\(n_y\\) \\(n_x\\) large (\\(\\geq 30\\)), Central Limit Theorem allows us make following assumptions:Expectation: \\[\nE(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x\n\\]Variance: \\[\n\\text{Var}(\\bar{y} - \\bar{x}) = \\frac{\\sigma_y^2}{n_y} + \\frac{\\sigma_x^2}{n_x}\n\\]test statistic :\\[\nZ = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\frac{\\sigma_y^2}{n_y} + \\frac{\\sigma_x^2}{n_x}}} \\sim N(0,1)\n\\]large samples, replace variances unbiased estimators \\(s_y^2\\) \\(s_x^2\\), yielding large sample distribution.Confidence IntervalAn approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) :\\[\n\\bar{y} - \\bar{x} \\pm z_{\\alpha/2} \\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}\n\\]Hypothesis TestTesting:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]test statistic:\\[\nz = \\frac{\\bar{y} - \\bar{x} - \\delta_0}{\\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}}\n\\]Reject \\(H_0\\) \\(\\alpha\\)-level :\\[\n|z| > z_{\\alpha/2}\n\\]\\(\\delta_0 = 0\\), tests whether two means equal.","code":"\n# Large sample test\ny <- c(10, 12, 14, 16, 18)\nx <- c(9, 11, 13, 15, 17)\n\n# Mean and variance\nmean_y <- mean(y)\nmean_x <- mean(x)\nvar_y <- var(y)\nvar_x <- var(x)\nn_y <- length(y)\nn_x <- length(x)\n\n# Test statistic\nz <- (mean_y - mean_x) / sqrt(var_y / n_y + var_x / n_x)\np_value <- 2 * (1 - pnorm(abs(z)))\n\nlist(z = z, p_value = p_value)\n#> $z\n#> [1] 0.5\n#> \n#> $p_value\n#> [1] 0.6170751"},{"path":"basic-statistical-inference.html","id":"small-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2 Small Sample Tests","text":"samples small, assume data come independent normal distributions:\\(y_i \\sim N(\\mu_y, \\sigma_y^2)\\)\\(y_i \\sim N(\\mu_y, \\sigma_y^2)\\)\\(x_i \\sim N(\\mu_x, \\sigma_x^2)\\)\\(x_i \\sim N(\\mu_x, \\sigma_x^2)\\)can inference based Student’s T Distribution, 2 cases:Equal VariancesEqual VariancesUnequal VariancesUnequal VariancesF-TestLevene’s TestModified Levene Test (Brown-Forsythe Test)Bartlett’s TestBoxplots overlayed meansResiduals spread plots","code":""},{"path":"basic-statistical-inference.html","id":"equal-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2.1 Equal Variances","text":"AssumptionsIndependence Identically Distributed (..d.) ObservationsAssume observations sample ..d., implies:\\[\nvar(\\bar{y}) = \\frac{\\sigma^2_y}{n_y}, \\quad var(\\bar{x}) = \\frac{\\sigma^2_x}{n_x}\n\\]Independence SamplesThe samples assumed independent, meaning observation one sample influences observations . independence allows us write:\\[\n\\begin{aligned}\nvar(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var(\\bar{x}) - 2cov(\\bar{y}, \\bar{x}) \\\\\n&= var(\\bar{y}) + var(\\bar{x}) \\\\\n&= \\frac{\\sigma^2_y}{n_y} + \\frac{\\sigma^2_x}{n_x}\n\\end{aligned}\n\\]calculation assumes \\(cov(\\bar{y}, \\bar{x}) = 0\\) due independence samples.Normality AssumptionWe assume underlying populations normally distributed. assumption justifies use Student’s T Distribution, critical hypothesis testing constructing confidence intervals.Equality VariancesIf population variances equal, .e., \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\), \\(s^2_y\\) \\(s^2_x\\) unbiased estimators \\(\\sigma^2\\). allows us pool variances.pooled variance estimator calculated :\\[\ns^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y - 1) + (n_x - 1)}\n\\]pooled variance estimate degrees freedom equal :\\[\ndf = (n_y + n_x - 2)\n\\]Test StatisticThe test statistic : \\[\nT = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}} \\sim t_{n_y + n_x - 2}\n\\]Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) : \\[\n\\bar{y} - \\bar{x} \\pm t_{n_y + n_x - 2, \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}\n\\]Hypothesis TestTesting: \\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]Reject \\(H_0\\) : \\[\n|T| > t_{n_y + n_x - 2, \\alpha/2}\n\\]","code":"\n# Small sample test with equal variance\nt_test_equal <- t.test(y, x, var.equal = TRUE)\nt_test_equal\n#> \n#>  Two Sample t-test\n#> \n#> data:  y and x\n#> t = 0.5, df = 8, p-value = 0.6305\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -3.612008  5.612008\n#> sample estimates:\n#> mean of x mean of y \n#>        14        13"},{"path":"basic-statistical-inference.html","id":"unequal-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2.2 Unequal Variances","text":"AssumptionsIndependence Identically Distributed (..d.) ObservationsAssume observations sample ..d., implies:\\[ var(\\bar{y}) = \\frac{\\sigma^2_y}{n_y}, \\quad var(\\bar{x}) = \\frac{\\sigma^2_x}{n_x} \\]Independence SamplesThe samples assumed independent, meaning observation one sample influences observations . independence allows us write:\\[ \\begin{aligned} var(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var(\\bar{x}) - 2cov(\\bar{y}, \\bar{x}) \\\\ &= var(\\bar{y}) + var(\\bar{x}) \\\\ &= \\frac{\\sigma^2_y}{n_y} + \\frac{\\sigma^2_x}{n_x} \\end{aligned} \\]calculation assumes \\(cov(\\bar{y}, \\bar{x}) = 0\\) due independence samples.Normality AssumptionWe assume underlying populations normally distributed. assumption justifies use Student’s T Distribution, critical hypothesis testing constructing confidence intervals.Unequal Variances\\(\\sigma_y^2 \\neq \\sigma_x^2\\)Test StatisticThe test statistic :\\[\nT = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}}\n\\]Degrees Freedom (Welch-Satterthwaite Approximation) (Satterthwaite 1946)degrees freedom approximated :\\[\nv = \\frac{\\left(\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}\\right)^2}{\\frac{\\left(\\frac{s_y^2}{n_y}\\right)^2}{n_y - 1} + \\frac{\\left(\\frac{s_x^2}{n_x}\\right)^2}{n_x - 1}}\n\\]Since \\(v\\) fractional, truncate nearest integer.Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) :\\[\n\\bar{y} - \\bar{x} \\pm t_{v, \\alpha/2} \\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}\n\\]Hypothesis TestTesting:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]Reject \\(H_0\\) :\\[\n|T| > t_{v, \\alpha/2}\n\\]\\[\nt = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}}\n\\]","code":"\n# Small sample test with unequal variance\nt_test_unequal <- t.test(y, x, var.equal = FALSE)\nt_test_unequal\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  y and x\n#> t = 0.5, df = 8, p-value = 0.6305\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -3.612008  5.612008\n#> sample estimates:\n#> mean of x mean of y \n#>        14        13"},{"path":"basic-statistical-inference.html","id":"for-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.2 For Variances","text":"compare variances two independent samples, can use F-test. test statistic defined :\\[\nF_{ndf,ddf} = \\frac{s_1^2}{s_2^2}\n\\]\\(s_1^2 > s_2^2\\), \\(ndf = n_1 - 1\\), \\(ddf = n_2 - 1\\) numerator denominator degrees freedom, respectively.","code":""},{"path":"basic-statistical-inference.html","id":"f-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.1 F-Test","text":"hypotheses F-test :\\[\n\\begin{aligned}\nH_0&: \\sigma_y^2 = \\sigma_x^2 \\quad \\text{(equal variances)} \\\\\nH_a&: \\sigma_y^2 \\neq \\sigma_x^2 \\quad \\text{(unequal variances)}\n\\end{aligned}\n\\]test statistic :\\[\nF = \\frac{s_y^2}{s_x^2}\n\\]\\(s_y^2\\) \\(s_x^2\\) sample variances two groups.Decision RuleReject \\(H_0\\) :\\(F > F_{n_y-1, n_x-1, \\alpha/2}\\) (upper critical value), \\(F > F_{n_y-1, n_x-1, \\alpha/2}\\) (upper critical value), \\(F < F_{n_y-1, n_x-1, 1-\\alpha/2}\\) (lower critical value).\\(F < F_{n_y-1, n_x-1, 1-\\alpha/2}\\) (lower critical value).:\\(F_{n_y-1, n_x-1, \\alpha/2}\\) \\(F_{n_y-1, n_x-1, 1-\\alpha/2}\\) critical points F-distribution, \\(n_y - 1\\) \\(n_x - 1\\) degrees freedom.AssumptionsThe F-test requires data groups follow normal distribution.F-test sensitive deviations normality (e.g., heavy-tailed distributions). normality assumption violated, may lead inflated Type error rate (false positives).Limitations AlternativesSensitivity Non-Normality:\ndata long-tailed distributions (positive kurtosis), F-test may produce misleading results.\nassess normality, see Normality Assessment.\ndata long-tailed distributions (positive kurtosis), F-test may produce misleading results.assess normality, see Normality Assessment.Nonparametric Alternatives:\nnormality assumption met, use robust tests Modified Levene Test (Brown-Forsythe Test), compares group variances based medians instead means.\nnormality assumption met, use robust tests Modified Levene Test (Brown-Forsythe Test), compares group variances based medians instead means.","code":"\n# Load iris dataset\ndata(iris)\n\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform F-test\nf_test <- var.test(irisVe, irisVi)\n\n# Display results\nf_test\n#> \n#>  F test to compare two variances\n#> \n#> data:  irisVe and irisVi\n#> F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\n#> alternative hypothesis: true ratio of variances is not equal to 1\n#> 95 percent confidence interval:\n#>  0.2941935 0.9135614\n#> sample estimates:\n#> ratio of variances \n#>          0.5184243"},{"path":"basic-statistical-inference.html","id":"levenes-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.2 Levene’s Test","text":"Levene’s Test robust method testing equality variances across multiple groups. Unlike F-test, less sensitive departures normality particularly useful handling non-normal distributions datasets outliers. test works analyzing deviations individual observations group mean median.Test ProcedureCompute absolute deviations observation group mean median:\ngroup \\(y\\): \\[\nd_{y,} = |y_i - \\text{Central Value}_y|\n\\]\ngroup \\(x\\): \\[\nd_{x,j} = |x_j - \\text{Central Value}_x|\n\\]\n“central value” can either mean (classic Levene’s test) median (Modified Levene Test (Brown-Forsythe Test) variation, robust non-normal data).\ngroup \\(y\\): \\[\nd_{y,} = |y_i - \\text{Central Value}_y|\n\\]group \\(x\\): \\[\nd_{x,j} = |x_j - \\text{Central Value}_x|\n\\]“central value” can either mean (classic Levene’s test) median (Modified Levene Test (Brown-Forsythe Test) variation, robust non-normal data).Perform one-way ANOVA absolute deviations test differences group variances.HypothesesNull Hypothesis (\\(H_0\\)): groups equal variances.Alternative Hypothesis (\\(H_a\\)): least one group variance different others.Test StatisticThe Levene test statistic calculated ANOVA absolute deviations. Let:\\(k\\): Number groups,\\(k\\): Number groups,\\(n_i\\): Number observations group \\(\\),\\(n_i\\): Number observations group \\(\\),\\(n\\): Total number observations.\\(n\\): Total number observations.test statistic :\\[\nW = \\frac{(n - k) \\sum_{=1}^k n_i (\\bar{d}_i - \\bar{d})^2}{(k - 1) \\sum_{=1}^k \\sum_{j=1}^{n_i} (d_{,j} - \\bar{d}_i)^2}\n\\]:\\(d_{,j}\\): Absolute deviations within group \\(\\),\\(d_{,j}\\): Absolute deviations within group \\(\\),\\(\\bar{d}_i\\): Mean absolute deviations group \\(\\),\\(\\bar{d}_i\\): Mean absolute deviations group \\(\\),\\(\\bar{d}\\): Overall mean absolute deviations.\\(\\bar{d}\\): Overall mean absolute deviations.null hypothesis, \\(W \\sim F_{k-1, n - k}\\).Decision RuleCompute test statistic \\(W\\).Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nW > F_{k-1, n-k, \\alpha}\n\\]output includes:Df: Degrees freedom numerator denominator.Df: Degrees freedom numerator denominator.F-value: computed value test statistic \\(W\\).F-value: computed value test statistic \\(W\\).p-value: probability observing value null hypothesis.p-value: probability observing value null hypothesis.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude group variances significantly different.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude group variances significantly different.Otherwise, fail reject \\(H_0\\) conclude evidence difference variances.Otherwise, fail reject \\(H_0\\) conclude evidence difference variances.Advantages Levene’s TestRobustness:\nHandles non-normal data outliers better F-test.\nRobustness:Handles non-normal data outliers better F-test.Flexibility:\nchoosing center value (mean median), can adapt different data characteristics:\nUse mean symmetric distributions.\nUse median non-normal skewed data.\n\nFlexibility:choosing center value (mean median), can adapt different data characteristics:\nUse mean symmetric distributions.\nUse median non-normal skewed data.\nchoosing center value (mean median), can adapt different data characteristics:Use mean symmetric distributions.Use mean symmetric distributions.Use median non-normal skewed data.Use median non-normal skewed data.Versatility:\nApplicable comparing variances across two groups, unlike Modified Levene Test (Brown-Forsythe Test), limited two groups.\nVersatility:Applicable comparing variances across two groups, unlike Modified Levene Test (Brown-Forsythe Test), limited two groups.","code":"\n# Load required package\nlibrary(car)\n\n# Perform Levene's Test (absolute deviations from the mean)\nlevene_test_mean <- leveneTest(Petal.Width ~ Species, data = iris)\n\n# Perform Levene's Test (absolute deviations from the median)\nlevene_test_median <-\n    leveneTest(Petal.Width ~ Species, data = iris, center = median)\n\n# Display results\nlevene_test_mean\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value    Pr(>F)    \n#> group   2  19.892 2.261e-08 ***\n#>       147                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlevene_test_median\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value    Pr(>F)    \n#> group   2  19.892 2.261e-08 ***\n#>       147                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"basic-statistical-inference.html","id":"modified-levene-test-brown-forsythe-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.3 Modified Levene Test (Brown-Forsythe Test)","text":"Modified Levene Test robust alternative F-test comparing variances two groups. Instead using squared deviations (F-test), test considers absolute deviations median, making less sensitive non-normal data long-tailed distributions. , however, still appropriate normally distributed data.sample, compute absolute deviations median:\\[\nd_{y,} = |y_i - y_{.5}| \\quad \\text{} \\quad d_{x,} = |x_i - x_{.5}|\n\\]Let:\\(\\bar{d}_y\\) \\(\\bar{d}_x\\) means absolute deviations groups \\(y\\) \\(x\\), respectively.test statistic :\\[\nt_L^* = \\frac{\\bar{d}_y - \\bar{d}_x}{s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}}\n\\]pooled variance \\(s^2\\) :\\[\ns^2 = \\frac{\\sum_{=1}^{n_y} (d_{y,} - \\bar{d}_y)^2 + \\sum_{j=1}^{n_x} (d_{x,j} - \\bar{d}_x)^2}{n_y + n_x - 2}\n\\]AssumptionsConstant Variance Error Terms:\ntest assumes equal error variances group null hypothesis.Constant Variance Error Terms:\ntest assumes equal error variances group null hypothesis.Moderate Sample Size:\napproximation \\(t_L^* \\sim t_{n_y + n_x - 2}\\) holds well moderate large sample sizes.Moderate Sample Size:\napproximation \\(t_L^* \\sim t_{n_y + n_x - 2}\\) holds well moderate large sample sizes.Decision RuleCompute \\(t_L^*\\) using formula .Reject null hypothesis equal variances : \\[\n|t_L^*| > t_{n_y + n_x - 2; \\alpha/2}\n\\]equivalent applying two-sample t-test absolute deviations.small sample sizes, use unequal variance t-test directly original data robust alternative:","code":"\n# Absolute deviations from the median\ndVe <- abs(irisVe - median(irisVe))\ndVi <- abs(irisVi - median(irisVi))\n\n# Perform t-test on absolute deviations\nlevene_test <- t.test(dVe, dVi, var.equal = TRUE)\n\n# Display results\nlevene_test\n#> \n#>  Two Sample t-test\n#> \n#> data:  dVe and dVi\n#> t = -2.5584, df = 98, p-value = 0.01205\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.12784786 -0.01615214\n#> sample estimates:\n#> mean of x mean of y \n#>     0.154     0.226\n# Small sample t-test with unequal variances\nsmall_sample_test <- t.test(irisVe, irisVi, var.equal = FALSE)\n\n# Display results\nsmall_sample_test\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  irisVe and irisVi\n#> t = -14.625, df = 89.043, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.7951002 -0.6048998\n#> sample estimates:\n#> mean of x mean of y \n#>     1.326     2.026"},{"path":"basic-statistical-inference.html","id":"bartletts-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.4 Bartlett’s Test","text":"Bartlett’s Test statistical procedure testing equality variances across multiple groups. assumes data group normally distributed sensitive deviations normality. assumption normality holds, Bartlett’s Test powerful Levene’s Test.Hypotheses Bartlett’s TestNull Hypothesis (\\(H_0\\)): groups equal variances.Alternative Hypothesis (\\(H_a\\)): least one group variance different others.test statistic Bartlett’s Test :\\[\nB = \\frac{(n - k) \\log(S_p^2) - \\sum_{=1}^k (n_i - 1) \\log(S_i^2)}{1 + \\frac{1}{3(k - 1)} \\left( \\sum_{=1}^k \\frac{1}{n_i - 1} - \\frac{1}{n - k} \\right)}\n\\]:\\(k\\): Number groups,\\(k\\): Number groups,\\(n_i\\): Number observations group \\(\\),\\(n_i\\): Number observations group \\(\\),\\(n = \\sum_{=1}^k n_i\\): Total number observations,\\(n = \\sum_{=1}^k n_i\\): Total number observations,\\(S_i^2\\): Sample variance group \\(\\),\\(S_i^2\\): Sample variance group \\(\\),\\(S_p^2\\): Pooled variance, given : \\[\n  S_p^2 = \\frac{\\sum_{=1}^k (n_i - 1) S_i^2}{n - k}\n  \\]\\(S_p^2\\): Pooled variance, given : \\[\n  S_p^2 = \\frac{\\sum_{=1}^k (n_i - 1) S_i^2}{n - k}\n  \\]null hypothesis, test statistic \\(B \\sim \\chi^2_{k - 1}\\).AssumptionsNormality: data group must follow normal distribution.Independence: Observations within groups must independent.Equal Sample Sizes (Optional): Bartlett’s Test robust sample sizes approximately equal.Decision RuleCompute test statistic \\(B\\).Compare \\(B\\) critical value Chi-Square distribution \\(\\alpha\\) \\(k - 1\\) degrees freedom.Reject \\(H_0\\) : \\[\nB > \\chi^2_{k-1, \\alpha}\n\\]Alternatively, use p-value:Reject \\(H_0\\) p-value \\(\\leq \\alpha\\).output includes:Bartlett’s K-squared: value test statistic \\(B\\).Bartlett’s K-squared: value test statistic \\(B\\).df: Degrees freedom (\\(k - 1\\)), \\(k\\) number groups.df: Degrees freedom (\\(k - 1\\)), \\(k\\) number groups.p-value: probability observing value \\(B\\) \\(H_0\\).p-value: probability observing value \\(B\\) \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude variances significantly different across groups.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude variances significantly different across groups.p-value greater \\(\\alpha\\), fail reject \\(H_0\\) conclude significant evidence variance differences.p-value greater \\(\\alpha\\), fail reject \\(H_0\\) conclude significant evidence variance differences.Limitations Bartlett’s TestSensitivity Non-Normality:\nBartlett’s Test highly sensitive departures normality. Even slight deviations can lead misleading results.Sensitivity Non-Normality:\nBartlett’s Test highly sensitive departures normality. Even slight deviations can lead misleading results.Robust Outliers:\nOutliers can disproportionately affect test result.Robust Outliers:\nOutliers can disproportionately affect test result.Alternatives:\nnormality assumption violated, use robust alternatives like:\nLevene’s Test (absolute deviations)\nModified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)\nAlternatives:\nnormality assumption violated, use robust alternatives like:Levene’s Test (absolute deviations)Levene’s Test (absolute deviations)Modified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)Modified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)Advantages Bartlett’s TestHigh Power: Bartlett’s Test powerful robust alternatives normality assumption holds.High Power: Bartlett’s Test powerful robust alternatives normality assumption holds.Simple Implementation: test easy perform interpret.Simple Implementation: test easy perform interpret.","code":"\n# Perform Bartlett's Test\nbartlett_test <- bartlett.test(Petal.Width ~ Species, data = iris)\n\n# Display results\nbartlett_test\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  Petal.Width by Species\n#> Bartlett's K-squared = 39.213, df = 2, p-value = 3.055e-09"},{"path":"basic-statistical-inference.html","id":"power","chapter":"4 Basic Statistical Inference","heading":"4.4.3 Power","text":"evaluate power test, consider situation variances equal across groups:\\[\n\\sigma_y^2 = \\sigma_x^2 = \\sigma^2\n\\]assumption equal variances, take equal sample sizes groups, .e., \\(n_y = n_x = n\\).Hypotheses One-Sided TestingWe testing:\\[\nH_0: \\mu_y - \\mu_x \\leq 0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x > 0\n\\]Test StatisticThe \\(\\alpha\\)-level z-test rejects \\(H_0\\) test statistic:\\[\nz = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{\\frac{2}{n}}} > z_\\alpha\n\\]:\\(\\bar{y}\\) \\(\\bar{x}\\) sample means,\\(\\bar{y}\\) \\(\\bar{x}\\) sample means,\\(\\sigma\\) common standard deviation,\\(\\sigma\\) common standard deviation,\\(z_\\alpha\\) critical value standard normal distribution.\\(z_\\alpha\\) critical value standard normal distribution.Power FunctionThe power test, denoted \\(\\pi(\\mu_y - \\mu_x)\\), probability correctly rejecting \\(H_0\\) \\(\\mu_y - \\mu_x\\) specified value. alternative hypothesis, power function :\\[\n\\pi(\\mu_y - \\mu_x) = \\Phi\\left(-z_\\alpha + \\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\right)\n\\]:\\(\\Phi\\) cumulative distribution function (CDF) standard normal distribution,\\(\\Phi\\) cumulative distribution function (CDF) standard normal distribution,\\(\\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\) represents standardized effect size.\\(\\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\) represents standardized effect size.Determining Required Sample SizeTo achieve desired power \\(1 - \\beta\\) true difference \\(\\delta\\) (smallest difference interest), solve required sample size \\(n\\). power equation :\\[\n\\Phi\\left(-z_\\alpha + \\frac{\\delta}{\\sigma} \\sqrt{\\frac{n}{2}}\\right) = 1 - \\beta\n\\]Rearranging \\(n\\), required sample size :\\[\nn = \\frac{2 \\sigma^2}{\\delta^2} \\left(z_\\alpha + z_\\beta\\right)^2\n\\]:\\(\\sigma\\): common standard deviation,\\(\\sigma\\): common standard deviation,\\(z_{\\alpha}\\): critical value Type error rate \\(\\alpha\\) (one-sided test),\\(z_{\\alpha}\\): critical value Type error rate \\(\\alpha\\) (one-sided test),\\(z_{\\beta}\\): critical value Type II error rate \\(\\beta\\) (related power \\(1 - \\beta\\)),\\(z_{\\beta}\\): critical value Type II error rate \\(\\beta\\) (related power \\(1 - \\beta\\)),\\(\\delta\\): minimum detectable difference means.\\(\\delta\\): minimum detectable difference means.Sample Size Two-Sided TestsFor two-sided test, replace \\(z_{\\alpha}\\) \\(z_{\\alpha/2}\\) account two-tailed critical region:\\[\nn = 2 \\left( \\frac{\\sigma (z_{\\alpha/2} + z_{\\beta})}{\\delta} \\right)^2\n\\]ensures test required power \\(1 - \\beta\\) detect difference size \\(\\delta\\) means significance level \\(\\alpha\\).Adjustment Exact t-TestWhen conducting exact two-sample t-test small sample sizes, sample size calculation involves non-central t-distribution. approximate correction can applied using critical values t-distribution instead z-distribution.adjusted sample size :\\[\nn^* = 2 \\left( \\frac{\\sigma (t_{2n-2; \\alpha/2} + t_{2n-2; \\beta})}{\\delta} \\right)^2\n\\]:\\(t_{2n-2; \\alpha/2}\\): critical value t-distribution \\(2n - 2\\) degrees freedom significance level \\(\\alpha/2\\),\\(t_{2n-2; \\alpha/2}\\): critical value t-distribution \\(2n - 2\\) degrees freedom significance level \\(\\alpha/2\\),\\(t_{2n-2; \\beta}\\): critical value t-distribution \\(2n - 2\\) degrees freedom power \\(1 - \\beta\\).\\(t_{2n-2; \\beta}\\): critical value t-distribution \\(2n - 2\\) degrees freedom power \\(1 - \\beta\\).correction adjusts increased variability t-distribution, especially important small sample sizes.Key InsightsZ-Test vs. T-Test:\nlarge samples, normal approximation (z-test) works well. small samples, t-test correction using t-distribution essential.Z-Test vs. T-Test:\nlarge samples, normal approximation (z-test) works well. small samples, t-test correction using t-distribution essential.Effect Power Significance Level:\nIncreasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.\nsmaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.\nEffect Power Significance Level:Increasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.Increasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.smaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.smaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.Two-Sided Tests:\nTwo-sided tests require larger sample sizes compared one-sided tests due split critical region.Two-Sided Tests:\nTwo-sided tests require larger sample sizes compared one-sided tests due split critical region.Formula Summary","code":"\n# Parameters\nalpha <- 0.05   # Significance level\nbeta <- 0.2     # Type II error rate (1 - Power = 0.2)\nsigma <- 1      # Common standard deviation\ndelta <- 0.5    # Minimum detectable difference\n\n# Critical values\nz_alpha <- qnorm(1 - alpha)\nz_beta <- qnorm(1 - beta)\n\n# Sample size calculation\nn <- (2 * sigma ^ 2 * (z_alpha + z_beta) ^ 2) / delta ^ 2\n\n# Output the required sample size (per group)\nceiling(n)\n#> [1] 50\n# Parameters\nalpha <- 0.05    # Significance level\npower <- 0.8     # Desired power\nsigma <- 1       # Common standard deviation\ndelta <- 0.5     # Minimum detectable difference\n\n# Calculate sample size for two-sided test\nsample_size <-\n    power.t.test(\n        delta = delta,\n        sd = sigma,\n        sig.level = alpha,\n        power = power,\n        type = \"two.sample\",\n        alternative = \"two.sided\"\n    )\n\n# Display results\nsample_size\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 63.76576\n#>           delta = 0.5\n#>              sd = 1\n#>       sig.level = 0.05\n#>           power = 0.8\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group"},{"path":"basic-statistical-inference.html","id":"matched-pair-designs","chapter":"4 Basic Statistical Inference","heading":"4.4.4 Matched Pair Designs","text":"matched pair designs, two treatments compared measuring responses subjects treatments. ensures effects subject--subject variability minimized, subject serves control.two treatments, data structured follows::\\(y_i\\) represents observation Treatment ,\\(y_i\\) represents observation Treatment ,\\(x_i\\) represents observation Treatment B,\\(x_i\\) represents observation Treatment B,\\(d_i = y_i - x_i\\) difference subject \\(\\).\\(d_i = y_i - x_i\\) difference subject \\(\\).AssumptionsObservations \\(y_i\\) \\(x_i\\) measured subjects, inducing correlation.differences \\(d_i\\) independent identically distributed (iid), follow normal distribution: \\[\nd_i \\sim N(\\mu_D, \\sigma_D^2)\n\\]Mean Variance DifferenceThe mean difference \\(\\mu_D\\) variance \\(\\sigma_D^2\\) given :\\[\n\\mu_D = E(y_i - x_i) = \\mu_y - \\mu_x\n\\]\\[\n\\sigma_D^2 = \\text{Var}(y_i - x_i) = \\text{Var}(y_i) + \\text{Var}(x_i) - 2 \\cdot \\text{Cov}(y_i, x_i)\n\\]covariance \\(y_i\\) \\(x_i\\) positive (typical case), variance differences \\(\\sigma_D^2\\) reduced compared independent sample case.key benefit Matched Pair Designs: reduced variability increases precision estimates.Sample StatisticsFor differences \\(d_i = y_i - x_i\\):sample mean differences: \\[\n\\bar{d} = \\frac{1}{n} \\sum_{=1}^n d_i = \\bar{y} - \\bar{x}\n\\]sample mean differences: \\[\n\\bar{d} = \\frac{1}{n} \\sum_{=1}^n d_i = \\bar{y} - \\bar{x}\n\\]sample variance differences: \\[\ns_d^2 = \\frac{1}{n-1} \\sum_{=1}^n (d_i - \\bar{d})^2\n\\]sample variance differences: \\[\ns_d^2 = \\frac{1}{n-1} \\sum_{=1}^n (d_i - \\bar{d})^2\n\\]data converted differences \\(d_i\\), problem reduces one-sample inference. can use tests confidence intervals (CIs) mean single sample.Hypothesis TestWe test following hypotheses:\\[\nH_0: \\mu_D = 0 \\quad \\text{vs.} \\quad H_a: \\mu_D \\neq 0\n\\]test statistic :\\[\nt = \\frac{\\bar{d}}{s_d / \\sqrt{n}} \\sim t_{n-1}\n\\]\\(n\\) number subjects.Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\n|t| > t_{n-1, \\alpha/2}\n\\]Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_D\\) :\\[\n\\bar{d} \\pm t_{n-1, \\alpha/2} \\cdot \\frac{s_d}{\\sqrt{n}}\n\\]output includes:t-statistic: calculated test statistic matched pairs.t-statistic: calculated test statistic matched pairs.p-value: probability observing difference null hypothesis.p-value: probability observing difference null hypothesis.Confidence Interval: range plausible values mean difference \\(\\mu_D\\).Confidence Interval: range plausible values mean difference \\(\\mu_D\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant difference two treatments.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant difference two treatments.confidence interval include 0, supports conclusion significant difference.confidence interval include 0, supports conclusion significant difference.Key InsightsReduced Variability: Positive correlation paired observations reduces variance differences, increasing test power.Reduced Variability: Positive correlation paired observations reduces variance differences, increasing test power.Use Differences: paired design converts data single-sample problem inference.Use Differences: paired design converts data single-sample problem inference.Robustness: paired t-test assumes normality differences \\(d_i\\). larger \\(n\\), Central Limit Theorem ensures robustness non-normality.Robustness: paired t-test assumes normality differences \\(d_i\\). larger \\(n\\), Central Limit Theorem ensures robustness non-normality.Matched pair designs powerful way control subject-specific variability, leading precise comparisons treatments.","code":"\n# Sample data\ntreatment_a <- c(85, 90, 78, 92, 88)\ntreatment_b <- c(80, 86, 75, 89, 85)\n\n# Compute differences\ndifferences <- treatment_a - treatment_b\n\n# Perform one-sample t-test on the differences\nt_test <- t.test(differences, mu = 0, alternative = \"two.sided\")\n\n# Display results\nt_test\n#> \n#>  One Sample t-test\n#> \n#> data:  differences\n#> t = 9, df = 4, p-value = 0.0008438\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  2.489422 4.710578\n#> sample estimates:\n#> mean of x \n#>       3.6"},{"path":"basic-statistical-inference.html","id":"nonparametric-tests-for-two-samples","chapter":"4 Basic Statistical Inference","heading":"4.4.5 Nonparametric Tests for Two Samples","text":"Matched Pair Designs independent samples normality assumed, use nonparametric tests. tests assume specific distribution data robust alternatives parametric methods.Stochastic Order Location ShiftSuppose \\(Y\\) \\(X\\) random variables cumulative distribution functions (CDFs) \\(F_Y\\) \\(F_X\\). \\(Y\\) stochastically larger \\(X\\) , real numbers \\(u\\):\\[\nP(Y > u) \\geq P(X > u) \\quad \\text{(equivalently, } F_Y(u) \\leq F_X(u)).\n\\]two distributions differ location parameters, say \\(\\theta_y\\) \\(\\theta_x\\), can frame relationship :\\[\nY > X \\quad \\text{} \\quad \\theta_y > \\theta_x.\n\\]test following hypotheses:Two-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y \\neq F_X\n\\]Upper One-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y < F_X\n\\]Lower One-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y > F_X\n\\]generally avoid completely non-directional alternative \\(H_a: F_Y \\neq F_X\\) allows arbitrary differences distributions, without requiring one distribution stochastically larger .Nonparametric TestsWhen focus whether two distributions differ location parameters, two equivalent nonparametric tests commonly used:Wilcoxon Signed Rank TestMann-Whitney U TestBoth tests mathematically equivalent test whether one sample systematically larger .","code":""},{"path":"basic-statistical-inference.html","id":"wilcoxon-rank-sum-test","chapter":"4 Basic Statistical Inference","heading":"4.4.5.1 Wilcoxon Rank-Sum Test","text":"Wilcoxon Rank Test nonparametric test used compare two independent samples assess whether distributions differ location. based ranks combined observations rather actual values.ProcedureCombine Rank Observations:\nCombine \\(n = n_y + n_x\\) observations (groups) single dataset rank ascending order. ties exist, assign average rank tied values.Combine Rank Observations:\nCombine \\(n = n_y + n_x\\) observations (groups) single dataset rank ascending order. ties exist, assign average rank tied values.Calculate Rank Sums:\nCompute sum ranks group:\n\\(w_y\\): Sum ranks group \\(y\\) (sample 1),\n\\(w_x\\): Sum ranks group \\(x\\) (sample 2).\ndefinition: \\[\nw_y + w_x = \\frac{n(n+1)}{2}\n\\]\nCalculate Rank Sums:\nCompute sum ranks group:\\(w_y\\): Sum ranks group \\(y\\) (sample 1),\\(w_x\\): Sum ranks group \\(x\\) (sample 2).\ndefinition: \\[\nw_y + w_x = \\frac{n(n+1)}{2}\n\\]Test Statistic:\ntest focuses rank sum \\(w_y\\). Reject \\(H_0\\) \\(w_y\\) large (indicating \\(y\\) systematically larger values) equivalently, \\(w_x\\) small.Test Statistic:\ntest focuses rank sum \\(w_y\\). Reject \\(H_0\\) \\(w_y\\) large (indicating \\(y\\) systematically larger values) equivalently, \\(w_x\\) small.Null Distribution:\n\\(H_0\\) (difference groups), possible arrangements ranks among \\(y\\) \\(x\\) equally likely. total number possible rank arrangements :\n\\[\n\\frac{(n_y + n_x)!}{n_y! \\, n_x!}\n\\]Null Distribution:\n\\(H_0\\) (difference groups), possible arrangements ranks among \\(y\\) \\(x\\) equally likely. total number possible rank arrangements :\\[\n\\frac{(n_y + n_x)!}{n_y! \\, n_x!}\n\\]Computational Considerations:\nsmall samples, exact null distribution rank sums can calculated.\nlarge samples, approximate normal distribution can used.\nComputational Considerations:small samples, exact null distribution rank sums can calculated.large samples, approximate normal distribution can used.HypothesesNull Hypothesis (\\(H_0\\)): two samples come identical distributions.Null Hypothesis (\\(H_0\\)): two samples come identical distributions.Alternative Hypothesis (\\(H_a\\)): two samples come different distributions, one distribution systematically larger.Alternative Hypothesis (\\(H_a\\)): two samples come different distributions, one distribution systematically larger.Two-Sided Test: \\[\nH_a: F_Y \\neq F_X\n\\]Two-Sided Test: \\[\nH_a: F_Y \\neq F_X\n\\]One-Sided Test: \\[\nH_a: F_Y > F_X \\quad \\text{} \\quad H_a: F_Y < F_X\n\\]One-Sided Test: \\[\nH_a: F_Y > F_X \\quad \\text{} \\quad H_a: F_Y < F_X\n\\]output wilcox.test includes:W: test statistic, smaller two rank sums.W: test statistic, smaller two rank sums.p-value: probability observing difference rank sums \\(H_0\\).p-value: probability observing difference rank sums \\(H_0\\).Alternative Hypothesis: Specifies whether test one-sided two-sided.Alternative Hypothesis: Specifies whether test one-sided two-sided.Confidence Interval (applicable): Provides range difference medians.Confidence Interval (applicable): Provides range difference medians.Decision RuleReject \\(H_0\\) significance level \\(\\alpha\\) p-value \\(\\leq \\alpha\\).Reject \\(H_0\\) significance level \\(\\alpha\\) p-value \\(\\leq \\alpha\\).large samples, compare test statistic critical value normal approximation.large samples, compare test statistic critical value normal approximation.Key FeaturesRobustness:\ntest require assumptions normality robust outliers.Robustness:\ntest require assumptions normality robust outliers.Distribution-Free:\nevaluates whether two samples differ location without assuming specific distribution.Distribution-Free:\nevaluates whether two samples differ location without assuming specific distribution.Rank-Based:\nuses ranks observations, makes scale-invariant (resistant data transformation).Rank-Based:\nuses ranks observations, makes scale-invariant (resistant data transformation).Computational ConsiderationsFor small sample sizes, exact distribution rank sums used.small sample sizes, exact distribution rank sums used.large sample sizes, normal approximation continuity correction applied computational efficiency.large sample sizes, normal approximation continuity correction applied computational efficiency.","code":"\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform Wilcoxon Rank Test (approximate version, large sample)\nwilcox_result <- wilcox.test(\n    irisVe,\n    irisVi,\n    alternative = \"two.sided\", # Two-sided test\n    conf.level = 0.95,         # Confidence level\n    exact = FALSE,             # Approximate test for large samples\n    correct = TRUE             # Apply continuity correction\n)\n\n# Display results\nwilcox_result\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  irisVe and irisVi\n#> W = 49, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"mann-whitney-u-test-1","chapter":"4 Basic Statistical Inference","heading":"4.4.5.2 Mann-Whitney U Test","text":"Mann-Whitney U Test nonparametric test used compare two independent samples. evaluates whether one sample tends produce larger observations , based pairwise comparisons. test assume normality robust outliers.ProcedurePairwise Comparisons:\nCompare observation \\(y_i\\) sample \\(Y\\) observation \\(x_j\\) sample \\(X\\).\nLet \\(u_y\\) number pairs \\(y_i > x_j\\).\nLet \\(u_x\\) number pairs \\(y_i < x_j\\).\ndefinition: \\[\nu_y + u_x = n_y n_x\n\\] \\(n_y\\) sample size group \\(Y\\), \\(n_x\\) sample size group \\(X\\).Pairwise Comparisons:\nCompare observation \\(y_i\\) sample \\(Y\\) observation \\(x_j\\) sample \\(X\\).Let \\(u_y\\) number pairs \\(y_i > x_j\\).Let \\(u_x\\) number pairs \\(y_i < x_j\\).definition: \\[\nu_y + u_x = n_y n_x\n\\] \\(n_y\\) sample size group \\(Y\\), \\(n_x\\) sample size group \\(X\\).Test Statistic:\nReject \\(H_0\\) \\(u_y\\) large (equivalently, \\(u_x\\) small).\nMann-Whitney U Test Wilcoxon Rank-Sum Test related rank sums:\n\\[\nu_y = w_y - \\frac{n_y (n_y + 1)}{2}, \\quad u_x = w_x - \\frac{n_x (n_x + 1)}{2}\n\\]\n, \\(w_y\\) \\(w_x\\) rank sums groups \\(Y\\) \\(X\\), respectively.Test Statistic:\nReject \\(H_0\\) \\(u_y\\) large (equivalently, \\(u_x\\) small).Mann-Whitney U Test Wilcoxon Rank-Sum Test related rank sums:\\[\nu_y = w_y - \\frac{n_y (n_y + 1)}{2}, \\quad u_x = w_x - \\frac{n_x (n_x + 1)}{2}\n\\], \\(w_y\\) \\(w_x\\) rank sums groups \\(Y\\) \\(X\\), respectively.HypothesesNull Hypothesis (\\(H_0\\)): two samples come identical distributions.Alternative Hypothesis (\\(H_a\\)):\nUpper One-Sided: \\(F_Y < F_X\\) (Sample \\(Y\\) stochastically larger).\nLower One-Sided: \\(F_Y > F_X\\) (Sample \\(X\\) stochastically larger).\nTwo-Sided: \\(F_Y \\neq F_X\\) (Distributions differ location).\nUpper One-Sided: \\(F_Y < F_X\\) (Sample \\(Y\\) stochastically larger).Lower One-Sided: \\(F_Y > F_X\\) (Sample \\(X\\) stochastically larger).Two-Sided: \\(F_Y \\neq F_X\\) (Distributions differ location).Test Statistic Large SamplesFor large sample sizes \\(n_y\\) \\(n_x\\), null distribution \\(U\\) can approximated normal distribution :Mean: \\[\nE(U) = \\frac{n_y n_x}{2}\n\\]Mean: \\[\nE(U) = \\frac{n_y n_x}{2}\n\\]Variance: \\[\n\\text{Var}(U) = \\frac{n_y n_x (n_y + n_x + 1)}{12}\n\\]Variance: \\[\n\\text{Var}(U) = \\frac{n_y n_x (n_y + n_x + 1)}{12}\n\\]standardized test statistic \\(z\\) :\\[\nz = \\frac{u_y - \\frac{n_y n_x}{2} - \\frac{1}{2}}{\\sqrt{\\frac{n_y n_x (n_y + n_x + 1)}{12}}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) :\\[\nz \\ge z_{\\alpha} \\quad \\text{(one-sided)} \\quad \\text{} \\quad |z| \\ge z_{\\alpha/2} \\quad \\text{(two-sided)}.\n\\]two-sided test, use:\\(u_{\\text{max}} = \\max(u_y, u_x)\\), \\(u_{\\text{max}} = \\max(u_y, u_x)\\), \\(u_{\\text{min}} = \\min(u_y, u_x)\\).\\(u_{\\text{min}} = \\min(u_y, u_x)\\).p-value given :\\[\np\\text{-value} = 2P(U \\ge u_{\\text{max}}) = 2P(U \\le u_{\\text{min}}).\n\\]\\(y_i = x_j\\) (ties), assign value \\(1/2\\) \\(u_y\\) \\(u_x\\) pair. exact sampling distribution differs slightly ties exist, large sample normal approximation remains reasonable.Decision RuleReject \\(H_0\\) p-value less \\(\\alpha\\).Reject \\(H_0\\) p-value less \\(\\alpha\\).large samples, check whether $z \\ge z_{\\alpha}$ (one-sided) $|z| \\ge z_{\\alpha/2}$ (two-sided).large samples, check whether $z \\ge z_{\\alpha}$ (one-sided) $|z| \\ge z_{\\alpha/2}$ (two-sided).Key InsightsRobustness: Mann-Whitney U Test assume normality robust outliers.Robustness: Mann-Whitney U Test assume normality robust outliers.Relationship Wilcoxon Test: test equivalent Wilcoxon Rank-Sum Test formulated differently (based pairwise comparisons).Relationship Wilcoxon Test: test equivalent Wilcoxon Rank-Sum Test formulated differently (based pairwise comparisons).Large Sample Approximation: large \\(n_y\\) \\(n_x\\), test statistic \\(U\\) follows approximate normal distribution, simplifying computation.Large Sample Approximation: large \\(n_y\\) \\(n_x\\), test statistic \\(U\\) follows approximate normal distribution, simplifying computation.Handling Ties: Ties accounted assigning fractional contributions \\(u_y\\) \\(u_x\\).Handling Ties: Ties accounted assigning fractional contributions \\(u_y\\) \\(u_x\\).","code":"\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform Mann-Whitney U Test\nmann_whitney <- wilcox.test(\n    irisVe, irisVi, \n    alternative = \"two.sided\", \n    conf.level = 0.95,\n    exact = FALSE,   # Approximate test for large samples\n    correct = TRUE   # Apply continuity correction\n)\n\n# Display results\nmann_whitney\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  irisVe and irisVi\n#> W = 49, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"categorical-data-analysis","chapter":"4 Basic Statistical Inference","heading":"4.5 Categorical Data Analysis","text":"Categorical Data Analysis used outcome variables categorical.Nominal Variables: Categories logical order (e.g., sex: male, female).Ordinal Variables: Categories logical order, relative distances values well defined (e.g., small, medium, large).categorical data, often analyze distribution one variable changes levels another variable. example, row percentages may differ across columns contingency table.","code":""},{"path":"basic-statistical-inference.html","id":"association-tests","chapter":"4 Basic Statistical Inference","heading":"4.5.1 Association Tests","text":"","code":""},{"path":"basic-statistical-inference.html","id":"small-samples","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1 Small Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"fishers-exact-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1.1 Fisher’s Exact Test","text":"small samples, approximate tests based asymptotic normality \\(\\hat{p}_1 - \\hat{p}_2\\) (difference proportions) hold. cases, use Fisher’s Exact Test evaluate:Null Hypothesis (\\(H_0\\)): \\(p_1 = p_2\\) (association variables),Alternative Hypothesis (\\(H_a\\)): \\(p_1 \\neq p_2\\) (association exists).Assumptions\\(X_1\\) \\(X_2\\) independent Binomial random variables:\n\\(X_1 \\sim \\text{Binomial}(n_1, p_1)\\),\n\\(X_2 \\sim \\text{Binomial}(n_2, p_2)\\).\n\\(X_1 \\sim \\text{Binomial}(n_1, p_1)\\),\\(X_2 \\sim \\text{Binomial}(n_2, p_2)\\).\\(x_1\\) \\(x_2\\) observed values (successes sample).Total sample size \\(n = n_1 + n_2\\).Total successes \\(m = x_1 + x_2\\).conditioning \\(m\\), total number successes, number successes sample 1 follows Hypergeometric distribution.Test StatisticTo test \\(H_0: p_1 = p_2\\) \\(H_a: p_1 \\neq p_2\\), use test statistic:\\[\nZ^2 = \\left( \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}} \\right)^2 \\sim \\chi^2_{1, \\alpha}\n\\]:\\(\\hat{p}_1\\) \\(\\hat{p}_2\\) observed proportions successes samples 1 2,\\(\\hat{p}_1\\) \\(\\hat{p}_2\\) observed proportions successes samples 1 2,\\(\\hat{p}\\) pooled proportion: \\[\n  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2},\n  \\]\\(\\hat{p}\\) pooled proportion: \\[\n  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2},\n  \\]\\(\\chi^2_{1, \\alpha}\\) upper \\(\\alpha\\) critical value Chi-squared distribution 1 degree freedom.\\(\\chi^2_{1, \\alpha}\\) upper \\(\\alpha\\) critical value Chi-squared distribution 1 degree freedom.Fisher’s Exact Test can extended contingency table setting test whether observed frequencies differ significantly expected frequencies null hypothesis association.output fisher.test() includes:p-value: probability observing contingency table null hypothesis.p-value: probability observing contingency table null hypothesis.Alternative Hypothesis: Indicates whether test two-sided one-sided.Alternative Hypothesis: Indicates whether test two-sided one-sided.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association two variables.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association two variables.","code":"\n# Create a 2x2 contingency table\ndata_table <- matrix(c(8, 2, 1, 5), nrow = 2, byrow = TRUE)\ncolnames(data_table) <- c(\"Success\", \"Failure\")\nrownames(data_table) <- c(\"Group 1\", \"Group 2\")\n\n# Display the table\ndata_table\n#>         Success Failure\n#> Group 1       8       2\n#> Group 2       1       5\n\n# Perform Fisher's Exact Test\nfisher_result <- fisher.test(data_table)\n\n# Display the results\nfisher_result\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  data_table\n#> p-value = 0.03497\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>     1.008849 1049.791446\n#> sample estimates:\n#> odds ratio \n#>   15.46969"},{"path":"basic-statistical-inference.html","id":"exact-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1.2 Exact Chi-Square Test","text":"small samples normal approximation apply, can compute exact Chi-Square test using Fisher’s Exact Test Monte Carlo simulation methods.Chi-Square test statistic 2x2 table :\\(\\chi^2 = \\sum_{=1}^r \\sum_{j=1}^c \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\):\\(O_{ij}\\): Observed frequency cell \\((, j)\\),\\(O_{ij}\\): Observed frequency cell \\((, j)\\),\\(E_{ij}\\): Expected frequency null hypothesis,\\(E_{ij}\\): Expected frequency null hypothesis,\\(r\\): Number rows,\\(r\\): Number rows,\\(c\\): Number columns.\\(c\\): Number columns.","code":""},{"path":"basic-statistical-inference.html","id":"large-samples","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2 Large Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"pearson-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2.1 Pearson Chi-Square Test","text":"Pearson Chi-Square Test commonly used test whether association two categorical variables. compares observed counts contingency table expected counts null hypothesis.test statistic :\\[\n\\chi^2 = \\sum_{\\text{cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]test applied settings multiple proportions frequencies compared across independent surveys experiments.Null Hypothesis (\\(H_0\\)): observed data consistent expected values (association deviation model).Alternative Hypothesis (\\(H_a\\)): observed data differ significantly expected values.Characteristics TestValidation Models:\ncases, \\(H_0\\) represents model whose validity tested. goal necessarily reject model check whether data consistent . Deviations may due random chance.Validation Models:\ncases, \\(H_0\\) represents model whose validity tested. goal necessarily reject model check whether data consistent . Deviations may due random chance.Strength Association:\nChi-Square Test detects whether association exists measure strength association. measuring strength, metrics like Cramér’s V Phi coefficient used.Strength Association:\nChi-Square Test detects whether association exists measure strength association. measuring strength, metrics like Cramér’s V Phi coefficient used.Effect Sample Size:\nChi-Square statistic reflects sample size. sample size doubled (e.g., duplicating observations), \\(\\chi^2\\) statistic also double, even though strength association remains unchanged.\nsensitivity can sometimes lead detecting significant results practically meaningful.\nEffect Sample Size:Chi-Square statistic reflects sample size. sample size doubled (e.g., duplicating observations), \\(\\chi^2\\) statistic also double, even though strength association remains unchanged.sensitivity can sometimes lead detecting significant results practically meaningful.Expected Cell Frequencies:\ntest appropriate 20% cells contingency table expected frequencies less 5.\nsmall sample sizes, Fisher’s Exact Test exact p-values used instead.\nExpected Cell Frequencies:test appropriate 20% cells contingency table expected frequencies less 5.small sample sizes, Fisher’s Exact Test exact p-values used instead.Test Single Proportion\ntest whether observed proportion successes equals 0.5.\\[\n\\begin{aligned}\nH_0&: p_J = 0.5 \\\\\nH_a&: p_J < 0.5\n\\end{aligned}\n\\]Test Equality Proportions Two Groups: test whether proportions successes July September equal.\\[\n\\begin{aligned}\nH_0&: p_J = p_S \\\\\nH_a&: p_j \\neq p_S\n\\end{aligned}\n\\]Comparison Proportions Multiple GroupsWe test null hypothesis:\\[\nH_0: p_1 = p_2 = \\dots = p_k\n\\]alternative least one proportion differs.Pooled ProportionAssuming \\(H_0\\) true, estimate common value probability success :\\[\n\\hat{p} = \\frac{x_1 + x_2 + \\dots + x_k}{n_1 + n_2 + \\dots + n_k}.\n\\]expected counts \\(H_0\\) :test statistic :\\[\n\\chi^2 = \\sum_{\\text{cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\\(k - 1\\) degrees freedom.Two-Way Contingency TablesWhen categorical data cross-classified, create two-way table observed counts.Sampling DesignsDesign 1: Total Sample Size Fixed\nsingle random sample size \\(n\\) drawn population.\nUnits cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.\ncell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]\nLet \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.\nNull Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]\nAlternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]\nDesign 1: Total Sample Size FixedA single random sample size \\(n\\) drawn population.single random sample size \\(n\\) drawn population.Units cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.Units cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.cell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]cell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]Let \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.Let \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.Null Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]Null Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]Alternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]Alternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]Design 2: Row Totals Fixed\nRandom samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.\nrow totals \\(n_{.}\\) fixed, column totals random.\nCounts row follow independent multinomial distributions.\nnull hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]\nAlternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]\nAlternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]\nDesign 2: Row Totals FixedRandom samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.Random samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.row totals \\(n_{.}\\) fixed, column totals random.row totals \\(n_{.}\\) fixed, column totals random.Counts row follow independent multinomial distributions.Counts row follow independent multinomial distributions.null hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]null hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]Alternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]Alternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]Alternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]Alternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]Designs?Real-World Sampling Constraints:\nSometimes, control row totals (e.g., fixed group sizes stratified sampling).\ntimes, collect data without predefined group sizes, totals emerge randomly.\nReal-World Sampling Constraints:Sometimes, control row totals (e.g., fixed group sizes stratified sampling).Sometimes, control row totals (e.g., fixed group sizes stratified sampling).times, collect data without predefined group sizes, totals emerge randomly.times, collect data without predefined group sizes, totals emerge randomly.Different Null Hypotheses:\nDesign 1 tests whether two variables independent (e.g., one variable predict ?).\nDesign 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).\nDifferent Null Hypotheses:Design 1 tests whether two variables independent (e.g., one variable predict ?).Design 1 tests whether two variables independent (e.g., one variable predict ?).Design 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).Design 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).counts contingency table come single multinomial sample row column totals random.counts contingency table come single multinomial sample row column totals random.Conclusion: Reject Null​. data suggests significant dependence row column variables.Conclusion: Reject Null​. data suggests significant dependence row column variables.Row totals fixed, column counts within row follow independent multinomial distributions.Row totals fixed, column counts within row follow independent multinomial distributions.Conclusion: Fail reject null. data provide evidence suggest differences column probabilities across rows.Conclusion: Fail reject null. data provide evidence suggest differences column probabilities across rows.Results Different?Data Generation Differences:\nDesign 1, entire table treated single multinomial sample. introduces dependencies counts table.\nDesign 2, rows generated independently, column probabilities tested consistency across rows.\nData Generation Differences:Design 1, entire table treated single multinomial sample. introduces dependencies counts table.Design 1, entire table treated single multinomial sample. introduces dependencies counts table.Design 2, rows generated independently, column probabilities tested consistency across rows.Design 2, rows generated independently, column probabilities tested consistency across rows.Null Hypotheses:\nDesign 1 tests independence row column variables (restrictive).\nDesign 2 tests homogeneity column probabilities across rows (less restrictive).\nNull Hypotheses:Design 1 tests independence row column variables (restrictive).Design 1 tests independence row column variables (restrictive).Design 2 tests homogeneity column probabilities across rows (less restrictive).Design 2 tests homogeneity column probabilities across rows (less restrictive).InterpretationThe results directly comparable null hypotheses different:\nDesign 1 focuses whether rows columns independent across entire table.\nDesign 2 focuses whether column distributions consistent across rows.\nresults directly comparable null hypotheses different:Design 1 focuses whether rows columns independent across entire table.Design 1 focuses whether rows columns independent across entire table.Design 2 focuses whether column distributions consistent across rows.Design 2 focuses whether column distributions consistent across rows.Real-World Implication:\ntesting independence (e.g., whether two variables unrelated), use Design 1.\ntesting consistency across groups (e.g., whether proportions across categories), use Design 2.\nReal-World Implication:testing independence (e.g., whether two variables unrelated), use Design 1.testing independence (e.g., whether two variables unrelated), use Design 1.testing consistency across groups (e.g., whether proportions across categories), use Design 2.testing consistency across groups (e.g., whether proportions across categories), use Design 2.TakeawaysThe tests use statistical machinery (Chi-squared test), interpretations differ based experimental design null hypothesis.tests use statistical machinery (Chi-squared test), interpretations differ based experimental design null hypothesis.dataset, differences assumptions can lead different conclusions.dataset, differences assumptions can lead different conclusions.","code":"\n# Observed data\njuly.x <- 480\njuly.n <- 1000\n# Test for single proportion\nprop.test(\n  x = july.x,\n  n = july.n,\n  p = 0.5,\n  alternative = \"less\",\n  correct = FALSE\n)\n#> \n#>  1-sample proportions test without continuity correction\n#> \n#> data:  july.x out of july.n, null probability 0.5\n#> X-squared = 1.6, df = 1, p-value = 0.103\n#> alternative hypothesis: true p is less than 0.5\n#> 95 percent confidence interval:\n#>  0.0000000 0.5060055\n#> sample estimates:\n#>    p \n#> 0.48\n# Observed data for two groups\nsept.x <- 704\nsept.n <- 1600\n# Test for equality of proportions\nprop.test(\n  x = c(july.x, sept.x),\n  n = c(july.n, sept.n),\n  correct = FALSE\n)\n#> \n#>  2-sample test for equality of proportions without continuity correction\n#> \n#> data:  c(july.x, sept.x) out of c(july.n, sept.n)\n#> X-squared = 3.9701, df = 1, p-value = 0.04632\n#> alternative hypothesis: two.sided\n#> 95 percent confidence interval:\n#>  0.0006247187 0.0793752813\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.48   0.44\n# Sampling Design 1: Total Sample Size Fixed\n# Parameters for the multinomial distribution\nr <- 3  # Number of rows\nc <- 4  # Number of columns\nn <- 100  # Total sample size\np <- matrix(c(0.1, 0.2, 0.1, 0.1,\n              0.05, 0.15, 0.05, 0.1,\n              0.05, 0.05, 0.025, 0.075), nrow = r, byrow = TRUE)\n\n# Generate a single random sample\nset.seed(123)  # For reproducibility\nn_ij <- rmultinom(1, size = n, prob = as.vector(p))\n\n# Reshape into a contingency table\ncontingency_table_fixed_total <- matrix(n_ij, nrow = r, ncol = c, byrow = TRUE)\nrownames(contingency_table_fixed_total) <- paste0(\"Row\", 1:r)\ncolnames(contingency_table_fixed_total) <- paste0(\"Col\", 1:c)\n\n# Hypothesis testing (Chi-squared test of independence)\nchisq_test_fixed_total <- chisq.test(contingency_table_fixed_total)\n\n# Display results\nprint(\"Contingency Table (Total Sample Size Fixed):\")\n#> [1] \"Contingency Table (Total Sample Size Fixed):\"\nprint(contingency_table_fixed_total)\n#>      Col1 Col2 Col3 Col4\n#> Row1    8    6    4   24\n#> Row2   18    1    9    7\n#> Row3    2    7    5    9\nprint(\"Chi-squared Test Results:\")\n#> [1] \"Chi-squared Test Results:\"\nprint(chisq_test_fixed_total)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  contingency_table_fixed_total\n#> X-squared = 28.271, df = 6, p-value = 8.355e-05\n# Sampling Design 2: Row Totals Fixed\n# Parameters for the fixed row totals\nn_row <- c(30, 40, 30)  # Row totals\nc <- 4  # Number of columns\np_col <- c(0.25, 0.25, 0.25, 0.25)  # Common column probabilities under H0\n\n# Generate independent multinomial samples for each row\nset.seed(123)  # For reproducibility\nrow_samples <- lapply(n_row, function(size) t(rmultinom(1, size, prob = p_col)))\n\n# Combine into a contingency table\ncontingency_table_fixed_rows <- do.call(rbind, row_samples)\nrownames(contingency_table_fixed_rows) <- paste0(\"Row\", 1:length(n_row))\ncolnames(contingency_table_fixed_rows) <- paste0(\"Col\", 1:c)\n\n# Hypothesis testing (Chi-squared test of homogeneity)\nchisq_test_fixed_rows <- chisq.test(contingency_table_fixed_rows)\n\n# Display results\nprint(\"Contingency Table (Row Totals Fixed):\")\n#> [1] \"Contingency Table (Row Totals Fixed):\"\nprint(contingency_table_fixed_rows)\n#>      Col1 Col2 Col3 Col4\n#> Row1    6   10    7    7\n#> Row2   13   13    4   10\n#> Row3    8   10    6    6\nprint(\"Chi-squared Test Results:\")\n#> [1] \"Chi-squared Test Results:\"\nprint(chisq_test_fixed_rows)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  contingency_table_fixed_rows\n#> X-squared = 3.2069, df = 6, p-value = 0.7825"},{"path":"basic-statistical-inference.html","id":"chi-square-test-for-independence","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2.2 Chi-Square Test for Independence","text":"expected frequencies \\(\\hat{e}_{ij}\\) null hypothesis :\\[\n\\hat{e}_{ij} = \\frac{n_{.} n_{.j}}{n_{..}},\n\\]\\(n_{.}\\) \\(n_{.j}\\) row column totals, respectively, \\(n_{..}\\) total sample size.test statistic :\\[\n\\chi^2 = \\sum_{=1}^r \\sum_{j=1}^c \\frac{(n_{ij} - \\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi^2_{(r-1)(c-1)}.\n\\]reject \\(H_0\\) significance level \\(\\alpha\\) :\\[\n\\chi^2 > \\chi^2_{(r-1)(c-1), \\alpha}.\n\\]Notes Pearson Chi-Square TestPurpose: Test association independence two categorical variables.Sensitivity Sample Size: \\(\\chi^2\\) statistic proportional sample size. Doubling sample size doubles \\(\\chi^2\\) even strength association remains unchanged.Assumption Expected Frequencies: test valid 20% expected cell counts less 5. cases, exact tests preferred.output includes:Chi-Square Statistic (\\(\\chi^2\\)): test statistic measuring deviation observed expected counts.Chi-Square Statistic (\\(\\chi^2\\)): test statistic measuring deviation observed expected counts.p-value: probability observing deviation \\(H_0\\).p-value: probability observing deviation \\(H_0\\).Degrees Freedom: \\((r-1)(c-1)\\) \\(r \\times c\\) table.Degrees Freedom: \\((r-1)(c-1)\\) \\(r \\times c\\) table.Expected Frequencies: table expected counts \\(H_0\\).Expected Frequencies: table expected counts \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association row column variables.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association row column variables.","code":"\n# Create a contingency table\ndata_table <- matrix(c(30, 10, 20, 40), nrow = 2, byrow = TRUE)\ncolnames(data_table) <- c(\"Category 1\", \"Category 2\")\nrownames(data_table) <- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nprint(data_table)\n#>         Category 1 Category 2\n#> Group 1         30         10\n#> Group 2         20         40\n\n# Perform Chi-Square Test\nchi_result <- chisq.test(data_table)\n\n# Display results\nchi_result\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  data_table\n#> X-squared = 15.042, df = 1, p-value = 0.0001052"},{"path":"basic-statistical-inference.html","id":"key-takeaways","chapter":"4 Basic Statistical Inference","heading":"4.5.1.3 Key Takeaways","text":"Fisher’s Exact Test specialized small samples fixed margins (2x2 tables).Fisher’s Exact Test specialized small samples fixed margins (2x2 tables).Exact Chi-Square Test broader version Fisher’s larger tables avoids asymptotic approximations.Exact Chi-Square Test broader version Fisher’s larger tables avoids asymptotic approximations.Pearson Chi-Square Test general framework, applications include:\nGoodness--fit testing.\nTesting independence (Chi-Square Test Independence).\nPearson Chi-Square Test general framework, applications include:Goodness--fit testing.Goodness--fit testing.Testing independence (Chi-Square Test Independence).Testing independence (Chi-Square Test Independence).Chi-Square Test Independence specific application Pearson Chi-Square Test.Chi-Square Test Independence specific application Pearson Chi-Square Test.essence:Fisher’s Exact Test Exact Chi-Square Test precise methods small datasets.Fisher’s Exact Test Exact Chi-Square Test precise methods small datasets.Pearson Chi-Square Test Chi-Square Test Independence interchangeable terms many contexts, focusing larger datasets.Pearson Chi-Square Test Chi-Square Test Independence interchangeable terms many contexts, focusing larger datasets.","code":""},{"path":"basic-statistical-inference.html","id":"ordinal-association","chapter":"4 Basic Statistical Inference","heading":"4.5.2 Ordinal Association","text":"Ordinal association refers relationship two variables levels one variable exhibit consistent pattern increase decrease response levels variable. type association particularly relevant dealing ordinal variables, naturally ordered categories, ratings (“poor”, “fair”, “good”, “excellent”) income brackets (“low”, “medium”, “high”).example:customer satisfaction ratings increase “poor” “excellent,” likelihood recommending product may also increase (positive ordinal association).customer satisfaction ratings increase “poor” “excellent,” likelihood recommending product may also increase (positive ordinal association).Alternatively, stress levels move “low” “high,” job performance may tend decrease (negative ordinal association).Alternatively, stress levels move “low” “high,” job performance may tend decrease (negative ordinal association).Key Characteristics Ordinal AssociationLogical Ordering Levels: levels variables must follow logical sequence. instance, “small,” “medium,” “large” logically ordered, whereas categories like “blue,” “round,” “tall” lack inherent order unsuitable ordinal association.Logical Ordering Levels: levels variables must follow logical sequence. instance, “small,” “medium,” “large” logically ordered, whereas categories like “blue,” “round,” “tall” lack inherent order unsuitable ordinal association.Monotonic Trends: association typically monotonic, meaning one variable moves specific direction, variable tends move consistent direction (either increasing decreasing).Monotonic Trends: association typically monotonic, meaning one variable moves specific direction, variable tends move consistent direction (either increasing decreasing).Tests Ordinal Association: Specialized statistical tests assess ordinal association, focusing rankings one variable relate . tests require data respect ordinal structure variables.Tests Ordinal Association: Specialized statistical tests assess ordinal association, focusing rankings one variable relate . tests require data respect ordinal structure variables.Practical ConsiderationsWhen using tests, keep mind:Ordinal Data Handling: Ensure data respects ordinal structure (e.g., categories correctly ranked coded).Ordinal Data Handling: Ensure data respects ordinal structure (e.g., categories correctly ranked coded).Sample Size: Larger sample sizes provide reliable estimates stronger test power.Sample Size: Larger sample sizes provide reliable estimates stronger test power.Contextual Relevance: Interpret results within context data research question. example, significant Spearman’s correlation imply causation rather consistent trend.Contextual Relevance: Interpret results within context data research question. example, significant Spearman’s correlation imply causation rather consistent trend.","code":""},{"path":"basic-statistical-inference.html","id":"mantel-haenszel-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.1 Mantel-Haenszel Chi-square Test","text":"Mantel-Haenszel Chi-square Test statistical tool evaluating ordinal associations, particularly data consists multiple \\(2 \\times 2\\) contingency tables examine association varying conditions strata. Unlike measures association correlation coefficients, test quantify strength association rather evaluates whether association exists controlling stratification.Mantel-Haenszel Test applicable \\(2 \\times 2 \\times K\\) contingency tables, \\(K\\) represents number strata. stratum \\(2 \\times 2\\) table corresponding different conditions subgroups.stratum \\(k\\), let marginal totals table :\\(n_{.1k}\\): Total observations column 1\\(n_{.1k}\\): Total observations column 1\\(n_{.2k}\\): Total observations column 2\\(n_{.2k}\\): Total observations column 2\\(n_{1.k}\\): Total observations row 1\\(n_{1.k}\\): Total observations row 1\\(n_{2.k}\\): Total observations row 2\\(n_{2.k}\\): Total observations row 2\\(n_{..k}\\): Total observations entire table\\(n_{..k}\\): Total observations entire tableThe observed cell count row 1 column 1 denoted \\(n_{11k}\\). Given marginal totals, sampling distribution \\(n_{11k}\\) follows hypergeometric distribution.assumption conditional independence:expected value \\(n_{11k}\\) : \\[\n  m_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}}\n  \\] variance \\(n_{11k}\\) : \\[\n  var(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2 (n_{..k} - 1)}\n  \\]Mantel Haenszel proposed test statistic:\\[\nM^2 = \\frac{\\left(|\\sum_k n_{11k} - \\sum_k m_{11k}| - 0.5\\right)^2}{\\sum_k var(n_{11k})} \\sim \\chi^2_{1}\n\\]whereThe 0.5 adjustment, known continuity correction, improves approximation \\(\\chi^2\\) distribution.0.5 adjustment, known continuity correction, improves approximation \\(\\chi^2\\) distribution.test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis conditional independence.test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis conditional independence.method can extended general \\(\\times J \\times K\\) contingency tables, \\(\\) \\(J\\) represent number rows columns, respectively, \\(K\\) number strata.Null Hypothesis (\\(H_0\\)):association two variables interest across strata, controlling confounder.\nmathematical terms:\\[\nH_0: \\text{Odds Ratio ()} = 1 \\; \\text{} \\; \\text{Risk Ratio (RR)} = 1\n\\]Alternative Hypothesis (\\(H_a\\)):association two variables interest across strata, controlling confounder.\nmathematical terms:\\[\nH_a: \\text{Odds Ratio ()} \\neq 1 \\; \\text{} \\; \\text{Risk Ratio (RR)} \\neq 1\n\\]Let’s consider scenario business wants evaluate relationship customer satisfaction (Satisfied vs. Satisfied) likelihood repeat purchases (Yes vs. ) across different regions (e.g., North, South, West). goal determine whether relationship holds consistently across regions.Calculate overall odds ratio (ignoring strata):Calculate conditional odds ratios region:Mantel-Haenszel Test evaluates whether relationship customer satisfaction repeat purchases remains consistent across regions:InterpretationOverall Odds Ratio: provides estimate overall association satisfaction repeat purchases, ignoring regional differences.Conditional Odds Ratios: show whether odds repeat purchases given satisfaction similar across regions.Mantel-Haenszel Test: significant test result (e.g., \\(p < 0.05\\)) suggests relationship satisfaction repeat purchases consistent across regions. Conversely, non-significant result implies regional differences may affect association. applying Mantel-Haenszel Test, businesses can determine marketing customer retention strategy uniformly applied customized account regional variations.\nstrong evidence suggest two variables interest associated across strata (North, South, West), even accounting potential confounding effects stratification.\ncommon odds ratio approximately \\(2.22\\) indicates substantial association, meaning outcome likely exposed group compared unexposed group.\nvariability stratum-specific odds ratios suggests strength association may differ slightly region, Mantel-Haenszel test assumes association consistent (homogeneous).\nstrong evidence suggest two variables interest associated across strata (North, South, West), even accounting potential confounding effects stratification.common odds ratio approximately \\(2.22\\) indicates substantial association, meaning outcome likely exposed group compared unexposed group.variability stratum-specific odds ratios suggests strength association may differ slightly region, Mantel-Haenszel test assumes association consistent (homogeneous).","code":"\n# Create a 2 x 2 x 3 contingency table\nCustomerData = array(\n    c(40, 30, 200, 300, 35, 20, 180, 265, 50, 25, 250, 275),\n    dim = c(2, 2, 3),\n    dimnames = list(\n        Satisfaction = c(\"Satisfied\", \"Not Satisfied\"),\n        RepeatPurchase = c(\"Yes\", \"No\"),\n        Region = c(\"North\", \"South\", \"West\")\n    )\n)\n\n# View marginal table (summarized across regions)\nmargin.table(CustomerData, c(1, 2))\n#>                RepeatPurchase\n#> Satisfaction    Yes  No\n#>   Satisfied     125 630\n#>   Not Satisfied  75 840\nlibrary(samplesizeCMH)\nmarginal_table = margin.table(CustomerData, c(1, 2))\nodds.ratio(marginal_table)\n#> [1] 2.222222\napply(CustomerData, 3, odds.ratio)\n#>    North    South     West \n#> 2.000000 2.576389 2.200000\nmantelhaen.test(CustomerData, correct = TRUE)\n#> \n#>  Mantel-Haenszel chi-squared test with continuity correction\n#> \n#> data:  CustomerData\n#> Mantel-Haenszel X-squared = 26.412, df = 1, p-value = 2.758e-07\n#> alternative hypothesis: true common odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  1.637116 3.014452\n#> sample estimates:\n#> common odds ratio \n#>          2.221488"},{"path":"basic-statistical-inference.html","id":"mcnemars-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.2 McNemar’s Test","text":"McNemar’s Test special case Mantel-Haenszel Chi-square Test, designed paired nominal data. particularly useful evaluating changes categorical responses treatment intervention, comparing paired responses matched samples. Unlike Mantel-Haenszel Test, handles stratified data, McNemar’s Test tailored situations single \\(2 \\times 2\\) table derived paired observations.McNemar’s Test assesses whether proportions discordant pairs (-diagonal elements \\(2 \\times 2\\) table) significantly different. Specifically, tests null hypothesis probabilities transitioning one category another equal.Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch B}) = P(\\text{Switch B })\n\\] implies probabilities transitioning one category equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) symmetric: \\[\nH_0: n_{12} = n_{21}\n\\]Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch B}) = P(\\text{Switch B })\n\\] implies probabilities transitioning one category equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) symmetric: \\[\nH_0: n_{12} = n_{21}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch B}) \\neq P(\\text{Switch B })\n\\] suggests probabilities transitioning categories equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) asymmetric: \\[\nH_A: n_{12} \\neq n_{21}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch B}) \\neq P(\\text{Switch B })\n\\] suggests probabilities transitioning categories equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) asymmetric: \\[\nH_A: n_{12} \\neq n_{21}\n\\]example, consider business analyzing whether new advertising campaign influences customer preference two products (B). customer surveyed campaign, resulting following \\(2 \\times 2\\) contingency table:rows: Preference Product B campaign.columns: Preference Product B campaign.Let table structure :\\(n_{12}\\): Customers switched Product B.\\(n_{21}\\): Customers switched Product B .test focuses \\(n_{12}\\) \\(n_{21}\\), represent discordant pairs.McNemar’s Test statistic : \\[\nM^2 = \\frac{(|n_{12} - n_{21}| - 0.5)^2}{n_{12} + n_{21}}\n\\] whereThe 0.5 continuity correction applied sample sizes small.0.5 continuity correction applied sample sizes small.null hypothesis preference change, \\(M^2\\) follows \\(\\chi^2\\) distribution 1 degree freedom.null hypothesis preference change, \\(M^2\\) follows \\(\\chi^2\\) distribution 1 degree freedom.Let’s analyze voting behavior study participants surveyed campaign. table represents:Rows: Voting preference campaign (Yes, ).Rows: Voting preference campaign (Yes, ).Columns: Voting preference campaign (Yes, ).Columns: Voting preference campaign (Yes, ).test provides:Test statistic (\\(M^2\\)): Quantifies asymmetry discordant pairs.Test statistic (\\(M^2\\)): Quantifies asymmetry discordant pairs.p-value: Indicates whether significant difference discordant proportions.p-value: Indicates whether significant difference discordant proportions.InterpretationTest Statistic: large \\(M^2\\) value suggests significant asymmetry discordant pairs.Test Statistic: large \\(M^2\\) value suggests significant asymmetry discordant pairs.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).\nhigh p-value fails reject null hypothesis, suggesting significant preference change.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).high p-value fails reject null hypothesis, suggesting significant preference change.high p-value fails reject null hypothesis, suggesting significant preference change.McNemar’s Test widely used business fields:Marketing Campaigns: Evaluating whether campaign shifts consumer preferences purchase intentions.Marketing Campaigns: Evaluating whether campaign shifts consumer preferences purchase intentions.Product Testing: Determining new feature redesign changes customer ratings.Product Testing: Determining new feature redesign changes customer ratings.Healthcare Studies: Analyzing treatment effects paired medical trials.Healthcare Studies: Analyzing treatment effects paired medical trials.","code":"\n# Voting preference before and after a campaign\nvote = matrix(c(682, 22, 86, 810), nrow = 2, byrow = TRUE,\n              dimnames = list(\n                \"Before\" = c(\"Yes\", \"No\"),\n                \"After\" = c(\"Yes\", \"No\")\n              ))\n\n# Perform McNemar's Test with continuity correction\nmcnemar_result <- mcnemar.test(vote, correct = TRUE)\nmcnemar_result\n#> \n#>  McNemar's Chi-squared test with continuity correction\n#> \n#> data:  vote\n#> McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09"},{"path":"basic-statistical-inference.html","id":"mcnemar-bowker-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.3 McNemar-Bowker Test","text":"McNemar-Bowker Test extension McNemar’s Test, designed analyzing paired nominal data two categories. evaluates symmetry full contingency table comparing -diagonal elements across categories. test particularly useful understanding whether changes categories uniformly distributed whether significant asymmetries exist.Let data structured \\(r \\times r\\) square contingency table, \\(r\\) number categories, -diagonal elements represent transitions categories.hypotheses McNemar-Bowker Test :Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) = P(\\text{Switch Category } j \\text{ Category } ) \\quad \\forall \\, \\neq j\n\\] implies -diagonal elements symmetric, directional preference category transitions.Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) = P(\\text{Switch Category } j \\text{ Category } ) \\quad \\forall \\, \\neq j\n\\] implies -diagonal elements symmetric, directional preference category transitions.Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) \\neq P(\\text{Switch Category } j \\text{ Category } ) \\quad \\text{least one pair } (, j)\n\\] suggests -diagonal elements symmetric, indicating directional preference transitions least one pair categories.Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) \\neq P(\\text{Switch Category } j \\text{ Category } ) \\quad \\text{least one pair } (, j)\n\\] suggests -diagonal elements symmetric, indicating directional preference transitions least one pair categories.McNemar-Bowker Test statistic : \\[\nB^2 = \\sum_{< j} \\frac{(n_{ij} - n_{ji})^2}{n_{ij} + n_{ji}}\n\\]\\(n_{ij}\\): Observed count transitions category \\(\\) category \\(j\\).\\(n_{ij}\\): Observed count transitions category \\(\\) category \\(j\\).\\(n_{ji}\\): Observed count transitions category \\(j\\) category \\(\\).\\(n_{ji}\\): Observed count transitions category \\(j\\) category \\(\\).null hypothesis, test statistic \\(B^2\\) approximately follows \\(\\chi^2\\) distribution \\(\\frac{r(r-1)}{2}\\) degrees freedom (corresponding number unique pairs categories).example, company surveys customers satisfaction implementing new policy. Satisfaction rated scale 1 3 (1 = Low, 2 = Medium, 3 = High). paired responses summarized following \\(3 \\times 3\\) contingency table.output includes:Test Statistic (\\(B^2\\)): measure asymmetry -diagonal elements.Test Statistic (\\(B^2\\)): measure asymmetry -diagonal elements.p-value: probability observing data null hypothesis symmetry.p-value: probability observing data null hypothesis symmetry.InterpretationTest Statistic: large $B^2$ value suggests substantial asymmetry transitions categories.Test Statistic: large $B^2$ value suggests substantial asymmetry transitions categories.p-value:\np-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.\np-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.\np-value:p-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.p-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.p-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.p-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.McNemar-Bowker Test broad applications business fields:Customer Feedback Analysis: Evaluating changes customer satisfaction levels interventions.Customer Feedback Analysis: Evaluating changes customer satisfaction levels interventions.Marketing Campaigns: Assessing shifts brand preferences across multiple brands response advertisement.Marketing Campaigns: Assessing shifts brand preferences across multiple brands response advertisement.Product Testing: Understanding user preferences among different product features change redesign.Product Testing: Understanding user preferences among different product features change redesign.","code":"\n# Satisfaction ratings before and after the intervention\nsatisfaction_table <- matrix(c(\n    30, 10, 5,  # Before: Low\n    8, 50, 12,  # Before: Medium\n    6, 10, 40   # Before: High\n), nrow = 3, byrow = TRUE,\ndimnames = list(\n    \"Before\" = c(\"Low\", \"Medium\", \"High\"),\n    \"After\" = c(\"Low\", \"Medium\", \"High\")\n))\n\n# Function to perform McNemar-Bowker Test\nmcnemar_bowker_test <- function(table) {\n  if (!all(dim(table)[1] == dim(table)[2])) {\n    stop(\"Input must be a square matrix.\")\n  }\n  \n  # Extract off-diagonal elements\n  n <- nrow(table)\n  stat <- 0\n  df <- 0\n  \n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      nij <- table[i, j]\n      nji <- table[j, i]\n      stat <- stat + (nij - nji)^2 / (nij + nji)\n      df <- df + 1\n    }\n  }\n  \n  p_value <- pchisq(stat, df = df, lower.tail = FALSE)\n  return(list(statistic = stat, df = df, p_value = p_value))\n}\n\n# Run the test\nresult <- mcnemar_bowker_test(satisfaction_table)\n\n# Print results\ncat(\"McNemar-Bowker Test Results:\\n\")\n#> McNemar-Bowker Test Results:\ncat(\"Test Statistic (B^2):\", result$statistic, \"\\n\")\n#> Test Statistic (B^2): 0.4949495\ncat(\"Degrees of Freedom:\", result$df, \"\\n\")\n#> Degrees of Freedom: 3\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.9199996"},{"path":"basic-statistical-inference.html","id":"stuart-maxwell-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.4 Stuart-Maxwell Test","text":"Stuart-Maxwell Test used analyzing changes paired categorical data two categories. generalization McNemar’s Test, applied square contingency tables -diagonal elements represent transitions categories. Unlike McNemar-Bowker Test, tests symmetry across pairs, Stuart-Maxwell Test focuses overall marginal homogeneity.test evaluates whether marginal distributions paired data consistent across categories. particularly useful investigating whether distribution responses shifted two conditions, intervention.Hypotheses Stuart-Maxwell TestNull Hypothesis (\\(H_0\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Stuart-Maxwell Test statistic calculated : \\[\nM^2 = \\mathbf{b}' \\mathbf{V}^{-1} \\mathbf{b}\n\\] :\\(\\mathbf{b}\\): Vector differences marginal totals paired categories.\\(\\mathbf{b}\\): Vector differences marginal totals paired categories.\\(\\mathbf{V}\\): Covariance matrix \\(\\mathbf{b}\\) null hypothesis.\\(\\mathbf{V}\\): Covariance matrix \\(\\mathbf{b}\\) null hypothesis.test statistic \\(M^2\\) follows \\(\\chi^2\\) distribution \\((r - 1)\\) degrees freedom, \\(r\\) number categories.company surveys employees satisfaction levels (Low, Medium, High) implementing new workplace policy. results summarized following \\(3 \\times 3\\) contingency table.InterpretationTest Statistic: Measures extent marginal differences table.Test Statistic: Measures extent marginal differences table.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.\nhigh p-value suggests evidence marginal differences, meaning distribution consistent across conditions.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.low p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.high p-value suggests evidence marginal differences, meaning distribution consistent across conditions.high p-value suggests evidence marginal differences, meaning distribution consistent across conditions.Practical Applications Stuart-Maxwell TestEmployee Surveys: Analyzing shifts satisfaction levels policy changes.Employee Surveys: Analyzing shifts satisfaction levels policy changes.Consumer Studies: Evaluating changes product preferences marketing campaign.Consumer Studies: Evaluating changes product preferences marketing campaign.Healthcare Research: Assessing changes patient responses treatments across categories.Healthcare Research: Assessing changes patient responses treatments across categories.","code":"\n# Employee satisfaction data before and after a policy change\nsatisfaction_table <- matrix(c(\n    40, 10, 5,  # Before: Low\n    8, 50, 12,  # Before: Medium\n    6, 10, 40   # Before: High\n), nrow = 3, byrow = TRUE,\ndimnames = list(\n    \"Before\" = c(\"Low\", \"Medium\", \"High\"),\n    \"After\" = c(\"Low\", \"Medium\", \"High\")\n))\n\n# Function to perform the Stuart-Maxwell Test\nstuart_maxwell_test <- function(table) {\n  if (!all(dim(table)[1] == dim(table)[2])) {\n    stop(\"Input must be a square matrix.\")\n  }\n  \n  # Marginal totals for each category\n  row_totals <- rowSums(table)\n  col_totals <- colSums(table)\n  \n  # Vector of differences between row and column marginal totals\n  b <- row_totals - col_totals\n  \n  # Covariance matrix under the null hypothesis\n  total <- sum(table)\n  V <- diag(row_totals + col_totals) - \n       (outer(row_totals, col_totals, \"+\") / total)\n  \n  # Calculate the test statistic\n  M2 <- t(b) %*% solve(V) %*% b\n  df <- nrow(table) - 1\n  p_value <- pchisq(M2, df = df, lower.tail = FALSE)\n  \n  return(list(statistic = M2, df = df, p_value = p_value))\n}\n\n# Run the Stuart-Maxwell Test\nresult <- stuart_maxwell_test(satisfaction_table)\n\n# Print the results\ncat(\"Stuart-Maxwell Test Results:\\n\")\n#> Stuart-Maxwell Test Results:\ncat(\"Test Statistic (M^2):\", result$statistic, \"\\n\")\n#> Test Statistic (M^2): 0.01802387\ncat(\"Degrees of Freedom:\", result$df, \"\\n\")\n#> Degrees of Freedom: 2\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.9910286"},{"path":"basic-statistical-inference.html","id":"cochran-mantel-haenszel-cmh-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.5 Cochran-Mantel-Haenszel (CMH) Test","text":"Cochran-Mantel-Haenszel (CMH) Test generalization Mantel-Haenszel Chi-square Test. evaluates association two variables controlling effect third stratifying variable. test particularly suited ordinal data, allowing researchers detect trends associations across strata.CMH Test addresses scenarios :Two variables (e.g., exposure outcome) ordinal nominal.third variable (e.g., demographic environmental factor) stratifies data \\(K\\) independent groups.test answers: consistent association two variables across strata defined third variable?CMH Test three main variations depending nature data:Correlation Test Ordinal Data: Assesses whether linear association two ordinal variables across strata.General Association Test: Tests association (necessarily ordinal) two variables stratifying third.Homogeneity Test: Checks whether strength association two variables consistent across strata.HypothesesNull Hypothesis (\\(H_0\\)): association two variables across strata, strength association consistent across strata.Null Hypothesis (\\(H_0\\)): association two variables across strata, strength association consistent across strata.Alternative Hypothesis (\\(H_A\\)): association two variables least one stratum, strength association varies across strata.Alternative Hypothesis (\\(H_A\\)): association two variables least one stratum, strength association varies across strata.CMH test statistic : \\[\nCMH = \\frac{\\left( \\sum_{k} \\left(O_k - E_k \\right)\\right)^2}{\\sum_{k} V_k}\n\\] :\\(O_k\\): Observed counts stratum \\(k\\).\\(O_k\\): Observed counts stratum \\(k\\).\\(E_k\\): Expected counts stratum \\(k\\), calculated null hypothesis.\\(E_k\\): Expected counts stratum \\(k\\), calculated null hypothesis.\\(V_k\\): Variance observed counts stratum \\(k\\).\\(V_k\\): Variance observed counts stratum \\(k\\).test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis.company evaluates whether sales performance (Low, Medium, High) associated product satisfaction (Low, Medium, High) across three experience levels (Junior, Mid-level, Senior). data organized \\(3 \\times 3 \\times 3\\) contingency table.InterpretationTest Statistic: large CMH statistic suggests significant association sales performance satisfaction accounting experience level.Test Statistic: large CMH statistic suggests significant association sales performance satisfaction accounting experience level.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.\nhigh p-value suggests evidence association relationship consistent across strata.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.low p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.high p-value suggests evidence association relationship consistent across strata.high p-value suggests evidence association relationship consistent across strata.Practical Applications CMH TestBusiness Performance Analysis: Investigating relationship customer satisfaction sales performance across different demographic groups.Business Performance Analysis: Investigating relationship customer satisfaction sales performance across different demographic groups.Healthcare Studies: Assessing effect treatment (e.g., dosage) outcomes controlling patient characteristics (e.g., age groups).Healthcare Studies: Assessing effect treatment (e.g., dosage) outcomes controlling patient characteristics (e.g., age groups).Educational Research: Analyzing relationship test scores study hours, stratified teaching method.Educational Research: Analyzing relationship test scores study hours, stratified teaching method.","code":"\n# Sales performance data\nsales_data <- array(\n    c(20, 15, 10, 12, 18, 15, 8, 12, 20,   # Junior\n      25, 20, 15, 20, 25, 30, 10, 15, 20,  # Mid-level\n      30, 25, 20, 28, 32, 35, 15, 20, 30), # Senior\n    dim = c(3, 3, 3),\n    dimnames = list(\n        SalesPerformance = c(\"Low\", \"Medium\", \"High\"),\n        Satisfaction = c(\"Low\", \"Medium\", \"High\"),\n        ExperienceLevel = c(\"Junior\", \"Mid-level\", \"Senior\")\n    )\n)\n\n# Load the vcd package for the CMH test\nlibrary(vcd)\n\n# Perform CMH Test\ncmh_result <- mantelhaen.test(sales_data, correct = FALSE)\ncmh_result\n#> \n#>  Cochran-Mantel-Haenszel test\n#> \n#> data:  sales_data\n#> Cochran-Mantel-Haenszel M^2 = 22.454, df = 4, p-value = 0.0001627"},{"path":"basic-statistical-inference.html","id":"summary-table-of-tests","chapter":"4 Basic Statistical Inference","heading":"4.5.2.6 Summary Table of Tests","text":"following table provides concise guide use test:Choose Right TestPaired vs. Stratified Data:\nUse McNemar’s Test McNemar-Bowker Test paired data.\nUse Mantel-Haenszel CMH Test stratified data.\nUse McNemar’s Test McNemar-Bowker Test paired data.Use Mantel-Haenszel CMH Test stratified data.Binary vs. Multi-category Variables:\nUse McNemar’s Test binary data.\nUse McNemar-Bowker Test Stuart-Maxwell Test multi-category data.\nUse McNemar’s Test binary data.Use McNemar-Bowker Test Stuart-Maxwell Test multi-category data.Ordinal Trends:\nUse CMH Test testing ordinal associations controlling stratifying variable.\nUse CMH Test testing ordinal associations controlling stratifying variable.","code":""},{"path":"basic-statistical-inference.html","id":"ordinal-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3 Ordinal Trend","text":"analyzing ordinal data, often important determine whether consistent trend exists variables. Tests trend specifically designed detect monotonic relationships changes one variable systematically associated changes another. tests widely used scenarios involving ordered categories, customer satisfaction ratings, income brackets, educational levels.primary objectives trend tests :detect monotonic relationships: Determine higher lower categories one variable associated higher lower categories another variable.account ordinal structure: Leverage inherent order data provide sensitive interpretable results compared tests designed nominal data.Key Considerations Trend TestsData Structure:\nEnsure variables natural order treated ordinal.\nVerify trend test chosen matches data structure (e.g., binary outcome vs. multi-level ordinal variables).\nEnsure variables natural order treated ordinal.Verify trend test chosen matches data structure (e.g., binary outcome vs. multi-level ordinal variables).Assumptions:\nMany tests assume monotonic trends, meaning relationship reverse direction.\nMany tests assume monotonic trends, meaning relationship reverse direction.Interpretation:\nsignificant result indicates presence trend imply causality.\ndirection strength trend carefully interpreted context data.\nsignificant result indicates presence trend imply causality.direction strength trend carefully interpreted context data.","code":""},{"path":"basic-statistical-inference.html","id":"cochran-armitage-test","chapter":"4 Basic Statistical Inference","heading":"4.5.3.1 Cochran-Armitage Test","text":"Cochran-Armitage Test Trend statistical method designed detect linear trend proportions across ordered categories predictor variable. particularly useful \\(2 \\times J\\) contingency tables, binary outcome (e.g., success/failure) ordinal predictor variable \\(J\\) ordered levels.Cochran-Armitage Test evaluates whether proportion binary outcome changes systematically across levels ordinal predictor. test leverages ordinal nature predictor enhance sensitivity power compared general chi-square tests.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{proportion binary outcome constant across levels ordinal predictor.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{proportion binary outcome constant across levels ordinal predictor.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{linear trend proportion binary outcome across levels ordinal predictor.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{linear trend proportion binary outcome across levels ordinal predictor.}\n\\]Cochran-Armitage Test statistic calculated :\\[\nZ = \\frac{\\sum_{j=1}^{J} w_j (n_{1j} - N_j \\hat{p})}{\\sqrt{\\hat{p} (1 - \\hat{p}) \\sum_{j=1}^{J} w_j^2 N_j}}\n\\]:\\(n_{1j}\\): Count binary outcome (e.g., “success”) category \\(j\\).\\(n_{1j}\\): Count binary outcome (e.g., “success”) category \\(j\\).\\(N_j\\): Total number observations category \\(j\\).\\(N_j\\): Total number observations category \\(j\\).\\(\\hat{p}\\): Overall proportion binary outcome, calculated : \\[\n  \\hat{p} = \\frac{\\sum_{j=1}^{J} n_{1j}}{\\sum_{j=1}^{J} N_j}\n  \\]\\(\\hat{p}\\): Overall proportion binary outcome, calculated : \\[\n  \\hat{p} = \\frac{\\sum_{j=1}^{J} n_{1j}}{\\sum_{j=1}^{J} N_j}\n  \\]\\(w_j\\): Score assigned \\(j\\)th category ordinal predictor, often set \\(j\\) equally spaced levels.\\(w_j\\): Score assigned \\(j\\)th category ordinal predictor, often set \\(j\\) equally spaced levels.test statistic \\(Z\\) follows standard normal distribution null hypothesis.Key AssumptionsOrdinal Predictor: categories predictor variable must natural order.Binary Outcome: response variable must dichotomous (e.g., success/failure).Independent Observations: Observations within across categories independent.Let’s consider study examining whether success rate marketing campaign varies across three income levels (Low, Medium, High). data structured \\(2 \\times 3\\) contingency table:InterpretationTest Statistic (\\(Z\\)):\n\\(Z\\) value indicates strength direction trend.\nPositive \\(Z\\): Proportions increase higher categories.\nNegative \\(Z\\): Proportions decrease higher categories.\nTest Statistic (\\(Z\\)):\\(Z\\) value indicates strength direction trend.\\(Z\\) value indicates strength direction trend.Positive \\(Z\\): Proportions increase higher categories.Positive \\(Z\\): Proportions increase higher categories.Negative \\(Z\\): Proportions decrease higher categories.Negative \\(Z\\): Proportions decrease higher categories.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.\nhigh p-value fails reject null hypothesis, suggesting evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.high p-value fails reject null hypothesis, suggesting evidence trend.high p-value fails reject null hypothesis, suggesting evidence trend.Practical ApplicationsMarketing: Analyzing whether customer success rates vary systematically across income levels demographics.Marketing: Analyzing whether customer success rates vary systematically across income levels demographics.Healthcare: Evaluating dose-response relationship medication levels recovery rates.Healthcare: Evaluating dose-response relationship medication levels recovery rates.Education: Studying whether pass rates improve higher levels educational support.Education: Studying whether pass rates improve higher levels educational support.","code":"\n# Data: Success and Failure counts by Income Level\nincome_levels <- c(\"Low\", \"Medium\", \"High\")\nsuccess <- c(20, 35, 45)\nfailure <- c(30, 15, 5)\ntotal <- success + failure\n\n# Scores for ordinal levels (can be custom weights)\nscores <- 1:length(income_levels)\n\n# Cochran-Armitage Test\n# Function to calculate Z statistic\ncochran_armitage_test <- function(success, failure, scores) {\n  N <- success + failure\n  p_hat <- sum(success) / sum(N)\n  weights <- scores\n  \n  # Calculate numerator\n  numerator <- sum(weights * (success - N * p_hat))\n  \n  # Calculate denominator\n  denominator <- sqrt(p_hat * (1 - p_hat) * sum(weights^2 * N))\n  \n  # Z statistic\n  Z <- numerator / denominator\n  p_value <- 2 * (1 - pnorm(abs(Z)))\n  \n  return(list(Z_statistic = Z, p_value = p_value))\n}\n\n# Perform the test\nresult <- cochran_armitage_test(success, failure, scores)\n\n# Print results\ncat(\"Cochran-Armitage Test for Trend Results:\\n\")\n#> Cochran-Armitage Test for Trend Results:\ncat(\"Z Statistic:\", result$Z_statistic, \"\\n\")\n#> Z Statistic: 2.004459\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.04502088"},{"path":"basic-statistical-inference.html","id":"jonckheere-terpstra-test","chapter":"4 Basic Statistical Inference","heading":"4.5.3.2 Jonckheere-Terpstra Test","text":"Jonckheere-Terpstra Test nonparametric test designed detect ordered differences groups. particularly suited ordinal data predictor response variables exhibit monotonic trend. Unlike general nonparametric tests like Kruskal-Wallis test, assess differences groups, Jonckheere-Terpstra Test specifically evaluates whether data follows prespecified ordering.Jonckheere-Terpstra Test determines whether:monotonic trend response variable across ordered groups predictor.data aligns priori hypothesized order (e.g., group 1 < group 2 < group 3).HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{trend response variable across ordered groups.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{trend response variable across ordered groups.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{response variable exhibits monotonic trend across ordered groups.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{response variable exhibits monotonic trend across ordered groups.}\n\\]trend can increasing, decreasing, otherwise hypothesized.Jonckheere-Terpstra Test statistic based number pairwise comparisons (\\(U\\)) consistent hypothesized trend. \\(k\\) groups:Compare possible pairs observations across groups.Count number pairs values consistent hypothesized order.test statistic \\(T\\) sum pairwise comparisons: \\[\nT = \\sum_{< j} T_{ij}\n\\] \\(T_{ij}\\) number concordant pairs groups \\(\\) \\(j\\).null hypothesis, \\(T\\) follows normal distribution :Mean: \\[\n  \\mu_T = \\frac{N (N - 1)}{4}\n  \\]Mean: \\[\n  \\mu_T = \\frac{N (N - 1)}{4}\n  \\]Variance: \\[\n  \\sigma_T^2 = \\frac{N (N - 1) (2N + 1)}{24}\n  \\] \\(N\\) total number observations.Variance: \\[\n  \\sigma_T^2 = \\frac{N (N - 1) (2N + 1)}{24}\n  \\] \\(N\\) total number observations.standardized test statistic : \\[\nZ = \\frac{T - \\mu_T}{\\sigma_T}\n\\]Key AssumptionsOrdinal Interval Data: response variable must least ordinal, groups must logical order.Independent Groups: Observations within groups independent.Consistent Hypothesis: trend (e.g., increasing decreasing) must specified advance.Let’s consider study analyzing whether customer satisfaction ratings (scale 1 5) improve increasing levels service tiers (Basic, Standard, Premium). data grouped service tier, hypothesize satisfaction ratings increase higher service tiers.InterpretationTest Statistic (\\(T\\)):\nRepresents sum pairwise comparisons consistent hypothesized order.\nIncludes 0.5 tied pairs.\nTest Statistic (\\(T\\)):Represents sum pairwise comparisons consistent hypothesized order.Includes 0.5 tied pairs.\\(Z\\) Statistic:\nstandardized measure strength trend.\nCalculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).\n\\(Z\\) Statistic:standardized measure strength trend.standardized measure strength trend.Calculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).Calculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.\nhigh p-value fails reject null hypothesis, suggesting evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.high p-value fails reject null hypothesis, suggesting evidence trend.high p-value fails reject null hypothesis, suggesting evidence trend.Practical ApplicationsCustomer Experience Analysis: Assessing whether customer satisfaction increases higher service levels product tiers.Customer Experience Analysis: Assessing whether customer satisfaction increases higher service levels product tiers.Healthcare Studies: Testing whether recovery rates improve increasing doses treatment.Healthcare Studies: Testing whether recovery rates improve increasing doses treatment.Education Research: Analyzing whether test scores improve higher levels educational intervention.Education Research: Analyzing whether test scores improve higher levels educational intervention.","code":"\n# Example Data: Customer Satisfaction Ratings by Service Tier\nsatisfaction <- list(\n  Basic = c(2, 3, 2, 4, 3),\n  Standard = c(3, 4, 3, 5, 4),\n  Premium = c(4, 5, 4, 5, 5)\n)\n\n# Prepare data\nratings <- unlist(satisfaction)\ngroups <- factor(rep(names(satisfaction), times = sapply(satisfaction, length)))\n\n# Calculate pairwise comparisons\nmanual_jonckheere <- function(ratings, groups) {\n  n_groups <- length(unique(groups))\n  pairwise_comparisons <- 0\n  total_pairs <- 0\n  \n  # Iterate over group pairs\n  for (i in 1:(n_groups - 1)) {\n    for (j in (i + 1):n_groups) {\n      group_i <- ratings[groups == levels(groups)[i]]\n      group_j <- ratings[groups == levels(groups)[j]]\n      \n      # Count concordant pairs\n      for (x in group_i) {\n        for (y in group_j) {\n          if (x < y) pairwise_comparisons <- pairwise_comparisons + 1\n          if (x == y) pairwise_comparisons <- pairwise_comparisons + 0.5\n          total_pairs <- total_pairs + 1\n        }\n      }\n    }\n  }\n  \n  # Compute test statistic\n  T <- pairwise_comparisons\n  N <- length(ratings)\n  mu_T <- total_pairs / 2\n  sigma_T <- sqrt(total_pairs * (N + 1) / 12)\n  \n  Z <- (T - mu_T) / sigma_T\n  p_value <- 2 * (1 - pnorm(abs(Z)))\n  \n  return(list(T_statistic = T, Z_statistic = Z, p_value = p_value))\n}\n\n# Perform the test\nresult <- manual_jonckheere(ratings, groups)\n\n# Print results\ncat(\"Jonckheere-Terpstra Test Results:\\n\")\n#> Jonckheere-Terpstra Test Results:\ncat(\"T Statistic (Sum of Concordant Pairs):\", result$T_statistic, \"\\n\")\n#> T Statistic (Sum of Concordant Pairs): 49.5\ncat(\"Z Statistic:\", result$Z_statistic, \"\\n\")\n#> Z Statistic: 1.2\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.2301393"},{"path":"basic-statistical-inference.html","id":"mantel-test-for-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3.3 Mantel Test for Trend","text":"Mantel Test Trend statistical method designed detect linear association two ordinal variables. extension Mantel-Haenszel Chi-square Test particularly suited analyzing trends ordinal contingency tables, \\(\\times J\\) tables variables ordinal.Mantel Test Trend evaluates whether increasing decreasing trend exists two ordinal variables. uses ordering categories assess linear relationships, making sensitive trends compared general association tests like chi-square.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{linear association two ordinal variables.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{linear association two ordinal variables.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear association two ordinal variables.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear association two ordinal variables.}\n\\]Mantel Test based Pearson correlation row column scores ordinal contingency table. test statistic : \\[\nM = \\frac{\\sum_{} \\sum_{j} w_i w_j n_{ij}}{\\sqrt{\\sum_{} w_i^2 n_{\\cdot} \\sum_{j} w_j^2 n_{\\cdot j}}}\n\\]:\\(n_{ij}\\): Observed frequency cell \\((, j)\\).\\(n_{ij}\\): Observed frequency cell \\((, j)\\).\\(n_{\\cdot}\\): Row marginal total row \\(\\).\\(n_{\\cdot}\\): Row marginal total row \\(\\).\\(n_{\\cdot j}\\): Column marginal total column \\(j\\).\\(n_{\\cdot j}\\): Column marginal total column \\(j\\).\\(w_i\\): Score \\(\\)th row.\\(w_i\\): Score \\(\\)th row.ore \\(j\\)th column.ore \\(j\\)th column.test statistic \\(M\\) asymptotically normally distributed null hypothesis.Key AssumptionsOrdinal Variables: variables must natural order.Linear Trend: Assumes linear relationship scores assigned rows columns.Independence: Observations must independent.Let’s consider marketing study evaluating whether customer satisfaction levels (Low, Medium, High) associated increasing purchase frequency (Low, Medium, High).InterpretationTest Statistic (\\(M\\)):\nRepresents strength direction linear association.\nPositive \\(M\\): Increasing trend.\nNegative \\(M\\): Decreasing trend.\nTest Statistic (\\(M\\)):Represents strength direction linear association.Represents strength direction linear association.Positive \\(M\\): Increasing trend.Positive \\(M\\): Increasing trend.Negative \\(M\\): Decreasing trend.Negative \\(M\\): Decreasing trend.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.\nhigh p-value suggests evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.low p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.high p-value suggests evidence trend.high p-value suggests evidence trend.Practical ApplicationsMarketing Analysis: Investigating whether satisfaction levels associated purchase behavior loyalty.Marketing Analysis: Investigating whether satisfaction levels associated purchase behavior loyalty.Healthcare Research: Testing dose-response relationship treatment levels outcomes.Healthcare Research: Testing dose-response relationship treatment levels outcomes.Social Sciences: Analyzing trends survey responses across ordered categories.Social Sciences: Analyzing trends survey responses across ordered categories.","code":"\n# Customer satisfaction and purchase frequency data\ndata <- matrix(\n  c(10, 5, 2, 15, 20, 8, 25, 30, 12), \n  nrow = 3, \n  byrow = TRUE,\n  dimnames = list(\n    Satisfaction = c(\"Low\", \"Medium\", \"High\"),\n    Frequency = c(\"Low\", \"Medium\", \"High\")\n  )\n)\n\n# Assign scores for rows and columns\nrow_scores <- 1:nrow(data)\ncol_scores <- 1:ncol(data)\n\n# Compute Mantel statistic manually\nmantel_test_manual <- function(data, row_scores, col_scores) {\n  numerator <- sum(outer(row_scores, col_scores, \"*\") * data)\n  row_marginals <- rowSums(data)\n  col_marginals <- colSums(data)\n  row_variance <- sum(row_scores^2 * row_marginals)\n  col_variance <- sum(col_scores^2 * col_marginals)\n  \n  M <- numerator / sqrt(row_variance * col_variance)\n  z_value <- M\n  p_value <- 2 * (1 - pnorm(abs(z_value))) # Two-tailed test\n  \n  return(list(Mantel_statistic = M, p_value = p_value))\n}\n\n# Perform the Mantel Test\nresult <- mantel_test_manual(data, row_scores, col_scores)\n\n# Display results\ncat(\"Mantel Test for Trend Results:\\n\")\n#> Mantel Test for Trend Results:\ncat(\"Mantel Statistic (M):\", result$Mantel_statistic, \"\\n\")\n#> Mantel Statistic (M): 0.8984663\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.368937"},{"path":"basic-statistical-inference.html","id":"chi-square-test-for-linear-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3.4 Chi-square Test for Linear Trend","text":"Chi-square Test Linear Trend statistical method used detect linear relationship ordinal predictor binary outcome. extension chi-square test, designed specifically ordered categories, making sensitive linear trends proportions compared general chi-square test independence.Chi-square Test Linear Trend evaluates whether proportions binary outcome (e.g., success/failure) change systematically across ordered categories predictor variable. widely used situations analyzing dose-response relationships evaluating trends survey responses.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{linear trend proportions binary outcome across ordered categories.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{linear trend proportions binary outcome across ordered categories.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear trend proportions binary outcome across ordered categories.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear trend proportions binary outcome across ordered categories.}\n\\]test statistic :\\[\nX^2_{\\text{trend}} = \\frac{\\left( \\sum_{j=1}^J w_j (p_j - \\bar{p}) N_j \\right)^2}{\\sum_{j=1}^J w_j^2 \\bar{p} (1 - \\bar{p}) N_j}\n\\]: - \\(J\\): Number ordered categories. - \\(w_j\\): Scores assigned \\(j\\)th category (typically \\(j = 1, 2, \\dots, J\\)). - \\(p_j\\): Proportion success \\(j\\)th category. - \\(\\bar{p}\\): Overall proportion success across categories. - \\(N_j\\): Total number observations \\(j\\)th category.test statistic follows chi-square distribution 1 degree freedom null hypothesis.Key AssumptionsBinary Outcome: response variable must binary (e.g., success/failure).Ordinal Predictor: predictor variable must natural order.Independent Observations: Data across categories must independent.Let’s consider study analyzing whether proportion customers recommend product increases customer satisfaction levels (Low, Medium, High).InterpretationChi-square Statistic (\\(X^2_{\\text{trend}}\\)):\nIndicates strength linear trend proportions.\nChi-square Statistic (\\(X^2_{\\text{trend}}\\)):Indicates strength linear trend proportions.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.\nhigh p-value suggests evidence linear trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.high p-value suggests evidence linear trend.high p-value suggests evidence linear trend.Practical ApplicationsMarketing: Analyzing whether customer satisfaction levels predict product recommendations repurchase intentions.Marketing: Analyzing whether customer satisfaction levels predict product recommendations repurchase intentions.Healthcare: Evaluating dose-response relationships clinical trials.Healthcare: Evaluating dose-response relationships clinical trials.Education: Testing whether higher levels intervention improve success rates.Education: Testing whether higher levels intervention improve success rates.","code":"\n# Example Data: Customer Satisfaction and Recommendation\nsatisfaction_levels <- c(\"Low\", \"Medium\", \"High\")\nsuccess <- c(20, 35, 50)  # Number of customers who recommend the product\nfailure <- c(30, 15, 10)  # Number of customers who do not recommend the product\ntotal <- success + failure\n\n# Assign ordinal scores\nscores <- 1:length(satisfaction_levels)\n\n# Calculate overall proportion of success\np_hat <- sum(success) / sum(total)"},{"path":"basic-statistical-inference.html","id":"key-takeways","chapter":"4 Basic Statistical Inference","heading":"4.5.3.5 Key Takeways","text":"","code":""},{"path":"basic-statistical-inference.html","id":"divergence-metrics-and-tests-for-comparing-distributions","chapter":"4 Basic Statistical Inference","heading":"4.6 Divergence Metrics and Tests for Comparing Distributions","text":"Divergence metrics powerful tools used measure similarity dissimilarity probability distributions. Unlike deviation deviance statistics, divergence metrics focus broader relationships entire distributions, rather individual data points specific model fit metrics. Let’s clarify differences:Deviation Statistics: Measure difference realization variable reference value (e.g., mean). Common statistics derived deviations include:\nStandard deviation\nAverage absolute deviation\nMedian absolute deviation\nMaximum absolute deviation\nStandard deviationAverage absolute deviationMedian absolute deviationMaximum absolute deviationDeviance Statistics: Assess goodness--fit statistical models. analogous sum squared residuals ordinary least squares (OLS) generalized use cases maximum likelihood estimation (MLE). Deviance statistics frequently employed generalized linear models (GLMs).Divergence statistics differ fundamentally focusing statistical distances entire probability distributions, rather individual data points model errors.1. Divergence MetricsDefinition: Divergence metrics measure much one probability distribution differs another.Definition: Divergence metrics measure much one probability distribution differs another.Key Properties:\nAsymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).\nNon-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).\nUnitless: Divergences often expressed terms information (e.g., bits nats).\nKey Properties:Asymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).Asymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).Non-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).Non-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).Unitless: Divergences often expressed terms information (e.g., bits nats).Unitless: Divergences often expressed terms information (e.g., bits nats).Use:\nUse divergence metrics assess degree mismatch two probability distributions, especially machine learning, statistical inference, model evaluation.\nUse:Use divergence metrics assess degree mismatch two probability distributions, especially machine learning, statistical inference, model evaluation.2. Distance MetricsDefinition: Distance metrics measure “distance” dissimilarity two objects, including probability distributions, datasets, points space.Definition: Distance metrics measure “distance” dissimilarity two objects, including probability distributions, datasets, points space.Key Properties:\nSymmetry: \\(D(P, Q) = D(Q, P)\\).\nTriangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).\nNon-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).\nKey Properties:Symmetry: \\(D(P, Q) = D(Q, P)\\).Symmetry: \\(D(P, Q) = D(Q, P)\\).Triangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).Triangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).Non-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).Non-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).Use:\nUse distance metrics compare datasets, distributions, clustering outcomes symmetry geometric properties important.\nUse:Use distance metrics compare datasets, distributions, clustering outcomes symmetry geometric properties important.Applications Divergence MetricsDivergence metrics found wide utility across domains, including:Detecting Data Drift Machine Learning: Used monitor whether distribution incoming data differs significantly training data.Feature Selection: Employed identify features distinguishing power comparing distributions across different classes.Variational Autoencoders (VAEs): Divergence metrics (Kullback-Leibler divergence) central loss functions used training VAEs.Reinforcement Learning: Measure similarity policy distributions improve decision-making processes.Assessing Consistency: Compare distributions two variables representing constructs test relationship agreement.Divergence metrics also highly relevant business settings, providing insights solutions variety applications, :Customer Segmentation Targeting: Compare distributions customer demographics purchase behavior across market segments identify key differences target strategies effectively.Customer Segmentation Targeting: Compare distributions customer demographics purchase behavior across market segments identify key differences target strategies effectively.Market Basket Analysis: Measure divergence distributions product co-purchases across regions customer groups optimize product bundling cross-selling strategies.Market Basket Analysis: Measure divergence distributions product co-purchases across regions customer groups optimize product bundling cross-selling strategies.Marketing Campaign Effectiveness: Evaluate whether distribution customer responses (e.g., click-rates conversions) differs significantly marketing campaign, providing insights success.Marketing Campaign Effectiveness: Evaluate whether distribution customer responses (e.g., click-rates conversions) differs significantly marketing campaign, providing insights success.Fraud Detection: Monitor divergence transaction patterns time detect anomalies may indicate fraudulent activities.Fraud Detection: Monitor divergence transaction patterns time detect anomalies may indicate fraudulent activities.Supply Chain Optimization: Compare demand distributions across time periods regions optimize inventory allocation reduce stock-outs overstocking.Supply Chain Optimization: Compare demand distributions across time periods regions optimize inventory allocation reduce stock-outs overstocking.Pricing Strategy Evaluation: Analyze divergence pricing purchase distributions across products customer segments refine pricing models improve profitability.Pricing Strategy Evaluation: Analyze divergence pricing purchase distributions across products customer segments refine pricing models improve profitability.Churn Prediction: Compare distributions engagement metrics (e.g., frequency transactions usage time) customers likely churn stay, design retention strategies.Churn Prediction: Compare distributions engagement metrics (e.g., frequency transactions usage time) customers likely churn stay, design retention strategies.Financial Portfolio Analysis: Assess divergence expected returns actual performance distributions different asset classes adjust investment strategies.Financial Portfolio Analysis: Assess divergence expected returns actual performance distributions different asset classes adjust investment strategies.","code":""},{"path":"basic-statistical-inference.html","id":"kolmogorov-smirnov-test-1","chapter":"4 Basic Statistical Inference","heading":"4.6.1 Kolmogorov-Smirnov Test","text":"Kolmogorov-Smirnov (KS) test non-parametric test used determine whether two distributions differ significantly whether sample distribution matches reference distribution. applicable continuous distributions widely used hypothesis testing model evaluation.Mathematical DefinitionThe KS statistic defined :\\[\nD = \\max |F_P(x) - F_Q(x)|\n\\]:\\(F_P(x)\\) cumulative distribution function (CDF) first distribution (sample).\\(F_P(x)\\) cumulative distribution function (CDF) first distribution (sample).\\(F_Q(x)\\) CDF second distribution (theoretical reference distribution).\\(F_Q(x)\\) CDF second distribution (theoretical reference distribution).\\(D\\) measures maximum vertical distance two CDFs.\\(D\\) measures maximum vertical distance two CDFs.HypothesesNull Hypothesis (\\(H_0\\)): empirical distribution follows specified distribution (two samples drawn distribution).Alternative Hypothesis (\\(H_1\\)): empirical distribution follow specified distribution (two samples drawn different distributions).Properties KS StatisticRange: \\[\nD \\[0, 1]\n\\]\n\\(D = 0\\): Perfect match distributions.\n\\(D = 1\\): Maximum dissimilarity distributions.\n\\(D = 0\\): Perfect match distributions.\\(D = 1\\): Maximum dissimilarity distributions.Non-parametric Nature: KS test makes assumptions underlying distribution data.KS test useful various scenarios, including:Comparing two empirical distributions evaluate similarity.Testing goodness--fit sample theoretical distribution.Detecting data drift shifts distributions time.Validating simulation outputs comparing real-world data.Example 1: Continuous DistributionsThis compares CDFs two samples. p-value indicates whether null hypothesis (samples come distribution) can rejected.Example 2: Discrete Data Bootstrapped KS TestFor discrete data, bootstrapped version KS test often used bypass continuity requirement.method performs bootstrapped version KS test, suitable discrete data. p-value indicates whether null hypothesis (samples come distribution) can rejected.Example 3: Comparing Multiple Distributions KL Divergence (Optional Enhancement)wish extend analysis include divergence measures like KL divergence, use following:calculates KL divergence pairs distributions list, offering additional insights relationships distributions.","code":"\n# Load necessary libraries\nlibrary(stats)\n\n# Generate two sample distributions\nset.seed(1)\nsample_1 <- rnorm(100)        # Sample from a standard normal distribution\nsample_2 <- rnorm(100, mean = 1)  # Sample with mean shifted to 1\n\n# Perform Kolmogorov-Smirnov test\nks_test_result <- ks.test(sample_1, sample_2)\nprint(ks_test_result)\n#> \n#>  Asymptotic two-sample Kolmogorov-Smirnov test\n#> \n#> data:  sample_1 and sample_2\n#> D = 0.36, p-value = 4.705e-06\n#> alternative hypothesis: two-sided\nlibrary(Matching)\n\n# Define two discrete samples\ndiscrete_sample_1 <- c(0:10)\ndiscrete_sample_2 <- c(0:10)\n\n# Perform bootstrapped KS test\nks_boot_result <- ks.boot(Tr = discrete_sample_1, Co = discrete_sample_2)\nprint(ks_boot_result)\n#> $ks.boot.pvalue\n#> [1] 1\n#> \n#> $ks\n#> \n#>  Exact two-sample Kolmogorov-Smirnov test\n#> \n#> data:  Tr and Co\n#> D = 0, p-value = 1\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $nboots\n#> [1] 1000\n#> \n#> attr(,\"class\")\n#> [1] \"ks.boot\"\nlibrary(entropy)\nlibrary(tidyverse)\n\n# Define multiple samples\nlst <- list(sample_1 = c(1:20), sample_2 = c(2:30), sample_3 = c(3:30))\n\n# Compute KL divergence between all pairs of distributions\nresult <- expand.grid(1:length(lst), 1:length(lst)) %>%\n    rowwise() %>%\n    mutate(KL = KL.empirical(lst[[Var1]], lst[[Var2]]))\n\nprint(result)\n#> # A tibble: 9 × 3\n#> # Rowwise: \n#>    Var1  Var2     KL\n#>   <int> <int>  <dbl>\n#> 1     1     1 0     \n#> 2     2     1 0.150 \n#> 3     3     1 0.183 \n#> 4     1     2 0.704 \n#> 5     2     2 0     \n#> 6     3     2 0.0679\n#> 7     1     3 0.622 \n#> 8     2     3 0.0870\n#> 9     3     3 0"},{"path":"basic-statistical-inference.html","id":"anderson-darling-test-1","chapter":"4 Basic Statistical Inference","heading":"4.6.2 Anderson-Darling Test","text":"Anderson-Darling (AD) test goodness--fit test evaluates whether sample data comes specific distribution. enhancement Kolmogorov-Smirnov test, greater sensitivity deviations tails distribution.Anderson-Darling test statistic defined :\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\left( \\log F(Y_i) + \\log(1 - F(Y_{n+1-})) \\right) \\right]\n\\]:\\(n\\) sample size.\\(n\\) sample size.\\(F\\) cumulative distribution function (CDF) theoretical distribution tested.\\(F\\) cumulative distribution function (CDF) theoretical distribution tested.\\(Y_i\\) ordered sample values.\\(Y_i\\) ordered sample values.AD test modifies basic framework KS test giving weight tails distribution, making particularly sensitive tail discrepancies.HypothesesNull Hypothesis (\\(H_0\\)): sample data follows specified distribution.Alternative Hypothesis (\\(H_1\\)): sample data follow specified distribution.Key PropertiesTail Sensitivity: Unlike Kolmogorov-Smirnov test, Anderson-Darling test emphasizes discrepancies tails distribution.Distribution-Specific Critical Values: AD test provides critical values tailored specific distribution tested (e.g., normal, exponential).Anderson-Darling test commonly used :Testing goodness--fit sample theoretical distributions normal, exponential, uniform.Evaluating appropriateness parametric models hypothesis testing.Assessing distributional assumptions quality control reliability analysis.Example: Testing Normality Anderson-Darling TestIf p-value chosen significance level (e.g., 0.05), null hypothesis data normally distributed rejected.Example: Comparing Two Empirical DistributionsThe AD test can also applied compare two empirical distributions using resampling techniques.evaluates whether two empirical distributions differ significantly.","code":"\nlibrary(nortest)\n\n# Generate a sample from a normal distribution\nset.seed(1)\nsample_data <- rnorm(100, mean = 0, sd = 1)\n\n# Perform the Anderson-Darling test for normality\nad_test_result <- ad.test(sample_data)\nprint(ad_test_result)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  sample_data\n#> A = 0.16021, p-value = 0.9471\n# Define two samples\nset.seed(1)\nsample_1 <- rnorm(100, mean = 0, sd = 1)\nsample_2 <- rnorm(100, mean = 1, sd = 1)\n\n# Perform resampling-based Anderson-Darling test (custom implementation or packages like twosamples)\nlibrary(twosamples)\nad_test_result_empirical <- ad_test(sample_1, sample_2)\nprint(ad_test_result_empirical)\n#>  Test Stat    P-Value \n#> 6796.70454    0.00025"},{"path":"basic-statistical-inference.html","id":"chi-square-goodness-of-fit-test","chapter":"4 Basic Statistical Inference","heading":"4.6.3 Chi-Square Goodness-of-Fit Test","text":"Chi-Square Goodness--Fit Test non-parametric statistical test used evaluate whether sample data set comes population specific distribution. compares observed frequencies expected frequencies hypothesized distribution.Null Hypothesis (\\(H_0\\)): data follow specified distribution.Alternative Hypothesis (\\(H_a\\)): data follow specified distribution.Chi-Square test statistic computed :\\[\n\\chi^2 = \\sum_{=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\\]:\\(O_i\\): Observed frequency category \\(\\).\\(O_i\\): Observed frequency category \\(\\).\\(E_i\\): Expected frequency category \\(\\).\\(E_i\\): Expected frequency category \\(\\).\\(k\\): Number categories.\\(k\\): Number categories.test statistic follows Chi-Square distribution degrees freedom:\\[\n\\nu = k - 1 - p\n\\]\\(p\\) number parameters estimated data.Assumptions TestRandom Sampling: sample data drawn randomly population.Minimum Expected Frequency: expected frequencies \\(E_i\\) sufficiently large (typically \\(E_i \\geq 5\\)).Independence: Observations sample independent .Decision RuleCompute test statistic \\(\\chi^2\\) using observed expected frequencies.Determine critical value \\(\\chi^2_{\\alpha, \\nu}\\) chosen significance level \\(\\alpha\\) degrees freedom \\(\\nu\\).Compare \\(\\chi^2\\) \\(\\chi^2_{\\alpha, \\nu}\\):\nReject \\(H_0\\) \\(\\chi^2 > \\chi^2_{\\alpha, \\nu}\\).\nAlternatively, use p-value approach:\nReject \\(H_0\\) \\(p \\leq \\alpha\\).\nFail reject \\(H_0\\) \\(p > \\alpha\\).\n\nReject \\(H_0\\) \\(\\chi^2 > \\chi^2_{\\alpha, \\nu}\\).Alternatively, use p-value approach:\nReject \\(H_0\\) \\(p \\leq \\alpha\\).\nFail reject \\(H_0\\) \\(p > \\alpha\\).\nReject \\(H_0\\) \\(p \\leq \\alpha\\).Fail reject \\(H_0\\) \\(p > \\alpha\\).Steps Chi-Square Goodness--Fit TestDefine expected frequencies based hypothesized distribution.Compute observed frequencies data.Calculate test statistic \\(\\chi^2\\).Determine degrees freedom \\(\\nu\\).Compare \\(\\chi^2\\) critical value use p-value decision-making.Example: Testing Fair DieSuppose testing whether six-sided die fair. die rolled 60 times, observed frequencies outcomes :Observed Frequencies: \\([10, 12, 8, 11, 9, 10]\\)Expected Frequencies: fair die equal probability face, \\(E_i = 60 / 6 = 10\\) face.Example: Testing Loaded DieFor die unequal probabilities (e.g., loaded die), expected probabilities defined explicitly:Limitations Chi-Square TestMinimum Expected Frequency: \\(E_i < 5\\) category, test may lose power. Consider merging categories meet criterion.Minimum Expected Frequency: \\(E_i < 5\\) category, test may lose power. Consider merging categories meet criterion.Independence: Assumes observations independent. Violations assumption can invalidate test.Independence: Assumes observations independent. Violations assumption can invalidate test.Sample Size Sensitivity: Large sample sizes may result significant \\(\\chi\\^2\\) values even minor deviations expected distribution.Sample Size Sensitivity: Large sample sizes may result significant \\(\\chi\\^2\\) values even minor deviations expected distribution.Chi-Square Goodness--Fit Test versatile tool evaluating fit observed data hypothesized distribution, widely used fields like quality control, genetics, market research.","code":"\n# Observed frequencies\nobserved <- c(10, 12, 8, 11, 9, 10)\n\n# Expected frequencies under a fair die\nexpected <- rep(10, 6)\n\n# Perform Chi-Square Goodness-of-Fit Test\nchisq_test <- chisq.test(x = observed, p = expected / sum(expected))\n\n# Display results\nchisq_test\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 1, df = 5, p-value = 0.9626\n# Observed frequencies\nobserved <- c(10, 12, 8, 11, 9, 10)\n\n# Expected probabilities (e.g., for a loaded die)\nprobabilities <- c(0.1, 0.2, 0.3, 0.1, 0.2, 0.1)\n\n# Expected frequencies\nexpected <- probabilities * sum(observed)\n\n# Perform Chi-Square Goodness-of-Fit Test\nchisq_test_loaded <- chisq.test(x = observed, p = probabilities)\n\n# Display results\nchisq_test_loaded\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 15.806, df = 5, p-value = 0.007422"},{"path":"basic-statistical-inference.html","id":"cramér-von-mises-test","chapter":"4 Basic Statistical Inference","heading":"4.6.4 Cramér-von Mises Test","text":"Cramér-von Mises (CvM) Test goodness--fit test evaluates whether sample data set comes specified distribution. Similar Kolmogorov-Smirnov Test (KS) Anderson-Darling Test (AD), assesses discrepancy empirical theoretical cumulative distribution functions (CDFs). However, CvM test equal sensitivity across entire distribution, unlike KS test (focused maximum difference) AD test (emphasizing tails).Cramér-von Mises test statistic defined :\\[\nW^2 = n \\int_{-\\infty}^{\\infty} \\left( F_n(x) - F(x) \\right)^2 dF(x)\n\\]:\\(n\\) sample size.\\(n\\) sample size.\\(F_n(x)\\) empirical cumulative distribution function (ECDF) sample.\\(F_n(x)\\) empirical cumulative distribution function (ECDF) sample.\\(F(x)\\) CDF specified theoretical distribution.\\(F(x)\\) CDF specified theoretical distribution.practical implementation, test statistic often computed :\\[\nW^2 = \\sum_{=1}^n \\left[ F(X_i) - \\frac{2i - 1}{2n} \\right]^2 + \\frac{1}{12n}\n\\]\\(X_i\\) ordered sample values.HypothesesNull Hypothesis (\\(H_0\\)): sample data follow specified distribution.Alternative Hypothesis (\\(H_a\\)): sample data follow specified distribution.Key PropertiesEqual Sensitivity:\nCvM test gives equal weight discrepancies across parts distribution, unlike AD test, emphasizes tails.\nCvM test gives equal weight discrepancies across parts distribution, unlike AD test, emphasizes tails.Non-parametric:\ntest makes strong parametric assumptions data, aside specified distribution.\ntest makes strong parametric assumptions data, aside specified distribution.Complementary KS AD Tests:\nKS test focuses maximum distance CDFs AD test emphasizes tails, CvM test provides balanced sensitivity across entire range distribution.\nKS test focuses maximum distance CDFs AD test emphasizes tails, CvM test provides balanced sensitivity across entire range distribution.Cramér-von Mises test widely used :Goodness--Fit Testing: Assessing whether data follow specified theoretical distribution (e.g., normal, exponential).Model Validation: Evaluating fit probabilistic models statistical machine learning contexts.Complementary Testing: Used alongside KS AD tests comprehensive analysis distributional assumptions.Example 1: Testing NormalityThe test evaluates whether sample data follow normal distribution.Example 2: Goodness--Fit Custom DistributionsFor distributions normal, can use resampling techniques custom implementations. ’s pseudo-implementation custom theoretical distribution:demonstrates custom calculation CvM statistic testing goodness--fit exponential distribution.Normality Test:\ncvm.test function evaluates whether sample data follow normal distribution.\nsmall p-value indicates significant deviation normality.\nNormality Test:cvm.test function evaluates whether sample data follow normal distribution.cvm.test function evaluates whether sample data follow normal distribution.small p-value indicates significant deviation normality.small p-value indicates significant deviation normality.Custom Goodness--Fit:\nCustom implementation allows testing distributions normal.\nstatistic measures squared differences empirical theoretical CDFs.\nCustom Goodness--Fit:Custom implementation allows testing distributions normal.Custom implementation allows testing distributions normal.statistic measures squared differences empirical theoretical CDFs.statistic measures squared differences empirical theoretical CDFs.Advantages LimitationsAdvantages:\nBalanced sensitivity across entire distribution.\nComplements KS AD tests providing different perspective goodness--fit.\nAdvantages:Balanced sensitivity across entire distribution.Balanced sensitivity across entire distribution.Complements KS AD tests providing different perspective goodness--fit.Complements KS AD tests providing different perspective goodness--fit.Limitations:\nCritical values distribution-specific.\ntest may less sensitive tail deviations compared AD test.\nLimitations:Critical values distribution-specific.Critical values distribution-specific.test may less sensitive tail deviations compared AD test.test may less sensitive tail deviations compared AD test.Cramér-von Mises test robust versatile goodness--fit test, offering balanced sensitivity across entire distribution. complementarity KS AD tests makes essential tool validating distributional assumptions theoretical applied contexts.","code":"\nlibrary(nortest)\n\n# Generate a sample from a normal distribution\nset.seed(1)\nsample_data <- rnorm(100, mean = 0, sd = 1)\n\n# Perform the Cramér-von Mises test for normality\ncvm_test_result <- cvm.test(sample_data)\nprint(cvm_test_result)\n#> \n#>  Cramer-von Mises normality test\n#> \n#> data:  sample_data\n#> W = 0.026031, p-value = 0.8945\n# Custom ECDF and theoretical CDF comparison\nset.seed(1)\nsample_data <-\n    rexp(100, rate = 1)  # Sample from exponential distribution\ntheoretical_cdf <-\n    function(x) {\n        pexp(x, rate = 1)\n    }  # Exponential CDF\n\n# Compute empirical CDF\nempirical_cdf <- ecdf(sample_data)\n\n# Compute CvM statistic\ncvm_statistic <-\n    sum((empirical_cdf(sample_data) - theoretical_cdf(sample_data)) ^ 2) / length(sample_data)\nprint(paste(\"Cramér-von Mises Statistic (Custom):\", round(cvm_statistic, 4)))\n#> [1] \"Cramér-von Mises Statistic (Custom): 0.0019\""},{"path":"basic-statistical-inference.html","id":"kullback-leibler-divergence","chapter":"4 Basic Statistical Inference","heading":"4.6.5 Kullback-Leibler Divergence","text":"Kullback-Leibler (KL) divergence, also known relative entropy, measure used quantify similarity two probability distributions. plays critical role statistical inference, machine learning, information theory. However, KL divergence true metric satisfy triangle inequality.Key Properties KL DivergenceNot Metric: KL divergence fails meet triangle inequality requirement, symmetric, meaning: \\[\nD_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)\n\\]Metric: KL divergence fails meet triangle inequality requirement, symmetric, meaning: \\[\nD_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)\n\\]Generalization Multivariate Case: KL divergence can extended multivariate distributions, making flexible complex analyses.Generalization Multivariate Case: KL divergence can extended multivariate distributions, making flexible complex analyses.Quantifies Information Loss: measures “information loss” approximating true distribution \\(P\\) predicted distribution \\(Q\\). Thus, smaller values indicate closer similarity distributions.Quantifies Information Loss: measures “information loss” approximating true distribution \\(P\\) predicted distribution \\(Q\\). Thus, smaller values indicate closer similarity distributions.Mathematical DefinitionsKL divergence defined differently discrete continuous distributions.1. Discrete Case\ntwo discrete probability distributions \\(P = \\{P_i\\}\\) \\(Q = \\{Q_i\\}\\), KL divergence given : \\[\nD_{KL}(P \\| Q) = \\sum_i P_i \\log\\left(\\frac{P_i}{Q_i}\\right)\n\\]2. Continuous Case\ncontinuous probability density functions \\(P(x)\\) \\(Q(x)\\): \\[\nD_{KL}(P \\| Q) = \\int P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right) dx\n\\]Range: \\[\nD_{KL}(P \\| Q) \\[0, \\infty)\n\\]\n\\(D_{KL} = 0\\) indicates identical distributions (\\(P = Q\\)).\nLarger values indicate greater dissimilarity \\(P\\) \\(Q\\).\n\\(D_{KL} = 0\\) indicates identical distributions (\\(P = Q\\)).Larger values indicate greater dissimilarity \\(P\\) \\(Q\\).Non-Symmetric Nature: noted, \\(D_{KL}(P \\| Q)\\) \\(D_{KL}(Q \\| P)\\) equal, emphasizing directed nature.Insights:Continuous case uses normalized probability values explicitly provided.Discrete case relies empirical estimation probabilities counts.Observe KL divergence quantifies “distance” two distributions.","code":"\nlibrary(philentropy)\n\n# Example 1: Continuous case\n# Define two continuous probability distributions with distinct patterns\nX_continuous <- c(0.1, 0.2, 0.3, 0.4)  # Normalized to sum to 1\nY_continuous <- c(0.4, 0.3, 0.2, 0.1)  # Normalized to sum to 1\n\n# Calculate KL divergence (logarithm base 2)\nKL_continuous <- KL(rbind(X_continuous, Y_continuous), unit = \"log2\")\nprint(paste(\"KL divergence (continuous):\", round(KL_continuous, 2)))\n#> [1] \"KL divergence (continuous): 0.66\"\n\n# Example 2: Discrete case\n# Define two discrete probability distributions\nX_discrete <- c(5, 10, 15, 20)  # Counts for events\nY_discrete <- c(20, 15, 10, 5)  # Counts for events\n\n# Estimate probabilities empirically and compute KL divergence\nKL_discrete <- KL(rbind(X_discrete, Y_discrete), est.prob = \"empirical\")\nprint(paste(\"KL divergence (discrete):\", round(KL_discrete, 2)))\n#> [1] \"KL divergence (discrete): 0.66\""},{"path":"basic-statistical-inference.html","id":"jensen-shannon-divergence","chapter":"4 Basic Statistical Inference","heading":"4.6.6 Jensen-Shannon Divergence","text":"Jensen-Shannon (JS) divergence symmetric bounded measure similarity two probability distributions. derived Kullback-Leibler Divergence (KL) addresses asymmetry unboundedness incorporating mixed distribution.Jensen-Shannon divergence defined : \\[\nD_{JS}(P \\| Q) = \\frac{1}{2} \\left( D_{KL}(P \\| M) + D_{KL}(Q \\| M) \\right)\n\\] :\\(M = \\frac{1}{2}(P + Q)\\) mixed distribution, representing average \\(P\\) \\(Q\\).\\(M = \\frac{1}{2}(P + Q)\\) mixed distribution, representing average \\(P\\) \\(Q\\).\\(D_{KL}\\) Kullback-Leibler divergence.\\(D_{KL}\\) Kullback-Leibler divergence.Key PropertiesSymmetry: Unlike KL divergence, JS divergence symmetric: \\[\nD_{JS}(P \\| Q) = D_{JS}(Q \\| P)\n\\]Symmetry: Unlike KL divergence, JS divergence symmetric: \\[\nD_{JS}(P \\| Q) = D_{JS}(Q \\| P)\n\\]Boundedness:\nbase-2 logarithms: \\[\nD_{JS} \\[0, 1]\n\\]\nnatural logarithms (base-\\(e\\)): \\[\nD_{JS} \\[0, \\ln(2)]\n\\]\nBoundedness:base-2 logarithms: \\[\nD_{JS} \\[0, 1]\n\\]natural logarithms (base-\\(e\\)): \\[\nD_{JS} \\[0, \\ln(2)]\n\\]Interpretability: JS divergence measures average information gain moving mixed distribution \\(M\\) either \\(P\\) \\(Q\\). bounded nature makes easier compare across datasets.Interpretability: JS divergence measures average information gain moving mixed distribution \\(M\\) either \\(P\\) \\(Q\\). bounded nature makes easier compare across datasets.","code":"\n# Load the required library\nlibrary(philentropy)\n\n# Example 1: Continuous case\n# Define two continuous distributions\nX_continuous <- 1:10  # Continuous sequence\nY_continuous <- 1:20  # Continuous sequence\n\n# Compute JS divergence (logarithm base 2)\nJS_continuous <- JSD(rbind(X_continuous, Y_continuous), unit = \"log2\")\nprint(paste(\"JS divergence (continuous):\", round(JS_continuous, 2)))\n#> [1] \"JS divergence (continuous): 20.03\"\n\n# X_continuous and Y_continuous represent continuous distributions.\n# The mixed distribution (M) is computed internally as the average of the two distributions.\n\n# Example 2: Discrete case\n# Define two discrete distributions\nX_discrete <- c(5, 10, 15, 20)  # Observed counts for events\nY_discrete <- c(20, 15, 10, 5)  # Observed counts for events\n\n# Compute JS divergence with empirical probability estimation\nJS_discrete <- JSD(rbind(X_discrete, Y_discrete), est.prob = \"empirical\")\nprint(paste(\"JS divergence (discrete):\", round(JS_discrete, 2)))\n#> [1] \"JS divergence (discrete): 0.15\"\n\n# X_discrete and Y_discrete represent event counts.\n# Probabilities are estimated empirically before calculating the divergence."},{"path":"basic-statistical-inference.html","id":"hellinger-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.7 Hellinger Distance","text":"Hellinger distance bounded symmetric measure similarity two probability distributions. widely used statistics machine learning quantify “close” two distributions , values ranging 0 (identical distributions) 1 (completely disjoint distributions).Mathematical DefinitionThe Hellinger distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nH(P, Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_x \\left(\\sqrt{P(x)} - \\sqrt{Q(x)}\\right)^2}\n\\]:\\(P(x)\\) \\(Q(x)\\) probability densities probabilities point \\(x\\) distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probability densities probabilities point \\(x\\) distributions \\(P\\) \\(Q\\).term \\(\\sqrt{P(x)}\\) square root probabilities, emphasizing geometric comparisons distributions.term \\(\\sqrt{P(x)}\\) square root probabilities, emphasizing geometric comparisons distributions.Alternatively, continuous distributions, Hellinger distance can expressed :\\[\nH(P, Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\int \\left(\\sqrt{P(x)} - \\sqrt{Q(x)}\\right)^2 dx}\n\\]Key PropertiesSymmetry: \\[\nH(P, Q) = H(Q, P)\n\\] distance symmetric, unlike Kullback-Leibler divergence.Symmetry: \\[\nH(P, Q) = H(Q, P)\n\\] distance symmetric, unlike Kullback-Leibler divergence.Boundedness: \\[\nH(P, Q) \\[0, 1]\n\\]\n\\(H = 0\\): distributions identical (\\(P(x) = Q(x)\\) \\(x\\)).\n\\(H = 1\\): distributions overlap (\\(P(x) \\neq Q(x)\\)).\nBoundedness: \\[\nH(P, Q) \\[0, 1]\n\\]\\(H = 0\\): distributions identical (\\(P(x) = Q(x)\\) \\(x\\)).\\(H = 1\\): distributions overlap (\\(P(x) \\neq Q(x)\\)).Interpretability:\nHellinger distance provides scale-invariant measure, making suitable comparing distributions various contexts.\nInterpretability:Hellinger distance provides scale-invariant measure, making suitable comparing distributions various contexts.Hellinger distance widely used :Hypothesis Testing: Comparing empirical distributions theoretical models.Machine Learning: Feature selection, classification, clustering tasks.Bayesian Analysis: Quantifying differences prior posterior distributions.Economics Ecology: Measuring dissimilarity distributions like income, species abundance, geographical data.","code":"\nlibrary(philentropy)\n\n# Example 1: Compute Hellinger Distance for Discrete Distributions\n# Define two discrete distributions as probabilities\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Hellinger distance\nhellinger_discrete <- distance(rbind(P_discrete, Q_discrete), method = \"hellinger\")\nprint(paste(\"Hellinger Distance (Discrete):\", round(hellinger_discrete, 4)))\n#> [1] \"Hellinger Distance (Discrete): 0.465\"\n\n# Example 2: Compute Hellinger Distance for Empirical Distributions\n# Define two empirical distributions (counts)\nP_empirical <- c(10, 20, 30, 40)  # Counts for distribution P\nQ_empirical <- c(30, 30, 20, 20)  # Counts for distribution Q\n\n# Normalize counts to probabilities\nP_normalized <- P_empirical / sum(P_empirical)\nQ_normalized <- Q_empirical / sum(Q_empirical)\n\n# Compute Hellinger distance\nhellinger_empirical <- distance(rbind(P_normalized, Q_normalized), method = \"hellinger\")\nprint(paste(\"Hellinger Distance (Empirical):\", round(hellinger_empirical, 4)))\n#> [1] \"Hellinger Distance (Empirical): 0.465\""},{"path":"basic-statistical-inference.html","id":"bhattacharyya-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.8 Bhattacharyya Distance","text":"Bhattacharyya Distance statistical measure used quantify similarity overlap two probability distributions. commonly used pattern recognition, signal processing, statistics evaluate closely related two distributions . Bhattacharyya distance particularly effective comparing discrete continuous distributions.Bhattacharyya distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nD_B(P, Q) = -\\ln \\left( \\sum_x \\sqrt{P(x) Q(x)} \\right)\n\\]continuous distributions, Bhattacharyya distance expressed :\\[\nD_B(P, Q) = -\\ln \\left( \\int \\sqrt{P(x) Q(x)} dx \\right)\n\\]:\\(P(x)\\) \\(Q(x)\\) probability densities probabilities distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probability densities probabilities distributions \\(P\\) \\(Q\\).term \\(\\int \\sqrt{P(x) Q(x)} dx\\) known Bhattacharyya coefficient.term \\(\\int \\sqrt{P(x) Q(x)} dx\\) known Bhattacharyya coefficient.Key PropertiesSymmetry: \\[\nD_B(P, Q) = D_B(Q, P)\n\\]Symmetry: \\[\nD_B(P, Q) = D_B(Q, P)\n\\]Range: \\[\nD_B(P, Q) \\[0, \\infty)\n\\]\n\\(D_B = 0\\): distributions identical (\\(P = Q\\)).\nLarger values indicate less overlap greater dissimilarity \\(P\\) \\(Q\\).\nRange: \\[\nD_B(P, Q) \\[0, \\infty)\n\\]\\(D_B = 0\\): distributions identical (\\(P = Q\\)).Larger values indicate less overlap greater dissimilarity \\(P\\) \\(Q\\).Relation Hellinger Distance:\nBhattacharyya coefficient related Hellinger distance: \\[\nH(P, Q) = \\sqrt{1 - \\sum_x \\sqrt{P(x) Q(x)}}\n\\]\nRelation Hellinger Distance:Bhattacharyya coefficient related Hellinger distance: \\[\nH(P, Q) = \\sqrt{1 - \\sum_x \\sqrt{P(x) Q(x)}}\n\\]Bhattacharyya distance widely used :Classification: Measuring similarity feature distributions machine learning.Hypothesis Testing: Evaluating closeness observed data theoretical model.Image Processing: Comparing pixel intensity distributions color histograms.Economics Ecology: Assessing similarity income distributions species abundance.Example 1: Discrete DistributionsA smaller Bhattacharyya distance indicates greater similarity two distributions.Example 2: Continuous Distributions (Approximation)continuous distributions, Bhattacharyya distance can approximated using numerical integration discretization.Continuous distributions discretized histograms compute Bhattacharyya coefficient distance.Discrete Case:\nBhattacharyya coefficient quantifies overlap \\(P\\) \\(Q\\).\nBhattacharyya distance translates overlap logarithmic measure dissimilarity.\nBhattacharyya coefficient quantifies overlap \\(P\\) \\(Q\\).Bhattacharyya distance translates overlap logarithmic measure dissimilarity.Continuous Case:\nDistributions discretized histograms approximate Bhattacharyya coefficient distance.\nDistributions discretized histograms approximate Bhattacharyya coefficient distance.","code":"\n# Define two discrete probability distributions\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Bhattacharyya coefficient\nbhattacharyya_coefficient <- sum(sqrt(P_discrete * Q_discrete))\n\n# Compute Bhattacharyya distance\nbhattacharyya_distance <- -log(bhattacharyya_coefficient)\n\n# Display results\nprint(paste(\n    \"Bhattacharyya Coefficient:\",\n    round(bhattacharyya_coefficient, 4)\n))\n#> [1] \"Bhattacharyya Coefficient: 0.9459\"\nprint(paste(\n    \"Bhattacharyya Distance (Discrete):\",\n    round(bhattacharyya_distance, 4)\n))\n#> [1] \"Bhattacharyya Distance (Discrete): 0.0556\"\n# Generate two continuous distributions\nset.seed(1)\nP_continuous <-\n    rnorm(1000, mean = 0, sd = 1)  # Standard normal distribution\nQ_continuous <-\n    rnorm(1000, mean = 1, sd = 1)  # Normal distribution with mean 1\n\n# Create histograms to approximate probabilities\nhist_P <- hist(P_continuous, breaks = 50, plot = FALSE)\nhist_Q <- hist(Q_continuous, breaks = 50, plot = FALSE)\n\n# Normalize histograms to probabilities\nprob_P <- hist_P$counts / sum(hist_P$counts)\nprob_Q <- hist_Q$counts / sum(hist_Q$counts)\n\n# Compute Bhattacharyya coefficient\nbhattacharyya_coefficient_continuous <- sum(sqrt(prob_P * prob_Q))\n\n# Compute Bhattacharyya distance\nbhattacharyya_distance_continuous <-\n    -log(bhattacharyya_coefficient_continuous)\n\n# Display results\nprint(paste(\n    \"Bhattacharyya Coefficient (Continuous):\",\n    round(bhattacharyya_coefficient_continuous, 4)\n))\n#> [1] \"Bhattacharyya Coefficient (Continuous): 0.9823\"\nprint(paste(\n    \"Bhattacharyya Distance (Continuous Approximation):\",\n    round(bhattacharyya_distance_continuous, 4)\n))\n#> [1] \"Bhattacharyya Distance (Continuous Approximation): 0.0178\""},{"path":"basic-statistical-inference.html","id":"wasserstein-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.9 Wasserstein Distance","text":"Wasserstein distance, also known Earth Mover’s Distance (EMD), measure similarity two probability distributions. quantifies “cost” transforming one distribution another, making particularly suitable continuous data applications geometry data matters.Mathematical DefinitionThe Wasserstein distance two probability distributions \\(P\\) \\(Q\\) domain \\(\\mathcal{X}\\) defined :\\[\nW_p(P, Q) = \\left( \\int_{\\mathcal{X}} |F_P(x) - F_Q(x)|^p dx \\right)^{\\frac{1}{p}}\n\\]:\\(F_P(x)\\) \\(F_Q(x)\\) cumulative distribution functions (CDFs) \\(P\\) \\(Q\\).\\(F_P(x)\\) \\(F_Q(x)\\) cumulative distribution functions (CDFs) \\(P\\) \\(Q\\).\\(p \\geq 1\\) order Wasserstein distance (commonly \\(p = 1\\)).\\(p \\geq 1\\) order Wasserstein distance (commonly \\(p = 1\\)).\\(|\\cdot|^p\\) absolute difference raised power \\(p\\).\\(|\\cdot|^p\\) absolute difference raised power \\(p\\).case \\(p = 1\\), formula simplifies :\\[\nW_1(P, Q) = \\int_{\\mathcal{X}} |F_P(x) - F_Q(x)| dx\n\\]represents minimum “cost” transforming distribution \\(P\\) \\(Q\\), cost proportional distance “unit mass” must move.Key PropertiesInterpretability: Represents “effort” required morph one distribution another.Metric: Wasserstein distance satisfies properties metric, including symmetry, non-negativity, triangle inequality.Flexibility: Can handle empirical continuous distributions.Wasserstein distance widely used various fields, including:Machine Learning:\nTraining generative models Wasserstein GANs.\nMonitoring data drift online systems.\nTraining generative models Wasserstein GANs.Monitoring data drift online systems.Statistics:\nComparing empirical distributions derived observed data.\nRobustness testing distributional shifts.\nComparing empirical distributions derived observed data.Robustness testing distributional shifts.Economics:\nQuantifying disparities income wealth distributions.\nQuantifying disparities income wealth distributions.Image Processing:\nMeasuring structural differences image distributions.\nMeasuring structural differences image distributions.","code":"\nlibrary(transport)\nlibrary(twosamples)\n\n# Example 1: Compute Wasserstein Distance (1D case)\nset.seed(1)\ndist_1 <- rnorm(100)               # Generate a sample from a standard normal distribution\ndist_2 <- rnorm(100, mean = 1)     # Generate a sample with mean shifted to 1\n\n# Calculate the Wasserstein distance\nwass_distance <- wasserstein1d(dist_1, dist_2)\nprint(paste(\"1D Wasserstein Distance:\", round(wass_distance, 4)))\n#> [1] \"1D Wasserstein Distance: 0.8533\"\n\n# Example 2: Wasserstein Metric as a Statistic\nset.seed(1)\nwass_stat_value <- wass_stat(dist_1, dist_2)\nprint(paste(\"Wasserstein Statistic:\", round(wass_stat_value, 4)))\n#> [1] \"Wasserstein Statistic: 0.8533\"\n\n# Example 3: Wasserstein Test (Permutation-based Two-sample Test)\nset.seed(1)\nwass_test_result <- wass_test(dist_1, dist_2)\nprint(wass_test_result)\n#> Test Stat   P-Value \n#> 0.8533046 0.0002500\n\n# - Example 1 calculates the simple Wasserstein distance between two distributions.\n# - Example 2 computes the Wasserstein distance as a statistical metric.\n# - Example 3 performs a permutation-based two-sample test using the Wasserstein metric."},{"path":"basic-statistical-inference.html","id":"energy-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.10 Energy Distance","text":"Energy Distance statistical metric used quantify similarity two probability distributions. particularly effective comparing multi-dimensional distributions.Energy Distance two distributions \\(P\\) \\(Q\\) defined :\\[\nE(P, Q) = 2 \\mathbb{E}[||X - Y||] - \\mathbb{E}[||X - X'||] - \\mathbb{E}[||Y - Y'||]\n\\]:\\(X\\) \\(X'\\) independent identically distributed (..d.) random variables \\(P\\).\\(X\\) \\(X'\\) independent identically distributed (..d.) random variables \\(P\\).\\(Y\\) \\(Y'\\) ..d. random variables \\(Q\\).\\(Y\\) \\(Y'\\) ..d. random variables \\(Q\\).\\(||\\cdot||\\) denotes Euclidean distance.\\(||\\cdot||\\) denotes Euclidean distance.Alternatively, empirical distributions, Energy Distance can approximated :\\[\nE(P, Q) = \\frac{2}{mn} \\sum_{=1}^m \\sum_{j=1}^n ||X_i - Y_j|| - \\frac{1}{m^2} \\sum_{=1}^m \\sum_{j=1}^m ||X_i - X_j|| - \\frac{1}{n^2} \\sum_{=1}^n \\sum_{j=1}^n ||Y_i - Y_j||\n\\]:\\(m\\) \\(n\\) sample sizes distributions \\(P\\) \\(Q\\) respectively.\\(m\\) \\(n\\) sample sizes distributions \\(P\\) \\(Q\\) respectively.\\(X_i\\) \\(Y_j\\) samples \\(P\\) \\(Q\\).\\(X_i\\) \\(Y_j\\) samples \\(P\\) \\(Q\\).Key PropertiesMetric:\nEnergy distance satisfies properties metric: symmetry, non-negativity, triangle inequality.\nEnergy distance satisfies properties metric: symmetry, non-negativity, triangle inequality.Range: \\[\nE(P, Q) \\geq 0\n\\]\n\\(E(P, Q) = 0\\): distributions identical.\nLarger values indicate greater dissimilarity.\n\\(E(P, Q) = 0\\): distributions identical.Larger values indicate greater dissimilarity.Effectiveness Multi-dimensional Data:\nEnergy distance designed work well higher-dimensional spaces, unlike traditional metrics.\nEnergy distance designed work well higher-dimensional spaces, unlike traditional metrics.Energy Distance widely used :Hypothesis Testing: Testing whether two distributions .Energy Test equality distributions.Clustering: Measuring dissimilarity clusters multi-dimensional data.Feature Selection: Comparing distributions features across different classes evaluate discriminative power.Example 1: Comparing Two DistributionsThis calculates energy distance two multi-dimensional distributions.Example 2: Energy Test Equality DistributionsThe energy test evaluates null hypothesis two distributions identical.Energy Distance:\nProvides single metric quantify dissimilarity two distributions, considering dimensions data.\nEnergy Distance:Provides single metric quantify dissimilarity two distributions, considering dimensions data.Energy Test:\nTests equality distributions using Energy Distance.\np-value indicates whether distributions significantly different.\nEnergy Test:Tests equality distributions using Energy Distance.Tests equality distributions using Energy Distance.p-value indicates whether distributions significantly different.p-value indicates whether distributions significantly different.Advantages Energy DistanceMulti-dimensional Applicability:\nWorks seamlessly high-dimensional data, unlike divergence metrics may suffer dimensionality issues.\nMulti-dimensional Applicability:Works seamlessly high-dimensional data, unlike divergence metrics may suffer dimensionality issues.Non-parametric:\nMakes assumptions form distributions.\nNon-parametric:Makes assumptions form distributions.Robustness:\nEffective even complex data structures.\nRobustness:Effective even complex data structures.","code":"\n# Load the 'energy' package\nlibrary(energy)\n\n# Generate two sample distributions\nset.seed(1)\nX <- matrix(rnorm(1000, mean = 0, sd = 1), ncol = 2)  # Distribution P\nY <- matrix(rnorm(1000, mean = 1, sd = 1), ncol = 2)  # Distribution Q\n\n# Combine X and Y and create a group identifier\ncombined <- rbind(X, Y)\ngroups <- c(rep(1, nrow(X)), rep(2, nrow(Y)))\n\n# Compute Energy Distance\nenergy_dist <- edist(combined, sizes = table(groups))\n\n# Print the Energy Distance\nprint(paste(\"Energy Distance:\", round(energy_dist, 4)))\n#> [1] \"Energy Distance: 201.9202\"\n# Perform the Energy Test\nenergy_test <-\n    eqdist.etest(rbind(X, Y), sizes = c(nrow(X), nrow(Y)), R = 999)\nprint(energy_test)\n#> \n#>  Multivariate 2-sample E-test of equal distributions\n#> \n#> data:  sample sizes 500 500, replicates 999\n#> E-statistic = 201.92, p-value = 0.001"},{"path":"basic-statistical-inference.html","id":"total-variation-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.11 Total Variation Distance","text":"Total Variation (TV) Distance measure maximum difference two probability distributions. widely used probability theory, statistics, machine learning quantify dissimilar two distributions .Total Variation Distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nD_{TV}(P, Q) = \\frac{1}{2} \\sum_x |P(x) - Q(x)|\n\\]:\\(P(x)\\) \\(Q(x)\\) probabilities assigned outcome \\(x\\) distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probabilities assigned outcome \\(x\\) distributions \\(P\\) \\(Q\\).factor \\(\\frac{1}{2}\\) ensures distance lies within range \\([0, 1]\\).factor \\(\\frac{1}{2}\\) ensures distance lies within range \\([0, 1]\\).Alternatively, continuous distributions, TV distance can expressed :\\[\nD_{TV}(P, Q) = \\frac{1}{2} \\int |P(x) - Q(x)| dx\n\\]Key PropertiesRange: \\[\nD_{TV}(P, Q) \\[0, 1]\n\\]\n\\(D_{TV} = 0\\): distributions identical (\\(P = Q\\)).\n\\(D_{TV} = 1\\): distributions completely disjoint (overlap).\nRange: \\[\nD_{TV}(P, Q) \\[0, 1]\n\\]\\(D_{TV} = 0\\): distributions identical (\\(P = Q\\)).\\(D_{TV} = 1\\): distributions completely disjoint (overlap).Symmetry: \\[\nD_{TV}(P, Q) = D_{TV}(Q, P)\n\\]Symmetry: \\[\nD_{TV}(P, Q) = D_{TV}(Q, P)\n\\]Interpretability:\n\\(D_{TV}(P, Q)\\) represents maximum probability mass needs shifted transform \\(P\\) \\(Q\\).\nInterpretability:\\(D_{TV}(P, Q)\\) represents maximum probability mass needs shifted transform \\(P\\) \\(Q\\).Total Variation Distance used :Hypothesis Testing: Quantifying difference observed expected distributions.Machine Learning: Evaluating similarity predicted true distributions.Information Theory: Comparing distributions contexts like communication cryptography.Example 1: Discrete DistributionsThis calculates maximum difference two distributions, scaled lie 0 1.Example 2: Continuous Distributions (Approximation)continuous distributions, TV distance can approximated using discretization numerical integration. ’s example using random samples:continuous distributions discretized histograms, TV distance computed based resulting probabilities.Discrete Case:\nTV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.\nexample, highlights much \\(P\\) \\(Q\\) diverge.\nDiscrete Case:TV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.TV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.example, highlights much \\(P\\) \\(Q\\) diverge.example, highlights much \\(P\\) \\(Q\\) diverge.Continuous Case:\ncontinuous distributions, TV distance approximated using discretized probabilities histograms.\napproach provides intuitive measure similarity large samples.\nContinuous Case:continuous distributions, TV distance approximated using discretized probabilities histograms.continuous distributions, TV distance approximated using discretized probabilities histograms.approach provides intuitive measure similarity large samples.approach provides intuitive measure similarity large samples.Total Variation Distance provides intuitive interpretable measure maximum difference two distributions. symmetry bounded nature make versatile tool comparing discrete continuous distributions.","code":"\n# Define two discrete probability distributions\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Total Variation Distance\ntv_distance <- sum(abs(P_discrete - Q_discrete)) / 2\nprint(paste(\"Total Variation Distance (Discrete):\", round(tv_distance, 4)))\n#> [1] \"Total Variation Distance (Discrete): 0.3\"\n# Generate two continuous distributions\nset.seed(1)\nP_continuous <-\n    rnorm(1000, mean = 0, sd = 1)  # Standard normal distribution\nQ_continuous <-\n    rnorm(1000, mean = 1, sd = 1)  # Normal distribution with mean 1\n\n# Create histograms to approximate probabilities\nhist_P <- hist(P_continuous, breaks = 50, plot = FALSE)\nhist_Q <- hist(Q_continuous, breaks = 50, plot = FALSE)\n\n# Normalize histograms to probabilities\nprob_P <- hist_P$counts / sum(hist_P$counts)\nprob_Q <- hist_Q$counts / sum(hist_Q$counts)\n\n# Compute Total Variation Distance\ntv_distance_continuous <- sum(abs(prob_P - prob_Q)) / 2\nprint(paste(\n    \"Total Variation Distance (Continuous Approximation):\",\n    round(tv_distance_continuous, 4)\n))\n#> [1] \"Total Variation Distance (Continuous Approximation): 0.125\""},{"path":"basic-statistical-inference.html","id":"summary","chapter":"4 Basic Statistical Inference","heading":"4.6.12 Summary","text":"1. Tests Comparing Distributions2. Divergence Metrics3. Distance Metrics","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"5 Linear Regression","heading":"5 Linear Regression","text":"Linear regression one fundamental tools statistics econometrics, widely used modeling relationships variables. forms cornerstone predictive analysis, enabling us understand quantify changes one explanatory variables associated dependent variable. simplicity versatility make essential tool fields ranging economics marketing healthcare environmental studies.core, linear regression addresses questions associations rather causation. example:advertising expenditures associated sales performance?relationship company’s revenue stock price?level education correlate income?questions patterns data—necessarily causal effects. regression can provide insights potential causal relationships, establishing causality requires just regression analysis. requires careful consideration study design, assumptions, potential confounding factors., called “linear”? term refers structure model, dependent variable (outcome) modeled linear combination one independent variables (predictors). example, simple linear regression, relationship represented :\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\\(Y\\) dependent variable, \\(X\\) independent variable, \\(\\beta_0\\) \\(\\beta_1\\) parameters estimated, \\(\\epsilon\\) error term capturing randomness unobserved factors.Linear regression serves foundation much applied data analysis wide-ranging applications:Understanding Patterns Data: Regression provides framework summarize explore relationships variables. allows us identify patterns trends associations, can guide analysis decision-making.Understanding Patterns Data: Regression provides framework summarize explore relationships variables. allows us identify patterns trends associations, can guide analysis decision-making.Prediction: Beyond exploring relationships, regression widely used making predictions. instance, given historical data, can use regression model predict future outcomes like sales, prices, demand.Prediction: Beyond exploring relationships, regression widely used making predictions. instance, given historical data, can use regression model predict future outcomes like sales, prices, demand.Building Blocks Advanced Techniques: Linear regression foundational many advanced statistical machine learning models, logistic regression, ridge regression, neural networks. Mastering linear regression equips skills tackle complex methods.Building Blocks Advanced Techniques: Linear regression foundational many advanced statistical machine learning models, logistic regression, ridge regression, neural networks. Mastering linear regression equips skills tackle complex methods.Regression Causality: Crucial DistinctionIt’s essential remember regression alone establish causation. instance, regression model might show strong association advertising sales, prove advertising directly causes sales increase. factors—seasonality, market trends, unobserved variables—also influence results.Establishing causality requires additional steps, controlled experiments, instrumental variable techniques, careful observational study designs. work details linear regression, ’ll revisit distinction highlight scenarios causality might might inferred.Estimator?heart regression lies process estimation—act using data determine unknown characteristics population model.estimator mathematical rule formula used calculate estimate unknown quantity based observed data. example, calculate average height sample estimate average height population, sample mean estimator.context regression, quantities typically estimate :Parameters: Fixed, unknown values describe relationship variables (e.g., coefficients regression equation).\nEstimating parameters → Parametric models (finite parameters, e.g., coefficients regression).\nEstimating parameters → Parametric models (finite parameters, e.g., coefficients regression).Functions: Unknown relationships patterns data, often modeled without assuming fixed functional form.\nEstimating functions → Non-parametric models (focus shapes trends, fixed number parameters).\nEstimating functions → Non-parametric models (focus shapes trends, fixed number parameters).Types EstimatorsTo better understand estimation process, let’s introduce two broad categories estimators ’ll work :Parametric Estimators\nParametric estimation focuses finite set parameters define model. example, simple linear regression:\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ntask estimate parameters \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Parametric estimators rely specific assumptions form model (e.g., linearity) distribution error term (e.g., normality).Parametric Estimators\nParametric estimation focuses finite set parameters define model. example, simple linear regression:\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ntask estimate parameters \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Parametric estimators rely specific assumptions form model (e.g., linearity) distribution error term (e.g., normality).Non-Parametric Estimators\nNon-parametric estimation avoids assuming specific functional form relationship variables. Instead, focuses estimating patterns trends directly data. example, using scatterplot smoothing technique visualize sales vary advertising spend without imposing linear quadratic relationship.Non-Parametric Estimators\nNon-parametric estimation avoids assuming specific functional form relationship variables. Instead, focuses estimating patterns trends directly data. example, using scatterplot smoothing technique visualize sales vary advertising spend without imposing linear quadratic relationship.two categories reflect fundamental trade-statistical analysis: parametric models often simpler interpretable require strong assumptions, non-parametric models flexible may require data computational resources.Desirable Properties EstimatorsRegardless whether estimating parameters functions, want estimators possess certain desirable properties. Think “golden standards” help us judge whether estimator reliable:Unbiasedness\nestimator unbiased hits true value parameter, average, repeated samples. Mathematically:\\[E[\\hat{\\beta}] = \\beta.\\]\nmeans , across multiple samples, estimator systematically overestimate underestimate true parameter.Unbiasedness\nestimator unbiased hits true value parameter, average, repeated samples. Mathematically:\\[E[\\hat{\\beta}] = \\beta.\\]\nmeans , across multiple samples, estimator systematically overestimate underestimate true parameter.Consistency\nConsistency ensures sample size increases, estimator converges true value parameter. Formally:\\[plim\\ \\hat{\\beta_n} = \\beta.\\]\nproperty relies Law Large Numbers, guarantees larger samples reduce random fluctuations, leading precise estimates.Consistency\nConsistency ensures sample size increases, estimator converges true value parameter. Formally:\\[plim\\ \\hat{\\beta_n} = \\beta.\\]\nproperty relies Law Large Numbers, guarantees larger samples reduce random fluctuations, leading precise estimates.Efficiency\nAmong unbiased estimators, efficient estimator smallest variance.\nOrdinary Least Squares method efficient Best Linear Unbiased Estimator (BLUE) Gauss-Markov Theorem.\nestimators meet specific distributional assumptions (e.g., normality), Maximum Likelihood Estimators (MLE) asymptotically efficient, meaning achieve lowest possible variance sample size grows.\nEfficiency\nAmong unbiased estimators, efficient estimator smallest variance.Ordinary Least Squares method efficient Best Linear Unbiased Estimator (BLUE) Gauss-Markov Theorem.estimators meet specific distributional assumptions (e.g., normality), Maximum Likelihood Estimators (MLE) asymptotically efficient, meaning achieve lowest possible variance sample size grows.Properties MatterUnderstanding properties crucial ensure methods use estimation reliable, precise, robust. Whether estimating coefficients regression model uncovering complex pattern data, properties provide foundation statistical inference decision-making.Now ’ve established estimators , types ’ll encounter, desirable properties, can move understanding concepts apply specifically Ordinary Least Squares method—backbone linear regression.Reference TableErrors independent, identically distributed (..d.) mean 0 constant variance.Linear relationship predictors response.Simple, well-understood method.Minimizes residual sum squares (easy interpret coefficients).Sensitive outliers violations normality.Can perform poorly predictors highly correlated (multicollinearity).Handles correlated non-constant-variance errors.flexible OLS noise structure known.Requires specifying (estimating) error covariance structure.Misspecification can lead biased estimates.Provides general framework estimating parameters well-defined probability models.Can extend complex likelihoods.Highly sensitive model misspecification.May require computation OLS GLS.Controls overfitting via regularization.Handles high-dimensional data many predictors.Can perform feature selection (e.g., Lasso).Requires choosing tuning parameter(s) (e.g., λ).Interpretation coefficients becomes less straightforward.Resistant large deviations outliers data.Often maintains good performance mild model misspecifications.Less efficient errors truly normal.Choice robust method tuning can subjective.Simultaneously reduces dimensionality fits regression.Works well collinear, high-dimensional data.Can harder interpret OLS (latent components instead original predictors).Requires choosing number components.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1 Ordinary Least Squares","text":"Ordinary Least Squares (OLS) backbone statistical modeling, method foundational often serves starting point understanding data relationships. Whether predicting sales, estimating economic trends, uncovering patterns scientific research, OLS remains critical tool. appeal lies simplicity: OLS models relationship dependent variable one predictors minimizing squared differences observed predicted values.OLS Works: Linear Nonlinear RelationshipsOLS rests Conditional Expectation Function (CEF), \\(E[Y | X]\\), describes expected value \\(Y\\) given \\(X\\). Regression shines two key scenarios:Perfect Fit (Linear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = + \\sum_{k=1}^K b_k X_{ki}\\), regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) exactly equals CEF. words, regression gives true average relationship \\(Y\\) \\(X\\).\ntrue relationship linear, regression delivers exact CEF. instance, imagine ’re estimating relationship advertising spend sales revenue. true impact linear, OLS perfectly capture .Perfect Fit (Linear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = + \\sum_{k=1}^K b_k X_{ki}\\), regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) exactly equals CEF. words, regression gives true average relationship \\(Y\\) \\(X\\).\ntrue relationship linear, regression delivers exact CEF. instance, imagine ’re estimating relationship advertising spend sales revenue. true impact linear, OLS perfectly capture .Approximation (Nonlinear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}]\\) nonlinear, OLS provides best linear approximation relationship. Specifically, minimizes expected squared deviation linear regression line nonlinear CEF.\nexample, effect advertising diminishes higher spending levels? OLS still works, providing best linear approximation nonlinear relationship minimizing squared deviations predictions true (unknown) CEF.Approximation (Nonlinear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}]\\) nonlinear, OLS provides best linear approximation relationship. Specifically, minimizes expected squared deviation linear regression line nonlinear CEF.\nexample, effect advertising diminishes higher spending levels? OLS still works, providing best linear approximation nonlinear relationship minimizing squared deviations predictions true (unknown) CEF.words, regression just tool “linear” relationships—’s workhorse adapts remarkably well messy, real-world data.","code":""},{"path":"linear-regression.html","id":"simple-regression-basic-model","chapter":"5 Linear Regression","heading":"5.1.1 Simple Regression (Basic) Model","text":"simplest form regression straight line:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\\(Y_i\\): dependent variable outcome ’re trying predict (e.g., sales, temperature).\\(X_i\\): independent variable predictor (e.g., advertising spend, time).\\(\\beta_0\\): intercept—line crosses \\(Y\\)-axis \\(X = 0\\).\\(\\beta_1\\): slope, representing change \\(Y\\) one-unit increase \\(X\\).\\(\\epsilon_i\\): error term, accounting random factors \\(X\\) explain.Assumptions Error Term (\\(\\epsilon_i\\)):\\[\n\\begin{aligned}\nE(\\epsilon_i) &= 0 \\\\\n\\text{Var}(\\epsilon_i) &= \\sigma^2 \\\\\n\\text{Cov}(\\epsilon_i, \\epsilon_j) &= 0 \\quad \\text{} \\neq j\n\\end{aligned}\n\\]Since \\(\\epsilon_i\\) random, \\(Y_i\\) also random:\\[\n\\begin{aligned}\nE(Y_i) &= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1 X_i\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\text{Var}(Y_i) &= \\text{Var}(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= \\text{Var}(\\epsilon_i) \\\\\n&= \\sigma^2\n\\end{aligned}\n\\]Since \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\), outcomes across observations independent. Hence, \\(Y_i\\) \\(Y_j\\) uncorrelated well, conditioned \\(X\\)’s.","code":""},{"path":"linear-regression.html","id":"estimation-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.1 Estimation in Ordinary Least Squares","text":"goal OLS estimate regression parameters (\\(\\beta_0\\), \\(\\beta_1\\)) best describe relationship dependent variable \\(Y\\) independent variable \\(X\\). achieve , minimize sum squared deviations observed values \\(Y_i\\) expected values predicted model.deviation observed value \\(Y_i\\) expected value, based regression model, :\\[\nY_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i).\n\\]deviation represents error prediction \\(\\)-th observation.ensure errors don’t cancel prioritize larger deviations, consider squared deviations. sum squared deviations, denoted \\(Q\\), defined :\\[\nQ = \\sum_{=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2.\n\\]goal OLS find values \\(\\beta_0\\) \\(\\beta_1\\) minimize \\(Q\\). values called OLS estimators.minimize \\(Q\\), take partial derivatives respect \\(\\beta_0\\) \\(\\beta_1\\), set zero, solve resulting system equations. simplifying, estimators slope (\\(b_1\\)) intercept (\\(b_0\\)) obtained follows:Slope (\\(b_1\\)):\\[\nb_1 = \\frac{\\sum_{=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\], \\(\\bar{X}\\) \\(\\bar{Y}\\) represent means \\(X\\) \\(Y\\), respectively. formula reveals slope proportional covariance \\(X\\) \\(Y\\), scaled variance \\(X\\).Intercept (\\(b_0\\)):\\[\nb_0 = \\frac{1}{n} \\left( \\sum_{=1}^{n} Y_i - b_1 \\sum_{=1}^{n} X_i \\right) = \\bar{Y} - b_1 \\bar{X}.\n\\]intercept determined aligning regression line center data.Intuition Behind Estimators\\(b_1\\) (Slope): measures average change \\(Y\\) one-unit increase \\(X\\). formula uses deviations mean ensure relationship captures joint variability \\(X\\) \\(Y\\).\\(b_1\\) (Slope): measures average change \\(Y\\) one-unit increase \\(X\\). formula uses deviations mean ensure relationship captures joint variability \\(X\\) \\(Y\\).\\(b_0\\) (Intercept): ensures regression line passes mean data points \\((\\bar{X}, \\bar{Y})\\), anchoring model center observed data.\\(b_0\\) (Intercept): ensures regression line passes mean data points \\((\\bar{X}, \\bar{Y})\\), anchoring model center observed data.Equivalently, can also write parameters terms covariances.covariance two variables defined :\\[ \\text{Cov}(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \\]Properties Covariance:\\(\\text{Cov}(X_i, X_i) = \\sigma^2_X\\)\\(E(X_i) = 0\\) \\(E(Y_i) = 0\\), \\(\\text{Cov}(X_i, Y_i) = E[X_i Y_i]\\)\\(W_i = + b X_i\\) \\(Z_i = c + d Y_i\\),\\(\\text{Cov}(W_i, Z_i) = bd \\cdot \\text{Cov}(X_i, Y_i)\\)bivariate regression, slope \\(\\beta\\) bivariate regression given :\\[ \\beta = \\frac{\\text{Cov}(Y_i, X_i)}{\\text{Var}(X_i)} \\]multivariate case, slope \\(X_k\\) :\\[ \\beta_k = \\frac{\\text{Cov}(Y_i, \\tilde{X}_{ki})}{\\text{Var}(\\tilde{X}_{ki})} \\]\\(\\tilde{X}_{ki}\\) represents residual regression \\(X_{ki}\\) \\(K-1\\) covariates model.intercept :\\[ \\beta_0 = E[Y_i] - \\beta_1 E(X_i) \\]Note:OLS require assumption specific distribution variables. robustness based minimization squared errors (.e., distributional assumptions).","code":""},{"path":"linear-regression.html","id":"properties-of-least-squares-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2 Properties of Least Squares Estimators","text":"properties Ordinary Least Squares estimators (\\(b_0\\) \\(b_1\\)) derived based statistical behavior. properties provide insights accuracy, variability, reliability estimates.","code":""},{"path":"linear-regression.html","id":"expectation-of-the-ols-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2.1 Expectation of the OLS Estimators","text":"OLS estimators \\(b_0\\) (intercept) \\(b_1\\) (slope) unbiased. means expected values equal true population parameters:\\[\n\\begin{aligned}\nE(b_1) &= \\beta_1, \\\\\nE(b_0) &= E(\\bar{Y}) - \\bar{X}\\beta_1.\n\\end{aligned}\n\\]Since expected value sample mean \\(Y\\), \\(E(\\bar{Y})\\), :\\[\nE(\\bar{Y}) = \\beta_0 + \\beta_1 \\bar{X},\n\\]expected value \\(b_0\\) simplifies :\\[\nE(b_0) = \\beta_0.\n\\]Thus, \\(b_0\\) \\(b_1\\) unbiased estimators respective population parameters \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"variance-of-the-ols-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2.2 Variance of the OLS Estimators","text":"variability OLS estimators depends spread predictor variable \\(X\\) error variance \\(\\sigma^2\\). variances given :Variance \\(b_1\\) (Slope):\\[\n\\text{Var}(b_1) = \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]Variance \\(b_0\\) (Intercept):\\[\n\\text{Var}(b_0) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]formulas highlight :\\(\\text{Var}(b_1) \\0\\) number observations increases, provided \\(X_i\\) values distributed around mean \\(\\bar{X}\\).\\(\\text{Var}(b_0) \\0\\) \\(n\\) increases, assuming \\(X_i\\) values appropriately selected (.e., clustered near mean).","code":""},{"path":"linear-regression.html","id":"mean-square-error-mse","chapter":"5 Linear Regression","heading":"5.1.1.3 Mean Square Error (MSE)","text":"Mean Square Error (MSE) quantifies average squared residual (error) model:\\[\nMSE = \\frac{SSE}{n-2} = \\frac{\\sum_{=1}^{n} e_i^2}{n-2} = \\frac{\\sum_{=1}^{n} (Y_i - \\hat{Y}_i)^2}{n-2},\n\\]\\(SSE\\) Sum Squared Errors \\(n-2\\) represents degrees freedom simple linear regression model (two parameters estimated: \\(\\beta_0\\) \\(\\beta_1\\)).expected value MSE equals error variance (.e., unbiased Estimator MSE:):\\[\nE(MSE) = \\sigma^2.\n\\]","code":""},{"path":"linear-regression.html","id":"estimating-variance-of-the-ols-coefficients","chapter":"5 Linear Regression","heading":"5.1.1.4 Estimating Variance of the OLS Coefficients","text":"sample-based estimates variances \\(b_0\\) \\(b_1\\) expressed follows:Estimated Variance \\(b_1\\) (Slope):\\[\ns^2(b_1) = \\widehat{\\text{Var}}(b_1) = \\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]Estimated Variance \\(b_0\\) (Intercept):\\[\ns^2(b_0) = \\widehat{\\text{Var}}(b_0) = MSE \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]estimates rely MSE approximate \\(\\sigma^2\\).variance estimates unbiased:\\[\n\\begin{aligned}\nE(s^2(b_1)) &= \\text{Var}(b_1), \\\\\nE(s^2(b_0)) &= \\text{Var}(b_0).\n\\end{aligned}\n\\]Implications PropertiesUnbiasedness: unbiased nature \\(b_0\\) \\(b_1\\) ensures , average, regression model accurately reflects true relationship population.Decreasing Variance: sample size \\(n\\) increases spread \\(X_i\\) values grows, variances \\(b_0\\) \\(b_1\\) decrease, leading precise estimates.Error Estimation MSE: MSE provides reliable estimate error variance \\(\\sigma^2\\), feeds directly assessing reliability \\(b_0\\) \\(b_1\\).","code":""},{"path":"linear-regression.html","id":"residuals-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.5 Residuals in Ordinary Least Squares","text":"Residuals differences observed values (\\(Y_i\\)) predicted counterparts (\\(\\hat{Y}_i\\)). play central role assessing model fit ensuring assumptions OLS met.residual \\(\\)-th observation defined :\\[\ne_i = Y_i - \\hat{Y}_i = Y_i - (b_0 + b_1 X_i),\n\\]:\\(e_i\\): Residual \\(\\)-th observation.\\(\\hat{Y}_i\\): Predicted value based regression model.\\(Y_i\\): Actual observed value.Residuals estimate unobservable error terms \\(\\epsilon_i\\):\\(e_i\\) estimate \\(\\epsilon_i = Y_i - E(Y_i)\\).\\(\\epsilon_i\\) always unknown know true values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"key-properties-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.1 Key Properties of Residuals","text":"Residuals exhibit several mathematical properties align OLS estimation process:Sum Residuals:\nresiduals sum zero:\n\\[\n\\sum_{=1}^{n} e_i = 0.\n\\]\nensures regression line passes centroid data, \\((\\bar{X}, \\bar{Y})\\).Sum Residuals:\nresiduals sum zero:\\[\n\\sum_{=1}^{n} e_i = 0.\n\\]ensures regression line passes centroid data, \\((\\bar{X}, \\bar{Y})\\).Orthogonality Residuals Predictors:\nresiduals orthogonal (uncorrelated) predictor variable \\(X\\):\n\\[\n\\sum_{=1}^{n} X_i e_i = 0.\n\\]\nreflects fact OLS minimizes squared deviations residuals along \\(Y\\)-axis, \\(X\\)-axis.Orthogonality Residuals Predictors:\nresiduals orthogonal (uncorrelated) predictor variable \\(X\\):\\[\n\\sum_{=1}^{n} X_i e_i = 0.\n\\]reflects fact OLS minimizes squared deviations residuals along \\(Y\\)-axis, \\(X\\)-axis.","code":""},{"path":"linear-regression.html","id":"expected-values-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.2 Expected Values of Residuals","text":"expected values residuals reinforce unbiased nature OLS:Mean Residuals:\nresiduals expected value zero:\n\\[\nE[e_i] = 0.\n\\]Mean Residuals:\nresiduals expected value zero:\\[\nE[e_i] = 0.\n\\]Orthogonality Predictors Fitted Values:\nResiduals uncorrelated predictor variables fitted values:\n\\[\n\\begin{aligned}\nE[X_i e_i] &= 0, \\\\\nE[\\hat{Y}_i e_i] &= 0.\n\\end{aligned}\n\\]Orthogonality Predictors Fitted Values:\nResiduals uncorrelated predictor variables fitted values:\\[\n\\begin{aligned}\nE[X_i e_i] &= 0, \\\\\nE[\\hat{Y}_i e_i] &= 0.\n\\end{aligned}\n\\]properties highlight residuals contain systematic information predictors fitted values, reinforcing idea model captured underlying relationship effectively.","code":""},{"path":"linear-regression.html","id":"practical-importance-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.3 Practical Importance of Residuals","text":"Model Diagnostics:\nResiduals analyzed check assumptions OLS, including linearity, homoscedasticity (constant variance), independence errors. Patterns residual plots can signal issues nonlinearity heteroscedasticity.Model Diagnostics:\nResiduals analyzed check assumptions OLS, including linearity, homoscedasticity (constant variance), independence errors. Patterns residual plots can signal issues nonlinearity heteroscedasticity.Goodness--Fit:\nsum squared residuals, \\(\\sum e_i^2\\), measures total unexplained variation \\(Y\\). smaller sum indicates better fit.Goodness--Fit:\nsum squared residuals, \\(\\sum e_i^2\\), measures total unexplained variation \\(Y\\). smaller sum indicates better fit.Influence Analysis:\nLarge residuals may indicate outliers influential points disproportionately affect regression line.Influence Analysis:\nLarge residuals may indicate outliers influential points disproportionately affect regression line.","code":""},{"path":"linear-regression.html","id":"inference-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.6 Inference in Ordinary Least Squares","text":"Inference allows us make probabilistic statements regression parameters (\\(\\beta_0\\), \\(\\beta_1\\)) predictions (\\(Y_h\\)). perform valid inference, certain assumptions distribution errors necessary.Normality AssumptionOLS estimation require assumption normality.However, conduct hypothesis tests construct confidence intervals \\(\\beta_0\\), \\(\\beta_1\\), predictions, distributional assumptions necessary.Inference \\(\\beta_0\\) \\(\\beta_1\\) robust moderate departures normality, especially large samples due Central Limit Theorem.Inference predicted values, \\(Y_{pred}\\), sensitive normality violations.assume normal error model, response variable \\(Y_i\\) modeled :\\[\nY_i \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma^2),\n\\]:\\(\\beta_0 + \\beta_1 X_i\\): Mean response\\(\\sigma^2\\): Variance errorsUnder model, sampling distributions OLS estimators, \\(b_0\\) \\(b_1\\), can derived.","code":""},{"path":"linear-regression.html","id":"inference-for-beta_1-slope","chapter":"5 Linear Regression","heading":"5.1.1.6.1 Inference for \\(\\beta_1\\) (Slope)","text":"normal error model:Sampling Distribution \\(b_1\\):\n\\[\nb_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right).\n\\]\nindicates \\(b_1\\) unbiased estimator \\(\\beta_1\\) variance proportional \\(\\sigma^2\\).Sampling Distribution \\(b_1\\):\\[\nb_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right).\n\\]indicates \\(b_1\\) unbiased estimator \\(\\beta_1\\) variance proportional \\(\\sigma^2\\).Test Statistic:\n\\[\nt = \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2},\n\\]\n\\(s(b_1)\\) standard error \\(b_1\\): \\[\ns(b_1) = \\sqrt{\\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}}.\n\\]Test Statistic:\\[\nt = \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2},\n\\]\\(s(b_1)\\) standard error \\(b_1\\): \\[\ns(b_1) = \\sqrt{\\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}}.\n\\]Confidence Interval:\n\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_1\\) :\n\\[\nb_1 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_1).\n\\]Confidence Interval:\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_1\\) :\\[\nb_1 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_1).\n\\]","code":""},{"path":"linear-regression.html","id":"inference-for-beta_0-intercept","chapter":"5 Linear Regression","heading":"5.1.1.6.2 Inference for \\(\\beta_0\\) (Intercept)","text":"Sampling Distribution \\(b_0\\):\nnormal error model, sampling distribution \\(b_0\\) :\n\\[\nb_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)\\right).\n\\]Sampling Distribution \\(b_0\\):normal error model, sampling distribution \\(b_0\\) :\\[\nb_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)\\right).\n\\]Test Statistic:\n\\[\nt = \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2},\n\\]\n\\(s(b_0)\\) standard error \\(b_0\\): \\[\ns(b_0) = \\sqrt{MSE \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)}.\n\\]Test Statistic:\\[\nt = \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2},\n\\]\\(s(b_0)\\) standard error \\(b_0\\): \\[\ns(b_0) = \\sqrt{MSE \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)}.\n\\]Confidence Interval:\n\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_0\\) :\n\\[\nb_0 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_0).\n\\]Confidence Interval:\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_0\\) :\\[\nb_0 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_0).\n\\]","code":""},{"path":"linear-regression.html","id":"mean-response","chapter":"5 Linear Regression","heading":"5.1.1.6.3 Mean Response","text":"regression, often estimate mean response dependent variable \\(Y\\) given level predictor variable \\(X\\), denoted \\(X_h\\). estimation provides predicted average outcome specific value \\(X\\) based fitted regression model.Let \\(X_h\\) represent level \\(X\\) want estimate mean response.mean response \\(X = X_h\\) denoted \\(E(Y_h)\\).point estimator \\(E(Y_h)\\) \\(\\hat{Y}_h\\), predicted value regression model:\\[\n\\hat{Y}_h = b_0 + b_1 X_h.\n\\]estimator \\(\\hat{Y}_h\\) unbiased expected value equals true mean response \\(E(Y_h)\\):\\[\n\\begin{aligned}\nE(\\hat{Y}_h) &= E(b_0 + b_1 X_h) \\\\\n&= \\beta_0 + \\beta_1 X_h \\\\\n&= E(Y_h).\n\\end{aligned}\n\\]Thus, \\(\\hat{Y}_h\\) provides reliable estimate mean response \\(X_h\\).variance \\(\\hat{Y}_h\\) reflects uncertainty estimate mean response:\\[\n\\begin{aligned}\n\\text{Var}(\\hat{Y}_h) &= \\text{Var}(b_0 + b_1 X_h) \\quad\\text{(definition }\\hat{Y}_h\\text{)}\\\\[6pt]&= \\text{Var}\\left((\\bar{Y} - b_1 \\bar{X}) + b_1 X_h\\right)\\quad\\text{(since } b_0 = \\bar{Y} - b_1 \\bar{X}\\text{)}\\\\[6pt]&= \\text{Var}\\left(\\bar{Y} + b_1(X_h - \\bar{X})\\right)\\quad\\text{(factor } b_1\\text{)}\\\\[6pt]&= \\text{Var}\\left(\\bar{Y} + b_1 (X_h - \\bar{X}) \\right) \\\\\n&= \\text{Var}(\\bar{Y}) + (X_h - \\bar{X})^2 \\text{Var}(b_1) + 2(X_h - \\bar{X}) \\text{Cov}(\\bar{Y}, b_1).\n\\end{aligned}\n\\]Since \\(\\text{Cov}(\\bar{Y}, b_1) = 0\\) (due independence errors, \\(\\epsilon_i\\)), variance simplifies :\\[\n\\text{Var}(\\hat{Y}_h) = \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]can also expressed :\\[\n\\text{Var}(\\hat{Y}_h) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]estimate variance \\(\\hat{Y}_h\\), replace \\(\\sigma^2\\) \\(MSE\\), mean squared error regression:\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]normal error model, sampling distribution \\(\\hat{Y}_h\\) :\\[\n\\begin{aligned}\n\\hat{Y}_h &\\sim N\\left(E(Y_h), \\text{Var}(\\hat{Y}_h)\\right), \\\\\n\\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} &\\sim t_{n-2}.\n\\end{aligned}\n\\]result follows \\(\\hat{Y}_h\\) linear combination normally distributed random variables, variance estimated using \\(s^2(\\hat{Y}_h)\\).\\(100(1-\\alpha)\\%\\) confidence interval mean response \\(E(Y_h)\\) given :\\[\n\\hat{Y}_h \\pm t_{1-\\alpha/2; n-2} \\cdot s(\\hat{Y}_h),\n\\]:\\(\\hat{Y}_h\\): Point estimate mean response,\\(s(\\hat{Y}_h)\\): Estimated standard error mean response,\\(t_{1-\\alpha/2; n-2}\\): Critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.","code":""},{"path":"linear-regression.html","id":"prediction-of-a-new-observation","chapter":"5 Linear Regression","heading":"5.1.1.6.4 Prediction of a New Observation","text":"analyzing regression results, important distinguish :Estimating mean response particular value \\(X\\).Predicting individual outcome particular value \\(X\\).Mean Response vs. Individual OutcomeSame Point Estimate\nformula estimated mean response predicted individual outcome \\(X = X_h\\) identical:\\[\n\\hat{Y}_{pred} = \\hat{Y}_h = b_0 + b_1 X_h.\n\\]Point Estimate\nformula estimated mean response predicted individual outcome \\(X = X_h\\) identical:\\[\n\\hat{Y}_{pred} = \\hat{Y}_h = b_0 + b_1 X_h.\n\\]Different Variance\nAlthough point estimates , level uncertainty differs. predicting individual outcome, must consider uncertainty estimating mean response (\\(\\hat{Y}_h\\)) also additional random variation within distribution \\(Y\\).Different Variance\nAlthough point estimates , level uncertainty differs. predicting individual outcome, must consider uncertainty estimating mean response (\\(\\hat{Y}_h\\)) also additional random variation within distribution \\(Y\\).Therefore, prediction intervals (individual outcomes) account uncertainty consequently wider confidence intervals (mean response).predict individual outcome given \\(X_h\\), combine mean response random error:\\[\nY_{pred} = \\beta_0 + \\beta_1 X_h + \\epsilon.\n\\]Using least squares predictor:\\[\n\\hat{Y}_{pred} = b_0 + b_1 X_h,\n\\]since \\(E(\\epsilon) = 0\\).variance predicted value new observation, \\(Y_{pred}\\), includes :Variance estimated mean response: \\[\n\\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right),\n\\]Variance error term, \\(\\epsilon\\), \\(\\sigma^2\\).Thus, total variance :\\[\n\\begin{aligned}\n\\text{Var}(Y_{pred}) &= \\text{Var}(b_0 + b_1 X_h + \\epsilon) \\\\\n&= \\text{Var}(b_0 + b_1 X_h) + \\text{Var}(\\epsilon) \\\\\n&= \\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right) + \\sigma^2 \\\\\n&= \\sigma^2 \\left( 1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\end{aligned}\n\\]estimate variance prediction using \\(MSE\\), mean squared error:\\[\ns^2(pred) = MSE \\left( 1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]normal error model, standardized predicted value follows \\(t\\)-distribution \\(n-2\\) degrees freedom:\\[\n\\frac{Y_{pred} - \\hat{Y}_h}{s(pred)} \\sim t_{n-2}.\n\\]\\(100(1-\\alpha)\\%\\) prediction interval \\(Y_{pred}\\) :\\[\n\\hat{Y}_{pred} \\pm t_{1-\\alpha/2; n-2} \\cdot s(pred).\n\\]","code":""},{"path":"linear-regression.html","id":"confidence-band","chapter":"5 Linear Regression","heading":"5.1.1.6.5 Confidence Band","text":"regression analysis, often want evaluate uncertainty around entire regression line, just single value predictor variable \\(X\\). achieved using confidence band, provides confidence interval mean response, \\(E(Y) = \\beta_0 + \\beta_1 X\\), entire range \\(X\\) values.Working-Hotelling confidence band method construct simultaneous confidence intervals regression line. given \\(X_h\\), confidence band expressed :\\[\n\\hat{Y}_h \\pm W s(\\hat{Y}_h),\n\\]:\\(W^2 = 2F_{1-\\alpha; 2, n-2}\\),\n\\(F_{1-\\alpha; 2, n-2}\\) critical value \\(F\\)-distribution 2 \\(n-2\\) degrees freedom.\n\\(W^2 = 2F_{1-\\alpha; 2, n-2}\\),\\(F_{1-\\alpha; 2, n-2}\\) critical value \\(F\\)-distribution 2 \\(n-2\\) degrees freedom.\\(s(\\hat{Y}_h)\\) standard error estimated mean response \\(X_h\\):\n\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]\\(s(\\hat{Y}_h)\\) standard error estimated mean response \\(X_h\\):\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]Key Properties Confidence BandWidth Interval:\nwidth confidence band changes \\(X_h\\) \\(s(\\hat{Y}_h)\\) depends far \\(X_h\\) mean \\(X\\) (\\(\\bar{X}\\)).\ninterval narrowest \\(X = \\bar{X}\\), variance estimated mean response minimized.\nwidth confidence band changes \\(X_h\\) \\(s(\\hat{Y}_h)\\) depends far \\(X_h\\) mean \\(X\\) (\\(\\bar{X}\\)).interval narrowest \\(X = \\bar{X}\\), variance estimated mean response minimized.Shape Band:\nboundaries confidence band form hyperbolic shape around regression line.\nreflects increasing uncertainty mean response \\(X_h\\) moves farther \\(\\bar{X}\\).\nboundaries confidence band form hyperbolic shape around regression line.reflects increasing uncertainty mean response \\(X_h\\) moves farther \\(\\bar{X}\\).Simultaneous Coverage:\nWorking-Hotelling band ensures true regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) lies within band across values \\(X\\) specified confidence level (e.g., \\(95\\%\\)).\nWorking-Hotelling band ensures true regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) lies within band across values \\(X\\) specified confidence level (e.g., \\(95\\%\\)).","code":""},{"path":"linear-regression.html","id":"analysis-of-variance-anova-in-regression","chapter":"5 Linear Regression","heading":"5.1.1.7 Analysis of Variance (ANOVA) in Regression","text":"ANOVA regression decomposes total variability response variable (\\(Y\\)) components attributed regression model residual error. context regression, ANOVA provides mechanism assess fit model test hypotheses relationship \\(X\\) \\(Y\\).corrected Total Sum Squares (SSTO) quantifies total variation \\(Y\\):\\[\nSSTO = \\sum_{=1}^n (Y_i - \\bar{Y})^2,\n\\]\\(\\bar{Y}\\) mean response variable. term “corrected” refers fact sum squares calculated relative mean (.e., uncorrected total sum squares given \\(\\sum Y_i^2\\))Using fitted regression model \\(\\hat{Y}_i = b_0 + b_1 X_i\\), estimate conditional mean \\(Y\\) \\(X_i\\). total sum squares can decomposed :\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\\n&= \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2 + 2 \\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) \\\\\n&= \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\n\\end{aligned}\n\\]cross-product term zero, shown .decomposition simplifies :\\[\nSSTO = SSE + SSR,\n\\]:\\(SSE = \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2\\): Error Sum Squares (variation unexplained model).\\(SSE = \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2\\): Error Sum Squares (variation unexplained model).\\(SSR = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Regression Sum Squares (variation explained model), measure conditional mean varies central value.\\(SSR = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Regression Sum Squares (variation explained model), measure conditional mean varies central value.Degrees freedom partitioned :\\[\n\\begin{aligned}\nSSTO &= SSR + SSE \\\\\n(n-1) &= (1) + (n-2) \\\\\n\\end{aligned}\n\\]confirm cross-product term zero:\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y})\n&= \\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\quad \\text{(Expand } Y_i - \\hat{Y}_i \\text{ } \\hat{Y}_i - \\bar{Y}\\text{)} \\\\\n&=\\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))( b_1 (X_i - \\bar{X}))  \\\\\n&= b_1 \\sum_{=1}^n (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 \\quad \\text{(Distribute terms product)} \\\\\n&= b_1 \\frac{\\sum_{=1}^n (Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{=1}^n (X_i - \\bar{X})^2} \\sum_{=1}^n (X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 \\quad \\text{(Substitute } b_1 \\text{ definition)} \\\\\n&= b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2  \\\\\n&= 0\n\\end{aligned}\n\\]ANOVA table summarizes partitioning variability:expected values mean squares :\\[\n\\begin{aligned}\nE(MSE) &= \\sigma^2, \\\\\nE(MSR) &= \\sigma^2 + \\beta_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2.\n\\end{aligned}\n\\]\\(\\beta_1 = 0\\):\nregression model explain variation \\(Y\\) beyond mean, \\(E(MSR) = E(MSE) = \\sigma^2\\).\ncondition corresponds null hypothesis, \\(H_0: \\beta_1 = 0\\).\nregression model explain variation \\(Y\\) beyond mean, \\(E(MSR) = E(MSE) = \\sigma^2\\).condition corresponds null hypothesis, \\(H_0: \\beta_1 = 0\\).\\(\\beta_1 \\neq 0\\):\nregression model explains variation \\(Y\\), \\(E(MSR) > E(MSE)\\).\nadditional term \\(\\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2\\) represents variance explained predictor \\(X\\).\nregression model explains variation \\(Y\\), \\(E(MSR) > E(MSE)\\).additional term \\(\\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2\\) represents variance explained predictor \\(X\\).difference \\(E(MSR)\\) \\(E(MSE)\\) allows us infer whether \\(\\beta_1 \\neq 0\\) comparing ratio.Assuming errors \\(\\epsilon_i\\) independent identically distributed \\(N(0, \\sigma^2)\\), null hypothesis \\(H_0: \\beta_1 = 0\\), :scaled \\(MSE\\) follows chi-square distribution \\(n-2\\) degrees freedom:\n\\[\n\\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2.\n\\]scaled \\(MSE\\) follows chi-square distribution \\(n-2\\) degrees freedom:\\[\n\\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2.\n\\]scaled \\(MSR\\) follows chi-square distribution \\(1\\) degree freedom:\n\\[\n\\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2.\n\\]scaled \\(MSR\\) follows chi-square distribution \\(1\\) degree freedom:\\[\n\\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2.\n\\]two chi-square random variables independent.two chi-square random variables independent.ratio two independent chi-square random variables, scaled respective degrees freedom, follows \\(F\\)-distribution. Therefore, \\(H_0\\):\\[\nF = \\frac{MSR}{MSE} \\sim F_{1, n-2}.\n\\]\\(F\\)-statistic tests whether regression model provides significant improvement null model (constant \\(E(Y)\\)).hypotheses \\(F\\)-test :Null Hypothesis (\\(H_0\\)): \\(\\beta_1 = 0\\) (relationship \\(X\\) \\(Y\\)).Alternative Hypothesis (\\(H_a\\)): \\(\\beta_1 \\neq 0\\) (significant relationship exists \\(X\\) \\(Y\\)).rejection rule \\(H_0\\) significance level \\(\\alpha\\) :\\[\nF > F_{1-\\alpha;1,n-2},\n\\]\\(F_{1-\\alpha;1,n-2}\\) critical value \\(F\\)-distribution \\(1\\) \\(n-2\\) degrees freedom.\\(F \\leq F_{1-\\alpha;1,n-2}\\):\nFail reject \\(H_0\\). insufficient evidence conclude \\(X\\) significantly explains variation \\(Y\\).\nFail reject \\(H_0\\). insufficient evidence conclude \\(X\\) significantly explains variation \\(Y\\).\\(F > F_{1-\\alpha;1,n-2}\\):\nReject \\(H_0\\). significant evidence \\(X\\) explains variation \\(Y\\).\nReject \\(H_0\\). significant evidence \\(X\\) explains variation \\(Y\\).","code":""},{"path":"linear-regression.html","id":"coefficient-of-determination-r2","chapter":"5 Linear Regression","heading":"5.1.1.8 Coefficient of Determination (\\(R^2\\))","text":"Coefficient Determination (\\(R^2\\)) measures well linear regression model accounts variability response variable \\(Y\\). defined :\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO},\n\\]:\\(SSR\\): Regression Sum Squares (variation explained model).\\(SSTO\\): Total Sum Squares (total variation \\(Y\\) mean).\\(SSE\\): Error Sum Squares (variation unexplained model).Properties \\(R^2\\)Range: \\[\n0 \\leq R^2 \\leq 1.\n\\]\n\\(R^2 = 0\\): model explains none variability \\(Y\\) (e.g., \\(\\beta_1 = 0\\)).\n\\(R^2 = 1\\): model explains variability \\(Y\\) (perfect fit).\nRange: \\[\n0 \\leq R^2 \\leq 1.\n\\]\\(R^2 = 0\\): model explains none variability \\(Y\\) (e.g., \\(\\beta_1 = 0\\)).\\(R^2 = 1\\): model explains variability \\(Y\\) (perfect fit).Proportionate Reduction Variance: \\(R^2\\) represents proportionate reduction total variation \\(Y\\) fitting model. quantifies much better model predicts \\(Y\\) compared simply using \\(\\bar{Y}\\).Proportionate Reduction Variance: \\(R^2\\) represents proportionate reduction total variation \\(Y\\) fitting model. quantifies much better model predicts \\(Y\\) compared simply using \\(\\bar{Y}\\).Potential Misinterpretation: really correct say \\(R^2\\) “variation \\(Y\\) explained \\(X\\).” term “variation explained” assumes causative deterministic explanation, always correct. example:\n\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.\ncases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).\nPotential Misinterpretation: really correct say \\(R^2\\) “variation \\(Y\\) explained \\(X\\).” term “variation explained” assumes causative deterministic explanation, always correct. example:\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.cases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).cases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).simple linear regression, \\(R^2\\) square Pearson correlation coefficient, \\(r\\):\\[\nR^2 = (r)^2,\n\\]:\\(r = \\text{corr}(X, Y)\\) sample correlation coefficient.relationship \\(b_1\\) (slope regression line) \\(r\\) given :\\[\nb_1 = \\left(\\frac{\\sum_{=1}^n (Y_i - \\bar{Y})^2}{\\sum_{=1}^n (X_i - \\bar{X})^2}\\right)^{1/2}.\n\\]Additionally, \\(r\\) can expressed :\\[\nr = \\frac{s_y}{s_x} \\cdot r,\n\\]\\(s_y\\) \\(s_x\\) sample standard deviations \\(Y\\) \\(X\\), respectively.","code":""},{"path":"linear-regression.html","id":"lack-of-fit-in-regression","chapter":"5 Linear Regression","heading":"5.1.1.9 Lack of Fit in Regression","text":"lack fit test evaluates whether chosen regression model adequately captures relationship predictor variable \\(X\\) response variable \\(Y\\). repeated observations specific values \\(X\\), can partition Error Sum Squares (\\(SSE\\)) two components:Pure ErrorLack Fit.Given observations:\\(Y_{ij}\\): \\(j\\)-th replicate \\(\\)-th distinct value \\(X\\),\n\\(Y_{11}, Y_{21}, \\dots, Y_{n_1, 1}\\): \\(n_1\\) repeated observations \\(X_1\\)\n\\(Y_{1c}, Y_{2c}, \\dots, Y_{n_c,c}\\): \\(n_c\\) repeated observations \\(X_c\\)\n\\(Y_{11}, Y_{21}, \\dots, Y_{n_1, 1}\\): \\(n_1\\) repeated observations \\(X_1\\)\\(Y_{1c}, Y_{2c}, \\dots, Y_{n_c,c}\\): \\(n_c\\) repeated observations \\(X_c\\)\\(\\bar{Y}_j\\): mean response replicates \\(X_j\\),\\(\\hat{Y}_{ij}\\): predicted value regression model \\(X_j\\),Error Sum Squares (\\(SSE\\)) can decomposed :\\[\n\\begin{aligned}\n\\sum_{} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j - \\hat{Y}_{ij})^2 \\\\\n&= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{j} n_j (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\\n&= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{j} n_j (\\bar{Y}_j - \\hat{Y}_{ij})^2\n\\end{aligned}\n\\]cross product term zero deviations within replicates deviations replicates orthogonal.simplifies :\\[\nSSE = SSPE + SSLF,\n\\]:\\(SSPE\\) (Pure Error Sum Squares): Variation within replicates \\(X_j\\), reflecting natural variability response.\nDegrees freedom: \\(df_{pe} = n - c\\), \\(n\\) total number observations, \\(c\\) number distinct \\(X\\) values.\nDegrees freedom: \\(df_{pe} = n - c\\), \\(n\\) total number observations, \\(c\\) number distinct \\(X\\) values.\\(SSLF\\) (Lack Fit Sum Squares): Variation replicate means \\(\\bar{Y}_j\\) model-predicted values \\(\\hat{Y}_{ij}\\). SSLF large, suggests model may adequately describe relationship \\(X\\) \\(Y\\).\nDegrees freedom: \\(df_{lf} = c - 2\\), 2 accounts parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)).\nDegrees freedom: \\(df_{lf} = c - 2\\), 2 accounts parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)).Mean Square Pure Error (MSPE):\\[\nMSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c}.\n\\]Mean Square Pure Error (MSPE):\\[\nMSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c}.\n\\]Mean Square Lack Fit (MSLF):\\[\nMSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}.\n\\]Mean Square Lack Fit (MSLF):\\[\nMSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}.\n\\]","code":""},{"path":"linear-regression.html","id":"the-f-test-for-lack-of-fit","chapter":"5 Linear Regression","heading":"5.1.1.9.1 The F-Test for Lack of Fit","text":"F-test lack fit evaluates whether chosen regression model adequately captures relationship predictor variable \\(X\\) response variable \\(Y\\). Specifically, tests whether systematic deviations model exist accounted random error.Null Hypothesis (\\(H_0\\)):\nregression model adequate: \\[\nH_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Null Hypothesis (\\(H_0\\)):\nregression model adequate: \\[\nH_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Alternative Hypothesis (\\(H_a\\)):\nregression model adequate includes additional function \\(f(X_i, Z_1, \\dots)\\) account lack fit: \\[\nH_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1, \\dots) + \\epsilon_{ij}^*, \\quad \\epsilon_{ij}^* \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Alternative Hypothesis (\\(H_a\\)):\nregression model adequate includes additional function \\(f(X_i, Z_1, \\dots)\\) account lack fit: \\[\nH_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1, \\dots) + \\epsilon_{ij}^*, \\quad \\epsilon_{ij}^* \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Expected Mean SquaresThe expected Mean Square Pure Error (MSPE) \\(H_0\\) \\(H_a\\):\n\\[\nE(MSPE) = \\sigma^2.\n\\]expected Mean Square Pure Error (MSPE) \\(H_0\\) \\(H_a\\):\\[\nE(MSPE) = \\sigma^2.\n\\]expected Mean Square Lack Fit (MSLF) depends whether \\(H_0\\) true:\n\\(H_0\\) (model adequate): \\[\nE(MSLF) = \\sigma^2.\n\\]\n\\(H_a\\) (model adequate): \\[\nE(MSLF) = \\sigma^2 + \\frac{\\sum n_j f(X_i, Z_1, \\dots)^2}{n-2}.\n\\]\nexpected Mean Square Lack Fit (MSLF) depends whether \\(H_0\\) true:\\(H_0\\) (model adequate): \\[\nE(MSLF) = \\sigma^2.\n\\]\\(H_a\\) (model adequate): \\[\nE(MSLF) = \\sigma^2 + \\frac{\\sum n_j f(X_i, Z_1, \\dots)^2}{n-2}.\n\\]test statistic lack--fit test :\\[\nF = \\frac{MSLF}{MSPE},\n\\]:\\(MSLF = \\frac{SSLF}{c-2}\\),\n\\(SSLF\\) Lack Fit Sum Squares.\\(MSPE = \\frac{SSPE}{n-c}\\),\n\\(SSPE\\) Pure Error Sum Squares.\\(H_0\\), \\(F\\)-statistic follows \\(F\\)-distribution:\\[\nF \\sim F_{c-2, n-c}.\n\\]Decision RuleReject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nF > F_{1-\\alpha; c-2, n-c}.\n\\]Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nF > F_{1-\\alpha; c-2, n-c}.\n\\]Failing reject \\(H_0\\):\nIndicates evidence lack fit.\nimply model “true,” suggests model provides reasonable approximation true relationship.\nFailing reject \\(H_0\\):Indicates evidence lack fit.imply model “true,” suggests model provides reasonable approximation true relationship.summarize, repeat observations exist levels \\(X\\), Error Sum Squares (SSE) can partitioned Lack Fit (SSLF) Pure Error (SSPE). leads extended ANOVA table:Repeat observations important implications coefficient determination (\\(R^2\\)):\\(R^2\\) Can’t Attain 1 Repeat Observations:\nrepeat observations, \\(SSE\\) (Error Sum Squares) reduced 0 \\(SSPE > 0\\) (variability within replicates).\nrepeat observations, \\(SSE\\) (Error Sum Squares) reduced 0 \\(SSPE > 0\\) (variability within replicates).Maximum \\(R^2\\):\nmaximum attainable \\(R^2\\) presence repeat observations :\n\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]\nmaximum attainable \\(R^2\\) presence repeat observations :\n\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]maximum attainable \\(R^2\\) presence repeat observations :\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]Importance Repeat Observations:\nlevels \\(X\\) need repeat observations, presence enables separation pure error lack fit, making \\(F\\)-test lack fit possible.\nlevels \\(X\\) need repeat observations, presence enables separation pure error lack fit, making \\(F\\)-test lack fit possible.Estimation \\(\\sigma^2\\) Repeat ObservationsUse MSE:\n\\(H_0\\) appropriate (model fits well), \\(MSE\\) typically used estimate \\(\\sigma^2\\) instead \\(MSPE\\) degrees freedom provides reliable estimate.\n\\(H_0\\) appropriate (model fits well), \\(MSE\\) typically used estimate \\(\\sigma^2\\) instead \\(MSPE\\) degrees freedom provides reliable estimate.Pooling Estimates:\npractice, \\(MSE\\) \\(MSPE\\) may pooled \\(H_0\\) holds, resulting precise estimate \\(\\sigma^2\\).\npractice, \\(MSE\\) \\(MSPE\\) may pooled \\(H_0\\) holds, resulting precise estimate \\(\\sigma^2\\).","code":""},{"path":"linear-regression.html","id":"joint-inference-for-regression-parameters","chapter":"5 Linear Regression","heading":"5.1.1.10 Joint Inference for Regression Parameters","text":"Joint inference considers simultaneous coverage confidence intervals multiple regression parameters, \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Ensuring adequate confidence parameters together requires adjustments maintain desired family-wise confidence level.Let:\\(\\bar{}_1\\): event confidence interval \\(\\beta_0\\) covers true value.\\(\\bar{}_2\\): event confidence interval \\(\\beta_1\\) covers true value.individual confidence levels :\\[\n\\begin{aligned}\nP(\\bar{}_1) &= 1 - \\alpha, \\\\\nP(\\bar{}_2) &= 1 - \\alpha.\n\\end{aligned}\n\\]joint confidence coefficient, \\(P(\\bar{}_1 \\cap \\bar{}_2)\\), :\\[\n\\begin{aligned}\nP(\\bar{}_1 \\cap \\bar{}_2) &= 1 - P(\\bar{}_1 \\cup \\bar{}_2), \\\\\n&= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2), \\\\\n&\\geq 1 - P(A_1) - P(A_2), \\\\\n&= 1 - 2\\alpha.\n\\end{aligned}\n\\]means \\(\\alpha\\) significance level parameter, joint confidence coefficient least \\(1 - 2\\alpha\\). inequality known Bonferroni Inequality.Bonferroni Confidence IntervalsTo ensure desired joint confidence level \\((1-\\alpha)\\) \\(\\beta_0\\) \\(\\beta_1\\), Bonferroni method adjusts confidence level parameter dividing \\(\\alpha\\) number parameters. two parameters:confidence level parameter \\((1-\\alpha/2)\\).confidence level parameter \\((1-\\alpha/2)\\).resulting Bonferroni-adjusted confidence intervals :\n\\[\n\\begin{aligned}\nb_0 &\\pm B \\cdot s(b_0), \\\\\nb_1 &\\pm B \\cdot s(b_1),\n\\end{aligned}\n\\]\n\\(B = t_{1-\\alpha/4; n-2}\\) critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.resulting Bonferroni-adjusted confidence intervals :\\[\n\\begin{aligned}\nb_0 &\\pm B \\cdot s(b_0), \\\\\nb_1 &\\pm B \\cdot s(b_1),\n\\end{aligned}\n\\]\\(B = t_{1-\\alpha/4; n-2}\\) critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.Interpretation Bonferroni Confidence IntervalsCoverage Probability:\nrepeated samples taken, \\((1-\\alpha)100\\%\\) joint intervals contain true values \\((\\beta_0, \\beta_1)\\).\nimplies \\(\\alpha \\times 100\\%\\) samples miss least one true parameter values.\nrepeated samples taken, \\((1-\\alpha)100\\%\\) joint intervals contain true values \\((\\beta_0, \\beta_1)\\).implies \\(\\alpha \\times 100\\%\\) samples miss least one true parameter values.Conservatism:\nBonferroni method ensures family-wise confidence level conservative. actual joint confidence level often higher \\((1-\\alpha)100\\%\\).\nconservatism reduces statistical power.\nBonferroni method ensures family-wise confidence level conservative. actual joint confidence level often higher \\((1-\\alpha)100\\%\\).conservatism reduces statistical power.red point represents estimated coefficients (b0_hat, b1_hat).blue lines represent Bonferroni-adjusted confidence intervals beta_0 beta_1.grey points represent joint confidence region based covariance matrix coefficients.Bonferroni intervals ensure family-wise confidence level conservative.Simulation results demonstrate often true values captured intervals repeated samples drawn.Notes:Conservatism Bonferroni Intervals\nBonferroni interval conservative:\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).\nconservatism results wider intervals, reducing statistical power test.\n\nAdjustments Conservatism:\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.\nhigher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.\n\nBonferroni interval conservative:\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).\nconservatism results wider intervals, reducing statistical power test.\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).conservatism results wider intervals, reducing statistical power test.Adjustments Conservatism:\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.\nhigher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.higher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.Extending Bonferroni Multiple Parameters: Bonferroni method limited two parameters. testing \\(g\\) parameters, \\(\\beta_0, \\beta_1, \\dots, \\beta_{g-1}\\):\nAdjusted Confidence Level Parameter:\nconfidence level individual parameter \\((1-\\alpha/g)\\).\n\nCritical \\(t\\)-Value:\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.\n\nExample:\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]\ncorresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.\n\nAdjusted Confidence Level Parameter:\nconfidence level individual parameter \\((1-\\alpha/g)\\).\nconfidence level individual parameter \\((1-\\alpha/g)\\).Critical \\(t\\)-Value:\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.Example:\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]\ncorresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]corresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.Limitations Large \\(g\\)\nWide Intervals:\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.\nissue stems conservatism Bonferroni method, prioritizes family-wise error control.\n\nSuitability Small \\(g\\):\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).\nlarger \\(g\\), alternative methods (discussed ) efficient.\n\nWide Intervals:\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.\nissue stems conservatism Bonferroni method, prioritizes family-wise error control.\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.issue stems conservatism Bonferroni method, prioritizes family-wise error control.Suitability Small \\(g\\):\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).\nlarger \\(g\\), alternative methods (discussed ) efficient.\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).larger \\(g\\), alternative methods (discussed ) efficient.Correlation Parameters: Correlation \\(b_0\\) \\(b_1\\):\nestimated regression coefficients \\(b_0\\) \\(b_1\\) often correlated:\nNegative correlation \\(\\bar{X} > 0\\).\nPositive correlation \\(\\bar{X} < 0\\).\n\ncorrelation can complicate joint inference affect validity Bonferroni-adjusted intervals.\nestimated regression coefficients \\(b_0\\) \\(b_1\\) often correlated:\nNegative correlation \\(\\bar{X} > 0\\).\nPositive correlation \\(\\bar{X} < 0\\).\nNegative correlation \\(\\bar{X} > 0\\).Positive correlation \\(\\bar{X} < 0\\).correlation can complicate joint inference affect validity Bonferroni-adjusted intervals.Alternatives BonferroniSeveral alternative procedures provide precise joint inference, especially larger \\(g\\):Scheffé’s Method:\nConstructs simultaneous confidence regions possible linear combinations parameters.\nSuitable exploratory analyses may result even wider intervals Bonferroni.\nConstructs simultaneous confidence regions possible linear combinations parameters.Suitable exploratory analyses may result even wider intervals Bonferroni.Tukey’s Honest Significant Difference:\nDesigned pairwise comparisons ANOVA can adapted regression parameters.\nDesigned pairwise comparisons ANOVA can adapted regression parameters.Holm’s Step-Procedure:\nsequential testing procedure less conservative Bonferroni still controlling family-wise error rate.\nsequential testing procedure less conservative Bonferroni still controlling family-wise error rate.Likelihood Ratio Tests:\nConstruct joint confidence regions based likelihood function, offering precision large \\(g\\).\nConstruct joint confidence regions based likelihood function, offering precision large \\(g\\).","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(MASS)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic data\nn <- 100  # Number of observations\nx <- rnorm(n, mean = 0, sd = 1)  # Predictor\nbeta_0 <- 2  # True intercept\nbeta_1 <- 3  # True slope\nsigma <- 1  # Standard deviation of error\ny <-\n    beta_0 + beta_1 * x + rnorm(n, mean = 0, sd = sigma)  # Response\n\n# Fit linear model\nmodel <- lm(y ~ x)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.9073 -0.6835 -0.0875  0.5806  3.2904 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.89720    0.09755   19.45   <2e-16 ***\n#> x            2.94753    0.10688   27.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9707 on 98 degrees of freedom\n#> Multiple R-squared:  0.8859, Adjusted R-squared:  0.8847 \n#> F-statistic: 760.6 on 1 and 98 DF,  p-value: < 2.2e-16\n\n# Extract coefficients and standard errors\nb0_hat <- coef(model)[1]\nb1_hat <- coef(model)[2]\ns_b0 <-\n    summary(model)$coefficients[1, 2]  # Standard error of intercept\ns_b1 <- summary(model)$coefficients[2, 2]  # Standard error of slope\n\n# Desired confidence level\nalpha <- 0.05  # Overall significance level\n\n# Bonferroni correction\nadjusted_alpha <- alpha / 2  # Adjusted alpha for each parameter\n\n# Critical t-value for Bonferroni adjustment\nt_crit <-\n    qt(1 - adjusted_alpha, df = n - 2)  # n-2 degrees of freedom\n\n# Bonferroni confidence intervals\nci_b0 <- c(b0_hat - t_crit * s_b0, b0_hat + t_crit * s_b0)\nci_b1 <- c(b1_hat - t_crit * s_b1, b1_hat + t_crit * s_b1)\n\n# Print results\ncat(\"Bonferroni Confidence Intervals:\\n\")\n#> Bonferroni Confidence Intervals:\ncat(\"Intercept (beta_0): [\",\n    round(ci_b0[1], 2),\n    \",\",\n    round(ci_b0[2], 2),\n    \"]\\n\")\n#> Intercept (beta_0): [ 1.7 , 2.09 ]\ncat(\"Slope (beta_1): [\",\n    round(ci_b1[1], 2),\n    \",\",\n    round(ci_b1[2], 2),\n    \"]\\n\")\n#> Slope (beta_1): [ 2.74 , 3.16 ]\n\n# Calculate the covariance matrix of coefficients\ncov_matrix <- vcov(model)\n\n# Generate points for confidence ellipse\nellipse_points <-\n    MASS::mvrnorm(n = 1000,\n                  mu = coef(model),\n                  Sigma = cov_matrix)\n\n# Convert to data frame for plotting\nellipse_df <- as.data.frame(ellipse_points)\ncolnames(ellipse_df) <- c(\"beta_0\", \"beta_1\")\n\n# Plot confidence intervals and ellipse\np <- ggplot() +\n    # Confidence ellipse\n    geom_point(\n        data = ellipse_df,\n        aes(x = beta_0, y = beta_1),\n        alpha = 0.1,\n        color = \"grey\"\n    ) +\n    # Point estimate\n    geom_point(aes(x = b0_hat, y = b1_hat),\n               color = \"red\",\n               size = 3) +\n    # Bonferroni confidence intervals\n    geom_errorbar(aes(x = b0_hat, ymin = ci_b1[1], ymax = ci_b1[2]),\n                  width = 0.1,\n                  color = \"blue\") +\n    geom_errorbarh(aes(y = b1_hat, xmin = ci_b0[1], xmax = ci_b0[2]),\n                   height = 0.1,\n                   color = \"blue\") +\n    labs(title = \"Bonferroni Confidence Intervals and Joint Confidence Region\",\n         x = \"Intercept (beta_0)\",\n         y = \"Slope (beta_1)\") +\n    theme_minimal()\n\nprint(p)"},{"path":"linear-regression.html","id":"assumptions-of-linear-regression","chapter":"5 Linear Regression","heading":"5.1.1.11 Assumptions of Linear Regression","text":"ensure valid inference reliable predictions linear regression, following assumptions must hold. ’ll cover depth next section.","code":""},{"path":"linear-regression.html","id":"diagnostics-for-model-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.12 Diagnostics for Model Assumptions","text":"Constant VarianceTo check homoscedasticity:\nPlot residuals vs. fitted values residuals vs. predictors.\nLook patterns funnel-shaped spread indicating heteroscedasticity.\nPlot residuals vs. fitted values residuals vs. predictors.Look patterns funnel-shaped spread indicating heteroscedasticity.OutliersDetect outliers using:\nResiduals vs. predictors plot.\nBox plots.\nStem--leaf plots.\nScatter plots.\nResiduals vs. predictors plot.Box plots.Stem--leaf plots.Scatter plots.Standardized Residuals:Residuals can standardized unit variance, known studentized residuals: \\[\n  r_i = \\frac{e_i}{s(e_i)}.\n  \\]Semi-Studentized Residuals:simplified standardization using mean squared error (MSE): \\[\n  e_i^* = \\frac{e_i}{\\sqrt{MSE}}.\n  \\]Non-Independent Error TermsTo detect non-independence:\nPlot residuals vs. time time-series data.\nResiduals \\(e_i\\) independent depend \\(\\hat{Y}_i\\), derived regression function.\nPlot residuals vs. time time-series data.Residuals \\(e_i\\) independent depend \\(\\hat{Y}_i\\), derived regression function.Detect dependency plotting residual \\(\\)-th response vs. \\((-1)\\)-th.Non-Normality Error TermsTo assess normality:\nPlot distribution residuals.\nCreate box plots, stem--leaf plots, normal probability plots.\nPlot distribution residuals.Create box plots, stem--leaf plots, normal probability plots.Issues incorrect regression function non-constant error variance can distort residual distribution.Normality tests require relatively large sample sizes detect deviations.Normality ResidualsUse tests based empirical cumulative distribution function (ECDF) (check Normality Assessment)Constancy Error VarianceStatistical tests homoscedasticity:\nBrown-Forsythe Test (Modified Levene Test):\nRobust non-normality, examines variance residuals across levels predictors.\n\nBreusch-Pagan Test (Cook-Weisberg Test):\nTests heteroscedasticity regressing squared residuals predictors.\n\nBrown-Forsythe Test (Modified Levene Test):\nRobust non-normality, examines variance residuals across levels predictors.\nRobust non-normality, examines variance residuals across levels predictors.Breusch-Pagan Test (Cook-Weisberg Test):\nTests heteroscedasticity regressing squared residuals predictors.\nTests heteroscedasticity regressing squared residuals predictors.","code":""},{"path":"linear-regression.html","id":"remedial-measures-for-violations-of-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.13 Remedial Measures for Violations of Assumptions","text":"assumptions simple linear regression violated, appropriate remedial measures can applied address issues. list measures specific deviations assumptions.","code":""},{"path":"linear-regression.html","id":"general-remedies","chapter":"5 Linear Regression","heading":"5.1.1.13.1 General Remedies","text":"Use complicated models (e.g., non-linear models, generalized linear models).Apply transformations (see Variable Transformation) \\(X\\) /\\(Y\\) stabilize variance, linearize relationships, normalize residuals. Note transformations may always yield “optimal” results.","code":""},{"path":"linear-regression.html","id":"specific-remedies-for-assumption-violations","chapter":"5 Linear Regression","heading":"5.1.1.13.2 Specific Remedies for Assumption Violations","text":"","code":""},{"path":"linear-regression.html","id":"remedies-in-detail","chapter":"5 Linear Regression","heading":"5.1.1.13.3 Remedies in Detail","text":"Non-Linearity:\nTransformations: Apply transformations response variable \\(Y\\) predictor variable \\(X\\). Common transformations include:\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).\nPolynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.\n\nAlternative Models:\nPolynomial regression splines flexibility modeling non-linear relationships.\n\nTransformations: Apply transformations response variable \\(Y\\) predictor variable \\(X\\). Common transformations include:\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).\nPolynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).Polynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.Alternative Models:\nPolynomial regression splines flexibility modeling non-linear relationships.\nPolynomial regression splines flexibility modeling non-linear relationships.Non-Constant Error Variance:\nWeighted Least Squares:\nAssigns weights observations inversely proportional variance.\n\nTransformations:\nUse log square root transformation stabilize variance.\n\nWeighted Least Squares:\nAssigns weights observations inversely proportional variance.\nAssigns weights observations inversely proportional variance.Transformations:\nUse log square root transformation stabilize variance.\nUse log square root transformation stabilize variance.Correlated Errors:\ntime-series data:\nUse serially correlated error models AR(1) ARIMA.\nmodels explicitly account dependency residuals time.\n\ntime-series data:\nUse serially correlated error models AR(1) ARIMA.\nmodels explicitly account dependency residuals time.\nUse serially correlated error models AR(1) ARIMA.models explicitly account dependency residuals time.Non-Normality:\nTransformations:\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.\n\nNon-parametric regression:\nMethods like LOESS Theil-Sen regression require normality assumption.\n\nTransformations:\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.Non-parametric regression:\nMethods like LOESS Theil-Sen regression require normality assumption.\nMethods like LOESS Theil-Sen regression require normality assumption.Omitted Variables:\nIntroduce additional predictors:\nUse multiple regression include relevant independent variables.\n\nCheck multicollinearity adding new variables.\nIntroduce additional predictors:\nUse multiple regression include relevant independent variables.\nUse multiple regression include relevant independent variables.Check multicollinearity adding new variables.Outliers:\nRobust Regression:\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.\n\nDiagnostics:\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.\n\nRobust Regression:\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.Diagnostics:\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.","code":""},{"path":"linear-regression.html","id":"transformations-in-regression-analysis","chapter":"5 Linear Regression","heading":"5.1.1.14 Transformations in Regression Analysis","text":"Transformations involve modifying one variables address issues non-linearity, non-constant variance, non-normality. However, ’s important note properties least-squares estimates apply transformed model, original variables.transforming dependent variable \\(Y\\), fit model :\\[\ng(Y_i) = b_0 + b_1 X_i,\n\\]\\(g(Y_i)\\) transformed response. interpret regression results terms original \\(Y\\), need transform back:\\[\n\\hat{Y}_i = g^{-1}(b_0 + b_1 X_i).\n\\]Direct back-transformation predictions can introduce bias. example, log-transformed model:\\[\n\\log(Y_i) = b_0 + b_1 X_i,\n\\]unbiased back-transformed prediction \\(Y_i\\) :\\[\n\\hat{Y}_i = \\exp(b_0 + b_1 X_i + \\frac{\\sigma^2}{2}),\n\\]\\(\\frac{\\sigma^2}{2}\\) accounts bias correction due log transformation.","code":""},{"path":"linear-regression.html","id":"box-cox-family-of-transformations","chapter":"5 Linear Regression","heading":"5.1.1.14.1 Box-Cox Family of Transformations","text":"Box-Cox transformation versatile family transformations defined :\\[\nY' =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\text{} \\lambda \\neq 0, \\\\\n\\ln(Y), & \\text{} \\lambda = 0.\n\\end{cases}\n\\]transformation introduces parameter \\(\\lambda\\) estimated data. Common transformations include:Choosing Transformation Parameter \\(\\lambda\\)value \\(\\lambda\\) can selected using one following methods:Trial Error:\nApply different transformations compare residual plots model fit statistics (e.g., \\(R^2\\) AIC).\nApply different transformations compare residual plots model fit statistics (e.g., \\(R^2\\) AIC).Maximum Likelihood Estimation:\nChoose \\(\\lambda\\) maximize likelihood function assumption normally distributed errors.\nChoose \\(\\lambda\\) maximize likelihood function assumption normally distributed errors.Numerical Search:\nUse computational optimization techniques minimize residual sum squares (RSS) another goodness--fit criterion.\nUse computational optimization techniques minimize residual sum squares (RSS) another goodness--fit criterion.NotesBenefits Transformations:\nStabilize Variance: Helps address heteroscedasticity.\nLinearize Relationships: Useful non-linear data.\nNormalize Residuals: Addresses non-normality issues.\nBenefits Transformations:Stabilize Variance: Helps address heteroscedasticity.Stabilize Variance: Helps address heteroscedasticity.Linearize Relationships: Useful non-linear data.Linearize Relationships: Useful non-linear data.Normalize Residuals: Addresses non-normality issues.Normalize Residuals: Addresses non-normality issues.Caveats:\nInterpretability: Transformed variables may complicate interpretation.\n-Transformation: Excessive transformations can distort relationship variables.\nCaveats:Interpretability: Transformed variables may complicate interpretation.Interpretability: Transformed variables may complicate interpretation.-Transformation: Excessive transformations can distort relationship variables.-Transformation: Excessive transformations can distort relationship variables.Applicability:\nTransformations effective issues like non-linearity non-constant variance. less effective correcting independence violations omitted variables.\nApplicability:Transformations effective issues like non-linearity non-constant variance. less effective correcting independence violations omitted variables.","code":"\n# Install and load the necessary library\nif (!require(\"MASS\")) install.packages(\"MASS\")\nlibrary(MASS)\n\n# Fit a linear model\nset.seed(123)\nn <- 50\nx <- rnorm(n, mean = 5, sd = 2)\ny <- 3 + 2 * x + rnorm(n, mean = 0, sd = 2)\nmodel <- lm(y ~ x)\n\n# Apply Box-Cox Transformation\nboxcox_result <- boxcox(model, lambda = seq(-2, 2, 0.1), plotit = TRUE)\n\n# Find the optimal lambda\noptimal_lambda <- boxcox_result$x[which.max(boxcox_result$y)]\ncat(\"Optimal lambda for Box-Cox transformation:\", optimal_lambda, \"\\n\")\n#> Optimal lambda for Box-Cox transformation: 0.8686869"},{"path":"linear-regression.html","id":"variance-stabilizing-transformations","chapter":"5 Linear Regression","heading":"5.1.1.14.2 Variance Stabilizing Transformations","text":"Variance stabilizing transformations used standard deviation response variable depends mean. delta method, applies Taylor series expansion, provides systematic approach find transformations.Given standard deviation \\(Y\\) function mean:\\[\n\\sigma = \\sqrt{\\text{var}(Y)} = f(\\mu),\n\\]\\(\\mu = E(Y)\\) \\(f(\\mu)\\) smooth function mean, aim find transformation \\(h(Y)\\) variance transformed variable \\(h(Y)\\) constant values \\(\\mu\\).Expanding \\(h(Y)\\) Taylor Expansion series around \\(\\mu\\):\\[\nh(Y) = h(\\mu) + h'(\\mu)(Y - \\mu) + \\text{higher-order terms}.\n\\]Ignoring higher-order terms, variance \\(h(Y)\\) can approximated :\\[\n\\text{var}(h(Y)) = \\text{var}(h(\\mu) + h'(\\mu)(Y - \\mu)).\n\\]Since \\(h(\\mu)\\) constant:\\[\n\\text{var}(h(Y)) = \\left(h'(\\mu)\\right)^2 \\text{var}(Y).\n\\]Substituting \\(\\text{var}(Y) = \\left(f(\\mu)\\right)^2\\), get:\\[\n\\text{var}(h(Y)) = \\left(h'(\\mu)\\right)^2 \\left(f(\\mu)\\right)^2.\n\\]stabilize variance (make constant \\(\\mu\\)), require:\\[\n\\left(h'(\\mu)\\right)^2 \\left(f(\\mu)\\right)^2 = \\text{constant}.\n\\]Thus, derivative \\(h(\\mu)\\) must proportional inverse \\(f(\\mu)\\):\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)}.\n\\]Integrating sides gives:\\[\nh(\\mu) = \\int \\frac{1}{f(\\mu)} \\, d\\mu.\n\\]specific form \\(h(\\mu)\\) depends function \\(f(\\mu)\\), describes relationship standard deviation mean.Examples Variance Stabilizing TransformationsVariance stabilizing transformations particularly useful :Poisson-distributed data: Use \\(h(Y) = 2\\sqrt{Y}\\) stabilize variance.Exponential multiplicative models: Use \\(h(Y) = \\ln(Y)\\) stabilization.Power law relationships: Use transformations like \\(h(Y) = Y^{-1}\\) forms derived \\(f(\\mu)\\).Example: Variance Stabilizing Transformation Poisson DistributionFor Poisson distribution, variance \\(Y\\) equal mean:\\[\n\\sigma^2 = \\text{var}(Y) = E(Y) = \\mu.\n\\]Thus, standard deviation :\\[\n\\sigma = f(\\mu) = \\sqrt{\\mu}.\n\\]Using relationship variance stabilizing transformations:\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)} = \\mu^{-0.5}.\n\\]Integrating \\(h'(\\mu)\\) gives variance stabilizing transformation:\\[\nh(\\mu) = \\int \\mu^{-0.5} \\, d\\mu = 2\\sqrt{\\mu}.\n\\]Hence, variance stabilizing transformation :\\[\nh(Y) = \\sqrt{Y}.\n\\]transformation widely used Poisson regression stabilize variance response variable.","code":"\n# Simulate Poisson data\nset.seed(123)\nn <- 500\nx <- rnorm(n, mean = 5, sd = 2)\ny <- rpois(n, lambda = exp(1 + 0.3 * x))  # Poisson-distributed Y\n\n# Fit linear model without transformation\nmodel_raw <- lm(y ~ x)\n\n# Apply square root transformation\ny_trans <- sqrt(y)\nmodel_trans <- lm(y_trans ~ x)\n\n# Compare residual plots\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\n\n# Residual plot for raw data\nplot(\n    fitted(model_raw),\n    resid(model_raw),\n    main = \"Residuals Raw Data\",\n    xlab = \"Fitted Values\",\n    ylab = \"Residuals\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n# Residual plot for transformed data\nplot(\n    fitted(model_trans),\n    resid(model_trans),\n    main = \"Residuals: Transformed Data (sqrt(Y))\",\n    xlab = \"Fitted Values\",\n    ylab = \"Residuals\"\n)\nabline(h = 0, col = \"blue\", lty = 2)"},{"path":"linear-regression.html","id":"general-strategy-when-fmu-is-unknown","chapter":"5 Linear Regression","heading":"5.1.1.14.3 General Strategy When \\(f(\\mu)\\) Is Unknown","text":"relationship \\(\\text{var}(Y)\\) \\(\\mu\\) (.e., \\(f(\\mu)\\)) unknown, following steps can help:Trial Error:\nApply common transformations (e.g., \\(\\log(Y)\\), \\(\\sqrt{Y}\\)) examine residual plots.\nSelect transformation results stabilized variance (residuals show pattern plots).\nApply common transformations (e.g., \\(\\log(Y)\\), \\(\\sqrt{Y}\\)) examine residual plots.Select transformation results stabilized variance (residuals show pattern plots).Leverage Prior Research:\nConsult researchers literature similar experiments determine transformations typically used.\nConsult researchers literature similar experiments determine transformations typically used.Analyze Observations Predictor Value:\nmultiple observations \\(Y_{ij}\\) available \\(X\\) value:\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.\nCheck \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\n\nPerform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).\n\nmultiple observations \\(Y_{ij}\\) available \\(X\\) value:\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.\nCheck \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\n\nPerform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.Check \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]Taking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]Perform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).Group Observations:\nindividual observations sparse, try grouping similar observations \\(X\\) values compute \\(\\bar{Y}_i\\) \\(s_i\\) group.\nindividual observations sparse, try grouping similar observations \\(X\\) values compute \\(\\bar{Y}_i\\) \\(s_i\\) group.","code":""},{"path":"linear-regression.html","id":"common-transformations-and-their-applications","chapter":"5 Linear Regression","heading":"5.1.1.14.4 Common Transformations and Their Applications","text":"table summarizes common transformations used stabilize variance various conditions, along appropriate contexts comments:Choosing Transformation:\nStart identifying relationship variance residuals (\\(var(\\epsilon_i)\\)) mean response variable (\\(E(Y_i)\\)).\nSelect transformation matches identified variance structure.\nStart identifying relationship variance residuals (\\(var(\\epsilon_i)\\)) mean response variable (\\(E(Y_i)\\)).Select transformation matches identified variance structure.Transformations Zero Values:\ndata zeros, transformations like \\(\\sqrt{Y+1}\\) \\(\\log(Y+1)\\) can used avoid undefined values. seriously jeopardize model assumption (J. Chen Roth 2024).\ndata zeros, transformations like \\(\\sqrt{Y+1}\\) \\(\\log(Y+1)\\) can used avoid undefined values. seriously jeopardize model assumption (J. Chen Roth 2024).Use Regression Models:\nApply transformations dependent variable \\(Y\\) regression model.\nAlways check residual plots confirm transformation stabilizes variance resolves non-linearity.\nApply transformations dependent variable \\(Y\\) regression model.Always check residual plots confirm transformation stabilizes variance resolves non-linearity.Interpretation Transformation:\ntransforming \\(Y\\), interpret results terms transformed variable.\npractical interpretation, back-transform predictions account associated bias.\ntransforming \\(Y\\), interpret results terms transformed variable.practical interpretation, back-transform predictions account associated bias.","code":""},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"5 Linear Regression","heading":"5.1.2 Multiple Linear Regression","text":"geometry least squares regression involves projecting response vector \\(\\mathbf{y}\\) onto space spanned columns design matrix \\(\\mathbf{X}\\). fitted values \\(\\mathbf{\\hat{y}}\\) can expressed :\\[\n\\begin{aligned}\n\\mathbf{\\hat{y}} &= \\mathbf{Xb} \\\\\n&= \\mathbf{X(X'X)^{-1}X'y} \\\\\n&= \\mathbf{Hy},\n\\end{aligned}\n\\]:\\(\\mathbf{H} = \\mathbf{X(X'X)^{-1}X'}\\) projection operator (sometimes denoted \\(\\mathbf{P}\\)).\\(\\mathbf{\\hat{y}}\\) projection \\(\\mathbf{y}\\) onto linear space spanned columns \\(\\mathbf{X}\\) (model space).dimension model space equal rank \\(\\mathbf{X}\\) (.e., number linearly independent columns \\(\\mathbf{X}\\)).Properties Projection Matrix \\(\\mathbf{H}\\)Symmetry:\nprojection matrix \\(\\mathbf{H}\\) symmetric: \\[\n\\mathbf{H} = \\mathbf{H}'.\n\\]\nprojection matrix \\(\\mathbf{H}\\) symmetric: \\[\n\\mathbf{H} = \\mathbf{H}'.\n\\]Idempotence:\nApplying \\(\\mathbf{H}\\) twice gives result: \\[\n\\mathbf{HH} = \\mathbf{H}.\n\\] Proof: \\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'} \\\\\n&= \\mathbf{H}.\n\\end{aligned}\n\\]\nApplying \\(\\mathbf{H}\\) twice gives result: \\[\n\\mathbf{HH} = \\mathbf{H}.\n\\] Proof: \\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'} \\\\\n&= \\mathbf{H}.\n\\end{aligned}\n\\]Dimensionality:\n\\(\\mathbf{H}\\) \\(n \\times n\\) matrix (\\(n\\) number observations).\nrank \\(\\mathbf{H}\\) equal rank \\(\\mathbf{X}\\), typically number predictors (including intercept).\n\\(\\mathbf{H}\\) \\(n \\times n\\) matrix (\\(n\\) number observations).rank \\(\\mathbf{H}\\) equal rank \\(\\mathbf{X}\\), typically number predictors (including intercept).Orthogonal Complement:\nmatrix \\(\\mathbf{(- H)}\\), : \\[\n\\mathbf{- H} = \\mathbf{- X(X'X)^{-1}X'},\n\\] also projection operator.\nprojects onto orthogonal complement space spanned columns \\(\\mathbf{X}\\) (.e., space orthogonal model space).\nmatrix \\(\\mathbf{(- H)}\\), : \\[\n\\mathbf{- H} = \\mathbf{- X(X'X)^{-1}X'},\n\\] also projection operator.projects onto orthogonal complement space spanned columns \\(\\mathbf{X}\\) (.e., space orthogonal model space).Orthogonality Projections:\n\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) orthogonal: \\[\n\\mathbf{H(- H)} = \\mathbf{0}.\n\\]\nSimilarly: \\[\n\\mathbf{(- H)H} = \\mathbf{0}.\n\\]\n\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) orthogonal: \\[\n\\mathbf{H(- H)} = \\mathbf{0}.\n\\]Similarly: \\[\n\\mathbf{(- H)H} = \\mathbf{0}.\n\\]Intuition \\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\)\\(\\mathbf{H}\\): Projects \\(\\mathbf{y}\\) onto model space, giving fitted values \\(\\mathbf{\\hat{y}}\\).\\(\\mathbf{- H}\\): Projects \\(\\mathbf{y}\\) onto residual space, giving residuals \\(\\mathbf{e}\\): \\[\n\\mathbf{e} = \\mathbf{(- H)y}.\n\\]\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) divide response vector \\(\\mathbf{y}\\) two components: \\[\n\\mathbf{y} = \\mathbf{\\hat{y}} + \\mathbf{e}.\n\\]\n\\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\) (fitted values).\n\\(\\mathbf{e} = \\mathbf{(- H)y}\\) (residuals).\n\\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\) (fitted values).\\(\\mathbf{e} = \\mathbf{(- H)y}\\) (residuals).properties \\(\\mathbf{H}\\) (symmetry, idempotence, dimensionality) reflect role linear transformation projects vectors onto model space.geometric perspective provides insight mechanics least squares regression, particularly response variable \\(\\mathbf{y}\\) decomposed fitted values residuals.Similar simple regression, total sum squares multiple regression analysis can partitioned components corresponding regression (model fit) residuals (errors).uncorrected total sum squares :\\[\n\\mathbf{y'y} = \\mathbf{\\hat{y}'\\hat{y} + e'e},\n\\]:\\(\\mathbf{\\hat{y} = Hy}\\) (fitted values, projected onto model space).\\(\\mathbf{\\hat{y} = Hy}\\) (fitted values, projected onto model space).\\(\\mathbf{e = (- H)y}\\) (residuals, projected onto orthogonal complement model space).\\(\\mathbf{e = (- H)y}\\) (residuals, projected onto orthogonal complement model space).Expanding using projection matrices:\\[\n\\begin{aligned}\n\\mathbf{y'y} &= \\mathbf{(Hy)'(Hy) + ((-H)y)'((-H)y)} \\\\\n&= \\mathbf{y'H'Hy + y'(-H)'(-H)y} \\\\\n&= \\mathbf{y'Hy + y'(-H)y}.\n\\end{aligned}\n\\]equation shows partition \\(\\mathbf{y'y}\\) components explained model (\\(\\mathbf{\\hat{y}}\\)) unexplained variation (residuals).corrected total sum squares, adjust mean (using projection matrix \\(\\mathbf{H_1}\\)):\\[\n\\mathbf{y'(-H_1)y = y'(H-H_1)y + y'(-H)y}.\n\\]:\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\), \\(\\mathbf{J}\\) \\(n \\times n\\) matrix ones.\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\), \\(\\mathbf{J}\\) \\(n \\times n\\) matrix ones.\\(\\mathbf{H - H_1}\\) projects onto subspace explained predictors centering.\\(\\mathbf{H - H_1}\\) projects onto subspace explained predictors centering.\\(\\mathbf{H}\\): Projects onto model space.\\(\\mathbf{-H}\\): Projects onto residuals.\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\): Adjusts mean.\\(\\mathbf{H-H_1}\\): Projects onto predictors centering.\\(\\mathbf{-H}\\): Projects onto residuals.Correction MattersIn ANOVA regression, removing contribution mean helps isolate variability explained predictors overall level response variable.ANOVA regression, removing contribution mean helps isolate variability explained predictors overall level response variable.Corrected sums squares common comparing models computing \\(R^2\\), requires centering ensure consistency proportionate variance explained.Corrected sums squares common comparing models computing \\(R^2\\), requires centering ensure consistency proportionate variance explained.corrected total sum squares can decomposed sum squares regression (SSR) sum squares error (SSE)::\\(p\\): Number parameters (including intercept).\\(p\\): Number parameters (including intercept).\\(n\\): Number observations.\\(n\\): Number observations.Alternatively, regression model can expressed :\\[\n\\mathbf{Y = X\\hat{\\beta} + (Y - X\\hat{\\beta})},\n\\]:\\(\\mathbf{\\hat{Y} = X\\hat{\\beta}}\\): Vector fitted values (subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{\\hat{Y} = X\\hat{\\beta}}\\): Vector fitted values (subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{e = Y - X\\hat{\\beta}}\\): Vector residuals (orthogonal complement subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{e = Y - X\\hat{\\beta}}\\): Vector residuals (orthogonal complement subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector \\(n\\)-dimensional space \\(\\mathbb{R}^n\\).\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector \\(n\\)-dimensional space \\(\\mathbb{R}^n\\).\\(\\mathbf{X}\\) \\(n \\times p\\) full-rank matrix, columns generating \\(p\\)-dimensional subspace \\(\\mathbb{R}^n\\). Hence, estimator \\(\\mathbf{X\\hat{\\beta}}\\) also subspace.\\(\\mathbf{X}\\) \\(n \\times p\\) full-rank matrix, columns generating \\(p\\)-dimensional subspace \\(\\mathbb{R}^n\\). Hence, estimator \\(\\mathbf{X\\hat{\\beta}}\\) also subspace.linear regression, Ordinary Least Squares estimator \\(\\hat{\\beta}\\) minimizes squared Euclidean distance \\(\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\\) observed response vector \\(\\mathbf{Y}\\) fitted values \\(\\mathbf{X}\\beta\\). minimization corresponds orthogonal projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).solve optimization problem:\\[\n\\min_{\\beta} \\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2.\n\\]objective function can expanded :\\[\n\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\n= (\\mathbf{Y} - \\mathbf{X}\\beta)^\\top (\\mathbf{Y} - \\mathbf{X}\\beta).\n\\]Perform multiplication:\\[\n\\begin{aligned}\n(\\mathbf{Y} - \\mathbf{X}\\beta)^\\top (\\mathbf{Y} - \\mathbf{X}\\beta)\n&= \\mathbf{Y}^\\top \\mathbf{Y}\n- \\mathbf{Y}^\\top \\mathbf{X}\\beta\n- \\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n+ \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta.\n\\end{aligned}\n\\]Since \\(\\mathbf{Y}^\\top \\mathbf{X}\\beta\\) scalar, equals \\(\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\\). Therefore, expanded expression becomes:\\[\n\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\n= \\mathbf{Y}^\\top \\mathbf{Y}\n- 2\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n+ \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta.\n\\]find \\(\\beta\\) minimizes expression, take derivative respect \\(\\beta\\) set 0:\\[\n\\frac{\\partial}{\\partial \\beta}\n\\left[\n  \\mathbf{Y}^\\top \\mathbf{Y}\n  - 2\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n  + \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta\n\\right]\n= 0\n\\]Computing gradient:\\[\n\\frac{\\partial}{\\partial \\beta} = -2\\mathbf{X}^\\top \\mathbf{Y} + 2(\\mathbf{X}^\\top \\mathbf{X})\\beta.\n\\]Setting zero:\\[\n-2\\mathbf{X}^\\top \\mathbf{Y} + 2\\mathbf{X}^\\top \\mathbf{X}\\beta = 0.\n\\]Simplify:\\[\n\\mathbf{X}^\\top \\mathbf{X}\\beta = \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\\(\\mathbf{X}^\\top \\mathbf{X}\\) invertible, solution :\\[\n\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}.\n\\]Orthogonal Projection InterpretationThe fitted values :\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\beta}.\n\\]normal equations, \\(\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}) = 0\\), implies residual vector \\(\\mathbf{Y} - \\hat{\\mathbf{Y}}\\) orthogonal every column \\(\\mathbf{X}\\). Therefore:\\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\beta}\\) orthogonal projection \\(\\mathbf{Y}\\) onto \\(\\mathrm{Col}(\\mathbf{X})\\).\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) lies orthogonal complement \\(\\mathrm{Col}(\\mathbf{X})\\).Pythagoras DecompositionThe geometric interpretation gives us decomposition:\\[\n\\mathbf{Y} = \\mathbf{X}\\hat{\\beta} + (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}),\n\\]:\\(\\mathbf{X}\\hat{\\beta}\\) projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).\\(\\mathbf{X}\\hat{\\beta}\\) projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).\\((\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\) residual vector, orthogonal \\(\\mathbf{X}\\hat{\\beta}\\).\\((\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\) residual vector, orthogonal \\(\\mathbf{X}\\hat{\\beta}\\).Since two components orthogonal, squared norms satisfy:\\[\n\\begin{aligned}\\|\\mathbf{Y}\\|^2 &= \\mathbf{Y}^\\top \\mathbf{Y}&& \\text{(definition norm squared)} \\\\[6pt]&= (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta} + \\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta} + \\mathbf{X}\\hat{\\beta})&& \\text{(add subtract term } \\mathbf{X}\\hat{\\beta}\\text{)} \\\\[6pt]&= (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; 2\\,(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; (\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{X}\\hat{\\beta})&& \\text{(expand }(+b)^\\top(+b)\\text{)} \\\\[6pt]&= \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2\\;+\\; 2\\,(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; \\|\\mathbf{X}\\hat{\\beta}\\|^2&& \\text{(rewrite quadratic form norm)} \\\\[6pt]&= \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\|\\mathbf{X}\\hat{\\beta}\\|^2&& \\text{(use }(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y}-\\mathbf{X}\\hat{\\beta}) = 0\\text{, .e. orthogonality)} \\\\[6pt]& \\quad = \\|\\mathbf{X}\\hat{\\beta}\\|^2 \\;+\\; \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2.\n\\end{aligned}\n\\]norm vector \\(\\mathbf{}\\) \\(\\mathbb{R}^p\\) defined :\\[\n\\|\\mathbf{}\\| = \\sqrt{\\mathbf{}^\\top \\mathbf{}} = \\sqrt{\\sum_{=1}^p a_i^2}.\n\\]saying \\(\\mathbf{Y}\\) decomposed two orthogonal components:\\(\\mathbf{X}\\hat{\\beta}\\) (projection onto \\(\\mathrm{Col}(\\mathbf{X})\\)\n\\(\\|\\mathbf{X}\\hat{\\beta}\\|\\) measures part \\(\\mathbf{Y}\\) explained model.\n\\(\\mathbf{X}\\hat{\\beta}\\) (projection onto \\(\\mathrm{Col}(\\mathbf{X})\\)\\(\\|\\mathbf{X}\\hat{\\beta}\\|\\) measures part \\(\\mathbf{Y}\\) explained model.\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) (residual lying orthogonal complement).\n\\(\\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|\\) measures residual error.\n\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) (residual lying orthogonal complement).\\(\\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|\\) measures residual error.geometric interpretation (projection plus orthogonal remainder) exactly call \\(\\mathbf{X}\\hat{\\beta}\\) orthogonal projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\). decomposition also underlies analysis variance (ANOVA) regression.coefficient multiple determination, denoted \\(R^2\\), measures proportion total variation response variable (\\(\\mathbf{Y}\\)) explained regression model. defined :\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO},\n\\]:\\(SSR\\): Regression sum squares (variation explained model).\\(SSR\\): Regression sum squares (variation explained model).\\(SSE\\): Error sum squares (unexplained variation).\\(SSE\\): Error sum squares (unexplained variation).\\(SSTO\\): Total sum squares (total variation \\(\\mathbf{Y}\\)).\\(SSTO\\): Total sum squares (total variation \\(\\mathbf{Y}\\)).adjusted \\(R^2\\) adjusts \\(R^2\\) number predictors model, penalizing adding predictors improve model’s fit substantially. defined :\\[\nR^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)SSE}{(n-p)SSTO},\n\\]:\\(n\\): Number observations.\\(n\\): Number observations.\\(p\\): Number parameters (including intercept).\\(p\\): Number parameters (including intercept).Key Differences \\(R^2\\) \\(R^2_a\\)multiple regression, \\(R^2_a\\) provides reliable measure model fit, especially comparing models different numbers predictors.regression model coefficients \\(\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_{p-1})^\\top\\), sums squares used evaluate contribution predictors explaining variation response variable.Model Sums Squares:\n\\(SSM\\): Total model sum squares, capturing variation explained predictors: \\[\nSSM = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1}).\n\\]\n\\(SSM\\): Total model sum squares, capturing variation explained predictors: \\[\nSSM = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1}).\n\\]Marginal Contribution:\n\\(SSM_m\\): Conditional model sum squares, capturing variation explained predictors accounting others: \\[\nSSM_m = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1} | \\beta_0).\n\\]\n\\(SSM_m\\): Conditional model sum squares, capturing variation explained predictors accounting others: \\[\nSSM_m = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1} | \\beta_0).\n\\]Decompositions \\(SSM_m\\)Sequential Sums Squares (Type SS)Definition:\nSequential SS depends order predictors added model.\nrepresents additional contribution predictor given predictors precede sequence.\nDefinition:Sequential SS depends order predictors added model.represents additional contribution predictor given predictors precede sequence.Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\dots, \\beta_{p-2}).\n\\]Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\dots, \\beta_{p-2}).\n\\]Key Points:\nSequential SS unique; depends order predictors.\nDefault many statistical software functions (e.g., anova() R).\nKey Points:Sequential SS unique; depends order predictors.Default many statistical software functions (e.g., anova() R).Marginal Sums Squares (Type II SS)Definition:\nMarginal SS evaluates contribution predictor accounting predictors except collinear.\nignores hierarchical relationships interactions, focusing independent contributions.\nMarginal SS evaluates contribution predictor accounting predictors except collinear.ignores hierarchical relationships interactions, focusing independent contributions.Formula: \\(SSM_m = SS(\\beta_j | \\beta_1, \\dots, \\beta_{j-1}, \\beta_{j + 1}, \\dots, \\beta_{p-1})\\) Type II SS evaluates contribution \\(\\beta_j\\) excluding terms collinear \\(\\beta_j\\).Key Points:\nType II SS independent predictor order.\nSuitable models without interaction terms predictors balanced.\nType II SS independent predictor order.Suitable models without interaction terms predictors balanced.Partial Sums Squares (Type III SS)Definition:\nPartial SS evaluates contribution predictor accounting predictors model.\nquantifies unique contribution predictor, controlling presence others.\nDefinition:Partial SS evaluates contribution predictor accounting predictors model.quantifies unique contribution predictor, controlling presence others.Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0, \\beta_2, \\dots, \\beta_{p-1}) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\beta_1, \\dots, \\beta_{p-2}).\n\\]Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0, \\beta_2, \\dots, \\beta_{p-1}) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\beta_1, \\dots, \\beta_{p-2}).\n\\]Key Points:\nPartial SS unique given model.\ncommonly used practice assessing individual predictor importance.\nKey Points:Partial SS unique given model.commonly used practice assessing individual predictor importance.Comparison Sequential, Marginal, Partial SSPractical NotesUse Type III SS (Partial SS) :\nfocus individual predictor contributions accounting others.\nConducting hypothesis tests predictors complex models interactions hierarchical structures.\nfocus individual predictor contributions accounting others.Conducting hypothesis tests predictors complex models interactions hierarchical structures.Use Type II SS (Marginal SS) :\nWorking balanced datasets models without interaction terms.\nIgnoring interactions focusing independent effects.\nWorking balanced datasets models without interaction terms.Ignoring interactions focusing independent effects.Use Type SS (Sequential SS) :\nInterested understanding incremental contribution predictors based specific order entry (e.g., stepwise regression).\nInterested understanding incremental contribution predictors based specific order entry (e.g., stepwise regression).","code":""},{"path":"linear-regression.html","id":"ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.1 OLS Assumptions","text":"A1 LinearityA2 Full RankA3 Exogeneity Independent VariablesA4 HomoskedasticityA5 Data Generation (Random Sampling)A6 Normal Distribution","code":""},{"path":"linear-regression.html","id":"a1-linearity","chapter":"5 Linear Regression","heading":"5.1.2.1.1 A1 Linearity","text":"linear regression model expressed :\\[\nA1: y = \\mathbf{x}\\beta + \\epsilon\n\\]assumption restrictive since \\(x\\) can include nonlinear transformations (e.g., interactions, natural logarithms, quadratic terms).However, combined A3 (Exogeneity Independent Variables), linearity can become restrictive.","code":""},{},{},{},{"path":"linear-regression.html","id":"a2-full-rank","chapter":"5 Linear Regression","heading":"5.1.2.1.2 A2 Full Rank","text":"full rank assumption ensures uniqueness existence parameter estimates population regression equation. expressed :\\[\nA2: \\text{rank}(E(\\mathbf{x'x})) = k\n\\]assumption also known identification condition.Key PointsNo Perfect Multicollinearity:\ncolumns \\(\\mathbf{x}\\) (matrix predictors) must linearly independent.\ncolumn \\(\\mathbf{x}\\) can written linear combination columns.\ncolumns \\(\\mathbf{x}\\) (matrix predictors) must linearly independent.column \\(\\mathbf{x}\\) can written linear combination columns.Implications:\nEnsures parameter regression equation identifiable unique.\nPrevents computational issues, inability invert \\(\\mathbf{x'x}\\), required estimating \\(\\hat{\\beta}\\).\nEnsures parameter regression equation identifiable unique.Prevents computational issues, inability invert \\(\\mathbf{x'x}\\), required estimating \\(\\hat{\\beta}\\).Example ViolationIf two predictors, \\(x_1\\) \\(x_2\\), perfectly correlated (e.g., \\(x_2 = 2x_1\\)), rank \\(\\mathbf{x}\\) reduced, \\(\\mathbf{x'x}\\) becomes singular. cases:regression coefficients uniquely estimated.regression coefficients uniquely estimated.model fails satisfy full rank assumption.model fails satisfy full rank assumption.","code":""},{"path":"linear-regression.html","id":"a3-exogeneity-of-independent-variables","chapter":"5 Linear Regression","heading":"5.1.2.1.3 A3 Exogeneity of Independent Variables","text":"exogeneity assumption ensures independent variables (\\(\\mathbf{x}\\)) systematically related error term (\\(\\epsilon\\)). expressed :\\[\nA3: E[\\epsilon | x_1, x_2, \\dots, x_k] = E[\\epsilon | \\mathbf{x}] = 0\n\\]assumption often referred strict exogeneity mean independence (see Correlation Independence.Key PointsStrict Exogeneity:\nIndependent variables carry information error term \\(\\epsilon\\).\n[Law Iterated Expectations], \\(E(\\epsilon) = 0\\), can satisfied always including intercept regression model.\nIndependent variables carry information error term \\(\\epsilon\\).[Law Iterated Expectations], \\(E(\\epsilon) = 0\\), can satisfied always including intercept regression model.Implication:\nA3 implies: \\[\nE(y | \\mathbf{x}) = \\mathbf{x}\\beta,\n\\] meaning conditional mean function linear function \\(\\mathbf{x}\\). aligns A1 Linearity.\nA3 implies: \\[\nE(y | \\mathbf{x}) = \\mathbf{x}\\beta,\n\\] meaning conditional mean function linear function \\(\\mathbf{x}\\). aligns A1 Linearity.Relationship Independence:\nAlso referred mean independence, weaker condition full independence (see Correlation Independence).\nAlso referred mean independence, weaker condition full independence (see Correlation Independence).","code":""},{},{"path":"linear-regression.html","id":"a4-homoskedasticity","chapter":"5 Linear Regression","heading":"5.1.2.1.4 A4 Homoskedasticity","text":"homoskedasticity assumption ensures variance error term (\\(\\epsilon\\)) constant across levels independent variables (\\(\\mathbf{x}\\)). expressed :\\[\nA4: \\text{Var}(\\epsilon | \\mathbf{x}) = \\text{Var}(\\epsilon) = \\sigma^2\n\\]Key PointsDefinition:\nvariance disturbance term \\(\\epsilon\\) observations, regardless values predictors \\(\\mathbf{x}\\).\nvariance disturbance term \\(\\epsilon\\) observations, regardless values predictors \\(\\mathbf{x}\\).Practical Implication:\nHomoskedasticity ensures errors systematically vary predictors.\ncritical valid inference, standard errors coefficients rely assumption.\nHomoskedasticity ensures errors systematically vary predictors.critical valid inference, standard errors coefficients rely assumption.Violation (Heteroskedasticity):\nvariance \\(\\epsilon\\) depends \\(\\mathbf{x}\\), assumption violated.\nCommon signs include funnel-shaped patterns residual plots varying error sizes.\nvariance \\(\\epsilon\\) depends \\(\\mathbf{x}\\), assumption violated.Common signs include funnel-shaped patterns residual plots varying error sizes.","code":""},{"path":"linear-regression.html","id":"a5-data-generation-random-sampling","chapter":"5 Linear Regression","heading":"5.1.2.1.5 A5 Data Generation (Random Sampling)","text":"random sampling assumption ensures observations \\((y_i, x_{i1}, \\dots, x_{ik-1})\\) drawn independently identically distributed (iid) joint distribution \\((y, \\mathbf{x})\\). expressed :\\[\nA5: \\{y_i, x_{i1}, \\dots, x_{ik-1} : = 1, \\dots, n\\}\n\\]Key PointsRandom Sampling:\ndataset assumed random sample population.\nobservation independent others follows probability distribution.\ndataset assumed random sample population.observation independent others follows probability distribution.Implications:\nA3 (Exogeneity Independent Variables) A4 (Homoskedasticity), random sampling implies:\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).\nNon-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.\nVariance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]\n\nA3 (Exogeneity Independent Variables) A4 (Homoskedasticity), random sampling implies:\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).\nNon-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.\nVariance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).Non-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.Variance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]A5 May Hold:\ntime series data, observations often autocorrelated.\nspatial data, neighboring observations may independent.\ntime series data, observations often autocorrelated.spatial data, neighboring observations may independent.Practical ConsiderationsTime Series Data:\nUse methods autoregressive models generalized least squares (GLS) address dependency observations.\nUse methods autoregressive models generalized least squares (GLS) address dependency observations.Spatial Data:\nSpatial econometric models may required handle correlation across geographic locations.\nSpatial econometric models may required handle correlation across geographic locations.Checking Random Sampling:\ntrue randomness always verified, exploratory analysis residuals (e.g., patterns autocorrelation) can help detect violations.\ntrue randomness always verified, exploratory analysis residuals (e.g., patterns autocorrelation) can help detect violations.","code":""},{},{},{"path":"linear-regression.html","id":"a6-normal-distribution","chapter":"5 Linear Regression","heading":"5.1.2.1.6 A6 Normal Distribution","text":"A6: \\(\\epsilon|\\mathbf{x} \\sim N(0, \\sigma^2 I_n)\\)assumption implies error term \\(\\epsilon\\) normally distributed mean zero variance \\(\\sigma^2 I_n\\). assumption fundamental statistical inference linear regression models.Using assumptions A1 Linearity, A2 Full Rank, A3 Exogeneity Independent Variables, derive identification (orthogonality condition) population parameter \\(\\beta\\):\\[\n\\begin{aligned}\ny &= x\\beta + \\epsilon && \\text{(A1: Model Specification)} \\\\\nx'y &= x'x\\beta + x'\\epsilon && \\text{(Multiply sides $x'$)} \\\\\nE(x'y) &= E(x'x)\\beta + E(x'\\epsilon) && \\text{(Taking expectation)} \\\\\nE(x'y) &= E(x'x)\\beta && \\text{(A3: Exogeneity, $E(x'\\epsilon) = 0$)} \\\\\n[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\\beta && \\text{(Invertibility $E(x'x)$, A2)} \\\\\n[E(x'x)]^{-1}E(x'y) &= \\beta && \\text{(Simplified solution $\\beta$)}\n\\end{aligned}\n\\]Thus, \\(\\beta\\) identified vector parameters minimizes expected squared error.find \\(\\beta\\), minimize expected value squared error:\\[\n\\underset{\\gamma}{\\operatorname{argmin}} \\ E\\big((y - x\\gamma)^2\\big)\n\\]first-order condition derived taking derivative objective function respect \\(\\gamma\\) setting zero:\\[\n\\begin{aligned}\n\\frac{\\partial E\\big((y - x\\gamma)^2\\big)}{\\partial \\gamma} &= 0 \\\\\n-2E(x'(y - x\\gamma)) &= 0 \\\\\nE(x'y) - E(x'x\\gamma) &= 0 \\\\\nE(x'y) &= E(x'x)\\gamma \\\\\n(E(x'x))^{-1}E(x'y) &= \\gamma\n\\end{aligned}\n\\]confirms \\(\\gamma = \\beta\\).second-order condition ensures solution minimizes objective function. Taking second derivative:\\[\n\\frac{\\partial^2 E\\big((y - x\\gamma)^2\\big)}{\\partial \\gamma'^2} = 0 = 2E(x'x)\n\\]assumption A3 Exogeneity Independent Variables holds, \\(E(x'x)\\) positive semi-definite (PSD). Thus, \\(2E(x'x)\\) also PSD, ensuring minimum.","code":""},{"path":"linear-regression.html","id":"hierarchy-of-ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.1.7 Hierarchy of OLS Assumptions","text":"table summarizes hierarchical nature assumptions required derive different properties OLS estimator.Usage AssumptionsA2 Full RankVariation \\(\\mathbf{X}\\)A5 Data Generation (Random Sampling)Random SamplingA1 LinearityLinearity ParametersA3 Exogeneity Independent VariablesZero Conditional MeanA4 Homoskedasticity\\(\\mathbf{H}\\) homoskedasticityA6 Normal DistributionNormality ErrorsIdentification Data Description: Ensures model identifiable coefficients can estimated.Unbiasedness Consistency: Guarantees OLS estimates unbiased converge true parameter values sample size increases.Gauss-Markov (BLUE) Asymptotic Inference: Requires additional assumptions (e.g., homoskedasticity) ensure minimum variance estimators valid inference using large-sample tests (z chi-squared).Classical LM (BUE) Small-sample Inference: Builds previous assumptions adds normality errors valid t F tests finite samples.","code":""},{"path":"linear-regression.html","id":"theorems","chapter":"5 Linear Regression","heading":"5.1.2.2 Theorems","text":"","code":""},{"path":"linear-regression.html","id":"frischwaughlovell-theorem-frischwaughlovell-theorem","chapter":"5 Linear Regression","heading":"5.1.2.2.1 Frisch–Waugh–Lovell Theorem {#Frisch–Waugh–Lovell Theorem}","text":"Frisch–Waugh–Lovell (FWL) Theorem fundamental result linear regression allows deeper understanding coefficients computed multiple regression setting (Lovell 2008). Informally, states:estimating effect subset variables (\\(X_1\\)) \\(y\\) presence variables (\\(X_2\\)), can “partial ” influence \\(X_2\\) \\(y\\) \\(X_1\\). , regressing residuals \\(y\\) residuals \\(X_1\\) produces coefficients \\(X_1\\) identical obtained full multiple regression.Consider multiple linear regression model:\\[ \\mathbf{y = X\\beta + \\epsilon = X_1\\beta_1 + X_2\\beta_2 + \\epsilon} \\]:\\(y\\) \\(n \\times 1\\) vector dependent variable.\\(X_1\\) \\(n \\times k_1\\) matrix regressors interest.\\(X_2\\) \\(n \\times k_2\\) matrix additional regressors.\\(\\beta_1\\) \\(\\beta_2\\) coefficient vectors sizes \\(k_1 \\times 1\\) \\(k_2 \\times 1\\), respectively.\\(\\epsilon\\) \\(n \\times 1\\) error term vector.can equivalently represented partitioned matrix form :\\[ \\left( \\begin{array}{cc} X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{array} \\right) \\left( \\begin{array}{c} \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\end{array} \\right) = \\left( \\begin{array}{c} X_1'y \\\\ X_2'y \\end{array} \\right) \\]OLS estimator vector \\(\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\\) :\\[\n\\begin{pmatrix}\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nX_1'X_1 & X_1'X_2 \\\\\nX_2'X_1 & X_2'X_2\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nX_1'y \\\\\nX_2'y\n\\end{pmatrix}.\n\\]want coefficients \\(X_1\\), known result partitioned-inversion gives:\\[\n\\hat{\\beta}_1\n=\n\\left(X_1' M_2\\, X_1\\right)^{-1}\n\\,X_1' M_2\\, y,\n\\]\\[\nM_2\n=\n\n-\nX_2 \\left(X_2'X_2\\right)^{-1} X_2'.\n\\]matrix \\(M_2\\) often called residual-maker annihilator matrix \\(X_2\\). \\(n \\times n\\) symmetric, idempotent projection matrix projects vector \\(\\mathbb{R}^n\\) onto orthogonal complement column space \\(X_2\\). \\(M_2\\) satisfies \\(M_2^2 = M_2\\), \\(M_2 = M_2'\\).Intuitively, \\(M_2\\) captures part \\(y\\) (vector) orthogonal columns \\(X_2\\). “partialling ” \\(X_2\\) \\(y\\) \\(X_1\\) lets us isolate \\(\\hat{\\beta}_1\\).Equivalently, can also represent \\(\\hat{\\beta_1}\\) :\\[ \\mathbf{\\hat{\\beta_1} = (X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\\hat{\\beta_2}} \\]equation, can see thatBetas Multiple vs. Simple Regressions:\ncoefficients (\\(\\beta\\)) multiple regression generally coefficients separate individual simple regressions.\ncoefficients (\\(\\beta\\)) multiple regression generally coefficients separate individual simple regressions.Impact Additional Variables (\\(X_2\\)):\ninclusion different sets explanatory variables (\\(X_2\\)) affects coefficient estimates, even \\(X_1\\).\ninclusion different sets explanatory variables (\\(X_2\\)) affects coefficient estimates, even \\(X_1\\).Special Cases:\n\\(X_1'X_2 = 0\\) (orthogonality \\(X_1\\) \\(X_2\\)) \\(\\hat{\\beta_2} = 0\\), points (1 2) hold. cases, interaction coefficients \\(X_1\\) \\(X_2\\), making coefficients \\(X_1\\) unaffected \\(X_2\\).\n\\(X_1'X_2 = 0\\) (orthogonality \\(X_1\\) \\(X_2\\)) \\(\\hat{\\beta_2} = 0\\), points (1 2) hold. cases, interaction coefficients \\(X_1\\) \\(X_2\\), making coefficients \\(X_1\\) unaffected \\(X_2\\).Steps FWL:Partial \\(X_2\\) \\(y\\): Regress \\(y\\) \\(X_2\\) obtain residuals:\n\\[\n\\tilde{y} = M_2y.\n\\]Partial \\(X_2\\) \\(y\\): Regress \\(y\\) \\(X_2\\) obtain residuals:\\[\n\\tilde{y} = M_2y.\n\\]Partial \\(X_2\\) \\(X_1\\): column \\(X_1\\), regress \\(X_2\\) obtain residuals:\n\\[\n\\tilde{X}_1 = M_2X_1.\n\\]Partial \\(X_2\\) \\(X_1\\): column \\(X_1\\), regress \\(X_2\\) obtain residuals:\\[\n\\tilde{X}_1 = M_2X_1.\n\\]Regression Residuals: Regress \\(\\tilde{y}\\) \\(\\tilde{X}_1\\):\n\\[\n\\tilde{y} = \\tilde{X}_1\\beta_1 + \\text{error}.\n\\]Regression Residuals: Regress \\(\\tilde{y}\\) \\(\\tilde{X}_1\\):\\[\n\\tilde{y} = \\tilde{X}_1\\beta_1 + \\text{error}.\n\\]coefficients \\(\\beta_1\\) obtained identical full model regression:\\[\ny = X_1\\beta_1 + X_2\\beta_2 + \\epsilon.\n\\]MattersInterpretation Partial Effects: FWL Theorem provides way interpret \\(\\beta_1\\) effect \\(X_1\\) \\(y\\) removing linear dependence \\(X_2\\).Interpretation Partial Effects: FWL Theorem provides way interpret \\(\\beta_1\\) effect \\(X_1\\) \\(y\\) removing linear dependence \\(X_2\\).Computational Simplicity: allows decomposition large regression problem smaller, computationally simpler pieces.Computational Simplicity: allows decomposition large regression problem smaller, computationally simpler pieces.","code":"\n# Simulate data\nset.seed(123)\nn <- 100\nX1 <- matrix(rnorm(n * 2), n, 2)  # Two regressors of interest\nX2 <- matrix(rnorm(n * 2), n, 2)  # Two additional regressors\nbeta1 <- c(2,-1)  # Coefficients for X1\nbeta2 <- c(1, 0.5)  # Coefficients for X2undefined\nu <- rnorm(n)  # Error term\ny <- X1 %*% beta1 + X2 %*% beta2 + u  # Generate dependent variable\n\n# Full regression\nfull_model <- lm(y ~ X1 + X2)\nsummary(full_model)\n#> \n#> Call:\n#> lm(formula = y ~ X1 + X2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.47336 -0.58010  0.07461  0.68778  2.46552 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.11614    0.10000   1.161    0.248    \n#> X11          1.77575    0.10899  16.293  < 2e-16 ***\n#> X12         -1.14151    0.10204 -11.187  < 2e-16 ***\n#> X21          0.94954    0.10468   9.071 1.60e-14 ***\n#> X22          0.47667    0.09506   5.014 2.47e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9794 on 95 degrees of freedom\n#> Multiple R-squared:  0.8297, Adjusted R-squared:  0.8225 \n#> F-statistic: 115.7 on 4 and 95 DF,  p-value: < 2.2e-16\n\n# Step 1: Partial out X2 from y\ny_residual <- residuals(lm(y ~ X2))\n\n# Step 2: Partial out X2 from X1\nX1_residual <- residuals(lm(X1 ~ X2))\n\n# Step 3: Regress residuals\nfwl_model <- lm(y_residual ~ X1_residual - 1)\nsummary(fwl_model)\n#> \n#> Call:\n#> lm(formula = y_residual ~ X1_residual - 1)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.47336 -0.58010  0.07461  0.68778  2.46552 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> X1_residual1   1.7758     0.1073   16.55   <2e-16 ***\n#> X1_residual2  -1.1415     0.1005  -11.36   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9643 on 98 degrees of freedom\n#> Multiple R-squared:  0.8109, Adjusted R-squared:  0.807 \n#> F-statistic: 210.1 on 2 and 98 DF,  p-value: < 2.2e-16\n\n# Comparison of coefficients\ncat(\"Full model coefficients (X1):\", coef(full_model)[2:3], \"\\n\")\n#> Full model coefficients (X1): 1.775754 -1.141514\ncat(\"FWL model coefficients:\", coef(fwl_model), \"\\n\")\n#> FWL model coefficients: 1.775754 -1.141514"},{"path":"linear-regression.html","id":"gauss-markov-theorem","chapter":"5 Linear Regression","heading":"5.1.2.2.2 Gauss-Markov Theorem","text":"linear regression model:\\[\n\\mathbf{y = X\\beta + \\epsilon},\n\\]assumptions:A1: Linearity model.A2: Full rank \\(\\mathbf{X}\\).A3: Exogeneity \\(\\mathbf{X}\\).A4: Homoskedasticity \\(\\epsilon\\).Ordinary Least Squares estimator:\\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y},\n\\]Best Linear Unbiased Estimator (BLUE). means \\(\\hat{\\beta}\\) minimum variance among linear unbiased estimators \\(\\beta\\).1. UnbiasednessSuppose consider linear estimator \\(\\beta\\) form:\\[\n\\tilde{\\beta} = \\mathbf{C\\,y},\n\\]\\(\\mathbf{y}\\) \\(n \\times 1\\) vector observations,\\(\\mathbf{C}\\) \\(k \\times n\\) matrix (\\(k\\) dimension \\(\\beta\\)) depends design matrix \\(\\mathbf{X}\\).regression model \\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\mathbf{\\epsilon},\n\\quad\nE[\\mathbf{\\epsilon} \\mid \\mathbf{X}] = \\mathbf{0},\n\\quad\n\\mathrm{Var}(\\mathbf{\\epsilon} \\mid \\mathbf{X}) = \\sigma^2 \\mathbf{}.\n\\]say \\(\\tilde{\\beta}\\) unbiased conditional expectation (given \\(\\mathbf{X}\\)) equals true parameter \\(\\beta\\):\\[\nE(\\tilde{\\beta} \\mid \\mathbf{X})\n=\nE(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\n\\beta.\n\\]Substitute \\(\\mathbf{y} = \\mathbf{X}\\beta + \\mathbf{\\epsilon}\\):\\[\nE(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\nE\\left(\\mathbf{C}(\\mathbf{X}\\beta + \\mathbf{\\epsilon}) \\mid \\mathbf{X}\\right)\n=\n\\mathbf{C\\,X}\\,\\beta\n+\n\\mathbf{C}\\,E(\\mathbf{\\epsilon} \\mid \\mathbf{X})\n=\n\\mathbf{C\\,X}\\,\\beta.\n\\]hold \\(\\beta\\), require\\[\n\\mathbf{C\\,X} = \\mathbf{}.\n\\]words, \\(\\mathbf{C}\\) must “right-inverse” \\(\\mathbf{X}\\).hand, OLS estimator \\(\\hat{\\beta}\\) given \\[\n\\hat{\\beta}\n=\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{y}.\n\\]can verify:Let \\(\\mathbf{C}_{\\text{OLS}} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\).\\[\n\\mathbf{C}_{\\text{OLS}}\\,\\mathbf{X}\n=\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X}\n=\n\\mathbf{}.\n\\]argument , makes \\(\\hat{\\beta}\\) unbiased.Hence, linear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased must satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\).2. Minimum Variance (Gauss–Markov Part)Among estimators form \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased (\\(\\mathbf{C\\,X} = \\mathbf{}\\)), OLS achieves smallest covariance matrix.Variance General Unbiased EstimatorIf \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) \\(\\mathbf{C\\,X} = \\mathbf{}\\), :\\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n=\n\\mathrm{Var}(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\n\\mathbf{C}\\,\\mathrm{Var}(\\mathbf{y} \\mid \\mathbf{X})\\,\\mathbf{C}'\n=\n\\mathbf{C}\\left(\\sigma^2 \\mathbf{}\\right)\\mathbf{C}'\n=\n\\sigma^2\\,\\mathbf{C}\\,\\mathbf{C}'.\n\\]Variance OLS EstimatorFor OLS, \\(\\hat{\\beta} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{y}\\). Thus,\\[\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2\\,(\\mathbf{X}'\\mathbf{X})^{-1}.\n\\]Comparing \\(\\mathrm{Var}(\\tilde{\\beta})\\) \\(\\mathrm{Var}(\\hat{\\beta})\\)want show:\\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n\\;\\;\\text{positive semi-definite.}\n\\]Since \\(\\tilde{\\beta}\\) \\(\\hat{\\beta}\\) unbiased, know: \\[\n\\mathbf{C\\,X} = \\mathbf{},\n\\quad\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X} = \\mathbf{}.\n\\]Since \\(\\tilde{\\beta}\\) \\(\\hat{\\beta}\\) unbiased, know: \\[\n\\mathbf{C\\,X} = \\mathbf{},\n\\quad\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X} = \\mathbf{}.\n\\]One can show algebraically (proof provided ) \\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2 \\left[\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\right].\n\\] condition \\(\\mathbf{C\\,X} = \\mathbf{}\\), difference \\(\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\) positive semi-definite.One can show algebraically (proof provided ) \\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2 \\left[\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\right].\n\\] condition \\(\\mathbf{C\\,X} = \\mathbf{}\\), difference \\(\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\) positive semi-definite.Positive semi-definite difference meansPositive semi-definite difference means\\[\n\\mathbf{v}' \\left(\\mathbf{C}\\mathbf{C}'\n-\n(\\mathbf{X}'\\mathbf{X})^{-1}\\right)\\mathbf{v}\n\\ge\n0\n\\quad\n\\text{vectors } \\mathbf{v}.\n\\]Hence, \\(\\hat{\\beta}\\) smallest variance (sense covariance matrices) among linear unbiased estimators \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\).Summary Key PointsUnbiasedness:\nlinear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased \\(E(\\tilde{\\beta}\\mid \\mathbf{X}) = \\beta\\).\nforces \\(\\mathbf{C\\,X} = \\mathbf{}\\).Unbiasedness:\nlinear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased \\(E(\\tilde{\\beta}\\mid \\mathbf{X}) = \\beta\\).\nforces \\(\\mathbf{C\\,X} = \\mathbf{}\\).OLS Unbiased:\nOLS estimator \\(\\hat{\\beta} = (X'X)^{-1} X' \\, y\\) satisfies \\((X'X)^{-1} X' \\, X = \\), hence unbiased.OLS Unbiased:\nOLS estimator \\(\\hat{\\beta} = (X'X)^{-1} X' \\, y\\) satisfies \\((X'X)^{-1} X' \\, X = \\), hence unbiased.OLS Minimum Variance:\nAmong \\(\\mathbf{C}\\) satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\), matrix \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) gives smallest possible \\(\\mathrm{Var}(\\tilde{\\beta})\\).\nmatrix form, \\(\\mathrm{Var}(\\tilde{\\beta}) - \\mathrm{Var}(\\hat{\\beta})\\) positive semi-definite, showing OLS optimal (Best Linear Unbiased Estimator, BLUE).OLS Minimum Variance:\nAmong \\(\\mathbf{C}\\) satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\), matrix \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) gives smallest possible \\(\\mathrm{Var}(\\tilde{\\beta})\\).\nmatrix form, \\(\\mathrm{Var}(\\tilde{\\beta}) - \\mathrm{Var}(\\hat{\\beta})\\) positive semi-definite, showing OLS optimal (Best Linear Unbiased Estimator, BLUE).","code":""},{"path":"linear-regression.html","id":"finite-sample-properties","chapter":"5 Linear Regression","heading":"5.1.2.3 Finite Sample Properties","text":"finite sample properties estimator considered sample size \\(n\\) fixed (asymptotically large). Key properties include bias, distribution, standard deviation estimator.Bias measures close estimator , average, true parameter value \\(\\beta\\). defined :\\[\n\\text{Bias} = E(\\hat{\\beta}) - \\beta\n\\]:\\(\\beta\\): True parameter value.\\(\\hat{\\beta}\\): Estimator \\(\\beta\\).Unbiased Estimator: estimator unbiased :\\[\n\\text{Bias} = E(\\hat{\\beta}) - \\beta = 0 \\quad \\text{equivalently} \\quad E(\\hat{\\beta}) = \\beta\n\\]means estimator produce estimates , average, equal value trying estimate.estimator function random variables (data). distribution describes estimates vary across repeated samples. Key aspects include:Center: Mean distribution, relates bias.Spread: Variability estimator, captured standard deviation variance.standard deviation estimator measures spread sampling distribution. indicates variability estimator across different samples.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares-properties","chapter":"5 Linear Regression","heading":"5.1.2.3.1 Ordinary Least Squares Properties","text":"standard assumptions OLS:A1: relationship \\(Y\\) \\(X\\) linear.A2: matrix \\(\\mathbf{X'X}\\) invertible.A3: \\(E(\\epsilon|X) = 0\\) (errors uncorrelated predictors).OLS unbiased assumptions. proof follows:\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= E(\\mathbf{(X'X)^{-1}X'y}) && \\text{A2}\\\\\n               &= E(\\mathbf{(X'X)^{-1}X'(X\\beta + \\epsilon)}) && \\text{A1}\\\\\n               &= E(\\mathbf{(X'X)^{-1}X'X\\beta + (X'X)^{-1}X'\\epsilon}) \\\\\n               &= E(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n               &= \\beta + E(E(\\mathbf{(X'X)^{-1}X'\\epsilon}|X)) && \\text{LIE (Law Iterated Expectation)} \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'}E(\\epsilon|X)) \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'} \\cdot 0) && \\text{A3}\\\\\n               &= \\beta\n\\end{aligned}\n\\]Key Points:Linearity Expectation: Used separate terms involving \\(\\beta\\) \\(\\epsilon\\).Law Iterated Expectation (LIE): Simplifies nested expectations.Exogeneity Errors (A3): Ensures \\(E(\\epsilon|X) = 0\\), eliminating bias.Implications UnbiasednessOLS estimators centered around true value \\(\\beta\\) across repeated samples.small samples, OLS estimators may exhibit variability, expected value remains \\(\\beta\\).assumption exogeneity (A3) violated, OLS estimator becomes biased. Specifically, omitted variables endogeneity can introduce systematic errors estimation.Frisch-Waugh-Lovell Theorem:omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) (non-zero effect) omitted variable correlated included predictors (\\(\\mathbf{X_1'X_2} \\neq 0\\)), OLS estimator biased.bias arises omitted variable contributes variation dependent variable, effect incorrectly attributed predictors.","code":""},{"path":"linear-regression.html","id":"conditional-variance-of-ols-estimator","chapter":"5 Linear Regression","heading":"5.1.2.3.2 Conditional Variance of OLS Estimator","text":"assumptions A1, A2, A3, A4, conditional variance OLS estimator :\\[\n\\begin{aligned}\nVar(\\hat{\\beta}|\\mathbf{X}) &= Var(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon|X}) && \\text{A1-A2} \\\\\n    &= Var((\\mathbf{X'X)^{-1}X'\\epsilon|X}) \\\\\n    &= \\mathbf{(X'X)^{-1}X'} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X'X)^{-1}} \\\\\n    &= \\mathbf{(X'X)^{-1}X'} \\sigma^2 \\mathbf{X(X'X)^{-1}} && \\text{A4} \\\\\n    &= \\sigma^2 \\mathbf{(X'X)^{-1}}\n\\end{aligned}\n\\]result shows variance \\(\\hat{\\beta}\\) depends :\\(\\sigma^2\\): variance errors.\\(\\mathbf{X'X}\\): information content design matrix \\(\\mathbf{X}\\).","code":""},{"path":"linear-regression.html","id":"sources-of-variation-in-ols-estimator","chapter":"5 Linear Regression","heading":"5.1.2.3.3 Sources of Variation in OLS Estimator","text":"Unexplained Variation Dependent Variable: \\(\\sigma^2 = Var(\\epsilon_i|\\mathbf{X})\\)Large \\(\\sigma^2\\) indicates amount unexplained variation (noise) high relative explained variation (\\(\\mathbf{x_i \\beta}\\)).increases variance OLS estimator.Small Variation Predictor VariablesIf variance predictors (\\(Var(x_{i1}), Var(x_{i2}), \\dots\\)) small, design matrix \\(\\mathbf{X}\\) lacks information, leading :\nHigh variability \\(\\hat{\\beta}\\).\nPotential issues estimating coefficients accurately.\nHigh variability \\(\\hat{\\beta}\\).Potential issues estimating coefficients accurately.Small sample size exacerbates issue, fewer observations reduce robustness parameter estimates.Correlation Explanatory Variables (Collinearity)Strong correlation among explanatory variables creates problems:\n\\(x_{i1}\\) highly correlated linear combination \\(1, x_{i2}, x_{i3}, \\dots\\) contributes inflated standard errors \\(\\hat{\\beta}_1\\).\nIncluding many irrelevant variables exacerbates issue.\n\\(x_{i1}\\) highly correlated linear combination \\(1, x_{i2}, x_{i3}, \\dots\\) contributes inflated standard errors \\(\\hat{\\beta}_1\\).Including many irrelevant variables exacerbates issue.Perfect Collinearity:\n\\(x_1\\) perfectly determined linear combination predictors, matrix \\(\\mathbf{X'X}\\) becomes singular.\nviolates A2, making OLS impossible compute.\n\\(x_1\\) perfectly determined linear combination predictors, matrix \\(\\mathbf{X'X}\\) becomes singular.violates A2, making OLS impossible compute.Multicollinearity:\n\\(x_1\\) highly correlated (perfectly) linear combination variables, variance \\(\\hat{\\beta}_1\\) increases.\nMulticollinearity violate OLS assumptions weakens inference inflating standard errors.\n\\(x_1\\) highly correlated (perfectly) linear combination variables, variance \\(\\hat{\\beta}_1\\) increases.Multicollinearity violate OLS assumptions weakens inference inflating standard errors.","code":""},{"path":"linear-regression.html","id":"standard-errors","chapter":"5 Linear Regression","heading":"5.1.2.3.4 Standard Errors","text":"Standard errors measure variability estimator, specifically standard deviation \\(\\hat{\\beta}\\). crucial inference, quantify uncertainty associated parameter estimates.variance OLS estimator \\(\\hat{\\beta}\\) :\\[\nVar(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}}\n\\]:\\(\\sigma^2\\): Variance error terms.\\(\\mathbf{(X'X)^{-1}}\\): Inverse design matrix product, capturing geometry predictors.Estimation \\(\\sigma^2\\)assumptions A1 A5, can estimate \\(\\sigma^2\\) :\\[\ns^2 = \\frac{1}{n-k} \\sum_{=1}^{n} e_i^2 = \\frac{1}{n-k} SSR\n\\]:\\(n\\): Number observations.\\(k\\): Number predictors, including intercept.\\(e_i\\): Residuals regression model (\\(e_i = y_i - \\hat{y}_i\\)).\\(SSR\\): Sum squared residuals (\\(\\sum e_i^2\\)).degrees freedom adjustment (\\(n-k\\)) accounts fact residuals \\(e_i\\) true errors \\(\\epsilon_i\\). Since regression model uses \\(k\\) parameters, lose \\(k\\) degrees freedom estimating variance.standard error \\(\\sigma\\) :\\[\ns = \\sqrt{s^2}\n\\]However, \\(s\\) biased estimator \\(\\sigma\\) due Jensen’s Inequality.standard error regression coefficient \\(\\hat{\\beta}_{j-1}\\) :\\[\nSE(\\hat{\\beta}_{j-1}) = s \\sqrt{[(\\mathbf{X'X})^{-1}]_{jj}}\n\\]Alternatively, can expressed terms \\(SST_{j-1}\\) \\(R_{j-1}^2\\):\\[\nSE(\\hat{\\beta}_{j-1}) = \\frac{s}{\\sqrt{SST_{j-1}(1 - R_{j-1}^2)}}\n\\]:\\(SST_{j-1}\\): Total sum squares \\(x_{j-1}\\) regression \\(x_{j-1}\\) predictors.\\(R_{j-1}^2\\): Coefficient determination regression.formulation highlights role multicollinearity, \\(R_{j-1}^2\\) reflects correlation \\(x_{j-1}\\) predictors.","code":""},{"path":"linear-regression.html","id":"summary-of-finite-sample-properties-of-ols-under-different-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.3.5 Summary of Finite Sample Properties of OLS Under Different Assumptions","text":"A1-A3: OLS unbiased. \\[ E(\\hat{\\beta}) = \\beta \\]A1-A3: OLS unbiased. \\[ E(\\hat{\\beta}) = \\beta \\]A1-A4: variance OLS estimator : \\[ Var(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}} \\]A1-A4: variance OLS estimator : \\[ Var(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}} \\]A1-A4, A6: OLS estimator normally distributed: \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 \\mathbf{(X'X)^{-1}}) \\]A1-A4, A6: OLS estimator normally distributed: \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 \\mathbf{(X'X)^{-1}}) \\]A1-A4, Gauss-Markov Theorem: OLS BLUE (Best Linear Unbiased Estimator).A1-A4, Gauss-Markov Theorem: OLS BLUE (Best Linear Unbiased Estimator).A1-A5: standard errors \\(\\hat{\\beta}\\) unbiased estimators standard deviation \\(\\hat{\\beta}\\).A1-A5: standard errors \\(\\hat{\\beta}\\) unbiased estimators standard deviation \\(\\hat{\\beta}\\).","code":""},{"path":"linear-regression.html","id":"large-sample-properties","chapter":"5 Linear Regression","heading":"5.1.2.4 Large Sample Properties","text":"Large sample properties provide framework evaluate quality estimators finite sample properties either uninformative computationally infeasible. perspective becomes crucial modern data analysis, especially methods like GLS MLE, assumptions finite sample analysis may hold.Use Finite vs. Large Sample AnalysisFinite Sample Analysis:\nSmall sample sizes (e.g., \\(n < 30\\)).\nCritical studies exact results needed.\nUseful experimental designs case studies.\nFinite Sample Analysis:Small sample sizes (e.g., \\(n < 30\\)).Small sample sizes (e.g., \\(n < 30\\)).Critical studies exact results needed.Critical studies exact results needed.Useful experimental designs case studies.Useful experimental designs case studies.Large Sample Analysis:\nLarge datasets (e.g., \\(n > 100\\)).\nNecessary asymptotic approximations improve computational simplicity.\nLarge Sample Analysis:Large datasets (e.g., \\(n > 100\\)).Large datasets (e.g., \\(n > 100\\)).Necessary asymptotic approximations improve computational simplicity.Necessary asymptotic approximations improve computational simplicity.Key Concepts:Consistency:\nConsistency ensures estimator converges probability true parameter value sample size increases.\nMathematically, estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) : \\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]\nConsistency imply unbiasedness, unbiasedness guarantee consistency.\nConsistency ensures estimator converges probability true parameter value sample size increases.Mathematically, estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) : \\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]Consistency imply unbiasedness, unbiasedness guarantee consistency.Asymptotic Distribution:\nlimiting distribution describes shape scaled estimator \\(n \\\\infty\\).\nAsymptotic distributions often follow normality due Central Limit Theorem, underpins much inferential statistics.\nlimiting distribution describes shape scaled estimator \\(n \\\\infty\\).Asymptotic distributions often follow normality due Central Limit Theorem, underpins much inferential statistics.Asymptotic Variance:\nRepresents spread estimator respect limiting distribution.\nSmaller asymptotic variance implies greater precision large samples.\nRepresents spread estimator respect limiting distribution.Smaller asymptotic variance implies greater precision large samples.MotivationFinite Sample Properties, unbiasedness, rely strong assumptions like:A1 LinearityA3 Exogeneity Independent VariablesA4 HomoskedasticityA6 Normal DistributionWhen assumptions violated impractical verify, finite sample properties lose relevance. cases, Large Sample Propertiesserve essential alternative evaluating estimators.example, let conditional expectation function (CEF) : \\[\n\\mu(\\mathbf{X}) = E(y | \\mathbf{X}),\n\\] represents minimum mean squared predictor possible functions \\(f(\\mathbf{X})\\): \\[\n\\min_f E((y - f(\\mathbf{X}))^2).\n\\]assumptions A1 A3, CEF simplifies : \\[\n\\mu(\\mathbf{X}) = \\mathbf{X}\\beta.\n\\]linear projection given : \\[\nL(y | 1, \\mathbf{X}) = \\gamma_0 + \\mathbf{X}\\text{Var}(\\mathbf{X})^{-1}\\text{Cov}(\\mathbf{X}, y),\n\\] : \\[\n\\gamma = \\mathbf{X}\\text{Var}(\\mathbf{X})^{-1}\\text{Cov}(\\mathbf{X}, y).\n\\]linear projection minimizes mean squared error: \\[\n(\\gamma_0, \\gamma) = \\arg\\min_{(, b)} E\\left[\\left(E(y|\\mathbf{X}) - \\left(+ \\mathbf{X}b\\right)\\right)^2\\right].\n\\]Implications OLSConsistency: OLS always consistent linear projection, ensuring convergence true parameter value \\(n \\\\infty\\).Causal Interpretation: linear projection inherent causal interpretation—approximates conditional mean function.Assumption Independence: Unlike CEF, linear projection depend assumptions A1 A3.Evaluating Estimators via Large Sample PropertiesConsistency:\nMeasures estimator’s centrality true value.\nconsistent estimator ensures larger samples, estimates become arbitrarily close population parameter.\nMeasures estimator’s centrality true value.consistent estimator ensures larger samples, estimates become arbitrarily close population parameter.Limiting Distribution:\nHelps infer sampling behavior estimator \\(n\\) grows.\nOften approximated normal distribution practical use hypothesis testing confidence interval construction.\nHelps infer sampling behavior estimator \\(n\\) grows.Often approximated normal distribution practical use hypothesis testing confidence interval construction.Asymptotic Variance:\nQuantifies dispersion estimator around limiting distribution.\nSmaller variance desirable greater reliability.\nQuantifies dispersion estimator around limiting distribution.Smaller variance desirable greater reliability.estimator \\(\\hat{\\theta}\\) consistent parameter \\(\\theta\\) , sample size \\(n\\) increases, \\(\\hat{\\theta}\\) converges probability \\(\\theta\\):\\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]Convergence Probability:\nprobability \\(\\hat{\\theta}\\) deviates \\(\\theta\\) small margin (matter small) approaches zero \\(n\\) increases.\nFormally: \\[\n\\forall \\epsilon > 0, \\quad P(|\\hat{\\theta}_n - \\theta| > \\epsilon) \\0 \\quad \\text{} n \\\\infty.\n\\]Convergence Probability:probability \\(\\hat{\\theta}\\) deviates \\(\\theta\\) small margin (matter small) approaches zero \\(n\\) increases.Formally: \\[\n\\forall \\epsilon > 0, \\quad P(|\\hat{\\theta}_n - \\theta| > \\epsilon) \\0 \\quad \\text{} n \\\\infty.\n\\]Interpretation: Consistency ensures estimator becomes arbitrarily close true population parameter \\(\\theta\\) sample size grows.Interpretation: Consistency ensures estimator becomes arbitrarily close true population parameter \\(\\theta\\) sample size grows.Asymptotic Behavior: Large sample properties rely consistency provide valid approximations estimator’s behavior finite samples.Asymptotic Behavior: Large sample properties rely consistency provide valid approximations estimator’s behavior finite samples.Relationship Consistency UnbiasednessUnbiasedness:\nestimator \\(\\hat{\\theta}\\) unbiased expected value equals true parameter: \\[\nE(\\hat{\\theta}) = \\theta.\n\\]\nUnbiasedness finite-sample property depend sample size.\nestimator \\(\\hat{\\theta}\\) unbiased expected value equals true parameter: \\[\nE(\\hat{\\theta}) = \\theta.\n\\]Unbiasedness finite-sample property depend sample size.Consistency:\nConsistency large-sample property requires \\(\\hat{\\theta}\\) converge \\(\\theta\\) \\(n \\\\infty\\).\nConsistency large-sample property requires \\(\\hat{\\theta}\\) converge \\(\\theta\\) \\(n \\\\infty\\).Important DistinctionsUnbiasedness Imply Consistency:\nExample: Consider unbiased estimator extremely high variance diminish \\(n\\) increases. estimator converge \\(\\theta\\) probability.\nExample: Consider unbiased estimator extremely high variance diminish \\(n\\) increases. estimator converge \\(\\theta\\) probability.Consistency Imply Unbiasedness:\nExample: \\(\\hat{\\theta}_n = \\frac{n-1}{n}\\theta\\) biased finite \\(n\\), \\(n \\\\infty\\), \\(\\hat{\\theta}_n \\^p \\theta\\), making consistent.\nExample: \\(\\hat{\\theta}_n = \\frac{n-1}{n}\\theta\\) biased finite \\(n\\), \\(n \\\\infty\\), \\(\\hat{\\theta}_n \\^p \\theta\\), making consistent.OLS formula: \\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y},\n\\] can expand : \\[\n\\hat{\\beta} = \\mathbf{(\\sum_{=1}^{n}x_i'x_i)^{-1} \\sum_{=1}^{n}x_i'y_i},\n\\] equivalently: \\[\n\\hat{\\beta} = (n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}.\n\\]Taking probability limit assumptions A2 A5, apply Weak Law Large Numbers (random sample, averages converge expectations \\(n \\\\infty\\)): \\[\nplim(\\hat{\\beta}) = plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}),\n\\] simplifies : \\[\n\\begin{aligned}\nplim(\\hat{\\beta})\n  &= (E(\\mathbf{x_i'x_i}))^{-1}E(\\mathbf{x_i'y_i}) & \\text{A1}\\\\\n  &= (E(\\mathbf{x_i'x_i}))^{-1} \\left(E(\\mathbf{x_i'x_i} \\,\\beta) + E(\\mathbf{x_i\\,\\epsilon_i})\\right) & (A3a) \\\\\n  &= (E(\\mathbf{x_i' x_i}))^{-1}E(\\mathbf{x_i' x_i})\\, \\beta & (A2)\\\\\n  &= \\beta\n\\end{aligned}\n\\]Proof:model \\(y_i = x_i \\beta + \\epsilon_i\\), \\(\\beta\\) true parameter vector, \\(\\epsilon_i\\) random error:Expanding \\(E(x_i y_i)\\):\\[\nE(x_i'y_i) = E(x_i'(x_i\\beta + \\epsilon_i))\n\\]linearity expectation:\\[\nE(x_i'y_i) = E(x_i'x_i \\beta) + E(x_i \\epsilon_i)\n\\]second term \\(E(x_i \\epsilon_i) = 0\\) assumption A3a.Thus, \\[\nE(x_i'y_i) = E(x_i'x_i)\\beta.\n\\]Consistency OLSHence, short, assumptions:A1 LinearityA2 Full RankA3a: Weaker Exogeneity AssumptionA5 Data Generation (Random Sampling)term \\(E(\\mathbf{x_i'\\epsilon_i}) = 0\\), OLS estimator consistent:\\[\nplim(\\hat{\\beta}) = \\beta.\n\\]However, OLS consistency guarantee unbiasedness small samples.","code":""},{"path":"linear-regression.html","id":"asymptotic-distribution-of-ols","chapter":"5 Linear Regression","heading":"5.1.2.4.1 Asymptotic Distribution of OLS","text":"assumptions :A1 LinearityA2 Full RankA3a: Weaker Exogeneity AssumptionA5 Data Generation (Random Sampling)\\(\\mathbf{x_i'x_i}\\) finite first second moments (required Central Limit Theorem), :Convergence \\(n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i}\\): \\[\nn^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i} \\^p E(\\mathbf{x_i'x_i}).\n\\]Convergence \\(n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i}\\): \\[\nn^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i} \\^p E(\\mathbf{x_i'x_i}).\n\\]Asymptotic normality \\(\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i})\\): \\[\n\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}) \\^d N(0, \\mathbf{B}),\n\\] \\(\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i})\\).Asymptotic normality \\(\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i})\\): \\[\n\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}) \\^d N(0, \\mathbf{B}),\n\\] \\(\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i})\\).results, scaled difference \\(\\hat{\\beta}\\) \\(\\beta\\) follows: \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) = (n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i})^{-1} \\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}).\n\\]Central Limit Theorem: \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\^d N(0, \\Sigma),\n\\] : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1} \\mathbf{B} (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]sandwich form \\(\\Sigma\\) standard.Implications Homoskedasticity (A4) vs. HeteroskedasticityNo Homoskedasticity (A4 Homoskedasticity) needed:\nCLT large-sample distribution \\(\\hat{\\beta}\\) require homoskedasticity. place homoskedasticity simplify things \\[\n\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i}) = \\sigma^2 E(\\mathbf{x_i'x_i}),\n\\]\n\\(Var(\\epsilon_i | \\mathbf{x}_i) \\sigma^2\\)\n\\[\n\\Sigma = \\sigma^2 (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Homoskedasticity (A4 Homoskedasticity) needed:CLT large-sample distribution \\(\\hat{\\beta}\\) require homoskedasticity. place homoskedasticity simplify things \\[\n\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i}) = \\sigma^2 E(\\mathbf{x_i'x_i}),\n\\]\\(Var(\\epsilon_i | \\mathbf{x}_i) \\sigma^2\\)\\[\n\\Sigma = \\sigma^2 (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Adjusting Heteroskedasticity:\npractice, \\(\\sigma_i^2\\) can vary \\(\\mathbf{x}_i\\)​, leading heteroskedasticity.\nstandard OLS formula \\(Var(\\hat{\\beta})\\) inconsistent heteroskedasticity, one uses robust (White) standard errors.\nHeteroskedasticity can arise (limited ):\nLimited dependent variables.\nDependent variables large skewed ranges.\n\nAdjusting Heteroskedasticity:practice, \\(\\sigma_i^2\\) can vary \\(\\mathbf{x}_i\\)​, leading heteroskedasticity.standard OLS formula \\(Var(\\hat{\\beta})\\) inconsistent heteroskedasticity, one uses robust (White) standard errors.Heteroskedasticity can arise (limited ):\nLimited dependent variables.\nDependent variables large skewed ranges.\nLimited dependent variables.Dependent variables large skewed ranges.","code":""},{"path":"linear-regression.html","id":"derivation-of-asymptotic-variance","chapter":"5 Linear Regression","heading":"5.1.2.4.2 Derivation of Asymptotic Variance","text":"asymptotic variance OLS estimator derived follows:\\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Substituting \\(\\mathbf{B} = Var(\\mathbf{x_i'}\\epsilon_i)\\): \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Substituting \\(\\mathbf{B} = Var(\\mathbf{x_i'}\\epsilon_i)\\): \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Using definition variance: \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i - 0)(\\mathbf{x_i'}\\epsilon_i - 0)'](E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Using definition variance: \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i - 0)(\\mathbf{x_i'}\\epsilon_i - 0)'](E(\\mathbf{x_i'x_i}))^{-1}.\n\\][Law Iterated Expectations] A3a: Weaker Exogeneity Assumption, : \\[\nE[(\\mathbf{x_i'}\\epsilon_i)(\\mathbf{x_i'}\\epsilon_i)'] = E[E(\\epsilon_i^2|\\mathbf{x_i})\\mathbf{x_i'x_i}],\n\\][Law Iterated Expectations] A3a: Weaker Exogeneity Assumption, : \\[\nE[(\\mathbf{x_i'}\\epsilon_i)(\\mathbf{x_i'}\\epsilon_i)'] = E[E(\\epsilon_i^2|\\mathbf{x_i})\\mathbf{x_i'x_i}],\n\\]Assuming homoskedasticity (A4 Homoskedasticity), \\(E(\\epsilon_i^2|\\mathbf{x_i}) = \\sigma^2\\), : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Assuming homoskedasticity (A4 Homoskedasticity), \\(E(\\epsilon_i^2|\\mathbf{x_i}) = \\sigma^2\\), : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Simplifying: \\[\n\\Sigma = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Simplifying: \\[\n\\Sigma = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Hence, assumptions:A1 LinearityA1 LinearityA2 Full RankA2 Full RankA3a: Weaker Exogeneity AssumptionA3a: Weaker Exogeneity AssumptionA4 HomoskedasticityA4 HomoskedasticityA5 Data Generation (Random Sampling)A5 Data Generation (Random Sampling)\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\^d N(0, \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}).\n\\]asymptotic variance provides approximation scaled estimator’s variance large \\(n\\). leads :\\[\nAvar(\\sqrt{n}(\\hat{\\beta} - \\beta)) = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]finite sample variance estimator can approximated using asymptotic variance large sample sizes:\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta)) &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta})\n\\end{aligned}\n\\]However, critical note asymptotic variance (\\(Avar(.)\\)) behave manner finite sample variance (\\(Var(.)\\)). distinction evident following expressions:\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\\n&\\neq Avar(\\hat{\\beta})\n\\end{aligned}\n\\]Thus, \\(Avar(.)\\) provides useful approximation large samples, conceptual properties differ finite sample variance.Finite Sample Properties, standard errors calculated estimates conditional standard deviation:\\[\nSE_{fs}(\\hat{\\beta}_{j-1}) = \\sqrt{\\hat{Var}(\\hat{\\beta}_{j-1}|\\mathbf{X})} = \\sqrt{s^2 \\cdot [\\mathbf{(X'X)}^{-1}]_{jj}},\n\\]:\\(s^2\\) estimator error variance,\\(s^2\\) estimator error variance,\\([\\mathbf{(X'X)}^{-1}]_{jj}\\) represents \\(j\\)th diagonal element inverse design matrix.\\([\\mathbf{(X'X)}^{-1}]_{jj}\\) represents \\(j\\)th diagonal element inverse design matrix.contrast, Large Sample Properties, standard errors calculated estimates square root asymptotic variance:\\[\nSE_{ls}(\\hat{\\beta}_{j-1}) = \\sqrt{\\hat{Avar}(\\sqrt{n} \\hat{\\beta}_{j-1})/n} = \\sqrt{s^2 \\cdot [\\mathbf{(X'X)}^{-1}]_{jj}}.\n\\]Interestingly, standard error estimator identical finite large samples:expressions \\(SE_{fs}\\) \\(SE_{ls}\\) mathematically .However, conceptually estimating two different quantities:\nFinite Sample Standard Error: estimate conditional standard deviation \\(\\hat{\\beta}_{j-1}\\) given \\(\\mathbf{X}\\).\nLarge Sample Standard Error: estimate square root asymptotic variance \\(\\hat{\\beta}_{j-1}\\).\nFinite Sample Standard Error: estimate conditional standard deviation \\(\\hat{\\beta}_{j-1}\\) given \\(\\mathbf{X}\\).Large Sample Standard Error: estimate square root asymptotic variance \\(\\hat{\\beta}_{j-1}\\).assumptions required estimators valid differ stringency:Finite Sample Variance (Conditional Variance):\nRequires stronger assumptions (A1-A5).\nRequires stronger assumptions (A1-A5).Asymptotic Variance:\nValid weaker assumptions (A1, A2, A3a, A4, A5).\nValid weaker assumptions (A1, A2, A3a, A4, A5).distinction highlights utility asymptotic properties providing robust approximations finite sample assumptions may hold.","code":""},{"path":"linear-regression.html","id":"diagnostics","chapter":"5 Linear Regression","heading":"5.1.2.5 Diagnostics","text":"","code":""},{"path":"linear-regression.html","id":"normality-of-errors","chapter":"5 Linear Regression","heading":"5.1.2.5.1 Normality of Errors","text":"Ensuring normality errors critical assumption many regression models. Deviations assumption can impact inference model interpretation. diagnoses assessing normality, see Normality Assessment.Plots invaluable visual inspection normality. One common approach Q-Q plot, compares quantiles residuals standard normal distribution:","code":"\n# Example Q-Q plot\nset.seed(123) # For reproducibility\ny <- 1:100\nx <- rnorm(100) # Generating random normal data\nqqplot(x,\n       y,\n       main = \"Q-Q Plot\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")"},{"path":"linear-regression.html","id":"influential-observations-and-outliers","chapter":"5 Linear Regression","heading":"5.1.2.5.2 Influential Observations and Outliers","text":"Identifying influential observations outliers essential robust regression modeling. hat matrix (\\(\\mathbf{H}\\)) plays key role diagnosing influence.","code":""},{},{},{"path":"linear-regression.html","id":"identifying-influential-cases","chapter":"5 Linear Regression","heading":"5.1.2.5.3 Identifying Influential Cases","text":"influential, refer observations whose exclusion causes major changes fitted regression model. Note outliers influential.Types Influence MeasuresInfluence Single Fitted Values: DFFITSInfluence Fitted Values: Cook’s DInfluence Regression Coefficients: DFBETASMeasures like Cook’s D, DFFITS, DFBETAS combine leverage (hat matrix) residual size (studentized residuals) assess influence observation model whole. Hence, effectively combine impact \\(X\\)-space \\(Y\\)-space.","code":""},{},{},{},{"path":"linear-regression.html","id":"collinearity","chapter":"5 Linear Regression","heading":"5.1.2.5.4 Collinearity","text":"Collinearity (multicollinearity) refers correlation among explanatory variables regression model. can lead various issues, including:Large changes estimated regression coefficients predictor variable added removed, observations altered.Non-significant results individual tests regression coefficients important predictor variables.Regression coefficients signs opposite theoretical expectations prior experience.Large coefficients simple correlation pairs predictor variables correlation matrix.Wide confidence intervals regression coefficients representing important predictor variables.\\(X\\) variables highly correlated, inverse \\((X'X)^{-1}\\) either exist computationally unstable. can result :Non-interpretability parameters: \\[\\mathbf{b = (X'X)^{-1}X'y}\\]Infinite sampling variability: \\[\\mathbf{s^2(b) = MSE (X'X)^{-1}}\\]predictor variables (\\(X\\)) “perfectly” correlated, system becomes undetermined, leading infinite number models fit data. Specifically:\\(X'X\\) singular, \\((X'X)^{-1}\\) exist.results poor parameter estimation invalid statistical inference.","code":""},{},{},{"path":"linear-regression.html","id":"constancy-of-error-variance","chapter":"5 Linear Regression","heading":"5.1.2.5.5 Constancy of Error Variance","text":"Testing constancy error variance (homoscedasticity) ensures assumptions linear regression met.two commonly used tests assess error variance.","code":""},{},{},{"path":"linear-regression.html","id":"independence","chapter":"5 Linear Regression","heading":"5.1.2.5.6 Independence","text":"Testing independence ensures residuals regression model uncorrelated. Violation assumption may lead biased inefficient parameter estimates. , discuss three primary methods diagnosing dependence: plots, Durbin-Watson test, specific methods time-series spatial data.","code":""},{},{},{},{},{"path":"linear-regression.html","id":"generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2 Generalized Least Squares","text":"","code":""},{"path":"linear-regression.html","id":"infeasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2.1 Infeasible Generalized Least Squares","text":"Motivation Efficient EstimatorThe Gauss-Markov Theorem guarantees OLS Best Linear Unbiased Estimator (BLUE) assumptions A1-A4:\nA4: \\(Var(\\epsilon | \\mathbf{X}) = \\sigma^2 \\mathbf{}_n\\) (homoscedasticity autocorrelation).\nA4: \\(Var(\\epsilon | \\mathbf{X}) = \\sigma^2 \\mathbf{}_n\\) (homoscedasticity autocorrelation).A4 hold:\nHeteroskedasticity: \\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma^2\\).\nSerial Correlation: \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\\) (\\(\\neq j\\)).\nHeteroskedasticity: \\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma^2\\).Serial Correlation: \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\\) (\\(\\neq j\\)).Without A4, OLS unbiased longer efficient. motivates need alternative approach identify efficient estimator.unweighted (standard) regression model given :\\[\n\\mathbf{y} = \\mathbf{X \\beta} + \\mathbf{\\epsilon}\n\\]Assuming A1-A3 hold (linearity, full rank, exogeneity), A4 , variance error term longer proportional identity\\[\nVar(\\mathbf{\\epsilon} | \\mathbf{X}) = \\mathbf{\\Omega} \\neq \\sigma^2 \\mathbf{}_n.\n\\]address violation A4 (\\(\\mathbf{\\Omega} \\neq \\sigma^2 \\mathbf{}_n\\)), one can transform model premultiplying sides full-rank matrix \\(\\mathbf{w}\\) weighted (transformed) regression model:\\[\n\\mathbf{w y} = \\mathbf{w X \\beta} + \\mathbf{w \\epsilon},\n\\]\\(\\mathbf{w}\\) full-rank matrix chosen :\\[\n\\mathbf{w'w} = \\mathbf{\\Omega}^{-1}.\n\\]\\(\\mathbf{w}\\) Cholesky Decomposition \\(\\mathbf{\\Omega}^{-1}\\).Cholesky decomposition ensures \\(\\mathbf{w}\\) satisfies \\(\\mathbf{w'w = \\Omega^{-1}}\\), \\(\\mathbf{w}\\) “square root” \\(\\mathbf{\\Omega}^{-1}\\) matrix terms.transforming original model, variance transformed errors becomes:\\[\n\\begin{aligned}\n\\mathbf{\\Omega} &= Var(\\mathbf{\\epsilon} | \\mathbf{X}), \\\\\n\\mathbf{\\Omega}^{-1} &= Var(\\mathbf{\\epsilon} | \\mathbf{X})^{-1}.\n\\end{aligned}\n\\]transformed equation allows us compute efficient estimator.Using transformed model, Infeasible Generalized Least Squares (IGLS) estimator :\\[\n\\begin{aligned}\n\\mathbf{\\hat{\\beta}_{IGLS}} &= \\mathbf{(X'w'wX)^{-1}X'w'wy} \\\\\n&= \\mathbf{(X' \\mathbf{\\Omega}^{-1} X)^{-1} X' \\mathbf{\\Omega}^{-1} y} \\\\\n&= \\mathbf{\\beta + (X' \\mathbf{\\Omega}^{-1} X)^{-1} X' \\mathbf{\\Omega}^{-1} \\mathbf{\\epsilon}}.\n\\end{aligned}\n\\]UnbiasednessSince assumptions A1-A3 hold unweighted model:\\[\n\\begin{aligned}\n\\mathbf{E(\\hat{\\beta}_{IGLS}|\\mathbf{X})}\n&= \\mathbf{E(\\beta + (X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)|\\mathbf{X})} \\\\\n&= \\mathbf{\\beta + E(X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}E(\\epsilon|\\mathbf{X})}  && \\text{since A3: } E(\\epsilon|\\mathbf{X})=0, \\\\\n&= \\mathbf{\\beta}.\n\\end{aligned}\n\\]Thus, IGLS estimator unbiased.VarianceThe variance transformed errors given :\\[\n\\begin{aligned}\n\\mathbf{Var(w\\epsilon|\\mathbf{X})}\n&= \\mathbf{wVar(\\epsilon|\\mathbf{X})w'} \\\\\n&= \\mathbf{w\\Omega w'} \\\\\n&= \\mathbf{w(w'w)^{-1}w'} && \\text{since } \\mathbf{w} \\text{ full-rank,} \\\\\n&= \\mathbf{ww^{-1}(w')^{-1}w'} \\\\\n&= \\mathbf{I_n}.\n\\end{aligned}\n\\]Hence, A4 holds transformed (weighted) equation, satisfying Gauss-Markov conditions.variance IGLS estimator :\\[\n\\begin{aligned}\n\\mathbf{Var(\\hat{\\beta}_{IGLS}|\\mathbf{X})}\n&= \\mathbf{Var(\\beta + (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{Var((X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1} Var(\\epsilon|\\mathbf{X}) \\Omega^{-1}X(X'\\Omega^{-1}X)^{-1}} && \\text{A4 holds}, \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X'\\Omega^{-1}X)^{-1}}, \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}}.\n\\end{aligned}\n\\]EfficiencyThe difference variances OLS IGLS :\\[\n\\mathbf{Var(\\hat{\\beta}_{OLS}|\\mathbf{X}) - Var(\\hat{\\beta}_{IGLS}|\\mathbf{X})} = \\mathbf{\\Omega '},\n\\]:\\[\n\\mathbf{= (X'X)^{-1}X' - (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}}.\n\\]Since \\(\\mathbf{\\Omega}\\) positive semi-definite, \\(\\mathbf{\\Omega '}\\) also positive semi-definite. Thus, IGLS estimator efficient OLS heteroskedasticity autocorrelation.short, properties \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\):Unbiasedness: \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\) remains unbiased long A1-A3 hold.Efficiency: \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\) efficient OLS heteroskedasticity serial correlation since accounts structure \\(\\mathbf{\\Omega}\\).IGLS “Infeasible”?name infeasible arises generally impossible compute estimator directly due structure \\(\\mathbf{w}\\) (equivalently \\(\\mathbf{\\Omega}^{-1}\\)). matrix \\(\\mathbf{w}\\) defined :\\[\n\\mathbf{w} =\n\\begin{pmatrix}\nw_{11} & 0 & 0 & \\cdots & 0 \\\\\nw_{21} & w_{22} & 0 & \\cdots & 0 \\\\\nw_{31} & w_{32} & w_{33} & \\cdots & \\cdots \\\\\nw_{n1} & w_{n2} & w_{n3} & \\cdots & w_{nn} \\\\\n\\end{pmatrix},\n\\]\\(n(n+1)/2\\) unique elements \\(n\\) observations. results parameters data points, making direct estimation infeasible.make estimation feasible, assumptions structure \\(\\mathbf{\\Omega}\\) required. Common approaches include:Heteroskedasticity Errors: Assume multiplicative exponential model variance, \\(Var(\\epsilon_i|\\mathbf{X}) = \\sigma_i^2\\).\nAssume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]\nEstimate \\(\\sigma_i^2\\) using methods :\nModeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).\n\nHeteroskedasticity Errors: Assume multiplicative exponential model variance, \\(Var(\\epsilon_i|\\mathbf{X}) = \\sigma_i^2\\).Assume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]Assume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]Estimate \\(\\sigma_i^2\\) using methods :\nModeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).\nEstimate \\(\\sigma_i^2\\) using methods :Modeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).Serial Correlation: Assume serial correlation follows autoregressive process AR(1) Model, e.g., \\(\\epsilon_t = \\rho \\epsilon_{t-1} + u_t\\) \\(Cov(\\epsilon_t, \\epsilon_{t -h}) = \\rho^h \\sigma^2\\), variance-covariance matrix -diagonal elements decaying geometrically: \\[ \\mathbf{\\Omega} = \\frac{\\sigma^2}{1-\\rho^2} \\begin{pmatrix} 1      & \\rho    & \\rho^2 & \\cdots & \\rho^{n-1} \\\\ \\rho   & 1       & \\rho   & \\cdots & \\rho^{n-2} \\\\ \\rho^2 & \\rho    & 1      & \\cdots & \\rho^{n-3} \\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots \\\\ \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\cdots & 1 \\end{pmatrix}. \\]Serial Correlation: Assume serial correlation follows autoregressive process AR(1) Model, e.g., \\(\\epsilon_t = \\rho \\epsilon_{t-1} + u_t\\) \\(Cov(\\epsilon_t, \\epsilon_{t -h}) = \\rho^h \\sigma^2\\), variance-covariance matrix -diagonal elements decaying geometrically: \\[ \\mathbf{\\Omega} = \\frac{\\sigma^2}{1-\\rho^2} \\begin{pmatrix} 1      & \\rho    & \\rho^2 & \\cdots & \\rho^{n-1} \\\\ \\rho   & 1       & \\rho   & \\cdots & \\rho^{n-2} \\\\ \\rho^2 & \\rho    & 1      & \\cdots & \\rho^{n-3} \\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots \\\\ \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\cdots & 1 \\end{pmatrix}. \\]Cluster Errors: Assume block-diagonal structure \\(\\mathbf{\\Omega}\\) account grouped panel data.Cluster Errors: Assume block-diagonal structure \\(\\mathbf{\\Omega}\\) account grouped panel data.assumption simplifies estimation \\(\\mathbf{\\Omega}\\) thus \\(\\mathbf{w}\\), enabling Feasible Generalized Least Squares fewer unknown parameters estimate.","code":""},{"path":"linear-regression.html","id":"feasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2.2 Feasible Generalized Least Squares","text":"","code":""},{"path":"linear-regression.html","id":"heteroskedasticity-errors","chapter":"5 Linear Regression","heading":"5.2.2.1 Heteroskedasticity Errors","text":"Heteroskedasticity occurs variance error term constant across observations. Specifically:\\[\nVar(\\epsilon_i | x_i) = E(\\epsilon_i^2 | x_i) \\neq \\sigma^2,\n\\]instead depends function \\(x_i\\):\\[\nVar(\\epsilon_i | x_i) = h(x_i) = \\sigma_i^2\n\\]violates assumption homoscedasticity (constant variance), impacting efficiency OLS estimates.model:\\[\ny_i = x_i\\beta + \\epsilon_i,\n\\]apply transformation standardize variance:\\[\n\\frac{y_i}{\\sigma_i} = \\frac{x_i}{\\sigma_i} \\beta + \\frac{\\epsilon_i}{\\sigma_i}.\n\\]scaling observation \\(1/\\sigma_i\\), variance transformed error term becomes:\\[\n\\begin{aligned}\nVar\\left(\\frac{\\epsilon_i}{\\sigma_i} \\bigg| X \\right) &= \\frac{1}{\\sigma_i^2} Var(\\epsilon_i | X) \\\\\n&= \\frac{1}{\\sigma_i^2} \\sigma_i^2 \\\\\n&= 1.\n\\end{aligned}\n\\]Thus, heteroskedasticity corrected transformed model.matrix notation, transformed model :\\[\n\\mathbf{w y} = \\mathbf{w X \\beta + w \\epsilon},\n\\]\\(\\mathbf{w}\\) weight matrix used standardize variance. weight matrix \\(\\mathbf{w}\\) defined :\\[\n\\mathbf{w} =\n\\begin{pmatrix}\n1/\\sigma_1 & 0          & 0          & \\cdots & 0 \\\\\n0          & 1/\\sigma_2 & 0          & \\cdots & 0 \\\\\n0          & 0          & 1/\\sigma_3 & \\cdots & 0 \\\\\n\\vdots     & \\vdots     & \\vdots     & \\ddots & \\vdots \\\\\n0          & 0          & 0          & \\cdots & 1/\\sigma_n\n\\end{pmatrix}.\n\\]presence heteroskedasticity, variance error term, \\(Var(\\epsilon_i|\\mathbf{x}_i)\\), constant across observations. leads inefficient OLS estimates.Infeasible Weighted Least Squares (IWLS) assumes variances \\(\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i)\\) known. allows us adjust regression equation correct heteroskedasticity.model transformed follows:\\[\ny_i = \\mathbf{x}_i\\beta + \\epsilon_i \\quad \\text{(original equation)},\n\\]\\(\\epsilon_i\\) variance \\(\\sigma_i^2\\). make errors homoskedastic, divide \\(\\sigma_i\\):\\[\n\\frac{y_i}{\\sigma_i} = \\frac{\\mathbf{x}_i}{\\sigma_i}\\beta + \\frac{\\epsilon_i}{\\sigma_i}.\n\\]Now, transformed error term \\(\\epsilon_i / \\sigma_i\\) constant variance 1:\\[\nVar\\left(\\frac{\\epsilon_i}{\\sigma_i} | \\mathbf{x}_i \\right) = 1.\n\\]IWLS estimator minimizes weighted sum squared residuals transformed model:\\[\n\\text{Minimize: } \\sum_{=1}^n \\left( \\frac{y_i - \\mathbf{x}_i\\beta}{\\sigma_i} \\right)^2.\n\\]matrix form, IWLS estimator :\\[\n\\hat{\\beta}_{IWLS} = (\\mathbf{X}'\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{W}\\mathbf{y},\n\\]\\(\\mathbf{W}\\) diagonal matrix weights:\\[\n\\mathbf{W} =\n\\begin{pmatrix}\n1/\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\sigma_n^2\n\\end{pmatrix}.\n\\]Properties IWLSValid Standard Errors:\n\\(Var(\\epsilon_i | \\mathbf{X}) = \\sigma_i^2\\), usual standard errors IWLS valid.\n\\(Var(\\epsilon_i | \\mathbf{X}) = \\sigma_i^2\\), usual standard errors IWLS valid.Robustness:\nvariance assumption incorrect (\\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma_i^2\\)), heteroskedasticity-robust standard errors must used instead.\nvariance assumption incorrect (\\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma_i^2\\)), heteroskedasticity-robust standard errors must used instead.primary issue IWLS \\(\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i)\\) generally unknown. Specifically, know:\\[\n\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i) = E(\\epsilon_i^2|\\mathbf{x}_i).\n\\]challenges :Single Observation:\nobservation \\(\\), one \\(\\epsilon_i\\), insufficient estimate variance \\(\\sigma_i^2\\) directly.\nobservation \\(\\), one \\(\\epsilon_i\\), insufficient estimate variance \\(\\sigma_i^2\\) directly.Dependence Assumptions:\nestimate \\(\\sigma_i^2\\), must impose assumptions relationship \\(\\mathbf{x}_i\\).\nestimate \\(\\sigma_i^2\\), must impose assumptions relationship \\(\\mathbf{x}_i\\).make IWLS feasible, model \\(\\sigma_i^2\\) function predictors \\(\\mathbf{x}_i\\). common approach :\\[\n\\epsilon_i^2 = v_i \\exp(\\mathbf{x}_i\\gamma),\n\\]:\\(v_i\\) independent error term strictly positive values, representing random noise.\\(v_i\\) independent error term strictly positive values, representing random noise.\\(\\exp(\\mathbf{x}_i\\gamma)\\) deterministic function predictors \\(\\mathbf{x}_i\\).\\(\\exp(\\mathbf{x}_i\\gamma)\\) deterministic function predictors \\(\\mathbf{x}_i\\).Taking natural logarithm sides linearizes model:\\[\n\\ln(\\epsilon_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i),\n\\]\\(\\ln(v_i)\\) independent \\(\\mathbf{x}_i\\). transformation enables us estimate \\(\\gamma\\) using standard OLS techniques.Estimation Procedure Feasible GLS (FGLS)Since observe true errors \\(\\epsilon_i\\), approximate using OLS residuals \\(e_i\\). ’s step--step process:Compute OLS Residuals: First, fit original model using OLS calculate residuals:\n\\[\ne_i = y_i - \\mathbf{x}_i\\hat{\\beta}_{OLS}.\n\\]Compute OLS Residuals: First, fit original model using OLS calculate residuals:\\[\ne_i = y_i - \\mathbf{x}_i\\hat{\\beta}_{OLS}.\n\\]Approximate \\(\\epsilon_i^2\\) \\(e_i^2\\): Use squared residuals proxy squared errors:\n\\[\ne_i^2 \\approx \\epsilon_i^2.\n\\]Approximate \\(\\epsilon_i^2\\) \\(e_i^2\\): Use squared residuals proxy squared errors:\\[\ne_i^2 \\approx \\epsilon_i^2.\n\\]Log-Linear Model: Fit log-transformed model estimate \\(\\gamma\\):\n\\[\n\\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i).\n\\]\nEstimate \\(\\gamma\\) using OLS, \\(\\ln(v_i)\\) treated error term.Log-Linear Model: Fit log-transformed model estimate \\(\\gamma\\):\\[\n\\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i).\n\\]Estimate \\(\\gamma\\) using OLS, \\(\\ln(v_i)\\) treated error term.Estimate Variances: Use fitted values \\(\\hat{\\gamma}\\) estimate \\(\\sigma_i^2\\) observation:\n\\[\n\\hat{\\sigma}_i^2 = \\exp(\\mathbf{x}_i\\hat{\\gamma}).\n\\]Estimate Variances: Use fitted values \\(\\hat{\\gamma}\\) estimate \\(\\sigma_i^2\\) observation:\\[\n\\hat{\\sigma}_i^2 = \\exp(\\mathbf{x}_i\\hat{\\gamma}).\n\\]Perform Weighted Least Squares: Use estimated variances \\(\\hat{\\sigma}_i^2\\) construct weight matrix \\(\\mathbf{\\hat{W}}\\):\n\\[\n\\mathbf{\\hat{W}} =\n\\begin{pmatrix}\n1/\\hat{\\sigma}_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\hat{\\sigma}_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\hat{\\sigma}_n^2\n\\end{pmatrix}.\n\\]\n, compute Feasible GLS (FGLS) estimator:\n\\[\n\\hat{\\beta}_{FGLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}.\n\\]Perform Weighted Least Squares: Use estimated variances \\(\\hat{\\sigma}_i^2\\) construct weight matrix \\(\\mathbf{\\hat{W}}\\):\\[\n\\mathbf{\\hat{W}} =\n\\begin{pmatrix}\n1/\\hat{\\sigma}_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\hat{\\sigma}_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\hat{\\sigma}_n^2\n\\end{pmatrix}.\n\\], compute Feasible GLS (FGLS) estimator:\\[\n\\hat{\\beta}_{FGLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}.\n\\]","code":""},{"path":"linear-regression.html","id":"serial-correlation","chapter":"5 Linear Regression","heading":"5.2.2.2 Serial Correlation","text":"Serial correlation (also called autocorrelation) occurs error terms regression model correlated across observations. Formally:\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0 \\quad \\text{} \\neq j.\n\\]violates Gauss-Markov assumption \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) = 0\\), leading inefficiencies OLS estimates.","code":""},{"path":"linear-regression.html","id":"covariance-stationarity","chapter":"5 Linear Regression","heading":"5.2.2.2.1 Covariance Stationarity","text":"errors covariance stationary, covariance errors depends relative time positional difference (\\(h\\)), absolute position:\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{+h} | \\mathbf{x}_i, \\mathbf{x}_{+h}) = \\gamma_h,\n\\]\\(\\gamma_h\\) represents covariance lag \\(h\\).covariance stationarity, variance-covariance matrix error term \\(\\mathbf{\\epsilon}\\) takes following form:\\[\nVar(\\mathbf{\\epsilon}|\\mathbf{X}) = \\mathbf{\\Omega} =\n\\begin{pmatrix}\n\\sigma^2 & \\gamma_1 & \\gamma_2 & \\cdots & \\gamma_{n-1} \\\\\n\\gamma_1 & \\sigma^2 & \\gamma_1 & \\cdots & \\gamma_{n-2} \\\\\n\\gamma_2 & \\gamma_1 & \\sigma^2 & \\cdots & \\vdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\gamma_1 \\\\\n\\gamma_{n-1} & \\gamma_{n-2} & \\cdots & \\gamma_1 & \\sigma^2\n\\end{pmatrix}.\n\\]Key Points:diagonal elements represent variance error term: \\(\\sigma^2\\).-diagonal elements \\(\\gamma_h\\) represent covariances different lags \\(h\\).Serial Correlation Problem?matrix \\(\\mathbf{\\Omega}\\) introduces \\(n\\) parameters estimate (e.g., \\(\\sigma^2, \\gamma_1, \\gamma_2, \\ldots, \\gamma_{n-1}\\)). Estimating large number parameters becomes impractical, especially large datasets. address , impose additional structure reduce number parameters.","code":""},{"path":"linear-regression.html","id":"ar1","chapter":"5 Linear Regression","heading":"5.2.2.2.2 AR(1) Model","text":"AR(1) process, errors follow first-order autoregressive process:\\[\n\\begin{aligned}\ny_t &= \\beta_0 + x_t\\beta_1 + \\epsilon_t, \\\\\n\\epsilon_t &= \\rho \\epsilon_{t-1} + u_t,\n\\end{aligned}\n\\]:\\(\\rho\\) first-order autocorrelation coefficient, capturing relationship consecutive errors.\\(\\rho\\) first-order autocorrelation coefficient, capturing relationship consecutive errors.\\(u_t\\) white noise, satisfying \\(Var(u_t) = \\sigma_u^2\\) \\(Cov(u_t, u_{t-h}) = 0\\) \\(h \\neq 0\\).\\(u_t\\) white noise, satisfying \\(Var(u_t) = \\sigma_u^2\\) \\(Cov(u_t, u_{t-h}) = 0\\) \\(h \\neq 0\\).AR(1) assumption, variance-covariance matrix error term \\(\\mathbf{\\epsilon}\\) becomes:\\[\nVar(\\mathbf{\\epsilon} | \\mathbf{X}) = \\frac{\\sigma_u^2}{1-\\rho^2}\n\\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\cdots & \\rho^{n-1} \\\\\n\\rho & 1 & \\rho & \\cdots & \\rho^{n-2} \\\\\n\\rho^2 & \\rho & 1 & \\cdots & \\rho^{n-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\cdots & \\rho & 1\n\\end{pmatrix}.\n\\]Key Features:diagonal elements represent variance: \\(Var(\\epsilon_t | \\mathbf{X}) = \\sigma_u^2 / (1-\\rho^2)\\).-diagonal elements decay exponentially lag \\(h\\): \\(Cov(\\epsilon_t, \\epsilon_{t-h} | \\mathbf{X}) = \\rho^h \\cdot Var(\\epsilon_t | \\mathbf{X})\\).AR(1), one parameter \\(\\rho\\) needs estimated (addition \\(\\sigma_u^2\\)), greatly simplifying structure \\(\\mathbf{\\Omega}\\).OLS Properties AR(1)Consistency: assumptions A1, A2, A3a, A5a hold, OLS remains consistent.Asymptotic Normality: OLS estimates asymptotically normal.Inference Serial Correlation:\nStandard OLS errors invalid.\nUse Newey-West standard errors obtain robust inference.\nStandard OLS errors invalid.Use Newey-West standard errors obtain robust inference.","code":""},{"path":"linear-regression.html","id":"infeasible-cochrane-orcutt","chapter":"5 Linear Regression","heading":"5.2.2.2.3 Infeasible Cochrane-Orcutt","text":"Infeasible Cochrane-Orcutt procedure addresses serial correlation error terms assuming AR(1) process errors:\\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t,\n\\]\\(u_t\\) white noise \\(\\rho\\) autocorrelation coefficient.transforming original regression equation:\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t,\n\\]subtract \\(\\rho\\) times lagged equation:\\[\n\\rho y_{t-1} = \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}),\n\\]obtain weighted first-difference equation:\\[\ny_t - \\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t.\n\\]Key Points:Dependent Variable: \\(y_t - \\rho y_{t-1}\\).Independent Variable: \\(x_t - \\rho x_{t-1}\\).Error Term: \\(u_t\\), satisfies Gauss-Markov assumptions (A3, A4, A5).ICO estimator minimizes sum squared residuals transformed equation.Standard Errors:\nerrors truly follow AR(1) process, standard errors transformed equation valid.\ncomplex error structures, Newey-West HAC standard errors required.\nerrors truly follow AR(1) process, standard errors transformed equation valid.complex error structures, Newey-West HAC standard errors required.Loss Observations:\ntransformation involves first differences, means first observation (\\(y_1\\)) used. reduces effective sample size one.\ntransformation involves first differences, means first observation (\\(y_1\\)) used. reduces effective sample size one.Problem: \\(\\rho\\) UnknownThe ICO procedure infeasible requires knowledge \\(\\rho\\), autocorrelation coefficient. practice, estimate \\(\\rho\\) data.estimate \\(\\rho\\), use OLS residuals (\\(e_t\\)) proxy errors (\\(\\epsilon_t\\)). estimate \\(\\hat{\\rho}\\) given :\\[\n\\hat{\\rho} = \\frac{\\sum_{t=2}^{T} e_t e_{t-1}}{\\sum_{t=2}^{T} e_t^2}.\n\\]Estimation via OLS:Regress OLS residuals \\(e_t\\) lagged values \\(e_{t-1}\\), without intercept: \\[\ne_t = \\rho e_{t-1} + u_t.\n\\]slope regression estimate \\(\\hat{\\rho}\\).estimation efficient AR(1) assumption provides practical approximation \\(\\rho\\).","code":""},{"path":"linear-regression.html","id":"feasiable-prais-winsten","chapter":"5 Linear Regression","heading":"5.2.2.2.4 Feasible Prais-Winsten","text":"Feasible Prais-Winsten (FPW) method addresses AR(1) serial correlation regression models transforming data eliminate serial dependence errors. Unlike Infeasible Cochrane-Orcutt procedure, discards first observation, Prais-Winsten method retains using weighted transformation.FPW transformation uses following weighting matrix \\(\\mathbf{w}\\):\\[\n\\mathbf{w} =\n\\begin{pmatrix}\n\\sqrt{1 - \\hat{\\rho}^2} & 0 & 0 & \\cdots & 0 \\\\\n-\\hat{\\rho} & 1 & 0 & \\cdots & 0 \\\\\n0 & -\\hat{\\rho} & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & -\\hat{\\rho} & 1\n\\end{pmatrix}.\n\\]whereThe first row accounts transformation first observation, using \\(\\sqrt{1 - \\hat{\\rho}^2}\\).Subsequent rows represent AR(1) transformation remaining observations.Step--Step ProcedureStep 1: Initial OLS EstimationEstimate regression model using OLS:\\[\ny_t = \\mathbf{x}_t \\beta + \\epsilon_t,\n\\]compute residuals:\\[\ne_t = y_t - \\mathbf{x}_t \\hat{\\beta}.\n\\]Step 2: Estimate AR(1) Correlation CoefficientEstimate AR(1) correlation coefficient \\(\\rho\\) regressing \\(e_t\\) \\(e_{t-1}\\) without intercept:\\[\ne_t = \\rho e_{t-1} + u_t.\n\\]slope regression estimated \\(\\hat{\\rho}\\).Step 3: Transform DataApply transformation using weighting matrix \\(\\mathbf{w}\\) transform dependent variable \\(\\mathbf{y}\\) independent variables \\(\\mathbf{X}\\):\\[\n\\mathbf{wy} = \\mathbf{wX} \\beta + \\mathbf{w\\epsilon}.\n\\]Specifically: 1. \\(t=1\\), transformed dependent independent variables : \\[\n   \\tilde{y}_1 = \\sqrt{1 - \\hat{\\rho}^2} \\cdot y_1, \\quad \\tilde{\\mathbf{x}}_1 = \\sqrt{1 - \\hat{\\rho}^2} \\cdot \\mathbf{x}_1.\n   \\] 2. \\(t=2, \\dots, T\\), transformed variables : \\[\n   \\tilde{y}_t = y_t - \\hat{\\rho} \\cdot y_{t-1}, \\quad \\tilde{\\mathbf{x}}_t = \\mathbf{x}_t - \\hat{\\rho} \\cdot \\mathbf{x}_{t-1}.\n   \\]Step 4: Feasible Prais-Winsten EstimationRun OLS transformed equation:\\[\n\\mathbf{wy} = \\mathbf{wX} \\beta + \\mathbf{w\\epsilon}.\n\\]resulting estimator Feasible Prais-Winsten (FPW) estimator:\\[\n\\hat{\\beta}_{FPW} = (\\mathbf{X}'\\mathbf{w}'\\mathbf{w}\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{w}'\\mathbf{w}\\mathbf{y}.\n\\]Properties Feasible Prais-Winsten EstimatorInfeasible Prais-Winsten Estimator:\ninfeasible Prais-Winsten (PW) estimator assumes AR(1) parameter \\(\\rho\\) known.\nassumptions A1, A2, A3 unweighted equation, infeasible PW estimator unbiased efficient.\ninfeasible Prais-Winsten (PW) estimator assumes AR(1) parameter \\(\\rho\\) known.assumptions A1, A2, A3 unweighted equation, infeasible PW estimator unbiased efficient.Feasible Prais-Winsten (FPW) Estimator: FPW estimator replaces unknown \\(\\rho\\) estimate \\(\\hat{\\rho}\\) derived OLS residuals, introducing bias small samples.\nBias:\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.\n\nConsistency:\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\n\nNote: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.\n\nEfficiency\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]\nStandard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\n\n\nBias:\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.Consistency:\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\n\nNote: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\nA1: model linear parameters.A2: independent variables linearly independent.A5: data generated random sampling.Additionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).Note: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.Efficiency\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]\nStandard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\n\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]Standard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.Robust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.","code":""},{"path":"linear-regression.html","id":"cluster-errors","chapter":"5 Linear Regression","heading":"5.2.2.3 Cluster Errors","text":"Consider regression model clustered errors:\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi},\n\\]:\\(g\\) indexes group (e.g., households, firms, schools).\\(g\\) indexes group (e.g., households, firms, schools).\\(\\) indexes individual within group.\\(\\) indexes individual within group.covariance structure errors \\(\\epsilon_{gi}\\) defined :\\[\nCov(\\epsilon_{gi}, \\epsilon_{hj})\n\\begin{cases}\n= 0 & \\text{} g \\neq h \\text{ (independent across groups)}, \\\\\n\\neq 0 & \\text{pair } (,j) \\text{ within group } g.\n\\end{cases}\n\\]Within group, individuals’ errors may correlated (.e., intra-group correlation), errors independent across groups. violates A4 (constant variance correlation errors).Suppose three groups varying sizes. variance-covariance matrix \\(\\mathbf{\\Omega}\\) errors \\(\\mathbf{\\epsilon}\\) :\\[\nVar(\\mathbf{\\epsilon}| \\mathbf{X}) = \\mathbf{\\Omega} =\n\\begin{pmatrix}\n\\sigma^2 & \\delta_{12}^1 & \\delta_{13}^1 & 0 & 0 & 0 \\\\\n\\delta_{12}^1 & \\sigma^2 & \\delta_{23}^1 & 0 & 0 & 0 \\\\\n\\delta_{13}^1 & \\delta_{23}^1 & \\sigma^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2 & \\delta_{12}^2 & 0 \\\\\n0 & 0 & 0 & \\delta_{12}^2 & \\sigma^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{pmatrix}.\n\\]\\(\\delta_{ij}^g = Cov(\\epsilon_{gi}, \\epsilon_{gj})\\) covariance errors individuals \\(\\) \\(j\\) group \\(g\\).\\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) \\(g \\neq h\\) (independent groups).Infeasible Generalized Least Squares (Cluster)Assume Known Variance-Covariance Matrix: \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, construct \\(\\mathbf{\\Omega}\\) compute inverse \\(\\mathbf{\\Omega}^{-1}\\).Assume Known Variance-Covariance Matrix: \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, construct \\(\\mathbf{\\Omega}\\) compute inverse \\(\\mathbf{\\Omega}^{-1}\\).Infeasible GLS Estimator: infeasible generalized least squares (IGLS) estimator :\n\\[\n\\hat{\\beta}_{IGLS} = (\\mathbf{X}'\\mathbf{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\Omega}^{-1}\\mathbf{y}.\n\\]Infeasible GLS Estimator: infeasible generalized least squares (IGLS) estimator :\\[\n\\hat{\\beta}_{IGLS} = (\\mathbf{X}'\\mathbf{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\Omega}^{-1}\\mathbf{y}.\n\\]Problem:know \\(\\sigma^2\\) \\(\\delta_{ij}^g\\), making approach infeasible.Even \\(\\mathbf{\\Omega}\\) estimated, incorrect assumptions structure may lead invalid inference.make estimation feasible, assume group-level random effects specification error:\\[\n\\begin{aligned}\ny_{gi} &= \\mathbf{x}_{gi}\\beta + c_g + u_{gi}, \\\\\nVar(c_g|\\mathbf{x}_i) &= \\sigma_c^2, \\\\\nVar(u_{gi}|\\mathbf{x}_i) &= \\sigma_u^2,\n\\end{aligned}\n\\]:\\(c_g\\) represents group-level random effect (common shocks within group, independent across groups).\\(c_g\\) represents group-level random effect (common shocks within group, independent across groups).\\(u_{gi}\\) represents individual-level error (idiosyncratic shocks within group, independent across individuals groups).\\(u_{gi}\\) represents individual-level error (idiosyncratic shocks within group, independent across individuals groups).\\(\\epsilon_{gi} = c_g + u_{gi}\\)\\(\\epsilon_{gi} = c_g + u_{gi}\\)Independence Assumptions:\\(c_g\\) \\(u_{gi}\\) independent .mean-independent \\(\\mathbf{x}_i\\).specification, variance-covariance matrix \\(\\mathbf{\\Omega}\\) becomes block diagonal, block corresponds group:\\[\nVar(\\mathbf{\\epsilon}| \\mathbf{X}) = \\mathbf{\\Omega} =\n\\begin{pmatrix}\n\\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & \\sigma_c^2 & 0 & 0 & 0 \\\\\n\\sigma_c^2 & \\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & 0 & 0 & 0 \\\\\n\\sigma_c^2 & \\sigma_c^2  & \\sigma_c^2 + \\sigma_u^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & 0 \\\\\n0 & 0 & 0 & \\sigma_c^2 & \\sigma_c^2 + \\sigma_u^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_c^2 + \\sigma_u^2\n\\end{pmatrix}.\n\\]variance components \\(\\sigma_c^2\\) \\(\\sigma_u^2\\) unknown, can use Feasible Group-Level Random Effects (RE) estimator simultaneously estimate variances regression coefficients \\(\\beta\\). practical approach allows us account intra-group correlation errors still obtain consistent efficient estimates parameters.Step--Step ProcedureStep 1: Initial OLS EstimationEstimate regression model using OLS:\\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}, \\]compute residuals:\\[ e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}. \\]Step 2: Estimate Variance ComponentsUse standard OLS variance estimator \\(s^2\\) estimate total variance:\\[ s^2 = \\frac{1}{n - k} \\sum_{=1}^{n} e_i^2, \\]\\(n\\) total number observations \\(k\\) number regressors (including intercept).Estimate -group variance \\(\\hat{\\sigma}_c^2\\) using:\\[ \\hat{\\sigma}_c^2 = \\frac{1}{G} \\sum_{g=1}^{G} \\left( \\frac{1}{\\sum_{=1}^{n_g - 1} } \\sum_{\\neq j} \\sum_{j=1}^{n_g} e_{gi} e_{gj} \\right), \\]:\\(G\\) total number groups,\\(G\\) total number groups,\\(n_g\\) size group \\(g\\),\\(n_g\\) size group \\(g\\),term \\(\\sum_{\\neq j} e_{gi} e_{gj}\\) accounts within-group covariance.term \\(\\sum_{\\neq j} e_{gi} e_{gj}\\) accounts within-group covariance.Estimate within-group variance :\\[ \\hat{\\sigma}_u^2 = s^2 - \\hat{\\sigma}_c^2. \\]Step 3: Construct Variance-Covariance MatrixUse estimated variances \\(\\hat{\\sigma}_c^2\\) \\(\\hat{\\sigma}_u^2\\) construct variance-covariance matrix \\(\\hat{\\Omega}\\) error term:\\[ \\hat{\\Omega}_{gi,gj} = \\begin{cases} \\hat{\\sigma}_c^2 + \\hat{\\sigma}_u^2 & \\text{} = j \\text{ (diagonal elements)}, \\\\ \\hat{\\sigma}_c^2 & \\text{} \\neq j \\text{ (-diagonal elements within group)}, \\\\ 0 & \\text{} g \\neq h \\text{ (across groups)}. \\end{cases} \\]Step 4: Feasible GLS EstimationWith \\(\\hat{\\Omega}\\) hand, perform Feasible Generalized Least Squares (FGLS) estimate \\(\\beta\\):\\[ \\hat{\\beta}_{RE} = (\\mathbf{X}'\\hat{\\Omega}^{-1}\\mathbf{X})^{-1} \\mathbf{X}'\\hat{\\Omega}^{-1}\\mathbf{y}. \\]assumptions \\(\\mathbf{\\Omega}\\) incorrect infeasible, use cluster-robust standard errors account intra-group correlation without explicitly modeling variance-covariance structure. standard errors remain valid arbitrary within-cluster dependence, provided clusters independent.Properties Feasible Group-Level Random Effects EstimatorInfeasible Group RE EstimatorThe infeasible RE estimator (assuming known variances) unbiased assumptions A1, A2, A3 unweighted equation.A3 requires: \\[ E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_g|\\mathbf{x}_i) + E(u_{gi}|\\mathbf{x}_i) = 0. \\] assumes:\n\\(E(c_g|\\mathbf{x}_i) = 0\\): random effects assumption (group-level effects uncorrelated regressors).\n\\(E(u_{gi}|\\mathbf{x}_i) = 0\\): endogeneity individual level.\n\\(E(c_g|\\mathbf{x}_i) = 0\\): random effects assumption (group-level effects uncorrelated regressors).\\(E(u_{gi}|\\mathbf{x}_i) = 0\\): endogeneity individual level.Feasible Group RE EstimatorThe feasible RE estimator biased variances \\(\\sigma_c^2\\) \\(\\sigma_u^2\\) estimated, introducing approximation errors.However, estimator consistent A1, A2, A3a (\\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_g) + E(\\mathbf{x}_i'u_{gi}) = 0\\)), A5a.Efficiency\nAsymptotic Efficiency:\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.\n\nStandard Errors:\nrandom effects specification correct, usual standard errors consistent.\nconcern complex dependence structures heteroskedasticity, use cluster robust standard errors.\n\nAsymptotic Efficiency:\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.Standard Errors:\nrandom effects specification correct, usual standard errors consistent.\nconcern complex dependence structures heteroskedasticity, use cluster robust standard errors.\nrandom effects specification correct, usual standard errors consistent.concern complex dependence structures heteroskedasticity, use cluster robust standard errors.","code":""},{"path":"linear-regression.html","id":"weighted-least-squares","chapter":"5 Linear Regression","heading":"5.2.3 Weighted Least Squares","text":"presence heteroskedasticity, errors \\(\\epsilon_i\\) non-constant variance \\(Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2\\). violates Gauss-Markov assumption homoskedasticity, leading inefficient OLS estimates.Weighted Least Squares (WLS) addresses applying weights inversely proportional variance errors, ensuring observations larger variances less influence estimation.Weighted Least Squares essentially Generalized Least Squares special case \\(\\mathbf{\\Omega}\\) diagonal matrix variances \\(\\sigma_i^2\\) diagonal (.e., errors uncorrelated non-constant variance).\n, assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\left(\\sigma_1^2, \\ldots, \\sigma_n^2\\right)\\)\n\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\left(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\right)\\)\nWeighted Least Squares essentially Generalized Least Squares special case \\(\\mathbf{\\Omega}\\) diagonal matrix variances \\(\\sigma_i^2\\) diagonal (.e., errors uncorrelated non-constant variance)., assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\left(\\sigma_1^2, \\ldots, \\sigma_n^2\\right)\\), assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\left(\\sigma_1^2, \\ldots, \\sigma_n^2\\right)\\)\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\left(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\right)\\)\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\left(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\right)\\)Steps Feasible Weighted Least Squares (FWLS)1. Initial OLS EstimationFirst, estimate model using OLS:\\[ y_i = \\mathbf{x}_i\\beta + \\epsilon_i, \\]compute residuals:\\[ e_i = y_i - \\mathbf{x}_i \\hat{\\beta}. \\]2. Model Error VarianceTransform residuals model variance function predictors:\\[ \\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i), \\]:\\(e_i^2\\) approximates \\(\\epsilon_i^2\\),\\(e_i^2\\) approximates \\(\\epsilon_i^2\\),\\(\\ln(v_i)\\) error term auxiliary regression, assumed independent \\(\\mathbf{x}_i\\).\\(\\ln(v_i)\\) error term auxiliary regression, assumed independent \\(\\mathbf{x}_i\\).Estimate equation using OLS obtain predicted values:\\[ \\hat{g}_i = \\mathbf{x}_i \\hat{\\gamma}. \\]3. Estimate WeightsUse predicted values auxiliary regression compute weights:\\[ \\hat{\\sigma}_i = \\sqrt{\\exp(\\hat{g}_i)}. \\]weights approximate standard deviation errors.4. Weighted RegressionTransform original equation dividing \\(\\hat{\\sigma}_i\\):\\[ \\frac{y_i}{\\hat{\\sigma}_i} = \\frac{\\mathbf{x}_i}{\\hat{\\sigma}_i}\\beta + \\frac{\\epsilon_i}{\\hat{\\sigma}_i}. \\]Estimate transformed equation using OLS. resulting estimator Feasible Weighted Least Squares (FWLS) estimator:\\[ \\hat{\\beta}_{FWLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}, \\]\\(\\mathbf{\\hat{W}}\\) diagonal weight matrix elements \\(1/\\hat{\\sigma}_i^2\\).Properties FWLS EstimatorUnbiasedness:\ninfeasible WLS estimator (\\(\\sigma_i\\) known) unbiased assumptions A1-A3 unweighted model.\nFWLS estimator unbiased due approximation \\(\\sigma_i\\) using \\(\\hat{\\sigma}_i\\).\ninfeasible WLS estimator (\\(\\sigma_i\\) known) unbiased assumptions A1-A3 unweighted model.FWLS estimator unbiased due approximation \\(\\sigma_i\\) using \\(\\hat{\\sigma}_i\\).Consistency:\nFWLS estimator consistent following assumptions:\nA1 (unweighted equation): model linear parameters.\nA2 (unweighted equation): independent variables linearly independent.\nA5: data randomly sampled.\n\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .\n\nFWLS estimator consistent following assumptions:\nA1 (unweighted equation): model linear parameters.\nA2 (unweighted equation): independent variables linearly independent.\nA5: data randomly sampled.\n\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .\nA1 (unweighted equation): model linear parameters.A2 (unweighted equation): independent variables linearly independent.A5: data randomly sampled.\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .Efficiency:\nFWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity: \\[ Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2 = \\exp(\\mathbf{x}_i\\gamma). \\]\nFWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity: \\[ Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2 = \\exp(\\mathbf{x}_i\\gamma). \\]FWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity.Usual Standard Errors:\nerrors truly multiplicative exponential heteroskedastic, usual standard errors FWLS valid.\nerrors truly multiplicative exponential heteroskedastic, usual standard errors FWLS valid.Heteroskedastic Robust Standard Errors:\npotential mis-specification multiplicative exponential model \\(\\sigma_i^2\\), heteroskedastic-robust standard errors reported ensure valid inference.\npotential mis-specification multiplicative exponential model \\(\\sigma_i^2\\), heteroskedastic-robust standard errors reported ensure valid inference.","code":""},{"path":"linear-regression.html","id":"maximum-likelihood-estimator","chapter":"5 Linear Regression","heading":"5.3 Maximum Likelihood","text":"Maximum Likelihood Estimation (MLE) statistical method used estimate parameters model maximizing likelihood observing given data. premise find parameter values maximize probability (likelihood) observed data.likelihood function, denoted \\(L(\\theta)\\), expressed :\\[\nL(\\theta) = \\prod_{=1}^{n} f(y_i|\\theta)\n\\]:\\(f(y|\\theta)\\) probability density mass function observing single value \\(Y\\) given parameter \\(\\theta\\).product runs \\(n\\) observations.different types data, \\(f(y|\\theta)\\) can take different forms. example, \\(y\\) dichotomous (e.g., success/failure), likelihood function becomes:\\[\nL(\\theta) = \\prod_{=1}^{n} \\theta^{y_i} (1-\\theta)^{1-y_i}\n\\], \\(\\hat{\\theta}\\) Maximum Likelihood Estimator (MLE) :\\[\nL(\\hat{\\theta}) > L(\\theta_0), \\quad \\forall \\theta_0 \\text{ parameter space.}\n\\]See Distributions review variable distributions.","code":""},{"path":"linear-regression.html","id":"motivation-for-mle","chapter":"5 Linear Regression","heading":"5.3.1 Motivation for MLE","text":"Suppose know conditional distribution \\(Y\\) given \\(X\\), denoted :\\[\nf_{Y|X}(y, x; \\theta)\n\\]\\(\\theta\\) unknown parameter distribution. Sometimes, concerned unconditional distribution \\(f_Y(y; \\theta)\\).sample independent identically distributed (..d.) data, joint probability sample :\\[\nf_{Y_1, \\ldots, Y_n|X_1, \\ldots, X_n}(y_1, \\ldots, y_n, x_1, \\ldots, x_n; \\theta) = \\prod_{=1}^{n} f_{Y|X}(y_i, x_i; \\theta)\n\\]joint distribution, evaluated observed data, defines likelihood function. goal MLE find parameter \\(\\theta\\) maximizes likelihood.estimate \\(\\theta\\), maximize likelihood function:\\[\n\\max_{\\theta} \\prod_{=1}^{n} f_{Y|X}(y_i, x_i; \\theta)\n\\]practice, easier work natural logarithm likelihood (log-likelihood), transforms product sum:\\[\n\\max_{\\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\theta))\n\\]Solving Maximum Likelihood EstimatorFirst-Order Condition: Solve first derivative log-likelihood function respect \\(\\theta\\):\n\\[\n\\frac{\\partial}{\\partial \\theta} \\ell(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\ln L(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) = 0\n\\]\nyields critical points likelihood maximized. derivative, sometimes written \\(U(\\theta)\\), called score. Intuitively, log-likelihood’s “peak” indicates parameter value(s) make observed data “likely.”First-Order Condition: Solve first derivative log-likelihood function respect \\(\\theta\\):\\[\n\\frac{\\partial}{\\partial \\theta} \\ell(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\ln L(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) = 0\n\\]yields critical points likelihood maximized. derivative, sometimes written \\(U(\\theta)\\), called score. Intuitively, log-likelihood’s “peak” indicates parameter value(s) make observed data “likely.”Second-Order Condition: Verify second derivative log-likelihood function negative critical point:\n\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) < 0\n\\]\nensures solution corresponds maximum.Second-Order Condition: Verify second derivative log-likelihood function negative critical point:\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) < 0\n\\]ensures solution corresponds maximum.Examples Likelihood FunctionsUnconditional Poisson DistributionThe Poisson distribution models count data, number website visits day product orders per hour. likelihood function :\\[\nL(\\theta) = \\prod_{=1}^{n} \\frac{\\theta^{y_i} e^{-\\theta}}{y_i!}\n\\]Exponential DistributionThe exponential distribution often used model time events, time machine fails. probability density function (PDF) :\\[\nf_{Y|X}(y, x; \\theta) = \\frac{\\exp(-y / (x \\theta))}{x \\theta}\n\\]joint likelihood \\(n\\) observations :\\[\nL(\\theta) = \\prod_{=1}^{n} \\frac{\\exp(-y_i / (x_i \\theta))}{x_i \\theta}\n\\]taking logarithm, obtain log-likelihood ease maximization.","code":""},{"path":"linear-regression.html","id":"key-quantities-for-inference","chapter":"5 Linear Regression","heading":"5.3.2 Key Quantities for Inference","text":"Score Function\nscore given \\[\nU(\\theta) \\;=\\; \\frac{d}{d\\theta} \\ell(\\theta).\n\\]\nSetting \\(U(\\hat{\\theta}_{\\mathrm{MLE}}) = 0\\) yields critical points log-likelihood, can find \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).Score Function\nscore given \\[\nU(\\theta) \\;=\\; \\frac{d}{d\\theta} \\ell(\\theta).\n\\]\nSetting \\(U(\\hat{\\theta}_{\\mathrm{MLE}}) = 0\\) yields critical points log-likelihood, can find \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).Observed Information\nsecond derivative log-likelihood, taken MLE, called observed information:\n\\[\nI_O(\\theta) \\;=\\; - \\frac{d^2}{d\\theta^2} \\ell(\\theta).\n\\]\n(negative sign often included \\(I_O(\\theta)\\) positive \\(\\ell(\\theta)\\) concave near maximum. texts, see defined without negative sign, idea : measures “pointedness” curvature \\(\\ell(\\theta)\\) maximum.)Observed Information\nsecond derivative log-likelihood, taken MLE, called observed information:\\[\nI_O(\\theta) \\;=\\; - \\frac{d^2}{d\\theta^2} \\ell(\\theta).\n\\](negative sign often included \\(I_O(\\theta)\\) positive \\(\\ell(\\theta)\\) concave near maximum. texts, see defined without negative sign, idea : measures “pointedness” curvature \\(\\ell(\\theta)\\) maximum.)Fisher Information\nFisher Information (expected information) expectation observed information distribution data:\n\\[\n(\\theta) \\;=\\; \\mathbb{E}\\left[I_O(\\theta)\\right].\n\\]\nquantifies much information data carry parameter \\(\\theta\\). larger Fisher information suggests can estimate \\(\\theta\\) precisely.Fisher Information\nFisher Information (expected information) expectation observed information distribution data:\\[\n(\\theta) \\;=\\; \\mathbb{E}\\left[I_O(\\theta)\\right].\n\\]quantifies much information data carry parameter \\(\\theta\\). larger Fisher information suggests can estimate \\(\\theta\\) precisely.Approximate Variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\)\nOne key results standard asymptotic theory , large \\(n\\), variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\) can approximated inverse Fisher information:\n\\[\n\\mathrm{Var}\\left(\\hat{\\theta}_{\\mathrm{MLE}}\\right) \\;\\approx\\; (\\theta)^{-1}.\n\\]\nalso lays groundwork constructing confidence intervals \\(\\theta\\) large samples.Approximate Variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\)\nOne key results standard asymptotic theory , large \\(n\\), variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\) can approximated inverse Fisher information:\\[\n\\mathrm{Var}\\left(\\hat{\\theta}_{\\mathrm{MLE}}\\right) \\;\\approx\\; (\\theta)^{-1}.\n\\]also lays groundwork constructing confidence intervals \\(\\theta\\) large samples.","code":""},{"path":"linear-regression.html","id":"assumptions-of-mle","chapter":"5 Linear Regression","heading":"5.3.3 Assumptions of MLE","text":"MLE desirable properties—consistency, asymptotic normality, efficiency—come “free.” Instead, rely certain assumptions. breakdown main regularity conditions. conditions typically mild many practical settings (example, exponential families, normal distribution), need checked complex models.High-Level Regulatory AssumptionsIndependence Identical Distribution (iid)\nsample \\(\\{(x_i, y_i)\\}\\) usually assumed composed independent identically distributed observations. independence assumption simplifies likelihood product individual densities: \\[\nL(\\theta) = \\prod_{=1}^n f_{Y\\mid X}(y_i, x_i; \\theta).\n\\] practice, dependent data (e.g., time series, spatial data), modifications required likelihood function.Independence Identical Distribution (iid)\nsample \\(\\{(x_i, y_i)\\}\\) usually assumed composed independent identically distributed observations. independence assumption simplifies likelihood product individual densities: \\[\nL(\\theta) = \\prod_{=1}^n f_{Y\\mid X}(y_i, x_i; \\theta).\n\\] practice, dependent data (e.g., time series, spatial data), modifications required likelihood function.Density Function\nobservations must come conditional probability density function \\(f_{Y\\mid X}(\\cdot,\\cdot;\\theta)\\). model changes across observations, simply multiply together one unified likelihood.Density Function\nobservations must come conditional probability density function \\(f_{Y\\mid X}(\\cdot,\\cdot;\\theta)\\). model changes across observations, simply multiply together one unified likelihood.Multivariate Normality (certain models)\nmany practical cases—especially continuous outcomes—might assume (multivariate) normal distributions finite second fourth moments (Little 1988). assumptions, MLE mean vector covariance matrix consistent (conditions) asymptotically normal. assumption quite common regression, ANOVA, classical statistical frameworks.Multivariate Normality (certain models)\nmany practical cases—especially continuous outcomes—might assume (multivariate) normal distributions finite second fourth moments (Little 1988). assumptions, MLE mean vector covariance matrix consistent (conditions) asymptotically normal. assumption quite common regression, ANOVA, classical statistical frameworks.","code":""},{"path":"linear-regression.html","id":"large-sample-properties-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1 Large Sample Properties of MLE","text":"","code":""},{"path":"linear-regression.html","id":"consistency-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1.1 Consistency of MLE","text":"Definition: estimator \\(\\hat{\\theta}_n\\) consistent converges probability true parameter value \\(\\theta_0\\) sample size \\(n \\\\infty\\):\\[\n\\hat{\\theta}_n \\;\\^p\\; \\theta_0.\n\\]MLE, set regularity conditions \\(R1\\)–\\(R4\\) commonly used ensure consistency:R1\n\\(\\theta \\neq \\theta_0\\), \\[\nf_{Y\\mid X}(y_i, x_i; \\theta) \\;\\neq\\; f_{Y\\mid X}(y_i, x_i; \\theta_0).\n\\]\nsimpler terms, model identifiable: two distinct parameter values generate exact distribution data.R1\n\\(\\theta \\neq \\theta_0\\), \\[\nf_{Y\\mid X}(y_i, x_i; \\theta) \\;\\neq\\; f_{Y\\mid X}(y_i, x_i; \\theta_0).\n\\]simpler terms, model identifiable: two distinct parameter values generate exact distribution data.R2\nparameter space \\(\\Theta\\) compact (closed bounded), contains true parameter \\(\\theta_0\\). ensures \\(\\theta\\) lies “nice” region (parameter going infinity, etc.), making easier prove maximum space indeed exists.R2\nparameter space \\(\\Theta\\) compact (closed bounded), contains true parameter \\(\\theta_0\\). ensures \\(\\theta\\) lies “nice” region (parameter going infinity, etc.), making easier prove maximum space indeed exists.R3\nlog-likelihood function \\(\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\) continuous \\(\\theta\\) probability \\(1\\). Continuity important can apply theorems (like Continuous Mapping Theorem Extreme Value Theorem) find maxima.R3\nlog-likelihood function \\(\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\) continuous \\(\\theta\\) probability \\(1\\). Continuity important can apply theorems (like Continuous Mapping Theorem Extreme Value Theorem) find maxima.R4\nexpected supremum absolute value log-likelihood finite:\n\\[\n\\mathbb{E}\\left(\\sup_{\\theta \\\\Theta} \\left|\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\right|\\right) < \\infty.\n\\]\ntechnical condition helps ensure can “exchange” expectations suprema, step needed many consistency proofs.R4\nexpected supremum absolute value log-likelihood finite:\\[\n\\mathbb{E}\\left(\\sup_{\\theta \\\\Theta} \\left|\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\right|\\right) < \\infty.\n\\]technical condition helps ensure can “exchange” expectations suprema, step needed many consistency proofs.conditions satisfied, can show via standard arguments (e.g., Law Large Numbers, uniform convergence log-likelihood) :\\[\n\\hat{\\theta}_{\\mathrm{MLE}} \\;\\^p\\; \\theta_0 \\quad (\\text{consistency}).\n\\]","code":""},{"path":"linear-regression.html","id":"asymptotic-normality-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1.2 Asymptotic Normality of MLE","text":"Definition: estimator \\(\\hat{\\theta}_n\\) asymptotically normal \\[\n\\sqrt{n}\\,(\\hat{\\theta}_n - \\theta_0) \\;\\^d\\; \\mathcal{N}\\left(0,\\Sigma\\right),\n\\]\\(\\^d\\) denotes convergence distribution \\(\\Sigma\\) covariance matrix. MLE, \\(\\Sigma\\) typically \\((\\theta_0)^{-1}\\), \\((\\theta_0)\\) Fisher information evaluated true parameter.Beyond \\(R1\\)–\\(R4\\), need following additional assumptions:R5\ntrue parameter \\(\\theta_0\\) interior parameter space \\(\\Theta\\). \\(\\theta_0\\) sits boundary, different arguments required handle edge effects.R5\ntrue parameter \\(\\theta_0\\) interior parameter space \\(\\Theta\\). \\(\\theta_0\\) sits boundary, different arguments required handle edge effects.R6\npdf \\(f_{Y\\mid X}(y_i, x_i; \\theta)\\) twice continuously differentiable (\\(\\theta\\)) strictly positive neighborhood \\(N\\) \\(\\theta_0\\). allows us use second-order Taylor expansions around \\(\\theta_0\\) get approximate distribution \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).R6\npdf \\(f_{Y\\mid X}(y_i, x_i; \\theta)\\) twice continuously differentiable (\\(\\theta\\)) strictly positive neighborhood \\(N\\) \\(\\theta_0\\). allows us use second-order Taylor expansions around \\(\\theta_0\\) get approximate distribution \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).R7\nfollowing integrals finite neighborhood \\(N\\) \\(\\theta_0\\):\n\\(\\int \\sup_{\\theta \\N} \\left\\|\\frac{\\partial f_{Y\\mid X}(y_i, x_i; \\theta)}{\\partial \\theta} \\right\\|\\, d(y,x) < \\infty\\).\n\\(\\int \\sup_{\\theta \\N} \\left\\|\\frac{\\partial^2 f_{Y\\mid X}(y_i, x_i; \\theta)}{\\partial \\theta \\partial \\theta'} \\right\\|\\, d(y,x) < \\infty\\).\n\\(\\mathbb{E}\\left(\\sup_{\\theta \\N} \\left\\|\\frac{\\partial^2 \\ln(f_{Y\\mid X}(y_i, x_i; \\theta))}{\\partial \\theta \\partial \\theta'} \\right\\|\\right) < \\infty\\).\nconditions ensure differentiating inside integrals justified (via dominated convergence theorem) can expand log-likelihood Taylor series safely.R7\nfollowing integrals finite neighborhood \\(N\\) \\(\\theta_0\\):\\(\\int \\sup_{\\theta \\N} \\left\\|\\frac{\\partial f_{Y\\mid X}(y_i, x_i; \\theta)}{\\partial \\theta} \\right\\|\\, d(y,x) < \\infty\\).\\(\\int \\sup_{\\theta \\N} \\left\\|\\frac{\\partial^2 f_{Y\\mid X}(y_i, x_i; \\theta)}{\\partial \\theta \\partial \\theta'} \\right\\|\\, d(y,x) < \\infty\\).\\(\\mathbb{E}\\left(\\sup_{\\theta \\N} \\left\\|\\frac{\\partial^2 \\ln(f_{Y\\mid X}(y_i, x_i; \\theta))}{\\partial \\theta \\partial \\theta'} \\right\\|\\right) < \\infty\\).conditions ensure differentiating inside integrals justified (via dominated convergence theorem) can expand log-likelihood Taylor series safely.R8\ninformation matrix \\((\\theta_0)\\) exists nonsingular:\n\\[\n(\\theta_0) = \\mathrm{Var}\\left(\\frac{\\partial}{\\partial \\theta} \\ln\\left(f_{Y\\mid X}(y_i, x_i; \\theta_0)\\right)\\right) \\neq 0.\n\\]\nNonsingularity implies enough information data estimate \\(\\theta\\) uniquely.R8\ninformation matrix \\((\\theta_0)\\) exists nonsingular:\\[\n(\\theta_0) = \\mathrm{Var}\\left(\\frac{\\partial}{\\partial \\theta} \\ln\\left(f_{Y\\mid X}(y_i, x_i; \\theta_0)\\right)\\right) \\neq 0.\n\\]Nonsingularity implies enough information data estimate \\(\\theta\\) uniquely.\\(R1\\)–\\(R8\\), can show \\[\n\\sqrt{n}\\,(\\hat{\\theta}_{\\mathrm{MLE}} - \\theta_0) \\^d \\mathcal{N}\\left(0,\\,(\\theta_0)^{-1}\\right).\n\\]result central frequentist inference, allowing construct approximate confidence intervals hypothesis tests using normal approximation large \\(n\\).","code":""},{"path":"linear-regression.html","id":"properties-of-mle","chapter":"5 Linear Regression","heading":"5.3.4 Properties of MLE","text":"established earlier sections Maximum Likelihood Estimators (MLEs) consistent (Consistency MLE) asymptotically normal (Asymptotic Normality MLE) standard regularity conditions, now highlight additional properties make MLE powerful estimation technique.Asymptotic EfficiencyDefinition: estimator asymptotically efficient attains smallest possible asymptotic variance among consistent estimators (.e., achieves Cramér-Rao Lower Bound).Interpretation: large samples, MLE typically smaller standard errors consistent estimators fully use assumed distributional form.Implication: true model correctly specified, MLE efficient among broad class estimators, leading precise inference \\(\\theta\\).\nCramér-Rao Lower Bound (CRLB): theoretical lower limit variance unbiased (asymptotically unbiased) estimator C. R. Rao (1992).\nMLE Meets CRLB: correct specification standard regularity conditions, asymptotic variance MLE matches CRLB, making asymptotically efficient.\nInterpretation: Achieving CRLB means unbiased estimator can consistently outperform MLE terms variance large \\(n\\).\nCramér-Rao Lower Bound (CRLB): theoretical lower limit variance unbiased (asymptotically unbiased) estimator C. R. Rao (1992).MLE Meets CRLB: correct specification standard regularity conditions, asymptotic variance MLE matches CRLB, making asymptotically efficient.Interpretation: Achieving CRLB means unbiased estimator can consistently outperform MLE terms variance large \\(n\\).InvarianceCore Idea: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), smooth transformation \\(g(\\theta)\\), MLE \\(g(\\theta)\\) simply \\(g(\\hat{\\theta})\\).Example: \\(\\theta\\) mean parameter want MLE variance \\(\\theta^2\\), can just square MLE \\(\\theta\\).Key Point: invariance property saves considerable effort—need re-derive new likelihood transformed parameter.Explicit vs. Implicit MLEExplicit MLE:\nOccurs score equation can solved closed form. classic example MLE mean variance normal distribution.Implicit MLE:\nHappens closed-form solution exists. Iterative numerical methods, Newton-Raphson, Expectation-Maximization (EM), optimization algorithms, used find \\(\\hat{\\theta}\\).Distributional Mis-SpecificationDefinition: assume distribution \\(f_{Y|X}(\\cdot;\\theta)\\) reflect true data-generating process, MLE may become inconsistent biased finite samples.Quasi-MLE:\nstrategy handle certain forms mis-specification.\nchosen distribution belongs flexible class meets certain conditions (e.g., generalized linear models robust link), resulting parameter estimates can remain consistent parameters interest.\nstrategy handle certain forms mis-specification.chosen distribution belongs flexible class meets certain conditions (e.g., generalized linear models robust link), resulting parameter estimates can remain consistent parameters interest.Nonparametric & Semiparametric Approaches:\nRequire minimal distributional assumptions.\nrobust mis-specification can harder implement may exhibit higher variance require larger sample sizes achieve comparable precision.\nRequire minimal distributional assumptions.robust mis-specification can harder implement may exhibit higher variance require larger sample sizes achieve comparable precision.","code":""},{"path":"linear-regression.html","id":"practical-considerations","chapter":"5 Linear Regression","heading":"5.3.5 Practical Considerations","text":"Use Cases\nMLE extremely popular :\nBinary Outcomes (logistic regression)\nCount Data (Poisson regression)\nStrictly Positive Outcomes (Gamma regression)\nHeteroskedastic Settings (models variance related mean, e.g., GLMs)\n\nMLE extremely popular :\nBinary Outcomes (logistic regression)\nCount Data (Poisson regression)\nStrictly Positive Outcomes (Gamma regression)\nHeteroskedastic Settings (models variance related mean, e.g., GLMs)\nBinary Outcomes (logistic regression)Count Data (Poisson regression)Strictly Positive Outcomes (Gamma regression)Heteroskedastic Settings (models variance related mean, e.g., GLMs)Distributional Assumptions\nefficiency gains MLE stem using specific probability model.\nassumed model closely reflects data-generating process, MLE gives accurate parameter estimates reliable standard errors.\nMLE assumes knowledge conditional distribution outcome variable. assumption parallels normality assumption linear regression models (e.g., A6 Normal Distribution).\nseverely mis-specified, consider robust semi-/nonparametric methods.\nefficiency gains MLE stem using specific probability model.assumed model closely reflects data-generating process, MLE gives accurate parameter estimates reliable standard errors.MLE assumes knowledge conditional distribution outcome variable. assumption parallels normality assumption linear regression models (e.g., A6 Normal Distribution).severely mis-specified, consider robust semi-/nonparametric methods.Comparison OLS: See Comparison MLE OLS details.\nOrdinary Least Squares special case MLE errors normally distributed homoscedastic.\ngeneral settings (e.g., non-Gaussian heteroskedastic data), MLE can outperform OLS terms smaller standard errors better inference.\nOrdinary Least Squares special case MLE errors normally distributed homoscedastic.general settings (e.g., non-Gaussian heteroskedastic data), MLE can outperform OLS terms smaller standard errors better inference.Numerical Stability & Computation\ncomplex likelihoods, iterative methods can fail converge converge local maxima.\nProper initialization diagnostics (e.g., checking multiple start points) crucial.\ncomplex likelihoods, iterative methods can fail converge converge local maxima.Proper initialization diagnostics (e.g., checking multiple start points) crucial.","code":""},{"path":"linear-regression.html","id":"comparison-of-mle-and-ols","chapter":"5 Linear Regression","heading":"5.3.6 Comparison of MLE and OLS","text":"Maximum Likelihood Estimation powerful estimation method, solve challenges associated Ordinary Least Squares. detailed comparison highlighting similarities, differences, limitations.Key Points ComparisonInference Methods:\nMLE:\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).\nmethods replace use F-statistics commonly associated OLS.\n\nOLS:\nRelies F-statistic hypothesis testing joint inference.\n\nMLE:\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).\nmethods replace use F-statistics commonly associated OLS.\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).methods replace use F-statistics commonly associated OLS.OLS:\nRelies F-statistic hypothesis testing joint inference.\nRelies F-statistic hypothesis testing joint inference.Sensitivity Functional Form:\nMLE OLS sensitive functional form model. Incorrect specification (e.g., linear vs. nonlinear relationships) can lead biased inefficient estimates cases.\nMLE OLS sensitive functional form model. Incorrect specification (e.g., linear vs. nonlinear relationships) can lead biased inefficient estimates cases.Perfect Collinearity Multicollinearity:\nmethods affected collinearity:\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.\nMulticollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.\n\nNeither MLE OLS directly resolves issues without additional measures, regularization variable selection.\nmethods affected collinearity:\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.\nMulticollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.Multicollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.Neither MLE OLS directly resolves issues without additional measures, regularization variable selection.Endogeneity:\nProblems like omitted variable bias simultaneous equations affect MLE OLS:\nrelevant predictors omitted, estimates methods likely biased inconsistent.\nSimilarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.\n\nMLE, efficient correct model specification, inherently address endogeneity.\nProblems like omitted variable bias simultaneous equations affect MLE OLS:\nrelevant predictors omitted, estimates methods likely biased inconsistent.\nSimilarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.\nrelevant predictors omitted, estimates methods likely biased inconsistent.Similarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.MLE, efficient correct model specification, inherently address endogeneity.Situations MLE OLS DifferPractical ConsiderationsWhen Use MLE:\nSituations dependent variable :\nBinary (e.g., logistic regression)\nCount data (e.g., Poisson regression)\nSkewed bounded (e.g., survival models)\n\nmodel naturally arises probabilistic framework.\nSituations dependent variable :\nBinary (e.g., logistic regression)\nCount data (e.g., Poisson regression)\nSkewed bounded (e.g., survival models)\nBinary (e.g., logistic regression)Count data (e.g., Poisson regression)Skewed bounded (e.g., survival models)model naturally arises probabilistic framework.Use OLS:\nSuitable continuous dependent variables approximately linear relationships predictors outcomes.\nSimpler implement interpret assumptions linear regression reasonably met.\nSuitable continuous dependent variables approximately linear relationships predictors outcomes.Simpler implement interpret assumptions linear regression reasonably met.","code":""},{"path":"linear-regression.html","id":"applications-of-mle","chapter":"5 Linear Regression","heading":"5.3.7 Applications of MLE","text":"MLE widely used across various applications estimate parameters models tailored specific data structures. key applications MLE, categorized problem type estimation method.Hours workedDonations charityHousehold consumption goodDependent variable often censored zero (another threshold).Large fraction observations corner (e.g., 0 hours, 0 donations).Tobit regression(latent variable approach censoring)Number arrestsNumber cigarettes smokedDoctor visits per yearDependent variable consists non-negative integer counts.Possible overdispersion (variance > mean).Poisson regression,Negative Binomial regressionPoisson assumes mean = variance, often Negative Binomial preferred real data.Zero-inflated models (ZIP/ZINB) may used data excess zeros.Demand different car brandsVotes primary electionChoice travel modeDependent variable categorical choice among 3+ alternatives.category distinct, inherent ordering (e.g., brand , B, C).Multinomial logit,Multinomial probitExtension binary choice (logit/probit) multiple categories.Independence Irrelevant Alternatives (IIA) can concern multinomial logit.Self-reported happiness (low/medium/high)Income level bracketsLikert-scale surveysDependent variable ordered (e.g., low < medium < high).Distances categories necessarily equal.Ordered logit,Ordered probitProbit/logit framework adapted preserve ordinal information.Interprets latent continuous variable mapped discrete ordered categories.","code":""},{"path":"linear-regression.html","id":"binary-response-models","chapter":"5 Linear Regression","heading":"5.3.7.1 Binary Response Models","text":"binary response variable (\\(y_i\\)) follows Bernoulli distribution:\\[\nf_Y(y_i; p) = p^{y_i}(1-p)^{(1-y_i)}\n\\]\\(p\\) probability success. conditional models, likelihood becomes:\\[\nf_{Y|X}(y_i, x_i; p(.)) = p(x_i)^{y_i}(1 - p(x_i))^{(1-y_i)}\n\\]model \\(p(x_i)\\), use function \\(x_i\\) unknown parameters \\(\\theta\\). common approach involves latent variable model:\\[\n\\begin{aligned}\ny_i &= 1\\{y_i^* > 0 \\}, \\\\\ny_i^* &= x_i \\beta - \\epsilon_i,\n\\end{aligned}\n\\]:\\(y_i^*\\) unobserved (latent) variable.\\(\\epsilon_i\\) random variable mean 0, representing unobserved noise.Rewriting terms observed data:\\[\ny_i = 1\\{x_i \\beta > \\epsilon_i\\}.\n\\]probability function becomes:\\[\n\\begin{aligned}\np(x_i) &= P(y_i = 1 | x_i) \\\\\n&= P(x_i \\beta > \\epsilon_i | x_i) \\\\\n&= F_{\\epsilon|X}(x_i \\beta | x_i),\n\\end{aligned}\n\\]\\(F_{\\epsilon|X}(.)\\) cumulative distribution function (CDF) \\(\\epsilon_i\\). Assuming independence \\(\\epsilon_i\\) \\(x_i\\), probability function simplifies :\\[\np(x_i) = F_\\epsilon(x_i \\beta).\n\\]conditional expectation function equivalent:\\[\nE(y_i | x_i) = P(y_i = 1 | x_i) = F_\\epsilon(x_i \\beta).\n\\]Common Distributional AssumptionsProbit Model:\nAssumes \\(\\epsilon_i\\) follows standard normal distribution.\n\\(F_\\epsilon(.) = \\Phi(.)\\), \\(\\Phi(.)\\) standard normal CDF.\nAssumes \\(\\epsilon_i\\) follows standard normal distribution.\\(F_\\epsilon(.) = \\Phi(.)\\), \\(\\Phi(.)\\) standard normal CDF.Logit Model:\nAssumes \\(\\epsilon_i\\) follows standard logistic distribution.\n\\(F_\\epsilon(.) = \\Lambda(.)\\), \\(\\Lambda(.)\\) logistic CDF.\nAssumes \\(\\epsilon_i\\) follows standard logistic distribution.\\(F_\\epsilon(.) = \\Lambda(.)\\), \\(\\Lambda(.)\\) logistic CDF.Steps Derive MLE Binary ModelsSpecify Log-Likelihood:\nchosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\n\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]\nchosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\n\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]chosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]Maximize Log-Likelihood:\nFind parameter estimates maximize log-likelihood:\n\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]\nFind parameter estimates maximize log-likelihood:\n\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]Find parameter estimates maximize log-likelihood:\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]Properties Probit Logit EstimatorsConsistency Asymptotic Normality:\nProbit Logit estimators consistent asymptotically normal :\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.\nA5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).\n\nProbit Logit estimators consistent asymptotically normal :\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.\nA5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.A5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).Distributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).Asymptotic Efficiency:\nassumptions, Probit Logit estimators asymptotically efficient variance:\n\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\n\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).\nassumptions, Probit Logit estimators asymptotically efficient variance:\n\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\n\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).assumptions, Probit Logit estimators asymptotically efficient variance:\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).Interpretation Binary Response ModelsBinary response models, Probit Logit, estimate probability event occurring (\\(y_i = 1\\)) given predictor variables \\(x_i\\). However, interpreting estimated coefficients (\\(\\beta\\)) models differs significantly linear models. , explore interpret coefficients concept partial effects.Interpreting \\(\\beta\\) Binary Response ModelsIn binary response models, coefficient \\(\\beta_j\\) represents average change latent variable \\(y_i^*\\) (unobserved variable) one-unit change \\(x_{ij}\\). provides insight direction relationship:Magnitudes \\(\\beta_j\\) direct, meaningful interpretation terms \\(y_i\\).Direction \\(\\beta_j\\) meaningful:\n\\(\\beta_j > 0\\): positive association \\(x_{ij}\\) probability \\(y_i = 1\\).\n\\(\\beta_j < 0\\): negative association \\(x_{ij}\\) probability \\(y_i = 1\\).\n\\(\\beta_j > 0\\): positive association \\(x_{ij}\\) probability \\(y_i = 1\\).\\(\\beta_j < 0\\): negative association \\(x_{ij}\\) probability \\(y_i = 1\\).Partial Effects Nonlinear Binary ModelsTo interpret effect change predictor \\(x_{ij}\\) probability event occurring (\\(P(y_i = 1|x_i)\\)), use partial effect:\\[\nE(y_i | x_i) = F_\\epsilon(x_i \\beta),\n\\]\\(F_\\epsilon(.)\\) cumulative distribution function (CDF) error term \\(\\epsilon_i\\) (e.g., standard normal Probit, logistic Logit). partial effect derivative expected probability respect \\(x_{ij}\\):\\[\nPE(x_{ij}) = \\frac{\\partial E(y_i | x_i)}{\\partial x_{ij}} = f_\\epsilon(x_i \\beta) \\beta_j,\n\\]:\\(f_\\epsilon(.)\\) probability density function (PDF) error term \\(\\epsilon_i\\).\\(f_\\epsilon(.)\\) probability density function (PDF) error term \\(\\epsilon_i\\).\\(\\beta_j\\) coefficient associated \\(x_{ij}\\).\\(\\beta_j\\) coefficient associated \\(x_{ij}\\).Key Characteristics Partial EffectsScaling Factor:\npartial effect depends scaling factor, \\(f_\\epsilon(x_i \\beta)\\), derived density function \\(f_\\epsilon(.)\\).\nscaling factor varies depending values \\(x_i\\), making partial effect nonlinear context-dependent.\npartial effect depends scaling factor, \\(f_\\epsilon(x_i \\beta)\\), derived density function \\(f_\\epsilon(.)\\).scaling factor varies depending values \\(x_i\\), making partial effect nonlinear context-dependent.Non-Constant Partial Effects:\nUnlike linear models coefficients directly represent constant marginal effects, partial effect binary models changes based \\(x_i\\).\nexample, Logit model, partial effect largest \\(P(y_i = 1 | x_i)\\) around 0.5 (midpoint S-shaped logistic curve) smaller extremes (close 0 1).\nUnlike linear models coefficients directly represent constant marginal effects, partial effect binary models changes based \\(x_i\\).example, Logit model, partial effect largest \\(P(y_i = 1 | x_i)\\) around 0.5 (midpoint S-shaped logistic curve) smaller extremes (close 0 1).Single Values Partial EffectsIn practice, researchers often summarize partial effects using either:Partial Effect Average (PEA):\npartial effect calculated “average individual,” \\(x_i = \\bar{x}\\) (sample mean predictors): \\[\nPEA = f_\\epsilon(\\bar{x}\\hat{\\beta}) \\hat{\\beta}_j.\n\\]\nprovides single, interpretable value assumes average effect applies individuals.\npartial effect calculated “average individual,” \\(x_i = \\bar{x}\\) (sample mean predictors): \\[\nPEA = f_\\epsilon(\\bar{x}\\hat{\\beta}) \\hat{\\beta}_j.\n\\]provides single, interpretable value assumes average effect applies individuals.Average Partial Effect (APE):\naverage individual-level partial effects across sample: \\[\nAPE = \\frac{1}{n} \\sum_{=1}^{n} f_\\epsilon(x_i \\hat{\\beta}) \\hat{\\beta}_j.\n\\]\naccounts nonlinearity partial effects provides accurate summary marginal effect population.\naverage individual-level partial effects across sample: \\[\nAPE = \\frac{1}{n} \\sum_{=1}^{n} f_\\epsilon(x_i \\hat{\\beta}) \\hat{\\beta}_j.\n\\]accounts nonlinearity partial effects provides accurate summary marginal effect population.Comparing Partial Effects Linear Nonlinear ModelsLinear Models:\nPartial effects constant: \\(APE = PEA\\).\ncoefficients directly represent marginal effects \\(E(y_i | x_i)\\).\nPartial effects constant: \\(APE = PEA\\).coefficients directly represent marginal effects \\(E(y_i | x_i)\\).Nonlinear Models:\nPartial effects constant due dependence \\(f_\\epsilon(x_i \\beta)\\).\nresult, \\(APE \\neq PEA\\) general.\nPartial effects constant due dependence \\(f_\\epsilon(x_i \\beta)\\).result, \\(APE \\neq PEA\\) general.","code":""},{"path":"linear-regression.html","id":"penalized-regularized-estimators","chapter":"5 Linear Regression","heading":"5.4 Penalized (Regularized) Estimators","text":"Penalized regularized estimators extensions Ordinary Least Squares designed address limitations, particularly high-dimensional settings. Regularization methods introduce penalty term loss function prevent overfitting, handle multicollinearity, improve model interpretability.three popular regularization techniques (limited ):Ridge RegressionLasso RegressionElastic Net","code":""},{"path":"linear-regression.html","id":"motivation-for-penalized-estimators","chapter":"5 Linear Regression","heading":"5.4.1 Motivation for Penalized Estimators","text":"OLS minimizes Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n \\left( y_i - \\hat{y}_i \\right)^2 = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2,\n\\]:\\(y_i\\) observed outcome,\\(y_i\\) observed outcome,\\(x_i\\) vector predictors observation \\(\\),\\(x_i\\) vector predictors observation \\(\\),\\(\\beta\\) vector coefficients.\\(\\beta\\) vector coefficients.OLS works well ideal conditions (e.g., low dimensionality, multicollinearity), struggles :Multicollinearity: Predictors highly correlated, leading large variances \\(\\beta\\) estimates.Multicollinearity: Predictors highly correlated, leading large variances \\(\\beta\\) estimates.High Dimensionality: number predictors (\\(p\\)) exceeds approaches sample size (\\(n\\)), making OLS inapplicable unstable.High Dimensionality: number predictors (\\(p\\)) exceeds approaches sample size (\\(n\\)), making OLS inapplicable unstable.Overfitting: \\(p\\) large, OLS fits noise data, reducing generalizability.Overfitting: \\(p\\) large, OLS fits noise data, reducing generalizability.address issues, penalized regression modifies OLS loss function adding penalty term shrinks coefficients toward zero. discourages overfitting improves predictive performance.general form penalized loss function :\\[\nL(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda P(\\beta),\n\\]:\\(\\lambda \\geq 0\\): Tuning parameter controlling strength regularization.\\(\\lambda \\geq 0\\): Tuning parameter controlling strength regularization.\\(P(\\beta)\\): Penalty term quantifies model complexity.\\(P(\\beta)\\): Penalty term quantifies model complexity.Different choices \\(P(\\beta)\\) lead ridge regression, lasso regression, elastic net.","code":""},{"path":"linear-regression.html","id":"ridge-regression","chapter":"5 Linear Regression","heading":"5.4.2 Ridge Regression","text":"Ridge regression, also known L2 regularization, penalizes sum squared coefficients:\\[\nP(\\beta) = \\sum_{j=1}^p \\beta_j^2.\n\\]ridge objective function becomes:\\[\nL_{ridge}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\]:\\(\\lambda \\geq 0\\) controls degree shrinkage. Larger \\(\\lambda\\) leads greater shrinkage.Ridge regression closed-form solution:\\[\n\\hat{\\beta}_{ridge} = \\left( X'X + \\lambda \\right)^{-1} X'y,\n\\]\\(\\) \\(p \\times p\\) identity matrix.Key FeaturesShrinks coefficients set exactly zero.Handles multicollinearity effectively stabilizing coefficient estimates (Hoerl Kennard 1970).Works well predictors contribute response.Example Use CaseRidge regression ideal applications many correlated predictors, :Predicting housing prices based large set features (e.g., size, location, age house).","code":""},{"path":"linear-regression.html","id":"lasso-regression","chapter":"5 Linear Regression","heading":"5.4.3 Lasso Regression","text":"Lasso regression, L1 regularization, penalizes sum absolute coefficients:\\[\nP(\\beta) = \\sum_{j=1}^p |\\beta_j|.\n\\]lasso objective function :\\[\nL_{lasso}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]Key FeaturesUnlike ridge regression, lasso can set coefficients exactly zero, performing automatic feature selection.Encourages sparse models, making suitable high-dimensional data (Tibshirani 1996).OptimizationLasso closed-form solution due non-differentiability \\(|\\beta_j|\\) \\(\\beta_j = 0\\). requires iterative algorithms, :Coordinate Descent,Coordinate Descent,Least Angle Regression (LARS).Least Angle Regression (LARS).Example Use CaseLasso regression useful many predictors irrelevant, :Genomics, subset genes associated disease outcome.","code":""},{"path":"linear-regression.html","id":"elastic-net","chapter":"5 Linear Regression","heading":"5.4.4 Elastic Net","text":"Elastic Net combines penalties ridge lasso regression:\\[\nP(\\beta) = \\alpha \\sum_{j=1}^p |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^p \\beta_j^2,\n\\]:\\(0 \\leq \\alpha \\leq 1\\) determines balance lasso (L1) ridge (L2) penalties.\\(0 \\leq \\alpha \\leq 1\\) determines balance lasso (L1) ridge (L2) penalties.\\(\\lambda\\) controls overall strength regularization.\\(\\lambda\\) controls overall strength regularization.elastic net objective function :\\[\nL_{elastic\\ net}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^p |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^p \\beta_j^2 \\right).\n\\]Key FeaturesCombines strengths lasso (sparse models) ridge (stability correlated predictors) (H. Zou Hastie 2005).Effective predictors highly correlated \\(p > n\\).Example Use CaseElastic net ideal high-dimensional datasets correlated predictors, :Predicting customer churn using demographic behavioral features.","code":""},{"path":"linear-regression.html","id":"tuning-parameter-selection","chapter":"5 Linear Regression","heading":"5.4.5 Tuning Parameter Selection","text":"Choosing regularization parameter \\(\\lambda\\) (\\(\\alpha\\) elastic net) critical balancing model complexity (fit) regularization (parsimony). \\(\\lambda\\) large, coefficients overly shrunk (even set zero case L1 penalty), leading underfitting. \\(\\lambda\\) small, model might overfit coefficients penalized sufficiently. Hence, systematic approach needed determine optimal \\(\\lambda\\). elastic net, also choose appropriate \\(\\alpha\\) balance L1 L2 penalties.","code":""},{"path":"linear-regression.html","id":"cross-validation","chapter":"5 Linear Regression","heading":"5.4.5.1 Cross-Validation","text":"common approach selecting \\(\\lambda\\) (\\(\\alpha\\)) \\(K\\)-Fold Cross-Validation:Partition data \\(K\\) roughly equal-sized “folds.”Train model \\(K-1\\) folds validate remaining fold, computing validation error.Repeat process folds, compute average validation error across \\(K\\) folds.Select value \\(\\lambda\\) (\\(\\alpha\\) tuning ) minimizes cross-validated error.method helps us maintain good bias-variance trade-every point used training validation exactly .","code":""},{"path":"linear-regression.html","id":"information-criteria","chapter":"5 Linear Regression","heading":"5.4.5.2 Information Criteria","text":"Alternatively, one can use information criteria—like Akaike Information Criterion (AIC) Bayesian Information Criterion (BIC)—guide model selection. criteria reward goodness--fit penalizing model complexity, thereby helping selecting appropriately regularized model.","code":""},{"path":"linear-regression.html","id":"properties-of-penalized-estimators","chapter":"5 Linear Regression","heading":"5.4.6 Properties of Penalized Estimators","text":"Bias-Variance Tradeoff:\nRegularization introduces bias exchange reducing variance, often resulting better predictive performance new data.\nRegularization introduces bias exchange reducing variance, often resulting better predictive performance new data.Shrinkage:\nRidge shrinks coefficients toward zero usually retains predictors.\nLasso shrinks coefficients exactly zero, performing inherent feature selection.\nRidge shrinks coefficients toward zero usually retains predictors.Lasso shrinks coefficients exactly zero, performing inherent feature selection.Flexibility:\nElastic net allows continuum ridge lasso, can adapt different data structures (e.g., many correlated features high-dimensional feature spaces).\nElastic net allows continuum ridge lasso, can adapt different data structures (e.g., many correlated features high-dimensional feature spaces).plot, curve represents coefficient’s value function \\(\\lambda\\).plot, curve represents coefficient’s value function \\(\\lambda\\).\\(\\lambda\\) increases (moving left right log-scale default), coefficients shrink toward zero typically stay non-zero.\\(\\lambda\\) increases (moving left right log-scale default), coefficients shrink toward zero typically stay non-zero.Ridge regression tends shrink coefficients force exactly zero.Ridge regression tends shrink coefficients force exactly zero., \\(\\lambda\\) grows, several coefficient paths hit zero exactly, illustrating variable selection property lasso.Elastic net combines ridge lasso penalties. \\(\\lambda = 0.5\\), see partial shrinkage coefficients going zero.Elastic net combines ridge lasso penalties. \\(\\lambda = 0.5\\), see partial shrinkage coefficients going zero.model often helpful suspect group-wise shrinkage (like ridge) sparse solutions (like lasso) might beneficial.model often helpful suspect group-wise shrinkage (like ridge) sparse solutions (like lasso) might beneficial.can refine choice \\(\\lambda\\) performing cross-validation lasso model:plot displays cross-validated error (often mean-squared error deviance) y-axis versus \\(\\log(\\lambda)\\) x-axis.plot displays cross-validated error (often mean-squared error deviance) y-axis versus \\(\\log(\\lambda)\\) x-axis.Two vertical dotted lines typically appear:\n\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\n\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).\nTwo vertical dotted lines typically appear:\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).best_lambda prints numeric value \\(\\lambda.min\\). \\(\\lambda\\) gave lowest cross-validation error lasso model.best_lambda prints numeric value \\(\\lambda.min\\). \\(\\lambda\\) gave lowest cross-validation error lasso model.Interpretation:using cv.glmnet, systematically compare different values \\(\\lambda\\) terms predictive performance (cross-validation error).using cv.glmnet, systematically compare different values \\(\\lambda\\) terms predictive performance (cross-validation error).selected \\(\\lambda\\) typically balances smaller model (due regularization) retaining sufficient predictive power.selected \\(\\lambda\\) typically balances smaller model (due regularization) retaining sufficient predictive power.used real-world data, might also look performance metrics hold-test set ensure chosen \\(\\lambda\\) generalizes well.used real-world data, might also look performance metrics hold-test set ensure chosen \\(\\lambda\\) generalizes well.","code":"\n# Load required libraries\nlibrary(glmnet)\n\n# Simulate data\nset.seed(123)\nn <- 100   # Number of observations\np <- 20    # Number of predictors\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)  # Predictor matrix\ny <- rnorm(n)                                 # Response vector\n\n# Ridge regression (alpha = 0)\nridge_fit <- glmnet(X, y, alpha = 0)\nplot(ridge_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Ridge Regression\")\n# Lasso regression (alpha = 1)\nlasso_fit <- glmnet(X, y, alpha = 1)\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Lasso Regression\")\n# Elastic net (alpha = 0.5)\nelastic_net_fit <- glmnet(X, y, alpha = 0.5)\nplot(elastic_net_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Elastic Net (alpha = 0.5)\")\ncv_lasso <- cv.glmnet(X, y, alpha = 1)\nplot(cv_lasso)\nbest_lambda <- cv_lasso$lambda.min\nbest_lambda\n#> [1] 0.1449586"},{"path":"linear-regression.html","id":"robust-estimators","chapter":"5 Linear Regression","heading":"5.5 Robust Estimators","text":"Robust estimators statistical techniques designed provide reliable parameter estimates even assumptions underlying classical methods, Ordinary Least Squares, violated. Specifically, address issues caused outliers, non-normal errors, heavy-tailed distributions, can render OLS inefficient biased.goal robust estimation reduce sensitivity estimator extreme aberrant data points, thereby ensuring reliable accurate fit majority data.cover key robust estimation techniques, properties, applications, along practical examples mathematical derivations. focus include \\(M\\)-estimators, \\(R\\)-estimators, \\(L\\)-estimators, \\(LTS\\), \\(S\\)-estimators, \\(MM\\)-estimators, .","code":""},{"path":"linear-regression.html","id":"motivation-for-robust-estimation","chapter":"5 Linear Regression","heading":"5.5.1 Motivation for Robust Estimation","text":"OLS seeks minimize Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n (y_i - x_i'\\beta)^2,\n\\]:\\(y_i\\) observed response \\(\\)th observation,\\(x_i\\) vector predictors \\(\\)th observation,\\(\\beta\\) vector coefficients.OLS assumes:Errors normally distributed outliers data (A6 Normal Distribution).Homoscedasticity (constant variance errors) (A4 Homoskedasticity).real-world scenarios:Outliers \\(y\\) \\(x\\) can disproportionately affect estimates, leading biased inefficient results.Outliers \\(y\\) \\(x\\) can disproportionately affect estimates, leading biased inefficient results.Heavy-tailed distributions (e.g., Cauchy) violate normality assumption, making OLS inappropriate.Heavy-tailed distributions (e.g., Cauchy) violate normality assumption, making OLS inappropriate.example, P. J. Huber (1964) demonstrates single extreme observation can arbitrarily distort OLS estimates, Hampel et al. (2005) define breakdown point measure robustness. Robust estimators aim mitigate problems limiting influence problematic observations.OLS inherently squares residuals \\(e_i = y_i - x_i'\\beta\\), amplifying influence large residuals. example, single residual much larger others, squared value can dominate RSS, distorting estimated coefficients.Consider simple case \\(y_i = \\beta_0 + \\beta_1 x_i + e_i\\), \\(e_i \\sim N(0, \\sigma^2)\\) classical assumptions. Now introduce outlier: single observation unusually large \\(e_i\\). squared residual point dominate RSS pull estimated regression line towards , leading biased estimates \\(\\beta_0\\) \\(\\beta_1\\).breakdown point estimator proportion contamination (e.g., outliers) estimator can tolerate yielding arbitrarily large incorrect results. OLS, breakdown point \\(1/n\\), meaning even one outlier can cause substantial distortion estimates.","code":""},{"path":"linear-regression.html","id":"m-estimators","chapter":"5 Linear Regression","heading":"5.5.2 \\(M\\)-Estimators","text":"address sensitivity OLS, robust estimators minimize different objective function:\\[\n\\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]:\\(\\rho(\\cdot)\\) robust loss function grows slower quadratic function used OLS,\\(\\sigma\\) scale parameter normalize residuals.OLS, quadratic loss function \\(\\rho(z) = z^2\\) penalizes large residuals disproportionately. Robust estimators replace alternative \\(\\rho\\) functions limit penalty large residuals, thus reducing influence parameter estimates.robust \\(\\rho\\) function satisfy following properties:Bounded Influence: Large residuals contribute finite amount objective function.Symmetry: \\(\\rho(z) = \\rho(-z)\\) ensures positive negative residuals treated equally.Differentiability: computational tractability, \\(\\rho\\) smooth differentiable.","code":""},{"path":"linear-regression.html","id":"examples-of-robust-rho-functions","chapter":"5 Linear Regression","heading":"5.5.2.1 Examples of Robust \\(\\rho\\) Functions","text":"Huber’s Loss Function (P. J. Huber 1964)Huber’s loss function transitions quadratic linear growth:\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Key features:small residuals (\\(|z| \\leq c\\)), loss quadratic, mimicking OLS.large residuals (\\(|z| > c\\)), loss grows linearly, limiting influence.parameter \\(c\\) controls threshold loss function transitions quadratic linear. Smaller values \\(c\\) make estimator robust potentially less efficient normality.Tukey’s Bisquare Function (Beaton Tukey 1974)Tukey’s bisquare function completely bounds influence large residuals:\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Key features:Residuals larger \\(c\\) contribute constant value objective function, effectively excluding estimation process.approach achieves high robustness cost lower efficiency small residuals.Andrews’ Sine Function (D. F. Andrews 1974):\nSmoothly downweights extreme residuals: \\[ \\rho(z) = \\begin{cases}  c^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\ c^2/2 & \\text{} |z| > \\pi c. \\end{cases} \\]\nSmoothly downweights extreme residuals: \\[ \\rho(z) = \\begin{cases}  c^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\ c^2/2 & \\text{} |z| > \\pi c. \\end{cases} \\]","code":""},{"path":"linear-regression.html","id":"weighting-scheme-influence-functions","chapter":"5 Linear Regression","heading":"5.5.2.2 Weighting Scheme: Influence Functions","text":"critical concept robust estimation influence function, describes sensitivity estimator individual observations. \\(M\\)-estimators, influence function derived derivative loss function \\(\\rho(z)\\) respect \\(z\\):\\[\n\\psi(z) = \\frac{d}{dz} \\rho(z).\n\\]function plays crucial role downweighting large residuals. weight assigned residual proportional \\(\\psi(z)/z\\), decreases \\(|z|\\) increases robust estimators.Huber’s loss function, influence function :\\[\n\\psi(z) =\n\\begin{cases}\nz & \\text{} |z| \\leq c, \\\\\nc \\cdot \\text{sign}(z) & \\text{} |z| > c.\n\\end{cases}\n\\]small residuals, \\(\\psi(z) = z\\), matching OLS.large residuals, \\(\\psi(z)\\) constant, ensuring bounded influence.key consideration selecting robust estimator trade-robustness (resistance outliers) efficiency (performance ideal conditions). tuning parameters \\(\\rho\\) functions (e.g., \\(c\\) Huber’s loss) directly affect balance:Smaller \\(c\\) increases robustness reduces efficiency normality.Larger \\(c\\) improves efficiency normality decreases robustness outliers.trade-reflects fundamental goal robust estimation: achieve balance reliability precision across wide range data scenarios.","code":""},{"path":"linear-regression.html","id":"properties-of-m-estimators","chapter":"5 Linear Regression","heading":"5.5.2.3 Properties of \\(M\\)-Estimators","text":"Robust estimators, particularly \\(M\\)-estimators, possess following mathematical properties:Asymptotic Normality: mild regularity conditions, \\(M\\)-estimators asymptotically normal: \\[\n\\sqrt{n} (\\hat{\\beta} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\] \\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.Consistency: \\(n \\\\infty\\), \\(\\hat{\\beta} \\\\beta\\) probability, provided majority data satisfies model assumptions.Breakdown Point: \\(M\\)-estimators typically moderate breakdown point, sufficient handle reasonable proportion contamination.","code":""},{"path":"linear-regression.html","id":"r-estimators","chapter":"5 Linear Regression","heading":"5.5.3 \\(R\\)-Estimators","text":"\\(R\\)-estimators class robust estimators rely ranks residuals rather raw magnitudes. approach makes naturally resistant influence outliers highly effective scenarios involving ordinal data heavy-tailed error distributions. leveraging rank-based methods, \\(R\\)-estimators particularly useful situations classical assumptions data, normality homoscedasticity, hold.general form \\(R\\)-estimator can expressed :\\[\n\\hat{\\beta}_R = \\arg\\min_\\beta \\sum_{=1}^n w_i R_i \\left(y_i - x_i'\\beta\\right),\n\\]:\\(R_i\\) ranks residuals \\(e_i = y_i - x_i'\\beta\\),\\(w_i\\) rank-based weights determined chosen scoring function,\\(y_i\\) observed responses, \\(x_i\\) predictor values, \\(\\beta\\) vector coefficients.formulation differs \\(M\\)-estimators, directly minimize loss function \\(\\rho\\), instead using ordering residuals drive estimation.","code":""},{"path":"linear-regression.html","id":"ranks-and-scoring-function","chapter":"5 Linear Regression","heading":"5.5.3.1 Ranks and Scoring Function","text":"","code":""},{"path":"linear-regression.html","id":"definition-of-ranks","chapter":"5 Linear Regression","heading":"5.5.3.1.1 Definition of Ranks","text":"rank \\(R_i\\) residual \\(e_i\\) position sorted sequence residuals:\\[\nR_i = \\text{rank}(e_i) = \\sum_{j=1}^n \\mathbb{}(e_j \\leq e_i),\n\\]\\(\\mathbb{}(\\cdot)\\) indicator function, equal 1 condition true 0 otherwise. step transforms residuals ordinal scale, eliminating dependency magnitude.","code":""},{"path":"linear-regression.html","id":"scoring-function","chapter":"5 Linear Regression","heading":"5.5.3.1.2 Scoring Function","text":"weights \\(w_i\\) derived scoring function \\(S(R_i)\\), assigns importance rank. common choice Wilcoxon scoring function, defined :\\[\nS(R_i) = \\frac{R_i}{n + 1},\n\\]gives equal weight ranks, scaled position relative total number observations \\(n\\).scoring functions can emphasize different parts rank distribution:Normal Scores: Derived quantiles standard normal distribution.Logarithmic Scores: Weight lower ranks heavily.flexibility scoring function allows \\(R\\)-estimators adapt various data structures assumptions.","code":""},{"path":"linear-regression.html","id":"properties-of-r-estimators","chapter":"5 Linear Regression","heading":"5.5.3.2 Properties of \\(R\\)-Estimators","text":"","code":""},{"path":"linear-regression.html","id":"influence-function-and-robustness","chapter":"5 Linear Regression","heading":"5.5.3.2.1 Influence Function and Robustness","text":"key feature \\(R\\)-estimators bounded influence function, ensures robustness. estimator depends ranks residuals, extreme values \\(y\\) \\(x\\) disproportionately affect results.\\(R\\)-estimators, influence function \\(\\psi(e_i)\\) proportional derivative rank-based objective function:\\[\n\\psi(e_i) = S'(R_i),\n\\]\\(S'(R_i)\\) derivative scoring function. Since \\(R_i\\) depends ordering residuals, outliers data produce excessive changes \\(R_i\\), resulting bounded influence.","code":""},{"path":"linear-regression.html","id":"breakdown-point","chapter":"5 Linear Regression","heading":"5.5.3.2.2 Breakdown Point","text":"breakdown point \\(R\\)-estimators higher OLS comparable robust methods. means can tolerate larger proportion contaminated data without yielding unreliable results.","code":""},{"path":"linear-regression.html","id":"asymptotic-efficiency","chapter":"5 Linear Regression","heading":"5.5.3.2.3 Asymptotic Efficiency","text":"specific scoring functions, \\(R\\)-estimators achieve high asymptotic efficiency. example, Wilcoxon \\(R\\)-estimator performs nearly well OLS normality retaining robustness non-normality.","code":""},{"path":"linear-regression.html","id":"derivation-of-r-estimators-for-simple-linear-regression","chapter":"5 Linear Regression","heading":"5.5.3.3 Derivation of \\(R\\)-Estimators for Simple Linear Regression","text":"Consider simple linear regression model:\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i,\n\\]\\(e_i = y_i - (\\beta_0 + \\beta_1 x_i)\\) residuals.Rank Residuals: Compute residuals \\(e_i\\) observations rank smallest largest.Rank Residuals: Compute residuals \\(e_i\\) observations rank smallest largest.Assign Weights: Compute weights \\(w_i\\) residual rank based scoring function \\(S(R_i)\\).Assign Weights: Compute weights \\(w_i\\) residual rank based scoring function \\(S(R_i)\\).Minimize Rank-Based Objective: Solve following optimization problem:\n\\[\n\\hat{\\beta}_R = \\arg\\min_{\\beta_0, \\beta_1} \\sum_{=1}^n w_i R_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right).\n\\]\nminimization can performed iteratively using numerical methods, rank-based nature function makes direct analytic solutions challenging.Minimize Rank-Based Objective: Solve following optimization problem:\\[\n\\hat{\\beta}_R = \\arg\\min_{\\beta_0, \\beta_1} \\sum_{=1}^n w_i R_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right).\n\\]minimization can performed iteratively using numerical methods, rank-based nature function makes direct analytic solutions challenging.","code":""},{"path":"linear-regression.html","id":"comparison-to-m-estimators","chapter":"5 Linear Regression","heading":"5.5.3.4 Comparison to \\(M\\)-Estimators","text":"\\(M\\)-estimators downweight large residuals using robust loss functions, \\(R\\)-estimators completely avoid reliance magnitude residuals using ranks. distinction important implications:\\(R\\)-estimators naturally robust leverage points extreme outliers.performance \\(R\\)-estimators less sensitive choice scale parameter compared \\(M\\)-estimators.However, \\(R\\)-estimators may less efficient \\(M\\)-estimators normality use full information contained residual magnitudes.","code":""},{"path":"linear-regression.html","id":"l-estimators","chapter":"5 Linear Regression","heading":"5.5.4 \\(L\\)-Estimators","text":"\\(L\\)-estimators class robust estimators constructed linear combinations order statistics, order statistics simply sorted values dataset. estimators particularly appealing due intuitive nature computational simplicity. using relative ranks observations, \\(L\\)-estimators offer robustness outliers heavy-tailed distributions.Order statistics denoted \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\), \\(y_{()}\\) \\(\\)th smallest observation sample.general form \\(L\\)-estimator :\\[\n\\hat{\\theta}_L = \\sum_{=1}^n c_i y_{()},\n\\]:\\(y_{()}\\) order statistics (sorted observations),\\(c_i\\) coefficients (weights) determine contribution order statistic estimator.appropriately choosing weights \\(c_i\\), different types \\(L\\)-estimators can constructed suit specific needs, handling outliers capturing central tendencies robustly.Examples \\(L\\)-EstimatorsSample Median: sample median simple \\(L\\)-estimator middle order statistic contributes (odd \\(n\\)) average two middle order statistics contributes (even \\(n\\)):\n\\[\n\\hat{\\mu}_{\\text{median}} =\n\\begin{cases}\ny_{\\left(\\frac{n+1}{2}\\right)} & \\text{} n \\text{ odd}, \\\\\n\\frac{1}{2}\\left(y_{\\left(\\frac{n}{2}\\right)} + y_{\\left(\\frac{n}{2} + 1\\right)}\\right) & \\text{} n \\text{ even}.\n\\end{cases}\n\\]\nRobustness: median breakdown point \\(50\\%\\), meaning remains unaffected unless half data corrupted.\nEfficiency: normality, efficiency median lower mean (\\(64\\%\\)).\nSample Median: sample median simple \\(L\\)-estimator middle order statistic contributes (odd \\(n\\)) average two middle order statistics contributes (even \\(n\\)):\\[\n\\hat{\\mu}_{\\text{median}} =\n\\begin{cases}\ny_{\\left(\\frac{n+1}{2}\\right)} & \\text{} n \\text{ odd}, \\\\\n\\frac{1}{2}\\left(y_{\\left(\\frac{n}{2}\\right)} + y_{\\left(\\frac{n}{2} + 1\\right)}\\right) & \\text{} n \\text{ even}.\n\\end{cases}\n\\]Robustness: median breakdown point \\(50\\%\\), meaning remains unaffected unless half data corrupted.Efficiency: normality, efficiency median lower mean (\\(64\\%\\)).Trimmed Mean: trimmed mean excludes smallest largest \\(k\\%\\) observations averaging remaining values:\n\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()},\n\\]\n:\n\\(k\\) number observations trimmed tail,\n\\(n\\) sample size.\nRobustness: trimmed mean less sensitive extreme values sample mean.\nEfficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.\nTrimmed Mean: trimmed mean excludes smallest largest \\(k\\%\\) observations averaging remaining values:\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()},\n\\]:\\(k\\) number observations trimmed tail,\\(k\\) number observations trimmed tail,\\(n\\) sample size.\\(n\\) sample size.Robustness: trimmed mean less sensitive extreme values sample mean.Robustness: trimmed mean less sensitive extreme values sample mean.Efficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.Efficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.Winsorized Mean: Similar trimmed mean, instead excluding extreme values, replaces nearest remaining observations:\n\\[\n\\hat{\\mu}_W = \\frac{1}{n} \\sum_{=1}^n y_{()}^*,\n\\]\n\\(y_{()}^*\\) “Winsorized” values: \\[\ny_{()}^* =\n\\begin{cases}\ny_{(k+1)} & \\text{} \\leq k, \\\\\ny_{()} & \\text{} k+1 \\leq \\leq n-k, \\\\\ny_{(n-k)} & \\text{} > n-k.\n\\end{cases}\n\\]\nRobustness: Winsorized mean reduces influence outliers without discarding data.\nEfficiency: Slightly less efficient trimmed mean normality.\nWinsorized Mean: Similar trimmed mean, instead excluding extreme values, replaces nearest remaining observations:\\[\n\\hat{\\mu}_W = \\frac{1}{n} \\sum_{=1}^n y_{()}^*,\n\\]\\(y_{()}^*\\) “Winsorized” values: \\[\ny_{()}^* =\n\\begin{cases}\ny_{(k+1)} & \\text{} \\leq k, \\\\\ny_{()} & \\text{} k+1 \\leq \\leq n-k, \\\\\ny_{(n-k)} & \\text{} > n-k.\n\\end{cases}\n\\]Robustness: Winsorized mean reduces influence outliers without discarding data.Efficiency: Slightly less efficient trimmed mean normality.Midrange: midrange average smallest largest observations:\n\\[\n\\hat{\\mu}_{\\text{midrange}} = \\frac{y_{(1)} + y_{(n)}}{2}.\n\\]\nRobustness: Poor robustness, depends entirely extreme observations.\nSimplicity: Highly intuitive computationally trivial.\nMidrange: midrange average smallest largest observations:\\[\n\\hat{\\mu}_{\\text{midrange}} = \\frac{y_{(1)} + y_{(n)}}{2}.\n\\]Robustness: Poor robustness, depends entirely extreme observations.Simplicity: Highly intuitive computationally trivial.","code":""},{"path":"linear-regression.html","id":"properties-of-l-estimators","chapter":"5 Linear Regression","heading":"5.5.4.1 Properties of \\(L\\)-Estimators","text":"Robustness Outliers: \\(L\\)-estimators gain robustness downweighting excluding extreme observations. instance:\ntrimmed mean completely removes outliers estimation process.\nWinsorized mean limits influence outliers bounding values.\ntrimmed mean completely removes outliers estimation process.Winsorized mean limits influence outliers bounding values.Breakdown Point:\nbreakdown point \\(L\\)-estimator depends many extreme observations excluded replaced.\nmedian highest possible breakdown point (\\(50\\%\\)), trimmed Winsorized means breakdown points proportional trimming percentage.\nbreakdown point \\(L\\)-estimator depends many extreme observations excluded replaced.median highest possible breakdown point (\\(50\\%\\)), trimmed Winsorized means breakdown points proportional trimming percentage.Efficiency:\nefficiency \\(L\\)-estimators varies depending underlying data distribution specific estimator.\nsymmetric distributions, trimmed mean Winsorized mean approach efficiency sample mean much robust.\nefficiency \\(L\\)-estimators varies depending underlying data distribution specific estimator.symmetric distributions, trimmed mean Winsorized mean approach efficiency sample mean much robust.Computational Simplicity:\n\\(L\\)-estimators involve simple operations like sorting averaging, making computationally efficient even large datasets.\n\\(L\\)-estimators involve simple operations like sorting averaging, making computationally efficient even large datasets.","code":""},{"path":"linear-regression.html","id":"derivation-of-the-trimmed-mean","chapter":"5 Linear Regression","heading":"5.5.4.2 Derivation of the Trimmed Mean","text":"understand robustness trimmed mean, consider dataset \\(n\\) observations. Sorting data gives \\(y_{(1)} \\leq y_{(2)} \\leq \\dots \\leq y_{(n)}\\). trimming smallest \\(k\\) largest \\(k\\) observations, remaining \\(n - 2k\\) observations used compute mean:\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()}.\n\\]Key observations:Impact \\(k\\): Larger \\(k\\) increases robustness removing extreme values reduces efficiency discarding data.Choosing \\(k\\): practice, \\(k\\) often chosen percentage total sample size, \\(10\\%\\) trimming (\\(k = 0.1n\\)).","code":""},{"path":"linear-regression.html","id":"least-trimmed-squares-lts","chapter":"5 Linear Regression","heading":"5.5.5 Least Trimmed Squares (LTS)","text":"Least Trimmed Squares (LTS) robust regression method minimizes sum smallest \\(h\\) squared residuals, rather using residuals Ordinary Least Squares. approach ensures large residuals, often caused outliers leverage points, influence parameter estimation.LTS estimator defined :\\[\n\\hat{\\beta}_{LTS} = \\arg\\min_\\beta \\sum_{=1}^h r_{[]}^2,\n\\]:\\(r_{[]}^2\\) ordered squared residuals, ranked smallest largest,\\(h\\) subset size residuals include minimization, typically chosen \\(h = \\lfloor n/2 \\rfloor + 1\\) (\\(n\\) sample size).trimming process ensures robustness focusing best-fitting \\(h\\) observations ignoring extreme residuals.","code":""},{"path":"linear-regression.html","id":"motivation-for-lts","chapter":"5 Linear Regression","heading":"5.5.5.1 Motivation for LTS","text":"OLS regression, objective minimize Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n r_i^2,\n\\]\\(r_i = y_i - x_i'\\beta\\) residuals. However, method highly sensitive outliers even one large residual (\\(r_i^2\\)) can dominate RSS, distorting parameter estimates \\(\\beta\\).LTS addresses issue trimming largest residuals focusing \\(h\\) smallest ones, thus preventing extreme values affecting fit. approach provides robust estimate regression coefficients \\(\\beta\\).","code":""},{"path":"linear-regression.html","id":"properties-of-lts","chapter":"5 Linear Regression","heading":"5.5.5.2 Properties of LTS","text":"Objective Function: LTS objective function non-differentiable involves ordering squared residuals. Formally, ordered residuals denoted :\n\\[\nr_{[1]}^2 \\leq r_{[2]}^2 \\leq \\dots \\leq r_{[n]}^2,\n\\]\nobjective minimize:\n\\[\n\\sum_{=1}^h r_{[]}^2.\n\\]\nrequires sorting squared residuals, making computation complex OLS.Objective Function: LTS objective function non-differentiable involves ordering squared residuals. Formally, ordered residuals denoted :\\[\nr_{[1]}^2 \\leq r_{[2]}^2 \\leq \\dots \\leq r_{[n]}^2,\n\\]objective minimize:\\[\n\\sum_{=1}^h r_{[]}^2.\n\\]requires sorting squared residuals, making computation complex OLS.Choice \\(h\\): parameter \\(h\\) determines number residuals included minimization. common choice :\n\\[\nh = \\lfloor n/2 \\rfloor + 1,\n\\]\nensures high breakdown point (discussed ). Smaller values \\(h\\) increase robustness reduce efficiency, larger \\(h\\) values improve efficiency decrease robustness.Choice \\(h\\): parameter \\(h\\) determines number residuals included minimization. common choice :\\[\nh = \\lfloor n/2 \\rfloor + 1,\n\\]ensures high breakdown point (discussed ). Smaller values \\(h\\) increase robustness reduce efficiency, larger \\(h\\) values improve efficiency decrease robustness.Breakdown Point: LTS breakdown point approximately \\(50\\%\\), highest possible regression estimator. means LTS can handle \\(50\\%\\) contaminated data (e.g., outliers) without yielding unreliable estimates.Breakdown Point: LTS breakdown point approximately \\(50\\%\\), highest possible regression estimator. means LTS can handle \\(50\\%\\) contaminated data (e.g., outliers) without yielding unreliable estimates.Robustness: focusing \\(h\\) best-fitting observations, LTS naturally excludes outliers estimation process, making highly robust vertical outliers (extreme values \\(y\\)) leverage points (extreme values \\(x\\)).Robustness: focusing \\(h\\) best-fitting observations, LTS naturally excludes outliers estimation process, making highly robust vertical outliers (extreme values \\(y\\)) leverage points (extreme values \\(x\\)).","code":""},{"path":"linear-regression.html","id":"algorithm-for-lts","chapter":"5 Linear Regression","heading":"5.5.5.3 Algorithm for LTS","text":"Computing LTS estimator involves following steps:Initialization: Select initial subset \\(h\\) observations compute preliminary fit \\(\\beta\\).Initialization: Select initial subset \\(h\\) observations compute preliminary fit \\(\\beta\\).Residual Calculation: observation, compute squared residuals:\n\\[\nr_i^2 = \\left(y_i - x_i'\\beta\\right)^2.\n\\]Residual Calculation: observation, compute squared residuals:\\[\nr_i^2 = \\left(y_i - x_i'\\beta\\right)^2.\n\\]Trimming: Rank residuals smallest largest retain \\(h\\) smallest residuals.Trimming: Rank residuals smallest largest retain \\(h\\) smallest residuals.Refitting: Use \\(h\\) retained observations recompute regression coefficients \\(\\beta\\).Refitting: Use \\(h\\) retained observations recompute regression coefficients \\(\\beta\\).Iterative Refinement: Repeat process (residual calculation, trimming, refitting) convergence, typically \\(\\beta\\) stabilizes.Iterative Refinement: Repeat process (residual calculation, trimming, refitting) convergence, typically \\(\\beta\\) stabilizes.Efficient algorithms, Fast-LTS algorithm, used practice reduce computational complexity.","code":""},{"path":"linear-regression.html","id":"comparison-of-lts-with-ols","chapter":"5 Linear Regression","heading":"5.5.5.4 Comparison of LTS with OLS","text":"","code":""},{"path":"linear-regression.html","id":"s-estimators","chapter":"5 Linear Regression","heading":"5.5.6 \\(S\\)-Estimators","text":"\\(S\\)-estimators class robust estimators focus minimizing robust measure dispersion residuals. Unlike methods \\(M\\)-estimators, directly minimize loss function based residuals, \\(S\\)-estimators aim find parameter values \\(\\beta\\) produce residuals smallest robust scale. estimators particularly useful handling datasets outliers, heavy-tailed distributions, violations classical assumptions.scale \\(\\sigma\\) estimated solving following minimization problem:\\[\n\\hat{\\sigma}_S = \\arg\\min_\\sigma \\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]:\\(\\rho\\) robust loss function controls influence residuals,\\(y_i\\) observed responses, \\(x_i\\) predictors, \\(\\beta\\) vector regression coefficients,\\(\\sigma\\) represents robust scale residuals.\\(\\sigma\\) estimated, \\(S\\)-estimator \\(\\beta\\) obtained solving:\\[\n\\hat{\\beta}_S = \\arg\\min_\\beta \\hat{\\sigma}_S.\n\\]","code":""},{"path":"linear-regression.html","id":"motivation-for-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.1 Motivation for \\(S\\)-Estimators","text":"regression analysis, classical methods Ordinary Least Squares rely minimizing Residual Sum Squares (RSS). However, OLS highly sensitive outliers even single extreme residual can dominate sum squared residuals, leading biased estimates \\(\\beta\\).\\(S\\)-estimators address limitation using robust scale \\(\\sigma\\) evaluate dispersion residuals. minimizing scale, \\(S\\)-estimators effectively downweight influence outliers, resulting parameter estimates resistant contamination data.","code":""},{"path":"linear-regression.html","id":"key-concepts-in-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.2 Key Concepts in \\(S\\)-Estimators","text":"Robust Scale Function: key idea \\(S\\)-estimators minimize robust measure scale. scale \\(\\sigma\\) computed residuals normalized \\(\\sigma\\) produce value close expected contribution well-behaved observations.\nFormally, \\(\\sigma\\) satisfies:\n\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta,\n\\]\n\\(\\delta\\) constant depends choice \\(\\rho\\) ensures consistency normality. equation balances residuals controls influence scale estimate.Robust Scale Function: key idea \\(S\\)-estimators minimize robust measure scale. scale \\(\\sigma\\) computed residuals normalized \\(\\sigma\\) produce value close expected contribution well-behaved observations.Formally, \\(\\sigma\\) satisfies:\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta,\n\\]\\(\\delta\\) constant depends choice \\(\\rho\\) ensures consistency normality. equation balances residuals controls influence scale estimate.Choice \\(\\rho\\)-Function: choice robust \\(\\rho\\) function critical determining behavior \\(S\\)-estimators. Common \\(\\rho\\) functions include:\nHuber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]\nTukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]\nAndrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]\nRobust \\(\\rho\\) functions grow slowly quadratic function used OLS, limiting impact large residuals.Choice \\(\\rho\\)-Function: choice robust \\(\\rho\\) function critical determining behavior \\(S\\)-estimators. Common \\(\\rho\\) functions include:Huber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]Huber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Andrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]Andrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]Robust \\(\\rho\\) functions grow slowly quadratic function used OLS, limiting impact large residuals.","code":""},{"path":"linear-regression.html","id":"properties-of-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.3 Properties of \\(S\\)-Estimators","text":"Breakdown Point: \\(S\\)-estimators breakdown point \\(50\\%\\), meaning can tolerate half data contaminated (e.g., outliers) without yielding unreliable estimates.Breakdown Point: \\(S\\)-estimators breakdown point \\(50\\%\\), meaning can tolerate half data contaminated (e.g., outliers) without yielding unreliable estimates.Efficiency: efficiency \\(S\\)-estimators depends choice \\(\\rho\\). highly robust, efficiency ideal conditions (e.g., normality) may lower OLS. Proper tuning \\(\\rho\\) can balance robustness efficiency.Efficiency: efficiency \\(S\\)-estimators depends choice \\(\\rho\\). highly robust, efficiency ideal conditions (e.g., normality) may lower OLS. Proper tuning \\(\\rho\\) can balance robustness efficiency.Influence Function: influence function measures sensitivity estimator small perturbation data. \\(S\\)-estimators, influence function bounded, ensuring robustness outliers.Influence Function: influence function measures sensitivity estimator small perturbation data. \\(S\\)-estimators, influence function bounded, ensuring robustness outliers.Consistency: mild regularity conditions, \\(S\\)-estimators consistent, meaning \\(\\hat{\\beta}_S \\\\beta\\) sample size \\(n \\\\infty\\).Consistency: mild regularity conditions, \\(S\\)-estimators consistent, meaning \\(\\hat{\\beta}_S \\\\beta\\) sample size \\(n \\\\infty\\).Asymptotic Normality: \\(S\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n}(\\hat{\\beta}_S - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.Asymptotic Normality: \\(S\\)-estimators asymptotically normal, :\\[\n\\sqrt{n}(\\hat{\\beta}_S - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.","code":""},{"path":"linear-regression.html","id":"algorithm-for-computing-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.4 Algorithm for Computing \\(S\\)-Estimators","text":"Initial Guess: Compute initial estimate \\(\\beta\\) using robust method (e.g., LTS \\(M\\)-estimator).Initial Guess: Compute initial estimate \\(\\beta\\) using robust method (e.g., LTS \\(M\\)-estimator).Scale Estimation: Compute robust estimate scale \\(\\hat{\\sigma}\\) solving:\n\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta.\n\\]Scale Estimation: Compute robust estimate scale \\(\\hat{\\sigma}\\) solving:\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta.\n\\]Iterative Refinement:\nRecalculate residuals \\(r_i = y_i - x_i'\\beta\\).\nUpdate \\(\\beta\\) \\(\\sigma\\) iteratively convergence, typically using numerical optimization techniques.\nIterative Refinement:Recalculate residuals \\(r_i = y_i - x_i'\\beta\\).Update \\(\\beta\\) \\(\\sigma\\) iteratively convergence, typically using numerical optimization techniques.","code":""},{"path":"linear-regression.html","id":"mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7 \\(MM\\)-Estimators","text":"\\(MM\\)-estimators robust regression method combines strengths two powerful techniques: \\(S\\)-estimators \\(M\\)-estimators. designed achieve high breakdown point (\\(50\\%\\)) high efficiency ideal conditions (e.g., normality). combination makes \\(MM\\)-estimators one versatile widely used robust regression methods.process computing \\(MM\\)-estimators involves three main steps:Compute initial robust estimate scale using \\(S\\)-estimator.Use robust scale define weights \\(M\\)-estimator.Estimate regression coefficients solving weighted \\(M\\)-estimation problem.stepwise approach ensures robustness initial scale estimation leveraging efficiency \\(M\\)-estimators final parameter estimates.Step 1: Robust Scale EstimationThe first step estimate robust scale \\(\\sigma\\) using \\(S\\)-estimator. involves solving:\\[\n\\hat{\\sigma}_S = \\arg\\min_\\sigma \\frac{1}{n} \\sum_{=1}^n \\rho_S\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]\\(\\rho_S\\) robust loss function chosen control influence extreme residuals. Common choices \\(\\rho_S\\) include Huber’s Tukey’s bisquare functions. scale estimation provides robust baseline weighting residuals subsequent \\(M\\)-estimation step.Step 2: Weight Definition \\(M\\)-EstimationUsing robust scale \\(\\hat{\\sigma}_S\\) obtained Step 1, weights \\(M\\)-estimator defined based second loss function, \\(\\rho_M\\). weights downweight residuals proportional deviation relative \\(\\hat{\\sigma}_S\\). residual \\(r_i = y_i - x_i'\\beta\\), weight computed :\\[\nw_i = \\psi_M\\left(\\frac{r_i}{\\hat{\\sigma}_S}\\right) / \\frac{r_i}{\\hat{\\sigma}_S},\n\\]:\\(\\psi_M\\) derivative robust \\(\\rho_M\\) function, known influence function.\\(\\rho_M\\) often chosen provide high efficiency normality, Huber’s Hampel’s function.weights reduce impact large residuals preserving influence small, well-behaved residuals.Step 3: Final \\(M\\)-EstimationThe final step involves solving \\(M\\)-estimation problem using weights defined Step 2. coefficients \\(\\hat{\\beta}_{MM}\\) estimated minimizing weighted residuals:\\[\n\\hat{\\beta}_{MM} = \\arg\\min_\\beta \\sum_{=1}^n w_i \\rho_M\\left(\\frac{y_i - x_i'\\beta}{\\hat{\\sigma}_S}\\right).\n\\]ensures final estimates combine robustness initial \\(S\\)-estimator efficiency \\(M\\)-estimator.","code":""},{"path":"linear-regression.html","id":"properties-of-mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7.1 Properties of \\(MM\\)-Estimators","text":"High Breakdown Point:\n\\(S\\)-estimator first step ensures breakdown point \\(50\\%\\), meaning estimator can handle half data contaminated without producing unreliable results.\n\\(S\\)-estimator first step ensures breakdown point \\(50\\%\\), meaning estimator can handle half data contaminated without producing unreliable results.Asymptotic Efficiency:\nuse efficient \\(\\rho_M\\) function final \\(M\\)-estimation step ensures \\(MM\\)-estimators achieve high asymptotic efficiency normality, often close OLS.\nuse efficient \\(\\rho_M\\) function final \\(M\\)-estimation step ensures \\(MM\\)-estimators achieve high asymptotic efficiency normality, often close OLS.Robustness:\ncombination robust scale estimation downweighting large residuals makes \\(MM\\)-estimators highly robust outliers leverage points.\ncombination robust scale estimation downweighting large residuals makes \\(MM\\)-estimators highly robust outliers leverage points.Influence Function:\ninfluence function \\(MM\\)-estimators bounded, ensuring single observation can exert disproportionate influence parameter estimates.\ninfluence function \\(MM\\)-estimators bounded, ensuring single observation can exert disproportionate influence parameter estimates.Consistency:\n\\(MM\\)-estimators consistent, converging true parameter values sample size increases, provided majority data satisfies model assumptions.\n\\(MM\\)-estimators consistent, converging true parameter values sample size increases, provided majority data satisfies model assumptions.Asymptotic Normality:\n\\(MM\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.\n\\(MM\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.\\(MM\\)-estimators asymptotically normal, :\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.","code":""},{"path":"linear-regression.html","id":"choice-of-rho-functions-for-mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7.2 Choice of \\(\\rho\\)-Functions for \\(MM\\)-Estimators","text":"robustness efficiency \\(MM\\)-estimators depend choice \\(\\rho_S\\) (scale) \\(\\rho_M\\) (final estimation). Common choices include:Huber’s \\(\\rho\\)-Function: Combines quadratic linear growth balance robustness efficiency:\n\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Huber’s \\(\\rho\\)-Function: Combines quadratic linear growth balance robustness efficiency:\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare Function: Provides high robustness completely bounding large residuals:\n\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare Function: Provides high robustness completely bounding large residuals:\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Hampel’s Three-Part Redescending Function: limits influence large residuals assigning constant penalty beyond certain threshold.\n\\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq , \\\\\n|z| - ^2/2 & \\text{} < |z| \\leq b, \\\\\n\\text{constant} & \\text{} |z| > b.\n\\end{cases}\n\\]Hampel’s Three-Part Redescending Function: limits influence large residuals assigning constant penalty beyond certain threshold.\\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq , \\\\\n|z| - ^2/2 & \\text{} < |z| \\leq b, \\\\\n\\text{constant} & \\text{} |z| > b.\n\\end{cases}\n\\]","code":""},{"path":"linear-regression.html","id":"practical-considerations-1","chapter":"5 Linear Regression","heading":"5.5.8 Practical Considerations","text":"following table summarizes key properties, advantages, limitations robust estimators discussed: +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | Estimator | Key Features | Breakdown Point | Efficiency (Normality) | Applications | Advantages | +=================+======================================================================+=========================+==============================+==========================================================================+===============================================+ | \\(M\\)-Estimators | Generalization OLS Robust \\(\\rho\\) reduces large residual influence | Moderate (\\(0.29\\)) | High proper tuning | Wide applicability regression moderate robustness | Balances robustness efficiency | | | | | | | | | | | | | | Flexible tuning via \\(\\rho\\)-function | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | \\(R\\)-Estimators | Rank-based method | High (depends ranks) | Moderate | Ordinal data heavily skewed distributions | Handles predictor response outliers | | | | | | | | | | Immune outliers \\(x\\) \\(y\\) | | | | Suitable ordinal rank-based data | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | \\(L\\)-Estimators | Linear combination order statistics | High (\\(50\\%\\)) | Moderate | Descriptive statistics, robust averages | Simple intuitive | | | | | | | | | | | | | | Easy compute, even large datasets | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | LTS | Minimizes smallest \\(h\\) squared residuals | High (\\(50\\%\\)) | Moderate | Data high contamination, fault detection | High robustness outliers | | | | | | | | | | | | | | Resistant leverage points | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | \\(S\\)-Estimators | Minimizes robust scale residuals | High (\\(50\\%\\)) | Low moderate | Outlier detection, data heavy-tailed distributions | Focus robust scale estimation | | | | | | | | | | | | | | Effective detecting extreme outliers | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+ | \\(MM\\)-Estimators | High robustness (scale) + high efficiency (coefficients) | High (\\(50\\%\\)) | High | Real-world applications mixed contamination heavy-tailed errors | Combinesrobustness efficiency effectively | | | | | | | | | | | | | | Versatile flexible | +—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+Notes Choosing Estimator\\(M\\)-Estimators: Best suited general-purpose robust regression, offering balance robustness efficiency moderate contamination.\\(R\\)-Estimators: Ideal rank-based data ordinal data, especially outliers present predictors responses.\\(L\\)-Estimators: Simple effective descriptive statistics data cleaning limited computational resources.LTS: Recommended datasets significant contamination leverage points due high breakdown point.\\(S\\)-Estimators: Focus robust scale estimation, suitable identifying mitigating influence extreme residuals.\\(MM\\)-Estimators: Combines robustness \\(S\\)-estimators efficiency \\(M\\)-estimators, making versatile choice heavily contaminated data.OLS coefficients highly influenced presence outliers. example, slope (x coefficient) intercept shifted fit outliers, resulting poor fit majority data.\\(M\\)-estimator reduces influence large residuals using Huber’s psi function. results coefficients less affected outliers compared OLS.LTS minimizes smallest squared residuals, ignoring extreme residuals. results robust fit, particularly presence vertical outliers leverage points.\\(MM\\)-estimators combine robust scale estimation (\\(S\\)-estimators) efficient coefficient estimation (\\(M\\)-estimators). achieves high robustness high efficiency normal conditions.Visualization shows differences regression fits: - OLS heavily influenced outliers provides poor fit majority data. ) - \\(M\\)-estimator downweights large residuals, resulting better fit. - LTS regression ignores extreme residuals entirely, providing robust fit. - \\(MM\\)-estimators balance robustness efficiency, producing coefficients close LTS improved efficiency normality.table shows coefficients vary across methods: - OLS coefficients distorted outliers. - \\(M\\)-estimators \\(MM\\)-estimators provide coefficients less influenced extreme values. - LTS regression, trimming mechanism, produces robust coefficients excluding largest residuals.","code":"\n# Load necessary libraries\nlibrary(MASS)       # For robust regression functions like rlm\nlibrary(robustbase) # For LTS regression and MM-estimators\nlibrary(dplyr)      # For data manipulation\nlibrary(ggplot2)    # For visualization\n\n# Simulate dataset\nset.seed(123)\nn <- 100\nx <- rnorm(n, mean = 5, sd = 2)   # Predictor\ny <- 3 + 2 * x + rnorm(n, sd = 1) # Response\n\n# Introduce outliers\ny[95:100] <- y[95:100] + 20  # Vertical outliers\nx[90:95] <- x[90:95] + 10    # Leverage points\n\ndata <- data.frame(x, y)\n\n# Visualize the data\nggplot(data, aes(x, y)) +\n    geom_point() +\n    labs(title = \"Scatterplot of Simulated Data with Outliers\",\n         x = \"Predictor (x)\",\n         y = \"Response (y)\") +\n    theme_minimal()\n\n# Ordinary Least Squares\nols_model <- lm(y ~ x, data = data)\nsummary(ols_model)\n#> \n#> Call:\n#> lm(formula = y ~ x, data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -12.6023  -2.4590  -0.5717   0.9247  24.4024 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   8.8346     1.1550   7.649 1.41e-11 ***\n#> x             0.9721     0.1749   5.558 2.36e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.583 on 98 degrees of freedom\n#> Multiple R-squared:  0.2396, Adjusted R-squared:  0.2319 \n#> F-statistic: 30.89 on 1 and 98 DF,  p-value: 2.358e-07\n# $M$-Estimators\nm_model <- rlm(y ~ x, data = data, psi = psi.huber)\nsummary(m_model)\n#> \n#> Call: rlm(formula = y ~ x, data = data, psi = psi.huber)\n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -18.43919  -0.97575  -0.03297   0.76967  21.85546 \n#> \n#> Coefficients:\n#>             Value   Std. Error t value\n#> (Intercept)  4.3229  0.2764    15.6421\n#> x            1.7250  0.0419    41.2186\n#> \n#> Residual standard error: 1.349 on 98 degrees of freedom\n# Least Trimmed Squares (LTS)\nlts_model <- ltsReg(y ~ x, data = data)\nlts_coefficients <- coef(lts_model)\n# $MM$-Estimators\nmm_model <- lmrob(y ~ x, data = data, setting = \"KS2014\")\nsummary(mm_model)\n#> \n#> Call:\n#> lmrob(formula = y ~ x, data = data, setting = \"KS2014\")\n#>  \\--> method = \"SMDM\"\n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -20.45989  -0.69436  -0.01455   0.73614  22.10173 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  3.02192    0.25850   11.69   <2e-16 ***\n#> x            1.96672    0.04538   43.34   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Robust residual standard error: 0.9458 \n#> Multiple R-squared:  0.9562, Adjusted R-squared:  0.9558 \n#> Convergence in 7 IRWLS iterations\n#> \n#> Robustness weights: \n#>  10 observations c(90,91,92,93,94,96,97,98,99,100)\n#>   are outliers with |weight| = 0 ( < 0.001); \n#>  67 weights are ~= 1. The remaining 23 ones are summarized as\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.2496  0.7969  0.9216  0.8428  0.9548  0.9943 \n#> Algorithmic parameters: \n#>       tuning.chi1       tuning.chi2       tuning.chi3       tuning.chi4 \n#>        -5.000e-01         1.500e+00                NA         5.000e-01 \n#>                bb       tuning.psi1       tuning.psi2       tuning.psi3 \n#>         5.000e-01        -5.000e-01         1.500e+00         9.500e-01 \n#>       tuning.psi4        refine.tol           rel.tol         scale.tol \n#>                NA         1.000e-07         1.000e-07         1.000e-10 \n#>         solve.tol          zero.tol       eps.outlier             eps.x \n#>         1.000e-07         1.000e-10         1.000e-03         3.223e-11 \n#> warn.limit.reject warn.limit.meanrw \n#>         5.000e-01         5.000e-01 \n#>      nResample         max.it       best.r.s       k.fast.s          k.max \n#>           1000            500             20              2           2000 \n#>    maxit.scale      trace.lev            mts     compute.rd      numpoints \n#>            200              0           1000              0             10 \n#> fast.s.large.n \n#>           2000 \n#>               setting                   psi           subsampling \n#>              \"KS2014\"                 \"lqq\"         \"nonsingular\" \n#>                   cov compute.outlier.stats \n#>             \".vcov.w\"                \"SMDM\" \n#> seed : int(0)\n# Visualizing results\ndata <- data %>%\n    mutate(\n        ols_fit = predict(ols_model, newdata = data),\n        m_fit = predict(m_model, newdata = data),\n        lts_fit = fitted(lts_model),\n        # Use `fitted()` for ltsReg objects\n        mm_fit = predict(mm_model, newdata = data)\n    )\n\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_line(\n        aes(y = ols_fit),\n        color = \"red\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"OLS\"\n    ) +\n    geom_line(\n        aes(y = m_fit),\n        color = \"blue\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"$M$-Estimator\"\n    ) +\n    geom_line(\n        aes(y = lts_fit),\n        color = \"green\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"LTS\"\n    ) +\n    geom_line(\n        aes(y = mm_fit),\n        color = \"purple\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"$MM$-Estimator\"\n    ) +\n    labs(title = \"Comparison of Regression Fits\",\n         x = \"Predictor (x)\",\n         y = \"Response (y)\") +\n    theme_minimal()\n# Comparing Coefficients\ncomparison <- data.frame(\n    Method = c(\"OLS\", \"$M$-Estimator\", \"LTS\", \"$MM$-Estimator\"),\n    Intercept = c(\n        coef(ols_model)[1],\n        coef(m_model)[1],\n        lts_coefficients[1],\n        coef(mm_model)[1]\n    ),\n    Slope = c(\n        coef(ols_model)[2],\n        coef(m_model)[2],\n        lts_coefficients[2],\n        coef(mm_model)[2]\n    )\n)\n\nprint(comparison)\n#>           Method Intercept     Slope\n#> 1            OLS  8.834553 0.9720994\n#> 2  $M$-Estimator  4.322869 1.7250441\n#> 3            LTS  2.954960 1.9777635\n#> 4 $MM$-Estimator  3.021923 1.9667208"},{"path":"linear-regression.html","id":"partial-least-squares","chapter":"5 Linear Regression","heading":"5.6 Partial Least Squares","text":"Partial Least Squares (PLS) dimensionality reduction technique used regression predictive modeling. particularly useful predictors highly collinear number predictors (\\(p\\)) exceeds number observations (\\(n\\)). Unlike methods Principal Component Regression (PCR), PLS simultaneously considers relationship predictors response variable.","code":""},{"path":"linear-regression.html","id":"motivation-for-pls","chapter":"5 Linear Regression","heading":"5.6.1 Motivation for PLS","text":"Limitations Classical MethodsMulticollinearity:\nOLS fails predictors highly correlated design matrix \\(X'X\\) becomes nearly singular, leading unstable estimates.\nOLS fails predictors highly correlated design matrix \\(X'X\\) becomes nearly singular, leading unstable estimates.High-Dimensional Data:\n\\(p > n\\), OLS directly applied \\(X'X\\) invertible.\n\\(p > n\\), OLS directly applied \\(X'X\\) invertible.Principal Component Regression (PCR):\nPCR addresses multicollinearity using principal components \\(X\\), account relationship predictors response variable \\(y\\) constructing components.\nPCR addresses multicollinearity using principal components \\(X\\), account relationship predictors response variable \\(y\\) constructing components.PLS overcomes limitations constructing components maximize covariance predictors \\(X\\) response \\(y\\). finds compromise explaining variance \\(X\\) predicting \\(y\\), making particularly suited regression high-dimensional collinear datasets.Let:\\(X\\) \\(n \\times p\\) matrix predictors,\\(X\\) \\(n \\times p\\) matrix predictors,\\(y\\) \\(n \\times 1\\) response vector,\\(y\\) \\(n \\times 1\\) response vector,\\(t_k\\) \\(k\\)-th latent component derived \\(X\\),\\(t_k\\) \\(k\\)-th latent component derived \\(X\\),\\(p_k\\) \\(q_k\\) loadings \\(X\\) \\(y\\), respectively.\\(p_k\\) \\(q_k\\) loadings \\(X\\) \\(y\\), respectively.PLS aims construct latent components \\(t_1, t_2, \\ldots, t_K\\) :\\(t_k\\) linear combination predictors: \\(t_k = X w_k\\), \\(w_k\\) weight vector. 2The covariance \\(t_k\\) \\(y\\) maximized: \\[\n   \\text{Maximize } Cov(t_k, y) = w_k' X' y.\n   \\]","code":""},{"path":"linear-regression.html","id":"steps-to-construct-pls-components","chapter":"5 Linear Regression","heading":"5.6.2 Steps to Construct PLS Components","text":"Compute Weights:\nweights \\(w_k\\) \\(k\\)-th component obtained solving: \\[\nw_k = \\frac{X'y}{\\|X'y\\|}.\n\\]\nweights \\(w_k\\) \\(k\\)-th component obtained solving: \\[\nw_k = \\frac{X'y}{\\|X'y\\|}.\n\\]Construct Latent Component:\nForm \\(k\\)-th latent component: \\[\nt_k = X w_k.\n\\]\nForm \\(k\\)-th latent component: \\[\nt_k = X w_k.\n\\]Deflate Predictors:\nextracting \\(t_k\\), predictors deflated remove information explained \\(t_k\\): \\[\nX \\leftarrow X - t_k p_k',\n\\] \\(p_k = \\frac{X't_k}{t_k't_k}\\) loadings \\(X\\).\nextracting \\(t_k\\), predictors deflated remove information explained \\(t_k\\): \\[\nX \\leftarrow X - t_k p_k',\n\\] \\(p_k = \\frac{X't_k}{t_k't_k}\\) loadings \\(X\\).Deflate Response:\nSimilarly, deflate \\(y\\) remove variance explained \\(t_k\\): \\[\ny \\leftarrow y - t_k q_k,\n\\] \\(q_k = \\frac{t_k'y}{t_k't_k}\\).\nSimilarly, deflate \\(y\\) remove variance explained \\(t_k\\): \\[\ny \\leftarrow y - t_k q_k,\n\\] \\(q_k = \\frac{t_k'y}{t_k't_k}\\).Repeat Components:\nRepeat steps \\(K\\) components extracted.\nRepeat steps \\(K\\) components extracted.constructing \\(K\\) components, response \\(y\\) modeled :\\[\ny = T C + \\epsilon,\n\\]:\\(T = [t_1, t_2, \\ldots, t_K]\\) matrix latent components,\\(T = [t_1, t_2, \\ldots, t_K]\\) matrix latent components,\\(C\\) vector regression coefficients.\\(C\\) vector regression coefficients.estimated coefficients original predictors :\\[\n\\hat{\\beta} = W (P' W)^{-1} C,\n\\]\\(W = [w_1, w_2, \\ldots, w_K]\\) \\(P = [p_1, p_2, \\ldots, p_K]\\).","code":""},{"path":"linear-regression.html","id":"properties-of-pls","chapter":"5 Linear Regression","heading":"5.6.3 Properties of PLS","text":"Dimensionality Reduction:\nPLS reduces \\(X\\) \\(K\\) components, \\(K \\leq \\min(n, p)\\).\nPLS reduces \\(X\\) \\(K\\) components, \\(K \\leq \\min(n, p)\\).Handles Multicollinearity:\nconstructing uncorrelated components, PLS avoids instability caused multicollinearity OLS.\nconstructing uncorrelated components, PLS avoids instability caused multicollinearity OLS.Supervised Dimensionality Reduction:\nUnlike PCR, PLS considers relationship \\(X\\) \\(y\\) constructing components.\nUnlike PCR, PLS considers relationship \\(X\\) \\(y\\) constructing components.Efficiency:\nPLS requires fewer components PCR achieve similar level predictive accuracy.\nPLS requires fewer components PCR achieve similar level predictive accuracy.Practical ConsiderationsNumber Components:\noptimal number components \\(K\\) can determined using cross-validation.\noptimal number components \\(K\\) can determined using cross-validation.Preprocessing:\nStandardizing predictors essential PLS, ensures variables scale.\nStandardizing predictors essential PLS, ensures variables scale.Comparison Methods:\nPLS outperforms OLS PCR cases multicollinearity \\(p > n\\), may less interpretable sparse methods like Lasso.\nPLS outperforms OLS PCR cases multicollinearity \\(p > n\\), may less interpretable sparse methods like Lasso.loadings provide contribution predictor PLS components. Higher absolute values indicate stronger contributions corresponding component.Summary Model:\nproportion variance explained indicates much variability predictors response captured PLS component.\ngoal retain enough components explain variance avoiding overfitting.\nSummary Model:proportion variance explained indicates much variability predictors response captured PLS component.proportion variance explained indicates much variability predictors response captured PLS component.goal retain enough components explain variance avoiding overfitting.goal retain enough components explain variance avoiding overfitting.Validation Plot:\nMean Squared Error Prediction (MSEP) curve used select optimal number components.\nAdding many components can lead overfitting, may underfit data.\nValidation Plot:Mean Squared Error Prediction (MSEP) curve used select optimal number components.Mean Squared Error Prediction (MSEP) curve used select optimal number components.Adding many components can lead overfitting, may underfit data.Adding many components can lead overfitting, may underfit data.Coefficients:\nextracted coefficients weights applied predictors final PLS model.\ncoefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.\nCoefficients:extracted coefficients weights applied predictors final PLS model.extracted coefficients weights applied predictors final PLS model.coefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.coefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.Actual vs Predicted Plot:\nvisualization evaluates well PLS model predicts response variable.\nPoints tightly clustered around diagonal indicate good performance.\nActual vs Predicted Plot:visualization evaluates well PLS model predicts response variable.visualization evaluates well PLS model predicts response variable.Points tightly clustered around diagonal indicate good performance.Points tightly clustered around diagonal indicate good performance.VIP Scores:\nVIP scores help identify important predictors PLS model.\nPredictors higher VIP scores contribute explaining response variable.\nVIP Scores:VIP scores help identify important predictors PLS model.VIP scores help identify important predictors PLS model.Predictors higher VIP scores contribute explaining response variable.Predictors higher VIP scores contribute explaining response variable.","code":"\n# Load required library\nlibrary(pls)\n\n# Step 1: Simulate data\nset.seed(123)  # Ensure reproducibility\nn <- 100       # Number of observations\np <- 10        # Number of predictors\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)  # Design matrix (predictors)\nbeta <- runif(p)                               # True coefficients\ny <- X %*% beta + rnorm(n)                     # Response variable with noise\n\n# Step 2: Fit Partial Least Squares (PLS) Regression\npls_fit <- plsr(y ~ X, ncomp = 5, validation = \"CV\")\n\n# Step 3: Summarize the PLS Model\nsummary(pls_fit)\n#> Data:    X dimension: 100 10 \n#>  Y dimension: 100 1\n#> Fit method: kernelpls\n#> Number of components considered: 5\n#> \n#> VALIDATION: RMSEP\n#> Cross-validated using 10 random segments.\n#>        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps\n#> CV           1.339    1.123    1.086    1.090    1.088    1.087\n#> adjCV        1.339    1.112    1.078    1.082    1.080    1.080\n#> \n#> TRAINING: % variance explained\n#>    1 comps  2 comps  3 comps  4 comps  5 comps\n#> X    10.88    20.06    30.80    42.19    51.61\n#> y    44.80    48.44    48.76    48.78    48.78\n\n# Step 4: Perform Cross-Validation and Select Optimal Components\nvalidationplot(pls_fit, val.type = \"MSEP\")\n\n# Step 5: Extract Coefficients for Predictors\npls_coefficients <- coef(pls_fit)\nprint(pls_coefficients)\n#> , , 5 comps\n#> \n#>               y\n#> X1   0.30192935\n#> X2  -0.03161151\n#> X3   0.22392538\n#> X4   0.42315637\n#> X5   0.33000198\n#> X6   0.66228763\n#> X7   0.40452691\n#> X8  -0.05704037\n#> X9  -0.02699757\n#> X10  0.05944765\n\n# Step 6: Evaluate Model Performance\npredicted_y <- predict(pls_fit, X)\nactual_vs_predicted <- data.frame(\n  Actual = y,\n  Predicted = predicted_y[, , 5]  # Predicted values using 5 components\n)\n\n# Plot Actual vs Predicted\nlibrary(ggplot2)\nggplot(actual_vs_predicted, aes(x = Actual, y = Predicted)) +\n    geom_point() +\n    geom_abline(\n        intercept = 0,\n        slope = 1,\n        color = \"red\",\n        linetype = \"dashed\"\n    ) +\n    labs(title = \"Actual vs Predicted Values (PLS Regression)\",\n         x = \"Actual Values\",\n         y = \"Predicted Values\") +\n    theme_minimal()\n\n# Step 7: Extract and Interpret Variable Importance (Loadings)\nloadings_matrix <- as.matrix(unclass(loadings(pls_fit)))\nvariable_importance <- as.data.frame(loadings_matrix)\ncolnames(variable_importance) <-\n    paste0(\"Component_\", 1:ncol(variable_importance))\nrownames(variable_importance) <-\n    paste0(\"X\", 1:nrow(variable_importance))\n\n# Print variable importance\nprint(variable_importance)\n#>     Component_1 Component_2 Component_3 Component_4 Component_5\n#> X1  -0.04991097   0.5774569  0.24349681 -0.41550345 -0.02098351\n#> X2   0.08913192  -0.1139342 -0.17582957 -0.05709948 -0.06707863\n#> X3   0.13773357   0.1633338  0.07622919 -0.07248620 -0.61962875\n#> X4   0.40369572  -0.2730457  0.69994206 -0.07949013  0.35239113\n#> X5   0.50562681  -0.1788131 -0.27936562  0.36197480 -0.41919645\n#> X6   0.57044281   0.3358522 -0.38683260  0.17656349  0.31154275\n#> X7   0.36258623   0.1202109 -0.01753715 -0.12980483 -0.06919411\n#> X8   0.12975452  -0.1164935 -0.30479310 -0.65654861  0.49948167\n#> X9  -0.29521786   0.6170234 -0.32082508 -0.01041860  0.04904396\n#> X10  0.23930055  -0.3259554  0.20006888 -0.53547258 -0.17963372"},{"path":"linear-regression.html","id":"comparison-with-related-methods","chapter":"5 Linear Regression","heading":"5.6.4 Comparison with Related Methods","text":"","code":""},{"path":"non-linear-regression.html","id":"non-linear-regression","chapter":"6 Non-Linear Regression","heading":"6 Non-Linear Regression","text":"Non-linear regression models differ fundamentally linear regression models derivatives mean function respect parameters depend one parameters. dependence adds complexity also provides greater flexibility model intricate relationships.Linear Regression:Model Form Example: typical linear regression model looks like \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) \\(\\beta_1\\) parameters.Model Form Example: typical linear regression model looks like \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) \\(\\beta_1\\) parameters.Parameter Effect: influence parameter \\(y\\) constant. example, \\(\\beta_1\\) increases 1, change \\(y\\) always \\(x\\), regardless current value \\(\\beta_1\\).Parameter Effect: influence parameter \\(y\\) constant. example, \\(\\beta_1\\) increases 1, change \\(y\\) always \\(x\\), regardless current value \\(\\beta_1\\).Derivatives: partial derivatives \\(y\\) respect parameter (e.g., \\(\\frac{\\partial y}{\\partial \\beta_1} = x\\)) depend parameters \\(\\beta_0\\) \\(\\beta_1\\) —depend data \\(x\\). makes mathematics finding best-fit line straightforward.Derivatives: partial derivatives \\(y\\) respect parameter (e.g., \\(\\frac{\\partial y}{\\partial \\beta_1} = x\\)) depend parameters \\(\\beta_0\\) \\(\\beta_1\\) —depend data \\(x\\). makes mathematics finding best-fit line straightforward.Straightforward estimation via closed-form solutions like Ordinary Least Squares.Straightforward estimation via closed-form solutions like Ordinary Least Squares.Non-linear Regression:Model Form Example: Consider \\(y = \\alpha \\cdot e^{\\beta x}\\). , \\(\\alpha\\) \\(\\beta\\) parameters, relationship straight line.Model Form Example: Consider \\(y = \\alpha \\cdot e^{\\beta x}\\). , \\(\\alpha\\) \\(\\beta\\) parameters, relationship straight line.Parameter Effect: effect changing \\(\\alpha\\) \\(\\beta\\) \\(y\\) constant. instance, change \\(\\beta\\), impact \\(y\\) depends \\(x\\) current value \\(\\beta\\). makes predictions adjustments complex.Parameter Effect: effect changing \\(\\alpha\\) \\(\\beta\\) \\(y\\) constant. instance, change \\(\\beta\\), impact \\(y\\) depends \\(x\\) current value \\(\\beta\\). makes predictions adjustments complex.Derivatives: Taking partial derivative respect \\(\\beta\\) gives \\(\\frac{\\partial y}{\\partial \\beta} = \\alpha x e^{\\beta x}\\). Notice derivative depends \\(\\alpha\\), \\(\\beta\\), \\(x\\). Unlike linear regression, sensitivity \\(y\\) changes \\(\\beta\\) changes \\(\\beta\\) changes.Derivatives: Taking partial derivative respect \\(\\beta\\) gives \\(\\frac{\\partial y}{\\partial \\beta} = \\alpha x e^{\\beta x}\\). Notice derivative depends \\(\\alpha\\), \\(\\beta\\), \\(x\\). Unlike linear regression, sensitivity \\(y\\) changes \\(\\beta\\) changes \\(\\beta\\) changes.Estimation requires iterative algorithms like Gauss-Newton Algorithm, closed-form solutions feasible.Estimation requires iterative algorithms like Gauss-Newton Algorithm, closed-form solutions feasible.Summary Table: Linear vs. Non-Linear RegressionKey Features Non-linear regression:Complex Functional Forms: Non-linear regression allows relationships straight lines planes.Interpretability Challenges: Non-linear models can difficult interpret, especially functional forms complex.Practical Use Cases:\nGrowth curves\nHigh-order polynomials\nLinear approximations (e.g., Taylor expansions)\nCollections locally linear models basis functions (e.g., splines)\nGrowth curvesHigh-order polynomialsLinear approximations (e.g., Taylor expansions)Collections locally linear models basis functions (e.g., splines)approaches can approximate data, may suffer interpretability issues may generalize well data sparse. Hence, intrinsically non-linear models often preferred.Intrinsically Non-Linear ModelsThe general form intrinsically non-linear regression model :\\[\nY_i = f(\\mathbf{x}_i; \\mathbf{\\theta}) + \\epsilon_i\n\\]:\\(f(\\mathbf{x}_i; \\mathbf{\\theta})\\): non-linear function relates \\(E(Y_i)\\) independent variables \\(\\mathbf{x}_i\\).\\(\\mathbf{x}_i\\): \\(k \\times 1\\) vector independent variables (fixed).\\(\\mathbf{\\theta}\\): \\(p \\times 1\\) vector parameters.\\(\\epsilon_i\\): Independent identically distributed random errors, often assumed mean 0 constant variance \\(\\sigma^2\\). cases, \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).Example: Exponential Growth ModelA common non-linear model exponential growth function:\\[\ny = \\theta_1 e^{\\theta_2 x} + \\epsilon\n\\]:\\(\\theta_1\\): Initial value.\\(\\theta_2\\): Growth rate.\\(x\\): Independent variable (e.g., time).\\(\\epsilon\\): Random error.","code":""},{"path":"non-linear-regression.html","id":"inference","chapter":"6 Non-Linear Regression","heading":"6.1 Inference","text":"Since \\(Y_i = f(\\mathbf{x}_i, \\theta) + \\epsilon_i\\), \\(\\epsilon_i \\sim \\text{iid}(0, \\sigma^2)\\), can estimate parameters (\\(\\hat{\\theta}\\)) minimizing sum squared errors:\\[\n\\sum_{=1}^{n} \\big(Y_i - f(\\mathbf{x}_i, \\theta)\\big)^2\n\\]Let \\(\\hat{\\theta}\\) minimizer, variance residuals estimated :\\[\ns^2 = \\hat{\\sigma}^2_{\\epsilon} = \\frac{\\sum_{=1}^{n} \\big(Y_i - f(\\mathbf{x}_i, \\hat{\\theta})\\big)^2}{n - p}\n\\]\\(p\\) number parameters \\(\\mathbf{\\theta}\\), \\(n\\) number observations.Asymptotic Distribution \\(\\hat{\\theta}\\)regularity conditions—notably \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) \\(n\\) sufficiently large central-limit-type argument—parameter estimates \\(\\hat{\\theta}\\) following asymptotic normal distribution:\\[\n\\hat{\\theta} \\sim (\\mathbf{\\theta}, \\sigma^2[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1})\n\\]\\(\\) stands “asymptotic normality.”\\(\\mathbf{F}(\\theta)\\) \\(n \\times p\\) Jacobian matrix partial derivatives \\(f(\\mathbf{x}_i, \\theta)\\) respect \\(\\mathbf{\\theta}\\), evaluated \\(\\hat{\\theta}\\). Specifically,\\[\n\\mathbf{F}(\\theta) = \\begin{pmatrix}\n\\frac{\\partial f(\\mathbf{x}_1, \\boldsymbol{\\theta})}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_1, \\boldsymbol{\\theta})}{\\partial \\theta_p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{x}_n, \\boldsymbol{\\theta})}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_n, \\boldsymbol{\\theta})}{\\partial \\theta_p}\n\\end{pmatrix}\n\\]Asymptotic normality means sample size \\(n\\) becomes large, sampling distribution \\(\\hat{\\theta}\\) approaches normal distribution, enables inference parameters.","code":""},{"path":"non-linear-regression.html","id":"linear-functions-of-the-parameters","chapter":"6 Non-Linear Regression","heading":"6.1.1 Linear Functions of the Parameters","text":"“linear function parameters” refers quantity can written \\(\\mathbf{}'\\boldsymbol{\\theta}\\), \\(\\mathbf{}\\) (constant) contrast vector. Common examples include:single parameter \\(\\theta_j\\) (using vector \\(\\mathbf{}\\) 1 \\(j\\)-th position 0 elsewhere).single parameter \\(\\theta_j\\) (using vector \\(\\mathbf{}\\) 1 \\(j\\)-th position 0 elsewhere).Differences, sums, contrasts, e.g. \\(\\theta_1 - \\theta_2\\).Differences, sums, contrasts, e.g. \\(\\theta_1 - \\theta_2\\).Suppose interested linear combination parameters, \\(\\theta_1 - \\theta_2\\). Define contrast vector \\(\\mathbf{}\\) :\\[\n\\mathbf{} = (0, 1, -1)'\n\\]consider inference \\(\\mathbf{'\\theta}\\) (\\(\\mathbf{}\\) can \\(p\\)-dimensional vector). Using rules expectation variance linear combination random vector \\(\\mathbf{Z}\\):\\[\n\\begin{aligned}\nE(\\mathbf{'Z}) &= \\mathbf{'}E(\\mathbf{Z}) \\\\\n\\text{Var}(\\mathbf{'Z}) &= \\mathbf{'} \\text{Var}(\\mathbf{Z}) \\mathbf{}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(\\mathbf{'\\hat{\\theta}}) &= \\mathbf{'}E(\\hat{\\theta}) \\approx \\mathbf{}' \\theta \\\\\n\\text{Var}(\\mathbf{'} \\hat{\\theta}) &= \\mathbf{'} \\text{Var}(\\hat{\\theta}) \\mathbf{} \\approx \\sigma^2 \\mathbf{'[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1}}\n\\end{aligned}\n\\]Hence,\\[\n\\mathbf{'\\hat{\\theta}} \\sim \\big(\\mathbf{'\\theta}, \\sigma^2 \\mathbf{'[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1}}\\big)\n\\]Confidence Intervals Linear ContrastsSince \\(\\mathbf{'\\hat{\\theta}}\\) asymptotically independent \\(s^2\\) (order \\(O1/n\\)), two-sided \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mathbf{'\\theta}\\) given :\\[\n\\mathbf{'\\theta} \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{'[\\mathbf{F}(\\hat{\\theta})'\\mathbf{F}(\\hat{\\theta})]^{-1}}}\n\\]\\(t_{(1-\\alpha/2, n-p)}\\) critical value \\(t\\)-distribution \\(n - p\\) degrees freedom.\\(s = \\sqrt{\\hat{\\sigma^2}_\\epsilon}\\) estimated standard deviation residuals.Special Case: Single Parameter \\(\\theta_j\\)focus single parameter \\(\\theta_j\\), let \\(\\mathbf{'} = (0, \\dots, 1, \\dots, 0)\\) (1 \\(j\\)-th position). , confidence interval \\(\\theta_j\\) becomes:\\[\n\\hat{\\theta}_j \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\hat{c}^j}\n\\]\\(\\hat{c}^j\\) \\(j\\)-th diagonal element \\([\\mathbf{F}(\\hat{\\theta})'\\mathbf{F}(\\hat{\\theta})]^{-1}\\).","code":""},{"path":"non-linear-regression.html","id":"nonlinear-functions-of-parameters","chapter":"6 Non-Linear Regression","heading":"6.1.2 Nonlinear Functions of Parameters","text":"many cases, interested nonlinear functions \\(\\boldsymbol{\\theta}\\). Let \\(h(\\boldsymbol{\\theta})\\) function (e.g., ratio parameters, difference exponentials, etc.).\\(h(\\theta)\\) nonlinear function parameters, can use Taylor series expansion \\(\\theta\\) approximate \\(h(\\hat{\\theta})\\):\\[\nh(\\hat{\\theta}) \\approx h(\\theta) + \\mathbf{h}' [\\hat{\\theta} - \\theta]\n\\]\\(\\mathbf{h} = \\left( \\frac{\\partial h}{\\partial \\theta_1}, \\frac{\\partial h}{\\partial \\theta_2}, \\dots, \\frac{\\partial h}{\\partial \\theta_p} \\right)'\\) gradient vector partial derivatives.Key Approximations:Expectation Variance \\(\\hat{\\theta}\\) (using asymptotic normality \\(\\hat{\\theta}\\): \\[\n\\begin{aligned}\nE(\\hat{\\theta}) &\\approx \\theta, \\\\\n\\text{Var}(\\hat{\\theta}) &\\approx \\sigma^2 [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1}.\n\\end{aligned}\n\\]Expectation Variance \\(\\hat{\\theta}\\) (using asymptotic normality \\(\\hat{\\theta}\\): \\[\n\\begin{aligned}\nE(\\hat{\\theta}) &\\approx \\theta, \\\\\n\\text{Var}(\\hat{\\theta}) &\\approx \\sigma^2 [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1}.\n\\end{aligned}\n\\]Expectation Variance \\(h(\\hat{\\theta})\\) (properties expectation variance (approximately) linear transformations): \\[\n\\begin{aligned}\nE(h(\\hat{\\theta})) &\\approx h(\\theta), \\\\\n\\text{Var}(h(\\hat{\\theta})) &\\approx \\sigma^2 \\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}.\n\\end{aligned}\\]Expectation Variance \\(h(\\hat{\\theta})\\) (properties expectation variance (approximately) linear transformations): \\[\n\\begin{aligned}\nE(h(\\hat{\\theta})) &\\approx h(\\theta), \\\\\n\\text{Var}(h(\\hat{\\theta})) &\\approx \\sigma^2 \\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}.\n\\end{aligned}\\]Combining results, find:\\[\nh(\\hat{\\theta}) \\sim (h(\\theta), \\sigma^2 \\mathbf{h}' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}),\n\\]\\(\\) represents asymptotic normality.Confidence Interval \\(h(\\theta)\\):approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(h(\\theta)\\) :\\[\nh(\\hat{\\theta}) \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}},\n\\]\\(\\mathbf{h}\\) \\(\\mathbf{F}(\\theta)\\) evaluated \\(\\hat{\\theta}\\).compute prediction interval new observation \\(Y_0\\) \\(x = x_0\\):Model Definition: \\[\nY_0 = f(x_0; \\theta) + \\epsilon_0, \\quad \\epsilon_0 \\sim N(0, \\sigma^2),\n\\] predicted value: \\[\n\\hat{Y}_0 = f(x_0, \\hat{\\theta}).\n\\]Model Definition: \\[\nY_0 = f(x_0; \\theta) + \\epsilon_0, \\quad \\epsilon_0 \\sim N(0, \\sigma^2),\n\\] predicted value: \\[\n\\hat{Y}_0 = f(x_0, \\hat{\\theta}).\n\\]Approximation \\(\\hat{Y}_0\\): \\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), : \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0, \\theta) + \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta],\n\\] : \\[\n\\mathbf{f}_0(\\theta) = \\left( \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_1}, \\dots, \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_p} \\right)'.\n\\]Approximation \\(\\hat{Y}_0\\): \\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), : \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0, \\theta) + \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta],\n\\] : \\[\n\\mathbf{f}_0(\\theta) = \\left( \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_1}, \\dots, \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_p} \\right)'.\n\\]Error Approximation: \\[\n\\begin{aligned}Y_0 - \\hat{Y}_0 &\\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\&= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\\end{aligned}\n\\]Error Approximation: \\[\n\\begin{aligned}Y_0 - \\hat{Y}_0 &\\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\&= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\\end{aligned}\n\\]Variance \\(Y_0 - \\hat{Y}_0\\): \\[\n\\begin{aligned}\n\\text{Var}(Y_0 - \\hat{Y}_0) &\\approx \\text{Var}(\\epsilon_0 - \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta]) \\\\\n&= \\sigma^2 + \\sigma^2 \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta) \\\\\n&= \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big).\n\\end{aligned}\n\\]Variance \\(Y_0 - \\hat{Y}_0\\): \\[\n\\begin{aligned}\n\\text{Var}(Y_0 - \\hat{Y}_0) &\\approx \\text{Var}(\\epsilon_0 - \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta]) \\\\\n&= \\sigma^2 + \\sigma^2 \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta) \\\\\n&= \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big).\n\\end{aligned}\n\\]Hence, prediction error \\(Y_0 - \\hat{Y}_0\\) follows asymptotic normal distribution:\\[\nY_0 - \\hat{Y}_0 \\sim \\big(0, \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big)\\big).\n\\]\\(100(1-\\alpha)\\%\\) prediction interval \\(Y_0\\) :\\[\n\\hat{Y}_0 \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{1 + \\mathbf{f}_0(\\hat{\\theta})' [\\mathbf{F}(\\hat{\\theta})' \\mathbf{F}(\\hat{\\theta})]^{-1} \\mathbf{f}_0(\\hat{\\theta})}.\n\\]substitute \\(\\hat{\\theta}\\) \\(\\mathbf{f}_0\\) \\(\\mathbf{F}\\). Recall \\(s\\) estiamte \\(\\sigma\\).Sometimes want confidence interval \\(E(Y_i)\\) (.e., mean response \\(x_0\\)), rather prediction interval individual future observation. case, variance term random error \\(\\epsilon_0\\) included. Hence, formula without “+1”:\\[\nE(Y_0) \\approx f(x_0; \\theta),\n\\]confidence interval :\\[\nf(x_0, \\hat{\\theta}) \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{f}_0(\\hat{\\theta})' [\\mathbf{F}(\\hat{\\theta})' \\mathbf{F}(\\hat{\\theta})]^{-1} \\mathbf{f}_0(\\hat{\\theta})}.\n\\]SummaryLinear Functions Parameters: function \\(f(x, \\theta)\\) linear \\(\\theta\\) can written form \\[f(x, \\theta) = \\theta_1 g_1(x) + \\theta_2 g_2(x) + \\dots + \\theta_p g_p(x)\\] \\(g_j(x)\\) depend \\(\\theta\\). case, Jacobian \\(\\mathbf{F}(\\theta)\\) depend \\(\\theta\\) (\\(x_i\\)), exact formulas often match familiar linear regression.Linear Functions Parameters: function \\(f(x, \\theta)\\) linear \\(\\theta\\) can written form \\[f(x, \\theta) = \\theta_1 g_1(x) + \\theta_2 g_2(x) + \\dots + \\theta_p g_p(x)\\] \\(g_j(x)\\) depend \\(\\theta\\). case, Jacobian \\(\\mathbf{F}(\\theta)\\) depend \\(\\theta\\) (\\(x_i\\)), exact formulas often match familiar linear regression.Nonlinear Functions Parameters: \\(f(x, \\theta)\\) depends \\(\\theta\\) nonlinear way (e.g., \\(\\theta_1 e^{\\theta_2 x}, \\beta_1/\\beta_2\\) complicated expressions), \\(\\mathbf{F}(\\theta)\\) depends \\(\\theta\\). Estimation generally requires iterative numerical methods (e.g., Gauss–Newton, Levenberg–Marquardt), asymptotic results rely evaluating partial derivatives \\(\\hat{\\theta}\\). Nevertheless, final inference formulas—confidence intervals, prediction intervals—similar form, thanks asymptotic normality arguments.Nonlinear Functions Parameters: \\(f(x, \\theta)\\) depends \\(\\theta\\) nonlinear way (e.g., \\(\\theta_1 e^{\\theta_2 x}, \\beta_1/\\beta_2\\) complicated expressions), \\(\\mathbf{F}(\\theta)\\) depends \\(\\theta\\). Estimation generally requires iterative numerical methods (e.g., Gauss–Newton, Levenberg–Marquardt), asymptotic results rely evaluating partial derivatives \\(\\hat{\\theta}\\). Nevertheless, final inference formulas—confidence intervals, prediction intervals—similar form, thanks asymptotic normality arguments.","code":""},{"path":"non-linear-regression.html","id":"non-linear-least-squares-estimation","chapter":"6 Non-Linear Regression","heading":"6.2 Non-linear Least Squares Estimation","text":"least squares (LS) estimate \\(\\theta\\), denoted \\(\\hat{\\theta}\\), minimizes residual sum squares:\\[\nS(\\hat{\\theta}) = SSE(\\hat{\\theta}) = \\sum_{=1}^{n} \\{Y_i - f(\\mathbf{x}_i; \\hat{\\theta})\\}^2\n\\]solve , consider partial derivatives \\(S(\\theta)\\) respect \\(\\theta_j\\) set zero, leading normal equations:\\[\n\\frac{\\partial S(\\theta)}{\\partial \\theta_j} = -2 \\sum_{=1}^{n} \\{Y_i - f(\\mathbf{x}_i; \\theta)\\} \\frac{\\partial f(\\mathbf{x}_i; \\theta)}{\\partial \\theta_j} = 0\n\\]However, equations inherently non-linear , cases, solved analytically. result, various estimation techniques employed approximate solutions efficiently. approaches include:Iterative Optimization – Methods refine estimates successive iterations minimize error.Derivative-Free Methods – Techniques rely gradient information, useful complex non-smooth functions.Stochastic Heuristic – Algorithms incorporate randomness, genetic algorithms simulated annealing, explore solution spaces.Linearization– Approximating non-linear models linear ones enable analytical numerical solutions.Hybrid Approaches – Combining multiple methods leverage respective strengths improved estimation.","code":""},{"path":"non-linear-regression.html","id":"iterative-optimization-nonlinear-regression","chapter":"6 Non-Linear Regression","heading":"6.2.1 Iterative Optimization","text":"","code":""},{"path":"non-linear-regression.html","id":"gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.1 Gauss-Newton Algorithm","text":"Gauss-Newton Algorithm iterative optimization method used estimate parameters nonlinear least squares problems. refines parameter estimates approximating Hessian matrix using first-order derivatives, making computationally efficient many practical applications (e.g., regression models finance marketing analytics). objective minimize Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} [Y_i - f_i(\\theta)]^2,\n\\]\\(\\mathbf{Y} = [Y_1, \\dots, Y_n]'\\) observed responses, \\(f_i(\\theta)\\) model-predicted values.Iterative Refinement via Taylor ExpansionThe Gauss-Newton algorithm iteratively refines initial estimate \\(\\hat{\\theta}^{(0)}\\) using Taylor series expansion \\(f(\\mathbf{x}_i; \\theta)\\) \\(\\hat{\\theta}^{(0)}\\). start observation model:\\[\nY_i = f(\\mathbf{x}_i; \\theta) + \\epsilon_i.\n\\]expanding \\(f(\\mathbf{x}_i; \\theta)\\) around \\(\\hat{\\theta}^{(0)}\\) ignoring higher-order terms (assuming remainder small), get:\\[\n\\begin{aligned}\nY_i &\\approx f(\\mathbf{x}_i; \\hat{\\theta}^{(0)})\n+ \\sum_{j=1}^{p} \\frac{\\partial f(\\mathbf{x}_i; \\theta)}{\\partial \\theta_j} \\bigg|_{\\theta = \\hat{\\theta}^{(0)}}\n\\left(\\theta_j - \\hat{\\theta}_j^{(0)}\\right)\n+ \\epsilon_i.\n\\end{aligned}\n\\]matrix form, let\\[\n\\mathbf{Y} =\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix},\n\\quad\n\\mathbf{f}(\\hat{\\theta}^{(0)}) =\n\\begin{bmatrix}\nf(\\mathbf{x}_1, \\hat{\\theta}^{(0)}) \\\\ \\vdots \\\\ f(\\mathbf{x}_n, \\hat{\\theta}^{(0)})\n\\end{bmatrix},\n\\]define Jacobian matrix partial derivatives\\[\n\\mathbf{F}(\\hat{\\theta}^{(0)}) =\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{x}_1, \\theta)}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_1, \\theta)}{\\partial \\theta_p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{x}_n, \\theta)}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_n, \\theta)}{\\partial \\theta_p}\n\\end{bmatrix}_{\\theta = \\hat{\\theta}^{(0)}}.\n\\],\\[\n\\mathbf{Y} \\approx \\mathbf{f}(\\hat{\\theta}^{(0)})\n+ \\mathbf{F}(\\hat{\\theta}^{(0)})\\,(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon,\n\\]\\(\\epsilon = [\\epsilon_1, \\dots, \\epsilon_n]'\\) assumed ..d. mean \\(0\\) variance \\(\\sigma^2\\).linear approximation,\\[\n\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(0)})\n\\approx \\mathbf{F}(\\hat{\\theta}^{(0)})\\,(\\theta - \\hat{\\theta}^{(0)}).\n\\]Solving \\(\\theta - \\hat{\\theta}^{(0)}\\) least squares sense gives Gauss increment \\(\\hat{\\delta}^{(1)}\\), can update:\\[\n\\hat{\\theta}^{(1)} = \\hat{\\theta}^{(0)} + \\hat{\\delta}^{(1)}.\n\\]Step--Step ProcedureInitialize: Start initial estimate \\(\\hat{\\theta}^{(0)}\\) set \\(j = 0\\).Compute Taylor Expansion: Calculate \\(\\mathbf{f}(\\hat{\\theta}^{(j)})\\) \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\).Solve Increment: Treating \\(\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(j)}) \\approx \\mathbf{F}(\\hat{\\theta}^{(j)})\\, (\\theta - \\hat{\\theta}^{(j)})\\) linear model, use Ordinary Least Squares compute \\(\\hat{\\delta}^{(j+1)}\\).Update Parameters: Set \\(\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)}\\).Check Convergence: convergence criteria met (see ), stop; otherwise, return Step 2.Estimate Variance: convergence, assume \\(\\epsilon \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{})\\). variance \\(\\sigma^2\\) can estimated \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\left(\\mathbf{Y} - \\mathbf{f}(\\mathbf{x}; \\hat{\\theta})\\right)' \\left(\\mathbf{Y} - \\mathbf{f}(\\mathbf{x}; \\hat{\\theta})\\right).\n\\]Convergence CriteriaCommon criteria deciding stop iterating include:Objective Function Change: \\[\n\\frac{\\left|SSE(\\hat{\\theta}^{(j+1)}) - SSE(\\hat{\\theta}^{(j)})\\right|}{SSE(\\hat{\\theta}^{(j)})} < \\gamma_1.\n\\]Parameter Change: \\[\n\\left|\\hat{\\theta}^{(j+1)} - \\hat{\\theta}^{(j)}\\right| < \\gamma_2.\n\\]Residual Projection Criterion: residuals satisfy convergence defined (D. M. Bates Watts 1981).Another way see update step viewing necessary condition minimum: gradient \\(SSE(\\theta)\\) respect \\(\\theta\\) zero. \\[\nSSE(\\theta) = \\sum_{=1}^{n} [Y_i - f_i(\\theta)]^2,\n\\]gradient \\[\n\\frac{\\partial SSE(\\theta)}{\\partial \\theta}\n= 2\\,\\mathbf{F}(\\theta)' \\left[\\mathbf{Y} - \\mathbf{f}(\\theta)\\right].\n\\]Using Gauss-Newton update rule iteration \\(j\\) \\(j+1\\):\\[\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)}\n&= \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)} \\\\\n&= \\hat{\\theta}^{(j)} + \\left[\\mathbf{F}(\\hat{\\theta}^{(j)})' \\,\\mathbf{F}(\\hat{\\theta}^{(j)})\\right]^{-1}\n\\,\\mathbf{F}(\\hat{\\theta}^{(j)})' \\left[\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(j)})\\right] \\\\\n&= \\hat{\\theta}^{(j)}\n- \\frac{1}{2} \\left[\\mathbf{F}(\\hat{\\theta}^{(j)})' \\,\\mathbf{F}(\\hat{\\theta}^{(j)})\\right]^{-1}\n\\, \\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\end{aligned}\n\\]:\\(\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector, pointing direction steepest ascent SSE.\\(\\left[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})\\right]^{-1}\\) determines step size, controlling far move direction improvement.factor \\(-\\tfrac{1}{2}\\) ensures movement direction steepest descent, helping minimize SSE.Gauss-Newton method works well nonlinear model can approximated accurately first-order Taylor expansion near solution. assumption near-linearity residual function \\(\\mathbf{r}(\\theta) = \\mathbf{Y} - \\mathbf{f}(\\theta)\\) violated, convergence may slow fail altogether. cases, robust methods like Levenberg-Marquardt Algorithm (modifies Gauss-Newton damping parameter) often preferred.defined model “nonlinear_model(theta, x)” returns Aexp(Bx).generated synthetic data using “true_theta” values added random noise.used nls.lm(...) minpack.lm package fit data:\npar = c(1, 0.1) initial parameter guess.\nfn = function(theta) y - nonlinear_model(theta, x) residual function, .e., observed minus predicted.\npar = c(1, 0.1) initial parameter guess.fn = function(theta) y - nonlinear_model(theta, x) residual function, .e., observed minus predicted.fit$par provides estimated parameters algorithm converges.","code":"\n# Load necessary libraries\nlibrary(minpack.lm)  # Provides nonlinear least squares functions\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    # theta is a vector of parameters: theta[1] = A, theta[2] = B\n    # x is the independent variable\n    # The model is A * exp(B * x)\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function for clarity\nsse <- function(theta, x, y) {\n    # SSE = sum of squared errors between actual y and model predictions\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)                     # for reproducibility\nx <- seq(0, 10, length.out = 100) # 100 points from 0 to 10\ntrue_theta <- c(2, 0.3)           # true parameter values\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Display the first few data points\nhead(data.frame(x, y))\n#>           x        y\n#> 1 0.0000000 1.719762\n#> 2 0.1010101 1.946445\n#> 3 0.2020202 2.904315\n#> 4 0.3030303 2.225593\n#> 5 0.4040404 2.322373\n#> 6 0.5050505 3.184724\n\n# Gauss-Newton optimization using nls.lm (Levenberg-Marquardt as extension).\n# Initial guess for theta: c(1, 0.1)\nfit <- nls.lm(\n    par = c(1, 0.1),\n    fn = function(theta)\n        y - nonlinear_model(theta, x)\n)\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B):\\n\")\n#> Estimated parameters (A, B):\nprint(fit$par)\n#> [1] 1.9934188 0.3008742\n# Visualize the data and the fitted model\nplot(\n  x,\n  y,\n  main = \"Data and Fitted Curve (Gauss-Newton/Levenberg-Marquardt)\",\n  xlab = \"x\",\n  ylab = \"y\",\n  pch = 19,\n  cex = 0.5\n)\ncurve(\n  nonlinear_model(fit$par, x),\n  from = 0,\n  to = 10,\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"modified-gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.2 Modified Gauss-Newton Algorithm","text":"Modified Gauss-Newton Algorithm introduces learning rate \\(\\alpha_j\\) control step size prevent overshooting local minimum. standard Gauss-Newton Algorithm assumes full step direction \\(\\hat{\\delta}^{(j+1)}\\) optimal, practice, especially highly nonlinear problems, can overstep minimum cause numerical instability. modification introduces step size reduction, making robust.redefine update step :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}, \\quad 0 < \\alpha_j < 1,\n\\]:\\(\\alpha_j\\) learning rate, controlling much step \\(\\hat{\\delta}^{(j+1)}\\) taken.\\(\\alpha_j = 1\\), recover standard Gauss-Newton method.\\(\\alpha_j\\) small, convergence slow; large, algorithm may diverge.learning rate \\(\\alpha_j\\) allows adaptive step size adjustments, helping prevent excessive parameter jumps ensuring SSE decreases iteration.common approach determine \\(\\alpha_j\\) step halving, ensuring iteration moves direction reduces SSE. Instead using fixed \\(\\alpha_j\\), iteratively reduce step size SSE decreases:\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\frac{1}{2^k}\\hat{\\delta}^{(j+1)},\n\\]:\\(k\\) smallest non-negative integer \\[\nSSE(\\hat{\\theta}^{(j)} + \\frac{1}{2^k} \\hat{\\delta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)}).\n\\]means start full step \\(\\hat{\\delta}^{(j+1)}\\), try \\(\\hat{\\delta}^{(j+1)}/2\\), \\(\\hat{\\delta}^{(j+1)}/4\\), , SSE reduced.Algorithm Step Halving:Compute Gauss-Newton step \\(\\hat{\\delta}^{(j+1)}\\).Set initial \\(\\alpha_j = 1\\).updated parameters \\(\\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}\\) increase SSE, divide \\(\\alpha_j\\) 2.Repeat SSE decreases.ensures monotonic SSE reduction, preventing divergence due overly aggressive step.Generalized Form Modified AlgorithmA general form update rule, incorporating step size control matrix \\(\\mathbf{}_j\\), :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_j \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\]:\\(\\mathbf{}_j\\) positive definite matrix preconditions update direction.\\(\\alpha_j\\) learning rate.\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient objective function \\(Q(\\theta)\\), typically SSE nonlinear regression.Connection Modified Gauss-Newton AlgorithmThe Modified Gauss-Newton Algorithm fits framework:\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} \\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}.\n\\], recognize:Objective function: \\(Q = SSE\\).Preconditioner matrix: \\([\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} = \\mathbf{}\\).Thus, standard Gauss-Newton method can interpreted special case broader optimization framework, preconditioned gradient descent approach.","code":"\n# Load required library\nlibrary(minpack.lm)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n  theta[1] * exp(theta[2] * x)\n}\n\n# Define the Sum of Squared Errors function\nsse <- function(theta, x, y) {\n  sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Gauss-Newton with Step Halving\ngauss_newton_modified <-\n  function(theta_init,\n           x,\n           y,\n           tol = 1e-6,\n           max_iter = 100) {\n    theta <- theta_init\n    for (j in 1:max_iter) {\n      # Compute Jacobian matrix numerically\n      epsilon <- 1e-6\n      F_matrix <-\n        matrix(0, nrow = length(y), ncol = length(theta))\n      for (p in 1:length(theta)) {\n        theta_perturb <- theta\n        theta_perturb[p] <- theta[p] + epsilon\n        F_matrix[, p] <-\n          (nonlinear_model(theta_perturb, x)-nonlinear_model(theta, x))/epsilon\n      }\n      \n      # Compute residuals\n      residuals <- y - nonlinear_model(theta, x)\n      \n      # Compute Gauss-Newton step\n      delta <-\n        solve(t(F_matrix) %*% F_matrix) %*% t(F_matrix) %*% residuals\n      \n      # Step Halving Implementation\n      alpha <- 1\n      k <- 0\n      while (sse(theta + alpha * delta, x, y) >= sse(theta, x, y) &&\n             k < 10) {\n        alpha <- alpha / 2\n        k <- k + 1\n      }\n      \n      # Update theta\n      theta_new <- theta + alpha * delta\n      \n      # Check for convergence\n      if (sum(abs(theta_new - theta)) < tol) {\n        break\n      }\n      \n      theta <- theta_new\n    }\n    return(theta)\n  }\n\n# Run Modified Gauss-Newton Algorithm\ntheta_init <- c(1, 0.1)  # Initial parameter guess\nestimated_theta <- gauss_newton_modified(theta_init, x, y)\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) with Modified Gauss-Newton:\\n\")\n#> Estimated parameters (A, B) with Modified Gauss-Newton:\nprint(estimated_theta)\n#>           [,1]\n#> [1,] 1.9934188\n#> [2,] 0.3008742\n\n# Plot data and fitted curve\nplot(\n  x,\n  y,\n  main = \"Modified Gauss-Newton: Data & Fitted Curve\",\n  pch = 19,\n  cex = 0.5,\n  xlab = \"x\",\n  ylab = \"y\"\n)\ncurve(\n  nonlinear_model(estimated_theta, x),\n  from = 0,\n  to = 10,\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"steepest-descent","chapter":"6 Non-Linear Regression","heading":"6.2.1.3 Steepest Descent (Gradient Descent)","text":"Steepest Descent Method, commonly known Gradient Descent, fundamental iterative optimization technique used finding parameter estimates minimize objective function \\(\\mathbf{Q}(\\theta)\\). special case Modified Gauss-Newton Algorithm, preconditioning matrix \\(\\mathbf{}_j\\) replaced identity matrix.update rule given :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_{p \\times p}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\]:\\(\\alpha_j\\) learning rate, determining step size.\\(\\mathbf{}_{p \\times p}\\) identity matrix, meaning updates occur direction negative gradient.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector, provides direction steepest ascent; negation ensures movement toward minimum.Characteristics Steepest DescentSlow converge: algorithm moves direction gradient take account curvature, may result slow convergence, especially ill-conditioned problems.Moves rapidly initially: method can exhibit fast initial progress, approaches minimum, step sizes become small, leading slow convergence.Useful initialization: Due simplicity ease implementation, often used obtain starting values advanced methods like Newton’s method Gauss-Newton Algorithm.Comparison Gauss-NewtonThe key difference Steepest Descent considers gradient direction, Gauss-Newton Newton’s method incorporate curvature information.Choosing Learning Rate \\(\\alpha_j\\)well-chosen learning rate crucial success gradient descent:large: algorithm may overshoot minimum diverge.small: Convergence slow.Adaptive strategies:\nFixed step size: \\(\\alpha_j\\) constant.\nStep size decay: \\(\\alpha_j\\) decreases iterations (e.g., \\(\\alpha_j = \\frac{1}{j}\\)).\nLine search: Choose \\(\\alpha_j\\) minimizing \\(\\mathbf{Q}(\\theta^{(j+1)})\\) along gradient direction.\nFixed step size: \\(\\alpha_j\\) constant.Step size decay: \\(\\alpha_j\\) decreases iterations (e.g., \\(\\alpha_j = \\frac{1}{j}\\)).Line search: Choose \\(\\alpha_j\\) minimizing \\(\\mathbf{Q}(\\theta^{(j+1)})\\) along gradient direction.common approach backtracking line search, \\(\\alpha_j\\) reduced iteratively decrease \\(\\mathbf{Q}(\\theta)\\) observed.Steepest Descent (Gradient Descent) moves direction steepest descent, can lead zigzagging behavior.Slow convergence occurs curvature function varies significantly across dimensions.Learning rate tuning critical:\nlarge, algorithm diverges.\nsmall, progress slow.\nlarge, algorithm diverges.large, algorithm diverges.small, progress slow.small, progress slow.Useful initialization: often used get close optimal solution switching advanced methods like Gauss-Newton Algorithm Newton’s method.Several advanced techniques improve performance steepest descent:Momentum Gradient Descent: Adds momentum term smooth updates, reducing oscillations.Momentum Gradient Descent: Adds momentum term smooth updates, reducing oscillations.Adaptive Learning Rates:\nAdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.\nRMSprop: Uses moving average past gradients scale updates.\nAdam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.\nAdaptive Learning Rates:AdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.AdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.RMSprop: Uses moving average past gradients scale updates.RMSprop: Uses moving average past gradients scale updates.Adam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.Adam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.practice, Adam widely used machine learning deep learning, Newton-based methods (including Gauss-Newton) preferred nonlinear regression.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\n\n\n# Define the nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define the Sum of Squared Errors function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Define Gradient of SSE w.r.t theta (computed numerically)\ngradient_sse <- function(theta, x, y) {\n    n <- length(y)\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Safe Gradient Descent Implementation\ngradient_descent <-\n    function(theta_init,\n             x,\n             y,\n             alpha = 0.01,\n             tol = 1e-6,\n             max_iter = 500) {\n        theta <- theta_init\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            \n            # Check for NaN or Inf values in the gradient (prevents divergence)\n            if (any(is.na(grad)) || any(is.infinite(grad))) {\n                cat(\"Numerical instability detected at iteration\",\n                    j,\n                    \"\\n\")\n                break\n            }\n            \n            # Update step\n            theta_new <- theta - alpha * grad\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Check for convergence\n            if (!is.finite(sse_values[j])) {\n                cat(\"Divergence detected at iteration\", j, \"\\n\")\n                break\n            }\n            \n            if (sum(abs(theta_new - theta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Gradient Descent with a Safe Implementation\ntheta_init <- c(1, 0.1)  # Initial guess\nalpha <- 0.001           # Learning rate\nresult <- gradient_descent(theta_init, x, y, alpha)\n#> Divergence detected at iteration 1\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Gradient Descent:\\n\")\n#> Estimated parameters (A, B) using Gradient Descent:\nprint(estimated_theta)\n#> [1] 1.0 0.1\n\n# Plot convergence of SSE over iterations\n# Ensure sse_values has valid data\nsse_df <- data.frame(\n  Iteration = seq_along(sse_values),\n  SSE = sse_values\n)\n\n# Generate improved plot using ggplot()\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Gradient Descent Convergence\",\n    x = \"Iteration\",\n    y = \"SSE\"\n  ) +\n  theme_minimal()"},{"path":"non-linear-regression.html","id":"levenberg-marquardt","chapter":"6 Non-Linear Regression","heading":"6.2.1.4 Levenberg-Marquardt Algorithm","text":"Levenberg-Marquardt Algorithm widely used optimization method solving nonlinear least squares problems. adaptive technique blends Gauss-Newton Algorithm Steepest Descent (Gradient Descent), dynamically switching based problem conditions.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) + \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\tau\\) damping parameter, controlling whether step behaves like Gauss-Newton Algorithm Steepest Descent (Gradient Descent).\\(\\mathbf{}_{p \\times p}\\) identity matrix, ensuring numerical stability.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate, determining step size.Levenberg-Marquardt algorithm particularly useful Jacobian matrix \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) nearly singular, meaning Gauss-Newton Algorithm alone may fail.\\(\\tau\\) large, method behaves like Steepest Descent, ensuring stability.\\(\\tau\\) small, behaves like Gauss-Newton, accelerating convergence.Adaptive control \\(\\tau\\):\n\\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), reduce \\(\\tau\\): \\[\n\\tau \\gets \\tau / 10\n\\]\nOtherwise, increase \\(\\tau\\) stabilize: \\[\n\\tau \\gets 10\\tau\n\\]\n\\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), reduce \\(\\tau\\): \\[\n\\tau \\gets \\tau / 10\n\\]Otherwise, increase \\(\\tau\\) stabilize: \\[\n\\tau \\gets 10\\tau\n\\]adjustment ensures algorithm moves efficiently avoiding instability.Early Stability (Flat SSE)\nSSE remains near zero first iterations, suggests algorithm initially behaving stably.\nmight indicate initial parameter guess reasonable, updates small significantly affect SSE.\nEarly Stability (Flat SSE)SSE remains near zero first iterations, suggests algorithm initially behaving stably.SSE remains near zero first iterations, suggests algorithm initially behaving stably.might indicate initial parameter guess reasonable, updates small significantly affect SSE.might indicate initial parameter guess reasonable, updates small significantly affect SSE.Sudden Explosion SSE (Iteration ~8-9)\nsharp spike SSE iteration 9 indicates numerical instability divergence optimization process.\ndue :\nill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.\nsudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.\nFloating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.\n\nSudden Explosion SSE (Iteration ~8-9)sharp spike SSE iteration 9 indicates numerical instability divergence optimization process.sharp spike SSE iteration 9 indicates numerical instability divergence optimization process.due :\nill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.\nsudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.\nFloating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.\ndue :ill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.ill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.sudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.sudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.Floating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.Floating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.Return Stability (Iteration 9)\nSSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.\nconsistent adaptive nature Levenberg-Marquardt:\nstep leads bad SSE increase, algorithm increases tau make next step conservative.\nnext step stabilizes, tau may reduced .\n\nReturn Stability (Iteration 9)SSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.SSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.consistent adaptive nature Levenberg-Marquardt:\nstep leads bad SSE increase, algorithm increases tau make next step conservative.\nnext step stabilizes, tau may reduced .\nconsistent adaptive nature Levenberg-Marquardt:step leads bad SSE increase, algorithm increases tau make next step conservative.step leads bad SSE increase, algorithm increases tau make next step conservative.next step stabilizes, tau may reduced .next step stabilizes, tau may reduced .","code":"\n# Load required libraries\nlibrary(minpack.lm)\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Robust Levenberg-Marquardt Optimization Implementation\nlevenberg_marquardt <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500,\n             tau_init = 1) {\n        theta <- theta_init\n        tau <- tau_init\n        lambda <- 1e-8  # Small regularization term\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            # Compute Jacobian matrix numerically\n            epsilon <- 1e-6\n            F_matrix <-\n                matrix(0, nrow = length(y), ncol = length(theta))\n            for (p in 1:length(theta)) {\n                theta_perturb <- theta\n                theta_perturb[p] <- theta[p] + epsilon\n                F_matrix[, p] <-\n                    (nonlinear_model(theta_perturb, x) \n                     - nonlinear_model(theta, x)) / epsilon\n            }\n            \n            # Compute residuals\n            residuals <- y - nonlinear_model(theta, x)\n            \n            # Compute Levenberg-Marquardt update\n            A <-\n                t(F_matrix) %*% F_matrix + tau * diag(length(theta)) \n            + lambda * diag(length(theta))  # Regularized A\n            delta <- tryCatch(\n                solve(A) %*% t(F_matrix) %*% residuals,\n                error = function(e) {\n                    cat(\"Singular matrix detected at iteration\",\n                        j,\n                        \"- Increasing tau\\n\")\n                    tau <<- tau * 10  # Increase tau to stabilize\n                    # Return zero delta to avoid NaN updates\n                    return(rep(0, length(theta)))  \n                }\n            )\n            \n            theta_new <- theta + delta\n            \n            # Compute new SSE\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Adjust tau dynamically\n            if (sse_values[j] < sse(theta, x, y)) {\n                # Reduce tau but prevent it from going too low\n                tau <-\n                    max(tau / 10, 1e-8)  \n            } else {\n                tau <- tau * 10  # Increase tau if SSE increases\n            }\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Levenberg-Marquardt\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- levenberg_marquardt(theta_init, x, y)\n#> Singular matrix detected at iteration 11 - Increasing tau\n#> Converged in 11 iterations.\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Levenberg-Marquardt:\\n\")\n#> Estimated parameters (A, B) using Levenberg-Marquardt:\nprint(estimated_theta)\n#>               [,1]\n#> [1,] -6.473440e-09\n#> [2,]  1.120637e+01\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", linewidth = 1) +\n    labs(title = \"Levenberg-Marquardt Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"newton-raphson","chapter":"6 Non-Linear Regression","heading":"6.2.1.5 Newton-Raphson Algorithm","text":"Newton-Raphson method second-order optimization technique used nonlinear least squares problems. Unlike first-order methods (Steepest Descent (Gradient Descent) Gauss-Newton Algorithm), Newton-Raphson uses first second derivatives objective function faster convergence.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\left[\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}\\right]^{-1} \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector (first derivative objective function).\\(\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}\\) Hessian matrix (second derivative objective function).\\(\\alpha_j\\) learning rate, controlling step size.Hessian matrix nonlinear least squares problems :\\[\n\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'} = 2 \\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) - 2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2 f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\n\\]:first term \\(2 \\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})\\) Gauss-Newton Algorithm.second term \\(-2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2 f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\\) contains second-order derivatives.Key ObservationsGauss-Newton vs. Newton-Raphson:\nGauss-Newton approximates Hessian ignoring second term.\nNewton-Raphson explicitly incorporates second-order derivatives, making precise computationally expensive.\nGauss-Newton approximates Hessian ignoring second term.Newton-Raphson explicitly incorporates second-order derivatives, making precise computationally expensive.Challenges:\nHessian matrix may singular, making impossible invert.\nComputing second derivatives often difficult complex functions.\nHessian matrix may singular, making impossible invert.Computing second derivatives often difficult complex functions.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Compute Gradient (First Derivative) of SSE\ngradient_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Compute Hessian (Second Derivative) of SSE\nhessian_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Compute second derivatives\n    H_11 <- 2 * sum(exp(2 * theta[2] * x))\n    H_12 <- 2 * sum(x * exp(2 * theta[2] * x) * theta[1])\n    H_21 <- H_12\n    \n    term1 <- 2 * sum((x ^ 2) * exp(2 * theta[2] * x) * theta[1] ^ 2)\n    term2 <- 2 * sum(residuals * (x ^ 2) * exp(theta[2] * x))\n    \n    H_22 <- term1 - term2\n    \n    return(matrix(\n        c(H_11, H_12, H_21, H_22),\n        nrow = 2,\n        byrow = TRUE\n    ))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Newton-Raphson Optimization Implementation\nnewton_raphson <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500) {\n        theta <- theta_init\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            hessian <- hessian_sse(theta, x, y)\n            \n            # Check if Hessian is invertible\n            if (det(hessian) == 0) {\n                cat(\"Hessian is singular at iteration\",\n                    j,\n                    \"- Using identity matrix instead.\\n\")\n                # Replace with identity matrix if singular\n                hessian <-\n                    diag(length(theta))  \n            }\n            \n            # Compute Newton update\n            delta <- solve(hessian) %*% grad\n            theta_new <- theta - delta\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Newton-Raphson\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- newton_raphson(theta_init, x, y)\n#> Converged in 222 iterations.\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Newton-Raphson:\\n\")\n#> Estimated parameters (A, B) using Newton-Raphson:\nprint(estimated_theta)\n#>           [,1]\n#> [1,] 1.9934188\n#> [2,] 0.3008742\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", size = 1) +\n    labs(title = \"Newton-Raphson Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"quasi-newton-method","chapter":"6 Non-Linear Regression","heading":"6.2.1.6 Quasi-Newton Method","text":"Quasi-Newton method optimization technique approximates Newton’s method without requiring explicit computation Hessian matrix. Instead, iteratively constructs approximation \\(\\mathbf{H}_j\\) Hessian based first derivative information.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{H}_j^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\mathbf{H}_j\\) symmetric positive definite approximation Hessian matrix.\\(j \\\\infty\\), \\(\\mathbf{H}_j\\) gets closer true Hessian.\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate, controlling step size.Use Quasi-Newton Instead Newton-Raphson Method?Newton-Raphson requires computing Hessian matrix explicitly, computationally expensive may singular.Quasi-Newton avoids computing Hessian directly approximating iteratively.Among first-order methods (require gradients, Hessians), Quasi-Newton methods perform best.Hessian ApproximationInstead directly computing Hessian \\(\\mathbf{H}_j\\), Quasi-Newton methods update approximation \\(\\mathbf{H}_j\\) iteratively.One widely used formulas Broyden-Fletcher-Goldfarb-Shanno (BFGS) update:\\[\n\\mathbf{H}_{j+1} = \\mathbf{H}_j + \\frac{(\\mathbf{s}_j \\mathbf{s}_j')}{\\mathbf{s}_j' \\mathbf{y}_j} - \\frac{\\mathbf{H}_j \\mathbf{y}_j \\mathbf{y}_j' \\mathbf{H}_j}{\\mathbf{y}_j' \\mathbf{H}_j \\mathbf{y}_j}\n\\]:\\(\\mathbf{s}_j = \\hat{\\theta}^{(j+1)} - \\hat{\\theta}^{(j)}\\) (change parameters).\\(\\mathbf{y}_j = \\nabla Q(\\hat{\\theta}^{(j+1)}) - \\nabla Q(\\hat{\\theta}^{(j)})\\) (change gradient).\\(\\mathbf{H}_j\\) current inverse Hessian approximation.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x))^2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Run BFGS Optimization using `optim()`\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- optim(\n    par = theta_init,\n    fn = function(theta) sse(theta, x, y),  # Minimize SSE\n    method = \"BFGS\",\n    control = list(trace = 0)  # Suppress optimization progress\n    # control = list(trace = 1, REPORT = 1)  # Print optimization progress\n)\n\n# Extract results\nestimated_theta <- result$par\nsse_final <- result$value\nconvergence_status <- result$convergence  # 0 means successful convergence\n\n# Display estimated parameters\ncat(\"\\n=== Optimization Results ===\\n\")\n#> \n#> === Optimization Results ===\ncat(\"Estimated parameters (A, B) using Quasi-Newton BFGS:\\n\")\n#> Estimated parameters (A, B) using Quasi-Newton BFGS:\nprint(estimated_theta)\n#> [1] 1.9954216 0.3007569\n\n# Display final SSE\ncat(\"\\nFinal SSE:\", sse_final, \"\\n\")\n#> \n#> Final SSE: 20.3227"},{"path":"non-linear-regression.html","id":"trust-region-reflective-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.7 Trust-Region Reflective Algorithm","text":"Trust-Region Reflective (TRR) algorithm optimization technique used nonlinear least squares problems. Unlike Newton’s method gradient-based approaches, TRR dynamically restricts updates trust region, ensuring stability preventing overshooting.goal minimize objective function \\(Q(\\theta)\\) (e.g., Sum Squared Errors, SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} Q(\\theta)\n\\]Instead taking full Newton step, TRR solves following quadratic subproblem:\\[\n\\min_{\\delta} m_j(\\delta) = Q(\\hat{\\theta}^{(j)}) + \\nabla Q(\\hat{\\theta}^{(j)})' \\delta + \\frac{1}{2} \\delta' \\mathbf{H}_j \\delta\n\\]subject :\\[\n\\|\\delta\\| \\leq \\Delta_j\n\\]:\\(\\mathbf{H}_j\\) approximation Hessian matrix.\\(\\mathbf{H}_j\\) approximation Hessian matrix.\\(\\nabla Q(\\hat{\\theta}^{(j)})\\) gradient vector.\\(\\nabla Q(\\hat{\\theta}^{(j)})\\) gradient vector.\\(\\Delta_j\\) trust-region radius, adjusted dynamically.\\(\\Delta_j\\) trust-region radius, adjusted dynamically.Trust-Region AdjustmentsThe algorithm modifies step size dynamically based ratio \\(\\rho_j\\):\\[\n\\rho_j = \\frac{Q(\\hat{\\theta}^{(j)}) - Q(\\hat{\\theta}^{(j)} + \\delta)}{m_j(0) - m_j(\\delta)}\n\\]\\(\\rho_j > 0.75\\) \\(\\|\\delta\\| = \\Delta_j\\), expand trust region: \\[\n\\Delta_{j+1} = 2 \\Delta_j\n\\]\\(\\rho_j > 0.75\\) \\(\\|\\delta\\| = \\Delta_j\\), expand trust region: \\[\n\\Delta_{j+1} = 2 \\Delta_j\n\\]\\(\\rho_j < 0.25\\), shrink trust region: \\[\n\\Delta_{j+1} = \\frac{1}{2} \\Delta_j\n\\]\\(\\rho_j < 0.25\\), shrink trust region: \\[\n\\Delta_{j+1} = \\frac{1}{2} \\Delta_j\n\\]\\(\\rho_j > 0\\), accept step; otherwise, reject .\\(\\rho_j > 0\\), accept step; otherwise, reject .step violates constraint, reflected back feasible region:\\[\n\\hat{\\theta}^{(j+1)} = \\max(\\hat{\\theta}^{(j)} + \\delta, \\theta_{\\min})\n\\]ensures optimization respects parameter bounds.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Compute Gradient (First Derivative) of SSE\ngradient_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Compute Hessian Approximation of SSE\nhessian_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Compute second derivatives\n    H_11 <- 2 * sum(exp(2 * theta[2] * x))\n    H_12 <- 2 * sum(x * exp(2 * theta[2] * x) * theta[1])\n    H_21 <- H_12\n    \n    term1 <- 2 * sum((x ^ 2) * exp(2 * theta[2] * x) * theta[1] ^ 2)\n    term2 <- 2 * sum(residuals * (x ^ 2) * exp(theta[2] * x))\n    \n    H_22 <- term1 - term2\n    \n    return(matrix(\n        c(H_11, H_12, H_21, H_22),\n        nrow = 2,\n        byrow = TRUE\n    ))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Manual Trust-Region Reflective Optimization Implementation\ntrust_region_reflective <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500,\n             delta_max = 1.0) {\n        theta <- theta_init\n        delta_j <- 0.5  # Initial trust-region size\n        n <- length(theta)\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            hessian <- hessian_sse(theta, x, y)\n            \n            # Check if Hessian is invertible\n            if (det(hessian) == 0) {\n                cat(\"Hessian is singular at iteration\",\n                    j,\n                    \"- Using identity matrix instead.\\n\")\n                hessian <-\n                    diag(n)  # Replace with identity matrix if singular\n            }\n            \n            # Compute Newton step\n            delta_full <- -solve(hessian) %*% grad\n            \n            # Apply trust-region constraint\n            if (sqrt(sum(delta_full ^ 2)) > delta_j) {\n                # Scale step\n                delta <-\n                    (delta_j / sqrt(sum(delta_full ^ 2))) * delta_full  \n            } else {\n                delta <- delta_full\n            }\n            \n            # Compute new theta and ensure it respects constraints\n            theta_new <-\n                pmax(theta + delta, c(0,-Inf))  # Reflect to lower bound\n            sse_new <- sse(theta_new, x, y)\n            \n            # Compute agreement ratio (rho_j)\n            predicted_reduction <-\n                -t(grad) %*% delta - 0.5 * t(delta) %*% hessian %*% delta\n            actual_reduction <- sse(theta, x, y) - sse_new\n            rho_j <- actual_reduction / predicted_reduction\n            \n            # Adjust trust region size\n            if (rho_j < 0.25) {\n                delta_j <- max(delta_j / 2, 1e-4)  # Shrink\n            } else if (rho_j > 0.75 &&\n                       sqrt(sum(delta ^ 2)) == delta_j) {\n                delta_j <- min(2 * delta_j, delta_max)  # Expand\n            }\n            \n            # Accept or reject step\n            if (rho_j > 0) {\n                theta <- theta_new  # Accept step\n            } else {\n                cat(\"Step rejected at iteration\", j, \"\\n\")\n            }\n            \n            sse_values[j] <- sse(theta, x, y)\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta, sse_values = sse_values[1:j]))\n            }\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Manual Trust-Region Reflective Algorithm\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- trust_region_reflective(theta_init, x, y)\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", size = 1) +\n    labs(title = \"Trust-Region Reflective Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"derivative-free","chapter":"6 Non-Linear Regression","heading":"6.2.2 Derivative-Free","text":"","code":""},{"path":"non-linear-regression.html","id":"secant-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.1 Secant Method","text":"Secant Method root-finding algorithm approximates derivative using finite differences, making derivative-free alternative Newton’s method. particularly useful exact gradient (Jacobian case optimization problems) unavailable expensive compute.nonlinear optimization, apply Secant Method iteratively refine parameter estimates without explicitly computing second-order derivatives.one dimension, Secant Method approximates derivative :\\[\nf'(\\theta) \\approx \\frac{f(\\theta_{j}) - f(\\theta_{j-1})}{\\theta_{j} - \\theta_{j-1}}.\n\\]Using approximation, update step Secant Method follows:\\[\n\\theta_{j+1} = \\theta_j - f(\\theta_j) \\frac{\\theta_j - \\theta_{j-1}}{f(\\theta_j) - f(\\theta_{j-1})}.\n\\]Instead computing exact derivative (Newton’s method), use difference last two iterates approximate . makes Secant Method efficient cases gradient computation expensive infeasible.higher dimensions, Secant Method extends approximate Quasi-Newton Method, often referred Broyden’s Method. iteratively approximate inverse Hessian matrix using past updates.update formula vector-valued function \\(F(\\theta)\\) :\\[\n\\theta^{(j+1)} = \\theta^{(j)} - \\mathbf{B}^{(j)} F(\\theta^{(j)}),\n\\]\\(\\mathbf{B}^{(j)}\\) approximation inverse Jacobian matrix, updated step using:\\[\n\\mathbf{B}^{(j+1)} = \\mathbf{B}^{(j)} + \\frac{(\\Delta \\theta^{(j)} - \\mathbf{B}^{(j)} \\Delta F^{(j)}) (\\Delta \\theta^{(j)})'}{(\\Delta \\theta^{(j)})' \\Delta F^{(j)}},\n\\]:\\(\\Delta \\theta^{(j)} = \\theta^{(j+1)} - \\theta^{(j)}\\),\\(\\Delta F^{(j)} = F(\\theta^{(j+1)}) - F(\\theta^{(j)})\\).secant-based update approximates behavior true Jacobian inverse, reducing computational cost compared full Newton’s method.Algorithm: Secant Method Nonlinear OptimizationThe Secant Method nonlinear optimization follows steps:Initialize parameters \\(\\theta^{(0)}\\) \\(\\theta^{(1)}\\) (two starting points).Compute function values \\(F(\\theta^{(0)})\\) \\(F(\\theta^{(1)})\\).Use Secant update formula compute next iterate \\(\\theta^{(j+1)}\\).Update approximate inverse Jacobian \\(\\mathbf{B}^{(j)}\\).Repeat convergence.","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n\n# Define a nonlinear function (logistic model)\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    return(sum((y - nonlinear_model(theta, x)) ^ 2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Improved Secant Method with Line Search\nsecant_method_improved <-\n    function(theta0,\n             theta1,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 100) {\n        theta_prev <- as.matrix(theta0)  # Convert to column vector\n        theta_curr <- as.matrix(theta1)\n        \n        alpha <- 1  # Initial step size\n        step_reduction_factor <- 0.5  # Reduce step if SSE increases\n        B_inv <- diag(length(theta0))  # Identity matrix initialization\n        \n        for (j in 1:max_iter) {\n            # Compute function values using numerical gradient\n            F_prev <-\n                as.matrix(grad(function(theta)\n                    sse(theta, x, y), theta_prev))\n            F_curr <-\n                as.matrix(grad(function(theta)\n                    sse(theta, x, y), theta_curr))\n            \n            # Compute secant step update (convert to column vectors)\n            delta_theta <- as.matrix(theta_curr - theta_prev)\n            delta_F <- as.matrix(F_curr - F_prev)\n            \n            # Prevent division by zero\n            if (sum(abs(delta_F)) < 1e-8) {\n                cat(\"Gradient diff is too small, stopping optimization.\\n\")\n                break\n            }\n            \n            # Ensure correct dimensions for Broyden update\n            numerator <-\n                (delta_theta - B_inv %*% delta_F) %*% t(delta_theta)\n            # Convert scalar to numeric\n            denominator <-\n                as.numeric(t(delta_theta) %*% delta_F)  \n            \n            # Updated inverse Jacobian approximation\n            B_inv <-\n                B_inv + numerator / denominator  \n            \n            # Compute next theta using secant update\n            theta_next <- theta_curr - alpha * B_inv %*% F_curr\n            \n            # Line search: Reduce step size if SSE increases\n            while (sse(as.vector(theta_next), x, y) > sse(as.vector(theta_curr), \n                                                          x, y)) {\n                alpha <- alpha * step_reduction_factor\n                theta_next <- theta_curr - alpha * B_inv %*% F_curr\n                \n                # Print progress\n                # cat(\"Reducing step size to\", alpha, \"\\n\")\n            }\n            \n            # Print intermediate results for debugging\n            cat(\n                sprintf(\n                    \"Iteration %d: Theta = (%.4f, %.4f, %.4f), Alpha = %.4f\\n\",\n                    j,\n                    theta_next[1],\n                    theta_next[2],\n                    theta_next[3],\n                    alpha\n                )\n            )\n            \n            # Check convergence\n            if (sum(abs(theta_next - theta_curr)) < tol) {\n                cat(\"Converged successfully.\\n\")\n                break\n            }\n            \n            # Update iterates\n            theta_prev <- theta_curr\n            theta_curr <- theta_next\n        }\n        \n        return(as.vector(theta_curr))  # Convert back to numeric vector\n    }\n\n# Adjusted initial parameter guesses\ntheta0 <- c(2, 0.8,-1)   # Closer to true parameters\ntheta1 <- c(4, 1.2,-0.5)  # Slightly refined\n\n# Run Improved Secant Method\nestimated_theta <- secant_method_improved(theta0, theta1, x, y)\n#> Iteration 1: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0156\n#> Iteration 2: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0000\n#> Converged successfully.\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B, C) using Secant Method:\\n\")\n#> Estimated parameters (A, B, C) using Secant Method:\nprint(estimated_theta)\n#> [1] 3.85213912 1.30538435 0.00566417\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Secant Method: Data & Fitted Curve\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = -5,\n    to = 5,\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"grid-search","chapter":"6 Non-Linear Regression","heading":"6.2.2.2 Grid Search","text":"Grid Search (GS) brute-force optimization method systematically explores grid possible parameter values identify combination minimizes residual sum squares (RSS). Unlike gradient-based optimization, moves iteratively towards minimum, grid search evaluates predefined parameter combinations, making robust computationally expensive.Grid search particularly useful :function highly nonlinear, making gradient methods unreliable.function highly nonlinear, making gradient methods unreliable.parameter space small, allowing exhaustive search.parameter space small, allowing exhaustive search.global minimum needed, prior knowledge parameter ranges exists.global minimum needed, prior knowledge parameter ranges exists.goal Grid Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\\\Theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]Grid search discretizes search space \\(\\Theta\\) finite set candidate values parameter:\\[\n\\Theta = \\theta_1 \\times \\theta_2 \\times \\dots \\times \\theta_p.\n\\]accuracy solution depends grid resolution—finer grid leads better accuracy higher computational cost.Grid Search AlgorithmDefine grid possible values parameter.Evaluate SSE combination parameters.Select parameter set minimizes SSE.","code":"\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Grid Search Optimization\ngrid_search_optimization <- function(x, y, grid_size = 10) {\n    # Define grid of parameter values\n    A_values <- seq(2, 6, length.out = grid_size)\n    B_values <- seq(0.5, 3, length.out = grid_size)\n    C_values <- seq(-2, 2, length.out = grid_size)\n    \n    # Generate all combinations of parameters\n    param_grid <- expand.grid(A = A_values, B = B_values, C = C_values)\n    \n    # Evaluate SSE for each parameter combination\n    param_grid$SSE <- apply(param_grid, 1, function(theta) {\n        sse(as.numeric(theta[1:3]), x, y)\n    })\n    \n    # Select the best parameter set\n    best_params <- param_grid[which.min(param_grid$SSE), 1:3]\n    return(as.numeric(best_params))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Grid Search\nestimated_theta <- grid_search_optimization(x, y, grid_size = 20)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Grid Search:\\n\")\n#> Estimated parameters (A, B, C) using Grid Search:\nprint(estimated_theta)\n#> [1] 4.1052632 1.4210526 0.1052632\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Grid Search: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"nelder-mead","chapter":"6 Non-Linear Regression","heading":"6.2.2.3 Nelder-Mead (Simplex)","text":"Nelder-Mead algorithm, also known Simplex method, derivative-free optimization algorithm iteratively adjusts geometric shape (simplex) find minimum objective function. particularly useful nonlinear regression gradient-based methods fail due non-differentiability noisy function evaluations.Nelder-Mead particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low-dimensional.parameter space low-dimensional.goal Nelder-Mead find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Simplex RepresentationThe algorithm maintains simplex, geometric shape \\(p+1\\) vertices \\(p\\)-dimensional parameter space.vertex represents parameter vector:\\[\nS = \\{ \\theta_1, \\theta_2, \\dots, \\theta_{p+1} \\}.\n\\]2. Simplex OperationsAt iteration, algorithm updates simplex based objective function values vertex:Reflection: Reflect worst point \\(\\theta_h\\) across centroid.\n\\[\n\\theta_r = \\theta_c + \\alpha (\\theta_c - \\theta_h)\n\\]Reflection: Reflect worst point \\(\\theta_h\\) across centroid.\\[\n\\theta_r = \\theta_c + \\alpha (\\theta_c - \\theta_h)\n\\]Expansion: reflection improves objective, expand step.\n\\[\n\\theta_e = \\theta_c + \\gamma (\\theta_r - \\theta_c)\n\\]Expansion: reflection improves objective, expand step.\\[\n\\theta_e = \\theta_c + \\gamma (\\theta_r - \\theta_c)\n\\]Contraction: reflection worsens objective, contract towards centroid.\n\\[\n\\theta_c = \\theta_c + \\rho (\\theta_h - \\theta_c)\n\\]Contraction: reflection worsens objective, contract towards centroid.\\[\n\\theta_c = \\theta_c + \\rho (\\theta_h - \\theta_c)\n\\]Shrink: contraction fails, shrink simplex.\n\\[\n\\theta_i = \\theta_1 + \\sigma (\\theta_i - \\theta_1), \\quad \\forall > 1\n\\]Shrink: contraction fails, shrink simplex.\\[\n\\theta_i = \\theta_1 + \\sigma (\\theta_i - \\theta_1), \\quad \\forall > 1\n\\]:\\(\\alpha = 1\\) (reflection coefficient),\\(\\alpha = 1\\) (reflection coefficient),\\(\\gamma = 2\\) (expansion coefficient),\\(\\gamma = 2\\) (expansion coefficient),\\(\\rho = 0.5\\) (contraction coefficient),\\(\\rho = 0.5\\) (contraction coefficient),\\(\\sigma = 0.5\\) (shrink coefficient).\\(\\sigma = 0.5\\) (shrink coefficient).process continues convergence.Nelder-Mead AlgorithmInitialize simplex \\(p+1\\) vertices.Evaluate SSE vertex.Perform reflection, expansion, contraction, shrink operations.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Nelder-Mead Optimization\nnelder_mead_optimization <- function(x, y) {\n    # Initial guess for parameters\n    initial_guess <- c(2, 1, 0)\n    \n    # Run Nelder-Mead optimization\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x,\n        y = y,\n        method = \"Nelder-Mead\",\n        control = list(maxit = 500)\n    )\n    \n    return(result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Nelder-Mead Optimization\nestimated_theta <- nelder_mead_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Nelder-Mead:\\n\")\n#> Estimated parameters (A, B, C) using Nelder-Mead:\nprint(estimated_theta)\n#> [1] 4.06873116 1.42759898 0.01119379\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Nelder-Mead: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"powells-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.4 Powell’s Method","text":"Powell’s Method derivative-free optimization algorithm minimizes function without using gradients. works iteratively refining set search directions efficiently navigate parameter space. Unlike Nelder-Mead (Simplex), adapts simplex, Powell’s method builds orthogonal basis search directions.Powell’s method particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low--moderate dimensional.parameter space low--moderate dimensional.goal Powell’s Method find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Search DirectionsPowell’s method maintains set search directions \\(\\mathbf{d}_1, \\mathbf{d}_2, \\dots, \\mathbf{d}_p\\):\\[\nD = \\{ \\mathbf{d}_1, \\mathbf{d}_2, ..., \\mathbf{d}_p \\}.\n\\]Initially, directions chosen unit basis vectors (optimizing single parameter independently).2. Line MinimizationFor direction \\(\\mathbf{d}_j\\), Powell’s method performs 1D optimization:\\[\n\\theta' = \\theta + \\lambda \\mathbf{d}_j,\n\\]\\(\\lambda\\) chosen minimize \\(SSE(\\theta')\\).3. Updating Search DirectionsAfter optimizing along directions:largest improvement direction \\(\\mathbf{d}_{\\max}\\) replaced :\\[\n\\mathbf{d}_{\\text{new}} = \\theta_{\\text{final}} - \\theta_{\\text{initial}}.\n\\]new direction set orthogonalized using Gram-Schmidt.ensures efficient exploration parameter space.4. Convergence CriteriaPowell’s method stops function improvement tolerance level:\\[\n|SSE(\\theta_{t+1}) - SSE(\\theta_t)| < \\epsilon.\n\\]Powell’s Method AlgorithmInitialize search directions (standard basis vectors).Perform 1D line searches along direction.Update search directions based largest improvement.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Powell's Method Optimization\npowell_optimization <- function(x, y) {\n    # Initial guess for parameters\n    initial_guess <- c(2, 1, 0)\n    \n    # Run Powell’s optimization (via BFGS without gradient)\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x,\n        y = y,\n        method = \"BFGS\",\n        control = list(maxit = 500),\n        gr = NULL  # No gradient used\n    )\n    \n    return(result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Powell's Method Optimization\nestimated_theta <- powell_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Powell’s Method:\\n\")\n#> Estimated parameters (A, B, C) using Powell’s Method:\nprint(estimated_theta)\n#> [1] 4.06876538 1.42765687 0.01128753\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Powell's Method: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"random-search","chapter":"6 Non-Linear Regression","heading":"6.2.2.5 Random Search","text":"Random Search (RS) simple yet effective optimization algorithm explores search space randomly sampling candidate solutions. Unlike grid search, evaluates predefined parameter combinations, random search selects random subset, reducing computational cost.Random search particularly useful :search space large, making grid search impractical.search space large, making grid search impractical.Gradient-based methods fail due non-differentiability noisy data.Gradient-based methods fail due non-differentiability noisy data.optimal region unknown, making global exploration essential.optimal region unknown, making global exploration essential.goal Random Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\\\Theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]Unlike grid search, random search evaluate parameter combinations instead randomly samples subset:\\[\n\\Theta_{\\text{sampled}} \\subset \\Theta.\n\\]accuracy solution depends number random samples—higher number increases likelihood finding good solution.Random Search AlgorithmDefine random sampling range parameter.Randomly sample \\(N\\) parameter sets.Evaluate SSE sampled set.Select parameter set minimizes SSE.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n  return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n  return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n  predictions <- nonlinear_model(theta, x)\n  return(sum((y - predictions) ^ 2))\n}\n\n# Random Search Optimization\nrandom_search_optimization <- function(x, y, num_samples = 100) {\n  # Define parameter ranges\n  A_range <-\n    runif(num_samples, 2, 6)   # Random values between 2 and 6\n  B_range <-\n    runif(num_samples, 0.5, 3) # Random values between 0.5 and 3\n  C_range <-\n    runif(num_samples,-2, 2)  # Random values between -2 and 2\n  \n  # Initialize best parameters\n  best_theta <- NULL\n  best_sse <- Inf\n  \n  # Evaluate randomly sampled parameter sets\n  for (i in 1:num_samples) {\n    theta <- c(A_range[i], B_range[i], C_range[i])\n    current_sse <- sse(theta, x, y)\n    \n    if (current_sse < best_sse) {\n      best_sse <- current_sse\n      best_theta <- theta\n    }\n  }\n  \n  return(best_theta)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Random Search\nestimated_theta <-\n  random_search_optimization(x, y, num_samples = 500)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Random Search:\\n\")\n#> Estimated parameters (A, B, C) using Random Search:\nprint(estimated_theta)\n#> [1] 4.0893431 1.4687456 0.1024474\n\n# Plot data and fitted curve\nplot(\n  x,\n  y,\n  main = \"Random Search: Nonlinear Regression Optimization\",\n  pch = 19,\n  cex = 0.5,\n  xlab = \"x\",\n  ylab = \"y\"\n)\ncurve(\n  nonlinear_model(estimated_theta, x),\n  from = min(x),\n  to = max(x),\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"hooke-jeeves","chapter":"6 Non-Linear Regression","heading":"6.2.2.6 Hooke-Jeeves Pattern Search","text":"Hooke-Jeeves Pattern Search derivative-free optimization algorithm searches optimal solution exploring adjusting parameter values iteratively. Unlike gradient-based methods, require differentiability, making effective non-smooth noisy functions.Hooke-Jeeves particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low--moderate dimensional.parameter space low--moderate dimensional.goal Hooke-Jeeves Pattern Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Exploratory MovesAt iteration, algorithm perturbs parameter find lower SSE:\\[\n\\theta' = \\theta \\pm \\delta.\n\\]new parameter \\(\\theta'\\) reduces SSE, becomes new base point.2. Pattern MovesAfter improvement, algorithm accelerates movement promising direction:\\[\n\\theta_{\\text{new}} = \\theta_{\\text{old}} + (\\theta_{\\text{old}} - \\theta_{\\text{prev}}).\n\\]speeds convergence towards optimum.3. Step Size AdaptationIf improvement found, step size \\(\\delta\\) reduced:\\[\n\\delta_{\\text{new}} = \\beta \\cdot \\delta.\n\\]\\(\\beta < 1\\) reduction factor.4. Convergence CriteriaThe algorithm stops step size sufficiently small:\\[\n\\delta < \\epsilon.\n\\]Hooke-Jeeves AlgorithmInitialize starting point \\(\\theta\\) step size \\(\\delta\\).Perform exploratory moves parameter direction.improvement found, perform pattern moves.Reduce step size improvement found.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Hooke-Jeeves Pattern Search Optimization\nhooke_jeeves_optimization <-\n    function(x,\n             y,\n             step_size = 0.5,\n             tol = 1e-6,\n             max_iter = 500) {\n        # Initial guess for parameters\n        theta <- c(2, 1, 0)\n        best_sse <- sse(theta, x, y)\n        step <- step_size\n        iter <- 0\n        \n        while (step > tol & iter < max_iter) {\n            iter <- iter + 1\n            improved <- FALSE\n            new_theta <- theta\n            \n            # Exploratory move in each parameter direction\n            for (i in 1:length(theta)) {\n                for (delta in c(step,-step)) {\n                    theta_test <- new_theta\n                    theta_test[i] <- theta_test[i] + delta\n                    sse_test <- sse(theta_test, x, y)\n                    \n                    if (sse_test < best_sse) {\n                        best_sse <- sse_test\n                        new_theta <- theta_test\n                        improved <- TRUE\n                    }\n                }\n            }\n            \n            # Pattern move if improvement found\n            if (improved) {\n                theta <- 2 * new_theta - theta\n                best_sse <- sse(theta, x, y)\n            } else {\n                step <- step / 2  # Reduce step size\n            }\n        }\n        \n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Hooke-Jeeves Optimization\nestimated_theta <- hooke_jeeves_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Hooke-Jeeves:\\n\")\n#> Estimated parameters (A, B, C) using Hooke-Jeeves:\nprint(estimated_theta)\n#> [1] 4 1 0\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Hooke-Jeeves: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"bisection-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.7 Bisection Method","text":"Bisection Method fundamental numerical technique primarily used root finding, can also adapted optimization problems goal minimize maximize nonlinear function.nonlinear regression, optimization often involves finding parameter values minimize sum squared errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta)\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} \\left( y_i - f(x_i; \\theta) \\right)^2.\n\\]Since minimum function occurs derivative equals zero, apply Bisection Method derivative SSE function:\\[\n\\frac{d}{d\\theta} SSE(\\theta) = 0.\n\\]transforms optimization problem root-finding problem.Bisection Method based Intermediate Value Theorem, states:continuous function \\(f(\\theta)\\) satisfies \\(f(\\theta_a) \\cdot f(\\theta_b) < 0\\),\nexists least one root interval \\((\\theta_a, \\theta_b)\\).optimization, apply principle first derivative objective function \\(Q(\\theta)\\):\\[\nQ'(\\theta) = 0.\n\\]Step--Step Algorithm OptimizationChoose interval \\([\\theta_a, \\theta_b]\\) : \\[ Q'(\\theta_a) \\cdot Q'(\\theta_b) < 0. \\]Compute midpoint: \\[\n\\theta_m = \\frac{\\theta_a + \\theta_b}{2}.\n\\]Evaluate \\(Q'(\\theta_m)\\):\n\\(Q'(\\theta_m) = 0\\), \\(\\theta_m\\) optimal point.\n\\(Q'(\\theta_a) \\cdot Q'(\\theta_m) < 0\\), set \\(\\theta_b = \\theta_m\\).\nOtherwise, set \\(\\theta_a = \\theta_m\\).\n\\(Q'(\\theta_m) = 0\\), \\(\\theta_m\\) optimal point.\\(Q'(\\theta_a) \\cdot Q'(\\theta_m) < 0\\), set \\(\\theta_b = \\theta_m\\).Otherwise, set \\(\\theta_a = \\theta_m\\).Repeat convergence: \\[\n|\\theta_b - \\theta_a| < \\epsilon.\n\\]Determining Nature Critical PointSince Bisection Method finds stationary point, use second derivative test classify :\\(Q''(\\theta) > 0\\), point local minimum.\\(Q''(\\theta) < 0\\), point local maximum.nonlinear regression, expect \\(Q(\\theta) = SSE(\\theta)\\), solution found Bisection minimum.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Optimize all three parameters simultaneously\nfind_optimal_parameters <- function(x, y) {\n    # Initial guess for parameters (based on data)\n    initial_guess <- c(max(y), 1, median(x))  \n\n    # Bounds for parameters\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n\n    # Run optim() with L-BFGS-B (bounded optimization)\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x, y = y,\n        method = \"L-BFGS-B\",\n        lower = lower_bounds,\n        upper = upper_bounds\n    )\n    \n    return(result$par)  # Extract optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Find optimal parameters using optim()\nestimated_theta <- find_optimal_parameters(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using optim():\\n\")\n#> Estimated parameters (A, B, C) using optim():\nprint(estimated_theta)\n#> [1] 4.06876536 1.42765688 0.01128756\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Optim(): Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"stochastic-heuristic-nolinear-regression","chapter":"6 Non-Linear Regression","heading":"6.2.3 Stochastic Heuristic","text":"","code":""},{"path":"non-linear-regression.html","id":"differential-evolution-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.3.1 Differential Evolution Algorithm","text":"Differential Evolution (DE) Algorithm stochastic, population-based optimization algorithm widely used solving complex global optimization problems. Unlike gradient-based methods Newton’s method Secant method, DE require derivatives well-suited optimizing non-differentiable, nonlinear, multimodal functions.Key Features Differential EvolutionPopulation-based approach: Maintains population candidate solutions instead single point.Mutation crossover: Introduces variations explore search space.Selection mechanism: Retains best candidates next generation.Global optimization: Avoids local minima using stochastic search strategies.Mathematical Formulation Differential EvolutionDifferential Evolution operates population candidate solutions \\(\\{\\theta_i\\}\\), \\(\\theta_i\\) vector parameters. algorithm iteratively updates population using three main operations:1. MutationFor candidate solution \\(\\theta_i\\), mutant vector \\(\\mathbf{v}_i^{(j)}\\) generated :\\[\n\\mathbf{v}_i^{(j)} = \\mathbf{\\theta}_{r_1}^{(j)} + F \\cdot (\\mathbf{\\theta}_{r_2}^{(j)} - \\mathbf{\\theta}_{r_3}^{(j)})\n\\]:\\(\\mathbf{\\theta}_{r_1}, \\mathbf{\\theta}_{r_2}, \\mathbf{\\theta}_{r_3}\\) randomly selected distinct vectors population.\\(\\mathbf{\\theta}_{r_1}, \\mathbf{\\theta}_{r_2}, \\mathbf{\\theta}_{r_3}\\) randomly selected distinct vectors population.\\(F \\(0,2)\\) mutation factor controlling step size.\\(F \\(0,2)\\) mutation factor controlling step size.2. CrossoverA trial vector \\(\\mathbf{u}_i^{(j)}\\) generated combining mutant vector \\(\\mathbf{v}_i^{(j)}\\) original solution \\(\\mathbf{\\theta}_i^{(j)}\\):\\[\nu_{,k}^{(j)} =\n\\begin{cases}\nv_{,k}^{(j)}  & \\text{} rand_k \\leq C_r \\text{ } k = k_{\\text{rand}}, \\\\\n\\theta_{,k}^{(j)}  & \\text{otherwise}.\n\\end{cases}\n\\]:\\(C_r \\(0,1)\\) crossover probability.\\(C_r \\(0,1)\\) crossover probability.\\(rand_k\\) random value 0 1.\\(rand_k\\) random value 0 1.\\(k_{\\text{rand}}\\) ensures least one parameter mutated.\\(k_{\\text{rand}}\\) ensures least one parameter mutated.3. SelectionThe new candidate solution accepted improves objective function:\\[\n\\mathbf{\\theta}_i^{(j+1)} =\n\\begin{cases}\n\\mathbf{u}_i^{(j)} & \\text{} Q(\\mathbf{u}_i^{(j)}) < Q(\\mathbf{\\theta}_i^{(j)}), \\\\\n\\mathbf{\\theta}_i^{(j)} & \\text{otherwise}.\n\\end{cases}\n\\]\\(Q(\\theta)\\) objective function (e.g., sum squared errors regression problems).Algorithm: Differential Evolution Nonlinear OptimizationInitialize population candidate solutions.Evaluate objective function candidate.Mutate individuals using difference strategy.Apply crossover create trial solutions.Select individuals based fitness (objective function value).Repeat convergence (stopping criterion met).","code":"\n# Load required library\nlibrary(DEoptim)\n\n# Define a nonlinear function (logistic model)\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    return(sum((y - nonlinear_model(theta, x))^2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Define the objective function for DEoptim\nobjective_function <- function(theta) {\n    return(sse(theta, x, y))\n}\n\n# Define parameter bounds\nlower_bounds <- c(0, 0, -5)\nupper_bounds <- c(10, 5, 5)\n\n# Run Differential Evolution Algorithm\nde_result <-\n    DEoptim(\n        objective_function,\n        lower_bounds,\n        upper_bounds,\n        DEoptim.control(\n            NP = 50,\n            itermax = 100,\n            F = 0.8,\n            CR = 0.9, \n            trace = F\n        )\n    )\n\n# Extract optimized parameters\nestimated_theta <- de_result$optim$bestmem\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B, C) using Differential Evolution:\\n\")\n#> Estimated parameters (A, B, C) using Differential Evolution:\nprint(estimated_theta)\n#>       par1       par2       par3 \n#> 4.06876562 1.42765614 0.01128768\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Differential Evolution: Data & Fitted Curve\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = -5,\n    to = 5,\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"simulated-annealing","chapter":"6 Non-Linear Regression","heading":"6.2.3.2 Simulated Annealing","text":"Simulated Annealing (SA) probabilistic global optimization algorithm inspired annealing metallurgy, material heated slowly cooled remove defects. optimization, SA gradually refines solution exploring search space, allowing occasional jumps escape local minima, converging optimal solution.Simulated Annealing particularly useful :function highly nonlinear multimodal.function highly nonlinear multimodal.Gradient-based methods struggle due non-differentiability poor initialization.Gradient-based methods struggle due non-differentiability poor initialization.global minimum needed, rather local one.global minimum needed, rather local one.1. Energy Function (Objective Function)goal SA minimize objective function \\(Q(\\theta)\\). nonlinear regression, Sum Squared Errors (SSE):\\[\nQ(\\theta) = SSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]2. Probability AcceptanceAt step, SA randomly perturbs parameters \\(\\theta\\) create new candidate solution \\(\\theta'\\) evaluates change SSE:\\[\n\\Delta Q = Q(\\theta') - Q(\\theta).\n\\]Metropolis Criterion determines whether accept new solution:\\[\nP(\\text{accept}) =\n\\begin{cases}\n1, & \\text{} \\Delta Q < 0 \\quad \\text{(new solution improves fit)} \\\\\n\\exp\\left( -\\frac{\\Delta Q}{T} \\right), & \\text{} \\Delta Q \\geq 0 \\quad \\text{(accept probability)}.\n\\end{cases}\n\\]:\\(T\\) temperature gradually decreases iterations.\\(T\\) temperature gradually decreases iterations.Worse solutions accepted small probability escape local minima.Worse solutions accepted small probability escape local minima.3. Cooling ScheduleThe temperature follows cooling schedule:\\[\nT_k = \\alpha T_{k-1},\n\\]\\(\\alpha \\(0,1)\\) decay factor controls cooling speed.Simulated Annealing AlgorithmInitialize parameters \\(\\theta\\) randomly.Set initial temperature \\(T_0\\) cooling rate \\(\\alpha\\).Repeat max iterations:\nGenerate perturbed candidate \\(\\theta'\\).\nCompute \\(\\Delta Q = Q(\\theta') - Q(\\theta)\\).\nAccept \\(\\Delta Q < 0\\) probability \\(\\exp(-\\Delta Q / T)\\).\nReduce temperature: \\(T \\leftarrow \\alpha T\\).\nGenerate perturbed candidate \\(\\theta'\\).Compute \\(\\Delta Q = Q(\\theta') - Q(\\theta)\\).Accept \\(\\Delta Q < 0\\) probability \\(\\exp(-\\Delta Q / T)\\).Reduce temperature: \\(T \\leftarrow \\alpha T\\).Return best solution found.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Simulated Annealing Algorithm\nsimulated_annealing <-\n    function(x,\n             y,\n             initial_theta,\n             T_init = 1.0,\n             alpha = 0.99,\n             max_iter = 5000) {\n        # Initialize parameters\n        theta <- initial_theta\n        best_theta <- theta\n        best_sse <- sse(theta, x, y)\n        T <- T_init  # Initial temperature\n        \n        for (iter in 1:max_iter) {\n            # Generate new candidate solution (small random perturbation)\n            theta_new <- theta + rnorm(length(theta), mean = 0, sd = T)\n            \n            # Compute new SSE\n            sse_new <- sse(theta_new, x, y)\n            \n            # Compute change in SSE\n            delta_Q <- sse_new - best_sse\n            \n            # Acceptance criteria\n            if (delta_Q < 0 || runif(1) < exp(-delta_Q / T)) {\n                theta <- theta_new\n                best_sse <- sse_new\n                best_theta <- theta_new\n            }\n            \n            # Reduce temperature\n            T <- alpha * T\n            \n            # Stopping condition (very low temperature)\n            if (T < 1e-6)\n                break\n        }\n        \n        return(best_theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Initial guess\ninitial_theta <-\n    c(runif(1, 1, 5), runif(1, 0.1, 3), runif(1,-2, 2))\n\n# Run Simulated Annealing\nestimated_theta <- simulated_annealing(x, y, initial_theta)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Simulated Annealing:\\n\")\n#> Estimated parameters (A, B, C) using Simulated Annealing:\nprint(estimated_theta)\n#> [1] 4.07180419 1.41457906 0.01422147\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Simulated Annealing: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"genetic-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.3.3 Genetic Algorithm","text":"Genetic Algorithms (GA) class evolutionary algorithms inspired principles natural selection genetics. Unlike deterministic optimization techniques, GA evolves population candidate solutions multiple generations, using genetic operators selection, crossover, mutation.GA particularly useful :function nonlinear, non-differentiable, highly multimodal.function nonlinear, non-differentiable, highly multimodal.Gradient-based methods fail due rugged function landscapes.Gradient-based methods fail due rugged function landscapes.global minimum required, rather local one.global minimum required, rather local one.goal Genetic Algorithm find optimal solution $\\hat{\\theta}$ minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Population RepresentationEach candidate solution (individual) represented chromosome, simply vector parameters: \\[\n\\theta = (\\theta_1, \\theta_2, \\theta_3)\n\\]entire population consists multiple solutions.2. SelectionEach individual’s fitness evaluated using:\\[\n\\text{Fitness}(\\theta) = -SSE(\\theta)\n\\]use Tournament Selection Roulette Wheel Selection choose parents reproduction.3. Crossover (Recombination)new solution \\(\\theta'\\) generated combining two parents:\\[\\theta' = \\alpha \\theta_{\\text{parent1}} + (1 - \\alpha) \\theta_{\\text{parent2}}, \\quad \\alpha \\sim U(0,1).\\]4. MutationRandom small changes introduced increase diversity:\\[\\theta_i' = \\theta_i + \\mathcal{N}(0, \\sigma),\\]\\(\\mathcal{N}(0, \\sigma)\\) small Gaussian perturbation.5. Evolutionary CycleThe algorithm iterates :SelectionSelectionCrossoverCrossoverMutationMutationSurvival fittestSurvival fittestTermination convergence reached.Termination convergence reached.","code":"\n# Load required library\nlibrary(GA)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Genetic Algorithm for Optimization\nga_optimization <- function(x, y) {\n    # Define fitness function (negative SSE for maximization)\n    fitness_function <- function(theta) {\n        # GA maximizes fitness, so we use negative SSE\n        return(-sse(theta, x, y))  \n    }\n    \n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n    \n    # Run GA optimization\n    ga_result <- ga(\n        type = \"real-valued\",\n        fitness = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        popSize = 50,\n        # Population size\n        maxiter = 200,\n        # Max generations\n        pmutation = 0.1,\n        # Mutation probability\n        monitor = FALSE\n    )\n    \n    return(ga_result@solution)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Genetic Algorithm\nestimated_theta <- ga_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Genetic Algorithm:\\n\")\n#> Estimated parameters (A, B, C) using Genetic Algorithm:\nprint(estimated_theta)\n#>            x1       x2         x3\n#> [1,] 4.066144 1.433886 0.00824126\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Genetic Algorithm: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"particle-swarm-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.3.4 Particle Swarm Optimization","text":"Particle Swarm Optimization (PSO) population-based global optimization algorithm inspired social behavior birds fish schools. Instead using genetic operators (like Genetic Algorithms), PSO models particles (solutions) flying search space, adjusting position based experience experience neighbors.PSO particularly useful :function nonlinear, noisy, lacks smooth gradients.function nonlinear, noisy, lacks smooth gradients.Gradient-based methods struggle due non-differentiability.Gradient-based methods struggle due non-differentiability.global minimum needed, rather local one.global minimum needed, rather local one.goal PSO find optimal solution \\(\\hat{\\theta}\\) minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Particle RepresentationEach particle represents candidate solution:\\[\n\\theta_i = (\\theta_{i1}, \\theta_{i2}, \\theta_{i3})\n\\]\\(\\theta_{ij}\\) \\(j^{th}\\) parameter particle \\(\\).2. Particle Velocity Position UpdatesEach particle moves search space velocity \\(v_i\\), updated :\\[\nv_i^{(t+1)} = \\omega v_i^{(t)} + c_1 r_1 (p_i - \\theta_i^{(t)}) + c_2 r_2 (g - \\theta_i^{(t)})\n\\]:\\(\\omega\\) inertia weight (controls exploration vs. exploitation),\\(\\omega\\) inertia weight (controls exploration vs. exploitation),\\(c_1, c_2\\) acceleration coefficients,\\(c_1, c_2\\) acceleration coefficients,\\(r_1, r_2 \\sim U(0,1)\\) random numbers,\\(r_1, r_2 \\sim U(0,1)\\) random numbers,\\(p_i\\) particle’s personal best position,\\(p_i\\) particle’s personal best position,\\(g\\) global best position.\\(g\\) global best position., position update :\\[\n\\theta_i^{(t+1)} = \\theta_i^{(t)} + v_i^{(t+1)}\n\\]process continues convergence criteria (like max number iterations minimum error) met.Particle Swarm Optimization AlgorithmInitialize particles randomly within search bounds.Set random initial velocities.Evaluate SSE particle.Update personal global best solutions.Update velocities positions using update equations.Repeat convergence.","code":"\n# Load required library\nlibrary(pso)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Particle Swarm Optimization (PSO) for Nonlinear Regression\npso_optimization <- function(x, y) {\n    # Define fitness function (minimize SSE)\n    fitness_function <- function(theta) {\n        return(sse(theta, x, y))\n    }\n    \n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n    \n    # Run PSO optimization\n    pso_result <- psoptim(\n        par = c(1, 1, 0),\n        # Initial guess\n        fn = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        control = list(maxit = 200, s = 50)  # 200 iterations, 50 particles\n    )\n    \n    return(pso_result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Particle Swarm Optimization\nestimated_theta <- pso_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Particle Swarm Optimization:\\n\")\n#> Estimated parameters (A, B, C) using Particle Swarm Optimization:\nprint(estimated_theta)\n#> [1] 4.06876562 1.42765613 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Particle Swarm Optimization: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"evolutionary-strategies","chapter":"6 Non-Linear Regression","heading":"6.2.3.5 Evolutionary Strategies","text":"Evolutionary Strategies (ES) class evolutionary optimization algorithms improve solutions mutating selecting individuals based fitness. Unlike Genetic Algorithm, ES focuses self-adaptive mutation rates selection pressure rather crossover. makes ES particularly robust continuous optimization problems like nonlinear regression.ES particularly useful :function complex, noisy, lacks smooth gradients.function complex, noisy, lacks smooth gradients.Gradient-based methods fail due non-differentiability.Gradient-based methods fail due non-differentiability.adaptive approach exploration exploitation needed.adaptive approach exploration exploitation needed.goal ES find optimal solution \\(\\hat{\\theta}\\) minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Population RepresentationEach individual solution \\(\\theta_i\\) parameter space:\\[\n\\theta_i = (\\theta_{i1}, \\theta_{i2}, \\theta_{i3}).\n\\]population consists multiple individuals, representing different candidate parameters.2. MutationNew candidate solutions generated adding random noise:\\[\n\\theta'_i = \\theta_i + \\sigma \\mathcal{N}(0, ),\n\\]:\\(\\sigma\\) mutation step size, adapts time.\\(\\sigma\\) mutation step size, adapts time.\\(\\mathcal{N}(0, )\\) standard normal distribution.\\(\\mathcal{N}(0, )\\) standard normal distribution.3. SelectionES employs \\((\\mu, \\lambda)\\)-selection:\\((\\mu, \\lambda)\\)-ES: Select best \\(\\mu\\) solutions \\(\\lambda\\) offspring.\\((\\mu, \\lambda)\\)-ES: Select best \\(\\mu\\) solutions \\(\\lambda\\) offspring.\\((\\mu + \\lambda)\\)-ES: Combine parents offspring, selecting top \\(\\mu\\).\\((\\mu + \\lambda)\\)-ES: Combine parents offspring, selecting top \\(\\mu\\).4. Step-Size AdaptationMutation strength \\(\\sigma\\) self-adapts using 1/5 success rule:\\[\n\\sigma_{t+1} =\n\\begin{cases}\n\\sigma_t / c, & \\text{success rate } > 1/5 \\\\\n\\sigma_t \\cdot c, & \\text{success rate } < 1/5\n\\end{cases}\n\\]\\(c > 1\\) scaling factor.Evolutionary Strategies AlgorithmInitialize population \\(\\lambda\\) solutions random parameters.Set mutation step size \\(\\sigma\\).Repeat max iterations:\nGenerate \\(\\lambda\\) offspring mutating parent solutions.\nEvaluate fitness (SSE) offspring.\nSelect best \\(\\mu\\) solutions next generation.\nAdapt mutation step size based success rate.\nGenerate \\(\\lambda\\) offspring mutating parent solutions.Evaluate fitness (SSE) offspring.Select best \\(\\mu\\) solutions next generation.Adapt mutation step size based success rate.Return best solution found.","code":"\n# Load required library\nlibrary(DEoptim)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Evolutionary Strategies Optimization (Using Differential Evolution)\nes_optimization <- function(x, y) {\n    # Define fitness function (minimize SSE)\n    fitness_function <- function(theta) {\n        return(sse(theta, x, y))\n    }\n\n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n\n    # Run Differential Evolution (mimicking ES)\n    es_result <- DEoptim(\n        fn = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        # 50 individuals, 200 generations, suppress iteration output\n        DEoptim.control(NP = 50, itermax = 200, trace = F)  \n    )\n\n    return(es_result$optim$bestmem)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Evolutionary Strategies Optimization\nestimated_theta <- es_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Evolutionary Strategies:\\n\")\n#> Estimated parameters (A, B, C) using Evolutionary Strategies:\nprint(estimated_theta)\n#>       par1       par2       par3 \n#> 4.06876561 1.42765613 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Evolutionary Strategies: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"linearization-nonlinear-regression-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4 Linearization","text":"","code":""},{"path":"non-linear-regression.html","id":"taylor-series-approximation-nonlinear-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4.1 Taylor Series Approximation","text":"Taylor Series Approximation fundamental tool nonlinear optimization, enabling local approximation complex functions using polynomial expansions. widely used linearize nonlinear models, facilitate derivative-based optimization, derive Newton-type methods.Taylor series approximation particularly useful :nonlinear function difficult compute directly.nonlinear function difficult compute directly.Optimization requires local gradient curvature information.Optimization requires local gradient curvature information.simpler, polynomial-based approximation improves computational efficiency.simpler, polynomial-based approximation improves computational efficiency.Given differentiable function \\(f(\\theta)\\), Taylor series expansion around point \\(\\theta_0\\) :\\[\nf(\\theta) = f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0) + \\frac{1}{2} f''(\\theta_0)(\\theta - \\theta_0)^2 + \\mathcal{O}((\\theta - \\theta_0)^3).\n\\]optimization, often use:First-order approximation (Linear Approximation): \\[\n   f(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0).\n   \\]Second-order approximation (Quadratic Approximation): \\[\nf(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0) + \\frac{1}{2} f''(\\theta_0)(\\theta - \\theta_0)^2.\n\\]gradient-based optimization, use Newton-Raphson update:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - [H_f(\\theta^{(k)})]^{-1} \\nabla f(\\theta^{(k)}),\n\\]:\\(\\nabla f(\\theta)\\) gradient (first derivative),\\(\\nabla f(\\theta)\\) gradient (first derivative),\\(H_f(\\theta)\\) Hessian matrix (second derivative).\\(H_f(\\theta)\\) Hessian matrix (second derivative).nonlinear regression, approximate Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. First-Order Approximation (Gradient Descent)gradient SSE w.r.t. parameters \\(\\theta\\) :\\[\n\\nabla SSE(\\theta) = -2 \\sum_{=1}^{n} (y_i - f(x_i; \\theta)) \\nabla f(x_i; \\theta).\n\\]Using first-order Taylor approximation, update parameters via gradient descent:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - \\alpha \\nabla SSE(\\theta^{(k)}),\n\\]\\(\\alpha\\) learning rate.2. Second-Order Approximation (Newton’s Method)Hessian matrix SSE :\\[\nH_{SSE}(\\theta) = 2 \\sum_{=1}^{n} \\nabla f(x_i; \\theta) \\nabla f(x_i; \\theta)^T - 2 \\sum_{=1}^{n} (y_i - f(x_i; \\theta)) H_f(x_i; \\theta).\n\\]Newton-Raphson update becomes:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - H_{SSE}(\\theta)^{-1} \\nabla SSE(\\theta).\n\\]","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(is.na(x) |\n                      x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2, na.rm = TRUE))  # Avoid NA errors\n}\n\n# First-Order Approximation: Gradient Descent Optimization\ngradient_descent <-\n    function(x,\n             y,\n             alpha = 0.005,\n             tol = 1e-6,\n             max_iter = 5000) {\n        theta <- c(2, 1, 0)  # Initial guess\n        for (i in 1:max_iter) {\n            grad_sse <-\n                grad(function(t)\n                    sse(t, x, y), theta)  # Compute gradient\n            theta_new <-\n                theta - alpha * grad_sse  # Update parameters\n            \n            if (sum(abs(theta_new - theta)) < tol)\n                break  # Check convergence\n            theta <- theta_new\n        }\n        return(theta)\n    }\n\n# Second-Order Approximation: Newton's Method with Regularization\nnewton_method <-\n    function(x,\n             y,\n             tol = 1e-6,\n             max_iter = 100,\n             lambda = 1e-4) {\n        theta <- c(2, 1, 0)  # Initial guess\n        for (i in 1:max_iter) {\n            grad_sse <-\n                grad(function(t)\n                    sse(t, x, y), theta)  # Compute gradient\n            hessian_sse <-\n                hessian(function(t)\n                    sse(t, x, y), theta)  # Compute Hessian\n            \n            # Regularize Hessian to avoid singularity\n            hessian_reg <-\n                hessian_sse + lambda * diag(length(theta))\n            \n            # Ensure Hessian is invertible\n            if (is.na(det(hessian_reg)) ||\n                det(hessian_reg) < 1e-10) {\n                message(\"Singular Hessian found; increasing regularization.\")\n                lambda <- lambda * 10  # Increase regularization\n                next\n            }\n            \n            # Newton update\n            theta_new <- theta - solve(hessian_reg) %*% grad_sse\n            \n            if (sum(abs(theta_new - theta)) < tol)\n                break  # Check convergence\n            theta <- theta_new\n        }\n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Gradient Descent\nestimated_theta_gd <- gradient_descent(x, y)\n\n# Run Newton's Method with Regularization\nestimated_theta_newton <- newton_method(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Gradient Descent:\\n\")\n#> Estimated parameters (A, B, C) using Gradient Descent:\nprint(estimated_theta_gd)\n#> [1] 4.06876224 1.42766371 0.01128539\n\ncat(\"Estimated parameters (A, B, C) using Newton's Method:\\n\")\n#> Estimated parameters (A, B, C) using Newton's Method:\nprint(estimated_theta_newton)\n#>            [,1]\n#> [1,] 4.06876368\n#> [2,] 1.42766047\n#> [3,] 0.01128636\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Taylor Series Approximation: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta_gd, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"blue\",\n    lwd = 2,\n    lty = 2  # Dashed line to differentiate Gradient Descent\n)\ncurve(\n    nonlinear_model(estimated_theta_newton, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Gradient Descent\", \"Newton's Method (Regularized)\"),\n    pch = c(19, NA, NA),\n    lty = c(NA, 2, 1),\n    col = c(\"black\", \"blue\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"log-linearization-nonlinear-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4.2 Log-Linearization","text":"Log-Linearization mathematical technique used transform nonlinear models linear models taking logarithm sides. transformation simplifies parameter estimation enables use linear regression techniques originally nonlinear functions.Log-linearization particularly useful :model exhibits exponential, power-law, logistic growth behavior.model exhibits exponential, power-law, logistic growth behavior.Linear regression methods preferred nonlinear optimization.Linear regression methods preferred nonlinear optimization.linearized version provides better interpretability computational efficiency.linearized version provides better interpretability computational efficiency.nonlinear model can often expressed form:\\[\ny = f(x; \\theta).\n\\]Applying log transformation, obtain:\\[\n\\log y = g(x; \\theta),\n\\]\\(g(x; \\theta)\\) now linear parameters. estimate \\(\\theta\\) using Ordinary Least Squares.Example 1: Exponential ModelConsider exponential growth model:\\[\ny = e^{Bx}.\n\\]Taking natural logarithm:\\[\n\\log y = \\log + Bx.\n\\]now linear \\(\\log y\\), allowing estimation via linear regression.Example 2: Power Law ModelFor power law function:\\[\ny = x^B.\n\\]Taking logs:\\[\n\\log y = \\log + B \\log x.\n\\], linearized, making solvable via OLS regression.Log-Linearization AlgorithmApply logarithm transformation dependent variable.Transform equation linear form.Use linear regression (OLS) estimate parameters.Convert parameters back original scale necessary.","code":"\n# Load required library\nlibrary(stats)\n\n# Generate synthetic data for an exponential model\nset.seed(123)\nx <- seq(1, 10, length.out = 100)\ntrue_A <- 2\ntrue_B <- 0.3\ny <- true_A * exp(true_B * x) + rnorm(length(x), sd = 0.5)\n\n# Apply logarithmic transformation\nlog_y <- log(y)\n\n# Fit linear regression model\nlog_linear_model <- lm(log_y ~ x)\n\n# Extract estimated parameters\nestimated_B <- coef(log_linear_model)[2]  # Slope in log-space\nestimated_A <-\n    exp(coef(log_linear_model)[1])  # Intercept (back-transformed)\n\n# Display results\ncat(\"Estimated parameters (A, B) using Log-Linearization:\\n\")\n#> Estimated parameters (A, B) using Log-Linearization:\nprint(c(estimated_A, estimated_B))\n#> (Intercept)           x \n#>   2.0012577   0.3001223\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Log-Linearization: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    estimated_A * exp(estimated_B * x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Log-Linear Model\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"hybrid-nonlinear-regression-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.5 Hybrid","text":"","code":""},{"path":"non-linear-regression.html","id":"adaptive-levenberg-marquardt","chapter":"6 Non-Linear Regression","heading":"6.2.5.1 Adaptive Levenberg-Marquardt","text":"Levenberg-Marquardt Algorithm (LMA) powerful nonlinear least squares optimization method adaptively combines:Gauss-Newton Algorithm fast convergence near solution.Gauss-Newton Algorithm fast convergence near solution.Steepest Descent (Gradient Descent) stability far solution.Steepest Descent (Gradient Descent) stability far solution.Adaptive Levenberg-Marquardt Algorithm adjusts damping parameter \\(\\tau\\) dynamically, making efficient practice.Given objective function Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]update rule LMA :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) + \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}.\n\\]:\\(\\tau\\) adaptive damping parameter.\\(\\tau\\) adaptive damping parameter.\\(\\mathbf{}_{p \\times p}\\) identity matrix.\\(\\mathbf{}_{p \\times p}\\) identity matrix.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate.\\(\\alpha_j\\) learning rate.key adaptation rule \\(\\tau\\):new step decreases SSE, reduce \\(\\tau\\): \\[\n  \\tau \\gets \\tau / 10.\n  \\]new step decreases SSE, reduce \\(\\tau\\): \\[\n  \\tau \\gets \\tau / 10.\n  \\]Otherwise, increase \\(\\tau\\) ensure stability: \\[\n  \\tau \\gets 10\\tau.\n  \\]Otherwise, increase \\(\\tau\\) ensure stability: \\[\n  \\tau \\gets 10\\tau.\n  \\]adjustment ensures balance stability efficiency.Adaptive Levenberg-Marquardt AlgorithmInitialize parameters \\(\\theta_0\\), damping factor \\(\\tau\\).Compute Jacobian \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\).Compute step direction using modified Gauss-Newton update.Adjust \\(\\tau\\) dynamically:\nDecrease \\(\\tau\\) SSE improves.\nIncrease \\(\\tau\\) SSE worsens.\nDecrease \\(\\tau\\) SSE improves.Increase \\(\\tau\\) SSE worsens.Repeat convergence.","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Adaptive Levenberg-Marquardt Optimization\nadaptive_lm_optimization <-\n    function(x, y, tol = 1e-6, max_iter = 100) {\n        theta <- c(2, 1, 0)  # Initial parameter guess\n        tau <- 1e-3  # Initial damping parameter\n        alpha <- 1  # Step size scaling\n        iter <- 0\n        \n        while (iter < max_iter) {\n            iter <- iter + 1\n            \n            # Compute Jacobian numerically\n            J <- jacobian(function(t)\n                nonlinear_model(t, x), theta)\n            \n            # Compute gradient of SSE\n            residuals <- y - nonlinear_model(theta, x)\n            grad_sse <- -2 * t(J) %*% residuals\n            \n            # Compute Hessian approximation\n            H <- 2 * t(J) %*% J + tau * diag(length(theta))\n            \n            # Compute parameter update step\n            delta_theta <- solve(H, grad_sse)\n            \n            # Trial step\n            theta_new <- theta - alpha * delta_theta\n            \n            # Compute SSE for new parameters\n            if (sse(theta_new, x, y) < sse(theta, x, y)) {\n                # Accept step, decrease tau\n                theta <- theta_new\n                tau <- tau / 10\n            } else {\n                # Reject step, increase tau\n                tau <- tau * 10\n            }\n            \n            # Check convergence\n            if (sum(abs(delta_theta)) < tol)\n                break\n        }\n        \n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Adaptive Levenberg-Marquardt Optimization\nestimated_theta <- adaptive_lm_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Adaptive Levenberg-Marquardt:\\n\")\n#> Estimated parameters (A, B, C) using Adaptive Levenberg-Marquardt:\nprint(estimated_theta)\n#>            [,1]\n#> [1,] 4.06876562\n#> [2,] 1.42765612\n#> [3,] 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Adaptive Levenberg-Marquardt: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"comparison-of-nonlinear-optimizers","chapter":"6 Non-Linear Regression","heading":"6.2.6 Comparison of Nonlinear Optimizers","text":"","code":"\n# ALL-IN-ONE R SCRIPT COMPARING MULTIPLE NONLINEAR-REGRESSION OPTIMIZERS\n\nlibrary(minpack.lm) # nlsLM (Levenberg-Marquardt)\nlibrary(dfoptim)    # Powell (nmk), Hooke-Jeeves\nlibrary(nloptr)     # trust-region reflective\nlibrary(GA)         # genetic algorithm\nlibrary(DEoptim)    # differential evolution\nlibrary(GenSA)      # simulated annealing\nlibrary(pso)        # particle swarm\nlibrary(MASS)       # for ginv fallback\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# -- 1) DEFINE MODELS (SIMPLE VS COMPLEX) ---\n\n# 3-parameter logistic\nf_logistic <- function(theta, x) {\n  A <- theta[1]\n  B <- theta[2]\n  C <- theta[3]\n  A / (1 + exp(-B * (x - C)))\n}\nsse_logistic <-\n  function(theta, x, y)\n    sum((y - f_logistic(theta, x)) ^ 2)\n\n# 4-parameter \"extended\" model\nf_complex <- function(theta, x) {\n  A <- theta[1]\n  B <- theta[2]\n  C <- theta[3]\n  D <- theta[4]\n  A / (1 + exp(-B * (x - C))) + D * exp(-0.5 * x)\n}\nsse_complex <-\n  function(theta, x, y)\n    sum((y - f_complex(theta, x)) ^ 2)\n\n# Generate synthetic data\nset.seed(123)\nn <- 100\nx_data <- seq(-5, 5, length.out = n)\n# \"simple\" scenario\ntrue_theta_simple <- c(4, 1.5, 0)\ny_data_simple <-\n  f_logistic(true_theta_simple, x_data) + rnorm(n, sd = 0.3)\n# \"complex\" scenario\ntrue_theta_complex <- c(4, 1.2, -1, 0.5)\ny_data_complex <-\n  f_complex(true_theta_complex, x_data) + rnorm(n, sd = 0.3)\n\n# -- 2) OPTIMIZERS (EXCEPT BISECTION) ----\n#\n# All methods share signature:\n#   FUN(par, x, y, sse_fn, model_fn, lower=NULL, upper=NULL, ...)\n# Some do not strictly use lower/upper if unconstrained.\n\n# 2.1 Gauss–Newton\ngauss_newton_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL,\n                             max_iter = 100,\n                             tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    nP  <- length(theta)\n    Fmat <- matrix(0, nrow = length(x), ncol = nP)\n    for (p in seq_len(nP)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      Fmat[, p] <-\n        (model_fn(pert, x) - model_fn(theta, x)) / eps\n    }\n    r <- y - model_fn(theta, x)\n    delta <- tryCatch(\n      solve(t(Fmat) %*% Fmat, t(Fmat) %*% r),\n      error = function(e) {\n        # fallback to pseudoinverse\n        MASS::ginv(t(Fmat) %*% Fmat) %*% (t(Fmat) %*% r)\n      }\n    )\n    theta_new <- theta + delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.2 Modified Gauss-Newton (with step halving)\nmodified_gauss_newton_fit <- function(par,\n                                      x,\n                                      y,\n                                      sse_fn,\n                                      model_fn,\n                                      lower = NULL,\n                                      upper = NULL,\n                                      max_iter = 100,\n                                      tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    nP  <- length(theta)\n    Fmat <- matrix(0, nrow = length(x), ncol = nP)\n    for (p in seq_len(nP)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      Fmat[, p] <-\n        (model_fn(pert, x) - model_fn(theta, x)) / eps\n    }\n    r <- y - model_fn(theta, x)\n    lhs <- t(Fmat) %*% Fmat\n    rhs <- t(Fmat) %*% r\n    delta <- tryCatch(\n      solve(lhs, rhs),\n      error = function(e)\n        MASS::ginv(lhs) %*% rhs\n    )\n    sse_old <- sse_fn(theta, x, y)\n    alpha <- 1\n    for (k in 1:10) {\n      new_sse <- sse_fn(theta + alpha * delta, x, y)\n      if (new_sse < sse_old)\n        break\n      alpha <- alpha / 2\n    }\n    theta_new <- theta + alpha * delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.3 Steepest Descent (Gradient Descent)\nsteepest_descent_fit <- function(par,\n                                 x,\n                                 y,\n                                 sse_fn,\n                                 model_fn,\n                                 lower = NULL,\n                                 upper = NULL,\n                                 lr = 0.001,\n                                 max_iter = 5000,\n                                 tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    f0  <- sse_fn(theta, x, y)\n    grad <- numeric(length(theta))\n    for (p in seq_along(theta)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      grad[p] <- (sse_fn(pert, x, y) - f0) / eps\n    }\n    theta_new <- theta - lr * grad\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.4 Levenberg–Marquardt (nlsLM)\nlm_fit <- function(par,\n                   x,\n                   y,\n                   sse_fn,\n                   model_fn,\n                   lower = NULL,\n                   upper = NULL,\n                   form = c(\"simple\", \"complex\"))\n{\n  form <- match.arg(form)\n  if (form == \"simple\") {\n    fit <- nlsLM(y ~ A / (1 + exp(-B * (x - C))),\n                 start = list(A = par[1],\n                              B = par[2],\n                              C = par[3]))\n  } else {\n    fit <- nlsLM(y ~ A / (1 + exp(-B * (x - C))) + D * exp(-0.5 * x),\n                 start = list(\n                   A = par[1],\n                   B = par[2],\n                   C = par[3],\n                   D = par[4]\n                 ))\n  }\n  coef(fit)\n}\n\n# 2.5 Newton–Raphson (with numeric Hessian, fallback if singular)\nnewton_raphson_fit <- function(par,\n                               x,\n                               y,\n                               sse_fn,\n                               model_fn,\n                               lower = NULL,\n                               upper = NULL,\n                               max_iter = 50,\n                               tol = 1e-6)\n{\n  theta <- par\n  for (i in seq_len(max_iter)) {\n    eps <- 1e-6\n    f0  <- sse_fn(theta, x, y)\n    grad <- numeric(length(theta))\n    for (p in seq_along(theta)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      grad[p] <- (sse_fn(pert, x, y) - f0) / eps\n    }\n    Hess <- matrix(0, length(theta), length(theta))\n    for (p in seq_along(theta)) {\n      pert_p <- theta\n      pert_p[p] <- pert_p[p] + eps\n      f_p <- sse_fn(pert_p, x, y)\n      for (q in seq_along(theta)) {\n        pert_q <- pert_p\n        pert_q[q] <- pert_q[q] + eps\n        Hess[p, q] <- (sse_fn(pert_q, x, y) -\n                         f_p - (f0 - sse_fn(theta, x, y))) / (eps ^\n                                                                2)\n      }\n    }\n    delta <- tryCatch(\n      solve(Hess, grad),\n      error = function(e)\n        MASS::ginv(Hess) %*% grad\n    )\n    theta_new <- theta - delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.6 Quasi–Newton (BFGS via optim)\nquasi_newton_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- optim(par, fn, method = \"BFGS\")\n  res$par\n}\n\n# 2.7 Trust-region reflective (nloptr)\ntrust_region_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL)\n{\n  # numeric gradient\n  grad_numeric <- function(pp, eps = 1e-6) {\n    g  <- numeric(length(pp))\n    f0 <- sse_fn(pp, x, y)\n    for (i in seq_along(pp)) {\n      p2 <- pp\n      p2[i] <- p2[i] + eps\n      g[i] <- (sse_fn(p2, x, y) - f0) / eps\n    }\n    g\n  }\n  eval_f <- function(pp) {\n    val <- sse_fn(pp, x, y)\n    gr  <- grad_numeric(pp)\n    list(objective = val, gradient = gr)\n  }\n  lb <- if (is.null(lower))\n    rep(-Inf, length(par))\n  else\n    lower\n  ub <- if (is.null(upper))\n    rep(Inf, length(par))\n  else\n    upper\n  res <- nloptr(\n    x0 = par,\n    eval_f = eval_f,\n    lb = lb,\n    ub = ub,\n    opts = list(\n      algorithm = \"NLOPT_LD_TNEWTON\",\n      maxeval = 500,\n      xtol_rel = 1e-6\n    )\n  )\n  res$solution\n}\n\n# 2.8 Grid search\ngrid_search_fit <- function(par,\n                            x,\n                            y,\n                            sse_fn,\n                            model_fn,\n                            lower = NULL,\n                            upper = NULL,\n                            grid_defs = NULL)\n{\n  if (is.null(grid_defs))\n    stop(\"Must provide grid_defs for multi-parameter grid search.\")\n  g <- expand.grid(grid_defs)\n  g$SSE <-\n    apply(g, 1, function(rowp)\n      sse_fn(as.numeric(rowp), x, y))\n  best_idx <- which.min(g$SSE)\n  as.numeric(g[best_idx, seq_along(grid_defs)])\n}\n\n# 2.9 Nelder-Mead\nnelder_mead_fit <- function(par,\n                            x,\n                            y,\n                            sse_fn,\n                            model_fn,\n                            lower = NULL,\n                            upper = NULL)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- optim(par, fn, method = \"Nelder-Mead\")\n  res$par\n}\n\n# 2.10 Powell’s method (dfoptim::nmk for unconstrained)\npowell_fit <-\n  function(par,\n           x,\n           y,\n           sse_fn,\n           model_fn,\n           lower = NULL,\n           upper = NULL) {\n    fn <- function(pp)\n      sse_fn(pp, x, y)\n    dfoptim::nmk(par, fn)$par\n  }\n\n# 2.11 Hooke-Jeeves (dfoptim::hjkb)\nhooke_jeeves_fit <-\n  function(par,\n           x,\n           y,\n           sse_fn,\n           model_fn,\n           lower = NULL,\n           upper = NULL) {\n    fn <- function(pp)\n      sse_fn(pp, x, y)\n    dfoptim::hjkb(par, fn)$par\n  }\n\n# 2.12 Random Search\nrandom_search_fit <- function(par,\n                              x,\n                              y,\n                              sse_fn,\n                              model_fn,\n                              lower,\n                              upper,\n                              max_iter = 2000,\n                              ...)\n{\n  best_par <- NULL\n  best_sse <- Inf\n  dimp <- length(lower)\n  for (i in seq_len(max_iter)) {\n    candidate <- runif(dimp, min = lower, max = upper)\n    val <- sse_fn(candidate, x, y)\n    if (val < best_sse) {\n      best_sse <- val\n      best_par <- candidate\n    }\n  }\n  best_par\n}\n\n# 2.13 Differential Evolution (DEoptim)\ndiff_evo_fit <- function(par,\n                         x,\n                         y,\n                         sse_fn,\n                         model_fn,\n                         lower,\n                         upper,\n                         max_iter = 100,\n                         ...)\n{\n  fn <- function(v)\n    sse_fn(v, x, y)\n  out <- DEoptim(fn,\n                 lower = lower,\n                 upper = upper,\n                 DEoptim.control(NP = 50, itermax = max_iter, trace = F))\n  out$optim$bestmem\n}\n\n# 2.14 Simulated Annealing (GenSA)\nsim_anneal_fit <- function(par,\n                           x,\n                           y,\n                           sse_fn,\n                           model_fn,\n                           lower = NULL,\n                           upper = NULL,\n                           ...)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  lb <- if (is.null(lower))\n    rep(-Inf, length(par))\n  else\n    lower\n  ub <- if (is.null(upper))\n    rep(Inf, length(par))\n  else\n    upper\n  # GenSA requires: GenSA(par, fn, lower, upper, control=list(...))\n  out <-\n    GenSA(\n      par,\n      fn,\n      lower = lb,\n      upper = ub,\n      control = list(max.call = 10000)\n    )\n  out$par\n}\n\n# 2.15 Genetic Algorithm (GA)\ngenetic_fit <- function(par,\n                        x,\n                        y,\n                        sse_fn,\n                        model_fn,\n                        lower,\n                        upper,\n                        max_iter = 100,\n                        ...)\n{\n  fitness_fun <- function(pp)\n    - sse_fn(pp, x, y)\n  gares <- ga(\n    type = \"real-valued\",\n    fitness = fitness_fun,\n    lower = lower,\n    upper = upper,\n    popSize = 50,\n    maxiter = max_iter,\n    run = 50\n  )\n  gares@solution[1,]\n}\n\n# 2.16 Particle Swarm (pso)\nparticle_swarm_fit <- function(par,\n                               x,\n                               y,\n                               sse_fn,\n                               model_fn,\n                               lower,\n                               upper,\n                               max_iter = 100,\n                               ...)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- psoptim(\n    par = (lower + upper) / 2,\n    fn = fn,\n    lower = lower,\n    upper = upper,\n    control = list(maxit = max_iter)\n  )\n  res$par\n}\n\n\n# -- 3) RUN METHOD WRAPPER ---\nrun_method <- function(method_name,\n                       FUN,\n                       par_init,\n                       x,\n                       y,\n                       sse_fn,\n                       model_fn,\n                       lower = NULL,\n                       upper = NULL,\n                       ...)\n{\n  mb <- microbenchmark(result = {\n    out <- FUN(par_init, x, y, sse_fn, model_fn, lower, upper, ...)\n    out\n  }, times = 1)\n  final_par <-\n    FUN(par_init, x, y, sse_fn, model_fn, lower, upper, ...)\n  if (is.null(final_par)) {\n    # e.g. placeholders that return NULL\n    return(data.frame(\n      Method = method_name,\n      Parameters = \"N/A\",\n      SSE = NA,\n      Time_ms = NA\n    ))\n  }\n  data.frame(\n    Method     = method_name,\n    Parameters = paste(round(final_par, 4), collapse = \", \"),\n    SSE        = round(sse_fn(final_par, x, y), 6),\n    Time_ms    = median(mb$time) / 1e6\n  )\n}\n\n# -- 4) MASTER FUNCTION TO COMPARE ALL METHODS (SIMPLE / COMPLEX) ---\n\ncompare_all_methods <- function(is_complex = FALSE) {\n  if (!is_complex) {\n    # SIMPLE (3-param logistic)\n    x <- x_data\n    y <- y_data_simple\n    sse_fn   <- sse_logistic\n    model_fn <- f_logistic\n    init_par <- c(3, 1, 0.5)\n    grid_defs <- list(\n      A = seq(2, 6, length.out = 10),\n      B = seq(0.5, 2, length.out = 10),\n      C = seq(-1, 1, length.out = 10)\n    )\n    lower <- c(1, 0.1,-3)\n    upper <- c(6, 3,  3)\n    lm_form <- \"simple\"\n  } else {\n    # COMPLEX (4-param model)\n    x <- x_data\n    y <- y_data_complex\n    sse_fn   <- sse_complex\n    model_fn <- f_complex\n    init_par <- c(3, 1,-0.5, 0.2)\n    grid_defs <- list(\n      A = seq(2, 6, length.out = 8),\n      B = seq(0.5, 2, length.out = 8),\n      C = seq(-2, 2, length.out = 8),\n      D = seq(0, 2, length.out = 8)\n    )\n    lower <- c(1, 0.1,-3, 0)\n    upper <- c(6, 3,  3, 2)\n    lm_form <- \"complex\"\n  }\n  \n  # RUN each method\n  out <- bind_rows(\n    run_method(\n      \"Gauss-Newton\",\n      gauss_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Modified Gauss-Newton\",\n      modified_gauss_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Steepest Descent\",\n      steepest_descent_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Levenberg-Marquardt (nlsLM)\",\n      lm_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      form = lm_form\n    ),\n    run_method(\n      \"Newton-Raphson\",\n      newton_raphson_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Quasi-Newton (BFGS)\",\n      quasi_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Trust-region Reflective\",\n      trust_region_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper\n    ),\n    run_method(\n      \"Grid Search\",\n      grid_search_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      grid_defs = grid_defs\n    ),\n    run_method(\n      \"Nelder-Mead\",\n      nelder_mead_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\"Powell's method\",\n               powell_fit,\n               init_par,\n               x,\n               y,\n               sse_fn,\n               model_fn),\n    run_method(\n      \"Hooke-Jeeves\",\n      hooke_jeeves_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Random Search\",\n      random_search_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 1000\n    ),\n    run_method(\n      \"Differential Evolution\",\n      diff_evo_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    ),\n    run_method(\n      \"Simulated Annealing\",\n      sim_anneal_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper\n    ),\n    run_method(\n      \"Genetic Algorithm\",\n      genetic_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    ),\n    run_method(\n      \"Particle Swarm\",\n      particle_swarm_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    )\n  )\n  out\n}\n\n# -- 5) RUN & VISUALIZE ----\n\n# Compare \"simple\" logistic (3 params)\nresults_simple  <- compare_all_methods(is_complex = FALSE)\nresults_simple$Problem <- \"Simple\"\n\n# Compare \"complex\" (4 params)\nresults_complex <- compare_all_methods(is_complex = TRUE)\nresults_complex$Problem <- \"Complex\"\n\n# Combine\nall_results <- rbind(results_simple, results_complex)\n# print(all_results)\n# DT::datatable(all_results)\n\n# Example: SSE by method & problem\nggplot(all_results, aes(x = Method, y = log(SSE), fill = Problem)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal(base_size = 11) +\n  labs(title = \"Comparison of SSE by Method & Problem Complexity\",\n       x = \"\", y = \"Log(Sum of Squared Errors)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Example: Time (ms) by method & problem\nggplot(all_results, aes(x = Method, y = Time_ms, fill = Problem)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal(base_size = 11) +\n  labs(title = \"Comparison of Computation Time by Method & Problem Complexity\",\n       x = \"\", y = \"Time (ms)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"},{"path":"non-linear-regression.html","id":"practical-considerations-2","chapter":"6 Non-Linear Regression","heading":"6.3 Practical Considerations","text":"optimization algorithms converge, require good initial estimates parameters. choice starting values, constraints, complexity model play role whether optimization algorithm successfully finds suitable solution.","code":""},{"path":"non-linear-regression.html","id":"selecting-starting-values","chapter":"6 Non-Linear Regression","heading":"6.3.1 Selecting Starting Values","text":"Choosing good starting values can significantly impact efficiency success optimization algorithms. Several approaches can used:Prior theoretical information: prior knowledge parameters available, incorporated choice initial values.Grid search graphical inspection \\(SSE(\\theta)\\): Evaluating sum squared errors (SSE) across grid possible values can help identify promising starting points.Ordinary Least Squares estimates: linear approximation model exists, using OLS obtain initial estimates can effective.Model interpretation: Understanding structure behavior model can provide intuition reasonable starting values.Expected Value Parameterization: Reformulating model based expected values may improve interpretability numerical stability estimation.","code":""},{"path":"non-linear-regression.html","id":"grid-search-for-optimal-starting-values","chapter":"6 Non-Linear Regression","heading":"6.3.1.1 Grid Search for Optimal Starting Values","text":"Note: nls_multstart package can perform grid search efficiently without requiring manual looping.Visualizing Prediction IntervalsOnce model fitted, useful visualize prediction intervals assess model uncertainty.","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate x as 100 integers using seq function\nx <- seq(0, 100, 1)\n\n# Generate coefficients for exponential function\na <- runif(1, 0, 20)  # Random coefficient a\nb <- runif(1, 0.005, 0.075)  # Random coefficient b\nc <- runif(101, 0, 5)  # Random noise\n\n# Generate y as a * e^(b*x) + c\ny <- a * exp(b * x) + c\n\n# Print the generated parameters\ncat(\"Generated coefficients:\\n\")\n#> Generated coefficients:\ncat(\"a =\", a, \"\\n\")\n#> a = 5.75155\ncat(\"b =\", b, \"\\n\")\n#> b = 0.06018136\n\n# Define our data frame\ndatf <- data.frame(x, y)\n\n# Define our model function\nmod <- function(a, b, x) {\n  a * exp(b * x)\n}\n# Ensure all y values are positive (avoid log issues)\ny_adj <-\n  ifelse(y > 0, y, min(y[y > 0]) + 1e-3)  # Shift small values slightly\n\n# Create adjusted dataframe\ndatf_adj <- data.frame(x, y_adj)\n\n# Linearize by taking log(y)\nlin_mod <- lm(log(y_adj) ~ x, data = datf_adj)\n\n# Extract starting values\nastrt <-\n  exp(coef(lin_mod)[1])  # Convert intercept back from log scale\nbstrt <- coef(lin_mod)[2]  # Slope remains the same\ncat(\"Starting values for non-linear fit:\\n\")\nprint(c(astrt, bstrt))\n\n# Fit nonlinear model with these starting values\nnlin_mod <- nls(y ~ mod(a, b, x),\n                start = list(a = astrt, b = bstrt),\n                data = datf)\n\n# Model summary\nsummary(nlin_mod)\n\n# Plot original data\nplot(\n  x,\n  y,\n  main = \"Exponential Growth Fit\",\n  col = \"blue\",\n  pch = 16,\n  xlab = \"x\",\n  ylab = \"y\"\n)\n\n# Add fitted curve in red\nlines(x, predict(nlin_mod), col = \"red\", lwd = 2)\n\n# Add legend\nlegend(\n  \"topleft\",\n  legend = c(\"Original Data\", \"Fitted Model\"),\n  col = c(\"blue\", \"red\"),\n  pch = c(16, NA),\n  lwd = c(NA, 2)\n)\n# Define grid of possible parameter values\naseq <- seq(10, 18, 0.2)\nbseq <- seq(0.001, 0.075, 0.001)\n\nna <- length(aseq)\nnb <- length(bseq)\nSSout <- matrix(0, na * nb, 3)  # Matrix to store SSE values\ncnt <- 0\n\n# Evaluate SSE across grid\nfor (k in 1:na) {\n  for (j in 1:nb) {\n    cnt <- cnt + 1\n    ypred <-\n      # Evaluate model at these parameter values\n      mod(aseq[k], bseq[j], x)  \n    \n    # Compute SSE\n    ss <- sum((y - ypred) ^ 2)  \n    SSout[cnt, 1] <- aseq[k]\n    SSout[cnt, 2] <- bseq[j]\n    SSout[cnt, 3] <- ss\n  }\n}\n\n# Identify optimal starting values\nmn_indx <- which.min(SSout[, 3])\nastrt <- SSout[mn_indx, 1]\nbstrt <- SSout[mn_indx, 2]\n\n# Fit nonlinear model using optimal starting values\nnlin_modG <-\n  nls(y ~ mod(a, b, x), start = list(a = astrt, b = bstrt))\n\n# Display model results\nsummary(nlin_modG)\n#> \n#> Formula: y ~ mod(a, b, x)\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> a 5.889e+00  1.986e-02   296.6   <2e-16 ***\n#> b 5.995e-02  3.644e-05  1645.0   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.135 on 99 degrees of freedom\n#> \n#> Number of iterations to convergence: 4 \n#> Achieved convergence tolerance: 7.213e-06\n# Load necessary package\nlibrary(nlstools)\n\n# Plot fitted model with confidence and prediction intervals\nplotFit(\n  nlin_modG,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = datf\n)  "},{"path":"non-linear-regression.html","id":"using-programmed-starting-values-in-nls","chapter":"6 Non-Linear Regression","heading":"6.3.1.2 Using Programmed Starting Values in nls","text":"Many nonlinear models well-established functional forms, allowing programmed starting values nls function. example, models logistic growth asymptotic regression built-self-starting functions.explore available self-starting models R, use:command lists functions names starting SS, typically denote self-starting functions nonlinear regression.","code":"\napropos(\"^SS\")"},{"path":"non-linear-regression.html","id":"custom-self-starting-functions","chapter":"6 Non-Linear Regression","heading":"6.3.1.3 Custom Self-Starting Functions","text":"model match built-nls functions, can define self-starting function. Self-starting functions R automate process estimating initial values, helps fitting nonlinear models efficiently.needed, self-starting function :Define nonlinear equation.Define nonlinear equation.Implement method computing starting values.Implement method computing starting values.Return function structure appropriate format.Return function structure appropriate format.","code":""},{"path":"non-linear-regression.html","id":"handling-constrained-parameters","chapter":"6 Non-Linear Regression","heading":"6.3.2 Handling Constrained Parameters","text":"cases, parameters must satisfy constraints (e.g., \\(\\theta_i > \\) \\(< \\theta_i < b\\)). following strategies help address constrained parameter estimation:Fit model without constraints first: unconstrained parameter estimates satisfy desired constraints, action needed.Re-parameterization: estimated parameters violate constraints, consider re-parameterizing model naturally enforce required bounds.","code":""},{"path":"non-linear-regression.html","id":"failure-to-converge","chapter":"6 Non-Linear Regression","heading":"6.3.3 Failure to Converge","text":"Several factors can cause algorithm fail converge:“flat” SSE function: sum squared errors \\(SSE(\\theta)\\) relatively constant neighborhood minimum, algorithm may struggle locate optimal solution.Poor starting values: Trying different better initial values can help.Overly complex models: model complex relative data, consider simplifying .","code":""},{"path":"non-linear-regression.html","id":"convergence-to-a-local-minimum","chapter":"6 Non-Linear Regression","heading":"6.3.4 Convergence to a Local Minimum","text":"Linear least squares models well-defined, unique minimum SSE function quadratic:\\[ SSE(\\theta) = (Y - X\\beta)'(Y - X\\beta) \\]Nonlinear least squares models may multiple local minima.Testing different starting values can help identify global minimum.Graphing \\(SSE(\\theta)\\) function individual parameters (feasible) can provide insights.Alternative optimization algorithms Genetic Algorithm particle swarm optimization may useful non-convex problems.","code":""},{"path":"non-linear-regression.html","id":"model-adequacy-and-estimation-considerations","chapter":"6 Non-Linear Regression","heading":"6.3.5 Model Adequacy and Estimation Considerations","text":"Assessing adequacy nonlinear model involves checking nonlinearity, goodness fit, residual behavior. Unlike linear models, nonlinear models always direct equivalent \\(R^2\\), issues collinearity, leverage, residual heteroscedasticity must carefully evaluated.","code":""},{"path":"non-linear-regression.html","id":"components-of-nonlinearity","chapter":"6 Non-Linear Regression","heading":"6.3.5.1 Components of Nonlinearity","text":"D. M. Bates Watts (1980) defines two key aspects nonlinearity statistical modeling:Intrinsic NonlinearityMeasures bending twisting function \\(f(\\theta)\\).Assumes function relatively flat (planar) neighborhood \\(\\hat{\\theta}\\).severe, distribution residuals distorted.Leads :\nSlow convergence optimization algorithms.\nDifficulties identifying parameter estimates.\nSlow convergence optimization algorithms.Difficulties identifying parameter estimates.Solution approaches:\nHigher-order Taylor expansions estimation.\nBayesian methods parameter estimation.\nHigher-order Taylor expansions estimation.Bayesian methods parameter estimation.Parameter-Effects NonlinearityMeasures curvature (nonlinearity) depends parameterization.Measures curvature (nonlinearity) depends parameterization.Strong parameter effects nonlinearity can cause problems inference \\(\\hat{\\theta}\\).Strong parameter effects nonlinearity can cause problems inference \\(\\hat{\\theta}\\).Can assessed using:\nrms.curv function MASS.\nBootstrap-based inference.\nCan assessed using:rms.curv function MASS.rms.curv function MASS.Bootstrap-based inference.Bootstrap-based inference.Solution: Try reparameterization stabilize function.Solution: Try reparameterization stabilize function.","code":"\n# Check intrinsic curvature\nmodD <- deriv3(~ a * exp(b * x), c(\"a\", \"b\"), function(a, b, x) NULL)\n\nnlin_modD <- nls(y ~ modD(a, b, x),\n                 start = list(a = astrt, b = bstrt),\n                 data = datf)\n\nrms.curv(nlin_modD)  # Function from the MASS package to assess curvature\n#> Parameter effects: c^theta x sqrt(F) = 0.0564 \n#>         Intrinsic: c^iota  x sqrt(F) = 9e-04"},{"path":"non-linear-regression.html","id":"goodness-of-fit-in-nonlinear-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.2 Goodness of Fit in Nonlinear Models","text":"linear regression, use standard coefficient determination ($R^2$):\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\n\\]​:\\(SSR\\) = Regression Sum Squares\\(SSR\\) = Regression Sum Squares\\(SSE\\) = Error Sum Squares\\(SSE\\) = Error Sum Squares\\(SSTO\\) = Total Sum Squares\\(SSTO\\) = Total Sum SquaresHowever, nonlinear models, error model sum squares necessarily add total corrected sum squares:\\[\nSSR + SSE \\neq SST\n\\]Thus, \\(R^2\\) directly valid nonlinear case. Instead, use pseudo-\\(R^2\\):\\[\nR^2_{pseudo} = 1 - \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\]Unlike true \\(R^2\\), interpreted proportion variability explained model.Unlike true \\(R^2\\), interpreted proportion variability explained model.used relative model comparison (e.g., comparing different nonlinear models).used relative model comparison (e.g., comparing different nonlinear models).","code":""},{"path":"non-linear-regression.html","id":"residual-analysis-in-nonlinear-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.3 Residual Analysis in Nonlinear Models","text":"Residual plots help assess model adequacy, particularly intrinsic curvature small.nonlinear models, studentized residuals :\\[\nr_i = \\frac{e_i}{s \\sqrt{1-\\hat{c}_i}}\n\\]:\\(e_i\\) = residual observation \\(\\)\\(e_i\\) = residual observation \\(\\)\\(\\hat{c}_i\\) = \\(\\)th diagonal element tangent-plane hat matrix:\\(\\hat{c}_i\\) = \\(\\)th diagonal element tangent-plane hat matrix:\\[\n\\mathbf{\\hat{H} = F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\n\\]","code":"\n# Residual diagnostics for nonlinear models\nlibrary(nlstools)\nresid_nls <- nlsResiduals(nlin_modD)\n\n# Generate residual plots\nplot(resid_nls)"},{"path":"non-linear-regression.html","id":"potential-issues-in-nonlinear-regression-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.4 Potential Issues in Nonlinear Regression Models","text":"","code":""},{"path":"non-linear-regression.html","id":"collinearity-1","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.1 Collinearity","text":"Measures correlated model’s predictors .Measures correlated model’s predictors .nonlinear models, collinearity assessed using condition number :nonlinear models, collinearity assessed using condition number :\\[\n\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\n\\]condition number > 30, collinearity concern.condition number > 30, collinearity concern.Solution: Consider reparameterization (Magel Hertsgaard 1987).Solution: Consider reparameterization (Magel Hertsgaard 1987).","code":""},{"path":"non-linear-regression.html","id":"leverage","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.2 Leverage","text":"Similar leverage Ordinary Least Squares.Similar leverage Ordinary Least Squares.nonlinear models, leverage assessed using tangent-plane hat matrix:nonlinear models, leverage assessed using tangent-plane hat matrix:\\[\n\\mathbf{\\hat{H} = F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\n\\]Solution: Identify influential points assess impact parameter estimates (St Laurent Cook 1992).","code":""},{"path":"non-linear-regression.html","id":"heterogeneous-errors","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.3 Heterogeneous Errors","text":"Non-constant variance across observations.Non-constant variance across observations.Solution: Use Weighted Nonlinear Least Squares (WNLS).Solution: Use Weighted Nonlinear Least Squares (WNLS).","code":""},{"path":"non-linear-regression.html","id":"correlated-errors","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.4 Correlated Errors","text":"Residuals may autocorrelated.Residuals may autocorrelated.Solution approaches:\nGeneralized Nonlinear Least Squares (GNLS)\nNonlinear Mixed Models (NLMEM)\nBayesian Methods\nSolution approaches:Generalized Nonlinear Least Squares (GNLS)Generalized Nonlinear Least Squares (GNLS)Nonlinear Mixed Models (NLMEM)Nonlinear Mixed Models (NLMEM)Bayesian MethodsBayesian Methods","code":""},{"path":"non-linear-regression.html","id":"application","chapter":"6 Non-Linear Regression","heading":"6.4 Application","text":"","code":""},{"path":"non-linear-regression.html","id":"nonlinear-estimation-using-gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.4.1 Nonlinear Estimation Using Gauss-Newton Algorithm","text":"section demonstrates nonlinear parameter estimation using Gauss-Newton algorithm compares results nls(). model given :\\[\ny_i = \\frac{\\theta_0 + \\theta_1 x_i}{1 + \\theta_2 \\exp(0.4 x_i)} + \\epsilon_i\n\\]\\(= 1, \\dots ,n\\)\\(\\theta_0\\), \\(\\theta_1\\), \\(\\theta_2\\) unknown parameters.\\(\\epsilon_i\\) represents errors.Loading Visualizing DataDeriving Starting Values ParametersSince nonlinear optimization sensitive starting values, estimate reasonable initial values based model interpretation.Finding Maximum \\(Y\\) ValueWhen \\(y = 2.6722\\), corresponding \\(x = 0.0094\\).\\(y = 2.6722\\), corresponding \\(x = 0.0094\\).model equation: \\(\\theta_0 + 0.0094 \\theta_1 = 2.6722\\)model equation: \\(\\theta_0 + 0.0094 \\theta_1 = 2.6722\\)Estimating \\(\\theta_2\\) Median \\(y\\) ValueThe equation simplifies : \\(1 + \\theta_2 \\exp(0.4 x) = 2\\)yields equation: \\(83.58967 \\theta_2 = 1\\)Finding Value \\(\\theta_0\\) \\(\\theta_1\\)provides another equation: \\(\\theta_0 + \\theta_1 \\times 0.9895 - 2.164479 \\theta_2 = 1.457\\)Solving \\(\\theta_0, \\theta_1, \\theta_2\\)Implementing Gauss-Newton AlgorithmUsing estimates, manually implement Gauss-Newton optimization.Defining Model DerivativesIterative Gauss-Newton OptimizationChecking Convergence VarianceValidating nls()","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the dataset\nmy_data <- read.delim(\"images/S21hw1pr4.txt\", header = FALSE, sep = \"\") %>%\n  dplyr::rename(x = V1, y = V2)\n\n# Plot data\nggplot(my_data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Observed Data\", x = \"X\", y = \"Y\") +\n  theme_minimal()\nmax(my_data$y)\n#> [1] 2.6722\nmy_data$x[which.max(my_data$y)]\n#> [1] 0.0094\n# find mean y\nmean(my_data$y) \n#> [1] -0.0747864\n\n# find y closest to its mean\nmy_data$y[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] -0.0773\n\n\n# find x closest to the mean y\nmy_data$x[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] 11.0648\n# find value of x closet to 1\nmy_data$x[which.min(abs(my_data$x - 1))] \n#> [1] 0.9895\n\n# find index of x closest to 1\nmatch(my_data$x[which.min(abs(my_data$x - 1))], my_data$x) \n#> [1] 14\n\n# find y value\nmy_data$y[match(my_data$x[which.min(abs(my_data$x - 1))], my_data$x)]\n#> [1] 1.4577\nlibrary(matlib)\n\n# Define coefficient matrix\nA = matrix(\n  c(0, 0.0094, 0, 0, 0, 83.58967, 1, 0.9895, -2.164479),\n  nrow = 3,\n  ncol = 3,\n  byrow = T\n)\n\n# Define constant vector\nb <- c(2.6722, 1, 1.457)\n\n# Display system of equations\nshowEqn(A, b)\n#> 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 \n#> 0*x1      + 0*x2 + 83.58967*x3  =       1 \n#> 1*x1 + 0.9895*x2 - 2.164479*x3  =   1.457\n\n# Solve for parameters\ntheta_start <- Solve(A, b, fractions = FALSE)\n#> x1      =  -279.80879739 \n#>   x2    =   284.27659574 \n#>     x3  =      0.0119632\ntheta_start\n#> [1] \"x1      =  -279.80879739\" \"  x2    =   284.27659574\"\n#> [3] \"    x3  =      0.0119632\"\n# Starting values\ntheta_0_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[1]))\ntheta_1_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[2]))\ntheta_2_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[3]))\n\n# Model function\nmod_4 <- function(theta_0, theta_1, theta_2, x) {\n  (theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x))\n}\n\n# Define function expression\nf_4 = expression((theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x)))\n\n# First derivatives\ndf_4.d_theta_0 <- D(f_4, 'theta_0')\ndf_4.d_theta_1 <- D(f_4, 'theta_1')\ndf_4.d_theta_2 <- D(f_4, 'theta_2')\n# Initialize\ntheta_vec <- matrix(c(theta_0_strt, theta_1_strt, theta_2_strt))\ndelta <- matrix(NA, nrow = 3, ncol = 1)\ni <- 1\n\n# Evaluate function at initial estimates\nf_theta <- as.matrix(eval(f_4, list(\n  x = my_data$x,\n  theta_0 = theta_vec[1, 1],\n  theta_1 = theta_vec[2, 1],\n  theta_2 = theta_vec[3, 1]\n)))\n\nrepeat {\n  # Compute Jacobian matrix\n  F_theta_0 <- as.matrix(cbind(\n    eval(df_4.d_theta_0, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    )),\n    eval(df_4.d_theta_1, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    )),\n    eval(df_4.d_theta_2, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    ))\n  ))\n  \n  # Compute parameter updates\n  delta[, i] = (solve(t(F_theta_0)%*%F_theta_0))%*%t(F_theta_0)%*%(my_data$y-f_theta[,i])\n    \n  \n  # Update parameter estimates\n  theta_vec <- cbind(theta_vec, theta_vec[, i] + delta[, i])\n  theta_vec[, i + 1] = theta_vec[, i] + delta[, i]\n  \n  # Increment iteration counter\n  i <- i + 1\n  \n  # Compute new function values\n  f_theta <- cbind(f_theta, as.matrix(eval(f_4, list(\n    x = my_data$x,\n    theta_0 = theta_vec[1, i],\n    theta_1 = theta_vec[2, i],\n    theta_2 = theta_vec[3, i]\n  ))))\n  \n  delta = cbind(delta, matrix(NA, nrow = 3, ncol = 1))\n  \n  # Convergence criteria based on SSE\n  if (abs(sum((my_data$y - f_theta[, i])^2) - sum((my_data$y - f_theta[, i - 1])^2)) / \n      sum((my_data$y - f_theta[, i - 1])^2) < 0.001) {\n    break\n  }\n}\n\n# Final parameter estimates\ntheta_vec[, ncol(theta_vec)]\n#> [1]  3.6335135 -1.3055166  0.5043502\n# Final objective function value (SSE)\nsum((my_data$y - f_theta[, i])^2)\n#> [1] 19.80165\n\nsigma2 <- 1 / (nrow(my_data) - 3) * \n  (t(my_data$y - f_theta[, ncol(f_theta)]) %*% \n   (my_data$y - f_theta[, ncol(f_theta)]))  # p = 3\n\n# Asymptotic variance-covariance matrix\nas.numeric(sigma2)*as.matrix(solve(crossprod(F_theta_0)))\n#>             [,1]        [,2]        [,3]\n#> [1,]  0.11552571 -0.04817428  0.02685848\n#> [2,] -0.04817428  0.02100861 -0.01158212\n#> [3,]  0.02685848 -0.01158212  0.00703916\nnlin_4 <- nls(\n  y ~ mod_4(theta_0, theta_1, theta_2, x),\n  start = list(\n    theta_0 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[1])),\n    theta_1 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[2])),\n    theta_2 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[3]))\n  ),\n  data = my_data\n)\nsummary(nlin_4)\n#> \n#> Formula: y ~ mod_4(theta_0, theta_1, theta_2, x)\n#> \n#> Parameters:\n#>         Estimate Std. Error t value Pr(>|t|)    \n#> theta_0  3.63591    0.36528   9.954  < 2e-16 ***\n#> theta_1 -1.30639    0.15561  -8.395 3.65e-15 ***\n#> theta_2  0.50528    0.09215   5.483 1.03e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2831 on 247 degrees of freedom\n#> \n#> Number of iterations to convergence: 9 \n#> Achieved convergence tolerance: 2.307e-07"},{"path":"non-linear-regression.html","id":"logistic-growth-model","chapter":"6 Non-Linear Regression","heading":"6.4.2 Logistic Growth Model","text":"classic logistic growth model follows equation:\\[\nP = \\frac{K}{1 + \\exp(P_0 + r t)} + \\epsilon\n\\]:\\(P\\) = population time \\(t\\)\\(P\\) = population time \\(t\\)\\(K\\) = carrying capacity (maximum population)\\(K\\) = carrying capacity (maximum population)\\(r\\) = population growth rate\\(r\\) = population growth rate\\(P_0\\) = initial population log-ratio\\(P_0\\) = initial population log-ratioHowever, R’s built-SSlogis function uses slightly different parameterization:\\[\nP = \\frac{asym}{1 + \\exp\\left(\\frac{xmid - t}{scal}\\right)}\n\\]:\\(asym\\) = carrying capacity (\\(K\\))\\(asym\\) = carrying capacity (\\(K\\))\\(xmid\\) = \\(x\\)-value inflection point curve\\(xmid\\) = \\(x\\)-value inflection point curve\\(scal\\) = scaling parameter\\(scal\\) = scaling parameterThis gives parameter relationships:\\(K = asym\\)\\(K = asym\\)\\(r = -1 / scal\\)\\(r = -1 / scal\\)\\(P_0 = -r \\cdot xmid\\)\\(P_0 = -r \\cdot xmid\\)fit model using alternative parameterization (\\(K, r, P_0\\)), convert estimated coefficients:Visualizing Logistic Model Fit","code":"\n# Simulated time-series data\ntime <- c(1, 2, 3, 5, 10, 15, 20, 25, 30, 35)\npopulation <- c(2.8, 4.2, 3.5, 6.3, 15.7, 21.3, 23.7, 25.1, 25.8, 25.9)\n\n# Plot data points\nplot(time, population, las = 1, pch = 16, main = \"Logistic Growth Model\")\n\n# Fit the logistic growth model using programmed starting values\nlogisticModelSS <- nls(population ~ SSlogis(time, Asym, xmid, scal))\n\n# Model summary\nsummary(logisticModelSS)\n#> \n#> Formula: population ~ SSlogis(time, Asym, xmid, scal)\n#> \n#> Parameters:\n#>      Estimate Std. Error t value Pr(>|t|)    \n#> Asym  25.5029     0.3666   69.56 3.34e-11 ***\n#> xmid   8.7347     0.3007   29.05 1.48e-08 ***\n#> scal   3.6353     0.2186   16.63 6.96e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 1.906e-06\n\n# Extract parameter estimates\ncoef(logisticModelSS)\n#>      Asym      xmid      scal \n#> 25.502890  8.734698  3.635333\n# Convert parameter estimates to alternative logistic model parameters\nKs <- as.numeric(coef(logisticModelSS)[1])  # Carrying capacity (K)\nrs <- -1 / as.numeric(coef(logisticModelSS)[3])  # Growth rate (r)\nPos <- -rs * as.numeric(coef(logisticModelSS)[2])  # P_0\n\n# Fit the logistic model with the alternative parameterization\nlogisticModel <- nls(\n    population ~ K / (1 + exp(Po + r * time)),\n    start = list(Po = Pos, r = rs, K = Ks)\n)\n\n# Model summary\nsummary(logisticModel)\n#> \n#> Formula: population ~ K/(1 + exp(Po + r * time))\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> Po  2.40272    0.12702   18.92 2.87e-07 ***\n#> r  -0.27508    0.01654  -16.63 6.96e-07 ***\n#> K  25.50289    0.36665   69.56 3.34e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 0 \n#> Achieved convergence tolerance: 1.91e-06\n# Plot original data\nplot(time,\n     population,\n     las = 1,\n     pch = 16,\n     main = \"Logistic Growth Model Fit\")\n\n# Overlay the fitted logistic curve\nlines(time,\n      predict(logisticModel),\n      col = \"red\",\n      lwd = 2)"},{"path":"non-linear-regression.html","id":"nonlinear-plateau-model","chapter":"6 Non-Linear Regression","heading":"6.4.3 Nonlinear Plateau Model","text":"example based (Schabenberger Pierce 2001) demonstrates use plateau model estimate relationship soil nitrate (\\(NO_3\\)) concentration relative yield percent (RYP) two different depths (30 cm 60 cm).suggested nonlinear plateau model given :\\[\nE(Y_{ij}) = (\\beta_{0j} + \\beta_{1j}N_{ij})I_{N_{ij}\\le \\alpha_j} + (\\beta_{0j} + \\beta_{1j}\\alpha_j)I_{N_{ij} > \\alpha_j}\n\\]:\\(N_{ij}\\) represents soil nitrate (\\(NO_3\\)) concentration observation \\(\\) depth \\(j\\).\\(N_{ij}\\) represents soil nitrate (\\(NO_3\\)) concentration observation \\(\\) depth \\(j\\).\\(\\) indexes individual observations.\\(\\) indexes individual observations.\\(j = 1, 2\\) corresponds depths 30 cm 60 cm.\\(j = 1, 2\\) corresponds depths 30 cm 60 cm.model assumes linear increase threshold (\\(\\alpha_j\\)), beyond response levels (plateaus).Defining Plateau Model FunctionCreating Self-Starting Function nlsSince model piecewise linear, can estimate starting values using:linear regression first half sorted predictor values estimate \\(b_0\\) \\(b_1\\).linear regression first half sorted predictor values estimate \\(b_0\\) \\(b_1\\).last predictor value used regression plateau threshold (\\(\\alpha\\))last predictor value used regression plateau threshold (\\(\\alpha\\))Combining Model Self-Start FunctionThe nls function used estimate parameters separately soil depth (30 cm 60 cm).generate separate plots 30 cm 60 cm depths, showing confidence prediction intervals.Modeling Soil Depths Together Comparing ModelsInstead fitting separate models different soil depths, first fit combined model observations share common slope, intercept, plateau. test whether modeling two depths separately provides significantly better fit.Fitting Reduced (Combined) ModelThe reduced model assumes soil depths follow nonlinear relationship.Examining Residuals Combined ModelChecking residuals helps diagnose potential lack fit.pattern residuals (e.g., systematic deviations based soil depth), suggests separate model depth may necessary.Testing Whether Depths Require Separate ModelsTo formally test whether soil depth significantly affects model parameters, introduce parameterization depth-specific parameters increments baseline model (30 cm depth):\\[\n\\begin{aligned}\n\\beta_{02} &= \\beta_{01} + d_0 \\\\\n\\beta_{12} &= \\beta_{11} + d_1 \\\\\n\\alpha_{2} &= \\alpha_{1} + d_a\n\\end{aligned}\n\\]:\\(\\beta_{01}, \\beta_{11}, \\alpha_1\\) parameters 30 cm depth.\\(\\beta_{01}, \\beta_{11}, \\alpha_1\\) parameters 30 cm depth.\\(d_0, d_1, d_a\\) represent depth-specific differences 60 cm depth.\\(d_0, d_1, d_a\\) represent depth-specific differences 60 cm depth.\\(d_0, d_1, d_a\\) significantly different 0, two depths modeled separately.\\(d_0, d_1, d_a\\) significantly different 0, two depths modeled separately.Defining Full (Depth-Specific) ModelFitting Full (Depth-Specific) ModelThe starting values taken separately fitted models depth.Model Comparison: Depth Matter?\\(d_0, d_1, d_a\\) significantly different 0, depths modeled separately.\\(d_0, d_1, d_a\\) significantly different 0, depths modeled separately.p-values parameters indicate whether depth-specific modeling necessary.p-values parameters indicate whether depth-specific modeling necessary.","code":"\n# Load data\ndat <- read.table(\"images/dat.txt\", header = TRUE)\n\n# Plot NO3 concentration vs. relative yield percent, colored by depth\nlibrary(ggplot2)\ndat.plot <- ggplot(dat) + \n  geom_point(aes(x = no3, y = ryp, color = as.factor(depth))) +\n  labs(color = 'Depth (cm)') + \n  xlab('Soil NO3 Concentration') + \n  ylab('Relative Yield Percent') +\n  theme_minimal()\n\n# Display plot\ndat.plot\n# Define the nonlinear plateau model function\nnonlinModel <- function(predictor, b0, b1, alpha) {\n  ifelse(predictor <= alpha, \n         b0 + b1 * predictor,  # Linear growth below threshold\n         b0 + b1 * alpha)      # Plateau beyond threshold\n}\n# Define initialization function for self-starting plateau model\nnonlinModelInit <- function(mCall, LHS, data) {\n  # Sort data by increasing predictor value\n  xy <- sortedXyData(mCall[['predictor']], LHS, data)\n  n <- nrow(xy)\n  \n  # Fit a simple linear model using the first half of the sorted data\n  lmFit <- lm(xy[1:(n / 2), 'y'] ~ xy[1:(n / 2), 'x'])\n  \n  # Extract initial estimates\n  b0 <- coef(lmFit)[1]  # Intercept\n  b1 <- coef(lmFit)[2]  # Slope\n  alpha <- xy[(n / 2), 'x']  # Last x-value in the fitted linear range\n  \n  # Return initial parameter estimates\n  value <- c(b0, b1, alpha)\n  names(value) <- mCall[c('b0', 'b1', 'alpha')]\n  value\n}\n# Define a self-starting nonlinear model for nls\nSS_nonlinModel <- selfStart(nonlinModel,\n                            nonlinModelInit,\n                            c('b0', 'b1', 'alpha'))\n# Fit the model for depth = 30 cm\nsep30_nls <- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n                  data = dat[dat$depth == 30,])\n\n# Fit the model for depth = 60 cm\nsep60_nls <- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n                  data = dat[dat$depth == 60,])\n# Set plotting layout\npar(mfrow = c(1, 2))\n\n# Plot model fit for 30 cm depth\nplotFit(\n  sep30_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = dat[dat$depth == 30,],\n  main = \"Results at 30 cm Depth\",\n  ylab = \"Relative Yield Percent\",\n  xlab = \"Soil NO3 Concentration\",\n  xlim = c(0, 120)\n)\n\n# Plot model fit for 60 cm depth\nplotFit(\n  sep60_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightpink4\",\n  col.pred = \"lightpink2\",\n  data = dat[dat$depth == 60,],\n  main = \"Results at 60 cm Depth\",\n  ylab = \"Relative Yield Percent\",\n  xlab = \"Soil NO3 Concentration\",\n  xlim = c(0, 120)\n)\nsummary(sep30_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0     15.1943     2.9781   5.102 6.89e-07 ***\n#> b1      3.5760     0.1853  19.297  < 2e-16 ***\n#> alpha  23.1324     0.5098  45.373  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.258 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 6 \n#> Achieved convergence tolerance: 1.166e-08\nsummary(sep60_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      5.4519     2.9785    1.83   0.0684 .  \n#> b1      5.6820     0.2529   22.46   <2e-16 ***\n#> alpha  16.2863     0.2818   57.80   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.427 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 5 \n#> Achieved convergence tolerance: 2.196e-08\n# Fit the combined model (common parameters across all depths)\nred_nls <- nls(\n  ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), \n  data = dat\n)\n\n# Display model summary\nsummary(red_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      8.7901     2.7688   3.175   0.0016 ** \n#> b1      4.8995     0.2207  22.203   <2e-16 ***\n#> alpha  18.0333     0.3242  55.630   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.13 on 477 degrees of freedom\n#> \n#> Number of iterations to convergence: 7 \n#> Achieved convergence tolerance: 8.568e-09\n\n# Visualizing the combined model fit\npar(mfrow = c(1, 1))\nplotFit(\n  red_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightblue4\",\n  col.pred = \"lightblue2\",\n  data = dat,\n  main = 'Results for Combined Model',\n  ylab = 'Relative Yield Percent',\n  xlab = 'Soil NO3 Concentration'\n)\nlibrary(nlstools)\n\n# Residual diagnostics using nlstools\nresid <- nlsResiduals(red_nls)\n\n# Plot residuals\nplot(resid)\nnonlinModelF <- function(predictor, soildep, b01, b11, a1, d0, d1, da) {\n  \n  # Define parameters for 60 cm depth as increments from 30 cm parameters\n  b02 <- b01 + d0\n  b12 <- b11 + d1\n  a2 <- a1 + da\n  \n  # Compute model output for 30 cm depth\n  y1 <- ifelse(\n    predictor <= a1, \n    b01 + b11 * predictor, \n    b01 + b11 * a1\n  )\n  \n  # Compute model output for 60 cm depth\n  y2 <- ifelse(\n    predictor <= a2, \n    b02 + b12 * predictor, \n    b02 + b12 * a2\n  )\n  \n  # Assign correct model output based on depth\n  y <- y1 * (soildep == 30) + y2 * (soildep == 60)\n  \n  return(y)\n}\nSoil_full <- nls(\n  ryp ~ nonlinModelF(\n    predictor = no3,\n    soildep = depth,\n    b01,\n    b11,\n    a1,\n    d0,\n    d1,\n    da\n  ),\n  data = dat,\n  start = list(\n    b01 = 15.2,   # Intercept for 30 cm depth\n    b11 = 3.58,   # Slope for 30 cm depth\n    a1 = 23.13,   # Plateau cutoff for 30 cm depth\n    d0 = -9.74,   # Intercept difference (60 cm - 30 cm)\n    d1 = 2.11,    # Slope difference (60 cm - 30 cm)\n    da = -6.85    # Plateau cutoff difference (60 cm - 30 cm)\n  )\n)\n\n# Display model summary\nsummary(Soil_full)\n#> \n#> Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, \n#>     a1, d0, d1, da)\n#> \n#> Parameters:\n#>     Estimate Std. Error t value Pr(>|t|)    \n#> b01  15.1943     2.8322   5.365 1.27e-07 ***\n#> b11   3.5760     0.1762  20.291  < 2e-16 ***\n#> a1   23.1324     0.4848  47.711  < 2e-16 ***\n#> d0   -9.7424     4.2357  -2.300   0.0219 *  \n#> d1    2.1060     0.3203   6.575 1.29e-10 ***\n#> da   -6.8461     0.5691 -12.030  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.854 on 474 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 3.742e-06"},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"7 Generalized Linear Models","heading":"7 Generalized Linear Models","text":"Generalized Linear Models (GLMs) extend traditional linear regression framework accommodate response variables necessarily follow normal distribution. provide flexible approach modeling relationships set predictors various types dependent variables.Ordinary Least Squares regression assumes response variable continuous normally distributed, GLMs allow response variables follow distributions exponential family, binomial, Poisson, gamma distributions. flexibility makes particularly useful wide range business research applications.GLM consists three key components:random component: response variable \\(Y_i\\) follows distribution exponential family (e.g., binomial, Poisson, gamma).systematic component: linear predictor \\(\\eta_i = \\mathbf{x'_i} \\beta\\), \\(\\mathbf{x'_i}\\) vector observed covariates (predictor variables) \\(\\beta\\) vector parameters estimated.link function: function \\(g(\\cdot)\\) relates expected value response variable, \\(\\mu_i = E(Y_i)\\), linear predictor (.e., \\(\\eta_i = g(\\mu_i)\\)).Although relationship predictors outcome may appear nonlinear original outcome scale (due link function), GLM still considered “linear” statistical sense remains linear parameters \\(\\beta\\). Consequently, GLMs generally classified nonlinear regression models. “generalize” traditional linear model allowing broader range response variable distributions link functions, retain linearity parameters.choice distribution link function depends nature response variable. following sections, explore several important GLM variants:Logistic Regression: Used binary response variables (e.g., customer churn, loan defaults).Probit Regression: Similar logistic regression assumes normal distribution underlying probability.Poisson Regression: Used modeling count data (e.g., number purchases, call center inquiries).Negative Binomial Regression: extension Poisson regression accounts overdispersion count data.Quasi-Poisson Regression: variation Poisson regression adjusts overdispersion allowing variance linear function mean.Multinomial Logistic Regression: generalization logistic regression categorical response variables two outcomes.Generalization Generalized Linear Model: flexible generalization ordinary linear regression allows response variables different distributions (e.g., normal, binomial, Poisson).","code":""},{"path":"generalized-linear-models.html","id":"sec-logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.1 Logistic Regression","text":"Logistic regression widely used Generalized Linear Model designed modeling binary response variables. particularly useful applications credit scoring, medical diagnosis, customer churn prediction.","code":""},{"path":"generalized-linear-models.html","id":"logistic-model","chapter":"7 Generalized Linear Models","heading":"7.1.1 Logistic Model","text":"Given set predictor variables \\(\\mathbf{x}_i\\), probability positive outcome (e.g., success, event occurring) modeled :\\[\np_i = f(\\mathbf{x}_i ; \\beta) = \\frac{\\exp(\\mathbf{x_i'\\beta})}{1 + \\exp(\\mathbf{x_i'\\beta})}\n\\]:\\(p_i = \\mathbb{E}[Y_i]\\) probability success observation \\(\\).\\(\\mathbf{x_i}\\) vector predictor variables.\\(\\beta\\) vector model coefficients.","code":""},{"path":"generalized-linear-models.html","id":"sec-logit-transformation","chapter":"7 Generalized Linear Models","heading":"7.1.1.1 Logit Transformation","text":"logistic function can rewritten terms log-odds, also known logit function:\\[\n\\text{logit}(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x_i'\\beta}\n\\]:\\(\\frac{p_i}{1 - p_i}\\) represents odds success (ratio probability success probability failure).logit function ensures linearity parameters, aligns GLM framework.Thus, logistic regression belongs family Generalized Linear Models function mean response (logit) linear predictors.","code":""},{"path":"generalized-linear-models.html","id":"sec-likelihood-function-logistic","chapter":"7 Generalized Linear Models","heading":"7.1.2 Likelihood Function","text":"Since \\(Y_i\\) follows Bernoulli distribution probability \\(p_i\\), likelihood function \\(n\\) independent observations :\\[\nL(p_i) = \\prod_{=1}^{n} p_i^{Y_i} (1 - p_i)^{1 - Y_i}\n\\]substituting logistic function \\(p_i\\):\\[\np_i = \\frac{\\exp(\\mathbf{x'_i \\beta})}{1+\\exp(\\mathbf{x'_i \\beta})}, \\quad 1 - p_i = \\frac{1}{1+\\exp(\\mathbf{x'_i \\beta})}\n\\]obtain:\\[\nL(\\beta) = \\prod_{=1}^{n} \\left( \\frac{\\exp(\\mathbf{x'_i \\beta})}{1+\\exp(\\mathbf{x'_i \\beta})} \\right)^{Y_i} \\left( \\frac{1}{1+\\exp(\\mathbf{x'_i \\beta})} \\right)^{1 - Y_i}\n\\]Taking natural logarithm likelihood function gives log-likelihood function:\\[\nQ(\\beta) = \\log L(\\beta) = \\sum_{=1}^n Y_i \\mathbf{x'_i \\beta} - \\sum_{=1}^n \\log(1 + \\exp(\\mathbf{x'_i \\beta}))\n\\]Since function concave, can maximize numerically using iterative optimization techniques, :Newton-Raphson MethodFisher Scoring AlgorithmThese methods allow us obtain Maximum Likelihood Estimates parameters, \\(\\hat{\\beta}\\).standard regularity conditions, MLEs logistic regression parameters asymptotically normal:\\[\n\\hat{\\beta} \\dot{\\sim} (\\beta, [\\mathbf{}(\\beta)]^{-1})\n\\]:\\(\\mathbf{}(\\beta)\\) Fisher Information Matrix, determines variance-covariance structure \\(\\hat{\\beta}\\).","code":""},{"path":"generalized-linear-models.html","id":"fisher-information-matrix","chapter":"7 Generalized Linear Models","heading":"7.1.3 Fisher Information Matrix","text":"Fisher Information Matrix quantifies amount information observable random variable carries unknown parameter \\(\\beta\\). crucial estimating variance-covariance matrix estimated coefficients logistic regression.Mathematically, Fisher Information Matrix defined :\\[\n\\mathbf{}(\\beta) = E\\left[ \\frac{\\partial \\log L(\\beta)}{\\partial \\beta} \\frac{\\partial \\log L(\\beta)}{\\partial \\beta'} \\right]\n\\]expands :\\[\n\\mathbf{}(\\beta) = E\\left[ \\left(\\frac{\\partial \\log L(\\beta)}{\\partial \\beta_i} \\frac{\\partial \\log L(\\beta)}{\\partial \\beta_j} \\right)_{ij} \\right]\n\\]regularity conditions, Fisher Information Matrix equivalent negative expected Hessian matrix:\\[\n\\mathbf{}(\\beta) = -E\\left[ \\frac{\\partial^2 \\log L(\\beta)}{\\partial \\beta \\partial \\beta'} \\right]\n\\]expands :\\[\n\\mathbf{}(\\beta) = -E \\left[ \\left( \\frac{\\partial^2 \\log L(\\beta)}{\\partial \\beta_i \\partial \\beta_j} \\right)_{ij} \\right]\n\\]representation particularly useful allows us compute Fisher Information Matrix directly Hessian log-likelihood function.Example: Fisher Information Matrix Logistic RegressionConsider simple logistic regression model one predictor:\\[\nx_i' \\beta = \\beta_0 + \\beta_1 x_i\n\\]log-likelihood function, second-order partial derivatives :\\[\n\\begin{aligned}\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_0} &= \\sum_{=1}^n \\frac{\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - \\left[\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}\\right]^2 & \\text{Intercept} \\\\\n&= \\sum_{=1}^n p_i (1-p_i)  \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_1} &= \\sum_{=1}^n \\frac{x_i^2\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - \\left[\\frac{x_i\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}\\right]^2 & \\text{Slope}\\\\\n&= \\sum_{=1}^n x_i^2p_i (1-p_i)  \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta_0 \\partial \\beta_1} &= \\sum_{=1}^n \\frac{x_i\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - x_i\\left[\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}\\right]^2 & \\text{Cross-derivative}\\\\\n&= \\sum_{=1}^n x_ip_i (1-p_i)\n\\end{aligned}\n\\]Combining elements, Fisher Information Matrix logistic regression model :\\[\n\\mathbf{} (\\beta) =\n\\begin{bmatrix}\n\\sum_{=1}^{n} p_i(1 - p_i) & \\sum_{=1}^{n} x_i p_i(1 - p_i) \\\\\n\\sum_{=1}^{n} x_i p_i(1 - p_i) & \\sum_{=1}^{n} x_i^2 p_i(1 - p_i)\n\\end{bmatrix}\n\\]:\\(p_i = \\frac{\\exp(x_i' \\beta)}{1+\\exp(x_i' \\beta)}\\) represents predicted probability.\\(p_i (1 - p_i)\\) variance Bernoulli response variable.diagonal elements represent variances estimated coefficients.-diagonal elements represent covariances \\(\\beta_0\\) \\(\\beta_1\\).inverse Fisher Information Matrix provides variance-covariance matrix estimated coefficients:\\[\n\\mathbf{Var}(\\hat{\\beta}) = \\mathbf{}(\\hat{\\beta})^{-1}\n\\]matrix essential :Estimating standard errors logistic regression coefficients.Constructing confidence intervals \\(\\beta\\).Performing hypothesis tests (e.g., Wald Test).","code":"\n# Load necessary library\nlibrary(stats)\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- rbinom(n, 1, prob = plogis(0.5 + 1.2 * x))\n\n# Fit logistic regression model\nmodel <- glm(y ~ x, family = binomial)\n\n# Extract the Fisher Information Matrix (Negative Hessian)\nfisher_info <- summary(model)$cov.unscaled\n\n# Display the Fisher Information Matrix\nprint(fisher_info)\n#>             (Intercept)          x\n#> (Intercept)  0.05718171 0.01564322\n#> x            0.01564322 0.10302992"},{"path":"generalized-linear-models.html","id":"inference-in-logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.1.4 Inference in Logistic Regression","text":"estimate model parameters \\(\\hat{\\beta}\\) using Maximum Likelihood Estimation, can conduct inference assess significance predictors, construct confidence intervals, perform hypothesis testing. two common inference approaches logistic regression :Likelihood Ratio TestWald StatisticsThese tests rely asymptotic normality MLEs properties Fisher Information Matrix.","code":""},{"path":"generalized-linear-models.html","id":"sec-likelihood-ratio-test-logistic","chapter":"7 Generalized Linear Models","heading":"7.1.4.1 Likelihood Ratio Test","text":"Likelihood Ratio Test compares two models:Restricted Model: simpler model parameters constrained specific values.Unrestricted Model: full model without constraints.test hypothesis subset parameters \\(\\beta_1\\), leave \\(\\beta_2\\) (nuisance parameters) unspecified.Hypothesis Setup:\\[\nH_0: \\beta_1 = \\beta_{1,0}\n\\]\\(\\beta_{1,0}\\) specified value (often zero). Let:\\(\\hat{\\beta}_{2,0}\\) MLE \\(\\beta_2\\) constraint \\(\\beta_1 = \\beta_{1,0}\\).\\(\\hat{\\beta}_1, \\hat{\\beta}_2\\) MLEs full model.likelihood ratio test statistic :\\[\n-2\\log\\Lambda = -2[\\log L(\\beta_{1,0}, \\hat{\\beta}_{2,0}) - \\log L(\\hat{\\beta}_1, \\hat{\\beta}_2)]\n\\]:first term log-likelihood restricted model.second term log-likelihood unrestricted model.null hypothesis:\\[\n-2 \\log \\Lambda \\sim \\chi^2_{\\upsilon}\n\\]\\(\\upsilon\\) number restricted parameters. reject \\(H_0\\) :\\[\n-2\\log \\Lambda > \\chi^2_{\\upsilon,1-\\alpha}\n\\]Interpretation: likelihood ratio test statistic large, suggests restricted model (\\(H_0\\)) fits significantly worse full model, leading us reject null hypothesis.","code":""},{"path":"generalized-linear-models.html","id":"sec-wald-test-logistic","chapter":"7 Generalized Linear Models","heading":"7.1.4.2 Wald Test","text":"Wald test based asymptotic normality MLEs:\\[\n\\hat{\\beta} \\sim (\\beta, [\\mathbf{}(\\beta)]^{-1})\n\\]test:\\[\nH_0: \\mathbf{L} \\hat{\\beta} = 0\n\\]\\(\\mathbf{L}\\) \\(q \\times p\\) matrix \\(q\\) linearly independent rows (often used test multiple coefficients simultaneously). Wald test statistic :\\[\nW = (\\mathbf{L\\hat{\\beta}})'(\\mathbf{L[(\\hat{\\beta})]^{-1}L'})^{-1}(\\mathbf{L\\hat{\\beta}})\n\\]\\(H_0\\):\\[\nW \\sim \\chi^2_q\n\\]Interpretation: \\(W\\) large, null hypothesis rejected, suggesting least one tested coefficients significantly different zero.Comparing Likelihood Ratio Wald Tests","code":"\n# Load necessary library\nlibrary(stats)\n\n# Simulate some binary outcome data\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- rbinom(n, 1, prob = plogis(0.5 + 1.2 * x))\n\n# Fit logistic regression model\nmodel <- glm(y ~ x, family = binomial)\n\n# Display model summary (includes Wald tests)\nsummary(model)\n#> \n#> Call:\n#> glm(formula = y ~ x, family = binomial)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   0.7223     0.2391   3.020 0.002524 ** \n#> x             1.2271     0.3210   3.823 0.000132 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 128.21  on 99  degrees of freedom\n#> Residual deviance: 108.29  on 98  degrees of freedom\n#> AIC: 112.29\n#> \n#> Number of Fisher Scoring iterations: 4\n\n# Perform likelihood ratio test using anova()\nanova(model, test=\"Chisq\")\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: y\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>      Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \n#> NULL                    99     128.21              \n#> x     1   19.913        98     108.29 8.105e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"generalized-linear-models.html","id":"confidence-intervals-for-coefficients","chapter":"7 Generalized Linear Models","heading":"7.1.4.3 Confidence Intervals for Coefficients","text":"95% confidence interval logistic regression coefficient \\(\\beta_i\\) given :\\[\n\\hat{\\beta}_i \\pm 1.96 \\hat{s}_{ii}\n\\]:\\(\\hat{\\beta}_i\\) estimated coefficient.\\(\\hat{s}_{ii}\\) standard error (square root diagonal element \\(\\mathbf{[(\\hat{\\beta})]}^{-1}\\)).confidence interval provides range plausible values \\(\\beta_i\\). interval include zero, conclude \\(\\beta_i\\) statistically significant.large sample sizes, Likelihood Ratio Test Wald Test yield similar results.small sample sizes, Likelihood Ratio Test preferred Wald test can less reliable.","code":""},{"path":"generalized-linear-models.html","id":"interpretation-of-logistic-regression-coefficients","chapter":"7 Generalized Linear Models","heading":"7.1.4.4 Interpretation of Logistic Regression Coefficients","text":"single predictor variable, logistic regression model :\\[\n\\text{logit}(\\hat{p}_i) = \\log\\left(\\frac{\\hat{p}_i}{1 - \\hat{p}_i} \\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]:\\(\\hat{p}_i\\) predicted probability success \\(x_i\\).\\(\\hat{\\beta}_1\\) represents log odds change one-unit increase \\(x\\).Interpreting \\(\\beta_1\\) Terms OddsWhen predictor variable increases one unit, logit probability changes \\(\\hat{\\beta}_1\\):\\[\n\\text{logit}(\\hat{p}_{x_i +1}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 (x_i + 1) = \\text{logit}(\\hat{p}_{x_i}) + \\hat{\\beta}_1\n\\]Thus, difference log odds :\\[\n\\begin{aligned}\n\\text{logit}(\\hat{p}_{x_i +1}) - \\text{logit}(\\hat{p}_{x_i})\n&= \\log ( \\text{odds}(\\hat{p}_{x_i + 1})) - \\log (\\text{odds}(\\hat{p}_{x_i}) )\\\\\n&= \\log\\left( \\frac{\\text{odds}(\\hat{p}_{x_i + 1})}{\\text{odds}(\\hat{p}_{x_i})} \\right) \\\\\n&= \\hat{\\beta}_1\n\\end{aligned}\n\\]Exponentiating sides:\\[\n\\exp(\\hat{\\beta}_1) = \\frac{\\text{odds}(\\hat{p}_{x_i + 1})}{\\text{odds}(\\hat{p}_{x_i})}\n\\]quantity, \\(\\exp(\\hat{\\beta}_1)\\), odds ratio, quantifies effect one-unit increase \\(x\\) odds success.Generalization: Odds Ratio Change \\(x\\)difference \\(c\\) units predictor \\(x\\), estimated odds ratio :\\[\n\\exp(c\\hat{\\beta}_1)\n\\]multiple predictors, \\(\\exp(\\hat{\\beta}_k)\\) represents odds ratio \\(x_k\\), holding variables constant.","code":""},{"path":"generalized-linear-models.html","id":"inference-on-the-mean-response","chapter":"7 Generalized Linear Models","heading":"7.1.4.5 Inference on the Mean Response","text":"given set predictor values \\(x_h = (1, x_{h1}, ..., x_{h,p-1})'\\), estimated mean response (probability success) :\\[\n\\hat{p}_h = \\frac{\\exp(\\mathbf{x'_h \\hat{\\beta}})}{1 + \\exp(\\mathbf{x'_h \\hat{\\beta}})}\n\\]variance estimated probability :\\[\ns^2(\\hat{p}_h) = \\mathbf{x'_h[(\\hat{\\beta})]^{-1}x_h}\n\\]:\\(\\mathbf{}(\\hat{\\beta})^{-1}\\) variance-covariance matrix \\(\\hat{\\beta}\\).\\(s^2(\\hat{p}_h)\\) provides estimate uncertainty \\(\\hat{p}_h\\).many applications, logistic regression used classification, predict whether observation belongs category 0 1. commonly used decision rule :Assign \\(y = 1\\) \\(\\hat{p}_h \\geq \\tau\\)Assign \\(y = 0\\) \\(\\hat{p}_h < \\tau\\)\\(\\tau\\) chosen cutoff threshold (typically \\(\\tau = 0.5\\)).","code":"\n# Load necessary library\nlibrary(stats)\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- rbinom(n, 1, prob = plogis(0.5 + 1.2 * x))\n\n# Fit logistic regression model\nmodel <- glm(y ~ x, family = binomial)\n\n# Display model summary\nsummary(model)\n#> \n#> Call:\n#> glm(formula = y ~ x, family = binomial)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   0.7223     0.2391   3.020 0.002524 ** \n#> x             1.2271     0.3210   3.823 0.000132 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 128.21  on 99  degrees of freedom\n#> Residual deviance: 108.29  on 98  degrees of freedom\n#> AIC: 112.29\n#> \n#> Number of Fisher Scoring iterations: 4\n\n# Extract coefficients and standard errors\ncoef_estimates <- coef(summary(model))\nbeta_hat <- coef_estimates[, 1]   # Estimated coefficients\nse_beta  <- coef_estimates[, 2]   # Standard errors\n\n# Compute 95% confidence intervals for coefficients\nconf_intervals <- cbind(\n  beta_hat - 1.96 * se_beta, \n  beta_hat + 1.96 * se_beta\n)\n\n# Compute Odds Ratios\nodds_ratios <- exp(beta_hat)\n\n# Display results\nprint(\"Confidence Intervals for Coefficients:\")\n#> [1] \"Confidence Intervals for Coefficients:\"\nprint(conf_intervals)\n#>                  [,1]     [,2]\n#> (Intercept) 0.2535704 1.190948\n#> x           0.5979658 1.856218\n\nprint(\"Odds Ratios:\")\n#> [1] \"Odds Ratios:\"\nprint(odds_ratios)\n#> (Intercept)           x \n#>    2.059080    3.411295\n\n# Predict probability for a new observation (e.g., x = 1)\nnew_x <- data.frame(x = 1)\npredicted_prob <- predict(model, newdata = new_x, type = \"response\")\n\nprint(\"Predicted Probability for x = 1:\")\n#> [1] \"Predicted Probability for x = 1:\"\nprint(predicted_prob)\n#>         1 \n#> 0.8753759"},{"path":"generalized-linear-models.html","id":"application-logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.1.5 Application: Logistic Regression","text":"section, demonstrate application logistic regression using simulated data. explore model fitting, inference, residual analysis, goodness--fit testing.1. Load Required Libraries2. Data GenerationWe generate dataset predictor variable \\(X\\) follows uniform distribution:\\[\nx \\sim Unif(-0.5,2.5)\n\\]linear predictor given :\\[\n\\eta = 0.5 + 0.75 x\n\\]Passing \\(\\eta\\) inverse-logit function, obtain:\\[\np = \\frac{\\exp(\\eta)}{1+ \\exp(\\eta)}\n\\]ensures \\(p \\[0,1]\\). generate binary response variable:\\[\ny \\sim Bernoulli(p)\n\\]3. Model FittingWe fit logistic regression model simulated data:\\[\n\\text{logit}(p) = \\beta_0 + \\beta_1 X\n\\]Interpretation Odds RatioWhen \\(x = 0\\), odds success 1.59.\\(x = 0\\), odds success 1.59.\\(x = 1\\), odds success increase factor 2.19, indicating 119.29% increase.\\(x = 1\\), odds success increase factor 2.19, indicating 119.29% increase.4. Deviance TestWe assess model’s significance using deviance test, compares:\\(H_0\\): predictors related response (intercept-model).\\(H_0\\): predictors related response (intercept-model).\\(H_1\\): least one predictor related response.\\(H_1\\): least one predictor related response.test statistic :\\[\nD = \\text{Null Deviance} - \\text{Residual Deviance}\n\\]Conclusion:Since p-value approximately 0, reject \\(H_0\\), confirming \\(X\\) significantly related \\(Y\\).5. Residual AnalysisWe compute deviance residuals plot \\(X\\).plot informative. insightful approach binned residual plots.6. Binned Residual PlotWe group residuals bins based predicted values.also examine predicted values vs residuals:Finally, compare predicted probabilities actual outcomes:7. Model Goodness--Fit: Hosmer-Lemeshow TestThe Hosmer-Lemeshow test evaluates whether model fits data well. test statistic : \\[\nX^2_{HL} = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{p}_j)^2}{m_j \\hat{p}_j(1-\\hat{p}_j)}\n\\] :\\(y_j\\) observed number successes bin \\(j\\).\\(y_j\\) observed number successes bin \\(j\\).\\(m_j\\) number observations bin \\(j\\).\\(m_j\\) number observations bin \\(j\\).\\(\\hat{p}_j\\) predicted probability bin \\(j\\).\\(\\hat{p}_j\\) predicted probability bin \\(j\\).\\(H_0\\), assume:\\[\nX^2_{HL} \\sim \\chi^2_{J-1}\n\\]Conclusion:Since \\(p\\)-value = 0.99, fail reject \\(H_0\\).Since \\(p\\)-value = 0.99, fail reject \\(H_0\\).indicates model fits data well.indicates model fits data well.","code":"\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(pscl)\nlibrary(ggplot2)\nlibrary(faraway)\nlibrary(nnet)\nlibrary(agridat)\nlibrary(nlstools)\nset.seed(23) # Set seed for reproducibility\nx <- runif(1000, min = -0.5, max = 2.5)  # Generate X values\neta1 <- 0.5 + 0.75 * x                   # Compute linear predictor\np <- exp(eta1) / (1 + exp(eta1))          # Compute probabilities\ny <- rbinom(1000, 1, p)                   # Generate binary response\nBinData <- data.frame(X = x, Y = y)       # Create data frame\nLogistic_Model <- glm(formula = Y ~ X,\n                      # Specifies the response distribution\n                      family = binomial,\n                      data = BinData)\n\nsummary(Logistic_Model) # Model summary\n#> \n#> Call:\n#> glm(formula = Y ~ X, family = binomial, data = BinData)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***\n#> X            0.78527    0.09296   8.447  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1106.7  on 999  degrees of freedom\n#> Residual deviance: 1027.4  on 998  degrees of freedom\n#> AIC: 1031.4\n#> \n#> Number of Fisher Scoring iterations: 4\nnlstools::confint2(Logistic_Model) # Confidence intervals\n#>                 2.5 %    97.5 %\n#> (Intercept) 0.2618709 0.6622204\n#> X           0.6028433 0.9676934\n\n# Compute odds ratios\nOddsRatio <- coef(Logistic_Model) %>% exp\nOddsRatio\n#> (Intercept)           X \n#>    1.587318    2.192995\nTest_Dev <- Logistic_Model$null.deviance - Logistic_Model$deviance\np_val_dev <- 1 - pchisq(q = Test_Dev, df = 1)\np_val_dev\n#> [1] 0\nLogistic_Resids <- residuals(Logistic_Model, type = \"deviance\")\n\nplot(\n    y = Logistic_Resids,\n    x = BinData$X,\n    xlab = 'X',\n    ylab = 'Deviance Residuals'\n)\nplot_bin <- function(Y,\n                     X,\n                     bins = 100,\n                     return.DF = FALSE) {\n  Y_Name <- deparse(substitute(Y))\n  X_Name <- deparse(substitute(X))\n  \n  Binned_Plot <- data.frame(Plot_Y = Y, Plot_X = X)\n  Binned_Plot$bin <-\n    cut(Binned_Plot$Plot_X, breaks = bins) %>% as.numeric\n  \n  Binned_Plot_summary <- Binned_Plot %>%\n    group_by(bin) %>%\n    summarise(\n      Y_ave = mean(Plot_Y),\n      X_ave = mean(Plot_X),\n      Count = n()\n    ) %>% as.data.frame\n  \n  plot(\n    y = Binned_Plot_summary$Y_ave,\n    x = Binned_Plot_summary$X_ave,\n    ylab = Y_Name,\n    xlab = X_Name\n  )\n  \n  if (return.DF)\n    return(Binned_Plot_summary)\n}\n\nplot_bin(Y = Logistic_Resids, X = BinData$X, bins = 100)\nLogistic_Predictions <- predict(Logistic_Model, type = \"response\")\nplot_bin(Y = Logistic_Resids, X = Logistic_Predictions, bins = 100)\nNumBins <- 10\nBinned_Data <- plot_bin(\n    Y = BinData$Y,\n    X = Logistic_Predictions,\n    bins = NumBins,\n    return.DF = TRUE\n)\nBinned_Data\n#>    bin     Y_ave     X_ave Count\n#> 1    1 0.5833333 0.5382095    72\n#> 2    2 0.5200000 0.5795887    75\n#> 3    3 0.6567164 0.6156540    67\n#> 4    4 0.7014925 0.6579674    67\n#> 5    5 0.6373626 0.6984765    91\n#> 6    6 0.7500000 0.7373341    72\n#> 7    7 0.7096774 0.7786747    93\n#> 8    8 0.8503937 0.8203819   127\n#> 9    9 0.8947368 0.8601232   133\n#> 10  10 0.8916256 0.9004734   203\nabline(0, 1, lty = 2, col = 'blue')\nHL_BinVals <- (Binned_Data$Count * Binned_Data$Y_ave - \n               Binned_Data$Count * Binned_Data$X_ave) ^ 2 /   \n               (Binned_Data$Count * Binned_Data$X_ave * (1 - Binned_Data$X_ave))\n\nHLpval <- pchisq(q = sum(HL_BinVals),\n                 df = NumBins - 1,\n                 lower.tail = FALSE)\nHLpval\n#> [1] 0.4150004"},{"path":"generalized-linear-models.html","id":"sec-probit-regression","chapter":"7 Generalized Linear Models","heading":"7.2 Probit Regression","text":"Probit regression type Generalized Linear Models used binary outcome variables. Unlike logistic regression, uses logit function, probit regression assumes probability success determined underlying normally distributed latent variable.","code":""},{"path":"generalized-linear-models.html","id":"probit-model","chapter":"7 Generalized Linear Models","heading":"7.2.1 Probit Model","text":"Let \\(Y_i\\) binary response variable:\\[\nY_i =\n\\begin{cases}\n1, & \\text{success occurs} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]assume \\(Y_i\\) follows Bernoulli distribution:\\[\nY_i \\sim \\text{Bernoulli}(p_i), \\quad \\text{} p_i = P(Y_i = 1 | \\mathbf{x_i})\n\\]Instead logit function logistic regression:\\[\n\\text{logit}(p_i) = \\log\\left( \\frac{p_i}{1 - p_i} \\right) = \\mathbf{x_i'\\beta}\n\\]Probit regression uses inverse standard normal CDF:\\[\n\\Phi^{-1}(p_i) = \\mathbf{x_i'\\theta}\n\\]:\\(\\Phi(\\cdot)\\) CDF standard normal distribution.\\(\\Phi(\\cdot)\\) CDF standard normal distribution.\\(\\mathbf{x_i}\\) vector predictors.\\(\\mathbf{x_i}\\) vector predictors.\\(\\theta\\) vector regression coefficients.\\(\\theta\\) vector regression coefficients.Thus, probability success :\\[\np_i = P(Y_i = 1 | \\mathbf{x_i}) = \\Phi(\\mathbf{x_i'\\theta})\n\\]:\\[\n\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} dt\n\\]","code":""},{"path":"generalized-linear-models.html","id":"application-probit-regression","chapter":"7 Generalized Linear Models","heading":"7.2.2 Application: Probit Regression","text":"","code":"\n# Load necessary library\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate data\nn <- 1000\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nlatent <- 0.5 * x1 + 0.7 * x2 + rnorm(n)  # Linear combination\ny <- ifelse(latent > 0, 1, 0)  # Binary outcome\n\n# Create dataframe\ndata <- data.frame(y, x1, x2)\n\n# Fit Probit model\nprobit_model <-\n    glm(y ~ x1 + x2, family = binomial(link = \"probit\"), data = data)\nsummary(probit_model)\n#> \n#> Call:\n#> glm(formula = y ~ x1 + x2, family = binomial(link = \"probit\"), \n#>     data = data)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -0.09781    0.04499  -2.174   0.0297 *  \n#> x1           0.43838    0.04891   8.963   <2e-16 ***\n#> x2           0.75538    0.05306  14.235   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1385.1  on 999  degrees of freedom\n#> Residual deviance: 1045.3  on 997  degrees of freedom\n#> AIC: 1051.3\n#> \n#> Number of Fisher Scoring iterations: 5\n\n# Fit Logit model\nlogit_model <-\n    glm(y ~ x1 + x2, family = binomial(link = \"logit\"), data = data)\nsummary(logit_model)\n#> \n#> Call:\n#> glm(formula = y ~ x1 + x2, family = binomial(link = \"logit\"), \n#>     data = data)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -0.16562    0.07600  -2.179   0.0293 *  \n#> x1           0.73234    0.08507   8.608   <2e-16 ***\n#> x2           1.25220    0.09486  13.201   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1385.1  on 999  degrees of freedom\n#> Residual deviance: 1048.4  on 997  degrees of freedom\n#> AIC: 1054.4\n#> \n#> Number of Fisher Scoring iterations: 4\n\n# Compare Coefficients\ncoef_comparison <- data.frame(\n    Variable = names(coef(probit_model)),\n    Probit_Coef = coef(probit_model),\n    Logit_Coef = coef(logit_model),\n    Logit_Probit_Ratio = coef(logit_model) / coef(probit_model)\n)\n\nprint(coef_comparison)\n#>                Variable Probit_Coef Logit_Coef Logit_Probit_Ratio\n#> (Intercept) (Intercept) -0.09780689 -0.1656216           1.693353\n#> x1                   x1  0.43837627  0.7323392           1.670572\n#> x2                   x2  0.75538259  1.2522008           1.657704\n\n# Compute predicted probabilities\ndata$probit_pred <- predict(probit_model, type = \"response\")\ndata$logit_pred <- predict(logit_model, type = \"response\")\n\n# Plot Probit vs Logit predictions\nggplot(data, aes(x = probit_pred, y = logit_pred)) +\n    geom_point(alpha = 0.5) +\n    geom_abline(slope = 1,\n                intercept = 0,\n                col = \"red\") +\n    labs(title = \"Comparison of Predicted Probabilities\",\n         x = \"Probit Predictions\", y = \"Logit Predictions\")\n\n# Classification Accuracy\nthreshold <- 0.5\ndata$probit_class <- ifelse(data$probit_pred > threshold, 1, 0)\ndata$logit_class <- ifelse(data$logit_pred > threshold, 1, 0)\n\nprobit_acc <- mean(data$probit_class == data$y)\nlogit_acc <- mean(data$logit_class == data$y)\n\nprint(paste(\"Probit Accuracy:\", round(probit_acc, 4)))\n#> [1] \"Probit Accuracy: 0.71\"\nprint(paste(\"Logit Accuracy:\", round(logit_acc, 4)))\n#> [1] \"Logit Accuracy: 0.71\""},{"path":"generalized-linear-models.html","id":"sec-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.3 Binomial Regression","text":"previous sections, introduced binomial regression models, including Logistic Regression probit regression, discussed theoretical foundations. Now, apply methods real-world data using esoph dataset, examines relationship esophageal cancer potential risk factors alcohol consumption age group.","code":""},{"path":"generalized-linear-models.html","id":"dataset-overview","chapter":"7 Generalized Linear Models","heading":"7.3.1 Dataset Overview","text":"esoph dataset consists :Successes (ncases): number individuals diagnosed esophageal cancer.Successes (ncases): number individuals diagnosed esophageal cancer.Failures (ncontrols): number individuals control group (without cancer).Failures (ncontrols): number individuals control group (without cancer).Predictors:\nagegp: Age group individuals.\nalcgp: Alcohol consumption category.\ntobgp: Tobacco consumption category.\nPredictors:agegp: Age group individuals.agegp: Age group individuals.alcgp: Alcohol consumption category.alcgp: Alcohol consumption category.tobgp: Tobacco consumption category.tobgp: Tobacco consumption category.fitting models, let’s inspect dataset visualize key relationships.","code":"\n# Load and inspect the dataset\ndata(\"esoph\")\nhead(esoph, n = 3)\n#>   agegp     alcgp    tobgp ncases ncontrols\n#> 1 25-34 0-39g/day 0-9g/day      0        40\n#> 2 25-34 0-39g/day    10-19      0        10\n#> 3 25-34 0-39g/day    20-29      0         6\n\n# Visualizing the proportion of cancer cases by alcohol consumption\nplot(\n  esoph$ncases / (esoph$ncases + esoph$ncontrols) ~ esoph$alcgp,\n  ylab = \"Proportion of Cancer Cases\",\n  xlab = \"Alcohol Consumption Group\",\n  main = \"Esophageal Cancer Data\"\n)\n\n# Ensure categorical variables are treated as factors\nclass(esoph$agegp) <- \"factor\"\nclass(esoph$alcgp) <- \"factor\"\nclass(esoph$tobgp) <- \"factor\""},{"path":"generalized-linear-models.html","id":"apply-logistic-model","chapter":"7 Generalized Linear Models","heading":"7.3.2 Apply Logistic Model","text":"first fit logistic regression model, response variable proportion cancer cases (ncases) relative total observations (ncases + ncontrols).InterpretationThe coefficients represent log-odds esophageal cancer relative baseline alcohol consumption group.coefficients represent log-odds esophageal cancer relative baseline alcohol consumption group.P-values indicate whether alcohol consumption levels significantly influence cancer risk.P-values indicate whether alcohol consumption levels significantly influence cancer risk.Model DiagnosticsTo improve model, include age group (agegp) additional predictor.Key TakeawaysAIC Reduction: lower AIC suggests adding age predictor improves model.AIC Reduction: lower AIC suggests adding age predictor improves model.Likelihood Ratio Test: test compares two models determines whether improvement statistically significant.Likelihood Ratio Test: test compares two models determines whether improvement statistically significant.","code":"\n# Logistic regression using alcohol consumption as a predictor\nmodel <-\n    glm(cbind(ncases, ncontrols) ~ alcgp,\n        data = esoph,\n        family = binomial)\n\n# Summary of the model\nsummary(model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.5885     0.1925 -13.444  < 2e-16 ***\n#> alcgp40-79    1.2712     0.2323   5.472 4.46e-08 ***\n#> alcgp80-119   2.0545     0.2611   7.868 3.59e-15 ***\n#> alcgp120+     3.3042     0.3237  10.209  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 221.46  on 84  degrees of freedom\n#> AIC: 344.51\n#> \n#> Number of Fisher Scoring iterations: 5\n# Convert coefficients to odds ratios\nexp(coefficients(model))\n#> (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ \n#>  0.07512953  3.56527094  7.80261593 27.22570533\n\n# Model goodness-of-fit measures\ndeviance(model) / df.residual(model)  # Closer to 1 suggests a better fit\n#> [1] 2.63638\nmodel$aic  # Lower AIC is preferable for model comparison\n#> [1] 344.5109\n# Logistic regression with alcohol consumption and age\nbetter_model <- glm(\n    cbind(ncases, ncontrols) ~ agegp + alcgp,\n    data = esoph,\n    family = binomial\n)\n\n# Summary of the improved model\nsummary(better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -6.1472     1.0419  -5.900 3.63e-09 ***\n#> agegp35-44    1.6311     1.0800   1.510 0.130973    \n#> agegp45-54    3.4258     1.0389   3.297 0.000976 ***\n#> agegp55-64    3.9435     1.0346   3.811 0.000138 ***\n#> agegp65-74    4.3568     1.0413   4.184 2.87e-05 ***\n#> agegp75+      4.4242     1.0914   4.054 5.04e-05 ***\n#> alcgp40-79    1.4343     0.2448   5.859 4.64e-09 ***\n#> alcgp80-119   2.0071     0.2776   7.230 4.84e-13 ***\n#> alcgp120+     3.6800     0.3763   9.778  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 105.88  on 79  degrees of freedom\n#> AIC: 238.94\n#> \n#> Number of Fisher Scoring iterations: 6\n\n# Model evaluation\nbetter_model$aic # Lower AIC is better\n#> [1] 238.9361\n\n# Convert coefficients to odds ratios\n# exp(coefficients(better_model))\ndata.frame(`Odds Ratios` = exp(coefficients(better_model)))\n#>              Odds.Ratios\n#> (Intercept)  0.002139482\n#> agegp35-44   5.109601844\n#> agegp45-54  30.748594216\n#> agegp55-64  51.596634690\n#> agegp65-74  78.005283850\n#> agegp75+    83.448437749\n#> alcgp40-79   4.196747169\n#> alcgp80-119  7.441782227\n#> alcgp120+   39.646885126\n\n# Compare models using likelihood ratio test (Chi-square test)\npchisq(\n    q = model$deviance - better_model$deviance,\n    df = model$df.residual - better_model$df.residual,\n    lower.tail = FALSE\n)\n#> [1] 2.713923e-23"},{"path":"generalized-linear-models.html","id":"apply-probit-model","chapter":"7 Generalized Linear Models","heading":"7.3.3 Apply Probit Model","text":"discussed earlier, probit model alternative logistic regression, using cumulative normal distribution instead logistic function.Consider Probit Model?Like logistic regression, probit regression estimates probabilities, assumes normal distribution latent variable.Like logistic regression, probit regression estimates probabilities, assumes normal distribution latent variable.interpretation coefficients differs, model comparisons can still made using AIC.interpretation coefficients differs, model comparisons can still made using AIC.","code":"\n# Probit regression model\nProb_better_model <- glm(\n    cbind(ncases, ncontrols) ~ agegp + alcgp,\n    data = esoph,\n    family = binomial(link = probit)\n)\n\n# Summary of the probit model\nsummary(Prob_better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), \n#>     data = esoph)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -3.3741     0.4922  -6.855 7.13e-12 ***\n#> agegp35-44    0.8562     0.5081   1.685 0.092003 .  \n#> agegp45-54    1.7829     0.4904   3.636 0.000277 ***\n#> agegp55-64    2.1034     0.4876   4.314 1.61e-05 ***\n#> agegp65-74    2.3374     0.4930   4.741 2.13e-06 ***\n#> agegp75+      2.3694     0.5275   4.491 7.08e-06 ***\n#> alcgp40-79    0.8080     0.1330   6.076 1.23e-09 ***\n#> alcgp80-119   1.1399     0.1558   7.318 2.52e-13 ***\n#> alcgp120+     2.1204     0.2060  10.295  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 104.48  on 79  degrees of freedom\n#> AIC: 237.53\n#> \n#> Number of Fisher Scoring iterations: 6"},{"path":"generalized-linear-models.html","id":"sec-poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.4 Poisson Regression","text":"","code":""},{"path":"generalized-linear-models.html","id":"the-poisson-distribution","chapter":"7 Generalized Linear Models","heading":"7.4.1 The Poisson Distribution","text":"Poisson regression used modeling count data, response variable represents number occurrences event within fixed period, space, unit. Poisson distribution defined :\\[\n\\begin{aligned} f(Y_i) &= \\frac{\\mu_i^{Y_i} \\exp(-\\mu_i)}{Y_i!}, \\quad Y_i = 0,1,2, \\dots \\\\ E(Y_i) &= \\mu_i \\\\ \\text{Var}(Y_i) &= \\mu_i \\end{aligned}\n\\] :\\(Y_i\\) count variable.\\(Y_i\\) count variable.\\(\\mu_i\\) expected count \\(\\)-th observation.\\(\\mu_i\\) expected count \\(\\)-th observation.mean variance equal \\(E(Y_i) = \\text{Var}(Y_i)\\), making Poisson regression suitable variance follows property.mean variance equal \\(E(Y_i) = \\text{Var}(Y_i)\\), making Poisson regression suitable variance follows property.However, real-world count data often exhibit overdispersion, variance exceeds mean. discuss remedies Quasi-Poisson Negative Binomial Regression later.","code":""},{"path":"generalized-linear-models.html","id":"poisson-model","chapter":"7 Generalized Linear Models","heading":"7.4.2 Poisson Model","text":"model expected count \\(\\mu_i\\) function predictors \\(\\mathbf{x_i}\\) parameters \\(\\boldsymbol{\\theta}\\):\\[\n\\mu_i = f(\\mathbf{x_i; \\theta})\n\\]","code":""},{"path":"generalized-linear-models.html","id":"link-function-choices","chapter":"7 Generalized Linear Models","heading":"7.4.3 Link Function Choices","text":"Since \\(\\mu_i\\) must positive, often use log-link function:\\[\nθ\\log(\\mu_i) = \\mathbf{x_i' \\theta}\n\\]ensures predicted counts always non-negative. analogous logistic regression, use logit link binary outcomes.Rewriting:\\[\n\\mu_i = \\exp(\\mathbf{x_i' \\theta})\n\\] ensures \\(\\mu_i > 0\\) parameter values.","code":""},{"path":"generalized-linear-models.html","id":"application-poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.4.4 Application: Poisson Regression","text":"apply Poisson regression bioChemists dataset (pscl package), contains information academic productivity terms published articles.","code":""},{"path":"generalized-linear-models.html","id":"dataset-overview-1","chapter":"7 Generalized Linear Models","heading":"7.4.4.1 Dataset Overview","text":"distribution number articles right-skewed, suggests Poisson model may appropriate.","code":"\nlibrary(tidyverse)\n# Load dataset\ndata(bioChemists, package = \"pscl\")\n\n# Rename columns for clarity\nbioChemists <- bioChemists %>%\n  rename(\n    Num_Article = art,\n    # Number of articles in last 3 years\n    Sex = fem,\n    # 1 if female, 0 if male\n    Married = mar,\n    # 1 if married, 0 otherwise\n    Num_Kid5 = kid5,\n    # Number of children under age 6\n    PhD_Quality = phd,\n    # Prestige of PhD program\n    Num_MentArticle = ment   # Number of articles by mentor in last 3 years\n  )\n\n# Visualize response variable distribution\nhist(bioChemists$Num_Article, \n     breaks = 25, \n     main = \"Number of Articles Published\")"},{"path":"generalized-linear-models.html","id":"fitting-a-poisson-regression-model","chapter":"7 Generalized Linear Models","heading":"7.4.4.2 Fitting a Poisson Regression Model","text":"model number articles published (Num_Article) function various predictors.Interpretation:Coefficients log scale, meaning represent log-rate ratios.Coefficients log scale, meaning represent log-rate ratios.Exponentiating coefficients gives rate ratios.Exponentiating coefficients gives rate ratios.Statistical significance tells us whether variable meaningful impact publication count.Statistical significance tells us whether variable meaningful impact publication count.","code":"\n# Poisson regression model\nPoisson_Mod <-\n  glm(Num_Article ~ .,\n      family = poisson,\n      data = bioChemists)\n\n# Summary of the model\nsummary(Poisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.304617   0.102981   2.958   0.0031 ** \n#> SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***\n#> MarriedMarried   0.155243   0.061374   2.529   0.0114 *  \n#> Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***\n#> PhD_Quality      0.012823   0.026397   0.486   0.6271    \n#> Num_MentArticle  0.025543   0.002006  12.733  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"generalized-linear-models.html","id":"model-diagnostics-goodness-of-fit","chapter":"7 Generalized Linear Models","heading":"7.4.4.3 Model Diagnostics: Goodness of Fit","text":"","code":""},{"path":"generalized-linear-models.html","id":"pearsons-chi-square-test-for-overdispersion","chapter":"7 Generalized Linear Models","heading":"7.4.4.3.1 Pearson’s Chi-Square Test for Overdispersion","text":"compute Pearson chi-square statistic check whether variance significantly exceeds mean. \\[\nX^2 = \\sum \\frac{(Y_i - \\hat{\\mu}_i)^2}{\\hat{\\mu}_i}\n\\]p-value small, overdispersion present.p-value small, overdispersion present.Large X² statistic suggests model may adequately capture variability.Large X² statistic suggests model may adequately capture variability.","code":"\n# Compute predicted means\nPredicted_Means <- predict(Poisson_Mod, type = \"response\")\n\n# Pearson chi-square test\nX2 <-\n  sum((bioChemists$Num_Article - Predicted_Means) ^ 2 / Predicted_Means)\nX2\n#> [1] 1662.547\npchisq(X2, Poisson_Mod$df.residual, lower.tail = FALSE)\n#> [1] 7.849882e-47"},{"path":"generalized-linear-models.html","id":"overdispersion-check-ratio-of-deviance-to-degrees-of-freedom","chapter":"7 Generalized Linear Models","heading":"7.4.4.3.2 Overdispersion Check: Ratio of Deviance to Degrees of Freedom","text":"compute: \\[\n\\hat{\\phi} = \\frac{\\text{deviance}}{\\text{degrees freedom}}\n\\]\\(\\hat{\\phi} > 1\\), overdispersion likely present.\\(\\hat{\\phi} > 1\\), overdispersion likely present.value significantly 1 suggests need alternative model.value significantly 1 suggests need alternative model.","code":"\n# Overdispersion check\nPoisson_Mod$deviance / Poisson_Mod$df.residual\n#> [1] 1.797988"},{"path":"generalized-linear-models.html","id":"addressing-overdispersion","chapter":"7 Generalized Linear Models","heading":"7.4.4.4 Addressing Overdispersion","text":"","code":""},{"path":"generalized-linear-models.html","id":"including-interaction-terms","chapter":"7 Generalized Linear Models","heading":"7.4.4.4.1 Including Interaction Terms","text":"One possible remedy incorporate interaction terms, capturing complex relationships predictors.may improve model fit, can lead overfitting.","code":"\n# Adding two-way and three-way interaction terms\nPoisson_Mod_All2way <-\n  glm(Num_Article ~ . ^ 2, family = poisson, data = bioChemists)\nPoisson_Mod_All3way <-\n  glm(Num_Article ~ . ^ 3, family = poisson, data = bioChemists)"},{"path":"generalized-linear-models.html","id":"quasi-poisson-model-adjusting-for-overdispersion","chapter":"7 Generalized Linear Models","heading":"7.4.4.4.2 Quasi-Poisson Model (Adjusting for Overdispersion)","text":"quick fix allow variance scale introducing \\(\\hat{\\phi}\\):\\[\n\\text{Var}(Y_i) = \\hat{\\phi} \\mu_i\n\\]Alternatively, refit using Quasi-Poisson model, adjusts standard errors:Quasi-Poisson corrects standard errors, introduce extra parameter overdispersion.","code":"\n# Estimate dispersion parameter\nphi_hat = Poisson_Mod$deviance / Poisson_Mod$df.residual\n\n# Adjusting Poisson model to account for overdispersion\nsummary(Poisson_Mod, dispersion = phi_hat)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.30462    0.13809   2.206  0.02739 *  \n#> SexWomen        -0.22459    0.07323  -3.067  0.00216 ** \n#> MarriedMarried   0.15524    0.08230   1.886  0.05924 .  \n#> Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***\n#> PhD_Quality      0.01282    0.03540   0.362  0.71715    \n#> Num_MentArticle  0.02554    0.00269   9.496  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1.797988)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5\n# Quasi-Poisson model\nquasiPoisson_Mod <- glm(Num_Article ~ ., family = quasipoisson, data = bioChemists)\nsummary(quasiPoisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      0.304617   0.139273   2.187 0.028983 *  \n#> SexWomen        -0.224594   0.073860  -3.041 0.002427 ** \n#> MarriedMarried   0.155243   0.083003   1.870 0.061759 .  \n#> Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***\n#> PhD_Quality      0.012823   0.035700   0.359 0.719544    \n#> Num_MentArticle  0.025543   0.002713   9.415  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for quasipoisson family taken to be 1.829006)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: NA\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"generalized-linear-models.html","id":"negative-binomial-regression-preferred-approach","chapter":"7 Generalized Linear Models","heading":"7.4.4.4.3 Negative Binomial Regression (Preferred Approach)","text":"Negative Binomial Regression explicitly models overdispersion introducing dispersion parameter \\(\\theta\\):\\[\n\\text{Var}(Y_i) = \\mu_i + \\theta \\mu_i^2\n\\]extends Poisson regression allowing variance grow quadratically rather linearly.model generally preferred Quasi-Poisson, explicitly accounts heterogeneity data.","code":"\n# Load MASS package\nlibrary(MASS)\n\n# Fit Negative Binomial regression\nNegBin_Mod <- glm.nb(Num_Article ~ ., data = bioChemists)\n\n# Model summary\nsummary(NegBin_Mod)\n#> \n#> Call:\n#> glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n#>     link = log)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n#> SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n#> MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n#> Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n#> PhD_Quality      0.015271   0.035873   0.426 0.670326    \n#> Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n#> \n#>     Null deviance: 1109.0  on 914  degrees of freedom\n#> Residual deviance: 1004.3  on 909  degrees of freedom\n#> AIC: 3135.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.264 \n#>           Std. Err.:  0.271 \n#> \n#>  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"sec-negative-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.5 Negative Binomial Regression","text":"modeling count data, Poisson regression assumes mean variance equal:\\[\n\\text{Var}(Y_i) = E(Y_i) = \\mu_i\n\\] However, many real-world datasets, variance exceeds mean—phenomenon known overdispersion. overdispersion present, Poisson model underestimates variance, leading :Inflated test statistics (small p-values).Inflated test statistics (small p-values).Overconfident predictions.Overconfident predictions.Poor model fit.Poor model fit.","code":""},{"path":"generalized-linear-models.html","id":"negative-binomial-distribution","chapter":"7 Generalized Linear Models","heading":"7.5.1 Negative Binomial Distribution","text":"address overdispersion, Negative Binomial regression introduces extra dispersion parameter \\(\\theta\\) allow variance greater mean: \\[\n\\text{Var}(Y_i) = \\mu_i + \\theta \\mu_i^2\n\\] :\\(\\mu_i = \\exp(\\mathbf{x_i' \\theta})\\) expected count.\\(\\mu_i = \\exp(\\mathbf{x_i' \\theta})\\) expected count.\\(\\theta\\) dispersion parameter.\\(\\theta\\) dispersion parameter.\\(\\theta \\0\\), NB model reduces Poisson model.\\(\\theta \\0\\), NB model reduces Poisson model.Thus, Negative Binomial regression generalization Poisson regression accounts overdispersion.","code":""},{"path":"generalized-linear-models.html","id":"application-negative-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.5.2 Application: Negative Binomial Regression","text":"apply Negative Binomial regression bioChemists dataset model number research articles (Num_Article) function several predictors.","code":""},{"path":"generalized-linear-models.html","id":"fitting-the-negative-binomial-model","chapter":"7 Generalized Linear Models","heading":"7.5.2.1 Fitting the Negative Binomial Model","text":"Interpretation:coefficients log scale.coefficients log scale.dispersion parameter \\(\\theta\\) (also called size parameter contexts) estimated 2.264 standard error 0.271. Check -Dispersion detail.dispersion parameter \\(\\theta\\) (also called size parameter contexts) estimated 2.264 standard error 0.271. Check -Dispersion detail.Since \\(\\theta\\) significantly different 1, confirms overdispersion, validating choice Negative Binomial model Poisson regression.Since \\(\\theta\\) significantly different 1, confirms overdispersion, validating choice Negative Binomial model Poisson regression.","code":"\n# Load necessary package\nlibrary(MASS)\n\n# Fit Negative Binomial model\nNegBinom_Mod <- MASS::glm.nb(Num_Article ~ ., data = bioChemists)\n\n# Model summary\nsummary(NegBinom_Mod)\n#> \n#> Call:\n#> MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n#>     link = log)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n#> SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n#> MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n#> Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n#> PhD_Quality      0.015271   0.035873   0.426 0.670326    \n#> Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n#> \n#>     Null deviance: 1109.0  on 914  degrees of freedom\n#> Residual deviance: 1004.3  on 909  degrees of freedom\n#> AIC: 3135.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.264 \n#>           Std. Err.:  0.271 \n#> \n#>  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"model-comparison-poisson-vs.-negative-binomial","chapter":"7 Generalized Linear Models","heading":"7.5.2.2 Model Comparison: Poisson vs. Negative Binomial","text":"","code":""},{"path":"generalized-linear-models.html","id":"checking-overdispersion-in-poisson-model","chapter":"7 Generalized Linear Models","heading":"7.5.2.2.1 Checking Overdispersion in Poisson Model","text":"using NB regression, confirm overdispersion computing:\\[\n\\hat{\\phi} = \\frac{\\text{deviance}}{\\text{degrees freedom}}\n\\]\\(\\hat{\\phi} > 1\\), overdispersion present.\\(\\hat{\\phi} > 1\\), overdispersion present.large value suggests Poisson regression underestimates variance.large value suggests Poisson regression underestimates variance.","code":"\n# Overdispersion check for Poisson model\nPoisson_Mod$deviance / Poisson_Mod$df.residual\n#> [1] 1.797988"},{"path":"generalized-linear-models.html","id":"likelihood-ratio-test-poisson-vs.-negative-binomial","chapter":"7 Generalized Linear Models","heading":"7.5.2.2.2 Likelihood Ratio Test: Poisson vs. Negative Binomial","text":"compare Poisson Negative Binomial models using likelihood ratio test, : \\[\nG^2 = 2 \\times ( \\log L_{NB} - \\log L_{Poisson})\n\\] \\(\\text{df} = 1\\)Small p-value (< 0.05) → Negative Binomial model significantly better.Small p-value (< 0.05) → Negative Binomial model significantly better.Large p-value (> 0.05) → Poisson model adequate.Large p-value (> 0.05) → Poisson model adequate.Since overdispersion confirmed, Negative Binomial model preferred.","code":"\n# Likelihood ratio test between Poisson and Negative Binomial\npchisq(2 * (logLik(NegBinom_Mod) - logLik(Poisson_Mod)),\n       df = 1,\n       lower.tail = FALSE)\n#> 'log Lik.' 4.391728e-41 (df=7)"},{"path":"generalized-linear-models.html","id":"model-diagnostics-and-evaluation","chapter":"7 Generalized Linear Models","heading":"7.5.2.3 Model Diagnostics and Evaluation","text":"","code":""},{"path":"generalized-linear-models.html","id":"checking-dispersion-parameter-theta","chapter":"7 Generalized Linear Models","heading":"7.5.2.3.1 Checking Dispersion Parameter \\(\\theta\\)","text":"Negative Binomial dispersion parameter \\(\\theta\\) can retrieved:large \\(\\theta\\) suggests overdispersion extreme, small \\(\\theta\\) (close 0) indicate Poisson model reasonable.","code":"\n# Extract dispersion parameter estimate\nNegBinom_Mod$theta\n#> [1] 2.264388"},{"path":"generalized-linear-models.html","id":"predictions-and-rate-ratios","chapter":"7 Generalized Linear Models","heading":"7.5.2.4 Predictions and Rate Ratios","text":"Negative Binomial regression, exponentiating coefficients gives rate ratios:rate ratio :> 1 \\(\\\\) Increases expected article count.> 1 \\(\\\\) Increases expected article count.< 1 \\(\\\\) Decreases expected article count.< 1 \\(\\\\) Decreases expected article count.= 1 \\(\\\\) effect.= 1 \\(\\\\) effect.example:PhD_Quality exponentiated coefficient 1.5, individuals higher-quality PhD programs expected publish 50% articles.PhD_Quality exponentiated coefficient 1.5, individuals higher-quality PhD programs expected publish 50% articles.Sex exponentiated coefficient 0.8, females publish 20% fewer articles males, else equal.Sex exponentiated coefficient 0.8, females publish 20% fewer articles males, else equal.","code":"\n# Convert coefficients to rate ratios\ndata.frame(`Odds Ratios` = exp(coef(NegBinom_Mod)))\n#>                 Odds.Ratios\n#> (Intercept)       1.2919388\n#> SexWomen          0.8053982\n#> MarriedMarried    1.1624030\n#> Num_Kid5          0.8382698\n#> PhD_Quality       1.0153884\n#> Num_MentArticle   1.0295094"},{"path":"generalized-linear-models.html","id":"alternative-approach-zero-inflated-models","chapter":"7 Generalized Linear Models","heading":"7.5.2.5 Alternative Approach: Zero-Inflated Models","text":"dataset excess zeros (many individuals publish articles), Zero-Inflated Negative Binomial (ZINB) models may required.\\[\n\\text{P}(Y_i = 0) = p + (1 - p) f(Y_i = 0 | \\mu, \\theta)\n\\]:\\(p\\) probability always zero (e.g., inactive researchers).\\(p\\) probability always zero (e.g., inactive researchers).\\(f(Y_i)\\) follows Negative Binomial distribution.\\(f(Y_i)\\) follows Negative Binomial distribution.","code":""},{"path":"generalized-linear-models.html","id":"fitting-a-zero-inflated-negative-binomial-model","chapter":"7 Generalized Linear Models","heading":"7.5.3 Fitting a Zero-Inflated Negative Binomial Model","text":"model accounts :Structural zero inflation.Structural zero inflation.Overdispersion.Overdispersion.ZINB often preferred many observations zero. However, since ZINB fall GLM framework, discuss Nonlinear Generalized Linear Mixed Models.ZINB GLM?Unlike GLMs, assume single response distribution exponential family, ZINB mixture model two components:\nCount model – negative binomial regression main count process.\nInflation model – logistic regression excess zeros.\nUnlike GLMs, assume single response distribution exponential family, ZINB mixture model two components:Count model – negative binomial regression main count process.Count model – negative binomial regression main count process.Inflation model – logistic regression excess zeros.Inflation model – logistic regression excess zeros.ZINB combines two distinct processes rather using single exponential family distribution, fit within standard GLM framework.ZINB Belongs ToZINB part finite mixture models sometimes considered within generalized linear mixed models (GLMMs) semi-parametric models.","code":"\n# Load package for zero-inflated models\nlibrary(pscl)\n\n# Fit ZINB model\nZINB_Mod <- zeroinfl(Num_Article ~ ., data = bioChemists, dist = \"negbin\")\n\n# Model summary\nsummary(ZINB_Mod)\n#> \n#> Call:\n#> zeroinfl(formula = Num_Article ~ ., data = bioChemists, dist = \"negbin\")\n#> \n#> Pearson residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.2942 -0.7601 -0.2909  0.4448  6.4155 \n#> \n#> Count model coefficients (negbin with log link):\n#>                   Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.4167465  0.1435966   2.902  0.00371 ** \n#> SexWomen        -0.1955068  0.0755926  -2.586  0.00970 ** \n#> MarriedMarried   0.0975826  0.0844520   1.155  0.24789    \n#> Num_Kid5        -0.1517325  0.0542061  -2.799  0.00512 ** \n#> PhD_Quality     -0.0007001  0.0362697  -0.019  0.98460    \n#> Num_MentArticle  0.0247862  0.0034927   7.097 1.28e-12 ***\n#> Log(theta)       0.9763565  0.1354695   7.207 5.71e-13 ***\n#> \n#> Zero-inflation model coefficients (binomial with logit link):\n#>                 Estimate Std. Error z value Pr(>|z|)   \n#> (Intercept)     -0.19169    1.32282  -0.145  0.88478   \n#> SexWomen         0.63593    0.84892   0.749  0.45379   \n#> MarriedMarried  -1.49947    0.93867  -1.597  0.11017   \n#> Num_Kid5         0.62843    0.44278   1.419  0.15582   \n#> PhD_Quality     -0.03771    0.30801  -0.122  0.90254   \n#> Num_MentArticle -0.88229    0.31623  -2.790  0.00527 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n#> \n#> Theta = 2.6548 \n#> Number of iterations in BFGS optimization: 43 \n#> Log-likelihood: -1550 on 13 Df"},{"path":"generalized-linear-models.html","id":"sec-quasi-poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.6 Quasi-Poisson Regression","text":"Poisson regression assumes mean variance equal:\\[\n\\text{Var}(Y_i) = E(Y_i) = \\mu_i\n\\]However, many real-world datasets exhibit overdispersion, variance exceeds mean:\\[\n\\text{Var}(Y_i) = \\phi \\mu_i\n\\]\\(\\phi\\) (dispersion parameter) allows variance scale beyond Poisson assumption.correct , use Quasi-Poisson regression, :Follows Generalized Linear Models structure strict GLM.Follows Generalized Linear Models structure strict GLM.Uses variance function proportional mean: \\(\\text{Var}(Y_i) = \\phi \\mu_i\\).Uses variance function proportional mean: \\(\\text{Var}(Y_i) = \\phi \\mu_i\\).assume specific probability distribution, unlike Poisson Negative Binomial models.assume specific probability distribution, unlike Poisson Negative Binomial models.","code":""},{"path":"generalized-linear-models.html","id":"is-quasi-poisson-regression-a-generalized-linear-model","chapter":"7 Generalized Linear Models","heading":"7.6.1 Is Quasi-Poisson Regression a Generalized Linear Model?","text":"✅ Yes, Quasi-Poisson GLM-like:Linear Predictor: Like Poisson regression, models log expected count function predictors: \\[\n\\log(E(Y)) = X\\beta\n\\]Linear Predictor: Like Poisson regression, models log expected count function predictors: \\[\n\\log(E(Y)) = X\\beta\n\\]Canonical Link Function: typically uses log link function, just like standard Poisson regression.Canonical Link Function: typically uses log link function, just like standard Poisson regression.Variance Structure: Unlike standard Poisson, assumes \\(\\text{Var}(Y) = E(Y)\\), Quasi-Poisson allows overdispersion: \\[\n\\text{Var}(Y) = \\phi E(Y)\n\\] \\(\\phi\\) estimated rather assumed 1.Variance Structure: Unlike standard Poisson, assumes \\(\\text{Var}(Y) = E(Y)\\), Quasi-Poisson allows overdispersion: \\[\n\\text{Var}(Y) = \\phi E(Y)\n\\] \\(\\phi\\) estimated rather assumed 1.❌ , Quasi-Poisson strict GLM :GLMs require full probability distribution exponential family.\nStandard Poisson regression assumes Poisson distribution (belongs exponential family).\nQuasi-Poisson assume full probability distribution, mean-variance relationship.\nGLMs require full probability distribution exponential family.Standard Poisson regression assumes Poisson distribution (belongs exponential family).Standard Poisson regression assumes Poisson distribution (belongs exponential family).Quasi-Poisson assume full probability distribution, mean-variance relationship.Quasi-Poisson assume full probability distribution, mean-variance relationship.use Maximum Likelihood Estimation.\nStandard GLMs use MLE estimate parameters.\nQuasi-Poisson uses quasi-likelihood methods, require specifying mean variance, full likelihood function.\nuse Maximum Likelihood Estimation.Standard GLMs use MLE estimate parameters.Standard GLMs use MLE estimate parameters.Quasi-Poisson uses quasi-likelihood methods, require specifying mean variance, full likelihood function.Quasi-Poisson uses quasi-likelihood methods, require specifying mean variance, full likelihood function.Likelihood-based inference valid.\nAIC, BIC, Likelihood Ratio Tests used Quasi-Poisson regression.\nLikelihood-based inference valid.AIC, BIC, Likelihood Ratio Tests used Quasi-Poisson regression.Use Quasi-Poisson:data exhibit overdispersion (variance > mean), making standard Poisson regression inappropriate.data exhibit overdispersion (variance > mean), making standard Poisson regression inappropriate.Negative Binomial Regression preferred, alternative needed handle overdispersion.Negative Binomial Regression preferred, alternative needed handle overdispersion.overdispersion present, Negative Binomial Regression often better alternative true GLM full likelihood function, whereas Quasi-Poisson quasi-likelihood approach.overdispersion present, Negative Binomial Regression often better alternative true GLM full likelihood function, whereas Quasi-Poisson quasi-likelihood approach.","code":""},{"path":"generalized-linear-models.html","id":"application-quasi-poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.6.2 Application: Quasi-Poisson Regression","text":"analyze bioChemists dataset, modeling number published articles (Num_Article) function various predictors.","code":""},{"path":"generalized-linear-models.html","id":"checking-overdispersion-in-the-poisson-model","chapter":"7 Generalized Linear Models","heading":"7.6.2.1 Checking Overdispersion in the Poisson Model","text":"first fit Poisson regression model check overdispersion using deviance--degrees--freedom ratio:\\(\\hat{\\phi} > 1\\), Poisson model underestimates variance.\\(\\hat{\\phi} > 1\\), Poisson model underestimates variance.large value (>> 1) suggests Poisson regression appropriate.large value (>> 1) suggests Poisson regression appropriate.","code":"\n# Fit Poisson regression model\nPoisson_Mod <-\n    glm(Num_Article ~ ., family = poisson, data = bioChemists)\n\n# Compute dispersion parameter\ndispersion_estimate <-\n    Poisson_Mod$deviance / Poisson_Mod$df.residual\ndispersion_estimate\n#> [1] 1.797988"},{"path":"generalized-linear-models.html","id":"fitting-the-quasi-poisson-model","chapter":"7 Generalized Linear Models","heading":"7.6.2.2 Fitting the Quasi-Poisson Model","text":"Since overdispersion present, refit model using Quasi-Poisson regression, scales standard errors \\(\\phi\\).Interpretation:coefficients remain Poisson regression.coefficients remain Poisson regression.Standard errors inflated account overdispersion.Standard errors inflated account overdispersion.P-values increase, leading conservative inference.P-values increase, leading conservative inference.","code":"\n# Fit Quasi-Poisson regression model\nquasiPoisson_Mod <-\n    glm(Num_Article ~ ., family = quasipoisson, data = bioChemists)\n\n# Summary of the model\nsummary(quasiPoisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      0.304617   0.139273   2.187 0.028983 *  \n#> SexWomen        -0.224594   0.073860  -3.041 0.002427 ** \n#> MarriedMarried   0.155243   0.083003   1.870 0.061759 .  \n#> Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***\n#> PhD_Quality      0.012823   0.035700   0.359 0.719544    \n#> Num_MentArticle  0.025543   0.002713   9.415  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for quasipoisson family taken to be 1.829006)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: NA\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"generalized-linear-models.html","id":"comparing-poisson-and-quasi-poisson","chapter":"7 Generalized Linear Models","heading":"7.6.2.3 Comparing Poisson and Quasi-Poisson","text":"see effect using Quasi-Poisson, compare standard errors:Quasi-Poisson larger standard errors Poisson.Quasi-Poisson larger standard errors Poisson.leads wider confidence intervals, reducing likelihood false positives.leads wider confidence intervals, reducing likelihood false positives.","code":"\n# Extract coefficients and standard errors\npoisson_se <- summary(Poisson_Mod)$coefficients[, 2]\nquasi_se <- summary(quasiPoisson_Mod)$coefficients[, 2]\n\n# Compare standard errors\nse_comparison <- data.frame(Poisson = poisson_se,\n                            Quasi_Poisson = quasi_se)\nse_comparison\n#>                     Poisson Quasi_Poisson\n#> (Intercept)     0.102981443   0.139272885\n#> SexWomen        0.054613488   0.073859696\n#> MarriedMarried  0.061374395   0.083003199\n#> Num_Kid5        0.040126898   0.054267922\n#> PhD_Quality     0.026397045   0.035699564\n#> Num_MentArticle 0.002006073   0.002713028"},{"path":"generalized-linear-models.html","id":"model-diagnostics-checking-residuals","chapter":"7 Generalized Linear Models","heading":"7.6.2.4 Model Diagnostics: Checking Residuals","text":"examine residuals assess model fit:residuals show pattern, additional predictors transformations may needed.residuals show pattern, additional predictors transformations may needed.Random scatter around zero suggests well-fitting model.Random scatter around zero suggests well-fitting model.","code":"\n# Residual plot\nplot(\n    quasiPoisson_Mod$fitted.values,\n    residuals(quasiPoisson_Mod, type = \"pearson\"),\n    xlab = \"Fitted Values\",\n    ylab = \"Pearson Residuals\",\n    main = \"Residuals vs. Fitted Values (Quasi-Poisson)\"\n)\nabline(h = 0, col = \"red\")"},{"path":"generalized-linear-models.html","id":"alternative-negative-binomial-vs.-quasi-poisson","chapter":"7 Generalized Linear Models","heading":"7.6.2.5 Alternative: Negative Binomial vs. Quasi-Poisson","text":"overdispersion severe, Negative Binomial regression may preferable explicitly models dispersion:","code":"\n# Fit Negative Binomial model\nlibrary(MASS)\nNegBinom_Mod <- glm.nb(Num_Article ~ ., data = bioChemists)\n\n# Model summaries\nsummary(quasiPoisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      0.304617   0.139273   2.187 0.028983 *  \n#> SexWomen        -0.224594   0.073860  -3.041 0.002427 ** \n#> MarriedMarried   0.155243   0.083003   1.870 0.061759 .  \n#> Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***\n#> PhD_Quality      0.012823   0.035700   0.359 0.719544    \n#> Num_MentArticle  0.025543   0.002713   9.415  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for quasipoisson family taken to be 1.829006)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: NA\n#> \n#> Number of Fisher Scoring iterations: 5\nsummary(NegBinom_Mod)\n#> \n#> Call:\n#> glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n#>     link = log)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n#> SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n#> MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n#> Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n#> PhD_Quality      0.015271   0.035873   0.426 0.670326    \n#> Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n#> \n#>     Null deviance: 1109.0  on 914  degrees of freedom\n#> Residual deviance: 1004.3  on 909  degrees of freedom\n#> AIC: 3135.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.264 \n#>           Std. Err.:  0.271 \n#> \n#>  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"key-differences-quasi-poisson-vs.-negative-binomial","chapter":"7 Generalized Linear Models","heading":"7.6.2.6 Key Differences: Quasi-Poisson vs. Negative Binomial","text":"Choose:Use Quasi-Poisson need robust standard errors require model selection via AIC/BIC.Use Quasi-Poisson need robust standard errors require model selection via AIC/BIC.Use Negative Binomial overdispersion large want true likelihood-based model.Use Negative Binomial overdispersion large want true likelihood-based model.Quasi-Poisson quick fix, Negative Binomial generally better choice modeling count data overdispersion.","code":""},{"path":"generalized-linear-models.html","id":"sec-multinomial-logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.7 Multinomial Logistic Regression","text":"dealing categorical response variables two possible outcomes, multinomial logistic regression natural extension binary logistic model.","code":""},{"path":"generalized-linear-models.html","id":"the-multinomial-distribution","chapter":"7 Generalized Linear Models","heading":"7.7.1 The Multinomial Distribution","text":"Suppose categorical response variable \\(Y_i\\) can take values \\(\\{1, 2, \\dots, J\\}\\). observation \\(\\), probability falls category \\(j\\) given :\\[\np_{ij} = P(Y_i = j), \\quad \\text{} \\quad \\sum_{j=1}^{J} p_{ij} = 1.\n\\]response follows multinomial distribution:\\[\nY_i \\sim \\text{Multinomial}(1; p_{i1}, p_{i2}, ..., p_{iJ}).\n\\]means observation belongs exactly one \\(J\\) categories.","code":""},{"path":"generalized-linear-models.html","id":"modeling-probabilities-using-log-odds","chapter":"7 Generalized Linear Models","heading":"7.7.2 Modeling Probabilities Using Log-Odds","text":"model probabilities \\(p_{ij}\\) directly must sum 1. Instead, use logit transformation, comparing category \\(j\\) baseline category (typically first category, \\(j=1\\)):\\[\n\\eta_{ij} = \\log \\frac{p_{ij}}{p_{i1}}, \\quad j = 2, \\dots, J.\n\\]Using linear function covariates \\(\\mathbf{x}_i\\), define:\\[\n\\eta_{ij} = \\mathbf{x}_i' \\beta_j = \\beta_{j0} + \\sum_{p=1}^{P} \\beta_{jp} x_{ip}.\n\\]Rearranging express probabilities explicitly:\\[\np_{ij} = p_{i1} \\exp(\\mathbf{x}_i' \\beta_j).\n\\]Since probabilities must sum 1:\\[\np_{i1} + \\sum_{j=2}^{J} p_{ij} = 1.\n\\]Substituting \\(p_{ij}\\):\\[\np_{i1} + \\sum_{j=2}^{J} p_{i1} \\exp(\\mathbf{x}_i' \\beta_j) = 1.\n\\]Solving \\(p_{i1}\\):\\[\np_{i1} = \\frac{1}{1 + \\sum_{j=2}^{J} \\exp(\\mathbf{x}_i' \\beta_j)}.\n\\]Thus, probability category \\(j\\) :\\[\np_{ij} = \\frac{\\exp(\\mathbf{x}_i' \\beta_j)}{1 + \\sum_{l=2}^{J} \\exp(\\mathbf{x}_i' \\beta_l)}, \\quad j = 2, \\dots, J.\n\\]formulation known multinomial logit model.","code":""},{"path":"generalized-linear-models.html","id":"softmax-representation","chapter":"7 Generalized Linear Models","heading":"7.7.3 Softmax Representation","text":"alternative formulation avoids choosing baseline category instead treats \\(J\\) categories symmetrically using softmax function:\\[\nP(Y_i = j | X_i = x) = \\frac{\\exp(\\beta_{j0} + \\sum_{p=1}^{P} \\beta_{jp} x_p)}{\\sum_{l=1}^{J} \\exp(\\beta_{l0} + \\sum_{p=1}^{P} \\beta_{lp} x_p)}.\n\\]representation often used neural networks general machine learning models.","code":""},{"path":"generalized-linear-models.html","id":"log-odds-ratio-between-two-categories","chapter":"7 Generalized Linear Models","heading":"7.7.4 Log-Odds Ratio Between Two Categories","text":"log-odds ratio two categories \\(k\\) \\(k'\\) :\\[\n\\log \\frac{P(Y = k | X = x)}{P(Y = k' | X = x)}\n= (\\beta_{k0} - \\beta_{k'0}) + \\sum_{p=1}^{P} (\\beta_{kp} - \\beta_{k'p}) x_p.\n\\]equation tells us :\\(\\beta_{kp} > \\beta_{k'p}\\), increasing \\(x_p\\) increases odds choosing category \\(k\\) \\(k'\\).\\(\\beta_{kp} < \\beta_{k'p}\\), increasing \\(x_p\\) decreases odds choosing \\(k\\) \\(k'\\).","code":""},{"path":"generalized-linear-models.html","id":"estimation","chapter":"7 Generalized Linear Models","heading":"7.7.5 Estimation","text":"estimate parameters \\(\\beta_j\\), use Maximum Likelihood estimation.Given \\(n\\) independent observations \\((Y_i, X_i)\\), likelihood function :\\[\nL(\\beta) = \\prod_{=1}^{n} \\prod_{j=1}^{J} p_{ij}^{Y_{ij}}.\n\\]Taking log-likelihood:\\[\n\\log L(\\beta) = \\sum_{=1}^{n} \\sum_{j=1}^{J} Y_{ij} \\log p_{ij}.\n\\]Since closed-form solution, numerical methods (see Non-linear Least Squares Estimation) used estimation.","code":""},{"path":"generalized-linear-models.html","id":"interpretation-of-coefficients","chapter":"7 Generalized Linear Models","heading":"7.7.6 Interpretation of Coefficients","text":"\\(\\beta_{jp}\\) represents effect \\(x_p\\) log-odds category \\(j\\) relative baseline.Positive coefficients mean increasing \\(x_p\\) makes category \\(j\\) likely relative baseline.Negative coefficients mean increasing \\(x_p\\) makes category \\(j\\) less likely relative baseline.","code":""},{"path":"generalized-linear-models.html","id":"application-multinomial-logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.7.7 Application: Multinomial Logistic Regression","text":"1. Load Necessary Libraries DataThe dataset nes96 contains survey responses, including political party identification (PID), age (age), education level (educ).2. Define Political Strength CategoriesWe classify political strength three categories:Strong: Strong Democrat Strong RepublicanStrong: Strong Democrat Strong RepublicanWeak: Weak Democrat Weak RepublicanWeak: Weak Democrat Weak RepublicanNeutral: Independents affiliationsNeutral: Independents affiliations3. Visualizing Political Strength AgeWe visualize proportion political strength category across age groups.4. Fit Multinomial Logistic ModelWe model political strength function age education.5. Stepwise Model Selection Based AICWe perform stepwise selection find best model.Compare best model full model based deviance:non-significant p-value suggests major difference full stepwise models.6. Predictions & VisualizationPredicting Political Strength Probabilities AgePredict Specific Ages","code":"\nlibrary(faraway)  # For the dataset\nlibrary(dplyr)    # For data manipulation\nlibrary(ggplot2)  # For visualization\nlibrary(nnet)     # For multinomial logistic regression\n\n# Load and inspect data\ndata(nes96, package=\"faraway\")\nhead(nes96, 3)\n#>   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote\n#> 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole\n#> 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton\n#> 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton\n# Check distribution of political identity\ntable(nes96$PID)\n#> \n#>  strDem weakDem  indDem  indind  indRep weakRep  strRep \n#>     200     180     108      37      94     150     175\n\n# Define Political Strength variable\nnes96 <- nes96 %>%\n  mutate(Political_Strength = case_when(\n    PID %in% c(\"strDem\", \"strRep\") ~ \"Strong\",\n    PID %in% c(\"weakDem\", \"weakRep\") ~ \"Weak\",\n    PID %in% c(\"indDem\", \"indind\", \"indRep\") ~ \"Neutral\",\n    TRUE ~ NA_character_\n  ))\n\n# Summarize\nnes96 %>% group_by(Political_Strength) %>% summarise(Count = n())\n#> # A tibble: 3 × 2\n#>   Political_Strength Count\n#>   <chr>              <int>\n#> 1 Neutral              239\n#> 2 Strong               375\n#> 3 Weak                 330\n# Prepare data for visualization\nPlot_DF <- nes96 %>%\n    mutate(Age_Grp = cut_number(age, 4)) %>%\n    group_by(Age_Grp, Political_Strength) %>%\n    summarise(count = n(), .groups = 'drop') %>%\n    group_by(Age_Grp) %>%\n    mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot age vs political strength\nAge_Plot <- ggplot(\n    Plot_DF,\n    aes(\n        x        = Age_Grp,\n        y        = proportion,\n        group    = Political_Strength,\n        linetype = Political_Strength,\n        color    = Political_Strength\n    )\n) +\n    geom_line(size = 2) +\n    labs(title = \"Political Strength by Age Group\",\n         x = \"Age Group\",\n         y = \"Proportion\")\n\n# Display plot\nAge_Plot\n# Fit multinomial logistic regression\nMultinomial_Model <-\n    multinom(Political_Strength ~ age + educ,\n             data = nes96,\n             trace = FALSE)\nsummary(Multinomial_Model)\n#> Call:\n#> multinom(formula = Political_Strength ~ age + educ, data = nes96, \n#>     trace = FALSE)\n#> \n#> Coefficients:\n#>        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4\n#> Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307\n#> Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634\n#>            educ^5     educ^6\n#> Strong -0.1664377 -0.1359449\n#> Weak   -0.1489030 -0.2173144\n#> \n#> Std. Errors:\n#>        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4\n#> Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776\n#> Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149\n#>           educ^5    educ^6\n#> Strong 0.2515012 0.2166774\n#> Weak   0.2643747 0.2199186\n#> \n#> Residual Deviance: 2024.596 \n#> AIC: 2056.596\nMultinomial_Step <- step(Multinomial_Model, trace = 0)\n#> trying - age \n#> trying - educ \n#> trying - age\nMultinomial_Step\n#> Call:\n#> multinom(formula = Political_Strength ~ age, data = nes96, trace = FALSE)\n#> \n#> Coefficients:\n#>        (Intercept)          age\n#> Strong -0.01988977  0.009832916\n#> Weak    0.59497046 -0.005954348\n#> \n#> Residual Deviance: 2030.756 \n#> AIC: 2038.756\npchisq(\n    q = deviance(Multinomial_Step) - deviance(Multinomial_Model),\n    df = Multinomial_Model$edf - Multinomial_Step$edf,\n    lower.tail = FALSE\n)\n#> [1] 0.9078172\n# Create data for prediction\nPlotData <- data.frame(age = seq(from = 19, to = 91))\n\n# Get predicted probabilities\nPreds <- PlotData %>%\n    bind_cols(data.frame(predict(Multinomial_Step, \n                                 PlotData, \n                                 type = \"probs\")))\n\n# Plot predicted probabilities across age\nplot(\n    x = Preds$age,\n    y = Preds$Neutral,\n    type = \"l\",\n    ylim = c(0.2, 0.6),\n    col = \"black\",\n    ylab = \"Proportion\",\n    xlab = \"Age\"\n)\n\nlines(x = Preds$age,\n      y = Preds$Weak,\n      col = \"blue\")\nlines(x = Preds$age,\n      y = Preds$Strong,\n      col = \"red\")\n\nlegend(\n    \"topleft\",\n    legend = c(\"Neutral\", \"Weak\", \"Strong\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = 1\n)\n# Predict class for a 34-year-old\npredict(Multinomial_Step, data.frame(age = 34))\n#> [1] Weak\n#> Levels: Neutral Strong Weak\n\n# Predict probabilities for 34 and 35-year-olds\npredict(Multinomial_Step, data.frame(age = c(34, 35)), type = \"probs\")\n#>     Neutral    Strong      Weak\n#> 1 0.2597275 0.3556910 0.3845815\n#> 2 0.2594080 0.3587639 0.3818281"},{"path":"generalized-linear-models.html","id":"application-gamma-regression","chapter":"7 Generalized Linear Models","heading":"7.7.8 Application: Gamma Regression","text":"response variables strictly positive, use Gamma regression.1. Load Prepare Data2. Visualization Inverse Yield3. Fit Gamma Regression ModelGamma regression models yield function seeding rate using inverse link: \\[\n\\eta_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij} + \\beta_2 x_{ij}^2, \\quad Y_{ij} = \\eta_{ij}^{-1}\n\\]4. Predictions Visualization","code":"\nlibrary(agridat)  # Agricultural dataset\n\n# Load and filter data\ndat <- agridat::streibig.competition\ngammaDat <- subset(dat, sseeds < 1)  # Keep only barley\ngammaDat <-\n    transform(gammaDat,\n              x = bseeds,\n              y = bdwt,\n              block = factor(block))\nggplot(gammaDat, aes(x = x, y = 1 / y)) +\n    geom_point(aes(color = block, shape = block)) +\n    labs(title = \"Inverse Yield vs Seeding Rate\",\n         x = \"Seeding Rate\",\n         y = \"Inverse Yield\")\nm1 <- glm(y ~ block + block * x + block * I(x^2),\n          data = gammaDat, family = Gamma(link = \"inverse\"))\n\nsummary(m1)\n#> \n#> Call:\n#> glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = \"inverse\"), \n#>     data = gammaDat)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***\n#> blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    \n#> blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    \n#> x              -2.075e-03  1.099e-03  -1.888 0.072884 .  \n#> I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    \n#> blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    \n#> blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    \n#> blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    \n#> blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Gamma family taken to be 0.3232083)\n#> \n#>     Null deviance: 13.1677  on 29  degrees of freedom\n#> Residual deviance:  7.8605  on 21  degrees of freedom\n#> AIC: 225.32\n#> \n#> Number of Fisher Scoring iterations: 5\n# Generate new data for prediction\nnewdf <-\n    expand.grid(x = seq(0, 120, length = 50), \n                block = factor(c(\"B1\", \"B2\", \"B3\")))\n\n# Predict responses\nnewdf$pred <- predict(m1, newdata = newdf, type = \"response\")\n\n# Plot predictions\nggplot(gammaDat, aes(x = x, y = y)) +\n    geom_point(aes(color = block, shape = block)) +\n    geom_line(data = newdf, aes(\n        x = x,\n        y = pred,\n        color = block,\n        linetype = block\n    )) +\n    labs(title = \"Predicted Yield by Seeding Rate\",\n         x = \"Seeding Rate\",\n         y = \"Yield\")"},{"path":"generalized-linear-models.html","id":"sec-generalization-of-generalized-linear-models","chapter":"7 Generalized Linear Models","heading":"7.8 Generalization of Generalized Linear Models","text":"seen Poisson regression bears similarities logistic regression. insight leads us broader class models known Generalized Linear Models, introduced Nelder Wedderburn (1972). models provide unified framework handling different types response variables maintaining fundamental principles linear modeling.","code":""},{"path":"generalized-linear-models.html","id":"exponential-family","chapter":"7 Generalized Linear Models","heading":"7.8.1 Exponential Family","text":"foundation GLMs built exponential family distributions, provides flexible class probability distributions share common form:\\[\nf(y;\\theta, \\phi) = \\exp\\left(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y, \\phi)\\right)\n\\]:\\(\\theta\\) natural parameter (canonical parameter),\\(\\phi\\) dispersion parameter,\\((\\phi)\\), \\(b(\\theta)\\), \\(c(y, \\phi)\\) functions ensuring proper distributional form.Distributions Exponential Family Can Used GLMs:Normal DistributionNormal DistributionBinomial DistributionBinomial DistributionPoisson DistributionPoisson DistributionGamma DistributionGamma DistributionInverse Gaussian DistributionInverse Gaussian DistributionNegative Binomial Distribution (used GLMs requires overdispersion adjustments)Negative Binomial Distribution (used GLMs requires overdispersion adjustments)Multinomial Distribution (categorical response)Multinomial Distribution (categorical response)Exponential Family Distributions Commonly Used GLMs:Beta DistributionBeta DistributionDirichlet DistributionDirichlet DistributionWishart DistributionWishart DistributionGeometric DistributionGeometric DistributionExponential Distribution (can used indirectly survival models)Exponential Distribution (can used indirectly survival models)Example: Normal DistributionConsider normally distributed response variable \\(Y \\sim N(\\mu, \\sigma^2)\\). probability density function (PDF) :\\[\n\\begin{aligned}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) \\\\\n&= \\exp\\left(-\\frac{y^2 - 2y\\mu + \\mu^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2)\\right)\n\\end{aligned}\n\\]Rewriting exponential family form:\\[\n\\begin{aligned}\nf(y; \\mu, \\sigma^2) &= \\exp\\left(\\frac{y \\mu - \\frac{\\mu^2}{2}}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2)\\right) \\\\\n&= \\exp\\left(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y, \\phi)\\right)\n\\end{aligned}\n\\]:Natural parameter: \\(\\theta = \\mu\\)Function \\(b(\\theta)\\): \\(b(\\theta) = \\frac{\\mu^2}{2}\\)Dispersion function: \\((\\phi) = \\sigma^2 = \\phi\\)Function \\(c(y, \\phi)\\): \\(c(y, \\phi) = -\\frac{1}{2} \\left(\\frac{y^2}{\\phi} + \\log(2\\pi \\sigma^2)\\right)\\)","code":""},{"path":"generalized-linear-models.html","id":"properties-of-glm-exponential-families","chapter":"7 Generalized Linear Models","heading":"7.8.2 Properties of GLM Exponential Families","text":"Expected Value (Mean) \\[\nE(Y) = b'(\\theta)\n\\] \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\). (Note: ' “prime,” transpose).Expected Value (Mean) \\[\nE(Y) = b'(\\theta)\n\\] \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\). (Note: ' “prime,” transpose).Variance \\[\n\\text{Var}(Y) = (\\phi) b''(\\theta) = (\\phi) V(\\mu)\n\\] :\n\\(V(\\mu) = b''(\\theta)\\) variance function, though represents variance \\((\\phi) = 1\\).\nVariance \\[\n\\text{Var}(Y) = (\\phi) b''(\\theta) = (\\phi) V(\\mu)\n\\] :\\(V(\\mu) = b''(\\theta)\\) variance function, though represents variance \\((\\phi) = 1\\).\\((\\phi)\\), \\(b(\\theta)\\), \\(c(y, \\phi)\\) identifiable, can derive expected value variance \\(Y\\).\\((\\phi)\\), \\(b(\\theta)\\), \\(c(y, \\phi)\\) identifiable, can derive expected value variance \\(Y\\).Examples Exponential Family Distributions1. Normal DistributionFor normal distribution \\(Y \\sim N(\\mu, \\sigma^2)\\), exponential family representation :\\[\nf(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(y - \\mu)^2}{2\\sigma^2} \\right)\n\\]can rewritten exponential form:\\[\n\\exp \\left( \\frac{y\\mu - \\frac{1}{2} \\mu^2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2) \\right)\n\\], identify:\\(\\theta = \\mu\\)\\(b(\\theta) = \\frac{\\theta^2}{2}\\)\\((\\phi) = \\sigma^2\\)Computing derivatives:\\[\nb'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta} = \\mu, \\quad V(\\mu) = b''(\\theta) = 1\n\\]Thus,\\[\nE(Y) = \\mu, \\quad \\text{Var}(Y) = (\\phi) V(\\mu) = \\sigma^2\n\\]2. Poisson DistributionFor Poisson-distributed response \\(Y \\sim \\text{Poisson}(\\mu)\\), probability mass function :\\[\nf(y; \\mu) = \\frac{\\mu^y e^{-\\mu}}{y!}\n\\]Rewriting exponential form:\\[\n\\exp(y \\log \\mu - \\mu - \\log y!)\n\\]Thus, identify:\\(\\theta = \\log \\mu\\)\\(b(\\theta) = e^\\theta\\)\\((\\phi) = 1\\)\\(c(y, \\phi) = \\log y!\\)Computing derivatives:\\[\nE(Y) = b'(\\theta) = e^\\theta = \\mu, \\quad \\text{Var}(Y) = b''(\\theta) = \\mu\n\\]Since \\(\\mu = E(Y)\\), confirm variance function:\\[\n\\text{Var}(Y) = V(\\mu) = \\mu\n\\]","code":""},{"path":"generalized-linear-models.html","id":"structure-of-a-generalized-linear-model","chapter":"7 Generalized Linear Models","heading":"7.8.3 Structure of a Generalized Linear Model","text":"GLM, model mean \\(\\mu\\) link function connects linear predictor:\\[\ng(\\mu) = g(b'(\\theta)) = \\mathbf{x' \\beta}\n\\]Equivalently,\\[\n\\mu = g^{-1}(\\mathbf{x' \\beta})\n\\]:\\(g(\\cdot)\\) link function, ensures transformation expected response \\(E(Y) = \\mu\\) linear predictor.\\(\\eta = \\mathbf{x' \\beta}\\) called linear predictor.","code":""},{"path":"generalized-linear-models.html","id":"components-of-a-glm","chapter":"7 Generalized Linear Models","heading":"7.8.4 Components of a GLM","text":"GLM consists two main components:","code":""},{"path":"generalized-linear-models.html","id":"random-component","chapter":"7 Generalized Linear Models","heading":"7.8.4.1 Random Component","text":"describes distribution response variable \\(Y_1, \\dots, Y_n\\). response variables assumed follow distribution exponential family, can written :\\[\nf(y_i ; \\theta_i, \\phi) = \\exp \\left( \\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) \\right)\n\\]:\\(Y_i\\) independent random variables.\\(Y_i\\) independent random variables.canonical parameter \\(\\theta_i\\) may differ observation.canonical parameter \\(\\theta_i\\) may differ observation.dispersion parameter \\(\\phi\\) assumed constant across \\(\\).dispersion parameter \\(\\phi\\) assumed constant across \\(\\).mean response given :\n\\[\n\\mu_i = E(Y_i)\n\\]mean response given :\\[\n\\mu_i = E(Y_i)\n\\]","code":""},{"path":"generalized-linear-models.html","id":"systematic-component","chapter":"7 Generalized Linear Models","heading":"7.8.4.2 Systematic Component","text":"specifies mean response \\(\\mu\\) related explanatory variables \\(\\mathbf{x}\\) linear predictor \\(\\eta\\):systematic component consists :\nlink function \\(g(\\mu)\\).\nlinear predictor \\(\\eta = \\mathbf{x' \\beta}\\).\nsystematic component consists :link function \\(g(\\mu)\\).linear predictor \\(\\eta = \\mathbf{x' \\beta}\\).Notation:\nassume:\n\\[\ng(\\mu_i) = \\mathbf{x' \\beta} = \\eta_i\n\\]\nparameter vector \\(\\mathbf{\\beta} = (\\beta_1, \\dots, \\beta_p)'\\) needs estimated.\nNotation:assume:\n\\[\ng(\\mu_i) = \\mathbf{x' \\beta} = \\eta_i\n\\]assume:\\[\ng(\\mu_i) = \\mathbf{x' \\beta} = \\eta_i\n\\]parameter vector \\(\\mathbf{\\beta} = (\\beta_1, \\dots, \\beta_p)'\\) needs estimated.parameter vector \\(\\mathbf{\\beta} = (\\beta_1, \\dots, \\beta_p)'\\) needs estimated.","code":""},{"path":"generalized-linear-models.html","id":"canonical-link","chapter":"7 Generalized Linear Models","heading":"7.8.5 Canonical Link","text":"GLM, link function \\(g(\\cdot)\\) relates mean \\(\\mu_i\\) response \\(Y_i\\) linear predictor \\(\\eta_i\\) via\\[\n\\eta_i = g(\\mu_i).\n\\]canonical link special case \\(g(\\cdot)\\) \\[\ng(\\mu_i) = \\eta_i = \\theta_i,\n\\]\\(\\theta_i\\) natural parameter exponential family. words, link function directly equates linear predictor \\(\\eta_i\\) distribution’s natural parameter \\(\\theta_i\\). Hence, \\(g(\\mu)\\) canonical \\(g(\\mu) = \\theta\\).Exponential Family Components\\(b(\\theta)\\): cumulant (moment generating) function, defines variance function.\\(g(\\mu)\\): link function, must \nMonotonically increasing\nContinuously differentiable\nInvertible\nMonotonically increasingContinuously differentiableInvertibleFor exponential-family distribution, function \\(b(\\theta)\\) called cumulant moment generating function, relates \\(\\theta\\) mean via derivative:\\[\n\\mu = b'(\\theta)\n\\quad\\Longleftrightarrow\\quad\n\\theta = b'^{-1}(\\mu).\n\\]defining link \\(g(\\mu) = \\theta\\), impose \\(\\eta_i = \\theta_i\\), \\(g(\\cdot)\\) termed canonical setting.link canonical, equivalent way express \\[\n\\gamma^{-1} \\circ g^{-1} = ,\n\\]indicating inverse link \\(g^{-1}(\\cdot)\\) directly maps linear predictor (now natural parameter \\(\\theta\\)) back \\(\\mu\\) way respects structure exponential family.Choosing \\(g(\\cdot)\\) canonical often simplifies mathematical derivations computations—especially parameter estimation—linear predictor \\(\\eta\\) natural parameter \\(\\theta\\) coincide. Common examples canonical links include:Identity link normal (Gaussian) distributionLog link Poisson distributionLogit link Bernoulli (binomial) distributionIn case, setting \\(\\eta = \\theta\\) streamlines relationship mean linear predictor, making model elegant practically convenient.","code":""},{"path":"generalized-linear-models.html","id":"inverse-link-functions","chapter":"7 Generalized Linear Models","heading":"7.8.6 Inverse Link Functions","text":"inverse link function \\(g^{-1}(\\eta)\\) (also called mean function) transforms linear predictor \\(\\eta\\) (can take real value) valid mean response \\(\\mu\\).Example 1: Normal Distribution (Identity Link)Random Component: \\(Y_i \\sim N(\\mu_i, \\sigma^2)\\).Random Component: \\(Y_i \\sim N(\\mu_i, \\sigma^2)\\).Mean Response: \\(\\mu_i = \\theta_i\\).Mean Response: \\(\\mu_i = \\theta_i\\).Canonical Link Function:\n\\[\ng(\\mu_i) = \\mu_i\n\\]\n(.e., identity function).Canonical Link Function:\\[\ng(\\mu_i) = \\mu_i\n\\](.e., identity function).Example 2: Binomial Distribution (Logit Link)Random Component: \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\).Random Component: \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\).Mean Response:\n\\[\n\\mu_i = \\frac{n_i e^{\\theta_i}}{1+e^{\\theta_i}}\n\\]Mean Response:\\[\n\\mu_i = \\frac{n_i e^{\\theta_i}}{1+e^{\\theta_i}}\n\\]Canonical Link Function:\n\\[\ng(\\mu_i) = \\log \\left( \\frac{\\mu_i}{n_i - \\mu_i} \\right)\n\\]\n(Logit link function).Canonical Link Function:\\[\ng(\\mu_i) = \\log \\left( \\frac{\\mu_i}{n_i - \\mu_i} \\right)\n\\](Logit link function).Example 3: Poisson Distribution (Log Link)Random Component: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\).Random Component: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\).Mean Response:\n\\[\n\\mu_i = e^{\\theta_i}\n\\]Mean Response:\\[\n\\mu_i = e^{\\theta_i}\n\\]Canonical Link Function:\n\\[\ng(\\mu_i) = \\log(\\mu_i)\n\\]\n(Log link function).Canonical Link Function:\\[\ng(\\mu_i) = \\log(\\mu_i)\n\\](Log link function).Example 4: Gamma Distribution (Inverse Link)Random Component: \\(Y_i \\sim \\text{Gamma}(\\alpha, \\mu_i)\\).Random Component: \\(Y_i \\sim \\text{Gamma}(\\alpha, \\mu_i)\\).Mean Response:\n\\[\n\\mu_i = -\\frac{1}{\\theta_i}\n\\]Mean Response:\\[\n\\mu_i = -\\frac{1}{\\theta_i}\n\\]Canonical Link Function:\n\\[\ng(\\mu_i) = -\\frac{1}{\\mu_i}\n\\]\n(Inverse link function).Canonical Link Function:\\[\ng(\\mu_i) = -\\frac{1}{\\mu_i}\n\\](Inverse link function).following table presents common GLM link functions corresponding inverse functions.\\(\\mu_i\\) expected value response.\\(\\mu_i\\) expected value response.\\(\\eta_i\\) linear predictor.\\(\\eta_i\\) linear predictor.\\(\\Phi(\\cdot)\\) represents CDF standard normal distribution.\\(\\Phi(\\cdot)\\) represents CDF standard normal distribution.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-parameters-in-glms","chapter":"7 Generalized Linear Models","heading":"7.8.7 Estimation of Parameters in GLMs","text":"GLM framework extends Linear Regression allowing response variables follow exponential family distributions.Maximum Likelihood Estimation used estimate parameters systematic component (\\(\\beta\\)), providing consistent efficient approach. derivation computation processes unified, thanks exponential form model, simplifies mathematical treatment implementation.Maximum Likelihood Estimation used estimate parameters systematic component (\\(\\beta\\)), providing consistent efficient approach. derivation computation processes unified, thanks exponential form model, simplifies mathematical treatment implementation.However, unification extend estimation dispersion parameter (\\(\\phi\\)), requires separate treatment, often involving alternative estimation methods moment-based approaches quasi-likelihood estimation.However, unification extend estimation dispersion parameter (\\(\\phi\\)), requires separate treatment, often involving alternative estimation methods moment-based approaches quasi-likelihood estimation.GLMs, response variable \\(Y_i\\) follows exponential family distribution characterized density function:\\[\nf(y_i ; \\theta_i, \\phi) = \\exp\\left(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) \\right)\n\\]:\\(\\theta_i\\) canonical parameter.\\(\\phi\\) dispersion parameter (may known estimated separately).\\(b(\\theta_i)\\) determines mean variance \\(Y_i\\).\\((\\phi)\\) scales variance.\\(c(y_i, \\phi)\\) ensures proper normalization.family, obtain:Mean \\(Y_i\\): \\[\nE(Y_i) = \\mu_i = b'(\\theta_i)\n\\]Variance \\(Y_i\\): \\[\n\\text{Var}(Y_i) = b''(\\theta_i) (\\phi) = V(\\mu_i)(\\phi)\n\\] \\(V(\\mu_i)\\) variance function.Systematic component (link function): \\[\ng(\\mu_i) = \\eta_i = \\mathbf{x}_i' \\beta\n\\]function \\(g(\\cdot)\\) link function, connects expected response \\(\\mu_i\\) linear predictor \\(\\mathbf{x}_i' \\beta\\).single observation \\(Y_i\\), log-likelihood function :\\[\nl_i(\\beta, \\phi) = \\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi)\n\\]\\(n\\) independent observations, total log-likelihood :\\[\nl(\\beta, \\phi) = \\sum_{=1}^n l_i(\\beta, \\phi)\n\\]Expanding ,\\[\nl(\\beta, \\phi) = \\sum_{=1}^n \\left( \\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) \\right).\n\\]estimate \\(\\beta\\), maximize log-likelihood function.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-systematic-component-beta","chapter":"7 Generalized Linear Models","heading":"7.8.7.1 Estimation of Systematic Component (\\(\\beta\\))","text":"differentiate \\(l(\\beta,\\phi)\\) respect \\(\\beta_j\\), apply chain rule:\\[\n\\frac{\\partial l_i(\\beta,\\phi)}{\\partial \\beta_j} =\n\\underbrace{\\frac{\\partial l_i(\\beta,\\phi)}{\\partial \\theta_i}}_{\\text{depends }(y_i - \\mu_i)}\n\\times\n\\underbrace{\\frac{\\partial \\theta_i}{\\partial \\mu_i}}_{= 1/V(\\mu_i)\\text{ canonical link}}\n\\times\n\\underbrace{\\frac{\\partial \\mu_i}{\\partial \\eta_i}}_{\\text{depends link}}\n\\times\n\\underbrace{\\frac{\\partial \\eta_i}{\\partial \\beta_j}}_{= x_{ij}}.\n\\]Let us see four pieces appear:\\(l_i(\\beta,\\phi)\\) depends \\(\\theta_i\\). start computing \\(\\frac{\\partial l_i}{\\partial \\theta_i}\\).\\(l_i(\\beta,\\phi)\\) depends \\(\\theta_i\\). start computing \\(\\frac{\\partial l_i}{\\partial \\theta_i}\\).\\(\\theta_i\\) (“natural parameter” exponential family) may turn function \\(\\mu_i\\).\ncanonical‐link GLMs, often \\(\\theta_i = \\eta_i\\).\ngeneral‐link GLMs,\\(\\theta_i\\) still function \\(\\mu_i\\).\nHence need \\(\\frac{\\partial \\theta_i}{\\partial \\mu_i}\\).\n\\(\\theta_i\\) (“natural parameter” exponential family) may turn function \\(\\mu_i\\).canonical‐link GLMs, often \\(\\theta_i = \\eta_i\\).canonical‐link GLMs, often \\(\\theta_i = \\eta_i\\).general‐link GLMs,\\(\\theta_i\\) still function \\(\\mu_i\\).\nHence need \\(\\frac{\\partial \\theta_i}{\\partial \\mu_i}\\).general‐link GLMs,\\(\\theta_i\\) still function \\(\\mu_i\\).\nHence need \\(\\frac{\\partial \\theta_i}{\\partial \\mu_i}\\).\\(\\mu_i\\) (mean) function linear predictor \\(\\eta_i\\). Typically, \\(\\eta_i = g(\\mu_i)\\) implies \\(\\mu_i = g^{-1}(\\eta_i)\\). need \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\).\\(\\mu_i\\) (mean) function linear predictor \\(\\eta_i\\). Typically, \\(\\eta_i = g(\\mu_i)\\) implies \\(\\mu_i = g^{-1}(\\eta_i)\\). need \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\).Finally, \\(\\eta_i = \\mathbf{x}_i^\\prime \\beta\\). derivative \\(\\frac{\\partial \\eta_i}{\\partial \\beta_j}\\) simply \\(x_{ij}\\), \\(j\\)‐th component covariate vector \\(\\mathbf{x}_i\\).Finally, \\(\\eta_i = \\mathbf{x}_i^\\prime \\beta\\). derivative \\(\\frac{\\partial \\eta_i}{\\partial \\beta_j}\\) simply \\(x_{ij}\\), \\(j\\)‐th component covariate vector \\(\\mathbf{x}_i\\).Let us look factor turn.First term:\\[\n\\displaystyle \\frac{\\partial l_i(\\beta,\\phi)}{\\partial \\theta_i}\n\\]Recall\\[\nl_i(\\theta_i,\\phi) = \\frac{\\theta_i \\,y_i - b(\\theta_i)}{(\\phi)} + c(y_i,\\phi).\n\\]Differentiate \\(\\theta_i y_i - b(\\theta_i)\\) respect \\(\\theta_i\\): \\[\n\\frac{\\partial}{\\partial \\theta_i} \\left[\\theta_i\\,y_i - b(\\theta_i)\\right] = y_i - b'(\\theta_i).\n\\] exponential‐family definitions,\\(b'(\\theta_i) = \\mu_i\\).\n\\(y_i - \\mu_i\\).Since everything divided \\((\\phi)\\), get \\[\n\\frac{\\partial l_i}{\\partial \\theta_i} = \\frac{1}{(\\phi)}\\,[\\,y_i - \\mu_i\\,].\n\\] Hence, \\[\n\\boxed{ \\frac{\\partial l_i(\\beta,\\phi)}{\\partial \\theta_i} = \\frac{y_i - \\mu_i}{(\\phi)}. }\n\\]Second term: \\[\n\\displaystyle \\frac{\\partial \\theta_i}{\\partial \\mu_i}\n\\] 1. exponential family canonical link, \\[\n    \\theta_i = \\eta_i = \\mathbf{x}_i^\\prime \\beta.\n    \\] \\(\\theta_i\\) actually \\(\\eta_i\\), \\(g(\\mu_i)\\). Recall also \\(\\mu_i = b'(\\theta_i)\\), \\(d\\mu_i/d\\theta_i = b''(\\theta_i)\\). \\(b''(\\theta_i) = V(\\mu_i)\\). Hence \\[\n    \\frac{d \\mu_i}{d \\theta_i} = V(\\mu_i) \\quad\\Longrightarrow\\quad \\frac{d \\theta_i}{d \\mu_i} = \\frac{1}{V(\\mu_i)}.\n    \\] identity well‐known property canonical links exponential family.general (non‐canonical) links, \\(\\theta_i\\) may smooth function \\(\\mu_i\\). key idea : \\(\\mu_i = b'(\\theta_i)\\) \\(\\eta_i \\neq \\theta_i\\), carefully derive \\(\\partial \\theta_i / \\partial \\mu_i\\). Often, canonical link assumed keep expressions simpler.assume canonical link throughout, \\[\n\\boxed{ \\frac{\\partial \\theta_i}{\\partial \\mu_i} = \\frac{1}{V(\\mu_i)}. }\n\\]Third term:\\[\n\\displaystyle \\frac{\\partial \\mu_i}{\\partial \\eta_i}\n\\]consider link function \\(g(\\cdot)\\), defined \\[\n\\eta_i = g(\\mu_i) \\quad\\Longrightarrow\\quad \\mu_i = g^{-1}(\\eta_i).\n\\]example,Bernoulli (logistic‐regression) model, \\(μg(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\). \\(\\mu = g^{-1}(\\eta) = \\frac{1}{1+e^{-\\eta}}\\). \\(\\frac{d\\mu}{d\\eta} = \\mu\\,(1-\\mu)\\).Poisson (log) link, \\(g(\\mu) = \\log(\\mu)\\). \\(\\mu = e^\\eta\\). \\(\\frac{d\\mu}{d\\eta} = e^\\eta = \\mu\\).identity link, \\(g(\\mu) = \\mu\\). \\(\\eta = \\mu\\) \\(\\frac{d\\mu}{d\\eta} = 1\\).general,\\[\n\\boxed{ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\left.\\frac{d}{d\\eta}\\left[g^{-1}(\\eta)\\right]\\right|_{\\eta=\\eta_i} = \\left(g^{-1}\\right)'(\\eta_i). }\n\\]link also canonical, typically \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i} = V(\\mu_i)\\). Indeed, consistent second term result.Fourth term:\\[\n\\displaystyle \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n\\]Finally, linear predictor \\[\n\\eta_i = \\mathbf{x}_i^\\prime \\beta = \\sum_{k=1}^p x_{ik}\\,\\beta_k.\n\\]Hence, derivative \\(\\eta_i\\) respect \\(\\beta_j\\) simply\\[\n\\boxed{ \\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}. }\n\\]Therefore, entire log‐likelihood \\(l(\\beta, \\phi) = \\sum_{=1}^n l_i(\\beta,\\phi)\\), sum \\(\\):\\[\n\\boxed{ \\frac{\\partial l(\\beta,\\phi)}{\\partial \\beta_j} = \\sum_{=1}^n \\left[ \\frac{y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij} \\right] }\n\\]simplify expressions, define weight:\\[\nw_i = \\left(\\left(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\right)^2 V(\\mu_i)\\right)^{-1}.\n\\]canonical links, often simplifies \\(w_i = V(\\mu_i)\\), :Bernoulli (logit link): \\(w_i = p_i(1-p_i)\\).Poisson (log link): \\(w_i = \\mu_i\\).Rewriting score function terms \\(w_i\\):\\[\n\\frac{\\partial l(\\beta,\\phi)}{\\partial \\beta_j} =\n\\sum_{=1}^n\n\\left[\n\\frac{y_i - \\mu_i}{(\\phi)}\n\\times\nw_i\n\\times\n\\frac{\\partial \\eta_i}{\\partial \\mu_i}\n\\times\nx_{ij}\n\\right].\n\\]use Newton-Raphson Algorithm estimating \\(\\beta\\), require expected second derivative:\\[\n- E\\left(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}\\right),\n\\]corresponds \\((j,k)\\)-th element Fisher information matrix \\(\\mathbf{}(\\beta)\\):\\[\n\\mathbf{}_{jk}(\\beta) = - E\\left(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}\\right) = \\sum_{=1}^n \\frac{w_i}{(\\phi)}x_{ij}x_{ik}.\n\\]Example: Bernoulli Model Logit LinkFor Bernoulli response logit link function (canonical link), :\\[\n\\begin{aligned}\nb(\\theta) &= \\log(1 + \\exp(\\theta)) = \\log(1 + \\exp(\\mathbf{x'}\\beta)), \\\\\n(\\phi) &= 1, \\\\\nc(y_i, \\phi) &= 0.\n\\end{aligned}\n\\]mean link function:\\[\n\\begin{aligned}\nE(Y) = b'(\\theta) &= \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\mu = p, \\\\\n\\eta = g(\\mu) &= \\log\\left(\\frac{\\mu}{1-\\mu}\\right) = \\theta = \\log\\left(\\frac{p}{1-p}\\right) = \\mathbf{x'}\\beta.\n\\end{aligned}\n\\]log-likelihood \\(Y_i\\) :\\[\nl_i (\\beta, \\phi) = \\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) = y_i \\mathbf{x'}_i \\beta - \\log(1+ \\exp(\\mathbf{x'}\\beta)).\n\\]also obtain:\\[\n\\begin{aligned}\nV(\\mu_i) &= \\mu_i(1-\\mu_i) = p_i (1-p_i), \\\\\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} &= p_i(1-p_i).\n\\end{aligned}\n\\]Thus, first derivative simplifies :\\[\n\\begin{aligned}\n\\frac{\\partial l(\\beta, \\phi)}{\\partial \\beta_j} &= \\sum_{=1}^n \\left[\\frac{y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)}\\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij} \\right] \\\\\n&= \\sum_{=1}^n (y_i - p_i) \\times \\frac{1}{p_i(1-p_i)} \\times p_i(1-p_i) \\times x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - p_i) x_{ij} \\\\\n&= \\sum_{=1}^n \\left(y_i - \\frac{\\exp(\\mathbf{x'}_i\\beta)}{1+ \\exp(\\mathbf{x'}_i\\beta)}\\right)x_{ij}.\n\\end{aligned}\n\\]weight term case :\\[\nw_i = \\left(\\left(\\frac{\\partial \\eta_i}{\\partial \\mu_i} \\right)^2 V(\\mu_i)\\right)^{-1} = p_i (1-p_i).\n\\]Thus, Fisher information matrix elements:\\[\n\\mathbf{}_{jk}(\\beta) = \\sum_{=1}^n \\frac{w_i}{(\\phi)} x_{ij}x_{ik} = \\sum_{=1}^n p_i (1-p_i)x_{ij}x_{ik}.\n\\]`Fisher-Scoring algorithm Maximum Likelihood estimate \\(\\mathbf{\\beta}\\) given :\\[\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m+1)}\n=\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m)} +\n\\mathbf{}^{-1}(\\mathbf{\\beta})\n\\left(\n\\begin{array}\n{c}\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_1} \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_2} \\\\\n\\vdots \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_p} \\\\\n\\end{array}\n\\right)\\Bigg|_{\\beta = \\beta^{(m)}}\n\\]similar Newton-Raphson Algorithm, except replace observed matrix second derivatives (Hessian) expected value.matrix representation, score function (gradient log-likelihood) :\\[\n\\begin{aligned}\n\\frac{\\partial l }{\\partial \\beta} &= \\frac{1}{(\\phi)}\\mathbf{X'W\\Delta(y - \\mu)} \\\\\n&= \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}(y - \\mu)}\n\\end{aligned}\n\\]expected Fisher information matrix :\\[\n\\mathbf{}(\\beta) = \\frac{1}{(\\phi)}\\mathbf{X'WX} = \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}F}\n\\]:\\(\\mathbf{X}\\) \\(n \\times p\\) matrix covariates.\\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\).\\(\\mathbf{\\Delta}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\).\\(\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\\) \\(n \\times p\\) matrix, \\(\\)-th row given \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = (\\frac{\\partial \\mu_i}{\\partial \\eta_i})\\mathbf{x}'_i\\).\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(V(\\mu_i)\\).Setting derivative log-likelihood equal zero gives MLE equations:\\[\n\\mathbf{F'V^{-1}y} = \\mathbf{F'V^{-1}\\mu}\n\\]Since components equation (except \\(\\mathbf{y}\\)) depend \\(\\beta\\), solve iteratively.Special CasesCanonical Link FunctionIf canonical link used, estimating equations simplify :\\[\n\\mathbf{X'y} = \\mathbf{X'\\mu}\n\\]Identity Link FunctionIf identity link used, estimating equation reduces :\\[\n\\mathbf{X'V^{-1}y} = \\mathbf{X'V^{-1}X\\hat{\\beta}}\n\\]leads Generalized Least Squares estimator:\\[\n\\hat{\\beta} = (\\mathbf{X'V^{-1}X})^{-1} \\mathbf{X'V^{-1}y}\n\\]Fisher-Scoring Algorithm General FormThe iterative update formula Fisher-scoring can rewritten :\\[\n\\beta^{(m+1)} = \\beta^{(m)} + \\mathbf{(\\hat{F}'\\hat{V}^{-1}\\hat{F})^{-1}\\hat{F}'\\hat{V}^{-1}(y- \\hat{\\mu})}\n\\]Since \\(\\hat{F}, \\hat{V}, \\hat{\\mu}\\) depend \\(\\beta\\), evaluate \\(\\beta^{(m)}\\).initial guess \\(\\beta^{(0)}\\), iterate convergence.Notes:\\((\\phi)\\) constant takes form \\(m_i \\phi\\) known \\(m_i\\), \\(\\phi\\) cancels equations, simplifying estimation.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-dispersion-parameter-phi","chapter":"7 Generalized Linear Models","heading":"7.8.7.2 Estimation of Dispersion Parameter (\\(\\phi\\))","text":"two common approaches estimating \\(\\phi\\):Maximum Likelihood EstimationThe derivative log-likelihood respect \\(\\phi\\) :\\[\n\\frac{\\partial l_i}{\\partial \\phi} = \\frac{(\\theta_i y_i - b(\\theta_i)'(\\phi))}{^2(\\phi)} + \\frac{\\partial c(y_i,\\phi)}{\\partial \\phi}\n\\]MLE \\(\\phi\\) satisfies equation:\\[\n\\frac{^2(\\phi)}{'(\\phi)}\\sum_{=1}^n \\frac{\\partial c(y_i, \\phi)}{\\partial \\phi} = \\sum_{=1}^n(\\theta_i y_i - b(\\theta_i))\n\\]Challenges:distributions normal case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) often complicated.distributions normal case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) often complicated.Even canonical link function constant \\((\\phi)\\), simple general expression expected Fisher information:Even canonical link function constant \\((\\phi)\\), simple general expression expected Fisher information:\\[\n  -E\\left(\\frac{\\partial^2 l}{\\partial \\phi^2} \\right)\n  \\]means unification GLMs provide estimating \\(\\beta\\) extend neatly \\(\\phi\\).Moment Estimation (Bias-Corrected \\(\\chi^2\\) Method)MLE conventional approach estimating \\(\\phi\\) Generalized Linear Models.MLE conventional approach estimating \\(\\phi\\) Generalized Linear Models.exponential family distribution, variance function :\n\\[\n\\text{Var}(Y) = V(\\mu)(\\phi)\n\\]\nimplies following moment-based estimator:\n\\[\n\\begin{aligned}\n(\\phi) &= \\frac{\\text{Var}(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\end{aligned}\n\\]\n\\(p\\) number parameters \\(\\beta\\).exponential family distribution, variance function :\\[\n\\text{Var}(Y) = V(\\mu)(\\phi)\n\\]implies following moment-based estimator:\\[\n\\begin{aligned}\n(\\phi) &= \\frac{\\text{Var}(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\end{aligned}\n\\]\\(p\\) number parameters \\(\\beta\\).GLM canonical link function \\(g(.)= (b'(.))^{-1}\\):\n\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]\nUsing , moment estimator \\((\\phi) = \\phi\\) becomes:\n\\[\n\\hat{\\phi} = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))}\n\\]GLM canonical link function \\(g(.)= (b'(.))^{-1}\\):\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]Using , moment estimator \\((\\phi) = \\phi\\) becomes:\\[\n\\hat{\\phi} = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"inference-1","chapter":"7 Generalized Linear Models","heading":"7.8.8 Inference","text":"estimated variance \\(\\hat{\\beta}\\) given :\\[\n\\hat{\\text{var}}(\\beta) = (\\phi)(\\mathbf{\\hat{F}'\\hat{V}^{-1}\\hat{F}})^{-1}\n\\]:\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix diagonal elements given \\(V(\\mu_i)\\).\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix diagonal elements given \\(V(\\mu_i)\\).\\(\\mathbf{F}\\) \\(n \\times p\\) matrix given :\n\\[\n\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\n\\]\\(\\mathbf{F}\\) \\(n \\times p\\) matrix given :\\[\n\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\n\\]Since \\(\\mathbf{V}\\) \\(\\mathbf{F}\\) depend mean \\(\\mu\\) (thus \\(\\beta\\)), estimates \\(\\mathbf{\\hat{V}}\\) \\(\\mathbf{\\hat{F}}\\) depend \\(\\hat{\\beta}\\).Since \\(\\mathbf{V}\\) \\(\\mathbf{F}\\) depend mean \\(\\mu\\) (thus \\(\\beta\\)), estimates \\(\\mathbf{\\hat{V}}\\) \\(\\mathbf{\\hat{F}}\\) depend \\(\\hat{\\beta}\\).test general hypothesis:\\[\nH_0: \\mathbf{L\\beta = d}\n\\]\\(\\mathbf{L}\\) \\(q \\times p\\) matrix, use Wald test:\\[\nW = \\mathbf{(L \\hat{\\beta}-d)'((\\phi)L(\\hat{F}'\\hat{V}^{-1}\\hat{F})L')^{-1}(L \\hat{\\beta}-d)}\n\\]\\(H_0\\), Wald statistic follows chi-square distribution:\\[\nW \\sim \\chi^2_q\n\\]\\(q\\) rank \\(\\mathbf{L}\\).Special Case: Testing Single CoefficientFor hypothesis form:\\[\nH_0: \\beta_j = 0\n\\]Wald test simplifies :\\[\nW = \\frac{\\hat{\\beta}_j^2}{\\hat{\\text{var}}(\\hat{\\beta}_j)} \\sim \\chi^2_1\n\\]asymptotically.Another common test likelihood ratio test, compares likelihoods full model reduced model:\\[\n\\Lambda = 2 \\big(l(\\hat{\\beta}_f) - l(\\hat{\\beta}_r)\\big) \\sim \\chi^2_q\n\\]:\\(l(\\hat{\\beta}_f)\\) log-likelihood full model.\\(l(\\hat{\\beta}_r)\\) log-likelihood reduced model.\\(q\\) number constraints used fitting reduced model.Wald test convenient, likelihood ratio test often preferred sample sizes small, tends better statistical properties.","code":""},{"path":"generalized-linear-models.html","id":"deviance","chapter":"7 Generalized Linear Models","heading":"7.8.9 Deviance","text":"Deviance plays crucial role :Goodness--fit assessment: Checking well model explains observed data.Statistical inference: Used hypothesis testing, particularly likelihood ratio tests.Estimating dispersion parameters: Helps refining variance estimates.Model comparison: Facilitates selection competing models.Assuming dispersion parameter \\(\\phi\\) known, let:\\(\\tilde{\\theta}\\) maximum likelihood estimate (MLE) full model.\\(\\tilde{\\theta}\\) maximum likelihood estimate (MLE) full model.\\(\\hat{\\theta}\\) MLE reduced model.\\(\\hat{\\theta}\\) MLE reduced model.likelihood ratio statistic (twice difference log-likelihoods) :\\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]exponential family distributions, mean parameter :\\[\n\\mu = E(y) = b'(\\theta)\n\\]Thus, natural parameter function \\(\\mu\\):\\[\n\\theta = \\theta(\\mu) = b'^{-1}(\\mu)\n\\]Rewriting likelihood ratio statistic terms \\(\\mu\\):\\[\n2 \\sum_{=1}^n \\frac{y_i\\{\\theta(\\tilde{\\mu}_i) - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]saturated model provides fullest possible fit, observation perfectly predicted:\\[\n\\tilde{\\mu}_i = y_i, \\quad = 1, \\dots, n\n\\]Setting \\(\\tilde{\\theta}_i^* = \\theta(y_i)\\) \\(\\hat{\\theta}_i^* = \\theta (\\hat{\\mu}_i)\\), likelihood ratio simplifies :\\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^*) - b(\\tilde{\\theta}_i^*) + b(\\hat{\\theta}_i^*)}{a_i(\\phi)}\n\\]Following McCullagh (2019), \\(a_i(\\phi) = \\phi\\), define scaled deviance :\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi} \\sum_{=1}^n \\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*) - b(\\tilde{\\theta}_i^*) + b(\\hat{\\theta}_i^*)\\}\n\\]deviance :\\[\nD(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\n\\]:\\(D^*(\\mathbf{y, \\hat{\\mu}})\\) scaled deviance.\\(D(\\mathbf{y, \\hat{\\mu}})\\) deviance.models, \\(a_i(\\phi) = \\phi m_i\\), \\(m_i\\) known scalar varies across observations. , scaled deviance :\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n \\frac{2\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*) - b(\\tilde{\\theta}_i^*) + b(\\hat{\\theta}_i^*)\\}}{\\phi m_i}\n\\]deviance often decomposed deviance contributions:\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\n\\]\\(d_i\\) deviance contribution \\(\\)-th observation.Uses Deviance:\\(D\\) used model selection.\\(D\\) used model selection.\\(D^*\\) used goodness--fit tests, likelihood ratio statistic:\\(D^*\\) used goodness--fit tests, likelihood ratio statistic:\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]individual deviance contributions \\(d_i\\) used form deviance residuals.Deviance Normal DistributionFor normal distribution:\\[\n\\begin{aligned}\n\\theta &= \\mu, \\\\\n\\phi &= \\sigma^2, \\\\\nb(\\theta) &= \\frac{1}{2} \\theta^2, \\\\\n(\\phi) &= \\phi.\n\\end{aligned}\n\\]MLEs :\\[\n\\begin{aligned}\n\\tilde{\\theta}_i &= y_i, \\\\\n\\hat{\\theta}_i &= \\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i).\n\\end{aligned}\n\\]deviance simplifies :\\[\n\\begin{aligned}\nD &= 2 \\sum_{=1}^n \\left(y_i^2 - y_i \\hat{\\mu}_i - \\frac{1}{2} y_i^2 + \\frac{1}{2} \\hat{\\mu}_i^2 \\right) \\\\\n&= \\sum_{=1}^n (y_i^2 - 2y_i \\hat{\\mu}_i + \\hat{\\mu}_i^2) \\\\\n&= \\sum_{=1}^n (y_i - \\hat{\\mu}_i)^2.\n\\end{aligned}\n\\]Thus, normal model, deviance corresponds residual sum squares.Deviance Poisson DistributionFor Poisson distribution:\\[\n\\begin{aligned}\nf(y) &= \\exp\\{y\\log(\\mu) - \\mu - \\log(y!)\\}, \\\\\n\\theta &= \\log(\\mu), \\\\\nb(\\theta) &= \\exp(\\theta), \\\\\n(\\phi) &= 1.\n\\end{aligned}\n\\]MLEs:\\[\n\\begin{aligned}\n\\tilde{\\theta}_i &= \\log(y_i), \\\\\n\\hat{\\theta}_i &= \\log(\\hat{\\mu}_i), \\\\\n\\hat{\\mu}_i &= g^{-1}(\\hat{\\eta}_i).\n\\end{aligned}\n\\]deviance simplifies :\\[\n\\begin{aligned}\nD &= 2 \\sum_{= 1}^n \\left(y_i \\log(y_i) - y_i \\log(\\hat{\\mu}_i) - y_i + \\hat{\\mu}_i \\right) \\\\\n&= 2 \\sum_{= 1}^n \\left[y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i) \\right].\n\\end{aligned}\n\\]deviance contribution observation:\\[\nd_i = 2 \\left[y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i)\\right].\n\\]","code":""},{"path":"generalized-linear-models.html","id":"analysis-of-deviance","chapter":"7 Generalized Linear Models","heading":"7.8.9.1 Analysis of Deviance","text":"Analysis Deviance likelihood-based approach comparing nested models GLM. analogous [Analysis Variance (ANOVA)] linear models.comparing reduced model (denoted \\(\\hat{\\mu}_r\\)) full model (denoted \\(\\hat{\\mu}_f\\)), difference scaled deviance follows asymptotic chi-square distribution:\\[ D^*(\\mathbf{y;\\hat{\\mu}_r}) - D^*(\\mathbf{y;\\hat{\\mu}_f}) = 2\\{l(\\mathbf{y;\\hat{\\mu}_f})-l(\\mathbf{y;\\hat{\\mu}_r})\\} \\sim \\chi^2_q \\]\\(q\\) difference number free parameters two models.test provides means assess whether additional parameters full model significantly improve model fit.dispersion parameter \\(\\phi\\) estimated :\\[ \\hat{\\phi} = \\frac{D(\\mathbf{y, \\hat{\\mu}})}{n - p} \\]:\\(D(\\mathbf{y, \\hat{\\mu}})\\) deviance fitted model.\\(n\\) total number observations.\\(p\\) number estimated parameters.Caution: frequent use \\(\\chi^2\\) tests can problematic due reliance asymptotic approximations, especially small samples overdispersed data (McCullagh 2019).","code":""},{"path":"generalized-linear-models.html","id":"deviance-residuals","chapter":"7 Generalized Linear Models","heading":"7.8.9.2 Deviance Residuals","text":"Since deviance plays role model evaluation, define deviance residuals examine individual data points. Given total deviance :\\[ D = \\sum_{=1}^{n}d_i \\]deviance residual observation \\(\\) :\\[ r_{D_i} = \\text{sign}(y_i -\\hat{\\mu}_i)\\sqrt{d_i} \\]:\\(d_i\\) deviance contribution observation \\(\\).sign function preserves direction residual.account varying leverage, define standardized deviance residual:\\[ r_{s,} = \\frac{y_i -\\hat{\\mu}_i}{\\hat{\\sigma}(1-h_{ii})^{1/2}} \\]:\\(h_{ii}\\) leverage observation \\(\\).\\(\\hat{\\sigma}\\) estimate standard deviation.Alternatively, using GLM hat matrix:\\[ \\mathbf{H}^{GLM} = \\mathbf{W}^{1/2}X(X'WX)^{-1}X'\\mathbf{W}^{-1/2} \\]\\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\) (see Estimation Systematic Component (\\(\\beta\\))), express standardized deviance residuals :\\[ r_{s, D_i} = \\frac{r_{D_i}}{\\{\\hat{\\phi}(1-h_{ii}^{glm})\\}^{1/2}} \\]\\(h_{ii}^{glm}\\) \\(\\)-th diagonal element \\(\\mathbf{H}^{GLM}\\).","code":""},{"path":"generalized-linear-models.html","id":"pearson-chi-square-residuals","chapter":"7 Generalized Linear Models","heading":"7.8.9.3 Pearson Chi-Square Residuals","text":"Another goodness--fit statistic Pearson Chi-Square statistic, defined :\\[ X^2 = \\sum_{=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\]:\\(\\hat{\\mu}_i\\) fitted mean response.\\(V(\\hat{\\mu}_i)\\) variance function response.Scaled Pearson \\(\\chi^2\\) statistic :\\[ \\frac{X^2}{\\phi} \\sim \\chi^2_{n-p} \\]\\(p\\) number estimated parameters.Pearson residuals :\\[ X^2_i = \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\]residuals assess difference observed fitted values, standardized variance.assumptions hold:Independent samplesIndependent samplesNo overdispersion: \\(\\phi = 1\\), expect:\n\\[ \\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p} \\approx 1, \\quad \\frac{X^2}{n-p} \\approx 1 \\]\nvalue substantially greater 1 suggests overdispersion model misspecification.overdispersion: \\(\\phi = 1\\), expect:\\[ \\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p} \\approx 1, \\quad \\frac{X^2}{n-p} \\approx 1 \\]value substantially greater 1 suggests overdispersion model misspecification.Multiple groups: model includes categorical predictors, separate calculations may needed group.Multiple groups: model includes categorical predictors, separate calculations may needed group.assumptions, :\\[ \\frac{X^2}{\\phi} \\quad \\text{} \\quad D^*(\\mathbf{y; \\hat{\\mu}}) \\]follow \\(\\chi^2_{n-p}\\) distribution.residuals suggest reasonably well-fitted model slight underprediction tendency.residuals suggest reasonably well-fitted model slight underprediction tendency.extreme deviations severe skewness.extreme deviations severe skewness.largest residuals indicate potential outliers, extreme enough immediately suggest model inadequacy.largest residuals indicate potential outliers, extreme enough immediately suggest model inadequacy.","code":"\nset.seed(123) \nn <- 100 \nx <- rnorm(n) \ny <- rpois(n, lambda = exp(0.5 + 0.3 * x))  \n\n# Poisson response  \n# Fit Poisson GLM \nfit <- glm(y ~ x, family = poisson(link = \"log\"))  \n\n# Extract deviance and Pearson residuals \ndeviance_residuals <- residuals(fit, type = \"deviance\") \npearson_residuals  <- residuals(fit, type = \"pearson\")  \n# Display residual summaries \nsummary(deviance_residuals) \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -2.1286 -1.0189 -0.1441 -0.1806  0.6171  1.8942\nsummary(pearson_residuals)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> -1.505136 -0.837798 -0.140873  0.002003  0.658076  2.366618"},{"path":"generalized-linear-models.html","id":"diagnostic-plots","chapter":"7 Generalized Linear Models","heading":"7.8.10 Diagnostic Plots","text":"Standardized residual plots help diagnose potential issues model specification, incorrect link function variance structure. Common residual plots include:Plot Standardized Deviance Residuals vs. Fitted Values\\[ \\text{plot}(r_{s, D_i}, \\hat{\\mu}_i) \\] \\[ \\text{plot}(r_{s, D_i}, T(\\hat{\\mu}_i)) \\] \\(T(\\hat{\\mu}_i)\\) constant information scale transformation, adjusts \\(\\hat{\\mu}_i\\) maintain stable variance across observations.Plot Standardized Deviance Residuals vs. Fitted Values\\[ \\text{plot}(r_{s, D_i}, \\hat{\\mu}_i) \\] \\[ \\text{plot}(r_{s, D_i}, T(\\hat{\\mu}_i)) \\] \\(T(\\hat{\\mu}_i)\\) constant information scale transformation, adjusts \\(\\hat{\\mu}_i\\) maintain stable variance across observations.Plot Standardized Deviance Residuals vs. Linear Predictor\\[ \\text{plot}(r_{s, D_i}, \\hat{\\eta}_i) \\] \\(\\hat{\\eta}_i\\) represents linear predictor applying link function.Plot Standardized Deviance Residuals vs. Linear Predictor\\[ \\text{plot}(r_{s, D_i}, \\hat{\\eta}_i) \\] \\(\\hat{\\eta}_i\\) represents linear predictor applying link function.following table summarizes commonly used transformations \\(T(\\hat{\\mu}_i)\\) different random components:Interpretation Residual PlotsTrend residuals: Indicates incorrect link function inappropriate scale transformation.Systematic change residual range \\(T(\\hat{\\mu})\\): Suggests incorrect choice random component—.e., assumed variance function match data.Plot absolute residuals vs. fitted values:\\[ \\text{plot}(|r_{D_i}|, \\hat{\\mu}_i) \\] checks whether variance function correctly specified.","code":""},{"path":"generalized-linear-models.html","id":"goodness-of-fit","chapter":"7 Generalized Linear Models","heading":"7.8.11 Goodness of Fit","text":"assess well model fits data, use:DeviancePearson Chi-square ResidualsFor nested models, likelihood-based information criteria provide comparison metric:\\[\n\\begin{aligned}\nAIC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p \\\\\nAICC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p \\left(\\frac{n}{n-p-1} \\right) \\\\\nBIC &= -2l(\\mathbf{\\hat{\\mu}}) + p \\log(n)\n\\end{aligned}\n\\]:\\(l(\\hat{\\mu})\\) log-likelihood estimated parameters.\\(p\\) number model parameters.\\(n\\) number observations.Important Considerations:data model structure (.e., link function assumed random component distribution) must used comparisons.data model structure (.e., link function assumed random component distribution) must used comparisons.Models can differ number parameters must remain consistent respects.Models can differ number parameters must remain consistent respects.traditional \\(R^2\\) directly applicable GLMs, analogous measure :\\[\nR^2_p = 1 - \\frac{l(\\hat{\\mu})}{l(\\hat{\\mu}_0)}\n\\]\\(l(\\hat{\\mu}_0)\\) log-likelihood model intercept.binary response models, rescaled generalized \\(R^2\\) often used:\\[\n\\bar{R}^2 = \\frac{R^2_*}{\\max(R^2_*)} = \\frac{1-\\exp\\left(-\\frac{2}{n}(l(\\hat{\\mu}) - l(\\hat{\\mu}_0))\\right)}{1 - \\exp\\left(\\frac{2}{n} l(\\hat{\\mu}_0) \\right)}\n\\]denominator ensures maximum possible \\(R^2\\) scaling.","code":""},{"path":"generalized-linear-models.html","id":"over-dispersion","chapter":"7 Generalized Linear Models","heading":"7.8.12 Over-Dispersion","text":"-dispersion occurs observed variance exceeds assumed model allows. common Poisson Binomial models.Variance Assumptions Common Random ComponentsBy default, \\(\\phi = 1\\), meaning variance follows assumed model.\\(\\phi \\neq 1\\), variance differs expectation:\n\\(\\phi > 1\\): -dispersion (greater variance expected).\n\\(\\phi < 1\\): -dispersion (less variance expected).\n\\(\\phi > 1\\): -dispersion (greater variance expected).\\(\\phi < 1\\): -dispersion (less variance expected).discrepancy suggests assumed random component distribution may inappropriate.account dispersion issues, can:Choose flexible random component distribution\nNegative Binomial -dispersed Poisson data.\nConway-Maxwell Poisson - -dispersed count data.\nNegative Binomial -dispersed Poisson data.Conway-Maxwell Poisson - -dispersed count data.Use Mixed Models account random effects\nNonlinear Generalized Linear Mixed Models introduce random effects, can help capture additional variance.\nNonlinear Generalized Linear Mixed Models introduce random effects, can help capture additional variance.Standardized Residuals vs. Fitted ValuesIf clear trend residual plot (e.g., curve), suggests incorrect link function.clear trend residual plot (e.g., curve), suggests incorrect link function.residual spread increases fitted values, variance function may misspecified.residual spread increases fitted values, variance function may misspecified.Absolute Residuals vs. Fitted ValuesA systematic pattern (e.g., funnel shape) suggests heteroscedasticity (.e., changing variance).systematic pattern (e.g., funnel shape) suggests heteroscedasticity (.e., changing variance).residuals increase fitted values, different variance function (e.g., Negative Binomial instead Poisson) may needed.residuals increase fitted values, different variance function (e.g., Negative Binomial instead Poisson) may needed.Goodness--Fit MetricsAIC/BIC: Lower values indicate better model, must compared across nested models.AIC/BIC: Lower values indicate better model, must compared across nested models.Log-likelihood: Higher values suggest better-fitting model.Log-likelihood: Higher values suggest better-fitting model.\\(R^2\\) GLMs: Since traditional \\(R^2\\) available, likelihood-based \\(R^2\\) used measure improvement null model.\\(R^2\\) GLMs: Since traditional \\(R^2\\) available, likelihood-based \\(R^2\\) used measure improvement null model.Overdispersion Check (\\(\\phi\\))\\(\\phi \\approx 1\\), Poisson assumption valid.\\(\\phi \\approx 1\\), Poisson assumption valid.\\(\\phi > 1\\), overdispersion, meaning Negative Binomial model may appropriate.\\(\\phi > 1\\), overdispersion, meaning Negative Binomial model may appropriate.\\(\\phi < 1\\), underdispersion present, requiring alternative distributions like Conway-Maxwell Poisson.\\(\\phi < 1\\), underdispersion present, requiring alternative distributions like Conway-Maxwell Poisson.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(MASS)   # For negative binomial models\nlibrary(MuMIn)  # For R^2 in GLMs\nlibrary(glmmTMB) # For handling overdispersion\n\n# Generate Example Data\nset.seed(123)\nn <- 100\nx <- rnorm(n)\nmu <- exp(0.5 + 0.3 * x)  # True mean function\ny <- rpois(n, lambda = mu)  # Poisson outcome\n\n# Fit a Poisson GLM\nmodel_pois <- glm(y ~ x, family = poisson(link = \"log\"))\n\n# Compute residuals\nresid_dev <- residuals(model_pois, type = \"deviance\")\nfitted_vals <- fitted(model_pois)\n\n# Standardized Residual Plot: Residuals vs Fitted Values\nggplot(data = data.frame(fitted_vals, resid_dev),\n       aes(x = fitted_vals, y = resid_dev)) +\n    geom_point(alpha = 0.6) +\n    geom_smooth(method = \"loess\",\n                color = \"red\",\n                se = FALSE) +\n    theme_minimal() +\n    labs(title = \"Standardized Deviance Residuals vs Fitted Values\",\n         x = \"Fitted Values\", y = \"Standardized Deviance Residuals\")\n\n# Absolute Residuals vs Fitted Values (Variance Function Check)\nggplot(data = data.frame(fitted_vals, abs_resid = abs(resid_dev)),\n       aes(x = fitted_vals, y = abs_resid)) +\n    geom_point(alpha = 0.6) +\n    geom_smooth(method = \"loess\",\n                color = \"blue\",\n                se = FALSE) +\n    theme_minimal() +\n    labs(title = \"Absolute Deviance Residuals vs Fitted Values\",\n         x = \"Fitted Values\", y = \"|Residuals|\")\n\n# Goodness-of-Fit Metrics\nAIC(model_pois)    # Akaike Information Criterion\n#> [1] 322.9552\nBIC(model_pois)    # Bayesian Information Criterion\n#> [1] 328.1655\nlogLik(model_pois) # Log-likelihood\n#> 'log Lik.' -159.4776 (df=2)\n\n# R-squared for GLM\nr_squared <-\n    1 - (logLik(model_pois) / logLik(glm(y ~ 1, family = poisson)))\nr_squared\n#> 'log Lik.' 0.05192856 (df=2)\n\n# Overdispersion Check\nresid_pearson <- residuals(model_pois, type = \"pearson\")\n# Estimated dispersion parameter\nphi_hat <-\n    sum(resid_pearson ^ 2) / (n - length(coef(model_pois)))  \nphi_hat\n#> [1] 1.055881\n\n# If phi > 1, \n# fit a Negative Binomial Model to correct for overdispersion\nif (phi_hat > 1) {\n    model_nb <- glm.nb(y ~ x)\n    summary(model_nb)\n}\n#> \n#> Call:\n#> glm.nb(formula = y ~ x, init.theta = 50.70707605, link = log)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.42913    0.08514   5.040 4.65e-07 ***\n#> x            0.35262    0.08654   4.075 4.61e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(50.7071) family taken to be 1)\n#> \n#>     Null deviance: 135.71  on 99  degrees of freedom\n#> Residual deviance: 118.92  on 98  degrees of freedom\n#> AIC: 324.91\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  51 \n#>           Std. Err.:  233 \n#> \n#>  2 x log-likelihood:  -318.906"},{"path":"sec-linear-mixed-models.html","id":"sec-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8 Linear Mixed Models","text":"","code":""},{"path":"sec-linear-mixed-models.html","id":"dependent-data","chapter":"8 Linear Mixed Models","heading":"8.1 Dependent Data","text":"many real-world applications, observations independent exhibit correlations due shared characteristics. common forms dependent data:Multivariate measurements individuals: Multiple attributes measured person may correlated (e.g., blood pressure, cholesterol, BMI).Clustered measurements: Individuals within shared environment (e.g., families, schools, hospitals) often exhibit correlated responses.Repeated measurements: individual measured multiple times, correlations arise naturally.\nExample: Tracking cholesterol levels person time.\nrepeated measurements follow experimental design treatments applied initially, referred repeated measures (Schabenberger Pierce 2001).\nExample: Tracking cholesterol levels person time.repeated measurements follow experimental design treatments applied initially, referred repeated measures (Schabenberger Pierce 2001).Longitudinal data: Repeated measurements taken time observational study known longitudinal data (Schabenberger Pierce 2001).Spatial data: Measurements taken individuals nearby locations (e.g., residents neighborhood) often exhibit spatial correlation.Since standard linear regression assumes independent observations, correlations violate assumptions. Thus, Linear Mixed Models (LMMs) provide framework account dependencies.Linear Mixed Model (also called Mixed Linear Model) consists two components:Fixed effects: Parameters associated variables effect across observations (e.g., gender, age, diet, time).Random effects: Individual-specific variations correlation structures (e.g., subject-specific effects, spatial correlations), leading dependent (correlated) errors.key advantage LMMs model random subject-specific effects, rather including individual dummy variables. provides:reduction number parameters estimate, avoiding overfitting.framework inference population level, rather restricting conclusions observed individuals.","code":""},{"path":"sec-linear-mixed-models.html","id":"motivation-a-repeated-measurements-example","chapter":"8 Linear Mixed Models","heading":"8.1.1 Motivation: A Repeated Measurements Example","text":"Consider scenario analyze repeated measurements response variable \\(Y_{ij}\\) \\(\\)-th subject time \\(j\\):\\(= 1, \\dots, N\\) (subjects)\\(j = 1, \\dots, n_i\\) (measurements per subject)define response vector subject \\(\\) :\\[\n\\mathbf{Y}_i =\n\\begin{bmatrix}\nY_{i1} \\\\\nY_{i2} \\\\\n\\vdots \\\\\nY_{in_i}\n\\end{bmatrix}\n\\]model data, use two-stage hierarchical approach:","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-stage-1-regression-model-within-subject-variation","chapter":"8 Linear Mixed Models","heading":"8.1.1.1 Stage 1: Regression Model (Within-Subject Variation)","text":"first model response changes time subject:\\[\n\\mathbf{Y}_i = Z_i \\beta_i + \\epsilon_i\n\\]:\\(Z_i\\) \\(n_i \\times q\\) matrix known covariates (e.g., time, treatment).\\(\\beta_i\\) \\(q \\times 1\\) vector subject-specific regression coefficients.\\(\\epsilon_i\\) represents random errors, often assumed follow \\(\\epsilon_i \\sim N(0, \\sigma^2 )\\).stage, individual unique regression coefficients \\(\\beta_i\\). However, estimating separate \\(\\beta_i\\) subject impractical \\(N\\) large. Thus, introduce [second stage](#sec-stage-2-parameter-model-(-subject-variation) impose structure \\(\\beta_i\\).","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-stage-2-parameter-model-between-subject-variation","chapter":"8 Linear Mixed Models","heading":"8.1.1.2 Stage 2: Parameter Model (Between-Subject Variation)","text":"assume subject-specific coefficients \\(\\beta_i\\) arise common population distribution:\\[\n\\beta_i = K_i \\beta + b_i\n\\]:\\(K_i\\) \\(q \\times p\\) matrix known covariates.\\(\\beta\\) \\(p \\times 1\\) vector global parameters (fixed effects).\\(\\mathbf{b}_i\\) random effects, assumed follow \\(\\mathbf{b}_i \\sim N(0, D)\\).Thus, individual’s regression coefficients \\(\\beta_i\\) modeled deviations population-level mean \\(\\beta\\), subject-specific variations \\(b_i\\). hierarchical structure enables:Borrowing strength: Individual estimates \\(\\beta_i\\) informed overall population distribution.Improved generalization: model captures fixed random variability efficiently.full Linear Mixed Model can written :\\[\n\\mathbf{Y}_i = Z_i K_i \\beta + Z_i b_i + \\epsilon_i\n\\]:\\(Z_i K_i \\beta\\) represents fixed effects component.\\(Z_i b_i\\) represents random effects component.\\(\\epsilon_i\\) represents residual errors.formulation accounts within-subject correlations (via random effects) -subject variability (via fixed effects), making powerful tool analyzing dependent data.","code":""},{"path":"sec-linear-mixed-models.html","id":"example-linear-mixed-model-for-repeated-measurements","chapter":"8 Linear Mixed Models","heading":"8.1.2 Example: Linear Mixed Model for Repeated Measurements","text":"","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-stage-1-subject-specific-model-lmm-example","chapter":"8 Linear Mixed Models","heading":"8.1.2.1 Stage 1: Subject-Specific Model","text":"first stage models response variable \\(Y_{ij}\\) subject \\(\\) time \\(t_{ij}\\):\\[\nY_{ij} = \\beta_{1i} + \\beta_{2i} t_{ij} + \\epsilon_{ij}\n\\]:\\(j = 1, \\dots, n_i\\) represents different time points subject \\(\\).\\(\\beta_{1i}\\) subject-specific intercept (baseline response subject \\(\\)).\\(\\beta_{2i}\\) subject-specific slope (rate change time).\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\) independent errors.matrix notation, model written :\\[\n\\mathbf{Y}_i =\n\\begin{bmatrix}\nY_{i1} \\\\\nY_{i2} \\\\\n\\vdots \\\\\nY_{in_i}\n\\end{bmatrix},\n\\quad\n\\mathbf{Z}_i =\n\\begin{bmatrix}\n1 & t_{i1} \\\\\n1 & t_{i2} \\\\\n\\vdots & \\vdots \\\\\n1 & t_{in_i}\n\\end{bmatrix},\n\\]\\[\n\\beta_i =\n\\begin{bmatrix}\n\\beta_{1i} \\\\\n\\beta_{2i}\n\\end{bmatrix},\n\\quad\n\\epsilon_i =\n\\begin{bmatrix}\n\\epsilon_{i1} \\\\\n\\epsilon_{i2} \\\\\n\\vdots \\\\\n\\epsilon_{in_i}\n\\end{bmatrix}.\n\\]Thus, model can rewritten :\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}.\n\\]","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-stage-2-population-level-model-lmm-example","chapter":"8 Linear Mixed Models","heading":"8.1.2.2 Stage 2: Population-Level Model","text":"Since estimating separate \\(\\beta_{1i}\\) \\(\\beta_{2i}\\) subject impractical, assume follow population distribution:\\[\n\\begin{aligned}\n\\beta_{1i} &= \\beta_0 + b_{1i}, \\\\\n\\beta_{2i} &= \\beta_1 L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i}.\n\\end{aligned}\n\\]:\\(L_i, H_i, C_i\\) indicator variables treatment groups:\n\\(L_i = 1\\) subject belongs treatment group 1, else 0.\n\\(H_i = 1\\) subject belongs treatment group 2, else 0.\n\\(C_i = 1\\) subject belongs treatment group 3, else 0.\n\\(L_i = 1\\) subject belongs treatment group 1, else 0.\\(H_i = 1\\) subject belongs treatment group 2, else 0.\\(C_i = 1\\) subject belongs treatment group 3, else 0.\\(\\beta_0\\) represents average baseline response across subjects.\\(\\beta_1, \\beta_2, \\beta_3\\) average time effects respective treatment groups.\\(b_{1i}, b_{2i}\\) random effects representing subject-specific deviations.structure implies intercept \\(\\beta_{1i}\\) varies randomly across subjects, slope \\(\\beta_{2i}\\) depends treatment random subject-specific deviations.matrix form, :\\[\n\\begin{aligned}\n\\mathbf{K}_i &=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i\n\\end{bmatrix}, \\\\\n\\beta &=\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3\n\\end{bmatrix}, \\\\\n\\mathbf{b}_i &=\n\\begin{bmatrix}\nb_{1i} \\\\\nb_{2i}\n\\end{bmatrix}, \\\\\n\\beta_i &= \\mathbf{K_i \\beta + b_i}.\n\\end{aligned}\n\\]","code":""},{"path":"sec-linear-mixed-models.html","id":"final-mixed-model-formulation","chapter":"8 Linear Mixed Models","heading":"8.1.2.3 Final Mixed Model Formulation","text":"Substituting Stage 2 Stage 1, obtain full mixed model:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i (\\mathbf{K}_i \\beta + \\mathbf{b}_i) + \\epsilon_i.\n\\]Expanding:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i.\n\\]Interpretation:\\(\\mathbf{Z}_i \\mathbf{K}_i \\beta\\) represents fixed effects (population-level trends).\\(\\mathbf{Z}_i \\mathbf{b}_i\\) represents random effects (subject-specific variations).\\(\\epsilon_i\\) represents residual errors.Assumptions:Random effects: \\(\\mathbf{b}_i \\sim N(0, D)\\), \\(D\\) variance-covariance matrix.Residual errors: \\(\\epsilon_i \\sim N(0, \\sigma^2 )\\).Independence: \\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) independent.estimate \\(\\hat{\\beta}\\), one might consider sequential approach:Estimate \\(\\hat{\\beta}_i\\) subject first stage.Estimate \\(\\hat{\\beta}\\) second stage replacing \\(\\beta_i\\) \\(\\hat{\\beta}_i\\).However, method introduces several problems:Loss information: Summarizing \\(\\mathbf{Y}_i\\) solely \\(\\hat{\\beta}_i\\) discards valuable within-subject variability.Ignoring uncertainty: Treating estimated values \\(\\hat{\\beta}_i\\) known values leads underestimated variability.Unequal sample sizes: Subjects may different numbers observations (\\(n_i\\)), affects variance estimation.address issues, adopt Linear Mixed Model (LMM) framework (Laird Ware 1982).","code":""},{"path":"sec-linear-mixed-models.html","id":"reformulating-the-linear-mixed-model","chapter":"8 Linear Mixed Models","heading":"8.1.2.4 Reformulating the Linear Mixed Model","text":"Substituting [Stage 2](#sec-stage-2-parameter-model-(-subject-variation) [Stage 1](#sec-stage-2-parameter-model-(-subject-variation), obtain:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i.\n\\]Defining \\(\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i\\) \\(n_i \\times p\\) matrix covariates, rewrite model :\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i.\n\\]:\\(= 1, \\dots, N\\) (subjects).\\(= 1, \\dots, N\\) (subjects).\\(\\beta\\) fixed effects, common subjects.\\(\\beta\\) fixed effects, common subjects.\\(\\mathbf{b}_i\\) subject-specific random effects, assumed follow:\n\\[\n\\mathbf{b}_i \\sim N_q(\\mathbf{0,D}).\n\\]\\(\\mathbf{b}_i\\) subject-specific random effects, assumed follow:\\[\n\\mathbf{b}_i \\sim N_q(\\mathbf{0,D}).\n\\]\\(\\mathbf{\\epsilon}_i\\) represents residual errors:\n\\[\n\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i}).\n\\]\\(\\mathbf{\\epsilon}_i\\) represents residual errors:\\[\n\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i}).\n\\]Independence assumption: \\(\\mathbf{b}_i\\) \\(\\mathbf{\\epsilon}_i\\) independent.Independence assumption: \\(\\mathbf{b}_i\\) \\(\\mathbf{\\epsilon}_i\\) independent.Dimension notation:\n\\(\\mathbf{Z}_i\\) \\(n_i \\times q\\) matrix known covariates random effects.\n\\(\\mathbf{X}_i\\) \\(n_i \\times p\\) matrix known covariates fixed effects.\nDimension notation:\\(\\mathbf{Z}_i\\) \\(n_i \\times q\\) matrix known covariates random effects.\\(\\mathbf{X}_i\\) \\(n_i \\times p\\) matrix known covariates fixed effects.","code":""},{"path":"sec-linear-mixed-models.html","id":"hierarchical-conditional-formulation","chapter":"8 Linear Mixed Models","heading":"8.1.2.5 Hierarchical (Conditional) Formulation","text":"Rewriting LMM hierarchical form:\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i), \\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D}).\n\\end{aligned}\n\\]:first equation states , given subject-specific random effects \\(\\mathbf{b}_i\\), response follows normal distribution mean \\(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i\\) covariance \\(\\mathbf{\\Sigma}_i\\).second equation states random effects \\(\\mathbf{b}_i\\) follow multivariate normal distribution mean zero covariance \\(D\\).denote respective probability density functions :\\[\nf(\\mathbf{Y}_i |\\mathbf{b}_i) \\quad \\text{} \\quad f(\\mathbf{b}_i).\n\\]Using general marginalization formula:\\[\n\\begin{aligned}\nf(,B) &= f(|B)f(B) \\\\\nf() &= \\int f(,B)dB = \\int f(|B) f(B) dB\n\\end{aligned}\n\\]obtain marginal density \\(\\mathbf{Y}_i\\):\\[\nf(\\mathbf{Y}_i) = \\int f(\\mathbf{Y}_i | \\mathbf{b}_i) f(\\mathbf{b}_i) d\\mathbf{b}_i.\n\\]Solving integral, obtain:\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X_i \\beta}, \\mathbf{Z_i D Z_i'} + \\mathbf{\\Sigma_i}).\n\\]Interpretation:Mean structure: expectation remains \\(\\mathbf{X}_i \\beta\\), fixed effects.Mean structure: expectation remains \\(\\mathbf{X}_i \\beta\\), fixed effects.Variance structure: covariance now incorporates random effect variability:\n\\[\n\\mathbf{Z_i D Z_i'} + \\mathbf{\\Sigma_i}.\n\\]\nshows random effects contribute additional correlation observations.Variance structure: covariance now incorporates random effect variability:\\[\n\\mathbf{Z_i D Z_i'} + \\mathbf{\\Sigma_i}.\n\\]shows random effects contribute additional correlation observations.🔹 Key Takeaway:\nmarginal formulation LMM longer includes \\(Z_i b_i\\) mean, instead incorporates covariance structure, introducing marginal dependence \\(\\mathbf{Y}_i\\).Technically, rather “averaging ” random effect \\(b_i\\), add contribution variance-covariance matrix.Continue example:\\[\nY_{ij} = (\\beta_0 + b_{1i}) + (\\beta_1L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i})t_{ij} + \\epsilon_{ij}.\n\\]treatment group:\\[\nY_{ij} =\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + b_{2i}) t_{ij} + \\epsilon_{ij}, & L_i = 1 \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + b_{2i}) t_{ij} + \\epsilon_{ij}, & H_i = 1 \\\\\n\\beta_0 + b_{1i} + (\\beta_3 + b_{2i}) t_{ij} + \\epsilon_{ij}, & C_i = 1.\n\\end{cases}\n\\]Interpretation:Intercepts slopes subject-specific: subject baseline response rate change.Treatment groups affect slopes, intercepts:\n\\(\\beta_0\\) common intercept across groups.\nSlopes differ treatment: \\(\\beta_1\\) \\(L\\), \\(\\beta_2\\) \\(H\\), \\(\\beta_3\\) \\(C\\).\n\\(\\beta_0\\) common intercept across groups.Slopes differ treatment: \\(\\beta_1\\) \\(L\\), \\(\\beta_2\\) \\(H\\), \\(\\beta_3\\) \\(C\\).Random effects introduce within-subject correlation:\n\\(b_{1i}\\) allows individual variation baseline response.\n\\(b_{2i}\\) allows subject-specific deviations slopes.\n\\(b_{1i}\\) allows individual variation baseline response.\\(b_{2i}\\) allows subject-specific deviations slopes.hierarchical model form, express LMM :\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i)\\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D})\n\\end{aligned}\n\\]:\\(\\mathbf{X}_i\\) \\(\\mathbf{Z}_i\\) design matrices fixed random effects, respectively.\\(\\mathbf{b}_i\\) represents subject-specific random effects.\\(\\mathbf{\\Sigma}_i\\) residual error covariance matrix.\\(\\mathbf{D}\\) random effects covariance matrix.fixed-effects parameter vector :\\[\n\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'.\n\\]model structure, define:\\[\n\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i.\n\\]Expanding,\\[\n\\mathbf{Z}_i =\n\\begin{bmatrix}\n1 & t_{i1} \\\\\n1 & t_{i2} \\\\\n\\vdots & \\vdots \\\\\n1 & t_{in_i}\n\\end{bmatrix},\n\\quad\n\\mathbf{K}_i =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i\n\\end{bmatrix}.\n\\]Multiplying:\\[\n\\mathbf{X}_i =\n\\begin{bmatrix}\n1 & t_{i1}L_i & t_{i1}H_i & t_{i1}C_i \\\\\n1 & t_{i2}L_i & t_{i2}H_i & t_{i2}C_i \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & t_{in_i}L_i & t_{in_i}H_i & t_{in_i}C_i\n\\end{bmatrix}.\n\\]random effects vector :\\[\n\\mathbf{b}_i =\n\\begin{bmatrix}\nb_{1i} \\\\\nb_{2i}\n\\end{bmatrix}.\n\\]covariance matrix \\(\\mathbf{D}\\) random effects :\\[\n\\mathbf{D} =\n\\begin{bmatrix}\nd_{11} & d_{12} \\\\\nd_{12} & d_{22}\n\\end{bmatrix}.\n\\]assume conditional independence:\\[\n\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}.\n\\]means , given random effects \\(\\mathbf{b}_i\\) \\(\\beta\\), responses subject \\(\\) independent.derive marginal model, integrate random effects \\(\\mathbf{b}_i\\). leads :\\[\nY_{ij} = \\beta_0 + \\beta_1 L_i t_{ij} + \\beta_2 H_i t_{ij} + \\beta_3 C_i t_{ij} + \\eta_{ij},\n\\]:\\[\n\\eta_i \\sim N(\\mathbf{0}, \\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i' + \\mathbf{\\Sigma}_i).\n\\]Thus, full marginal model :\\[\n\\mathbf{Y_i} \\sim N(\\mathbf{X}_i \\beta, \\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i' + \\mathbf{\\Sigma}_i).\n\\]Example: Case \\(n_i = 2\\)subject two observations, compute:\\[\n\\mathbf{Z}_i =\n\\begin{bmatrix}\n1 & t_{i1} \\\\\n1 & t_{i2}\n\\end{bmatrix}.\n\\]:\\[\n\\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i' =\n\\begin{bmatrix}\n1 & t_{i1} \\\\\n1 & t_{i2}\n\\end{bmatrix}\n\\begin{bmatrix}\nd_{11} & d_{12} \\\\\nd_{12} & d_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\nt_{i1} & t_{i2}\n\\end{bmatrix}.\n\\]Expanding multiplication:\\[\n\\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i' =\n\\begin{bmatrix}\nd_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\\\\nd_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22} t_{i2}^2\n\\end{bmatrix}.\n\\]Finally, incorporating residual variance:\\[\n\\text{Var}(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \\sigma^2.\n\\]Interpretation Marginal ModelCorrelation Errors:\n-diagonal elements \\(\\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i'\\) introduce correlation observations within subject.\naccounts fact repeated measurements individual independent.\n-diagonal elements \\(\\mathbf{Z}_i \\mathbf{D} \\mathbf{Z}_i'\\) introduce correlation observations within subject.accounts fact repeated measurements individual independent.Variance Structure:\nvariance function \\(Y_{ij}\\) quadratic time:\n\\[\n\\text{Var}(Y_{ij}) = d_{11} + 2d_{12}t_{ij} + d_{22}t_{ij}^2 + \\sigma^2.\n\\]\ncurvature variance function determined \\(d_{22}\\).\n\\(d_{22} > 0\\), variance increases time.\nvariance function \\(Y_{ij}\\) quadratic time:\n\\[\n\\text{Var}(Y_{ij}) = d_{11} + 2d_{12}t_{ij} + d_{22}t_{ij}^2 + \\sigma^2.\n\\]variance function \\(Y_{ij}\\) quadratic time:\\[\n\\text{Var}(Y_{ij}) = d_{11} + 2d_{12}t_{ij} + d_{22}t_{ij}^2 + \\sigma^2.\n\\]curvature variance function determined \\(d_{22}\\).curvature variance function determined \\(d_{22}\\).\\(d_{22} > 0\\), variance increases time.\\(d_{22} > 0\\), variance increases time.🔹 Key Takeaway:\nhierarchical model, random effects contribute mean structure.\nmarginal model, affect covariance structure, leading heteroskedasticity (changing variance time) correlation repeated measures.","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-random-intercepts-model-lmm","chapter":"8 Linear Mixed Models","heading":"8.1.3 Random-Intercepts Model","text":"Random-Intercepts Model obtained removing random slopes, meaning:subject-specific variability slopes attributed treatment differences.model allows subject intercept, within treatment group, subjects share slope.hierarchical (conditional) model :\\[\n\\begin{aligned}\n\\mathbf{Y}_i | b_i &\\sim N(\\mathbf{X}_i \\beta + 1 b_i, \\mathbf{\\Sigma}_i), \\\\\nb_i &\\sim N(0, d_{11}).\n\\end{aligned}\n\\]:\\(b_i\\) random intercept subject \\(\\), assumed follow \\(N(0, d_{11})\\).\\(\\mathbf{X}_i\\) contains fixed effects, include treatment time.\\(\\mathbf{\\Sigma}_i\\) represents residual variance, typically assumed \\(\\sigma^2 \\mathbf{}\\) (independent errors).Since random slopes, source subject-specific variability intercept.Integrating \\(b_i\\) assuming conditional independence \\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}\\), marginal distribution \\(\\mathbf{Y}_i\\) :\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, 11'd_{11} + \\sigma^2 \\mathbf{}).\n\\]marginal covariance matrix :\\[\n\\begin{aligned}\n\\text{Cov}(\\mathbf{Y}_i) &= 11'd_{11} + \\sigma^2 \\\\\n&=\n\\begin{bmatrix}\nd_{11} + \\sigma^2 & d_{11} & d_{11} & \\dots & d_{11} \\\\\nd_{11} & d_{11} + \\sigma^2 & d_{11} & \\dots & d_{11} \\\\\nd_{11} & d_{11} & d_{11} + \\sigma^2 & \\dots & d_{11} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nd_{11} & d_{11} & d_{11} & \\dots & d_{11} + \\sigma^2\n\\end{bmatrix}.\n\\end{aligned}\n\\]correlation matrix obtained standardizing covariance matrix:\\[\n\\text{Corr}(\\mathbf{Y}_i) =\n\\begin{bmatrix}\n1 & \\rho & \\rho & \\dots & \\rho \\\\\n\\rho & 1 & \\rho & \\dots & \\rho \\\\\n\\rho & \\rho & 1 & \\dots & \\rho \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho & \\rho & \\rho & \\dots & 1\n\\end{bmatrix},\n\\]:\\[\n\\rho = \\frac{d_{11}}{d_{11} + \\sigma^2}.\n\\]correlation structure known compound symmetry, meaning:Constant variance: \\(\\text{Var}(Y_{ij}) = d_{11} + \\sigma^2\\) \\(j\\).Equal correlation: \\(\\text{Corr}(Y_{ij}, Y_{ik}) = \\rho\\) \\(j \\neq k\\).Positive correlation: two observations subject equally correlated.Interpretation \\(\\rho\\) (Intra-Class Correlation)\\(\\rho\\) called intra-class correlation coefficient (ICC).\\(\\rho\\) called intra-class correlation coefficient (ICC).measures proportion total variability due -subject variability:\n\\[\n\\rho = \\frac{\\text{-Subject Variance}}{\\text{Total Variance}} = \\frac{d_{11}}{d_{11} + \\sigma^2}.\n\\]measures proportion total variability due -subject variability:\\[\n\\rho = \\frac{\\text{-Subject Variance}}{\\text{Total Variance}} = \\frac{d_{11}}{d_{11} + \\sigma^2}.\n\\]\\(\\rho\\) large:\ninter-subject variability (\\(d_{11}\\)) large relative intra-subject variability (\\(\\sigma^2\\)).\nmeans subjects differ substantially intercepts.\nResponses subject highly correlated.\n\\(\\rho\\) large:inter-subject variability (\\(d_{11}\\)) large relative intra-subject variability (\\(\\sigma^2\\)).means subjects differ substantially intercepts.Responses subject highly correlated.\\(\\rho\\) small:\nresidual error variance dominates, meaning individual differences intercepts small.\nMeasurements subject weakly correlated.\n\\(\\rho\\) small:residual error variance dominates, meaning individual differences intercepts small.Measurements subject weakly correlated.Summary Random-Intercepts ModelLarge \\(\\rho\\) → Strong subject-level differences,Small \\(\\rho\\) → Mostly residual noise","code":""},{"path":"sec-linear-mixed-models.html","id":"covariance-models-in-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.1.4 Covariance Models in Linear Mixed Models","text":"Previously, assumed within-subject errors conditionally independent, meaning:\\[\n\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}.\n\\]However, real-world data often exhibit correlated errors, particularly longitudinal studies observations time influenced underlying stochastic processes.model dependence, decompose error term two components:\\[\n\\epsilon_i = \\epsilon_{(1)} + \\epsilon_{(2)},\n\\]:\\(\\epsilon_{(1)}\\) (Serial Correlation Component):\nRepresents subject-specific response time-varying stochastic processes.\nCaptures dependency across observations subject.\nRepresents subject-specific response time-varying stochastic processes.Captures dependency across observations subject.\\(\\epsilon_{(2)}\\) (Measurement Error Component):\nRepresents pure measurement error, assumed independent \\(\\epsilon_{(1)}\\).\nRepresents pure measurement error, assumed independent \\(\\epsilon_{(1)}\\).Thus, full LMM formulation becomes:\\[\n\\mathbf{Y_i} = \\mathbf{X_i \\beta} + \\mathbf{Z_i b_i} + \\mathbf{\\epsilon_{(1)}} + \\mathbf{\\epsilon_{(2)}}.\n\\]:Random effects: \\(\\mathbf{b_i} \\sim N(\\mathbf{0, D})\\).Measurement errors: \\(\\epsilon_{(2)} \\sim N(\\mathbf{0, \\sigma^2 I_{n_i}})\\).Serial correlation errors: \\(\\epsilon_{(1)} \\sim N(\\mathbf{0, \\tau^2 H_i})\\).Independence assumption: \\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independent.model correlation structure serial correlation component \\(\\epsilon_{(1)}\\), define \\(n_i \\times n_i\\) correlation (covariance) matrix \\(\\mathbf{H}_i\\).\\((j,k)\\)th element \\(\\mathbf{H}_i\\) denoted :\\[\nh_{ijk} = g(t_{ij}, t_{ik}),\n\\]:\\(h_{ijk}\\) represents covariance (correlation) time points \\(t_{ij}\\) \\(t_{ik}\\).\\(h_{ijk}\\) represents covariance (correlation) time points \\(t_{ij}\\) \\(t_{ik}\\).\\(g(t_{ij}, t_{ik})\\) decreasing function defines correlation time points \\(t_{ij}\\) \\(t_{ik}\\).\\(g(t_{ij}, t_{ik})\\) decreasing function defines correlation time points \\(t_{ij}\\) \\(t_{ik}\\).Typically, assume function depends time difference (stationarity assumption):\n\\[\nh_{ijk} = g(|t_{ij} - t_{ik}|)\n\\]\nmeaning correlation depends absolute time lag.Typically, assume function depends time difference (stationarity assumption):\\[\nh_{ijk} = g(|t_{ij} - t_{ik}|)\n\\]meaning correlation depends absolute time lag.valid correlation matrix, require:\n\\[\ng(0) = 1.\n\\]\nensures observation perfect correlation .valid correlation matrix, require:\\[\ng(0) = 1.\n\\]ensures observation perfect correlation .Common Choices \\(g(|t_{ij} - t_{ik}|)\\)Several functions can used define decay correlation time differences increase.1. Exponential Correlation (Continuous-Time AR(1))\\[\ng(|t_{ij} - t_{ik}|) = \\exp(-\\phi |t_{ij} - t_{ik}|)\n\\]Decay rate: Controlled \\(\\phi > 0\\).Interpretation:\nObservations closer time correlated.\ncorrelation decreases exponentially time separation increases.\nObservations closer time correlated.correlation decreases exponentially time separation increases.Use case: Often used biological economic time-series models.2. Gaussian Correlation (Squared Exponential Kernel)\\[\ng(|t_{ij} - t_{ik}|) = \\exp(-\\phi (t_{ij} - t_{ik})^2)\n\\]Decay rate: Faster exponential.Interpretation:\n\\(\\phi\\) large, correlation drops sharply time separation increases.\nProduces smooth correlation structures.\n\\(\\phi\\) large, correlation drops sharply time separation increases.Produces smooth correlation structures.Use case: Used spatial statistics machine learning (Gaussian processes).3. Autoregressive (AR(1)) CorrelationA First-Order Autoregressive Model (AR(1)) assumes value time \\(t\\) depends previous value:\\[\n\\alpha_t = \\phi \\alpha_{t-1} + \\eta_t,\n\\]\\(\\eta_t \\sim \\text{..d. } N(0, \\sigma^2_\\eta)\\) (white noise process).\\(\\eta_t \\sim \\text{..d. } N(0, \\sigma^2_\\eta)\\) (white noise process).\\(\\phi\\) autocorrelation coefficient.\\(\\phi\\) autocorrelation coefficient., covariance two observations different times :\\[ \\text{Cov}(\\alpha_t, \\alpha_{t+h}) = \\frac{\\sigma^2_\\eta \\phi^{|h|}}{1 - \\phi^2}. \\]Thus, correlation time points \\(t\\) \\(t+h\\) :\\[ \\text{Corr}(\\alpha_t, \\alpha_{t+h}) = \\phi^{|h|}. \\]sequence \\(T\\) time points, resulting Toeplitz correlation matrix :\\[ \\text{Corr}(\\alpha_T) = \\begin{bmatrix} 1 & \\phi^1 & \\phi^2 & \\dots & \\phi^{T-1} \\\\ \\phi^1 & 1 & \\phi^1 & \\dots & \\phi^{T-2} \\\\ \\phi^2 & \\phi^1 & 1 & \\dots & \\phi^{T-3} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\phi^{T-1} & \\phi^{T-2} & \\phi^{T-3} & \\dots & 1 \\end{bmatrix}. \\]Decay rate: \\(\\phi\\) controls fast correlation decreases.Use case:\nCommon time series models.\nAppropriate observations equally spaced time.\nCommon time series models.Appropriate observations equally spaced time.Properties AR(1) Structure:Correlation decreases time lag:\nObservations closer time correlated.\nDecay rate controlled \\(\\phi\\).\nObservations closer time correlated.Decay rate controlled \\(\\phi\\).Toeplitz structure:\ncovariance matrix exhibits banded diagonal pattern.\nUseful longitudinal time-series data.\ncovariance matrix exhibits banded diagonal pattern.Useful longitudinal time-series data.Applicability:\nSmall \\(\\phi\\) (\\(\\approx 0\\)) → Weak autocorrelation (errors mostly independent).\nLarge \\(\\phi\\) (\\(\\approx 1\\)) → Strong autocorrelation (highly dependent observations).\nSmall \\(\\phi\\) (\\(\\approx 0\\)) → Weak autocorrelation (errors mostly independent).Large \\(\\phi\\) (\\(\\approx 1\\)) → Strong autocorrelation (highly dependent observations).","code":""},{"path":"sec-linear-mixed-models.html","id":"covariance-structures-in-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.1.5 Covariance Structures in Mixed Models","text":"Choosing Right Covariance Structure","code":""},{"path":"sec-linear-mixed-models.html","id":"estimation-in-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.2 Estimation in Linear Mixed Models","text":"general Linear Mixed Model :\\[ \\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i, \\]:\\(\\beta\\): Fixed effects (parameters shared across subjects).\\(\\mathbf{b}_i\\): Random effects (subject-specific deviations).\\(\\mathbf{X}_i\\): Design matrix fixed effects.\\(\\mathbf{Z}_i\\): Design matrix random effects.\\(\\mathbf{D}\\): Covariance matrix random effects.\\(\\mathbf{\\Sigma}_i\\): Covariance matrix residual errors.Since \\(\\beta\\), \\(\\mathbf{b}_i\\), \\(\\mathbf{D}\\), \\(\\mathbf{\\Sigma}_i\\) unknown, must estimated data.\\(\\beta, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) fixed parameters → must estimated.\\(\\mathbf{b}_i\\) random variable → must predicted (estimated). words, random thing/variable can’t estimated.Thus, define:Estimator \\(\\beta\\): \\(\\hat{\\beta}\\) (fixed effect estimation).Predictor \\(\\mathbf{b}_i\\): \\(\\hat{\\mathbf{b}}_i\\) (random effect prediction).:population-level estimate \\(\\mathbf{Y}_i\\) :\n\\[\n\\hat{\\mathbf{Y}}_i = \\mathbf{X}_i \\hat{\\beta}.\n\\]population-level estimate \\(\\mathbf{Y}_i\\) :\\[\n\\hat{\\mathbf{Y}}_i = \\mathbf{X}_i \\hat{\\beta}.\n\\]subject-specific prediction :\n\\[\n\\hat{\\mathbf{Y}}_i = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{\\mathbf{b}}_i.\n\\]subject-specific prediction :\\[\n\\hat{\\mathbf{Y}}_i = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{\\mathbf{b}}_i.\n\\]\\(N\\) subjects, stack equations Mixed Model Equations (Henderson 1975):\\[\n\\mathbf{Y} = \\mathbf{X} \\beta + \\mathbf{Z} \\mathbf{b} + \\epsilon.\n\\]\\[\n\\mathbf{Y} \\sim N(\\mathbf{X \\beta, ZBZ' + \\Sigma})\n\\]:\\[\n\\mathbf{Y} =\n\\begin{bmatrix}\n\\mathbf{y}_1 \\\\\n\\vdots \\\\\n\\mathbf{y}_N\n\\end{bmatrix},\n\\quad\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{X}_1 \\\\\n\\vdots \\\\\n\\mathbf{X}_N\n\\end{bmatrix},\n\\quad\n\\mathbf{b} =\n\\begin{bmatrix}\n\\mathbf{b}_1 \\\\\n\\vdots \\\\\n\\mathbf{b}_N\n\\end{bmatrix},\n\\quad\\mathbf{\\epsilon} =    \\begin{bmatrix}\\mathbf{\\epsilon}_1 \\\\\\vdots \\\\\\mathbf{\\epsilon}_N\\end{bmatrix}.\n\\]covariance structure :\\[\n\\text{Cov}(\\mathbf{b}) = \\mathbf{B}, \\quad \\text{Cov}(\\epsilon) = \\mathbf{\\Sigma}, \\quad \\text{Cov}(\\mathbf{b}, \\epsilon) = 0.\n\\]Expanding \\(\\mathbf{Z}\\) \\(\\mathbf{B}\\) block diagonal matrices:\\[\n\\mathbf{Z} =\n\\begin{bmatrix}\n\\mathbf{Z}_1 & 0 & \\dots & 0 \\\\\n0 & \\mathbf{Z}_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\mathbf{Z}_N\n\\end{bmatrix},\n\\quad\n\\mathbf{B} =\n\\begin{bmatrix}\n\\mathbf{D} & 0 & \\dots & 0 \\\\\n0 & \\mathbf{D} & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\mathbf{D}\n\\end{bmatrix}.\n\\]best linear unbiased estimator (BLUE) \\(\\beta\\) best linear unbiased predictor (BLUP) \\(\\mathbf{b}\\) obtained solving (Henderson 1975):\\[\n\\left[\n\\begin{array}{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{cc}\n\\mathbf{X' \\Sigma^{-1} X} & \\mathbf{X' \\Sigma^{-1} Z} \\\\\n\\mathbf{Z' \\Sigma^{-1} X} & \\mathbf{Z' \\Sigma^{-1} Z + B^{-1}}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}{c}\n\\mathbf{X' \\Sigma^{-1} Y} \\\\\n\\mathbf{Z' \\Sigma^{-1} Y}\n\\end{array}\n\\right].\n\\]:\\(\\hat{\\beta}\\) Generalized Least Squares estimator \\(\\beta\\).\\(\\hat{\\mathbf{b}}\\) BLUP \\(\\mathbf{b}\\).define:\\[\n\\mathbf{V} = \\mathbf{Z B Z'} + \\mathbf{\\Sigma}.\n\\], solutions Mixed Model Equations :\\[\n\\begin{aligned}\n\\hat{\\beta} &= (\\mathbf{X'V^{-1}X})^{-1} \\mathbf{X'V^{-1}Y}, \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{BZ'V^{-1}(Y - X\\hat{\\beta})}.\n\\end{aligned}\n\\]:\\(\\hat{\\beta}\\) obtained using Generalized Least Squares.\\(\\hat{\\mathbf{b}}\\) Weighted Least Squares predictor, weights come \\(\\mathbf{B}\\) \\(\\mathbf{V}\\).Properties EstimatorsFor \\(\\hat{\\beta}\\):\\[\nE(\\hat{\\beta}) = \\beta, \\quad \\text{Var}(\\hat{\\beta}) = (\\mathbf{X'V^{-1}X})^{-1}.\n\\]\\(\\hat{\\mathbf{b}}\\):\\[\nE(\\hat{\\mathbf{b}}) = 0.\n\\]variance prediction error (Mean Squared Prediction Error, MSPE) :\\[\n\\text{Var}(\\hat{\\mathbf{b}} - \\mathbf{b}) =\n\\mathbf{B - BZ'V^{-1}ZB + BZ'V^{-1}X (X'V^{-1}X)^{-1} X'V^{-1}B}.\n\\]🔹 Key Insight:\nMean Squared Prediction Error meaningful \\(\\text{Var}(\\hat{\\mathbf{b}})\\) alone, since accounts variance bias prediction.","code":""},{"path":"sec-linear-mixed-models.html","id":"interpretation-of-the-mixed-model-equations","chapter":"8 Linear Mixed Models","heading":"8.2.1 Interpretation of the Mixed Model Equations","text":"system:\\[\n\\left[\n\\begin{array}{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\n\\beta \\\\\n\\mathbf{b}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right].\n\\]can understood :Fixed Effects Estimation (\\(\\hat{\\beta}\\))\nUses Generalized Least Squares.\nAdjusted random effects correlated errors.\nUses Generalized Least Squares.Adjusted random effects correlated errors.Random Effects Prediction (\\(\\hat{\\mathbf{b}}\\))\nComputed using BLUP formula.\nShrinks subject-specific estimates toward population mean.\nComputed using BLUP formula.Shrinks subject-specific estimates toward population mean.","code":""},{"path":"sec-linear-mixed-models.html","id":"derivation-of-the-mixed-model-equations","chapter":"8 Linear Mixed Models","heading":"8.2.2 Derivation of the Mixed Model Equations","text":"derive Mixed Model Equations, consider:\\[\n\\mathbf{\\epsilon} = \\mathbf{Y} - \\mathbf{X \\beta} - \\mathbf{Z b}.\n\\]Define:\\(T = \\sum_{=1}^N n_i\\) → Total number observations.\\(Nq\\) → Total number random effects.joint distribution \\((\\mathbf{b}, \\mathbf{\\epsilon})\\) :\\[\n\\begin{aligned}\nf(\\mathbf{b}, \\epsilon) &= \\frac{1}{(2\\pi)^{(T+Nq)/2}}\n\\left|\n\\begin{array}{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right|^{-1/2} \\\\\n&\n\\exp\n\\left(\n-\\frac{1}{2}\n\\begin{bmatrix}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{bmatrix}'\n\\begin{bmatrix}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{bmatrix}\n\\right).\n\\end{aligned}\n\\]Maximizing \\(f(\\mathbf{b},\\epsilon)\\) respect \\(\\mathbf{b}\\) \\(\\beta\\) requires minimization :\\[\n\\begin{aligned}\nQ &= \\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right] \\\\\n&=\\mathbf{b' B^{-1} b} + (\\mathbf{Y - X \\beta - Z b})' \\mathbf{\\Sigma^{-1}} (\\mathbf{Y - X \\beta - Z b}).\n\\end{aligned}\n\\]Setting derivatives \\(Q\\) respect \\(\\mathbf{b}\\) \\(\\mathbf{\\beta}\\) zero leads system equations:\\[\n\\begin{aligned}\n\\mathbf{X'\\Sigma^{-1}X\\beta + X'\\Sigma^{-1}Zb} &= \\mathbf{X'\\Sigma^{-1}Y}\\\\\n\\mathbf{(Z'\\Sigma^{-1}Z + B^{-1})b + Z'\\Sigma^{-1}X\\beta} &= \\mathbf{Z'\\Sigma^{-1}Y}\n\\end{aligned}\n\\]Rearranging\\[\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\beta \\\\\n\\mathbf{b}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Thus, solution mixed model equations give:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right] ^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]","code":""},{"path":"sec-linear-mixed-models.html","id":"bayesian-interpretation-of-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.2.3 Bayesian Interpretation of Linear Mixed Models","text":"Bayesian framework, posterior distribution random effects \\(\\mathbf{b}\\) given observed data \\(\\mathbf{Y}\\) derived using Bayes’ theorem:\\[\nf(\\mathbf{b}| \\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b})}{\\int f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b}) d\\mathbf{b}}.\n\\]:\\(f(\\mathbf{Y}|\\mathbf{b})\\) likelihood function, describing data generated given random effects.\\(f(\\mathbf{b})\\) prior distribution random effects.denominator \\(\\int f(\\mathbf{Y}|\\mathbf{b}) f(\\mathbf{b}) d\\mathbf{b}\\) normalizing constant ensures posterior integrates 1.\\(f(\\mathbf{b}|\\mathbf{Y})\\) posterior distribution, updates belief \\(\\mathbf{b}\\) given observed data \\(\\mathbf{Y}\\).Linear Mixed Model, assume:\\[\n\\begin{aligned}\n\\mathbf{Y} | \\mathbf{b} &\\sim N(\\mathbf{X\\beta+Zb, \\Sigma}), \\\\\n\\mathbf{b} &\\sim N(\\mathbf{0, B}).\n\\end{aligned}\n\\]means:Likelihood: Given \\(\\mathbf{b}\\), data \\(\\mathbf{Y}\\) follows multivariate normal distribution mean \\(\\mathbf{X\\beta+Zb}\\) covariance \\(\\mathbf{\\Sigma}\\).Prior \\(\\mathbf{b}\\): random effects assumed follow multivariate normal distribution mean 0 covariance \\(\\mathbf{B}\\).applying Bayes’ theorem, posterior distribution \\(\\mathbf{b}\\) given \\(\\mathbf{Y}\\) :\\[\n\\mathbf{b} | \\mathbf{Y} \\sim N(\\mathbf{BZ'V^{-1}(Y - X\\beta)}, (\\mathbf{Z'\\Sigma^{-1}Z} + \\mathbf{B^{-1}})^{-1}).\n\\]:Mean: \\(\\mathbf{BZ'V^{-1}(Y - X\\beta)}\\)\nBLUP.\nrepresents expected value \\(\\mathbf{b}\\) given \\(\\mathbf{Y}\\) squared-error loss.\nBLUP.represents expected value \\(\\mathbf{b}\\) given \\(\\mathbf{Y}\\) squared-error loss.Covariance: \\((\\mathbf{Z'\\Sigma^{-1}Z} + \\mathbf{B^{-1}})^{-1}\\)\nposterior variance accounts prior uncertainty (\\(\\mathbf{B}\\)) data uncertainty (\\(\\mathbf{\\Sigma}\\)).\nposterior variance accounts prior uncertainty (\\(\\mathbf{B}\\)) data uncertainty (\\(\\mathbf{\\Sigma}\\)).Thus, Bayesian posterior mean \\(\\mathbf{b}\\) coincides BLUP predictor:\\[\nE(\\mathbf{b}|\\mathbf{Y}) = \\mathbf{BZ'V^{-1}(Y-X\\beta)}.\n\\]Interpretation Posterior DistributionPosterior Mean Shrinkage Estimator (BLUP)\nexpectation \\(E(\\mathbf{b}|\\mathbf{Y})\\) shrinks individual estimates toward population mean.\nSubjects less data variability estimates closer zero.\nsimilar Ridge Regression penalized estimation.\nexpectation \\(E(\\mathbf{b}|\\mathbf{Y})\\) shrinks individual estimates toward population mean.Subjects less data variability estimates closer zero.similar Ridge Regression penalized estimation.Posterior Variance Quantifies Uncertainty\nmatrix \\((\\mathbf{Z'\\Sigma^{-1}Z} + \\mathbf{B^{-1}})^{-1}\\) captures remaining uncertainty \\(\\mathbf{b}\\) seeing \\(\\mathbf{Y}\\).\n\\(\\mathbf{Z'\\Sigma^{-1}Z}\\) large, data provide strong information \\(\\mathbf{b}\\), reducing posterior variance.\n\\(\\mathbf{B^{-1}}\\) dominates, prior information heavily influences estimates.\nmatrix \\((\\mathbf{Z'\\Sigma^{-1}Z} + \\mathbf{B^{-1}})^{-1}\\) captures remaining uncertainty \\(\\mathbf{b}\\) seeing \\(\\mathbf{Y}\\).\\(\\mathbf{Z'\\Sigma^{-1}Z}\\) large, data provide strong information \\(\\mathbf{b}\\), reducing posterior variance.\\(\\mathbf{B^{-1}}\\) dominates, prior information heavily influences estimates.Connection Bayesian Inference\nrandom effects \\(\\mathbf{b}\\) follow Gaussian posterior due conjugacy.\nanalogous Bayesian hierarchical models, random effects latent variables estimated data.\nrandom effects \\(\\mathbf{b}\\) follow Gaussian posterior due conjugacy.analogous Bayesian hierarchical models, random effects latent variables estimated data.","code":""},{"path":"sec-linear-mixed-models.html","id":"estimating-the-variance-covariance-matrix","chapter":"8 Linear Mixed Models","heading":"8.2.4 Estimating the Variance-Covariance Matrix","text":"estimate \\(\\tilde{\\mathbf{V}}\\) \\(\\mathbf{V}\\), can estimate fixed random effects :\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'\\tilde{V}^{-1}X)^{-1}X'\\tilde{V}^{-1}Y}, \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{B Z' \\tilde{V}^{-1} (Y - X \\hat{\\beta})}.\n\\end{aligned}\n\\]:\\(\\hat{\\beta}\\) estimated fixed effects.\\(\\hat{\\mathbf{b}}\\) Empirical Best Linear Unbiased Predictor (EBLUP), also called Empirical Bayes estimate \\(\\mathbf{b}\\).Properties \\(\\hat{\\beta}\\) Variance EstimationConsistency: \\(\\hat{\\text{Var}}(\\hat{\\beta})\\) consistent estimator \\(\\text{Var}(\\hat{\\beta})\\) \\(\\tilde{\\mathbf{V}}\\) consistent estimator \\(\\mathbf{V}\\).Bias Issue: \\(\\hat{\\text{Var}}(\\hat{\\beta})\\) biased account uncertainty estimating \\(\\mathbf{V}\\).Implication: means \\(\\hat{\\text{Var}}(\\hat{\\beta})\\) underestimates true variability.estimate \\(\\mathbf{V}\\), several approaches can used:Maximum Likelihood Estimation (MLE)Restricted Maximum Likelihood (REML)Estimated Generalized Least Squares (EGLS)Bayesian Hierarchical Models (BHM)","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-maximum-likelihood-estimation-lmm","chapter":"8 Linear Mixed Models","heading":"8.2.4.1 Maximum Likelihood Estimation","text":"MLE finds parameter estimates maximizing likelihood function.Define parameter vector \\(\\theta\\) includes unknown variance components \\(\\mathbf{\\Sigma}\\) \\(\\mathbf{B}\\). , assume:\\[\n\\mathbf{Y} \\sim N(\\mathbf{X\\beta}, \\mathbf{V}(\\theta)).\n\\]log-likelihood function (ignoring constant terms) :\\[\n-2\\log L(\\mathbf{y}; \\theta, \\beta) =\n\\log |\\mathbf{V}(\\theta)| + (\\mathbf{Y - X\\beta})' \\mathbf{V}(\\theta)^{-1} (\\mathbf{Y - X\\beta}).\n\\]Steps MLE EstimationEstimate \\(\\hat{\\beta}\\), assuming \\(\\theta\\) known:\n\\[\n\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\theta)^{-1}X})^{-1} \\mathbf{X'V(\\theta)^{-1}Y}.\n\\]Estimate \\(\\hat{\\beta}\\), assuming \\(\\theta\\) known:\\[\n\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\theta)^{-1}X})^{-1} \\mathbf{X'V(\\theta)^{-1}Y}.\n\\]Obtain \\(\\hat{\\theta}_{MLE}\\) maximizing log-likelihood:\n\\[\n\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} -2\\log L(\\mathbf{y}; \\theta, \\beta).\n\\]Obtain \\(\\hat{\\theta}_{MLE}\\) maximizing log-likelihood:\\[\n\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} -2\\log L(\\mathbf{y}; \\theta, \\beta).\n\\]Substitute \\(\\hat{\\theta}_{MLE}\\) get updated estimates:\n\\[\n\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\hat{\\theta}_{MLE})^{-1}X})^{-1} \\mathbf{X'V(\\hat{\\theta}_{MLE})^{-1}Y}.\n\\]Substitute \\(\\hat{\\theta}_{MLE}\\) get updated estimates:\\[\n\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\hat{\\theta}_{MLE})^{-1}X})^{-1} \\mathbf{X'V(\\hat{\\theta}_{MLE})^{-1}Y}.\n\\]Predict random effects:\n\\[\n\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B}(\\hat{\\theta}_{MLE}) \\mathbf{Z'V}(\\hat{\\theta}_{MLE})^{-1} (\\mathbf{Y - X \\hat{\\beta}_{MLE}}).\n\\]Predict random effects:\\[\n\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B}(\\hat{\\theta}_{MLE}) \\mathbf{Z'V}(\\hat{\\theta}_{MLE})^{-1} (\\mathbf{Y - X \\hat{\\beta}_{MLE}}).\n\\]Key Observations MLEMLE tends underestimate \\(\\theta\\) account estimation fixed effects.Bias variance estimates can corrected using REML.","code":""},{"path":"sec-linear-mixed-models.html","id":"restricted-maximum-likelihood-lmm","chapter":"8 Linear Mixed Models","heading":"8.2.4.2 Restricted Maximum Likelihood","text":"Restricted Maximum Likelihood (REML) estimation method improves upon Maximum Likelihood Estimation accounting loss degrees freedom due estimation fixed effects.Unlike MLE, estimates fixed effects (\\(\\beta\\)) variance components (\\(\\theta\\)) simultaneously, REML focuses estimating variance components considering linear combinations data independent fixed effects.Consider Linear Mixed Model:\\[\n\\mathbf{y} = \\mathbf{X} \\beta + \\mathbf{Z} \\mathbf{b} + \\epsilon,\n\\]:\\(\\mathbf{y}\\): Response vector length \\(N\\)\\(\\mathbf{X}\\): Design matrix fixed effects (\\(N \\times p\\))\\(\\beta\\): Fixed effects parameter vector (\\(p \\times 1\\))\\(\\mathbf{Z}\\): Design matrix random effects\\(\\mathbf{b} \\sim N(\\mathbf{0, D})\\): Random effects\\(\\epsilon \\sim N(\\mathbf{0, \\Sigma})\\): Residual errorsThe marginal distribution \\(\\mathbf{y}\\) :\\[\n\\mathbf{y} \\sim N(\\mathbf{X} \\beta, \\mathbf{V}(\\theta)),\n\\]:\\[\n\\mathbf{V}(\\theta) = \\mathbf{Z D Z'} + \\mathbf{\\Sigma}.\n\\]eliminate dependence \\(\\beta\\), consider linear transformations \\(\\mathbf{y}\\) orthogonal fixed effects.Let \\(\\mathbf{K}\\) full-rank contrast matrix size \\(N \\times (N - p)\\) :\\[\n\\mathbf{K}' \\mathbf{X} = 0.\n\\], consider transformed data:\\[\n\\mathbf{K}' \\mathbf{y} \\sim N(\\mathbf{0}, \\mathbf{K}' \\mathbf{V}(\\theta) \\mathbf{K}).\n\\]transformation removes \\(\\beta\\) likelihood, focusing solely variance components \\(\\theta\\).Importantly, choice \\(\\mathbf{K}\\) affect final REML estimates.REML log-likelihood :\\[\n-2 \\log L_{REML}(\\theta) = \\log |\\mathbf{K}' \\mathbf{V}(\\theta) \\mathbf{K}| + \\mathbf{y}' \\mathbf{K} (\\mathbf{K}' \\mathbf{V}(\\theta) \\mathbf{K})^{-1} \\mathbf{K}' \\mathbf{y}.\n\\]equivalent form REML log-likelihood, avoiding explicit use \\(\\mathbf{K}\\), :\\[\n-2 \\log L_{REML}(\\theta) = \\log |\\mathbf{V}(\\theta)| + \\log |\\mathbf{X}' \\mathbf{V}(\\theta)^{-1} \\mathbf{X}| + (\\mathbf{y} - \\mathbf{X} \\hat{\\beta})' \\mathbf{V}(\\theta)^{-1} (\\mathbf{y} - \\mathbf{X} \\hat{\\beta}),\n\\]:\\[\n\\hat{\\beta} = (\\mathbf{X}' \\mathbf{V}(\\theta)^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{V}(\\theta)^{-1} \\mathbf{y}.\n\\]form highlights REML adjusts estimation fixed effects via second term \\(\\log |\\mathbf{X}' \\mathbf{V}^{-1} \\mathbf{X}|\\).Steps REML EstimationTransform data using \\(\\mathbf{K}' \\mathbf{y}\\) remove \\(\\beta\\) likelihood.Transform data using \\(\\mathbf{K}' \\mathbf{y}\\) remove \\(\\beta\\) likelihood.Maximize restricted likelihood estimate \\(\\hat{\\theta}_{REML}\\).Maximize restricted likelihood estimate \\(\\hat{\\theta}_{REML}\\).Estimate fixed effects using:\n\\[\n\\hat{\\beta}_{REML} = (\\mathbf{X}' \\mathbf{V}(\\hat{\\theta}_{REML})^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{V}(\\hat{\\theta}_{REML})^{-1} \\mathbf{y}.\n\\]Estimate fixed effects using:\\[\n\\hat{\\beta}_{REML} = (\\mathbf{X}' \\mathbf{V}(\\hat{\\theta}_{REML})^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{V}(\\hat{\\theta}_{REML})^{-1} \\mathbf{y}.\n\\]Properties REMLUnbiased Variance Component Estimates: REML produces unbiased estimates variance components accounting degrees freedom used estimate fixed effects.Invariance Fixed Effects: restricted likelihood constructed independent fixed effects \\(\\beta\\).Asymptotic Normality: REML estimates consistent asymptotically normal standard regularity conditions.Efficiency: REML estimates variance components efficiently, maximize joint likelihood parameters, \\(\\beta\\) estimates slightly less efficient compared MLE.Comparison REML MLE","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-estimated-generalized-least-squares-lmm","chapter":"8 Linear Mixed Models","heading":"8.2.4.3 Estimated Generalized Least Squares","text":"MLE REML rely Gaussian assumption, may always hold.\nEGLS provides alternative relying first two moments (mean variance).LMM framework :\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i.\n\\]:Random effects: \\(\\mathbf{b}_i \\sim N(\\mathbf{0, D})\\).Residual errors: \\(\\epsilon_i \\sim N(\\mathbf{0, \\Sigma_i})\\).Independence assumption: \\(\\text{Cov}(\\epsilon_i, \\mathbf{b}_i) = 0\\).Thus, first two moments :\\[\nE(\\mathbf{Y}_i) = \\mathbf{X}_i \\beta, \\quad \\text{Var}(\\mathbf{Y}_i) = \\mathbf{V}_i.\n\\]EGLS estimator :\\[\n\\hat{\\beta}_{GLS} = \\left\\{ \\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}X_i}  \\right\\}^{-1}\n\\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}Y_i}.\n\\]Writing matrix form:\\[\n\\hat{\\beta}_{GLS} = \\left\\{ \\mathbf{X'V(\\theta)^{-1}X} \\right\\}^{-1} \\mathbf{X'V(\\theta)^{-1}Y}.\n\\]Since \\(\\mathbf{V}(\\theta)\\) unknown, estimate \\(\\hat{\\mathbf{V}}\\), leading EGLS estimator:\\[\n\\hat{\\beta}_{EGLS} = \\left\\{ \\mathbf{X'\\hat{V}^{-1}X} \\right\\}^{-1} \\mathbf{X'\\hat{V}^{-1}Y}.\n\\]Key Insights EGLSComputational Simplicity:\nEGLS require iterative maximization likelihood function, making computationally attractive.\nEGLS require iterative maximization likelihood function, making computationally attractive.Form MLE/REML:\nfixed effects estimators MLE, REML, EGLS form, differing \\(\\mathbf{V}\\) estimated.\nfixed effects estimators MLE, REML, EGLS form, differing \\(\\mathbf{V}\\) estimated.Robust Non-Gaussian Data:\nSince depends first second moments, can handle cases MLE REML struggle non-normality.\nSince depends first second moments, can handle cases MLE REML struggle non-normality.Use EGLS?normality assumption MLE/REML questionable.\\(\\mathbf{V}\\) can estimated efficiently without requiring complex optimization.non-iterative approaches, computational simplicity priority.","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-bayesian-hierarchical-models-lmm","chapter":"8 Linear Mixed Models","heading":"8.2.4.4 Bayesian Hierarchical Models","text":"Bayesian methods offer fully probabilistic framework estimate \\(\\mathbf{V}\\) incorporating prior distributions.joint distribution can decomposed hierarchically:\\[\nf(, B, C) = f(| B, C) f(B | C) f(C).\n\\]Applying LMMs:\\[\n\\begin{aligned}\nf(\\mathbf{Y, \\beta, b, \\theta}) &= f(\\mathbf{Y | \\beta, b, \\theta}) f(\\mathbf{b | \\theta, \\beta}) f(\\mathbf{\\beta | \\theta}) f(\\mathbf{\\theta}) \\\\\n&= f(\\mathbf{Y | \\beta, b, \\theta}) f(\\mathbf{b | \\theta}) f(\\mathbf{\\beta}) f(\\mathbf{\\theta}).\n\\end{aligned}\n\\]:first equality follows probability decomposition.second equality assumes conditional independence, meaning:\nGiven \\(\\theta\\), additional information \\(\\mathbf{b}\\) obtained knowing \\(\\beta\\).\nGiven \\(\\theta\\), additional information \\(\\mathbf{b}\\) obtained knowing \\(\\beta\\).Using Bayes’ theorem, posterior distribution :\\[\nf(\\mathbf{\\beta, b, \\theta | Y}) \\propto f(\\mathbf{Y | \\beta, b, \\theta}) f(\\mathbf{b | \\theta}) f(\\mathbf{\\beta}) f(\\mathbf{\\theta}).\n\\]:\\[\n\\begin{aligned}\n\\mathbf{Y | \\beta, b, \\theta} &\\sim N(\\mathbf{X\\beta + Zb}, \\mathbf{\\Sigma(\\theta)}), \\\\\n\\mathbf{b | \\theta} &\\sim N(\\mathbf{0, B(\\theta)}).\n\\end{aligned}\n\\]complete Bayesian model, specify prior distributions:\\(f(\\beta)\\): Prior fixed effects.\\(f(\\theta)\\): Prior variance components.Since analytical solutions generally unavailable, use Markov Chain Monte Carlo (MCMC) sample posterior:Gibbs sampling (conjugate priors used).Hamiltonian Monte Carlo (HMC) (complex models).AdvantagesAccounts Parameter Uncertainty\nUnlike MLE/REML, Bayesian methods propagate uncertainty variance component estimation.\nUnlike MLE/REML, Bayesian methods propagate uncertainty variance component estimation.Flexible Model Specification\nCan incorporate prior knowledge via informative priors.\nExtends naturally beyond Gaussian assumptions (e.g., Student-\\(t\\) distributions heavy-tailed errors).\nCan incorporate prior knowledge via informative priors.Extends naturally beyond Gaussian assumptions (e.g., Student-\\(t\\) distributions heavy-tailed errors).Robustness Small Samples\nBayesian methods can stabilize variance estimation small datasets MLE/REML unreliable.\nBayesian methods can stabilize variance estimation small datasets MLE/REML unreliable.ChallengesComputational Complexity\nRequires MCMC algorithms, can computationally expensive.\nRequires MCMC algorithms, can computationally expensive.Convergence Issues\nMCMC chains must checked convergence (e.g., using R-hat diagnostic).\nMCMC chains must checked convergence (e.g., using R-hat diagnostic).Choice Priors\nPoorly chosen priors can bias estimates slow convergence.\nPoorly chosen priors can bias estimates slow convergence.Comparison Estimation Methods \\(\\mathbf{V}\\)","code":""},{"path":"sec-linear-mixed-models.html","id":"inference-in-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.3 Inference in Linear Mixed Models","text":"","code":""},{"path":"sec-linear-mixed-models.html","id":"inference-for-fixed-effects-beta","chapter":"8 Linear Mixed Models","heading":"8.3.1 Inference for Fixed Effects (\\(\\beta\\))","text":"goal test hypotheses fixed effects parameters \\(\\beta\\) using various statistical tests:Wald TestWald TestF-TestF-TestLikelihood Ratio TestLikelihood Ratio Test","code":""},{"path":"sec-linear-mixed-models.html","id":"wald-test-lmm","chapter":"8 Linear Mixed Models","heading":"8.3.1.1 Wald Test","text":"Wald test assesses whether certain linear combinations fixed effects equal specified values.Given:\\[\n\\hat{\\beta}(\\theta) = \\left( \\mathbf{X}' \\mathbf{V}^{-1}(\\theta) \\mathbf{X} \\right)^{-1} \\mathbf{X}' \\mathbf{V}^{-1}(\\theta) \\mathbf{Y},\n\\]variance:\\[\n\\text{Var}(\\hat{\\beta}(\\theta)) = \\left( \\mathbf{X}' \\mathbf{V}^{-1}(\\theta) \\mathbf{X} \\right)^{-1}.\n\\]practice, substitute \\(\\hat{\\theta}\\) (estimate \\(\\theta\\)) obtain:Hypotheses:\n\\[\nH_0: \\mathbf{\\beta} = \\mathbf{d}\n\\]\n:\n\\(\\mathbf{}\\) contrast matrix specifying linear combinations \\(\\beta\\).\n\\(\\mathbf{d}\\) constant vector representing null hypothesis values.\nHypotheses:\\[\nH_0: \\mathbf{\\beta} = \\mathbf{d}\n\\]:\\(\\mathbf{}\\) contrast matrix specifying linear combinations \\(\\beta\\).\\(\\mathbf{d}\\) constant vector representing null hypothesis values.Wald Test Statistic:\n\\[\nW = (\\mathbf{} \\hat{\\beta} - \\mathbf{d})' \\left[ \\mathbf{} \\left( \\mathbf{X}' \\hat{\\mathbf{V}}^{-1} \\mathbf{X} \\right)^{-1} \\mathbf{}' \\right]^{-1} (\\mathbf{} \\hat{\\beta} - \\mathbf{d}).\n\\]Wald Test Statistic:\\[\nW = (\\mathbf{} \\hat{\\beta} - \\mathbf{d})' \\left[ \\mathbf{} \\left( \\mathbf{X}' \\hat{\\mathbf{V}}^{-1} \\mathbf{X} \\right)^{-1} \\mathbf{}' \\right]^{-1} (\\mathbf{} \\hat{\\beta} - \\mathbf{d}).\n\\]Distribution \\(H_0\\):\n\\[\nW \\sim \\chi^2_{\\text{rank}(\\mathbf{})}.\n\\]Distribution \\(H_0\\):\\[\nW \\sim \\chi^2_{\\text{rank}(\\mathbf{})}.\n\\]Caution Wald Test:Underestimation Variance:\nWald test ignores variability estimating \\(\\hat{\\theta}\\), leading underestimated standard errors potentially inflated Type error rates.Small Sample Issues:\nLess reliable small samples variance components near boundary values (e.g., variances close zero).","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-f-test-lmm","chapter":"8 Linear Mixed Models","heading":"8.3.1.2 F-Test","text":"alternative Wald test, F-test adjusts estimation \\(\\sigma^2\\) provides better performance small samples.Assume:\\[\n\\text{Var}(\\mathbf{Y}) = \\sigma^2 \\mathbf{V}(\\theta).\n\\]F-statistic :\\[\nF^* = \\frac{(\\mathbf{} \\hat{\\beta} - \\mathbf{d})' \\left[ \\mathbf{} \\left( \\mathbf{X}' \\hat{\\mathbf{V}}^{-1} \\mathbf{X} \\right)^{-1} \\mathbf{}' \\right]^{-1} (\\mathbf{} \\hat{\\beta} - \\mathbf{d})}{\\hat{\\sigma}^2 \\, \\text{rank}(\\mathbf{})}.\n\\]Distribution \\(H_0\\):\n\\[\nF^* \\sim F_{\\text{rank}(\\mathbf{}), \\, \\text{df}_{\\text{denominator}}}.\n\\]Distribution \\(H_0\\):\\[\nF^* \\sim F_{\\text{rank}(\\mathbf{}), \\, \\text{df}_{\\text{denominator}}}.\n\\]Approximating Denominator Degrees Freedom:\nSatterthwaite approximation\nKenward-Roger approximation (provides bias-corrected standard errors)\nApproximating Denominator Degrees Freedom:Satterthwaite approximationKenward-Roger approximation (provides bias-corrected standard errors)F-Test Advantages:accurate small samples compared Wald test.Adjusts variance estimation, reducing bias hypothesis testing.Wald Test vs. F-Test:","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-likelihood-ratio-test-lmm","chapter":"8 Linear Mixed Models","heading":"8.3.1.3 Likelihood Ratio Test","text":"Likelihood Ratio Test (LRT) compares fit nested models:Null Hypothesis:\n\\[\nH_0: \\beta \\\\Theta_{\\beta,0}\n\\]\n\\(\\Theta_{\\beta,0}\\) subset full parameter space \\(\\Theta_{\\beta}\\).Null Hypothesis:\\[\nH_0: \\beta \\\\Theta_{\\beta,0}\n\\]\\(\\Theta_{\\beta,0}\\) subset full parameter space \\(\\Theta_{\\beta}\\).Test Statistic:\n\\[\n-2 \\log \\lambda = -2 \\log \\left( \\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}} \\right),\n\\]\n:\n\\(\\hat{L}_{ML,0}\\) = Maximized likelihood \\(H_0\\) (restricted model)\n\\(\\hat{L}_{ML}\\) = Maximized likelihood alternative (full model)\nTest Statistic:\\[\n-2 \\log \\lambda = -2 \\log \\left( \\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}} \\right),\n\\]:\\(\\hat{L}_{ML,0}\\) = Maximized likelihood \\(H_0\\) (restricted model)\\(\\hat{L}_{ML}\\) = Maximized likelihood alternative (full model)Distribution \\(H_0\\):\n\\[\n-2 \\log \\lambda \\sim \\chi^2_{df}\n\\]\n\\(df = \\dim(\\Theta_{\\beta}) - \\dim(\\Theta_{\\beta,0})\\) (difference number parameters).Distribution \\(H_0\\):\\[\n-2 \\log \\lambda \\sim \\chi^2_{df}\n\\]\\(df = \\dim(\\Theta_{\\beta}) - \\dim(\\Theta_{\\beta,0})\\) (difference number parameters).Important Notes:LRT applicable ML estimates (REML) comparing models different fixed effects.REML-based LRT can used comparing models differ random effects (variance components), fixed effects.","code":""},{"path":"sec-linear-mixed-models.html","id":"inference-for-variance-components-theta","chapter":"8 Linear Mixed Models","heading":"8.3.2 Inference for Variance Components (\\(\\theta\\))","text":"ML REML estimators:\\[\n\\hat{\\theta} \\sim N(\\theta, (\\theta)^{-1}),\n\\]\\((\\theta)\\) Fisher Information Matrix.normal approximation holds well large samples, enabling Wald-type tests confidence intervals.","code":""},{"path":"sec-linear-mixed-models.html","id":"wald-test-for-variance-components","chapter":"8 Linear Mixed Models","heading":"8.3.2.1 Wald Test for Variance Components","text":"Wald test variance components follows structure fixed effects:Test Statistic:\n\\[\nW = \\frac{(\\hat{\\theta} - \\theta_0)^2}{\\widehat{\\text{Var}}(\\hat{\\theta})}.\n\\]Test Statistic:\\[\nW = \\frac{(\\hat{\\theta} - \\theta_0)^2}{\\widehat{\\text{Var}}(\\hat{\\theta})}.\n\\]Distribution \\(H_0\\):\n\\[\nW \\sim \\chi^2_1.\n\\]Distribution \\(H_0\\):\\[\nW \\sim \\chi^2_1.\n\\]Limitations Wald Test Variance Components:Boundary Issues: normal approximation fails true variance component near zero (boundary parameter space).Less reliable variance parameters covariance parameters.","code":""},{"path":"sec-linear-mixed-models.html","id":"likelihood-ratio-test-for-variance-components","chapter":"8 Linear Mixed Models","heading":"8.3.2.2 Likelihood Ratio Test for Variance Components","text":"LRT can also applied variance components:Test Statistic:\n\\[\n-2 \\log \\lambda = -2 \\log \\left( \\frac{\\hat{L}_{REML,0}}{\\hat{L}_{REML}} \\right).\n\\]Test Statistic:\\[\n-2 \\log \\lambda = -2 \\log \\left( \\frac{\\hat{L}_{REML,0}}{\\hat{L}_{REML}} \\right).\n\\]Distribution \\(H_0\\):\nalways \\(\\chi^2\\)-distributed variance components boundary (e.g., testing \\(\\sigma^2 = 0\\)).\nMay require mixture distributions adjusted critical values.\nDistribution \\(H_0\\):always \\(\\chi^2\\)-distributed variance components boundary (e.g., testing \\(\\sigma^2 = 0\\)).May require mixture distributions adjusted critical values.","code":""},{"path":"sec-linear-mixed-models.html","id":"information-criteria-for-model-selection","chapter":"8 Linear Mixed Models","heading":"8.4 Information Criteria for Model Selection","text":"Information Criteria statistical tools used compare competing models balancing model fit (likelihood) model complexity (number parameters).\nhelp identifying parsimonious model adequately explains data without overfitting.three commonly used criteria :Akaike Information Criterion (AIC)Corrected Akaike Information Criterion (AICc)Bayesian Information Criterion (BIC)","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-akaike-information-criterion-lmm","chapter":"8 Linear Mixed Models","heading":"8.4.1 Akaike Information Criterion","text":"Akaike Information Criterion derived Kullback-Leibler divergence, measures difference true data-generating process fitted model.AIC Formula:\\[\nAIC = -2 \\, l(\\hat{\\theta}, \\hat{\\beta}) + 2q\n\\]:\\(l(\\hat{\\theta}, \\hat{\\beta})\\): maximized log-likelihood model, evaluated estimates \\(\\hat{\\theta}\\) (variance components) \\(\\hat{\\beta}\\) (fixed effects).\\(q\\): effective number parameters, including:\nnumber fixed effects.\nnumber variance-covariance parameters (random effects).\nExcludes parameters constrained boundary values (e.g., variances estimated zero).\nnumber fixed effects.number variance-covariance parameters (random effects).Excludes parameters constrained boundary values (e.g., variances estimated zero).Key Points AICModel Selection Rule:\nLower AIC indicates better model.\nOccasionally, software may report AIC \\(l - q\\), case higher AIC better (rare).\nLower AIC indicates better model.Occasionally, software may report AIC \\(l - q\\), case higher AIC better (rare).Comparing Random Effects Models:\nrecommended comparing models different random effects ’s difficult accurately count effective number parameters.\nrecommended comparing models different random effects ’s difficult accurately count effective number parameters.Sample Size Considerations:\nRequires large sample sizes reliable comparisons.\nsmall samples, AIC tends favor complex models due insufficient penalty model complexity.\nRequires large sample sizes reliable comparisons.small samples, AIC tends favor complex models due insufficient penalty model complexity.Potential Bias:\nCan negatively biased (.e., favoring overly complex models) sample size small relative number parameters.\nCan negatively biased (.e., favoring overly complex models) sample size small relative number parameters.Use AIC:Comparing models random effects structure different fixed effects.Selecting covariance structures mixed models sample size large.","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-corrected-aic-lmm","chapter":"8 Linear Mixed Models","heading":"8.4.2 Corrected AIC","text":"Corrected AIC (AICc) addresses bias AIC small sample sizes. developed (Hurvich Tsai 1989).AICc Formula:\\[\nAICc = AIC + \\frac{2q(q + 1)}{n - q - 1}\n\\]:\\(n\\): sample size.\\(q\\): number estimated parameters.Key Points AICcSmall Sample Correction:\nProvides stronger penalty model complexity sample size small.\nProvides stronger penalty model complexity sample size small.Applicability:\nValid comparing models fixed covariance structures.\nrecommended models general covariance structures due difficulties bias correction.\nValid comparing models fixed covariance structures.recommended models general covariance structures due difficulties bias correction.Model Selection Rule:\nLower AICc indicates better model.\nLower AICc indicates better model.Use AICc:Small sample sizes (\\(n/q\\) ratio low).Models fixed random effects simple covariance structures.","code":""},{"path":"sec-linear-mixed-models.html","id":"sec-bayesian-information-criterion-lmm","chapter":"8 Linear Mixed Models","heading":"8.4.3 Bayesian Information Criterion","text":"Bayesian Information Criterion derived Bayesian framework incorporates stronger penalty model complexity compared AIC.BIC Formula\\[\nBIC = -2 \\, l(\\hat{\\theta}, \\hat{\\beta}) + q \\log(n)\n\\]:\\(n\\): number observations.\\(q\\): number effective parameters.\\(l(\\hat{\\theta}, \\hat{\\beta})\\): maximized log-likelihood.Key Points BICModel Selection Rule:\nLower BIC indicates better model.\nLower BIC indicates better model.Stronger Penalty:\npenalty term \\(q \\log(n)\\) grows sample size, leading BIC favor simpler models AIC.\npenalty term \\(q \\log(n)\\) grows sample size, leading BIC favor simpler models AIC.Applicability MLE REML:\nBIC can used MLE REML, :\nUse MLE comparing models different fixed effects.\nUse REML comparing models different random effects (fixed effects).\n\nBIC can used MLE REML, :\nUse MLE comparing models different fixed effects.\nUse REML comparing models different random effects (fixed effects).\nUse MLE comparing models different fixed effects.Use REML comparing models different random effects (fixed effects).Consistency:\nBIC consistent, meaning sample size increases, select true model probability 1 (true model among candidates).\nBIC consistent, meaning sample size increases, select true model probability 1 (true model among candidates).Use BIC:Large sample sizes model simplicity prioritized.Model selection hypothesis testing (due connection Bayesian inference).Comparison AIC, AICc, BICKey TakeawaysAIC suitable large datasets general model comparisons may favor overly complex models small samples.AICc corrects AIC’s bias small sample sizes.BIC favors simpler models, especially sample size increases, making suitable hypothesis testing situations parsimony essential.Use MLE comparing models different fixed effects, REML comparing models different random effects (fixed effects).comparing random effects structures, AIC BIC may reliable due difficulty counting effective parameters accurately.","code":""},{"path":"sec-linear-mixed-models.html","id":"practical-example-with-linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.4.4 Practical Example with Linear Mixed Models","text":"Consider Linear Mixed Model:\\[\nY_{ik} =\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + b_{2i}) t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + b_{2i}) t_{ij} + \\epsilon_{ij} & H \\\\\n\\beta_0 + b_{1i} + (\\beta_3 + b_{2i}) t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]:\\(= 1, \\dots, N\\) (subjects)\\(j = 1, \\dots, n_i\\) (repeated measures time \\(t_{ij}\\))\\[\n\\begin{aligned}\n\\mathbf{Y}_i | b_i &\\sim N(\\mathbf{X}_i \\beta + \\mathbf{1} b_i, \\sigma^2 \\mathbf{}) \\\\\nb_i &\\sim N(0, d_{11})\n\\end{aligned}\n\\]aim estimate:Fixed effects: \\(\\beta\\)Variance components: \\(\\sigma^2\\), \\(d_{11}\\)Random effects: Predict \\(b_i\\)comparing models (e.g., different random slopes covariance structures), can compute:AIC: Penalizes model complexity \\(2q\\).BIC: Stronger penalty via \\(q \\log(n)\\), favoring simpler models.AICc: Adjusted AIC small sample sizes.Interpretation Results:Model 2 (random intercepts slopes) lowest AIC, BIC, AICc, indicating best fit among models.Model 2 (random intercepts slopes) lowest AIC, BIC, AICc, indicating best fit among models.Model 1 (random intercepts ) performs worse, suggesting allowing random slopes improves model fit.Model 1 (random intercepts ) performs worse, suggesting allowing random slopes improves model fit.Model 3 (simpler fixed effects without interaction) highest AIC/BIC/AICc, indicating poor fit compared Models 1 2.Model 3 (simpler fixed effects without interaction) highest AIC/BIC/AICc, indicating poor fit compared Models 1 2.Model Selection Criteria:","code":"\n# Practical Example with Linear Mixed Models in R\n\n\n# Load required libraries\nlibrary(lme4)     # For fitting linear mixed-effects models\nlibrary(MuMIn)    # For calculating AICc\nlibrary(dplyr)    # For data manipulation\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate Data\nN <- 50             # Number of subjects\nn_i <- 5            # Number of repeated measures per subject\nt_ij <- rep(1:n_i, N)  # Time points\n\n# Treatment groups (L, H, C)\ntreatment <- rep(c(\"L\", \"H\", \"C\"), length.out = N)\ngroup <- factor(rep(treatment, each = n_i))\n\n# Simulate random effects\nb1_i <- rnorm(N, mean = 0, sd = 2)    # Random intercepts\nb2_i <- rnorm(N, mean = 0, sd = 1)    # Random slopes\n\n# Fixed effects\nbeta_0 <- 5\nbeta_1 <- 0.5\nbeta_2 <- 1\nbeta_3 <- 1.5\n\n# Generate response variable Y based on the specified model\nY <- numeric(N * n_i)\nsubject_id <- rep(1:N, each = n_i)\n\nfor (i in 1:N) {\n    for (j in 1:n_i) {\n        idx <- (i - 1) * n_i + j\n        time <- t_ij[idx]\n        \n        # Treatment-specific model\n        if (group[idx] == \"L\") {\n            Y[idx] <-\n                beta_0 + b1_i[i] + (beta_1 + b2_i[i]) * time + rnorm(1, 0, 1)\n        } else if (group[idx] == \"H\") {\n            Y[idx] <-\n                beta_0 + b1_i[i] + (beta_2 + b2_i[i]) * time + rnorm(1, 0, 1)\n        } else {\n            Y[idx] <-\n                beta_0 + b1_i[i] + (beta_3 + b2_i[i]) * time + rnorm(1, 0, 1)\n        }\n    }\n}\n\n# Combine into a data frame\ndata <- data.frame(\n    Y = Y,\n    time = t_ij,\n    group = group,\n    subject = factor(subject_id)\n)\n\n# Fit Linear Mixed Models\n# Model 1: Random Intercepts Only\nmodel1 <-\n    lmer(Y ~ time * group + (1 | subject), data = data, REML = FALSE)\n\n# Model 2: Random Intercepts and Random Slopes\nmodel2 <-\n    lmer(Y ~ time * group + (1 + time |\n                                 subject),\n         data = data,\n         REML = FALSE)\n\n# Model 3: Simpler Model (No Interaction)\nmodel3 <-\n    lmer(Y ~ time + group + (1 | subject), data = data, REML = FALSE)\n\n# Extract Information Criteria\nresults <- data.frame(\n    Model = c(\n        \"Random Intercepts\",\n        \"Random Intercepts + Slopes\",\n        \"No Interaction\"\n    ),\n    AIC = c(AIC(model1), AIC(model2), AIC(model3)),\n    BIC = c(BIC(model1), BIC(model2), BIC(model3)),\n    AICc = c(AICc(model1), AICc(model2), AICc(model3))\n)\n\n# Display the results\nprint(results)\n#>                        Model       AIC      BIC      AICc\n#> 1          Random Intercepts 1129.2064 1157.378 1129.8039\n#> 2 Random Intercepts + Slopes  974.5514 1009.766  975.4719\n#> 3             No Interaction 1164.2797 1185.408 1164.6253"},{"path":"sec-linear-mixed-models.html","id":"split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5 Split-Plot Designs","text":"Split-plot designs commonly used experimental settings two factors, least one requires larger experimental units compared (s). situation often arises agricultural, industrial, business experiments certain treatments harder expensive apply.Key CharacteristicsTwo factors different experimental unit requirements:\nFactor (Whole-plot factor): Requires large experimental units (e.g., different fields, production batches).\nFactor B (Sub-plot factor): Can applied smaller units within larger experimental units (e.g., plots within fields, products within batches).\nFactor (Whole-plot factor): Requires large experimental units (e.g., different fields, production batches).Factor B (Sub-plot factor): Can applied smaller units within larger experimental units (e.g., plots within fields, products within batches).Blocking: experiment typically divided blocks (replicates) account variability. However, unlike Randomized Block Designs, randomization process split-plot designs occurs two levels:\nWhole-plot randomization: Factor randomized across large units within block.\nSub-plot randomization: Factor B randomized within whole plot.\nWhole-plot randomization: Factor randomized across large units within block.Sub-plot randomization: Factor B randomized within whole plot.","code":""},{"path":"sec-linear-mixed-models.html","id":"example-setup","chapter":"8 Linear Mixed Models","heading":"8.5.1 Example Setup","text":"Factor : 3 levels (applied large units).Factor B: 2 levels (applied within large units).4 Blocks (replicates): containing combinations B.Unlike Randomized Block Designs, randomization Factor restricted due larger unit size. Factor applied per block, Factor B can applied multiple times within block.","code":""},{"path":"sec-linear-mixed-models.html","id":"statistical-model-for-split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5.2 Statistical Model for Split-Plot Designs","text":"Factor Primary Focus (Whole-Plot Analysis)\\[\nY_{ij} = \\mu + \\rho_i + \\alpha_j + e_{ij}\n\\]:\\(Y_{ij}\\) = Response \\(j\\)-th level factor \\(\\)-th block.\\(Y_{ij}\\) = Response \\(j\\)-th level factor \\(\\)-th block.\\(\\mu\\) = Overall mean.\\(\\mu\\) = Overall mean.\\(\\rho_i\\) = Random effect \\(\\)-th block (\\(\\rho_i \\sim N(0, \\sigma^2_{\\rho})\\)).\\(\\rho_i\\) = Random effect \\(\\)-th block (\\(\\rho_i \\sim N(0, \\sigma^2_{\\rho})\\)).\\(\\alpha_j\\) = Fixed effect factor (main effect).\\(\\alpha_j\\) = Fixed effect factor (main effect).\\(e_{ij} \\sim N(0, \\sigma^2_{e})\\) = Whole-plot error (random), representing variability within blocks due factor .\\(e_{ij} \\sim N(0, \\sigma^2_{e})\\) = Whole-plot error (random), representing variability within blocks due factor .Factor B Primary Focus (Sub-Plot Analysis)\\[\nY_{ijk} = \\mu + \\phi_{ij} + \\beta_k + \\epsilon_{ijk}\n\\]:\\(Y_{ijk}\\) = Response \\(k\\)-th level factor B within \\(j\\)-th level factor \\(\\)-th block.\\(Y_{ijk}\\) = Response \\(k\\)-th level factor B within \\(j\\)-th level factor \\(\\)-th block.\\(\\phi_{ij}\\) = Combined effect block factor : \\[\n  \\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\n  \\]\\(\\phi_{ij}\\) = Combined effect block factor : \\[\n  \\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\n  \\]\\(\\beta_k\\) = Fixed effect factor B (main effect).\\(\\beta_k\\) = Fixed effect factor B (main effect).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_{\\epsilon})\\) = Sub-plot error (random), capturing variability within whole plots.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_{\\epsilon})\\) = Sub-plot error (random), capturing variability within whole plots.Full Split-Plot Model (Including Interaction)\\[\nY_{ijk} = \\mu + \\rho_i + \\alpha_j + e_{ij} + \\beta_k + (\\alpha \\beta)_{jk} + \\epsilon_{ijk}\n\\]:\\(\\) = Block (replication). - \\(j\\) = Level factor (whole-plot factor).\\(\\) = Block (replication). - \\(j\\) = Level factor (whole-plot factor).\\(k\\) = Level factor B (sub-plot factor).\\(k\\) = Level factor B (sub-plot factor).\\(\\mu\\) = Overall mean.\\(\\mu\\) = Overall mean.\\(\\rho_i\\) = Random effect \\(\\)-th block.\\(\\rho_i\\) = Random effect \\(\\)-th block.\\(\\alpha_j\\) = Fixed main effect factor .\\(\\alpha_j\\) = Fixed main effect factor .\\(e_{ij} \\sim N(0, \\sigma^2_{e})\\) = Whole-plot error (random).\\(e_{ij} \\sim N(0, \\sigma^2_{e})\\) = Whole-plot error (random).\\(\\beta_k\\) = Fixed main effect factor B.\\(\\beta_k\\) = Fixed main effect factor B.\\((\\alpha \\beta)_{jk}\\) = Fixed interaction effect factors B.\\((\\alpha \\beta)_{jk}\\) = Fixed interaction effect factors B.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_{\\epsilon})\\) = Sub-plot error (random).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_{\\epsilon})\\) = Sub-plot error (random).","code":""},{"path":"sec-linear-mixed-models.html","id":"approaches-to-analyzing-split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5.3 Approaches to Analyzing Split-Plot Designs","text":"ANOVA PerspectiveWhole-Plot Comparisons:Factor vs. Whole-Plot Error:\nCompare variation due factor (\\(\\alpha_j\\)) whole-plot error (\\(e_{ij}\\)).Blocks vs. Whole-Plot Error:\nCompare variation due blocks (\\(\\rho_i\\)) whole-plot error (\\(e_{ij}\\)).Sub-Plot Comparisons:Factor B vs. Sub-Plot Error:\nCompare variation due factor B (\\(\\beta_k\\)) sub-plot error (\\(\\epsilon_{ijk}\\)).Interaction (× B) vs. Sub-Plot Error:\nCompare interaction effect (\\((\\alpha \\beta)_{jk}\\)) sub-plot error (\\(\\epsilon_{ijk}\\)).Mixed Model PerspectiveA flexible approach treat split-plot designs using mixed-effects models, can handle fixed random effects explicitly:\\[\n\\mathbf{Y = X \\beta + Zb + \\epsilon}\n\\]:\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{X}\\) = Design matrix fixed effects (e.g., factors , B, interaction).\\(\\mathbf{X}\\) = Design matrix fixed effects (e.g., factors , B, interaction).\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients (e.g., \\(\\mu\\), \\(\\alpha_j\\), \\(\\beta_k\\), \\((\\alpha \\beta)_{jk}\\)).\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients (e.g., \\(\\mu\\), \\(\\alpha_j\\), \\(\\beta_k\\), \\((\\alpha \\beta)_{jk}\\)).\\(\\mathbf{Z}\\) = Design matrix random effects (e.g., blocks whole-plot errors).\\(\\mathbf{Z}\\) = Design matrix random effects (e.g., blocks whole-plot errors).\\(\\mathbf{b}\\) = Vector random-effect coefficients (e.g., \\(\\rho_i\\), \\(e_{ij}\\)).\\(\\mathbf{b}\\) = Vector random-effect coefficients (e.g., \\(\\rho_i\\), \\(e_{ij}\\)).\\(\\boldsymbol{\\epsilon}\\) = Vector residuals (sub-plot error).\\(\\boldsymbol{\\epsilon}\\) = Vector residuals (sub-plot error).Mixed models particularly useful :unbalanced designs (missing data).unbalanced designs (missing data).need account complex correlation structures within data.need account complex correlation structures within data.","code":""},{"path":"sec-linear-mixed-models.html","id":"application-split-plot-design","chapter":"8 Linear Mixed Models","heading":"8.5.4 Application: Split-Plot Design","text":"Consider agricultural experiment designed study effects irrigation crop variety yield. scenario well-suited split-plot design irrigation treatments applied large plots (fields), different crop varieties planted within plots.","code":""},{"path":"sec-linear-mixed-models.html","id":"model-specification","chapter":"8 Linear Mixed Models","heading":"8.5.4.1 Model Specification","text":"linear mixed-effects model defined :\\[\ny_{ijk} = \\mu + i_i + v_j + (iv)_{ij} + f_k + \\epsilon_{ijk}\n\\]:\\(y_{ijk}\\) = Observed yield \\(\\)-th irrigation, \\(j\\)-th variety, \\(k\\)-th field.\\(y_{ijk}\\) = Observed yield \\(\\)-th irrigation, \\(j\\)-th variety, \\(k\\)-th field.\\(\\mu\\) = Overall mean yield.\\(\\mu\\) = Overall mean yield.\\(i_i\\) = Fixed effect \\(\\)-th irrigation level.\\(i_i\\) = Fixed effect \\(\\)-th irrigation level.\\(v_j\\) = Fixed effect \\(j\\)-th crop variety.\\(v_j\\) = Fixed effect \\(j\\)-th crop variety.\\((iv)_{ij}\\) = Interaction effect irrigation variety (fixed).\\((iv)_{ij}\\) = Interaction effect irrigation variety (fixed).\\(f_k \\sim N(0, \\sigma^2_f)\\) = Random effect field (captures variability fields).\\(f_k \\sim N(0, \\sigma^2_f)\\) = Random effect field (captures variability fields).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = Residual error.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = Residual error.Note:\nSince variety-field combination observed , model random interaction variety field.","code":""},{"path":"sec-linear-mixed-models.html","id":"data-exploration","chapter":"8 Linear Mixed Models","heading":"8.5.4.2 Data Exploration","text":"plot helps visualize yield varies across fields, different irrigation treatments, different varieties.","code":"\nlibrary(ggplot2)\ndata(irrigation, package = \"faraway\")  # Load the dataset\n\n# Summary statistics and preview\nsummary(irrigation)\n#>      field   irrigation variety     yield      \n#>  f1     :2   i1:4       v1:8    Min.   :34.80  \n#>  f2     :2   i2:4       v2:8    1st Qu.:37.60  \n#>  f3     :2   i3:4               Median :40.15  \n#>  f4     :2   i4:4               Mean   :40.23  \n#>  f5     :2                      3rd Qu.:42.73  \n#>  f6     :2                      Max.   :47.60  \n#>  (Other):4\nhead(irrigation, 4)\n#>   field irrigation variety yield\n#> 1    f1         i1      v1  35.4\n#> 2    f1         i1      v2  37.9\n#> 3    f2         i2      v1  36.7\n#> 4    f2         i2      v2  38.2\n\n# Exploratory plot: Yield by field, colored by variety and shaped by irrigation\nggplot(irrigation,\n       aes(\n           x     = field,\n           y     = yield,\n           shape = irrigation,\n           color = variety\n       )) +\n    geom_point(size = 3) +\n    labs(title = \"Yield by Field, Irrigation, and Variety\",\n         x = \"Field\",\n         y = \"Yield\") +\n    theme_minimal()"},{"path":"sec-linear-mixed-models.html","id":"fitting-the-initial-mixed-effects-model","chapter":"8 Linear Mixed Models","heading":"8.5.4.3 Fitting the Initial Mixed-Effects Model","text":"fit mixed-effects model :Irrigation variety (interaction) fixed effects.Irrigation variety (interaction) fixed effects.Field modeled random effect account variability fields.Field modeled random effect account variability fields.Check p-value interaction term (irrigation:variety).\ninsignificant, suggests strong evidence interaction effect, may simplify model removing .\nCheck p-value interaction term (irrigation:variety).insignificant, suggests strong evidence interaction effect, may simplify model removing .","code":"\nlibrary(lmerTest)  # Provides p-values for lmer models\n\n# Full model with interaction term\nsp_model <-\n    lmer(yield ~ irrigation * variety + (1 | field), data = irrigation)\nsummary(sp_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ irrigation * variety + (1 | field)\n#>    Data: irrigation\n#> \n#> REML criterion at convergence: 45.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -0.7448 -0.5509  0.0000  0.5509  0.7448 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  field    (Intercept) 16.200   4.025   \n#>  Residual              2.107   1.452   \n#> Number of obs: 16, groups:  field, 8\n#> \n#> Fixed effects:\n#>                        Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***\n#> irrigationi2              1.200      4.279  4.487   0.280 0.791591    \n#> irrigationi3              0.700      4.279  4.487   0.164 0.877156    \n#> irrigationi4              3.500      4.279  4.487   0.818 0.454584    \n#> varietyv2                 0.600      1.452  4.000   0.413 0.700582    \n#> irrigationi2:varietyv2   -0.400      2.053  4.000  -0.195 0.855020    \n#> irrigationi3:varietyv2   -0.200      2.053  4.000  -0.097 0.927082    \n#> irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2\n#> irrigation2 -0.707                                          \n#> irrigation3 -0.707  0.500                                   \n#> irrigation4 -0.707  0.500  0.500                            \n#> varietyv2   -0.240  0.170  0.170  0.170                     \n#> irrgtn2:vr2  0.170 -0.240 -0.120 -0.120 -0.707              \n#> irrgtn3:vr2  0.170 -0.120 -0.240 -0.120 -0.707  0.500       \n#> irrgtn4:vr2  0.170 -0.120 -0.120 -0.240 -0.707  0.500  0.500\n\n# ANOVA table using Kenward-Roger approximation for accurate p-values\nanova(sp_model, ddf = \"Kenward-Roger\")\n#> Type III Analysis of Variance Table with Kenward-Roger's method\n#>                    Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> irrigation         2.4545 0.81818     3     4  0.3882 0.7685\n#> variety            2.2500 2.25000     1     4  1.0676 0.3599\n#> irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612"},{"path":"sec-linear-mixed-models.html","id":"model-simplification-testing-for-additivity","chapter":"8 Linear Mixed Models","heading":"8.5.4.4 Model Simplification: Testing for Additivity","text":"compare full model (interaction) additive model (without interaction):Hypotheses:\n\\(H_0\\): additive model (without interaction) fits data adequately.\n\\(H_a\\): interaction model provides significantly better fit.\nHypotheses:\\(H_0\\): additive model (without interaction) fits data adequately.\\(H_0\\): additive model (without interaction) fits data adequately.\\(H_a\\): interaction model provides significantly better fit.\\(H_a\\): interaction model provides significantly better fit.Interpretation:\np-value insignificant, fail reject \\(H_0\\), meaning simpler additive model sufficient.\nCheck AIC BIC: Lower values indicate better-fitting model, supporting use additive model consistent hypothesis test.\nInterpretation:p-value insignificant, fail reject \\(H_0\\), meaning simpler additive model sufficient.p-value insignificant, fail reject \\(H_0\\), meaning simpler additive model sufficient.Check AIC BIC: Lower values indicate better-fitting model, supporting use additive model consistent hypothesis test.Check AIC BIC: Lower values indicate better-fitting model, supporting use additive model consistent hypothesis test.","code":"\nlibrary(lme4)\n\n# Additive model (no interaction)\nsp_model_additive <- lmer(yield ~ irrigation + variety + (1 | field), data = irrigation)\n\n# Likelihood ratio test comparing the two models\nanova(sp_model_additive, sp_model, ddf = \"Kenward-Roger\")\n#> Data: irrigation\n#> Models:\n#> sp_model_additive: yield ~ irrigation + variety + (1 | field)\n#> sp_model: yield ~ irrigation * variety + (1 | field)\n#>                   npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)\n#> sp_model_additive    7 83.959 89.368 -34.980    69.959                     \n#> sp_model            10 88.609 96.335 -34.305    68.609 1.3503  3     0.7172"},{"path":"sec-linear-mixed-models.html","id":"assessing-the-random-effect-exact-restricted-likelihood-ratio-test","chapter":"8 Linear Mixed Models","heading":"8.5.4.5 Assessing the Random Effect: Exact Restricted Likelihood Ratio Test","text":"verify whether random field effect necessary, conduct exact RLRT:Hypotheses:\n\\(H_0\\): \\(\\sigma^2_f = 0\\) (variability fields; random effect unnecessary).\n\\(H_a\\): \\(\\sigma^2_f > 0\\) (random field effect significant).\nHypotheses:\\(H_0\\): \\(\\sigma^2_f = 0\\) (variability fields; random effect unnecessary).\\(H_0\\): \\(\\sigma^2_f = 0\\) (variability fields; random effect unnecessary).\\(H_a\\): \\(\\sigma^2_f > 0\\) (random field effect significant).\\(H_a\\): \\(\\sigma^2_f > 0\\) (random field effect significant).Interpretation:p-value significant, reject \\(H_0\\), confirming random field effect essential.p-value significant, reject \\(H_0\\), confirming random field effect essential.significant random effect implies substantial variability fields must accounted model.significant random effect implies substantial variability fields must accounted model.","code":"\nlibrary(RLRsim)\n\n# RLRT for the random effect of field\nexactRLRT(sp_model)\n#> \n#>  simulated finite sample distribution of RLRT.\n#>  \n#>  (p-value based on 10000 simulated values)\n#> \n#> data:  \n#> RLRT = 6.1118, p-value = 0.01"},{"path":"sec-linear-mixed-models.html","id":"repeated-measures-in-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.6 Repeated Measures in Mixed Models","text":"Repeated measures data arise multiple observations collected subjects time different conditions. introduces correlation observations subject, must accounted statistical model.Mixed-effects models particularly effective repeated measures allow us model fixed effects (e.g., treatment, time) random effects (e.g., subject-specific variability).general form mixed-effects model repeated measures :\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\delta_{(k)} + \\epsilon_{ijk}\n\\]:\\(Y_{ijk}\\) = Response \\(\\)-th group, \\(j\\)-th time point, \\(k\\)-th subject.\\(Y_{ijk}\\) = Response \\(\\)-th group, \\(j\\)-th time point, \\(k\\)-th subject.\\(\\mu\\) = Overall mean.\\(\\mu\\) = Overall mean.\\(\\alpha_i\\) = Fixed effect \\(\\)-th group (e.g., treatment group).\\(\\alpha_i\\) = Fixed effect \\(\\)-th group (e.g., treatment group).\\(\\beta_j\\) = Fixed effect \\(j\\)-th time point (repeated measure effect).\\(\\beta_j\\) = Fixed effect \\(j\\)-th time point (repeated measure effect).\\((\\alpha \\beta)_{ij}\\) = Interaction effect group time (fixed).\\((\\alpha \\beta)_{ij}\\) = Interaction effect group time (fixed).\\(\\delta_{(k)} \\sim N(0, \\sigma^2_\\delta)\\) = Random effect \\(k\\)-th subject within \\(\\)-th group (captures subject-specific deviations).\\(\\delta_{(k)} \\sim N(0, \\sigma^2_\\delta)\\) = Random effect \\(k\\)-th subject within \\(\\)-th group (captures subject-specific deviations).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) = Residual error (independent across observations).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) = Residual error (independent across observations)., \\(= 1, \\dots, n_A\\) (number groups), \\(j = 1, \\dots, n_B\\) (number repeated measures), \\(k = 1, \\dots, n_i\\) (number subjects group \\(\\)).variance-covariance matrix repeated observations \\(k\\)-th subject \\(\\)-th group given :\\[\n\\mathbf{Y}_{ik} =\n\\begin{pmatrix}\nY_{i1k} \\\\\nY_{i2k} \\\\\n\\vdots \\\\\nY_{in_Bk}\n\\end{pmatrix}\n\\]Compound Symmetry (CS) StructureUnder compound symmetry assumption (common random-intercepts models), covariance matrix :\\[\n\\mathbf{\\Sigma}_{\\text{subject}} =\n\\begin{pmatrix}\n\\sigma^2_\\delta + \\sigma^2 & \\sigma^2_\\delta & \\cdots & \\sigma^2_\\delta \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta + \\sigma^2 & \\cdots & \\sigma^2_\\delta \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta & \\cdots & \\sigma^2_\\delta + \\sigma^2\n\\end{pmatrix}\n\\]matrix can rewritten :\\[\n\\mathbf{\\Sigma}_{\\text{subject}} = (\\sigma^2_\\delta + \\sigma^2)\n\\begin{pmatrix}\n1 & \\rho & \\cdots & \\rho \\\\\n\\rho & 1 & \\cdots & \\rho \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho & \\rho & \\cdots & 1\n\\end{pmatrix}\n\\]:\\(\\sigma^2_\\delta\\) = Variance due subject-specific random effects.\\(\\sigma^2_\\delta\\) = Variance due subject-specific random effects.\\(\\sigma^2\\) = Residual variance.\\(\\sigma^2\\) = Residual variance.\\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\) = Intra-class correlation coefficient (ICC).\\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\) = Intra-class correlation coefficient (ICC).Key Points:Compound Symmetry Structure product scalar correlation matrix.Compound Symmetry Structure product scalar correlation matrix.correlation two repeated measures subject constant (\\(\\rho\\)).correlation two repeated measures subject constant (\\(\\rho\\)).structure assumes equal correlation across time points, may hold measurements collected time.structure assumes equal correlation across time points, may hold measurements collected time.Refer Random-Intercepts Model detailed discussion compound symmetry.Autoregressive (AR(1)) StructureIf repeated measures collected time, may appropriate assume autoregressive correlation structure, correlations decay time gap increases.AR(1) variance-covariance matrix :\\[\n\\mathbf{\\Sigma}_{\\text{subject}} =\n\\sigma^2\n\\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\cdots & \\rho^{n_B-1} \\\\\n\\rho & 1 & \\rho & \\cdots & \\rho^{n_B-2} \\\\\n\\rho^2 & \\rho & 1 & \\cdots & \\rho^{n_B-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{n_B-1} & \\rho^{n_B-2} & \\rho^{n_B-3} & \\cdots & 1\n\\end{pmatrix}\n\\]:\\(\\sigma^2\\) = Residual variance.\\(\\sigma^2\\) = Residual variance.\\(\\rho\\) = Autoregressive parameter (\\(|\\rho| < 1\\)), representing correlation consecutive time points.\\(\\rho\\) = Autoregressive parameter (\\(|\\rho| < 1\\)), representing correlation consecutive time points.Key Characteristics:Correlations decrease exponentially time lag increases.Correlations decrease exponentially time lag increases.Appropriate longitudinal data temporal proximity influences correlation.Appropriate longitudinal data temporal proximity influences correlation.matrix notation, mixed model can written :\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]:\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{X}\\) = Design matrix fixed effects (e.g., group, time, interaction).\\(\\mathbf{X}\\) = Design matrix fixed effects (e.g., group, time, interaction).\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients.\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients.\\(\\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) = Random error vector.\\(\\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) = Random error vector.\\(\\mathbf{\\Sigma}\\) = Variance-covariance matrix residuals:\nBlock diagonal structure covariance structure identical subject.\nWithin block (subject), structure can compound symmetry, AR(1), another suitable structure depending data.\n\\(\\mathbf{\\Sigma}\\) = Variance-covariance matrix residuals:Block diagonal structure covariance structure identical subject.Within block (subject), structure can compound symmetry, AR(1), another suitable structure depending data.Choosing Right Covariance StructureCompound Symmetry:\nSuitable correlations constant across repeated measures (e.g., randomized controlled trials).\nSimple interpretable may restrictive longitudinal data.\nSuitable correlations constant across repeated measures (e.g., randomized controlled trials).Simple interpretable may restrictive longitudinal data.Autoregressive (AR(1)):\nBest measurements taken equally spaced time intervals correlations decay time.\nAssumes stronger correlation adjacent time points.\nBest measurements taken equally spaced time intervals correlations decay time.Assumes stronger correlation adjacent time points.Unstructured (UN):\nAllows different variances covariances time point.\nProvides maximum flexibility requires parameters larger sample sizes.\nAllows different variances covariances time point.Provides maximum flexibility requires parameters larger sample sizes.Model selection criteria (AIC, BIC, likelihood ratio tests) can help determine appropriate covariance structure.","code":""},{"path":"sec-linear-mixed-models.html","id":"unbalanced-or-unequally-spaced-data","chapter":"8 Linear Mixed Models","heading":"8.7 Unbalanced or Unequally Spaced Data","text":"many real-world applications, data unbalanced (.e., different numbers observations per subject) unequally spaced time. common longitudinal studies, clinical trials, business analytics subjects may observed irregular intervals miss certain time points.Mixed-effects models flexible enough handle data structures, especially carefully model variance-covariance structure repeated measurements.Consider following mixed-effects model:\\[\nY_{ikt} = \\beta_0 + \\beta_{0i} + \\beta_{1} t + \\beta_{1i} t + \\beta_{2} t^2 + \\beta_{2i} t^2 + \\epsilon_{ikt}\n\\]:\\(Y_{ikt}\\) = Response \\(k\\)-th subject \\(\\)-th group time \\(t\\).\\(Y_{ikt}\\) = Response \\(k\\)-th subject \\(\\)-th group time \\(t\\).\\(= 1, 2\\) = Groups (e.g., treatment vs. control).\\(= 1, 2\\) = Groups (e.g., treatment vs. control).\\(k = 1, \\dots, n_i\\) = Individuals within group \\(\\).\\(k = 1, \\dots, n_i\\) = Individuals within group \\(\\).\\(t = (t_1, t_2, t_3, t_4)\\) = Time points (may unequally spaced).\\(t = (t_1, t_2, t_3, t_4)\\) = Time points (may unequally spaced).Model Components:Fixed Effects:\n\\(\\beta_0\\) = Overall intercept (baseline).\n\\(\\beta_1\\) = Common linear time trend.\n\\(\\beta_2\\) = Common quadratic time trend.\n\\(\\beta_0\\) = Overall intercept (baseline).\\(\\beta_1\\) = Common linear time trend.\\(\\beta_2\\) = Common quadratic time trend.Random Effects:\n\\(\\beta_{0i}\\) = Random intercept group \\(\\) (captures group-specific baseline variation).\n\\(\\beta_{1i}\\) = Random slope time group \\(\\) (captures group-specific linear trends).\n\\(\\beta_{2i}\\) = Random quadratic effect group \\(\\) (captures group-specific curvature time).\n\\(\\beta_{0i}\\) = Random intercept group \\(\\) (captures group-specific baseline variation).\\(\\beta_{1i}\\) = Random slope time group \\(\\) (captures group-specific linear trends).\\(\\beta_{2i}\\) = Random quadratic effect group \\(\\) (captures group-specific curvature time).Residual Error:\n\\(\\epsilon_{ikt} \\sim N(0, \\sigma^2)\\) = Measurement error, assumed independent random effects.\n\\(\\epsilon_{ikt} \\sim N(0, \\sigma^2)\\) = Measurement error, assumed independent random effects.","code":""},{"path":"sec-linear-mixed-models.html","id":"variance-covariance-structure-power-model","chapter":"8 Linear Mixed Models","heading":"8.7.1 Variance-Covariance Structure: Power Model","text":"Since observations taken unequally spaced time points, rely simple structures like compound symmetry AR(1). Instead, use power covariance model, allows correlation depend distance time points.variance-covariance matrix repeated measurements subject \\(k\\) group \\(\\) :\\[\n\\mathbf{\\Sigma}_{ik} = \\sigma^2\n\\begin{pmatrix}\n1 & \\rho^{|t_2 - t_1|} & \\rho^{|t_3 - t_1|} & \\rho^{|t_4 - t_1|} \\\\\n\\rho^{|t_2 - t_1|} & 1 & \\rho^{|t_3 - t_2|} & \\rho^{|t_4 - t_2|} \\\\\n\\rho^{|t_3 - t_1|} & \\rho^{|t_3 - t_2|} & 1 & \\rho^{|t_4 - t_3|} \\\\\n\\rho^{|t_4 - t_1|} & \\rho^{|t_4 - t_2|} & \\rho^{|t_4 - t_3|} & 1\n\\end{pmatrix}\n\\]:\\(\\sigma^2\\) = Residual variance.\\(\\sigma^2\\) = Residual variance.\\(\\rho\\) = Correlation parameter (\\(0 < |\\rho| < 1\\)), controlling correlation decays increasing time gaps.\\(\\rho\\) = Correlation parameter (\\(0 < |\\rho| < 1\\)), controlling correlation decays increasing time gaps.\\(|t_j - t_i|\\) = Absolute time difference measurements times \\(t_i\\) \\(t_j\\).\\(|t_j - t_i|\\) = Absolute time difference measurements times \\(t_i\\) \\(t_j\\).Key Characteristics:correlation observations decreases time difference increases, similar AR(1), flexible enough handle unequal time intervals.structure sometimes referred continuous-time autoregressive model power covariance model.fitting full model, can evaluate whether terms necessary, focusing random effects:\\(\\beta_{0i}\\) (Random Intercepts):\nsignificant baseline variability groups?\\(\\beta_{0i}\\) (Random Intercepts):\nsignificant baseline variability groups?\\(\\beta_{1i}\\) (Random Slopes Time):\ngroups exhibit different linear trends time?\\(\\beta_{1i}\\) (Random Slopes Time):\ngroups exhibit different linear trends time?\\(\\beta_{2i}\\) (Random Quadratic Terms):\ngroup-specific curvature response time?\\(\\beta_{2i}\\) (Random Quadratic Terms):\ngroup-specific curvature response time?Model Comparison Approach:Fit Full Model:\nIncludes random effects.Fit Full Model:\nIncludes random effects.Fit Reduced Models:\nSystematically remove random effects (e.g., quadratic terms) create simpler models.Fit Reduced Models:\nSystematically remove random effects (e.g., quadratic terms) create simpler models.Compare Models Using:\nLikelihood Ratio Tests (LRT):\nTest whether complex model significantly improves fit.\nInformation Criteria (AIC, BIC):\nLower values indicate better trade-fit complexity.\nCompare Models Using:Likelihood Ratio Tests (LRT):\nTest whether complex model significantly improves fit.Likelihood Ratio Tests (LRT):\nTest whether complex model significantly improves fit.Information Criteria (AIC, BIC):\nLower values indicate better trade-fit complexity.Information Criteria (AIC, BIC):\nLower values indicate better trade-fit complexity.Assess Random Effects:\nUse exactRLRT test determine random effects significant.\nCheck variance estimates: variance random effect near zero, may necessary.\nAssess Random Effects:Use exactRLRT test determine random effects significant.Check variance estimates: variance random effect near zero, may necessary.matrix notation, model can written :\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\mathbf{b} + \\boldsymbol{\\epsilon}\n\\]:\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{Y}\\) = Vector observed responses.\\(\\mathbf{X}\\) = Design matrix fixed effects (intercept, time, time², group interactions).\\(\\mathbf{X}\\) = Design matrix fixed effects (intercept, time, time², group interactions).\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients.\\(\\boldsymbol{\\beta}\\) = Vector fixed-effect coefficients.\\(\\mathbf{Z}\\) = Design matrix random effects (random intercepts, slopes, etc.).\\(\\mathbf{Z}\\) = Design matrix random effects (random intercepts, slopes, etc.).\\(\\mathbf{b} \\sim N(0, \\mathbf{G})\\) = Vector random effects covariance matrix \\(\\mathbf{G}\\).\\(\\mathbf{b} \\sim N(0, \\mathbf{G})\\) = Vector random effects covariance matrix \\(\\mathbf{G}\\).\\(\\boldsymbol{\\epsilon} \\sim N(0, \\mathbf{R})\\) = Vector residual errors covariance matrix \\(\\mathbf{R}\\), \\(\\mathbf{R}\\) follows power covariance structure.\\(\\boldsymbol{\\epsilon} \\sim N(0, \\mathbf{R})\\) = Vector residual errors covariance matrix \\(\\mathbf{R}\\), \\(\\mathbf{R}\\) follows power covariance structure.","code":""},{"path":"sec-linear-mixed-models.html","id":"application-mixed-models-in-practice","chapter":"8 Linear Mixed Models","heading":"8.8 Application: Mixed Models in Practice","text":"Several R packages available fitting mixed-effects models, unique strengths:nlme\nSupports nested crossed random effects.\nFlexible complex covariance structures.\nLess intuitive syntax compared lme4.\nSupports nested crossed random effects.Flexible complex covariance structures.Less intuitive syntax compared lme4.lme4\nComputationally efficient widely used.\nUser-friendly formula syntax.\nCan handle non-normal responses (e.g., GLMMs).\ndetailed documentation, refer D. Bates et al. (2015).\nComputationally efficient widely used.User-friendly formula syntax.Can handle non-normal responses (e.g., GLMMs).detailed documentation, refer D. Bates et al. (2015).Others:\nBayesian Mixed Models: MCMCglmm, brms.\nGenetics/Plant Breeding: ASReml.\nBayesian Mixed Models: MCMCglmm, brms.Genetics/Plant Breeding: ASReml.","code":""},{"path":"sec-linear-mixed-models.html","id":"example-1-pulp-brightness-analysis","chapter":"8 Linear Mixed Models","heading":"8.8.1 Example 1: Pulp Brightness Analysis","text":"","code":""},{"path":"sec-linear-mixed-models.html","id":"model-specification-1","chapter":"8 Linear Mixed Models","heading":"8.8.1.1 Model Specification","text":"start random-intercepts model pulp brightness:\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]:\\(= 1, \\dots, \\) = Groups random effect \\(\\alpha_i\\).\\(= 1, \\dots, \\) = Groups random effect \\(\\alpha_i\\).\\(j = 1, \\dots, n\\) = Observations per group.\\(j = 1, \\dots, n\\) = Observations per group.\\(\\mu\\) = Overall mean brightness (fixed effect).\\(\\mu\\) = Overall mean brightness (fixed effect).\\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) = Group-specific random effect.\\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) = Group-specific random effect.\\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) = Residual error.\\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) = Residual error.implies compound symmetry structure, intraclass correlation coefficient :\\[\n\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\epsilon}\n\\]\\(\\sigma^2_\\alpha \\0\\): Low correlation within groups (\\(\\rho \\0\\)) (.e., factor \\(\\) doesn’t explain much variation).\\(\\sigma^2_\\alpha \\\\infty\\): High correlation within groups (\\(\\rho \\1\\)).","code":""},{"path":"sec-linear-mixed-models.html","id":"data-exploration-1","chapter":"8 Linear Mixed Models","heading":"8.8.1.2 Data Exploration","text":"","code":"\ndata(pulp, package = \"faraway\")\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Visualize brightness by operator\nggplot(pulp, aes(x = operator, y = bright)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(title = \"Pulp Brightness by Operator\",\n         x = \"Operator\",\n         y = \"Brightness\") +\n    theme_minimal()\n\n# Group-wise summary\npulp %>%\n    group_by(operator) %>%\n    summarise(average_brightness = mean(bright), .groups = 'drop')\n#> # A tibble: 4 × 2\n#>   operator average_brightness\n#>   <fct>                 <dbl>\n#> 1 a                      60.2\n#> 2 b                      60.1\n#> 3 c                      60.6\n#> 4 d                      60.7"},{"path":"sec-linear-mixed-models.html","id":"fitting-the-mixed-model-with-lme4","chapter":"8 Linear Mixed Models","heading":"8.8.1.3 Fitting the Mixed Model with lme4","text":"","code":"\nlibrary(lme4)\n\n# Random intercepts model: operator as a random effect\nmixed_model <- lmer(bright ~ 1 + (1 | operator), data = pulp)\n\n# Model summary\nsummary(mixed_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: bright ~ 1 + (1 | operator)\n#>    Data: pulp\n#> \n#> REML criterion at convergence: 18.6\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4666 -0.7595 -0.1244  0.6281  1.6012 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  operator (Intercept) 0.06808  0.2609  \n#>  Residual             0.10625  0.3260  \n#> Number of obs: 20, groups:  operator, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error      df t value Pr(>|t|)    \n#> (Intercept)  60.4000     0.1494  3.0000   404.2 3.34e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Fixed effects (overall mean)\nfixef(mixed_model)\n#> (Intercept) \n#>        60.4\n\n# Random effects (BLUPs)\nranef(mixed_model)\n#> $operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n#> \n#> with conditional variances for \"operator\"\n\n# Variance components\nVarCorr(mixed_model)\n#>  Groups   Name        Std.Dev.\n#>  operator (Intercept) 0.26093 \n#>  Residual             0.32596\nre_dat <- as.data.frame(VarCorr(mixed_model))\n\n# Intraclass Correlation Coefficient\nrho <- re_dat[1, 'vcov'] / (re_dat[1, 'vcov'] + re_dat[2, 'vcov'])\nrho\n#> [1] 0.3905354"},{"path":"sec-linear-mixed-models.html","id":"inference-with-lmertest","chapter":"8 Linear Mixed Models","heading":"8.8.1.4 Inference with lmerTest","text":"obtain p-values fixed effects using Satterthwaite’s approximation:example, can see confidence interval computed confint lme4 package close one computed confint lmerTest package.","code":"\nlibrary(lmerTest)\n\n# Model with p-values\nsummary(lmer(bright ~ 1 + (1 | operator), data = pulp))$coefficients\n#>             Estimate Std. Error df  t value     Pr(>|t|)\n#> (Intercept)     60.4  0.1494434  3 404.1664 3.340265e-08\n\n# Confidence interval for the fixed effect\nconfint(mixed_model)[3,]\n#>   2.5 %  97.5 % \n#> 60.0713 60.7287"},{"path":"sec-linear-mixed-models.html","id":"bayesian-mixed-model-with-mcmcglmm","chapter":"8 Linear Mixed Models","heading":"8.8.1.5 Bayesian Mixed Model with MCMCglmm","text":"Bayesian credible intervals may differ slightly frequentist confidence intervals due prior assumptions.","code":"\nlibrary(MCMCglmm)\n\n# Bayesian mixed model\nmixed_model_bayes <- MCMCglmm(\n  bright ~ 1,\n  random = ~ operator,\n  data = pulp,\n  verbose = FALSE\n)\n\n# Posterior summaries\nsummary(mixed_model_bayes)$solutions\n#>             post.mean l-95% CI u-95% CI eff.samp pMCMC\n#> (Intercept)  60.40789  60.1488 60.69595     1000 0.001"},{"path":"sec-linear-mixed-models.html","id":"predictions","chapter":"8 Linear Mixed Models","heading":"8.8.1.6 Predictions","text":"bootstrap confidence intervals, use:","code":"\n# Random effects predictions (BLUPs)\nranef(mixed_model)$operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n\n# Predictions per operator\nfixef(mixed_model) + ranef(mixed_model)$operator\n#>   (Intercept)\n#> a    60.27806\n#> b    60.14088\n#> c    60.56767\n#> d    60.61340\n\n# Equivalent using predict()\npredict(mixed_model, newdata = data.frame(operator = c('a', 'b', 'c', 'd')))\n#>        1        2        3        4 \n#> 60.27806 60.14088 60.56767 60.61340\nbootMer(mixed_model, FUN = fixef, nsim = 100)\n#> \n#> PARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> bootMer(x = mixed_model, FUN = fixef, nsim = 100)\n#> \n#> \n#> Bootstrap Statistics :\n#>     original        bias    std. error\n#> t1*     60.4 -0.0005452538    0.156374"},{"path":"sec-linear-mixed-models.html","id":"example-2-penicillin-yield-glmm-with-blocking","chapter":"8 Linear Mixed Models","heading":"8.8.2 Example 2: Penicillin Yield (GLMM with Blocking)","text":"Testing Treatment EffectSince \\(p\\)-value \\(> .05\\), fail reject null hypothesis (treatment effect).Model Comparison Kenward-Roger ApproximationThe results consistent earlier ANOVA: significant treatment effect.","code":"\ndata(penicillin, package = \"faraway\")\nlibrary(ggplot2)\n\n# Visualize yield by treatment and blend\nggplot(penicillin, aes(y = yield, x = treat, shape = blend, color = blend)) +\n  geom_point(size = 3) +\n  labs(title = \"Penicillin Yield by Treatment and Blend\") +\n  theme_minimal()\n\n# Mixed model: blend as random effect, treatment as fixed\nmixed_model <- lmer(yield ~ treat + (1 | blend), data = penicillin)\nsummary(mixed_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ treat + (1 | blend)\n#>    Data: penicillin\n#> \n#> REML criterion at convergence: 103.8\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4152 -0.5017 -0.1644  0.6830  1.2836 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  blend    (Intercept) 11.79    3.434   \n#>  Residual             18.83    4.340   \n#> Number of obs: 20, groups:  blend, 5\n#> \n#> Fixed effects:\n#>             Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)   84.000      2.475 11.075  33.941 1.51e-12 ***\n#> treatB         1.000      2.745 12.000   0.364   0.7219    \n#> treatC         5.000      2.745 12.000   1.822   0.0935 .  \n#> treatD         2.000      2.745 12.000   0.729   0.4802    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>        (Intr) treatB treatC\n#> treatB -0.555              \n#> treatC -0.555  0.500       \n#> treatD -0.555  0.500  0.500\n\n# BLUPs for each blend\nranef(mixed_model)$blend\n#>        (Intercept)\n#> Blend1   4.2878788\n#> Blend2  -2.1439394\n#> Blend3  -0.7146465\n#> Blend4   1.4292929\n#> Blend5  -2.8585859\n# ANOVA for fixed effects\nanova(mixed_model)\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>       Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> treat     70  23.333     3    12  1.2389 0.3387\nlibrary(pbkrtest)\n\n# Full model vs. null model\nfull_model <-\n    lmer(yield ~ treat + (1 | blend), penicillin, REML = FALSE)\nnull_model <-\n    lmer(yield ~ 1 + (1 | blend), penicillin, REML = FALSE)\n\n# Kenward-Roger approximation\nKRmodcomp(full_model, null_model)\n#> large : yield ~ treat + (1 | blend)\n#> small : yield ~ 1 + (1 | blend)\n#>          stat     ndf     ddf F.scaling p.value\n#> Ftest  1.2389  3.0000 12.0000         1  0.3387"},{"path":"sec-linear-mixed-models.html","id":"example-3-growth-in-rats-over-time","chapter":"8 Linear Mixed Models","heading":"8.8.3 Example 3: Growth in Rats Over Time","text":"Model FittingSince p-value significant, conclude treatment effect varies time.","code":"\nrats <- read.csv(\n    \"images/rats.dat\",\n    header = FALSE,\n    sep = ' ',\n    col.names = c('Treatment', 'rat', 'age', 'y')\n)\n\n# Log-transformed time variable\nrats$t <- log(1 + (rats$age - 45) / 10)\n# Treatment as fixed effect, random intercepts for rats\nrat_model <- lmer(y ~ t:Treatment + (1 | rat), data = rats)\nsummary(rat_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: y ~ t:Treatment + (1 | rat)\n#>    Data: rats\n#> \n#> REML criterion at convergence: 932.4\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.25574 -0.65898 -0.01163  0.58356  2.88309 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  rat      (Intercept) 3.565    1.888   \n#>  Residual             1.445    1.202   \n#> Number of obs: 252, groups:  rat, 50\n#> \n#> Fixed effects:\n#>                Estimate Std. Error       df t value Pr(>|t|)    \n#> (Intercept)     68.6074     0.3312  89.0275  207.13   <2e-16 ***\n#> t:Treatmentcon   7.3138     0.2808 247.2762   26.05   <2e-16 ***\n#> t:Treatmenthig   6.8711     0.2276 247.7097   30.19   <2e-16 ***\n#> t:Treatmentlow   7.5069     0.2252 247.5196   33.34   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) t:Trtmntc t:Trtmnth\n#> t:Tretmntcn -0.327                    \n#> t:Tretmnthg -0.340  0.111             \n#> t:Tretmntlw -0.351  0.115     0.119\nanova(rat_model)\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>             Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \n#> t:Treatment 3181.9  1060.6     3 223.21  734.11 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-linear-mixed-models.html","id":"example-4-tree-water-use-agridat","chapter":"8 Linear Mixed Models","heading":"8.8.4 Example 4: Tree Water Use (Agridat)","text":"Remove outlierPlot water use day one age species groupFitting nlme using lmeFitting lme4 using lmerNotes:|| double pipes = uncorrelated random effectsTo remove intercept term:\n(0+ti|tree)\n(ti-1|tree)\n(0+ti|tree)(ti-1|tree)Adding Correlation StructureKey Takeawayslme4 preferred general mixed models due efficiency simplicity.lme4 preferred general mixed models due efficiency simplicity.nlme powerful complex correlation structures nested designs.nlme powerful complex correlation structures nested designs.Bayesian models (e.g., MCMCglmm) offer flexible inference uncertainty.Bayesian models (e.g., MCMCglmm) offer flexible inference uncertainty.Always consider model diagnostics random effects structure carefully.Always consider model diagnostics random effects structure carefully.","code":"\nlibrary(agridat)\ndat <- harris.wateruse\n\n# Visualizing water use by species and age\nlibrary(latticeExtra)\nuseOuterStrips(\n  xyplot(water ~ day | species * age, \n         dat, group = tree,\n         type = c('p', 'smooth'), \n         main = \"harris.wateruse 2 species, 2 ages (10 trees each)\",\n         as.table = TRUE)\n)\ndat <- subset(dat, day!=268)\nxyplot(\n    water ~ day | tree,\n    dat,\n    subset   = age == \"A2\" & species == \"S2\",\n    as.table = TRUE,\n    type     = c('p', 'smooth'),\n    ylab     = \"Water use profiles of individual trees\",\n    main     = \"harris.wateruse (Age 2, Species 2)\"\n)\n# Rescale day for nicer output, and convergence issues\ndat <- transform(dat, ti = day / 100)\n# add quadratic term\ndat <- transform(dat, ti2 = ti * ti)\n\n# Start with a subgroup: age 2, species 2\nd22 <- droplevels(subset(dat, age == \"A2\" & species == \"S2\"))\nlibrary(nlme)\n\n## We use pdDiag() to get uncorrelated random effects\nm1n <- lme(\n    water ~ 1 + ti + ti2,\n    #intercept, time and time-squared = fixed effects\n    data = d22,\n    na.action = na.omit,\n    random = list(tree = pdDiag(~ 1 + ti + ti2)) \n    # random intercept, time \n    # and time squared per tree = random effects\n)\n\n# for all trees\n# m1n <- lme(\n#   water ~ 1 + ti + ti2,\n#   random = list(tree = pdDiag(~ 1 + ti + ti2)),\n#   data = dat,\n#   na.action = na.omit\n# )\n\nsummary(m1n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC     BIC    logLik\n#>   276.5142 300.761 -131.2571\n#> \n#> Random effects:\n#>  Formula: ~1 + ti + ti2 | tree\n#>  Structure: Diagonal\n#>         (Intercept)           ti          ti2  Residual\n#> StdDev:   0.5187869 1.631223e-05 4.374982e-06 0.3836614\n#> \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -10.798799 0.8814666 227 -12.25094       0\n#> ti           12.346704 0.7827112 227  15.77428       0\n#> ti2          -2.838503 0.1720614 227 -16.49704       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.979       \n#> ti2  0.970 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.07588246 -0.58531056  0.01210209  0.65402695  3.88777402 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10\n\nranef(m1n)\n#>     (Intercept)            ti           ti2\n#> T04   0.1985796  2.070606e-09  6.397103e-10\n#> T05   0.3492827  3.199664e-10 -6.211457e-11\n#> T19  -0.1978989 -9.879555e-10 -2.514502e-10\n#> T23   0.4519003 -4.206418e-10 -3.094113e-10\n#> T38  -0.6457494 -2.069198e-09 -4.227912e-10\n#> T40   0.3739432  4.199061e-10 -3.260161e-11\n#> T49   0.8620648  1.160387e-09 -6.925457e-12\n#> T53  -0.5655049 -1.064849e-09 -5.870462e-11\n#> T67  -0.4394623 -4.482549e-10  2.752922e-11\n#> T71  -0.3871552  1.020034e-09  4.767595e-10\n\nfixef(m1n)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nlibrary(lme4)\n\nm1lmer <-\n    lmer(water ~ 1 + ti + ti2 + (ti + ti2 ||\n                                     tree),\n         data = d22,\n         na.action = na.omit)\n\n# for all trees\n# m1lmer <- lmer(water ~ 1 + ti + ti2 + (ti + ti2 || tree),\n#                data = dat, na.action = na.omit)\n\n# summary(m1lmer)\nranef(m1lmer)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\n\nfixef(m1lmer)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm1l <-\n    lmer(water ~ 1 + ti + ti2 \n         + (1 | tree) + (0 + ti | tree) \n         + (0 + ti2 | tree), data = d22)\nranef(m1l)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\nfixef(m1l)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm2n <- lme(\n    water ~ 1 + ti + ti2,\n    data = d22,\n    random = ~ 1 | tree,\n    cor = corExp(form =  ~ day | tree),\n    na.action = na.omit\n)\n\n# for all trees\n# m2n <- lme(\n#   water ~ 1 + ti + ti2,\n#   random = ~ 1 | tree,\n#   cor = corExp(form = ~ day | tree),\n#   data = dat,\n#   na.action = na.omit\n# )\n\nsummary(m2n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC      BIC   logLik\n#>   263.3081 284.0911 -125.654\n#> \n#> Random effects:\n#>  Formula: ~1 | tree\n#>         (Intercept)  Residual\n#> StdDev:   0.5154042 0.3925777\n#> \n#> Correlation Structure: Exponential spatial correlation\n#>  Formula: ~day | tree \n#>  Parameter estimate(s):\n#>    range \n#> 3.794624 \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -11.223310 1.0988725 227 -10.21348       0\n#> ti           12.712094 0.9794235 227  12.97916       0\n#> ti2          -2.913682 0.2148551 227 -13.56115       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.985       \n#> ti2  0.976 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.04861039 -0.55703950  0.00278101  0.62558762  3.80676991 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10\nranef(m2n)\n#>     (Intercept)\n#> T04   0.1929971\n#> T05   0.3424631\n#> T19  -0.1988495\n#> T23   0.4538660\n#> T38  -0.6413664\n#> T40   0.3769378\n#> T49   0.8410043\n#> T53  -0.5528236\n#> T67  -0.4452930\n#> T71  -0.3689358\nfixef(m2n)\n#> (Intercept)          ti         ti2 \n#>  -11.223310   12.712094   -2.913682"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"sec-nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9 Nonlinear and Generalized Linear Mixed Models","text":"Nonlinear Mixed Models (NLMMs) Generalized Linear Mixed Models (GLMMs) extend traditional models incorporating fixed effects random effects, allowing greater flexibility modeling complex data structures.NLMMs extend nonlinear models include fixed random effects, accommodating nonlinear relationships data.GLMMs extend generalized linear models include random effects, allowing correlated data non-constant variance structures.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"sec-nonlinear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1 Nonlinear Mixed Models","text":"general form nonlinear mixed model :\\[\nY_{ij} = f(\\mathbf{x}_{ij}, \\boldsymbol{\\theta}, \\boldsymbol{\\alpha}_i) + \\epsilon_{ij}\n\\]\\(j\\)-th response \\(\\)-th cluster (subject), :\\(= 1, \\ldots, n\\) (number clusters/subjects),\\(j = 1, \\ldots, n_i\\) (number observations per cluster),\\(\\boldsymbol{\\theta}\\) represents fixed effects,\\(\\boldsymbol{\\alpha}_i\\) random effects cluster \\(\\),\\(\\mathbf{x}_{ij}\\) regressors design variables,\\(f(\\cdot)\\) nonlinear mean response function,\\(\\epsilon_{ij}\\) represents residual error, often assumed normally distributed mean 0.NLMMs particularly useful relationship predictors response adequately captured linear model.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"sec-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2 Generalized Linear Mixed Models","text":"GLMMs extend GLMs incorporating random effects, allows modeling data hierarchical clustered structures.conditional distribution \\(y_i\\) given random effects \\(\\boldsymbol{\\alpha}_i\\) :\\[\ny_i \\mid \\boldsymbol{\\alpha}_i \\sim \\text{independent } f(y_i \\mid \\boldsymbol{\\alpha})\n\\]\\(f(y_i \\mid \\boldsymbol{\\alpha})\\) belongs exponential family distributions:\\[\nf(y_i \\mid \\boldsymbol{\\alpha}) = \\exp \\left( \\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} - c(y_i, \\phi) \\right)\n\\]\\(\\theta_i\\) canonical parameter,\\((\\phi)\\) dispersion parameter,\\(b(\\theta_i)\\) \\(c(y_i, \\phi)\\) specific functions defining exponential family.conditional mean \\(y_i\\) related \\(\\theta_i\\) :\\[\n\\mu_i = \\frac{\\partial b(\\theta_i)}{\\partial \\theta_i}\n\\]Applying link function \\(g(\\cdot)\\), relate mean response fixed random effects:\\[\n\\begin{aligned}\nE(y_i \\mid \\boldsymbol{\\alpha}) &= \\mu_i \\\\\ng(\\mu_i) &= \\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha}\n\\end{aligned}\n\\]\\(g(\\cdot)\\) known link function,\\(\\mathbf{x}_i\\) \\(\\mathbf{z}_i\\) design matrices fixed random effects, respectively,\\(\\boldsymbol{\\beta}\\) represents fixed effects, \\(\\boldsymbol{\\alpha}\\) represents random effects.also specify distribution random effects:\\[\n\\boldsymbol{\\alpha} \\sim f(\\boldsymbol{\\alpha})\n\\]distribution often assumed multivariate normal (Law large Number applies fixed effects) can chosen (subjectively) based context.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"relationship-between-nlmms-and-glmms","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.3 Relationship Between NLMMs and GLMMs","text":"NLMMs can viewed special case GLMMs inverse link function corresponds nonlinear transformation linear predictor:\\[\n\\begin{aligned}\n\\mathbf{Y}_i &= \\mathbf{f}(\\mathbf{x}_i, \\boldsymbol{\\theta}, \\boldsymbol{\\alpha}_i) + \\boldsymbol{\\epsilon}_i \\\\\n\\mathbf{Y}_i &= g^{-1}(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha}_i) + \\boldsymbol{\\epsilon}_i\n\\end{aligned}\n\\], \\(g^{-1}(\\cdot)\\) represents inverse link function, corresponding nonlinear transformation fixed random effects.Note:\ncan’t derive analytical formulation marginal distribution nonlinear combination normal variables normally distributed, even case additive error (\\(\\epsilon_i\\)) random effects (\\(\\alpha_i\\)) normal.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"marginal-properties-of-glmms","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.4 Marginal Properties of GLMMs","text":"","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"marginal-mean-of-y_i","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.4.1 Marginal Mean of \\(y_i\\)","text":"marginal mean obtained integrating distribution random effects:\\[\nE(y_i) = E_{\\boldsymbol{\\alpha}}(E(y_i \\mid \\boldsymbol{\\alpha})) = E_{\\boldsymbol{\\alpha}}(\\mu_i) = E\\left(g^{-1}(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha})\\right)\n\\]Since \\(g^{-1}(\\cdot)\\) nonlinear, expectation simplified without specific distributional assumptions.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"special-case-log-link-function","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.4.1.1 Special Case: Log Link Function","text":"log-link function, \\(g(\\mu) = \\log(\\mu)\\), inverse link exponential function:\\[\nE(y_i) = E\\left(\\exp(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha})\\right)\n\\]Using properties moment-generating function (MGF):\\[\nE(y_i) = \\exp(\\mathbf{x}_i' \\boldsymbol{\\beta}) \\cdot E\\left(\\exp(\\mathbf{z}_i' \\boldsymbol{\\alpha})\\right)\n\\], \\(E(\\exp(\\mathbf{z}_i' \\boldsymbol{\\alpha}))\\) MGF \\(\\boldsymbol{\\alpha}\\) evaluated \\(\\mathbf{z}_i\\).","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"marginal-variance-of-y_i","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.4.2 Marginal Variance of \\(y_i\\)","text":"variance decomposition formula applies:\\[\n\\begin{aligned}\n\\operatorname{Var}(y_i) &= \\operatorname{Var}_{\\boldsymbol{\\alpha}}\\left(E(y_i \\mid \\boldsymbol{\\alpha})\\right) + E_{\\boldsymbol{\\alpha}}\\left(\\operatorname{Var}(y_i \\mid \\boldsymbol{\\alpha})\\right) \\\\\n&= \\operatorname{Var}(\\mu_i) + E\\left((\\phi) V(\\mu_i)\\right)\n\\end{aligned}\n\\]Expressed explicitly:\\[\n\\operatorname{Var}(y_i) = \\operatorname{Var}\\left(g^{-1}(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha})\\right) + E\\left((\\phi) V\\left(g^{-1}(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha})\\right)\\right)\n\\]Without specific assumptions \\(g(\\cdot)\\) distribution \\(\\boldsymbol{\\alpha}\\), general form.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"marginal-covariance-of-mathbfy","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.4.3 Marginal Covariance of \\(\\mathbf{y}\\)","text":"Random effects induce correlation observations within cluster. covariance \\(y_i\\) \\(y_j\\) :\\[\n\\begin{aligned}\n\\operatorname{Cov}(y_i, y_j) &= \\operatorname{Cov}_{\\boldsymbol{\\alpha}}\\left(E(y_i \\mid \\boldsymbol{\\alpha}), E(y_j \\mid \\boldsymbol{\\alpha})\\right) + E_{\\boldsymbol{\\alpha}}\\left(\\operatorname{Cov}(y_i, y_j \\mid \\boldsymbol{\\alpha})\\right) \\\\\n&= \\operatorname{Cov}(\\mu_i, \\mu_j) + E(0) \\\\\n&= \\operatorname{Cov}\\left(g^{-1}(\\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha}), g^{-1}(\\mathbf{x}_j' \\boldsymbol{\\beta} + \\mathbf{z}_j' \\boldsymbol{\\alpha})\\right)\n\\end{aligned}\n\\]second term vanishes \\(y_i\\) \\(y_j\\) conditionally independent given \\(\\boldsymbol{\\alpha}\\). dependency structure hallmark mixed models.Example: Repeated Measurements Poisson GLMMConsider repeated count measurements subjects:Let \\(y_{ij}\\) \\(j\\)-th count subject \\(\\).Assume \\(y_{ij} \\mid \\alpha_i \\sim \\text{independent } \\text{Poisson}(\\mu_{ij})\\).model specified :\\[\n\\log(\\mu_{ij}) = \\mathbf{x}_{ij}' \\boldsymbol{\\beta} + \\alpha_i\n\\]:\\(\\alpha_i \\sim \\text{..d. } N(0, \\sigma^2_{\\alpha})\\) represents subject-specific random effects,log-link GLMM random intercepts subjects.inclusion \\(\\alpha_i\\) accounts subject-level heterogeneity, capturing unobserved variability across individuals.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-in-nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5 Estimation in Nonlinear and Generalized Linear Mixed Models","text":"Linear Mixed Models, marginal likelihood observed data \\(\\mathbf{y}\\) derived integrating random effects hierarchical formulation:\\[\nf(\\mathbf{y}) = \\int f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}) \\, f(\\boldsymbol{\\alpha}) \\, d\\boldsymbol{\\alpha}\n\\]LMMs, component distributions—conditional distribution \\(f(\\mathbf{y} \\mid \\boldsymbol{\\alpha})\\), andthe conditional distribution \\(f(\\mathbf{y} \\mid \\boldsymbol{\\alpha})\\), andthe random effects distribution \\(f(\\boldsymbol{\\alpha})\\)—random effects distribution \\(f(\\boldsymbol{\\alpha})\\)—typically assumed Gaussian linear relationships. assumptions imply marginal distribution \\(\\mathbf{y}\\) also Gaussian, allowing integral solved analytically using properties multivariate normal distribution.contrast:GLMMs, conditional distribution \\(f(\\mathbf{y} \\mid \\boldsymbol{\\alpha})\\) belongs exponential family Gaussian general.GLMMs, conditional distribution \\(f(\\mathbf{y} \\mid \\boldsymbol{\\alpha})\\) belongs exponential family Gaussian general.NLMMs, relationship mean response random (fixed) effects nonlinear, complicating integral.NLMMs, relationship mean response random (fixed) effects nonlinear, complicating integral.cases, marginal likelihood integral:\\[\nL(\\boldsymbol{\\beta}; \\mathbf{y}) = \\int f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}) \\, f(\\boldsymbol{\\alpha}) \\, d\\boldsymbol{\\alpha}\n\\]solved analytically. Consequently, estimation requires:Numerical IntegrationLinearization Model","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-numerical-integration","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.1 Estimation by Numerical Integration","text":"marginal likelihood parameter estimation given :\\[\nL(\\boldsymbol{\\beta}; \\mathbf{y}) = \\int f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}) \\, f(\\boldsymbol{\\alpha}) \\, d\\boldsymbol{\\alpha}\n\\]estimate fixed effects \\(\\boldsymbol{\\beta}\\), often maximize log-likelihood:\\[\n\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta}; \\mathbf{y})\n\\]Optimization requires score function (gradient):\\[\n\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}\n\\]Since integral \\(L(\\boldsymbol{\\beta}; \\mathbf{y})\\) generally intractable, rely numerical techniques approximate .","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"methods-for-numerical-integration","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.1.1 Methods for Numerical Integration","text":"Gaussian Quadrature\nSuitable low-dimensional random effects (\\(\\dim(\\boldsymbol{\\alpha})\\) small).\nApproximates integral using weighted sums function evaluations specific points (nodes).\nGauss-Hermite quadrature commonly used random effects normally distributed.\nLimitation: Computational cost grows exponentially dimension \\(\\boldsymbol{\\alpha}\\) (curse dimensionality).Gaussian QuadratureSuitable low-dimensional random effects (\\(\\dim(\\boldsymbol{\\alpha})\\) small).Approximates integral using weighted sums function evaluations specific points (nodes).Gauss-Hermite quadrature commonly used random effects normally distributed.Limitation: Computational cost grows exponentially dimension \\(\\boldsymbol{\\alpha}\\) (curse dimensionality).Laplace Approximation\nApproximates integral expanding log-likelihood around mode integrand (.e., likely value \\(\\boldsymbol{\\alpha}\\)).\nProvides accurate results moderate-sized random effects large sample sizes.\nFirst-order Laplace approximation commonly used; higher-order versions improve accuracy increase complexity.\nKey Idea: Approximate integral :\n\\[\n\\int e^{h(\\boldsymbol{\\alpha})} d\\boldsymbol{\\alpha} \\approx e^{h(\\hat{\\boldsymbol{\\alpha}})} \\sqrt{\\frac{(2\\pi)^q}{|\\mathbf{H}|}}\n\\]\n:\n\\(\\hat{\\boldsymbol{\\alpha}}\\) mode \\(h(\\boldsymbol{\\alpha})\\),\n\\(\\mathbf{H}\\) Hessian matrix second derivatives \\(\\hat{\\boldsymbol{\\alpha}}\\),\n\\(q\\) dimension \\(\\boldsymbol{\\alpha}\\).\nLaplace ApproximationApproximates integral expanding log-likelihood around mode integrand (.e., likely value \\(\\boldsymbol{\\alpha}\\)).Provides accurate results moderate-sized random effects large sample sizes.First-order Laplace approximation commonly used; higher-order versions improve accuracy increase complexity.Key Idea: Approximate integral :\\[\n\\int e^{h(\\boldsymbol{\\alpha})} d\\boldsymbol{\\alpha} \\approx e^{h(\\hat{\\boldsymbol{\\alpha}})} \\sqrt{\\frac{(2\\pi)^q}{|\\mathbf{H}|}}\n\\]:\\(\\hat{\\boldsymbol{\\alpha}}\\) mode \\(h(\\boldsymbol{\\alpha})\\),\\(\\mathbf{H}\\) Hessian matrix second derivatives \\(\\hat{\\boldsymbol{\\alpha}}\\),\\(q\\) dimension \\(\\boldsymbol{\\alpha}\\).Monte Carlo Integration\nUses random sampling approximate integral.\nImportance Sampling improves efficiency sampling distribution better matches integrand.\nMarkov Chain Monte Carlo methods, Gibbs sampling Metropolis-Hastings, used posterior distribution complex.\nAdvantage: Scales better high-dimensional random effects compared quadrature methods.\nLimitation: Computationally intensive, variance estimates can high without careful tuning.Monte Carlo IntegrationUses random sampling approximate integral.Importance Sampling improves efficiency sampling distribution better matches integrand.Markov Chain Monte Carlo methods, Gibbs sampling Metropolis-Hastings, used posterior distribution complex.Advantage: Scales better high-dimensional random effects compared quadrature methods.Limitation: Computationally intensive, variance estimates can high without careful tuning.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"choosing-an-integration-method","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.1.2 Choosing an Integration Method","text":"small random effect dimensions, quadrature methods effective.moderate dimensions, Laplace approximation offers good balance.high dimensions complex models, Monte Carlo techniques often method choice.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"sec-estimation-by-linearization-glmm","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2 Estimation by Linearization","text":"estimating parameters NLMMs GLMMs, one common effective approach linearization. technique approximates nonlinear non-Gaussian components linear counterparts, enabling use standard LMM estimation methods. Linearization simplifies estimation process also allows leveraging well-established statistical tools methods developed linear models.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"concept-of-linearization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.1 Concept of Linearization","text":"core idea create linearized version response variable, known working response pseudo-response, denoted \\(\\tilde{y}_i\\). pseudo-response designed approximate original nonlinear relationship linear form, facilitating easier estimation model parameters. conditional mean pseudo-response expressed :\\[\nE(\\tilde{y}_i \\mid \\boldsymbol{\\alpha}) = \\mathbf{x}_i' \\boldsymbol{\\beta} + \\mathbf{z}_i' \\boldsymbol{\\alpha}\n\\]:\\(\\mathbf{x}_i\\) design matrix fixed effects,\\(\\mathbf{x}_i\\) design matrix fixed effects,\\(\\boldsymbol{\\beta}\\) represents fixed effect parameters,\\(\\boldsymbol{\\beta}\\) represents fixed effect parameters,\\(\\mathbf{z}_i\\) design matrix random effects,\\(\\mathbf{z}_i\\) design matrix random effects,\\(\\boldsymbol{\\alpha}\\) denotes random effects.\\(\\boldsymbol{\\alpha}\\) denotes random effects.addition conditional mean, essential estimate conditional variance pseudo-response fully characterize linearized model:\\[\n\\operatorname{Var}(\\tilde{y}_i \\mid \\boldsymbol{\\alpha})\n\\]variance estimation ensures model accounts inherent variability data, maintaining integrity statistical inferences.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"application-of-linearization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.2 Application of Linearization","text":"linearized, model structure closely resembles linear mixed model, allowing us apply standard estimation techniques LMMs. techniques include methods MLE REML, computationally efficient statistically robust.primary difference various linearization-based methods lies linearization performed. often involves expanding nonlinear function \\(f(\\mathbf{x}, \\boldsymbol{\\theta}, \\boldsymbol{\\alpha})\\) inverse link function \\(g^{-1}(\\cdot)\\). goal approximate complex functions simpler linear expressions retaining much original model’s characteristics possible.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"taylor-series-expansion","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.2.1 Taylor Series Expansion","text":"widely used method linearization Taylor series expansion. approach approximates nonlinear mean function around initial estimates random effects. first-order Taylor series expansion nonlinear function given :\\[\nf(\\mathbf{x}_{ij}, \\boldsymbol{\\theta}, \\boldsymbol{\\alpha}_i) \\approx f(\\mathbf{x}_{ij}, \\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\alpha}}_i) + \\frac{\\partial f}{\\partial \\boldsymbol{\\alpha}_i} \\bigg|_{\\hat{\\boldsymbol{\\alpha}}_i} (\\boldsymbol{\\alpha}_i - \\hat{\\boldsymbol{\\alpha}}_i)\n\\]expression:\\(f(\\mathbf{x}_{ij}, \\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\alpha}}_i)\\) function evaluated initial estimates random effects \\(\\hat{\\boldsymbol{\\alpha}}_i\\),\\(f(\\mathbf{x}_{ij}, \\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\alpha}}_i)\\) function evaluated initial estimates random effects \\(\\hat{\\boldsymbol{\\alpha}}_i\\),\\(\\frac{\\partial f}{\\partial \\boldsymbol{\\alpha}_i} \\big|_{\\hat{\\boldsymbol{\\alpha}}_i}\\) represents gradient (derivative) function respect random effects, evaluated \\(\\hat{\\boldsymbol{\\alpha}}_i\\),\\(\\frac{\\partial f}{\\partial \\boldsymbol{\\alpha}_i} \\big|_{\\hat{\\boldsymbol{\\alpha}}_i}\\) represents gradient (derivative) function respect random effects, evaluated \\(\\hat{\\boldsymbol{\\alpha}}_i\\),\\((\\boldsymbol{\\alpha}_i - \\hat{\\boldsymbol{\\alpha}}_i)\\) captures deviation initial estimates.\\((\\boldsymbol{\\alpha}_i - \\hat{\\boldsymbol{\\alpha}}_i)\\) captures deviation initial estimates.initial estimates \\(\\hat{\\boldsymbol{\\alpha}}_i\\) often set zero simplicity, especially early stages model fitting. approximation reduces model linear form, making amenable standard LMM estimation techniques.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"advantages-and-considerations","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.2.2 Advantages and Considerations","text":"Linearization offers several advantages:Simplified Computation: transforming complex nonlinear relationships linear forms, linearization reduces computational complexity.Flexibility: Despite simplification, linearized models retain ability capture key features original data structure.Statistical Robustness: use established LMM estimation techniques ensures robust parameter estimation.However, linearization also comes considerations:Approximation Error: accuracy linearized model depends well linear approximation captures original nonlinear relationship.Approximation Error: accuracy linearized model depends well linear approximation captures original nonlinear relationship.Choice Expansion Point: selection initial estimates \\(\\hat{\\boldsymbol{\\alpha}}_i\\) can influence quality approximation.Choice Expansion Point: selection initial estimates \\(\\hat{\\boldsymbol{\\alpha}}_i\\) can influence quality approximation.Higher-Order Terms: cases first-order approximation insufficient, higher-order Taylor series terms may needed, increasing model complexity.Higher-Order Terms: cases first-order approximation insufficient, higher-order Taylor series terms may needed, increasing model complexity.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"penalized-quasi-likelihood","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.3 Penalized Quasi-Likelihood","text":"Penalized Quasi-Likelihood (PQL) one popular linearization-based estimation methods GLMMs.linearization achieved first-order Taylor expansion inverse link function around current estimates parameters. working response \\(k\\)-th iteration given :\\[\n\\tilde{y}_i^{(k)} = \\hat{\\eta}_i^{(k-1)} + \\left(y_i - \\hat{\\mu}_i^{(k-1)}\\right) \\cdot \\left.\\frac{d \\eta}{d \\mu}\\right|_{\\hat{\\eta}_i^{(k-1)}}\n\\]:\\(\\eta_i = g(\\mu_i)\\) linear predictor,\\(\\hat{\\eta}_i^{(k-1)}\\) \\(\\hat{\\mu}_i^{(k-1)}\\) estimates previous iteration \\((k-1)\\),\\(g(\\cdot)\\) link function, \\(\\mu_i = g^{-1}(\\eta_i)\\).PQL Estimation AlgorithmInitialization:\nStart initial estimates \\(\\boldsymbol{\\beta}\\) \\(\\boldsymbol{\\alpha}\\) (commonly set zeros).Initialization:\nStart initial estimates \\(\\boldsymbol{\\beta}\\) \\(\\boldsymbol{\\alpha}\\) (commonly set zeros).Compute Working Response:\nUse formula compute \\(\\tilde{y}_i^{(k)}\\) based current parameter estimates.Compute Working Response:\nUse formula compute \\(\\tilde{y}_i^{(k)}\\) based current parameter estimates.Fit Linear Mixed Model:\nApply standard LMM estimation techniques pseudo-response \\(\\tilde{y}_i^{(k)}\\) update estimates \\(\\boldsymbol{\\beta}\\) \\(\\boldsymbol{\\alpha}\\).Fit Linear Mixed Model:\nApply standard LMM estimation techniques pseudo-response \\(\\tilde{y}_i^{(k)}\\) update estimates \\(\\boldsymbol{\\beta}\\) \\(\\boldsymbol{\\alpha}\\).Update Variance Components:\nEstimate \\(\\operatorname{Var}(\\tilde{y}_i \\mid \\boldsymbol{\\alpha})\\) based updated parameter estimates.Update Variance Components:\nEstimate \\(\\operatorname{Var}(\\tilde{y}_i \\mid \\boldsymbol{\\alpha})\\) based updated parameter estimates.Iteration:\nRepeat steps 2–4 estimates converge.Iteration:\nRepeat steps 2–4 estimates converge.Comments PQL:Advantages:\nEasy implement using existing LMM software.\nFast convergence many practical datasets.\nEasy implement using existing LMM software.Fast convergence many practical datasets.Limitations:\nInference asymptotically correct due linearization approximation.\nBiased estimates common, especially:\nbinomial responses small group sizes,\nBernoulli models (worst-case scenario),\nPoisson models small counts. (Faraway 2016)\n\nHypothesis testing confidence intervals can unreliable.\nInference asymptotically correct due linearization approximation.Biased estimates common, especially:\nbinomial responses small group sizes,\nBernoulli models (worst-case scenario),\nPoisson models small counts. (Faraway 2016)\nbinomial responses small group sizes,Bernoulli models (worst-case scenario),Poisson models small counts. (Faraway 2016)Hypothesis testing confidence intervals can unreliable.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"generalized-estimating-equations","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.4 Generalized Estimating Equations","text":"Generalized Estimating Equations (GEE) offer alternative approach parameter estimation models correlated data, particularly marginal models focus population-averaged effects rather subject-specific effects.GEE estimates obtained solving estimating equations rather maximizing likelihood function.Consider marginal generalized linear model:\\[\n\\operatorname{logit}(E(\\mathbf{y})) = \\mathbf{X} \\boldsymbol{\\beta}\n\\]Assuming working covariance matrix \\(\\mathbf{V}\\) elements \\(\\mathbf{y}\\), estimating equation \\(\\boldsymbol{\\beta}\\) :\\[\n\\mathbf{X}' \\mathbf{V}^{-1} (\\mathbf{y} - E(\\mathbf{y})) = 0\n\\]\\(\\mathbf{V}\\) correctly specifies covariance structure, estimator unbiased. practice, often assume simple structure (e.g., independence) obtain robust standard errors even covariance misspecified.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"gee-for-repeated-measures","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.4.1 GEE for Repeated Measures","text":"Let \\(y_{ij}\\) denote \\(j\\)-th measurement \\(\\)-th subject, :\\[\n\\mathbf{y}_i =\n\\begin{pmatrix}\ny_{i1} \\\\\n\\vdots \\\\\ny_{in_i}\n\\end{pmatrix},\n\\quad\n\\boldsymbol{\\mu}_i =\n\\begin{pmatrix}\n\\mu_{i1} \\\\\n\\vdots \\\\\n\\mu_{in_i}\n\\end{pmatrix},\n\\quad\n\\mathbf{x}_{ij} =\n\\begin{pmatrix}\nX_{ij1} \\\\\n\\vdots \\\\\nX_{ijp}\n\\end{pmatrix}\n\\]Define working covariance matrix \\(\\mathbf{y}_i\\) :\\[\n\\mathbf{V}_i = \\operatorname{Cov}(\\mathbf{y}_i)\n\\]Following (Liang Zeger 1986), GEE estimating \\(\\boldsymbol{\\beta}\\) :\\[\nS(\\boldsymbol{\\beta}) = \\sum_{=1}^K \\frac{\\partial \\boldsymbol{\\mu}_i'}{\\partial \\boldsymbol{\\beta}} \\, \\mathbf{V}_i^{-1} (\\mathbf{y}_i - \\boldsymbol{\\mu}_i) = 0\n\\]:\\(K\\) number subjects (clusters),\\(K\\) number subjects (clusters),\\(\\boldsymbol{\\mu}_i = E(\\mathbf{y}_i)\\),\\(\\boldsymbol{\\mu}_i = E(\\mathbf{y}_i)\\),\\(\\mathbf{V}_i\\) working covariance matrix.\\(\\mathbf{V}_i\\) working covariance matrix.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"working-correlation-structures","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.4.2 Working Correlation Structures","text":"covariance matrix \\(\\mathbf{V}_i\\) modeled :\\[\n\\mathbf{V}_i = (\\phi) \\, \\mathbf{B}_i^{1/2} \\, \\mathbf{R}(\\boldsymbol{c}) \\, \\mathbf{B}_i^{1/2}\n\\]\\((\\phi)\\) dispersion parameter,\\(\\mathbf{B}_i\\) diagonal matrix variance functions \\(V(\\mu_{ij})\\) diagonal,\\(\\mathbf{R}(\\boldsymbol{c})\\) working correlation matrix, parameterized \\(\\boldsymbol{c}\\). \\(\\mathbf{R}(\\boldsymbol{c})\\) true correlation matrix \\(\\mathbf{y}_i\\), \\(\\mathbf{V}_i\\) true covariance matrix.Common Working Correlation Structures:Independence: \\(\\mathbf{R} = \\mathbf{}\\) (simplest, ignores correlation).Exchangeable: Constant correlation pairs within cluster.Autoregressive (AR(1)): Correlation decreases time lag.Unstructured: pair correlation parameter.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"iterative-algorithm-for-gee-estimation","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.2.4.3 Iterative Algorithm for GEE Estimation","text":"Initialization:\nCompute initial estimate \\(\\boldsymbol{\\beta}\\) using GLM independence assumption (\\(\\mathbf{R} = \\mathbf{}\\)).\nInitialization:Compute initial estimate \\(\\boldsymbol{\\beta}\\) using GLM independence assumption (\\(\\mathbf{R} = \\mathbf{}\\)).Estimate Working Correlation Matrix:\nBased residuals initial fit, estimate \\(\\mathbf{R}(\\boldsymbol{c})\\).\nEstimate Working Correlation Matrix:Based residuals initial fit, estimate \\(\\mathbf{R}(\\boldsymbol{c})\\).Update Covariance Matrix:\nCalculate \\(\\hat{\\mathbf{V}}_i\\) using updated working correlation matrix.\nUpdate Covariance Matrix:Calculate \\(\\hat{\\mathbf{V}}_i\\) using updated working correlation matrix.Update \\(\\boldsymbol{\\beta}\\):\n\\[\n\\boldsymbol{\\beta}^{(r+1)} = \\boldsymbol{\\beta}^{(r)} + \\left(\\sum_{=1}^K \\frac{\\partial \\boldsymbol{\\mu}_i'}{\\partial \\boldsymbol{\\beta}} \\, \\hat{\\mathbf{V}}_i^{-1} \\, \\frac{\\partial \\boldsymbol{\\mu}_i}{\\partial \\boldsymbol{\\beta}} \\right)^{-1}\n\\left( \\sum_{=1}^K \\frac{\\partial \\boldsymbol{\\mu}_i'}{\\partial \\boldsymbol{\\beta}} \\, \\hat{\\mathbf{V}}_i^{-1} (\\mathbf{y}_i - \\boldsymbol{\\mu}_i) \\right)\n\\]Update \\(\\boldsymbol{\\beta}\\):\\[\n\\boldsymbol{\\beta}^{(r+1)} = \\boldsymbol{\\beta}^{(r)} + \\left(\\sum_{=1}^K \\frac{\\partial \\boldsymbol{\\mu}_i'}{\\partial \\boldsymbol{\\beta}} \\, \\hat{\\mathbf{V}}_i^{-1} \\, \\frac{\\partial \\boldsymbol{\\mu}_i}{\\partial \\boldsymbol{\\beta}} \\right)^{-1}\n\\left( \\sum_{=1}^K \\frac{\\partial \\boldsymbol{\\mu}_i'}{\\partial \\boldsymbol{\\beta}} \\, \\hat{\\mathbf{V}}_i^{-1} (\\mathbf{y}_i - \\boldsymbol{\\mu}_i) \\right)\n\\]Iteration:\nRepeat steps 2–4 convergence (.e., changes \\(\\boldsymbol{\\beta}\\) negligible).\nIteration:Repeat steps 2–4 convergence (.e., changes \\(\\boldsymbol{\\beta}\\) negligible).Comments GEE:Advantages:\nProvides consistent estimates \\(\\boldsymbol{\\beta}\\) even working correlation matrix misspecified.\nRobust standard errors (also known “sandwich” estimators) account potential misspecification.\nProvides consistent estimates \\(\\boldsymbol{\\beta}\\) even working correlation matrix misspecified.Robust standard errors (also known “sandwich” estimators) account potential misspecification.Limitations:\nlikelihood-based method, likelihood-ratio tests appropriate.\nEfficiency loss working correlation matrix poorly specified.\nEstimation random effects possible—GEE focuses marginal (population-averaged) effects.\nlikelihood-based method, likelihood-ratio tests appropriate.Efficiency loss working correlation matrix poorly specified.Estimation random effects possible—GEE focuses marginal (population-averaged) effects.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-bayesian-hierarchical-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.3 Estimation by Bayesian Hierarchical Models","text":"Bayesian methods provide flexible framework estimating parameters NLMMs GLMMs. Unlike frequentist approaches rely MLE linearization techniques, Bayesian estimation fully incorporates prior information naturally accounts uncertainty parameter estimation predictions.Bayesian context, interested posterior distribution model parameters, given observed data \\(\\mathbf{y}\\):\\[\nf(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta} \\mid \\mathbf{y}) \\propto f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) \\, f(\\boldsymbol{\\alpha}) \\, f(\\boldsymbol{\\beta})\n\\]:\\(f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})\\) likelihood data,\\(f(\\boldsymbol{\\alpha})\\) prior distribution random effects,\\(f(\\boldsymbol{\\beta})\\) prior distribution fixed effects,\\(f(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta} \\mid \\mathbf{y})\\) posterior distribution, combines prior beliefs observed data.Advantages Bayesian EstimationNo Need Simplifying Approximations: Bayesian methods require linearization asymptotic approximations.Full Uncertainty Quantification: Provides credible intervals parameters predictive distributions new data.Flexible Modeling: Easily accommodates complex hierarchical structures, non-standard distributions, prior information.Computational ChallengesDespite advantages, Bayesian estimation can computationally intensive complex, especially high-dimensional models. Key implementation issues include:Non-Valid Joint Distributions:\nhierarchical models, specifying valid joint distributions data, random effects, parameters can challenging.Non-Valid Joint Distributions:\nhierarchical models, specifying valid joint distributions data, random effects, parameters can challenging.Constraints Mean-Variance Relationships:\ninherent relationship mean variance GLMMs, combined random effects, imposes constraints marginal covariance structure.Constraints Mean-Variance Relationships:\ninherent relationship mean variance GLMMs, combined random effects, imposes constraints marginal covariance structure.Computational Intensity:\nFitting Bayesian models often requires advanced numerical techniques like Markov Chain Monte Carlo, can slow converge, especially large datasets complex models.Computational Intensity:\nFitting Bayesian models often requires advanced numerical techniques like Markov Chain Monte Carlo, can slow converge, especially large datasets complex models.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"bayesian-estimation-methods","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.3.1 Bayesian Estimation Methods","text":"Bayesian estimation can proceed two general approaches:Approximating Objective Function (Marginal Likelihood)marginal likelihood typically intractable requires integrating random effects:\\[\nf(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\int f(\\mathbf{y} \\mid \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) \\, f(\\boldsymbol{\\alpha}) \\, d\\boldsymbol{\\alpha}\n\\]Since integral solved analytically, approximate using following methods:Laplace Approximation\nApproximates integral expanding log-likelihood around mode integrand.\nProvides efficient, asymptotically accurate approximation posterior approximately Gaussian near mode.\nLaplace ApproximationApproximates integral expanding log-likelihood around mode integrand.Approximates integral expanding log-likelihood around mode integrand.Provides efficient, asymptotically accurate approximation posterior approximately Gaussian near mode.Provides efficient, asymptotically accurate approximation posterior approximately Gaussian near mode.Quadrature Methods\nGaussian quadrature (e.g., Gauss-Hermite quadrature) effective low-dimensional random effects.\nApproximates integral summing weighted evaluations function specific points.\nQuadrature MethodsGaussian quadrature (e.g., Gauss-Hermite quadrature) effective low-dimensional random effects.Gaussian quadrature (e.g., Gauss-Hermite quadrature) effective low-dimensional random effects.Approximates integral summing weighted evaluations function specific points.Approximates integral summing weighted evaluations function specific points.Monte Carlo Integration\nUses random sampling approximate integral.\nImportance sampling improves efficiency drawing samples distribution closely resembles target distribution.\nMonte Carlo IntegrationUses random sampling approximate integral.Uses random sampling approximate integral.Importance sampling improves efficiency drawing samples distribution closely resembles target distribution.Importance sampling improves efficiency drawing samples distribution closely resembles target distribution.Approximating Model (Linearization)Alternatively, can approximate model using Taylor series linearization around current estimates parameters. approach simplifies model, making Bayesian estimation computationally feasible, though cost approximation error.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"markov-chain-monte-carlo-methods","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.3.2 Markov Chain Monte Carlo Methods","text":"common approach fully Bayesian estimation MCMC, generates samples posterior distribution iterative simulation. Popular MCMC algorithms include:Gibbs Sampling:\nEfficient full conditional distributions available closed form.Gibbs Sampling:\nEfficient full conditional distributions available closed form.Metropolis-Hastings Algorithm:\ngeneral flexible, used full conditionals easily sampled.Metropolis-Hastings Algorithm:\ngeneral flexible, used full conditionals easily sampled.Hamiltonian Monte Carlo (HMC):\nImplemented packages like Stan, provides faster convergence complex models leveraging gradient information.Hamiltonian Monte Carlo (HMC):\nImplemented packages like Stan, provides faster convergence complex models leveraging gradient information.posterior distribution approximated using generated samples:\\[\nf(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta} \\mid \\mathbf{y}) \\approx \\frac{1}{N} \\sum_{=1}^N \\delta(\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^{()}, \\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{()})\n\\]\\(N\\) number MCMC samples \\(\\delta(\\cdot)\\) Dirac delta function.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"practical-implementation-in-r","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.5.4 Practical Implementation in R","text":"Several R packages facilitate Bayesian estimation GLMMs NLMMs:GLMM Estimation:\nMASS::glmmPQL — Penalized Quasi-Likelihood GLMMs.\nlme4::glmer — Frequentist estimation GLMMs using Laplace approximation.\nglmmTMB — Handles complex random effects structures efficiently.\nMASS::glmmPQL — Penalized Quasi-Likelihood GLMMs.lme4::glmer — Frequentist estimation GLMMs using Laplace approximation.glmmTMB — Handles complex random effects structures efficiently.NLMM Estimation:\nnlme::nlme — Nonlinear mixed-effects modeling.\nlme4::nlmer — Extends lme4 nonlinear mixed models.\nbrms::brm — Bayesian estimation via Stan, supporting NLMMs.\nnlme::nlme — Nonlinear mixed-effects modeling.lme4::nlmer — Extends lme4 nonlinear mixed models.brms::brm — Bayesian estimation via Stan, supporting NLMMs.Bayesian Estimation:\nMCMCglmm — Implements MCMC algorithms GLMMs.\nbrms::brm — High-level interface Bayesian regression models, leveraging Stan efficient MCMC sampling.\nMCMCglmm — Implements MCMC algorithms GLMMs.brms::brm — High-level interface Bayesian regression models, leveraging Stan efficient MCMC sampling.Example: Non-Gaussian Repeated MeasurementsConsider case repeated measurements:data Gaussian: Use Linear Mixed Models.data non-Gaussian: Use Nonlinear Generalized Linear Mixed Models.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"application-nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6 Application: Nonlinear and Generalized Linear Mixed Models","text":"","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-data-cbpp-dataset","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.1 Binomial Data: CBPP Dataset","text":"use CBPP dataset lme4 package demonstrate different estimation approaches binomial mixed models.data contain information contagious bovine pleuropneumonia (CBPP) cases across different herds periods.Penalized Quasi-LikelihoodPros:Linearizes response create pseudo-response, similar linear mixed models.Linearizes response create pseudo-response, similar linear mixed models.Computationally efficient.Computationally efficient.Cons:Biased binary Poisson data small counts.Biased binary Poisson data small counts.Random effects must interpreted link scale.Random effects must interpreted link scale.AIC/BIC values interpretable since PQL rely full likelihood.AIC/BIC values interpretable since PQL rely full likelihood.InterpretationThe result shows herd-specific odds vary, accounting random effects.fixed effects interpreted similarly logistic regression. example, logit link:log odds case period 2 -1.016 less period 1 (baseline).Numerical Integration glmerPros:accurate estimation since method directly integrates random effects.Cons:Computationally expensive, especially high-dimensional random effects.Computationally expensive, especially high-dimensional random effects.May struggle convergence complex models.May struggle convergence complex models.Comparing PQL Numerical IntegrationFor small datasets, difference PQL numerical integration may minimal.Improving Accuracy Gauss-Hermite QuadratureSetting nAGQ > 1 increases accuracy likelihood approximation:Bayesian Approach MCMCglmmPros:Incorporates prior information handles complex models intractable likelihoods.Incorporates prior information handles complex models intractable likelihoods.Provides full posterior distributions parameters.Provides full posterior distributions parameters.Cons:Computationally intensive, especially large datasets complex hierarchical structures.MCMCglmm fits residual variance component (useful dispersion issues).Variance Component AnalysisPosterior SummariesMCMC DiagnosticsThere trend (.e., well-mixed).herd variable, many values 0, suggests problem. address instability herd effect sampling, can either:Modify prior distributions,Modify prior distributions,Increase number iterationsIncrease number iterationsTo change shape priors, MCMCglmm use:V controls location distribution (default = 1)V controls location distribution (default = 1)nu controls concentration around V (default = 0)nu controls concentration around V (default = 0)","code":"\nlibrary(lme4)\ndata(cbpp, package = \"lme4\")\nhead(cbpp)\n#>   herd incidence size period\n#> 1    1         2   14      1\n#> 2    1         3   12      2\n#> 3    1         4    9      3\n#> 4    1         0    5      4\n#> 5    2         3   22      1\n#> 6    2         1   18      2\nlibrary(MASS)\npql_cbpp <- glmmPQL(\n    cbind(incidence, size - incidence) ~ period,\n    random  = ~ 1 | herd,\n    data    = cbpp,\n    family  = binomial(link = \"logit\"),\n    verbose = FALSE\n)\nsummary(pql_cbpp)\n#> Linear mixed-effects model fit by maximum likelihood\n#>   Data: cbpp \n#>   AIC BIC logLik\n#>    NA  NA     NA\n#> \n#> Random effects:\n#>  Formula: ~1 | herd\n#>         (Intercept) Residual\n#> StdDev:   0.5563535 1.184527\n#> \n#> Variance function:\n#>  Structure: fixed weights\n#>  Formula: ~invwt \n#> Fixed effects:  cbind(incidence, size - incidence) ~ period \n#>                 Value Std.Error DF   t-value p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372  0.0000\n#> period2     -1.016126 0.3684079 38 -2.758156  0.0089\n#> period3     -1.149984 0.3937029 38 -2.920944  0.0058\n#> period4     -1.605217 0.5178388 38 -3.099839  0.0036\n#>  Correlation: \n#>         (Intr) perid2 perid3\n#> period2 -0.399              \n#> period3 -0.373  0.260       \n#> period4 -0.282  0.196  0.182\n#> \n#> Standardized Within-Group Residuals:\n#>        Min         Q1        Med         Q3        Max \n#> -2.0591168 -0.6493095 -0.2747620  0.5170492  2.6187632 \n#> \n#> Number of Observations: 56\n#> Number of Groups: 15\nexp(0.556)\n#> [1] 1.743684\nsummary(pql_cbpp)$tTable\n#>                 Value Std.Error DF   t-value      p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372 2.333216e-06\n#> period2     -1.016126 0.3684079 38 -2.758156 8.888179e-03\n#> period3     -1.149984 0.3937029 38 -2.920944 5.843007e-03\n#> period4     -1.605217 0.5178388 38 -3.099839 3.637000e-03\nnumint_cbpp <- glmer(\n    cbind(incidence, size - incidence) ~ period + (1 | herd),\n    data = cbpp,\n    family = binomial(link = \"logit\")\n)\nsummary(numint_cbpp)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(incidence, size - incidence) ~ period + (1 | herd)\n#>    Data: cbpp\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>     194.1     204.2     -92.0     184.1        51 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.3816 -0.7889 -0.2026  0.5142  2.8791 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  herd   (Intercept) 0.4123   0.6421  \n#> Number of obs: 56, groups:  herd, 15\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3983     0.2312  -6.048 1.47e-09 ***\n#> period2      -0.9919     0.3032  -3.272 0.001068 ** \n#> period3      -1.1282     0.3228  -3.495 0.000474 ***\n#> period4      -1.5797     0.4220  -3.743 0.000182 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr) perid2 perid3\n#> period2 -0.363              \n#> period3 -0.340  0.280       \n#> period4 -0.260  0.213  0.198\nlibrary(rbenchmark)\nbenchmark(\n    \"PQL (MASS)\" = {\n        glmmPQL(\n            cbind(incidence, size - incidence) ~ period,\n            random = ~ 1 | herd,\n            data = cbpp,\n            family = binomial(link = \"logit\"),\n            verbose = FALSE\n        )\n    },\n    \"Numerical Integration (lme4)\" = {\n        glmer(\n            cbind(incidence, size - incidence) ~ period + (1 | herd),\n            data = cbpp,\n            family = binomial(link = \"logit\")\n        )\n    },\n    replications = 50,\n    columns = c(\"test\", \"replications\", \"elapsed\", \"relative\"),\n    order = \"relative\"\n)\n#>                           test replications elapsed relative\n#> 1                   PQL (MASS)           50    4.48    1.000\n#> 2 Numerical Integration (lme4)           50    9.16    2.045\nnumint_cbpp_GH <- glmer(\n    cbind(incidence, size - incidence) ~ period + (1 | herd),\n    data = cbpp,\n    family = binomial(link = \"logit\"),\n    nAGQ = 20\n)\nsummary(numint_cbpp_GH)$coefficients[, 1] - \n    summary(numint_cbpp)$coefficients[, 1]\n#>   (Intercept)       period2       period3       period4 \n#> -0.0008808634  0.0005160912  0.0004066218  0.0002644629\nlibrary(MCMCglmm)\nBayes_cbpp <- MCMCglmm(\n    cbind(incidence, size - incidence) ~ period,\n    random = ~ herd,\n    data = cbpp,\n    family = \"multinomial2\",\n    verbose = FALSE\n)\nsummary(Bayes_cbpp)\n#> \n#>  Iterations = 3001:12991\n#>  Thinning interval  = 10\n#>  Sample size  = 1000 \n#> \n#>  DIC: 537.6541 \n#> \n#>  G-structure:  ~herd\n#> \n#>      post.mean  l-95% CI u-95% CI eff.samp\n#> herd   0.01121 1.144e-16   0.0425    71.03\n#> \n#>  R-structure:  ~units\n#> \n#>       post.mean l-95% CI u-95% CI eff.samp\n#> units     1.128   0.2475    2.123    285.4\n#> \n#>  Location effects: cbind(incidence, size - incidence) ~ period \n#> \n#>             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n#> (Intercept)   -1.5410  -2.1748  -0.8358    797.6 <0.001 ***\n#> period2       -1.2464  -2.3298  -0.2327    708.7  0.014 *  \n#> period3       -1.3737  -2.4446  -0.2574    719.4  0.018 *  \n#> period4       -1.9347  -3.2662  -0.7931    580.2  0.002 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# explains less variability\napply(Bayes_cbpp$VCV, 2, sd)\n#>       herd      units \n#> 0.05822934 0.51923602\nsummary(Bayes_cbpp)$solutions\n#>             post.mean  l-95% CI   u-95% CI eff.samp pMCMC\n#> (Intercept) -1.541044 -2.174758 -0.8357819 797.6463 0.001\n#> period2     -1.246428 -2.329763 -0.2327008 708.6562 0.014\n#> period3     -1.373747 -2.444636 -0.2574092 719.3625 0.018\n#> period4     -1.934747 -3.266209 -0.7930924 580.1757 0.002\nlibrary(lattice)\nxyplot(as.mcmc(Bayes_cbpp$Sol), layout = c(2, 2))\nxyplot(as.mcmc(Bayes_cbpp$VCV), layout = c(2, 1))\nBayes_cbpp2 <- MCMCglmm(\n    cbind(incidence, size - incidence) ~ period,\n    random = ~ herd,\n    data = cbpp,\n    family = \"multinomial2\",\n    nitt = 20000,\n    burnin = 10000,\n    prior = list(G = list(list(V = 1, nu = 0.1))),\n    verbose = FALSE\n)\nxyplot(as.mcmc(Bayes_cbpp2$VCV), layout = c(2, 1))"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"count-data-owl-dataset","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.2 Count Data: Owl Dataset","text":"’ll now model count data using Owl datasetPoisson GLMMModeling call counts Poisson distribution:typical Poisson model, Poisson mean \\(\\lambda\\) modeled : \\[\n\\log(\\lambda) = x' \\beta\n\\] However, response variable represents rate (e.g., counts per BroodSize), can model : \\[\n\\log\\left(\\frac{\\lambda}{b}\\right) = x' \\beta\n\\] equivalent : \\[\n\\log(\\lambda) = \\log(b) + x' \\beta\n\\] \\(b\\) represents BroodSize. formulation, “offset” mean including logarithm \\(b\\) offset term model. adjustment accounts varying exposure denominator rate-based responses.Nest explains relatively large proportion variability (standard deviation larger coefficients).model fit isn’t great (deviance 5202 594 df).Negative Binomial ModelAddressing overdispersion using negative binomial distribution:improvement using negative binomial considering -dispersionZero-Inflated ModelHandling excess zeros zero-inflated Poisson model:glmmTMB can handle ZIP GLMMs since adds automatic differentiation existing estimation strategies.can see ZIP GLMM arrival time covariate zero best.Arrival time positive effect observing nonzero number callsArrival time positive effect observing nonzero number callsInteractions non significant, food treatment significant (fewer calls eating)Interactions non significant, food treatment significant (fewer calls eating)Nest variability large magnitude (without , parameter estimates change)Nest variability large magnitude (without , parameter estimates change)","code":"\nlibrary(glmmTMB)\nlibrary(dplyr)\ndata(Owls, package = \"glmmTMB\")\nOwls <- Owls %>% rename(Ncalls = SiblingNegotiation)\nowls_glmer <- glmer(\n    Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent + (1 | Nest),\n    family = poisson,\n    data = Owls\n)\nsummary(owls_glmer)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: poisson  ( log )\n#> Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +  \n#>     (1 | Nest)\n#>    Data: Owls\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>    5212.8    5234.8   -2601.4    5202.8       594 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.5529 -1.7971 -0.6842  1.2689 11.4312 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.2063   0.4542  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Fixed effects:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.65585    0.09567   6.855 7.12e-12 ***\n#> FoodTreatmentSatiated               -0.65612    0.05606 -11.705  < 2e-16 ***\n#> SexParentMale                       -0.03705    0.04501  -0.823   0.4104    \n#> FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) FdTrtS SxPrnM\n#> FdTrtmntStt -0.225              \n#> SexParentMl -0.292  0.491       \n#> FdTrtmS:SPM  0.170 -0.768 -0.605\nowls_glmerNB <- glmer.nb(\n    Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent + (1 | Nest),\n    data = Owls\n)\nsummary(owls_glmerNB)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: Negative Binomial(0.8423)  ( log )\n#> Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +  \n#>     (1 | Nest)\n#>    Data: Owls\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>    3495.6    3522.0   -1741.8    3483.6       593 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -0.8859 -0.7737 -0.2701  0.4443  6.1432 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.1245   0.3529  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Fixed effects:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.69005    0.13400   5.150 2.61e-07 ***\n#> FoodTreatmentSatiated               -0.76657    0.16509  -4.643 3.43e-06 ***\n#> SexParentMale                       -0.02605    0.14575  -0.179    0.858    \n#> FoodTreatmentSatiated:SexParentMale  0.15680    0.20512   0.764    0.445    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) FdTrtS SxPrnM\n#> FdTrtmntStt -0.602              \n#> SexParentMl -0.683  0.553       \n#> FdTrtmS:SPM  0.450 -0.744 -0.671\nhist(Owls$Ncalls,breaks=30)\nlibrary(glmmTMB)\nowls_glmm <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 0,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\nowls_glmm_zi <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 1,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\n\n# Scale Arrival time to use as a covariate for zero-inflation parameter\nOwls$ArrivalTime <- scale(Owls$ArrivalTime)\nowls_glmm_zi_cov <- glmmTMB(\n    Ncalls ~ FoodTreatment * SexParent +\n        offset(log(BroodSize)) +\n        (1 | Nest),\n    ziformula =  ~ ArrivalTime,\n    family = nbinom2(link = \"log\"),\n    data = Owls\n)\n\nas.matrix(anova(owls_glmm, owls_glmm_zi))\n#>              Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm     6 3495.610 3521.981 -1741.805 3483.610       NA     NA\n#> owls_glmm_zi  7 3431.646 3462.413 -1708.823 3417.646 65.96373      1\n#>                Pr(>Chisq)\n#> owls_glmm              NA\n#> owls_glmm_zi 4.592983e-16\n\n\nas.matrix(anova(owls_glmm_zi, owls_glmm_zi_cov))\n#>                  Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm_zi      7 3431.646 3462.413 -1708.823 3417.646       NA     NA\n#> owls_glmm_zi_cov  8 3422.532 3457.694 -1703.266 3406.532 11.11411      1\n#>                    Pr(>Chisq)\n#> owls_glmm_zi               NA\n#> owls_glmm_zi_cov 0.0008567362\n\n\nsummary(owls_glmm_zi_cov)\n#>  Family: nbinom2  ( log )\n#> Formula:          \n#> Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)\n#> Zero inflation:          ~ArrivalTime\n#> Data: Owls\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>    3422.5    3457.7   -1703.3    3406.5       591 \n#> \n#> Random effects:\n#> \n#> Conditional model:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.07487  0.2736  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Dispersion parameter for nbinom2 family (): 2.22 \n#> \n#> Conditional model:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.84778    0.09961   8.511  < 2e-16 ***\n#> FoodTreatmentSatiated               -0.39529    0.13742  -2.877  0.00402 ** \n#> SexParentMale                       -0.07025    0.10435  -0.673  0.50079    \n#> FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero-inflation model:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3018     0.1261  -10.32  < 2e-16 ***\n#> ArrivalTime   0.3545     0.1074    3.30 0.000966 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-example-gotway-hessian-fly-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.3 Binomial Example: Gotway Hessian Fly Data","text":"analyze Gotway Hessian Fly dataset agridat package model binomial outcomes using frequentist Bayesian approaches.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"data-visualization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.3.1 Data Visualization","text":"","code":"\nlibrary(agridat)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(spaMM)\n\ndata(gotway.hessianfly)\ndat <- gotway.hessianfly\ndat$prop <- dat$y / dat$n  # Proportion of successes\n\nggplot(dat, aes(x = lat, y = long, fill = prop)) +\n    geom_tile() +\n    scale_fill_gradient(low = 'white', high = 'black') +\n    geom_text(aes(label = gen, color = block)) +\n    ggtitle('Gotway Hessian Fly: Proportion of Infestation')"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"model-specification-2","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.3.2 Model Specification","text":"Fixed Effects (\\(\\boldsymbol{\\beta}\\)): Genotype (gen)Fixed Effects (\\(\\boldsymbol{\\beta}\\)): Genotype (gen)Random Effects (\\(\\boldsymbol{\\alpha}\\)): Block (block), accounting spatial experimental design variabilityRandom Effects (\\(\\boldsymbol{\\alpha}\\)): Block (block), accounting spatial experimental design variabilityFrequentist Approach glmerInterpretation:fixed effects (gen) indicate different genotypes influence infestation probability.fixed effects (gen) indicate different genotypes influence infestation probability.random effect block captures variability due experimental blocks, improving model robustness.random effect block captures variability due experimental blocks, improving model robustness.Odds Ratios: Exponentiating coefficients helps interpret impact infestation odds.Odds Ratios: Exponentiating coefficients helps interpret impact infestation odds.Bayesian Approach MCMCglmmMCMC DiagnosticsTrace Plot: Checks chain mixing convergence.Trace Plot: Checks chain mixing convergence.Autocorrelation Plot: Evaluates dependency MCMC samples.Autocorrelation Plot: Evaluates dependency MCMC samples.Bayesian Interpretation:Posterior Means: Represent central tendency parameter estimates.Posterior Means: Represent central tendency parameter estimates.Credible Intervals: Unlike frequentist confidence intervals, can interpreted directly probability parameter lies within interval.Credible Intervals: Unlike frequentist confidence intervals, can interpreted directly probability parameter lies within interval.","code":"\nflymodel <- glmer(\n    cbind(y, n - y) ~ gen + (1 | block),\n    data   = dat,\n    family = binomial,\n    nAGQ   = 5  # Using adaptive Gauss-Hermite quadrature for accuracy\n)\nsummary(flymodel)\n#> Generalized linear mixed model fit by maximum likelihood (Adaptive\n#>   Gauss-Hermite Quadrature, nAGQ = 5) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(y, n - y) ~ gen + (1 | block)\n#>    Data: dat\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>     162.2     198.9     -64.1     128.2        47 \n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.38644 -1.01188  0.09631  1.03468  2.75479 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  block  (Intercept) 0.001022 0.03196 \n#> Number of obs: 64, groups:  block, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   1.5035     0.3914   3.841 0.000122 ***\n#> genG02       -0.1939     0.5302  -0.366 0.714645    \n#> genG03       -0.5408     0.5103  -1.060 0.289263    \n#> genG04       -1.4342     0.4714  -3.043 0.002346 ** \n#> genG05       -0.2037     0.5429  -0.375 0.707487    \n#> genG06       -0.9783     0.5046  -1.939 0.052534 .  \n#> genG07       -0.6041     0.5111  -1.182 0.237237    \n#> genG08       -1.6774     0.4907  -3.418 0.000630 ***\n#> genG09       -1.3984     0.4725  -2.960 0.003079 ** \n#> genG10       -0.6817     0.5333  -1.278 0.201183    \n#> genG11       -1.4630     0.4843  -3.021 0.002522 ** \n#> genG12       -1.4591     0.4918  -2.967 0.003010 ** \n#> genG13       -3.5528     0.6600  -5.383 7.31e-08 ***\n#> genG14       -2.5073     0.5264  -4.763 1.90e-06 ***\n#> genG15       -2.0872     0.4851  -4.302 1.69e-05 ***\n#> genG16       -2.9697     0.5383  -5.517 3.46e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(MCMCglmm)\nlibrary(coda)\n\nBayes_flymodel <- MCMCglmm(\n    cbind(y, n - y) ~ gen,\n    random  = ~ block,\n    data    = dat,\n    family  = \"multinomial2\",\n    verbose = FALSE\n)\nsummary(Bayes_flymodel)\n#> \n#>  Iterations = 3001:12991\n#>  Thinning interval  = 10\n#>  Sample size  = 1000 \n#> \n#>  DIC: 877.3603 \n#> \n#>  G-structure:  ~block\n#> \n#>       post.mean l-95% CI u-95% CI eff.samp\n#> block   0.02354 6.92e-17  0.05404    798.1\n#> \n#>  R-structure:  ~units\n#> \n#>       post.mean l-95% CI u-95% CI eff.samp\n#> units    0.9667   0.3118    1.821    380.8\n#> \n#>  Location effects: cbind(y, n - y) ~ gen \n#> \n#>             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n#> (Intercept)   1.94362  0.60729  3.33723    815.1  0.002 ** \n#> genG02       -0.40383 -2.36504  1.26737   1016.4  0.694    \n#> genG03       -0.72396 -2.72598  0.95420    847.0  0.418    \n#> genG04       -1.81903 -3.70454 -0.09682    907.4  0.044 *  \n#> genG05       -0.37390 -2.39632  1.32895    865.5  0.708    \n#> genG06       -1.24261 -3.10044  0.37768    845.7  0.140    \n#> genG07       -0.76143 -2.57149  1.03524    831.5  0.390    \n#> genG08       -2.08644 -3.67326 -0.17025    825.0  0.012 *  \n#> genG09       -1.82256 -3.57204 -0.16156    899.8  0.026 *  \n#> genG10       -0.79585 -2.66493  1.07283   1121.7  0.400    \n#> genG11       -1.95648 -3.58516 -0.15710    884.8  0.018 *  \n#> genG12       -1.92374 -3.89023 -0.29812    813.6  0.030 *  \n#> genG13       -4.40453 -6.55523 -2.39214    666.9 <0.001 ***\n#> genG14       -3.20863 -5.22849 -1.44975    912.6 <0.001 ***\n#> genG15       -2.76870 -4.65470 -1.04934    825.6 <0.001 ***\n#> genG16       -3.81851 -5.74736 -1.98829   1035.0 <0.001 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Trace plot for the first fixed effect\nplot(Bayes_flymodel$Sol[, 1],\n     main = colnames(Bayes_flymodel$Sol)[1])\n\n# Autocorrelation plot\nautocorr.plot(Bayes_flymodel$Sol[, 1],\n              main = colnames(Bayes_flymodel$Sol)[1])"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"nonlinear-mixed-model-yellow-poplar-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4 Nonlinear Mixed Model: Yellow Poplar Data","text":"dataset comes Schabenberger Pierce (2001)","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"data-preparation","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.1 Data Preparation","text":"","code":"\ndat2 <- read.table(\"images/YellowPoplarData_r.txt\")\nnames(dat2) <-\n    c('tn', 'k', 'dbh', 'totht', 'dob', 'ht', 'maxd', 'cumv')\ndat2$t <- dat2$dob / dat2$dbh\ndat2$r <- 1 - dat2$dob / dat2$totht"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"data-visualization-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.2 Data Visualization","text":"","code":"\nlibrary(dplyr)\n\ndat2 <- dat2 %>% group_by(tn) %>% mutate(\n    z = case_when(\n        totht < 74 ~ 'a: 0-74ft',\n        totht < 88 ~ 'b: 74-88',\n        totht < 95 ~ 'c: 88-95',\n        totht < 99 ~ 'd: 95-99',\n        totht < 104 ~ 'e: 99-104',\n        totht < 109 ~ 'f: 104-109',\n        totht < 115 ~ 'g: 109-115',\n        totht < 120 ~ 'h: 115-120',\n        totht < 140 ~ 'i: 120-150',\n        TRUE ~ 'j: 150+'\n    )\n)\n\nggplot(dat2, aes(x = r, y = cumv)) + \n    geom_point(size = 0.5) + \n    facet_wrap(vars(z)) +\n    labs(title = \"Cumulative Volume vs. Relative Height by Tree Height Group\")"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"model-specification-3","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.3 Model Specification","text":"proposed Nonlinear Mixed Model : \\[\nV_{ij} = \\left(\\beta_0 + (\\beta_1 + b_{1i})\\frac{D_i^2 H_i}{1000}\\right) \\exp\\left[-(\\beta_2 + b_{2i}) t_{ij} \\exp(\\beta_3 t_{ij})\\right] + e_{ij}\n\\] :\\(b_{1i}, b_{2i}\\) random effects tree \\(\\).\\(b_{1i}, b_{2i}\\) random effects tree \\(\\).\\(e_{ij}\\) residual errors.\\(e_{ij}\\) residual errors.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"fitting-the-nonlinear-mixed-model","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.4 Fitting the Nonlinear Mixed Model","text":"","code":"\nlibrary(nlme)\n\ntmp <- nlme(\n    cumv ~ (b0 + (b1 + u1) * (dbh^2 * totht / 1000)) * \n        exp(-(b2 + u2) * (t / 1000) * exp(b3 * t)), \n    data = dat2,\n    fixed = b0 + b1 + b2 + b3 ~ 1,\n    random = pdDiag(u1 + u2 ~ 1),  # Uncorrelated random effects\n    groups = ~ tn,                 # Grouping by tree\n    start = list(fixed = c(b0 = 0.25, b1 = 2.3, b2 = 2.87, b3 = 6.7))\n)\nsummary(tmp)\n#> Nonlinear mixed-effects model fit by maximum likelihood\n#>   Model: cumv ~ (b0 + (b1 + u1) * (dbh^2 * totht/1000)) * exp(-(b2 + u2) *      (t/1000) * exp(b3 * t)) \n#>   Data: dat2 \n#>        AIC      BIC    logLik\n#>   31103.73 31151.33 -15544.86\n#> \n#> Random effects:\n#>  Formula: list(u1 ~ 1, u2 ~ 1)\n#>  Level: tn\n#>  Structure: Diagonal\n#>                u1       u2 Residual\n#> StdDev: 0.1508094 0.447829 2.226361\n#> \n#> Fixed effects:  b0 + b1 + b2 + b3 ~ 1 \n#>       Value  Std.Error   DF  t-value p-value\n#> b0 0.249386 0.12894686 6297   1.9340  0.0532\n#> b1 2.288832 0.01266804 6297 180.6777  0.0000\n#> b2 2.500497 0.05606686 6297  44.5985  0.0000\n#> b3 6.848871 0.02140677 6297 319.9395  0.0000\n#>  Correlation: \n#>    b0     b1     b2    \n#> b1 -0.639              \n#> b2  0.054  0.056       \n#> b3 -0.011 -0.066 -0.850\n#> \n#> Standardized Within-Group Residuals:\n#>           Min            Q1           Med            Q3           Max \n#> -6.694575e+00 -3.081861e-01 -8.907041e-05  3.469469e-01  7.855665e+00 \n#> \n#> Number of Observations: 6636\n#> Number of Groups: 336\nnlme::intervals(tmp)\n#> Approximate 95% confidence intervals\n#> \n#>  Fixed effects:\n#>           lower      est.     upper\n#> b0 -0.003317833 0.2493858 0.5020894\n#> b1  2.264006069 2.2888323 2.3136585\n#> b2  2.390620116 2.5004971 2.6103742\n#> b3  6.806919325 6.8488712 6.8908232\n#> \n#>  Random Effects:\n#>   Level: tn \n#>            lower      est.     upper\n#> sd(u1) 0.1376068 0.1508094 0.1652787\n#> sd(u2) 0.4056207 0.4478290 0.4944295\n#> \n#>  Within-group standard error:\n#>    lower     est.    upper \n#> 2.187259 2.226361 2.266161"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"interpretation","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.5 Interpretation:","text":"Fixed Effects (\\(\\beta\\)): Describe average growth pattern across trees.Fixed Effects (\\(\\beta\\)): Describe average growth pattern across trees.Random Effects (\\(b_i\\)): Capture tree-specific deviations average trend.Random Effects (\\(b_i\\)): Capture tree-specific deviations average trend.result bit different original study different implementation nonlinear mixed models.","code":""},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"visualizing-model-predictions","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.6.4.6 Visualizing Model Predictions","text":"Red Line: Model predictions tree-specific random effects.Red Line: Model predictions tree-specific random effects.Teal Line: Model predictions based fixed effects (ignoring tree-specific variation).Teal Line: Model predictions based fixed effects (ignoring tree-specific variation).Dots: Observed cumulative volume tree.Dots: Observed cumulative volume tree.","code":"\nlibrary(cowplot)\n\n# Prediction function\nnlmmfn <- function(fixed, rand, dbh, totht, t) {\n    (fixed[1] + (fixed[2] + rand[1]) * (dbh ^ 2 * totht / 1000)) *\n        exp(-(fixed[3] + rand[2]) * (t / 1000) * exp(fixed[4] * t))\n}\n\n# Function to generate plots for selected trees\nplot_tree <- function(tree_id) {\n    pred <- data.frame(dob = seq(1, max(dat2$dob), length.out = 100))\n    pred$tn <- tree_id\n    pred$dbh <- unique(dat2$dbh[dat2$tn == tree_id])\n    pred$t <- pred$dob / pred$dbh\n    pred$totht <- unique(dat2$totht[dat2$tn == tree_id])\n    pred$r <- 1 - pred$dob / pred$totht\n    \n    pred$with_random <- predict(tmp, pred)\n    pred$without_random <-\n        nlmmfn(tmp$coefficients$fixed,\n               c(0, 0),\n               pred$dbh,\n               pred$totht,\n               pred$t)\n    \n    ggplot(pred) +\n        geom_line(aes(x = r, y = with_random, color = 'With Random Effects')) +\n        geom_line(aes(x = r, y = without_random, color = 'Without Random Effects')) +\n        geom_point(data = dat2[dat2$tn == tree_id,], aes(x = r, y = cumv)) +\n        labs(title = paste('Tree', tree_id), colour = \"\") +\n        theme(legend.position = \"bottom\")\n}\n\n# Plotting for selected trees\np1 <- plot_tree(1)\np2 <- plot_tree(151)\np3 <- plot_tree(279)\n\nplot_grid(p1, p2, p3)"},{"path":"sec-nonlinear-and-generalized-linear-mixed-models.html","id":"summary-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.7 Summary","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-nonparametric-regression","chapter":"10 Nonparametric Regression","heading":"10 Nonparametric Regression","text":"Nonparametric regression refers class regression techniques assume specific functional form (e.g., linear, polynomial fixed degree) relationship predictor \\(x \\\\mathbb{R}\\) (\\(\\mathbf{x} \\\\mathbb{R}^p\\)) response variable \\(y \\\\mathbb{R}\\). Instead, nonparametric methods aim estimate relationship directly data, allowing data “speak .”standard regression framework, response variable \\(Y\\) one predictors \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_p)\\). Let us start univariate setting simplicity. assume following model:\\[\nY = m(x) + \\varepsilon,\n\\]:\\(m(x) = \\mathbb{E}[Y \\mid X = x]\\) regression function aim estimate,\\(\\varepsilon\\) random error term (noise) \\(\\mathbb{E}[\\varepsilon \\mid X = x] = 0\\) constant variance \\(\\operatorname{Var}(\\varepsilon) = \\sigma^2\\).parametric regression (e.g., Linear Regression), might assume \\(m(x)\\) specific form, :\\[\nm(x) = \\beta_0 + \\beta_1 x + \\cdots + \\beta_d x^d,\n\\]\\(\\beta_0, \\beta_1, \\ldots, \\beta_d\\) parameters estimated. contrast, nonparametric regression relaxes assumption employs methods can adapt potentially complex shapes \\(m(x)\\) without pre-specifying structure.","code":""},{"path":"sec-nonparametric-regression.html","id":"why-nonparametric","chapter":"10 Nonparametric Regression","heading":"10.1 Why Nonparametric?","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"flexibility","chapter":"10 Nonparametric Regression","heading":"10.1.1 Flexibility","text":"Nonparametric methods can capture nonlinear relationships complex patterns data effectively many traditional parametric methods.Adaptive Fit: rely data determine shape relationship, rather forcing specific equation like \\(Y = \\beta_0 + \\beta_1 x\\) (linear) polynomial.Local Structures: Techniques like kernel smoothing local regression focus small neighborhoods around observation, allowing model adjust dynamically local variations.Matters:Highly Variable Data: data shows multiple peaks, sharp transitions, irregular patterns.Highly Variable Data: data shows multiple peaks, sharp transitions, irregular patterns.Exploratory Analysis: ’re trying uncover hidden structures trends dataset without strong prior assumptions.Exploratory Analysis: ’re trying uncover hidden structures trends dataset without strong prior assumptions.","code":""},{"path":"sec-nonparametric-regression.html","id":"fewer-assumptions","chapter":"10 Nonparametric Regression","heading":"10.1.2 Fewer Assumptions","text":"Parametric methods typically assume:specific functional form (e.g., linear, quadratic).specific functional form (e.g., linear, quadratic).specific error distribution (e.g., normal, Poisson).specific error distribution (e.g., normal, Poisson).Nonparametric methods, hand, relax assumptions, making :Robust Misspecification: Less risk biased estimates due incorrect modeling choices.Flexible Error Structure: can handle complex error distributions without explicitly modeling .Matters:Heterogeneous Populations: fields like ecology, genomics, finance, data might come unknown mixtures distributions.Heterogeneous Populations: fields like ecology, genomics, finance, data might come unknown mixtures distributions.Lack Theoretical Guidance: theory suggest strong functional form distribution family.Lack Theoretical Guidance: theory suggest strong functional form distribution family.","code":""},{"path":"sec-nonparametric-regression.html","id":"interpretability","chapter":"10 Nonparametric Regression","heading":"10.1.3 Interpretability","text":"Nonparametric models can still offer valuable insights:Visual Interpretations: Methods like kernel smoothing provide smooth curves can plot see \\(Y\\) changes \\(x\\).Tree-Based Methods: Random forests gradient boosting (also nonparametric nature) can interpreted via variable importance measures partial dependence plots, although can complex simple curves.don’t get simple coefficient estimates Linear Regression, can still convey certain predictors influence response plots importance metrics.","code":""},{"path":"sec-nonparametric-regression.html","id":"practical-considerations-3","chapter":"10 Nonparametric Regression","heading":"10.1.4 Practical Considerations","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"when-to-prefer-nonparametric","chapter":"10 Nonparametric Regression","heading":"10.1.4.1 When to Prefer Nonparametric","text":"Larger Sample Sizes: Nonparametric methods often need data let data “speak” rather relying fixed formula.Unknown Complex Relationships: suspect strong nonlinearity strong theory functional form, nonparametric approaches provide flexibility discover patterns.Exploratory Predictive Goals: data-driven machine learning contexts, minimizing predictive error often takes precedence strict parametric assumptions.","code":""},{"path":"sec-nonparametric-regression.html","id":"when-to-be-cautious","chapter":"10 Nonparametric Regression","heading":"10.1.4.2 When to Be Cautious","text":"Small Sample Sizes: Nonparametric methods can overfit exhibit high variance isn’t enough data reliably estimate relationship.Computational Cost: nonparametric methods (e.g., kernel methods, large random forests) can computationally heavier parametric approaches like linear regression.Strong Theoretical Models: domain knowledge strongly suggests specific parametric form, ignoring might reduce clarity conflict established theory.Extrapolation: Nonparametric models typically extrapolate well beyond observed data range, rely heavily local patterns.","code":""},{"path":"sec-nonparametric-regression.html","id":"balancing-parametric-and-nonparametric-approaches","chapter":"10 Nonparametric Regression","heading":"10.1.5 Balancing Parametric and Nonparametric Approaches","text":"practice, ’s always either/decision. Consider:Semiparametric Models: Combine parametric components (known relationships effects) nonparametric components (unknown parts).Model Selection & Regularization: Use techniques like cross-validation choose bandwidths (kernel smoothing), number knots (splines), hyperparameters (tree depth) avoid overfitting.Diagnostic Tools: Start simple parametric model, look residual plots identify patterns might warrant nonparametric approach.Drawbacks ChallengesCurse Dimensionality: number predictors \\(p\\) increases, nonparametric methods often require exponentially larger sample sizes maintain accuracy. phenomenon, known curse dimensionality, leads sparse data high-dimensional spaces, making harder obtain reliable estimates.Choice Hyperparameters: Methods kernel smoothing splines depend hyperparameters like bandwidth smoothing parameters, must carefully selected balance bias variance.Computational Complexity: Nonparametric methods can computationally intensive, especially large datasets high-dimensional settings.","code":""},{"path":"sec-nonparametric-regression.html","id":"basic-concepts-in-nonparametric-estimation","chapter":"10 Nonparametric Regression","heading":"10.2 Basic Concepts in Nonparametric Estimation","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"bias-variance-trade-off","chapter":"10 Nonparametric Regression","heading":"10.2.1 Bias-Variance Trade-Off","text":"given method estimating \\(m(x)\\), denote estimator \\(\\hat{m}(x)\\). mean squared error (MSE) point \\(x\\) defined :\\[\n\\operatorname{MSE}(x) = \\mathbb{E}\\left[\\{\\hat{m}(x) - m(x)\\}^2\\right].\n\\]MSE can decomposed two key components: bias variance:\\[\n\\operatorname{MSE}(x) = \\left[\\mathbb{E}[\\hat{m}(x)] - m(x)\\right]^2 + \\operatorname{Var}(\\hat{m}(x)).\n\\]:Bias: Measures systematic error estimator: \\[\n\\operatorname{Bias}^2 = \\left[\\mathbb{E}[\\hat{m}(x)] - m(x)\\right]^2.\n\\]Bias: Measures systematic error estimator: \\[\n\\operatorname{Bias}^2 = \\left[\\mathbb{E}[\\hat{m}(x)] - m(x)\\right]^2.\n\\]Variance: Measures variability estimator around expected value: \\[\n\\operatorname{Var}(\\hat{m}(x)) = \\mathbb{E}\\left[\\{\\hat{m}(x) - \\mathbb{E}[\\hat{m}(x)]\\}^2\\right].\n\\]Variance: Measures variability estimator around expected value: \\[\n\\operatorname{Var}(\\hat{m}(x)) = \\mathbb{E}\\left[\\{\\hat{m}(x) - \\mathbb{E}[\\hat{m}(x)]\\}^2\\right].\n\\]Nonparametric methods often low bias can adapt wide range functions. However, flexibility can lead high variance, especially model captures noise rather underlying signal.bandwidth smoothing parameter nonparametric methods typically controls trade-:Large bandwidth \\(\\Rightarrow\\) smoother function \\(\\Rightarrow\\) higher bias, lower variance.Small bandwidth \\(\\Rightarrow\\) wiggly function \\(\\Rightarrow\\) lower bias, higher variance.Selecting optimal bandwidth critical, determines balance underfitting (high bias) overfitting (high variance).","code":""},{"path":"sec-nonparametric-regression.html","id":"kernel-smoothing-and-local-averages","chapter":"10 Nonparametric Regression","heading":"10.2.2 Kernel Smoothing and Local Averages","text":"Many nonparametric regression estimators can viewed weighted local averages observed responses \\(\\{Y_i\\}\\). univariate case, \\(x_i\\) observations predictor \\(y_i\\) corresponding responses, nonparametric estimator point \\(x\\) often takes form:\\[\n\\hat{m}(x) = \\sum_{=1}^n w_i(x) \\, y_i,\n\\]weights \\(w_i(x)\\) depend distance \\(x_i\\) \\(x\\), satisfy:\\[\n\\sum_{=1}^n w_i(x) = 1.\n\\]see arises concretely kernel regression .","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-kernel-regression","chapter":"10 Nonparametric Regression","heading":"10.3 Kernel Regression","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"basic-setup","chapter":"10 Nonparametric Regression","heading":"10.3.1 Basic Setup","text":"kernel function \\(K(\\cdot)\\) non-negative, symmetric function whose integral (sum, discrete setting) equals 1. nonparametric statistics—kernel density estimation local regression—kernels serve weighting mechanisms, assigning higher weights points closer target location lower weights points farther away. Specifically, valid kernel function must satisfy:Non-negativity:\\[\nK(u) \\ge 0 \\quad \\text{} u.\n\\]Non-negativity:\\[\nK(u) \\ge 0 \\quad \\text{} u.\n\\]Normalization:\\[\n\\int_{-\\infty}^{\\infty} K(u)\\,du = 1.\n\\]Normalization:\\[\n\\int_{-\\infty}^{\\infty} K(u)\\,du = 1.\n\\]Symmetry:\\[\nK(u) = K(-u) \\quad \\text{} u.\n\\]Symmetry:\\[\nK(u) = K(-u) \\quad \\text{} u.\n\\]practice, bandwidth (sometimes called smoothing parameter) used alongside kernel usually greater impact quality estimate particular form kernel. However, choosing suitable kernel can still influence computational efficiency smoothness resulting estimates.","code":""},{"path":"sec-nonparametric-regression.html","id":"common-kernel-functions","chapter":"10 Nonparametric Regression","heading":"10.3.1.1 Common Kernel Functions","text":"kernel function essentially measures proximity, assigning higher weights observations \\(x_i\\) close target point \\(x\\), smaller weights farther away.Gaussian Kernel \\[\nK(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}.\n\\]Shape: Bell-shaped infinite support (.e., \\(K(u)\\) technically nonzero \\(u \\(-\\infty,\\infty)\\)), though values decay rapidly \\(|u|\\) grows.Shape: Bell-shaped infinite support (.e., \\(K(u)\\) technically nonzero \\(u \\(-\\infty,\\infty)\\)), though values decay rapidly \\(|u|\\) grows.Usage: Due smoothness mathematical convenience (especially closed-form expressions asymptotic analysis), widely used kernel density estimation regression smoothing.Usage: Due smoothness mathematical convenience (especially closed-form expressions asymptotic analysis), widely used kernel density estimation regression smoothing.Properties: Gaussian kernel minimizes mean square error many asymptotic scenarios, making common “default choice.”Properties: Gaussian kernel minimizes mean square error many asymptotic scenarios, making common “default choice.”Epanechnikov Kernel \\[\nK(u) = \\begin{cases} \\frac{3}{4}(1 - u^2) & \\text{} |u| \\le 1,\\\\ 0 & \\text{otherwise}. \\end{cases}\n\\]Shape: Parabolic (inverted) \\([-1, 1]\\), dropping 0 \\(|u|=1\\).Shape: Parabolic (inverted) \\([-1, 1]\\), dropping 0 \\(|u|=1\\).Usage: Known optimal minimax sense certain classes problems, frequently preferred compact support (zero weights outside \\(|u|\\le 1\\)) desirable.Usage: Known optimal minimax sense certain classes problems, frequently preferred compact support (zero weights outside \\(|u|\\le 1\\)) desirable.Efficiency: supported finite interval, computations often involve fewer points (outside \\(|u|\\le 1\\) zero weight), can computationally efficient large datasets.Efficiency: supported finite interval, computations often involve fewer points (outside \\(|u|\\le 1\\) zero weight), can computationally efficient large datasets.Uniform (Rectangular) Kernel \\[\nK(u) = \\begin{cases} \\frac{1}{2} & \\text{} |u| \\le 1,\\\\ 0 & \\text{otherwise}. \\end{cases}\n\\]Shape: simple “flat top” distribution \\([-1, 1]\\).Shape: simple “flat top” distribution \\([-1, 1]\\).Usage: Sometimes used simplicity. certain methods (e.g., “moving average” approach), uniform kernel equates giving points within fixed window weight.Usage: Sometimes used simplicity. certain methods (e.g., “moving average” approach), uniform kernel equates giving points within fixed window weight.Drawback: Lacks smoothness boundaries ∣u∣=1|u|=1∣u∣=1, can introduce sharper transitions estimates compared smoother kernels.Drawback: Lacks smoothness boundaries ∣u∣=1|u|=1∣u∣=1, can introduce sharper transitions estimates compared smoother kernels.Triangular Kernel \\[\nK(u) = \\begin{cases} 1 - |u| & \\text{} |u| \\le 1,\\\\ 0 & \\text{otherwise}. \\end{cases}\n\\]Shape: Forms triangle peak \\(u=0\\) linearly descends 0 \\(|u|=1\\).Shape: Forms triangle peak \\(u=0\\) linearly descends 0 \\(|u|=1\\).Usage: Provides continuous piecewise-linear alternative uniform kernel; places relatively weight near center compared uniform kernel.Usage: Provides continuous piecewise-linear alternative uniform kernel; places relatively weight near center compared uniform kernel.Biweight (Quartic) Kernel \\[\nK(u) = \\begin{cases} \\frac{15}{16} \\left(1 - u^2\\right)^2 & \\text{} |u| \\le 1,\\\\ 0 & \\text{otherwise}. \\end{cases}\n\\]Shape: Smooth “bump-shaped,” similar Epanechnikov steeper drop-near \\(|u|=1\\).Shape: Smooth “bump-shaped,” similar Epanechnikov steeper drop-near \\(|u|=1\\).Usage: Popular smoother, polynomial-based kernel compact support desired.Usage: Popular smoother, polynomial-based kernel compact support desired.Cosine Kernel \\[\nK(u) = \\begin{cases} \\frac{\\pi}{4}\\cos\\left(\\frac{\\pi}{2}u\\right) & \\text{} |u| \\le 1,\\\\ 0 & \\text{otherwise}. \\end{cases}\n\\]Shape: single “arch” cosine wave interval \\([-1,1]\\).Shape: single “arch” cosine wave interval \\([-1,1]\\).Usage: Used less frequently can appealing certain smoothness criteria specific signal processing contexts.Usage: Used less frequently can appealing certain smoothness criteria specific signal processing contexts.comparison widely used kernel functions, functional forms, support, main characteristics.Smooth, bell-shapedNonzero \\(u\\), decays quicklyOften default choice due favorable analytical propertiesParabolic shapeCompact supportMinimizes mean integrated squared error certain theoretical contextsFlat (rectangular) shapeEqual weight points within \\([-1,1]\\)Sharp boundary can lead less smooth estimatesLinear decrease center \\(u=0\\) 0 \\(|u|=1\\)Compact supportA bit smoother uniform kernelPolynomial shape, smoothCompact supportOften used relatively smooth taper near boundariesSingle arch cosine waveCompact supportLess commonly used, still mathematically straightforward","code":""},{"path":"sec-nonparametric-regression.html","id":"additional-details-and-usage-notes","chapter":"10 Nonparametric Regression","heading":"10.3.1.2 Additional Details and Usage Notes","text":"Smoothness Differentiability\nKernels infinite support (like Gaussian) can yield smooth estimates require summing (practically) data points.\nKernels compact support (like Epanechnikov, biweight, triangular, etc.) go zero outside fixed interval. can make computations efficient since data within certain range target point matter.\nKernels infinite support (like Gaussian) can yield smooth estimates require summing (practically) data points.Kernels compact support (like Epanechnikov, biweight, triangular, etc.) go zero outside fixed interval. can make computations efficient since data within certain range target point matter.Choice Kernel vs. Choice Bandwidth\nkernel shape effect estimator’s smoothness, choice bandwidth (sometimes denoted \\(h\\)) typically critical. \\(h\\) large, estimate can excessively smooth (high bias). \\(h\\) small, estimate can exhibit high variance appear “noisy.”\nkernel shape effect estimator’s smoothness, choice bandwidth (sometimes denoted \\(h\\)) typically critical. \\(h\\) large, estimate can excessively smooth (high bias). \\(h\\) small, estimate can exhibit high variance appear “noisy.”Local Weighting Principle\ntarget location \\(x\\), kernel function \\(K\\left(\\frac{x - x_i}{h}\\right)\\) -weights data points \\((x_i)\\) farther \\(x\\). Nearer points larger kernel values, hence exert greater influence local estimate.\ntarget location \\(x\\), kernel function \\(K\\left(\\frac{x - x_i}{h}\\right)\\) -weights data points \\((x_i)\\) farther \\(x\\). Nearer points larger kernel values, hence exert greater influence local estimate.Interpretation Density Estimation\nkernel density estimation, data point contributes small “bump” (shaped kernel) overall density. Summing integrating bumps yields continuous estimate underlying density function, contrast discrete histograms.\nkernel density estimation, data point contributes small “bump” (shaped kernel) overall density. Summing integrating bumps yields continuous estimate underlying density function, contrast discrete histograms.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-nadaraya-watson-kernel-estimator","chapter":"10 Nonparametric Regression","heading":"10.3.2 Nadaraya-Watson Kernel Estimator","text":"widely used kernel-based regression estimator Nadaraya-Watson estimator (Nadaraya 1964; Watson 1964), defined :\\[\n\\hat{m}_h(x) = \\frac{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right) y_i}{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right)},\n\\]\\(h > 0\\) bandwidth parameter. Intuitively, formula computes weighted average observed \\(y_i\\) values, weights determined kernel function applied scaled distance \\(x\\) \\(x_i\\).Interpretation:\\(|x - x_i|\\) small (.e., \\(x_i\\) close \\(x\\)), kernel value \\(K\\!\\left(\\frac{x - x_i}{h}\\right)\\) large, giving weight \\(y_i\\).\\(|x - x_i|\\) large, kernel value becomes small (even zero compactly supported kernels like Epanechnikov), reducing influence \\(y_i\\) \\(\\hat{m}_h(x)\\).Thus, observations near \\(x\\) larger impact estimated value \\(\\hat{m}_h(x)\\) distant ones.","code":""},{"path":"sec-nonparametric-regression.html","id":"weights-representation","chapter":"10 Nonparametric Regression","heading":"10.3.2.1 Weights Representation","text":"can define normalized weights:\\[\nw_i(x) = \\frac{K\\!\\left(\\frac{x - x_i}{h}\\right)}{\\sum_{j=1}^n K\\!\\left(\\frac{x - x_j}{h}\\right)},\n\\]estimator can rewritten :\\[\n\\hat{m}_h(x) = \\sum_{=1}^n w_i(x) y_i,\n\\]\\(\\sum_{=1}^n w_i(x) = 1\\) \\(x\\). Notice \\(0 \\le w_i(x) \\le 1\\) \\(\\).","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-priestley-chao-kernel-estimator","chapter":"10 Nonparametric Regression","heading":"10.3.3 Priestley–Chao Kernel Estimator","text":"Priestley–Chao kernel estimator (Priestley Chao 1972) early kernel-based regression estimator designed estimate regression function \\(m(x)\\) observed data \\(\\{(x_i, y_i)\\}_{=1}^n\\). Unlike Nadaraya–Watson estimator, uses pointwise kernel weighting, Priestley–Chao estimator incorporates differences predictor variable approximate integrals accurately.estimator defined :\\[\n\\hat{m}_h(x) = \\frac{1}{h} \\sum_{=1}^{n-1} K\\!\\left(\\frac{x - x_i}{h}\\right) \\cdot (x_{+1} - x_i) \\cdot y_i,\n\\]:\\(K(\\cdot)\\) kernel function,\\(K(\\cdot)\\) kernel function,\\(h > 0\\) bandwidth parameter,\\(h > 0\\) bandwidth parameter,\\((x_{+1} - x_i)\\) represents spacing consecutive observations.\\((x_{+1} - x_i)\\) represents spacing consecutive observations.","code":""},{"path":"sec-nonparametric-regression.html","id":"interpretation-1","chapter":"10 Nonparametric Regression","heading":"10.3.3.1 Interpretation","text":"estimator can viewed Riemann sum approximation integral, kernel-weighted \\(y_i\\) values scaled spacing \\((x_{+1} - x_i)\\).Observations \\(x_i\\) close \\(x\\) receive weight due kernel function.inclusion \\((x_{+1} - x_i)\\) accounts non-uniform spacing data, making estimator accurate predictor values irregularly spaced.estimator particularly useful design points \\(\\{x_i\\}\\) unevenly distributed.","code":""},{"path":"sec-nonparametric-regression.html","id":"weights-representation-1","chapter":"10 Nonparametric Regression","heading":"10.3.3.2 Weights Representation","text":"can express estimator weighted sum observed responses \\(y_i\\):\\[\n\\hat{m}_h(x) = \\sum_{=1}^{n-1} w_i(x) \\, y_i,\n\\]weights defined :\\[\nw_i(x) = \\frac{1}{h} \\cdot K\\!\\left(\\frac{x - x_i}{h}\\right) \\cdot (x_{+1} - x_i).\n\\]Properties weights:Non-negativity: \\(K(u) \\ge 0\\), \\(w_i(x) \\ge 0\\).Non-negativity: \\(K(u) \\ge 0\\), \\(w_i(x) \\ge 0\\).Adaptation spacing: Larger gaps \\((x_{+1} - x_i)\\) increase corresponding weight.Adaptation spacing: Larger gaps \\((x_{+1} - x_i)\\) increase corresponding weight.Unlike Nadaraya–Watson, weights sum 1, approximate integral rather normalized average.Unlike Nadaraya–Watson, weights sum 1, approximate integral rather normalized average.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-gasser-mueller-kernel-estimator","chapter":"10 Nonparametric Regression","heading":"10.3.4 Gasser–Müller Kernel Estimator","text":"Gasser–Müller kernel estimator (Gasser Müller 1979) improves upon Priestley–Chao estimator using cumulative kernel function smooth predictor space. estimator particularly effective irregularly spaced data aims reduce bias boundaries.estimator defined :\\[\n\\hat{m}_h(x) = \\frac{1}{h} \\sum_{=1}^{n-1} \\left[ K^*\\!\\left(\\frac{x - x_i}{h}\\right) - K^*\\!\\left(\\frac{x - x_{+1}}{h}\\right) \\right] \\cdot y_i,\n\\]:\\(K^*(u) = \\int_{-\\infty}^{u} K(v) \\, dv\\) cumulative distribution function (CDF) kernel \\(K\\),\\(K^*(u) = \\int_{-\\infty}^{u} K(v) \\, dv\\) cumulative distribution function (CDF) kernel \\(K\\),\\(h > 0\\) bandwidth parameter.\\(h > 0\\) bandwidth parameter.","code":""},{"path":"sec-nonparametric-regression.html","id":"interpretation-2","chapter":"10 Nonparametric Regression","heading":"10.3.4.1 Interpretation","text":"estimator computes difference cumulative kernel functions two consecutive design points, effectively assigning weight interval \\(x_i\\) \\(x_{+1}\\).Observations contribute \\(\\hat{m}_h(x)\\) \\(x\\) lies \\(x_i\\) \\(x_{+1}\\), contribution decreasing distance \\(x\\) increases.method smooths intervals rather just points, reducing bias near boundaries improving performance unevenly spaced data.","code":""},{"path":"sec-nonparametric-regression.html","id":"weights-representation-2","chapter":"10 Nonparametric Regression","heading":"10.3.4.2 Weights Representation","text":"Gasser–Müller estimator can also expressed weighted sum:\\[\n\\hat{m}_h(x) = \\sum_{=1}^{n-1} w_i(x) \\, y_i,\n\\]weights :\\[\nw_i(x) = \\frac{1}{h} \\left[ K^*\\!\\left(\\frac{x - x_i}{h}\\right) - K^*\\!\\left(\\frac{x - x_{+1}}{h}\\right) \\right].\n\\]Properties weights:Non-negativity: weights non-negative \\(K^*\\) non-decreasing (holds \\(K\\) non-negative).Non-negativity: weights non-negative \\(K^*\\) non-decreasing (holds \\(K\\) non-negative).Adaptation spacing: weights account spacing \\(x_i\\) \\(x_{+1}\\).Adaptation spacing: weights account spacing \\(x_i\\) \\(x_{+1}\\).Similar Priestley–Chao estimator, weights sum 1 estimator approximates integral rather normalized sum.Similar Priestley–Chao estimator, weights sum 1 estimator approximates integral rather normalized sum.","code":""},{"path":"sec-nonparametric-regression.html","id":"comparison-of-kernel-based-estimators","chapter":"10 Nonparametric Regression","heading":"10.3.5 Comparison of Kernel-Based Estimators","text":"","code":""},{"path":"sec-nonparametric-regression.html","id":"bandwidth-selection","chapter":"10 Nonparametric Regression","heading":"10.3.6 Bandwidth Selection","text":"choice bandwidth \\(h\\) crucial controls trade-bias variance:\\(h\\) large, estimator becomes overly smooth, incorporating many distant data points. leads high bias low variance.\\(h\\) small, estimator becomes noisy sensitive fluctuations data, resulting low bias high variance.","code":""},{"path":"sec-nonparametric-regression.html","id":"mean-squared-error-and-optimal-bandwidth","chapter":"10 Nonparametric Regression","heading":"10.3.6.1 Mean Squared Error and Optimal Bandwidth","text":"analyze performance kernel estimators, often examine mean integrated squared error (MISE):\\[\n\\text{MISE}(\\hat{m}_h) = \\mathbb{E}\\left[\\int \\left\\{\\hat{m}_h(x) - m(x)\\right\\}^2 dx \\right].\n\\]\\(n \\\\infty\\), smoothness assumptions \\(m(x)\\) regularity conditions kernel \\(K\\), MISE following asymptotic expansion:\\[\n\\text{MISE}(\\hat{m}_h) \\approx \\frac{R(K)}{n h} \\, \\sigma^2 + \\frac{1}{4} \\mu_2^2(K) \\, h^4 \\int \\left\\{m''(x)\\right\\}^2 dx,\n\\]:\\(R(K) = \\int_{-\\infty}^{\\infty} K(u)^2 du\\) measures roughness kernel.\\(\\mu_2(K) = \\int_{-\\infty}^{\\infty} u^2 K(u) du\\) second moment kernel (related spread).\\(\\sigma^2\\) variance noise, assuming \\(\\operatorname{Var}(\\varepsilon \\mid X = x) = \\sigma^2\\).\\(m''(x)\\) second derivative true regression function \\(m(x)\\).find asymptotically optimal bandwidth, differentiate MISE respect \\(h\\), set derivative zero, solve \\(h\\):\\[\nh_{\\mathrm{opt}} = \\left(\\frac{R(K) \\, \\sigma^2}{\\mu_2^2(K) \\int \\left\\{m''(x)\\right\\}^2 dx} \\cdot \\frac{1}{n}\\right)^{1/5}.\n\\]practice, \\(\\sigma^2\\) \\(\\int \\{m''(x)\\}^2 dx\\) unknown must estimated data. common data-driven approach cross-validation.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-cross-validation-kernel-regression","chapter":"10 Nonparametric Regression","heading":"10.3.6.2 Cross-Validation","text":"leave-one-cross-validation (LOOCV) method widely used bandwidth selection:\\(= 1, \\dots, n\\), fit kernel estimator \\(\\hat{m}_{h,-}(x)\\) using data except \\(\\)-th observation \\((x_i, y_i)\\).Compute squared prediction error left-point: \\((y_i - \\hat{m}_{h,-}(x_i))^2\\).Average errors across observations:\\[\n\\mathrm{CV}(h) = \\frac{1}{n} \\sum_{=1}^n \\left\\{y_i - \\hat{m}_{h,-}(x_i)\\right\\}^2.\n\\]bandwidth \\(h\\) minimizes \\(\\mathrm{CV}(h)\\) selected optimal bandwidth.","code":""},{"path":"sec-nonparametric-regression.html","id":"asymptotic-properties","chapter":"10 Nonparametric Regression","heading":"10.3.7 Asymptotic Properties","text":"Nadaraya-Watson estimator, regularity conditions assuming \\(h \\0\\) \\(n \\\\infty\\) (fast), :Consistency: \\[\n\\hat{m}_h(x) \\overset{p}{\\longrightarrow} m(x),\n\\] meaning estimator converges probability true regression function.Consistency: \\[\n\\hat{m}_h(x) \\overset{p}{\\longrightarrow} m(x),\n\\] meaning estimator converges probability true regression function.Rate Convergence: mean squared error (MSE) decreases rate: \\[\n\\text{MSE}(\\hat{m}_h(x)) = O\\left(n^{-4/5}\\right)\n\\] one-dimensional case. rate results balancing variance term (\\(O(1/(nh))\\)) squared bias term (\\(O(h^4)\\)).Rate Convergence: mean squared error (MSE) decreases rate: \\[\n\\text{MSE}(\\hat{m}_h(x)) = O\\left(n^{-4/5}\\right)\n\\] one-dimensional case. rate results balancing variance term (\\(O(1/(nh))\\)) squared bias term (\\(O(h^4)\\)).","code":""},{"path":"sec-nonparametric-regression.html","id":"derivation-of-the-nadaraya-watson-estimator","chapter":"10 Nonparametric Regression","heading":"10.3.8 Derivation of the Nadaraya-Watson Estimator","text":"Nadaraya-Watson estimator can derived density-based perspective:definition conditional expectation: \\[\nm(x) = \\mathbb{E}[Y \\mid X = x] = \\frac{\\int y \\, f_{X,Y}(x, y) \\, dy}{f_X(x)},\n\\] \\(f_{X,Y}(x, y)\\) joint density \\((X, Y)\\), \\(f_X(x)\\) marginal density \\(X\\).definition conditional expectation: \\[\nm(x) = \\mathbb{E}[Y \\mid X = x] = \\frac{\\int y \\, f_{X,Y}(x, y) \\, dy}{f_X(x)},\n\\] \\(f_{X,Y}(x, y)\\) joint density \\((X, Y)\\), \\(f_X(x)\\) marginal density \\(X\\).Estimate \\(f_X(x)\\) using kernel density estimator: \\[\n\\hat{f}_X(x) = \\frac{1}{n} \\sum_{=1}^n \\frac{1}{h} K\\!\\left(\\frac{x - x_i}{h}\\right).\n\\]Estimate \\(f_X(x)\\) using kernel density estimator: \\[\n\\hat{f}_X(x) = \\frac{1}{n} \\sum_{=1}^n \\frac{1}{h} K\\!\\left(\\frac{x - x_i}{h}\\right).\n\\]Estimate joint density \\(f_{X,Y}(x, y)\\): \\[\n\\hat{f}_{X,Y}(x, y) = \\frac{1}{n} \\sum_{=1}^n \\frac{1}{h} K\\!\\left(\\frac{x - x_i}{h}\\right) \\delta_{y_i}(y),\n\\] \\(\\delta_{y_i}(y)\\) Dirac delta function (point mass \\(y_i\\)).Estimate joint density \\(f_{X,Y}(x, y)\\): \\[\n\\hat{f}_{X,Y}(x, y) = \\frac{1}{n} \\sum_{=1}^n \\frac{1}{h} K\\!\\left(\\frac{x - x_i}{h}\\right) \\delta_{y_i}(y),\n\\] \\(\\delta_{y_i}(y)\\) Dirac delta function (point mass \\(y_i\\)).kernel regression estimator becomes: \\[\n\\hat{m}_h(x) = \\frac{\\int y \\, \\hat{f}_{X,Y}(x, y) \\, dy}{\\hat{f}_X(x)}\n= \\frac{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right) y_i}{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right)},\n\\] exactly Nadaraya-Watson estimator.kernel regression estimator becomes: \\[\n\\hat{m}_h(x) = \\frac{\\int y \\, \\hat{f}_{X,Y}(x, y) \\, dy}{\\hat{f}_X(x)}\n= \\frac{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right) y_i}{\\sum_{=1}^n K\\!\\left(\\frac{x - x_i}{h}\\right)},\n\\] exactly Nadaraya-Watson estimator.green curve Nadaraya–Watson estimate.green curve Nadaraya–Watson estimate.dashed red line true regression function.dashed red line true regression function.blue dots observed noisy data.blue dots observed noisy data.estimator smooths data, assigning weight points close evaluation point based Gaussian kernel.estimator smooths data, assigning weight points close evaluation point based Gaussian kernel.orange curve Priestley–Chao estimate.orange curve Priestley–Chao estimate.estimator incorporates spacing consecutive data points (diff(x)), making sensitive non-uniform data spacing.estimator incorporates spacing consecutive data points (diff(x)), making sensitive non-uniform data spacing.performs similarly Nadaraya–Watson data evenly spaced.performs similarly Nadaraya–Watson data evenly spaced.purple curve Gasser–Müller estimate.purple curve Gasser–Müller estimate.estimator uses cumulative kernel functions handle irregular data spacing reduce boundary bias.estimator uses cumulative kernel functions handle irregular data spacing reduce boundary bias.performs well data unevenly distributed.performs well data unevenly distributed.estimators approximate true function well bandwidth appropriately chosen.estimators approximate true function well bandwidth appropriately chosen.Nadaraya–Watson estimator sensitive bandwidth selection assumes uniform data spacing.Nadaraya–Watson estimator sensitive bandwidth selection assumes uniform data spacing.Priestley–Chao estimator accounts data spacing, making flexible uneven data.Priestley–Chao estimator accounts data spacing, making flexible uneven data.Gasser–Müller estimator reduces boundary bias handles irregular data effectively.Gasser–Müller estimator reduces boundary bias handles irregular data effectively.red point indicates optimal bandwidth minimizes cross-validation error.red point indicates optimal bandwidth minimizes cross-validation error.Selecting right bandwidth critical, balances bias variance estimator.Selecting right bandwidth critical, balances bias variance estimator.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n# 1. Simulate Data \nset.seed(123)\n\n\n# Generate predictor x and response y\nn <- 100\nx <-\n    sort(runif(n, 0, 10))  # Sorted for Priestley–Chao and Gasser–Müller\ntrue_function <-\n    function(x)\n        sin(x) + 0.5 * cos(2 * x)  # True regression function\n\n# Add Gaussian noise\ny <-\n    true_function(x) + rnorm(n, sd = 0.3)              \n\n# Visualization of the data\nggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(color = \"darkblue\") +\n    geom_line(aes(y = true_function(x)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(title = \"Simulated Data with True Regression Function\",\n         x = \"x\", y = \"y\") +\n    theme_minimal()\n# Gaussian Kernel Function\ngaussian_kernel <- function(u) {\n    (1 / sqrt(2 * pi)) * exp(-0.5 * u ^ 2)\n}\n\n# Epanechnikov Kernel Function\nepanechnikov_kernel <- function(u) {\n    ifelse(abs(u) <= 1, 0.75 * (1 - u ^ 2), 0)\n}\n\n# Cumulative Kernel for Gasser–Müller (CDF of Gaussian Kernel)\ngaussian_cdf_kernel <- function(u) {\n    pnorm(u, mean = 0, sd = 1)\n}\n# Nadaraya-Watson Estimator\nnadaraya_watson <-\n    function(x_eval, x, y, h, kernel = gaussian_kernel) {\n        sapply(x_eval, function(x0) {\n            weights <- kernel((x0 - x) / h)\n            sum(weights * y) / sum(weights)\n        })\n    }\n\n# Bandwidth Selection (fixed for simplicity)\nh_nw <- 0.5  # Bandwidth for Nadaraya–Watson\n\n# Apply Nadaraya–Watson Estimator\nx_grid <- seq(0, 10, length.out = 200)\nnw_estimate <- nadaraya_watson(x_grid, x, y, h_nw)\n\n# Plot Nadaraya–Watson Estimate\nggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(x_grid, nw_estimate),\n              color = \"green\",\n              linewidth = 1.2) +\n    geom_line(aes(x_grid, true_function(x_grid)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(\n        title = \"Nadaraya–Watson Kernel Estimator\",\n        subtitle = paste(\"Bandwidth (h) =\", h_nw),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n# Priestley–Chao Estimator\npriestley_chao <-\n    function(x_eval, x, y, h, kernel = gaussian_kernel) {\n        sapply(x_eval, function(x0) {\n            weights <- kernel((x0 - x[-length(x)]) / h) * diff(x)\n            sum(weights * y[-length(y)]) / h\n        })\n    }\n\n# Apply Priestley–Chao Estimator\nh_pc <- 0.5\npc_estimate <- priestley_chao(x_grid, x, y, h_pc)\n\n# Plot Priestley–Chao Estimate\nggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(x_grid, pc_estimate),\n              color = \"orange\",\n              size = 1.2) +\n    geom_line(aes(x_grid, true_function(x_grid)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(\n        title = \"Priestley–Chao Kernel Estimator\",\n        subtitle = paste(\"Bandwidth (h) =\", h_pc),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n# Gasser–Müller Estimator\ngasser_mueller <-\n    function(x_eval, x, y, h, cdf_kernel = gaussian_cdf_kernel) {\n        sapply(x_eval, function(x0) {\n            weights <-\n                (cdf_kernel((x0 - x[-length(x)]) / h) - cdf_kernel((x0 - x[-1]) / h))\n            sum(weights * y[-length(y)]) / h\n        })\n    }\n\n# Apply Gasser–Müller Estimator\nh_gm <- 0.5\ngm_estimate <- gasser_mueller(x_grid, x, y, h_gm)\n\n# Plot Gasser–Müller Estimate\nggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(x_grid, gm_estimate),\n              color = \"purple\",\n              size = 1.2) +\n    geom_line(aes(x_grid, true_function(x_grid)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(\n        title = \"Gasser–Müller Kernel Estimator\",\n        subtitle = paste(\"Bandwidth (h) =\", h_gm),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n# Combine all estimates for comparison\nestimates_df <- data.frame(\n    x = x_grid,\n    Nadaraya_Watson = nw_estimate,\n    Priestley_Chao = pc_estimate,\n    Gasser_Mueller = gm_estimate,\n    True_Function = true_function(x_grid)\n)\n\n# Plot all estimators together\nggplot() +\n  geom_point(aes(x, y), color = \"gray60\", alpha = 0.5) +\n  geom_line(aes(x, Nadaraya_Watson, color = \"Nadaraya–Watson\"),\n            data = estimates_df, size = 1.1) +\n  geom_line(aes(x, Priestley_Chao, color = \"Priestley–Chao\"),\n            data = estimates_df, size = 1.1) +\n  geom_line(aes(x, Gasser_Mueller, color = \"Gasser–Müller\"),\n            data = estimates_df, size = 1.1) +\n  geom_line(aes(x, True_Function, color = \"True Function\"),\n            data = estimates_df, linetype = \"dashed\", size = 1) +\n  scale_color_manual(\n    name = \"Estimator\",\n    values = c(\"Nadaraya–Watson\" = \"green\",\n               \"Priestley–Chao\"   = \"orange\",\n               \"Gasser–Müller\"    = \"purple\",\n               \"True Function\"    = \"red\")\n  ) +\n  labs(\n    title = \"Comparison of Kernel-Based Regression Estimators\",\n    x = \"x\",\n    y = \"Estimated m(x)\"\n  ) +\n  theme_minimal()\n# Cross-validation for bandwidth selection (for Nadaraya–Watson)\ncv_bandwidth <- function(h, x, y, kernel = gaussian_kernel) {\n  n <- length(y)\n  cv_error <- 0\n  for (i in 1:n) {\n    x_train <- x[-i]\n    y_train <- y[-i]\n    y_pred <- nadaraya_watson(x[i], x_train, y_train, h, kernel)\n    cv_error <- cv_error + (y[i] - y_pred)^2\n  }\n  return(cv_error / n)\n}\n\n# Optimize bandwidth\nbandwidths <- seq(0.1, 2, by = 0.1)\ncv_errors <- sapply(bandwidths, cv_bandwidth, x = x, y = y)\n\n# Optimal bandwidth\noptimal_h <- bandwidths[which.min(cv_errors)]\noptimal_h\n#> [1] 0.3\n\n# Plot CV errors\nggplot(data.frame(bandwidths, cv_errors), aes(bandwidths, cv_errors)) +\n    geom_line(color = \"blue\") +\n    geom_point(aes(x = optimal_h, y = min(cv_errors)),\n               color = \"red\",\n               size = 3) +\n    labs(title = \"Cross-Validation for Bandwidth Selection\",\n         x = \"Bandwidth (h)\", y = \"CV Error\") +\n    theme_minimal()"},{"path":"sec-nonparametric-regression.html","id":"sec-local-polynomial-regression","chapter":"10 Nonparametric Regression","heading":"10.4 Local Polynomial Regression","text":"Nadaraya-Watson estimator effectively local constant estimator (approximates \\(m(x)\\) constant small neighborhood \\(x\\)), local polynomial regression extends idea fitting local polynomial around point \\(x\\). advantage local polynomials can better handle boundary bias can capture local curvature effectively.","code":""},{"path":"sec-nonparametric-regression.html","id":"local-polynomial-fitting","chapter":"10 Nonparametric Regression","heading":"10.4.1 Local Polynomial Fitting","text":"local polynomial regression degree \\(p\\) point \\(x\\) estimates polynomial function:\\[\nm_x(t) = \\beta_0 + \\beta_1 (t - x) + \\beta_2 (t - x)^2 + \\cdots + \\beta_p (t - x)^p\n\\]best fits data \\(\\{(x_i, y_i)\\}\\) within neighborhood \\(x\\), weighted kernel. Specifically, solve:\\[\n(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p) = \\underset{\\beta_0, \\ldots, \\beta_p}{\\arg\\min} \\sum_{=1}^n \\left[y_i - \\left\\{\\beta_0 + \\beta_1 (x_i - x) + \\cdots + \\beta_p (x_i - x)^p\\right\\}\\right]^2 \\, K\\!\\left(\\frac{x_i - x}{h}\\right).\n\\]estimate:\\[\n\\hat{m}(x) = \\hat{\\beta}_0,\n\\]\\(\\beta_0\\) constant term local polynomial expansion around \\(x\\), represents estimated value point.center polynomial \\(x\\) rather 0?\nCentering \\(x\\) ensures fitted polynomial provides best approximation around \\(x\\). conceptually similar Taylor expansion, local approximations accurate near point expansion.","code":""},{"path":"sec-nonparametric-regression.html","id":"mathematical-form-of-the-solution","chapter":"10 Nonparametric Regression","heading":"10.4.2 Mathematical Form of the Solution","text":"Let \\(\\mathbf{X}_x\\) design matrix local polynomial expansion point \\(x\\). polynomial degree \\(p\\), row \\(\\) \\(\\mathbf{X}_x\\) :\\[\n\\left(1,\\; x_i - x,\\; (x_i - x)^2,\\; \\ldots,\\; (x_i - x)^p \\right).\n\\]Define \\(\\mathbf{W}_x\\) diagonal matrix entries:\\[\n(\\mathbf{W}_x)_{ii} = K\\!\\left(\\frac{x_i - x}{h}\\right),\n\\]representing kernel weights. parameter vector \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T\\) estimated via weighted least squares:\\[\n\\hat{\\boldsymbol{\\beta}}(x) = \\left(\\mathbf{X}_x^T \\mathbf{W}_x \\mathbf{X}_x\\right)^{-1} \\mathbf{X}_x^T \\mathbf{W}_x \\mathbf{y},\n\\]\\(\\mathbf{y} = (y_1, y_2, \\ldots, y_n)^T\\). local polynomial estimator \\(m(x)\\) given :\\[\n\\hat{m}(x) = \\hat{\\beta}_0(x).\n\\]Alternatively, can express concisely using selection vector:\\[\n\\hat{m}(x) = \\mathbf{e}_1^T \\left(\\mathbf{X}_x^T \\mathbf{W}_x \\mathbf{X}_x\\right)^{-1} \\mathbf{X}_x^T \\mathbf{W}_x \\mathbf{y},\n\\]\\(\\mathbf{e}_1 = (1, 0, \\ldots, 0)^T\\) picks intercept term.","code":""},{"path":"sec-nonparametric-regression.html","id":"bias-variance-and-asymptotics","chapter":"10 Nonparametric Regression","heading":"10.4.3 Bias, Variance, and Asymptotics","text":"Local polynomial estimators well-characterized bias variance properties, depend polynomial degree \\(p\\), bandwidth \\(h\\), smoothness true regression function \\(m(x)\\).","code":""},{"path":"sec-nonparametric-regression.html","id":"bias","chapter":"10 Nonparametric Regression","heading":"10.4.3.1 Bias","text":"leading bias term proportional \\(h^{p+1}\\) involves \\((p+1)\\)-th derivative \\(m(x)\\):\n\\[\n\\operatorname{Bias}\\left[\\hat{m}(x)\\right] \\approx \\frac{h^{p+1}}{(p+1)!} m^{(p+1)}(x) \\cdot B(K, p),\n\\]\n\\(B(K, p)\\) constant depending kernel polynomial degree.leading bias term proportional \\(h^{p+1}\\) involves \\((p+1)\\)-th derivative \\(m(x)\\):\\[\n\\operatorname{Bias}\\left[\\hat{m}(x)\\right] \\approx \\frac{h^{p+1}}{(p+1)!} m^{(p+1)}(x) \\cdot B(K, p),\n\\]\\(B(K, p)\\) constant depending kernel polynomial degree.local linear regression (\\(p=1\\)), bias order \\(O(h^2)\\), local quadratic regression (\\(p=2\\)), ’s order \\(O(h^3)\\).local linear regression (\\(p=1\\)), bias order \\(O(h^2)\\), local quadratic regression (\\(p=2\\)), ’s order \\(O(h^3)\\).","code":""},{"path":"sec-nonparametric-regression.html","id":"variance","chapter":"10 Nonparametric Regression","heading":"10.4.3.2 Variance","text":"variance approximately:\n\\[\n\\operatorname{Var}\\left[\\hat{m}(x)\\right] \\approx \\frac{\\sigma^2}{n h} \\cdot V(K, p),\n\\]\n\\(\\sigma^2\\) error variance, \\(V(K, p)\\) another kernel-dependent constant.variance approximately:\\[\n\\operatorname{Var}\\left[\\hat{m}(x)\\right] \\approx \\frac{\\sigma^2}{n h} \\cdot V(K, p),\n\\]\\(\\sigma^2\\) error variance, \\(V(K, p)\\) another kernel-dependent constant.variance decreases larger \\(n\\) larger \\(h\\), increasing \\(h\\) also increases bias, illustrating classic bias-variance trade-.variance decreases larger \\(n\\) larger \\(h\\), increasing \\(h\\) also increases bias, illustrating classic bias-variance trade-.","code":""},{"path":"sec-nonparametric-regression.html","id":"boundary-issues","chapter":"10 Nonparametric Regression","heading":"10.4.3.3 Boundary Issues","text":"One key advantages local polynomial regression—especially local linear regression—ability reduce boundary bias, major issue Nadaraya-Watson estimator. linear term allows fit adjust slope changes near boundaries, kernel becomes asymmetric due fewer data points one side.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-special-case-local-linear-regression","chapter":"10 Nonparametric Regression","heading":"10.4.4 Special Case: Local Linear Regression","text":"Local linear regression (often called local polynomial fit degree 1) particularly popular :mitigates boundary bias effectively, making superior Nadaraya-Watson near edges data.remains computationally simple yet provides better performance local-constant (degree 0) models.robust heteroscedasticity, adapts varying data densities.resulting estimator \\(\\hat{m}(x)\\) simplifies :\\[\n\\hat{m}(x)\n= \\frac{S_{2}(x)\\,S_{0y}(x) \\;-\\; S_{1}(x)\\,S_{1y}(x)}\n       {S_{0}(x)\\,S_{2}(x) \\;-\\; S_{1}^2(x)},\n\\]\\[\nS_k(x) = \\sum_{=1}^n (x_i - x)^k\\, K\\left(\\tfrac{x_i - x}{h}\\right)\n\\]\\[\nS_{k y}(x)\n\\;=\\; \\sum_{=1}^n (x_i - x)^k \\, y_i \\, K\\!\\left(\\tfrac{x_i - x}{h}\\right).\n\\]see local linear fit helps reduce bias, consider approximating \\(m\\) around point \\(x\\) via first-order Taylor expansion:\\[\nm(t) \\;\\approx\\; m(x) \\;+\\; m'(x)\\,(t - x).\n\\]perform local linear regression, solve weighted least squares problem\\[\n\\min_{\\beta_0, \\beta_1} \\sum_{=1}^n \\left[y_i - \\left\\{\\beta_0 + \\beta_1 (x_i - x)\\right\\}\\right]^2\\, K\\left(\\tfrac{x_i - x}{h}\\right).\n\\]assume \\(y_i = m(x_i) + \\varepsilon_i\\), expanding \\(m(x_i)\\) Taylor series around \\(x\\) gives:\\[\nm(x_i)\n\\;=\\; m(x) \\;+\\; m'(x)\\,(x_i - x)\n       \\;+\\; \\tfrac{1}{2}\\,m''(x)\\,(x_i - x)^2\n       \\;+\\; \\cdots.\n\\]\\(x_i\\) close \\(x\\), higher-order terms may small, contribute bias truncate linear term.Let us denote:\\[\nS_0(x)\n\\;=\\; \\sum_{=1}^n K\\!\\left(\\tfrac{x_i - x}{h}\\right),\n\\quad\nS_1(x)\n\\;=\\; \\sum_{=1}^n (x_i - x)\\,K\\!\\left(\\tfrac{x_i - x}{h}\\right),\n\\quad\nS_2(x)\n\\;=\\; \\sum_{=1}^n (x_i - x)^2\\,K\\!\\left(\\tfrac{x_i - x}{h}\\right).\n\\]Similarly, define \\[\n\\sum_{=1}^n y_i\\,K\\!\\left(\\tfrac{x_i - x}{h}\\right)\n\\quad\\text{}\\quad\n\\sum_{=1}^n (x_i - x)\\,y_i\\,K\\!\\left(\\tfrac{x_i - x}{h}\\right)\n\\] right-hand sides. estimated coefficients \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) found solving:\\[\n\\begin{pmatrix}\nS_0(x) & S_1(x)\\\\[6pt]\nS_1(x) & S_2(x)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{=1}^n y_i \\,K\\!\\left(\\tfrac{x_i - x}{h}\\right)\\\\[6pt]\n\\sum_{=1}^n (x_i - x)\\,y_i \\,K\\!\\left(\\tfrac{x_i - x}{h}\\right)\n\\end{pmatrix}.\n\\]\\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) found, identify \\(\\hat{m}(x) = \\hat{\\beta}_0\\).substituting Taylor expansion \\(y_i = m(x_i) + \\varepsilon_i\\) taking expectations, one can derive “extra” \\(\\tfrac12\\,m''(x)\\,(x_i - x)^2\\) terms feed local fit’s bias.expansions associated algebra, one finds:Bias: leading bias term local linear regression typically order \\(h^2\\), often written \\(\\tfrac12\\,m''(x)\\,\\mu_2(K)\\,h^2\\) constant \\(\\mu_2(K)\\) depending kernel’s second moment.Variance: leading variance term single point \\(x\\) order \\(\\tfrac{1}{n\\,h}\\).Balancing two orders magnitude—.e., setting \\(h^2 \\sim \\tfrac{1}{n\\,h}\\)—gives \\(h \\sim n^{-1/3}\\). Consequently, mean squared error \\(x\\) behaves like\\[\n\\left(\\hat{m}(x) - m(x)\\right)^2 \\;=\\; O_p\\!\\left(n^{-2/3}\\right).\n\\]local constant (Nadaraya–Watson) local linear estimators often interior rate, local linear approach can eliminate leading-order bias near boundaries, making preferable many practical settings.","code":""},{"path":"sec-nonparametric-regression.html","id":"bandwidth-selection-1","chapter":"10 Nonparametric Regression","heading":"10.4.5 Bandwidth Selection","text":"Just like kernel regression, bandwidth \\(h\\) controls smoothness local polynomial estimator.Small \\(h\\): Captures fine local details increases variance (potential overfitting).Large \\(h\\): Smooths noise may miss important local structure (potential underfitting).","code":""},{"path":"sec-nonparametric-regression.html","id":"cross-validation-for-local-polynomial-regression","chapter":"10 Nonparametric Regression","heading":"10.4.5.1 Cross-Validation for Local Polynomial Regression","text":"Bandwidth selection via cross-validation also common . leave-one-CV criterion :\\[\n\\mathrm{CV}(h) = \\frac{1}{n} \\sum_{=1}^n \\left(y_i - \\hat{m}_{-,h}(x_i)\\right)^2,\n\\]\\(\\hat{m}_{-,h}(x_i)\\) estimate \\(x_i\\) obtained leaving \\(\\)-th observation.Alternatively, local linear regression, computational shortcuts (like generalized cross-validation) can significantly speed bandwidth selection.Comparison: Nadaraya-Watson vs. Local Polynomial Regression","code":""},{"path":"sec-nonparametric-regression.html","id":"asymptotic-properties-summary","chapter":"10 Nonparametric Regression","heading":"10.4.6 Asymptotic Properties Summary","text":"Consistency: \\(\\hat{m}(x) \\overset{p}{\\longrightarrow} m(x)\\) \\(n \\\\infty\\), mild conditions.Rate Convergence: local linear regression, MSE converges rate \\(O(n^{-4/5})\\), similar kernel regression, better performance boundaries.Optimal Bandwidth: Balances bias (\\(O(h^{p+1})\\)) variance (\\(O(1/(nh))\\)), cross-validation practical selection method.green curve represents local linear regression estimate.green curve represents local linear regression estimate.orange curve represents local quadratic regression estimate.orange curve represents local quadratic regression estimate.dashed red line true regression function.dashed red line true regression function.Boundary effects better handled local polynomial methods, especially quadratic fits capture curvature effectively.Boundary effects better handled local polynomial methods, especially quadratic fits capture curvature effectively.optimal bandwidth minimizes cross-validation error.optimal bandwidth minimizes cross-validation error.red points mark bandwidths yield lowest errors linear quadratic fits.red points mark bandwidths yield lowest errors linear quadratic fits.Smaller bandwidths can overfit, larger bandwidths may oversmooth.Smaller bandwidths can overfit, larger bandwidths may oversmooth.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# 1. Simulate Data \nset.seed(123)\n\n# Generate predictor x and response y\nn <- 100\nx <- sort(runif(n, 0, 10))  # Sorted for local regression\ntrue_function <-\n    function(x)\n        sin(x) + 0.5 * cos(2 * x)  # True regression function\ny <-\n    true_function(x) + rnorm(n, sd = 0.3) # Add Gaussian noise\n\n# Visualization of the data\nggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(color = \"darkblue\") +\n    geom_line(aes(y = true_function(x)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(title = \"Simulated Data with True Regression Function\",\n         x = \"x\", y = \"y\") +\n    theme_minimal()\n# Gaussian Kernel Function\ngaussian_kernel <- function(u) {\n    (1 / sqrt(2 * pi)) * exp(-0.5 * u ^ 2)\n}\n\n# Local Polynomial Regression Function\nlocal_polynomial_regression <-\n    function(x_eval,\n             x,\n             y,\n             h,\n             p = 1,\n             kernel = gaussian_kernel) {\n        sapply(x_eval, function(x0) {\n            # Design matrix for polynomial of degree p\n            X <- sapply(0:p, function(k)\n                (x - x0) ^ k)\n            \n            # Kernel weights\n            W <- diag(kernel((x - x0) / h))\n            \n            # Weighted least squares estimation\n            beta_hat <- solve(t(X) %*% W %*% X, t(X) %*% W %*% y)\n            \n            # Estimated value at x0 (intercept term)\n            beta_hat[1]\n        })\n    }\n\n# Evaluation grid\nx_grid <- seq(0, 10, length.out = 200)\n\n# Apply Local Linear Regression (p = 1)\nh_linear <- 0.8\nllr_estimate <-\n    local_polynomial_regression(x_grid, x, y, h = h_linear, p = 1)\n\n# Apply Local Quadratic Regression (p = 2)\nh_quadratic <- 0.8\nlqr_estimate <-\n    local_polynomial_regression(x_grid, x, y, h = h_quadratic, p = 2)\n# Plot Local Linear Regression\np1 <- ggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(x_grid, llr_estimate),\n              color = \"green\",\n              size = 1.2) +\n    geom_line(aes(x_grid, true_function(x_grid)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(\n        title = \"Local Linear Regression (p = 1)\",\n        subtitle = paste(\"Bandwidth (h) =\", h_linear),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n\n# Plot Local Quadratic Regression\np2 <- ggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(x_grid, lqr_estimate),\n              color = \"orange\",\n              size = 1.2) +\n    geom_line(aes(x_grid, true_function(x_grid)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(\n        title = \"Local Quadratic Regression (p = 2)\",\n        subtitle = paste(\"Bandwidth (h) =\", h_quadratic),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n\n# Display plots side by side\ngrid.arrange(p1, p2, ncol = 2)\n# Leave-One-Out Cross-Validation for Bandwidth Selection\ncv_bandwidth_lp <- function(h, x, y, p = 1, kernel = gaussian_kernel) {\n  n <- length(y)\n  cv_error <- 0\n  \n  for (i in 1:n) {\n    # Leave-one-out data\n    x_train <- x[-i]\n    y_train <- y[-i]\n    \n    # Predict the left-out point\n    y_pred <- local_polynomial_regression(x[i], x_train, y_train, h = h, p = p, kernel = kernel)\n    \n    # Accumulate squared error\n    cv_error <- cv_error + (y[i] - y_pred)^2\n  }\n  \n  return(cv_error / n)\n}\n\n# Bandwidth grid for optimization\nbandwidths <- seq(0.1, 2, by = 0.1)\n\n# Cross-validation errors for local linear regression\ncv_errors_linear <- sapply(bandwidths, cv_bandwidth_lp, x = x, y = y, p = 1)\n\n# Cross-validation errors for local quadratic regression\ncv_errors_quadratic <- sapply(bandwidths, cv_bandwidth_lp, x = x, y = y, p = 2)\n\n# Optimal bandwidths\noptimal_h_linear <- bandwidths[which.min(cv_errors_linear)]\noptimal_h_quadratic <- bandwidths[which.min(cv_errors_quadratic)]\n\n# Display optimal bandwidths\noptimal_h_linear\n#> [1] 0.4\noptimal_h_quadratic\n#> [1] 0.7\n# CV Error Plot for Linear and Quadratic Fits\ncv_data <- data.frame(\n    Bandwidth = rep(bandwidths, 2),\n    CV_Error = c(cv_errors_linear, cv_errors_quadratic),\n    Degree = rep(c(\"Linear (p=1)\", \"Quadratic (p=2)\"), each = length(bandwidths))\n)\n\nggplot(cv_data, aes(x = Bandwidth, y = CV_Error, color = Degree)) +\n    geom_line(size = 1) +\n    geom_point(\n        data = subset(\n            cv_data,\n            Bandwidth %in% c(optimal_h_linear, optimal_h_quadratic)\n        ),\n        aes(x = Bandwidth, y = CV_Error),\n        color = \"red\",\n        size = 3\n    ) +\n    labs(\n        title = \"Cross-Validation for Bandwidth Selection\",\n        subtitle = \"Red points indicate optimal bandwidths\",\n        x = \"Bandwidth (h)\",\n        y = \"CV Error\"\n    ) +\n    theme_minimal() +\n    scale_color_manual(values = c(\"green\", \"orange\"))\n# Apply Local Polynomial Regression with Optimal Bandwidths\nfinal_llr_estimate <-\n    local_polynomial_regression(x_grid, x, y, h = optimal_h_linear, p = 1)\nfinal_lqr_estimate <-\n    local_polynomial_regression(x_grid, x, y, h = optimal_h_quadratic, p = 2)\n\n# Plot final fits\nggplot() +\n    geom_point(aes(x, y), color = \"gray60\", alpha = 0.5) +\n    geom_line(\n        aes(x_grid, final_llr_estimate, color = \"Linear Estimate\"),\n        size = 1.2,\n        linetype = \"solid\"\n    ) +\n    geom_line(\n        aes(x_grid, final_lqr_estimate, color = \"Quadratic Estimate\"),\n        size = 1.2,\n        linetype = \"solid\"\n    ) +\n    geom_line(\n        aes(x_grid, true_function(x_grid), color = \"True Function\"),\n        linetype = \"dashed\"\n    ) +\n    labs(\n        x = \"x\",\n        y = \"Estimated m(x)\",\n        color = \"Legend\"  # Add a legend title\n    ) +\n    scale_color_manual(\n        values = c(\n            \"Linear Estimate\" = \"green\",\n            \"Quadratic Estimate\" = \"orange\",\n            \"True Function\" = \"red\"\n        )\n    ) +\n    theme_minimal()"},{"path":"sec-nonparametric-regression.html","id":"sec-smoothing-splines","chapter":"10 Nonparametric Regression","heading":"10.5 Smoothing Splines","text":"spline piecewise polynomial function smooth junction points (called knots). Smoothing splines provide flexible nonparametric regression technique balancing trade-closely fitting data maintaining smoothness. achieved penalty function’s curvature.univariate case, suppose data \\(\\{(x_i, y_i)\\}_{=1}^n\\) \\(0 \\le x_1 < x_2 < \\cdots < x_n \\le 1\\) (rescaling always possible needed). smoothing spline estimator \\(\\hat{m}(x)\\) defined solution following optimization problem:\\[\n\\hat{m}(x) = \\underset{f \\\\mathcal{H}}{\\arg\\min} \\left\\{ \\sum_{=1}^n \\left(y_i - f(x_i)\\right)^2 + \\lambda \\int_{0}^{1} \\left(f''(t)\\right)^2 \\, dt \\right\\},\n\\]:first term measures lack fit (residual sum squares).second term roughness penalty discourages excessive curvature \\(f\\), controlled smoothing parameter \\(\\lambda \\ge 0\\).space \\(\\mathcal{H}\\) denotes set twice-differentiable functions \\([0,1]\\).Special Cases:\\(\\lambda = 0\\): penalty applied, solution interpolates data exactly (interpolating spline).\\(\\lambda \\\\infty\\): penalty dominates, forcing solution smooth possible—reducing linear regression (since second derivative straight line zero).","code":""},{"path":"sec-nonparametric-regression.html","id":"properties-and-form-of-the-smoothing-spline","chapter":"10 Nonparametric Regression","heading":"10.5.1 Properties and Form of the Smoothing Spline","text":"key result spline theory minimizer \\(\\hat{m}(x)\\) natural cubic spline knots observed data points \\(\\{x_1, \\ldots, x_n\\}\\). result holds despite fact minimizing infinite-dimensional space functions.solution can expressed :\\[\n\\hat{m}(x) = a_0 + a_1 x + \\sum_{j=1}^n b_j \\, (x - x_j)_+^3,\n\\]:\\((u)_+ = \\max(u, 0)\\) positive part function (cubic spline basis function),coefficients \\(\\{a_0, a_1, b_1, \\ldots, b_n\\}\\) determined solving system linear equations derived optimization problem.form implies spline cubic polynomial within interval data points, smooth transitions knots. smoothness conditions ensure continuity function first second derivatives knot.","code":""},{"path":"sec-nonparametric-regression.html","id":"choice-of-lambda","chapter":"10 Nonparametric Regression","heading":"10.5.2 Choice of \\(\\lambda\\)","text":"smoothing parameter \\(\\lambda\\) plays crucial role controlling trade-goodness--fit smoothness:Large \\(\\lambda\\): Imposes strong penalty roughness, leading smoother (potentially underfitted) function captures broad trends.Small \\(\\lambda\\): Allows function closely follow data, possibly resulting overfitting data noisy.common approach selecting \\(\\lambda\\) generalized cross-validation (GCV), provides efficient approximation leave-one-cross-validation:\\[\n\\mathrm{GCV}(\\lambda) = \\frac{\\frac{1}{n} \\sum_{=1}^n \\left(y_i - \\hat{m}_{\\lambda}(x_i)\\right)^2}{\\left[1 - \\frac{\\operatorname{tr}(\\mathbf{S}_\\lambda)}{n}\\right]^2},\n\\]:\\(\\hat{m}_{\\lambda}(x_i)\\) fitted value \\(x_i\\) given \\(\\lambda\\),\\(\\mathbf{S}_\\lambda\\) smoothing matrix (influence matrix) \\(\\hat{\\mathbf{y}} = \\mathbf{S}_\\lambda \\mathbf{y}\\),\\(\\operatorname{tr}(\\mathbf{S}_\\lambda)\\) effective degrees freedom, reflecting model’s flexibility.optimal \\(\\lambda\\) minimizes GCV score, balancing fit complexity without need refit model multiple times (traditional cross-validation).","code":""},{"path":"sec-nonparametric-regression.html","id":"connection-to-reproducing-kernel-hilbert-spaces","chapter":"10 Nonparametric Regression","heading":"10.5.3 Connection to Reproducing Kernel Hilbert Spaces","text":"Smoothing splines can viewed lens reproducing kernel Hilbert spaces (RKHS). penalty term:\\[\n\\int_{0}^{1} \\left(f''(t)\\right)^2 \\, dt\n\\]defines semi-norm corresponds squared norm \\(f\\) particular RKHS associated cubic spline kernel. interpretation reveals smoothing splines equivalent solving regularization problem RKHS, penalty controls smoothness solution.connection extends naturally general kernel-based methods (e.g., Gaussian process regression, kernel ridge regression) higher-dimensional spline models.green curve fit optimal \\(\\lambda\\) selected automatically via GCV.orange curve (spar = 0.8) smoother, capturing broad trends missing finer details.purple curve (spar = 0.4) flexible, fitting data closely, potentially overfitting noise.red solid line represents true regression function.blue curve shows GCV score changes different smoothing parameters (spar).red point indicates optimal smoothing parameter minimizes GCV score.Low spar values correspond flexible fits (risking overfitting), high spar values produce smoother fits (risking underfitting).","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# 1. Simulate Data\nset.seed(123)\n\n\n# Generate predictor x and response y\nn <- 100\nx <- sort(runif(n, 0, 10))  # Sorted for smoother visualization\ntrue_function <-\n    function(x)\n        sin(x) + 0.5 * cos(2 * x)  # True regression function\ny <-\n    true_function(x) + rnorm(n, sd = 0.3) # Add Gaussian noise\n\n# Visualization of the data\nggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(color = \"darkblue\", alpha = 0.6) +\n    geom_line(aes(y = true_function(x)),\n              color = \"red\",\n              linetype = \"dashed\") +\n    labs(title = \"Simulated Data with True Regression Function\",\n         x = \"x\", y = \"y\") +\n    theme_minimal()\n# Apply Smoothing Spline with Default Lambda \n# (automatically selected using GCV)\nspline_fit_default <- smooth.spline(x, y)\n\n# Apply Smoothing Spline with Manual Lambda \n# (via smoothing parameter 'spar')\nspline_fit_smooth <- smooth.spline(x, y, spar = 0.8)  # Smoother fit\nspline_fit_flexible <-\n    smooth.spline(x, y, spar = 0.4)  # More flexible fit\n\n# Create grid for prediction\nx_grid               <- seq(0, 10, length.out = 200)\nspline_pred_default  <- predict(spline_fit_default, x_grid)\nspline_pred_smooth   <- predict(spline_fit_smooth, x_grid)\nspline_pred_flexible <- predict(spline_fit_flexible, x_grid)\n# Plot Smoothing Splines with Different Smoothness Levels\nggplot() +\n    geom_point(aes(x, y), color = \"darkblue\", alpha = 0.5) +\n    geom_line(aes(x_grid, spline_pred_default$y),\n              color = \"green\",\n              size = 1.2) +\n    geom_line(\n        aes(x_grid, spline_pred_smooth$y),\n        color = \"orange\",\n        size = 1.2,\n        linetype = \"dotted\"\n    ) +\n    geom_line(\n        aes(x_grid, spline_pred_flexible$y),\n        color = \"purple\",\n        size = 1.2,\n        linetype = \"dashed\"\n    ) +\n    geom_line(\n        aes(x_grid, true_function(x_grid)),\n        color = \"red\",\n        linetype = \"solid\",\n        size = 1\n    ) +\n    labs(\n        title = \"Smoothing Spline Fits\",\n        subtitle = \"Green: GCV-selected | Orange: Smooth (spar=0.8) | Purple: Flexible (spar=0.4)\",\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()\n# Extract Generalized Cross-Validation (GCV) Scores\nspline_fit_default$cv.crit  # GCV criterion for the default fit\n#> [1] 0.09698728\n\n# Compare GCV for different spar values\nspar_values <- seq(0.1, 1.5, by = 0.05)\ngcv_scores <- sapply(spar_values, function(spar) {\n    fit <- smooth.spline(x, y, spar = spar)\n    fit$cv.crit\n})\n\n# Optimal spar corresponding to the minimum GCV score\noptimal_spar <- spar_values[which.min(gcv_scores)]\noptimal_spar\n#> [1] 0.7\n# GCV Plot\nggplot(data.frame(spar_values, gcv_scores),\n       aes(x = spar_values, y = gcv_scores)) +\n    geom_line(color = \"blue\", size = 1) +\n    geom_point(aes(x = optimal_spar, y = min(gcv_scores)),\n               color = \"red\",\n               size = 3) +\n    labs(\n        title = \"GCV for Smoothing Parameter Selection\",\n        subtitle = paste(\"Optimal spar =\", round(optimal_spar, 2)),\n        x = \"Smoothing Parameter (spar)\",\n        y = \"GCV Score\"\n    ) +\n    theme_minimal()\n# Apply Smoothing Spline with Optimal spar\nspline_fit_optimal <- smooth.spline(x, y, spar = optimal_spar)\nspline_pred_optimal <- predict(spline_fit_optimal, x_grid)\n\n# Plot Final Fit\nggplot() +\n    geom_point(aes(x, y), color = \"gray60\", alpha = 0.5) +\n    geom_line(aes(x_grid, spline_pred_optimal$y),\n              color = \"green\",\n              size = 1.5) +\n    geom_line(\n        aes(x_grid, true_function(x_grid)),\n        color = \"red\",\n        linetype = \"dashed\",\n        size = 1.2\n    ) +\n    labs(\n        title = \"Smoothing Spline with Optimal Smoothing Parameter\",\n        subtitle = paste(\"Optimal spar =\", round(optimal_spar, 2)),\n        x = \"x\",\n        y = \"Estimated m(x)\"\n    ) +\n    theme_minimal()"},{"path":"sec-nonparametric-regression.html","id":"confidence-intervals-in-nonparametric-regression","chapter":"10 Nonparametric Regression","heading":"10.6 Confidence Intervals in Nonparametric Regression","text":"Constructing confidence intervals (bands) nonparametric regression estimators like kernel smoothers, local polynomials, smoothing splines complex parametric models. key challenges arise flexible nature models dependence bias variance local data density smoothing parameters.","code":""},{"path":"sec-nonparametric-regression.html","id":"asymptotic-normality","chapter":"10 Nonparametric Regression","heading":"10.6.1 Asymptotic Normality","text":"regularity conditions, nonparametric estimators asymptotically normal. given point \\(x\\), :\\[\n\\sqrt{n h} \\left\\{\\hat{m}(x) - m(x)\\right\\} \\overset{\\mathcal{D}}{\\longrightarrow} \\mathcal{N}\\left(0, \\sigma^2 \\, \\nu(x)\\right),\n\\]:\\(n\\) sample size,\\(h\\) bandwidth (kernel local polynomial estimators) function \\(\\lambda\\) (smoothing splines),\\(\\sigma^2\\) variance errors,\\(\\nu(x)\\) function depends estimator, kernel, local data density.approximate \\((1 - \\alpha)\\) pointwise confidence interval \\(m(x)\\) given :\\[\n\\hat{m}(x) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\widehat{\\operatorname{Var}}[\\hat{m}(x)]},\n\\]:\\(z_{\\alpha/2}\\) \\((1 - \\alpha/2)\\) quantile standard normal distribution,\\(\\widehat{\\operatorname{Var}}[\\hat{m}(x)]\\) estimate variance, can obtained using plug-methods, sandwich estimators, resampling techniques.","code":""},{"path":"sec-nonparametric-regression.html","id":"bootstrap-methods","chapter":"10 Nonparametric Regression","heading":"10.6.2 Bootstrap Methods","text":"bootstrap provides powerful alternative constructing confidence intervals bands, particularly asymptotic approximations unreliable (e.g., small sample sizes near boundaries).","code":""},{"path":"sec-nonparametric-regression.html","id":"bootstrap-approaches","chapter":"10 Nonparametric Regression","heading":"10.6.2.1 Bootstrap Approaches","text":"Residual Bootstrap:\nFit nonparametric model obtain residuals \\(\\hat{\\varepsilon}_i = y_i - \\hat{m}(x_i)\\).\nGenerate bootstrap samples \\(y_i^* = \\hat{m}(x_i) + \\varepsilon_i^*\\), \\(\\varepsilon_i^*\\) resampled (replacement) \\(\\{\\hat{\\varepsilon}_i\\}\\).\nRefit model bootstrap sample obtain \\(\\hat{m}^*(x)\\).\nRepeat many times build empirical distribution \\(\\hat{m}^*(x)\\).\nFit nonparametric model obtain residuals \\(\\hat{\\varepsilon}_i = y_i - \\hat{m}(x_i)\\).Generate bootstrap samples \\(y_i^* = \\hat{m}(x_i) + \\varepsilon_i^*\\), \\(\\varepsilon_i^*\\) resampled (replacement) \\(\\{\\hat{\\varepsilon}_i\\}\\).Refit model bootstrap sample obtain \\(\\hat{m}^*(x)\\).Repeat many times build empirical distribution \\(\\hat{m}^*(x)\\).Wild Bootstrap:\nParticularly useful heteroscedastic data. Instead resampling residuals directly, multiply random variables mean zero unit variance preserve variance structure.","code":""},{"path":"sec-nonparametric-regression.html","id":"bootstrap-confidence-bands","chapter":"10 Nonparametric Regression","heading":"10.6.2.2 Bootstrap Confidence Bands","text":"pointwise confidence intervals cover true function specific \\(x\\) probability \\((1 - \\alpha)\\), simultaneous confidence bands cover entire function interval desired confidence level. Bootstrap methods can adapted estimate bands capturing distribution maximum deviation \\(\\hat{m}(x)\\) \\(m(x)\\) range \\(x\\).","code":""},{"path":"sec-nonparametric-regression.html","id":"practical-considerations-4","chapter":"10 Nonparametric Regression","heading":"10.6.3 Practical Considerations","text":"Bias Correction:\nNonparametric estimators often non-negligible bias, especially near boundaries. Bias correction techniques undersmoothing (choosing smaller bandwidth) sometimes used improve interval coverage.Bias Correction:\nNonparametric estimators often non-negligible bias, especially near boundaries. Bias correction techniques undersmoothing (choosing smaller bandwidth) sometimes used improve interval coverage.Effective Degrees Freedom:\nsmoothing splines, effective degrees freedom (related \\(\\operatorname{tr}(\\mathbf{S}_\\lambda)\\)) provide insight model complexity influence confidence interval construction.Effective Degrees Freedom:\nsmoothing splines, effective degrees freedom (related \\(\\operatorname{tr}(\\mathbf{S}_\\lambda)\\)) provide insight model complexity influence confidence interval construction.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-generalized-additive-models","chapter":"10 Nonparametric Regression","heading":"10.7 Generalized Additive Models","text":"generalized additive model (GAM) extends generalized linear models allowing additive smooth terms:\\[\ng(\\mathbb{E}[Y]) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p),\n\\]:\\(g(\\cdot)\\) link function (GLMs),\\(\\beta_0\\) intercept,\\(f_j\\) smooth, potentially nonparametric function (e.g., spline, kernel smoother, local polynomial smoother),\\(p\\) number predictors, \\(p \\ge 2\\) highlighting flexibility GAMs handling multivariate data.structure allows nonlinear relationships predictor \\(X_j\\) response \\(Y\\), maintaining additivity, simplifies interpretation compared fully nonparametric models.Traditional linear models assume strictly linear relationship:\\[\ng(\\mathbb{E}[Y]) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p.\n\\]However, real-world data often exhibit complex, nonlinear patterns. generalized linear models extend linear models non-Gaussian responses, still rely linear predictors. GAMs address replacing linear terms smooth functions:GLMs: Linear effects (e.g., \\(\\beta_1 X_1\\))GAMs: Nonlinear smooth effects (e.g., \\(f_1(X_1)\\))general form GAM :\\[\ng(\\mathbb{E}[Y \\mid \\mathbf{X}]) = \\beta_0 + \\sum_{j=1}^p f_j(X_j),\n\\]:\\(Y\\) response variable,\\(\\mathbf{X} = (X_1, X_2, \\ldots, X_p)\\) predictors,\\(f_j\\) smooth functions capturing potentially nonlinear effects,link function \\(g(\\cdot)\\) connects mean \\(Y\\) additive predictor.Special Cases:\\(g\\) identity function \\(Y\\) continuous: reduces additive model (special case GAM).\\(g\\) logit function \\(Y\\) binary: logistic GAM classification tasks.\\(g\\) log function \\(Y\\) follows Poisson distribution: Poisson GAM count data.","code":""},{"path":"sec-nonparametric-regression.html","id":"estimation-via-penalized-likelihood","chapter":"10 Nonparametric Regression","heading":"10.7.1 Estimation via Penalized Likelihood","text":"GAMs typically estimated using penalized likelihood methods balance model fit smoothness. objective function :\\[\n\\mathcal{L}_{\\text{pen}} = \\ell(\\beta_0, f_1, \\ldots, f_p) - \\frac{1}{2} \\sum_{j=1}^p \\lambda_j \\int \\left(f_j''(x)\\right)^2 dx,\n\\]:\\(\\ell(\\beta_0, f_1, \\ldots, f_p)\\) (log-)likelihood data,\\(\\lambda_j \\ge 0\\) smoothing parameters controlling smoothness \\(f_j\\),penalty term \\(\\int (f_j'')^2 dx\\) discourages excessive curvature, similar smoothing splines.","code":""},{"path":"sec-nonparametric-regression.html","id":"backfitting-algorithm","chapter":"10 Nonparametric Regression","heading":"10.7.1.1 Backfitting Algorithm","text":"continuous responses, classic backfitting algorithm often used:Initialize: Start initial guess \\(f_j\\), typically zero.Iterate: \\(j = 1, \\dots, p\\):\nCompute partial residuals: \\[\nr_j = y - \\beta_0 - \\sum_{k \\neq j} f_k(X_k)\n\\]\nUpdate \\(f_j\\) fitting smoother \\((X_j, r_j)\\).\nCompute partial residuals: \\[\nr_j = y - \\beta_0 - \\sum_{k \\neq j} f_k(X_k)\n\\]Update \\(f_j\\) fitting smoother \\((X_j, r_j)\\).Convergence: Repeat functions \\(f_j\\) stabilize.approach works additive structure, allows smooth term updated conditionally others.","code":""},{"path":"sec-nonparametric-regression.html","id":"generalized-additive-model-estimation-for-glms","chapter":"10 Nonparametric Regression","heading":"10.7.1.2 Generalized Additive Model Estimation (for GLMs)","text":"\\(Y\\) non-Gaussian (e.g., binary, count data), use iteratively reweighted least squares (IRLS) combination backfitting. Popular implementations, mgcv package R, use penalized likelihood estimation efficient computational algorithms (e.g., penalized iteratively reweighted least squares).","code":""},{"path":"sec-nonparametric-regression.html","id":"interpretation-of-gams","chapter":"10 Nonparametric Regression","heading":"10.7.2 Interpretation of GAMs","text":"One key advantages GAMs interpretability, especially compared fully nonparametric black-box machine learning models.Additive Structure: predictor’s effect modeled separately via \\(f_j(X_j)\\), making easy interpret marginal effects.Partial Dependence Plots: Visualization \\(f_j(X_j)\\) shows predictor affects response, holding variables constant.Example:marketing dataset predicting customer purchase probability:\\[\n\\log\\left(\\frac{\\mathbb{P}(\\text{Purchase})}{1 - \\mathbb{P}(\\text{Purchase})}\\right) = \\beta_0 + f_1(\\text{Age}) + f_2(\\text{Income}) + f_3(\\text{Ad Exposure})\n\\]\\(f_1(\\text{Age})\\) might show peak purchase likelihood middle-aged customers.\\(f_2(\\text{Income})\\) reveal threshold effect purchases increase beyond certain income level.\\(f_3(\\text{Ad Exposure})\\) might show diminishing returns repeated exposures.","code":""},{"path":"sec-nonparametric-regression.html","id":"model-selection-and-smoothing-parameter-estimation","chapter":"10 Nonparametric Regression","heading":"10.7.3 Model Selection and Smoothing Parameter Estimation","text":"smoothing parameters \\(\\lambda_j\\) control complexity smooth term:Large \\(\\lambda_j\\): Strong smoothing, leading nearly linear fits.Small \\(\\lambda_j\\): Flexible, wiggly fits may overfit \\(\\lambda_j\\) small.Methods Choosing \\(\\lambda_j\\):Generalized Cross-Validation (GCV): \\[\n\\mathrm{GCV} = \\frac{1}{n} \\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{\\left(1 - \\frac{\\operatorname{tr}(\\mathbf{S})}{n}\\right)^2}\n\\] \\(\\mathbf{S}\\) smoother matrix. GCV popular method selecting smoothing parameter \\(\\lambda_j\\) approximates leave-one-cross-validation without requiring explicit refitting model. term \\(\\text{tr}(\\mathbf{S})\\) represents effective degrees freedom smoother, denominator penalizes overfitting.Generalized Cross-Validation (GCV): \\[\n\\mathrm{GCV} = \\frac{1}{n} \\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{\\left(1 - \\frac{\\operatorname{tr}(\\mathbf{S})}{n}\\right)^2}\n\\] \\(\\mathbf{S}\\) smoother matrix. GCV popular method selecting smoothing parameter \\(\\lambda_j\\) approximates leave-one-cross-validation without requiring explicit refitting model. term \\(\\text{tr}(\\mathbf{S})\\) represents effective degrees freedom smoother, denominator penalizes overfitting.Unbiased Risk Estimation: method extends idea GCV non-Gaussian families (e.g., Poisson, binomial). aims minimize unbiased estimate risk (expected prediction error). Gaussian models, often reduces form similar GCV, distributions, incorporates appropriate likelihood deviance.Unbiased Risk Estimation: method extends idea GCV non-Gaussian families (e.g., Poisson, binomial). aims minimize unbiased estimate risk (expected prediction error). Gaussian models, often reduces form similar GCV, distributions, incorporates appropriate likelihood deviance.Akaike Information Criterion (AIC): \\[\nAIC=−2\\log⁡(L)+2tr⁡(S)\n\\] \\(L\\) likelihood model. AIC balances model fit (measured likelihood) complexity (measured effective degrees freedom \\(tr⁡(S)\\)). smoothing parameter \\(\\lambda_j\\) chosen minimize AIC.Akaike Information Criterion (AIC): \\[\nAIC=−2\\log⁡(L)+2tr⁡(S)\n\\] \\(L\\) likelihood model. AIC balances model fit (measured likelihood) complexity (measured effective degrees freedom \\(tr⁡(S)\\)). smoothing parameter \\(\\lambda_j\\) chosen minimize AIC.Bayesian Information Criterion (BIC): \\[\nBIC=−2\\log⁡(L)+\\log⁡(n)tr⁡(S)\n\\] \\(n\\) sample size. BIC similar AIC imposes stronger penalty model complexity, making suitable larger datasets.Bayesian Information Criterion (BIC): \\[\nBIC=−2\\log⁡(L)+\\log⁡(n)tr⁡(S)\n\\] \\(n\\) sample size. BIC similar AIC imposes stronger penalty model complexity, making suitable larger datasets.Leave-One-Cross-Validation (LOOCV): \\[\nLOOCV = \\frac{1}{n}\\sum_{= 1}^n ( y_i - \\hat{y}_i^{(-)})^2,\n\\] \\(y_i^{(−)}\\) predicted value ii-th observation model fitted without . LOOCV computationally intensive provides direct estimate prediction error.Leave-One-Cross-Validation (LOOCV): \\[\nLOOCV = \\frac{1}{n}\\sum_{= 1}^n ( y_i - \\hat{y}_i^{(-)})^2,\n\\] \\(y_i^{(−)}\\) predicted value ii-th observation model fitted without . LOOCV computationally intensive provides direct estimate prediction error.Empirical Risk Minimization:\nnon-parametric regression methods, \\(\\lambda_j\\) can chosen minimizing empirical risk (e.g., mean squared error) validation set via resampling techniques like \\(k\\)-fold cross-validation.Empirical Risk Minimization:\nnon-parametric regression methods, \\(\\lambda_j\\) can chosen minimizing empirical risk (e.g., mean squared error) validation set via resampling techniques like \\(k\\)-fold cross-validation.","code":""},{"path":"sec-nonparametric-regression.html","id":"extensions-of-gams","chapter":"10 Nonparametric Regression","heading":"10.7.4 Extensions of GAMs","text":"GAM Interaction Terms: \\[\ng(\\mathbb{E}[Y]) = \\beta_0 + f_1(X_1) + f_2(X_2) + f_{12}(X_1, X_2)\n\\] \\(f_{12}\\) captures interaction \\(X_1\\) \\(X_2\\) (using tensor product smooths).GAM Interaction Terms: \\[\ng(\\mathbb{E}[Y]) = \\beta_0 + f_1(X_1) + f_2(X_2) + f_{12}(X_1, X_2)\n\\] \\(f_{12}\\) captures interaction \\(X_1\\) \\(X_2\\) (using tensor product smooths).GAMMs (Generalized Additive Mixed Models): Incorporate random effects handle hierarchical grouped data.GAMMs (Generalized Additive Mixed Models): Incorporate random effects handle hierarchical grouped data.Varying Coefficient Models: Allow regression coefficients vary smoothly another variable, e.g., \\[\nY = \\beta_0 + f_1(Z) \\cdot X + \\varepsilon\n\\]Varying Coefficient Models: Allow regression coefficients vary smoothly another variable, e.g., \\[\nY = \\beta_0 + f_1(Z) \\cdot X + \\varepsilon\n\\]Lower AIC indicates better model balancing fit complexity.GCV score helps select optimal level smoothness.Compare models prevent overfitting (flexible) underfitting (simple).tensor product smooth te(x1, x2) captures nonlinear interactions x1 x2.contour plot visualizes joint effect influences response.logistic GAM models nonlinear effects log-odds binary outcome.Smooth plots indicate predictors’ influence probability success.Residual plots assess model fit.QQ plot checks normality residuals.K-index evaluates adequacy smoothness selection.","code":"\n# Load necessary libraries\nlibrary(mgcv)    # For fitting GAMs\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Simulate Data\nset.seed(123)\n\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, 0, 5)\nx3 <- rnorm(n, 5, 2)\n\n# True nonlinear functions\nf1 <- function(x)\n    sin(x)                 # Nonlinear effect of x1\nf2 <- function(x)\n    log(x + 1)             # Nonlinear effect of x2\nf3 <- function(x)\n    0.5 * (x - 5) ^ 2        # Quadratic effect for x3\n\n# Generate response variable with noise\ny <- 3 + f1(x1) + f2(x2) - f3(x3) + rnorm(n, sd = 1)\n\n# Data frame for analysis\ndata_gam <- data.frame(y, x1, x2, x3)\n\n# Plotting the true functions with simulated data\np1 <-\n    ggplot(data_gam, aes(x1, y)) + \n    geom_point() + \n    labs(title = \"Effect of x1 (sin(x1))\")\np2 <-\n    ggplot(data_gam, aes(x2, y)) + \n    geom_point() + \n    labs(title = \"Effect of x2 (log(x2+1))\")\np3 <-\n    ggplot(data_gam, aes(x3, y)) + \n    geom_point() + \n    labs(title = \"Effect of x3 (quadratic)\")\n\n# Display plots side by side\ngrid.arrange(p1, p2, p3, ncol = 3)\n# Fit a GAM using mgcv\ngam_model <-\n    gam(y ~ s(x1) + s(x2) + s(x3),\n        data = data_gam, method = \"REML\")\n\n# Summary of the model\nsummary(gam_model)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> y ~ s(x1) + s(x2) + s(x3)\n#> \n#> Parametric coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.63937    0.09511   27.75   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>         edf Ref.df       F p-value    \n#> s(x1) 5.997  7.165   7.966   5e-07 ***\n#> s(x2) 1.000  1.000  10.249 0.00192 ** \n#> s(x3) 6.239  7.343 105.551 < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =   0.91   Deviance explained = 92.2%\n#> -REML = 155.23  Scale est. = 0.90463   n = 100\n# Plot smooth terms\npar(mfrow = c(1, 3))  # Arrange plots in one row\nplot(gam_model, shade = TRUE, seWithMean = TRUE)\npar(mfrow = c(1, 1))  # Reset plotting layout\n# Using ggplot2 with mgcv's predict function\npred_data <- with(data_gam, expand.grid(\n    x1 = seq(min(x1), max(x1), length.out = 100),\n    x2 = mean(x2),\n    x3 = mean(x3)\n))\n\n# Predictions for x1 effect\npred_data$pred_x1 <-\n    predict(gam_model, newdata = pred_data, type = \"response\")\n\nggplot(pred_data, aes(x1, pred_x1)) +\n    geom_line(color = \"blue\", size = 1.2) +\n    labs(title = \"Partial Effect of x1\",\n         x = \"x1\",\n         y = \"Effect on y\") +\n    theme_minimal()\n# Check AIC and GCV score\nAIC(gam_model)\n#> [1] 289.8201\ngam_model$gcv.ubre  # GCV/UBRE score\n#>     REML \n#> 155.2314 \n#> attr(,\"Dp\")\n#> [1] 47.99998\n\n\n# Compare models with different smoothness\ngam_model_simple <-\n    gam(y ~ s(x1, k = 4) + s(x2, k = 4) + s(x3, k = 4),\n        data = data_gam)\ngam_model_complex <-\n    gam(y ~ s(x1, k = 20) + s(x2, k = 20) + s(x3, k = 20),\n        data = data_gam)\n\n# Compare models using AIC\nAIC(gam_model, gam_model_simple, gam_model_complex)\n#>                          df      AIC\n#> gam_model         15.706428 289.8201\n#> gam_model_simple   8.429889 322.1502\n#> gam_model_complex 13.571165 287.4171\n# GAM with interaction using tensor product smooths\ngam_interaction <- gam(y ~ te(x1, x2) + s(x3),\n                       data = data_gam)\n\n# Summary of the interaction model\nsummary(gam_interaction)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> y ~ te(x1, x2) + s(x3)\n#> \n#> Parametric coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.63937    0.09364   28.19   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>             edf Ref.df       F p-value    \n#> te(x1,x2) 8.545  8.923   9.218  <2e-16 ***\n#> s(x3)     4.766  5.834 147.595  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.912   Deviance explained = 92.4%\n#> GCV = 1.0233  Scale est. = 0.87688   n = 100\n\n# Visualization of interaction effect\nvis.gam(\n    gam_interaction,\n    view = c(\"x1\", \"x2\"),\n    plot.type = \"contour\",\n    color = \"terrain\"\n)\n# Simulate binary response\nset.seed(123)\nprob <- plogis(1 + f1(x1) - f2(x2) + 0.3 * x3)  # Logistic function\ny_bin <- rbinom(n, 1, prob)                     # Binary outcome\n\n# Fit GAM for binary classification\ngam_logistic <-\n    gam(y_bin ~ s(x1) + s(x2) + s(x3),\n        family = binomial,\n        data = data_gam)\n\n# Summary and visualization\nsummary(gam_logistic)\n#> \n#> Family: binomial \n#> Link function: logit \n#> \n#> Formula:\n#> y_bin ~ s(x1) + s(x2) + s(x3)\n#> \n#> Parametric coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)    22.30      32.18   0.693    0.488\n#> \n#> Approximate significance of smooth terms:\n#>         edf Ref.df Chi.sq p-value\n#> s(x1) 4.472  5.313  2.645   0.775\n#> s(x2) 1.000  1.000  1.925   0.165\n#> s(x3) 1.000  1.000  1.390   0.238\n#> \n#> R-sq.(adj) =      1   Deviance explained = 99.8%\n#> UBRE = -0.84802  Scale est. = 1         n = 100\npar(mfrow = c(1, 3))\nplot(gam_logistic, shade = TRUE)\npar(mfrow = c(1, 1))\n# Diagnostic plots\npar(mfrow = c(2, 2))\ngam.check(gam_model)#> \n#> Method: REML   Optimizer: outer newton\n#> full convergence after 9 iterations.\n#> Gradient range [-5.387854e-05,2.006026e-05]\n#> (score 155.2314 & scale 0.9046299).\n#> Hessian positive definite, eigenvalue range [5.387409e-05,48.28647].\n#> Model rank =  28 / 28 \n#> \n#> Basis dimension (k) checking results. Low p-value (k-index<1) may\n#> indicate that k is too low, especially if edf is close to k'.\n#> \n#>         k'  edf k-index p-value\n#> s(x1) 9.00 6.00    1.01    0.46\n#> s(x2) 9.00 1.00    1.16    0.92\n#> s(x3) 9.00 6.24    1.07    0.72\npar(mfrow = c(1, 1))"},{"path":"sec-nonparametric-regression.html","id":"regression-trees-and-random-forests","chapter":"10 Nonparametric Regression","heading":"10.8 Regression Trees and Random Forests","text":"Though typically framed “kernel” “spline,” tree-based methods—Classification Regression Trees (CART) random forests—also nonparametric models. assume predetermined functional form relationship predictors response. Instead, adaptively partition predictor space regions, fitting simple models (usually constants linear models) within region.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-regression-trees","chapter":"10 Nonparametric Regression","heading":"10.8.1 Regression Trees","text":"Classification Regression Trees (CART) algorithm foundation tree-based models (Breiman 2017). regression settings, CART models response variable piecewise constant function.regression tree recursively partitions predictor space disjoint regions, \\(R_1, R_2, \\ldots, R_M\\), predicts response constant within region:\\[\n\\hat{m}(x) = \\sum_{m=1}^{M} c_m \\cdot \\mathbb{}(x \\R_m),\n\\]:\\(c_m\\) predicted value (usually mean \\(y_i\\)) observations region \\(R_m\\),\\(\\mathbb{}(\\cdot)\\) indicator function.Tree-Building Algorithm (Greedy Recursive Partitioning):Start full dataset single region.Find best split:\nConsider possible splits predictors (e.g., \\(X_j < s\\)).\nChoose split minimizes residual sum squares (RSS): \\[\n\\text{RSS} = \\sum_{\\R_1} (y_i - \\bar{y}_{R_1})^2 + \\sum_{\\R_2} (y_i - \\bar{y}_{R_2})^2,\n\\] \\(\\bar{y}_{R_k}\\) mean response region \\(R_k\\).\nConsider possible splits predictors (e.g., \\(X_j < s\\)).Choose split minimizes residual sum squares (RSS): \\[\n\\text{RSS} = \\sum_{\\R_1} (y_i - \\bar{y}_{R_1})^2 + \\sum_{\\R_2} (y_i - \\bar{y}_{R_2})^2,\n\\] \\(\\bar{y}_{R_k}\\) mean response region \\(R_k\\).Recursively repeat splitting process new region (node) stopping criterion met (e.g., minimum number observations per leaf, maximum tree depth).Assign constant prediction terminal node (leaf) based average response observations within node.Stopping Criteria Pruning:Pre-pruning (early stopping): Halt tree growth predefined condition met (e.g., minimal node size, maximal depth).Post-pruning (cost-complexity pruning): Grow large tree first, prune back avoid overfitting. cost-complexity criterion : \\[\nC_\\alpha(T) = \\text{RSS}(T) + \\alpha |T|,\n\\] \\(|T|\\) number terminal nodes (leaves) \\(\\alpha\\) controls penalty complexity.Advantages Regression Trees:Interpretability: Easy visualize understand.Handling different data types: Can naturally handle numerical categorical variables.Nonlinear relationships interactions: Captures complex patterns without explicit modeling.Limitations:High variance: Trees sensitive small changes data (unstable).Overfitting risk: Without pruning regularization, deep trees can overfit noise.","code":""},{"path":"sec-nonparametric-regression.html","id":"sec-random-forests","chapter":"10 Nonparametric Regression","heading":"10.8.2 Random Forests","text":"address high variance single trees, random forests combine many regression trees create ensemble model improved predictive performance stability (Breiman 2001).random forest builds multiple decision trees aggregates predictions reduce variance. regression, final prediction average predictions individual trees:\\[\n\\hat{m}_{\\text{RF}}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{m}^{(b)}(x),\n\\]:\\(B\\) number trees forest,\\(\\hat{m}^{(b)}(x)\\) prediction \\(b\\)-th tree.Random Forest Algorithm:Bootstrap Sampling: tree, draw bootstrap sample training data (sampling replacement).Random Feature Selection: split tree:\nRandomly select subset predictors (usually \\(\\sqrt{p}\\) classification \\(p/3\\) regression).\nFind best split among selected features.\nRandomly select subset predictors (usually \\(\\sqrt{p}\\) classification \\(p/3\\) regression).Find best split among selected features.Tree Growth: Grow tree full depth without pruning.Aggregation: regression, average predictions trees. classification, use majority voting.Random Forest Work?Bagging (Bootstrap Aggregating): Reduces variance averaging multiple models.Random Feature Selection: Decorrelates trees, reducing variance.","code":""},{"path":"sec-nonparametric-regression.html","id":"theoretical-insights","chapter":"10 Nonparametric Regression","heading":"10.8.3 Theoretical Insights","text":"Bias-Variance Trade-offRegression Trees: Low bias high variance.Random Forests: Slightly higher bias single tree (due randomization) significantly reduced variance, leading lower overall prediction error.--Bag (OOB) ErrorRandom forests provide internal estimate prediction error using --bag samples (data included bootstrap sample given tree). OOB error computed :observation, predict response using trees included bootstrap sample.Calculate error comparing OOB predictions true responses.OOB error serves efficient, unbiased estimate test error without need cross-validation.","code":""},{"path":"sec-nonparametric-regression.html","id":"feature-importance-in-random-forests","chapter":"10 Nonparametric Regression","heading":"10.8.4 Feature Importance in Random Forests","text":"Random forests naturally provide measures variable importance, helping identify predictors contribute model.Mean Decrease Impurity (MDI): Measures total reduction impurity (e.g., RSS) attributed variable across trees.Permutation Importance: Measures increase prediction error values predictor randomly permuted, breaking relationship response.","code":""},{"path":"sec-nonparametric-regression.html","id":"advantages-and-limitations-of-tree-based-methods","chapter":"10 Nonparametric Regression","heading":"10.8.5 Advantages and Limitations of Tree-Based Methods","text":"Splits made based conditions (e.g., x1 < 4.2), partitioning predictor space.Terminal nodes (leaves) show predicted value (mean response region).tree depth affects interpretability overfitting risk.Pruning reduces overfitting simplifying tree.optimal CP minimizes cross-validation error, balancing complexity fit.shallower tree improves generalization unseen data.MSE decreases trees added.% Variance Explained reflects predictive performance.mtry = 2 indicates 2 random predictors considered split.OOB error stabilizes trees added, providing unbiased estimate test error.Helps determine trees improve performance model converged.Mean Decrease MSE indicates much model’s error increases variable permuted.Mean Decrease Node Impurity reflects much variable reduces variance splits.Variables higher importance influential model.pruned regression tree (blue) shows step-like predictions, characteristic piecewise constant fits.random forest (green) provides smoother fit averaging across many trees, reducing variance.OOB error (Random Forest) provides efficient, unbiased estimate without cross-validation.Cross-validation error (Regression Tree) evaluates generalization resampling.Random Forest often shows lower MSE due reduced variance.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(rpart)           # For regression trees\nlibrary(rpart.plot)       # For visualizing trees\nlibrary(randomForest)     # For random forests\nlibrary(gridExtra)\n\n# Simulate Data\nset.seed(123)\n\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, 0, 5)\nx3 <- rnorm(n, 5, 2)\n\n# Nonlinear functions\nf1 <- function(x)\n    sin(x)\nf2 <- function(x)\n    log(x + 1)\nf3 <- function(x)\n    0.5 * (x - 5) ^ 2\n\n# Generate response variable with noise\ny <- 3 + f1(x1) + f2(x2) - f3(x3) + rnorm(n, sd = 1)\n\n# Data frame\ndata_tree <- data.frame(y, x1, x2, x3)\n\n# Quick visualization of data\np1 <- ggplot(data_tree, aes(x1, y)) +\n    geom_point() +\n    labs(title = \"Effect of x1\")\np2 <- ggplot(data_tree, aes(x2, y)) +\n    geom_point() +\n    labs(title = \"Effect of x2\")\np3 <- ggplot(data_tree, aes(x3, y)) +\n    geom_point() +\n    labs(title = \"Effect of x3\")\n\ngrid.arrange(p1, p2, p3, ncol = 3)\n# Fit a Regression Tree using rpart\ntree_model <-\n    rpart(\n        y ~ x1 + x2 + x3,\n        data = data_tree,\n        method = \"anova\",\n        control = rpart.control(cp = 0.01)\n    )  # cp = complexity parameter\n\n# Summary of the tree\nsummary(tree_model)\n#> Call:\n#> rpart(formula = y ~ x1 + x2 + x3, data = data_tree, method = \"anova\", \n#>     control = rpart.control(cp = 0.01))\n#>   n= 100 \n#> \n#>           CP nsplit rel error    xerror      xstd\n#> 1 0.39895879      0 1.0000000 1.0134781 0.3406703\n#> 2 0.17470339      1 0.6010412 0.8649973 0.3336272\n#> 3 0.04607373      2 0.4263378 0.5707932 0.1880333\n#> 4 0.02754858      3 0.3802641 0.5287366 0.1866728\n#> 5 0.01584638      4 0.3527155 0.5061104 0.1867491\n#> 6 0.01032524      5 0.3368691 0.5136765 0.1861020\n#> 7 0.01000000      7 0.3162187 0.4847072 0.1861849\n#> \n#> Variable importance\n#> x3 x2 x1 \n#> 91  6  3 \n#> \n#> Node number 1: 100 observations,    complexity param=0.3989588\n#>   mean=2.639375, MSE=9.897038 \n#>   left son=2 (7 obs) right son=3 (93 obs)\n#>   Primary splits:\n#>       x3 < 7.707736  to the right, improve=0.39895880, (0 missing)\n#>       x1 < 6.84138   to the left,  improve=0.07685517, (0 missing)\n#>       x2 < 2.627429  to the left,  improve=0.04029839, (0 missing)\n#> \n#> Node number 2: 7 observations\n#>   mean=-4.603469, MSE=24.47372 \n#> \n#> Node number 3: 93 observations,    complexity param=0.1747034\n#>   mean=3.184535, MSE=4.554158 \n#>   left son=6 (18 obs) right son=7 (75 obs)\n#>   Primary splits:\n#>       x3 < 2.967495  to the left,  improve=0.40823990, (0 missing)\n#>       x2 < 1.001856  to the left,  improve=0.07353453, (0 missing)\n#>       x1 < 6.84138   to the left,  improve=0.07049507, (0 missing)\n#>   Surrogate splits:\n#>       x2 < 0.3435293 to the left,  agree=0.828, adj=0.111, (0 split)\n#> \n#> Node number 6: 18 observations\n#>   mean=0.4012593, MSE=3.4521 \n#> \n#> Node number 7: 75 observations,    complexity param=0.04607373\n#>   mean=3.852521, MSE=2.513258 \n#>   left son=14 (12 obs) right son=15 (63 obs)\n#>   Primary splits:\n#>       x3 < 6.324486  to the right, improve=0.24191360, (0 missing)\n#>       x2 < 1.603258  to the left,  improve=0.10759280, (0 missing)\n#>       x1 < 6.793804  to the left,  improve=0.09106168, (0 missing)\n#> \n#> Node number 14: 12 observations\n#>   mean=2.065917, MSE=2.252311 \n#> \n#> Node number 15: 63 observations,    complexity param=0.02754858\n#>   mean=4.192826, MSE=1.839163 \n#>   left son=30 (9 obs) right son=31 (54 obs)\n#>   Primary splits:\n#>       x3 < 3.548257  to the left,  improve=0.2353119, (0 missing)\n#>       x2 < 1.349633  to the left,  improve=0.1103019, (0 missing)\n#>       x1 < 7.006669  to the left,  improve=0.1019295, (0 missing)\n#> \n#> Node number 30: 9 observations\n#>   mean=2.581411, MSE=0.3669647 \n#> \n#> Node number 31: 54 observations,    complexity param=0.01584638\n#>   mean=4.461396, MSE=1.579623 \n#>   left son=62 (10 obs) right son=63 (44 obs)\n#>   Primary splits:\n#>       x2 < 1.130662  to the left,  improve=0.18386040, (0 missing)\n#>       x1 < 6.209961  to the left,  improve=0.14561510, (0 missing)\n#>       x3 < 4.517029  to the left,  improve=0.01044883, (0 missing)\n#> \n#> Node number 62: 10 observations\n#>   mean=3.330957, MSE=2.001022 \n#> \n#> Node number 63: 44 observations,    complexity param=0.01032524\n#>   mean=4.718314, MSE=1.127413 \n#>   left son=126 (27 obs) right son=127 (17 obs)\n#>   Primary splits:\n#>       x1 < 6.468044  to the left,  improve=0.16079230, (0 missing)\n#>       x3 < 5.608708  to the right, improve=0.05277854, (0 missing)\n#>       x2 < 2.784688  to the left,  improve=0.03145241, (0 missing)\n#>   Surrogate splits:\n#>       x2 < 3.074905  to the left,  agree=0.636, adj=0.059, (0 split)\n#>       x3 < 5.888028  to the left,  agree=0.636, adj=0.059, (0 split)\n#> \n#> Node number 126: 27 observations,    complexity param=0.01032524\n#>   mean=4.380469, MSE=1.04313 \n#>   left son=252 (12 obs) right son=253 (15 obs)\n#>   Primary splits:\n#>       x1 < 3.658072  to the right, improve=0.4424566, (0 missing)\n#>       x3 < 4.270123  to the right, improve=0.1430466, (0 missing)\n#>       x2 < 2.658809  to the left,  improve=0.1121999, (0 missing)\n#>   Surrogate splits:\n#>       x2 < 2.707432  to the left,  agree=0.815, adj=0.583, (0 split)\n#>       x3 < 4.010151  to the right, agree=0.593, adj=0.083, (0 split)\n#> \n#> Node number 127: 17 observations\n#>   mean=5.25489, MSE=0.7920812 \n#> \n#> Node number 252: 12 observations\n#>   mean=3.620914, MSE=0.6204645 \n#> \n#> Node number 253: 15 observations\n#>   mean=4.988114, MSE=0.5504908\n\n# Visualize the Regression Tree\nrpart.plot(\n    tree_model,\n    type = 2,\n    extra = 101,\n    fallen.leaves = TRUE,\n    main = \"Regression Tree\"\n)\n# Optimal pruning based on cross-validation error\nprintcp(tree_model) # Displays CP table with cross-validation error\n#> \n#> Regression tree:\n#> rpart(formula = y ~ x1 + x2 + x3, data = data_tree, method = \"anova\", \n#>     control = rpart.control(cp = 0.01))\n#> \n#> Variables actually used in tree construction:\n#> [1] x1 x2 x3\n#> \n#> Root node error: 989.7/100 = 9.897\n#> \n#> n= 100 \n#> \n#>         CP nsplit rel error  xerror    xstd\n#> 1 0.398959      0   1.00000 1.01348 0.34067\n#> 2 0.174703      1   0.60104 0.86500 0.33363\n#> 3 0.046074      2   0.42634 0.57079 0.18803\n#> 4 0.027549      3   0.38026 0.52874 0.18667\n#> 5 0.015846      4   0.35272 0.50611 0.18675\n#> 6 0.010325      5   0.33687 0.51368 0.18610\n#> 7 0.010000      7   0.31622 0.48471 0.18618\noptimal_cp <-\n    tree_model$cptable[which.min(tree_model$cptable[, \"xerror\"]), \"CP\"]\n\n# Prune the tree\npruned_tree <- prune(tree_model, cp = optimal_cp)\n\n# Visualize the pruned tree\nrpart.plot(\n    pruned_tree,\n    type = 2,\n    extra = 101,\n    fallen.leaves = TRUE,\n    main = \"Pruned Regression Tree\"\n)\n# Fit a Random Forest\nset.seed(123)\nrf_model <- randomForest(\n    y ~ x1 + x2 + x3,\n    data = data_tree,\n    ntree = 500,\n    mtry = 2,\n    importance = TRUE\n)\n\n# Summary of the Random Forest\nprint(rf_model)\n#> \n#> Call:\n#>  randomForest(formula = y ~ x1 + x2 + x3, data = data_tree, ntree = 500,      mtry = 2, importance = TRUE) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 2\n#> \n#>           Mean of squared residuals: 3.031589\n#>                     % Var explained: 69.37\n# Plot OOB Error vs. Number of Trees\nplot(rf_model, main = \"Out-of-Bag Error for Random Forest\")\n# Variable Importance\nimportance(rf_model)                # Numerical importance measures\n#>      %IncMSE IncNodePurity\n#> x1 10.145674     137.09918\n#> x2  1.472662      77.41256\n#> x3 44.232816     718.49567\nvarImpPlot(rf_model, main = \"Variable Importance (Random Forest)\")\n# Predictions on new data\nx_new <- seq(0, 10, length.out = 200)\ntest_data <- data.frame(x1 = x_new,\n                        x2 = mean(x2),\n                        x3 = mean(x3))\n\n# Predictions\ntree_pred <- predict(pruned_tree, newdata = test_data)\nrf_pred <- predict(rf_model, newdata = test_data)\n\n# Visualization\nggplot() +\n    geom_point(aes(x1, y),\n               data = data_tree,\n               alpha = 0.5,\n               color = \"gray\") +\n    geom_line(\n        aes(x_new, tree_pred),\n        color = \"blue\",\n        size = 1.2,\n        linetype = \"dashed\"\n    ) +\n    geom_line(aes(x_new, rf_pred), color = \"green\", size = 1.2) +\n    labs(\n        title = \"Regression Tree vs. Random Forest\",\n        subtitle = \"Blue: Pruned Tree | Green: Random Forest\",\n        x = \"x1\",\n        y = \"Predicted y\"\n    ) +\n    theme_minimal()\n# OOB Error (Random Forest)\noob_mse <- rf_model$mse[length(rf_model$mse)]  # Final OOB MSE\n\n# Cross-Validation Error (Regression Tree)\ncv_mse_tree <-\n    min(tree_model$cptable[, \"xerror\"]) * var(data_tree$y)\n\n# Compare OOB and CV errors\ndata.frame(\n    Model = c(\"Pruned Regression Tree\", \"Random Forest\"),\n    MSE = c(cv_mse_tree, oob_mse)\n)\n#>                    Model      MSE\n#> 1 Pruned Regression Tree 4.845622\n#> 2          Random Forest 3.031589"},{"path":"sec-nonparametric-regression.html","id":"sec-wavelet-regression","chapter":"10 Nonparametric Regression","heading":"10.9 Wavelet Regression","text":"Wavelet regression nonparametric regression technique represents target function combination wavelet basis functions. Unlike traditional basis functions (e.g., polynomials splines), wavelets excellent localization properties time (space) frequency domains. makes wavelet regression particularly effective capturing local features, sharp changes, discontinuities, transient patterns.wavelet function \\(\\psi(t)\\) oscillates (like wave) localized time frequency. key idea represent function linear combination shifted scaled versions mother wavelet \\(\\psi(t)\\).Wavelet basis functions generated scaling translating mother wavelet:\\[\n\\psi_{j,k}(t) = 2^{j/2} \\, \\psi(2^j t - k),\n\\]:\\(j\\) (scale parameter): Controls frequency—larger \\(j\\) captures finer details (high-frequency components).\\(k\\) (translation parameter): Controls location—shifting wavelet along time space axis.factor \\(2^{j/2}\\) ensures wavelet basis functions orthonormal.addition mother wavelet \\(\\psi(t)\\), ’s also scaling function \\(\\phi(t)\\), captures low-frequency (smooth) components data.","code":""},{"path":"sec-nonparametric-regression.html","id":"wavelet-series-expansion","chapter":"10 Nonparametric Regression","heading":"10.9.1 Wavelet Series Expansion","text":"Just Fourier analysis represents functions sums sines cosines, wavelet analysis represents function sum wavelet basis functions:\\[\nf(t) = \\sum_{k} c_{J_0, k} \\, \\phi_{J_0, k}(t) + \\sum_{j = J_0}^{J_{\\max}} \\sum_{k} d_{j, k} \\, \\psi_{j, k}(t),\n\\]:\\(c_{J_0, k}\\) approximation coefficients coarsest scale \\(J_0\\), capturing smooth trends,\\(d_{j, k}\\) detail coefficients scale \\(j\\), capturing finer details,\\(\\phi_{J_0, k}(t)\\) scaling functions, \\(\\psi_{j, k}(t)\\) wavelet functions.goal wavelet regression estimate coefficients based observed data.","code":""},{"path":"sec-nonparametric-regression.html","id":"wavelet-regression-model","chapter":"10 Nonparametric Regression","heading":"10.9.2 Wavelet Regression Model","text":"Given data \\(\\{(x_i, y_i)\\}_{=1}^n\\), wavelet regression model assumes:\\[\ny_i = f(x_i) + \\varepsilon_i,\n\\]:\\(f(x)\\) unknown regression function,\\(\\varepsilon_i\\) ..d. errors mean zero variance \\(\\sigma^2\\).approximate \\(f(x)\\) using finite number wavelet basis functions:\\[\n\\hat{f}(x) = \\sum_{k} \\hat{c}_{J_0, k} \\, \\phi_{J_0, k}(x) + \\sum_{j = J_0}^{J_{\\max}} \\sum_{k} \\hat{d}_{j, k} \\, \\psi_{j, k}(x),\n\\]coefficients \\(\\hat{c}_{J_0, k}\\) \\(\\hat{d}_{j, k}\\) estimated data.Coefficient Estimation:Linear Wavelet Regression: Estimate coefficients via least squares, projecting data onto wavelet basis.Nonlinear Wavelet Regression (Thresholding): Apply shrinkage thresholding (e.g., hard soft thresholding) detail coefficients \\(d_{j, k}\\) reduce noise prevent overfitting. especially useful true signal sparse wavelet domain.","code":""},{"path":"sec-nonparametric-regression.html","id":"wavelet-shrinkage-and-thresholding","chapter":"10 Nonparametric Regression","heading":"10.9.3 Wavelet Shrinkage and Thresholding","text":"Wavelet shrinkage powerful denoising technique introduced Donoho Johnstone (1995). idea suppress small coefficients (likely noise) retaining large coefficients (likely contain true signal).Thresholding Rules:Hard Thresholding:\\[\n\\hat{d}_{j, k}^{\\text{(hard)}} =\n\\begin{cases}\nd_{j, k}, & \\text{} |d_{j, k}| > \\tau, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\]Soft Thresholding:\\[\n\\hat{d}_{j, k}^{\\text{(soft)}} =\n\\operatorname{sign}(d_{j, k}) \\cdot \\max\\left(|d_{j, k}| - \\tau, \\, 0\\right),\n\\]\\(\\tau\\) threshold parameter, often chosen based noise level (e.g., via cross-validation universal thresholding).Advantages:Local Feature Detection: Excellent capturing sharp changes, discontinuities, localized phenomena.Local Feature Detection: Excellent capturing sharp changes, discontinuities, localized phenomena.Multiresolution Analysis: Analyzes data multiple scales, making effective global trends fine details.Multiresolution Analysis: Analyzes data multiple scales, making effective global trends fine details.Denoising Capability: Powerful noise reduction thresholding wavelet domain.Denoising Capability: Powerful noise reduction thresholding wavelet domain.Limitations:Complexity: Requires careful selection wavelet basis, decomposition levels, thresholding methods.Complexity: Requires careful selection wavelet basis, decomposition levels, thresholding methods.Less Interpretability: Coefficients less interpretable compared spline tree-based methods.Less Interpretability: Coefficients less interpretable compared spline tree-based methods.Boundary Effects: May suffer artifacts near data boundaries without proper treatment.Boundary Effects: May suffer artifacts near data boundaries without proper treatment.wf = \"haar\": Haar wavelet, simple effective detecting discontinuities.n.levels = 4: Number decomposition levels (captures details different scales).DWT output contains approximation coefficients detail coefficients level.Hard Thresholding: Keeps coefficients threshold, sets others zero.Soft Thresholding: Shrinks coefficients toward zero, reducing potential noise smoothly.Lower MSE indicates better denoising performance.Soft thresholding often achieves smoother results lower MSE.","code":"\n# Simulated data: Noisy signal with discontinuities\nset.seed(123)\nn <- 96\nx <- seq(0, 1, length.out = n)\nsignal <-\n    sin(4 * pi * x) + ifelse(x > 0.5, 1, 0)  # Discontinuity at x = 0.5\ny <- signal + rnorm(n, sd = 0.2)\n\n# Wavelet Regression using Discrete Wavelet Transform (DWT)\nlibrary(waveslim)\n\n# Apply DWT with the correct parameter name\ndwt_result <- dwt(y, wf = \"haar\", n.levels = 4)\n\n# Thresholding (currently hard thresholding)\nthreshold <- 0.2\ndwt_result_thresh <- dwt_result\nfor (i in 1:4) {\n    dwt_result_thresh[[i]] <-\n        ifelse(abs(dwt_result[[i]]) > threshold, dwt_result[[i]], 0)\n}\n\n# for soft thresholding\n# for (i in 1:4) {\n#     dwt_result_thresh[[i]] <-\n#         sign(dwt_result[[i]]) * pmax(abs(dwt_result[[i]]) - threshold, 0)\n# }\n\n\n# Inverse DWT to reconstruct the signal\ny_hat <- idwt(dwt_result_thresh)\n\n# Plotting\nplot(\n    x,\n    y,\n    type = \"l\",\n    col = \"gray\",\n    lwd = 1,\n    main = \"Wavelet Regression (Denoising)\"\n)\nlines(x,\n      signal,\n      col = \"blue\",\n      lwd = 2,\n      lty = 2)  # True signal\nlines(x, y_hat, col = \"red\", lwd = 2)             # Denoised estimate\nlegend(\n    \"topright\",\n    legend = c(\"Noisy Data\", \"True Signal\", \"Wavelet Estimate\"),\n    col = c(\"gray\", \"blue\", \"red\"),\n    lty = c(1, 2, 1),\n    lwd = c(1, 2, 2)\n)\n# Load necessary libraries\nlibrary(waveslim)   # For Discrete Wavelet Transform (DWT)\nlibrary(ggplot2)\n\n\n# Simulate Data: Noisy signal with discontinuities\nset.seed(123)\nn <- 96\nx <- seq(0, 1, length.out = n)\n\n# True signal: Sinusoidal with a discontinuity at x = 0.5\nsignal <- sin(4 * pi * x) + ifelse(x > 0.5, 1, 0)\n\n# Add Gaussian noise\ny <- signal + rnorm(n, sd = 0.2)\n\n# Plot the noisy data and true signal\nggplot() +\n    geom_line(aes(x, y),\n              color = \"gray\",\n              size = 0.8,\n              alpha = 0.7) +\n    geom_line(\n        aes(x, signal),\n        color = \"blue\",\n        linetype = \"dashed\",\n        size = 1\n    ) +\n    labs(title = \"Noisy Data with Underlying True Signal\",\n         x = \"x\", y = \"Signal\") +\n    theme_minimal()\n# Apply Discrete Wavelet Transform (DWT) using Haar wavelet\ndwt_result <- dwt(y, wf = \"haar\", n.levels = 4)\n\n# View DWT structure\nstr(dwt_result)\n#> List of 5\n#>  $ d1: num [1:48] 0.14 -0.1221 0.3017 -0.1831 0.0745 ...\n#>  $ d2: num [1:24] 0.50003 -0.06828 0.35298 0.01582 -0.00853 ...\n#>  $ d3: num [1:12] 0.669 0.128 -0.746 -0.532 -0.258 ...\n#>  $ d4: num [1:6] 1.07 -1.766 0.752 0.663 -1.963 ...\n#>  $ s4: num [1:6] 2.963 -0.159 -2.758 6.945 3.845 ...\n#>  - attr(*, \"class\")= chr \"dwt\"\n#>  - attr(*, \"wavelet\")= chr \"haar\"\n#>  - attr(*, \"boundary\")= chr \"periodic\"\n# Hard Thresholding\nthreshold <- 0.2  # Chosen threshold for demonstration\ndwt_hard_thresh <- dwt_result\n\n# Apply hard thresholding to detail coefficients\nfor (i in 1:4) {\n  dwt_hard_thresh[[i]] <- ifelse(abs(dwt_result[[i]]) > threshold, dwt_result[[i]], 0)\n}\n# Soft Thresholding\ndwt_soft_thresh <- dwt_result\n\n# Apply soft thresholding to detail coefficients\nfor (i in 1:4) {\n  dwt_soft_thresh[[i]] <- sign(dwt_result[[i]]) * pmax(abs(dwt_result[[i]]) - threshold, 0)\n}\n# Reconstruct the denoised signals using Inverse DWT\ny_hat_hard <- idwt(dwt_hard_thresh)\ny_hat_soft <- idwt(dwt_soft_thresh)\n# Combine data for ggplot\ndf_plot <- data.frame(\n    x = rep(x, 4),\n    y = c(y, signal, y_hat_hard, y_hat_soft),\n    Type = rep(\n        c(\n            \"Noisy Data\",\n            \"True Signal\",\n            \"Hard Thresholding\",\n            \"Soft Thresholding\"\n        ),\n        each = n\n    )\n)\n\n# Plotting\nggplot(df_plot, aes(x, y, color = Type, linetype = Type)) +\n    geom_line(size = 1) +\n    scale_color_manual(values = c(\"gray\", \"blue\", \"red\", \"green\")) +\n    scale_linetype_manual(values = c(\"solid\", \"dashed\", \"solid\", \"solid\")) +\n    labs(\n        title = \"Wavelet Regression (Denoising)\",\n        subtitle = \"Comparison of Hard vs. Soft Thresholding\",\n        x = \"x\",\n        y = \"Signal\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n# Compute Mean Squared Error (MSE) for each method\nmse_noisy <- mean((y - signal) ^ 2)\nmse_hard <- mean((y_hat_hard - signal) ^ 2)\nmse_soft <- mean((y_hat_soft - signal) ^ 2)\n\n# Display MSE comparison\ndata.frame(\n    Method = c(\"Noisy Data\", \"Hard Thresholding\", \"Soft Thresholding\"),\n    MSE = c(mse_noisy, mse_hard, mse_soft)\n)\n#>              Method        MSE\n#> 1        Noisy Data 0.03127707\n#> 2 Hard Thresholding 0.02814465\n#> 3 Soft Thresholding 0.02267171"},{"path":"sec-nonparametric-regression.html","id":"multivariate-nonparametric-regression","chapter":"10 Nonparametric Regression","heading":"10.10 Multivariate Nonparametric Regression","text":"Nonparametric regression higher dimensions (\\(p > 1\\)) presents significant challenges compared univariate case. difficulty arises primarily curse dimensionality, refers exponential growth data requirements number predictors increases.","code":""},{"path":"sec-nonparametric-regression.html","id":"the-curse-of-dimensionality","chapter":"10 Nonparametric Regression","heading":"10.10.1 The Curse of Dimensionality","text":"curse dimensionality refers various phenomena occur analyzing organizing data high-dimensional spaces. context nonparametric regression:Data sparsity: number dimensions increases, data become sparse. Even large datasets may adequately cover predictor space.Exponential sample size growth: achieve level accuracy, required sample size grows exponentially number dimensions. example, maintain density points moving 1D 2D, need roughly square sample size.Illustration:Consider estimating function unit cube \\([0,1]^p\\). need 10 points per dimension capture structure:1D: 10 points suffice.2D: \\(10^2 = 100\\) points needed.10D: \\(10^{10} = 10,000,000,000\\) points required.makes traditional nonparametric methods, like kernel smoothing, impractical high dimensions without additional structure assumptions.","code":""},{"path":"sec-nonparametric-regression.html","id":"multivariate-kernel-regression","chapter":"10 Nonparametric Regression","heading":"10.10.2 Multivariate Kernel Regression","text":"straightforward extension kernel regression higher dimensions multivariate Nadaraya-Watson estimator:\\[\n\\hat{m}_h(\\mathbf{x}) = \\frac{\\sum_{=1}^n K_h(\\mathbf{x} - \\mathbf{x}_i) \\, y_i}{\\sum_{=1}^n K_h(\\mathbf{x} - \\mathbf{x}_i)},\n\\]:\\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) predictor vector,\\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) predictor vector,\\(K_h(\\cdot)\\) multivariate kernel function, often product univariate kernels:\n\\[\nK_h(\\mathbf{x} - \\mathbf{x}_i) = \\prod_{j=1}^p K\\left(\\frac{x_j - x_{ij}}{h_j}\\right),\n\\]\\(K_h(\\cdot)\\) multivariate kernel function, often product univariate kernels:\\[\nK_h(\\mathbf{x} - \\mathbf{x}_i) = \\prod_{j=1}^p K\\left(\\frac{x_j - x_{ij}}{h_j}\\right),\n\\]\\(h_j\\) bandwidth \\(j\\)-th predictor.\\(h_j\\) bandwidth \\(j\\)-th predictor.Challenges:product kernel suffers inefficiency high dimensions data points far given target point \\(\\mathbf{x}\\), resulting small kernel weights.Selecting optimal multivariate bandwidth matrix complex computationally intensive.","code":""},{"path":"sec-nonparametric-regression.html","id":"multivariate-splines","chapter":"10 Nonparametric Regression","heading":"10.10.3 Multivariate Splines","text":"Extending splines multiple dimensions involves sophisticated techniques, simplicity piecewise polynomials 1D generalize easily.","code":""},{"path":"sec-nonparametric-regression.html","id":"thin-plate-splines","chapter":"10 Nonparametric Regression","heading":"10.10.3.1 Thin-Plate Splines","text":"Thin-plate splines generalize cubic splines higher dimensions. minimize smoothness penalty depends derivatives function:\\[\n\\hat{m}(\\mathbf{x}) = \\underset{f}{\\arg\\min} \\left\\{ \\sum_{=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\lambda \\int \\|\\nabla^2 f(\\mathbf{x})\\|^2 d\\mathbf{x} \\right\\},\n\\]\\(\\nabla^2 f(\\mathbf{x})\\) Hessian matrix second derivatives, \\(\\|\\cdot\\|^2\\) represents sum squared elements.Thin-plate splines rotation-invariant require explicit placement knots, become computationally expensive number dimensions increases.","code":""},{"path":"sec-nonparametric-regression.html","id":"tensor-product-splines","chapter":"10 Nonparametric Regression","heading":"10.10.3.2 Tensor Product Splines","text":"structured multivariate data, tensor product splines commonly used. construct basis predictor form multivariate basis via tensor products:\\[\n\\hat{m}(\\mathbf{x}) = \\sum_{=1}^{K_1} \\sum_{j=1}^{K_2} \\beta_{ij} \\, B_i(x_1) \\, B_j(x_2),\n\\]\\(B_i(x_1)\\) \\(B_j(x_2)\\) spline basis functions \\(x_1\\) \\(x_2\\), respectively.Tensor products allow flexible modeling interactions predictors can lead large numbers parameters number dimensions increases.","code":""},{"path":"sec-nonparametric-regression.html","id":"additive-models-gams","chapter":"10 Nonparametric Regression","heading":"10.10.4 Additive Models (GAMs)","text":"powerful approach mitigating curse dimensionality assume additive structure regression function:\\[\nm(\\mathbf{x}) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p),\n\\]\\(f_j\\) univariate smooth function estimated nonparametrically.","code":""},{"path":"sec-nonparametric-regression.html","id":"why-additive-models-help","chapter":"10 Nonparametric Regression","heading":"10.10.4.1 Why Additive Models Help:","text":"Dimensionality Reduction: Instead estimating full \\(p\\)-dimensional surface, estimate \\(p\\) separate functions 1D.Interpretability: function \\(f_j(x_j)\\) represents effect predictor \\(X_j\\) response, holding variables constant.Extensions:Interactions: Additive models can extended include interactions:\n\\[\nm(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j) + \\sum_{j < k} f_{jk}(x_j, x_k),\n\\]\n\\(f_{jk}\\) bivariate smooth functions capturing interaction effects.Interactions: Additive models can extended include interactions:\\[\nm(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j) + \\sum_{j < k} f_{jk}(x_j, x_k),\n\\]\\(f_{jk}\\) bivariate smooth functions capturing interaction effects.","code":""},{"path":"sec-nonparametric-regression.html","id":"radial-basis-functions","chapter":"10 Nonparametric Regression","heading":"10.10.5 Radial Basis Functions","text":"Radial basis functions (RBF) another approach multivariate nonparametric regression, particularly effective scattered data interpolation.typical RBF model :\\[\n\\hat{m}(\\mathbf{x}) = \\sum_{=1}^n \\alpha_i \\, \\phi(\\|\\mathbf{x} - \\mathbf{x}_i\\|),\n\\]:\\(\\phi(\\cdot)\\) radial basis function (e.g., Gaussian: \\(\\phi(r) = e^{-\\gamma r^2}\\)),\\(\\|\\mathbf{x} - \\mathbf{x}_i\\|\\) Euclidean distance \\(\\mathbf{x}\\) data point \\(\\mathbf{x}_i\\),\\(\\alpha_i\\) coefficients estimated data.Kernel regression captures nonlinear interactions suffers data sparsity high dimensions (curse dimensionality).Bandwidth selection critical performance.Thin-plate splines handle smooth surfaces well rotation-invariant.Computational cost increases higher dimensions due matrix operations.Tensor product splines model interactions explicitly variables.Suitable data structured dependencies can lead many parameters higher dimensions.GAMs Help:Dimensionality reduction: Instead estimating full multivariate function, GAM estimates separate 1D functions predictor.Interpretability: Easy understand individual effects.Limitations: capture complex interactions unless explicitly added.RBFs capture complex, smooth surfaces interactions based distance.Perform well scattered data can computationally expensive large datasets.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(np)       # For multivariate kernel regression\nlibrary(mgcv)     # For thin-plate and tensor product splines\nlibrary(fields)   # For radial basis functions (RBF)\nlibrary(reshape2) # For data manipulation\n\n\n# Simulate Multivariate Data\nset.seed(123)\nn <- 100\nx1 <- runif(n, 0, 5)\nx2 <- runif(n, 0, 5)\nx3 <- runif(n, 0, 5)\n\n# True nonlinear multivariate function with interaction effects\ntrue_function <- function(x1, x2, x3) {\n    sin(pi * x1) * cos(pi * x2) + exp(-((x3 - 2.5) ^ 2)) + 0.5 * x1 * x3\n}\n\n# Response with noise\ny <- true_function(x1, x2, x3) + rnorm(n, sd = 0.5)\n\n# Data frame\ndata_multi <- data.frame(y, x1, x2, x3)\n\n# Visualization of marginal relationships\np1 <-\n    ggplot(data_multi, aes(x1, y)) +\n    geom_point(alpha = 0.5) +\n    labs(title = \"Effect of x1\")\np2 <-\n    ggplot(data_multi, aes(x2, y)) +\n    geom_point(alpha = 0.5) +\n    labs(title = \"Effect of x2\")\np3 <-\n    ggplot(data_multi, aes(x3, y)) +\n    geom_point(alpha = 0.5) +\n    labs(title = \"Effect of x3\")\n\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)# Multivariate Kernel Regression using np package\nbw <-\n    npregbw(y ~ x1 + x2 + x3, data = data_multi)  # Bandwidth selection\n#> \nMultistart 1 of 3 |\nMultistart 1 of 3 |\nMultistart 1 of 3 |\nMultistart 1 of 3 /\nMultistart 1 of 3 -\nMultistart 1 of 3 |\nMultistart 1 of 3 |\nMultistart 1 of 3 /\nMultistart 1 of 3 -\nMultistart 1 of 3 \\\nMultistart 2 of 3 |\nMultistart 2 of 3 |\nMultistart 2 of 3 /\nMultistart 2 of 3 -\nMultistart 2 of 3 \\\nMultistart 2 of 3 |\nMultistart 2 of 3 |\nMultistart 2 of 3 |\nMultistart 2 of 3 /\nMultistart 3 of 3 |\nMultistart 3 of 3 |\nMultistart 3 of 3 /\nMultistart 3 of 3 -\nMultistart 3 of 3 \\\nMultistart 3 of 3 |\nMultistart 3 of 3 |\nMultistart 3 of 3 |\nMultistart 3 of 3 /\n                   \nkernel_model <- npreg(bws = bw)\n\n# Predict on a grid for visualization\ngrid_data <- expand.grid(\n    x1 = seq(0, 5, length.out = 50),\n    x2 = seq(0, 5, length.out = 50),\n    x3 = mean(data_multi$x3)\n)\npred_kernel <- predict(kernel_model, newdata = grid_data)\n\n# Visualization\ngrid_data$pred <- pred_kernel\nggplot(grid_data, aes(x1, x2, fill = pred)) +\n    geom_raster() +\n    scale_fill_viridis_c() +\n    labs(title = \"Multivariate Kernel Regression (x3 fixed)\",\n         x = \"x1\",\n         y = \"x2\") +\n    theme_minimal()\n# Fit Thin-Plate Spline\ntps_model <- gam(y ~ s(x1, x2, x3, bs = \"tp\", k = 5), data = data_multi)\n\n# Predictions\ngrid_data$pred_tps <- predict(tps_model, newdata = grid_data)\n\n# Visualization\nggplot(grid_data, aes(x1, x2, fill = pred_tps)) +\n    geom_raster() +\n    scale_fill_viridis_c() +\n    labs(title = \"Thin-Plate Spline (x3 fixed)\",\n         x = \"x1\", y = \"x2\") +\n    theme_minimal()\n# Fit Tensor Product Spline\ntensor_model <- gam(y ~ te(x1, x2, x3), data = data_multi)\n\n# Predictions\ngrid_data$pred_tensor <- predict(tensor_model, newdata = grid_data)\n\n# Visualization\nggplot(grid_data, aes(x1, x2, fill = pred_tensor)) +\n    geom_raster() +\n    scale_fill_viridis_c() +\n    labs(title = \"Tensor Product Spline (x3 fixed)\",\n         x = \"x1\", y = \"x2\") +\n    theme_minimal()\n# Additive Model (GAM)\ngam_model <- gam(y ~ s(x1) + s(x2) + s(x3), data = data_multi)\n\n# Predictions\ngrid_data$pred_gam <- predict(gam_model, newdata = grid_data)\n\n# Visualization\nggplot(grid_data, aes(x1, x2, fill = pred_gam)) +\n    geom_raster() +\n    scale_fill_viridis_c() +\n    labs(title = \"Additive Model (GAM, x3 fixed)\", \n         x = \"x1\", y = \"x2\") +\n    theme_minimal()\n# Radial Basis Function Model\nrbf_model <- Tps(cbind(x1, x2, x3), y)  # Thin-plate spline RBF\n#> Warning: \n#> Grid searches over lambda (nugget and sill variances) with  minima at the endpoints: \n#>   (GCV) Generalized Cross-Validation \n#>    minimum at  right endpoint  lambda  =  0.0002622876 (eff. df= 95.00001 )\n\n# Predictions\ngrid_data$pred_rbf <-\n    predict(rbf_model, cbind(grid_data$x1, grid_data$x2, grid_data$x3))\n\n# Visualization\nggplot(grid_data, aes(x1, x2, fill = pred_rbf)) +\n    geom_raster() +\n    scale_fill_viridis_c() +\n    labs(title = \"Radial Basis Function Regression (x3 fixed)\", \n         x = \"x1\", y = \"x2\") +\n    theme_minimal()\n# Compute Mean Squared Error for each model\nmse_kernel <- mean((predict(kernel_model) - data_multi$y) ^ 2)\nmse_tps <- mean((predict(tps_model) - data_multi$y) ^ 2)\nmse_tensor <- mean((predict(tensor_model) - data_multi$y) ^ 2)\nmse_gam <- mean((predict(gam_model) - data_multi$y) ^ 2)\nmse_rbf <-\n    mean((predict(rbf_model, cbind(x1, x2, x3)) - data_multi$y) ^ 2)\n\n# Display MSE comparison\ndata.frame(\n    Model = c(\n        \"Kernel Regression\",\n        \"Thin-Plate Spline\",\n        \"Tensor Product Spline\",\n        \"Additive Model (GAM)\",\n        \"Radial Basis Functions\"\n    ),\n    MSE = c(mse_kernel, mse_tps, mse_tensor, mse_gam, mse_rbf)\n)\n#>                    Model         MSE\n#> 1      Kernel Regression 0.253019836\n#> 2      Thin-Plate Spline 0.449096723\n#> 3  Tensor Product Spline 0.304680406\n#> 4   Additive Model (GAM) 1.595616092\n#> 5 Radial Basis Functions 0.001361915"},{"path":"sec-nonparametric-regression.html","id":"conclusion-the-evolving-landscape-of-regression-analysis","chapter":"10 Nonparametric Regression","heading":"10.11 Conclusion: The Evolving Landscape of Regression Analysis","text":"conclude exploration regression analysis, reflect vast landscape navigated—spanning foundational principles linear regression intricate complexities generalized linear models, linear mixed models, nonlinear mixed models, now, flexible world nonparametric regression.Regression analysis just statistical tool; versatile framework underpins decision-making across disciplines—marketing finance healthcare, engineering, beyond. journey shown regression serves method modeling relationships also lens interpret complex data ever-changing world.","code":""},{"path":"sec-nonparametric-regression.html","id":"key-takeaways-1","chapter":"10 Nonparametric Regression","heading":"10.11.1 Key Takeaways","text":"Power Simplicity: core, simple linear regression illustrates relationships variables can modeled clarity elegance. Mastering fundamentals lays groundwork complex techniques.Power Simplicity: core, simple linear regression illustrates relationships variables can modeled clarity elegance. Mastering fundamentals lays groundwork complex techniques.Beyond Linearity: Nonlinear regression generalized linear models extend capabilities handle data defy linear assumptions—capturing curved relationships, non-normal error structures, diverse outcome distributions.Beyond Linearity: Nonlinear regression generalized linear models extend capabilities handle data defy linear assumptions—capturing curved relationships, non-normal error structures, diverse outcome distributions.Accounting Hierarchies Dependencies: Real-world data often exhibit structures nested observations repeated measures. Linear mixed models generalized linear mixed models enable us account fixed effects random variability, ensuring robust nuanced inferences.Accounting Hierarchies Dependencies: Real-world data often exhibit structures nested observations repeated measures. Linear mixed models generalized linear mixed models enable us account fixed effects random variability, ensuring robust nuanced inferences.Complex Systems, Flexible Models: Nonlinear mixed models allow us capture dynamic, non-linear processes hierarchical structures, bridging gap theoretical models real-world complexity.Complex Systems, Flexible Models: Nonlinear mixed models allow us capture dynamic, non-linear processes hierarchical structures, bridging gap theoretical models real-world complexity.Flexibility Nonparametric Regression: Nonparametric methods, kernel regression, local polynomial regression, smoothing splines, wavelet regression, regression trees, provide powerful tools parametric assumptions restrictive. models excel capturing complex, nonlinear patterns without assuming specific functional form, offering greater adaptability diverse applications.Flexibility Nonparametric Regression: Nonparametric methods, kernel regression, local polynomial regression, smoothing splines, wavelet regression, regression trees, provide powerful tools parametric assumptions restrictive. models excel capturing complex, nonlinear patterns without assuming specific functional form, offering greater adaptability diverse applications.","code":""},{"path":"sec-nonparametric-regression.html","id":"the-art-and-science-of-regression","chapter":"10 Nonparametric Regression","heading":"10.11.2 The Art and Science of Regression","text":"statistical formulas algorithms form backbone regression analysis, true art lies model selection, diagnostic evaluation, interpretation. model inherently perfect; approximation reality, shaped assumptions make data collect. effective analysts approach models critically—testing assumptions, validating results, recognizing limitations analyses.Nonparametric methods remind us flexibility often comes cost interpretability efficiency, just parametric models offer simplicity may risk oversimplification. key choose paradigms, understand appropriate.","code":""},{"path":"sec-nonparametric-regression.html","id":"looking-forward","chapter":"10 Nonparametric Regression","heading":"10.11.3 Looking Forward","text":"field regression continues evolve, driven rapid advancements computational power, data availability, methodological innovation. evolution given rise wide range modern techniques extend beyond traditional frameworks:Machine Learning Algorithms: methods like random forests, support vector machines, gradient boosting well-established, recent developments include:\nExtreme Gradient Boosting (XGBoost) LightGBM, optimized speed performance large-scale data environments.\nCatBoost, handles categorical features effectively without extensive preprocessing.\nExtreme Gradient Boosting (XGBoost) LightGBM, optimized speed performance large-scale data environments.CatBoost, handles categorical features effectively without extensive preprocessing.Bayesian Regression Techniques: Modern Bayesian approaches go beyond simple hierarchical models include:\nBayesian Additive Regression Trees (BART): flexible, nonparametric Bayesian method combines power regression trees probabilistic inference.\nBayesian Neural Networks (BNNs): Extending deep learning uncertainty quantification, enabling robust decision-making high-stakes applications.\nBayesian Additive Regression Trees (BART): flexible, nonparametric Bayesian method combines power regression trees probabilistic inference.Bayesian Neural Networks (BNNs): Extending deep learning uncertainty quantification, enabling robust decision-making high-stakes applications.High-Dimensional Data Analysis: Regularization methods like LASSO ridge regression paved way advanced techniques, :\nGraphical Models Sparse Precision Matrices: capturing complex dependency structures high-dimensional data.\nGraphical Models Sparse Precision Matrices: capturing complex dependency structures high-dimensional data.Deep Learning Regression: Deep neural networks (DNNs) increasingly used regression tasks, particularly dealing :\nStructured Data (e.g., tabular datasets): architectures like TabNet.\nUnstructured Data (e.g., images, text): Using convolutional neural networks (CNNs) transformer-based models.\nStructured Data (e.g., tabular datasets): architectures like TabNet.Unstructured Data (e.g., images, text): Using convolutional neural networks (CNNs) transformer-based models.Causal Inference Regression: integration causal modeling techniques regression frameworks advanced significantly:\nDouble Machine Learning (DML): Combining machine learning econometric methods robust causal effect estimation.\nCausal Forests: extension random forests designed estimate heterogeneous treatment effects.\nDouble Machine Learning (DML): Combining machine learning econometric methods robust causal effect estimation.Causal Forests: extension random forests designed estimate heterogeneous treatment effects.Functional Data Analysis (FDA): analyzing data predictors responses functions (e.g., curves, time series), using methods like:\nFunctional Linear Models (FLM) Functional Additive Models (FAM).\nDynamic Regression Models real-time prediction streaming data environments.\nFunctional Linear Models (FLM) Functional Additive Models (FAM).Dynamic Regression Models real-time prediction streaming data environments.modern approaches differ implementation, many rooted fundamental concepts covered book. Whether parametric precision nonparametric flexibility, principles regression remain central data-driven inquiry.","code":""},{"path":"sec-nonparametric-regression.html","id":"final-thoughts","chapter":"10 Nonparametric Regression","heading":"10.11.4 Final Thoughts","text":"apply techniques work, remember regression just fitting models—’s :Asking right questionsAsking right questionsInterpreting results thoughtfullyInterpreting results thoughtfullyUsing data generate meaningful insightsUsing data generate meaningful insightsWhether ’re developing marketing strategies, forecasting financial trends, optimizing healthcare interventions, conducting academic research, tools ’ve gained serve strong foundation.George E.P. Box put : “models wrong”—yet , noted, still useful. (Box 1976)goal analysts find models useful also enlightening—models reveal patterns, guide decisions, deepen understanding world.","code":""},{"path":"data.html","id":"data","chapter":"11 Data","heading":"11 Data","text":"Data can defined broadly set values, facts, statistics can used reference, analysis, drawing inferences. research, data drives process understanding phenomena, testing hypotheses, formulating evidence-based conclusions. Choosing right type data (understanding strengths limitations) critical validity reliability findings.","code":""},{"path":"data.html","id":"data-types","chapter":"11 Data","heading":"11.1 Data Types","text":"","code":""},{"path":"data.html","id":"qualitative-vs.-quantitative-data","chapter":"11 Data","heading":"11.1.1 Qualitative vs. Quantitative Data","text":"foundational way categorize data whether qualitative (non-numerical) quantitative (numerical). distinctions often guide research designs, data collection methods, analytical techniques.","code":""},{"path":"data.html","id":"uses-and-advantages-of-qualitative-data","chapter":"11 Data","heading":"11.1.1.1 Uses and Advantages of Qualitative Data","text":"Deep Understanding: Captures context, motivations, perceptions depth.Deep Understanding: Captures context, motivations, perceptions depth.Flexibility: Elicits new insights open-ended inquiry.Flexibility: Elicits new insights open-ended inquiry.Inductive Approaches: Often used build new theories conceptual frameworks.Inductive Approaches: Often used build new theories conceptual frameworks.","code":""},{"path":"data.html","id":"uses-and-advantages-of-quantitative-data","chapter":"11 Data","heading":"11.1.1.2 Uses and Advantages of Quantitative Data","text":"Measurement Comparison: Facilitates measuring variables comparing across groups time.Measurement Comparison: Facilitates measuring variables comparing across groups time.Generalizability: proper sampling, findings can often generalized broader populations.Generalizability: proper sampling, findings can often generalized broader populations.Hypothesis Testing: Permits use statistical methods test specific predictions relationships.Hypothesis Testing: Permits use statistical methods test specific predictions relationships.","code":""},{"path":"data.html","id":"limitations-of-qualitative-and-quantitative-data","chapter":"11 Data","heading":"11.1.1.3 Limitations of Qualitative and Quantitative Data","text":"Qualitative:\nFindings may difficult generalize samples small non-representative.\nAnalysis can time-consuming due coding interpreting text.\nPotential researcher bias interpretation.\nQualitative:Findings may difficult generalize samples small non-representative.Findings may difficult generalize samples small non-representative.Analysis can time-consuming due coding interpreting text.Analysis can time-consuming due coding interpreting text.Potential researcher bias interpretation.Potential researcher bias interpretation.Quantitative:\nMay oversimplify complex human behaviors contextual factors reducing numbers.\nValidity depends heavily well constructs operationalized.\nCan miss underlying meanings nuances captured numeric measures.\nQuantitative:May oversimplify complex human behaviors contextual factors reducing numbers.May oversimplify complex human behaviors contextual factors reducing numbers.Validity depends heavily well constructs operationalized.Validity depends heavily well constructs operationalized.Can miss underlying meanings nuances captured numeric measures.Can miss underlying meanings nuances captured numeric measures.","code":""},{"path":"data.html","id":"levels-of-measurement","chapter":"11 Data","heading":"11.1.1.4 Levels of Measurement","text":"Even within quantitative data, distinctions based level measurement. classification crucial determining statistical techniques appropriate:Nominal: Categorical data inherent order (e.g., gender, blood type, eye color).Nominal: Categorical data inherent order (e.g., gender, blood type, eye color).Ordinal: Categorical data specific order ranking without consistent intervals ranks (e.g., Likert scale responses: “strongly disagree,” “disagree,” “neutral,” “agree,” “strongly agree”).Ordinal: Categorical data specific order ranking without consistent intervals ranks (e.g., Likert scale responses: “strongly disagree,” “disagree,” “neutral,” “agree,” “strongly agree”).Interval: Numeric data equal intervals true zero (e.g., temperature Celsius Fahrenheit).Interval: Numeric data equal intervals true zero (e.g., temperature Celsius Fahrenheit).Ratio: Numeric data equal intervals meaningful zero (e.g., height, weight, income).Ratio: Numeric data equal intervals meaningful zero (e.g., height, weight, income).level measurement affects statistical tests (like t-tests, ANOVA, correlations, regressions) valid can interpret differences ratios data.","code":""},{"path":"data.html","id":"other-ways-to-classify-data","chapter":"11 Data","heading":"11.1.2 Other Ways to Classify Data","text":"Beyond observational structure, multiple dimensions used classify data:","code":""},{"path":"data.html","id":"primary-vs.-secondary-data","chapter":"11 Data","heading":"11.1.2.1 Primary vs. Secondary Data","text":"Primary Data: Collected directly researcher specific purpose (e.g., firsthand surveys, experiments, direct measurements).Primary Data: Collected directly researcher specific purpose (e.g., firsthand surveys, experiments, direct measurements).Secondary Data: Originally gathered someone else different purpose (e.g., government census data, administrative records, previously published datasets).Secondary Data: Originally gathered someone else different purpose (e.g., government census data, administrative records, previously published datasets).","code":""},{"path":"data.html","id":"structured-semi-structured-and-unstructured-data","chapter":"11 Data","heading":"11.1.2.2 Structured, Semi-Structured, and Unstructured Data","text":"Structured Data: Organized predefined manner, typically rows columns (e.g., spreadsheets, relational databases).Structured Data: Organized predefined manner, typically rows columns (e.g., spreadsheets, relational databases).Semi-Structured Data: Contains organizational markers strictly tabular (e.g., JSON, XML logs, HTML).Semi-Structured Data: Contains organizational markers strictly tabular (e.g., JSON, XML logs, HTML).Unstructured Data: Lacks clear, consistent format (e.g., raw text, images, videos, audio files).\nOften analyzed using natural language processing (NLP), image recognition, advanced techniques.\nUnstructured Data: Lacks clear, consistent format (e.g., raw text, images, videos, audio files).Often analyzed using natural language processing (NLP), image recognition, advanced techniques.","code":""},{"path":"data.html","id":"big-data","chapter":"11 Data","heading":"11.1.2.3 Big Data","text":"Characterized “3 Vs”: Volume (large amounts), Variety (diverse forms), Velocity (high-speed generation).Characterized “3 Vs”: Volume (large amounts), Variety (diverse forms), Velocity (high-speed generation).Requires specialized computational tools (e.g., Hadoop, Spark) often cloud-based infrastructure storage processing.Requires specialized computational tools (e.g., Hadoop, Spark) often cloud-based infrastructure storage processing.Can structured unstructured (e.g., social media feeds, sensor data, clickstream data).Can structured unstructured (e.g., social media feeds, sensor data, clickstream data).","code":""},{"path":"data.html","id":"internal-vs.-external-data-in-organizational-contexts","chapter":"11 Data","heading":"11.1.2.4 Internal vs. External Data (in Organizational Contexts)","text":"Internal Data: Generated within organization (e.g., sales records, HR data, production metrics).Internal Data: Generated within organization (e.g., sales records, HR data, production metrics).External Data: Sourced outside (e.g., macroeconomic indicators, market research reports, social media analytics).External Data: Sourced outside (e.g., macroeconomic indicators, market research reports, social media analytics).","code":""},{"path":"data.html","id":"proprietary-vs.-public-datas","chapter":"11 Data","heading":"11.1.2.5 Proprietary vs. Public Datas","text":"Proprietary Data: Owned organization entity, freely available public use.Proprietary Data: Owned organization entity, freely available public use.Public/Open Data: Freely accessible data provided governments, NGOs, institutions (e.g., data.gov, World Bank Open Data).Public/Open Data: Freely accessible data provided governments, NGOs, institutions (e.g., data.gov, World Bank Open Data).","code":""},{"path":"data.html","id":"data-by-observational-structure-over-time","chapter":"11 Data","heading":"11.1.3 Data by Observational Structure Over Time","text":"Another primary way categorize data observations collected time. classification shapes research design, analytic methods, types inferences can make. Four major types :Cross-Sectional DataTime Series DataRepeated Cross-Sectional DataPanel (Longitudinal) Data","code":""},{"path":"data.html","id":"sec-cross-sectional-data","chapter":"11 Data","heading":"11.2 Cross-Sectional Data","text":"Cross-sectional data consists observations multiple entities (e.g., individuals, firms, regions, countries) single point time short period, time primary dimension variation.observation represents different entity, rather entity tracked time.Unlike time series data, order observations carry temporal meaning.ExamplesLabor Economics: Wage demographic data 1,000 workers 2024.Marketing Analytics: Customer satisfaction ratings purchasing behavior 500 online shoppers surveyed Q1 year.Corporate Finance: Financial statements 1,000 firms fiscal year 2023.Key CharacteristicsObservations independent (ideal setting): unit drawn population intrinsic dependence others.natural ordering: Unlike time series data, sequence observations affect analysis.Variation occurs across entities, time: Differences observed outcomes arise differences individuals, firms, regions.AdvantagesStraightforward Interpretation: Since time effects present, focus remains relationships variables single point.Easier Collect Analyze: Compared time series panel data, cross-sectional data often simpler collect model.Suitable causal inference (exogeneity conditions hold).ChallengesOmitted Variable Bias: Unobserved confounders may drive dependent independent variables.Endogeneity: Reverse causality measurement error can introduce bias.Heteroskedasticity: Variance errors may differ across entities, requiring robust standard errors.typical cross-sectional regression model:\\[\ny_i = \\beta_0 + x_{i1}\\beta_1 + x_{i2}\\beta_2 + \\dots + x_{(k-1)}\\beta_{k-1} + \\epsilon_i\n\\]:\\(y_i\\) outcome variable entity \\(\\),\\(x_{ij}\\) explanatory variables,\\(\\epsilon_i\\) error term capturing unobserved factors.","code":""},{"path":"data.html","id":"sec-time-series-data","chapter":"11 Data","heading":"11.3 Time Series Data","text":"Time series data consists observations variable(s) recorded multiple time periods single entity (aggregated entity). data points typically collected consistent intervals—hourly, daily, monthly, quarterly, annually—allowing analysis trends, patterns, forecasting.ExamplesStock Market: Daily closing prices company’s stock five years.Economics: Monthly unemployment rates country decade.Macroeconomics: Annual GDP country 1960 2020.Key CharacteristicsThe primary goal analyze trends, seasonality, cyclic patterns, forecast future values.Time series data requires specialized statistical methods, :\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal ARIMA (SARIMA)\nExponential Smoothing\nVector Autoregression (VAR)\nAutoregressive Integrated Moving Average (ARIMA)Seasonal ARIMA (SARIMA)Exponential SmoothingVector Autoregression (VAR)AdvantagesCaptures temporal patterns trends, seasonal fluctuations, economic cycles.Essential forecasting policy-making, setting interest rates based economic indicators.ChallengesAutocorrelation: Observations close time often correlated.Structural Breaks: Sudden changes due policy shifts economic crises can distort analysis.Seasonality: Must accounted avoid misleading conclusions.time series typically consists four key components:Trend: Long-term directional movement data time.Seasonality: Regular, periodic fluctuations (e.g., increased retail sales December).Cyclical Patterns: Long-term economic cycles irregular recurrent.Irregular (Random) Component: Unpredictable variations explained trend, seasonality, cycles.general linear time series model can expressed :\\[\ny_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + \\dots + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t\n\\]Common Model TypesStatic ModelA simple time series regression:\\[\ny_t = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon_t\n\\]Finite Distributed Lag ModelCaptures effect past values explanatory variable:\\[\ny_t = \\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 + pe_{t-2}\\delta_2 + \\epsilon_t\n\\]Long-Run Propensity: Measures cumulative effect explanatory variables time:\n\\[\nLRP = \\delta_0 + \\delta_1 + \\delta_2\n\\]Long-Run Propensity: Measures cumulative effect explanatory variables time:\\[\nLRP = \\delta_0 + \\delta_1 + \\delta_2\n\\]Dynamic ModelA model incorporating lagged dependent variables:\\[\nGDP_t = \\beta_0 + \\beta_1 GDP_{t-1} + \\epsilon_t\n\\]","code":""},{"path":"data.html","id":"statistical-properties-of-time-series-models","chapter":"11 Data","heading":"11.3.1 Statistical Properties of Time Series Models","text":"time series regression, standard OLS assumptions must carefully examined. following conditions affect estimation:Finite Sample PropertiesA1-A3: OLS remains unbiased.A1-A4: Standard errors consistent, Gauss-Markov Theorem holds (OLS BLUE).A1-A6: Finite sample Wald tests (e.g., t-tests F-tests) remain valid.However, time series settings, A3 often fails due :Spurious Time Trends (fixable including time trend)Strict vs. Contemporaneous Exogeneity (sometimes unavoidable)","code":""},{"path":"data.html","id":"common-time-series-processes","chapter":"11 Data","heading":"11.3.2 Common Time Series Processes","text":"Several key models describe different time series behaviors:Autoregressive Model (AR(p)): process current values depend past values.Autoregressive Model (AR(p)): process current values depend past values.Moving Average Model (MA(q)): process past error terms influence current values.Moving Average Model (MA(q)): process past error terms influence current values.Autoregressive Moving Average (ARMA(p, q)): combination AR MA processes.Autoregressive Moving Average (ARMA(p, q)): combination AR MA processes.Autoregressive Conditional Heteroskedasticity (ARCH(p)): Models time-varying volatility.Autoregressive Conditional Heteroskedasticity (ARCH(p)): Models time-varying volatility.Generalized ARCH (GARCH(p, q)): Extends ARCH including past conditional variances.Generalized ARCH (GARCH(p, q)): Extends ARCH including past conditional variances.","code":""},{"path":"data.html","id":"deterministic-time-trends","chapter":"11 Data","heading":"11.3.3 Deterministic Time Trends","text":"dependent independent variables exhibit trending behavior, regression may produce spurious results.Spurious Regression ExampleA simple regression trending variables:\\[\ny_t = \\alpha_0 + t\\alpha_1 + v_t\n\\]\\[\nx_t = \\lambda_0 + t\\lambda_1 + u_t\n\\]\\(\\alpha_1 \\neq 0\\) \\(\\lambda_1 \\neq 1\\)\\(\\alpha_1 \\neq 0\\) \\(\\lambda_1 \\neq 1\\)\\(v_t\\) \\(u_t\\) independent.\\(v_t\\) \\(u_t\\) independent.Despite true relationship \\(x_t\\) \\(y_t\\), estimating:\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]results :Inconsistency: \\(plim(\\hat{\\beta}_1) = \\frac{\\alpha_1}{\\lambda_1}\\)Invalid Inference: \\(|t| \\^d \\infty\\) \\(H_0: \\beta_1=0\\), leading rejection null hypothesis \\(n \\\\infty\\).Misleading \\(R^2\\): \\(plim(R^2) = 1\\), falsely implying perfect predictive power.can also rewrite equation :\\[\n\\begin{aligned}\ny_t &=\\beta_0 + \\beta_1 x_t + \\epsilon_t \\\\\n\\epsilon_t &= \\alpha_1 t + v_t\n\\end{aligned}\n\\]\\(\\beta_0 = \\alpha_0\\) \\(\\beta_1 = 0\\). Since \\(x_t\\) deterministic function time, \\(\\epsilon_t\\) correlated \\(x_t\\), leading usual omitted variable bias.Solutions Spurious TrendsInclude Time Trend (\\(t\\)) Control Variable\nProvides consistent parameter estimates valid inference.\nProvides consistent parameter estimates valid inference.Detrend Variables\nRegress \\(y_t\\) \\(x_t\\) time, use residuals second regression.\nEquivalent applying Frisch-Waugh-Lovell Theorem.\nRegress \\(y_t\\) \\(x_t\\) time, use residuals second regression.Equivalent applying Frisch-Waugh-Lovell Theorem.","code":""},{"path":"data.html","id":"violations-of-exogeneity-in-time-series-models","chapter":"11 Data","heading":"11.3.4 Violations of Exogeneity in Time Series Models","text":"exogeneity assumption (A3) plays crucial role ensuring unbiased consistent estimation time series models. However, many cases, assumption violated due inherent nature time-dependent processes.standard regression framework, assume:\\[\nE(\\epsilon_t | x_1, x_2, ..., x_T) = 0\n\\]requires error term uncorrelated past, present, future values independent variables.Common Violations ExogeneityFeedback EffectThe error term \\(\\epsilon_t\\) influences future values independent variables.classic example occurs economic models past shocks affect future decisions.Dynamic SpecificationThe dependent variable includes lagged version explanatory variable, introducing correlation \\(\\epsilon_t\\) past \\(y_{t-1}\\).Dynamic CompletenessIn finite distributed lag (FDL) models, failing include correct number lags leads omitted variable bias correlation regressors errors.","code":""},{"path":"data.html","id":"sec-feedback-effect","chapter":"11 Data","heading":"11.3.4.1 Feedback Effect","text":"simple regression model:\\[\ny_t = \\beta_0 + x_t \\beta_1 + \\epsilon_t\n\\]standard exogeneity assumption (A3) requires:\\[\nE(\\epsilon_t | x_1, x_2, ..., x_t, x_{t+1}, ..., x_T) = 0\n\\]However, presence feedback, past errors affect future values \\(x_t\\), leading :\\[\nE(\\epsilon_t | x_{t+1}, ..., x_T) \\neq 0\n\\]occurs current shocks (e.g., economic downturns) influence future decisions (e.g., government spending, firm investments).Strict exogeneity violated, now dependence across time.Implication:Standard OLS estimators become biased inconsistent.One common solution using Instrumental Variables isolate exogenous variation \\(x_t\\).","code":""},{"path":"data.html","id":"sec-dynamic-specification","chapter":"11 Data","heading":"11.3.4.2 Dynamic Specification","text":"dynamically specified model includes lagged dependent variables:\\[\ny_t = \\beta_0 + y_{t-1} \\beta_1 + \\epsilon_t\n\\]Exogeneity (A3) require:\\[\nE(\\epsilon_t | y_1, y_2, ..., y_t, y_{t+1}, ..., y_T) = 0\n\\]However, since \\(y_{t-1}\\) depends \\(\\epsilon_{t-1}\\) previous period, obtain:\\[\nCov(y_{t-1}, \\epsilon_t) \\neq 0\n\\]Implication:Strict exogeneity (A3) fails, \\(y_{t-1}\\) \\(\\epsilon_t\\) correlated.OLS estimates biased inconsistent.Standard autoregressive models (AR) require alternative estimation techniques like Generalized Method Moments Maximum Likelihood Estimation.","code":""},{"path":"data.html","id":"sec-dynamic-completeness-and-omitted-lags","chapter":"11 Data","heading":"11.3.4.3 Dynamic Completeness and Omitted Lags","text":"finite distributed lag (FDL) model:\\[\ny_t = \\beta_0 + x_t \\delta_0 + x_{t-1} \\delta_1 + \\epsilon_t\n\\]assumes included lags fully capture relationship \\(y_t\\) past values \\(x_t\\). However, omit relevant lags, exogeneity assumption (A3):\\[\nE(\\epsilon_t | x_1, x_2, ..., x_t, x_{t+1}, ..., x_T) = 0\n\\]fails, unmodeled lag effects create correlation \\(x_{t-2}\\) \\(\\epsilon_t\\).Implication:regression suffers omitted variable bias, making OLS estimates unreliable.Solution:\nInclude additional lags \\(x_t\\).\nUse lag selection criteria (e.g., AIC, BIC) determine appropriate lag structure.\nInclude additional lags \\(x_t\\).Use lag selection criteria (e.g., AIC, BIC) determine appropriate lag structure.","code":""},{"path":"data.html","id":"consequences-of-exogeneity-violations","chapter":"11 Data","heading":"11.3.5 Consequences of Exogeneity Violations","text":"strict exogeneity (A3) fails, standard OLS assumptions longer hold:OLS biased.Gauss-Markov Theorem longer applies.Finite Sample Properties (unbiasedness) invalid.address issues, can:Rely Large Sample Properties: certain conditions, consistency may still hold.Use Weaker Forms Exogeneity: Shift strict exogeneity (A3) contemporaneous exogeneity (A3a).strict exogeneity hold, can instead assume A3a (Contemporaneous Exogeneity):\\[\nE(\\mathbf{x}_t' \\epsilon_t) = 0\n\\]weaker assumption requires \\(x_t\\) uncorrelated error time period.Key Differences Strict ExogeneityWith contemporaneous exogeneity, \\(\\epsilon_t\\) can correlated past future values \\(x_t\\).contemporaneous exogeneity, \\(\\epsilon_t\\) can correlated past future values \\(x_t\\).allows dynamic specifications :\n\\[\ny_t = \\beta_0 + y_{t-1} \\beta_1 + \\epsilon_t\n\\]\nstill maintaining consistency certain assumptions.allows dynamic specifications :\\[\ny_t = \\beta_0 + y_{t-1} \\beta_1 + \\epsilon_t\n\\]still maintaining consistency certain assumptions.Deriving Large Sample Properties Time SeriesTo establish consistency asymptotic normality, rely following assumptions:A1: LinearityA2: Full Rank (Perfect Multicollinearity)A3a: Contemporaneous ExogeneityHowever, standard Weak Law Large Numbers Central Limit Theorem OLS depend A5 (Random Sampling), hold time series settings.Since time series data exhibits dependence time, replace A5 (Random Sampling) weaker assumption:A5a: Weak Dependence (Stationarity)Asymptotic Variance Serial CorrelationThe derivation asymptotic variance depends A4 (Homoskedasticity).derivation asymptotic variance depends A4 (Homoskedasticity).However, time series settings, often encounter serial correlation:\n\\[\nCov(\\epsilon_t, \\epsilon_s) \\neq 0 \\quad \\text{} \\quad |t - s| > 0\n\\]However, time series settings, often encounter serial correlation:\\[\nCov(\\epsilon_t, \\epsilon_s) \\neq 0 \\quad \\text{} \\quad |t - s| > 0\n\\]ensure valid inference, standard errors must corrected using methods Newey-West HAC estimators.ensure valid inference, standard errors must corrected using methods Newey-West HAC estimators.","code":""},{"path":"data.html","id":"highly-persistent-data","chapter":"11 Data","heading":"11.3.6 Highly Persistent Data","text":"time series analysis, key assumption OLS consistency data-generating process exhibits A5a weak dependence (.e., observations strongly correlated time). However, \\(y_t\\) \\(x_t\\) highly persistent, standard OLS assumptions break .time series weakly dependent, means:\\(y_t\\) \\(y_{t-h}\\) remain strongly correlated even large lags (\\(h \\\\infty\\)).A5a (Weak Dependence Assumption) fails, leading :\nOLS inconsistency.\nvalid limiting distribution (asymptotic normality hold).\nOLS inconsistency.valid limiting distribution (asymptotic normality hold).Example: classic example highly persistent process random walk:\\[\ny_t = y_{t-1} + u_t\n\\]drift:\\[\ny_t = \\alpha + y_{t-1} + u_t\n\\]\\(u_t\\) white noise error term.\\(y_t\\) revert mean—infinite variance \\(t \\\\infty\\).Shocks accumulate, making standard regression analysis unreliable.","code":""},{"path":"data.html","id":"solution-first-differencing","chapter":"11 Data","heading":"11.3.6.1 Solution: First Differencing","text":"common way transform non-stationary series stationary ones first differencing:\\[\n\\Delta y_t = y_t - y_{t-1} = u_t\n\\]\\(u_t\\) weakly dependent process (.e., \\((0)\\), stationary), \\(y_t\\) said difference-stationary integrated order 1, \\((1)\\).\\(y_t\\) \\(x_t\\) follow random walk (\\((1)\\)), estimate:\\[\n\\begin{aligned}\n\\Delta y_t &= (\\Delta \\mathbf{x}_t \\beta) + (\\epsilon_t - \\epsilon_{t-1}) \\\\\n\\Delta y_t &= \\Delta \\mathbf{x}_t \\beta + \\Delta u_t\n\\end{aligned}\n\\]ensures OLS estimation remains valid.","code":""},{"path":"data.html","id":"sec-unit-root-testing","chapter":"11 Data","heading":"11.3.7 Unit Root Testing","text":"formally determine whether time series contains unit root (.e., non-stationary), test:\\[\ny_t = \\alpha + \\rho y_{t-1} + u_t\n\\]Hypothesis Testing\\(H_0: \\rho = 1\\) (unit root, non-stationary)\nOLS consistent asymptotically normal.\nOLS consistent asymptotically normal.\\(H_a: \\rho < 1\\) (stationary process)\nOLS consistent asymptotically normal.\nOLS consistent asymptotically normal.Key IssuesThe usual t-test valid OLS \\(H_0\\) standard distribution.Instead, specialized tests Dickey-Fuller Augmented Dickey-Fuller tests required.","code":""},{"path":"data.html","id":"sec-dickey-fuller-test-for-unit-roots","chapter":"11 Data","heading":"11.3.7.1 Dickey-Fuller Test for Unit Roots","text":"Dickey-Fuller test transforms original equation subtracting \\(y_{t-1}\\) sides:\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + v_t\n\\]:\\[\n\\theta = \\rho - 1\n\\]Null Hypothesis (\\(H_0: \\theta = 0\\)) → Implies \\(\\rho = 1\\) (unit root, non-stationary).Alternative (\\(H_a: \\theta < 0\\)) → Implies \\(\\rho < 1\\) (stationary).Since \\(y_t\\) follows non-standard asymptotic distribution \\(H_0\\), Dickey Fuller derived specialized critical values.Decision RuleIf test statistic negative critical value, reject \\(H_0\\) → \\(y_t\\) stationary.Otherwise, fail reject \\(H_0\\) → \\(y_t\\) unit root (non-stationary).standard DF test may fail due two key limitations:Simplistic Dynamic RelationshipThe DF test assumes one lag autoregressive structure.However, reality, higher-order lags \\(\\Delta y_t\\) may needed.Solution:\nUse Augmented Dickey-Fuller test, includes extra lags:\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta y_{t-1} + \\dots + \\gamma_p \\Delta y_{t-p} + v_t\n\\]\\(H_0\\), \\(\\Delta y_t\\) follows AR(1) process.\\(H_a\\), \\(y_t\\) follows AR(2) higher process.Including lags \\(\\Delta y_t\\) ensures better-specified model.Ignoring Deterministic Time TrendsIf series exhibits deterministic trend, failing include biases unit root test.Example: \\(y_t\\) grows time, test without trend component falsely detect unit root.Solution: Include deterministic time trend (\\(t\\)) regression:\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t\n\\]Allows quadratic relationships time.Changes critical values, requiring adjusted statistical test.","code":""},{"path":"data.html","id":"sec-augmented-dickey-fuller-test","chapter":"11 Data","heading":"11.3.7.2 Augmented Dickey-Fuller Test","text":"ADF test generalizes DF test allowing :Lags \\(\\Delta y_t\\) (correct serial correlation).Time trends (handle deterministic trends).Regression Equation\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + \\dots + \\gamma_p \\Delta y_{t-p} + v_t\n\\]\\(\\theta = 1 - \\rho\\).Hypotheses\\(H_0: \\theta = 0\\) (Unit root: non-stationary)\\(H_a: \\theta < 0\\) (Stationary)","code":""},{"path":"data.html","id":"sec-newey-west-standard-errors","chapter":"11 Data","heading":"11.3.8 Newey-West Standard Errors","text":"Newey-West standard errors, also known Heteroskedasticity Autocorrelation Consistent (HAC) estimators, provide valid inference errors exhibit heteroskedasticity (.e., A4 Homoskedasticity assumption violated) serial correlation. standard errors adjust dependence error structure, ensuring hypothesis tests remain valid.Key FeaturesAccounts autocorrelation: Handles time dependence error terms.Accounts heteroskedasticity: Allows non-constant variance across observations.Ensures positive semi-definiteness: Downweights longer-lagged covariances maintain mathematical validity.estimator computed :\\[\n\\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x'_t x_t} + \\sum_{h=1}^{g} \\left(1 - \\frac{h}{g+1} \\right) T^{-1} \\sum_{t=h+1}^{T} e_t e_{t-h} (\\mathbf{x_t' x_{t-h}} + \\mathbf{x_{t-h}' x_t})\n\\]:\\(T\\) sample size,\\(g\\) chosen lag truncation parameter (bandwidth),\\(e_t\\) residuals OLS regression,\\(\\mathbf{x}_t\\) explanatory variables.Choosing Lag Length (\\(g\\))Selecting appropriate lag truncation parameter (\\(g\\)) crucial balancing efficiency bias. Common guidelines include:Yearly data: \\(g = 1\\) \\(2\\) usually suffices.Quarterly data: \\(g = 4\\) \\(8\\) accounts seasonal dependencies.Monthly data: \\(g = 12\\) \\(14\\) captures typical cyclical effects.Alternatively, data-driven methods can used:Newey-West Rule: \\(g = \\lfloor 4(T/100)^{2/9} \\rfloor\\)Newey-West Rule: \\(g = \\lfloor 4(T/100)^{2/9} \\rfloor\\)Alternative Heuristic: \\(g = \\lfloor T^{1/4} \\rfloor\\)Alternative Heuristic: \\(g = \\lfloor T^{1/4} \\rfloor\\)","code":"\n# Load necessary libraries\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Simulate data\nset.seed(42)\nT <- 100  # Sample size\ntime <- 1:T\nx <- rnorm(T)\nepsilon <- arima.sim(n = T, list(ar = 0.5))  # Autocorrelated errors\ny <- 2 + 3 * x + epsilon  # True model\n\n# Estimate OLS model\nmodel <- lm(y ~ x)\n\n# Compute Newey-West standard errors\nlag_length <- floor(4 * (T / 100) ^ (2 / 9))  # Newey-West rule\nnw_se <- NeweyWest(model, lag = lag_length, prewhite = FALSE)\n\n# Display robust standard errors\ncoeftest(model, vcov = nw_se)\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)  1.71372    0.13189  12.993 < 2.2e-16 ***\n#> x            3.15831    0.13402  23.567 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"testing-for-serial-correlation","chapter":"11 Data","heading":"11.3.8.1 Testing for Serial Correlation","text":"Serial correlation (also known autocorrelation) occurs error terms correlated across time:\\[\nE(\\epsilon_t \\epsilon_{t-h}) \\neq 0 \\quad \\text{} h \\neq 0\n\\]Steps Detecting Serial CorrelationEstimate OLS regression:\nRun regression \\(y_t\\) \\(\\mathbf{x}_t\\) obtain residuals \\(e_t\\).\nRun regression \\(y_t\\) \\(\\mathbf{x}_t\\) obtain residuals \\(e_t\\).Test autocorrelation residuals:\nRegress \\(e_t\\) \\(\\mathbf{x}_t\\) lagged residual \\(e_{t-1}\\):\n\\[\ne_t = \\gamma_0 + \\mathbf{x}_t' \\gamma + \\rho e_{t-1} + v_t\n\\]\nTest whether \\(\\rho\\) significantly different zero.\nRegress \\(e_t\\) \\(\\mathbf{x}_t\\) lagged residual \\(e_{t-1}\\):\n\\[\ne_t = \\gamma_0 + \\mathbf{x}_t' \\gamma + \\rho e_{t-1} + v_t\n\\]Regress \\(e_t\\) \\(\\mathbf{x}_t\\) lagged residual \\(e_{t-1}\\):\\[\ne_t = \\gamma_0 + \\mathbf{x}_t' \\gamma + \\rho e_{t-1} + v_t\n\\]Test whether \\(\\rho\\) significantly different zero.Test whether \\(\\rho\\) significantly different zero.Decision Rule:\n\\(\\rho\\) statistically significant 5% level, reject null hypothesis serial correlation.\n\\(\\rho\\) statistically significant 5% level, reject null hypothesis serial correlation.Higher-Order Serial CorrelationTo test higher-order autocorrelation, extend previous regression:\\[\ne_t = \\gamma_0 + \\mathbf{x}_t' \\gamma + \\rho_1 e_{t-1} + \\rho_2 e_{t-2} + \\dots + \\rho_p e_{t-p} + v_t\n\\]Jointly test \\(\\rho_1 = \\rho_2 = \\dots = \\rho_p = 0\\) using F-test.null rejected, autocorrelation order \\(p\\) present.Step 1: Estimate OLS Regression Obtain ResidualsStep 2: Test Autocorrelation ResidualsStep 3: Testing Higher-Order Serial CorrelationCorrections Serial CorrelationIf serial correlation detected, following adjustments made:","code":"\n# Load necessary libraries\nlibrary(lmtest)\nlibrary(sandwich)\n\n# Generate some example data\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- 1 + 0.5 * x + rnorm(n)  # True model: y = 1 + 0.5*x + e\n\n# Estimate the OLS regression\nmodel <- lm(y ~ x)\n\n# Obtain residuals\nresiduals <- resid(model)\n# Create lagged residuals\nlagged_residuals <- c(NA, residuals[-length(residuals)])\n\n# Regress residuals on x and lagged residuals\nautocorr_test_model <- lm(residuals ~ x + lagged_residuals)\n\n# Summary of the regression\nsummary(autocorr_test_model)\n#> \n#> Call:\n#> lm(formula = residuals ~ x + lagged_residuals)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.94809 -0.72539 -0.08105  0.58503  3.12941 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)       0.008175   0.098112   0.083    0.934\n#> x                -0.002841   0.107167  -0.027    0.979\n#> lagged_residuals -0.127605   0.101746  -1.254    0.213\n#> \n#> Residual standard error: 0.9707 on 96 degrees of freedom\n#>   (1 observation deleted due to missingness)\n#> Multiple R-squared:  0.01614,    Adjusted R-squared:  -0.004354 \n#> F-statistic: 0.7876 on 2 and 96 DF,  p-value: 0.4579\n\n# Test if the coefficient of lagged_residuals is significant\nrho <- coef(autocorr_test_model)[\"lagged_residuals\"]\nrho_p_value <-\n    summary(autocorr_test_model)$coefficients[\"lagged_residuals\", \"Pr(>|t|)\"]\n\n# Decision Rule\nif (rho_p_value < 0.05) {\n    cat(\"Reject the null hypothesis: There is evidence of serial correlation.\\n\")\n} else {\n    cat(\"Fail to reject the null hypothesis: No evidence of serial correlation.\\n\")\n}\n#> Fail to reject the null hypothesis: No evidence of serial correlation.\n# Number of lags to test\np <- 2  # Example: testing for 2nd order autocorrelation\n\n# Create a matrix of lagged residuals\nlagged_residuals_matrix <- sapply(1:p, function(i) c(rep(NA, i), residuals[1:(n - i)]))\n\n# Regress residuals on x and lagged residuals\nhigher_order_autocorr_test_model <- lm(residuals ~ x + lagged_residuals_matrix)\n\n# Summary of the regression\nsummary(higher_order_autocorr_test_model)\n#> \n#> Call:\n#> lm(formula = residuals ~ x + lagged_residuals_matrix)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.9401 -0.7290 -0.1036  0.6359  3.0253 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)               0.006263   0.099104   0.063    0.950\n#> x                         0.010442   0.108370   0.096    0.923\n#> lagged_residuals_matrix1 -0.140426   0.103419  -1.358    0.178\n#> lagged_residuals_matrix2 -0.107385   0.103922  -1.033    0.304\n#> \n#> Residual standard error: 0.975 on 94 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.02667,    Adjusted R-squared:  -0.004391 \n#> F-statistic: 0.8587 on 3 and 94 DF,  p-value: 0.4655\n\n# Joint F-test for the significance of lagged residuals\nf_test <- car::linearHypothesis(higher_order_autocorr_test_model, \n                           paste0(\"lagged_residuals_matrix\", 1:p, \" = 0\"))\n\n# Print the F-test results\nprint(f_test)\n#> \n#> Linear hypothesis test:\n#> lagged_residuals_matrix1 = 0\n#> lagged_residuals_matrix2 = 0\n#> \n#> Model 1: restricted model\n#> Model 2: residuals ~ x + lagged_residuals_matrix\n#> \n#>   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n#> 1     96 91.816                           \n#> 2     94 89.368  2    2.4479 1.2874 0.2808\n\n# Decision Rule\nif (f_test$`Pr(>F)`[2] < 0.05) {\n  cat(\"Reject the null hypothesis: There is evidence of higher-order serial correlation.\\n\")\n} else {\n  cat(\"Fail to reject the null hypothesis: No evidence of higher-order serial correlation.\\n\")\n}\n#> Fail to reject the null hypothesis: No evidence of higher-order serial correlation."},{"path":"data.html","id":"sec-repeated-cross-sectional-data","chapter":"11 Data","heading":"11.4 Repeated Cross-Sectional Data","text":"Repeated cross-sectional data consists multiple independent cross-sections collected different points time. Unlike panel data, individuals tracked time, repeated cross-sections draw fresh sample wave.approach allows researchers analyze aggregate trends time, track individual-level changes.ExamplesGeneral Social Survey (GSS) (U.S.) – Conducted every two years new sample respondents.Political Opinion Polls – Monthly voter surveys track shifts public sentiment.National Health Surveys – Annual studies fresh samples monitor population-wide health trends.Educational Surveys – Sampling different groups students year assess learning outcomes.","code":""},{"path":"data.html","id":"key-characteristics","chapter":"11 Data","heading":"11.4.1 Key Characteristics","text":"Fresh Sample Wave\nsurvey represents independent cross-section.\nrespondent tracked across waves.\nsurvey represents independent cross-section.respondent tracked across waves.Population-Level Trends Time\nResearchers can study distribution characteristics (e.g., income, attitudes, behaviors) changes time.\nHowever, individual trajectories observed.\nResearchers can study distribution characteristics (e.g., income, attitudes, behaviors) changes time.However, individual trajectories observed.Sample Design Consistency\nensure comparability across waves, researchers must maintain consistent:\nSampling methods\nQuestionnaire design\nDefinitions key variables\n\nensure comparability across waves, researchers must maintain consistent:\nSampling methods\nQuestionnaire design\nDefinitions key variables\nSampling methodsQuestionnaire designDefinitions key variables","code":""},{"path":"data.html","id":"statistical-modeling-for-repeated-cross-sections","chapter":"11 Data","heading":"11.4.2 Statistical Modeling for Repeated Cross-Sections","text":"Since repeated cross-sections track individuals, specific regression methods used analyze changes time.Pooled Cross-Sectional Regression (Time Fixed Effects)Combines multiple survey waves single dataset controlling time effects:\\[\ny_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i\n\\]:\\(y_i\\) outcome individual \\(\\),\\(y_i\\) outcome individual \\(\\),\\(\\mathbf{x}_i\\) explanatory variables,\\(\\mathbf{x}_i\\) explanatory variables,\\(y_t\\) time period dummies,\\(y_t\\) time period dummies,\\(\\delta_t\\) captures average change outcomes across time periods.\\(\\delta_t\\) captures average change outcomes across time periods.Key Features:Allows different intercepts across time periods, capturing shifts baseline outcomes.Allows different intercepts across time periods, capturing shifts baseline outcomes.Tracks overall population trends without assuming constant effect \\(\\mathbf{x}_i\\) time.Tracks overall population trends without assuming constant effect \\(\\mathbf{x}_i\\) time.Allowing Structural Change Pooled Cross-Sections (Time-Dependent Effects)test whether relationships variables change time (structural breaks), interactions time dummies explanatory variables can introduced:\\[\ny_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i\n\\]Interacting \\(x_i\\) time period dummies allows :\nDifferent slopes time period.\nTime-dependent effects explanatory variables.\nDifferent slopes time period.Time-dependent effects explanatory variables.Practical Application:\\(\\mathbf{x}_i\\) represents education level \\(y_t\\) represents survey year, interaction term can test whether effect education income changed time.\\(\\mathbf{x}_i\\) represents education level \\(y_t\\) represents survey year, interaction term can test whether effect education income changed time.Structural break tests help determine whether time-varying effects statistically significant.Structural break tests help determine whether time-varying effects statistically significant.Useful policy analysis, policy might impact certain subgroups differently across time.Useful policy analysis, policy might impact certain subgroups differently across time.Difference--Means TimeA simple approach comparing aggregate trends:\\[ \\bar{y}_t - \\bar{y}_{t-1} \\]Measures whether average outcome changed time.Common policy evaluations (e.g., assessing effect minimum wage increases average income).Synthetic Cohort AnalysisSince repeated cross-sections track individuals, synthetic cohort can created grouping observations based shared characteristics:Example: education levels collected multiple waves, can track average income changes within education groups approximate trends.","code":""},{"path":"data.html","id":"advantages-of-repeated-cross-sectional-data","chapter":"11 Data","heading":"11.4.3 Advantages of Repeated Cross-Sectional Data","text":"","code":""},{"path":"data.html","id":"disadvantages-of-repeated-cross-sectional-data","chapter":"11 Data","heading":"11.4.4 Disadvantages of Repeated Cross-Sectional Data","text":"ensure valid comparisons across time:Consistent Sampling: wave use sampling frame methodology.Standardized Questions: Small variations question wording can introduce inconsistencies.Weighting Adjustments: sampling strategies change, apply survey weights maintain representativeness.Accounting Structural Changes: Economic, demographic, social changes may impact comparability.","code":""},{"path":"data.html","id":"sec-panel-data","chapter":"11 Data","heading":"11.5 Panel Data","text":"Panel data (also called longitudinal data) consists observations entities multiple time periods. Unlike repeated cross-sections, new samples drawn wave, panel data tracks individuals, households, firms, regions time, enabling richer statistical analysis.Panel data combines cross-sectional variation (differences across entities) time-series variation (changes within entities time).ExamplesPanel Study Income Dynamics – Follows households annually, collecting data income, employment, expenditures.Medical Longitudinal Studies – Tracks patients months years study disease progression.Firm-Level Financial Data – Follows set companies multiple years financial statements.Student Achievement Studies – Follows students across different grade levels assess academic progress.Structure\\(N\\) entities (individuals, firms, etc.) observed \\(T\\) time periods.dataset can :\nBalanced Panel: entities observed every time period.\nUnbalanced Panel: entities missing observations certain periods.\nBalanced Panel: entities observed every time period.Unbalanced Panel: entities missing observations certain periods.Types PanelsShort Panel: Many individuals (\\(N\\)) time periods (\\(T\\)).Long Panel: Many time periods (\\(T\\)) individuals (\\(N\\)).Large: Large \\(N\\) \\(T\\) (e.g., firm-level data decades).","code":""},{"path":"data.html","id":"advantages-of-panel-data","chapter":"11 Data","heading":"11.5.1 Advantages of Panel Data","text":"","code":""},{"path":"data.html","id":"disadvantages-of-panel-data","chapter":"11 Data","heading":"11.5.2 Disadvantages of Panel Data","text":"","code":""},{"path":"data.html","id":"sources-of-variation-in-panel-data","chapter":"11 Data","heading":"11.5.3 Sources of Variation in Panel Data","text":"Since observe individuals time periods, distinguish three types variation:Overall variation: Differences across time individuals.variation: Differences individuals (cross-sectional variation).Within variation: Differences within individuals (time variation).Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\)","code":""},{"path":"data.html","id":"sec-pooled-ols-estimator","chapter":"11 Data","heading":"11.5.4 Pooled OLS Estimator","text":"Pooled Ordinary Least Squares estimator simplest way estimate relationships panel data. treats panel data large cross-sectional dataset, ignoring individual-specific effects time dependence.pooled OLS model specified :\\[\ny_{} = \\mathbf{x}_{} \\beta + \\epsilon_{}\n\\]:\\(y_{}\\) dependent variable individual \\(\\) time \\(t\\),\\(y_{}\\) dependent variable individual \\(\\) time \\(t\\),\\(\\mathbf{x}_{}\\) vector explanatory variables,\\(\\mathbf{x}_{}\\) vector explanatory variables,\\(\\beta\\) vector coefficients estimated,\\(\\beta\\) vector coefficients estimated,\\(\\epsilon_{} = c_i + u_{}\\) composite error term.\n\\(c_i\\) unobserved individual heterogeneity.\n\\(u_{}\\) idiosyncratic shock.\n\\(\\epsilon_{} = c_i + u_{}\\) composite error term.\\(c_i\\) unobserved individual heterogeneity.\\(c_i\\) unobserved individual heterogeneity.\\(u_{}\\) idiosyncratic shock.\\(u_{}\\) idiosyncratic shock.treating observations independent, pooled OLS assumes systematic differences across individuals beyond captured \\(\\mathbf{x}_{}\\).pooled OLS consistent unbiased, following conditions must hold:Linearity Parameters (A1)\nrelationship \\(y_{}\\) \\(\\mathbf{x}_{}\\) correctly specified linear.\nrelationship \\(y_{}\\) \\(\\mathbf{x}_{}\\) correctly specified linear.Full Rank Condition (A2)\nregressors perfectly collinear across individuals time.\nregressors perfectly collinear across individuals time.Strict Exogeneity (A3)\ncorrelation regressors error terms: \\[\nE(\\epsilon_{} | \\mathbf{x}_{}) = 0\n\\]\nensures OLS remains unbiased.\ncorrelation regressors error terms: \\[\nE(\\epsilon_{} | \\mathbf{x}_{}) = 0\n\\]ensures OLS remains unbiased.Homoskedasticity (A4)\nConstant variance errors: \\[\nVar(\\epsilon_{} | \\mathbf{x}_{}) = \\sigma^2\n\\]\nviolated (heteroskedasticity exists), standard errors must adjusted using clustered robust approach, OLS still consistent.\nConstant variance errors: \\[\nVar(\\epsilon_{} | \\mathbf{x}_{}) = \\sigma^2\n\\]violated (heteroskedasticity exists), standard errors must adjusted using clustered robust approach, OLS still consistent.Autocorrelation Across Time (A5)\nerror term correlated time given individual: \\[\nE(\\epsilon_{}, \\epsilon_{}) = 0, \\quad \\forall t \\neq s\n\\]\nassumption fails, clustered standard errors needed.\nerror term correlated time given individual: \\[\nE(\\epsilon_{}, \\epsilon_{}) = 0, \\quad \\forall t \\neq s\n\\]assumption fails, clustered standard errors needed.Random Sampling (A6)\nObservations independent across individuals: \\[\n(y_{i1},..., y_{}, x_{i1},..., x_{}) \\perp (y_{j1},..., y_{jT}, x_{j1},..., x_{jT}) \\quad \\forall \\neq j\n\\]\nassumption often reasonable always valid (e.g., firm-level country-level panel data).\nObservations independent across individuals: \\[\n(y_{i1},..., y_{}, x_{i1},..., x_{}) \\perp (y_{j1},..., y_{jT}, x_{j1},..., x_{jT}) \\quad \\forall \\neq j\n\\]assumption often reasonable always valid (e.g., firm-level country-level panel data).pooled OLS consistent, require A3a:\\[\nE(\\mathbf{x}_{}'(c_i + u_{})) = 0\n\\]holds :Exogeneity \\(u_{}\\) (Time-varying error):\\[\nE(\\mathbf{x}_{}' u_{}) = 0\n\\]\nEnsures regressors correlated random error component.\nEnsures regressors correlated random error component.Random Effects Assumption (Time-invariant error):\\[\nE(\\mathbf{x}_{}' c_i) = 0\n\\]\nEnsures unobserved heterogeneity (\\(c_i\\)) uncorrelated regressors. assumption fails, pooled OLS suffers omitted variable bias.\nEnsures unobserved heterogeneity (\\(c_i\\)) uncorrelated regressors. assumption fails, pooled OLS suffers omitted variable bias.Implication:\\(c_i\\) correlated \\(\\mathbf{x}_{}\\), pooled OLS biased inconsistent.\\(c_i\\) correlated \\(\\mathbf{x}_{}\\), pooled OLS biased inconsistent.\\(c_i\\) uncorrelated \\(\\mathbf{x}_{}\\), pooled OLS consistent inefficient.\\(c_i\\) uncorrelated \\(\\mathbf{x}_{}\\), pooled OLS consistent inefficient.Variance Decomposition Panel DataSince panel data contains -entity within-entity variation, total variance can decomposed :\\[\ns_O^2 = s_B^2 + s_W^2\n\\]:\\(s_O^2\\) = Overall variance (variation time across individuals),\\(s_O^2\\) = Overall variance (variation time across individuals),\\(s_B^2\\) = variance (variation individuals),\\(s_B^2\\) = variance (variation individuals),\\(s_W^2\\) = Within variance (variation within individuals time).\\(s_W^2\\) = Within variance (variation within individuals time).Key Insight:Pooled OLS separate within-individual -individual variation.Pooled OLS separate within-individual -individual variation.Fixed Effects models eliminate \\(c_i\\) use within-individual variation.Fixed Effects models eliminate \\(c_i\\) use within-individual variation.Random Effects models use within variation.Random Effects models use within variation.Robust Inference Pooled OLSIf standard assumptions fail, adjustments necessary:Heteroskedasticity: A4 (Homoskedasticity) violated, standard errors must adjusted using:\nWhite’s Robust Standard Errors (cross-sectional heteroskedasticity).\nCluster-Robust Standard Errors (panel-specific heteroskedasticity).\nWhite’s Robust Standard Errors (cross-sectional heteroskedasticity).Cluster-Robust Standard Errors (panel-specific heteroskedasticity).Serial Correlation (Autocorrelation): errors correlated across time:\nUse Newey-West Standard Errors time dependence.\nUse Clustered Standard Errors Individual Level.\nUse Newey-West Standard Errors time dependence.Use Clustered Standard Errors Individual Level.Multicollinearity: regressors highly correlated:\nRemove redundant variables.\nUse Variance Inflation Factor diagnostics.\nRemove redundant variables.Use Variance Inflation Factor diagnostics.Comparing Pooled OLS Alternative Panel ModelsWhen Use Pooled OLS?individual heterogeneity negligibleIf individual heterogeneity negligibleIf panel short (\\(T\\) small) cross-section large (\\(N\\) big)panel short (\\(T\\) small) cross-section large (\\(N\\) big)random effects assumption holds (\\(E(\\mathbf{x}_{}' c_i) = 0\\))random effects assumption holds (\\(E(\\mathbf{x}_{}' c_i) = 0\\))conditions fail, Fixed Effects Random Effects models used instead.","code":""},{"path":"data.html","id":"individual-specific-effects-model","chapter":"11 Data","heading":"11.5.5 Individual-Specific Effects Model","text":"panel data, unobserved heterogeneity can arise individual-specific factors (\\(c_i\\)) influence dependent variable. effects can :Correlated regressors (\\(E(\\mathbf{x}_{}' c_i) \\neq 0\\)): Use Fixed Effects estimator.Uncorrelated regressors (\\(E(\\mathbf{x}_{}' c_i) = 0\\)): Use Random Effects estimator.general model :\\[\ny_{} = \\mathbf{x}_{} \\beta + c_i + u_{}\n\\]:\\(c_i\\) individual-specific effect (time-invariant),\\(c_i\\) individual-specific effect (time-invariant),\\(u_{}\\) idiosyncratic error (time-variant).\\(u_{}\\) idiosyncratic error (time-variant).Comparing Fixed Effects Random Effects","code":""},{"path":"data.html","id":"sec-random-effects-estimator","chapter":"11 Data","heading":"11.5.6 Random Effects Estimator","text":"Random Effects (RE) estimator Feasible Generalized Least Squares method used panel data analysis. assumes individual-specific effects (\\(c_i\\)) uncorrelated explanatory variables (\\(\\mathbf{x}_{}\\)), allowing estimation using within-group (time variation) -group (cross-sectional variation).standard Random Effects model :\\[\ny_{} = \\mathbf{x}_{} \\beta + c_i + u_{}\n\\]:\\(y_{}\\) dependent variable entity \\(\\) time \\(t\\),\\(y_{}\\) dependent variable entity \\(\\) time \\(t\\),\\(\\mathbf{x}_{}\\) vector explanatory variables,\\(\\mathbf{x}_{}\\) vector explanatory variables,\\(\\beta\\) represents coefficients interest,\\(\\beta\\) represents coefficients interest,\\(c_i\\) unobserved individual-specific effect (time-invariant),\\(c_i\\) unobserved individual-specific effect (time-invariant),\\(u_{}\\) idiosyncratic error (time-varying).\\(u_{}\\) idiosyncratic error (time-varying).contrast Fixed Effects model, eliminates \\(c_i\\) demeaning data, Random Effects model treats \\(c_i\\) random variable incorporates error structure.","code":""},{"path":"data.html","id":"key-assumptions-for-random-effects","chapter":"11 Data","heading":"11.5.6.1 Key Assumptions for Random Effects","text":"Random Effects estimator consistent, following assumptions must hold:Exogeneity Time-Varying Error (\\(u_{}\\)) (A3a)idiosyncratic error term (\\(u_{}\\)) must uncorrelated regressors:\\[\nE(\\mathbf{x}_{}' u_{}) = 0\n\\]assumption ensures within-period variation regressors systematically affect error term.Exogeneity Individual-Specific Effects (\\(c_i\\)) (A3a)crucial assumption RE model individual effect (\\(c_i\\)) uncorrelated explanatory variables:\\[\nE(\\mathbf{x}_{}' c_i) = 0\n\\]means individual-specific unobserved characteristics systematically affect choice explanatory variables.assumption fails, RE model produces biased inconsistent estimates due omitted variable bias.assumption holds, RE efficient FE retains within-group -group variation.Serial Correlation \\(u_{}\\)error term (\\(u_{}\\)) must uncorrelated across time:\\[\nE(u_{} u_{}) = 0, \\quad \\forall t \\neq s\n\\]assumption fails:Standard errors incorrect.Standard errors incorrect.Generalized Least Squares adjustments cluster-robust standard errors required.Generalized Least Squares adjustments cluster-robust standard errors required.","code":""},{"path":"data.html","id":"efficiency-of-random-effects","chapter":"11 Data","heading":"11.5.6.2 Efficiency of Random Effects","text":"RE estimator GLS estimator, meaning BLUE (Best Linear Unbiased Estimator) homoskedasticity.variance errors differs across individuals, RE can still used must adjusted robust standard errors.errors correlated time, standard Newey-West cluster-robust standard errors applied.efficiently estimate \\(\\beta\\), transform RE model using Generalized Least Squares.Define quasi-demeaned transformation:\\[\n\\tilde{y}_{} = y_{} - \\theta \\bar{y}_i\n\\]\\[\n\\tilde{\\mathbf{x}}_{} = \\mathbf{x}_{} - \\theta \\bar{\\mathbf{x}}_i\n\\]:\\[\n\\theta = 1 - \\sqrt{\\frac{\\sigma^2_u}{T\\sigma^2_c + \\sigma^2_u}}\n\\]\\(\\theta = 1\\), RE becomes FE estimator.\\(\\theta = 0\\), RE becomes Pooled OLS.final RE regression equation :\\[\n\\tilde{y}_{} = \\tilde{\\mathbf{x}}_{} \\beta + \\tilde{u}_{}\n\\]estimated using GLS.","code":""},{"path":"data.html","id":"sec-fixed-effects-estimator","chapter":"11 Data","heading":"11.5.7 Fixed Effects Estimator","text":"Also known Within Estimator, FE model controls individual-specific effects removing transformation.Key AssumptionIf RE assumption fails (\\(E(\\mathbf{x}_{}' c_i) \\neq 0\\)), :Pooled OLS RE become biased inconsistent (due omitted variable bias).FE still consistent eliminates \\(c_i\\).However, FE corrects bias time-invariant factors handle time-variant omitted variables.Challenges FEBias Lagged Dependent Variables: FE biased dynamic models (Nickell 1981; Narayanan Nair 2013).Exacerbates Measurement Error: FE can worsen errors--variables bias.","code":""},{"path":"data.html","id":"sec-demean-within-transformation","chapter":"11 Data","heading":"11.5.7.1 Demean (Within) Transformation","text":"remove \\(c_i\\), take individual mean regression equation:\\[\ny_{} = \\mathbf{x}_{} \\beta + c_i + u_{}\n\\]Averaging time (\\(T\\)):\\[\n\\bar{y}_i = \\bar{\\mathbf{x}}_i \\beta + c_i + \\bar{u}_i\n\\]Subtracting second equation first (.e., within transformation):\\[\n(y_{} - \\bar{y}_i) = (\\mathbf{x}_{} - \\bar{\\mathbf{x}}_i) \\beta + (u_{} - \\bar{u}_i)\n\\]transformation:Eliminates \\(c_i\\), solving omitted variable bias.Eliminates \\(c_i\\), solving omitted variable bias.uses within-individual variation.uses within-individual variation.transformed regression estimated via OLS:\\[\ny_{} - \\bar{y}_i = (\\mathbf{x}_{} - \\bar{\\mathbf{x}}_i) \\beta + d_1 \\delta_1 + \\dots + d_{T-2} \\delta_{T-2} + (u_{} - \\bar{u}_i)\n\\]\\(d_t\\) time dummy variable, equals 1 observation time periods \\(t\\), 0 otherwise. variable period \\(t = 1, \\dots, T - 1\\) (one period omitted avoid perfect multicollinearity).\\(d_t\\) time dummy variable, equals 1 observation time periods \\(t\\), 0 otherwise. variable period \\(t = 1, \\dots, T - 1\\) (one period omitted avoid perfect multicollinearity).\\(\\delta_t\\) coefficient time dummy, capturing aggregate shocks affect individual period \\(t\\).\\(\\delta_t\\) coefficient time dummy, capturing aggregate shocks affect individual period \\(t\\).Key Conditions Consistency:Strict Exogeneity (A3):\\[\n  E[(\\mathbf{x}_{} - \\bar{\\mathbf{x}}_i)' (u_{} - \\bar{u}_i)] = 0\n  \\]Strict Exogeneity (A3):\\[\n  E[(\\mathbf{x}_{} - \\bar{\\mathbf{x}}_i)' (u_{} - \\bar{u}_i)] = 0\n  \\]Time-invariant variables dropped (e.g., gender, ethnicity). ’re interested effect time-invariant variables, consider using either OLS estimator.Time-invariant variables dropped (e.g., gender, ethnicity). ’re interested effect time-invariant variables, consider using either OLS estimator.Cluster-Robust Standard Errors used.Cluster-Robust Standard Errors used.","code":""},{"path":"data.html","id":"sec-dummy-variable-approach","chapter":"11 Data","heading":"11.5.7.2 Dummy Variable Approach","text":"Dummy Variable Approach alternative way estimate Fixed Effects panel data. Instead transforming data demeaning (Within Transformation), method explicitly includes individual dummy variables control entity-specific heterogeneity.general FE model :\\[\ny_{} = \\mathbf{x}_{} \\beta + c_i + u_{}\n\\]:\\(c_i\\) unobserved, time-invariant individual effect (e.g., ability, cultural preferences, managerial style).\\(c_i\\) unobserved, time-invariant individual effect (e.g., ability, cultural preferences, managerial style).\\(u_{}\\) idiosyncratic error term (fluctuates time across individuals).\\(u_{}\\) idiosyncratic error term (fluctuates time across individuals).estimate model using Dummy Variable Approach, include separate dummy variable individual:\\[\ny_{} = \\mathbf{x}_{} \\beta + d_1 \\delta_1 + ... + d_{T-2} \\delta_{T-2} + c_1 \\gamma_1 + ... + c_{n-1} \\gamma_{n-1} + u_{}\n\\]:\\(c_i\\) now modeled explicitly dummy variable (\\(c_i \\gamma_i\\)) individual.\\(c_i\\) now modeled explicitly dummy variable (\\(c_i \\gamma_i\\)) individual.\\(d_t\\) time dummies, capturing time-specific shocks.\\(d_t\\) time dummies, capturing time-specific shocks.\\(\\delta_t\\) coefficients time dummies, controlling common time effects.\\(\\delta_t\\) coefficients time dummies, controlling common time effects.Interpretation Dummy VariablesThe dummy variable \\(c_i\\) takes value 1 individual \\(\\) 0 otherwise:\n\\[\nc_i =\n\\begin{cases}\n1 &\\text{observation individual } \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]dummy variable \\(c_i\\) takes value 1 individual \\(\\) 0 otherwise:\\[\nc_i =\n\\begin{cases}\n1 &\\text{observation individual } \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]\\(N\\) dummy variables absorb individual-specific variation, ensuring within-individual (-time) variation remains.\\(N\\) dummy variables absorb individual-specific variation, ensuring within-individual (-time) variation remains.Advantages Dummy Variable ApproachEasy Interpret: Explicitly includes entity-specific effects, making easier understand individual heterogeneity modeled.Equivalent Within (Demean) Transformation: Mathematically, approach produces coefficient estimates Within Transformation.Allows Inclusion Time Dummies: model can easily incorporate time dummies (\\(d_t\\)) control period-specific shocks.Limitations Dummy Variable ApproachComputational Complexity Large \\(N\\)\nAdding \\(N\\) dummy variables significantly increases number parameters estimated.\n\\(N\\) large (e.g., 10,000 individuals), approach can computationally expensive.\nAdding \\(N\\) dummy variables significantly increases number parameters estimated.\\(N\\) large (e.g., 10,000 individuals), approach can computationally expensive.Standard Errors Incorrectly Estimated\nstandard errors \\(c_i\\) dummy variables often incorrectly calculated, absorb within-individual variation.\nWithin Transformation (Demeaning Approach) generally preferred.\nstandard errors \\(c_i\\) dummy variables often incorrectly calculated, absorb within-individual variation.Within Transformation (Demeaning Approach) generally preferred.Consumes Degrees Freedom\nIntroducing \\(N\\) additional parameters reduces degrees freedom, can lead overfitting.\nIntroducing \\(N\\) additional parameters reduces degrees freedom, can lead overfitting.","code":""},{"path":"data.html","id":"sec-first-difference-approach","chapter":"11 Data","heading":"11.5.7.3 First-Difference Approach","text":"alternative way eliminate individual-specific effects (\\(c_i\\)) take first differences across time, rather subtracting individual mean.FE model:\\[\ny_{} = \\mathbf{x}_{} \\beta + c_i + u_{}\n\\]Since \\(c_i\\) constant time, taking first difference:\\[\ny_{} - y_{(t-1)} = (\\mathbf{x}_{} - \\mathbf{x}_{(t-1)}) \\beta + (u_{} - u_{(t-1)})\n\\]transformation removes \\(c_i\\) completely, leaving model can estimated using Pooled OLS.Advantages First-Difference ApproachEliminates Individual Effects (\\(c_i\\))\nSince \\(c_i\\) time-invariant, differencing removes equation.\nSince \\(c_i\\) time-invariant, differencing removes equation.Works Well Time Periods (\\(T\\) Small)\n\\(T\\) small, first-differencing often preferred Within Transformation, require averaging many periods.\n\\(T\\) small, first-differencing often preferred Within Transformation, require averaging many periods.Less Computationally Intensive\nUnlike Dummy Variable Approach, requires estimating \\(N\\) additional parameters, First-Difference Approach reduces dimensionality problem.\nUnlike Dummy Variable Approach, requires estimating \\(N\\) additional parameters, First-Difference Approach reduces dimensionality problem.Limitations First-Difference ApproachCannot Handle Missing Observations Well\ndata missing period \\(t-1\\) individual, corresponding first-difference observation lost.\ncan significantly reduce sample size unbalanced panels.\ndata missing period \\(t-1\\) individual, corresponding first-difference observation lost.can significantly reduce sample size unbalanced panels.Reduces Number Observations One\nSince first differences require \\(y_{(t-1)}\\) exist, model loses one time period (\\(T-1\\) observations per individual instead \\(T\\)).\nSince first differences require \\(y_{(t-1)}\\) exist, model loses one time period (\\(T-1\\) observations per individual instead \\(T\\)).Can Introduce Serial Correlation\nSince differencing \\(u_{} - u_{(t-1)}\\), error term now exhibits autocorrelation.\nmeans standard OLS assumptions (independent errors) longer hold, requiring use robust standard errors.\nSince differencing \\(u_{} - u_{(t-1)}\\), error term now exhibits autocorrelation.means standard OLS assumptions (independent errors) longer hold, requiring use robust standard errors.","code":""},{"path":"data.html","id":"comparison-of-fe-approaches","chapter":"11 Data","heading":"11.5.7.4 Comparison of FE Approaches","text":"Key InsightsThe Dummy Variable Approach explicitly models \\(c_i\\) computationally expensive large \\(N\\).Within (Demean) Transformation commonly used FE method computationally efficient produces correct standard errors.First-Difference Approach useful \\(T\\) small, reduces sample size introduces autocorrelation.data many missing values, First-Difference recommended due sensitivity gaps observations.Time dummies (\\(d_t\\)) can included FE model control time shocks affect individuals.FE exploits within variation, meaning status changes contribute \\(\\beta\\) estimates.limited status changes, standard errors explode (small number switchers leads high variance).Treatment effect non-directional can parameterized.Switchers vs. Non-Switchers:\nswitchers differ fundamentally, FE estimator may still biased.\nDescriptive statistics switchers/non-switchers help verify robustness.\nswitchers differ fundamentally, FE estimator may still biased.Descriptive statistics switchers/non-switchers help verify robustness.","code":""},{"path":"data.html","id":"variance-of-errors-in-fe","chapter":"11 Data","heading":"11.5.7.5 Variance of Errors in FE","text":"FE reduces variation removing \\(c_i\\), affects error variance:\\[ \\hat{\\sigma}^2_{\\epsilon} = \\frac{SSR_{OLS}}{NT - K} \\]\\[ \\hat{\\sigma}^2_u = \\frac{SSR_{FE}}{NT - (N+K)} = \\frac{SSR_{FE}}{N(T-1)-K} \\]Implication:variance error may increase decrease :\n\\(SSR\\) can increase (since FE eliminates variation).\nDegrees freedom decrease (parameters estimated).\nvariance error may increase decrease :\\(SSR\\) can increase (since FE eliminates variation).\\(SSR\\) can increase (since FE eliminates variation).Degrees freedom decrease (parameters estimated).Degrees freedom decrease (parameters estimated).","code":""},{"path":"data.html","id":"fixed-effects-examples","chapter":"11 Data","heading":"11.5.7.6 Fixed Effects Examples","text":"","code":""},{"path":"data.html","id":"intergenerational-mobility-blau1999","chapter":"11 Data","heading":"11.5.7.6.1 Intergenerational Mobility – Blau (1999)","text":"Research QuestionsDoes transferring resources low-income families improve upward mobility children?mechanisms intergenerational mobility?Mechanisms Intergenerational MobilityThere multiple pathways parental income influences child outcomes:Genetics (Ability Endowment)\nmobility purely genetic, policy affect outcomes.\nmobility purely genetic, policy affect outcomes.Environmental Indirect Effects\nFamily background, peer influences, school quality.\nFamily background, peer influences, school quality.Environmental Direct Effects\nParental investments education, health, social capital.\nParental investments education, health, social capital.Financial Transfers\nDirect monetary support, inheritance, wealth accumulation.\nDirect monetary support, inheritance, wealth accumulation.One way measure impact income human capital accumulation :\\[\n\\frac{\\% \\Delta \\text{Human Capital}}{\\% \\Delta \\text{Parental Income}}\n\\]human capital includes education, skills, job market outcomes.Income measured different ways capture long-term effects:Total household incomeWage incomeNon-wage incomeAnnual vs. Permanent Income (important distinction long-term analysis)Key control variables must exogenous avoid bias. Bad Controls jointly determined dependent variable (e.g., mother’s labor force participation).Exogenous controls:Mother’s raceMother’s raceBirth locationBirth locationParental educationParental educationHousehold structure age 14Household structure age 14The estimated model :\\[\nY_{ijt} = X_{jt} \\beta_i + I_{jt} \\alpha_i + \\epsilon_{ijt}\n\\]:\\(\\) = test (e.g., academic test score).\\(\\) = test (e.g., academic test score).\\(j\\) = individual (child).\\(j\\) = individual (child).\\(t\\) = time.\\(t\\) = time.\\(X_{jt}\\) = observable child characteristics.\\(X_{jt}\\) = observable child characteristics.\\(I_{jt}\\) = parental income.\\(I_{jt}\\) = parental income.\\(\\epsilon_{ijt}\\) = error term.\\(\\epsilon_{ijt}\\) = error term.Grandmother’s Fixed-Effects ModelSince child (\\(j\\)) nested within mother (\\(m\\)), mother nested within grandmother (\\(g\\)), estimate:\\[\nY_{ijgmt} = X_{} \\beta_{} + I_{jt} \\alpha_i + \\gamma_g + u_{ijgmt}\n\\]:\\(g\\) = Grandmother, \\(m\\) = Mother, \\(j\\) = Child, \\(t\\) = Time.\\(g\\) = Grandmother, \\(m\\) = Mother, \\(j\\) = Child, \\(t\\) = Time.\\(\\gamma_g\\) captures grandmother mother fixed effects.\\(\\gamma_g\\) captures grandmother mother fixed effects.nested structure controls genetic fixed family environment effects.nested structure controls genetic fixed family environment effects.Cluster standard errors family level account correlation errors across generations.Cluster standard errors family level account correlation errors across generations.Pros Grandmother FE ModelControls genetics + fixed family backgroundControls genetics + fixed family backgroundAllows estimation income effects independent family backgroundAllows estimation income effects independent family backgroundConsMight fully control unobserved heterogeneityMight fully control unobserved heterogeneityMeasurement errors income can exaggerate attenuation biasMeasurement errors income can exaggerate attenuation bias","code":""},{"path":"data.html","id":"fixed-effects-in-teacher-quality-studies-babcock2010","chapter":"11 Data","heading":"11.5.7.6.2 Fixed Effects in Teacher Quality Studies – Babcock (2010)","text":"study investigates:teacher quality influences student performance.teacher quality influences student performance.Whether students adjust course selection behavior based past grading experiences.Whether students adjust course selection behavior based past grading experiences.properly estimate teacher fixed effects addressing selection bias measurement error.properly estimate teacher fixed effects addressing selection bias measurement error.initial model estimates student performance (\\(T_{ijct}\\)) based class expectations student characteristics:\\[\nT_{ijct} = \\alpha_0 + S_{jct} \\alpha_1 + X_{ijct} \\alpha_2 + u_{ijct}\n\\]:\\(T_{ijct}\\) = Student test score.\\(T_{ijct}\\) = Student test score.\\(S_{jct}\\) = Class-level grading expectation (e.g., expected GPA course).\\(S_{jct}\\) = Class-level grading expectation (e.g., expected GPA course).\\(X_{ijct}\\) = Individual student characteristics.\\(X_{ijct}\\) = Individual student characteristics.\\(\\) = Student, \\(j\\) = Instructor, \\(c\\) = Course, \\(t\\) = Time.\\(\\) = Student, \\(j\\) = Instructor, \\(c\\) = Course, \\(t\\) = Time.\\(u_{ijct}\\) = Idiosyncratic error term.\\(u_{ijct}\\) = Idiosyncratic error term.key issue model grading expectations may randomly assigned. students select courses based grading expectations, simultaneity bias can arise.control instructor course heterogeneity, model introduces teacher-course fixed effects (\\(\\mu_{jc}\\)):\\[\nT_{ijct} = \\beta_0+ S_{jct} \\beta_1+ X_{ijct} \\beta_2 +\\mu_{jc} + \\epsilon_{ijct}\n\\]:\\(\\mu_{jc}\\) unique fixed effect instructor-course combination.\\(\\mu_{jc}\\) unique fixed effect instructor-course combination.controls instructor-specific grading policies course difficulty.controls instructor-specific grading policies course difficulty.differs simple instructor effect (\\(\\theta_j\\)) course effect (\\(\\delta_c\\)) captures interaction effects.differs simple instructor effect (\\(\\theta_j\\)) course effect (\\(\\delta_c\\)) captures interaction effects.Implications Instructor-Course Fixed EffectsReduces Bias Course Shopping\nStudents may select courses based grading expectations.\nIncluding \\(\\mu_{jc}\\) controls fact instructors systematically assign easier grades.\nStudents may select courses based grading expectations.Including \\(\\mu_{jc}\\) controls fact instructors systematically assign easier grades.Shifts Student Expectations\nEven course content remains constant, students adjust expectations based past grading experiences.\ninfluences future course selection behavior.\nEven course content remains constant, students adjust expectations based past grading experiences.influences future course selection behavior.Identification StrategyA key challenge estimating teacher effects endogeneity :Simultaneity Bias\nGrading expectations (\\(S_{jct}\\)) student performance may jointly determined.\ngrading expectations based past student performance, OLS biased.\nGrading expectations (\\(S_{jct}\\)) student performance may jointly determined.grading expectations based past student performance, OLS biased.Unobserved Teacher Characteristics\nteachers may innate ability motivate students, leading higher student performance independent observable teacher traits.\nteachers may innate ability motivate students, leading higher student performance independent observable teacher traits.address concerns, model first controls observable teacher characteristics:\\[\n\\begin{aligned}\nY_{ijt} &= X_{} \\beta_1 + \\text{Teacher Experience}_{jt} \\beta_2 + \\text{Teacher Education}_{jt} \\beta_3 \\\\\n&+ \\text{Teacher Score}_{}\\beta_4 + \\dots + \\epsilon_{ijt}\n\\end{aligned}\n\\]However, teacher characteristics correlated unobserved ability, replace teacher fixed effects:\\[\nY_{ijt} = X_{} \\alpha + \\Gamma_{} \\theta_j + u_{ijt}\n\\]:\\(\\theta_j\\) = Teacher Fixed Effect, capturing time-invariant teacher characteristics.\\(\\theta_j\\) = Teacher Fixed Effect, capturing time-invariant teacher characteristics.\\(\\Gamma_{}\\) represents within-teacher variation.\\(\\Gamma_{}\\) represents within-teacher variation.analyze teacher impact, express student test scores :\\[\nY_{ijt} = X_{} \\gamma + \\epsilon_{ijt}\n\\]:\\(\\gamma\\) represents within variation.\\(\\gamma\\) represents within variation.\\(e_{ijt}\\) prediction error.\\(e_{ijt}\\) prediction error.Decomposing error term:\\[\ne_{ijt} = T_{} \\delta_j + \\tilde{e}_{ijt}\n\\]:\\(\\delta_j\\) = Group-level teacher effect.\\(\\delta_j\\) = Group-level teacher effect.\\(\\tilde{e}_{ijt}\\) = Residual error.\\(\\tilde{e}_{ijt}\\) = Residual error.control prior student performance, introduce lagged test scores:\\[\nY_{ijkt} = Y_{ijkt-1} + X_{} \\beta + T_{} \\tau_j + (W_i + P_k + \\epsilon_{ijkt})\n\\]:\\(Y_{ijkt-1}\\) = Lagged student test score.\\(Y_{ijkt-1}\\) = Lagged student test score.\\(\\tau_j\\) = Teacher Fixed Effect.\\(\\tau_j\\) = Teacher Fixed Effect.\\(W_i\\) = Student Fixed Effect.\\(W_i\\) = Student Fixed Effect.\\(P_k\\) = School Fixed Effect.\\(P_k\\) = School Fixed Effect.\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\).\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\).major issue selection bias:students sort better teachers, teacher effect (\\(\\tau\\)) may overestimated.students sort better teachers, teacher effect (\\(\\tau\\)) may overestimated.Bias \\(\\tau\\) teacher \\(j\\) :Bias \\(\\tau\\) teacher \\(j\\) :\\[\n\\frac{1}{N_j} \\sum_{= 1}^{N_j} (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(N_j\\) number students class teacher \\(j\\).Smaller class sizes → Higher bias teacher effect estimates \\(\\frac{1}{N_j} \\sum_{= 1}^{N_j} \\epsilon_{ijkt} \\neq 0\\) inflate teacher fixed effect. use random teacher effects instead, \\(\\tau\\) still contain bias know direction bias.teachers switch schools, can separately estimate:Teacher Fixed Effects (\\(\\tau_j\\))Teacher Fixed Effects (\\(\\tau_j\\))School Fixed Effects (\\(P_k\\))School Fixed Effects (\\(P_k\\))mobility web refers network teacher transitions across schools, helps identifying teacher school fixed effects.Thin mobility web: teachers switch schools, making harder separate teacher effects school effects.Thin mobility web: teachers switch schools, making harder separate teacher effects school effects.Thick mobility web: Many teachers switch schools, improving identification teacher quality independent school characteristics.Thick mobility web: Many teachers switch schools, improving identification teacher quality independent school characteristics.panel data model capturing student performance time :\\[\nY_{ijkt} = Y_{ijk(t-1)} \\alpha + X_{} \\beta + T_{} \\tau + P_k + \\epsilon_{ijkt}\n\\]:\\(Y_{ijkt}\\) = Student performance time \\(t\\).\\(Y_{ijkt}\\) = Student performance time \\(t\\).\\(Y_{ijk(t-1)}\\) = Lagged student test score.\\(Y_{ijk(t-1)}\\) = Lagged student test score.\\(X_{}\\) = Student characteristics.\\(X_{}\\) = Student characteristics.\\(T_{}\\) = Teacher effect (\\(\\tau\\)).\\(T_{}\\) = Teacher effect (\\(\\tau\\)).\\(P_k\\) = School fixed effect.\\(P_k\\) = School fixed effect.\\(\\epsilon_{ijkt}\\) = Idiosyncratic error term.\\(\\epsilon_{ijkt}\\) = Idiosyncratic error term.apply fixed effects (demeaning transformation):\\[\nY_{ijkt} - \\bar{Y}_{ijk} = (X_{} - \\bar{X}_i) \\beta + (T_{} - \\bar{T}_i) \\tau + (P_k - \\bar{P}) + (\\epsilon_{ijkt} - \\bar{\\epsilon}_{ijk})\n\\]transformation removes teacher fixed effects (\\(\\tau\\)).want explicitly estimate \\(\\tau\\), must include teacher fixed effects demeaning.paper argues controlling school fixed effects (\\(P_k\\)) ensures selection bias, meaning students randomly assigned within schools.key claim paper teacher quality (\\(\\tau\\)) depend number students per teacher (\\(N_j\\)).test , examine variance estimated teacher effects:\\[\nvar(\\tau)\n\\]:\\[\nvar(\\tau) = 0\n\\]implies teacher quality impact student performance.empirically test , study analyzes:\\[\n\\frac{1}{N_j} \\sum_{= 1}^{N_j} \\epsilon_{ijkt}\n\\]represents teacher-level average residuals.Key Finding:variance teacher effects remains stable across different class sizes (\\(N_j\\)).variance teacher effects remains stable across different class sizes (\\(N_j\\)).suggests random assignment students across teachers biasing \\(\\tau\\).suggests random assignment students across teachers biasing \\(\\tau\\).Since teacher effects (\\(\\tau_j\\)) estimated error (Spin-[Measurement Error]), decompose :\\[\n\\hat{\\tau}_j = \\tau_j + \\lambda_j\n\\]:\\(\\tau_j\\) = True teacher effect.\\(\\tau_j\\) = True teacher effect.\\(\\lambda_j\\) = Measurement error (e.g., sampling error, estimation noise).\\(\\lambda_j\\) = Measurement error (e.g., sampling error, estimation noise).Assuming \\(\\tau_j\\) \\(\\lambda_j\\) uncorrelated:\\[\ncov(\\tau_j, \\lambda_j) = 0\n\\]means randomness student assignments systematically bias teacher quality estimates.total observed variance estimated teacher effects :\\[\nvar(\\hat{\\tau}) = var(\\tau) + var(\\lambda)\n\\]Rearranging:\\[\nvar(\\tau) = var(\\hat{\\tau}) - var(\\lambda)\n\\]Since observe \\(var(\\hat{\\tau})\\), need estimate \\(var(\\lambda)\\).Measurement error variance (\\(var(\\lambda)\\)) can approximated using average squared standard error teacher effects:\\[\nvar(\\lambda) = \\frac{1}{J} \\sum_{j=1}^J \\hat{\\sigma}^2_j\n\\]\\(\\hat{\\sigma}^2_j\\) squared standard error teacher \\(j\\) (depends sample size \\(N_j\\)).signal--noise ratio (reliability) teacher effect estimates :\\[\n\\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{Reliability}\n\\]:Higher reliability indicates variation comes true teacher effects (\\(\\tau\\)) rather noise.Higher reliability indicates variation comes true teacher effects (\\(\\tau\\)) rather noise.Lower reliability suggests large portion variation due measurement error.Lower reliability suggests large portion variation due measurement error.proportion error variance estimated teacher effects :\\[\n1 - \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{Noise}\n\\]Even true teacher quality depends class size (\\(N_j\\)), method estimating \\(\\lambda\\) remains unaffected.check whether teacher effects biased sampling error, regress estimated teacher effects (\\(\\hat{\\tau}_j\\)) teacher characteristics (\\(X_j\\)):\\[\n\\hat{\\tau}_j = \\beta_0 + X_j \\beta_1 + \\epsilon_j\n\\]teacher characteristics predict sampling error, :\\[\nR^2 \\approx 0\n\\]confirm teacher characteristics uncorrelated measurement error, validating identification strategy.","code":""},{"path":"data.html","id":"tests-for-assumptions-in-panel-data-analysis","chapter":"11 Data","heading":"11.5.8 Tests for Assumptions in Panel Data Analysis","text":"typically don’t test heteroskedasticity explicitly robust covariance matrix estimation used. However, key assumptions tested choosing appropriate panel model.","code":"\nlibrary(\"plm\")\ndata(\"EmplUK\", package=\"plm\")\ndata(\"Produc\", package=\"plm\")\ndata(\"Grunfeld\", package=\"plm\")\ndata(\"Wages\", package=\"plm\")"},{"path":"data.html","id":"poolability-test","chapter":"11 Data","heading":"11.5.8.1 Poolability Test","text":"Tests whether coefficients across individuals (also known \\(F\\)-test stability Chow test).\\(H_0\\): individuals coefficients (.e., equal coefficients individuals).\\(H_0\\): individuals coefficients (.e., equal coefficients individuals).\\(H_a\\): Different individuals different coefficients.\\(H_a\\): Different individuals different coefficients.Notes:fixed effects model assumes different intercepts per individual.fixed effects model assumes different intercepts per individual.random effects model assumes common intercept.random effects model assumes common intercept.null rejected, use pooled OLS model.","code":"\nlibrary(plm)\nplm::pooltest(inv ~ value + capital, \n              data = Grunfeld, \n              model = \"within\")\n#> \n#>  F statistic\n#> \n#> data:  inv ~ value + capital\n#> F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10\n#> alternative hypothesis: unstability"},{"path":"data.html","id":"testing-for-individual-and-time-effects","chapter":"11 Data","heading":"11.5.8.2 Testing for Individual and Time Effects","text":"Checks presence individual time effects, .Types tests:\nhonda: Default test individual effects (Honda 1985)\nbp: Breusch-Pagan test unbalanced panels (Breusch Pagan 1980)\nkw: King-Wu test unbalanced panels two-way effects (M. L. King Wu 1997)\nghm: Gourieroux, Holly, Monfort test two-way effects (Gourieroux, Holly, Monfort 1982)\nTypes tests:honda: Default test individual effects (Honda 1985)honda: Default test individual effects (Honda 1985)bp: Breusch-Pagan test unbalanced panels (Breusch Pagan 1980)bp: Breusch-Pagan test unbalanced panels (Breusch Pagan 1980)kw: King-Wu test unbalanced panels two-way effects (M. L. King Wu 1997)kw: King-Wu test unbalanced panels two-way effects (M. L. King Wu 1997)ghm: Gourieroux, Holly, Monfort test two-way effects (Gourieroux, Holly, Monfort 1982)ghm: Gourieroux, Holly, Monfort test two-way effects (Gourieroux, Holly, Monfort 1982)null hypothesis rejected, fixed effects model appropriate.","code":"\npFtest(inv ~ value + capital, \n       data = Grunfeld, \n       effect = \"twoways\")\n#> \n#>  F test for twoways effects\n#> \n#> data:  inv ~ value + capital\n#> F = 17.403, df1 = 28, df2 = 169, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv ~ value + capital, \n       data = Grunfeld, \n       effect = \"individual\")\n#> \n#>  F test for individual effects\n#> \n#> data:  inv ~ value + capital\n#> F = 49.177, df1 = 9, df2 = 188, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv ~ value + capital, \n       data = Grunfeld, \n       effect = \"time\")\n#> \n#>  F test for time effects\n#> \n#> data:  inv ~ value + capital\n#> F = 0.23451, df1 = 19, df2 = 178, p-value = 0.9997\n#> alternative hypothesis: significant effects"},{"path":"data.html","id":"cross-sectional-dependence-contemporaneous-correlation","chapter":"11 Data","heading":"11.5.8.3 Cross-Sectional Dependence (Contemporaneous Correlation)","text":"Tests whether residuals across entities correlated.","code":""},{"path":"data.html","id":"global-cross-sectional-dependence","chapter":"11 Data","heading":"11.5.8.3.1 Global Cross-Sectional Dependence","text":"","code":"\npcdtest(inv ~ value + capital, \n        data = Grunfeld, model = \"within\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  inv ~ value + capital\n#> z = 4.6612, p-value = 3.144e-06\n#> alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"local-cross-sectional-dependence","chapter":"11 Data","heading":"11.5.8.3.2 Local Cross-Sectional Dependence","text":"Uses spatial weight matrix w.null rejected, cross-sectional correlation exists addressed.","code":"\npcdtest(inv ~ value + capital, \n        data = Grunfeld, \n        model = \"within\", \n        w = weight_matrix)"},{"path":"data.html","id":"serial-correlation-in-panel-data","chapter":"11 Data","heading":"11.5.8.4 Serial Correlation in Panel Data","text":"Null hypothesis: serial correlation.Serial correlation typically observed macro panels long time series (large \\(N\\) \\(T\\)). less relevant micro panels short time series (small \\(T\\) large \\(N\\)).Sources Serial Correlation:\nUnobserved individual effects: Time-invariant error components.\nIdiosyncratic error terms: Often modeled autoregressive process (e.g., AR(1)).\nTypically, “serial correlation” refers second type (idiosyncratic errors).\nUnobserved individual effects: Time-invariant error components.Idiosyncratic error terms: Often modeled autoregressive process (e.g., AR(1)).Typically, “serial correlation” refers second type (idiosyncratic errors).Types Serial Correlation TestsMarginal tests: Test one type dependence time may biased towards rejection.Joint tests: Detect sources dependence distinguish source problem.Conditional tests: Assume one dependence structure correctly specified test additional departures.","code":""},{"path":"data.html","id":"unobserved-effects-test","chapter":"11 Data","heading":"11.5.8.4.1 Unobserved Effects Test","text":"semi-parametric test unobserved effects, test statistic \\(W \\sim N\\) regardless error distribution.Null hypothesis (\\(H_0\\)): unobserved effects (\\(\\sigma^2_\\mu = 0\\)), supports using pooled OLS.\\(H_0\\): covariance matrix residuals diagonal (-diagonal correlations).Robustness: test robust unobserved individual effects serial correlation.Interpretation: reject \\(H_0\\), pooled OLS inappropriate due presence unobserved effects.","code":"\nlibrary(plm)\ndata(\"Produc\", package = \"plm\")\n\n# Wooldridge test for unobserved individual effects\npwtest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n       data = Produc)\n#> \n#>  Wooldridge's test for unobserved individual effects\n#> \n#> data:  formula\n#> z = 3.9383, p-value = 8.207e-05\n#> alternative hypothesis: unobserved effect"},{"path":"data.html","id":"locally-robust-tests-for-serial-correlation-and-random-effects","chapter":"11 Data","heading":"11.5.8.4.2 Locally Robust Tests for Serial Correlation and Random Effects","text":"Joint LM Test Random Effects Serial CorrelationJoint LM Test Random Effects Serial CorrelationA Lagrange Multiplier test jointly detect:\nRandom effects (panel-level variance components).\nSerial correlation (time-series dependence).\nLagrange Multiplier test jointly detect:Random effects (panel-level variance components).Serial correlation (time-series dependence).Null Hypothesis: Normality homoskedasticity idiosyncratic errors (Baltagi Li 1991, 1995).\nequivalent assuming presence serial correlation, random effects.\nNull Hypothesis: Normality homoskedasticity idiosyncratic errors (Baltagi Li 1991, 1995).equivalent assuming presence serial correlation, random effects.Interpretation: reject \\(H_0\\), either serial correlation, random effects, present. don’t know source dependence.distinguish source dependence, use either (tests assume normality homoskedasticity) (Bera, Sosa-Escudero, Yoon 2001):BSY Test Serial CorrelationBSY Test Random EffectsIf serial correlation “known” absent (based BSY test), LM test random effects superior.random effects absent (based BSY test), use Breusch-Godfrey’s serial correlation test (Breusch 1978; Godfrey 1978).Random Effects Present: Use Baltagi Li’s TestBaltagi Li’s test detects serial correlation AR(1) MA(1) processes random effects.Null hypothesis (\\(H_0\\)): Uncorrelated errors.Null hypothesis (\\(H_0\\)): Uncorrelated errors.Note:\ntest power positive serial correlation (one-sided).\napplicable balanced panels\nNote:test power positive serial correlation (one-sided).test power positive serial correlation (one-sided).applicable balanced panelsIt applicable balanced panels","code":"\n# Baltagi and Li's joint test for serial correlation and random effects\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n         data = Produc, \n         test = \"j\")\n#> \n#>  Baltagi and Li AR-RE joint test\n#> \n#> data:  formula\n#> chisq = 4187.6, df = 2, p-value < 2.2e-16\n#> alternative hypothesis: AR(1) errors or random effects\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n         data = Produc)\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test\n#> \n#> data:  formula\n#> chisq = 52.636, df = 1, p-value = 4.015e-13\n#> alternative hypothesis: AR(1) errors sub random effects\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n         data = Produc, \n         test = \"re\")\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test (one-sided)\n#> \n#> data:  formula\n#> z = 57.914, p-value < 2.2e-16\n#> alternative hypothesis: random effects sub AR(1) errors\nplmtest(inv ~ value + capital, \n        data = Grunfeld, \n        type = \"honda\")\n#> \n#>  Lagrange Multiplier Test - (Honda)\n#> \n#> data:  inv ~ value + capital\n#> normal = 28.252, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\nlmtest::bgtest()\npbltest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n        data = Produc, \n        alternative = \"onesided\")\n#> \n#>  Baltagi and Li one-sided LM test\n#> \n#> data:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\n#> z = 21.69, p-value < 2.2e-16\n#> alternative hypothesis: AR(1)/MA(1) errors in RE panel model"},{"path":"data.html","id":"general-serial-correlation-tests","chapter":"11 Data","heading":"11.5.8.4.3 General Serial Correlation Tests","text":"Applicable random effects, pooled OLS, fixed effects models.Applicable random effects, pooled OLS, fixed effects models.Can test higher-order serial correlation.Can test higher-order serial correlation.short panels (Small \\(T\\), Large \\(N\\)), use Wooldridge’s test:","code":"\n# Baltagi-Griffin test for higher-order serial correlation\nplm::pbgtest(plm::plm(inv ~ value + capital, \n                      data = Grunfeld, \n                      model = \"within\"), \n             order = 2)\n#> \n#>  Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n#> \n#> data:  inv ~ value + capital\n#> chisq = 42.587, df = 2, p-value = 5.655e-10\n#> alternative hypothesis: serial correlation in idiosyncratic errors\npwartest(log(emp) ~ log(wage) + log(capital), \n         data = EmplUK)\n#> \n#>  Wooldridge's test for serial correlation in FE panels\n#> \n#> data:  plm.model\n#> F = 312.3, df1 = 1, df2 = 889, p-value < 2.2e-16\n#> alternative hypothesis: serial correlation"},{"path":"data.html","id":"unit-roots-and-stationarity-in-panel-data","chapter":"11 Data","heading":"11.5.8.5 Unit Roots and Stationarity in Panel Data","text":"","code":""},{"path":"data.html","id":"dickey-fuller-test-for-stochastic-trends","chapter":"11 Data","heading":"11.5.8.5.1 Dickey-Fuller Test for Stochastic Trends","text":"Purpose: Tests presence unit root (non-stationarity) time series.Null hypothesis (\\(H_0\\)): series non-stationary (.e., unit root).Alternative hypothesis (\\(H_A\\)): series stationary (unit root).Decision Rule:\ntest statistic less critical value (\\(p < 0.05\\)), reject \\(H_0\\), indicating stationarity.\ntest statistic greater critical value (\\(p \\geq 0.05\\)), fail reject \\(H_0\\), suggesting presence unit root.\ntest statistic less critical value (\\(p < 0.05\\)), reject \\(H_0\\), indicating stationarity.test statistic greater critical value (\\(p \\geq 0.05\\)), fail reject \\(H_0\\), suggesting presence unit root.reject \\(H_0\\), series stationary exhibit stochastic trend.","code":"\nlibrary(tseries)\n\n# Example: Test for unit root in GDP data\nadf.test(Produc$gsp, alternative = \"stationary\")\n#> \n#>  Augmented Dickey-Fuller Test\n#> \n#> data:  Produc$gsp\n#> Dickey-Fuller = -6.5425, Lag order = 9, p-value = 0.01\n#> alternative hypothesis: stationary"},{"path":"data.html","id":"levin-lin-chu-unit-root-test","chapter":"11 Data","heading":"11.5.8.5.2 Levin-Lin-Chu Unit Root Test","text":"Purpose: Tests presence unit root panel dataset.Null hypothesis (\\(H_0\\)): series unit root (non-stationary).Alternative hypothesis (\\(H_A\\)): series stationary.Assumptions: Requires large \\(N\\) (cross-sections) moderate \\(T\\) (time periods).Decision Rule: test statistic less critical value \\(p < 0.05\\), reject \\(H_0\\) (evidence stationarity).reject \\(H_0\\), series stationary.","code":"\nlibrary(tseries)\nlibrary(plm)\n\n# Levin-Lin-Chu (LLC) Unit Root Test\npurtest(Grunfeld, test = \"levinlin\")\n#> \n#>  Levin-Lin-Chu Unit-Root Test (ex. var.: None)\n#> \n#> data:  Grunfeld\n#> z = 0.39906, p-value = 0.6551\n#> alternative hypothesis: stationarity"},{"path":"data.html","id":"heteroskedasticity-in-panel-data","chapter":"11 Data","heading":"11.5.8.6 Heteroskedasticity in Panel Data","text":"","code":""},{"path":"data.html","id":"breusch-pagan-test","chapter":"11 Data","heading":"11.5.8.6.1 Breusch-Pagan Test","text":"Purpose: Detects heteroskedasticity regression residuals.Purpose: Detects heteroskedasticity regression residuals.Null hypothesis (\\(H_0\\)): data homoskedastic (constant variance).Null hypothesis (\\(H_0\\)): data homoskedastic (constant variance).Alternative hypothesis (\\(H_A\\)): data exhibits heteroskedasticity (non-constant variance).Alternative hypothesis (\\(H_A\\)): data exhibits heteroskedasticity (non-constant variance).Decision Rule:\np-value small (e.g., \\(p < 0.05\\)), reject \\(H_0\\), suggesting heteroskedasticity.\np-value large (\\(p \\geq 0.05\\)), fail reject \\(H_0\\), implying homoskedasticity.\nDecision Rule:p-value small (e.g., \\(p < 0.05\\)), reject \\(H_0\\), suggesting heteroskedasticity.p-value small (e.g., \\(p < 0.05\\)), reject \\(H_0\\), suggesting heteroskedasticity.p-value large (\\(p \\geq 0.05\\)), fail reject \\(H_0\\), implying homoskedasticity.p-value large (\\(p \\geq 0.05\\)), fail reject \\(H_0\\), implying homoskedasticity.heteroskedasticity detected, need adjust using robust standard errors.","code":"\nlibrary(lmtest)\n\n# Fit a panel model (pooled OLS)\nmodel <- lm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n            data = Produc)\n\n# Breusch-Pagan Test for Heteroskedasticity\nbptest(model)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  model\n#> BP = 80.033, df = 4, p-value < 2.2e-16"},{"path":"data.html","id":"robust-covariance-matrix-estimation-sandwich-estimator","chapter":"11 Data","heading":"11.5.8.6.2 Robust Covariance Matrix Estimation (Sandwich Estimator)","text":"heteroskedasticity present, robust covariance matrix estimation recommended. Different estimators apply depending whether serial correlation also issue.Choosing Correct Robust Covariance Matrix EstimatorUsing robust covariance matrix corrects heteroskedasticity /serial correlation, ensuring valid inference.","code":"\nlibrary(plm)\n\n# Fit a fixed effects model\nfe_model <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n                data = Produc, \n                model = \"within\")\n\n# Compute robust standard errors using Arellano's method\ncoeftest(fe_model, vcov = vcovHC(fe_model, method = \"arellano\"))\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value  Pr(>|t|)    \n#> log(pcap) -0.0261497  0.0603262 -0.4335   0.66480    \n#> log(pc)    0.2920069  0.0617425  4.7294 2.681e-06 ***\n#> log(emp)   0.7681595  0.0816652  9.4062 < 2.2e-16 ***\n#> unemp     -0.0052977  0.0024958 -2.1226   0.03411 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"model-selection-in-panel-data","chapter":"11 Data","heading":"11.5.9 Model Selection in Panel Data","text":"Panel data models must chosen based structure data underlying assumptions. section provides guidance selecting Pooled OLS, Random Effects, Fixed Effects models.","code":""},{"path":"data.html","id":"pooled-ols-vs.-random-effects","chapter":"11 Data","heading":"11.5.9.1 Pooled OLS vs. Random Effects","text":"choice POLS RE depends whether unobserved individual effects.Breusch-Pagan Lagrange Multiplier TestPurpose: Tests whether random effects model preferable pooled OLS model.Null hypothesis (\\(H_0\\)): Variance across entities zero (.e., panel effect → POLS preferred).Alternative hypothesis (\\(H_A\\)): significant panel-level variation → RE preferable POLS.Decision Rule: \\(p < 0.05\\), reject \\(H_0\\), indicating RE preferred.test significant, RE appropriate POLS.","code":"\nlibrary(plm)\n\n# Breusch-Pagan LM Test\nplmtest(plm(inv ~ value + capital, data = Grunfeld, \n            model = \"pooling\"), type = \"bp\")\n#> \n#>  Lagrange Multiplier Test - (Breusch-Pagan)\n#> \n#> data:  inv ~ value + capital\n#> chisq = 798.16, df = 1, p-value < 2.2e-16\n#> alternative hypothesis: significant effects"},{"path":"data.html","id":"fixed-effects-vs.-random-effects","chapter":"11 Data","heading":"11.5.9.2 Fixed Effects vs. Random Effects","text":"choice FE RE depends whether individual-specific effects correlated regressors.Key Assumptions PropertiesHausman TestPurpose: Determines whether FE RE appropriate.Hausman test work, need assume thatStrict exogeneity holdA4 hold \\(u_{}\\),Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})'(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) \\(n(X)\\) number parameters time-varying regressors.Null hypothesis (\\(H_0\\)): RE estimator consistent efficient.Alternative hypothesis (\\(H_A\\)): RE estimator inconsistent, meaning FE used.Decision Rule:\n\\(p < 0.05\\): Reject \\(H_0\\), meaning FE preferred.\n\\(p \\geq 0.05\\): Fail reject \\(H_0\\), meaning RE can used.\n\\(p < 0.05\\): Reject \\(H_0\\), meaning FE preferred.\\(p < 0.05\\): Reject \\(H_0\\), meaning FE preferred.\\(p \\geq 0.05\\): Fail reject \\(H_0\\), meaning RE can used.\\(p \\geq 0.05\\): Fail reject \\(H_0\\), meaning RE can used.null hypothesis rejected, use FE. , RE appropriate.","code":"\nlibrary(plm)\n\n# Fit FE and RE models\nfe_model <- plm(inv ~ value + capital, data = Grunfeld, model = \"within\")\nre_model <- plm(inv ~ value + capital, data = Grunfeld, model = \"random\")\n\n# Hausman test\nphtest(fe_model, re_model)\n#> \n#>  Hausman Test\n#> \n#> data:  inv ~ value + capital\n#> chisq = 2.3304, df = 2, p-value = 0.3119\n#> alternative hypothesis: one model is inconsistent"},{"path":"data.html","id":"summary-of-model-assumptions-and-consistency","chapter":"11 Data","heading":"11.5.9.3 Summary of Model Assumptions and Consistency","text":"three estimators (POLS, RE, FE) require:A1 LinearityA1 LinearityA2 Full rankA2 Full rankA5 Data generation (random sampling) individualsA5 Data generation (random sampling) individualsHowever, additional assumptions determine whether estimator consistent efficient.POLSConsistent :\nA3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)\nRE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)\nConsistent :A3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)A3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)RE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)RE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)A4 Homoskedasticity hold: Use cluster-robust standard errors, POLS efficient.A4 Homoskedasticity hold: Use cluster-robust standard errors, POLS efficient.REConsistent :\nA3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)\nRE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)\nConsistent :A3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)A3a Exogeneity holds: \\(E(\\mathbf{x}_{}' u_{}) = 0\\)RE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)RE assumption holds: \\(E(\\mathbf{x}_{}' c_{}) = 0\\)A4 Homoskedasticity holds: RE efficient.A4 Homoskedasticity holds: RE efficient.A4 Homoskedasticity hold: Use cluster-robust standard errors. RE remains efficient POLS efficient.A4 Homoskedasticity hold: Use cluster-robust standard errors. RE remains efficient POLS efficient.FEConsistent :\nA3a Exogeneity holds: \\(E((\\mathbf{x}_{} - \\bar{\\mathbf{x}}_{})'(u_{} - \\bar{u}_{})) = 0\\)\nConsistent :A3a Exogeneity holds: \\(E((\\mathbf{x}_{} - \\bar{\\mathbf{x}}_{})'(u_{} - \\bar{u}_{})) = 0\\)Limitations:\nestimate effects time-constant variables.\nA4 Homoskedasticity generally hold, cluster-robust SEs required.\nLimitations:estimate effects time-constant variables.estimate effects time-constant variables.A4 Homoskedasticity generally hold, cluster-robust SEs required.A4 Homoskedasticity generally hold, cluster-robust SEs required.Estimator Selection Guide","code":""},{"path":"data.html","id":"alternative-estimators","chapter":"11 Data","heading":"11.5.10 Alternative Estimators","text":"estimators available depending model violations additional considerations:Violation Estimators: Adjust assumption violations.Violation Estimators: Adjust assumption violations.Basic Estimators: Standard POLS, RE, FE.Basic Estimators: Standard POLS, RE, FE.Instrumental Variable Estimator: Used endogeneity.Instrumental Variable Estimator: Used endogeneity.Variable Coefficient Estimator: Allows varying coefficients.Variable Coefficient Estimator: Allows varying coefficients.Generalized Method Moments Estimator: dynamic panel models.Generalized Method Moments Estimator: dynamic panel models.General Feasible GLS Estimator: Accounts heteroskedasticity serial correlation.General Feasible GLS Estimator: Accounts heteroskedasticity serial correlation.Means Groups Estimator: Averages individual-specific estimates.Means Groups Estimator: Averages individual-specific estimates.Common Correlated Effects Mean Group Estimator: Accounts cross-sectional dependence.Common Correlated Effects Mean Group Estimator: Accounts cross-sectional dependence.Limited Dependent Variable Estimators: Used binary censored data.Limited Dependent Variable Estimators: Used binary censored data.","code":""},{"path":"data.html","id":"application-1","chapter":"11 Data","heading":"11.5.11 Application","text":"","code":""},{"path":"data.html","id":"plm-package","chapter":"11 Data","heading":"11.5.11.1 plm Package","text":"plm package R designed panel data analysis, allowing users estimate various models, including pooled OLS, fixed effects, random effects, specifications commonly used econometrics.detailed guide, refer :official vignette plm functions.official vignette plm functions.model components reference.model components reference.specify panel data, define individual (cross-sectional) time identifiers:plm package allows estimation several different panel data models.Pooled OLS EstimatorA simple pooled OLS model assumes common intercept ignores individual-specific effects.EstimatorThis estimator takes average time entity, reducing within-group variation.First-Differences EstimatorThe first-differences model eliminates time-invariant effects differencing adjacent periods.Fixed Effects (Within) EstimatorControls time-invariant heterogeneity demeaning data within individuals.Random Effects EstimatorAccounts unobserved heterogeneity modeling random component.Model Selection Diagnostic TestsLagrange Multiplier Test Random EffectsThe Breusch-Pagan LM test compares random effects pooled OLS.Null Hypothesis: OLS preferred.Null Hypothesis: OLS preferred.Alternative Hypothesis: Random effects model appropriate.Alternative Hypothesis: Random effects model appropriate.test types: \"honda\", \"kw\", \"ghm\". effects: \"time\", \"twoways\".Cross-Sectional Dependence TestsBreusch-Pagan LM test cross-sectional dependencePesaran’s CD statisticSerial Correlation Test (Panel Version Breusch-Godfrey Test)Used check autocorrelation panel data.Stationarity Test (Augmented Dickey-Fuller Test)Checks whether time series variable stationary.F-Test Fixed Effects vs. Pooled OLSNull Hypothesis: Pooled OLS appropriate.Null Hypothesis: Pooled OLS appropriate.Alternative Hypothesis: Fixed effects model preferred.Alternative Hypothesis: Fixed effects model preferred.Hausman Test Fixed vs. Random EffectsNull Hypothesis: Random effects appropriate.Null Hypothesis: Random effects appropriate.Alternative Hypothesis: Fixed effects preferred (RE assumptions violated).Alternative Hypothesis: Fixed effects preferred (RE assumptions violated).Heteroskedasticity Robust Standard ErrorsBreusch-Pagan Test HeteroskedasticityTests whether heteroskedasticity present panel dataset.Correcting HeteroskedasticityIf heteroskedasticity detected, use robust standard errors:Random Effects ModelHC0: Default heteroskedasticity-consistent (White’s estimator).HC0: Default heteroskedasticity-consistent (White’s estimator).HC1, HC2, HC3: Recommended small samples.HC1, HC2, HC3: Recommended small samples.HC4: Useful small samples influential observations.HC4: Useful small samples influential observations.Fixed Effects ModelSummary Model SelectionVariance Components StructureBeyond standard random effects model, plm package provides additional methods estimating variance components models instrumental variable techniques dealing endogeneity panel data.Different estimators variance components structure exist literature, plm allows users specify random.method argument.Random Effects Estimators:\"swar\" (default): Swamy Arora estimator (Swamy Arora 1972).\"swar\" (default): Swamy Arora estimator (Swamy Arora 1972).\"walhus\": Wallace Hussain estimator (Wallace Hussain 1969).\"walhus\": Wallace Hussain estimator (Wallace Hussain 1969).\"amemiya\": Amemiya estimator (Amemiya 1971).\"amemiya\": Amemiya estimator (Amemiya 1971).\"nerlove\": Nerlove estimator (Nerlove 1971) (Note: available two-way random effects).\"nerlove\": Nerlove estimator (Nerlove 1971) (Note: available two-way random effects).Effects Panel Models:Individual effects (default).Individual effects (default).Time effects (effect = \"time\").Time effects (effect = \"time\").Two-way effects (effect = \"twoways\").Two-way effects (effect = \"twoways\").ercomp() function retrieves estimates variance components random effects model. , extract variance decomposition using Amemiya’s method:output includes:Variance individual effect.Variance individual effect.Variance time effect (applicable).Variance time effect (applicable).Variance idiosyncratic error.Variance idiosyncratic error.Checking Panel Data BalancePanel datasets may balanced (individual observations time periods) unbalanced (individuals missing observations). punbalancedness() function measures degree balance data, values closer 1 indicating balanced panel (Ahrens Pincus 1981).Instrumental Variables Panel DataInstrumental variables (IV) used address endogeneity, arises regressors correlated error term. plm provides various IV estimation methods inst.method argument.Instrumental Variable Estimators\"bvk\": Balestra-Varadharajan-Krishnakumar estimator (default) (Balestra Varadharajan-Krishnakumar 1987).\"bvk\": Balestra-Varadharajan-Krishnakumar estimator (default) (Balestra Varadharajan-Krishnakumar 1987).\"baltagi\": Baltagi estimator (Baltagi 1981).\"baltagi\": Baltagi estimator (Baltagi 1981).\"\": Amemiya-MaCurdy estimator (Amemiya MaCurdy 1986).\"\": Amemiya-MaCurdy estimator (Amemiya MaCurdy 1986).\"bms\": Breusch-Mizon-Schmidt estimator (Breusch, Mizon, Schmidt 1989).\"bms\": Breusch-Mizon-Schmidt estimator (Breusch, Mizon, Schmidt 1989).Estimators Panel Data ModelsBeyond standard fixed effects random effects models, plm package provides additional estimation techniques tailored heterogeneous coefficients, dynamic panel models, feasible generalized least squares (FGLS) methods.Variable Coefficients Model (pvcm)variable coefficients model (VCM) allows coefficients vary across cross-sectional units, accounting unobserved heterogeneity flexibly.Two Estimation Approaches:Fixed effects (within): Assumes coefficients constant time vary across individuals.Fixed effects (within): Assumes coefficients constant time vary across individuals.Random effects (random): Assumes coefficients drawn random distribution.Random effects (random): Assumes coefficients drawn random distribution.Generalized Method Moments Estimator (pgmm)Generalized Method Moments estimator commonly used dynamic panel models, especially :concern endogeneity lagged dependent variables.concern endogeneity lagged dependent variables.Instrumental variables used estimation.Instrumental variables used estimation.Explanation Arguments:log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + lag(log(capital), 0:1)\n→ Specifies dynamic model, log(emp) depends first lag contemporaneous plus lagged values log(wage) log(capital).log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + lag(log(capital), 0:1)\n→ Specifies dynamic model, log(emp) depends first lag contemporaneous plus lagged values log(wage) log(capital).| lag(log(emp), 2:99) + lag(log(wage), 2:99) + lag(log(capital), 2:99)\n→ Instruments endogenous regressors, using lags.| lag(log(emp), 2:99) + lag(log(wage), 2:99) + lag(log(capital), 2:99)\n→ Instruments endogenous regressors, using lags.effect = \"twoways\"\n→ Includes individual time effects.effect = \"twoways\"\n→ Includes individual time effects.model = \"onestep\"\n→ Uses one-step GMM (alternative: \"twostep\" efficiency gain).model = \"onestep\"\n→ Uses one-step GMM (alternative: \"twostep\" efficiency gain).transformation = \"ld\"\n→ Uses lagged differences transformation.transformation = \"ld\"\n→ Uses lagged differences transformation.Generalized Feasible Generalized Least Squares Models (pggls)FGLS estimator (pggls) robust :Intragroup heteroskedasticity.Intragroup heteroskedasticity.Serial correlation (within groups).Serial correlation (within groups).However, assumes cross-sectional correlation suitable NNN (cross-sectional units) much larger TTT (time periods), .e., long panels.Random Effects FGLS Model:Fixed Effects FGLS Model:Key Considerations:Efficient assumption homoskedasticity.Efficient assumption homoskedasticity.Inefficient group-wise heteroskedasticity.Inefficient group-wise heteroskedasticity.Ideal large-N, small-T panels.Ideal large-N, small-T panels.Summary Alternative Panel Data Estimators","code":"\n# Load the package\nlibrary(\"plm\")\n\ndata(\"Produc\", package = \"plm\")\n\n# Display first few rows\nhead(Produc)\n#>     state year region     pcap     hwy   water    util       pc   gsp    emp\n#> 1 ALABAMA 1970      6 15032.67 7325.80 1655.68 6051.20 35793.80 28418 1010.5\n#> 2 ALABAMA 1971      6 15501.94 7525.94 1721.02 6254.98 37299.91 29375 1021.9\n#> 3 ALABAMA 1972      6 15972.41 7765.42 1764.75 6442.23 38670.30 31303 1072.3\n#> 4 ALABAMA 1973      6 16406.26 7907.66 1742.41 6756.19 40084.01 33430 1135.5\n#> 5 ALABAMA 1974      6 16762.67 8025.52 1734.85 7002.29 42057.31 33749 1169.8\n#> 6 ALABAMA 1975      6 17316.26 8158.23 1752.27 7405.76 43971.71 33604 1155.4\n#>   unemp\n#> 1   4.7\n#> 2   5.2\n#> 3   4.7\n#> 4   3.9\n#> 5   5.5\n#> 6   7.7\n# Convert data to panel format\npdata <- pdata.frame(Produc, index = c(\"state\", \"year\"))\n\n# Check structure\nsummary(pdata)\n#>          state          year         region         pcap             hwy       \n#>  ALABAMA    : 17   1970   : 48   5      :136   Min.   :  2627   Min.   : 1827  \n#>  ARIZONA    : 17   1971   : 48   8      :136   1st Qu.:  7097   1st Qu.: 3858  \n#>  ARKANSAS   : 17   1972   : 48   4      :119   Median : 17572   Median : 7556  \n#>  CALIFORNIA : 17   1973   : 48   1      :102   Mean   : 25037   Mean   :10218  \n#>  COLORADO   : 17   1974   : 48   3      : 85   3rd Qu.: 27692   3rd Qu.:11267  \n#>  CONNECTICUT: 17   1975   : 48   6      : 68   Max.   :140217   Max.   :47699  \n#>  (Other)    :714   (Other):528   (Other):170                                   \n#>      water              util               pc              gsp        \n#>  Min.   :  228.5   Min.   :  538.5   Min.   :  4053   Min.   :  4354  \n#>  1st Qu.:  764.5   1st Qu.: 2488.3   1st Qu.: 21651   1st Qu.: 16503  \n#>  Median : 2266.5   Median : 7008.8   Median : 40671   Median : 39987  \n#>  Mean   : 3618.8   Mean   :11199.5   Mean   : 58188   Mean   : 61014  \n#>  3rd Qu.: 4318.7   3rd Qu.:11598.5   3rd Qu.: 64796   3rd Qu.: 68126  \n#>  Max.   :24592.3   Max.   :80728.1   Max.   :375342   Max.   :464550  \n#>                                                                       \n#>       emp              unemp       \n#>  Min.   :  108.3   Min.   : 2.800  \n#>  1st Qu.:  475.0   1st Qu.: 5.000  \n#>  Median : 1164.8   Median : 6.200  \n#>  Mean   : 1747.1   Mean   : 6.602  \n#>  3rd Qu.: 2114.1   3rd Qu.: 7.900  \n#>  Max.   :11258.0   Max.   :18.000  \n#> \npooling <- plm(log(gsp) ~ log(pcap) + log(emp) + unemp, \n               data = pdata, \n               model = \"pooling\")\nsummary(pooling)\n#> Pooling Model\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"pooling\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Residuals:\n#>      Min.   1st Qu.    Median   3rd Qu.      Max. \n#> -0.302260 -0.085204 -0.018166  0.051783  0.500144 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t-value Pr(>|t|)    \n#> (Intercept)  2.2124123  0.0790988 27.9703  < 2e-16 ***\n#> log(pcap)    0.4121307  0.0216314 19.0525  < 2e-16 ***\n#> log(emp)     0.6205834  0.0199495 31.1078  < 2e-16 ***\n#> unemp       -0.0035444  0.0020539 -1.7257  0.08478 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    849.81\n#> Residual Sum of Squares: 13.326\n#> R-Squared:      0.98432\n#> Adj. R-Squared: 0.98426\n#> F-statistic: 16990.2 on 3 and 812 DF, p-value: < 2.22e-16\nbetween <- plm(log(gsp) ~ log(pcap) + log(emp) + unemp, \n               data = pdata, model = \"between\")\nsummary(between)\n#> Oneway (individual) effect Between Model\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"between\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> Observations used in estimation: 48\n#> \n#> Residuals:\n#>      Min.   1st Qu.    Median   3rd Qu.      Max. \n#> -0.172055 -0.086456 -0.013203  0.038100  0.394336 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t-value  Pr(>|t|)    \n#> (Intercept)  2.0784403  0.3277756  6.3410 1.063e-07 ***\n#> log(pcap)    0.4585009  0.0892620  5.1366 6.134e-06 ***\n#> log(emp)     0.5751005  0.0828921  6.9379 1.410e-08 ***\n#> unemp       -0.0031585  0.0145683 -0.2168    0.8294    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    48.875\n#> Residual Sum of Squares: 0.65861\n#> R-Squared:      0.98652\n#> Adj. R-Squared: 0.98561\n#> F-statistic: 1073.73 on 3 and 44 DF, p-value: < 2.22e-16\nfirstdiff <- plm(log(gsp) ~ log(pcap) + log(emp) + unemp, \n                 data = pdata, model = \"fd\")\nsummary(firstdiff)\n#> Oneway (individual) effect First-Difference Model\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"fd\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> Observations used in estimation: 768\n#> \n#> Residuals:\n#>       Min.    1st Qu.     Median    3rd Qu.       Max. \n#> -0.0846921 -0.0108511  0.0016861  0.0124968  0.1018911 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t-value  Pr(>|t|)    \n#> (Intercept)  0.0101353  0.0013206  7.6749 5.058e-14 ***\n#> log(pcap)   -0.0167634  0.0453958 -0.3693     0.712    \n#> log(emp)     0.8212694  0.0362737 22.6409 < 2.2e-16 ***\n#> unemp       -0.0061615  0.0007516 -8.1978 1.032e-15 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    1.0802\n#> Residual Sum of Squares: 0.33394\n#> R-Squared:      0.69086\n#> Adj. R-Squared: 0.68965\n#> F-statistic: 569.123 on 3 and 764 DF, p-value: < 2.22e-16\nfixed <- plm(log(gsp) ~ log(pcap) + log(emp) + unemp, \n             data = pdata, model = \"within\")\nsummary(fixed)\n#> Oneway (individual) effect Within Model\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"within\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Residuals:\n#>       Min.    1st Qu.     Median    3rd Qu.       Max. \n#> -0.1253873 -0.0248746 -0.0054276  0.0184698  0.2026394 \n#> \n#> Coefficients:\n#>              Estimate  Std. Error t-value Pr(>|t|)    \n#> log(pcap)  0.03488447  0.03092191  1.1281   0.2596    \n#> log(emp)   1.03017988  0.02161353 47.6636   <2e-16 ***\n#> unemp     -0.00021084  0.00096121 -0.2194   0.8264    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    18.941\n#> Residual Sum of Squares: 1.3077\n#> R-Squared:      0.93096\n#> Adj. R-Squared: 0.92645\n#> F-statistic: 3438.48 on 3 and 765 DF, p-value: < 2.22e-16\nrandom <- plm(log(gsp) ~ log(pcap) + log(emp) + unemp, \n              data = pdata, model = \"random\")\nsummary(random)\n#> Oneway (individual) effect Random Effect Model \n#>    (Swamy-Arora's transformation)\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"random\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Effects:\n#>                    var  std.dev share\n#> idiosyncratic 0.001709 0.041345 0.103\n#> individual    0.014868 0.121934 0.897\n#> theta: 0.918\n#> \n#> Residuals:\n#>       Min.    1st Qu.     Median    3rd Qu.       Max. \n#> -0.1246674 -0.0268273 -0.0049657  0.0214145  0.2389889 \n#> \n#> Coefficients:\n#>               Estimate Std. Error z-value Pr(>|z|)    \n#> (Intercept) 3.10569727 0.14715985 21.1042   <2e-16 ***\n#> log(pcap)   0.03708054 0.02747015  1.3498   0.1771    \n#> log(emp)    1.00937552 0.02103951 47.9752   <2e-16 ***\n#> unemp       0.00004806 0.00092301  0.0521   0.9585    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    24.523\n#> Residual Sum of Squares: 1.4425\n#> R-Squared:      0.94118\n#> Adj. R-Squared: 0.94096\n#> Chisq: 12992.5 on 3 DF, p-value: < 2.22e-16\nplmtest(pooling, effect = \"individual\", type = \"bp\") \n#> \n#>  Lagrange Multiplier Test - (Breusch-Pagan)\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> chisq = 4567.1, df = 1, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npcdtest(fixed, test = \"lm\")\n#> \n#>  Breusch-Pagan LM test for cross-sectional dependence in panels\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> chisq = 6490.4, df = 1128, p-value < 2.2e-16\n#> alternative hypothesis: cross-sectional dependence\npcdtest(fixed, test = \"cd\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> z = 37.13, p-value < 2.2e-16\n#> alternative hypothesis: cross-sectional dependence\npbgtest(fixed)\n#> \n#>  Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> chisq = 476.92, df = 17, p-value < 2.2e-16\n#> alternative hypothesis: serial correlation in idiosyncratic errors\nlibrary(tseries)\nadf.test(pdata$gsp, k = 2)\n#> \n#>  Augmented Dickey-Fuller Test\n#> \n#> data:  pdata$gsp\n#> Dickey-Fuller = -5.9028, Lag order = 2, p-value = 0.01\n#> alternative hypothesis: stationary\npFtest(fixed, pooling)\n#> \n#>  F test for individual effects\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> F = 149.58, df1 = 47, df2 = 765, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\nphtest(random, fixed)\n#> \n#>  Hausman Test\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> chisq = 84.924, df = 3, p-value < 2.2e-16\n#> alternative hypothesis: one model is inconsistent\nlibrary(lmtest)\nbptest(log(gsp) ~ log(pcap) + log(emp) + unemp,  data = pdata)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  log(gsp) ~ log(pcap) + log(emp) + unemp\n#> BP = 98.223, df = 3, p-value < 2.2e-16\n# Original coefficients\ncoeftest(random)\n#> \n#> t test of coefficients:\n#> \n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 3.10569727 0.14715985 21.1042   <2e-16 ***\n#> log(pcap)   0.03708054 0.02747015  1.3498   0.1774    \n#> log(emp)    1.00937552 0.02103951 47.9752   <2e-16 ***\n#> unemp       0.00004806 0.00092301  0.0521   0.9585    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Heteroskedasticity-consistent standard errors\ncoeftest(random, vcovHC)\n#> \n#> t test of coefficients:\n#> \n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 3.10569727 0.23261788 13.3511   <2e-16 ***\n#> log(pcap)   0.03708054 0.06125725  0.6053   0.5451    \n#> log(emp)    1.00937552 0.06395880 15.7817   <2e-16 ***\n#> unemp       0.00004806 0.00215219  0.0223   0.9822    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Different HC types\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(vcovHC(random, type = x)))\n)) \n#>     (Intercept)  log(pcap)   log(emp)       unemp\n#> HC0   0.2326179 0.06125725 0.06395880 0.002152189\n#> HC1   0.2331901 0.06140795 0.06411614 0.002157484\n#> HC2   0.2334857 0.06161618 0.06439057 0.002160392\n#> HC3   0.2343595 0.06197939 0.06482756 0.002168646\n#> HC4   0.2342815 0.06235576 0.06537813 0.002168867\n# Original coefficients\ncoeftest(fixed)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate  Std. Error t value Pr(>|t|)    \n#> log(pcap)  0.03488447  0.03092191  1.1281   0.2596    \n#> log(emp)   1.03017988  0.02161353 47.6636   <2e-16 ***\n#> unemp     -0.00021084  0.00096121 -0.2194   0.8264    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Heteroskedasticity-consistent standard errors\ncoeftest(fixed, vcovHC)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate  Std. Error t value Pr(>|t|)    \n#> log(pcap)  0.03488447  0.06661083  0.5237   0.6006    \n#> log(emp)   1.03017988  0.06413365 16.0630   <2e-16 ***\n#> unemp     -0.00021084  0.00217453 -0.0970   0.9228    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Arellano method for robust errors\ncoeftest(fixed, vcovHC(fixed, method = \"arellano\"))\n#> \n#> t test of coefficients:\n#> \n#>              Estimate  Std. Error t value Pr(>|t|)    \n#> log(pcap)  0.03488447  0.06661083  0.5237   0.6006    \n#> log(emp)   1.03017988  0.06413365 16.0630   <2e-16 ***\n#> unemp     -0.00021084  0.00217453 -0.0970   0.9228    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Different HC types\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(vcovHC(fixed, type = x)))\n)) \n#>      log(pcap)   log(emp)       unemp\n#> HC0 0.06661083 0.06413365 0.002174525\n#> HC1 0.06673362 0.06425187 0.002178534\n#> HC2 0.06689078 0.06441024 0.002182114\n#> HC3 0.06717278 0.06468886 0.002189747\n#> HC4 0.06742431 0.06496436 0.002193150\namemiya <- plm(\n    log(gsp) ~ log(pcap) + log(emp) + unemp, \n    data = pdata,\n    model = \"random\",\n    random.method = \"amemiya\",\n    effect = \"twoways\"\n)\n\nsummary(amemiya)\n#> Twoways effects Random Effect Model \n#>    (Amemiya's transformation)\n#> \n#> Call:\n#> plm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     effect = \"twoways\", model = \"random\", random.method = \"amemiya\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Effects:\n#>                    var  std.dev share\n#> idiosyncratic 0.001228 0.035039 0.028\n#> individual    0.041201 0.202981 0.941\n#> time          0.001359 0.036859 0.031\n#> theta: 0.9582 (id) 0.8641 (time) 0.8622 (total)\n#> \n#> Residuals:\n#>        Min.     1st Qu.      Median     3rd Qu.        Max. \n#> -0.13796209 -0.01951506 -0.00053384  0.01807398  0.20452581 \n#> \n#> Coefficients:\n#>               Estimate Std. Error z-value  Pr(>|z|)    \n#> (Intercept)  3.9581876  0.1767036 22.4001 < 2.2e-16 ***\n#> log(pcap)    0.0378443  0.0253963  1.4902  0.136184    \n#> log(emp)     0.8891887  0.0227677 39.0548 < 2.2e-16 ***\n#> unemp       -0.0031568  0.0011240 -2.8086  0.004976 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Total Sum of Squares:    5.3265\n#> Residual Sum of Squares: 0.98398\n#> R-Squared:      0.81527\n#> Adj. R-Squared: 0.81458\n#> Chisq: 3583.53 on 3 DF, p-value: < 2.22e-16\nercomp(log(gsp) ~ log(pcap) + log(emp) + unemp, \n       data = pdata,\n       method = \"amemiya\",\n       effect = \"twoways\")\n#>                    var  std.dev share\n#> idiosyncratic 0.001228 0.035039 0.028\n#> individual    0.041201 0.202981 0.941\n#> time          0.001359 0.036859 0.031\n#> theta: 0.9582 (id) 0.8641 (time) 0.8622 (total)\npunbalancedness(random)\n#> gamma    nu \n#>     1     1\nfixed_pvcm  <-\n    pvcm(log(gsp) ~ log(pcap) + log(emp) + unemp,\n         data = pdata,\n         model = \"within\")\nrandom_pvcm <-\n    pvcm(log(gsp) ~ log(pcap) + log(emp) + unemp,\n         data = pdata,\n         model = \"random\")\n\nsummary(fixed_pvcm)\n#> Oneway (individual) effect No-pooling model\n#> \n#> Call:\n#> pvcm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"within\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Residuals:\n#>         Min.      1st Qu.       Median      3rd Qu.         Max. \n#> -0.075247625 -0.013247956  0.000666934  0.013852996  0.118966807 \n#> \n#> Coefficients:\n#>   (Intercept)        log(pcap)           log(emp)          unemp           \n#>  Min.   :-3.8868   Min.   :-1.11962   Min.   :0.3790   Min.   :-1.597e-02  \n#>  1st Qu.: 0.9917   1st Qu.:-0.38475   1st Qu.:0.8197   1st Qu.:-5.319e-03  \n#>  Median : 2.9848   Median :-0.03147   Median :1.1506   Median : 5.335e-05  \n#>  Mean   : 2.8079   Mean   :-0.06028   Mean   :1.1656   Mean   : 9.024e-04  \n#>  3rd Qu.: 4.3553   3rd Qu.: 0.25573   3rd Qu.:1.3779   3rd Qu.: 8.374e-03  \n#>  Max.   :12.8800   Max.   : 1.16922   Max.   :2.4276   Max.   : 2.507e-02  \n#> \n#> Total Sum of Squares: 15729\n#> Residual Sum of Squares: 0.40484\n#> Multiple R-Squared: 0.99997\nsummary(random_pvcm)\n#> Oneway (individual) effect Random coefficients model\n#> \n#> Call:\n#> pvcm(formula = log(gsp) ~ log(pcap) + log(emp) + unemp, data = pdata, \n#>     model = \"random\")\n#> \n#> Balanced Panel: n = 48, T = 17, N = 816\n#> \n#> Residuals:\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.23364 -0.03401  0.05558  0.09811  0.19349  1.14326 \n#> \n#> Estimated mean of the coefficients:\n#>                Estimate  Std. Error z-value  Pr(>|z|)    \n#> (Intercept)  2.79030044  0.53104167  5.2544 1.485e-07 ***\n#> log(pcap)   -0.04195768  0.08621579 -0.4867    0.6265    \n#> log(emp)     1.14988911  0.07225221 15.9149 < 2.2e-16 ***\n#> unemp        0.00031135  0.00163864  0.1900    0.8493    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Estimated variance of the coefficients:\n#>             (Intercept) log(pcap)   log(emp)       unemp\n#> (Intercept)  11.2648882 -1.335932  0.2035824  0.00827707\n#> log(pcap)    -1.3359322  0.287021 -0.1872915 -0.00345298\n#> log(emp)      0.2035824 -0.187291  0.2134845  0.00336374\n#> unemp         0.0082771 -0.003453  0.0033637  0.00009425\n#> \n#> Total Sum of Squares: 15729\n#> Residual Sum of Squares: 40.789\n#> Multiple R-Squared: 0.99741\n#> Chisq: 739.334 on 3 DF, p-value: < 2.22e-16\n#> Test for parameter homogeneity: Chisq = 21768.8 on 188 DF, p-value: < 2.22e-16\nlibrary(plm)\n\n# estimates a dynamic labor demand function using one-step GMM, \n# applying lagged variables as instruments\nz2 <- pgmm(\n    log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) +\n        lag(log(capital), 0:1) | \n        lag(log(emp), 2:99) +\n        lag(log(wage), 2:99) + lag(log(capital), 2:99),\n    data = EmplUK,\n    effect = \"twoways\",\n    model = \"onestep\",\n    transformation = \"ld\"\n)\n\nsummary(z2, robust = TRUE)\n#> Twoways effects One-step model System GMM \n#> \n#> Call:\n#> pgmm(formula = log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + \n#>     lag(log(capital), 0:1) | lag(log(emp), 2:99) + lag(log(wage), \n#>     2:99) + lag(log(capital), 2:99), data = EmplUK, effect = \"twoways\", \n#>     model = \"onestep\", transformation = \"ld\")\n#> \n#> Unbalanced Panel: n = 140, T = 7-9, N = 1031\n#> \n#> Number of Observations Used: 1642\n#> Residuals:\n#>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n#> -0.7530341 -0.0369030  0.0000000  0.0002882  0.0466069  0.6001503 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error z-value  Pr(>|z|)    \n#> lag(log(emp), 1)         0.935605   0.026295 35.5810 < 2.2e-16 ***\n#> lag(log(wage), 0:1)0    -0.630976   0.118054 -5.3448 9.050e-08 ***\n#> lag(log(wage), 0:1)1     0.482620   0.136887  3.5257 0.0004224 ***\n#> lag(log(capital), 0:1)0  0.483930   0.053867  8.9838 < 2.2e-16 ***\n#> lag(log(capital), 0:1)1 -0.424393   0.058479 -7.2572 3.952e-13 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sargan test: chisq(100) = 118.763 (p-value = 0.097096)\n#> Autocorrelation test (1): normal = -4.808434 (p-value = 1.5212e-06)\n#> Autocorrelation test (2): normal = -0.2800133 (p-value = 0.77947)\n#> Wald test for coefficients: chisq(5) = 11174.82 (p-value = < 2.22e-16)\n#> Wald test for time dummies: chisq(7) = 14.71138 (p-value = 0.039882)\nzz <- pggls(\n    log(emp) ~ log(wage) + log(capital),\n    data = EmplUK,\n    model = \"pooling\"\n)\n\nsummary(zz)\n#> Oneway (individual) effect General FGLS model\n#> \n#> Call:\n#> pggls(formula = log(emp) ~ log(wage) + log(capital), data = EmplUK, \n#>     model = \"pooling\")\n#> \n#> Unbalanced Panel: n = 140, T = 7-9, N = 1031\n#> \n#> Residuals:\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -1.80696 -0.36552  0.06181  0.03230  0.44279  1.58719 \n#> \n#> Coefficients:\n#>               Estimate Std. Error z-value  Pr(>|z|)    \n#> (Intercept)   2.023480   0.158468 12.7690 < 2.2e-16 ***\n#> log(wage)    -0.232329   0.048001 -4.8401 1.298e-06 ***\n#> log(capital)  0.610484   0.017434 35.0174 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Total Sum of Squares: 1853.6\n#> Residual Sum of Squares: 402.55\n#> Multiple R-squared: 0.78283\nzz <- pggls(\n    log(emp) ~ log(wage) + log(capital),\n    data = EmplUK,\n    model = \"within\"\n)\n\nsummary(zz)\n#> Oneway (individual) effect Within FGLS model\n#> \n#> Call:\n#> pggls(formula = log(emp) ~ log(wage) + log(capital), data = EmplUK, \n#>     model = \"within\")\n#> \n#> Unbalanced Panel: n = 140, T = 7-9, N = 1031\n#> \n#> Residuals:\n#>         Min.      1st Qu.       Median      3rd Qu.         Max. \n#> -0.508362414 -0.074254395 -0.002442181  0.076139063  0.601442300 \n#> \n#> Coefficients:\n#>               Estimate Std. Error z-value  Pr(>|z|)    \n#> log(wage)    -0.617617   0.030794 -20.056 < 2.2e-16 ***\n#> log(capital)  0.561049   0.017185  32.648 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Total Sum of Squares: 1853.6\n#> Residual Sum of Squares: 17.368\n#> Multiple R-squared: 0.99063"},{"path":"data.html","id":"fixest-package","chapter":"11 Data","heading":"11.5.11.2 fixest Package","text":"fixest package provides efficient flexible methods estimating fixed effects generalized linear models panel data. optimized handling large datasets high-dimensional fixed effects allows multiple model estimation, robust standard errors, split-sample estimation.details, refer official fixest vignette.Available Estimation Functions fixestNote: functions work fixest objects.example demonstrates:Fixed effects estimation (| csw(Month, Day)).Fixed effects estimation (| csw(Month, Day)).Stepwise selection (sw0(Wind + Temp)).Stepwise selection (sw0(Wind + Temp)).Clustering standard errors (cluster = ~ Day).Clustering standard errors (cluster = ~ Day).Extracting plotting fixed effects.Extracting plotting fixed effects.Multiple Model EstimationEstimating Multiple Dependent Variables (LHS)Use feols() estimate models multiple dependent variables simultaneously:Alternatively, define list dependent variables loop :Estimating Multiple Specifications (RHS)Use stepwise functions estimate different model specifications efficiently.Options write functionssw (stepwise): sequentially analyze elements\ny ~ sw(x1, x2) estimated y ~ x1 y ~ x2\nsw (stepwise): sequentially analyze elementsy ~ sw(x1, x2) estimated y ~ x1 y ~ x2sw0 (stepwise 0): similar sw also estimate model without elements set first\ny ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2\nsw0 (stepwise 0): similar sw also estimate model without elements set firsty ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2csw (cumulative stepwise): sequentially add element set formula\ny ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2\ncsw (cumulative stepwise): sequentially add element set formulay ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2csw0 (cumulative stepwise 0): similar csw also estimate model without elements set first\ny ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2\ncsw0 (cumulative stepwise 0): similar csw also estimate model without elements set firsty ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2mvsw (multiverse stepwise): possible combination elements set (get large quick).\nmvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)\nmvsw (multiverse stepwise): possible combination elements set (get large quick).mvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)Split-Sample EstimationEstimate separate regressions different subgroups dataset using fsplit.estimates models separately Month dataset.Robust Standard Errors fixestfixest supports variety robust standard error estimators, including:iid: errors homoskedastic independent identically distributediid: errors homoskedastic independent identically distributedhetero: errors heteroskedastic using White correctionhetero: errors heteroskedastic using White correctioncluster: errors correlated within cluster groupscluster: errors correlated within cluster groupsnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.\nvcov = newey_west ~ id + period id subject id period time period panel.\nspecify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.\nnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.vcov = newey_west ~ id + period id subject id period time period panel.vcov = newey_west ~ id + period id subject id period time period panel.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.driscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.\nvcov = discoll_kraay ~ period\ndriscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.vcov = discoll_kraay ~ periodconley: (Conley 1999) cross-section data. Errors spatially correlated\nvcov = conley ~ latitude + longitude\nspecify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.\nconley: (Conley 1999) cross-section data. Errors spatially correlatedvcov = conley ~ latitude + longitudevcov = conley ~ latitude + longitudeto specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.hc: sandwich package\nvcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))\nhc: sandwich packagevcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))let R know SE estimation want use, insert vcov = vcov_type ~ variablesExample: Newey-West Standard ErrorsSpecify number lag periods consider:Conley Spatial Correlation: vcov = conley ~ latitude + longitudeTo specify distance cutoff: vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\")Using Standard Errors sandwich PackageSmall Sample CorrectionApply small sample adjustments using ssc():corrects bias working small samples.","code":"\nlibrary(fixest)\ndata(airquality)\n\n# Setting a variable dictionary for output labeling\nsetFixest_dict(\n    c(\n        Ozone   = \"Ozone (ppb)\",\n        Solar.R = \"Solar Radiation (Langleys)\",\n        Wind    = \"Wind Speed (mph)\",\n        Temp    = \"Temperature\"\n    )\n)\n\n# Fixed effects OLS with stepwise estimation and clustering\nest <- feols(\n    Ozone ~ Solar.R + sw0(Wind + Temp) | csw(Month, Day),\n    data = airquality,\n    cluster = ~ Day\n)\n\n# Display results\netable(est)\n#>                                         est.1              est.2\n#> Dependent Var.:                   Ozone (ppb)        Ozone (ppb)\n#>                                                                 \n#> Solar Radiation (Langleys) 0.1148*** (0.0234)   0.0522* (0.0202)\n#> Wind Speed (mph)                              -3.109*** (0.7986)\n#> Temperature                                    1.875*** (0.3671)\n#> Fixed-Effects:             ------------------ ------------------\n#> Month                                     Yes                Yes\n#> Day                                        No                 No\n#> __________________________ __________________ __________________\n#> S.E.: Clustered                       by: Day            by: Day\n#> Observations                              111                111\n#> R2                                    0.31974            0.63686\n#> Within R2                             0.12245            0.53154\n#> \n#>                                        est.3              est.4\n#> Dependent Var.:                  Ozone (ppb)        Ozone (ppb)\n#>                                                                \n#> Solar Radiation (Langleys) 0.1078** (0.0329)   0.0509* (0.0236)\n#> Wind Speed (mph)                             -3.289*** (0.7777)\n#> Temperature                                   2.052*** (0.2415)\n#> Fixed-Effects:             ----------------- ------------------\n#> Month                                    Yes                Yes\n#> Day                                      Yes                Yes\n#> __________________________ _________________ __________________\n#> S.E.: Clustered                      by: Day            by: Day\n#> Observations                             111                111\n#> R2                                   0.58018            0.81604\n#> Within R2                            0.12074            0.61471\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Output in LaTeX format\netable(est, tex = TRUE)\n#> \\begingroup\n#> \\centering\n#> \\begin{tabular}{lcccc}\n#>    \\tabularnewline \\midrule \\midrule\n#>    Dependent Variable: & \\multicolumn{4}{c}{Ozone (ppb)}\\\\\n#>    Model:                     & (1)            & (2)            & (3)            & (4)\\\\  \n#>    \\midrule\n#>    \\emph{Variables}\\\\\n#>    Solar Radiation (Langleys) & 0.1148$^{***}$ & 0.0522$^{**}$  & 0.1078$^{***}$ & 0.0509$^{**}$\\\\   \n#>                               & (0.0234)       & (0.0202)       & (0.0329)       & (0.0236)\\\\   \n#>    Wind Speed (mph)           &                & -3.109$^{***}$ &                & -3.289$^{***}$\\\\   \n#>                               &                & (0.7986)       &                & (0.7777)\\\\   \n#>    Temperature                &                & 1.875$^{***}$  &                & 2.052$^{***}$\\\\   \n#>                               &                & (0.3671)       &                & (0.2415)\\\\   \n#>    \\midrule\n#>    \\emph{Fixed-effects}\\\\\n#>    Month                      & Yes            & Yes            & Yes            & Yes\\\\  \n#>    Day                        &                &                & Yes            & Yes\\\\  \n#>    \\midrule\n#>    \\emph{Fit statistics}\\\\\n#>    Observations               & 111            & 111            & 111            & 111\\\\  \n#>    R$^2$                      & 0.31974        & 0.63686        & 0.58018        & 0.81604\\\\  \n#>    Within R$^2$               & 0.12245        & 0.53154        & 0.12074        & 0.61471\\\\  \n#>    \\midrule \\midrule\n#>    \\multicolumn{5}{l}{\\emph{Clustered (Day) standard-errors in parentheses}}\\\\\n#>    \\multicolumn{5}{l}{\\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\\\\n#> \\end{tabular}\n#> \\par\\endgroup\n\n# Extract fixed-effects coefficients\nfixedEffects <- fixef(est[[1]])\nsummary(fixedEffects)\n#> Fixed_effects coefficients\n#> Number of fixed-effects for variable Month is 5.\n#>  Mean = 19.6 Variance = 272\n#> \n#> COEFFICIENTS:\n#>   Month:     5     6     7     8     9\n#>          3.219 8.288 34.26 40.12 12.13\n\n# View fixed effects for one dimension\nfixedEffects$Month\n#>         5         6         7         8         9 \n#>  3.218876  8.287899 34.260812 40.122257 12.130971\n\n# Plot fixed effects\nplot(fixedEffects)\netable(feols(c(Sepal.Length, Sepal.Width) ~\n                 Petal.Length + Petal.Width,\n             data = iris))\n#>                 feols(c(Sepal.L..1 feols(c(Sepal.Le..2\n#> Dependent Var.:       Sepal.Length         Sepal.Width\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> Petal.Length    0.5418*** (0.0693) -0.2571*** (0.0669)\n#> Petal.Width      -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndepvars <- c(\"Sepal.Length\", \"Sepal.Width\")\n\nres <- lapply(depvars, function(var) {\n    feols(xpd(..lhs ~ Petal.Length + Petal.Width, ..lhs = var), data = iris)\n})\n\netable(res)\n#>                            model 1             model 2\n#> Dependent Var.:       Sepal.Length         Sepal.Width\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> Petal.Length    0.5418*** (0.0693) -0.2571*** (0.0669)\n#> Petal.Width      -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Example: Cumulative Stepwise Estimation\netable(feols(Ozone ~ csw(Solar.R, Wind, Temp), data = airquality))\n#>                            feols(Ozone ~ c..1 feols(Ozone ~ c..2\n#> Dependent Var.:                   Ozone (ppb)        Ozone (ppb)\n#>                                                                 \n#> Constant                      18.60** (6.748)   77.25*** (9.068)\n#> Solar Radiation (Langleys) 0.1272*** (0.0328) 0.1004*** (0.0263)\n#> Wind Speed (mph)                              -5.402*** (0.6732)\n#> Temperature                                                     \n#> __________________________ __________________ __________________\n#> S.E. type                                 IID                IID\n#> Observations                              111                111\n#> R2                                    0.12134            0.44949\n#> Adj. R2                               0.11328            0.43930\n#> \n#>                            feols(Ozone ~ c..3\n#> Dependent Var.:                   Ozone (ppb)\n#>                                              \n#> Constant                     -64.34** (23.05)\n#> Solar Radiation (Langleys)   0.0598* (0.0232)\n#> Wind Speed (mph)           -3.334*** (0.6544)\n#> Temperature                 1.652*** (0.2535)\n#> __________________________ __________________\n#> S.E. type                                 IID\n#> Observations                              111\n#> R2                                    0.60589\n#> Adj. R2                               0.59484\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\netable(feols(Ozone ~ Solar.R + Wind, fsplit = ~ Month, data = airquality))\n#>                            feols(Ozone ~ S..1 feols(Ozone ..2 feols(Ozone ~..3\n#> Sample (Month)                    Full sample               5                6\n#> Dependent Var.:                   Ozone (ppb)     Ozone (ppb)      Ozone (ppb)\n#>                                                                               \n#> Constant                     77.25*** (9.068)  50.55* (18.30)    8.997 (16.83)\n#> Solar Radiation (Langleys) 0.1004*** (0.0263) 0.0294 (0.0379) 0.1518. (0.0676)\n#> Wind Speed (mph)           -5.402*** (0.6732) -2.762* (1.300)  -0.6177 (1.674)\n#> __________________________ __________________ _______________ ________________\n#> S.E. type                                 IID             IID              IID\n#> Observations                              111              24                9\n#> R2                                    0.44949         0.22543          0.52593\n#> Adj. R2                               0.43930         0.15166          0.36790\n#> \n#>                            feols(Ozone ~ ..4 feols(Ozone ~ ..5\n#> Sample (Month)                             7                 8\n#> Dependent Var.:                  Ozone (ppb)       Ozone (ppb)\n#>                                                               \n#> Constant                    88.39*** (20.81)  95.76*** (19.83)\n#> Solar Radiation (Langleys)  0.1135. (0.0582) 0.2146** (0.0654)\n#> Wind Speed (mph)           -6.319*** (1.559) -8.228*** (1.528)\n#> __________________________ _________________ _________________\n#> S.E. type                                IID               IID\n#> Observations                              26                23\n#> R2                                   0.52423           0.70640\n#> Adj. R2                              0.48286           0.67704\n#> \n#>                            feols(Ozone ~ ..6\n#> Sample (Month)                             9\n#> Dependent Var.:                  Ozone (ppb)\n#>                                             \n#> Constant                    67.10*** (14.35)\n#> Solar Radiation (Langleys)   0.0373 (0.0463)\n#> Wind Speed (mph)           -4.161*** (1.071)\n#> __________________________ _________________\n#> S.E. type                                IID\n#> Observations                              29\n#> R2                                   0.38792\n#> Adj. R2                              0.34084\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\netable(feols(\n    Ozone ~ Solar.R + Wind,\n    data = airquality,\n    vcov = newey_west ~ Month + Day\n))\n#>                            feols(Ozone ~ So..\n#> Dependent Var.:                   Ozone (ppb)\n#>                                              \n#> Constant                     77.25*** (10.03)\n#> Solar Radiation (Langleys) 0.1004*** (0.0258)\n#> Wind Speed (mph)           -5.402*** (0.8353)\n#> __________________________ __________________\n#> S.E. type                    Newey-West (L=2)\n#> Observations                              111\n#> R2                                    0.44949\n#> Adj. R2                               0.43930\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\netable(feols(\n    Ozone ~ Solar.R + Wind,\n    data = airquality,\n    vcov = newey_west(2) ~ Month + Day\n))\n#>                            feols(Ozone ~ So..\n#> Dependent Var.:                   Ozone (ppb)\n#>                                              \n#> Constant                     77.25*** (10.03)\n#> Solar Radiation (Langleys) 0.1004*** (0.0258)\n#> Wind Speed (mph)           -5.402*** (0.8353)\n#> __________________________ __________________\n#> S.E. type                    Newey-West (L=2)\n#> Observations                              111\n#> R2                                    0.44949\n#> Adj. R2                               0.43930\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\netable(feols(\n    Ozone ~ Solar.R + Wind,\n    data = airquality,\n    vcov = function(x)\n        sandwich::vcovHC(x, type = \"HC1\")\n))\n#>                            feols(Ozone ~ So..\n#> Dependent Var.:                   Ozone (ppb)\n#>                                              \n#> Constant                     77.25*** (9.590)\n#> Solar Radiation (Langleys) 0.1004*** (0.0231)\n#> Wind Speed (mph)           -5.402*** (0.8134)\n#> __________________________ __________________\n#> S.E. type                  vcovHC(type=\"HC1\")\n#> Observations                              111\n#> R2                                    0.44949\n#> Adj. R2                               0.43930\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\netable(feols(\n    Ozone ~ Solar.R + Wind,\n    data = airquality,\n    ssc = ssc(adj = TRUE, cluster.adj = TRUE)\n))\n#>                            feols(Ozone ~ So..\n#> Dependent Var.:                   Ozone (ppb)\n#>                                              \n#> Constant                     77.25*** (9.068)\n#> Solar Radiation (Langleys) 0.1004*** (0.0263)\n#> Wind Speed (mph)           -5.402*** (0.6732)\n#> __________________________ __________________\n#> S.E. type                                 IID\n#> Observations                              111\n#> R2                                    0.44949\n#> Adj. R2                               0.43930\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"choosing-the-right-type-of-data","chapter":"11 Data","heading":"11.6 Choosing the Right Type of Data","text":"Selecting appropriate data type depends :Research Questions: need understand changes time individual level (panel) just snapshot comparison one point (cross-sectional)?Research Questions: need understand changes time individual level (panel) just snapshot comparison one point (cross-sectional)?Resources: Longitudinal panel studies can resource-intensive.Resources: Longitudinal panel studies can resource-intensive.Time Constraints: need fast results, cross-sectional repeated cross-sectional might practical.Time Constraints: need fast results, cross-sectional repeated cross-sectional might practical.Analytical Goals: Time-series forecasting, causal inference, descriptive comparison different data requirements.Analytical Goals: Time-series forecasting, causal inference, descriptive comparison different data requirements.Availability: Sometimes secondary repeated cross-sectional data available, constrains design.Availability: Sometimes secondary repeated cross-sectional data available, constrains design.","code":""},{"path":"data.html","id":"data-quality-and-ethical-considerations","chapter":"11 Data","heading":"11.7 Data Quality and Ethical Considerations","text":"Regardless data type, data quality crucial. Poor data—incomplete, biased, improperly measured—can lead incorrect conclusions. Researchers :Ensure Validity Reliability: Use well-designed instruments consistent measurement techniques.Ensure Validity Reliability: Use well-designed instruments consistent measurement techniques.Address Missing Data: Apply appropriate imputation methods feasible.Address Missing Data: Apply appropriate imputation methods feasible.Manage Attrition (Panel Data): Consider weighting sensitivity analyses deal dropouts.Manage Attrition (Panel Data): Consider weighting sensitivity analyses deal dropouts.Check Representativeness: Especially cross-sectional repeated cross-sectional surveys, ensure sampling frames match target population.Check Representativeness: Especially cross-sectional repeated cross-sectional surveys, ensure sampling frames match target population.Protect Confidentiality Privacy: Particularly panel studies repeated contact, store data securely follow ethical guidelines.Protect Confidentiality Privacy: Particularly panel studies repeated contact, store data securely follow ethical guidelines.Obtain Proper Consent: Inform participants study details, usage data, rights withdraw.Obtain Proper Consent: Inform participants study details, usage data, rights withdraw.","code":""},{"path":"variable-transformation.html","id":"variable-transformation","chapter":"12 Variable Transformation","heading":"12 Variable Transformation","text":"","code":""},{"path":"variable-transformation.html","id":"continuous-variables","chapter":"12 Variable Transformation","heading":"12.1 Continuous Variables","text":"Transforming continuous variables can useful various reasons, including:Changing scale variables make interpretable comparable.Reducing skewness approximate normal distribution, can improve statistical inference.Stabilizing variance cases heteroskedasticity.Enhancing interpretability business applications (e.g., logarithmic transformations financial data).","code":""},{"path":"variable-transformation.html","id":"standardization-z-score-normalization","chapter":"12 Variable Transformation","heading":"12.1.1 Standardization (Z-score Normalization)","text":"common transformation center scale data:\\[\nx_i' = \\frac{x_i - \\bar{x}}{s}\n\\]:\\(x_i\\) original value,\\(\\bar{x}\\) sample mean,\\(s\\) sample standard deviation.Use:variables different units measurement need common scale.variables different units measurement need common scale.large numbers dominate dataset.large numbers dominate dataset.","code":""},{"path":"variable-transformation.html","id":"min-max-scaling-normalization","chapter":"12 Variable Transformation","heading":"12.1.2 Min-Max Scaling (Normalization)","text":"Rescales data fixed range, typically \\([0,1]\\):\\[\nx_i' = \\frac{x_i - x_{\\min}}{x_{\\max} - x_{\\min}}\n\\]Use:working fixed-interval data (e.g., percentages, proportions).working fixed-interval data (e.g., percentages, proportions).preserving relative relationships values important.preserving relative relationships values important.Caution: method sensitive outliers, extreme values determine range.Caution: method sensitive outliers, extreme values determine range.","code":""},{"path":"variable-transformation.html","id":"square-root-and-cube-root-transformations","chapter":"12 Variable Transformation","heading":"12.1.3 Square Root and Cube Root Transformations","text":"Useful handling positive skewness heteroskedasticity:Square root: Reduces moderate skewness variance.Cube root: Works extreme skewness allows negative values.Common Use Cases:Frequency count data (e.g., website visits, sales transactions).Frequency count data (e.g., website visits, sales transactions).Data many small values zeros (e.g., income distributions microfinance).Data many small values zeros (e.g., income distributions microfinance).","code":""},{"path":"variable-transformation.html","id":"sec-logarithmic-transformation","chapter":"12 Variable Transformation","heading":"12.1.4 Logarithmic Transformation","text":"Logarithmic transformations particularly useful handling highly skewed data. compress large values expanding small values, helps heteroskedasticity normality assumptions.","code":""},{"path":"variable-transformation.html","id":"common-log-transformations","chapter":"12 Variable Transformation","heading":"12.1.4.1 Common Log Transformations","text":"Selecting constant \\(c\\) critical:\\(c\\) large, can obscure true nature data.\\(c\\) small, transformation might effectively reduce skewness.statistical modeling perspective:inference-based models, choice \\(c\\) can significantly impact fit. See (Ekwaru Veugelers 2018).causal inference (e.g., , IV), improper log transformations (e.g., logging zero values) can introduce bias (J. Chen Roth 2024).","code":""},{"path":"variable-transformation.html","id":"when-is-log-transformation-problematic","chapter":"12 Variable Transformation","heading":"12.1.4.2 When is Log Transformation Problematic?","text":"zero values meaningful interpretation (e.g., income unemployed individuals).data censored (e.g., income data truncated reporting thresholds).measurement error exists (e.g., rounding errors survey responses).zeros small meaningful (e.g., revenue startups), using \\(\\log(x + c)\\) may acceptable.","code":"\nlibrary(tidyverse)\n\n# Load dataset\ncars = datasets::cars\n\n# Original values\nhead(cars$speed)\n#> [1] 4 4 7 7 8 9\n\n# Log transformation (basic)\nlog(cars$speed) %>% head()\n#> [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225\n\n# Log transformation for zero-inflated data\nlog1p(cars$speed) %>% head()\n#> [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585"},{"path":"variable-transformation.html","id":"exponential-transformation","chapter":"12 Variable Transformation","heading":"12.1.5 Exponential Transformation","text":"exponential transformation useful data exhibit negative skewness underlying logarithmic trend suspected, survival analysis decay models.Use:Negatively skewed distributions.Negatively skewed distributions.Processes follow exponential trend (e.g., population growth, depreciation assets).Processes follow exponential trend (e.g., population growth, depreciation assets).","code":""},{"path":"variable-transformation.html","id":"power-transformation","chapter":"12 Variable Transformation","heading":"12.1.6 Power Transformation","text":"Power transformations help adjust skewness, particularly negatively skewed data.Use:variables negatively skewed distribution.variables negatively skewed distribution.relationship variables non-linear.relationship variables non-linear.Common power transformations include:Square transformation: \\(x^2\\) (moderate adjustment).Square transformation: \\(x^2\\) (moderate adjustment).Cubic transformation: \\(x^3\\) (stronger adjustment).Cubic transformation: \\(x^3\\) (stronger adjustment).Fourth-root transformation: \\(x^{1/4}\\) (subtle square root).Fourth-root transformation: \\(x^{1/4}\\) (subtle square root).","code":""},{"path":"variable-transformation.html","id":"inverse-reciprocal-transformation","chapter":"12 Variable Transformation","heading":"12.1.7 Inverse (Reciprocal) Transformation","text":"inverse transformation useful handling platykurtic (flat) distributions positively skewed data.Formula:\\[\nx_i' = \\frac{1}{x_i}\n\\]Use:Reducing extreme values positively skewed distributions.Reducing extreme values positively skewed distributions.Ratio data (e.g., speed = distance/time).Ratio data (e.g., speed = distance/time).variable natural lower bound (e.g., time completion).variable natural lower bound (e.g., time completion).","code":"\n# data(cars)\ncars = datasets::cars\n\n# Original distribution\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\nplot(cars$dist)\n\n# Reciprocal transformation\nplot(1 / cars$dist)"},{"path":"variable-transformation.html","id":"hyperbolic-arcsine-transformation","chapter":"12 Variable Transformation","heading":"12.1.8 Hyperbolic Arcsine Transformation","text":"arcsinh (inverse hyperbolic sine) transformation useful handling proportion variables (0-1) skewed distributions. behaves similarly logarithmic transformation advantage handling zero negative values.Formula:\\[\n\\text{arcsinh}(Y) = \\log(\\sqrt{1 + Y^2} + Y)\n\\]Use:Proportion variables (e.g., market share, probability estimates).Proportion variables (e.g., market share, probability estimates).Data extreme skewness log transformation problematic.Data extreme skewness log transformation problematic.Variables containing zeros negative values (unlike log, arcsinh handles zeros naturally).Variables containing zeros negative values (unlike log, arcsinh handles zeros naturally).Alternative log transformation handling zeros.Alternative log transformation handling zeros.Consider simple regression model: \\[ Y = \\beta X + \\epsilon \\] \\(Y\\) \\(X\\) transformed:coefficient estimate \\(\\beta\\) represents elasticity: 1% increase \\(X\\) leads \\(\\beta\\)% change \\(Y\\).\\(Y\\) transformed:coefficient estimate represents percentage change \\(Y\\) one-unit change \\(X\\).makes arcsinh transformation particularly valuable log-linear models zero values exist.","code":"\n# Visualize original distribution \ncars$dist %>% hist() \n# Alternative histogram  \ncars$dist %>% MASS::truehist()  \n\n# Apply arcsinh transformation \nas_dist <- bestNormalize::arcsinh_x(cars$dist) \nas_dist\n#> Standardized asinh(x) Transformation with 50 nonmissing obs.:\n#>  Relevant statistics:\n#>  - mean (before standardization) = 4.230843 \n#>  - sd (before standardization) = 0.7710887\nas_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"ordered-quantile-normalization-rank-based-transformation","chapter":"12 Variable Transformation","heading":"12.1.9 Ordered Quantile Normalization (Rank-Based Transformation)","text":"Ordered Quantile Normalization (OQN) technique transforms data normal distribution using rank-based methods (Bartlett 1947).Formula:\\[\nx_i' = \\Phi^{-1} \\left( \\frac{\\text{rank}(x_i) - 1/2}{\\text{length}(x)} \\right)\n\\]\\(\\Phi^{-1}\\) inverse normal cumulative distribution function.Use:data heavily skewed contain extreme values.data heavily skewed contain extreme values.normality required parametric tests.normality required parametric tests.","code":"\nord_dist <- bestNormalize::orderNorm(cars$dist)\nord_dist\n#> orderNorm Transformation with 50 nonmissing obs and ties\n#>  - 35 unique values \n#>  - Original quantiles:\n#>   0%  25%  50%  75% 100% \n#>    2   26   36   56  120\nord_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"lambert-w-x-f-transformation","chapter":"12 Variable Transformation","heading":"12.1.10 Lambert W x F Transformation","text":"Lambert W transformation advanced method normalizes data removing skewness heavy tails.Use:traditional transformations (e.g., log, Box-Cox) fail.traditional transformations (e.g., log, Box-Cox) fail.dealing heavy-tailed distributions.dealing heavy-tailed distributions.","code":"\ncars = datasets::cars\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\ncars$dist %>% hist()\n\n# Apply Lambert W transformation\nl_dist <- LambertW::Gaussianize(cars$dist)\nl_dist %>% hist()"},{"path":"variable-transformation.html","id":"inverse-hyperbolic-sine-transformation","chapter":"12 Variable Transformation","heading":"12.1.11 Inverse Hyperbolic Sine Transformation","text":"Inverse Hyperbolic Sine (IHS) transformation similar log transformation handles zero negative values (N. L. Johnson 1949).Formula:\\[\nf(x,\\theta) = \\frac{\\sinh^{-1} (\\theta x)}{\\theta} = \\frac{\\log(\\theta x + (\\theta^2 x^2 + 1)^{1/2})}{\\theta}\n\\]Use:data contain zeros negative values.data contain zeros negative values.Alternative log transformation economic financial modeling.Alternative log transformation economic financial modeling.","code":""},{"path":"variable-transformation.html","id":"sec-box-cox-transformation","chapter":"12 Variable Transformation","heading":"12.1.12 Box-Cox Transformation","text":"Box-Cox transformation power transformation designed improve linearity normality (Manly 1976; Bickel Doksum 1981; Box Cox 1981).Formula:\\[\nx_i'^\\lambda = \\begin{cases} \\frac{x_i^\\lambda-1}{\\lambda} & \\text{} \\lambda \\neq 0\\\\ \\log(x_i) & \\text{} \\lambda = 0 \\end{cases}\n\\]Use:fix non-linearity error terms regression models.fix non-linearity error terms regression models.data strictly positiveWhen data strictly positiveFor two-parameter Box-Cox transformation, use:\\[\nx_i' (\\lambda_1, \\lambda_2) = \\begin{cases} \\frac{(x_i + \\lambda_2)^{\\lambda_1}-1}{\\lambda_1} & \\text{} \\lambda_1 \\neq 0 \\\\ \\log(x_i + \\lambda_2) & \\text{} \\lambda_1 = 0 \\end{cases}\n\\]","code":"\nlibrary(MASS)\n# data(cars)\ncars = datasets::cars\nmod <- lm(cars$speed ~ cars$dist, data = cars)\n\n# Check residuals\nplot(mod)\n\n# Find optimal lambda\nbc <- boxcox(mod, lambda = seq(-3, 3))\nbest_lambda <- bc$x[which.max(bc$y)]\n\n# Apply transformation\nmod_lambda = lm(cars$speed ^ best_lambda ~ cars$dist, data = cars)\nplot(mod_lambda)\n# Two-parameter Box-Cox transformation\ntwo_bc <- geoR::boxcoxfit(cars$speed)\ntwo_bc\n#> Fitted parameters:\n#>    lambda      beta   sigmasq \n#>  1.028798 15.253008 31.935297 \n#> \n#> Convergence code returned by optim: 0\nplot(two_bc)"},{"path":"variable-transformation.html","id":"yeo-johnson-transformation","chapter":"12 Variable Transformation","heading":"12.1.13 Yeo-Johnson Transformation","text":"Similar Box-Cox (\\(\\lambda = 1\\)), allows negative values.Formula:\\[\nx_i'^\\lambda = \\begin{cases} \\frac{(x_i+1)^\\lambda -1}{\\lambda} & \\text{} \\lambda \\neq0, x_i \\ge 0 \\\\ \\log(x_i + 1) & \\text{} \\lambda = 0, x_i \\ge 0 \\\\ \\frac{-[(-x_i+1)^{2-\\lambda}-1]}{2 - \\lambda} & \\text{} \\lambda \\neq 2, x_i <0 \\\\ -\\log(-x_i + 1) & \\text{} \\lambda = 2, x_i <0 \\end{cases}\n\\]","code":"\n# data(cars)\ncars = datasets::cars\nyj_speed <- bestNormalize::yeojohnson(cars$speed)\nyj_speed$x.t %>% hist()"},{"path":"variable-transformation.html","id":"rankgauss-transformation","chapter":"12 Variable Transformation","heading":"12.1.14 RankGauss Transformation","text":"rank-based transformation maps values normal distribution.Use:handle skewed data preserving rank order.","code":""},{"path":"variable-transformation.html","id":"automatically-choosing-the-best-transformation","chapter":"12 Variable Transformation","heading":"12.1.15 Automatically Choosing the Best Transformation","text":"bestNormalize package selects best transformation given dataset.","code":"\nbestdist <- bestNormalize::bestNormalize(cars$dist)\nbestdist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"categorical-variables","chapter":"12 Variable Transformation","heading":"12.2 Categorical Variables","text":"Transforming categorical variables numerical representations essential machine learning models statistical analysis. key objectives include:Converting categorical data format suitable numerical models.Improving model interpretability performance.Handling high-cardinality categorical variables efficiently.multiple ways transform categorical variables, advantages use cases. choice depends factors like cardinality, ordinality, model type.","code":""},{"path":"variable-transformation.html","id":"one-hot-encoding-dummy-variables","chapter":"12 Variable Transformation","heading":"12.2.1 One-Hot Encoding (Dummy Variables)","text":"Creates binary indicator variables category.Formula:\ncategorical variable \\(k\\) unique values, create \\(k\\) binary columns:\\[\nx_i' =\n\\begin{cases}\n1 & \\text{} x_i = \\text{category} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Use:Low-cardinality categorical variables (e.g., “Red”, “Blue”, “Green”).Low-cardinality categorical variables (e.g., “Red”, “Blue”, “Green”).Tree-based models (e.g., Random Forest, XGBoost).Tree-based models (e.g., Random Forest, XGBoost).Linear regression models (dummy variables prevent information loss).Linear regression models (dummy variables prevent information loss).","code":"\nlibrary(caret)\n\ndata(iris)\ndummy_vars <- dummyVars(~ Species, data = iris)\none_hot_encoded <- predict(dummy_vars, newdata = iris)\nhead(one_hot_encoded)\n#>   Species.setosa Species.versicolor Species.virginica\n#> 1              1                  0                 0\n#> 2              1                  0                 0\n#> 3              1                  0                 0\n#> 4              1                  0                 0\n#> 5              1                  0                 0\n#> 6              1                  0                 0"},{"path":"variable-transformation.html","id":"label-encoding","chapter":"12 Variable Transformation","heading":"12.2.2 Label Encoding","text":"Assigns integer values categories.Formula:\ncategorical variable \\(k\\) unique values:\\[\n\\text{Category } \\rightarrow \\text{Integer}\n\\]Example:Use:Ordinal categorical variables (e.g., “Low”, “Medium”, “High”).Ordinal categorical variables (e.g., “Low”, “Medium”, “High”).Neural networks (use embeddings instead one-hot).Neural networks (use embeddings instead one-hot).Memory-efficient encoding high-cardinality features.Memory-efficient encoding high-cardinality features.","code":"\niris$Species_encoded <- as.numeric(factor(iris$Species))\nhead(iris$Species_encoded)\n#> [1] 1 1 1 1 1 1"},{"path":"variable-transformation.html","id":"feature-hashing-hash-encoding","chapter":"12 Variable Transformation","heading":"12.2.3 Feature Hashing (Hash Encoding)","text":"Maps categories fixed number hash bins, reducing memory usage.Use:High-cardinality categorical variables (e.g., user IDs, URLs).High-cardinality categorical variables (e.g., user IDs, URLs).Scenarios exact category match isn’t needed.Scenarios exact category match isn’t needed.Sparse models (e.g., text data NLP).Sparse models (e.g., text data NLP).word_tokenizer: function splits character vector tokens. Since iris$Species already categorical variable values like \"setosa\", \"versicolor\", \"virginica\", value becomes token.word_tokenizer: function splits character vector tokens. Since iris$Species already categorical variable values like \"setosa\", \"versicolor\", \"virginica\", value becomes token.itoken: Creates iterator tokens.itoken: Creates iterator tokens.hash_vectorizer: Sets hashing vectorizer transforms tokens sparse feature space size 2^3 = 8 (hash_size = 8 means \\(2^8\\) bins; intend exactly 8 bins, might adjust parameter accordingly).hash_vectorizer: Sets hashing vectorizer transforms tokens sparse feature space size 2^3 = 8 (hash_size = 8 means \\(2^8\\) bins; intend exactly 8 bins, might adjust parameter accordingly).create_dtm: Builds document-term matrix (case analogous feature matrix observation).create_dtm: Builds document-term matrix (case analogous feature matrix observation).","code":"\nlibrary(text2vec)\nlibrary(Matrix)\n\ndata(iris)\n\n# Convert the 'Species' factor to character tokens\ntokens <- word_tokenizer(as.character(iris$Species))\n\n# Create an iterator over tokens\nit <- itoken(tokens, progressbar = FALSE)\n\n# Define the hash_vectorizer with a specified hash size (8 in this case)\nvectorizer <- hash_vectorizer(hash_size = 8)\n\n# Create a Document-Term Matrix (DTM) using the hashed features\nhashed_dtm <- create_dtm(it, vectorizer)\n\n# Inspect the first few rows of the hashed feature matrix\nhead(hashed_dtm)\n#> 6 x 8 sparse Matrix of class \"dgCMatrix\"\n#>                  \n#> 1 . . . . . . 1 .\n#> 2 . . . . . . 1 .\n#> 3 . . . . . . 1 .\n#> 4 . . . . . . 1 .\n#> 5 . . . . . . 1 .\n#> 6 . . . . . . 1 ."},{"path":"variable-transformation.html","id":"binary-encoding","chapter":"12 Variable Transformation","heading":"12.2.4 Binary Encoding","text":"Converts categories binary representations distributes across multiple columns.Example:\nfour categories (“”, “B”, “C”, “D”):Use:High-cardinality categorical features (less memory one-hot encoding).High-cardinality categorical features (less memory one-hot encoding).Tree-based models (preserves ordinal information).Tree-based models (preserves ordinal information).","code":"\nlibrary(mltools)\nlibrary(data.table)\n\n# Convert the Species column to a data.table and perform one-hot encoding\nbinary_encoded <- one_hot(as.data.table(iris[, \"Species\"]))\nhead(binary_encoded)\n#>    V1_setosa V1_versicolor V1_virginica\n#>        <int>         <int>        <int>\n#> 1:         1             0            0\n#> 2:         1             0            0\n#> 3:         1             0            0\n#> 4:         1             0            0\n#> 5:         1             0            0\n#> 6:         1             0            0"},{"path":"variable-transformation.html","id":"base-n-encoding-generalized-binary-encoding","chapter":"12 Variable Transformation","heading":"12.2.5 Base-N Encoding (Generalized Binary Encoding)","text":"Expands Binary Encoding base \\(N\\) instead binary.Use:Similar Binary Encoding, allows greater flexibility.","code":""},{"path":"variable-transformation.html","id":"frequency-encoding","chapter":"12 Variable Transformation","heading":"12.2.6 Frequency Encoding","text":"Replaces category frequency (proportion) dataset.Formula: \\[\nx_i' = \\frac{\\text{count}(x_i)}{\\text{total count}}\n\\] Use:High-cardinality categorical variables.High-cardinality categorical variables.Feature engineering boosting algorithms (e.g., LightGBM).Feature engineering boosting algorithms (e.g., LightGBM).","code":"\nfreq_encoding <- table(iris$Species) / length(iris$Species)\niris$Species_freq <-\n    iris$Species %>% as.character() %>% map_dbl(~ freq_encoding[.])\nhead(iris$Species_freq)\n#> [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333"},{"path":"variable-transformation.html","id":"target-encoding-mean-encoding","chapter":"12 Variable Transformation","heading":"12.2.7 Target Encoding (Mean Encoding)","text":"Encodes categories using mean target variable.Formula: \\[\nx_i' = E[Y | X = x_i]\n\\] Use:Predictive models categorical features strongly correlated target.Predictive models categorical features strongly correlated target.High-cardinality categorical variables.High-cardinality categorical variables.Risk: Can lead data leakage (use cross-validation).Risk: Can lead data leakage (use cross-validation).","code":"\nlibrary(data.table)\niris_dt <- as.data.table(iris)\niris_dt[, Species_mean := mean(Sepal.Length), by = Species]\nhead(iris_dt$Species_mean)\n#> [1] 5.006 5.006 5.006 5.006 5.006 5.006"},{"path":"variable-transformation.html","id":"ordinal-encoding","chapter":"12 Variable Transformation","heading":"12.2.8 Ordinal Encoding","text":"Maps categories ordered integer values based logical ranking.Example:Use:Ordinal variables meaningful order (e.g., satisfaction ratings).","code":"\niris$Species_ordinal <-\n    as.numeric(factor(iris$Species, \n                      levels = c(\"setosa\", \"versicolor\", \"virginica\")))\nhead(iris$Species_ordinal)\n#> [1] 1 1 1 1 1 1"},{"path":"variable-transformation.html","id":"weight-of-evidence-encoding","chapter":"12 Variable Transformation","heading":"12.2.9 Weight of Evidence Encoding","text":"Concept:\nWoE method convert categorical data numerical values capture strength relationship feature (category) binary outcome (like default vs. non-default).Concept:\nWoE method convert categorical data numerical values capture strength relationship feature (category) binary outcome (like default vs. non-default).Formula: \\[\n\\text{WoE} = \\log \\left( \\frac{P(X_i | Y=1)}{P(X_i | Y=0)} \\right)\n\\]\n\\((X_i | Y=1)\\): probability (proportion) observing category \\(X_i\\) given positive outcome (e.g., “good” credit event).\n\\(P(X_i | Y=0)\\): probability observing category \\(X_i\\) given negative outcome (e.g., “bad” credit event).\nLogarithm: Taking log ratio gives us symmetric scale :\npositive WoE indicates category associated positive outcome.\nnegative WoE indicates category associated negative outcome.\n\nFormula: \\[\n\\text{WoE} = \\log \\left( \\frac{P(X_i | Y=1)}{P(X_i | Y=0)} \\right)\n\\]\\((X_i | Y=1)\\): probability (proportion) observing category \\(X_i\\) given positive outcome (e.g., “good” credit event).\\((X_i | Y=1)\\): probability (proportion) observing category \\(X_i\\) given positive outcome (e.g., “good” credit event).\\(P(X_i | Y=0)\\): probability observing category \\(X_i\\) given negative outcome (e.g., “bad” credit event).\\(P(X_i | Y=0)\\): probability observing category \\(X_i\\) given negative outcome (e.g., “bad” credit event).Logarithm: Taking log ratio gives us symmetric scale :\npositive WoE indicates category associated positive outcome.\nnegative WoE indicates category associated negative outcome.\nLogarithm: Taking log ratio gives us symmetric scale :positive WoE indicates category associated positive outcome.positive WoE indicates category associated positive outcome.negative WoE indicates category associated negative outcome.negative WoE indicates category associated negative outcome.Use WoE Encoding?Logistic Regression Credit Scoring:\nLogistic regression models predict probabilities terms log-odds. WoE encoding aligns well essentially expresses odds positive outcome change different categories. ’s popular credit scoring models.Logistic Regression Credit Scoring:\nLogistic regression models predict probabilities terms log-odds. WoE encoding aligns well essentially expresses odds positive outcome change different categories. ’s popular credit scoring models.Interpretability:\nWoE transformation makes easier understand interpret relationship category variable outcome. category’s WoE value tells whether increases decreases odds particular event (e.g., default).Interpretability:\nWoE transformation makes easier understand interpret relationship category variable outcome. category’s WoE value tells whether increases decreases odds particular event (e.g., default).Imagine feature “Employment Status” categories “Employed” “Unemployed”:Calculate Proportions:\n\\(P(\\text{Employed} | Y=1) = 0.8\\) (80% good credit cases employed)\n\\(P(\\text{Employed} | Y=0) = 0.4\\) (40% bad credit cases employed)\nCalculate Proportions:\\(P(\\text{Employed} | Y=1) = 0.8\\) (80% good credit cases employed)\\(P(\\text{Employed} | Y=1) = 0.8\\) (80% good credit cases employed)\\(P(\\text{Employed} | Y=0) = 0.4\\) (40% bad credit cases employed)\\(P(\\text{Employed} | Y=0) = 0.4\\) (40% bad credit cases employed)Compute WoE “Employed”: \\[\n\\text{WoE}_{\\text{Employed}} = \\log \\left( \\frac{0.8}{0.4} \\right) = \\log(2) \\approx 0.693\n\\] positive value indicates employed increases odds good credit outcome.Compute WoE “Employed”: \\[\n\\text{WoE}_{\\text{Employed}} = \\log \\left( \\frac{0.8}{0.4} \\right) = \\log(2) \\approx 0.693\n\\] positive value indicates employed increases odds good credit outcome.Repeat “Unemployed”:\nSuppose:\n\\(P(\\text{Unemployed} | Y=1) = 0.2\\)\n\\(P(\\text{Unemployed} | Y=0) = 0.6\\) \\[\n\\text{WoE}_{\\text{Unemployed}} = \\log \\left( \\frac{0.2}{0.6} \\right) = \\log\\left(\\frac{1}{3}\\right) \\approx -1.099\n\\] negative value indicates unemployed associated higher likelihood bad credit outcome.\nRepeat “Unemployed”:\nSuppose:\\(P(\\text{Unemployed} | Y=1) = 0.2\\)\\(P(\\text{Unemployed} | Y=1) = 0.2\\)\\(P(\\text{Unemployed} | Y=0) = 0.6\\) \\[\n\\text{WoE}_{\\text{Unemployed}} = \\log \\left( \\frac{0.2}{0.6} \\right) = \\log\\left(\\frac{1}{3}\\right) \\approx -1.099\n\\] negative value indicates unemployed associated higher likelihood bad credit outcome.\\(P(\\text{Unemployed} | Y=0) = 0.6\\) \\[\n\\text{WoE}_{\\text{Unemployed}} = \\log \\left( \\frac{0.2}{0.6} \\right) = \\log\\left(\\frac{1}{3}\\right) \\approx -1.099\n\\] negative value indicates unemployed associated higher likelihood bad credit outcome.WoE Valuable?Linear Relationship:\nplug WoE values logistic regression, model essentially adds values linearly, fits nicely logistic regression models log-odds.Linear Relationship:\nplug WoE values logistic regression, model essentially adds values linearly, fits nicely logistic regression models log-odds.Stability & Handling Missing Values:\nWoE can also help smoothing fluctuations categorical data, especially many levels levels observations.Stability & Handling Missing Values:\nWoE can also help smoothing fluctuations categorical data, especially many levels levels observations.Regulatory Acceptance:\nindustries like banking, WoE widely accepted clear interpretability, crucial compliance transparency credit risk modeling.Regulatory Acceptance:\nindustries like banking, WoE widely accepted clear interpretability, crucial compliance transparency credit risk modeling.fit logistic regression using WoE-encoded variable, model essentially: \\[\n\\log\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{WoE}\n\\] , WoE represents Weight Evidence value given category.Log Odds Change:\\(\\beta_1\\) indicates much log odds good credit outcome change one-unit increase WoE. example, \\(\\beta_1 = 0.5\\), one-unit increase WoE associated increase 0.5 log odds good credit outcome.Log Odds Change:\\(\\beta_1\\) indicates much log odds good credit outcome change one-unit increase WoE. example, \\(\\beta_1 = 0.5\\), one-unit increase WoE associated increase 0.5 log odds good credit outcome.Odds Ratio:\nexponentiate \\(\\beta_1\\), get odds ratio. instance, \\(\\beta_1 = 0.5\\), \\(\\exp(0.5) \\approx 1.65\\). means one-unit increase WoE, odds good credit outcome multiplied 1.65.Odds Ratio:\nexponentiate \\(\\beta_1\\), get odds ratio. instance, \\(\\beta_1 = 0.5\\), \\(\\exp(0.5) \\approx 1.65\\). means one-unit increase WoE, odds good credit outcome multiplied 1.65.Meaningful?Direct Link Data:\nWoE value transformation original categorical variable reflects ratio proportions good bad outcomes category. using WoE, ’re directly incorporating information model.Direct Link Data:\nWoE value transformation original categorical variable reflects ratio proportions good bad outcomes category. using WoE, ’re directly incorporating information model.Interpretability:\ninterpretation becomes intuitive:\npositive WoE indicates category associated good outcome.\nnegative WoE indicates category associated bad outcome.\nThus, \\(\\beta_1\\) positive, suggests category moves one higher WoE (.e., favorable good outcome), likelihood good outcome increases.Interpretability:\ninterpretation becomes intuitive:positive WoE indicates category associated good outcome.positive WoE indicates category associated good outcome.negative WoE indicates category associated bad outcome.negative WoE indicates category associated bad outcome.Thus, \\(\\beta_1\\) positive, suggests category moves one higher WoE (.e., favorable good outcome), likelihood good outcome increases.","code":"\n# Load required packages\nlibrary(dplyr)\nlibrary(knitr)\n\n# Create a sample dataset\n# We assume 100 good credit cases and 100 bad credit cases\n# Good credit: 80 \"Employed\" and 20 \"Unemployed\"\n# Bad credit: 40 \"Employed\" and 60 \"Unemployed\"\ndata <- data.frame(\n  employment_status = c(rep(\"Employed\", 80), rep(\"Unemployed\", 20), \n                        rep(\"Employed\", 40), rep(\"Unemployed\", 60)),\n  credit = c(rep(1, 100), rep(0, 100))\n)\n\n# Calculate counts for each category\nwoe_table <- data %>%\n  group_by(employment_status) %>%\n  summarise(\n    good = sum(credit == 1),\n    bad = sum(credit == 0)\n  ) %>%\n  # Calculate the distribution for good and bad credit cases\n  mutate(\n    dist_good = good / sum(good),\n    dist_bad = bad / sum(bad),\n    WoE = log(dist_good / dist_bad)\n  )\n\n# Print the WoE table\nkable(woe_table)\n\n# Merge the WoE values into the original data\ndata_woe <- data %>%\n  left_join(woe_table %>% dplyr::select(employment_status, WoE), by = \"employment_status\")\n\nhead(data_woe)\n#>   employment_status credit       WoE\n#> 1          Employed      1 0.6931472\n#> 2          Employed      1 0.6931472\n#> 3          Employed      1 0.6931472\n#> 4          Employed      1 0.6931472\n#> 5          Employed      1 0.6931472\n#> 6          Employed      1 0.6931472\n\n\n# Fit a logistic regression model using WoE as predictor\nmodel <- glm(credit ~ WoE, data = data_woe, family = binomial)\n\n# Summary of the model\nsummary(model)\n#> \n#> Call:\n#> glm(formula = credit ~ WoE, family = binomial, data = data_woe)\n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) 1.023e-12  1.552e-01   0.000        1    \n#> WoE         1.000e+00  1.801e-01   5.552 2.83e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 277.26  on 199  degrees of freedom\n#> Residual deviance: 242.74  on 198  degrees of freedom\n#> AIC: 246.74\n#> \n#> Number of Fisher Scoring iterations: 4"},{"path":"variable-transformation.html","id":"helmert-encoding","chapter":"12 Variable Transformation","heading":"12.2.10 Helmert Encoding","text":"Compares category mean previous categories.Use:ANOVA models categorical regression.","code":""},{"path":"variable-transformation.html","id":"probability-ratio-encoding","chapter":"12 Variable Transformation","heading":"12.2.11 Probability Ratio Encoding","text":"Encodes categories using probability ratio target variable.","code":""},{"path":"variable-transformation.html","id":"backward-difference-encoding","chapter":"12 Variable Transformation","heading":"12.2.12 Backward Difference Encoding","text":"Compares category mean remaining categories.","code":""},{"path":"variable-transformation.html","id":"leave-one-out-encoding","chapter":"12 Variable Transformation","heading":"12.2.13 Leave-One-Out Encoding","text":"Similar target encoding, excludes current observation avoid bias.","code":""},{"path":"variable-transformation.html","id":"james-stein-encoding","chapter":"12 Variable Transformation","heading":"12.2.14 James-Stein Encoding","text":"smoothed version target encoding, reducing overfitting.","code":""},{"path":"variable-transformation.html","id":"m-estimator-encoding","chapter":"12 Variable Transformation","heading":"12.2.15 M-Estimator Encoding","text":"Uses Bayesian prior smooth target encoding.","code":""},{"path":"variable-transformation.html","id":"thermometer-encoding","chapter":"12 Variable Transformation","heading":"12.2.16 Thermometer Encoding","text":"Similar one-hot encoding, retains ordinal structure.","code":""},{"path":"variable-transformation.html","id":"choosing-the-right-encoding-method","chapter":"12 Variable Transformation","heading":"12.2.17 Choosing the Right Encoding Method","text":"","code":""},{"path":"imputation-missing-data.html","id":"imputation-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13 Imputation (Missing Data)","text":"","code":""},{"path":"imputation-missing-data.html","id":"introduction-to-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.1 Introduction to Missing Data","text":"Missing data common problem statistical analyses data science, impacting quality reliability insights derived datasets. One widely used approach address issue imputation, missing data replaced reasonable estimates.","code":""},{"path":"imputation-missing-data.html","id":"types-of-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.1.1 Types of Imputation","text":"Imputation can categorized :Unit Imputation: Replacing entire missing observation (.e., features single data point missing).Item Imputation: Replacing missing values specific variables (features) within dataset.imputation offers means make use incomplete datasets, historically viewed skeptically. skepticism arises :Frequent misapplication imputation techniques, can introduce significant bias estimates.Limited applicability, imputation works well certain assumptions missing data mechanism research objectives.Biases imputation can arise various factors, including:Imputation method: chosen method can influence results introduce biases.Imputation method: chosen method can influence results introduce biases.Missing data mechanism: nature missing data—whether Missing Completely Random (MCAR) Missing Random (MAR)—affects accuracy imputation.Missing data mechanism: nature missing data—whether Missing Completely Random (MCAR) Missing Random (MAR)—affects accuracy imputation.Proportion missing data: amount missing data significantly impacts reliability imputation.Proportion missing data: amount missing data significantly impacts reliability imputation.Available information dataset: Limited information reduces robustness imputed values.Available information dataset: Limited information reduces robustness imputed values.","code":""},{"path":"imputation-missing-data.html","id":"when-and-why-to-use-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.1.2 When and Why to Use Imputation","text":"appropriateness imputation depends nature missing data research goal:Missing Data Outcome Variable (\\(y\\)): Imputation cases generally problematic, can distort statistical models lead misleading conclusions. example, imputing outcomes regression classification problems can alter underlying relationship dependent independent variables.Missing Data Outcome Variable (\\(y\\)): Imputation cases generally problematic, can distort statistical models lead misleading conclusions. example, imputing outcomes regression classification problems can alter underlying relationship dependent independent variables.Missing Data Predictive Variables (\\(x\\)): Imputation commonly applied , especially non-random missing data. Properly handled, imputation can enable use incomplete datasets minimizing bias.Missing Data Predictive Variables (\\(x\\)): Imputation commonly applied , especially non-random missing data. Properly handled, imputation can enable use incomplete datasets minimizing bias.","code":""},{"path":"imputation-missing-data.html","id":"objectives-of-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.1.2.1 Objectives of Imputation","text":"utility imputation methods differs substantially depending whether goal analysis inference/explanation prediction. goal distinct priorities tolerances bias, variance, assumptions missing data mechanism:","code":""},{"path":"imputation-missing-data.html","id":"inferenceexplanation","chapter":"13 Imputation (Missing Data)","heading":"13.1.2.1.1 Inference/Explanation","text":"causal inference explanatory analyses, primary objective ensure valid statistical inference, emphasizing unbiased estimation parameters accurate representation uncertainty. treatment missing data must align closely assumptions mechanism behind missing data—whether Missing Completely Random (MCAR), Missing Random (MAR), Missing Random (MNAR):Bias Sensitivity: Inference analyses require imputed data preserve integrity relationships among variables. Poorly executed imputation can introduce bias, even addresses missingness superficially.Bias Sensitivity: Inference analyses require imputed data preserve integrity relationships among variables. Poorly executed imputation can introduce bias, even addresses missingness superficially.Variance Confidence Intervals: inference, quality standard errors, confidence intervals, test statistics critical. Naive imputation methods (e.g., mean imputation) often fail appropriately reflect uncertainty due missingness, leading overconfidence parameter estimates.Variance Confidence Intervals: inference, quality standard errors, confidence intervals, test statistics critical. Naive imputation methods (e.g., mean imputation) often fail appropriately reflect uncertainty due missingness, leading overconfidence parameter estimates.Mechanism Considerations: Imputation methods, multiple imputation (MI), attempt generate values consistent observed data distribution accounting missing data uncertainty. However, MI’s performance depends heavily validity MAR assumption. missingness mechanism MNAR addressed adequately, imputed data yield biased parameter estimates, undermining purpose inference.Mechanism Considerations: Imputation methods, multiple imputation (MI), attempt generate values consistent observed data distribution accounting missing data uncertainty. However, MI’s performance depends heavily validity MAR assumption. missingness mechanism MNAR addressed adequately, imputed data yield biased parameter estimates, undermining purpose inference.","code":""},{"path":"imputation-missing-data.html","id":"prediction","chapter":"13 Imputation (Missing Data)","heading":"13.1.2.1.2 Prediction","text":"predictive modeling, primary goal maximize model accuracy (e.g., minimizing mean squared error continuous outcomes maximizing classification accuracy). , focus shifts optimizing predictive performance rather ensuring unbiased parameter estimates:Loss Information: Missing data reduces amount usable information dataset. Imputation allows model leverage available data, rather excluding incomplete cases via listwise deletion, can significantly reduce sample size model performance.Loss Information: Missing data reduces amount usable information dataset. Imputation allows model leverage available data, rather excluding incomplete cases via listwise deletion, can significantly reduce sample size model performance.Impact Model Fit: predictive contexts, imputation can reduce standard errors predictions stabilize model coefficients incorporating plausible estimates missing values.Impact Model Fit: predictive contexts, imputation can reduce standard errors predictions stabilize model coefficients incorporating plausible estimates missing values.Flexibility Mechanism: Predictive models less sensitive missing data mechanism inferential models, long imputed values help reduce variability align patterns observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, even machine learning models (e.g., random forests imputation) can valuable, regardless strict adherence MAR MCAR assumptions.Flexibility Mechanism: Predictive models less sensitive missing data mechanism inferential models, long imputed values help reduce variability align patterns observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, even machine learning models (e.g., random forests imputation) can valuable, regardless strict adherence MAR MCAR assumptions.Trade-offs: Overimputation, much noise complexity introduced imputation process, can harm prediction introducing artifacts degrade model generalizability.Trade-offs: Overimputation, much noise complexity introduced imputation process, can harm prediction introducing artifacts degrade model generalizability.","code":""},{"path":"imputation-missing-data.html","id":"key-takeaways-2","chapter":"13 Imputation (Missing Data)","heading":"13.1.2.1.3 Key Takeaways","text":"usefulness imputation depends whether goal analysis inference prediction:Inference/Explanation: primary concern valid statistical inference, biased estimates unacceptable. Imputation often limited value purpose, may address underlying missing data mechanism appropriately (Rubin 1996).Inference/Explanation: primary concern valid statistical inference, biased estimates unacceptable. Imputation often limited value purpose, may address underlying missing data mechanism appropriately (Rubin 1996).Prediction: Imputation can useful predictive modeling, reduces loss information incomplete cases. leveraging observed data, imputation can lower standard errors improve model accuracy.Prediction: Imputation can useful predictive modeling, reduces loss information incomplete cases. leveraging observed data, imputation can lower standard errors improve model accuracy.","code":""},{"path":"imputation-missing-data.html","id":"importance-of-missing-data-treatment-in-statistical-modeling","chapter":"13 Imputation (Missing Data)","heading":"13.1.3 Importance of Missing Data Treatment in Statistical Modeling","text":"Proper handling missing data ensures:Unbiased Estimates: Avoiding distortions parameter estimates.Accurate Standard Errors: Ensuring valid hypothesis testing confidence intervals.Adequate Statistical Power: Maximizing use available data.Ignoring mishandling missing data can lead :Bias: Systematic errors parameter estimates, especially MAR MNAR mechanisms.Loss Power: Reduced sample size leads larger standard errors weaker statistical significance.Misleading Conclusions: -simplistic imputation methods (e.g., mean substitution) can distort relationships among variables.","code":""},{"path":"imputation-missing-data.html","id":"prevalence-of-missing-data-across-domains","chapter":"13 Imputation (Missing Data)","heading":"13.1.4 Prevalence of Missing Data Across Domains","text":"Missing data affects virtually fields:Business: Non-responses customer surveys, incomplete sales records, transactional errors.Healthcare: Missing data electronic health records (EHRs) due incomplete patient histories inconsistent data entry.Social Sciences: Non-responses partial responses large-scale surveys, leading biased conclusions.","code":""},{"path":"imputation-missing-data.html","id":"practical-considerations-for-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.1.5 Practical Considerations for Imputation","text":"Diagnostic Checks: Always examine patterns mechanisms missing data applying imputation (Diagnosing Missing Data Mechanism).Model Selection: Align imputation method missing data mechanism research goal.Validation: Assess impact imputation results sensitivity analyses cross-validation.","code":""},{"path":"imputation-missing-data.html","id":"theoretical-foundations-of-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.2 Theoretical Foundations of Missing Data","text":"","code":""},{"path":"imputation-missing-data.html","id":"definition-and-classification-of-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.2.1 Definition and Classification of Missing Data","text":"Missing data refers absence values variables dataset. mechanisms underlying missingness significantly impact validity statistical analyses choice handling methods. mechanisms classified three categories:Missing Completely Random (MCAR): probability missingness independent observed unobserved data. words, missing data occur entirely random unrelated values dataset.Missing Completely Random (MCAR): probability missingness independent observed unobserved data. words, missing data occur entirely random unrelated values dataset.Missing Random (MAR): probability missingness related observed data missing data . means , controlling observed variables, missingness random.Missing Random (MAR): probability missingness related observed data missing data . means , controlling observed variables, missingness random.Missing Random (MNAR): probability missingness depends unobserved data missing values . case, missingness related information missing, making challenging type handle analysis.Missing Random (MNAR): probability missingness depends unobserved data missing values . case, missingness related information missing, making challenging type handle analysis.","code":""},{"path":"imputation-missing-data.html","id":"missing-completely-at-random-mcar","chapter":"13 Imputation (Missing Data)","heading":"13.2.1.1 Missing Completely at Random (MCAR)","text":"MCAR occurs probability missingness entirely random unrelated either observed unobserved variables. mechanism, missing data introduce bias parameter estimates ignored, although statistical efficiency reduced due smaller sample size.Mathematical Definition: missingness independent data, observed unobserved:\\[\nP(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}})\n\\]Characteristics MCAR:Missingness completely unrelated observed unobserved data.Analyses remain unbiased even missing data ignored, though may lack efficiency due reduced sample size.missing data points represent random subset overall data.Examples:sensor randomly fails specific time points, unrelated environmental operational conditions.Survey participants randomly omit responses certain questions without systematic pattern.Methods Testing MCAR:Little’s MCAR Test: formal statistical test assess whether data MCAR. significant result suggests deviation MCAR.Little’s MCAR Test: formal statistical test assess whether data MCAR. significant result suggests deviation MCAR.Mean Comparison Tests:\nT-tests similar approaches compare observed missing data groups variables. Significant differences indicate potential bias.\nFailure reject null hypothesis difference confirm MCAR suggests consistency MCAR assumption.\nMean Comparison Tests:T-tests similar approaches compare observed missing data groups variables. Significant differences indicate potential bias.Failure reject null hypothesis difference confirm MCAR suggests consistency MCAR assumption.Handling MCAR:Since MCAR data introduce bias, can handled using following techniques:Complete Case Analysis (Listwise Deletion):\nAnalyses performed cases complete data. unbiased MCAR, method reduces sample size efficiency.\nAnalyses performed cases complete data. unbiased MCAR, method reduces sample size efficiency.Universal Singular Value Thresholding (USVT):\ntechnique effective MCAR data recovery can recover mean structure, entire true distribution (Chatterjee 2015).\ntechnique effective MCAR data recovery can recover mean structure, entire true distribution (Chatterjee 2015).SoftImpute:\nmatrix completion method useful missing data problems less effective missingness MCAR (Hastie et al. 2015).\nmatrix completion method useful missing data problems less effective missingness MCAR (Hastie et al. 2015).Synthetic Nearest Neighbor Imputation:\nrobust method imputing missing data. primarily designed MCAR, can also handle certain cases missing random (MNAR) (Agarwal et al. 2023). Available GitHub: syntheticNN.\nrobust method imputing missing data. primarily designed MCAR, can also handle certain cases missing random (MNAR) (Agarwal et al. 2023). Available GitHub: syntheticNN.Notes:“missingness” one variable can correlated “missingness” another variable without violating MCAR assumption.Absence evidence bias (e.g., failing reject t-test) confirm data MCAR.","code":""},{"path":"imputation-missing-data.html","id":"missing-at-random-mar","chapter":"13 Imputation (Missing Data)","heading":"13.2.1.2 Missing at Random (MAR)","text":"Missing Random (MAR) occurs missingness depends observed variables missing values . mechanism assumes observed data provide sufficient information explain missingness. words, systematic relationship propensity missing values observed data, missing data.Mathematical Definition:probability missingness conditional observed data:\\[\nP(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}} | X)\n\\]implies whether observation missing unrelated missing values related observed values variables.Characteristics MAR:Missingness systematically related observed variables.propensity data point missing related missing data related observed data.Analyses must account observed data mitigate bias.Examples:Women less likely disclose weight, gender recorded. case, weight MAR.Missing income data correlated education, observed. example, individuals higher education levels might less likely reveal income.Challenges MAR:MAR weaker Missing Completely Random (MCAR).impossible directly test MAR. Evidence MAR relies domain expertise indirect statistical checks rather direct tests.Handling MAR:Common methods handling MAR include:Multiple Imputation Chained Equations (MICE): Iteratively imputes missing values based observed data.Multiple Imputation Chained Equations (MICE): Iteratively imputes missing values based observed data.Maximum Likelihood Estimation: Estimates model parameters directly accounting MAR assumptions.Maximum Likelihood Estimation: Estimates model parameters directly accounting MAR assumptions.Regression-Based Imputation: Predicts missing values using observed covariates.Regression-Based Imputation: Predicts missing values using observed covariates.methods assume observed variables fully explain missingness. Effective handling MAR requires careful modeling often domain-specific knowledge validate assumptions underlying analysis.","code":""},{"path":"imputation-missing-data.html","id":"missing-not-at-random-mnar","chapter":"13 Imputation (Missing Data)","heading":"13.2.1.3 Missing Not at Random (MNAR)","text":"Missing Random (MNAR) complex missing data mechanism. , missingness depends unobserved variables values missing data . makes MNAR particularly challenging, ignoring dependency introduces significant bias analyses.Mathematical Definition:probability missingness depends missing values:\\[\nP(Y_{\\text{missing}} | Y, X) \\neq P(Y_{\\text{missing}} | X)\n\\]Characteristics MNAR:Missingness fully explained observed data.cause missingness directly related unobserved values.Ignoring MNAR introduces significant bias parameter estimates, often leading invalid conclusions.Examples:High-income individuals less likely disclose income, income unobserved.Patients severe symptoms drop clinical study, leaving health outcomes unrecorded.Challenges MNAR:MNAR difficult missingness mechanism address missing data mechanism must explicitly modeled.Identifying MNAR often requires domain knowledge auxiliary information beyond observed dataset.Handling MNAR:MNAR requires explicit modeling missingness mechanism. Common approaches include:Heckman Selection Models: models explicitly account selection process leading missing data, adjusting potential bias (J. J. Heckman 1976a).Heckman Selection Models: models explicitly account selection process leading missing data, adjusting potential bias (J. J. Heckman 1976a).Instrumental Variables: Variables predictive missingness unrelated outcome can used mitigate bias (B. Sun et al. 2018; E. J. Tchetgen Tchetgen Wirth 2017).Instrumental Variables: Variables predictive missingness unrelated outcome can used mitigate bias (B. Sun et al. 2018; E. J. Tchetgen Tchetgen Wirth 2017).Pattern-Mixture Models: models separate data groups (patterns) based missingness model group separately. particularly useful relationship missingness missing values complex.Pattern-Mixture Models: models separate data groups (patterns) based missingness model group separately. particularly useful relationship missingness missing values complex.Sensitivity Analysis: Examines conclusions change different assumptions missing data mechanism.Sensitivity Analysis: Examines conclusions change different assumptions missing data mechanism.Use Auxiliary Data\nAuxiliary data refers external data sources variables can help explain missingness mechanism.\nSurrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.\nLinking External Datasets: Merging datasets different sources can provide additional context predictors missingness.\nApplications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.\nUse Auxiliary DataAuxiliary data refers external data sources variables can help explain missingness mechanism.Surrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.Surrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.Linking External Datasets: Merging datasets different sources can provide additional context predictors missingness.Linking External Datasets: Merging datasets different sources can provide additional context predictors missingness.Applications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.Applications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.Additionally, data collection strategies, follow-surveys targeted sampling, can help mitigate MNAR effects collecting information directly addresses missingness mechanism. However, approaches can resource-intensive require careful planning.","code":""},{"path":"imputation-missing-data.html","id":"missing-data-mechanisms","chapter":"13 Imputation (Missing Data)","heading":"13.2.2 Missing Data Mechanisms","text":"","code":""},{"path":"imputation-missing-data.html","id":"relationship-between-mechanisms-and-ignorability","chapter":"13 Imputation (Missing Data)","heading":"13.2.3 Relationship Between Mechanisms and Ignorability","text":"concept ignorability central determining whether missingness process must explicitly modeled. Ignorability impacts choice methods handling missing data whether missing data mechanism can safely disregarded must explicitly accounted .","code":""},{"path":"imputation-missing-data.html","id":"ignorable-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.2.3.1 Ignorable Missing Data","text":"Missing data ignorable following conditions:missing data mechanism MAR MCAR.parameters governing missing data process unrelated parameters interest analysis.cases ignorable missing data, need model missingness mechanism explicitly unless aim improve efficiency precision parameter estimates. Common imputation techniques, multiple imputation maximum likelihood estimation, rely assumption ignorability produce unbiased parameter estimates.Practical Considerations Ignorable MissingnessEven though ignorable mechanisms simplify analysis, researchers must rigorously assess whether missingness mechanism meets MAR MCAR criteria. Violations can lead biased results, even unintentionally overlooked.example: survey income may assume MAR missingness associated respondent age (observed variable) income (unobserved variable). However, income directly influences nonresponse, assumption MAR violated.","code":""},{"path":"imputation-missing-data.html","id":"non-ignorable","chapter":"13 Imputation (Missing Data)","heading":"13.2.3.2 Non-Ignorable Missing Data","text":"Missing data non-ignorable :missingness mechanism depends values missing data unobserved variables.missing data mechanism related parameters interest, resulting bias mechanism modeled explicitly.type missingness (.e., Missing Random (MNAR) requires modeling missing data mechanism directly produce unbiased estimates.Characteristics Non-Ignorable MissingnessDependence Missing Values: likelihood missingness associated missing values .\nExample: study health, individuals severe conditions likely drop , leading underrepresentation sickest individuals data.\nExample: study health, individuals severe conditions likely drop , leading underrepresentation sickest individuals data.Bias Complete Case Analysis: Analyses based solely complete cases can lead substantial bias.\nExample: income surveys, wealthier individuals less likely report income, estimated mean income systematically lower true population mean.\nExample: income surveys, wealthier individuals less likely report income, estimated mean income systematically lower true population mean.Need Explicit Modeling: address MNAR, analyst must model missing data mechanism. often involves specifying relationships observed data, missing data, missingness process .","code":""},{"path":"imputation-missing-data.html","id":"implications-of-non-ignorable-missingness","chapter":"13 Imputation (Missing Data)","heading":"13.2.3.3 Implications of Non-Ignorable Missingness","text":"Non-ignorable mechanisms often associated sensitive personal data:Examples:\nIndividuals lower education levels may omit education information.\nParticipants controversial stigmatized health conditions might opt surveys entirely.\nExamples:Individuals lower education levels may omit education information.Individuals lower education levels may omit education information.Participants controversial stigmatized health conditions might opt surveys entirely.Participants controversial stigmatized health conditions might opt surveys entirely.Impact Policy Decision-Making:\nBiases introduced MNAR can serious consequences policymaking, underestimating prevalence poverty mischaracterizing population health needs.\nImpact Policy Decision-Making:Biases introduced MNAR can serious consequences policymaking, underestimating prevalence poverty mischaracterizing population health needs.explicitly addressing non-ignorable missingness, researchers can mitigate biases ensure findings accurately reflect underlying population.","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-the-missing-data-mechanism","chapter":"13 Imputation (Missing Data)","heading":"13.3 Diagnosing the Missing Data Mechanism","text":"Understanding mechanism behind missing data critical choosing appropriate methods handling . three main mechanisms missing data MCAR (Missing Completely Random), MAR (Missing Random), MNAR (Missing Random). section discusses methods diagnosing mechanisms, including descriptive inferential approaches.","code":""},{"path":"imputation-missing-data.html","id":"descriptive-methods","chapter":"13 Imputation (Missing Data)","heading":"13.3.1 Descriptive Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"visualizing-missing-data-patterns","chapter":"13 Imputation (Missing Data)","heading":"13.3.1.1 Visualizing Missing Data Patterns","text":"Visualization tools essential detecting patterns missing data. Heatmaps correlation plots can help identify systematic missingness provide insights underlying mechanism.Heatmaps: Highlight missingness occurs dataset.Heatmaps: Highlight missingness occurs dataset.Correlation Plots: Show relationships missingness indicators different variables.Correlation Plots: Show relationships missingness indicators different variables.Exploring Univariate Multivariate MissingnessUnivariate Analysis: Calculate proportion missing data variable.Multivariate Analysis: Examine whether missingness one variable related others. can visualized using scatterplots observed vs. missing values.","code":"\n# Example: Visualizing missing data\nlibrary(Amelia)\nmissmap(\n    airquality,\n    main = \"Missing Data Heatmap\",\n    col = c(\"yellow\", \"black\"),\n    legend = TRUE\n)\n# Example: Proportion of missing values\nmissing_proportions <- colSums(is.na(airquality)) / nrow(airquality)\nprint(missing_proportions)\n#>      Ozone    Solar.R       Wind       Temp      Month        Day \n#> 0.24183007 0.04575163 0.00000000 0.00000000 0.00000000 0.00000000\n# Example: Missingness correlation\nlibrary(naniar)\nvis_miss(airquality)\ngg_miss_upset(airquality) # Displays a missingness upset plot"},{"path":"imputation-missing-data.html","id":"statistical-tests-for-missing-data-mechanisms","chapter":"13 Imputation (Missing Data)","heading":"13.3.2 Statistical Tests for Missing Data Mechanisms","text":"","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-mcar-littles-test","chapter":"13 Imputation (Missing Data)","heading":"13.3.2.1 Diagnosing MCAR: Little’s Test","text":"Little’s test hypothesis test determine missing data mechanism MCAR. tests whether means observed missing data significantly different. null hypothesis data MCAR.\\[\n\\chi^2 = \\sum_{=1}^n \\frac{(O_i - E_i)^2}{E_i}\n\\]:\\(O_i\\)= Observed frequency\\(O_i\\)= Observed frequency\\(E_i\\)= Expected frequency MCAR\\(E_i\\)= Expected frequency MCAR","code":"\n# Example: Little's test\nnaniar::mcar_test(airquality)\n#> # A tibble: 1 × 4\n#>   statistic    df p.value missing.patterns\n#>       <dbl> <dbl>   <dbl>            <int>\n#> 1      35.1    14 0.00142                4\nmisty::na.test(airquality)\n#>  Little's MCAR Test\n#> \n#>     n nIncomp nPattern  chi2 df  pval \n#>   152      42        4 35.68 14 0.001"},{"path":"imputation-missing-data.html","id":"diagnosing-mcar-via-dummy-variables","chapter":"13 Imputation (Missing Data)","heading":"13.3.2.2 Diagnosing MCAR via Dummy Variables","text":"Creating binary indicator missingness allows test whether presence missing data related observed data. instance:Create dummy variable:\n1 = Missing\n0 = Observed\nCreate dummy variable:1 = Missing1 = Missing0 = Observed0 = ObservedConduct chi-square test t-test:\nChi-square: Compare proportions missingness across groups.\nT-test: Compare means () observed variables missingness indicators.\nConduct chi-square test t-test:Chi-square: Compare proportions missingness across groups.Chi-square: Compare proportions missingness across groups.T-test: Compare means () observed variables missingness indicators.T-test: Compare means () observed variables missingness indicators.","code":"\n# Example: Chi-square test\nairquality$missing_var <- as.factor(ifelse(is.na(airquality$Ozone), 1, 0))\n# Across groups of months\ntable(airquality$missing_var, airquality$Month)\n#>    \n#>      5  6  7  8  9\n#>   0 26  9 26 26 29\n#>   1  5 21  5  5  1\nchisq.test(table(airquality$missing_var, airquality$Month))\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  table(airquality$missing_var, airquality$Month)\n#> X-squared = 44.751, df = 4, p-value = 4.48e-09\n\n# Example: T-test (of other variable)\nt.test(Wind ~ missing_var, data = airquality)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  Wind by missing_var\n#> t = -0.60911, df = 63.646, p-value = 0.5446\n#> alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.6893132  0.8999377\n#> sample estimates:\n#> mean in group 0 mean in group 1 \n#>        9.862069       10.256757"},{"path":"imputation-missing-data.html","id":"assessing-mar-and-mnar","chapter":"13 Imputation (Missing Data)","heading":"13.3.3 Assessing MAR and MNAR","text":"","code":""},{"path":"imputation-missing-data.html","id":"sensitivity-analysis","chapter":"13 Imputation (Missing Data)","heading":"13.3.3.1 Sensitivity Analysis","text":"Sensitivity analysis involves simulating different scenarios missing data assessing results change. example, imputing missing values different assumptions can provide insight whether data MAR MNAR.","code":""},{"path":"imputation-missing-data.html","id":"proxy-variables-and-external-data","chapter":"13 Imputation (Missing Data)","heading":"13.3.3.2 Proxy Variables and External Data","text":"Using proxy variables external data sources can help assess whether missingness depends unobserved variables (MNAR). example, surveys, follow-ups non-respondents can reveal systematic differences.","code":""},{"path":"imputation-missing-data.html","id":"practical-challenges-in-distinguishing-mar-from-mnar","chapter":"13 Imputation (Missing Data)","heading":"13.3.3.3 Practical Challenges in Distinguishing MAR from MNAR","text":"Distinguishing Missing Random (MAR) Missing Random (MNAR) critical challenging task data analysis. Properly identifying nature missing data significant implications choice imputation strategies, model robustness, validity conclusions. statistical tests can sometimes aid determination, process often relies heavily domain knowledge, intuition, exploratory analysis. , discuss key considerations examples highlight challenges:Sensitive Topics: Missing data related sensitive stigmatized topics, income, drug use, health conditions, often MNAR. example, individuals higher incomes might deliberately choose report earnings due privacy concerns. Similarly, participants health survey may avoid answering questions smoking perceive social disapproval. cases, probability missingness directly related unobserved value , making MNAR likely.Sensitive Topics: Missing data related sensitive stigmatized topics, income, drug use, health conditions, often MNAR. example, individuals higher incomes might deliberately choose report earnings due privacy concerns. Similarly, participants health survey may avoid answering questions smoking perceive social disapproval. cases, probability missingness directly related unobserved value , making MNAR likely.Field-Specific Norms: Understanding norms typical data collection practices specific field can provide insights missingness patterns. instance, marketing surveys, respondents may skip questions spending habits consider questions intrusive. Prior research historical data domain can help infer whether missingness likely MAR (e.g., random skipping due survey fatigue) MNAR (e.g., deliberate omission higher spenders).Field-Specific Norms: Understanding norms typical data collection practices specific field can provide insights missingness patterns. instance, marketing surveys, respondents may skip questions spending habits consider questions intrusive. Prior research historical data domain can help infer whether missingness likely MAR (e.g., random skipping due survey fatigue) MNAR (e.g., deliberate omission higher spenders).Analyzing Auxiliary Variables: Leveraging auxiliary variables—correlated missing variable—can help infer missingness mechanism. example, missing income data strongly correlates employment status, suggests MAR mechanism, missingness depends observed variables. However, missingness persists even accounting observable predictors, MNAR might play.Analyzing Auxiliary Variables: Leveraging auxiliary variables—correlated missing variable—can help infer missingness mechanism. example, missing income data strongly correlates employment status, suggests MAR mechanism, missingness depends observed variables. However, missingness persists even accounting observable predictors, MNAR might play.Experimental Design Follow-: longitudinal studies, dropout rates can signal MAR MNAR patterns. example, dropouts occur disproportionately among participants reporting lower satisfaction early surveys, indicates MNAR mechanism. Designing follow-surveys specifically investigate dropout reasons can clarify missingness patterns.Experimental Design Follow-: longitudinal studies, dropout rates can signal MAR MNAR patterns. example, dropouts occur disproportionately among participants reporting lower satisfaction early surveys, indicates MNAR mechanism. Designing follow-surveys specifically investigate dropout reasons can clarify missingness patterns.Sensitivity Analysis: account uncertainty missingness mechanism, researchers can conduct sensitivity analyses comparing results different assumptions (e.g., imputing data using MAR MNAR approaches). process helps quantify potential impact misclassifying missingness mechanism study conclusions.Sensitivity Analysis: account uncertainty missingness mechanism, researchers can conduct sensitivity analyses comparing results different assumptions (e.g., imputing data using MAR MNAR approaches). process helps quantify potential impact misclassifying missingness mechanism study conclusions.Real-World Examples:\ncustomer feedback surveys, higher ratings might overrepresented due non-response bias. Customers negative experiences might less likely complete surveys, leading MNAR scenario.\nfinancial reporting, missing audit data might correlate companies financial distress, classic MNAR case missingness depends unobserved financial health metrics.\nReal-World Examples:customer feedback surveys, higher ratings might overrepresented due non-response bias. Customers negative experiences might less likely complete surveys, leading MNAR scenario.financial reporting, missing audit data might correlate companies financial distress, classic MNAR case missingness depends unobserved financial health metrics.SummaryMCAR: pattern missingness; use Little’s test dummy variable analysis.MCAR: pattern missingness; use Little’s test dummy variable analysis.MAR: Missingness related observed data; requires modeling assumptions proxy analysis.MAR: Missingness related observed data; requires modeling assumptions proxy analysis.MNAR: Missingness depends unobserved data; requires external validation sensitivity analysis.MNAR: Missingness depends unobserved data; requires external validation sensitivity analysis.","code":""},{"path":"imputation-missing-data.html","id":"methods-for-handling-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.4 Methods for Handling Missing Data","text":"","code":""},{"path":"imputation-missing-data.html","id":"basic-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.1 Basic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"complete-case-analysis-listwise-deletion","chapter":"13 Imputation (Missing Data)","heading":"13.4.1.1 Complete Case Analysis (Listwise Deletion)","text":"Listwise deletion retains cases complete data features, discarding rows missing values.Advantages:Universally applicable various statistical tests (e.g., SEM, multilevel regression).data Missing Completely Random (MCAR), parameter estimates standard errors unbiased.specific Missing Random (MAR) conditions, probability missing data depends independent variables, listwise deletion can still yield unbiased estimates. instance, model \\(y = \\beta_{0} + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\), missingness \\(X_1\\) independent \\(y\\) depends \\(X_1\\) \\(X_2\\), estimates remain unbiased (Little 1992).\naligns principles stratified sampling, bias estimates.\nlogistic regression, missing data depend dependent variable independent variables, listwise deletion produces consistent slope estimates, though intercept may biased (Vach Vach 1994).\naligns principles stratified sampling, bias estimates.logistic regression, missing data depend dependent variable independent variables, listwise deletion produces consistent slope estimates, though intercept may biased (Vach Vach 1994).regression analysis, listwise deletion robust Maximum Likelihood (ML) Multiple Imputation (MI) MAR assumption violated.Disadvantages:Results larger standard errors compared advanced methods.data MAR MCAR, biased estimates can occur.non-regression contexts, sophisticated methods often outperform listwise deletion.","code":""},{"path":"imputation-missing-data.html","id":"available-case-analysis-pairwise-deletion","chapter":"13 Imputation (Missing Data)","heading":"13.4.1.2 Available Case Analysis (Pairwise Deletion)","text":"Pairwise deletion calculates estimates using available data pair variables, without requiring complete cases. particularly suitable methods like linear regression, factor analysis, SEM, rely correlation covariance matrices.Advantages:MCAR, pairwise deletion produces consistent unbiased estimates large samples.Compared listwise deletion (Glasser 1964):\nvariable correlations low, pairwise deletion provides efficient estimates.\ncorrelations high, listwise deletion becomes efficient.\nvariable correlations low, pairwise deletion provides efficient estimates.correlations high, listwise deletion becomes efficient.Disadvantages:Yields biased estimates MAR conditions.small samples, covariance matrices might positive definite, rendering coefficient estimation infeasible.Software implementation varies sample size handled, potentially affecting standard errors.Note: Carefully review software documentation understand sample size treated, influences standard error calculations.","code":""},{"path":"imputation-missing-data.html","id":"indicator-method-dummy-variable-adjustment","chapter":"13 Imputation (Missing Data)","heading":"13.4.1.3 Indicator Method (Dummy Variable Adjustment)","text":"Also known Missing Indicator Method, approach introduces additional variable indicate missingness dataset.Implementation:Create indicator variable:\\[\nD =\n\\begin{cases}\n1 & \\text{data } X \\text{ missing} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Modify original variable accommodate missingness:\\[\nX^* =\n\\begin{cases}\nX & \\text{data available} \\\\\nc & \\text{data missing}\n\\end{cases}\n\\]Note: common choice \\(c\\) mean \\(X\\).Interpretation:coefficient \\(D\\) represents difference expected value \\(Y\\) cases missing data without.coefficient \\(X^*\\) reflects effect \\(X\\) \\(Y\\) cases observed data.Disadvantages:Produces biased estimates coefficients, even MCAR conditions (Jones 1996).May lead overinterpretation “missingness effect,” complicating model interpretation.","code":""},{"path":"imputation-missing-data.html","id":"advantages-and-limitations-of-basic-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.1.4 Advantages and Limitations of Basic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"single-imputation-techniques","chapter":"13 Imputation (Missing Data)","heading":"13.4.2 Single Imputation Techniques","text":"Single imputation methods replace missing data single value, generating complete dataset can analyzed using standard techniques. convenient, single imputation generally underestimates variability risks biasing results.","code":""},{"path":"imputation-missing-data.html","id":"deterministic-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.1 Deterministic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"mean-median-mode-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.1.1 Mean, Median, Mode Imputation","text":"method replaces missing values mean, median, mode observed data.Advantages:Simplicity ease implementation.Useful quick exploratory data analysis.Disadvantages:Bias Variances Relationships: Mean imputation reduces variance disrupts relationships among variables, leading biased estimates variances covariances (Haitovsky 1968).Underestimated Standard Errors: Results overly optimistic conclusions increased risk Type errors.Dependency Structure Ignored: Particularly problematic high-dimensional data, fails capture dependencies among features.","code":""},{"path":"imputation-missing-data.html","id":"forward-and-backward-filling-time-series-contexts","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.1.2 Forward and Backward Filling (Time Series Contexts)","text":"Used time series analysis, method replaces missing values using preceding (forward filling) succeeding (backward filling) values.Advantages:Simple preserves temporal ordering.Suitable datasets adjacent values strongly correlated.Disadvantages:Biased missingness spans long gaps occurs systematically.capture trends changes underlying process.","code":""},{"path":"imputation-missing-data.html","id":"statistical-prediction-models","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.2 Statistical Prediction Models","text":"","code":""},{"path":"imputation-missing-data.html","id":"linear-regression-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.2.1 Linear Regression Imputation","text":"Missing values variable imputed based linear regression model using observed values variables.Advantages:Preserves relationships variables.sophisticated mean median imputation.Disadvantages:Assumes linear relationships, may hold datasets.Fails capture variability, leading downwardly biased standard errors.","code":""},{"path":"imputation-missing-data.html","id":"logistic-regression-for-categorical-variables","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.2.2 Logistic Regression for Categorical Variables","text":"Similar linear regression imputation used categorical variables. missing category predicted using logistic regression model.Advantages:Useful binary multinomial categorical data.Preserves relationships variables.Disadvantages:Assumes underlying logistic model appropriate.account uncertainty imputed values.","code":""},{"path":"imputation-missing-data.html","id":"non-parametric-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.3 Non-Parametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"hot-deck-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.3.1 Hot Deck Imputation","text":"Hot Deck Imputation method handling missing data missing values replaced observed values “donor” cases similar characteristics. technique widely used survey data, including organizations like U.S. Census Bureau, due flexibility ability maintain observed data distributions.Advantages Hot Deck ImputationRetains observed data distributions: Since missing values imputed using actual observed data, overall distribution remains realistic.Flexible: method applicable categorical continuous variables.Constrained imputations: Imputed values always feasible, come observed cases.Adds variability: randomly selecting donors, method introduces variability, can aid robust standard error estimation.Disadvantages Hot Deck ImputationSensitivity similarity definitions: quality imputed values depends criteria used define similarity cases.Computational intensity: Identifying similar cases randomly selecting donors can computationally expensive, especially large datasets.Subjectivity: Deciding define “similar” can introduce subjectivity bias.Algorithm Hot Deck ImputationLet \\(n_1\\) represent number cases complete data variable \\(Y\\), \\(n_0\\) represent number cases missing data \\(Y\\). steps follows:\\(n_1\\) cases complete data, take random sample (replacement) \\(n_1\\) cases.sampled pool, take another random sample (replacement) size \\(n_0\\).Assign values sampled \\(n_0\\) cases cases missing data \\(Y\\).Repeat process every variable dataset.multiple imputation, repeat four steps multiple times create multiple imputed datasets.Variations ConsiderationsSkipping Step 1: Step 1 skipped, variability imputed values reduced. approach might fully account uncertainty missing data, can underestimate standard errors.Defining similarity: major challenge method deciding constitutes “similarity” cases. Common approaches include matching based distance metrics (e.g., Euclidean distance) grouping cases strata clusters.Practical ExampleThe U.S. Census Bureau employs approximate Bayesian bootstrap variation Hot Deck Imputation. approach:Similar cases identified based shared characteristics grouping variables.Similar cases identified based shared characteristics grouping variables.randomly chosen value similar individual sample used replace missing value.randomly chosen value similar individual sample used replace missing value.method ensures imputed values plausible incorporating variability.Key NotesGood aspects:\nImputed values constrained observed possibilities.\nRandom selection introduces variability, helpful multiple imputation scenarios.\nImputed values constrained observed possibilities.Random selection introduces variability, helpful multiple imputation scenarios.Challenges:\nDefining operationalizing “similarity” remains critical step applying method effectively.\nDefining operationalizing “similarity” remains critical step applying method effectively.example code snippet illustrating Hot Deck Imputation R:code randomly imputes missing values Age column based observed data using Hmisc package’s impute function.","code":"\nlibrary(Hmisc)\n\n# Example dataset with missing values\ndata <- data.frame(\n  ID = 1:10,\n  Age = c(25, 30, NA, 40, NA, 50, 60, NA, 70, 80),\n  Gender = c(\"M\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\")\n)\n\n# Perform Hot Deck Imputation using Hmisc::impute\ndata$Age_imputed <- impute(data$Age, \"random\")\n\n# Display the imputed dataset\nprint(data)\n#>    ID Age Gender Age_imputed\n#> 1   1  25      M          25\n#> 2   2  30      F          30\n#> 3   3  NA      F          60\n#> 4   4  40      M          40\n#> 5   5  NA      M          60\n#> 6   6  50      F          50\n#> 7   7  60      M          60\n#> 8   8  NA      F          50\n#> 9   9  70      M          70\n#> 10 10  80      F          80"},{"path":"imputation-missing-data.html","id":"cold-deck-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.3.2 Cold Deck Imputation","text":"Cold Deck Imputation systematic variant Hot Deck Imputation donor pool predefined. Instead selecting donors dynamically within dataset, Cold Deck Imputation relies external reference dataset, historical data high-quality external sources.Advantages Cold Deck ImputationUtilizes high-quality external data: method particularly useful reliable external reference datasets available, allowing accurate consistent imputations.Consistency: donor pool used across multiple datasets, imputations remain consistent, can advantageous longitudinal studies standardized processes.Disadvantages Cold Deck ImputationLack adaptability: External data may adequately reflect unique characteristics variability current dataset.Potential systematic bias: donor pool significantly different target dataset, imputations may introduce bias.Reduces variability: Unlike Hot Deck Imputation, Cold Deck Imputation systematically selects values, removes random variation. can affect estimation standard errors inferential statistics.Key CharacteristicsSystematic Selection: Cold Deck Imputation selects donor values systematically based predefined rules matching criteria, rather using random sampling.External Donor Pool: Donors typically drawn separate dataset historical records.Algorithm Cold Deck ImputationIdentify external reference dataset predefined donor pool.Define matching criteria find “similar” cases donor pool current dataset (e.g., based covariates stratification).Systematically assign values donor pool missing values current dataset based matching criteria.Repeat process variable missing data.Practical ConsiderationsCold Deck Imputation works well external data closely resemble target dataset. However, significant differences distributions relationships variables, imputations may biased unrealistic.method less useful datasets without access reliable external reference data.Suppose current dataset missing values historical dataset similar variables. following example demonstrates Cold Deck Imputation can implemented:Comparison Hot Deck ImputationThis method suits situations external reference datasets trusted representative. However, careful consideration required ensure alignment donor pool target dataset avoid systematic biases.","code":"\n# Current dataset with missing values\ncurrent_data <- data.frame(\n  ID = 1:5,\n  Age = c(25, 30, NA, 45, NA),\n  Gender = c(\"M\", \"F\", \"F\", \"M\", \"M\")\n)\n\n# External reference dataset (donor pool)\nreference_data <- data.frame(\n  Age = c(28, 35, 42, 50),\n  Gender = c(\"M\", \"F\", \"F\", \"M\")\n)\n\n# Perform Cold Deck Imputation\nlibrary(dplyr)\n\n# Define a matching function to find closest donor\nimpute_cold_deck <- function(missing_row, reference_data) {\n  # Filter donors with the same gender\n  possible_donors <- reference_data %>%\n    filter(Gender == missing_row$Gender)\n  \n  # Return the mean age of matching donors as an example of systematic imputation\n  return(mean(possible_donors$Age, na.rm = TRUE))\n}\n\n# Apply Cold Deck Imputation to the missing rows\ncurrent_data <- current_data %>%\n  rowwise() %>%\n  mutate(\n    Age_imputed = ifelse(\n      is.na(Age),\n      impute_cold_deck(cur_data(), reference_data),\n      Age\n    )\n  )\n\n# Display the imputed dataset\nprint(current_data)\n#> # A tibble: 5 × 4\n#> # Rowwise: \n#>      ID   Age Gender Age_imputed\n#>   <int> <dbl> <chr>        <dbl>\n#> 1     1    25 M             25  \n#> 2     2    30 F             30  \n#> 3     3    NA F             38.8\n#> 4     4    45 M             45  \n#> 5     5    NA M             38.8"},{"path":"imputation-missing-data.html","id":"random-draw-from-observed-distribution","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.3.3 Random Draw from Observed Distribution","text":"imputation method replaces missing values randomly sampling observed distribution variable missing data. simple, non-parametric approach retains variability original data.AdvantagesPreserves variability:\nrandomly drawing values observed data, method ensures imputed values reflect inherent variability variable.\nrandomly drawing values observed data, method ensures imputed values reflect inherent variability variable.Computational simplicity:\nprocess straightforward require model fitting complex calculations.\nprocess straightforward require model fitting complex calculations.DisadvantagesIgnores relationships among variables:\nSince imputation based solely observed distribution variable, consider relationships dependencies variables.\nSince imputation based solely observed distribution variable, consider relationships dependencies variables.May align trends:\nImputed values random may fail align patterns trends present data, time series structures interactions.\nImputed values random may fail align patterns trends present data, time series structures interactions.Steps Random Draw ImputationIdentify observed (non-missing) values variable.missing value, randomly sample one value observed distribution without replacement.Replace missing value randomly sampled value.following example demonstrates use random draw imputation fill missing values:ConsiderationsWhen Use:\nmethod suitable exploratory analysis quick way handle missing data univariate contexts.\nUse:method suitable exploratory analysis quick way handle missing data univariate contexts.Limitations:\nRandom draws may result values fit well broader context dataset, especially cases variable strong relationships others.\nLimitations:Random draws may result values fit well broader context dataset, especially cases variable strong relationships others.method quick computationally efficient way address missing data best complemented sophisticated methods relationships variables important.","code":"\n# Example dataset with missing values\nset.seed(123)\ndata <- data.frame(\n  ID = 1:10,\n  Value = c(10, 20, NA, 30, 40, NA, 50, 60, NA, 70)\n)\n\n# Perform random draw imputation\nrandom_draw_impute <- function(data, variable) {\n  observed_values <- data[[variable]][!is.na(data[[variable]])] # Observed values\n  data[[variable]][is.na(data[[variable]])] <- sample(observed_values, \n                                                      sum(is.na(data[[variable]])), \n                                                      replace = TRUE)\n  return(data)\n}\n\n# Apply the imputation\nimputed_data <- random_draw_impute(data, variable = \"Value\")\n\n# Display the imputed dataset\nprint(imputed_data)\n#>    ID Value\n#> 1   1    10\n#> 2   2    20\n#> 3   3    70\n#> 4   4    30\n#> 5   5    40\n#> 6   6    70\n#> 7   7    50\n#> 8   8    60\n#> 9   9    30\n#> 10 10    70"},{"path":"imputation-missing-data.html","id":"semi-parametric-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.4 Semi-Parametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"predictive-mean-matching-pmm","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.4.1 Predictive Mean Matching (PMM)","text":"Predictive Mean Matching (PMM) imputes missing values finding observed values closest predicted value (based regression model) missing data. donor values used fill gaps.Advantages:Maintains observed variability data.Maintains observed variability data.Ensures imputed values realistic since drawn observed data.Ensures imputed values realistic since drawn observed data.Disadvantages:Requires suitable predictive model.Requires suitable predictive model.Computationally intensive large datasets.Computationally intensive large datasets.Steps PMM:Regress \\(Y\\) \\(X\\) (matrix covariates) \\(n_1\\) (non-missing cases) estimate coefficients \\(\\hat{b}\\) residual variance \\(s^2\\).Draw posterior predictive distribution residual variance: \\[s^2_{[1]} = \\frac{(n_1-k)s^2}{\\chi^2},\\] \\(\\chi^2\\) random draw \\(\\chi^2_{n_1-k}\\).Randomly sample posterior distribution \\(\\hat{b}\\): \\[b_{[1]} \\sim MVN(\\hat{b}, s^2_{[1]}(X'X)^{-1}).\\]Standardize residuals \\(n_1\\) cases: \\[e_i = \\frac{y_i - \\hat{b}x_i}{\\sqrt{s^2(1-k/n_1)}}.\\]Randomly draw sample (replacement) \\(n_0\\) residuals Step 4.Calculate imputed values \\(n_0\\) missing cases: \\[y_i = b_{[1]}x_i + s_{[1]}e_i.\\]Repeat Steps 2–6 (except Step 4) create multiple imputations.Notes:PMM can handle heteroskedasticityPMM can handle heteroskedasticityworks multiple variables, imputing using others predictors.works multiple variables, imputing using others predictors.Example:Example Statistics GlobeExample UCLA Statistical Consultingrr = number observations pairs values observedrm = number observations variables missing valuesmr = number observations first variable’s value (e.g. row variable) observed second (column) variable missingmm = number observations second variable’s value (e.g. col variable) observed first (row) variable missing","code":"\nset.seed(1) # Seed\nN  <- 100                                    # Sample size\ny  <- round(runif(N,-10, 10))                 # Target variable Y\nx1 <- y + round(runif(N, 0, 50))              # Auxiliary variable 1\nx2 <- round(y + 0.25 * x1 + rnorm(N,-3, 15))  # Auxiliary variable 2\nx3 <- round(0.1 * x1 + rpois(N, 2))           # Auxiliary variable 3\n# (categorical variable)\nx4 <- as.factor(round(0.02 * y + runif(N)))   # Auxiliary variable 4 \n\n# Insert 20% missing data in Y\ny[rbinom(N, 1, 0.2) == 1] <- NA               \n\ndata <- data.frame(y, x1, x2, x3, x4)         # Store data in dataset\nhead(data) # First 6 rows of our data\n#>    y x1  x2 x3 x4\n#> 1 NA 28 -10  5  0\n#> 2 NA 15  -2  2  1\n#> 3  1 15 -12  6  1\n#> 4  8 58  22 10  1\n#> 5 NA 26 -12  7  0\n#> 6 NA 19  36  5  1\n\nlibrary(\"mice\") # Load mice package\n\n##### Impute data via predictive mean matching (single imputation)#####\n\nimp_single <- mice::mice(data, m = 1, method = \"pmm\") # Impute missing values\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_imp_single <- mice::complete(imp_single)         # Store imputed data\n# head(data_imp_single)\n\n# Since single imputation underestiamtes stnadard errors, \n# we use multiple imputaiton\n\n##### Predictive mean matching (multiple imputation) #####\n\n# Impute missing values multiple times\nimp_multi <- mice(data, m = 5, method = \"pmm\")  \n#> \n#>  iter imp variable\n#>   1   1  y\n#>   1   2  y\n#>   1   3  y\n#>   1   4  y\n#>   1   5  y\n#>   2   1  y\n#>   2   2  y\n#>   2   3  y\n#>   2   4  y\n#>   2   5  y\n#>   3   1  y\n#>   3   2  y\n#>   3   3  y\n#>   3   4  y\n#>   3   5  y\n#>   4   1  y\n#>   4   2  y\n#>   4   3  y\n#>   4   4  y\n#>   4   5  y\n#>   5   1  y\n#>   5   2  y\n#>   5   3  y\n#>   5   4  y\n#>   5   5  y\ndata_imp_multi_all <-\n    # Store multiply imputed data\n    mice::complete(imp_multi,       \n             \"repeated\",\n             include = TRUE)\n\ndata_imp_multi <-\n    # Combine imputed Y and X1-X4 (for convenience)\n    data.frame(data_imp_multi_all[, 1:6], data[, 2:5])\n\nhead(data_imp_multi)\n#>   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4\n#> 1  NA  -1   6  -1  -3   3 28 -10  5  0\n#> 2  NA -10  10   4   0   2 15  -2  2  1\n#> 3   1   1   1   1   1   1 15 -12  6  1\n#> 4   8   8   8   8   8   8 58  22 10  1\n#> 5  NA   0  -1  -6   2   0 26 -12  7  0\n#> 6  NA   4   0   3   3   3 19  36  5  1\nlibrary(mice)\nlibrary(VIM)\nlibrary(lattice)\nlibrary(ggplot2)\n## set observations to NA\nanscombe <- within(anscombe, {\n    y1[1:3] <- NA\n    y4[3:5] <- NA\n})\n## view\nhead(anscombe)\n#>   x1 x2 x3 x4   y1   y2    y3   y4\n#> 1 10 10 10  8   NA 9.14  7.46 6.58\n#> 2  8  8  8  8   NA 8.14  6.77 5.76\n#> 3 13 13 13  8   NA 8.74 12.74   NA\n#> 4  9  9  9  8 8.81 8.77  7.11   NA\n#> 5 11 11 11  8 8.33 9.26  7.81   NA\n#> 6 14 14 14  8 9.96 8.10  8.84 7.04\n\n## check missing data patterns\nmd.pattern(anscombe)#>   x1 x2 x3 x4 y2 y3 y1 y4  \n#> 6  1  1  1  1  1  1  1  1 0\n#> 2  1  1  1  1  1  1  1  0 1\n#> 2  1  1  1  1  1  1  0  1 1\n#> 1  1  1  1  1  1  1  0  0 2\n#>    0  0  0  0  0  0  3  3 6\n\n## Number of observations per patterns for all pairs of variables\np <- md.pairs(anscombe)\np \n#> $rr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1 11 11 11 11  8 11 11  8\n#> x2 11 11 11 11  8 11 11  8\n#> x3 11 11 11 11  8 11 11  8\n#> x4 11 11 11 11  8 11 11  8\n#> y1  8  8  8  8  8  8  8  6\n#> y2 11 11 11 11  8 11 11  8\n#> y3 11 11 11 11  8 11 11  8\n#> y4  8  8  8  8  6  8  8  8\n#> \n#> $rm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  3  0  0  3\n#> x2  0  0  0  0  3  0  0  3\n#> x3  0  0  0  0  3  0  0  3\n#> x4  0  0  0  0  3  0  0  3\n#> y1  0  0  0  0  0  0  0  2\n#> y2  0  0  0  0  3  0  0  3\n#> y3  0  0  0  0  3  0  0  3\n#> y4  0  0  0  0  2  0  0  0\n#> \n#> $mr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  3  3  3  3  0  3  3  2\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  3  3  3  3  2  3  3  0\n#> \n#> $mm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  0  0  0  0  3  0  0  1\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  0  0  0  0  1  0  0  3\n## Margin plot of y1 and y4\nmarginplot(anscombe[c(5, 8)], col = c(\"blue\", \"red\", \"orange\"))\n\n## 5 imputations for all missing values\nimp1 <- mice(anscombe, m = 5)\n#> \n#>  iter imp variable\n#>   1   1  y1  y4\n#>   1   2  y1  y4\n#>   1   3  y1  y4\n#>   1   4  y1  y4\n#>   1   5  y1  y4\n#>   2   1  y1  y4\n#>   2   2  y1  y4\n#>   2   3  y1  y4\n#>   2   4  y1  y4\n#>   2   5  y1  y4\n#>   3   1  y1  y4\n#>   3   2  y1  y4\n#>   3   3  y1  y4\n#>   3   4  y1  y4\n#>   3   5  y1  y4\n#>   4   1  y1  y4\n#>   4   2  y1  y4\n#>   4   3  y1  y4\n#>   4   4  y1  y4\n#>   4   5  y1  y4\n#>   5   1  y1  y4\n#>   5   2  y1  y4\n#>   5   3  y1  y4\n#>   5   4  y1  y4\n#>   5   5  y1  y4\n\n## linear regression for each imputed data set - 5 regression are run\nfitm <- with(imp1, lm(y1 ~ y4 + x1))\nsummary(fitm)\n#> # A tibble: 15 × 7\n#>    term        estimate std.error statistic p.value  nobs df.residual\n#>    <chr>          <dbl>     <dbl>     <dbl>   <dbl> <int>       <dbl>\n#>  1 (Intercept)    7.33      2.44       3.01  0.0169    11           8\n#>  2 y4            -0.416     0.223     -1.86  0.0996    11           8\n#>  3 x1             0.371     0.141      2.63  0.0302    11           8\n#>  4 (Intercept)    7.27      2.90       2.51  0.0365    11           8\n#>  5 y4            -0.435     0.273     -1.59  0.150     11           8\n#>  6 x1             0.387     0.160      2.41  0.0422    11           8\n#>  7 (Intercept)    6.54      2.80       2.33  0.0479    11           8\n#>  8 y4            -0.322     0.255     -1.26  0.243     11           8\n#>  9 x1             0.362     0.156      2.32  0.0491    11           8\n#> 10 (Intercept)    5.93      3.08       1.92  0.0907    11           8\n#> 11 y4            -0.286     0.282     -1.02  0.339     11           8\n#> 12 x1             0.418     0.176      2.37  0.0451    11           8\n#> 13 (Intercept)    8.16      2.67       3.05  0.0158    11           8\n#> 14 y4            -0.489     0.251     -1.95  0.0867    11           8\n#> 15 x1             0.326     0.151      2.17  0.0622    11           8\n\n## pool coefficients and standard errors across all 5 regression models\nmice::pool(fitm)\n#> Class: mipo    m = 5 \n#>          term m   estimate       ubar           b          t dfcom       df\n#> 1 (Intercept) 5  7.0445966 7.76794670 0.719350800 8.63116766     8 5.805314\n#> 2          y4 5 -0.3896685 0.06634920 0.006991497 0.07473900     8 5.706243\n#> 3          x1 5  0.3727865 0.02473847 0.001134293 0.02609962     8 6.178032\n#>          riv     lambda       fmi\n#> 1 0.11112601 0.10001207 0.3044313\n#> 2 0.12644909 0.11225460 0.3161877\n#> 3 0.05502168 0.05215218 0.2586992\n\n## output parameter estimates\nsummary(mice::pool(fitm))\n#>          term   estimate std.error statistic       df    p.value\n#> 1 (Intercept)  7.0445966 2.9378849  2.397846 5.805314 0.05483678\n#> 2          y4 -0.3896685 0.2733843 -1.425350 5.706243 0.20638512\n#> 3          x1  0.3727865 0.1615538  2.307508 6.178032 0.05923999"},{"path":"imputation-missing-data.html","id":"stochastic-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.4.2 Stochastic Imputation","text":"Stochastic Imputation enhancement regression imputation introduces randomness imputation process adding random residual predicted values regression model. approach aims retain variability original data reducing bias introduced deterministic regression imputation.Stochastic Imputation can described :\\[\n\\text{Imputed Value} = \\text{Predicted Value (regression)} + \\text{Random Residual}\n\\]method commonly used foundation multiple imputation techniques.Advantages Stochastic ImputationRetains benefits regression imputation:\nPreserves relationships variables dataset.\nUtilizes information observed data inform imputations.\nPreserves relationships variables dataset.Utilizes information observed data inform imputations.Introduces randomness:\nAdds variability including random residual term, making imputed values realistic better representing uncertainty missing data.\nAdds variability including random residual term, making imputed values realistic better representing uncertainty missing data.Supports multiple imputation:\ngenerating different random residuals iteration, facilitates creation multiple plausible datasets robust statistical analysis.\ngenerating different random residuals iteration, facilitates creation multiple plausible datasets robust statistical analysis.Disadvantages Stochastic ImputationImplausible values:\nDepending random residuals, imputed values may fall outside plausible range (e.g., negative values variables like age income).\nDepending random residuals, imputed values may fall outside plausible range (e.g., negative values variables like age income).handle heteroskedasticity:\ndata exhibit heteroskedasticity (.e., non-constant variance residuals), randomness added stochastic imputation may accurately reflect underlying variability.\ndata exhibit heteroskedasticity (.e., non-constant variance residuals), randomness added stochastic imputation may accurately reflect underlying variability.Steps Stochastic ImputationFit regression model using cases complete data variable missing values.Predict missing values using fitted model.Generate random residuals based distribution residuals regression model.Add random residuals predicted values impute missing values.NotesMultiple Imputation: multiple imputation methods extensions stochastic regression imputation. repeating imputation process different random seeds, multiple datasets can generated account uncertainty imputed values.Multiple Imputation: multiple imputation methods extensions stochastic regression imputation. repeating imputation process different random seeds, multiple datasets can generated account uncertainty imputed values.Dealing Implausible Values: Additional constraints transformations (e.g., truncating imputed values plausible range) may necessary address issue implausible values.Dealing Implausible Values: Additional constraints transformations (e.g., truncating imputed values plausible range) may necessary address issue implausible values.Comparison Deterministic Regression ImputationSingle stochastic regression imputationSingle predictive mean matchingStochastic regression imputation contains negative valuesEvidence heteroskadastic dataSingle stochastic regression imputationSingle predictive mean matchingComparison predictive mean matching stochastic regression imputation","code":"\n# Example dataset with missing values\nset.seed(123)\ndata <- data.frame(\n  X = rnorm(10, mean = 50, sd = 10),\n  Y = c(100, 105, 110, NA, 120, NA, 130, 135, 140, NA)\n)\n\n# Perform stochastic imputation\nstochastic_impute <- function(data, predictor, target) {\n  # Subset data with complete cases\n  complete_data <- data[!is.na(data[[target]]), ]\n  \n  # Fit a regression model\n  model <- lm(as.formula(paste(target, \"~\", predictor)), data = complete_data)\n  \n  # Predict missing values\n  missing_data <- data[is.na(data[[target]]), ]\n  predictions <- predict(model, newdata = missing_data)\n  \n  # Add random residuals\n  residual_sd <- sd(model$residuals, na.rm = TRUE)\n  stochastic_values <- predictions + rnorm(length(predictions), mean = 0, sd = residual_sd)\n  \n  # Impute missing values\n  data[is.na(data[[target]]), target] <- stochastic_values\n  return(data)\n}\n\n# Apply stochastic imputation\nimputed_data <- stochastic_impute(data, predictor = \"X\", target = \"Y\")\n\n# Display the imputed dataset\nprint(imputed_data)\n#>           X        Y\n#> 1  44.39524 100.0000\n#> 2  47.69823 105.0000\n#> 3  65.58708 110.0000\n#> 4  50.70508 137.0070\n#> 5  51.29288 120.0000\n#> 6  67.15065 114.9107\n#> 7  54.60916 130.0000\n#> 8  37.34939 135.0000\n#> 9  43.13147 140.0000\n#> 10 45.54338 127.9359\n# Income data\nset.seed(1)                              # Set seed\nN <- 1000                                    # Sample size\n\nincome <-\n  round(rnorm(N, 0, 500))            # Create some synthetic income data\nincome[income < 0] <- income[income < 0] * (-1)\n\nx1 <- income + rnorm(N, 1000, 1500)          # Auxiliary variables\nx2 <- income + rnorm(N,-5000, 2000)\n\n\n# Create 10% missingness in income\nincome[rbinom(N, 1, 0.1) == 1] <- NA\n\ndata_inc_miss <- data.frame(income, x1, x2)\nimp_inc_sri  <- mice(data_inc_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_sri <- mice::complete(imp_inc_sri)\nimp_inc_pmm  <- mice(data_inc_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_pmm <- mice::complete(imp_inc_pmm)\ndata_inc_sri$income[data_inc_sri$income < 0]\n#>  [1]  -23.85404  -58.37790  -61.86396  -57.47909  -21.29221  -73.26549\n#>  [7]  -61.76194  -42.45942 -351.02991 -317.69090\n# No values below 0\ndata_inc_pmm$income[data_inc_pmm$income < 0] \n#> numeric(0)\n# Heteroscedastic data\n \nset.seed(1)                             # Set seed\nN <- 1:1000                                  # Sample size\n \na <- 0\nb <- 1\nsigma2 <- N^2\neps <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n \ny <- a + b * N + eps                         # Heteroscedastic variable\nx <- 30 * N + rnorm(N[length(N)], 1000, 200) # Correlated variable\n \ny[rbinom(N[length(N)], 1, 0.3) == 1] <- NA   # 30% missing\n \ndata_het_miss <- data.frame(y, x)\nimp_het_sri  <- mice(data_het_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_sri <- mice::complete(imp_het_sri)\nimp_het_pmm  <- mice(data_het_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_pmm <- mice::complete(imp_het_pmm)\npar(mfrow = c(1, 2))                              # Both plots in one graphic\n\n# Plot of observed values\nplot(x[!is.na(data_het_sri$y)],\n     data_het_sri$y[!is.na(data_het_sri$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n# Plot of missing values\npoints(x[is.na(y)], data_het_sri$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Stochastic Regression Imputation\",        \n      line = 0.5)\n\n# Regression line\nabline(lm(y ~ x, data_het_sri),                   \n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\n# Plot of observed values\nplot(x[!is.na(data_het_pmm$y)],\n     data_het_pmm$y[!is.na(data_het_pmm$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n\n\n# Plot of missing values\npoints(x[is.na(y)], data_het_pmm$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Predictive Mean Matching\",\n      line = 0.5)\nabline(lm(y ~ x, data_het_pmm),\n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\nmtext(\n  \"Imputation of Heteroscedastic Data\",\n  # Main title of plot\n  side = 3,\n  line = -1.5,\n  outer = TRUE,\n  cex = 2\n)"},{"path":"imputation-missing-data.html","id":"matrix-completion","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.5 Matrix Completion","text":"Matrix completion method used impute missing data feature matrix accounting dependence features. approach leverages principal components approximate data matrix, process referred matrix completion (James et al. 2013, Sec 12.3).Problem SetupConsider \\(n \\times p\\) feature matrix \\(\\mathbf{X}\\), element \\(x_{ij}\\) represents value \\(\\)th observation \\(j\\)th feature. elements \\(\\mathbf{X}\\) missing, aim impute missing values.matrix \\(\\mathbf{X}\\) can approximated using leading principal components. Specifically, consider \\(M\\) principal components minimize following objective:\\[\n\\underset{\\mathbf{} \\\\mathbb{R}^{n \\times M}, \\mathbf{B} \\\\mathbb{R}^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\]\\(\\mathcal{O}\\) set observed indices \\((,j)\\), subset total \\(n \\times p\\) pairs. : - \\(\\mathbf{}\\) \\(n \\times M\\) matrix principal component scores. - \\(\\mathbf{B}\\) \\(p \\times M\\) matrix principal component loadings.Imputation Missing ValuesAfter solving minimization problem:Missing observations \\(x_{ij}\\) can imputed using formula: \\[\n   \\hat{x}_{ij} = \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n   \\] \\(\\hat{}_{im}\\) \\(\\hat{b}_{jm}\\) estimated elements \\(\\mathbf{}\\) \\(\\mathbf{B}\\), respectively.leading \\(M\\) principal component scores loadings can approximately recovered, done complete data scenarios.Iterative AlgorithmThe eigen-decomposition used standard principal component analysis applicable missing values. Instead, iterative algorithm, described (James et al. 2013, Alg 12.1), employed:Initialize Complete Matrix: Construct initial complete matrix \\(\\tilde{\\mathbf{X}}\\) dimension \\(n \\times p\\) : \\[\n\\tilde{x}_{ij} =\n\\begin{cases}\nx_{ij} & \\text{} (,j) \\\\mathcal{O} \\\\\n\\bar{x}_j & \\text{} (,j) \\notin \\mathcal{O}\n\\end{cases}\n\\] , \\(\\bar{x}_j\\) mean observed values \\(j\\)th variable incomplete data matrix \\(\\mathbf{X}\\). \\(\\mathcal{O}\\) indexes observed elements \\(\\mathbf{X}\\).Initialize Complete Matrix: Construct initial complete matrix \\(\\tilde{\\mathbf{X}}\\) dimension \\(n \\times p\\) : \\[\n\\tilde{x}_{ij} =\n\\begin{cases}\nx_{ij} & \\text{} (,j) \\\\mathcal{O} \\\\\n\\bar{x}_j & \\text{} (,j) \\notin \\mathcal{O}\n\\end{cases}\n\\] , \\(\\bar{x}_j\\) mean observed values \\(j\\)th variable incomplete data matrix \\(\\mathbf{X}\\). \\(\\mathcal{O}\\) indexes observed elements \\(\\mathbf{X}\\).Iterative Steps: Repeat following steps convergence:\nMinimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).\nUpdate Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]\nRecalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]\nIterative Steps: Repeat following steps convergence:Minimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).Minimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).Update Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]Update Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]Recalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]Recalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]Return Imputed Values: algorithm converges, return estimated missing entries \\(\\tilde{x}_{ij}\\) \\((,j) \\notin \\mathcal{O}\\).Return Imputed Values: algorithm converges, return estimated missing entries \\(\\tilde{x}_{ij}\\) \\((,j) \\notin \\mathcal{O}\\).Key ConsiderationsThis approach assumes missing data missing random (MAR).Convergence criteria iterative algorithm often involve achieving threshold change objective function limiting number iterations.choice \\(M\\), number principal components, can guided cross-validation model selection techniques.","code":""},{"path":"imputation-missing-data.html","id":"comparison-of-single-imputation-techniques","chapter":"13 Imputation (Missing Data)","heading":"13.4.2.6 Comparison of Single Imputation Techniques","text":"Single imputation techniques straightforward accessible, often underestimate uncertainty fail fully leverage relationships among variables. limitations make less ideal rigorous analyses compared multiple imputation model-based approaches.","code":""},{"path":"imputation-missing-data.html","id":"machine-learning-and-modern-approaches","chapter":"13 Imputation (Missing Data)","heading":"13.4.3 Machine Learning and Modern Approaches","text":"","code":""},{"path":"imputation-missing-data.html","id":"tree-based-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.1 Tree-Based Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"random-forest-imputation-missforest","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.1.1 Random Forest Imputation (missForest)","text":"Random Forest Imputation uses iterative process random forest model predicts missing values one variable time, treating variables predictors. process continues convergence.Mathematical Framework:\nvariable \\(X_j\\) missing values, treat \\(X_j\\) response variable.\nFit random forest model \\(f(X_{-j})\\) using variables \\(X_{-j}\\) predictors.\nPredict missing values \\(\\hat{X}_j = f(X_{-j})\\).\nRepeat variables missing data imputed values stabilize.\nvariable \\(X_j\\) missing values, treat \\(X_j\\) response variable.Fit random forest model \\(f(X_{-j})\\) using variables \\(X_{-j}\\) predictors.Predict missing values \\(\\hat{X}_j = f(X_{-j})\\).Repeat variables missing data imputed values stabilize.Advantages:\nCaptures complex interactions non-linearities.\nHandles mixed data types seamlessly.\nCaptures complex interactions non-linearities.Handles mixed data types seamlessly.Limitations:\nComputationally intensive large datasets.\nSensitive quality data relationships.\nComputationally intensive large datasets.Sensitive quality data relationships.","code":""},{"path":"imputation-missing-data.html","id":"gradient-boosting-machines-gbm","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.1.2 Gradient Boosting Machines (GBM)","text":"Gradient Boosting Machines iteratively build models minimize loss functions. imputation, missing values treated target variable predicted.Mathematical Framework: GBM algorithm minimizes loss function: \\[\n  L = \\sum_{=1}^n \\ell(y_i, f(x_i)),\n  \\] \\(\\ell\\) loss function (e.g., mean squared error), \\(y_i\\) observed values, \\(f(x_i)\\) predictions.Mathematical Framework: GBM algorithm minimizes loss function: \\[\n  L = \\sum_{=1}^n \\ell(y_i, f(x_i)),\n  \\] \\(\\ell\\) loss function (e.g., mean squared error), \\(y_i\\) observed values, \\(f(x_i)\\) predictions.Missing values treated \\(y_i\\) predicted iteratively.Missing values treated \\(y_i\\) predicted iteratively.Advantages:\nHighly accurate predictions.\nCaptures variable importance.\nAdvantages:Highly accurate predictions.Captures variable importance.Limitations:\nOverfitting risks.\nRequires careful parameter tuning.\nLimitations:Overfitting risks.Requires careful parameter tuning.","code":""},{"path":"imputation-missing-data.html","id":"neural-network-based-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.2 Neural Network-Based Imputation","text":"","code":""},{"path":"imputation-missing-data.html","id":"autoencoders","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.2.1 Autoencoders","text":"Autoencoders unsupervised neural networks compress reconstruct data. Missing values estimated reconstruction.Mathematical Framework: autoencoder consists :\nencoder function: \\(h = g(Wx + b)\\), compresses input \\(x\\).\ndecoder function: \\(\\hat{x} = g'(W'h + b')\\), reconstructs data.\nMathematical Framework: autoencoder consists :encoder function: \\(h = g(Wx + b)\\), compresses input \\(x\\).decoder function: \\(\\hat{x} = g'(W'h + b')\\), reconstructs data.network minimizes reconstruction loss: \\[\n  L = \\sum_{=1}^n (x_i - \\hat{x}_i)^2.\n  \\]network minimizes reconstruction loss: \\[\n  L = \\sum_{=1}^n (x_i - \\hat{x}_i)^2.\n  \\]Advantages:\nHandles high-dimensional non-linear data.\nUnsupervised learning.\nAdvantages:Handles high-dimensional non-linear data.Unsupervised learning.Limitations:\nComputationally demanding.\nRequires large datasets effective training.\nLimitations:Computationally demanding.Requires large datasets effective training.","code":""},{"path":"imputation-missing-data.html","id":"generative-adversarial-networks-gans-for-data-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.2.2 Generative Adversarial Networks (GANs) for Data Imputation","text":"GANs consist generator discriminator. imputation, generator fills missing values, discriminator evaluates quality imputations.Mathematical Framework: GAN training involves optimizing: \\[\n  \\min_G \\max_D \\mathbb{E}[\\log D(x)] + \\mathbb{E}[\\log(1 - D(G(z)))].\n  \\]\n\\(D(x)\\): Discriminator’s probability \\(x\\) real.\n\\(G(z)\\): Generator’s output latent input \\(z\\).\n\\(D(x)\\): Discriminator’s probability \\(x\\) real.\\(G(z)\\): Generator’s output latent input \\(z\\).Advantages:\nRealistic imputations reflect underlying distributions.\nHandles complex data types.\nRealistic imputations reflect underlying distributions.Handles complex data types.Limitations:\nDifficult train tune.\nComputationally intensive.\nDifficult train tune.Computationally intensive.","code":""},{"path":"imputation-missing-data.html","id":"matrix-factorization-and-matrix-completion","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.3 Matrix Factorization and Matrix Completion","text":"","code":""},{"path":"imputation-missing-data.html","id":"singular-value-decomposition-svd","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.3.1 Singular Value Decomposition (SVD)","text":"SVD decomposes matrix \\(\\) three matrices: \\[\n= U\\Sigma V^T,\n\\] \\(U\\) \\(V\\) orthogonal matrices, \\(\\Sigma\\) contains singular values. Missing values estimated reconstructing \\(\\) using low-rank approximation: \\[\n\\hat{} = U_k \\Sigma_k V_k^T.\n\\]Advantages:\nCaptures global patterns.\nEfficient structured data.\nCaptures global patterns.Efficient structured data.Limitations:\nAssumes linear relationships.\nSensitive sparsity.\nAssumes linear relationships.Sensitive sparsity.","code":""},{"path":"imputation-missing-data.html","id":"collaborative-filtering-approaches","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.3.2 Collaborative Filtering Approaches","text":"Collaborative filtering uses similarities rows (users) columns (items) impute missing data. instance, value \\(X_{ij}\\) predicted : \\[\n\\hat{X}_{ij} = \\frac{\\sum_{k \\N()} w_{ik} X_{kj}}{\\sum_{k \\N()} w_{ik}},\n\\] \\(w_{ik}\\) represents similarity weights \\(N()\\) set neighbors.","code":""},{"path":"imputation-missing-data.html","id":"k-nearest-neighbor-knn-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.4 K-Nearest Neighbor (KNN) Imputation","text":"KNN identifies \\(k\\) nearest observations based distance metric imputes missing values using weighted average (continuous variables) mode (categorical variables).Mathematical Framework: missing value \\(x\\), imputed value : \\[\n  \\hat{x} = \\frac{\\sum_{=1}^k w_i x_i}{\\sum_{=1}^k w_i},\n  \\] \\(w_i = \\frac{1}{d(x, x_i)}\\) \\(d(x, x_i)\\) distance metric (e.g., Euclidean Manhattan).Mathematical Framework: missing value \\(x\\), imputed value : \\[\n  \\hat{x} = \\frac{\\sum_{=1}^k w_i x_i}{\\sum_{=1}^k w_i},\n  \\] \\(w_i = \\frac{1}{d(x, x_i)}\\) \\(d(x, x_i)\\) distance metric (e.g., Euclidean Manhattan).Advantages:\nSimple interpretable.\nNon-parametric.\nAdvantages:Simple interpretable.Non-parametric.Limitations:\nComputationally expensive large datasets.\nLimitations:Computationally expensive large datasets.","code":""},{"path":"imputation-missing-data.html","id":"hybrid-methods","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.5 Hybrid Methods","text":"Hybrid methods combine statistical machine learning approaches. example, mean imputation followed fine-tuning machine learning models. methods aim leverage strengths multiple techniques.","code":""},{"path":"imputation-missing-data.html","id":"summary-table","chapter":"13 Imputation (Missing Data)","heading":"13.4.3.6 Summary Table","text":"","code":""},{"path":"imputation-missing-data.html","id":"multiple-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.4 Multiple Imputation","text":"Multiple Imputation (MI) statistical technique handling missing data creating several plausible datasets imputation, analyzing dataset separately, combining results account uncertainty imputations. MI operates assumption missing data either Missing Completely Random (MCAR) Missing Random (MAR).Unlike Single Imputation Techniques, MI reflects uncertainty inherent missing data introducing variability imputed values. avoids biases introduced ad hoc methods produces reliable statistical inferences.three fundamental steps MI :Imputation: Replace missing values set plausible values create multiple “completed” datasets.Analysis: Perform desired statistical analysis imputed dataset.Combination: Combine results using rules account within- -imputation variability.","code":""},{"path":"imputation-missing-data.html","id":"why-multiple-imputation-is-important","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.1 Why Multiple Imputation is Important","text":"Imputed values estimates inherently include random error. However, estimates treated exact values subsequent analysis, software may overlook additional error. oversight results underestimated standard errors overly small p-values, leading misleading conclusions.Multiple imputation addresses issue generating multiple estimates missing value. estimates differ slightly due random component, reintroduces variation. variation helps software incorporate uncertainty imputed values, resulting :Unbiased parameter estimatesUnbiased parameter estimatesAccurate standard errorsAccurate standard errorsImproved p-valuesImproved p-valuesMultiple imputation significant breakthrough statistics approximately 20 years ago. provides solutions many missing data issues (though ) , applied correctly, leads reliable parameter estimates.proportion missing data small (e.g., 2-3%), choice imputation method less critical.","code":""},{"path":"imputation-missing-data.html","id":"goals-of-multiple-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.2 Goals of Multiple Imputation","text":"primary goals missing data technique, including multiple imputation, :Unbiased parameter estimates: Ensuring accurate regression coefficients, group means, odds ratios, etc.Unbiased parameter estimates: Ensuring accurate regression coefficients, group means, odds ratios, etc.Accurate standard errors: leads reliable p-values appropriate statistical inferences.Accurate standard errors: leads reliable p-values appropriate statistical inferences.Adequate power: detect meaningful significant parameter values.Adequate power: detect meaningful significant parameter values.","code":""},{"path":"imputation-missing-data.html","id":"overview-of-rubins-framework","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.3 Overview of Rubin’s Framework","text":"Rubin’s Framework provides theoretical foundation MI. uses Bayesian model-based approach generating imputations frequentist approach evaluating results. central goals Rubin’s framework ensure imputations:Retain statistical relationships present data.Reflect uncertainty true values missing data.Rubin’s framework, MI offers following advantages:Generalizability: Unlike Maximum Likelihood Estimation (MLE), MI can applied wide range models.Statistical Properties: data MAR MCAR, MI estimates consistent, asymptotically normal, efficient.Rubin also emphasized importance using multiple imputations, single imputations fail account variability imputed values, leading underestimated standard errors overly optimistic test statistics.","code":""},{"path":"imputation-missing-data.html","id":"multivariate-imputation-via-chained-equations-mice","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.4 Multivariate Imputation via Chained Equations (MICE)","text":"Multivariate Imputation via Chained Equations (MICE) widely used algorithm implementing MI, particularly datasets mixed variable types. steps MICE include:Initialization: Replace missing values initial guesses, mean median observed data.Iterative Imputation:\nvariable missing values, regress variables (subset relevant predictors).\nUse regression model predict missing values, adding random error term drawn residual distribution.\nvariable missing values, regress variables (subset relevant predictors).Use regression model predict missing values, adding random error term drawn residual distribution.Convergence: Repeat imputation process parameter estimates stabilize.MICE offers flexibility specifying regression models variable, accommodating continuous, categorical, binary data.","code":""},{"path":"imputation-missing-data.html","id":"bayesian-ridge-regression-for-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.5 Bayesian Ridge Regression for Imputation","text":"Bayesian ridge regression advanced imputation method incorporates prior distributions regression coefficients, making particularly useful :Predictors highly correlated.Sample sizes small.Missingness substantial.method treats regression coefficients random variables samples posterior distribution, introducing variability imputation process. Bayesian ridge regression computationally intensive simpler methods like MICE offers greater robustness.","code":""},{"path":"imputation-missing-data.html","id":"combining-results-from-mi-rubins-rules","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.6 Combining Results from MI (Rubin’s Rules)","text":"multiple datasets imputed analyzed, Rubin’s Rules used combine results. goal properly account uncertainty introduced missing data. parameter interest \\(\\theta\\):Estimate Combination: \\[\n\\bar{\\theta} = \\frac{1}{M} \\sum_{m=1}^M \\theta_m\n\\] \\(\\theta_m\\) estimate \\(m\\)th imputed dataset, \\(M\\) number imputations.Estimate Combination: \\[\n\\bar{\\theta} = \\frac{1}{M} \\sum_{m=1}^M \\theta_m\n\\] \\(\\theta_m\\) estimate \\(m\\)th imputed dataset, \\(M\\) number imputations.Variance Combination: \\[\nT = \\bar{W} + \\left(1 + \\frac{1}{M}\\right) B\n\\] :\n\\(\\bar{W}\\) average within-imputation variance.\n\\(B\\) -imputation variance: \\[\nB = \\frac{1}{M-1} \\sum_{m=1}^M (\\theta_m - \\bar{\\theta})^2\n\\]\nVariance Combination: \\[\nT = \\bar{W} + \\left(1 + \\frac{1}{M}\\right) B\n\\] :\\(\\bar{W}\\) average within-imputation variance.\\(B\\) -imputation variance: \\[\nB = \\frac{1}{M-1} \\sum_{m=1}^M (\\theta_m - \\bar{\\theta})^2\n\\]formulas adjust final variance reflect uncertainty within across imputations.","code":""},{"path":"imputation-missing-data.html","id":"challenges","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.6.1 Challenges","text":"Stochastic Variability: MI results vary slightly runs due reliance random draws. ensure reproducibility, always set random seed.Convergence: Iterative algorithms like MICE may struggle converge, especially high proportions missing data.Assumption MAR: MI assumes missing data MAR. data Missing Random (MNAR), MI can produce biased results.","code":""},{"path":"imputation-missing-data.html","id":"best-practices","chapter":"13 Imputation (Missing Data)","heading":"13.4.4.6.2 Best Practices","text":"Algorithm Selection:\nUse Multiple Imputation Chained Equations (MICE) datasets mixed data types relationships variables complex.\nApply Bayesian Ridge Regression small datasets predictors highly correlated.\nUse Multiple Imputation Chained Equations (MICE) datasets mixed data types relationships variables complex.Apply Bayesian Ridge Regression small datasets predictors highly correlated.Diagnostic Checks:\nEvaluate quality imputations assess convergence using trace plots diagnostic statistics ensure reliable results.\nEvaluate quality imputations assess convergence using trace plots diagnostic statistics ensure reliable results.Data Transformations:\nskewed proportion data, consider applying log logit transformations imputation inverse-transforming afterward preserve data’s original scale.\nskewed proportion data, consider applying log logit transformations imputation inverse-transforming afterward preserve data’s original scale.Handling Non-Linear Relationships:\nnon-linear relationships interactions, stratify imputations levels categorical variable involved ensure accurate estimates.\nnon-linear relationships interactions, stratify imputations levels categorical variable involved ensure accurate estimates.Number Imputations:\nUse least 20 imputations small datasets datasets high missingness. ensures robust reliable results downstream analyses.\nUse least 20 imputations small datasets datasets high missingness. ensures robust reliable results downstream analyses.Avoid Rounding Imputations Dummy Variables:\nMany imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even dummy variables. historically recommended round imputed values 0 1 binary variables, research shows introduces bias parameter estimates. Instead, leave imputed values fractional, even though may seem counter-intuitive.\nMany imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even dummy variables. historically recommended round imputed values 0 1 binary variables, research shows introduces bias parameter estimates. Instead, leave imputed values fractional, even though may seem counter-intuitive.Transform Skewed Variables Imputation:\nTransforming variables meet normality assumptions imputation can distort relationships variables, leading biased imputations possibly introducing outliers. better directly impute skewed variable.\nTransforming variables meet normality assumptions imputation can distort relationships variables, leading biased imputations possibly introducing outliers. better directly impute skewed variable.Use Imputations:\nTraditional advice suggests 5–10 imputations sufficient unbiased estimates, inconsistencies may arise repeated analyses. [@Bodner_2008] suggests using number imputations equal percentage missing data. additional imputations generally significantly increase computational workload, using imputations prudent choice.\nTraditional advice suggests 5–10 imputations sufficient unbiased estimates, inconsistencies may arise repeated analyses. [@Bodner_2008] suggests using number imputations equal percentage missing data. additional imputations generally significantly increase computational workload, using imputations prudent choice.Create Multiplicative Terms Imputation:\nmodel includes interaction quadratic terms, generate terms imputing missing values. Imputing first generating terms can introduce bias regression parameters, highlighted (Von Hippel 2009).\nmodel includes interaction quadratic terms, generate terms imputing missing values. Imputing first generating terms can introduce bias regression parameters, highlighted (Von Hippel 2009).","code":""},{"path":"imputation-missing-data.html","id":"evaluation-of-imputation-methods","chapter":"13 Imputation (Missing Data)","heading":"13.5 Evaluation of Imputation Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"statistical-metrics-for-assessing-imputation-quality","chapter":"13 Imputation (Missing Data)","heading":"13.5.1 Statistical Metrics for Assessing Imputation Quality","text":"evaluate quality imputed data, several statistical metrics commonly used. metrics compare imputed values observed values (cases missingness simulated artificially introduced) assess overall impact imputation quality subsequent analyses. Key metrics include:Root Mean Squared Error (RMSE): RMSE calculated : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2}\n\\] measures average magnitude errors true imputed values. Lower RMSE indicates better imputation accuracy.Root Mean Squared Error (RMSE): RMSE calculated : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2}\n\\] measures average magnitude errors true imputed values. Lower RMSE indicates better imputation accuracy.Mean Absolute Error (MAE): MAE measures average absolute difference observed imputed values: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\] MAE provides straightforward assessment imputation performance less sensitive outliers RMSE.Mean Absolute Error (MAE): MAE measures average absolute difference observed imputed values: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\] MAE provides straightforward assessment imputation performance less sensitive outliers RMSE.Log-Likelihood Deviance Measures: Log-likelihood can used evaluate well imputation model fits data. Deviance measures, based likelihood comparisons, assess relative goodness fit imputation models. particularly useful evaluating methods like maximum likelihood estimation.Log-Likelihood Deviance Measures: Log-likelihood can used evaluate well imputation model fits data. Deviance measures, based likelihood comparisons, assess relative goodness fit imputation models. particularly useful evaluating methods like maximum likelihood estimation.practice, metrics may combined graphical methods density plots residual analysis understand imputation performance thoroughly.","code":""},{"path":"imputation-missing-data.html","id":"bias-variance-tradeoff-in-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.5.2 Bias-Variance Tradeoff in Imputation","text":"Imputation methods must balance bias variance achieve reliable results. Simpler methods, mean mode imputation, often lead biased parameter estimates, particularly missingness mechanism non-random. methods underestimate variability, shrinking standard errors potentially leading overconfidence statistical inferences.Conversely, advanced methods like Multiple Imputation Full Information Maximum Likelihood (FIML) typically yield unbiased estimates appropriately calibrated variances. However, methods may increase computational complexity require careful tuning assumptions parameters.tradeoff summarized follows:High Bias, Low Variance: Simpler methods (e.g., single imputation, mean imputation).High Bias, Low Variance: Simpler methods (e.g., single imputation, mean imputation).Low Bias, Moderate Variance: Advanced methods (e.g., MI, FIML, Bayesian methods).Low Bias, Moderate Variance: Advanced methods (e.g., MI, FIML, Bayesian methods).","code":""},{"path":"imputation-missing-data.html","id":"sensitivity-analysis-1","chapter":"13 Imputation (Missing Data)","heading":"13.5.3 Sensitivity Analysis","text":"Sensitivity analysis crucial assess robustness imputation methods varying assumptions. Two primary areas focus include:Assessing Robustness Assumptions: Imputation models often rely assumptions missingness mechanism (See Definition Classification Missing Data). Sensitivity analysis involves testing results vary assumptions slightly relaxed modified.Assessing Robustness Assumptions: Imputation models often rely assumptions missingness mechanism (See Definition Classification Missing Data). Sensitivity analysis involves testing results vary assumptions slightly relaxed modified.Impact Downstream Analysis: quality imputation also evaluated based influence downstream analyses (Objectives Imputation). instance:\nimputation affect causal inference regression models?\nconclusions hypothesis testing predictive modeling robust imputation technique?\nImpact Downstream Analysis: quality imputation also evaluated based influence downstream analyses (Objectives Imputation). instance:imputation affect causal inference regression models?conclusions hypothesis testing predictive modeling robust imputation technique?","code":""},{"path":"imputation-missing-data.html","id":"validation-using-simulated-data-and-real-world-case-studies","chapter":"13 Imputation (Missing Data)","heading":"13.5.4 Validation Using Simulated Data and Real-World Case Studies","text":"Validation imputation methods best performed combination simulated data real-world examples:Simulated Data: - Create datasets known missingness patterns true values. - Apply various imputation methods assess performance using RMSE, MAE, metrics.Real-World Case Studies:\nUse datasets actual studies, customer transaction data marketing financial data portfolio analysis.\nEvaluate impact imputation actionable outcomes (e.g., market segmentation, risk assessment).\nUse datasets actual studies, customer transaction data marketing financial data portfolio analysis.Evaluate impact imputation actionable outcomes (e.g., market segmentation, risk assessment).Combining approaches ensures methods generalize well across different contexts data structures.","code":""},{"path":"imputation-missing-data.html","id":"criteria-for-choosing-an-effective-approach","chapter":"13 Imputation (Missing Data)","heading":"13.6 Criteria for Choosing an Effective Approach","text":"Choosing appropriate imputation method depends following criteria:Unbiased Parameter Estimates: technique ensure key estimates, means, variances, regression coefficients, unbiased, particularly presence MAR MNAR data.Unbiased Parameter Estimates: technique ensure key estimates, means, variances, regression coefficients, unbiased, particularly presence MAR MNAR data.Adequate Power: method preserve statistical power, enabling robust hypothesis testing model estimation. ensures important effects missed due inflated type II error.Adequate Power: method preserve statistical power, enabling robust hypothesis testing model estimation. ensures important effects missed due inflated type II error.Accurate Standard Errors: Accurate estimation standard errors critical reliable p-values confidence intervals. Methods like single imputation often underestimate standard errors, leading overconfident conclusions.Accurate Standard Errors: Accurate estimation standard errors critical reliable p-values confidence intervals. Methods like single imputation often underestimate standard errors, leading overconfident conclusions.Preferred Methods: Multiple Imputation Full Information Maximum LikelihoodMultiple Imputation (MI):MI replaces missing values multiple plausible values drawn predictive distribution. generates multiple complete datasets, analyzes dataset, combines results.MI replaces missing values multiple plausible values drawn predictive distribution. generates multiple complete datasets, analyzes dataset, combines results.Pros: Handles uncertainty well, provides valid standard errors, robust MAR.Pros: Handles uncertainty well, provides valid standard errors, robust MAR.Cons: Computationally intensive, sensitive model mis-specification.Cons: Computationally intensive, sensitive model mis-specification.Full Information Maximum Likelihood (FIML):FIML uses available data estimate parameters directly, avoiding need impute missing values explicitly.FIML uses available data estimate parameters directly, avoiding need impute missing values explicitly.Pros: Efficient, unbiased MAR, computationally elegant.Pros: Efficient, unbiased MAR, computationally elegant.Cons: Requires correctly specified models may sensitive MNAR data.Cons: Requires correctly specified models may sensitive MNAR data.Methods AvoidSingle Imputation (e.g., Mean, Mode):\nLeads biased estimates underestimates variability.\nLeads biased estimates underestimates variability.Listwise Deletion:\nDiscards rows missing data, reducing sample size potentially introducing bias data MCAR.\nDiscards rows missing data, reducing sample size potentially introducing bias data MCAR.Practical ConsiderationsComputational efficiency ease implementation.Compatibility downstream analysis methods.Alignment data’s missingness mechanism.","code":""},{"path":"imputation-missing-data.html","id":"challenges-and-ethical-considerations","chapter":"13 Imputation (Missing Data)","heading":"13.7 Challenges and Ethical Considerations","text":"","code":""},{"path":"imputation-missing-data.html","id":"challenges-in-high-dimensional-data","chapter":"13 Imputation (Missing Data)","heading":"13.7.1 Challenges in High-Dimensional Data","text":"High-dimensional data, number variables exceeds number observations, poses unique challenges missing data analysis.Curse Dimensionality: Standard imputation methods, mean regression imputation, struggle high-dimensional spaces due sparse data distribution.Curse Dimensionality: Standard imputation methods, mean regression imputation, struggle high-dimensional spaces due sparse data distribution.Regularized Methods: Techniques LASSO, Ridge Regression, Elastic Net can used handle high-dimensional missing data. methods shrink model coefficients, preventing overfitting.Regularized Methods: Techniques LASSO, Ridge Regression, Elastic Net can used handle high-dimensional missing data. methods shrink model coefficients, preventing overfitting.Matrix Factorization: Methods like Principal Component Analysis (PCA) Singular Value Decomposition (SVD) often adapted impute missing values high-dimensional datasets reducing dimensionality first.Matrix Factorization: Methods like Principal Component Analysis (PCA) Singular Value Decomposition (SVD) often adapted impute missing values high-dimensional datasets reducing dimensionality first.","code":""},{"path":"imputation-missing-data.html","id":"missing-data-in-big-data-contexts","chapter":"13 Imputation (Missing Data)","heading":"13.7.2 Missing Data in Big Data Contexts","text":"advent big data introduces additional complexities missing data handling, including computational scalability storage constraints.","code":""},{"path":"imputation-missing-data.html","id":"distributed-imputation-techniques","chapter":"13 Imputation (Missing Data)","heading":"13.7.2.1 Distributed Imputation Techniques","text":"MapReduce Frameworks: Algorithms like k-nearest neighbor (KNN) imputation multiple imputation can adapted distributed environments using MapReduce similar frameworks.MapReduce Frameworks: Algorithms like k-nearest neighbor (KNN) imputation multiple imputation can adapted distributed environments using MapReduce similar frameworks.Federated Learning: scenarios data distributed across multiple locations (e.g., healthcare banking), federated learning allows imputation without centralizing data, ensuring privacy.Federated Learning: scenarios data distributed across multiple locations (e.g., healthcare banking), federated learning allows imputation without centralizing data, ensuring privacy.","code":""},{"path":"imputation-missing-data.html","id":"cloud-based-implementations","chapter":"13 Imputation (Missing Data)","heading":"13.7.2.2 Cloud-Based Implementations","text":"Cloud-Native Algorithms: Cloud platforms like AWS, Google Cloud, Azure provide scalable solutions implementing advanced imputation algorithms large datasets.Cloud-Native Algorithms: Cloud platforms like AWS, Google Cloud, Azure provide scalable solutions implementing advanced imputation algorithms large datasets.AutoML Integration: Automated Machine Learning (AutoML) pipelines often include missing data handling preprocessing step, leveraging cloud-based computational power.AutoML Integration: Automated Machine Learning (AutoML) pipelines often include missing data handling preprocessing step, leveraging cloud-based computational power.Real-Time Imputation: e-commerce, cloud-based solutions enable real-time imputation recommendation systems fraud detection, ensuring seamless user experiences.Real-Time Imputation: e-commerce, cloud-based solutions enable real-time imputation recommendation systems fraud detection, ensuring seamless user experiences.","code":""},{"path":"imputation-missing-data.html","id":"ethical-concerns","chapter":"13 Imputation (Missing Data)","heading":"13.7.3 Ethical Concerns","text":"","code":""},{"path":"imputation-missing-data.html","id":"bias-amplification","chapter":"13 Imputation (Missing Data)","heading":"13.7.3.1 Bias Amplification","text":"Introduction Systematic Bias: Imputation methods can inadvertently reinforce existing biases. example, imputing salary data based demographic variables may propagate societal inequalities.Introduction Systematic Bias: Imputation methods can inadvertently reinforce existing biases. example, imputing salary data based demographic variables may propagate societal inequalities.Business Implications: credit scoring, biased imputation missing financial data can lead unfair credit decisions, disproportionately affecting marginalized groups.Business Implications: credit scoring, biased imputation missing financial data can lead unfair credit decisions, disproportionately affecting marginalized groups.Mitigation Strategies: Techniques fairness-aware machine learning bias auditing can help identify reduce bias introduced imputation.Mitigation Strategies: Techniques fairness-aware machine learning bias auditing can help identify reduce bias introduced imputation.","code":""},{"path":"imputation-missing-data.html","id":"transparency-in-reporting-imputation-decisions","chapter":"13 Imputation (Missing Data)","heading":"13.7.3.2 Transparency in Reporting Imputation Decisions","text":"Reproducibility Documentation: Transparent reporting imputation methods assumptions essential reproducibility. Analysts provide clear documentation imputation pipeline.Reproducibility Documentation: Transparent reporting imputation methods assumptions essential reproducibility. Analysts provide clear documentation imputation pipeline.Stakeholder Communication: business settings, communicating imputation decisions stakeholders ensures informed decision-making trust results.Stakeholder Communication: business settings, communicating imputation decisions stakeholders ensures informed decision-making trust results.Ethical Frameworks: Ethical guidelines, provided European Union’s GDPR industry-specific codes, emphasize importance transparency data handling.Ethical Frameworks: Ethical guidelines, provided European Union’s GDPR industry-specific codes, emphasize importance transparency data handling.","code":""},{"path":"imputation-missing-data.html","id":"emerging-trends-in-missing-data-handling","chapter":"13 Imputation (Missing Data)","heading":"13.8 Emerging Trends in Missing Data Handling","text":"","code":""},{"path":"imputation-missing-data.html","id":"advances-in-neural-network-approaches","chapter":"13 Imputation (Missing Data)","heading":"13.8.1 Advances in Neural Network Approaches","text":"Neural networks transformed landscape missing data imputation, offering flexible, scalable, powerful solutions go beyond traditional methods.","code":""},{"path":"imputation-missing-data.html","id":"variational-autoencoders-vaes","chapter":"13 Imputation (Missing Data)","heading":"13.8.1.1 Variational Autoencoders (VAEs)","text":"Overview: Variational Autoencoders (VAEs) generative models encode data latent space reconstruct , filling missing values reconstruction.Overview: Variational Autoencoders (VAEs) generative models encode data latent space reconstruct , filling missing values reconstruction.Advantages:\nHandle complex, non-linear relationships variables.\nScalable high-dimensional datasets.\nGenerate probabilistic imputations, reflecting uncertainty.\nAdvantages:Handle complex, non-linear relationships variables.Scalable high-dimensional datasets.Generate probabilistic imputations, reflecting uncertainty.Applications:\nmarketing, VAEs can impute missing customer behavior data accounting seasonal demographic variations.\nfinance, VAEs assist imputing missing stock price data modeling dependencies among assets.\nApplications:marketing, VAEs can impute missing customer behavior data accounting seasonal demographic variations.finance, VAEs assist imputing missing stock price data modeling dependencies among assets.","code":""},{"path":"imputation-missing-data.html","id":"gans-for-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.8.1.2 GANs for Missing Data","text":"Generative Adversarial Networks (GANs): GANs consist generator discriminator, generator imputing missing data discriminator evaluating quality.Generative Adversarial Networks (GANs): GANs consist generator discriminator, generator imputing missing data discriminator evaluating quality.Advantages:\nPreserve data distributions avoid -smoothing.\nSuitable imputation datasets complex patterns multi-modal distributions.\nAdvantages:Preserve data distributions avoid -smoothing.Suitable imputation datasets complex patterns multi-modal distributions.Applications:\nhealthcare, GANs used impute missing patient records preserving patient privacy data integrity.\nretail, GANs can model missing sales data predict trends optimize inventory.\nApplications:healthcare, GANs used impute missing patient records preserving patient privacy data integrity.retail, GANs can model missing sales data predict trends optimize inventory.","code":""},{"path":"imputation-missing-data.html","id":"integration-with-reinforcement-learning","chapter":"13 Imputation (Missing Data)","heading":"13.8.2 Integration with Reinforcement Learning","text":"Reinforcement learning (RL) increasingly integrated missing data strategies, particularly dynamic sequential data environments.Markov Decision Processes (MDPs): RL models missing data handling MDP, actions (imputations) optimized based rewards (accuracy predictions decisions).Markov Decision Processes (MDPs): RL models missing data handling MDP, actions (imputations) optimized based rewards (accuracy predictions decisions).Active Imputation:\nRL can used actively query missing data points, prioritizing highest impact downstream tasks.\nExample: customer churn prediction, RL can optimize imputation high-value customer records.\nActive Imputation:RL can used actively query missing data points, prioritizing highest impact downstream tasks.Example: customer churn prediction, RL can optimize imputation high-value customer records.Applications:\nFinancial forecasting: RL models used impute missing transaction data dynamically, optimizing portfolio decisions.\nSmart cities: RL-based models handle missing sensor data enhance real-time decision-making traffic management.\nApplications:Financial forecasting: RL models used impute missing transaction data dynamically, optimizing portfolio decisions.Smart cities: RL-based models handle missing sensor data enhance real-time decision-making traffic management.","code":""},{"path":"imputation-missing-data.html","id":"synthetic-data-generation-for-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.8.3 Synthetic Data Generation for Missing Data","text":"Synthetic data generation emerged robust solution address missing data, providing flexibility privacy.Data Augmentation: Synthetic data generated augment datasets missing values, reducing biases introduced imputation.Data Augmentation: Synthetic data generated augment datasets missing values, reducing biases introduced imputation.Techniques:\nSimulations: Monte Carlo simulations create plausible data points based observed distributions.\nGenerative Models: GANs VAEs generate realistic synthetic data aligns existing patterns.\nTechniques:Simulations: Monte Carlo simulations create plausible data points based observed distributions.Generative Models: GANs VAEs generate realistic synthetic data aligns existing patterns.Applications:\nfraud detection, synthetic datasets balance impact missing values anomaly detection.\ninsurance, synthetic data supports pricing models filling gaps incomplete policyholder records.\nApplications:fraud detection, synthetic datasets balance impact missing values anomaly detection.insurance, synthetic data supports pricing models filling gaps incomplete policyholder records.","code":""},{"path":"imputation-missing-data.html","id":"federated-learning-and-privacy-preserving-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.8.4 Federated Learning and Privacy-Preserving Imputation","text":"Federated learning gained traction method collaborative analysis preserving data privacy.Federated Imputation:\nDistributed imputation algorithms operate decentralized data, ensuring sensitive information remains local.\nExample: Hospitals collaboratively impute missing patient data without sharing individual records.\nDistributed imputation algorithms operate decentralized data, ensuring sensitive information remains local.Example: Hospitals collaboratively impute missing patient data without sharing individual records.Privacy Mechanisms:\nDifferential privacy adds noise imputed values, protecting individual-level data.\nHomomorphic encryption allows computations encrypted data, ensuring privacy throughout imputation process.\nDifferential privacy adds noise imputed values, protecting individual-level data.Homomorphic encryption allows computations encrypted data, ensuring privacy throughout imputation process.Applications:\nHealthcare: Federated learning imputes missing diagnostic data across clinics.\nBanking: Collaborative imputation financial transaction data supports risk modeling adhering regulations.\nHealthcare: Federated learning imputes missing diagnostic data across clinics.Banking: Collaborative imputation financial transaction data supports risk modeling adhering regulations.","code":""},{"path":"imputation-missing-data.html","id":"imputation-in-streaming-and-online-data-environments","chapter":"13 Imputation (Missing Data)","heading":"13.8.5 Imputation in Streaming and Online Data Environments","text":"increasing use streaming data business technology requires real-time imputation methods ensure uninterrupted analysis.Challenges:\nImputation must occur dynamically data streams .\nLow latency high accuracy essential maintain real-time decision-making.\nImputation must occur dynamically data streams .Low latency high accuracy essential maintain real-time decision-making.Techniques:\nOnline Learning Algorithms: Update imputation models incrementally new data arrives.\nSliding Window Methods: Use recent data estimate impute missing values real time.\nOnline Learning Algorithms: Update imputation models incrementally new data arrives.Sliding Window Methods: Use recent data estimate impute missing values real time.Applications:\nIoT devices: Imputation sensor networks smart homes industrial monitoring ensures continuous operation despite data transmission issues.\nFinancial markets: Streaming imputation models predict fill gaps real-time stock price feeds inform trading algorithms.\nIoT devices: Imputation sensor networks smart homes industrial monitoring ensures continuous operation despite data transmission issues.Financial markets: Streaming imputation models predict fill gaps real-time stock price feeds inform trading algorithms.","code":""},{"path":"imputation-missing-data.html","id":"application-of-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.9 Application of Imputation","text":"section demonstrates visualize missing data handle using different imputation techniques.","code":""},{"path":"imputation-missing-data.html","id":"visualizing-missing-data","chapter":"13 Imputation (Missing Data)","heading":"13.9.1 Visualizing Missing Data","text":"Visualizing missing data essential first step understanding patterns extent missingness dataset.details, read Missing Book Nicholas Tierney & Allison Horst.","code":"\nlibrary(visdat)\nlibrary(naniar)\nlibrary(ggplot2)\n\n# Visualizing missing data\nvis_miss(airquality)\n\n# Missingness patterns using an upset plot\ngg_miss_upset(airquality)\n# Scatter plot of missing data with faceting\nggplot(airquality, aes(x, y)) +\n  geom_miss_point() +\n  facet_wrap(~ group)\n\n# Missing values by variable\ngg_miss_var(data, facet = group)\n\n# Missingness in relation to factors\ngg_miss_fct(x = variable1, fct = variable2)"},{"path":"imputation-missing-data.html","id":"how-many-imputations","chapter":"13 Imputation (Missing Data)","heading":"13.9.2 How Many Imputations?","text":"Usually, 5 imputations sufficient unless extremely high proportion missing data. High proportions require revisiting data collection processes.Rubin’s Rule Relative EfficiencyAccording Rubin, relative efficiency estimate based \\(m\\) imputations (relative infinite imputations) given :\\[\n\\text{Relative Efficiency} = ( 1 + \\frac{\\lambda}{m})^{-1}\n\\]\\(\\lambda\\) rate missing data.example, 50% missing data (\\(\\lambda = 0.5\\)), standard deviation estimate based 5 imputations 5% wider infinite imputations:\\[\n\\sqrt{1 + \\frac{0.5}{5}} = 1.049\n\\]","code":""},{"path":"imputation-missing-data.html","id":"generating-missing-data-for-demonstration","chapter":"13 Imputation (Missing Data)","heading":"13.9.3 Generating Missing Data for Demonstration","text":"","code":"\nlibrary(missForest)\n\n# Load the data\ndata <- iris\n\n# Generate 10% missing values at random\nset.seed(1)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n# Remove categorical variables for numeric imputation\niris.mis.cat <- iris.mis\niris.mis <- subset(iris.mis, select = -c(Species))"},{"path":"imputation-missing-data.html","id":"imputation-with-mean-median-and-mode","chapter":"13 Imputation (Missing Data)","heading":"13.9.4 Imputation with Mean, Median, and Mode","text":"Mean, median, mode imputation simple yet commonly used technique.Checking AccuracyAccuracy can checked comparing predictions actual values.","code":"\n# Imputation for the entire dataset\ne1071::impute(iris.mis, what = \"mean\")        # Replace with mean\ne1071::impute(iris.mis, what = \"median\")      # Replace with median\n\n# Imputation by variable\nHmisc::impute(iris.mis$Sepal.Length, mean)    # Replace with mean\nHmisc::impute(iris.mis$Sepal.Length, median)  # Replace with median\nHmisc::impute(iris.mis$Sepal.Length, 0)       # Replace with a specific value\n# Example data\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- rep(mean(iris$Sepal.Width, na.rm = TRUE), length(actuals))\n\n# Using MLmetrics package\nlibrary(MLmetrics)\n\nMAE(predicteds, actuals)\n#> [1] 0.2870303\nMSE(predicteds, actuals)\n#> [1] 0.1301598\nRMSE(predicteds, actuals)\n#> [1] 0.3607767"},{"path":"imputation-missing-data.html","id":"k-nearest-neighbors-knn-imputation","chapter":"13 Imputation (Missing Data)","heading":"13.9.5 K-Nearest Neighbors (KNN) Imputation","text":"KNN sophisticated method, leveraging similar observations fill missing values.KNN typically improves upon mean median imputation terms predictive accuracy.","code":"\nlibrary(DMwR2)\nknnOutput <-\n  DMwR2::knnImputation(data = iris.mis.cat,\n                       meth = \"median\")\nanyNA(knnOutput)  # Check for remaining missing values\n#> [1] FALSE\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- knnOutput[is.na(iris.mis$Sepal.Width), \"Sepal.Width\"]\n# Using MLmetrics package\nlibrary(MLmetrics)\n\nMAE(predicteds, actuals)\n#> [1] 0.2318182\nMSE(predicteds, actuals)\n#> [1] 0.1038636\nRMSE(predicteds, actuals)\n#> [1] 0.3222788"},{"path":"imputation-missing-data.html","id":"imputation-with-decision-trees-rpart","chapter":"13 Imputation (Missing Data)","heading":"13.9.6 Imputation with Decision Trees (rpart)","text":"Decision trees, implemented rpart, effective numeric categorical variables.","code":"\nlibrary(rpart)\n\n# Imputation for a categorical variable\nclass_mod <- rpart(\n  Species ~ . - Sepal.Length,\n  data = iris.mis.cat[!is.na(iris.mis.cat$Species), ],\n  method = \"class\",\n  na.action = na.omit\n)\n\n# Imputation for a numeric variable\nanova_mod <- rpart(\n  Sepal.Width ~ . - Sepal.Length,\n  data = iris.mis[!is.na(iris.mis$Sepal.Width), ],\n  method = \"anova\",\n  na.action = na.omit\n)\n\n# Predictions\nspecies_pred <- predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species), ])\nwidth_pred <- predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width), ])"},{"path":"imputation-missing-data.html","id":"mice-multivariate-imputation-via-chained-equations","chapter":"13 Imputation (Missing Data)","heading":"13.9.7 MICE (Multivariate Imputation via Chained Equations)","text":"MICE assumes data Missing Random (MAR). imputes data variable specifying imputation model tailored variable type.","code":""},{"path":"imputation-missing-data.html","id":"how-mice-works","chapter":"13 Imputation (Missing Data)","heading":"13.9.7.1 How MICE Works","text":"dataset variables \\(X_1, X_2, \\dots, X_k\\):\\(X_1\\) missing data, regressed variables.\\(X_1\\) missing data, regressed variables.process repeated variables missing data, using previously predicted values needed.process repeated variables missing data, using previously predicted values needed.default:Continuous variables use linear regression.Continuous variables use linear regression.Categorical variables use logistic regression.Categorical variables use logistic regression.","code":""},{"path":"imputation-missing-data.html","id":"methods-available-in-mice","chapter":"13 Imputation (Missing Data)","heading":"13.9.7.2 Methods Available in MICE","text":"pmm (Predictive Mean Matching): numeric variables.logreg (Logistic Regression): binary variables (2 levels).polyreg (Bayesian polytomous regression): factor variables (≥2 levels).Proportional Odds Model: ordered factor variables (≥2 levels).Imputing DataEvaluating Imputed DataAccessing Using Imputed DataRegression Model Imputed Dataset","code":"\n# Load packages\nlibrary(mice)\nlibrary(VIM)\n\n# Check missing values pattern\nmd.pattern(iris.mis)#>     Sepal.Width Sepal.Length Petal.Length Petal.Width   \n#> 100           1            1            1           1  0\n#> 15            1            1            1           0  1\n#> 8             1            1            0           1  1\n#> 2             1            1            0           0  2\n#> 11            1            0            1           1  1\n#> 1             1            0            1           0  2\n#> 1             1            0            0           1  2\n#> 1             1            0            0           0  3\n#> 7             0            1            1           1  1\n#> 3             0            1            0           1  2\n#> 1             0            0            1           1  2\n#>              11           15           15          19 60\n\n# Plot missing values\naggr(\n  iris.mis,\n  col = c('navyblue', 'yellow'),\n  numbers = TRUE,\n  sortVars = TRUE,\n  labels = names(iris.mis),\n  cex.axis = 0.7,\n  gap = 3,\n  ylab = c(\"Missing data\", \"Pattern\")\n)#> \n#>  Variables sorted by number of missings: \n#>      Variable      Count\n#>   Petal.Width 0.12666667\n#>  Sepal.Length 0.10000000\n#>  Petal.Length 0.10000000\n#>   Sepal.Width 0.07333333\n# Perform multiple imputation using MICE\nimputed_Data <- mice(\n  iris.mis,\n  m = 5,             # Number of imputed datasets\n  maxit = 10,        # Number of iterations\n  method = 'pmm',    # Imputation method\n  seed = 500         # Random seed for reproducibility\n)\n# Summary of imputed data\nsummary(imputed_Data)\n#> Class: mids\n#> Number of multiple imputations:  5 \n#> Imputation methods:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>        \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\" \n#> PredictorMatrix:\n#>              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> Sepal.Length            0           1            1           1\n#> Sepal.Width             1           0            1           1\n#> Petal.Length            1           1            0           1\n#> Petal.Width             1           1            1           0\n\n# Density plot: compare imputed values (red) with observed values (blue)\ndensityplot(imputed_Data)\n# Access the complete datasets\ncompleteData1 <- mice::complete(imputed_Data, 1)  # First imputed dataset\ncompleteData2 <- mice::complete(imputed_Data, 2)  # Second imputed dataset\n# Fit a regression model using imputed datasets\nfit <-\n  with(data = imputed_Data,\n       exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width))\n\n# Combine results of all 5 models\ncombine <- mice::pool(fit)\nsummary(combine)\n#>           term   estimate  std.error statistic        df      p.value\n#> 1  (Intercept)  1.9054698 0.33454626  5.695684 105.12438 1.127064e-07\n#> 2 Sepal.Length  0.2936285 0.07011405  4.187870  88.69066 6.625536e-05\n#> 3  Petal.Width -0.4742921 0.08138313 -5.827892  46.94941 4.915270e-07"},{"path":"imputation-missing-data.html","id":"amelia","chapter":"13 Imputation (Missing Data)","heading":"13.9.8 Amelia","text":"Amelia uses bootstrap-based Expectation-Maximization Bootstrapping (EMB) algorithm imputation, making faster suitable cross-sectional time-series data.","code":""},{"path":"imputation-missing-data.html","id":"assumptions","chapter":"13 Imputation (Missing Data)","heading":"13.9.8.1 Assumptions","text":"variables must follow Multivariate Normal Distribution (MVN). Transformations may required non-normal data.variables must follow Multivariate Normal Distribution (MVN). Transformations may required non-normal data.Data must Missing Random (MAR).Data must Missing Random (MAR).","code":""},{"path":"imputation-missing-data.html","id":"comparison-amelia-vs.-mice","chapter":"13 Imputation (Missing Data)","heading":"13.9.8.2 Comparison: Amelia vs. MICE","text":"MICE imputes variable--variable basis using separate models.MICE imputes variable--variable basis using separate models.Amelia uses joint modeling approach based MVN.Amelia uses joint modeling approach based MVN.MICE handles multiple data types, Amelia requires variables approximate normality.MICE handles multiple data types, Amelia requires variables approximate normality.","code":""},{"path":"imputation-missing-data.html","id":"imputation-with-amelia","chapter":"13 Imputation (Missing Data)","heading":"13.9.8.3 Imputation with Amelia","text":"Amelia’s workflow includes bootstrapping multiple imputations generate robust estimates means variances. process ensures flexibility speed large datasets.","code":"\nlibrary(Amelia)\ndata(\"iris\")\n\n# Seed 10% missing values\nset.seed(123)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n# Specify columns and run Amelia\namelia_fit <- amelia(\n  iris.mis,\n  m = 5,                      # Number of imputations\n  parallel = \"multicore\",     # Use multicore processing\n  noms = \"Species\"            # Nominal variables\n)\n#> -- Imputation 1 --\n#> \n#>   1  2  3  4  5  6  7\n#> \n#> -- Imputation 2 --\n#> \n#>   1  2  3  4  5\n#> \n#> -- Imputation 3 --\n#> \n#>   1  2  3  4  5\n#> \n#> -- Imputation 4 --\n#> \n#>   1  2  3  4  5  6\n#> \n#> -- Imputation 5 --\n#> \n#>   1  2  3  4  5  6  7  8  9 10\n\n# Access imputed outputs\n# amelia_fit$imputations[[1]]"},{"path":"imputation-missing-data.html","id":"missforest","chapter":"13 Imputation (Missing Data)","heading":"13.9.9 missForest","text":"missForest package provides robust non-parametric imputation method using Random Forest algorithm. versatile, handling continuous categorical variables without requiring assumptions underlying functional forms.Key Features missForestNon-Parametric: assumptions functional form.Variable-Specific Models: Builds random forest model variable impute missing values.Error Estimates: Provides --bag (OOB) imputation error estimates.\nNRMSE (Normalized Root Mean Squared Error) continuous variables.\nPFC (Proportion Falsely Classified) categorical variables.\nNRMSE (Normalized Root Mean Squared Error) continuous variables.PFC (Proportion Falsely Classified) categorical variables.High Control: Offers customizable parameters like mtry ntree.","code":"\nlibrary(missForest)\n\n# Impute missing values using default parameters\niris.imp <- missForest(iris.mis)\n\n# Check imputed values\n# View the imputed dataset\n# iris.imp$ximp\n# Out-of-bag error estimates\niris.imp$OOBerror\n#>      NRMSE        PFC \n#> 0.14004144 0.02877698\n\n# Compare imputed data with original data to calculate error\niris.err <- mixError(iris.imp$ximp, iris.mis, iris)\niris.err\n#>      NRMSE        PFC \n#> 0.14420833 0.09090909"},{"path":"imputation-missing-data.html","id":"hmisc","chapter":"13 Imputation (Missing Data)","heading":"13.9.10 Hmisc","text":"Hmisc package provides suite tools imputing missing data, offering simple methods (like mean median imputation) advanced approaches like aregImpute.Features Hmiscimpute(): Simple imputation using user-defined methods like mean, median, random value.impute(): Simple imputation using user-defined methods like mean, median, random value.aregImpute():\nCombines additive regression, bootstrapping, predictive mean matching.\nHandles continuous categorical variables.\nAutomatically recognizes variable types applies appropriate methods.\naregImpute():Combines additive regression, bootstrapping, predictive mean matching.Combines additive regression, bootstrapping, predictive mean matching.Handles continuous categorical variables.Handles continuous categorical variables.Automatically recognizes variable types applies appropriate methods.Automatically recognizes variable types applies appropriate methods.AssumptionsLinearity variables predicted.Linearity variables predicted.Fisher’s optimum scoring used categorical variable prediction.Fisher’s optimum scoring used categorical variable prediction.Note: missForest often outperforms Hmisc terms accuracy, latter useful datasets simpler requirements.","code":"library(Hmisc)\n\n# Impute using mean\niris.mis$imputed_SepalLength <- with(iris.mis, impute(Sepal.Length, mean))\n\n# Impute using random value\niris.mis$imputed_SepalLength2 <- with(iris.mis, impute(Sepal.Length, 'random'))\n\n# Advanced imputation using aregImpute\nimpute_arg <- aregImpute(\n  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species,\n  data = iris.mis,\n  n.impute = 5\n)\n#> Iteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \n\n# Check R-squared values for predicted missing values\nimpute_arg\n#> \n#> Multiple Imputation using Bootstrap and PMM\n#> \n#> aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + \n#>     Petal.Width + Species, data = iris.mis, n.impute = 5)\n#> \n#> n: 150   p: 5    Imputations: 5      nk: 3 \n#> \n#> Number of NAs:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>           17           19           12           16           11 \n#> \n#>              type d.f.\n#> Sepal.Length    s    2\n#> Sepal.Width     s    2\n#> Petal.Length    s    2\n#> Petal.Width     s    2\n#> Species         c    2\n#> \n#> Transformation of Target Variables Forced to be Linear\n#> \n#> R-squares for Predicting Non-Missing Values for Each Variable\n#> Using Last Imputations of Predictors\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>        0.895        0.536        0.987        0.967        0.984\n\n# Access imputed values for Sepal.Length\nimpute_arg$imputed$Sepal.Length\n#>     [,1] [,2] [,3] [,4] [,5]\n#> 13   4.4  4.9  4.9  5.0  4.9\n#> 14   4.8  4.4  5.0  4.5  4.5\n#> 23   4.8  5.1  5.1  5.1  4.8\n#> 26   5.0  4.8  4.9  4.9  5.0\n#> 34   5.0  5.8  6.0  5.7  5.8\n#> 39   4.4  4.9  5.0  4.5  4.6\n#> 41   5.2  5.1  4.8  5.0  4.8\n#> 69   5.8  6.0  6.3  6.0  6.1\n#> 72   5.6  5.7  5.7  5.8  6.1\n#> 89   6.1  5.7  5.7  5.6  6.9\n#> 90   5.5  6.2  5.2  6.0  5.8\n#> 91   5.7  6.9  6.0  6.4  6.4\n#> 116  5.9  6.8  6.4  6.6  6.9\n#> 118  7.9  7.9  7.9  7.9  7.9\n#> 135  6.7  6.7  6.7  6.9  6.7\n#> 141  7.0  6.3  5.9  6.7  7.0\n#> 143  5.7  6.7  5.8  6.3  5.4"},{"path":"imputation-missing-data.html","id":"mi","chapter":"13 Imputation (Missing Data)","heading":"13.9.11 mi","text":"mi package powerful tool imputation, using Bayesian methods providing rich diagnostics model evaluation convergence.Features miGraphical Diagnostics: Visualize imputation models convergence.Graphical Diagnostics: Visualize imputation models convergence.Bayesian Regression: Handles separation issues data.Bayesian Regression: Handles separation issues data.Irregularity Detection: Automatically detects issues like high collinearity.Irregularity Detection: Automatically detects issues like high collinearity.Noise Addition: Adds noise address additive constraints.Noise Addition: Adds noise address additive constraints.","code":"\nlibrary(mi)\n\n# Perform imputation using mi\nmi_data <- mi(iris.mis, seed = 1)\n\n# Summary of the imputation process\nsummary(mi_data)\n#> $Sepal.Length\n#> $Sepal.Length$is_missing\n#> missing\n#> FALSE  TRUE \n#>   133    17 \n#> \n#> $Sepal.Length$imputed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.28900 -0.05467 -0.01368 -0.01573  0.03970  0.15427 \n#> \n#> $Sepal.Length$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.90110 -0.47329 -0.04549  0.00000  0.32120  1.23792 \n#> \n#> \n#> $Sepal.Width\n#> $Sepal.Width$is_missing\n#> missing\n#> FALSE  TRUE \n#>   131    19 \n#> \n#> $Sepal.Width$imputed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -1.27054 -0.48086 -0.01793 -0.04736  0.29911  1.78435 \n#> \n#> $Sepal.Width$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -1.01272 -0.30642 -0.07099  0.00000  0.39988  1.34161 \n#> \n#> \n#> $Petal.Length\n#> $Petal.Length$is_missing\n#> missing\n#> FALSE  TRUE \n#>   138    12 \n#> \n#> $Petal.Length$imputed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.8370 -0.6256  0.2117  0.0303  0.4575  0.7208 \n#> \n#> $Petal.Length$observed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.7797 -0.6088  0.1459  0.0000  0.3880  0.9006 \n#> \n#> \n#> $Petal.Width\n#> $Petal.Width$is_missing\n#> missing\n#> FALSE  TRUE \n#>   134    16 \n#> \n#> $Petal.Width$imputed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.77086  0.02848  0.19666  0.17639  0.47746  1.02418 \n#> \n#> $Petal.Width$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.69624 -0.56602  0.08503  0.00000  0.41055  0.86629 \n#> \n#> \n#> $Species\n#> $Species$crosstab\n#>             \n#>              observed imputed\n#>   setosa          180      21\n#>   versicolor      192       6\n#>   virginica       184      17\n#> \n#> \n#> $imputed_SepalLength\n#> $imputed_SepalLength$is_missing\n#> [1] \"all values observed\"\n#> \n#> $imputed_SepalLength$observed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.9574 -0.4379  0.0000  0.0000  0.3413  1.3152 \n#> \n#> \n#> $imputed_SepalLength2\n#> $imputed_SepalLength2$is_missing\n#> [1] \"all values observed\"\n#> \n#> $imputed_SepalLength2$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.90570 -0.48398 -0.06225  0.00000  0.35947  1.20292"},{"path":"model-specification-tests.html","id":"model-specification-tests","chapter":"14 Model Specification Tests","heading":"14 Model Specification Tests","text":"Model specification tests critical econometric analysis verify whether assumptions underlying model hold true. tests help determine model correctly specified, ensuring estimators reliable efficient. mis-specified model can lead biased, inconsistent, inefficient estimates, undermines validity inferences drawn analysis.chapter addresses various model specification tests, including tests :Nested Model TestsNon-Nested Model TestsHeteroskedasticity TestsFunctional Form TestsAutocorrelation TestsMulticollinearity DiagnosticsUnderstanding tests allows researchers evaluate robustness models make necessary adjustments improve model performance.","code":""},{"path":"model-specification-tests.html","id":"nested-model-tests","chapter":"14 Model Specification Tests","heading":"14.1 Nested Model Tests","text":"Nested models restricted model special case unrestricted model. words, restricted model can derived unrestricted model imposing constraints certain parameters, typically setting equal zero. structure allows us formally test whether additional variables unrestricted model significantly improve model’s explanatory power. following tests help compare models:Wald Test: Assesses significance individual coefficients groups coefficients.Likelihood Ratio Test: Compares goodness--fit restricted unrestricted models.F-Test: Evaluates joint significance multiple coefficients.Chow Test: Evaluates whether coefficients regression model across different groups time periods.Consider following models:\\[\n\\begin{aligned}\n\\textbf{Unrestricted Model:} \\quad & y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon \\\\\n\\textbf{Restricted Model:} \\quad & y = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\end{aligned}\n\\]unrestricted model includes potential explanatory variables: \\(x_1\\), \\(x_2\\), \\(x_3\\).restricted model nested within unrestricted model, containing subset variables (case, excluding \\(x_2\\) \\(x_3\\)).goal test null hypothesis restrictions valid:\\[\nH_0: \\beta_2 = \\beta_3 = 0 \\quad \\text{(restrictions valid)}\n\\]alternative hypothesis:\\[\nH_1: \\text{least one } \\beta_2, \\beta_3 \\neq 0 \\quad \\text{(restrictions invalid)}\n\\]conduct test, use following methods:","code":""},{"path":"model-specification-tests.html","id":"sec-wald-test-nested","chapter":"14 Model Specification Tests","heading":"14.1.1 Wald Test","text":"Wald Test assesses whether certain linear restrictions parameters model valid. commonly used testing joint significance multiple coefficients.Consider set linear restrictions expressed :\\[\nH_0: R\\boldsymbol{\\beta} = r\n\\]:\\(R\\) \\(q \\times k\\) restriction matrix,\\(R\\) \\(q \\times k\\) restriction matrix,\\(\\boldsymbol{\\beta}\\) \\(k \\times 1\\) vector parameters,\\(\\boldsymbol{\\beta}\\) \\(k \\times 1\\) vector parameters,\\(r\\) \\(q \\times 1\\) vector representing hypothesized values (often zeros),\\(r\\) \\(q \\times 1\\) vector representing hypothesized values (often zeros),\\(q\\) number restrictions tested.\\(q\\) number restrictions tested.example, want test \\(H_0: \\beta_2 = \\beta_3 = 0\\), :\\[\nR = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}, \\quad r = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n\\]Wald statistic calculated :\\[\nW = (R\\hat{\\boldsymbol{\\beta}} - r)' \\left[ R \\, \\hat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) \\, R' \\right]^{-1} (R\\hat{\\boldsymbol{\\beta}} - r)\n\\]:\\(\\hat{\\boldsymbol{\\beta}}\\) vector estimated coefficients unrestricted model,\\(\\hat{\\boldsymbol{\\beta}}\\) vector estimated coefficients unrestricted model,\\(\\hat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}})\\) estimated covariance matrix \\(\\hat{\\boldsymbol{\\beta}}\\).\\(\\hat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}})\\) estimated covariance matrix \\(\\hat{\\boldsymbol{\\beta}}\\).Distribution Decision RuleUnder \\(H_0\\), Wald statistic follows \\(\\chi^2\\) distribution \\(q\\) degrees freedom:\\[\nW \\sim \\chi^2_q\n\\]Decision Rule:\nReject \\(H_0\\) \\(W > \\chi^2_{q,\\alpha}\\), \\(\\alpha\\) significance level.\nlarge Wald statistic indicates restrictions invalid.\nReject \\(H_0\\) \\(W > \\chi^2_{q,\\alpha}\\), \\(\\alpha\\) significance level.large Wald statistic indicates restrictions invalid.","code":""},{"path":"model-specification-tests.html","id":"sec-likelihood-ratio-test-nested","chapter":"14 Model Specification Tests","heading":"14.1.2 Likelihood Ratio Test","text":"Likelihood Ratio Test compares goodness--fit restricted unrestricted models. evaluates whether additional parameters unrestricted model significantly improve likelihood observing data.:\\[\nH_0: \\beta_2 = \\beta_3 = 0 \\quad \\text{vs.} \\quad H_1: \\text{least one } \\beta_2, \\beta_3 \\neq 0\n\\]LR statistic calculated :\\[\nLR = -2 \\left( \\ln L_R - \\ln L_U \\right)\n\\]:\\(L_R\\) maximized likelihood restricted model,\\(L_R\\) maximized likelihood restricted model,\\(L_U\\) maximized likelihood unrestricted model.\\(L_U\\) maximized likelihood unrestricted model.Distribution Decision RuleUnder \\(H_0\\), LR statistic follows \\(\\chi^2\\) distribution \\(q\\) degrees freedom:\\[\nLR \\sim \\chi^2_q\n\\]Decision Rule:\nReject \\(H_0\\) \\(LR > \\chi^2_{q,\\alpha}\\).\nlarge LR statistic suggests unrestricted model provides significantly better fit.\nReject \\(H_0\\) \\(LR > \\chi^2_{q,\\alpha}\\).large LR statistic suggests unrestricted model provides significantly better fit.Connection OLSIn case linear regression normally distributed errors, LR statistic can expressed terms sum squared residuals (SSR):\\[\nLR = n \\ln \\left( \\frac{SSR_R}{SSR_U} \\right)\n\\]\\(n\\) sample size.","code":""},{"path":"model-specification-tests.html","id":"sec-f-test-for-linear-regression-nested","chapter":"14 Model Specification Tests","heading":"14.1.3 F-Test (for Linear Regression)","text":"F-Test commonly used linear regression evaluate joint significance multiple coefficients. compares fit restricted unrestricted models using sum squared residuals.:\\[\nH_0: \\beta_2 = \\beta_3 = 0 \\quad \\text{vs.} \\quad H_1: \\text{least one } \\beta_2, \\beta_3 \\neq 0\n\\]F-statistic calculated :\\[\nF = \\frac{(SSR_R - SSR_U) / q}{SSR_U / (n - k)}\n\\]:\\(SSR_R\\) = Sum Squared Residuals restricted model,\\(SSR_U\\) = Sum Squared Residuals unrestricted model,\\(q\\) = Number restrictions (, 2),\\(n\\) = Sample size,\\(k\\) = Number parameters unrestricted model.Distribution Decision RuleUnder \\(H_0\\), F-statistic follows \\(F\\)-distribution \\((q, n - k)\\) degrees freedom:\\[\nF \\sim F_{q, n - k}\n\\]Decision Rule:\nReject \\(H_0\\) \\(F > F_{q, n - k, \\alpha}\\).\nlarge F-statistic indicates restricted model fits significantly worse, suggesting excluded variables important.\nReject \\(H_0\\) \\(F > F_{q, n - k, \\alpha}\\).large F-statistic indicates restricted model fits significantly worse, suggesting excluded variables important.","code":""},{"path":"model-specification-tests.html","id":"sec-chow-test","chapter":"14 Model Specification Tests","heading":"14.1.4 Chow Test","text":"Chow Test evaluates whether coefficients regression model across different groups time periods. often used detect structural breaks data.Key Question:run two different regressions two groups, can pool data use single regression?Chow Test ProcedureEstimate regression model pooled data (observations).Estimate model separately Group 1 Group 2.Compare sum squared residuals (SSR) models.test statistic follows F-distribution:\\[\nF = \\frac{(SSR_P - (SSR_1 + SSR_2)) / k}{(SSR_1 + SSR_2) / (n_1 + n_2 - 2k)}\n\\]:\\(SSR_P\\) = Sum Squared Residuals pooled model\\(SSR_1\\), \\(SSR_2\\) = SSRs Group 1 Group 2 models\\(k\\) = Number parameters\\(n_1\\), \\(n_2\\) = Number observations groupInterpretation:significant F-statistic suggests structural differences groups, implying separate regressions appropriate.significant F-statistic suggests structural differences groups, implying separate regressions appropriate.non-significant F-statistic indicates structural break, supporting use pooled model.non-significant F-statistic indicates structural break, supporting use pooled model.Interpretation ResultsWald Test\nNull Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (coefficients \\(x_2\\) \\(x_3\\) zero).\nDecision Rule:\nReject \\(H_0\\) p-value < 0.05: \\(x_2\\) \\(x_3\\) jointly significant.\nFail reject \\(H_0\\) p-value ≥ 0.05: \\(x_2\\) \\(x_3\\) significantly contribute model.\n\nWald TestNull Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (coefficients \\(x_2\\) \\(x_3\\) zero).Null Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (coefficients \\(x_2\\) \\(x_3\\) zero).Decision Rule:\nReject \\(H_0\\) p-value < 0.05: \\(x_2\\) \\(x_3\\) jointly significant.\nFail reject \\(H_0\\) p-value ≥ 0.05: \\(x_2\\) \\(x_3\\) significantly contribute model.\nDecision Rule:Reject \\(H_0\\) p-value < 0.05: \\(x_2\\) \\(x_3\\) jointly significant.Fail reject \\(H_0\\) p-value ≥ 0.05: \\(x_2\\) \\(x_3\\) significantly contribute model.Likelihood Ratio Test (LR Test)\nNull Hypothesis (\\(H_0\\)): restricted model fits data well unrestricted model.\nDecision Rule:\nReject \\(H_0\\) p-value < 0.05: unrestricted model fits better, indicating \\(x_2\\) \\(x_3\\) improve model.\nFail reject \\(H_0\\) p-value ≥ 0.05: Adding \\(x_2\\) \\(x_3\\) doesn’t improve model significantly.\n\nLikelihood Ratio Test (LR Test)Null Hypothesis (\\(H_0\\)): restricted model fits data well unrestricted model.Null Hypothesis (\\(H_0\\)): restricted model fits data well unrestricted model.Decision Rule:\nReject \\(H_0\\) p-value < 0.05: unrestricted model fits better, indicating \\(x_2\\) \\(x_3\\) improve model.\nFail reject \\(H_0\\) p-value ≥ 0.05: Adding \\(x_2\\) \\(x_3\\) doesn’t improve model significantly.\nDecision Rule:Reject \\(H_0\\) p-value < 0.05: unrestricted model fits better, indicating \\(x_2\\) \\(x_3\\) improve model.Fail reject \\(H_0\\) p-value ≥ 0.05: Adding \\(x_2\\) \\(x_3\\) doesn’t improve model significantly.F-Test (Linear Regression)\nNull Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (similar Wald Test).\nDecision Rule:\nReject \\(H_0\\) p-value < 0.05: excluded variables significant.\nFail reject \\(H_0\\) p-value ≥ 0.05: excluded variables significant.\n\nF-Test (Linear Regression)Null Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (similar Wald Test).Null Hypothesis (\\(H_0\\)): \\(\\beta_2 = \\beta_3 = 0\\) (similar Wald Test).Decision Rule:\nReject \\(H_0\\) p-value < 0.05: excluded variables significant.\nFail reject \\(H_0\\) p-value ≥ 0.05: excluded variables significant.\nDecision Rule:Reject \\(H_0\\) p-value < 0.05: excluded variables significant.Fail reject \\(H_0\\) p-value ≥ 0.05: excluded variables significant.Chow Test (Using group Variable)\nNull Hypothesis (\\(H_0\\)): structural break exists; regression coefficients across Group 0 Group 1.\nDecision Rule:\nReject \\(H_0\\) p-value < 0.05: structural break exists—model coefficients differ significantly groups.\nFail reject \\(H_0\\) p-value ≥ 0.05: significant structural break detected; model coefficients stable across groups.\n\nChow Test (Using group Variable)Null Hypothesis (\\(H_0\\)): structural break exists; regression coefficients across Group 0 Group 1.Null Hypothesis (\\(H_0\\)): structural break exists; regression coefficients across Group 0 Group 1.Decision Rule:\nReject \\(H_0\\) p-value < 0.05: structural break exists—model coefficients differ significantly groups.\nFail reject \\(H_0\\) p-value ≥ 0.05: significant structural break detected; model coefficients stable across groups.\nDecision Rule:Reject \\(H_0\\) p-value < 0.05: structural break exists—model coefficients differ significantly groups.Fail reject \\(H_0\\) p-value ≥ 0.05: significant structural break detected; model coefficients stable across groups.","code":"\n# Load necessary libraries\nlibrary(car)        # For Wald Test\nlibrary(lmtest)     # For Likelihood Ratio Test\nlibrary(strucchange)  # For Chow Test\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nepsilon <- rnorm(n)\ny <- 2 + 1.5 * x1 + 0.5 * x2 - 0.7 * x3 + epsilon\n\n# Creating a group variable (simulating a structural break)\ngroup <- rep(c(0, 1), each = n / 2)  # Group 0 and Group 1\n\n# ----------------------------------------------------------------------\n# Wald Test\n# ----------------------------------------------------------------------\nunrestricted_model <- lm(y ~ x1 + x2 + x3)    # Unrestricted model\nrestricted_model <- lm(y ~ x1)                # Restricted model\n\nwald_test <- linearHypothesis(unrestricted_model, c(\"x2 = 0\", \"x3 = 0\"))\nprint(wald_test)\n#> \n#> Linear hypothesis test:\n#> x2 = 0\n#> x3 = 0\n#> \n#> Model 1: restricted model\n#> Model 2: y ~ x1 + x2 + x3\n#> \n#>   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n#> 1     98 182.26                                  \n#> 2     96 106.14  2    76.117 34.421 5.368e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ----------------------------------------------------------------------\n# Likelihood Ratio Test\n# ----------------------------------------------------------------------\nlr_test <- lrtest(unrestricted_model, restricted_model)\nprint(lr_test)\n#> Likelihood ratio test\n#> \n#> Model 1: y ~ x1 + x2 + x3\n#> Model 2: y ~ x1\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)    \n#> 1   5 -144.88                         \n#> 2   3 -171.91 -2 54.064  1.821e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ----------------------------------------------------------------------\n# F-Test (for Linear Regression)\n# ----------------------------------------------------------------------\nSSR_U <- sum(residuals(unrestricted_model)^2)  # SSR for unrestricted model\nSSR_R <- sum(residuals(restricted_model)^2)    # SSR for restricted model\nq <- 2                                        # Number of restrictions\nn <- length(y)                                # Sample size\nk <- length(coef(unrestricted_model))         # Number of parameters in unrestricted model\n\n# F-statistic formula\nF_statistic <- ((SSR_R - SSR_U) / q) / (SSR_U / (n - k))\np_value_F <- pf(F_statistic, df1 = q, df2 = n - k, lower.tail = FALSE)\n\ncat(\"F-statistic:\", F_statistic, \"\\n\")\n#> F-statistic: 34.42083\ncat(\"P-value:\", p_value_F, \"\\n\")\n#> P-value: 5.367912e-12\n\n# ----------------------------------------------------------------------\n# Chow Test (Proper Use of the Group Variable)\n# ----------------------------------------------------------------------\n# Pooled model (all data)\npooled_model <- lm(y ~ x1 + x2 + x3)\n\n# Separate models for Group 0 and Group 1\nmodel_group0 <- lm(y[group == 0] ~ x1[group == 0] + x2[group == 0] + x3[group == 0])\nmodel_group1 <- lm(y[group == 1] ~ x1[group == 1] + x2[group == 1] + x3[group == 1])\n\n# Calculating SSRs\nSSR_pooled <- sum(residuals(pooled_model)^2)\nSSR_group0 <- sum(residuals(model_group0)^2)\nSSR_group1 <- sum(residuals(model_group1)^2)\n\n# Chow Test formula\nk_chow <- length(coef(pooled_model))  # Number of parameters (including intercept)\nn0 <- sum(group == 0)                 # Sample size for Group 0\nn1 <- sum(group == 1)                 # Sample size for Group 1\n\nF_chow <- ((SSR_pooled - (SSR_group0 + SSR_group1)) / k_chow) /\n          ((SSR_group0 + SSR_group1) / (n0 + n1 - 2 * k_chow))\n\n# Corresponding p-value\np_value_chow <-\n    pf(\n        F_chow,\n        df1 = k_chow,\n        df2 = (n0 + n1 - 2 * k_chow),\n        lower.tail = FALSE\n    )\n\ncat(\"Chow Test F-statistic:\", F_chow, \"\\n\")\n#> Chow Test F-statistic: 0.3551197\ncat(\"P-value:\", p_value_chow, \"\\n\")\n#> P-value: 0.8398657"},{"path":"model-specification-tests.html","id":"non-nested-model-tests","chapter":"14 Model Specification Tests","heading":"14.2 Non-Nested Model Tests","text":"Comparing models essential identify specification best explains data. nested model comparisons rely one model restricted version another, non-nested models share hierarchical structure. situation commonly arises comparing models :Different functional forms (e.g., linear vs. logarithmic relationships),Different sets explanatory variables,Competing theoretical frameworks.compare non-nested models, rely specialized statistical tests designed handle complexities. two widely used approaches :Vuong Test used compare fit two non-nested models determine model better explains data.Vuong Test used compare fit two non-nested models determine model better explains data.Davidson–MacKinnon J-Test regression-based approach comparing non-nested models. evaluates model better fits data incorporating predicted values competing model additional regressor.Davidson–MacKinnon J-Test regression-based approach comparing non-nested models. evaluates model better fits data incorporating predicted values competing model additional regressor.Consider two competing models:Model :\\[\ny = \\alpha_0 + \\alpha_1 f(X) + \\epsilon_A\n\\]Model B:\\[\ny = \\beta_0 + \\beta_1 g(Z) + \\epsilon_B\n\\]:\\(f(X)\\) \\(g(Z)\\) represent different functional forms different sets explanatory variables.\\(f(X)\\) \\(g(Z)\\) represent different functional forms different sets explanatory variables.models non-nested one obtained restricting parameters.models non-nested one obtained restricting parameters.goal determine model better explains data.","code":""},{"path":"model-specification-tests.html","id":"sec-vuong-test","chapter":"14 Model Specification Tests","heading":"14.2.1 Vuong Test","text":"Vuong Test likelihood-ratio-based approach comparing non-nested models (Vuong 1989). particularly useful models estimated via Maximum Likelihood Estimation.HypothesesNull Hypothesis (\\(H_0\\)): models equally close true data-generating process (.e., models equal predictive power).Alternative Hypothesis (\\(H_1\\)):\nPositive Test Statistic (\\(V > 0\\)): Model preferred.\nNegative Test Statistic (\\(V < 0\\)): Model B preferred.\nPositive Test Statistic (\\(V > 0\\)): Model preferred.Negative Test Statistic (\\(V < 0\\)): Model B preferred.Vuong Test StatisticThe Vuong test based difference log-likelihood contributions observation two models. Let:\\(\\ell_{,}\\) = log-likelihood observation \\(\\) Model ,\\(\\ell_{B,}\\) = log-likelihood observation \\(\\) Model B.Define difference log-likelihoods:\\[\nm_i = \\ell_{,} - \\ell_{B,}\n\\]Vuong test statistic :\\[\nV = \\frac{\\sqrt{n} \\, \\bar{m}}{s_m}\n\\]:\\(\\bar{m} = \\frac{1}{n} \\sum_{=1}^n m_i\\) sample mean log-likelihood differences,\\(\\bar{m} = \\frac{1}{n} \\sum_{=1}^n m_i\\) sample mean log-likelihood differences,\\(s_m = \\sqrt{\\frac{1}{n} \\sum_{=1}^n (m_i - \\bar{m})^2}\\) sample standard deviation \\(m_i\\),\\(s_m = \\sqrt{\\frac{1}{n} \\sum_{=1}^n (m_i - \\bar{m})^2}\\) sample standard deviation \\(m_i\\),\\(n\\) sample size.\\(n\\) sample size.Distribution Decision RuleUnder \\(H_0\\), Vuong statistic asymptotically follows standard normal distribution:\\[\nV \\sim N(0, 1)\n\\]Decision Rule:\n\\(|V| > z_{\\alpha/2}\\) (critical value standard normal distribution), reject \\(H_0\\).\n\\(V > 0\\): Prefer Model .\n\\(V < 0\\): Prefer Model B.\n\n\\(|V| \\leq z_{\\alpha/2}\\): Fail reject \\(H_0\\); significant difference models.\n\\(|V| > z_{\\alpha/2}\\) (critical value standard normal distribution), reject \\(H_0\\).\n\\(V > 0\\): Prefer Model .\n\\(V < 0\\): Prefer Model B.\n\\(V > 0\\): Prefer Model .\\(V < 0\\): Prefer Model B.\\(|V| \\leq z_{\\alpha/2}\\): Fail reject \\(H_0\\); significant difference models.Corrections Model ComplexityWhen comparing models different numbers parameters, penalized version Vuong test can used, similar adjusting model complexity criteria like AIC BIC. corrected statistic :\\[\nV_{\\text{adjusted}} = V - \\frac{(k_A - k_B) \\ln(n)}{2 s_m \\sqrt{n}}\n\\]\\(k_A\\) \\(k_B\\) number parameters Models B, respectively.Limitations Vuong TestRequires models estimated via Maximum Likelihood.Sensitive model misspecification heteroskedasticity.Assumes independent identically distributed (..d.) errors.","code":""},{"path":"model-specification-tests.html","id":"sec-davidson--mackinnon-j-test","chapter":"14 Model Specification Tests","heading":"14.2.2 Davidson–MacKinnon J-Test","text":"Davidson–MacKinnon J-Test provides flexible, regression-based approach comparing non-nested models (Davidson MacKinnon 1981). evaluates whether predictions one model contain information captured competing model. test can thought comparing models transformed independent variables, opposed next section, Comparing Models Transformed Dependent Variables.HypothesesNull Hypothesis (\\(H_0\\)): competing model provide additional explanatory power beyond current model.Alternative Hypothesis (\\(H_1\\)): competing model provides additional explanatory power.ProcedureConsider two competing models:Model :\\[\ny = \\alpha_0 + \\alpha_1 x + \\epsilon_A\n\\]Model B:\\[\ny = \\beta_0 + \\beta_1 \\ln(x) + \\epsilon_B\n\\]Step 1: Testing Model Model BEstimate Model B obtain predicted values \\(\\hat{y}_B\\).Run auxiliary regression:\\[\ny = \\alpha_0 + \\alpha_1 x + \\gamma \\hat{y}_B + u\n\\]Test null hypothesis:\\[\nH_0: \\gamma = 0\n\\]\\(\\gamma\\) significant, Model B adds explanatory power beyond Model .\\(\\gamma\\) significant, Model sufficiently explains data.Step 2: Testing Model B Model AEstimate Model obtain predicted values \\(\\hat{y}_A\\).Run auxiliary regression:\\[\ny = \\beta_0 + \\beta_1 \\ln(x) + \\gamma \\hat{y}_A + u\n\\]Test null hypothesis:\\[\nH_0: \\gamma = 0\n\\]Decision RulesReject \\(H_0\\) Step 1, Fail Reject Step 2: Prefer Model B.Fail Reject \\(H_0\\) Step 1, Reject Step 2: Prefer Model .Reject \\(H_0\\) Steps: Neither model adequate; reconsider functional form.Fail Reject \\(H_0\\) Steps: strong evidence prefer one model; rely criteria (e.g., theory, simplicity). Alternatively, \\(R^2_{adj}\\) can also used choose two.Adjusted \\(R^2\\)\\(R^2\\) always increase variables includedAdjusted \\(R^2\\) tries correct penalizing inclusion unnecessary variables.\\[ \\begin{aligned} {R}^2 &= 1 - \\frac{SSR/n}{SST/n} \\\\ {R}^2_{adj} &= 1 - \\frac{SSR/(n-k)}{SST/(n-1)} \\\\ &= 1 - \\frac{(n-1)(1-R^2)}{(n-k)} \\end{aligned} \\]\\({R}^2_{adj}\\) increases t-statistic additional variable greater 1 absolute value.\\({R}^2_{adj}\\) valid models heteroskedasticitythere fore used determining variables included model (t F-tests appropriate)","code":""},{"path":"model-specification-tests.html","id":"adjusted-r2","chapter":"14 Model Specification Tests","heading":"14.2.3 Adjusted \\(R^2\\)","text":"coefficient determination (\\(R^2\\)) measures proportion variance dependent variable explained model. However, key limitation \\(R^2\\) always increases (least stays ) additional explanatory variables added model, even variables statistically significant.address issue, adjusted \\(R^2\\) introduces penalty including unnecessary variables, making reliable measure comparing models different numbers predictors.FormulasUnadjusted \\(R^2\\):\\[\nR^2 = 1 - \\frac{SSR}{SST}\n\\]:\\(SSR\\) = Sum Squared Residuals (measures unexplained variance),\\(SSR\\) = Sum Squared Residuals (measures unexplained variance),\\(SST\\) = Total Sum Squares (measures total variance dependent variable).\\(SST\\) = Total Sum Squares (measures total variance dependent variable).Adjusted \\(R^2\\):\\[\nR^2_{\\text{adj}} = 1 - \\frac{SSR / (n - k)}{SST / (n - 1)}\n\\]Alternatively, can expressed terms \\(R^2\\) :\\[\nR^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k} \\right)\n\\]:\\(n\\) = Number observations,\\(n\\) = Number observations,\\(k\\) = Number estimated parameters model (including intercept).\\(k\\) = Number estimated parameters model (including intercept).Key InsightsPenalty Complexity: Unlike \\(R^2\\), adjusted \\(R^2\\) can decrease irrelevant variables added model adjusts number predictors relative sample size.Interpretation: represents proportion variance explained accounting model complexity.Comparison Across Models: Adjusted \\(R^2\\) useful comparing models different numbers predictors, discourages overfitting.Adjusted \\(R^2\\) Increase?adjusted \\(R^2\\) increase inclusion new variable improves model expected chance.Mathematically: typically occurs absolute value \\(t\\)-statistic new variable greater 1 (assuming large samples standard model assumptions).Limitations Adjusted \\(R^2\\)Sensitive Assumptions: Adjusted \\(R^2\\) assumes homoskedasticity (constant variance errors) autocorrelation. presence heteroskedasticity, interpretation may misleading.Substitute Hypothesis Testing: primary criterion deciding variables include model.\nUse \\(t\\)-tests evaluate significance individual coefficients.\nUse \\(F\\)-tests assessing joint significance multiple variables.\nUse \\(t\\)-tests evaluate significance individual coefficients.Use \\(F\\)-tests assessing joint significance multiple variables.","code":""},{"path":"model-specification-tests.html","id":"comparing-models-with-transformed-dependent-variables","chapter":"14 Model Specification Tests","heading":"14.2.4 Comparing Models with Transformed Dependent Variables","text":"comparing regression models different transformations dependent variable, level log-linear models, direct comparisons using traditional goodness--fit metrics like \\(R^2\\) adjusted \\(R^2\\) invalid. transformation changes scale dependent variable, affecting calculation Total Sum Squares (SST), denominator \\(R^2\\) calculations.Model SpecificationsLevel Model (Linear):\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]Log-Linear Model:\\[\n\\ln(y) = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]:\\(y\\) dependent variable,\\(y\\) dependent variable,\\(x_1\\) independent variable,\\(x_1\\) independent variable,\\(\\epsilon\\) represents error term.\\(\\epsilon\\) represents error term.Interpretation CoefficientsIn Level Model:\neffect \\(x_1\\) \\(y\\) constant, regardless magnitude \\(y\\). Specifically, one-unit increase \\(x_1\\) results change \\(\\beta_1\\) units \\(y\\). implies:\n\\[\n\\Delta y = \\beta_1 \\cdot \\Delta x_1\n\\]Level Model:\neffect \\(x_1\\) \\(y\\) constant, regardless magnitude \\(y\\). Specifically, one-unit increase \\(x_1\\) results change \\(\\beta_1\\) units \\(y\\). implies:\\[\n\\Delta y = \\beta_1 \\cdot \\Delta x_1\n\\]Log Model:\neffect \\(x_1\\) \\(y\\) proportional current level \\(y\\). one-unit increase \\(x_1\\) leads percentage change \\(y\\), approximately equal \\(100 \\times \\beta_1\\%\\). Specifically:\n\\[\n\\Delta \\ln(y) = \\beta_1 \\cdot \\Delta x_1 \\quad \\Rightarrow \\quad \\% \\Delta y \\approx 100 \\times \\beta_1\n\\]\nsmall values \\(y\\), absolute change small.\nlarge values \\(y\\), absolute change larger, reflecting multiplicative nature model.\nLog Model:\neffect \\(x_1\\) \\(y\\) proportional current level \\(y\\). one-unit increase \\(x_1\\) leads percentage change \\(y\\), approximately equal \\(100 \\times \\beta_1\\%\\). Specifically:\\[\n\\Delta \\ln(y) = \\beta_1 \\cdot \\Delta x_1 \\quad \\Rightarrow \\quad \\% \\Delta y \\approx 100 \\times \\beta_1\n\\]small values \\(y\\), absolute change small.large values \\(y\\), absolute change larger, reflecting multiplicative nature model.Compare \\(R^2\\) Adjusted \\(R^2\\) DirectlyThe level model explains variance original scale \\(y\\), log model explains variance logarithmic scale \\(y\\).SST (Total Sum Squares) differs across models dependent variable transformed, making direct comparisons \\(R^2\\) invalid.Adjusted \\(R^2\\) resolve issue also depends scale dependent variable.Approach Compare Model Fit Across TransformationsTo compare models scale original dependent variable (\\(y\\)), need “un-transform” predictions log model. ’s step--step procedure:Step--Step ProcedureEstimate Log Model\nFit log-linear model obtain predicted values:\n\\[\n\\widehat{\\ln(y)} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1\n\\]Estimate Log Model\nFit log-linear model obtain predicted values:\\[\n\\widehat{\\ln(y)} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1\n\\]Un-Transform Predictions\nConvert predicted values back original scale \\(y\\) using exponential function:\n\\[\n\\hat{m} = \\exp(\\widehat{\\ln(y)})\n\\]\ntransformation assumes errors homoskedastic log model.\nNote: correct potential bias due Jensen’s inequality, smearing estimator can applied, simplicity, use basic exponential transformation .\nUn-Transform Predictions\nConvert predicted values back original scale \\(y\\) using exponential function:\\[\n\\hat{m} = \\exp(\\widehat{\\ln(y)})\n\\]transformation assumes errors homoskedastic log model.Note: correct potential bias due Jensen’s inequality, smearing estimator can applied, simplicity, use basic exponential transformation .Fit Regression Without Intercept\nRegress actual \\(y\\) un-transformed predictions \\(\\hat{m}\\) without intercept:\n\\[\ny = \\alpha \\hat{m} + u\n\\]\ncoefficient \\(\\alpha\\) adjusts scaling differences predicted actual values.\nresidual term \\(u\\) captures unexplained variance.\nFit Regression Without Intercept\nRegress actual \\(y\\) un-transformed predictions \\(\\hat{m}\\) without intercept:\\[\ny = \\alpha \\hat{m} + u\n\\]coefficient \\(\\alpha\\) adjusts scaling differences predicted actual values.residual term \\(u\\) captures unexplained variance.Compute Scaled \\(R^2\\)\nCalculate squared correlation observed \\(y\\) predicted values \\(\\hat{y}\\) regression:\n\\[\nR^2_{\\text{scaled}} = \\left( \\text{Corr}(y, \\hat{y}) \\right)^2\n\\]\nscaled \\(R^2\\) represents well log-transformed model predicts original \\(y\\) natural scale.\nNow, can compare \\(R^2_{\\text{scaled}}\\) log model regular \\(R^2\\) level model.\nCompute Scaled \\(R^2\\)\nCalculate squared correlation observed \\(y\\) predicted values \\(\\hat{y}\\) regression:\\[\nR^2_{\\text{scaled}} = \\left( \\text{Corr}(y, \\hat{y}) \\right)^2\n\\]scaled \\(R^2\\) represents well log-transformed model predicts original \\(y\\) natural scale.Now, can compare \\(R^2_{\\text{scaled}}\\) log model regular \\(R^2\\) level model.Key InsightsIf \\(R^2_{\\text{scaled}}\\) (log model) > \\(R^2\\) (level model): log model fits data better.\\(R^2_{\\text{scaled}}\\) < \\(R^2\\) (level model): level model provides better fit.similar: Consider model diagnostics, theoretical justification, model simplicity.Caveats ConsiderationsHeteroskedasticity: heteroskedasticity present, un-transformation may introduce bias.Heteroskedasticity: heteroskedasticity present, un-transformation may introduce bias.Error Distribution: Log-transformed models assume multiplicative errors, may appropriate contexts.Error Distribution: Log-transformed models assume multiplicative errors, may appropriate contexts.Smearing Estimator (Advanced Correction): adjust bias back-transformation, apply smearing estimator:\n\\[\n\\hat{y} = \\exp(\\widehat{\\ln(y)}) \\cdot \\hat{S}\n\\]\n\\(\\hat{S}\\) mean exponentiated residuals log model.Smearing Estimator (Advanced Correction): adjust bias back-transformation, apply smearing estimator:\\[\n\\hat{y} = \\exp(\\widehat{\\ln(y)}) \\cdot \\hat{S}\n\\]\\(\\hat{S}\\) mean exponentiated residuals log model.","code":"\n# Install and load necessary libraries\n# install.packages(\"nonnest2\")  # Uncomment if not already installed\nlibrary(nonnest2)    # For Vuong Test\nlibrary(lmtest)      # For J-Test\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx <- rnorm(n, mean = 50, sd = 10)\nz <- rnorm(n, mean = 100, sd = 20)\nepsilon <- rnorm(n)\n\n# Competing models (non-nested)\n# Model A: Linear relationship with x\ny <- 5 + 0.3 * x + epsilon\nmodel_A <- lm(y ~ x)\n\n# Model B: Log-linear relationship with z\nmodel_B <- lm(y ~ log(z))\n\n# ----------------------------------------------------------------------\n# Vuong Test (Correct Function)\n# ----------------------------------------------------------------------\nvuong_test <- vuongtest(model_A, model_B)\nprint(vuong_test)\n#> \n#> Model 1 \n#>  Class: lm \n#>  Call: lm(formula = y ~ x)\n#> \n#> Model 2 \n#>  Class: lm \n#>  Call: lm(formula = y ~ log(z))\n#> \n#> Variance test \n#>   H0: Model 1 and Model 2 are indistinguishable \n#>   H1: Model 1 and Model 2 are distinguishable \n#>     w2 = 0.681,   p = 2.35e-08\n#> \n#> Non-nested likelihood ratio test \n#>   H0: Model fits are equal for the focal population \n#>   H1A: Model 1 fits better than Model 2 \n#>     z = 13.108,   p = <2e-16\n#>   H1B: Model 2 fits better than Model 1 \n#>     z = 13.108,   p = 1\n\n# ----------------------------------------------------------------------\n# Davidson–MacKinnon J-Test\n# ----------------------------------------------------------------------\n\n# Step 1: Testing Model A against Model B\n# Obtain fitted values from Model B\nfitted_B <- fitted(model_B)\n\n# Auxiliary regression: Add fitted_B to Model A\nj_test_A_vs_B <- lm(y ~ x + fitted_B)\nsummary(j_test_A_vs_B)\n#> \n#> Call:\n#> lm(formula = y ~ x + fitted_B)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.8717 -0.6573 -0.1223  0.6154  2.0952 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 14.70881   25.98307   0.566    0.573    \n#> x            0.28671    0.01048  27.358   <2e-16 ***\n#> fitted_B    -0.43702    1.27500  -0.343    0.733    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.951 on 97 degrees of freedom\n#> Multiple R-squared:  0.8854, Adjusted R-squared:  0.883 \n#> F-statistic: 374.5 on 2 and 97 DF,  p-value: < 2.2e-16\n\n# Step 2: Testing Model B against Model A\n# Obtain fitted values from Model A\nfitted_A <- fitted(model_A)\n\n# Auxiliary regression: Add fitted_A to Model B\nj_test_B_vs_A <- lm(y ~ log(z) + fitted_A)\nsummary(j_test_B_vs_A)\n#> \n#> Call:\n#> lm(formula = y ~ log(z) + fitted_A)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.8717 -0.6573 -0.1223  0.6154  2.0952 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.77868    2.39275  -0.325    0.746    \n#> log(z)       0.16829    0.49097   0.343    0.733    \n#> fitted_A     1.00052    0.03657  27.358   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.951 on 97 degrees of freedom\n#> Multiple R-squared:  0.8854, Adjusted R-squared:  0.883 \n#> F-statistic: 374.5 on 2 and 97 DF,  p-value: < 2.2e-16"},{"path":"model-specification-tests.html","id":"heteroskedasticity-tests","chapter":"14 Model Specification Tests","heading":"14.3 Heteroskedasticity Tests","text":"Heteroskedasticity occurs variance error terms (\\(\\epsilon_i\\)) regression model constant across observations. violates Classical OLS Assumption, specifically assumption homoskedasticity (Assumption A4 Homoskedasticity Gauss-Markov Theorem), states:\\[\n\\text{Var}(\\epsilon_i) = \\sigma^2 \\quad \\forall \\, \n\\]heteroskedasticity present:Ordinary Least Squares estimators remain unbiased become inefficient (.e., longer Best Linear Unbiased Estimators—BLUE).Ordinary Least Squares estimators remain unbiased become inefficient (.e., longer Best Linear Unbiased Estimators—BLUE).standard errors estimates biased, leading unreliable hypothesis tests (e.g., \\(t\\)-tests \\(F\\)-tests).standard errors estimates biased, leading unreliable hypothesis tests (e.g., \\(t\\)-tests \\(F\\)-tests).Detecting heteroskedasticity crucial ensuring validity regression results. section covers key tests used identify heteroskedasticity:Breusch–Pagan TestWhite TestGoldfeld–Quandt TestPark TestGlejser Test","code":""},{"path":"model-specification-tests.html","id":"sec-breusch--pagan-test","chapter":"14 Model Specification Tests","heading":"14.3.1 Breusch–Pagan Test","text":"Breusch–Pagan (BP) Test one widely used tests detecting heteroskedasticity (Breusch Pagan 1979). examines whether variance residuals depends independent variables.HypothesesNull Hypothesis (\\(H_0\\)): Homoskedasticity (\\(\\text{Var}(\\epsilon_i) = \\sigma^2\\) constant).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists; variance \\(\\epsilon_i\\) depends independent variables.ProcedureEstimate original regression model:\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + \\epsilon_i\n\\]Obtain residuals \\(\\hat{\\epsilon}_i\\) regression.Compute squared residuals:\\[\n\\hat{\\epsilon}_i^2\n\\]Auxiliary Regression: Regress squared residuals independent variables:\\[\n\\hat{\\epsilon}_i^2 = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\dots + \\alpha_k x_{ki} + u_i\n\\]Calculate Test Statistic:BP test statistic :\\[\n\\text{BP} = n \\cdot R^2_{\\text{aux}}\n\\]:\\(n\\) sample size,\\(n\\) sample size,\\(R^2_{\\text{aux}}\\) \\(R^2\\) auxiliary regression.\\(R^2_{\\text{aux}}\\) \\(R^2\\) auxiliary regression.Decision Rule:\\(H_0\\), BP statistic follows chi-squared distribution \\(k\\) degrees freedom (\\(k\\) number independent variables):\\[\n\\text{BP} \\sim \\chi^2_k\n\\]Reject \\(H_0\\) BP statistic exceeds critical value chi-squared distribution.Advantages LimitationsAdvantage: Simple implement; directly tests relationship residual variance regressors.Limitation: Sensitive non-normality; less effective heteroskedasticity linearly related independent variables.","code":""},{"path":"model-specification-tests.html","id":"sec-white-test-hetero","chapter":"14 Model Specification Tests","heading":"14.3.2 White Test","text":"White Test general heteroskedasticity test require specifying form heteroskedasticity (White 1980). can detect linear nonlinear forms.HypothesesNull Hypothesis (\\(H_0\\)): Homoskedasticity.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity (form).ProcedureEstimate original regression model obtain residuals \\(\\hat{\\epsilon}_i\\).Estimate original regression model obtain residuals \\(\\hat{\\epsilon}_i\\).Auxiliary Regression: Regress squared residuals :\noriginal independent variables (\\(x_{1i}, x_{2i}, \\dots, x_{ki}\\)),\nsquares (\\(x_{1i}^2, x_{2i}^2, \\dots, x_{ki}^2\\)),\ncross-products (e.g., \\(x_{1i} x_{2i}\\)).\nauxiliary regression :\n\\[\n\\hat{\\epsilon}_i^2 = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\dots + \\alpha_k x_{ki} + \\alpha_{k+1} x_{1i}^2 + \\dots + \\alpha_{2k} x_{ki}^2 + \\alpha_{2k+1} (x_{1i} x_{2i}) + u_i\n\\]Auxiliary Regression: Regress squared residuals :original independent variables (\\(x_{1i}, x_{2i}, \\dots, x_{ki}\\)),squares (\\(x_{1i}^2, x_{2i}^2, \\dots, x_{ki}^2\\)),cross-products (e.g., \\(x_{1i} x_{2i}\\)).auxiliary regression :\\[\n\\hat{\\epsilon}_i^2 = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\dots + \\alpha_k x_{ki} + \\alpha_{k+1} x_{1i}^2 + \\dots + \\alpha_{2k} x_{ki}^2 + \\alpha_{2k+1} (x_{1i} x_{2i}) + u_i\n\\]Calculate Test Statistic:\n\\[\n\\text{White} = n \\cdot R^2_{\\text{aux}}\n\\]Calculate Test Statistic:\\[\n\\text{White} = n \\cdot R^2_{\\text{aux}}\n\\]Decision Rule:Decision Rule:\\(H_0\\), statistic follows chi-squared distribution degrees freedom equal number auxiliary regressors:\n\\[\n\\text{White} \\sim \\chi^2_{\\text{df}}\n\\]\\(H_0\\), statistic follows chi-squared distribution degrees freedom equal number auxiliary regressors:\\[\n\\text{White} \\sim \\chi^2_{\\text{df}}\n\\]Reject \\(H_0\\) statistic exceeds critical chi-squared value.Reject \\(H_0\\) statistic exceeds critical chi-squared value.Advantages LimitationsAdvantage: Can detect wide range heteroskedasticity patterns.Limitation: May suffer overfitting small samples due many auxiliary regressors.","code":""},{"path":"model-specification-tests.html","id":"sec-goldfeld--quandt-test","chapter":"14 Model Specification Tests","heading":"14.3.3 Goldfeld–Quandt Test","text":"Goldfeld–Quandt Test simple test detects heteroskedasticity comparing variance residuals two different subsets data (Goldfeld Quandt 1965).HypothesesNull Hypothesis (\\(H_0\\)): Homoskedasticity.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity; variances differ groups.ProcedureSort data based independent variable suspected cause heteroskedasticity.Sort data based independent variable suspected cause heteroskedasticity.Split data three groups:\nGroup 1: Lower values,\nGroup 2: Middle values (often omitted),\nGroup 3: Higher values.\nSplit data three groups:Group 1: Lower values,Group 2: Middle values (often omitted),Group 3: Higher values.Estimate regression model separately Groups 1 3. Obtain residual sum squares (\\(SSR_1\\) \\(SSR_2\\)).Estimate regression model separately Groups 1 3. Obtain residual sum squares (\\(SSR_1\\) \\(SSR_2\\)).Calculate Test Statistic:\n\\[\nF = \\frac{SSR_2 / (n_2 - k)}{SSR_1 / (n_1 - k)}\n\\]\n:\n\\(n_1\\) \\(n_2\\) number observations Groups 1 3, respectively,\n\\(k\\) number estimated parameters.\nCalculate Test Statistic:\\[\nF = \\frac{SSR_2 / (n_2 - k)}{SSR_1 / (n_1 - k)}\n\\]:\\(n_1\\) \\(n_2\\) number observations Groups 1 3, respectively,\\(k\\) number estimated parameters.Decision Rule:Decision Rule:\\(H_0\\), test statistic follows \\(F\\)-distribution \\((n_2 - k, n_1 - k)\\) degrees freedom:\n\\[\nF \\sim F_{(n_2 - k, n_1 - k)}\n\\]\\(H_0\\), test statistic follows \\(F\\)-distribution \\((n_2 - k, n_1 - k)\\) degrees freedom:\\[\nF \\sim F_{(n_2 - k, n_1 - k)}\n\\]Reject \\(H_0\\) \\(F\\) exceeds critical value.Reject \\(H_0\\) \\(F\\) exceeds critical value.Advantages LimitationsAdvantage: Simple apply heteroskedasticity suspected vary systematically independent variable.Limitation: Requires arbitrary splitting data assumes error variance changes abruptly groups.","code":""},{"path":"model-specification-tests.html","id":"sec-park-test","chapter":"14 Model Specification Tests","heading":"14.3.4 Park Test","text":"Park Test identifies heteroskedasticity modeling error variance function independent variable (Park 1966).HypothesesNull Hypothesis (\\(H_0\\)): Homoskedasticity.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity; variance depends independent variable.ProcedureEstimate original regression obtain residuals \\(\\hat{\\epsilon}_i\\).Estimate original regression obtain residuals \\(\\hat{\\epsilon}_i\\).Transform residuals: Take natural logarithm squared residuals:\n\\[\n\\ln(\\hat{\\epsilon}_i^2)\n\\]Transform residuals: Take natural logarithm squared residuals:\\[\n\\ln(\\hat{\\epsilon}_i^2)\n\\]Auxiliary Regression: Regress \\(\\ln(\\hat{\\epsilon}_i^2)\\) independent variable(s):\n\\[\n\\ln(\\hat{\\epsilon}_i^2) = \\alpha_0 + \\alpha_1 \\ln(x_i) + u_i\n\\]Auxiliary Regression: Regress \\(\\ln(\\hat{\\epsilon}_i^2)\\) independent variable(s):\\[\n\\ln(\\hat{\\epsilon}_i^2) = \\alpha_0 + \\alpha_1 \\ln(x_i) + u_i\n\\]Decision Rule:Decision Rule:Test whether \\(\\alpha_1 = 0\\) using \\(t\\)-test.Reject \\(H_0\\) \\(\\alpha_1\\) statistically significant, indicating heteroskedasticity.Advantages LimitationsAdvantage: Simple implement; works well variance follows log-linear relationship.Limitation: Assumes specific functional form variance, may hold practice.","code":""},{"path":"model-specification-tests.html","id":"sec-glejser-test","chapter":"14 Model Specification Tests","heading":"14.3.5 Glejser Test","text":"Glejser Test detects heteroskedasticity regressing absolute value residuals independent variables (Glejser 1969).HypothesesNull Hypothesis (\\(H_0\\)): Homoskedasticity.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists.ProcedureEstimate original regression obtain residuals \\(\\hat{\\epsilon}_i\\).Estimate original regression obtain residuals \\(\\hat{\\epsilon}_i\\).Auxiliary Regression: Regress absolute residuals independent variables:\n\\[\n|\\hat{\\epsilon}_i| = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\dots + \\alpha_k x_{ki} + u_i\n\\]Auxiliary Regression: Regress absolute residuals independent variables:\\[\n|\\hat{\\epsilon}_i| = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\dots + \\alpha_k x_{ki} + u_i\n\\]Decision Rule:Decision Rule:Test significance coefficients (\\(\\alpha_1, \\alpha_2, \\dots\\)) using \\(t\\)-tests.Reject \\(H_0\\) coefficient statistically significant, indicating heteroskedasticity.Advantages LimitationsAdvantage: Flexible; can detect various forms heteroskedasticity.Limitation: Sensitive outliers since relies absolute residuals.","code":""},{"path":"model-specification-tests.html","id":"summary-of-heteroskedasticity-tests","chapter":"14 Model Specification Tests","heading":"14.3.6 Summary of Heteroskedasticity Tests","text":"Detecting heteroskedasticity critical ensuring reliability regression models. test strengths limitations, combining multiple tests can provide robust insights. heteroskedasticity detected, consider using robust standard errors alternative estimation techniques (e.g., Generalized Least Squares Weighted Least Squares) address issue.Interpretation ResultsBreusch–Pagan Test\nNull Hypothesis (\\(H_0\\)): Homoskedasticity (constant error variance).\nAlternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists (error variance depends predictors).\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence heteroskedasticity.\n\nBreusch–Pagan TestNull Hypothesis (\\(H_0\\)): Homoskedasticity (constant error variance).Null Hypothesis (\\(H_0\\)): Homoskedasticity (constant error variance).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists (error variance depends predictors).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists (error variance depends predictors).Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence heteroskedasticity.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Evidence heteroskedasticity.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence heteroskedasticity.White Test\nNull Hypothesis (\\(H_0\\)): Homoskedasticity.\nAlternative Hypothesis (\\(H_1\\)): Heteroskedasticity (form, linear nonlinear).\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Presence heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Homoskedasticity likely holds.\n\nWhite TestNull Hypothesis (\\(H_0\\)): Homoskedasticity.Null Hypothesis (\\(H_0\\)): Homoskedasticity.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity (form, linear nonlinear).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity (form, linear nonlinear).Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Presence heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Homoskedasticity likely holds.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Presence heteroskedasticity.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Homoskedasticity likely holds.Goldfeld–Quandt Test\nNull Hypothesis (\\(H_0\\)): Homoskedasticity (equal variances across groups).\nAlternative Hypothesis (\\(H_1\\)): Heteroskedasticity (unequal variances groups).\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Variances differ groups, indicating heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → significant evidence heteroskedasticity.\n\nGoldfeld–Quandt TestNull Hypothesis (\\(H_0\\)): Homoskedasticity (equal variances across groups).Null Hypothesis (\\(H_0\\)): Homoskedasticity (equal variances across groups).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity (unequal variances groups).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity (unequal variances groups).Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Variances differ groups, indicating heteroskedasticity.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → significant evidence heteroskedasticity.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Variances differ groups, indicating heteroskedasticity.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → significant evidence heteroskedasticity.Park Test\nNull Hypothesis (\\(H_0\\)): relationship variance errors predictor(s) (homoskedasticity).\nAlternative Hypothesis (\\(H_1\\)): Variance errors depends predictor(s).\nDecision Rule:\nReject \\(H_0\\) coefficient \\(\\log(x_1)\\) statistically significant (p-value \\(< 0.05\\)).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\).\n\nPark TestNull Hypothesis (\\(H_0\\)): relationship variance errors predictor(s) (homoskedasticity).Null Hypothesis (\\(H_0\\)): relationship variance errors predictor(s) (homoskedasticity).Alternative Hypothesis (\\(H_1\\)): Variance errors depends predictor(s).Alternative Hypothesis (\\(H_1\\)): Variance errors depends predictor(s).Decision Rule:\nReject \\(H_0\\) coefficient \\(\\log(x_1)\\) statistically significant (p-value \\(< 0.05\\)).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\).\nDecision Rule:Reject \\(H_0\\) coefficient \\(\\log(x_1)\\) statistically significant (p-value \\(< 0.05\\)).Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\).Glejser Test\nNull Hypothesis (\\(H_0\\)): Homoskedasticity (relationship absolute residuals predictors).\nAlternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists.\nDecision Rule:\nReject \\(H_0\\) predictor statistically significant (p-value \\(< 0.05\\)).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\).\n\nGlejser TestNull Hypothesis (\\(H_0\\)): Homoskedasticity (relationship absolute residuals predictors).Null Hypothesis (\\(H_0\\)): Homoskedasticity (relationship absolute residuals predictors).Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists.Alternative Hypothesis (\\(H_1\\)): Heteroskedasticity exists.Decision Rule:\nReject \\(H_0\\) predictor statistically significant (p-value \\(< 0.05\\)).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\).\nDecision Rule:Reject \\(H_0\\) predictor statistically significant (p-value \\(< 0.05\\)).Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\).","code":"\n# Install and load necessary libraries\n# install.packages(\"lmtest\")      # For Breusch–Pagan Test\n# install.packages(\"car\")         # For additional regression diagnostics\n# install.packages(\"sandwich\")    # For robust covariance estimation\n\n\nlibrary(lmtest)\nlibrary(car)\nlibrary(sandwich)\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx1 <- rnorm(n, mean = 50, sd = 10)\nx2 <- rnorm(n, mean = 30, sd = 5)\nepsilon <-\n    rnorm(n, sd = x1 * 0.1)  # Heteroskedastic errors increasing with x1\ny <- 5 + 0.4 * x1 - 0.3 * x2 + epsilon\n\n# Original regression model\nmodel <- lm(y ~ x1 + x2)\n\n# ----------------------------------------------------------------------\n# 1. Breusch–Pagan Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: Homoskedasticity\nbp_test <- bptest(model)\nprint(bp_test)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  model\n#> BP = 7.8141, df = 2, p-value = 0.0201\n\n# ----------------------------------------------------------------------\n# 2. White Test (using Breusch–Pagan framework with squares & interactions)\n# ----------------------------------------------------------------------\n# Create squared and interaction terms\nmodel_white <-\n    lm(residuals(model) ^ 2 ~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2) + I(x1 * x2))\nwhite_statistic <-\n    summary(model_white)$r.squared * n  # White Test Statistic\ndf_white <-\n    length(coef(model_white)) - 1              # Degrees of freedom\np_value_white <- 1 - pchisq(white_statistic, df_white)\n\n# Display White Test result\ncat(\"White Test Statistic:\", white_statistic, \"\\n\")\n#> White Test Statistic: 11.85132\ncat(\"Degrees of Freedom:\", df_white, \"\\n\")\n#> Degrees of Freedom: 5\ncat(\"P-value:\", p_value_white, \"\\n\")\n#> P-value: 0.0368828\n\n# ----------------------------------------------------------------------\n# 3. Goldfeld–Quandt Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: Homoskedasticity\n# Sort data by x1 (suspected source of heteroskedasticity)\ngq_test <-\n    gqtest(model, order.by = ~ x1, fraction = 0.2)  # Omit middle 20% of data\nprint(gq_test)\n#> \n#>  Goldfeld-Quandt test\n#> \n#> data:  model\n#> GQ = 1.8352, df1 = 37, df2 = 37, p-value = 0.03434\n#> alternative hypothesis: variance increases from segment 1 to 2\n\n# ----------------------------------------------------------------------\n# 4. Park Test\n# ----------------------------------------------------------------------\n# Step 1: Get residuals and square them\nresiduals_squared <- residuals(model) ^ 2\n\n# Step 2: Log-transform squared residuals\nlog_residuals_squared <- log(residuals_squared)\n\n# Step 3: Regress log(residuals^2) on log(x1) (assuming variance depends on x1)\npark_test <- lm(log_residuals_squared ~ log(x1))\nsummary(park_test)\n#> \n#> Call:\n#> lm(formula = log_residuals_squared ~ log(x1))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3633 -1.3424  0.4218  1.6089  3.0697 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)  -1.6319     4.5982  -0.355    0.723\n#> log(x1)       0.8903     1.1737   0.759    0.450\n#> \n#> Residual standard error: 2.171 on 98 degrees of freedom\n#> Multiple R-squared:  0.005837,   Adjusted R-squared:  -0.004308 \n#> F-statistic: 0.5754 on 1 and 98 DF,  p-value: 0.4499\n\n# ----------------------------------------------------------------------\n# 5. Glejser Test\n# ----------------------------------------------------------------------\n# Step 1: Absolute value of residuals\nabs_residuals <- abs(residuals(model))\n\n# Step 2: Regress absolute residuals on independent variables\nglejser_test <- lm(abs_residuals ~ x1 + x2)\nsummary(glejser_test)\n#> \n#> Call:\n#> lm(formula = abs_residuals ~ x1 + x2)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3096 -2.2680 -0.4564  1.9554  8.3921 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.755846   2.554842   0.296   0.7680  \n#> x1           0.064896   0.032852   1.975   0.0511 .\n#> x2          -0.008495   0.062023  -0.137   0.8913  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.98 on 97 degrees of freedom\n#> Multiple R-squared:  0.0392, Adjusted R-squared:  0.01939 \n#> F-statistic: 1.979 on 2 and 97 DF,  p-value: 0.1438"},{"path":"model-specification-tests.html","id":"functional-form-tests","chapter":"14 Model Specification Tests","heading":"14.4 Functional Form Tests","text":"Functional form misspecification occurs chosen regression model correctly represent true relationship dependent independent variables. can happen due :Omitted variables (important predictors included),Incorrect transformations variables (e.g., missing nonlinear relationships),Incorrect interaction terms (missing interaction effects variables),Inappropriate linearity assumptions.Functional form errors can lead biased inconsistent estimators, undermining validity statistical inferences. detect issues, several diagnostic tests available.Key Functional Form Tests:Ramsey RESET Test (Regression Equation Specification Error Test)Harvey–Collier TestRainbow TestEach test focuses identifying different aspects potential model misspecification.","code":""},{"path":"model-specification-tests.html","id":"sec-ramsey-reset-test","chapter":"14 Model Specification Tests","heading":"14.4.1 Ramsey RESET Test (Regression Equation Specification Error Test)","text":"Ramsey RESET Test one widely used tests detect functional form misspecification (Ramsey 1969). examines whether adding nonlinear transformations fitted values (regressors) improves model fit.HypothesesNull Hypothesis (\\(H_0\\)): model correctly specified.Alternative Hypothesis (\\(H_1\\)): model suffers omitted variables, incorrect functional form, specification errors.ProcedureEstimate original regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + \\epsilon_i\n\\]Estimate original regression model:\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + \\epsilon_i\n\\]Obtain fitted values:\n\\[\n\\hat{y}_i\n\\]Obtain fitted values:\\[\n\\hat{y}_i\n\\]Augment model powers fitted values (squared, cubed, etc.):\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_k x_{ki} + \\gamma_1 \\hat{y}_i^2 + \\gamma_2 \\hat{y}_i^3 + u_i\n\\]Augment model powers fitted values (squared, cubed, etc.):\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_k x_{ki} + \\gamma_1 \\hat{y}_i^2 + \\gamma_2 \\hat{y}_i^3 + u_i\n\\]Test joint significance added terms:\n\\[\nH_0: \\gamma_1 = \\gamma_2 = 0\n\\]Test joint significance added terms:\\[\nH_0: \\gamma_1 = \\gamma_2 = 0\n\\]Compute F-statistic:\n\\[\nF = \\frac{(SSR_{\\text{restricted}} - SSR_{\\text{unrestricted}}) / q}{SSR_{\\text{unrestricted}} / (n - k - q - 1)}\n\\]\n:\n\\(SSR_{\\text{restricted}}\\) = Sum Squared Residuals original model,\n\\(SSR_{\\text{unrestricted}}\\) = SSR augmented model,\n\\(q\\) = Number additional terms (e.g., 2 adding \\(\\hat{y}^2\\) \\(\\hat{y}^3\\)),\n\\(n\\) = Sample size,\n\\(k\\) = Number predictors original model.\nCompute F-statistic:\\[\nF = \\frac{(SSR_{\\text{restricted}} - SSR_{\\text{unrestricted}}) / q}{SSR_{\\text{unrestricted}} / (n - k - q - 1)}\n\\]:\\(SSR_{\\text{restricted}}\\) = Sum Squared Residuals original model,\\(SSR_{\\text{unrestricted}}\\) = SSR augmented model,\\(q\\) = Number additional terms (e.g., 2 adding \\(\\hat{y}^2\\) \\(\\hat{y}^3\\)),\\(n\\) = Sample size,\\(k\\) = Number predictors original model.Decision RuleUnder \\(H_0\\), F-statistic follows \\(F\\)-distribution \\((q, n - k - q - 1)\\) degrees freedom.Reject \\(H_0\\) F-statistic exceeds critical value, indicating functional form misspecification.Advantages LimitationsAdvantage: Simple implement; detects omitted variables incorrect functional forms.Limitation: identify variable functional form incorrect—indicates presence issue.","code":""},{"path":"model-specification-tests.html","id":"sec-harvey--collier-test","chapter":"14 Model Specification Tests","heading":"14.4.2 Harvey–Collier Test","text":"Harvey–Collier Test evaluates whether model’s residuals display systematic patterns, indicate functional form misspecification (Harvey Collier 1977). based testing non-zero mean residuals projection onto specific components.HypothesesNull Hypothesis (\\(H_0\\)): model correctly specified (residuals random noise zero mean).Alternative Hypothesis (\\(H_1\\)): model misspecified (residuals contain systematic patterns).ProcedureEstimate original regression model obtain residuals \\(\\hat{\\epsilon}_i\\).Estimate original regression model obtain residuals \\(\\hat{\\epsilon}_i\\).Project residuals onto space spanned specially constructed test vector (often derived inverse design matrix linear regression).Project residuals onto space spanned specially constructed test vector (often derived inverse design matrix linear regression).Calculate Harvey–Collier statistic:\n\\[\nt = \\frac{\\bar{\\epsilon}}{\\text{SE}(\\bar{\\epsilon})}\n\\]\n:\n\\(\\bar{\\epsilon}\\) mean projected residuals,\n\\(\\text{SE}(\\bar{\\epsilon})\\) standard error mean residual.\nCalculate Harvey–Collier statistic:\\[\nt = \\frac{\\bar{\\epsilon}}{\\text{SE}(\\bar{\\epsilon})}\n\\]:\\(\\bar{\\epsilon}\\) mean projected residuals,\\(\\text{SE}(\\bar{\\epsilon})\\) standard error mean residual.Decision Rule:test statistic follows \\(t\\)-distribution \\(H_0\\).Reject \\(H_0\\) \\(t\\)-statistic significantly different zero.Advantages LimitationsAdvantage: Simple apply interpret; good detecting subtle misspecifications.Limitation: Sensitive outliers; may reduced power small samples.","code":""},{"path":"model-specification-tests.html","id":"sec-rainbow-test","chapter":"14 Model Specification Tests","heading":"14.4.3 Rainbow Test","text":"Rainbow Test general-purpose diagnostic tool functional form misspecification (Utts 1982). compares performance model full sample versus central subsample, central subsample contains observations near median independent variables.HypothesesNull Hypothesis (\\(H_0\\)): model correctly specified.Alternative Hypothesis (\\(H_1\\)): model misspecified.ProcedureEstimate regression model full dataset record residuals.Estimate regression model full dataset record residuals.Identify central subsample (e.g., observations near median key predictors).Identify central subsample (e.g., observations near median key predictors).Estimate model central subsample.Estimate model central subsample.Compare predictive accuracy full sample subsample using F-statistic:\n\\[\nF = \\frac{(SSR_{\\text{full}} - SSR_{\\text{subsample}}) / q}{SSR_{\\text{subsample}} / (n - k - q)}\n\\]\n\\(q\\) number restrictions implied using subsample.Compare predictive accuracy full sample subsample using F-statistic:\\[\nF = \\frac{(SSR_{\\text{full}} - SSR_{\\text{subsample}}) / q}{SSR_{\\text{subsample}} / (n - k - q)}\n\\]\\(q\\) number restrictions implied using subsample.Decision RuleUnder \\(H_0\\), test statistic follows \\(F\\)-distribution.Reject \\(H_0\\) F-statistic significant, indicating model misspecification.Advantages LimitationsAdvantage: Robust various forms misspecification.Limitation: Choice subsample may influence results; less informative specific nature misspecification.","code":""},{"path":"model-specification-tests.html","id":"summary-of-functional-form-tests","chapter":"14 Model Specification Tests","heading":"14.4.4 Summary of Functional Form Tests","text":"Functional form misspecification can severely distort regression results, leading biased estimates invalid inferences. single test can detect types misspecification, using combination tests provides robust framework model diagnostics.Interpretation ResultsRamsey RESET Test (Regression Equation Specification Error Test)\nNull Hypothesis (\\(H_0\\)): model correctly specified.\nAlternative Hypothesis (\\(H_1\\)): model suffers omitted variables, incorrect functional form, specification errors.\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (e.g., missing nonlinear terms).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence misspecification.\n\nRamsey RESET Test (Regression Equation Specification Error Test)Null Hypothesis (\\(H_0\\)): model correctly specified.Null Hypothesis (\\(H_0\\)): model correctly specified.Alternative Hypothesis (\\(H_1\\)): model suffers omitted variables, incorrect functional form, specification errors.Alternative Hypothesis (\\(H_1\\)): model suffers omitted variables, incorrect functional form, specification errors.Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (e.g., missing nonlinear terms).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence misspecification.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (e.g., missing nonlinear terms).Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → strong evidence misspecification.Harvey–Collier Test\nNull Hypothesis (\\(H_0\\)): model correctly specified (residuals random noise zero mean).\nAlternative Hypothesis (\\(H_1\\)): model misspecified (residuals contain systematic patterns).\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Model misspecification detected (non-random residual patterns).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → evidence misspecification.\n\nHarvey–Collier TestNull Hypothesis (\\(H_0\\)): model correctly specified (residuals random noise zero mean).Null Hypothesis (\\(H_0\\)): model correctly specified (residuals random noise zero mean).Alternative Hypothesis (\\(H_1\\)): model misspecified (residuals contain systematic patterns).Alternative Hypothesis (\\(H_1\\)): model misspecified (residuals contain systematic patterns).Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Model misspecification detected (non-random residual patterns).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → evidence misspecification.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Model misspecification detected (non-random residual patterns).Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → evidence misspecification.Rainbow Test\nNull Hypothesis (\\(H_0\\)): model correctly specified.\nAlternative Hypothesis (\\(H_1\\)): model misspecified.\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (model performs differently subsets).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Model specification appears valid.\n\nRainbow TestNull Hypothesis (\\(H_0\\)): model correctly specified.Null Hypothesis (\\(H_0\\)): model correctly specified.Alternative Hypothesis (\\(H_1\\)): model misspecified.Alternative Hypothesis (\\(H_1\\)): model misspecified.Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (model performs differently subsets).\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Model specification appears valid.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\) → Evidence model misspecification (model performs differently subsets).Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\) → Model specification appears valid.","code":"\n# Install and load necessary libraries\n# install.packages(\"lmtest\")      # For RESET and Harvey–Collier Test\n# install.packages(\"car\")         # For diagnostic tests\n# install.packages(\"strucchange\") # For Rainbow Test\n\nlibrary(lmtest)\nlibrary(car)\nlibrary(strucchange)\n\n# Simulated dataset\nset.seed(123)\nn <- 100\nx1 <- rnorm(n, mean = 50, sd = 10)\nx2 <- rnorm(n, mean = 30, sd = 5)\nepsilon <- rnorm(n)\ny <- 5 + 0.4 * x1 - 0.3 * x2 + epsilon\n\n# Original regression model\nmodel <- lm(y ~ x1 + x2)\n\n# ----------------------------------------------------------------------\n# 1. Ramsey RESET Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: The model is correctly specified\nreset_test <-\n    resettest(model, power = 2:3, type = \"fitted\")  # Adds ŷ² and ŷ³\nprint(reset_test)\n#> \n#>  RESET test\n#> \n#> data:  model\n#> RESET = 0.1921, df1 = 2, df2 = 95, p-value = 0.8255\n\n# ----------------------------------------------------------------------\n# 2. Harvey–Collier Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: The model is correctly specified (residuals have zero mean)\nhc_test <- harvtest(model)\nprint(hc_test)\n#> \n#>  Harvey-Collier test\n#> \n#> data:  model\n#> HC = 0.041264, df = 96, p-value = 0.9672\n\n# ----------------------------------------------------------------------\n# 3. Rainbow Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: The model is correctly specified\nrainbow_test <- lmtest::raintest (model)\nprint(rainbow_test)\n#> \n#>  Rainbow test\n#> \n#> data:  model\n#> Rain = 1.1857, df1 = 50, df2 = 47, p-value = 0.279"},{"path":"model-specification-tests.html","id":"autocorrelation-tests","chapter":"14 Model Specification Tests","heading":"14.5 Autocorrelation Tests","text":"Autocorrelation (also known serial correlation) occurs error terms (\\(\\epsilon_t\\)) regression model correlated across observations, violating assumption independence classical linear regression model. issue particularly common time-series data, observations ordered time.Consequences Autocorrelation:OLS estimators remain unbiased become inefficient, meaning minimum variance among linear unbiased estimators.Standard errors biased, leading unreliable hypothesis tests (e.g., \\(t\\)-tests \\(F\\)-tests).Potential underestimation standard errors, increasing risk Type errors (false positives).","code":""},{"path":"model-specification-tests.html","id":"sec-durbin--watson-test","chapter":"14 Model Specification Tests","heading":"14.5.1 Durbin–Watson Test","text":"Durbin–Watson (DW) Test widely used test detecting first-order autocorrelation, current error term correlated previous one:\\[\n\\epsilon_t = \\rho \\, \\epsilon_{t-1} + u_t\n\\]:\\(\\rho\\) autocorrelation coefficient,\\(\\rho\\) autocorrelation coefficient,\\(u_t\\) white noise error term.\\(u_t\\) white noise error term.HypothesesNull Hypothesis (\\(H_0\\)): first-order autocorrelation (\\(\\rho = 0\\)).Alternative Hypothesis (\\(H_1\\)): First-order autocorrelation exists (\\(\\rho \\neq 0\\)).Durbin–Watson Test StatisticThe DW statistic calculated :\\[\nDW = \\frac{\\sum_{t=2}^{n} (\\hat{\\epsilon}_t - \\hat{\\epsilon}_{t-1})^2}{\\sum_{t=1}^{n} \\hat{\\epsilon}_t^2}\n\\]:\\(\\hat{\\epsilon}_t\\) residuals regression,\\(\\hat{\\epsilon}_t\\) residuals regression,\\(n\\) number observations.\\(n\\) number observations.Decision RuleThe DW statistic ranges 0 4:\nDW ≈ 2: autocorrelation.\nDW < 2: Positive autocorrelation.\nDW > 2: Negative autocorrelation.\nDW ≈ 2: autocorrelation.DW < 2: Positive autocorrelation.DW > 2: Negative autocorrelation.precise interpretation:Use Durbin–Watson critical values (\\(d_L\\) \\(d_U\\)) form decision boundaries.Use Durbin–Watson critical values (\\(d_L\\) \\(d_U\\)) form decision boundaries.test statistic falls inconclusive range, consider alternative tests like Breusch–Godfrey test.test statistic falls inconclusive range, consider alternative tests like Breusch–Godfrey test.Advantages LimitationsAdvantage: Simple compute; specifically designed detecting first-order autocorrelation.Limitation: Inconclusive cases; invalid lagged dependent variables included model.","code":""},{"path":"model-specification-tests.html","id":"sec-breusch--godfrey-test","chapter":"14 Model Specification Tests","heading":"14.5.2 Breusch–Godfrey Test","text":"Breusch–Godfrey (BG) Test general approach can detect higher-order autocorrelation (e.g., second-order, third-order) valid even lagged dependent variables included model (Breusch 1978; Godfrey 1978).HypothesesNull Hypothesis (\\(H_0\\)): autocorrelation order (lag \\(p\\)).Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists lag(s).ProcedureEstimate original regression model obtain residuals \\(\\hat{\\epsilon}_t\\):\n\\[\ny_t = \\beta_0 + \\beta_1 x_{1t} + \\dots + \\beta_k x_{kt} + \\epsilon_t\n\\]Estimate original regression model obtain residuals \\(\\hat{\\epsilon}_t\\):\\[\ny_t = \\beta_0 + \\beta_1 x_{1t} + \\dots + \\beta_k x_{kt} + \\epsilon_t\n\\]Run auxiliary regression regressing residuals original regressors plus \\(p\\) lagged residuals:\n\\[\n\\hat{\\epsilon}_t = \\alpha_0 + \\alpha_1 x_{1t} + \\dots + \\alpha_k x_{kt} + \\rho_1 \\hat{\\epsilon}_{t-1} + \\dots + \\rho_p \\hat{\\epsilon}_{t-p} + u_t\n\\]Run auxiliary regression regressing residuals original regressors plus \\(p\\) lagged residuals:\\[\n\\hat{\\epsilon}_t = \\alpha_0 + \\alpha_1 x_{1t} + \\dots + \\alpha_k x_{kt} + \\rho_1 \\hat{\\epsilon}_{t-1} + \\dots + \\rho_p \\hat{\\epsilon}_{t-p} + u_t\n\\]Calculate test statistic:\n\\[\n\\text{BG} = n \\cdot R^2_{\\text{aux}}\n\\]\n:\n\\(n\\) sample size,\n\\(R^2_{\\text{aux}}\\) \\(R^2\\) auxiliary regression.\nCalculate test statistic:\\[\n\\text{BG} = n \\cdot R^2_{\\text{aux}}\n\\]:\\(n\\) sample size,\\(R^2_{\\text{aux}}\\) \\(R^2\\) auxiliary regression.Decision Rule:\\(H_0\\), BG statistic follows chi-squared distribution \\(p\\) degrees freedom:\n\\[\n\\text{BG} \\sim \\chi^2_p\n\\]\\(H_0\\), BG statistic follows chi-squared distribution \\(p\\) degrees freedom:\\[\n\\text{BG} \\sim \\chi^2_p\n\\]Reject \\(H_0\\) statistic exceeds critical chi-squared value, indicating presence autocorrelation.Reject \\(H_0\\) statistic exceeds critical chi-squared value, indicating presence autocorrelation.Advantages LimitationsAdvantage: Can detect higher-order autocorrelation; valid lagged dependent variables.Limitation: computationally intensive Durbin–Watson test.","code":""},{"path":"model-specification-tests.html","id":"sec-ljung--box-test","chapter":"14 Model Specification Tests","heading":"14.5.3 Ljung–Box Test (or Box–Pierce Test)","text":"Ljung–Box Test portmanteau test designed detect autocorrelation multiple lags simultaneously (Box Pierce 1970; Ljung Box 1978). commonly used time-series analysis check residual autocorrelation model estimation (e.g., ARIMA models).HypothesesNull Hypothesis (\\(H_0\\)): autocorrelation lag \\(h\\).Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.Ljung–Box Test StatisticThe Ljung–Box statistic calculated :\\[\nQ = n(n + 2) \\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n - k}\n\\]:\\(n\\) = Sample size,\\(h\\) = Number lags tested,\\(\\hat{\\rho}_k\\) = Sample autocorrelation lag \\(k\\).Decision RuleUnder \\(H_0\\), \\(Q\\) statistic follows chi-squared distribution \\(h\\) degrees freedom:\n\\[\nQ \\sim \\chi^2_h\n\\]\\(H_0\\), \\(Q\\) statistic follows chi-squared distribution \\(h\\) degrees freedom:\\[\nQ \\sim \\chi^2_h\n\\]Reject \\(H_0\\) \\(Q\\) exceeds critical value, indicating significant autocorrelation.Reject \\(H_0\\) \\(Q\\) exceeds critical value, indicating significant autocorrelation.Advantages LimitationsAdvantage: Detects autocorrelation across multiple lags simultaneously.Limitation: Less powerful detecting specific lag structures; sensitive model misspecification.","code":""},{"path":"model-specification-tests.html","id":"sec-runs-test","chapter":"14 Model Specification Tests","heading":"14.5.4 Runs Test","text":"Runs Test non-parametric test examines randomness residuals. based number runs—sequences consecutive residuals sign.HypothesesNull Hypothesis (\\(H_0\\)): Residuals randomly distributed (autocorrelation).Alternative Hypothesis (\\(H_1\\)): Residuals exhibit non-random patterns (indicating autocorrelation).ProcedureClassify residuals positive negative.Classify residuals positive negative.Count number runs: run sequence consecutive positive negative residuals.Count number runs: run sequence consecutive positive negative residuals.Calculate expected number runs randomness:\n\\[\nE(R) = \\frac{2 n_+ n_-}{n} + 1\n\\]\n:\n\\(n_+\\) = Number positive residuals,\n\\(n_-\\) = Number negative residuals,\n\\(n = n_+ + n_-\\).\nCalculate expected number runs randomness:\\[\nE(R) = \\frac{2 n_+ n_-}{n} + 1\n\\]:\\(n_+\\) = Number positive residuals,\\(n_-\\) = Number negative residuals,\\(n = n_+ + n_-\\).Compute test statistic (Z-score):\n\\[\nZ = \\frac{R - E(R)}{\\sqrt{\\text{Var}(R)}}\n\\]\n\\(\\text{Var}(R)\\) variance number runs null hypothesis.Compute test statistic (Z-score):\\[\nZ = \\frac{R - E(R)}{\\sqrt{\\text{Var}(R)}}\n\\]\\(\\text{Var}(R)\\) variance number runs null hypothesis.Decision RuleUnder \\(H_0\\), \\(Z\\)-statistic follows standard normal distribution:\n\\[\nZ \\sim N(0, 1)\n\\]\\(H_0\\), \\(Z\\)-statistic follows standard normal distribution:\\[\nZ \\sim N(0, 1)\n\\]Reject \\(H_0\\) \\(|Z|\\) exceeds critical value standard normal distribution.Reject \\(H_0\\) \\(|Z|\\) exceeds critical value standard normal distribution.Advantages LimitationsAdvantage: Non-parametric; assume normality linearity.Limitation: Less powerful parametric tests; primarily useful supplementary diagnostic.","code":""},{"path":"model-specification-tests.html","id":"summary-of-autocorrelation-tests","chapter":"14 Model Specification Tests","heading":"14.5.5 Summary of Autocorrelation Tests","text":"Detecting autocorrelation crucial ensuring efficiency reliability regression models, especially time-series analysis. Durbin–Watson Test suitable detecting first-order autocorrelation, Breusch–Godfrey Test Ljung–Box Test offer flexibility higher-order multi-lag dependencies. Non-parametric tests like Runs Test serve useful supplementary diagnostics.Interpretation ResultsDurbin–Watson Test\nNull Hypothesis (\\(H_0\\)): first-order autocorrelation (\\(\\rho = 0\\)).\nAlternative Hypothesis (\\(H_1\\)): First-order autocorrelation exists (\\(\\rho \\neq 0\\)).\nDecision Rule:\nReject \\(H_0\\) DW \\(< 1.5\\) (positive autocorrelation) DW \\(> 2.5\\) (negative autocorrelation).\nFail reject \\(H_0\\) DW \\(\\approx 2\\), suggesting significant autocorrelation.\n\nDurbin–Watson TestNull Hypothesis (\\(H_0\\)): first-order autocorrelation (\\(\\rho = 0\\)).Null Hypothesis (\\(H_0\\)): first-order autocorrelation (\\(\\rho = 0\\)).Alternative Hypothesis (\\(H_1\\)): First-order autocorrelation exists (\\(\\rho \\neq 0\\)).Alternative Hypothesis (\\(H_1\\)): First-order autocorrelation exists (\\(\\rho \\neq 0\\)).Decision Rule:\nReject \\(H_0\\) DW \\(< 1.5\\) (positive autocorrelation) DW \\(> 2.5\\) (negative autocorrelation).\nFail reject \\(H_0\\) DW \\(\\approx 2\\), suggesting significant autocorrelation.\nDecision Rule:Reject \\(H_0\\) DW \\(< 1.5\\) (positive autocorrelation) DW \\(> 2.5\\) (negative autocorrelation).Fail reject \\(H_0\\) DW \\(\\approx 2\\), suggesting significant autocorrelation.Breusch–Godfrey Test\nNull Hypothesis (\\(H_0\\)): autocorrelation lag \\(p\\) (, \\(p = 2\\)).\nAlternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.\n\nBreusch–Godfrey TestNull Hypothesis (\\(H_0\\)): autocorrelation lag \\(p\\) (, \\(p = 2\\)).Null Hypothesis (\\(H_0\\)): autocorrelation lag \\(p\\) (, \\(p = 2\\)).Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.Ljung–Box Test\nNull Hypothesis (\\(H_0\\)): autocorrelation lag \\(h\\) (, \\(h = 10\\)).\nAlternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.\n\nLjung–Box TestNull Hypothesis (\\(H_0\\)): autocorrelation lag \\(h\\) (, \\(h = 10\\)).Null Hypothesis (\\(H_0\\)): autocorrelation lag \\(h\\) (, \\(h = 10\\)).Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.Alternative Hypothesis (\\(H_1\\)): Autocorrelation exists one lags.Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\), indicating significant autocorrelation.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting evidence autocorrelation.Runs Test (Non-parametric)\nNull Hypothesis (\\(H_0\\)): Residuals randomly distributed (autocorrelation).\nAlternative Hypothesis (\\(H_1\\)): Residuals exhibit non-random patterns (indicating autocorrelation).\nDecision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating non-randomness potential autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting randomness residuals.\n\nRuns Test (Non-parametric)Null Hypothesis (\\(H_0\\)): Residuals randomly distributed (autocorrelation).Null Hypothesis (\\(H_0\\)): Residuals randomly distributed (autocorrelation).Alternative Hypothesis (\\(H_1\\)): Residuals exhibit non-random patterns (indicating autocorrelation).Alternative Hypothesis (\\(H_1\\)): Residuals exhibit non-random patterns (indicating autocorrelation).Decision Rule:\nReject \\(H_0\\) p-value \\(< 0.05\\), indicating non-randomness potential autocorrelation.\nFail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting randomness residuals.\nDecision Rule:Reject \\(H_0\\) p-value \\(< 0.05\\), indicating non-randomness potential autocorrelation.Fail reject \\(H_0\\) p-value \\(\\ge 0.05\\), suggesting randomness residuals.","code":"\n# Install and load necessary libraries\n# install.packages(\"lmtest\")  # For Durbin–Watson and Breusch–Godfrey Tests\n# install.packages(\"tseries\") # For Runs Test\n# install.packages(\"forecast\")# For Ljung–Box Test\n\nlibrary(lmtest)\nlibrary(tseries)\nlibrary(forecast)\n\n# Simulated time-series dataset\nset.seed(123)\nn <- 100\ntime <- 1:n\nx1 <- rnorm(n, mean = 50, sd = 10)\nx2 <- rnorm(n, mean = 30, sd = 5)\n\n# Introducing autocorrelation in errors\nepsilon <- arima.sim(model = list(ar = 0.6), n = n) \n# AR(1) process with ρ = 0.6\ny <- 5 + 0.4 * x1 - 0.3 * x2 + epsilon\n\n# Original regression model\nmodel <- lm(y ~ x1 + x2)\n\n# ----------------------------------------------------------------------\n# 1. Durbin–Watson Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: No first-order autocorrelation\ndw_test <- dwtest(model)\nprint(dw_test)\n#> \n#>  Durbin-Watson test\n#> \n#> data:  model\n#> DW = 0.77291, p-value = 3.323e-10\n#> alternative hypothesis: true autocorrelation is greater than 0\n\n# ----------------------------------------------------------------------\n# 2. Breusch–Godfrey Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: No autocorrelation up to lag 2\nbg_test <- bgtest(model, order = 2)  # Testing for autocorrelation up to lag 2\nprint(bg_test)\n#> \n#>  Breusch-Godfrey test for serial correlation of order up to 2\n#> \n#> data:  model\n#> LM test = 40.314, df = 2, p-value = 1.762e-09\n\n# ----------------------------------------------------------------------\n# 3. Ljung–Box Test\n# ----------------------------------------------------------------------\n# Null Hypothesis: No autocorrelation up to lag 10\nljung_box_test <- Box.test(residuals(model), lag = 10, type = \"Ljung-Box\")\nprint(ljung_box_test)\n#> \n#>  Box-Ljung test\n#> \n#> data:  residuals(model)\n#> X-squared = 50.123, df = 10, p-value = 2.534e-07\n\n# ----------------------------------------------------------------------\n# 4. Runs Test (Non-parametric)\n# ----------------------------------------------------------------------\n# Null Hypothesis: Residuals are randomly distributed\nruns_test <- runs.test(as.factor(sign(residuals(model))))\nprint(runs_test)\n#> \n#>  Runs Test\n#> \n#> data:  as.factor(sign(residuals(model)))\n#> Standard Normal = -4.2214, p-value = 2.428e-05\n#> alternative hypothesis: two.sided"},{"path":"model-specification-tests.html","id":"multicollinearity-diagnostics","chapter":"14 Model Specification Tests","heading":"14.6 Multicollinearity Diagnostics","text":"Multicollinearity occurs two independent variables regression model highly correlated, leading several issues:Unstable coefficient estimates: Small changes data can cause large fluctuations parameter estimates.Inflated standard errors: Reduces precision estimated coefficients, making difficult determine significance predictors.Difficulty assessing variable importance: becomes challenging isolate effect individual predictors dependent variable.Multicollinearity affect overall fit model (e.g., \\(R^2\\) remains high), distorts reliability individual coefficient estimates.Key Multicollinearity Diagnostics:Variance Inflation FactorTolerance StatisticCondition Index Eigenvalue DecompositionPairwise Correlation MatrixDeterminant Correlation Matrix","code":""},{"path":"model-specification-tests.html","id":"sec-variance-inflation-factor","chapter":"14 Model Specification Tests","heading":"14.6.1 Variance Inflation Factor","text":"Variance Inflation Factor (VIF) commonly used diagnostic detecting multicollinearity. measures much variance estimated regression coefficient inflated due multicollinearity compared predictors uncorrelated.predictor \\(X_j\\), VIF defined :\\[\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n\\]:\\(R_j^2\\) coefficient determination obtained regressing \\(X_j\\) independent variables model.Interpretation VIFVIF = 1: multicollinearity (perfect independence).1 < VIF < 5: Moderate correlation, typically problematic.VIF ≥ 5: High correlation; consider investigating .VIF ≥ 10: Severe multicollinearity; corrective action recommended.ProcedureRegress independent variable (\\(X_j\\)) remaining predictors.Compute \\(R_j^2\\) regression.Calculate \\(\\text{VIF}_j = 1 / (1 - R_j^2)\\).Analyze VIF values identify problematic predictors.Advantages LimitationsAdvantage: Easy compute interpret.Limitation: Detects linear relationships; may capture complex multicollinearity patterns involving multiple variables simultaneously.","code":""},{"path":"model-specification-tests.html","id":"sec-tolerance-statistic","chapter":"14 Model Specification Tests","heading":"14.6.2 Tolerance Statistic","text":"Tolerance Statistic reciprocal VIF measures proportion variance independent variable explained predictors.\\[\n\\text{Tolerance}_j = 1 - R_j^2\n\\]\\(R_j^2\\) defined VIF calculation.Interpretation ToleranceTolerance close 1: Low multicollinearity.Tolerance < 0.2: Potential multicollinearity problem.Tolerance < 0.1: Severe multicollinearity.Since low tolerance implies high VIF, metrics provide consistent information.Advantages LimitationsAdvantage: Provides intuitive measure much variance “free” multicollinearity.Limitation: Similar VIF, focuses linear dependencies.","code":""},{"path":"model-specification-tests.html","id":"sec-condition-index-and-eigenvalue-decomposition","chapter":"14 Model Specification Tests","heading":"14.6.3 Condition Index and Eigenvalue Decomposition","text":"Condition Index advanced diagnostic detects multicollinearity involving multiple variables simultaneously. based eigenvalues scaled independent variable matrix.Compute scaled design matrix \\(X'X\\), \\(X\\) matrix independent variables.Compute scaled design matrix \\(X'X\\), \\(X\\) matrix independent variables.Perform eigenvalue decomposition obtain eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\).Perform eigenvalue decomposition obtain eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\).Calculate Condition Index:\n\\[\n\\text{CI}_j = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_j}}\n\\]\n:\n\\(\\lambda_{\\max}\\) largest eigenvalue,\n\\(\\lambda_j\\) \\(j\\)-th eigenvalue.\nCalculate Condition Index:\\[\n\\text{CI}_j = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_j}}\n\\]:\\(\\lambda_{\\max}\\) largest eigenvalue,\\(\\lambda_j\\) \\(j\\)-th eigenvalue.Interpretation Condition IndexCI < 10: serious multicollinearity.10 ≤ CI < 30: Moderate strong multicollinearity.CI ≥ 30: Severe multicollinearity.high condition index indicates near-linear dependence among variables.Variance Decomposition ProportionsTo identify variables contribute multicollinearity:Compute Variance Decomposition Proportions (VDP) coefficient across eigenvalues.Compute Variance Decomposition Proportions (VDP) coefficient across eigenvalues.two variables high VDPs (e.g., > 0.5) associated high condition index, indicates severe multicollinearity.two variables high VDPs (e.g., > 0.5) associated high condition index, indicates severe multicollinearity.Advantages LimitationsAdvantage: Detects multicollinearity involving multiple variables, VIF may miss.Limitation: Requires matrix algebra knowledge; less intuitive VIF tolerance.","code":""},{"path":"model-specification-tests.html","id":"sec-pairwise-correlation-matrix","chapter":"14 Model Specification Tests","heading":"14.6.4 Pairwise Correlation Matrix","text":"Pairwise Correlation Matrix provides simple diagnostic computing correlation coefficients pair independent variables.variables \\(X_i\\) \\(X_j\\), correlation coefficient :\\[\n\\rho_{ij} = \\frac{\\text{Cov}(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}}\n\\]:\\(\\text{Cov}(X_i, X_j)\\) covariance,\\(\\text{Cov}(X_i, X_j)\\) covariance,\\(\\sigma_{X_i}\\) \\(\\sigma_{X_j}\\) standard deviations.\\(\\sigma_{X_i}\\) \\(\\sigma_{X_j}\\) standard deviations.Interpretation Correlation Coefficients\\(|\\rho| < 0.5\\): Weak correlation (unlikely cause multicollinearity).\\(0.5 \\leq |\\rho| < 0.8\\): Moderate correlation; monitor carefully.\\(|\\rho| ≥ 0.8\\): Strong correlation; potential multicollinearity issue.Advantages LimitationsAdvantage: Quick easy compute; useful initial screening.Limitation: Detects pairwise relationships; may miss multicollinearity involving two variables.","code":""},{"path":"model-specification-tests.html","id":"sec-determinant-of-the-correlation-matrix","chapter":"14 Model Specification Tests","heading":"14.6.5 Determinant of the Correlation Matrix","text":"Determinant Correlation Matrix provides global measure multicollinearity. small determinant indicates high multicollinearity.Form correlation matrix \\(R\\) independent variables.Form correlation matrix \\(R\\) independent variables.Compute determinant:\n\\[\n\\det(R)\n\\]Compute determinant:\\[\n\\det(R)\n\\]Interpretation\\(\\det(R) \\approx 1\\): multicollinearity (perfect independence).\\(\\det(R) \\approx 0\\): Severe multicollinearity.determinant close zero suggests correlation matrix nearly singular, indicating strong multicollinearity.Advantages LimitationsAdvantage: Provides single summary statistic overall multicollinearity.Limitation: indicate variables causing problem.","code":""},{"path":"model-specification-tests.html","id":"summary-of-multicollinearity-diagnostics","chapter":"14 Model Specification Tests","heading":"14.6.6 Summary of Multicollinearity Diagnostics","text":"","code":""},{"path":"model-specification-tests.html","id":"addressing-multicollinearity","chapter":"14 Model Specification Tests","heading":"14.6.7 Addressing Multicollinearity","text":"multicollinearity detected, consider following solutions:Remove combine correlated variables: Drop one correlated predictors create index/aggregate.Principal Component Analysis: Reduce dimensionality transforming correlated variables uncorrelated components.Ridge Regression (L2 regularization): Introduces penalty term stabilize coefficient estimates presence multicollinearity.Centering variables: Mean-centering can help reduce multicollinearity, especially interaction terms.Multicollinearity can significantly distort regression estimates, leading misleading interpretations. VIF Tolerance commonly used diagnostics, advanced techniques like Condition Index Eigenvalue Decomposition provide deeper insights, especially dealing complex datasets.Interpretation ResultsVariance Inflation Factor (VIF)\nFormula: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)\nDecision Rule:\nVIF \\(\\approx 1\\): multicollinearity.\n\\(1 < \\text{VIF} < 5\\): Moderate correlation, usually acceptable.\n\\(\\text{VIF} \\ge 5\\): High correlation; investigate .\n\\(\\text{VIF} \\ge 10\\): Severe multicollinearity; corrective action recommended.\n\nVariance Inflation Factor (VIF)Formula: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)Formula: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)Decision Rule:\nVIF \\(\\approx 1\\): multicollinearity.\n\\(1 < \\text{VIF} < 5\\): Moderate correlation, usually acceptable.\n\\(\\text{VIF} \\ge 5\\): High correlation; investigate .\n\\(\\text{VIF} \\ge 10\\): Severe multicollinearity; corrective action recommended.\nDecision Rule:VIF \\(\\approx 1\\): multicollinearity.\\(1 < \\text{VIF} < 5\\): Moderate correlation, usually acceptable.\\(\\text{VIF} \\ge 5\\): High correlation; investigate .\\(\\text{VIF} \\ge 10\\): Severe multicollinearity; corrective action recommended.Tolerance Statistic\nFormula: \\(\\text{Tolerance}_j = 1 - R_j^2 = \\frac{1}{\\text{VIF}_j}\\)\nDecision Rule:\nTolerance \\(> 0.2\\): Low risk multicollinearity.\nTolerance \\(< 0.2\\): Possible multicollinearity problem.\nTolerance \\(< 0.1\\): Severe multicollinearity.\n\nTolerance StatisticFormula: \\(\\text{Tolerance}_j = 1 - R_j^2 = \\frac{1}{\\text{VIF}_j}\\)Formula: \\(\\text{Tolerance}_j = 1 - R_j^2 = \\frac{1}{\\text{VIF}_j}\\)Decision Rule:\nTolerance \\(> 0.2\\): Low risk multicollinearity.\nTolerance \\(< 0.2\\): Possible multicollinearity problem.\nTolerance \\(< 0.1\\): Severe multicollinearity.\nDecision Rule:Tolerance \\(> 0.2\\): Low risk multicollinearity.Tolerance \\(< 0.2\\): Possible multicollinearity problem.Tolerance \\(< 0.1\\): Severe multicollinearity.Condition Index Eigenvalue Decomposition\nFormula: \\(\\text{CI}_j = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_j}}\\)\nDecision Rule:\nCI \\(< 10\\): significant multicollinearity.\n\\(10 \\le \\text{CI} < 30\\): Moderate strong multicollinearity.\nCI \\(\\ge 30\\): Severe multicollinearity.\n\nVariance Decomposition Proportions (VDP):\nHigh VDP (\\(> 0.5\\)) associated high CI indicates problematic variables.\n\nCondition Index Eigenvalue DecompositionFormula: \\(\\text{CI}_j = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_j}}\\)Formula: \\(\\text{CI}_j = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_j}}\\)Decision Rule:\nCI \\(< 10\\): significant multicollinearity.\n\\(10 \\le \\text{CI} < 30\\): Moderate strong multicollinearity.\nCI \\(\\ge 30\\): Severe multicollinearity.\nDecision Rule:CI \\(< 10\\): significant multicollinearity.\\(10 \\le \\text{CI} < 30\\): Moderate strong multicollinearity.CI \\(\\ge 30\\): Severe multicollinearity.Variance Decomposition Proportions (VDP):\nHigh VDP (\\(> 0.5\\)) associated high CI indicates problematic variables.\nVariance Decomposition Proportions (VDP):High VDP (\\(> 0.5\\)) associated high CI indicates problematic variables.Pairwise Correlation Matrix\nDecision Rule:\n\\(|\\rho| < 0.5\\): Weak correlation.\n\\(0.5 \\le |\\rho| < 0.8\\): Moderate correlation; monitor.\n\\(|\\rho| \\ge 0.8\\): Strong correlation; potential multicollinearity issue.\n\nPairwise Correlation MatrixDecision Rule:\n\\(|\\rho| < 0.5\\): Weak correlation.\n\\(0.5 \\le |\\rho| < 0.8\\): Moderate correlation; monitor.\n\\(|\\rho| \\ge 0.8\\): Strong correlation; potential multicollinearity issue.\n\\(|\\rho| < 0.5\\): Weak correlation.\\(0.5 \\le |\\rho| < 0.8\\): Moderate correlation; monitor.\\(|\\rho| \\ge 0.8\\): Strong correlation; potential multicollinearity issue.Determinant Correlation Matrix\nDecision Rule:\n\\(\\det(R) \\approx 1\\): multicollinearity.\n\\(\\det(R) \\approx 0\\): Severe multicollinearity (near-singular matrix).\n\nDeterminant Correlation MatrixDecision Rule:\n\\(\\det(R) \\approx 1\\): multicollinearity.\n\\(\\det(R) \\approx 0\\): Severe multicollinearity (near-singular matrix).\n\\(\\det(R) \\approx 1\\): multicollinearity.\\(\\det(R) \\approx 0\\): Severe multicollinearity (near-singular matrix).Model specification tests essential diagnosing validating econometric models. ensure model assumptions hold true, thereby improving accuracy reliability estimations. systematically applying tests, researchers can identify issues related nested non-nested models, heteroskedasticity, functional form, endogeneity, autocorrelation, multicollinearity, leading robust credible econometric analyses.","code":"\n# Install and load necessary libraries\n# install.packages(\"car\")        # For VIF calculation\n# install.packages(\"corpcor\")    # For determinant of correlation matrix\n\n\nlibrary(car)\nlibrary(corpcor)\n\n# Simulated dataset with multicollinearity\nset.seed(123)\nn <- 100\nx1 <- rnorm(n, mean = 50, sd = 10)\nx2 <- 0.8 * x1 + rnorm(n, sd = 2)   # Highly correlated with x1\nx3 <- rnorm(n, mean = 30, sd = 5)\ny <- 5 + 0.4 * x1 - 0.3 * x2 + 0.2 * x3 + rnorm(n)\n\n# Original regression model\nmodel <- lm(y ~ x1 + x2 + x3)\n\n# ----------------------------------------------------------------------\n# 1. Variance Inflation Factor (VIF)\n# ----------------------------------------------------------------------\n# Null Hypothesis: No multicollinearity (VIF = 1)\nvif_values <- vif(model)\nprint(vif_values)\n#>        x1        x2        x3 \n#> 14.969143 14.929013  1.017576\n\n# ----------------------------------------------------------------------\n# 2. Tolerance Statistic (Reciprocal of VIF)\n# ----------------------------------------------------------------------\ntolerance_values <- 1 / vif_values\nprint(tolerance_values)\n#>         x1         x2         x3 \n#> 0.06680409 0.06698366 0.98272742\n\n# ----------------------------------------------------------------------\n# 3. Condition Index and Eigenvalue Decomposition\n# ----------------------------------------------------------------------\n# Scaling the independent variables\nX <- model.matrix(model)[,-1]  # Removing intercept\neigen_decomp <-\n    eigen(cor(X))   # Eigenvalue decomposition of the correlation matrix\n\n# Condition Index\ncondition_index <-\n    sqrt(max(eigen_decomp$values) / eigen_decomp$values)\nprint(condition_index)\n#> [1] 1.000000 1.435255 7.659566\n\n# Variance Decomposition Proportions (VDP)\n# Proportions calculated based on the squared coefficients\nloadings <- eigen_decomp$vectors\nvdp <- apply(loadings ^ 2, 2, function(x)\n    x / sum(x))\nprint(vdp)\n#>            [,1]       [,2]         [,3]\n#> [1,] 0.48567837 0.01363318 5.006885e-01\n#> [2,] 0.48436754 0.01638399 4.992485e-01\n#> [3,] 0.02995409 0.96998283 6.307954e-05\n\n# ----------------------------------------------------------------------\n# 4. Pairwise Correlation Matrix\n# ----------------------------------------------------------------------\ncorrelation_matrix <- cor(X)\nprint(correlation_matrix)\n#>           x1         x2         x3\n#> x1  1.000000  0.9659070 -0.1291760\n#> x2  0.965907  1.0000000 -0.1185042\n#> x3 -0.129176 -0.1185042  1.0000000\n\n# ----------------------------------------------------------------------\n# 5. Determinant of the Correlation Matrix\n# ----------------------------------------------------------------------\ndeterminant_corr_matrix <- det(correlation_matrix)\ncat(\"Determinant of the Correlation Matrix:\",\n    determinant_corr_matrix,\n    \"\\n\")\n#> Determinant of the Correlation Matrix: 0.06586594"},{"path":"variable-selection.html","id":"variable-selection","chapter":"15 Variable Selection","heading":"15 Variable Selection","text":"Imagine ’re detective standing pinboard covered clues—glaringly obvious, others might red herrings. mission? pick pieces evidence crack case. essence variable selection statistics: deciding variables best uncover story behind data. Far mechanical chore, ’s high-stakes balancing act blending analytical goals, domain insights, data realities, computational feasibility.Variable Selection Matter?Focus Clarity: Models cluttered unnecessary variables can obscure real relationships patterns data. identifying variables truly drive results, sharpen model’s focus interpretability.Focus Clarity: Models cluttered unnecessary variables can obscure real relationships patterns data. identifying variables truly drive results, sharpen model’s focus interpretability.Efficiency Performance: many variables can lead overfitting—fitting quirks single dataset rather underlying trends. Streamlined models often run faster generalize better.Efficiency Performance: many variables can lead overfitting—fitting quirks single dataset rather underlying trends. Streamlined models often run faster generalize better.Practical Constraints: many real-world scenarios, data collection processing costs money, time, effort. Prioritizing meaningful variables becomes just statistical concern, strategic one.Practical Constraints: many real-world scenarios, data collection processing costs money, time, effort. Prioritizing meaningful variables becomes just statistical concern, strategic one.Key Influences Variable SelectionObjectives Goals\nPrediction vs. Inference: trying forecast future outcomes explain certain events happen? Prediction-focused models might include many relevant features possible accuracy, whereas inference-driven models often strive parsimony clearer relationships.\nBalance: analyses blend objectives, requiring careful negotiation complexity (maximize predictive ability) simplicity (maintain interpretability).\nObjectives GoalsPrediction vs. Inference: trying forecast future outcomes explain certain events happen? Prediction-focused models might include many relevant features possible accuracy, whereas inference-driven models often strive parsimony clearer relationships.Prediction vs. Inference: trying forecast future outcomes explain certain events happen? Prediction-focused models might include many relevant features possible accuracy, whereas inference-driven models often strive parsimony clearer relationships.Balance: analyses blend objectives, requiring careful negotiation complexity (maximize predictive ability) simplicity (maintain interpretability).Balance: analyses blend objectives, requiring careful negotiation complexity (maximize predictive ability) simplicity (maintain interpretability).Previously Acquired Expertise\nDomain Knowledge: Whether ’re analyzing financial trends studying medical records, familiarity subject can reveal variables naturally linked phenomenon.\nSubtle Clues: Experts can uncover hidden confounders—variables outwardly seem irrelevant yet dramatically influence results.\nPreviously Acquired ExpertiseDomain Knowledge: Whether ’re analyzing financial trends studying medical records, familiarity subject can reveal variables naturally linked phenomenon.Domain Knowledge: Whether ’re analyzing financial trends studying medical records, familiarity subject can reveal variables naturally linked phenomenon.Subtle Clues: Experts can uncover hidden confounders—variables outwardly seem irrelevant yet dramatically influence results.Subtle Clues: Experts can uncover hidden confounders—variables outwardly seem irrelevant yet dramatically influence results.Availability Quality Data\nCompleteness: Missing data sparse measurements can force discard transform variables. Sometimes ideal variable simply isn’t present dataset.\nReliability: variable riddled measurement errors inconsistencies may harm good.\nAvailability Quality DataCompleteness: Missing data sparse measurements can force discard transform variables. Sometimes ideal variable simply isn’t present dataset.Completeness: Missing data sparse measurements can force discard transform variables. Sometimes ideal variable simply isn’t present dataset.Reliability: variable riddled measurement errors inconsistencies may harm good.Reliability: variable riddled measurement errors inconsistencies may harm good.Computational Resources Software\nToolset Capabilities: statistical techniques advanced machine learning methods thrive large sets variables, others become unwieldy.\nTime Memory Constraints: Even sophisticated algorithms can choke much data hardware resources limited.\nComputational Resources SoftwareToolset Capabilities: statistical techniques advanced machine learning methods thrive large sets variables, others become unwieldy.Toolset Capabilities: statistical techniques advanced machine learning methods thrive large sets variables, others become unwieldy.Time Memory Constraints: Even sophisticated algorithms can choke much data hardware resources limited.Time Memory Constraints: Even sophisticated algorithms can choke much data hardware resources limited.Selecting right subset variables enhances model interpretability, reduces computational cost, prevents overfitting. Broadly, variable selection methods fall three categories:Filter Methods: Use statistical properties data select features modeling.\nInformation Criteria-Based Selection\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nMallows’s C Statistic\nHannan-Quinn Criterion (HQC)\nMinimum Description Length (MDL)\nAdjusted \\(R^2\\)\nPrediction Error Sum Squares (PRESS)\n\nUnivariate Selection Methods\nCorrelation-Based Feature Selection\nVariance Thresholding\nInformation Criteria-Based Selection\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nMallows’s C Statistic\nHannan-Quinn Criterion (HQC)\nMinimum Description Length (MDL)\nAdjusted \\(R^2\\)\nPrediction Error Sum Squares (PRESS)\nAkaike Information Criterion (AIC)Bayesian Information Criterion (BIC)Mallows’s C StatisticHannan-Quinn Criterion (HQC)Minimum Description Length (MDL)Adjusted \\(R^2\\)Prediction Error Sum Squares (PRESS)Univariate Selection MethodsCorrelation-Based Feature SelectionVariance ThresholdingWrapper Methods: Evaluate different subsets features based model performance.\nExhaustive Search (Best Subsets Algorithm)\nBest Subsets Algorithm\nStepwise Selection Methods\nForward Selection\nBackward Elimination\nStepwise (Directions) Selection\n\nBranch--Bound Algorithm\nRecursive Feature Elimination\nExhaustive Search (Best Subsets Algorithm)Best Subsets AlgorithmStepwise Selection Methods\nForward Selection\nBackward Elimination\nStepwise (Directions) Selection\nForward SelectionBackward EliminationStepwise (Directions) SelectionBranch--Bound AlgorithmRecursive Feature EliminationEmbedded Methods: Perform feature selection part model training process.\nLasso Regression (L1 Regularization)\nRidge Regression (L2 Regularization)\nElastic Net (Combination L1 L2)\nTree-Based Feature Importance\nGenetic Algorithms\nLasso Regression (L1 Regularization)Ridge Regression (L2 Regularization)Elastic Net (Combination L1 L2)Tree-Based Feature ImportanceGenetic AlgorithmsThroughout chapter, let \\(P\\) denote number potential predictor variables (\\(X_1, X_2, \\dots, X_{P-1}\\)).","code":""},{"path":"variable-selection.html","id":"sec-filter-methods","chapter":"15 Variable Selection","heading":"15.1 Filter Methods (Statistical Criteria, Model-Agnostic)","text":"","code":""},{"path":"variable-selection.html","id":"information-criteria-based-selection","chapter":"15 Variable Selection","heading":"15.1.1 Information Criteria-Based Selection","text":"","code":""},{"path":"variable-selection.html","id":"mallowss-c-statistic","chapter":"15 Variable Selection","heading":"15.1.1.1 Mallows’s C Statistic","text":"\\(C_p\\) statistic (Mallows, 1973, Technometrics, 15, 661-675) (Mallows 1995) criterion used evaluate predictive ability fitted model. balances model complexity goodness--fit.model \\(p\\) parameters, let \\(\\hat{Y}_{ip}\\) predicted value \\(Y_i\\). total standardized mean square error prediction :\\[\n\\Gamma_p = \\frac{\\sum_{=1}^n E(\\hat{Y}_{ip} - E(Y_i))^2}{\\sigma^2}\n\\]Expanding \\(\\Gamma_p\\):\\[\n\\Gamma_p = \\frac{\\sum_{=1}^n [E(\\hat{Y}_{ip}) - E(Y_i)]^2 + \\sum_{=1}^n \\text{Var}(\\hat{Y}_{ip})}{\\sigma^2}\n\\]first term numerator represents squared bias.second term represents prediction variance.Key InsightsBias-Variance Tradeoff:\nbias decreases variables added model.\nfull model (\\(p = P\\)) assumed true model, \\(E(\\hat{Y}_{ip}) - E(Y_i) = 0\\), implying bias.\nprediction variance increases variables added: \\(\\sum \\text{Var}(\\hat{Y}_{ip}) = p \\sigma^2\\).\nTherefore, optimal model balances bias variance minimizing \\(\\Gamma_p\\).\nBias-Variance Tradeoff:bias decreases variables added model.full model (\\(p = P\\)) assumed true model, \\(E(\\hat{Y}_{ip}) - E(Y_i) = 0\\), implying bias.prediction variance increases variables added: \\(\\sum \\text{Var}(\\hat{Y}_{ip}) = p \\sigma^2\\).Therefore, optimal model balances bias variance minimizing \\(\\Gamma_p\\).Estimating \\(\\Gamma_p\\): Since \\(\\Gamma_p\\) depends unknown parameters (e.g., \\(\\beta\\)), use estimate:\n\\[\nC_p = \\frac{SSE_p}{\\hat{\\sigma}^2} - (n - 2p)\n\\]\n\\(SSE_p\\): Sum squared errors model \\(p\\) predictors.\n\\(\\hat{\\sigma}^2\\): Mean squared error (MSE) full model \\(P-1\\) predictors.\nEstimating \\(\\Gamma_p\\): Since \\(\\Gamma_p\\) depends unknown parameters (e.g., \\(\\beta\\)), use estimate:\\[\nC_p = \\frac{SSE_p}{\\hat{\\sigma}^2} - (n - 2p)\n\\]\\(SSE_p\\): Sum squared errors model \\(p\\) predictors.\\(\\hat{\\sigma}^2\\): Mean squared error (MSE) full model \\(P-1\\) predictors.Properties \\(C_p\\):\nvariables added, \\(SSE_p\\) decreases, penalty term \\(2p\\) increases.\nbias, \\(E(C_p) \\approx p\\). Hence, good models \\(C_p\\) values close \\(p\\).\nProperties \\(C_p\\):variables added, \\(SSE_p\\) decreases, penalty term \\(2p\\) increases.bias, \\(E(C_p) \\approx p\\). Hence, good models \\(C_p\\) values close \\(p\\).Model Selection Criteria:\nPrediction-focused models: Consider models \\(C_p \\leq p\\).\nParameter estimation-focused models: Consider models \\(C_p \\leq 2p - (P - 1)\\) avoid excess bias.\nModel Selection Criteria:Prediction-focused models: Consider models \\(C_p \\leq p\\).Parameter estimation-focused models: Consider models \\(C_p \\leq 2p - (P - 1)\\) avoid excess bias.Mallows’s \\(C_p\\) criterion, lower values preferred. Specifically:Ideal Value: model good fit correct number predictors, \\(C_p\\) close number predictors \\(p\\) plus 1 (.e., \\(p + 1\\)).Ideal Value: model good fit correct number predictors, \\(C_p\\) close number predictors \\(p\\) plus 1 (.e., \\(p + 1\\)).Model Comparison: Among competing models, generally prefer one smallest \\(C_p\\), long close \\(p + 1\\).Model Comparison: Among competing models, generally prefer one smallest \\(C_p\\), long close \\(p + 1\\).Overfitting Indicator: \\(C_p\\) significantly lower \\(p + 1\\), may suggest overfitting.Overfitting Indicator: \\(C_p\\) significantly lower \\(p + 1\\), may suggest overfitting.Underfitting Indicator: \\(C_p\\) much higher \\(p + 1\\), suggests model underfitting data missing important predictors.Underfitting Indicator: \\(C_p\\) much higher \\(p + 1\\), suggests model underfitting data missing important predictors.","code":"\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\ny <- 5 + 3*x1 - 2*x2 + rnorm(n, sd=2)\n\n# Full model and candidate models\nfull_model <- lm(y ~ x1 + x2 + x3)\nmodel_1 <- lm(y ~ x1)\nmodel_2 <- lm(y ~ x1 + x2)\n\n# Extract SSE and calculate Cp\ncalculate_cp <- function(model, full_model_sse, full_model_mse, n) {\n  sse <- sum(residuals(model)^2)\n  p <- length(coefficients(model))\n  cp <- (sse / full_model_mse) - (n - 2 * p)\n  return(cp)\n}\n\n# Full model statistics\nfull_model_sse <- sum(residuals(full_model)^2)\nfull_model_mse <- mean(residuals(full_model)^2)\n\n# Cp values for each model\ncp_1 <- calculate_cp(model_1, full_model_sse, full_model_mse, n)\ncp_2 <- calculate_cp(model_2, full_model_sse, full_model_mse, n)\n\n# Display results\ncat(\"C_p values:\\n\")\n#> C_p values:\ncat(\"Model 1 (y ~ x1):\", round(cp_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 83.64\ncat(\"Model 2 (y ~ x1 + x2):\", round(cp_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 6.27"},{"path":"variable-selection.html","id":"akaike-information-criterion-aic","chapter":"15 Variable Selection","heading":"15.1.1.2 Akaike Information Criterion (AIC)","text":"Akaike Information Criterion (AIC) widely used model selection metric evaluates tradeoff model fit complexity. introduced Hirotugu Akaike rooted information theory, measuring relative quality statistical models given dataset.model \\(p\\) parameters, AIC given :\\[\nAIC = n \\ln\\left(\\frac{SSE_p}{n}\\right) + 2p\n\\]:\\(n\\) number observations.\\(n\\) number observations.\\(SSE_p\\) sum squared errors model \\(p\\) parameters.\\(SSE_p\\) sum squared errors model \\(p\\) parameters.Key InsightsComponents AIC:\nfirst term (\\(n \\ln(SSE_p / n)\\)): Reflects goodness--fit model. decreases \\(SSE_p\\) decreases, meaning model better explains data.\nsecond term (\\(2p\\)): Represents penalty model complexity. increases number parameters discourage overfitting.\nfirst term (\\(n \\ln(SSE_p / n)\\)): Reflects goodness--fit model. decreases \\(SSE_p\\) decreases, meaning model better explains data.second term (\\(2p\\)): Represents penalty model complexity. increases number parameters discourage overfitting.Model Selection Principle:\nSmaller AIC values indicate better balance fit complexity.\nAdding parameters generally reduces \\(SSE_p\\), increases penalty term (\\(2p\\)). AIC increases parameter added, parameter likely unnecessary.\nSmaller AIC values indicate better balance fit complexity.Adding parameters generally reduces \\(SSE_p\\), increases penalty term (\\(2p\\)). AIC increases parameter added, parameter likely unnecessary.Tradeoff:\nAIC emphasizes tradeoff :\nPrecision fit: Reducing error explaining data.\nParsimony: Avoiding unnecessary parameters maintain model simplicity.\n\nAIC emphasizes tradeoff :\nPrecision fit: Reducing error explaining data.\nParsimony: Avoiding unnecessary parameters maintain model simplicity.\nPrecision fit: Reducing error explaining data.Parsimony: Avoiding unnecessary parameters maintain model simplicity.Comparative Criterion:\nAIC provide absolute measure model quality; instead, compares relative performance. model lowest AIC preferred.\nAIC provide absolute measure model quality; instead, compares relative performance. model lowest AIC preferred.InterpretationCompare AIC values across models:\nsmaller AIC indicates model better balance fit complexity.\nAIC increases moving one model another (e.g., Model 2 Model 3), additional parameter(s) larger model may justified.\nCompare AIC values across models:smaller AIC indicates model better balance fit complexity.smaller AIC indicates model better balance fit complexity.AIC increases moving one model another (e.g., Model 2 Model 3), additional parameter(s) larger model may justified.AIC increases moving one model another (e.g., Model 2 Model 3), additional parameter(s) larger model may justified.Advantages:Simple compute widely applicable.Simple compute widely applicable.Penalizes overfitting, encouraging parsimonious models.Penalizes overfitting, encouraging parsimonious models.Limitations:Assumes model errors normally distributed independent.Assumes model errors normally distributed independent.evaluate absolute model fit, relative performance.evaluate absolute model fit, relative performance.Sensitive sample size; smaller samples, consider using corrected version, AICc.Sensitive sample size; smaller samples, consider using corrected version, AICc.Corrected AIC (AICc)small sample sizes (\\(n / p \\leq 40\\)), corrected AIC, \\(AICc\\), adjusts sample size:\\[\nAICc = AIC + \\frac{2p(p+1)}{n-p-1}\n\\]adjustment prevents -penalizing models parameters \\(n\\) small.","code":"\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\ny <- 5 + 3*x1 - 2*x2 + rnorm(n, sd=2)\n\n# Candidate models\nmodel_1 <- lm(y ~ x1)\nmodel_2 <- lm(y ~ x1 + x2)\nmodel_3 <- lm(y ~ x1 + x2 + x3)\n\n# Function to manually compute AIC\ncalculate_aic <- function(model, n) {\n  sse <- sum(residuals(model)^2)\n  p <- length(coefficients(model))\n  aic <- n * log(sse / n) + 2 * p\n  return(aic)\n}\n\n# Calculate AIC for each model\naic_1 <- calculate_aic(model_1, n)\naic_2 <- calculate_aic(model_2, n)\naic_3 <- calculate_aic(model_3, n)\n\n# Display results\ncat(\"AIC values:\\n\")\n#> AIC values:\ncat(\"Model 1 (y ~ x1):\", round(aic_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 207.17\ncat(\"Model 2 (y ~ x1 + x2):\", round(aic_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 150.87\ncat(\"Model 3 (y ~ x1 + x2 + x3):\", round(aic_3, 2), \"\\n\")\n#> Model 3 (y ~ x1 + x2 + x3): 152.59"},{"path":"variable-selection.html","id":"bayesian-information-criterion-bic","chapter":"15 Variable Selection","heading":"15.1.1.3 Bayesian Information Criterion (BIC)","text":"Bayesian Information Criterion (BIC), also known Schwarz Criterion, another popular metric model selection. extends concept AIC introducing stronger penalty model complexity, particularly number observations large. BIC grounded Bayesian probability theory provides framework selecting plausible model among set candidates.model \\(p\\) parameters, BIC defined :\\[\nBIC = n \\ln\\left(\\frac{SSE_p}{n}\\right) + p \\ln(n)\n\\]:\\(n\\) number observations.\\(n\\) number observations.\\(SSE_p\\) sum squared errors model \\(p\\) parameters.\\(SSE_p\\) sum squared errors model \\(p\\) parameters.\\(p\\) number parameters, including intercept.\\(p\\) number parameters, including intercept.Key InsightsComponents BIC:\nfirst term (\\(n \\ln(SSE_p / n)\\)): Measures goodness--fit, similar AIC. decreases \\(SSE_p\\) decreases, indicating better fit data.\nsecond term (\\(p \\ln(n)\\)): Penalizes model complexity. Unlike AIC’s penalty (\\(2p\\)), penalty BIC increases \\(\\ln(n)\\), making sensitive number observations.\nfirst term (\\(n \\ln(SSE_p / n)\\)): Measures goodness--fit, similar AIC. decreases \\(SSE_p\\) decreases, indicating better fit data.second term (\\(p \\ln(n)\\)): Penalizes model complexity. Unlike AIC’s penalty (\\(2p\\)), penalty BIC increases \\(\\ln(n)\\), making sensitive number observations.Model Selection Principle:\nSmaller BIC values indicate better model.\nAdding parameters reduces \\(SSE_p\\), penalty term \\(p \\ln(n)\\) grows rapidly AIC’s \\(2p\\) large \\(n\\). makes BIC conservative AIC selecting models additional parameters.\nSmaller BIC values indicate better model.Adding parameters reduces \\(SSE_p\\), penalty term \\(p \\ln(n)\\) grows rapidly AIC’s \\(2p\\) large \\(n\\). makes BIC conservative AIC selecting models additional parameters.Tradeoff:\nLike AIC, BIC balances:\nPrecision fit: Capturing underlying structure data.\nParsimony: Avoiding overfitting discouraging unnecessary parameters.\n\nBIC tends favor simpler models compared AIC, particularly \\(n\\) large.\nLike AIC, BIC balances:\nPrecision fit: Capturing underlying structure data.\nParsimony: Avoiding overfitting discouraging unnecessary parameters.\nPrecision fit: Capturing underlying structure data.Parsimony: Avoiding overfitting discouraging unnecessary parameters.BIC tends favor simpler models compared AIC, particularly \\(n\\) large.Comparative Criterion:\nBIC, like AIC, used compare models. model smallest BIC preferred.\nBIC, like AIC, used compare models. model smallest BIC preferred.InterpretationCompare BIC values across models:\nSmaller BIC values suggest better model.\nBIC increases moving larger model, added complexity may justify reduction \\(SSE_p\\).\nCompare BIC values across models:Smaller BIC values suggest better model.Smaller BIC values suggest better model.BIC increases moving larger model, added complexity may justify reduction \\(SSE_p\\).BIC increases moving larger model, added complexity may justify reduction \\(SSE_p\\).Comparison AICBIC generally prefers simpler models AIC, especially \\(n\\) large.BIC generally prefers simpler models AIC, especially \\(n\\) large.small datasets, AIC may perform better BIC’s penalty grows significantly \\(\\ln(n)\\).small datasets, AIC may perform better BIC’s penalty grows significantly \\(\\ln(n)\\).Advantages:Strong penalty complexity makes robust overfitting.Strong penalty complexity makes robust overfitting.Incorporates sample size explicitly, favoring simpler models $n$ grows.Incorporates sample size explicitly, favoring simpler models $n$ grows.Easy compute interpret.Easy compute interpret.Limitations:Assumes model errors normally distributed independent.Assumes model errors normally distributed independent.May underfit smaller datasets penalty term dominates.May underfit smaller datasets penalty term dominates.Like AIC, BIC absolute measure model quality relative one.Like AIC, BIC absolute measure model quality relative one.","code":"\n# Function to manually compute BIC\ncalculate_bic <- function(model, n) {\n  sse <- sum(residuals(model)^2)\n  p <- length(coefficients(model))\n  bic <- n * log(sse / n) + p * log(n)\n  return(bic)\n}\n\n# Calculate BIC for each model\nbic_1 <- calculate_bic(model_1, n)\nbic_2 <- calculate_bic(model_2, n)\nbic_3 <- calculate_bic(model_3, n)\n\n# Display results\ncat(\"BIC values:\\n\")\n#> BIC values:\ncat(\"Model 1 (y ~ x1):\", round(bic_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 212.38\ncat(\"Model 2 (y ~ x1 + x2):\", round(bic_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 158.68\ncat(\"Model 3 (y ~ x1 + x2 + x3):\", round(bic_3, 2), \"\\n\")\n#> Model 3 (y ~ x1 + x2 + x3): 163.01"},{"path":"variable-selection.html","id":"hannan-quinn-criterion-hqc","chapter":"15 Variable Selection","heading":"15.1.1.4 Hannan-Quinn Criterion (HQC)","text":"Hannan-Quinn Criterion (HQC) statistical metric model selection, similar AIC BIC. evaluates tradeoff model fit complexity, offering middle ground conservative penalty BIC less stringent penalty AIC. HQC especially useful time-series modeling situations large datasets involved.HQC model \\(p\\) parameters defined :\\[\nHQC = n \\ln\\left(\\frac{SSE_p}{n}\\right) + 2p \\ln(\\ln(n))\n\\]:\\(n\\): Number observations.\\(n\\): Number observations.\\(SSE_p\\): Sum Squared Errors model \\(p\\) predictors.\\(SSE_p\\): Sum Squared Errors model \\(p\\) predictors.\\(p\\): Number parameters, including intercept.\\(p\\): Number parameters, including intercept.Key InsightsComponents:\nfirst term (\\(n \\ln(SSE_p / n)\\)): Measures goodness--fit, similar AIC BIC. Smaller SSE indicates better fit.\nsecond term (\\(2p \\ln(\\ln(n))\\)): Penalizes model complexity. penalty grows logarithmically sample size, similar BIC less severe.\nfirst term (\\(n \\ln(SSE_p / n)\\)): Measures goodness--fit, similar AIC BIC. Smaller SSE indicates better fit.second term (\\(2p \\ln(\\ln(n))\\)): Penalizes model complexity. penalty grows logarithmically sample size, similar BIC less severe.Model Selection Principle:\nSmaller HQC values indicate better balance model fit complexity.\nModels lower HQC preferred.\nSmaller HQC values indicate better balance model fit complexity.Models lower HQC preferred.Penalty Comparison:\nHQC’s penalty lies AIC BIC:\nAIC: \\(2p\\) (less conservative, favors complex models).\nBIC: \\(p \\ln(n)\\) (conservative, favors simpler models).\nHQC: \\(2p \\ln(\\ln(n))\\) (balances AIC BIC).\n\nHQC’s penalty lies AIC BIC:\nAIC: \\(2p\\) (less conservative, favors complex models).\nBIC: \\(p \\ln(n)\\) (conservative, favors simpler models).\nHQC: \\(2p \\ln(\\ln(n))\\) (balances AIC BIC).\nAIC: \\(2p\\) (less conservative, favors complex models).BIC: \\(p \\ln(n)\\) (conservative, favors simpler models).HQC: \\(2p \\ln(\\ln(n))\\) (balances AIC BIC).Use Case:\nHQC particularly suited large datasets time-series models overfitting concern, BIC may overly penalize model complexity.\nHQC particularly suited large datasets time-series models overfitting concern, BIC may overly penalize model complexity.InterpretationComparing HQC Values:\nSmaller HQC values indicate better balance goodness--fit parsimony.\nSelect model smallest HQC.\nComparing HQC Values:Smaller HQC values indicate better balance goodness--fit parsimony.Smaller HQC values indicate better balance goodness--fit parsimony.Select model smallest HQC.Select model smallest HQC.Tradeoffs:\nHQC balances fit complexity conservatively AIC less BIC.\nparticularly useful overfitting concern avoiding overly simplistic models also important.\nTradeoffs:HQC balances fit complexity conservatively AIC less BIC.HQC balances fit complexity conservatively AIC less BIC.particularly useful overfitting concern avoiding overly simplistic models also important.particularly useful overfitting concern avoiding overly simplistic models also important.Comparison CriteriaAdvantages:Less sensitive sample size BIC, avoiding excessive penalization large datasets.Less sensitive sample size BIC, avoiding excessive penalization large datasets.Provides balanced approach model selection, reducing risk overfitting avoiding overly simplistic models.Provides balanced approach model selection, reducing risk overfitting avoiding overly simplistic models.Particularly useful time-series analysis.Particularly useful time-series analysis.Limitations:Like AIC BIC, assumes model errors normally distributed independent.Like AIC BIC, assumes model errors normally distributed independent.HQC widely implemented statistical software, requiring custom calculations.HQC widely implemented statistical software, requiring custom calculations.Practical ConsiderationsWhen use HQC?\nAIC BIC provide conflicting recommendations.\nlarge datasets time-series models BIC may overly penalize complexity.\nuse HQC?AIC BIC provide conflicting recommendations.AIC BIC provide conflicting recommendations.large datasets time-series models BIC may overly penalize complexity.large datasets time-series models BIC may overly penalize complexity.use AIC BIC?\nAIC smaller datasets goal prediction.\nBIC large datasets parsimony critical.\nuse AIC BIC?AIC smaller datasets goal prediction.AIC smaller datasets goal prediction.BIC large datasets parsimony critical.BIC large datasets parsimony critical.","code":"\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nx4 <- rnorm(n)\ny <- 5 + 3*x1 - 2*x2 + x3 + rnorm(n, sd=2)\n\n# Prepare models\ndata <- data.frame(y, x1, x2, x3, x4)\nmodel_1 <- lm(y ~ x1, data = data)\nmodel_2 <- lm(y ~ x1 + x2, data = data)\nmodel_3 <- lm(y ~ x1 + x2 + x3, data = data)\n\n# Function to calculate HQC\ncalculate_hqc <- function(model, n) {\n  sse <- sum(residuals(model)^2)\n  p <- length(coefficients(model))\n  hqc <- n * log(sse / n) + 2 * p * log(log(n))\n  return(hqc)\n}\n\n# Calculate HQC for each model\nhqc_1 <- calculate_hqc(model_1, n)\nhqc_2 <- calculate_hqc(model_2, n)\nhqc_3 <- calculate_hqc(model_3, n)\n\n# Display results\ncat(\"HQC values:\\n\")\n#> HQC values:\ncat(\"Model 1 (y ~ x1):\", round(hqc_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 226.86\ncat(\"Model 2 (y ~ x1 + x2):\", round(hqc_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 156.44\ncat(\"Model 3 (y ~ x1 + x2 + x3):\", round(hqc_3, 2), \"\\n\")\n#> Model 3 (y ~ x1 + x2 + x3): 141.62"},{"path":"variable-selection.html","id":"minimum-description-length-mdl","chapter":"15 Variable Selection","heading":"15.1.1.5 Minimum Description Length (MDL)","text":"Minimum Description Length (MDL) principle model selection framework rooted information theory. balances model complexity goodness--fit seeking model minimizes total length encoding data model . MDL generalization model selection criteria like AIC BIC offers theoretical foundation.Theoretical FoundationMDL based idea best model one compresses data efficiently. represents tradeoff :Model Complexity: cost describing model, including number parameters.Data Fit: cost describing data given model.total description length expressed :\\[\nL(M, D) = L(M) + L(D | M)\n\\]:\\(L(M)\\): length encoding model (complexity model).\\(L(M)\\): length encoding model (complexity model).\\(L(D | M)\\): length encoding data given model (fit data).\\(L(D | M)\\): length encoding data given model (fit data).Key InsightsModel Complexity (\\(L(M)\\)):\ncomplex models require longer descriptions, involve parameters.\nSimpler models favored unless added complexity significantly improves fit.\ncomplex models require longer descriptions, involve parameters.Simpler models favored unless added complexity significantly improves fit.Data Fit (\\(L(D | M)\\)):\nMeasures well model explains data.\nPoorly fitting models require bits describe residual error.\nMeasures well model explains data.Poorly fitting models require bits describe residual error.Tradeoff:\nMDL balances two components, selecting model minimizes total description length.\nMDL balances two components, selecting model minimizes total description length.Connection CriteriaMDL closely related BIC. fact, BIC criterion can derived approximation MDL certain statistical models:\n\\[\nBIC = n \\ln(SSE_p / n) + p \\ln(n)\n\\]MDL closely related BIC. fact, BIC criterion can derived approximation MDL certain statistical models:\\[\nBIC = n \\ln(SSE_p / n) + p \\ln(n)\n\\]However, MDL flexible rely specific assumptions error distribution.However, MDL flexible rely specific assumptions error distribution.Practical Use CasesTime-Series Modeling: MDL particularly effective selecting models time-series data, overfitting common.Time-Series Modeling: MDL particularly effective selecting models time-series data, overfitting common.Machine Learning: MDL used regularization techniques decision tree pruning prevent overfitting.Machine Learning: MDL used regularization techniques decision tree pruning prevent overfitting.Signal Processing: applications compression coding, MDL directly guides optimal model selection.Signal Processing: applications compression coding, MDL directly guides optimal model selection.InterpretationChoosing Best Model:\nmodel smallest MDL value preferred, achieves best tradeoff fit complexity.\nChoosing Best Model:model smallest MDL value preferred, achieves best tradeoff fit complexity.Practical Implications:\nMDL discourages overfitting penalizing complex models significantly improve data fit.\nPractical Implications:MDL discourages overfitting penalizing complex models significantly improve data fit.Advantages:Theoretically grounded information theory.Theoretically grounded information theory.Offers natural framework balancing complexity fit.Offers natural framework balancing complexity fit.Flexible can applied across various modeling frameworks.Flexible can applied across various modeling frameworks.Limitations:Computationally intensive, especially non-linear models.Computationally intensive, especially non-linear models.Requires careful formulation $L(M)$ $L(D | M)$ non-standard models.Requires careful formulation $L(M)$ $L(D | M)$ non-standard models.Less common standard statistical software compared AIC BIC.Less common standard statistical software compared AIC BIC.","code":"\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\ny <- 5 + 3*x1 - 2*x2 + x3 + rnorm(n, sd=2)\n\n# Prepare models\ndata <- data.frame(y, x1, x2, x3)\nmodel_1 <- lm(y ~ x1, data = data)\nmodel_2 <- lm(y ~ x1 + x2, data = data)\nmodel_3 <- lm(y ~ x1 + x2 + x3, data = data)\n\n# Function to calculate MDL\ncalculate_mdl <- function(model, n) {\n  sse <- sum(residuals(model)^2)\n  p <- length(coefficients(model))\n  mdl <- p * log(n) + n * log(sse / n)\n  return(mdl)\n}\n\n# Calculate MDL for each model\nmdl_1 <- calculate_mdl(model_1, n)\nmdl_2 <- calculate_mdl(model_2, n)\nmdl_3 <- calculate_mdl(model_3, n)\n\n# Display results\ncat(\"MDL values:\\n\")\n#> MDL values:\ncat(\"Model 1 (y ~ x1):\", round(mdl_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 219.87\ncat(\"Model 2 (y ~ x1 + x2):\", round(mdl_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 173.42\ncat(\"Model 3 (y ~ x1 + x2 + x3):\", round(mdl_3, 2), \"\\n\")\n#> Model 3 (y ~ x1 + x2 + x3): 163.01"},{"path":"variable-selection.html","id":"prediction-error-sum-of-squares-press","chapter":"15 Variable Selection","heading":"15.1.1.6 Prediction Error Sum of Squares (PRESS)","text":"Prediction Error Sum Squares (PRESS) statistic measures predictive ability model evaluating well performs data used fitting model. PRESS particularly useful assessing model validity identifying overfitting.PRESS statistic model \\(p\\) parameters defined :\\[\nPRESS_p = \\sum_{=1}^{n} (Y_i - \\hat{Y}_{()})^2\n\\]:\\(\\hat{Y}_{()}\\) prediction \\(\\)-th response \\(\\)-th observation omitted model fitting.\\(\\hat{Y}_{()}\\) prediction \\(\\)-th response \\(\\)-th observation omitted model fitting.\\(Y_i\\) observed response \\(\\)-th observation.\\(Y_i\\) observed response \\(\\)-th observation.Key InsightsLeave-One-Cross-Validation (LOOCV):\nPRESS computed excluding observation one time predicting response using remaining data.\nprocess evaluates model’s generalizability reduces overfitting.\nPRESS computed excluding observation one time predicting response using remaining data.process evaluates model’s generalizability reduces overfitting.Model Selection Principle:\nSmaller values \\(PRESS_p\\) indicate better predictive performance.\nsmall \\(PRESS_p\\) suggests model captures underlying structure data without overfitting.\nSmaller values \\(PRESS_p\\) indicate better predictive performance.small \\(PRESS_p\\) suggests model captures underlying structure data without overfitting.Computational Complexity:\nComputing \\(\\hat{Y}_{()}\\) observation can computationally intensive models large \\(p\\) datasets many observations.\nAlternative approximations (e.g., using leverage values) can simplify computations.\nComputing \\(\\hat{Y}_{()}\\) observation can computationally intensive models large \\(p\\) datasets many observations.Alternative approximations (e.g., using leverage values) can simplify computations.InterpretationCompare PRESS values across models:\nModels smaller PRESS values preferred exhibit better predictive ability.\nlarge PRESS value indicates potential overfitting poor model generalizability.\nCompare PRESS values across models:Models smaller PRESS values preferred exhibit better predictive ability.Models smaller PRESS values preferred exhibit better predictive ability.large PRESS value indicates potential overfitting poor model generalizability.large PRESS value indicates potential overfitting poor model generalizability.Advantages:Provides unbiased measure predictive performance.Provides unbiased measure predictive performance.Helps identify overfitting simulating model’s performance unseen data.Helps identify overfitting simulating model’s performance unseen data.Limitations:Computationally intensive large datasets models many predictors.Computationally intensive large datasets models many predictors.Sensitive influential observations; high-leverage points can disproportionately affect results.Sensitive influential observations; high-leverage points can disproportionately affect results.Alternative ApproachesTo address computational challenges PRESS, alternative methods can employed:Approximation using leverage values: shown example, leverage values simplify calculation \\(\\hat{Y}_{()}\\).Approximation using leverage values: shown example, leverage values simplify calculation \\(\\hat{Y}_{()}\\).K-Fold Cross-Validation: Dividing dataset \\(k\\) folds reduces computational burden compared LOOCV still providing robust estimates.K-Fold Cross-Validation: Dividing dataset \\(k\\) folds reduces computational burden compared LOOCV still providing robust estimates.","code":"\n# Function to compute PRESS\ncalculate_press <- function(model) {\n  residuals <- residuals(model)\n  h <- lm.influence(model)$hat # leverage values\n  press <- sum((residuals / (1 - h))^2) # PRESS formula using leverage\n  return(press)\n}\n\n# Calculate PRESS for each model\npress_1 <- calculate_press(model_1)\npress_2 <- calculate_press(model_2)\npress_3 <- calculate_press(model_3)\n\n# Display results\ncat(\"PRESS values:\\n\")\n#> PRESS values:\ncat(\"Model 1 (y ~ x1):\", round(press_1, 2), \"\\n\")\n#> Model 1 (y ~ x1): 854.36\ncat(\"Model 2 (y ~ x1 + x2):\", round(press_2, 2), \"\\n\")\n#> Model 2 (y ~ x1 + x2): 524.56\ncat(\"Model 3 (y ~ x1 + x2 + x3):\", round(press_3, 2), \"\\n\")\n#> Model 3 (y ~ x1 + x2 + x3): 460"},{"path":"variable-selection.html","id":"univariate-selection-methods","chapter":"15 Variable Selection","heading":"15.1.2 Univariate Selection Methods","text":"Univariate selection methods evaluate individual variables isolation determine relationship target variable. methods often categorized filter methods, involve predictive model instead rely statistical significance information-theoretic measures.Univariate selection particularly useful :Preprocessing large datasets eliminating irrelevant features.Preprocessing large datasets eliminating irrelevant features.Reducing dimensionality applying complex feature selection techniques.Reducing dimensionality applying complex feature selection techniques.Improving interpretability identifying relevant features.Improving interpretability identifying relevant features.two main categories univariate selection methods :Statistical Tests: Evaluate significance relationships individual features target variable.Information-Theoretic Measures: Assess dependency variables based information gain mutual information.","code":""},{"path":"variable-selection.html","id":"statistical-tests","chapter":"15 Variable Selection","heading":"15.1.2.1 Statistical Tests","text":"Statistical tests assess significance relationships individual predictors target variable. choice test depends type data:Check Descriptive Statistics Basic Statistical Inference details.","code":""},{"path":"variable-selection.html","id":"information-theoretic-measures","chapter":"15 Variable Selection","heading":"15.1.2.2 Information-Theoretic Measures","text":"Information-theoretic measures assess variable relevance based much information provide target.","code":""},{"path":"variable-selection.html","id":"information-gain","chapter":"15 Variable Selection","heading":"15.1.2.2.1 Information Gain","text":"Information Gain (IG) measures reduction uncertainty target variable predictor known. used extensively decision trees.Formula: \\[\nIG = H(Y) - H(Y | X)\n\\] :\\(H(Y)\\) = Entropy target variable\\(H(Y)\\) = Entropy target variable\\(H(Y | X)\\) = Conditional entropy target given predictor\\(H(Y | X)\\) = Conditional entropy target given predictorA higher IG indicates informative variable.","code":"\n# Load Library\nlibrary(FSelector)\n\n# Example: Computing Information Gain\ndata(iris)\ninformation.gain(Species ~ ., iris)\n#>              attr_importance\n#> Sepal.Length       0.4521286\n#> Sepal.Width        0.2672750\n#> Petal.Length       0.9402853\n#> Petal.Width        0.9554360"},{"path":"variable-selection.html","id":"mutual-information","chapter":"15 Variable Selection","heading":"15.1.2.2.2 Mutual Information","text":"Mutual Information (MI) quantifies much knowing one variable reduces uncertainty another. Unlike correlation, captures linear non-linear relationships.Pros: Captures non-linear relationships, robust outliers.Pros: Captures non-linear relationships, robust outliers.Cons: computationally intensive correlation.Cons: computationally intensive correlation.Formula: \\[\nMI(X, Y) = \\sum_{x,y} P(x, y) \\log \\frac{P(x, y)}{P(x) P(y)}\n\\] :\\(P(x,y)\\) = Joint probability distribution \\(X\\) \\(Y\\).\\(P(x,y)\\) = Joint probability distribution \\(X\\) \\(Y\\).\\(P(x)\\), \\(P(y)\\) = Marginal probability distributions.\\(P(x)\\), \\(P(y)\\) = Marginal probability distributions.Since X Y independently sampled, expect mutual dependence.Since X Y independently sampled, expect mutual dependence.MI value close 0, indicating knowing X provides almost information Y, vice versa.MI value close 0, indicating knowing X provides almost information Y, vice versa.MI value significantly greater 0, due random fluctuations, especially small samples.MI value significantly greater 0, due random fluctuations, especially small samples.","code":"\n# Load Library\nlibrary(infotheo)\n\n# Compute Mutual Information Between Two Features\nset.seed(123)\nX <- sample(1:5, 100, replace=TRUE)\nY <- sample(1:5, 100, replace=TRUE)\n\nmutinformation(X, Y)\n#> [1] 0.06852247"},{"path":"variable-selection.html","id":"correlation-based-feature-selection","chapter":"15 Variable Selection","heading":"15.1.3 Correlation-Based Feature Selection","text":"Evaluates features based correlation target redundancy features. Check Descriptive Statistics Basic Statistical Inference details.","code":""},{"path":"variable-selection.html","id":"variance-thresholding","chapter":"15 Variable Selection","heading":"15.1.4 Variance Thresholding","text":"Variance Thresholding simple yet effective filter method used feature selection. removes features low variance, assuming low-variance features contribute little model prediction. technique particularly useful :Handling high-dimensional datasets many features contain little useful information.Reducing computational complexity removing uninformative features.Avoiding overfitting eliminating features nearly constant across samples.method effective dealing binary features (e.g., categorical variables encoded 0s 1s) numerical features low variance.Variance measures spread feature’s values. feature low variance contains nearly value observations, making less useful predictive modeling.feature \\(X\\), variance calculated :\\[\nVar(X) = \\frac{1}{n} \\sum_{=1}^{n} (X_i - \\bar{X})^2\n\\]:\\(X_i\\) individual observation,\\(X_i\\) individual observation,\\(\\bar{X}\\) mean \\(X\\),\\(\\bar{X}\\) mean \\(X\\),\\(n\\) number observations.\\(n\\) number observations.Example: Features Low High Variance","code":""},{"path":"variable-selection.html","id":"identifying-low-variance-features","chapter":"15 Variable Selection","heading":"15.1.4.1 Identifying Low-Variance Features","text":"variance threshold set remove features certain variance level. default threshold 0, removes features single constant value across samples.","code":"\n# Load necessary library\nlibrary(caret)\n\n# Generate synthetic dataset\nset.seed(123)\ndata <- data.frame(\n  Feature1 = c(rep(0, 50), rep(1, 50)), # Low variance\n  Feature2 = rnorm(100, mean=10, sd=1), # High variance\n  Feature3 = runif(100, min=5, max=15), # Moderate variance\n  Feature4 = c(rep(3, 95), rep(4, 5))  # Almost constant\n)\n\n\n# Compute Variance of Features\nvariances <- apply(data, 2, stats::var)\nprint(variances)\n#>  Feature1  Feature2  Feature3  Feature4 \n#> 0.2525253 0.8332328 8.6631461 0.0479798\n\n# Set threshold and remove low-variance features\nthreshold <- 0.1\nselected_features <- names(variances[variances > threshold])\nfiltered_data <- data[, selected_features]\n\nprint(selected_features)  # Remaining features after filtering\n#> [1] \"Feature1\" \"Feature2\" \"Feature3\""},{"path":"variable-selection.html","id":"handling-binary-categorical-features","chapter":"15 Variable Selection","heading":"15.1.4.2 Handling Binary Categorical Features","text":"binary features (0/1 values), variance computed : \\[\nVar(X) = p(1 - p)\n\\] \\(p\\) proportion ones.\\(p \\approx 0\\) \\(p \\approx 1\\), variance low, meaning feature almost constant.\\(p \\approx 0\\) \\(p \\approx 1\\), variance low, meaning feature almost constant.\\(p = 0.5\\), variance highest, meaning equal distribution 0s 1s.\\(p = 0.5\\), variance highest, meaning equal distribution 0s 1s.","code":"\n# Binary feature dataset\nbinary_data <- data.frame(\n  Feature_A = c(rep(0, 98), rep(1, 2)), # Low variance (almost all 0s)\n  Feature_B = sample(0:1, 100, replace=TRUE) # Higher variance\n)\n\n# Compute variance for binary features\nbinary_variances <- apply(binary_data, 2, stats::var)\nprint(binary_variances)\n#>  Feature_A  Feature_B \n#> 0.01979798 0.24757576\n\n# Apply threshold (removing features with variance < 0.01)\nthreshold <- 0.01\nfiltered_binary <- binary_data[, binary_variances > threshold]\nprint(colnames(filtered_binary))\n#> [1] \"Feature_A\" \"Feature_B\""},{"path":"variable-selection.html","id":"wrapper-methods-model-based-subset-evaluation","chapter":"15 Variable Selection","heading":"15.2 Wrapper Methods (Model-Based Subset Evaluation)","text":"","code":""},{"path":"variable-selection.html","id":"best-subsets-algorithm-1","chapter":"15 Variable Selection","heading":"15.2.1 Best Subsets Algorithm","text":"Best Subsets Algorithm systematic method selecting best combination predictors. Unlike exhaustive search, evaluates possible subsets predictors, algorithm efficiently narrows search space guaranteeing identification best subset size.algorithm based “leap bounds” method introduced (Furnival Wilson 2000).combines:\nComparison SSE: Evaluates models Sum Squared Errors.\nControl sequence: Optimizes order subset models computed.\nComparison SSE: Evaluates models Sum Squared Errors.Control sequence: Optimizes order subset models computed.Guarantees: Finds best \\(m\\) subset models within subset size reducing computational burden compared evaluating possible subsets.Key FeaturesSubset Comparison:\nalgorithm ranks subsets based criterion \\(R^2\\), adjusted \\(R^2\\), AIC, BIC.\nevaluates models varying sizes, starting 1 predictor \\(p\\) predictors.\nalgorithm ranks subsets based criterion \\(R^2\\), adjusted \\(R^2\\), AIC, BIC.evaluates models varying sizes, starting 1 predictor \\(p\\) predictors.Efficiency:\nleveraging “leap bounds,” algorithm avoids evaluating subsets unlikely yield best results.\nreduces computational cost significantly compared evaluating \\(2^p\\) subsets exhaustive search.\nleveraging “leap bounds,” algorithm avoids evaluating subsets unlikely yield best results.reduces computational cost significantly compared evaluating \\(2^p\\) subsets exhaustive search.Output:\nProduces best subsets model size, can compared using criteria like AIC, BIC, PRESS.\nProduces best subsets model size, can compared using criteria like AIC, BIC, PRESS.InterpretationModel Size:\nExamine metrics models 1, 2, 3, 4 predictors.\nChoose model size optimizes preferred metric (e.g., maximized adjusted \\(R^2\\), minimized BIC).\nModel Size:Examine metrics models 1, 2, 3, 4 predictors.Examine metrics models 1, 2, 3, 4 predictors.Choose model size optimizes preferred metric (e.g., maximized adjusted \\(R^2\\), minimized BIC).Choose model size optimizes preferred metric (e.g., maximized adjusted \\(R^2\\), minimized BIC).Model Comparison:\nsmall datasets, adjusted \\(R^2\\) often reliable criterion.\nlarger datasets, use BIC AIC avoid overfitting.\nModel Comparison:small datasets, adjusted \\(R^2\\) often reliable criterion.small datasets, adjusted \\(R^2\\) often reliable criterion.larger datasets, use BIC AIC avoid overfitting.larger datasets, use BIC AIC avoid overfitting.Efficiency:\nalgorithm evaluates far fewer models exhaustive search still guaranteeing optimal results subset size.\nEfficiency:algorithm evaluates far fewer models exhaustive search still guaranteeing optimal results subset size.Advantages:Computationally efficient compared evaluating possible subsets.Computationally efficient compared evaluating possible subsets.Guarantees best subsets model size.Guarantees best subsets model size.Flexibility use different selection criteria (e.g., \\(R^2\\), AIC, BIC).Flexibility use different selection criteria (e.g., \\(R^2\\), AIC, BIC).Limitations:May become computationally intensive large \\(p\\) (e.g., hundreds predictors).May become computationally intensive large \\(p\\) (e.g., hundreds predictors).Assumes linear relationships among predictors outcome.Assumes linear relationships among predictors outcome.Practical ConsiderationsWhen use Best Subsets?\ndatasets moderate number predictors (\\(p \\leq 20\\)).\nneed optimal solution subset size.\nuse Best Subsets?datasets moderate number predictors (\\(p \\leq 20\\)).datasets moderate number predictors (\\(p \\leq 20\\)).need optimal solution subset size.need optimal solution subset size.Alternatives:\nStepwise Selection: Faster less reliable.\nRegularization Techniques: LASSO Ridge regression handle large \\(p\\) collinearity effectively.\nAlternatives:Stepwise Selection: Faster less reliable.Stepwise Selection: Faster less reliable.Regularization Techniques: LASSO Ridge regression handle large \\(p\\) collinearity effectively.Regularization Techniques: LASSO Ridge regression handle large \\(p\\) collinearity effectively.","code":"\n# Load the leaps package\nlibrary(\"leaps\")\n\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nx4 <- rnorm(n)\ny <- 5 + 3*x1 - 2*x2 + x3 + rnorm(n, sd=2)\n\n# Prepare data for best subsets model\ndata <- data.frame(y, x1, x2, x3, x4)\n\n# Perform best subsets model\nbest_subsets <- regsubsets(y ~ ., data = data, nvmax = 4)\n\n# Summarize results\nbest_summary <- summary(best_subsets)\n\n# Display model selection metrics\ncat(\"Best Subsets Summary:\\n\")\n#> Best Subsets Summary:\ncat(\"Adjusted R^2:\\n\", best_summary$adjr2, \"\\n\")\n#> Adjusted R^2:\n#>  0.3651194 0.69237 0.7400424 0.7374724\ncat(\"Cp:\\n\", best_summary$cp, \"\\n\")\n#> Cp:\n#>  140.9971 19.66463 3.060205 5\ncat(\"BIC:\\n\", best_summary$bic, \"\\n\")\n#> BIC:\n#>  -37.23673 -106.1111 -119.3802 -114.8383\n\n# Visualize results\nplot(best_subsets, scale = \"adjr2\")\ntitle(\"Best Subsets: Adjusted R^2\")"},{"path":"variable-selection.html","id":"stepwise-selection-methods-1","chapter":"15 Variable Selection","heading":"15.2.2 Stepwise Selection Methods","text":"Stepwise selection procedures iterative methods selecting predictor variables. techniques balance model simplicity predictive accuracy systematically adding removing variables based predefined criteria.Notes:Computer implementations often replace exact F-values “significance” levels:\nSLE: Significance level enter.\nSLS: Significance level stay.\nSLE: Significance level enter.SLS: Significance level stay.thresholds serve guides rather strict tests significance.Balancing SLE SLS:Large SLE values: May include many variables, risking overfitting.Small SLE values: May exclude important variables, leading underfitting overestimation \\(\\sigma^2\\).reasonable range SLE 0.05 0.5.Practical advice:\nSLE > SLS, cycling may occur (adding removing variable repeatedly). fix , set \\(SLS = SLE / 2\\) (Bendel Afifi 1977).\nSLE < SLS, procedure becomes conservative, retaining variables minimal contribution.\nSLE > SLS, cycling may occur (adding removing variable repeatedly). fix , set \\(SLS = SLE / 2\\) (Bendel Afifi 1977).SLE < SLS, procedure becomes conservative, retaining variables minimal contribution.","code":""},{"path":"variable-selection.html","id":"forward-selection","chapter":"15 Variable Selection","heading":"15.2.2.1 Forward Selection","text":"Forward Selection starts empty model (intercept) sequentially adds predictors. step, variable improves model fit (based criteria like \\(R^2\\), AIC, F-statistic) added. process stops variable improves model significantly.StepsBegin null model: \\(Y = \\beta_0\\).Evaluate predictor’s contribution model (e.g., using F-statistic AIC).Add predictor significant improvement model.Repeat remaining variable significantly improves model.","code":""},{"path":"variable-selection.html","id":"backward-elimination","chapter":"15 Variable Selection","heading":"15.2.2.2 Backward Elimination","text":"Backward Elimination starts full model, containing predictors, sequentially removes least significant predictor (based criteria like p-value F-statistic). process stops remaining predictors meet significance threshold.StepsBegin full model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\).Begin full model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\).Identify predictor smallest contribution model (e.g., highest p-value).Identify predictor smallest contribution model (e.g., highest p-value).Remove least significant predictor.Remove least significant predictor.Repeat remaining predictors statistically significant.Repeat remaining predictors statistically significant.","code":""},{"path":"variable-selection.html","id":"stepwise-both-directions-selection","chapter":"15 Variable Selection","heading":"15.2.2.3 Stepwise (Both Directions) Selection","text":"Stepwise Selection combines forward selection backward elimination. step, evaluates whether add remove predictors based predefined criteria. iterative process ensures variables added earlier steps can removed later longer contribute significantly.StepsStart null model user-specified initial model.Start null model user-specified initial model.Evaluate predictors model inclusion (forward step).Evaluate predictors model inclusion (forward step).Evaluate predictors currently model removal (backward step).Evaluate predictors currently model removal (backward step).Repeat steps 2 3 addition removal improves model.Repeat steps 2 3 addition removal improves model.","code":""},{"path":"variable-selection.html","id":"comparison-of-methods","chapter":"15 Variable Selection","heading":"15.2.2.4 Comparison of Methods","text":"Interpretation ResultsForward Selection:\nStarts predictors sequentially adds variables.\nVariables chosen based contribution improving model fit (e.g., reducing SSE, increasing \\(R^2\\)).\nForward Selection:Starts predictors sequentially adds variables.Starts predictors sequentially adds variables.Variables chosen based contribution improving model fit (e.g., reducing SSE, increasing \\(R^2\\)).Variables chosen based contribution improving model fit (e.g., reducing SSE, increasing \\(R^2\\)).Backward Elimination:\nBegins predictors removes least significant one step.\nStops remaining predictors meet significance criterion.\nBackward Elimination:Begins predictors removes least significant one step.Begins predictors removes least significant one step.Stops remaining predictors meet significance criterion.Stops remaining predictors meet significance criterion.Stepwise Selection:\nCombines forward selection backward elimination.\nstep, evaluates whether add remove variables based predefined criteria.\nStepwise Selection:Combines forward selection backward elimination.Combines forward selection backward elimination.step, evaluates whether add remove variables based predefined criteria.step, evaluates whether add remove variables based predefined criteria.Practical ConsiderationsSLE SLS Tuning:\nChoose thresholds carefully balance model simplicity predictive performance.\napplications, set \\(SLS = SLE / 2\\) prevent cycling.\nSLE SLS Tuning:Choose thresholds carefully balance model simplicity predictive performance.Choose thresholds carefully balance model simplicity predictive performance.applications, set \\(SLS = SLE / 2\\) prevent cycling.applications, set \\(SLS = SLE / 2\\) prevent cycling.Order Entry:\nStepwise selection unaffected order variable entry. Results depend data significance criteria.\nOrder Entry:Stepwise selection unaffected order variable entry. Results depend data significance criteria.Automated Procedures:\nForward selection simpler less robust forward stepwise.\nBackward elimination works well starting predictors.\nAutomated Procedures:Forward selection simpler less robust forward stepwise.Forward selection simpler less robust forward stepwise.Backward elimination works well starting predictors.Backward elimination works well starting predictors.Advantages:Automates variable selection, reducing manual effort.Automates variable selection, reducing manual effort.Balances model fit parsimony using predefined criteria.Balances model fit parsimony using predefined criteria.Flexible: Works different metrics (e.g., SSE, \\(R^2\\), AIC, BIC).Flexible: Works different metrics (e.g., SSE, \\(R^2\\), AIC, BIC).Limitations:Can sensitive significance thresholds (SLE, SLS).Can sensitive significance thresholds (SLE, SLS).Risk excluding important variables datasets multicollinearity.Risk excluding important variables datasets multicollinearity.May overfit underfit SLE/SLS thresholds poorly chosen.May overfit underfit SLE/SLS thresholds poorly chosen.Practical Use CasesForward Stepwise:\nstarting minimal knowledge predictor importance.\nForward Stepwise:starting minimal knowledge predictor importance.Backward Elimination:\nstarting many predictors needing reduce model complexity.\nBackward Elimination:starting many predictors needing reduce model complexity.Stepwise (Directions):\nbalanced approach adapts variables added removed.\nStepwise (Directions):balanced approach adapts variables added removed.","code":"\n# Simulated Data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nx4 <- rnorm(n)\ny <- 5 + 3 * x1 - 2 * x2 + x3 + rnorm(n, sd = 2)\n\n# Prepare data\ndata <- data.frame(y, x1, x2, x3, x4)\n\n# Null and Full Models\nnull_model <- lm(y ~ 1, data = data)\nfull_model <- lm(y ~ ., data = data)\n\n# Forward Selection\nforward_model <- step(\n    null_model,\n    scope = list(lower = null_model, upper = full_model),\n    direction = \"forward\"\n)\n#> Start:  AIC=269.2\n#> y ~ 1\n#> \n#>        Df Sum of Sq     RSS    AIC\n#> + x1    1    537.56  909.32 224.75\n#> + x2    1    523.27  923.62 226.31\n#> <none>              1446.88 269.20\n#> + x3    1     23.56 1423.32 269.56\n#> + x4    1      8.11 1438.78 270.64\n#> \n#> Step:  AIC=224.75\n#> y ~ x1\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> + x2    1    473.21 436.11 153.27\n#> + x3    1     62.65 846.67 219.61\n#> <none>              909.32 224.75\n#> + x4    1      3.34 905.98 226.38\n#> \n#> Step:  AIC=153.27\n#> y ~ x1 + x2\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> + x3    1    71.382 364.73 137.40\n#> <none>              436.11 153.27\n#> + x4    1     0.847 435.27 155.08\n#> \n#> Step:  AIC=137.4\n#> y ~ x1 + x2 + x3\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> <none>              364.73 137.40\n#> + x4    1     0.231 364.50 139.34\n\n# Backward Elimination\nbackward_model <- step(full_model, direction = \"backward\")\n#> Start:  AIC=139.34\n#> y ~ x1 + x2 + x3 + x4\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> - x4    1      0.23 364.73 137.40\n#> <none>              364.50 139.34\n#> - x3    1     70.77 435.27 155.08\n#> - x2    1    480.14 844.64 221.37\n#> - x1    1    525.72 890.22 226.63\n#> \n#> Step:  AIC=137.4\n#> y ~ x1 + x2 + x3\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> <none>              364.73 137.40\n#> - x3    1     71.38 436.11 153.27\n#> - x2    1    481.94 846.67 219.61\n#> - x1    1    528.02 892.75 224.91\n\n# Stepwise Selection (Both Directions)\nstepwise_model <- step(\n    null_model,\n    scope = list(lower = null_model, upper = full_model),\n    direction = \"both\"\n)\n#> Start:  AIC=269.2\n#> y ~ 1\n#> \n#>        Df Sum of Sq     RSS    AIC\n#> + x1    1    537.56  909.32 224.75\n#> + x2    1    523.27  923.62 226.31\n#> <none>              1446.88 269.20\n#> + x3    1     23.56 1423.32 269.56\n#> + x4    1      8.11 1438.78 270.64\n#> \n#> Step:  AIC=224.75\n#> y ~ x1\n#> \n#>        Df Sum of Sq     RSS    AIC\n#> + x2    1    473.21  436.11 153.27\n#> + x3    1     62.65  846.67 219.61\n#> <none>               909.32 224.75\n#> + x4    1      3.34  905.98 226.38\n#> - x1    1    537.56 1446.88 269.20\n#> \n#> Step:  AIC=153.27\n#> y ~ x1 + x2\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> + x3    1     71.38 364.73 137.40\n#> <none>              436.11 153.27\n#> + x4    1      0.85 435.27 155.08\n#> - x2    1    473.21 909.32 224.75\n#> - x1    1    487.50 923.62 226.31\n#> \n#> Step:  AIC=137.4\n#> y ~ x1 + x2 + x3\n#> \n#>        Df Sum of Sq    RSS    AIC\n#> <none>              364.73 137.40\n#> + x4    1      0.23 364.50 139.34\n#> - x3    1     71.38 436.11 153.27\n#> - x2    1    481.94 846.67 219.61\n#> - x1    1    528.02 892.75 224.91\n\n# Summarize Results\ncat(\"Forward Selection:\\n\")\n#> Forward Selection:\nprint(summary(forward_model))\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2 + x3, data = data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.9675 -1.1364  0.1726  1.3983  4.9332 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   5.2332     0.1990  26.300  < 2e-16 ***\n#> x1            2.5541     0.2167  11.789  < 2e-16 ***\n#> x2           -2.2852     0.2029 -11.263  < 2e-16 ***\n#> x3            0.9018     0.2080   4.335  3.6e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.949 on 96 degrees of freedom\n#> Multiple R-squared:  0.7479, Adjusted R-squared:   0.74 \n#> F-statistic: 94.94 on 3 and 96 DF,  p-value: < 2.2e-16\n\ncat(\"\\nBackward Elimination:\\n\")\n#> \n#> Backward Elimination:\nprint(summary(backward_model))\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2 + x3, data = data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.9675 -1.1364  0.1726  1.3983  4.9332 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   5.2332     0.1990  26.300  < 2e-16 ***\n#> x1            2.5541     0.2167  11.789  < 2e-16 ***\n#> x2           -2.2852     0.2029 -11.263  < 2e-16 ***\n#> x3            0.9018     0.2080   4.335  3.6e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.949 on 96 degrees of freedom\n#> Multiple R-squared:  0.7479, Adjusted R-squared:   0.74 \n#> F-statistic: 94.94 on 3 and 96 DF,  p-value: < 2.2e-16\n\ncat(\"\\nStepwise Selection:\\n\")\n#> \n#> Stepwise Selection:\nprint(summary(stepwise_model))\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2 + x3, data = data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.9675 -1.1364  0.1726  1.3983  4.9332 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   5.2332     0.1990  26.300  < 2e-16 ***\n#> x1            2.5541     0.2167  11.789  < 2e-16 ***\n#> x2           -2.2852     0.2029 -11.263  < 2e-16 ***\n#> x3            0.9018     0.2080   4.335  3.6e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.949 on 96 degrees of freedom\n#> Multiple R-squared:  0.7479, Adjusted R-squared:   0.74 \n#> F-statistic: 94.94 on 3 and 96 DF,  p-value: < 2.2e-16"},{"path":"variable-selection.html","id":"branch-and-bound-algorithm-1","chapter":"15 Variable Selection","heading":"15.2.3 Branch-and-Bound Algorithm","text":"Branch--Bound Algorithm systematic optimization method used solving subset selection problems (Furnival Wilson 2000). identifies best subsets predictors exploring solution space efficiently, avoiding need evaluate possible subsets.algorithm particularly suited problems large number potential predictors. systematically evaluates subsets predictors, using bounds prune search space reduce computational effort.Branching: Divides solution space smaller subsets (branches).Bounding: Calculates bounds best possible solution within branch decide whether explore discard.Key FeaturesSubset Selection:\nUsed identify best subset predictors.\nEvaluates subsets based criteria like \\(R^2\\), adjusted \\(R^2\\), AIC, BIC, Mallows’s \\(C_p\\).\nUsed identify best subset predictors.Evaluates subsets based criteria like \\(R^2\\), adjusted \\(R^2\\), AIC, BIC, Mallows’s \\(C_p\\).Efficiency:\nAvoids exhaustive search, evaluates \\(2^p\\) subsets \\(p\\) predictors.\nReduces computational burden eliminating branches contain optimal solution.\nAvoids exhaustive search, evaluates \\(2^p\\) subsets \\(p\\) predictors.Reduces computational burden eliminating branches contain optimal solution.Guarantee:\nFinds globally optimal subset specified criterion.\nFinds globally optimal subset specified criterion.Algorithm StepsInitialization:\nStart full set predictors.\nDefine criterion evaluation (e.g., adjusted \\(R^2\\), AIC, BIC).\nStart full set predictors.Define criterion evaluation (e.g., adjusted \\(R^2\\), AIC, BIC).Branching:\nDivide predictors smaller subsets (branches) systematically.\nDivide predictors smaller subsets (branches) systematically.Bounding:\nCompute bounds criterion branch.\nbound indicates branch improve current best solution, discard .\nCompute bounds criterion branch.bound indicates branch improve current best solution, discard .Pruning:\nSkip evaluating subsets within discarded branches.\nSkip evaluating subsets within discarded branches.Stopping:\nalgorithm terminates branches either evaluated pruned.\nalgorithm terminates branches either evaluated pruned.InterpretationOptimal Subsets:\nalgorithm identifies best subset predictors model size.\nEvaluate models based metrics like adjusted \\(R^2\\), BIC, AIC.\nOptimal Subsets:algorithm identifies best subset predictors model size.algorithm identifies best subset predictors model size.Evaluate models based metrics like adjusted \\(R^2\\), BIC, AIC.Evaluate models based metrics like adjusted \\(R^2\\), BIC, AIC.Visualization:\nplot adjusted \\(R^2\\) helps identify model size best tradeoff fit complexity.\nVisualization:plot adjusted \\(R^2\\) helps identify model size best tradeoff fit complexity.Efficiency:\nBranch--Bound Algorithm achieves optimal results without evaluating \\(2^p\\) subsets.\nEfficiency:Branch--Bound Algorithm achieves optimal results without evaluating \\(2^p\\) subsets.Advantages:Guarantees best subset given criterion.Guarantees best subset given criterion.Reduces computational cost compared exhaustive search.Reduces computational cost compared exhaustive search.Handles moderate-sized problems efficiently.Handles moderate-sized problems efficiently.Limitations:Computationally intensive large datasets many predictors (\\(p > 20\\)).Computationally intensive large datasets many predictors (\\(p > 20\\)).Requires clearly defined evaluation criterion.Requires clearly defined evaluation criterion.Practical ConsiderationsWhen use Branch--Bound?\n\\(p\\) (number predictors) moderate (\\(p \\leq 20\\)).\nglobal optimality subset selection essential.\nuse Branch--Bound?\\(p\\) (number predictors) moderate (\\(p \\leq 20\\)).\\(p\\) (number predictors) moderate (\\(p \\leq 20\\)).global optimality subset selection essential.global optimality subset selection essential.Alternatives:\nlarger \\(p\\), consider heuristic methods like stepwise selection regularization techniques (e.g., LASSO, Ridge).\nAlternatives:larger \\(p\\), consider heuristic methods like stepwise selection regularization techniques (e.g., LASSO, Ridge).Applications:\nBranch--Bound widely used fields like statistics, operations research, machine learning optimal subset selection crucial.\nApplications:Branch--Bound widely used fields like statistics, operations research, machine learning optimal subset selection crucial.","code":"\n# Load the leaps package\nlibrary(\"leaps\")\n\n# Simulated data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nx4 <- rnorm(n)\ny <- 5 + 3 * x1 - 2 * x2 + x3 + rnorm(n, sd = 2)\n\n# Prepare data for subset selection\ndata <- data.frame(y, x1, x2, x3, x4)\n\n# Perform best subset selection using branch-and-bound\nbest_subsets <-\n    regsubsets(y ~ .,\n               data = data,\n               nvmax = 4,\n               method = \"seqrep\")\n\n# Summarize results\nbest_summary <- summary(best_subsets)\n\n# Display results\ncat(\"Best Subsets Summary:\\n\")\n#> Best Subsets Summary:\nprint(best_summary)\n#> Subset selection object\n#> Call: regsubsets.formula(y ~ ., data = data, nvmax = 4, method = \"seqrep\")\n#> 4 Variables  (and intercept)\n#>    Forced in Forced out\n#> x1     FALSE      FALSE\n#> x2     FALSE      FALSE\n#> x3     FALSE      FALSE\n#> x4     FALSE      FALSE\n#> 1 subsets of each size up to 4\n#> Selection Algorithm: 'sequential replacement'\n#>          x1  x2  x3  x4 \n#> 1  ( 1 ) \"*\" \" \" \" \" \" \"\n#> 2  ( 1 ) \"*\" \"*\" \" \" \" \"\n#> 3  ( 1 ) \"*\" \"*\" \"*\" \" \"\n#> 4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n# Visualize best subsets\nplot(best_subsets, scale = \"adjr2\")\ntitle(\"Best Subsets: AdjusteR^2\")"},{"path":"variable-selection.html","id":"recursive-feature-elimination","chapter":"15 Variable Selection","heading":"15.2.4 Recursive Feature Elimination","text":"Recursive Feature Elimination (RFE) feature selection method systematically removes predictors model identify relevant subset. RFE commonly used machine learning regression tasks improve model interpretability performance eliminating irrelevant redundant features.Theoretical FoundationObjective:\nSelect subset predictors maximizes performance model (e.g., minimizes prediction error maximizes explained variance).\nSelect subset predictors maximizes performance model (e.g., minimizes prediction error maximizes explained variance).Approach:\nRFE backward selection method recursively removes least important predictors based contribution model’s performance.\nRFE backward selection method recursively removes least important predictors based contribution model’s performance.Key Features:\nFeature Ranking: Predictors ranked based importance (e.g., weights linear model, coefficients regression, feature importance scores tree-based models).\nRecursive Elimination: step, least important predictor removed, model refit remaining predictors.\nFeature Ranking: Predictors ranked based importance (e.g., weights linear model, coefficients regression, feature importance scores tree-based models).Recursive Elimination: step, least important predictor removed, model refit remaining predictors.Evaluation Criterion:\nModel performance evaluated using metrics \\(R^2\\), AIC, BIC, cross-validation scores.\nModel performance evaluated using metrics \\(R^2\\), AIC, BIC, cross-validation scores.Steps RFEInitialize:\nStart full set predictors.\nStart full set predictors.Rank Features:\nTrain model compute feature importance scores (e.g., coefficients, weights, feature importance).\nTrain model compute feature importance scores (e.g., coefficients, weights, feature importance).Eliminate Features:\nRemove least important feature(s) based ranking.\nRemove least important feature(s) based ranking.Refit Model:\nRefit model reduced set predictors.\nRefit model reduced set predictors.Repeat:\nContinue process desired number features reached.\nContinue process desired number features reached.InterpretationSelected Predictors:\nalgorithm identifies relevant predictors based impact model performance.\nSelected Predictors:algorithm identifies relevant predictors based impact model performance.Model Performance:\ncross-validation results indicate model performs different subset sizes, helping select optimal number features.\nModel Performance:cross-validation results indicate model performs different subset sizes, helping select optimal number features.Feature Ranking:\nRFE ranks features importance, providing insights predictors influential.\nFeature Ranking:RFE ranks features importance, providing insights predictors influential.Advantages:Improved Interpretability: Reduces number predictors, simplifying model.Improved Interpretability: Reduces number predictors, simplifying model.Performance Optimization: Eliminates irrelevant redundant features, improving predictive accuracy.Performance Optimization: Eliminates irrelevant redundant features, improving predictive accuracy.Flexibility: Works various model types (e.g., linear models, tree-based methods, SVMs).Flexibility: Works various model types (e.g., linear models, tree-based methods, SVMs).Limitations:Computationally Intensive: Repeatedly trains models, can slow large datasets complex models.Computationally Intensive: Repeatedly trains models, can slow large datasets complex models.Model Dependency: Feature importance rankings depend underlying model, may introduce bias.Model Dependency: Feature importance rankings depend underlying model, may introduce bias.Practical ConsiderationsWhen use RFE?\nhigh-dimensional datasets feature selection critical model interpretability performance.\nmachine learning workflows feature importance scores available.\nuse RFE?high-dimensional datasets feature selection critical model interpretability performance.high-dimensional datasets feature selection critical model interpretability performance.machine learning workflows feature importance scores available.machine learning workflows feature importance scores available.Extensions:\nCombine RFE regularization methods (e.g., LASSO, Ridge) additional feature selection.\nUse advanced models (e.g., Random Forest, Gradient Boosting) feature ranking.\nExtensions:Combine RFE regularization methods (e.g., LASSO, Ridge) additional feature selection.Combine RFE regularization methods (e.g., LASSO, Ridge) additional feature selection.Use advanced models (e.g., Random Forest, Gradient Boosting) feature ranking.Use advanced models (e.g., Random Forest, Gradient Boosting) feature ranking.","code":"\n# Install and load caret package\nif (!requireNamespace(\"caret\", quietly = TRUE)) install.packages(\"caret\")\nlibrary(caret)\n\n# Simulated data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)\ny <- 5 + rowSums(X[, 1:3]) + rnorm(n, sd = 2)  # Only first 3 variables are relevant\n\n# Convert to a data frame\ndata <- as.data.frame(X)\ndata$y <- y\n\n# Define control for RFE\ncontrol <- rfeControl(functions = lmFuncs, # Use linear model for evaluation\n                      method = \"cv\",      # Use cross-validation\n                      number = 10)        # Number of folds\n\n# Perform RFE\nset.seed(123)\nrfe_result <- rfe(data[, -ncol(data)], data$y, \n                  sizes = c(1:10),          # Subset sizes to evaluate\n                  rfeControl = control)\n\n# Display RFE results\ncat(\"Selected Predictors:\\n\")\n#> Selected Predictors:\nprint(predictors(rfe_result))\n#> [1] \"V1\" \"V2\" \"V3\"\n\ncat(\"\\nModel Performance:\\n\")\n#> \n#> Model Performance:\nprint(rfe_result$results)\n#>    Variables     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n#> 1          1 2.342391 0.2118951 1.840089 0.5798450  0.2043123 0.4254393\n#> 2          2 2.146397 0.3549917 1.726721 0.5579553  0.2434206 0.4490311\n#> 3          3 2.063923 0.4209751 1.662022 0.5362445  0.2425727 0.4217594\n#> 4          4 2.124035 0.3574168 1.698954 0.5469199  0.2261644 0.4212677\n#> 5          5 2.128605 0.3526426 1.684169 0.5375931  0.2469891 0.4163273\n#> 6          6 2.153916 0.3226917 1.712140 0.5036119  0.2236060 0.4058412\n#> 7          7 2.162787 0.3223240 1.716382 0.5089243  0.2347677 0.3940646\n#> 8          8 2.152186 0.3222999 1.698816 0.5064040  0.2419215 0.3826101\n#> 9          9 2.137741 0.3288444 1.687136 0.5016526  0.2436893 0.3684675\n#> 10        10 2.139102 0.3290236 1.684362 0.5127830  0.2457675 0.3715176\n\n# Plot performance\nplot(rfe_result, type = \"l\")"},{"path":"variable-selection.html","id":"embedded-methods-integrated-into-model-training","chapter":"15 Variable Selection","heading":"15.3 Embedded Methods (Integrated into Model Training)","text":"","code":""},{"path":"variable-selection.html","id":"regularization-based-selection","chapter":"15 Variable Selection","heading":"15.3.1 Regularization-Based Selection","text":"Lasso (L1 Regularization): Shrinks coefficients zero, performing automatic selection.Ridge (L2 Regularization): Shrinks coefficients eliminate variables.Elastic Net: Combines L1 L2 penalties better feature selection.","code":""},{"path":"variable-selection.html","id":"tree-based-feature-importance","chapter":"15 Variable Selection","heading":"15.3.2 Tree-Based Feature Importance","text":"Decision trees ensemble methods (Random Forests, Gradient Boosting) rank features based contribution predictions.","code":""},{"path":"variable-selection.html","id":"genetic-algorithms-1","chapter":"15 Variable Selection","heading":"15.3.3 Genetic Algorithms","text":"Genetic Algorithms (GA) inspired principles natural selection genetics. metaheuristic optimization techniques iteratively evolve population solutions find optimal near-optimal subset predictors regression classification tasks.Theoretical FoundationObjective:\nSelect subset predictors optimizes predefined fitness function (e.g., \\(R^2\\), AIC, BIC, prediction error).\nSelect subset predictors optimizes predefined fitness function (e.g., \\(R^2\\), AIC, BIC, prediction error).Key Concepts:\nPopulation: collection candidate solutions (subsets predictors).\nChromosome: Represents solution binary vector (e.g., 1 variable included, 0 otherwise).\nFitness Function: Evaluates quality solution based selected variables.\nCrossover: Combines two solutions create new solution.\nMutation: Randomly alters solution maintain diversity population.\nSelection: Chooses solutions higher fitness create next generation.\nPopulation: collection candidate solutions (subsets predictors).Chromosome: Represents solution binary vector (e.g., 1 variable included, 0 otherwise).Fitness Function: Evaluates quality solution based selected variables.Crossover: Combines two solutions create new solution.Mutation: Randomly alters solution maintain diversity population.Selection: Chooses solutions higher fitness create next generation.Advantages:\nExplores wide solution space effectively.\nEscapes local optima introducing randomness.\nExplores wide solution space effectively.Escapes local optima introducing randomness.Steps Genetic AlgorithmsInitialization:\nGenerate initial population candidate solutions randomly.\nGenerate initial population candidate solutions randomly.Evaluation:\nCompute fitness function solution population.\nCompute fitness function solution population.Selection:\nSelect solutions higher fitness values parents next generation.\nSelect solutions higher fitness values parents next generation.Crossover:\nCombine pairs parent solutions create offspring.\nCombine pairs parent solutions create offspring.Mutation:\nRandomly modify solutions introduce variability.\nRandomly modify solutions introduce variability.Replacement:\nReplace old population new generation.\nReplace old population new generation.Stopping Criteria:\nTerminate algorithm fixed number generations improvement fitness threshold.\nTerminate algorithm fixed number generations improvement fitness threshold.InterpretationSelected Variables:\ngenetic algorithm identifies subset predictors minimizes fitness function (negative AIC example).\nSelected Variables:genetic algorithm identifies subset predictors minimizes fitness function (negative AIC example).Model Performance:\nfinal model can evaluated using metrics adjusted $R^2$, prediction error, cross-validation.\nModel Performance:final model can evaluated using metrics adjusted $R^2$, prediction error, cross-validation.Convergence:\nalgorithm evolves towards better solutions generations, indicated improvements best fitness value.\nConvergence:algorithm evolves towards better solutions generations, indicated improvements best fitness value.Advantages:Can handle high-dimensional datasets complex fitness functions.Can handle high-dimensional datasets complex fitness functions.Avoids getting trapped local optima introducing randomness.Avoids getting trapped local optima introducing randomness.Flexible: Can optimize user-defined fitness function.Flexible: Can optimize user-defined fitness function.Limitations:Computationally intensive large datasets many predictors.Computationally intensive large datasets many predictors.Requires careful tuning hyperparameters (e.g., population size, mutation rate).Requires careful tuning hyperparameters (e.g., population size, mutation rate).May guarantee globally optimal solution.May guarantee globally optimal solution.Practical ConsiderationsWhen use Genetic Algorithms?\ncomplex variable selection problems traditional methods (e.g., stepwise selection) insufficient.\nfitness function non-linear involves interactions among predictors.\nuse Genetic Algorithms?complex variable selection problems traditional methods (e.g., stepwise selection) insufficient.complex variable selection problems traditional methods (e.g., stepwise selection) insufficient.fitness function non-linear involves interactions among predictors.fitness function non-linear involves interactions among predictors.Tuning Tips:\nAdjust population size mutation rate based dataset size complexity.\nUse parallel computing speed evaluation fitness functions.\nTuning Tips:Adjust population size mutation rate based dataset size complexity.Adjust population size mutation rate based dataset size complexity.Use parallel computing speed evaluation fitness functions.Use parallel computing speed evaluation fitness functions.","code":"\n# Install and load GA package\nif (!requireNamespace(\"GA\", quietly = TRUE))\n    install.packages(\"GA\")\nlibrary(GA)\n\n\n# Simulated data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Only first 3 variables are relevant\ny <-\n    5 + rowSums(X[, 1:3]) + rnorm(n, sd = 2) \n\n# Convert to a data frame\ndata <- as.data.frame(X)\ndata$y <- y\n\n# Define the fitness function\nfitness_function <- function(binary_vector) {\n    selected_vars <- which(binary_vector == 1)\n    if (length(selected_vars) == 0)\n        return(-Inf) # Penalize empty subsets\n    model <- lm(y ~ ., data = data[, c(selected_vars, ncol(data))])\n    - AIC(model)  # Return negative AIC (minimization problem)\n}\n\n# Run Genetic Algorithm\nset.seed(123)\nga_result <-\n    ga(\n        type = \"binary\",\n        # Binary encoding for variable selection\n        fitness = fitness_function,\n        nBits = p,\n        # Number of predictors\n        popSize = 50,\n        # Population size\n        maxiter = 100,\n        # Maximum number of generations\n        run = 10,\n        # Stop if no improvement in 10 generations\n        seed = 123\n    )\n\n# Extract the best solution\nbest_solution <- ga_result@solution[1, ]\nselected_vars <- which(best_solution == 1)\ncat(\"Selected Variables (Column Indices):\\n\", selected_vars, \"\\n\")\n#> Selected Variables (Column Indices):\n#>  1 2 3 4\n\n# Fit the final model with selected variables\nfinal_model <-\n    lm(y ~ ., data = data[, c(selected_vars, ncol(data))])\ncat(\"Final Model Summary:\\n\")\n#> Final Model Summary:\nprint(summary(final_model))\n#> \n#> Call:\n#> lm(formula = y ~ ., data = data[, c(selected_vars, ncol(data))])\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.4013 -1.3823  0.0151  1.0796  5.1537 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   5.2726     0.2101  25.096  < 2e-16 ***\n#> V1            1.1477     0.2290   5.012 2.49e-06 ***\n#> V2            0.9469     0.2144   4.416 2.66e-05 ***\n#> V3            0.6864     0.2199   3.121  0.00239 ** \n#> V4            0.3881     0.1997   1.943  0.05496 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.058 on 95 degrees of freedom\n#> Multiple R-squared:  0.3574, Adjusted R-squared:  0.3304 \n#> F-statistic: 13.21 on 4 and 95 DF,  p-value: 1.352e-08"},{"path":"variable-selection.html","id":"summary-table-1","chapter":"15 Variable Selection","heading":"15.4 Summary Table","text":"","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"16 Hypothesis Testing","heading":"16 Hypothesis Testing","text":"Hypothesis testing one cornerstones statistical inference, used widely across disciplines economics, finance, psychology, . Researchers employ hypothesis testing draw conclusions population parameters based sample data. Central process concept p-value, helps quantify unlikely observed data (extreme data) null hypothesis true.However, data collection become easier cheaper—especially age big data—growing awareness large sample sizes (large \\(n\\)) can inflate likelihood finding statistically significant, practically negligible, effects. Moreover, can lead “p-value hacking,” researchers run numerous tests adopt flexible analytical approaches find (sometimes minuscule) effect achieves conventional significance level (often \\(p < .05\\)).","code":""},{"path":"hypothesis-testing.html","id":"sec-null-hypothesis-significance-testing","chapter":"16 Hypothesis Testing","heading":"16.1 Null Hypothesis Significance Testing","text":"Null Hypothesis Significance Testing (NHST) foundation statistical inference. provides structured approach evaluating whether observed data provides sufficient evidence reject null hypothesis (\\(H_0\\)) favor alternative hypothesis (\\(H_a\\)).NHST follows key steps:Define Hypotheses\nnull hypothesis (\\(H_0\\)) represents default assumption (e.g., effect, difference).\nalternative hypothesis (\\(H_a\\)) represents competing claim (e.g., nonzero effect, relationship variables).\nnull hypothesis (\\(H_0\\)) represents default assumption (e.g., effect, difference).alternative hypothesis (\\(H_a\\)) represents competing claim (e.g., nonzero effect, relationship variables).Select Test Statistic\ntest statistic (e.g., \\(T\\), \\(W\\), \\(F\\)) quantifies evidence \\(H_0\\).\nfollows known distribution \\(H_0\\) (e.g., normal, chi-square, F-distribution).\ntest statistic (e.g., \\(T\\), \\(W\\), \\(F\\)) quantifies evidence \\(H_0\\).follows known distribution \\(H_0\\) (e.g., normal, chi-square, F-distribution).Decision Rule & p-value\ntest statistic exceeds critical value p-value \\(\\alpha\\), reject \\(H_0\\).\nOtherwise, fail reject \\(H_0\\), meaning evidence insufficient rule .\ntest statistic exceeds critical value p-value \\(\\alpha\\), reject \\(H_0\\).Otherwise, fail reject \\(H_0\\), meaning evidence insufficient rule .","code":""},{"path":"hypothesis-testing.html","id":"error-types-in-hypothesis-testing","chapter":"16 Hypothesis Testing","heading":"16.1.1 Error Types in Hypothesis Testing","text":"hypothesis testing, may incorrectly reject fail reject null hypothesis, leading two types errors:Type Error (False Positive):\nRejecting \\(H_0\\) actually true.\nExample: Concluding effect exists .\nRejecting \\(H_0\\) actually true.Example: Concluding effect exists .Type II Error (False Negative):\nFailing reject \\(H_0\\) actually false.\nExample: Missing real effect test lacked power.\nFailing reject \\(H_0\\) actually false.Example: Missing real effect test lacked power.power test probability correctly rejecting \\(H_0\\) false:\\[\n\\text{Power} = 1 - P(\\text{Type II Error})\n\\]higher power (typically \\(\\geq 0.8\\)) reduces Type II errors increases likelihood detecting true effects.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing-framework-1","chapter":"16 Hypothesis Testing","heading":"16.1.2 Hypothesis Testing Framework","text":"Hypothesis tests can two-sided one-sided, depending research question.","code":""},{"path":"hypothesis-testing.html","id":"two-sided-test","chapter":"16 Hypothesis Testing","heading":"16.1.2.1 Two-Sided Test","text":"two-sided test, examine whether parameter significantly different hypothesized value (usually zero):\\[ \\begin{aligned} &H_0: \\beta_j = 0 \\\\ &H_1: \\beta_j \\neq 0  \\end{aligned} \\]null hypothesis, assuming standard ordinary least squares assumptions (A1-A3a, A5), asymptotic distribution OLS estimator :\\[ \\sqrt{n} \\hat{\\beta_j} \\sim N(0, \\text{Avar}(\\sqrt{n} \\hat{\\beta}_j)) \\]\\(\\text{Avar}(\\cdot)\\) denotes asymptotic variance.","code":""},{"path":"hypothesis-testing.html","id":"one-sided-test","chapter":"16 Hypothesis Testing","heading":"16.1.2.2 One-Sided Test","text":"one-sided hypothesis test, null hypothesis includes range values, test directional alternative:\\[ \\begin{aligned} &H_0: \\beta_j \\geq 0 \\\\ &H_1: \\beta_j < 0 \\end{aligned} \\]“hardest” null value reject \\(\\beta_j = 0\\). specific null, estimator follows asymptotic distribution:\\[ \\sqrt{n} \\hat{\\beta_j} \\sim N(0, \\text{Avar}(\\sqrt{n} \\hat{\\beta}_j)) \\]","code":""},{"path":"hypothesis-testing.html","id":"interpreting-hypothesis-testing-results","chapter":"16 Hypothesis Testing","heading":"16.1.3 Interpreting Hypothesis Testing Results","text":"conducting hypothesis tests, essential distinguish population parameters sample estimates:Hypotheses always written terms population parameter (\\(\\beta\\)), sample estimate (\\(\\hat{\\beta}\\)).disciplines use different notations:\n\\(\\beta\\): Standardized coefficient (useful comparing relative effects, scale-free).\n\\(\\mathbf{b}\\): Unstandardized coefficient (interpretable practical applications, e.g., policy decisions).\n\\(\\beta\\): Standardized coefficient (useful comparing relative effects, scale-free).\\(\\mathbf{b}\\): Unstandardized coefficient (interpretable practical applications, e.g., policy decisions).relationship coefficients :\\[\n\\beta_j = \\mathbf{b}_j \\frac{s_{x_j}}{s_y}\n\\]\\(s_{x_j}\\) \\(s_y\\) standard deviations independent dependent variables.","code":""},{"path":"hypothesis-testing.html","id":"understanding-p-values","chapter":"16 Hypothesis Testing","heading":"16.1.4 Understanding p-Values","text":"p-value probability, assumption \\(H_0\\) true, observing test statistic least extreme one computed sample data. Formally,\\[\np\\text{-value} = P(\\text{Test Statistic} \\geq \\text{observed value} \\mid H_0 \\ \\text{true})\n\\]InterpretationA small p-value indicates \\(H_0\\) true, seeing observed data (something extreme) unlikely.small p-value indicates \\(H_0\\) true, seeing observed data (something extreme) unlikely.convention, \\(p < \\alpha\\) (often 0.05), result deemed “statistically significant,” reject \\(H_0\\).convention, \\(p < \\alpha\\) (often 0.05), result deemed “statistically significant,” reject \\(H_0\\).Important Caveat: “Statistically significant” “practically significant” “economically significant.” difference can statistically significant yet trivial magnitude, negligible real-world implications.Important Caveat: “Statistically significant” “practically significant” “economically significant.” difference can statistically significant yet trivial magnitude, negligible real-world implications.MisconceptionsThe p-value probability \\(H_0\\) true false.p-value probability \\(H_0\\) true false.p-value \\(0.05\\) prove “effect.” simply suggests data provide sufficient evidence (chosen significance level) reject \\(H_0\\).p-value \\(0.05\\) prove “effect.” simply suggests data provide sufficient evidence (chosen significance level) reject \\(H_0\\).p-value 0.05 prove effect “real” large. indicates data unusual enough \\(H_0\\) decide reject \\(H_0\\), given chosen threshold.p-value 0.05 prove effect “real” large. indicates data unusual enough \\(H_0\\) decide reject \\(H_0\\), given chosen threshold.","code":""},{"path":"hypothesis-testing.html","id":"the-role-of-sample-size","chapter":"16 Hypothesis Testing","heading":"16.1.5 The Role of Sample Size","text":"critical factor influencing outcome hypothesis tests sample size (\\(n\\)).Increasing Power Large \\(n\\)Statistical Power: probability correctly rejecting \\(H_0\\) \\(H_0\\) false. Large sample sizes increase statistical power, making easier detect even tiny deviations \\(H_0\\).Statistical Power: probability correctly rejecting \\(H_0\\) \\(H_0\\) false. Large sample sizes increase statistical power, making easier detect even tiny deviations \\(H_0\\).Implication: true effect size population small (e.g., 0.2% difference average returns two trading strategies), study large enough \\(n\\) might still find statistically significant (p-value < 0.05).Implication: true effect size population small (e.g., 0.2% difference average returns two trading strategies), study large enough \\(n\\) might still find statistically significant (p-value < 0.05).Tendency Toward -SensitivityAs \\(n\\) grows, standard errors decrease. Thus, even minuscule differences null hypothesis become less likely attributed random chance, yielding low p-values. can lead findings statistically significant negligible real-world impact.Example: Suppose economist testing policy intervention changes employment rates 0.1%. small sample size, test might detect difference. massive dataset, 0.1% difference might yield \\(p\\)-value < 0.05, even though 0.1% change may economically meaningful.","code":""},{"path":"hypothesis-testing.html","id":"p-value-hacking","chapter":"16 Hypothesis Testing","heading":"16.1.6 p-Value Hacking","text":"p-Hacking refers process manipulating data analysis statistically significant result (\\(p\\)-value < 0.05) achieved. can include:Running multiple tests dataset reporting yield significance.Running multiple tests dataset reporting yield significance.Stopping data collection significant p-value reached.Stopping data collection significant p-value reached.Trying various model specifications (e.g., adding removing control variables) one finds significant effect.Trying various model specifications (e.g., adding removing control variables) one finds significant effect.Selectively reporting outcomes (publication bias).Selectively reporting outcomes (publication bias).large datasets, “search space” potential analyses grows exponentially. researchers test many hypotheses sift wide range variables subgroups, can almost always find “significant” result chance alone.Multiple Comparison Problem: multiple tests conducted, chance finding least one “significant” result purely coincidence increases. instance, 20 independent tests \\(\\alpha = .05\\), 64% chance (\\(1 - 0.95^{20}\\)) incorrectly rejecting least one null hypothesis.","code":""},{"path":"hypothesis-testing.html","id":"practical-vs.-statistical-significance","chapter":"16 Hypothesis Testing","heading":"16.1.7 Practical vs. Statistical Significance","text":"economics finance, crucial distinguish results statistically significant economically meaningful. Economic financial significance asks: effect tangible importance policymakers, businesses, investors?result might show new trading algorithm yields returns statistically different zero, difference 0.0001% average, might profitable accounting transaction fees, taxes, frictions—hence lacking economic significance.","code":""},{"path":"hypothesis-testing.html","id":"mitigating-the-misuse-of-p-values","chapter":"16 Hypothesis Testing","heading":"16.1.8 Mitigating the Misuse of p-Values","text":"","code":""},{"path":"hypothesis-testing.html","id":"pre-registration-and-replication","chapter":"16 Hypothesis Testing","heading":"16.1.8.1 Pre-Registration and Replication","text":"Pre-Registration: Researchers specify hypotheses analytical methods seeing data, reducing temptation p-hack.Pre-Registration: Researchers specify hypotheses analytical methods seeing data, reducing temptation p-hack.Replication: Independent replication studies help confirm whether result robust merely fluke.Replication: Independent replication studies help confirm whether result robust merely fluke.","code":""},{"path":"hypothesis-testing.html","id":"using-alternatives-to-or-supplements-for-p-values","chapter":"16 Hypothesis Testing","heading":"16.1.8.2 Using Alternatives to (or Supplements for) p-Values","text":"Bayesian Methods: Provide posterior probabilities incorporate prior information, often giving nuanced understanding uncertainty.Bayesian Methods: Provide posterior probabilities incorporate prior information, often giving nuanced understanding uncertainty.Effect Size & Confidence Intervals: Shift focus “significant?” “large effect, plausible range?”Effect Size & Confidence Intervals: Shift focus “significant?” “large effect, plausible range?”Equivalence Testing: Sometimes goal show effect larger certain threshold. Equivalence tests can used conclude “clinically (economically) significant difference.”Equivalence Testing: Sometimes goal show effect larger certain threshold. Equivalence tests can used conclude “clinically (economically) significant difference.”","code":""},{"path":"hypothesis-testing.html","id":"adjusting-for-multiple-comparisons","chapter":"16 Hypothesis Testing","heading":"16.1.8.3 Adjusting for Multiple Comparisons","text":"Bonferroni Correction: Requires using stringent significance threshold multiple tests performed (e.g., \\(\\alpha/m\\) \\(m\\) tests).Bonferroni Correction: Requires using stringent significance threshold multiple tests performed (e.g., \\(\\alpha/m\\) \\(m\\) tests).False Discovery Rate Control: Allows flexible approach, controlling expected proportion false positives among significant findings.False Discovery Rate Control: Allows flexible approach, controlling expected proportion false positives among significant findings.","code":""},{"path":"hypothesis-testing.html","id":"emphasizing-relevance-over-statistical-stars","chapter":"16 Hypothesis Testing","heading":"16.1.8.4 Emphasizing Relevance Over Statistical “Stars”","text":"Encourage journals, reviewers, academic circles stress magnitude effects robustness checks whether result crosses conventional p-value threshold (like 0.05).three commonly used methods hypothesis testing:Likelihood Ratio Test: Compares likelihood null alternative models. Often used nested models.Likelihood Ratio Test: Compares likelihood null alternative models. Often used nested models.Wald Test: Assesses whether estimated parameter significantly different hypothesized value. Requires one maximization (full model).Wald Test: Assesses whether estimated parameter significantly different hypothesized value. Requires one maximization (full model).Lagrange Multiplier (Score) Test: Evaluates slope likelihood function null hypothesis value. Performs well small moderate samples.Lagrange Multiplier (Score) Test: Evaluates slope likelihood function null hypothesis value. Performs well small moderate samples.","code":""},{"path":"hypothesis-testing.html","id":"sec-wald-test","chapter":"16 Hypothesis Testing","heading":"16.1.9 Wald Test","text":"Wald test assesses whether estimated parameters significantly different hypothesized values, based asymptotic distribution estimator.general form Wald statistic :\\[\n\\begin{aligned}\nW &= (\\hat{\\theta}-\\theta_0)'[cov(\\hat{\\theta})]^{-1}(\\hat{\\theta}-\\theta_0) \\\\\nW &\\sim \\chi_q^2\n\\end{aligned}\n\\]:\\(cov(\\hat{\\theta})\\) given inverse Fisher Information matrix evaluated \\(\\hat{\\theta}\\),\\(cov(\\hat{\\theta})\\) given inverse Fisher Information matrix evaluated \\(\\hat{\\theta}\\),\\(q\\) rank \\(cov(\\hat{\\theta})\\), corresponds number non-redundant parameters \\(\\theta\\).\\(q\\) rank \\(cov(\\hat{\\theta})\\), corresponds number non-redundant parameters \\(\\theta\\).Wald statistic can also expressed different ways:Quadratic form test statistic:\\[\nt_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{(\\theta_0)^{-1}} \\sim \\chi^2_{(v)}\n\\]\\(v\\) degree freedom.Standardized Wald test statistic:\\[\ns_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{(\\hat{\\theta})^{-1}}} \\sim Z\n\\]represents far sample estimate hypothesized population parameter.Significance Level Confidence LevelThe significance level (\\(\\alpha\\)) probability threshold reject null hypothesis.confidence level (\\(1-\\alpha\\)) determines range within population parameter expected fall given probability.standardize estimator null value, define test statistic OLS estimator:\\[\nT = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]Equivalently:\\[\nT = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]:\\(T\\) test statistic (function data null hypothesis),\\(T\\) test statistic (function data null hypothesis),\\(t\\) observed realization \\(T\\).\\(t\\) observed realization \\(T\\).","code":""},{"path":"hypothesis-testing.html","id":"evaluating-the-test-statistic","chapter":"16 Hypothesis Testing","heading":"16.1.9.1 Evaluating the Test Statistic","text":"three equivalent methods evaluating hypothesis tests:Critical Value MethodFor given significance level \\(\\alpha\\), determine critical value (\\(c\\)):One-sided test: \\(H_0: \\beta_j \\geq \\beta_{j0}\\)\\[\nP(T < c | H_0) = \\alpha\n\\]Reject \\(H_0\\) \\(t < c\\).One-sided test: \\(H_0: \\beta_j \\leq \\beta_{j0}\\)\\[\nP(T > c | H_0) = \\alpha\n\\]Reject \\(H_0\\) \\(t > c\\).Two-sided test: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\nP(|T| > c | H_0) = \\alpha\n\\]Reject \\(H_0\\) \\(|t| > c\\).p-value MethodThe p-value probability observing test statistic extreme one obtained, given null hypothesis true.One-sided test: \\(H_0: \\beta_j \\geq \\beta_{j0}\\)\\[\n\\text{p-value} = P(T < t | H_0)\n\\]One-sided test: \\(H_0: \\beta_j \\leq \\beta_{j0}\\)\\[\n\\text{p-value} = P(T > t | H_0)\n\\]Two-sided test: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\n\\text{p-value} = P(|T| > |t| | H_0)\n\\]Reject \\(H_0\\) \\(\\text{p-value} < \\alpha\\).Confidence Interval MethodUsing critical value associated given significance level, construct confidence interval:\\[\nCI(\\hat{\\beta}_j)_{\\alpha} = \\left[\\hat{\\beta}_j - c \\times SE(\\hat{\\beta}_j), \\hat{\\beta}_j + c \\times SE(\\hat{\\beta}_j)\\right]\n\\]Reject \\(H_0\\) hypothesized value falls outside confidence interval.testing whether true population value close estimate. Instead, testing:Given fixed true population value parameter, likely observed estimate?testing whether true population value close estimate. Instead, testing:Given fixed true population value parameter, likely observed estimate?can interpreted :believe \\((1-\\alpha)\\times 100 \\%\\) probability confidence interval captures true parameter value.can interpreted :believe \\((1-\\alpha)\\times 100 \\%\\) probability confidence interval captures true parameter value.Finite Sample PropertiesUnder stronger assumptions (A1-A6), can consider finite sample properties:\\[\nT = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k)\n\\]:derivation distribution depends strongly :\nA4 (Homoskedasticity)\nA5 (Data Generation via Random Sampling)\nderivation distribution depends strongly :A4 (Homoskedasticity)A4 (Homoskedasticity)A5 (Data Generation via Random Sampling)A5 (Data Generation via Random Sampling)\\(T\\)-statistic follows Student’s t-distribution :\nnumerator normally distributed.\n\\(T\\)-statistic follows Student’s t-distribution :numerator normally distributed.denominator follows \\(\\chi^2\\) distribution.denominator follows \\(\\chi^2\\) distribution.Critical values p-values computed using Student’s t-distribution instead standard normal distribution.Critical values p-values computed using Student’s t-distribution instead standard normal distribution.\\(n \\\\infty\\), \\(T(n-k)\\) distribution converges standard normal distribution.\\(n \\\\infty\\), \\(T(n-k)\\) distribution converges standard normal distribution.Rule ThumbIf \\(n-k > 120\\):\nt-distribution critical values p-values closely approximate standard normal distribution.\nt-distribution critical values p-values closely approximate standard normal distribution.\\(n-k < 120\\):\n(A1-A6) hold, t-test exact finite-sample test.\n(A1-A3a, A5) hold, t-distribution asymptotically normal.\nUsing t-distribution critical values valid asymptotic test.\ndiscrepancy critical values disappears \\(n \\\\infty\\).\n\n(A1-A6) hold, t-test exact finite-sample test.(A1-A3a, A5) hold, t-distribution asymptotically normal.\nUsing t-distribution critical values valid asymptotic test.\ndiscrepancy critical values disappears \\(n \\\\infty\\).\nUsing t-distribution critical values valid asymptotic test.discrepancy critical values disappears \\(n \\\\infty\\).","code":""},{"path":"hypothesis-testing.html","id":"multiple-hypothesis-testing","chapter":"16 Hypothesis Testing","heading":"16.1.9.2 Multiple Hypothesis Testing","text":"often need test multiple parameters simultaneously:Example 1: \\(H_0: \\beta_1 = 0\\) \\(\\beta_2 = 0\\)Example 2: \\(H_0: \\beta_1 = 1\\) \\(\\beta_2 = 0\\)Performing separate hypothesis tests individual parameters answer question joint significance.\nneed test accounts joint distributions rather evaluating two marginal distributions separately.Consider multiple regression model:\\[\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\epsilon\n\\]null hypothesis \\(H_0: \\beta_1 = 0\\) \\(\\beta_2 = 0\\) can rewritten matrix form :\\[\nH_0: \\mathbf{R} \\beta - \\mathbf{q} = 0\n\\]:\\(\\mathbf{R}\\) \\(m \\times k\\) matrix, :\n\\(m\\) = number restrictions.\n\\(k\\) = number parameters.\n\\(m\\) = number restrictions.\\(k\\) = number parameters.\\(\\mathbf{q}\\) \\(k \\times 1\\) vector contains null hypothesis values.example \\(H_0: \\beta_1 = 0\\) \\(\\beta_2 = 0\\), define:\\[\n\\mathbf{R} =\n\\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}, \\quad\n\\mathbf{q} =\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}\n\\]OLS estimator multiple hypotheses, use F-statistic:\\[\nF = \\frac{(\\mathbf{R\\hat{\\beta} - q})' \\hat{\\Sigma}^{-1} (\\mathbf{R\\hat{\\beta} - q})}{m} \\sim^F(m, n-k)\n\\]:\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrix.\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrix.\\(m\\) number restrictions.\\(m\\) number restrictions.\\(n-k\\) residual degrees freedom.\\(n-k\\) residual degrees freedom.Assumptions Variance EstimationIf A4 (Homoskedasticity) holds:\nhomoskedastic heteroskedastic variance estimators valid.\nhomoskedastic heteroskedastic variance estimators valid.A4 hold:\nheteroskedastic variance estimator remains valid.\nheteroskedastic variance estimator remains valid.Relationship F t-TestsWhen \\(m = 1\\) (one restriction), F-statistic simply squared t-statistic:\\[\nF = t^2\n\\]Since F-distribution strictly positive, one-sided definition.","code":""},{"path":"hypothesis-testing.html","id":"linear-combination-testing","chapter":"16 Hypothesis Testing","heading":"16.1.9.3 Linear Combination Testing","text":"testing multiple parameters simultaneously, often assess linear combinations parameters rather testing individually.example, consider following hypotheses:\\[\n\\begin{aligned}\nH_0 &: \\beta_1 - \\beta_2 = 0 \\\\\nH_0 &: \\beta_1 - \\beta_2 > 0 \\\\\nH_0 &: \\beta_1 - 2\\beta_2 = 0\n\\end{aligned}\n\\]represents single restriction function parameters.null hypothesis:\\[\nH_0: \\beta_1 - \\beta_2 = 0\n\\]can rewritten matrix form :\\[\nH_0: \\mathbf{R} \\beta - \\mathbf{q} = 0\n\\]:\\[\n\\mathbf{R} =\n\\begin{bmatrix}\n0 & 1 & -1 & 0 & 0\n\\end{bmatrix}, \\quad\n\\mathbf{q} =\n\\begin{bmatrix}\n0\n\\end{bmatrix}\n\\]Interpretation:\\(\\mathbf{R}\\) \\(1 \\times k\\) matrix selects relevant parameters hypothesis.\\(\\mathbf{q}\\) \\(k \\times 1\\) vector containing hypothesized values linear combination.formulation allows us use generalized Wald test assess whether constraint holds.Wald test statistic linear hypothesis:\\[\nW = \\frac{(\\mathbf{R} \\hat{\\beta} - \\mathbf{q})' \\left( \\mathbf{R} \\hat{\\Sigma} \\mathbf{R}' \\right)^{-1} (\\mathbf{R} \\hat{\\beta} - \\mathbf{q})}{s^2 q}\n\\sim F_{q, n-k}\n\\]:\\(\\hat{\\beta}\\) vector estimated coefficients.\\(\\hat{\\beta}\\) vector estimated coefficients.\\(\\hat{\\Sigma}\\) variance-covariance matrix \\(\\hat{\\beta}\\).\\(\\hat{\\Sigma}\\) variance-covariance matrix \\(\\hat{\\beta}\\).\\(s^2\\) estimated error variance.\\(s^2\\) estimated error variance.\\(q\\) number restrictions.\\(q\\) number restrictions.test follows F-distribution degrees freedom \\((q, n-k)\\).test follows F-distribution degrees freedom \\((q, n-k)\\).tests whether \\(\\beta_1 = \\beta_2\\) (.e., whether income education effect prestige).tests whether \\(\\beta_1 = \\beta_2\\) (.e., whether income education effect prestige).p-value low, reject null hypothesis conclude income education contribute differently prestige.p-value low, reject null hypothesis conclude income education contribute differently prestige.","code":"\nlibrary(car)\n\n# Fit a multiple regression model\nmod.duncan <- lm(prestige ~ income + education, data=Duncan)\n\n# Test whether income and education coefficients are equal\nlinearHypothesis(mod.duncan, \"1*income - 1*education = 0\")\n#> \n#> Linear hypothesis test:\n#> income - education = 0\n#> \n#> Model 1: restricted model\n#> Model 2: prestige ~ income + education\n#> \n#>   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n#> 1     43 7518.9                           \n#> 2     42 7506.7  1    12.195 0.0682 0.7952"},{"path":"hypothesis-testing.html","id":"estimating-the-difference-between-two-coefficients","chapter":"16 Hypothesis Testing","heading":"16.1.9.4 Estimating the Difference Between Two Coefficients","text":"cases, may interested comparing two regression coefficients directly rather evaluating separately. example, might want test:\\[\nH_0: \\beta_1 = \\beta_2\n\\]equivalent testing whether difference zero:\\[\nH_0: \\beta_1 - \\beta_2 = 0\n\\]Alternatively, can directly estimate difference two regression coefficients.demonstrate function using Duncan dataset {car} package:","code":"\ndifftest_lm <- function(x1, x2, model) {\n    # Compute coefficient difference\n    diffest <-\n        summary(model)$coef[x1, \"Estimate\"] - summary(model)$coef[x2, \"Estimate\"]\n    \n    # Compute variance of the difference\n    vardiff <- (summary(model)$coef[x1, \"Std. Error\"] ^ 2 +\n                    summary(model)$coef[x2, \"Std. Error\"] ^ 2) - (2 * vcov(model)[x1, x2])\n    \n    # Compute standard error of the difference\n    diffse <- sqrt(vardiff)\n    \n    # Compute t-statistic\n    tdiff <- diffest / diffse\n    \n    # Compute p-value (two-sided test)\n    ptdiff <- 2 * (1 - pt(abs(tdiff), model$df.residual))\n    \n    # Compute confidence interval\n    upr <- diffest + qt(0.975, df = model$df.residual) * diffse\n    lwr <- diffest - qt(0.975, df = model$df.residual) * diffse\n    \n    # Return results as a named list\n    return(\n        list(\n            estimate = round(diffest, 2),\n            t_stat = round(tdiff, 2),\n            p_value = round(ptdiff, 4),\n            lower_CI = round(lwr, 2),\n            upper_CI = round(upr, 2),\n            df = model$df.residual\n        )\n    )\n}\nlibrary(car)\n\n# Load Duncan dataset\ndata(Duncan)\n\n# Fit a linear regression model\nmod.duncan <- lm(prestige ~ income + education, data = Duncan)\n\n# Compare the effects of income and education\ndifftest_lm(\"income\", \"education\", mod.duncan)\n#> $estimate\n#> [1] 0.05\n#> \n#> $t_stat\n#> [1] 0.26\n#> \n#> $p_value\n#> [1] 0.7952\n#> \n#> $lower_CI\n#> [1] -0.36\n#> \n#> $upper_CI\n#> [1] 0.46\n#> \n#> $df\n#> [1] 42"},{"path":"hypothesis-testing.html","id":"nonlinear-hypothesis-testing","chapter":"16 Hypothesis Testing","heading":"16.1.9.5 Nonlinear Hypothesis Testing","text":"many applications, may need test nonlinear restrictions parameters. can expressed set \\(q\\) nonlinear functions:\\[\n\\mathbf{h}(\\theta) = \\{ h_1 (\\theta), ..., h_q (\\theta)\\}'\n\\]\\(h_j(\\theta)\\) nonlinear function parameter vector \\(\\theta\\).approximate nonlinear restrictions, use Jacobian matrix, denoted \\(\\mathbf{H}(\\theta)\\), contains first-order partial derivatives \\(\\mathbf{h}(\\theta)\\) respect parameters:\\[\n\\mathbf{H}_{q \\times p}(\\theta) =\n\\begin{bmatrix}\n\\frac{\\partial h_1(\\theta)}{\\partial \\theta_1} & \\dots & \\frac{\\partial h_1(\\theta)}{\\partial \\theta_p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial h_q(\\theta)}{\\partial \\theta_1} & \\dots & \\frac{\\partial h_q(\\theta)}{\\partial \\theta_p}\n\\end{bmatrix}\n\\]:\\(q\\) number nonlinear restrictions,\\(q\\) number nonlinear restrictions,\\(p\\) number estimated parameters.\\(p\\) number estimated parameters.Jacobian matrix linearizes nonlinear restrictions allows approximation hypothesis test using Wald statistic.test null hypothesis:\\[\nH_0: \\mathbf{h} (\\theta) = 0\n\\]two-sided alternative using Wald statistic:\\[\nW = \\frac{\\mathbf{h(\\hat{\\theta})}' \\left\\{ \\mathbf{H}(\\hat{\\theta}) \\left[ \\mathbf{F}(\\hat{\\theta})' \\mathbf{F}(\\hat{\\theta}) \\right]^{-1} \\mathbf{H}(\\hat{\\theta})' \\right\\}^{-1} \\mathbf{h}(\\hat{\\theta})}{s^2 q} \\sim F_{q, n-p}\n\\]:\\(\\hat{\\theta}\\) estimated parameter vector,\\(\\hat{\\theta}\\) estimated parameter vector,\\(\\mathbf{H}(\\hat{\\theta})\\) Jacobian matrix evaluated \\(\\hat{\\theta}\\),\\(\\mathbf{H}(\\hat{\\theta})\\) Jacobian matrix evaluated \\(\\hat{\\theta}\\),\\(\\mathbf{F}(\\hat{\\theta})\\) Fisher Information Matrix,\\(\\mathbf{F}(\\hat{\\theta})\\) Fisher Information Matrix,\\(s^2\\) estimated error variance,\\(s^2\\) estimated error variance,\\(q\\) number restrictions,\\(q\\) number restrictions,\\(n\\) sample size,\\(n\\) sample size,\\(p\\) number parameters.\\(p\\) number parameters.test statistic follows F-distribution degrees freedom \\((q, n - p)\\).Wald statistic large, reject \\(H_0\\) conclude nonlinear restriction hold.Wald statistic large, reject \\(H_0\\) conclude nonlinear restriction hold.p-value provides probability observing extreme test statistic null hypothesis.p-value provides probability observing extreme test statistic null hypothesis.F-distribution accounts fact multiple nonlinear restrictions tested.F-distribution accounts fact multiple nonlinear restrictions tested.","code":"\nlibrary(car)\nlibrary(nlWaldTest)\n\n# Load example data\ndata(Duncan)\n\n# Fit a multiple regression model\nmod.duncan <- lm(prestige ~ income + education, data = Duncan)\n\n# Define a nonlinear hypothesis: income squared equals education\nnl_hypothesis <- \"b[2]^2 - b[3] = 0\"\n\n# Conduct the nonlinear Wald test\nnlWaldtest(mod.duncan, texts = nl_hypothesis)\n#> \n#>  Wald Chi-square test of a restriction on model parameters\n#> \n#> data:  mod.duncan\n#> Chisq = 0.69385, df = 1, p-value = 0.4049"},{"path":"hypothesis-testing.html","id":"sec-likelihood-ratio-test","chapter":"16 Hypothesis Testing","heading":"16.1.10 Likelihood Ratio Test","text":"Likelihood Ratio Test (LRT) general method comparing two nested models:reduced model null hypothesis (\\(H_0\\)), imposes constraints parameters.full model, allows flexibility alternative hypothesis (\\(H_a\\)).test evaluates much likely data full model compared restricted model.likelihood ratio test statistic given :\\[\nt_{LR} = 2[l(\\hat{\\theta}) - l(\\theta_0)] \\sim \\chi^2_v\n\\]:\\(l(\\hat{\\theta})\\) log-likelihood evaluated estimated parameter \\(\\hat{\\theta}\\) (full model),\\(l(\\hat{\\theta})\\) log-likelihood evaluated estimated parameter \\(\\hat{\\theta}\\) (full model),\\(l(\\theta_0)\\) log-likelihood evaluated hypothesized parameter \\(\\theta_0\\) (reduced model),\\(l(\\theta_0)\\) log-likelihood evaluated hypothesized parameter \\(\\theta_0\\) (reduced model),\\(v\\) degrees freedom (difference number parameters full reduced models).\\(v\\) degrees freedom (difference number parameters full reduced models).test compares height log-likelihood sample estimate versus hypothesized population parameter.test also considers ratio two maximized likelihoods:\\[\n\\begin{aligned}\nL_r &= \\text{maximized likelihood } H_0 \\text{ (reduced model)} \\\\\nL_f &= \\text{maximized likelihood } H_0 \\cup H_a \\text{ (full model)}\n\\end{aligned}\n\\], likelihood ratio defined :\\[\n\\Lambda = \\frac{L_r}{L_f}\n\\]:\\(\\Lambda\\) exceed 1, \\(L_f\\) (likelihood full model) always least large \\(L_r\\).likelihood ratio test statistic :\\[\n\\begin{aligned}\n-2 \\ln(\\Lambda) &= -2 \\ln \\left( \\frac{L_r}{L_f} \\right) = -2 (l_r - l_f) \\\\\n\\lim_{n \\\\infty}(-2 \\ln(\\Lambda)) &\\sim \\chi^2_v\n\\end{aligned}\n\\]:\\(v\\) difference number parameters full reduced models.likelihood ratio small (.e., \\(L_r\\) much smaller \\(L_f\\)), :test statistic exceeds critical value \\(\\chi^2_v\\) distribution.reject reduced model accept full model \\(\\alpha \\times 100\\%\\) significance level.p-value small, reduced model significantly worse, reject \\(H_0\\).p-value small, reduced model significantly worse, reject \\(H_0\\).large test statistic indicates removing predictor leads substantial drop model fit.large test statistic indicates removing predictor leads substantial drop model fit.","code":"\nlibrary(lmtest)\n\n# Load example dataset\ndata(mtcars)\n\n# Fit a full model with two predictors\nfull_model <- lm(mpg ~ hp + wt, data = mtcars)\n\n# Fit a reduced model with only one predictor\nreduced_model <- lm(mpg ~ hp, data = mtcars)\n\n# Perform the likelihood ratio test\nlrtest(reduced_model, full_model)\n#> Likelihood ratio test\n#> \n#> Model 1: mpg ~ hp\n#> Model 2: mpg ~ hp + wt\n#>   #Df  LogLik Df  Chisq Pr(>Chisq)    \n#> 1   3 -87.619                         \n#> 2   4 -74.326  1 26.586   2.52e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"hypothesis-testing.html","id":"lagrange-multiplier-score","chapter":"16 Hypothesis Testing","heading":"16.1.11 Lagrange Multiplier (Score) Test","text":"Lagrange Multiplier (LM) Test, also known Score Test, evaluates whether restricted model (\\(H_0\\)) significantly underperforms compared unrestricted model (\\(H_a\\)) without estimating full model.Unlike Likelihood Ratio Test, requires estimating models, LM test requires estimation restricted model (\\(H_0\\)).LM test statistic based first derivative (score function) log-likelihood function, evaluated parameter estimate null hypothesis (\\(\\theta_0\\)):\\[\nt_S = \\frac{S(\\theta_0)^2}{(\\theta_0)} \\sim \\chi^2_v\n\\]:\\(S(\\theta_0) = \\frac{\\partial l(\\theta)}{\\partial \\theta} \\bigg|_{\\theta=\\theta_0}\\) score function, .e., first derivative log-likelihood function evaluated \\(\\theta_0\\).\\(S(\\theta_0) = \\frac{\\partial l(\\theta)}{\\partial \\theta} \\bigg|_{\\theta=\\theta_0}\\) score function, .e., first derivative log-likelihood function evaluated \\(\\theta_0\\).\\((\\theta_0)\\) Fisher Information Matrix, quantifies curvature (second derivative) log-likelihood.\\((\\theta_0)\\) Fisher Information Matrix, quantifies curvature (second derivative) log-likelihood.\\(v\\) degrees freedom, equal number constraints imposed \\(H_0\\).\\(v\\) degrees freedom, equal number constraints imposed \\(H_0\\).test compares:slope log-likelihood function \\(\\theta_0\\) (flat \\(H_0\\)).slope log-likelihood function \\(\\theta_0\\) (flat \\(H_0\\)).curvature log-likelihood function (captured \\((\\theta_0)\\)).curvature log-likelihood function (captured \\((\\theta_0)\\)).Interpretation LM TestIf \\(t_S\\) large, slope log-likelihood function \\(\\theta_0\\) steep, indicating model fit improves significantly moving away \\(\\theta_0\\).\\(t_S\\) small, log-likelihood function remains nearly flat \\(\\theta_0\\), meaning additional parameters unrestricted model substantially improve fit.score function \\(S(\\theta_0)\\) significantly different zero, reject \\(H_0\\) suggests likelihood function increasing, implying better model fit moving away \\(\\theta_0\\).bptest: function lmtest package performs Breusch-Pagan test, Lagrange Multiplier test heteroscedasticity.bptest: function lmtest package performs Breusch-Pagan test, Lagrange Multiplier test heteroscedasticity.Null Hypothesis: null hypothesis variance residuals constant (homoscedasticity).Null Hypothesis: null hypothesis variance residuals constant (homoscedasticity).Alternative Hypothesis: alternative hypothesis variance residuals constant (heteroscedasticity).Alternative Hypothesis: alternative hypothesis variance residuals constant (heteroscedasticity).","code":"\n# Load necessary libraries\nlibrary(lmtest)  # For the Lagrange Multiplier test\nlibrary(car)     # For example data\n\n# Load example data\ndata(Prestige)\n\n# Fit a linear regression model\nmodel <- lm(prestige ~ income + education, data = Prestige)\n\n# Perform the Lagrange Multiplier test for heteroscedasticity\n# Using the Breusch-Pagan test (a type of LM test)\nlm_test <- bptest(model)\n\n# Print the results\nprint(lm_test)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  model\n#> BP = 4.1838, df = 2, p-value = 0.1235"},{"path":"hypothesis-testing.html","id":"comparing-hypothesis-tests","chapter":"16 Hypothesis Testing","heading":"16.1.12 Comparing Hypothesis Tests","text":"visual comparison hypothesis tests shown :test approaches hypothesis evaluation differently:Likelihood Ratio Test: Compares heights log-likelihood \\(\\hat{\\theta}\\) (full model) vs. \\(\\theta_0\\) (restricted model).Wald Test: Measures distance \\(\\hat{\\theta}\\) \\(\\theta_0\\).Lagrange Multiplier Test: Examines slope log-likelihood \\(\\theta_0\\) check movement towards \\(\\hat{\\theta}\\) significantly improves fit.Likelihood Ratio Test Lagrange Multiplier Test perform well small moderate samples, Wald Test computationally simpler requires one model estimation.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Generate data for a normal likelihood function\ntheta <- seq(-3, 3, length.out = 200)  # Theta values\n\n# Likelihood function with theta_hat = 1\nlikelihood <-\n    dnorm(theta, mean = 1, sd = 1)  \ndf <- data.frame(theta, likelihood)\n\n# Define key points\ntheta_0   <- 0  # Null hypothesis value\ntheta_hat <- 1  # Estimated parameter (full model)\nlikelihood_0 <-\n    dnorm(theta_0, mean = 1, sd = 1)  # Likelihood at theta_0\nlikelihood_hat <-\n    dnorm(theta_hat, mean = 1, sd = 1)  # Likelihood at theta_hat\n\n# Plot likelihood function\nggplot(df, aes(x = theta, y = likelihood)) +\n    geom_line(color = \"blue\", linewidth = 1.2) +  # Likelihood curve\n    \n    # Vertical lines for theta_0 and theta_hat\n    geom_vline(\n        xintercept = theta_0,\n        linetype = \"dashed\",\n        color = \"black\",\n        linewidth = 1\n    ) +\n    geom_vline(\n        xintercept = theta_hat,\n        linetype = \"dashed\",\n        color = \"red\",\n        linewidth = 1\n    ) +\n    \n    # Labels for theta_0 and theta_hat\n    annotate(\n        \"text\",\n        x = theta_0 - 0.1,\n        y = -0.02,\n        label = expression(theta[0]),\n        color = \"black\",\n        size = 5,\n        fontface = \"bold\"\n    ) +\n    annotate(\n        \"text\",\n        x = theta_hat + 0.1,\n        y = -0.02,\n        label = expression(hat(theta)),\n        color = \"red\",\n        size = 5,\n        fontface = \"bold\"\n    ) +\n    \n    # LRT: Compare heights of likelihood at theta_0 and theta_hat\n    annotate(\n        \"segment\",\n        x = theta_0,\n        xend = theta_0,\n        y = likelihood_0,\n        yend = likelihood_hat,\n        color = \"purple\",\n        linewidth = 1.2,\n        arrow = arrow(length = unit(0.15, \"inches\"))\n    ) +\n    annotate(\n        \"text\",\n        x = -2,\n        y = (likelihood_0 + likelihood_hat) / 2 + 0.02,\n        label = \"LRT: Height\",\n        color = \"purple\",\n        hjust = 0,\n        fontface = \"bold\",\n        size = 5\n    ) +\n    # Add horizontal lines at both ends of LRT height comparison\n    annotate(\n        \"segment\",\n        x = -2.5,\n        xend = 2.5,\n        y = likelihood_0,\n        yend = likelihood_0,\n        color = \"purple\",\n        linetype = \"dotted\",\n        linewidth = 1\n    ) +\n    annotate(\n        \"segment\",\n        x = -2.5,\n        xend = 2.5,\n        y = likelihood_hat,\n        yend = likelihood_hat,\n        color = \"purple\",\n        linetype = \"dotted\",\n        linewidth = 1\n    ) +\n    \n    # Wald Test: Distance between theta_0 and theta_hat\n    annotate(\n        \"segment\",\n        x = theta_0,\n        xend = theta_hat,\n        y = 0.05,\n        yend = 0.05,\n        color = \"green\",\n        linewidth = 1.2,\n        arrow = arrow(length = unit(0.15, \"inches\"))\n    ) +\n    annotate(\n        \"text\",\n        x = (theta_0 + theta_hat) / 2,\n        y = 0.07,\n        label = \"Wald: Distance\",\n        color = \"green\",\n        hjust = 0.5,\n        fontface = \"bold\",\n        size = 5\n    ) +\n    \n    # LM Test: Slope at theta_0\n    annotate(\n        \"segment\",\n        x = theta_0 - 0.2,\n        xend = theta_0 + 0.2,\n        y = dnorm(theta_0 - 0.2, mean = 1, sd = 1),\n        yend = dnorm(theta_0 + 0.2, mean = 1, sd = 1),\n        color = \"orange\",\n        linewidth = 1.2,\n        arrow = arrow(length = unit(0.15, \"inches\"))\n    ) +\n    annotate(\n        \"text\",\n        x = -1.5,\n        y = dnorm(-1, mean = 1, sd = 1) + .2,\n        label = \"LM: Slope\",\n        color = \"orange\",\n        hjust = 0,\n        fontface = \"bold\",\n        size = 5\n    ) +\n    \n    # Titles and themes\n    theme_minimal() +\n    labs(title = \"Comparison of Hypothesis Tests\",\n         x = expression(theta),\n         y = \"Likelihood\") +\n    theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)\n    )"},{"path":"hypothesis-testing.html","id":"sec-two-one-sided-tests-equivalence-testing","chapter":"16 Hypothesis Testing","heading":"16.2 Two One-Sided Tests Equivalence Testing","text":"Two One-Sided Tests (TOST) procedure method used equivalence testing determine whether population effect size falls within range practical equivalence.Unlike traditional null hypothesis significance testing (NHST), focuses detecting differences, TOST tests similarity checking whether effect small enough practically insignificant.","code":""},{"path":"hypothesis-testing.html","id":"when-to-use-tost","chapter":"16 Hypothesis Testing","heading":"16.2.1 When to Use TOST?","text":"Bioequivalence Testing\nExample: Determining whether generic drug equivalent brand-name drug terms effectiveness.\nExample: Determining whether generic drug equivalent brand-name drug terms effectiveness.Non-Inferiority Testing\nExample: Assessing whether new teaching method worse traditional method meaningful margin.\nExample: Assessing whether new teaching method worse traditional method meaningful margin.Equivalence Business & Finance\nExample: Comparing performance two financial models determine produce practically results.\nExample: Comparing performance two financial models determine produce practically results.Psychological & Behavioral Research\nExample: Determining whether new intervention equally effective existing one.\nExample: Determining whether new intervention equally effective existing one.traditional hypothesis testing, assess:\\[\nH_0: \\theta = \\theta_0 \\quad vs. \\quad H_a: \\theta \\neq \\theta_0\n\\]\\(\\theta\\) population parameter (e.g., mean difference, regression coefficient, effect size).However, equivalence testing, interested whether \\(\\theta\\) falls within predefined equivalence margin (\\(-\\Delta, \\Delta\\)).leads TOST procedure, conduct two one-sided tests:1st One-Sided Test:\\[\nH_0: \\theta \\leq -\\Delta \\quad vs. \\quad H_a: \\theta > -\\Delta\n\\]2nd One-Sided Test:\\[\nH_0: \\theta \\geq \\Delta \\quad vs. \\quad H_a: \\theta < \\Delta\n\\]null hypotheses rejected, conclude equivalence (.e., \\(\\theta\\) within equivalence range).","code":""},{"path":"hypothesis-testing.html","id":"interpretation-of-the-tost-procedure","chapter":"16 Hypothesis Testing","heading":"16.2.2 Interpretation of the TOST Procedure","text":"p-value one-sided tests less \\(\\alpha\\), conclude effect size falls within equivalence bounds.one p-values greater \\(\\alpha\\), fail reject null hypothesis claim equivalence.TOST procedure provides stronger evidence similarity traditional NHST, assesses whether effect statistically different zero rather practically insignificant.","code":""},{"path":"hypothesis-testing.html","id":"relationship-to-confidence-intervals","chapter":"16 Hypothesis Testing","heading":"16.2.3 Relationship to Confidence Intervals","text":"Another way interpret TOST confidence intervals (CIs):entire \\((1 - 2\\alpha) \\times 100\\%\\) confidence interval lies within \\([-\\Delta, \\Delta]\\), conclude equivalence.confidence interval extends beyond equivalence range, fail establish equivalence.relationship ensures TOST consistent CI-based inference.","code":""},{"path":"hypothesis-testing.html","id":"example-1-testing-the-equivalence-of-two-means","chapter":"16 Hypothesis Testing","heading":"16.2.4 Example 1: Testing the Equivalence of Two Means","text":"Suppose two groups want test whether mean difference practically insignificant within range \\([-0.5, 0.5]\\).p-values less 0.05, conclude groups equivalent within given range.p-values less 0.05, conclude groups equivalent within given range.confidence interval helps visualize whether effect size falls entirely within \\([-0.5, 0.5]\\).confidence interval helps visualize whether effect size falls entirely within \\([-0.5, 0.5]\\).","code":"\nlibrary(TOSTER)\n\n# Simulated data: Two groups with similar means\nset.seed(123)\ngroup1 <- rnorm(30, mean = 5, sd = 1)\ngroup2 <- rnorm(30, mean = 5.1, sd = 1)\n\n# Perform TOST equivalence test\nTOSTtwo(\n    m1 = mean(group1),\n    sd1 = sd(group1),\n    n1 = length(group1),\n    m2 = mean(group2),\n    sd2 = sd(group2),\n    n2 = length(group2),\n    low_eqbound = -0.5,\n    high_eqbound = 0.5,\n    alpha = 0.05\n)#> TOST results:\n#> t-value lower bound: 0.553   p-value lower bound: 0.291\n#> t-value upper bound: -3.32   p-value upper bound: 0.0008\n#> degrees of freedom : 56.56\n#> \n#> Equivalence bounds (Cohen's d):\n#> low eqbound: -0.5 \n#> high eqbound: 0.5\n#> \n#> Equivalence bounds (raw scores):\n#> low eqbound: -0.4555 \n#> high eqbound: 0.4555\n#> \n#> TOST confidence interval:\n#> lower bound 90% CI: -0.719\n#> upper bound 90% CI:  0.068\n#> \n#> NHST confidence interval:\n#> lower bound 95% CI: -0.797\n#> upper bound 95% CI:  0.146\n#> \n#> Equivalence Test Result:\n#> The equivalence test was non-significant, t(56.56) = 0.553, p = 0.291, given equivalence bounds of -0.456 and 0.456 (on a raw scale) and an alpha of 0.05.\n#> \n#> Null Hypothesis Test Result:\n#> The null hypothesis test was non-significant, t(56.56) = -1.384, p = 0.172, given an alpha of 0.05."},{"path":"hypothesis-testing.html","id":"example-2-tost-for-correlation-equivalence","chapter":"16 Hypothesis Testing","heading":"16.2.4.1 Example 2: TOST for Correlation Equivalence","text":"can also use TOST test whether correlation coefficient effectively zero.tests whether correlation within \\([-0.1, 0.1]\\), meaning “practically zero”.tests whether correlation within \\([-0.1, 0.1]\\), meaning “practically zero”.p-values significant, conclude correlation effectively negligible.p-values significant, conclude correlation effectively negligible.","code":"\n# Simulated correlation data\nset.seed(123)\nx <- rnorm(50)\ny <- x * 0.02 + rnorm(50, sd = 1)  # Very weak correlation\n\n# TOST for correlation\nTOSTr(\n    n = length(x),\n    r = cor(x, y),\n    low_eqbound_r = -0.1,\n    high_eqbound_r = 0.1,\n    alpha = 0.05\n)#> TOST results:\n#> p-value lower bound: 0.280\n#> p-value upper bound: 0.214\n#> \n#> Equivalence bounds (r):\n#> low eqbound: -0.1 \n#> high eqbound: 0.1\n#> \n#> TOST confidence interval:\n#> lower bound 90% CI: -0.25\n#> upper bound 90% CI:  0.221\n#> \n#> NHST confidence interval:\n#> lower bound 95% CI: -0.293\n#> upper bound 95% CI:  0.264\n#> \n#> Equivalence Test Result:\n#> The equivalence test was non-significant, p = 0.280, given equivalence bounds of -0.100 and 0.100 and an alpha of 0.05.\n#> \n#> Null Hypothesis Test Result:\n#> The null hypothesis test was non-significant, p = 0.915, given an alpha of 0.05."},{"path":"hypothesis-testing.html","id":"advantages-of-tost-equivalence-testing","chapter":"16 Hypothesis Testing","heading":"16.2.5 Advantages of TOST Equivalence Testing","text":"Avoids Misinterpretation Non-Significance\nTraditional NHST failing reject \\(H_0\\) imply equivalence.\nTOST explicitly tests equivalence, preventing misinterpretation.\nAvoids Misinterpretation Non-SignificanceTraditional NHST failing reject \\(H_0\\) imply equivalence.Traditional NHST failing reject \\(H_0\\) imply equivalence.TOST explicitly tests equivalence, preventing misinterpretation.TOST explicitly tests equivalence, preventing misinterpretation.Aligned Confidence Intervals\nTOST conclusions align confidence interval-based reasoning.\nAligned Confidence IntervalsTOST conclusions align confidence interval-based reasoning.Applicable Various Statistical Tests\nCan used means, correlations, regression coefficients, .\nApplicable Various Statistical TestsCan used means, correlations, regression coefficients, .Commonly Used Regulatory & Clinical Studies\nRequired bioequivalence trials organizations like FDA (Schuirmann 1987).\nCommonly Used Regulatory & Clinical StudiesRequired bioequivalence trials organizations like FDA (Schuirmann 1987).","code":""},{"path":"hypothesis-testing.html","id":"when-not-to-use-tost","chapter":"16 Hypothesis Testing","heading":"16.2.6 When Not to Use TOST","text":"research question detecting difference rather establishing equivalence.research question detecting difference rather establishing equivalence.equivalence bounds wide meaningful practice.equivalence bounds wide meaningful practice.sample size small, making difficult detect equivalence reliably.sample size small, making difficult detect equivalence reliably.","code":""},{"path":"hypothesis-testing.html","id":"sec-false-discovery-rate","chapter":"16 Hypothesis Testing","heading":"16.3 False Discovery Rate","text":"conducting multiple hypothesis tests simultaneously, increase probability false positives (Type errors). Traditional correction methods like Bonferroni correction conservative, reducing statistical power.False Discovery Rate (FDR), introduced Benjamini Hochberg (1995), flexible powerful approach controls proportion false discoveries (incorrect rejections null hypothesis) maintaining reasonable chance detecting true effects.Suppose perform \\(m\\) independent hypothesis tests, significance level \\(\\alpha\\). probability making least one Type error (false positive) :\\[\nP(\\text{least one false positive}) = 1 - (1 - \\alpha)^m\n\\]example, \\(\\alpha = 0.05\\) \\(m = 20\\) tests:\\[\nP(\\text{least one false positive}) = 1 - (0.95)^{20} \\approx 0.64\n\\]Thus, adjust multiple testing, highly likely reject least one true null hypothesis just chance.Family-Wise Error Rate vs. False Discovery RateWhy FDR?FWER control (Bonferroni, Holm) strict, reducing true discoveries.FDR control allows small fraction false positives keeping discoveries valid.Let:\\(m\\) = total number hypotheses tested\\(m\\) = total number hypotheses tested\\(V\\) = number false discoveries (Type errors)\\(V\\) = number false discoveries (Type errors)\\(R\\) = total number rejected null hypotheses\\(R\\) = total number rejected null hypothesesThen, False Discovery Rate :\\[\n\\text{FDR} = E\\left[\\frac{V}{\\max(R,1)}\\right]\n\\]null hypotheses rejected (\\(R = 0\\)), define FDR = 0.Unlike FWER, controls probability false positives, FDR controls expected proportion false positives.","code":""},{"path":"hypothesis-testing.html","id":"sec-benjamini-hochberg-procedure","chapter":"16 Hypothesis Testing","heading":"16.3.1 Benjamini-Hochberg Procedure","text":"Benjamini-Hochberg (BH) procedure widely used FDR-controlling method (Benjamini Hochberg 1995). works follows:Step--Step Algorithm:Perform \\(m\\) hypothesis tests obtain \\(p\\)-values: \\(p_1, p_2, ..., p_m\\).Perform \\(m\\) hypothesis tests obtain \\(p\\)-values: \\(p_1, p_2, ..., p_m\\).Rank \\(p\\)-values ascending order: \\(p_{(1)} \\leq p_{(2)} \\leq ... \\leq p_{(m)}\\).Rank \\(p\\)-values ascending order: \\(p_{(1)} \\leq p_{(2)} \\leq ... \\leq p_{(m)}\\).Calculate Benjamini-Hochberg critical value test:\n\\[\np_{()} \\leq \\frac{}{m} \\alpha\n\\]Calculate Benjamini-Hochberg critical value test:\\[\np_{()} \\leq \\frac{}{m} \\alpha\n\\]Find largest \\(\\) \\(p_{()} \\leq \\frac{}{m} \\alpha\\).Find largest \\(\\) \\(p_{()} \\leq \\frac{}{m} \\alpha\\).Reject hypotheses \\(p \\leq p_{()}\\).Reject hypotheses \\(p \\leq p_{()}\\).Interpretation:ensures expected proportion false discoveries controlled level \\(\\alpha\\).Unlike Bonferroni, require independence tests, making powerful.","code":""},{"path":"hypothesis-testing.html","id":"example-1-fdr-correction-on-simulated-data","chapter":"16 Hypothesis Testing","heading":"16.3.1.1 Example 1: FDR Correction on Simulated Data","text":"Adjusted p-values control expected proportion false discoveries.Adjusted p-values control expected proportion false discoveries.adjusted p-value \\(\\alpha\\), reject null hypothesis.adjusted p-value \\(\\alpha\\), reject null hypothesis.","code":"\nset.seed(123)\n\n# Generate 20 random p-values\np_values <-\n    runif(20, 0, 0.1)  # Simulating p-values from multiple tests\n\n# Apply FDR correction (Benjamini-Hochberg)\nadjusted_p <- p.adjust(p_values, method = \"BH\")\n\n# Compare raw and adjusted p-values\ndata.frame(Raw_p = round(p_values, 3),\n           Adjusted_p = round(adjusted_p, 3)) |> head()\n#>   Raw_p Adjusted_p\n#> 1 0.029      0.095\n#> 2 0.079      0.096\n#> 3 0.041      0.095\n#> 4 0.088      0.096\n#> 5 0.094      0.096\n#> 6 0.005      0.046"},{"path":"hypothesis-testing.html","id":"example-2-fdr-correction-in-gene-expression-analysis","chapter":"16 Hypothesis Testing","heading":"16.3.1.2 Example 2: FDR Correction in Gene Expression Analysis","text":"FDR widely used genomics, thousands genes tested differential expression.Bonferroni results discoveries (low power).Bonferroni results discoveries (low power).Benjamini-Hochberg allows discoveries, controlling proportion false positives.Benjamini-Hochberg allows discoveries, controlling proportion false positives.Use FDR :perform many hypothesis tests (e.g., genomics, finance, /B testing).perform many hypothesis tests (e.g., genomics, finance, /B testing).want balance false positives false negatives.want balance false positives false negatives.Bonferroni strict, leading low power.Bonferroni strict, leading low power.use FDR :Strict control false positives required (e.g., drug approval studies).Strict control false positives required (e.g., drug approval studies).tests, Bonferroni appropriate.tests, Bonferroni appropriate.","code":"\nlibrary(multtest)\n\n# Simulated gene expression study with 1000 genes\nset.seed(42)\np_values <- runif(100, 0, 0.1)\n\n# Apply different multiple testing corrections\n\n# Bonferroni (very strict)\np_bonf <- p.adjust(p_values, method = \"bonferroni\")  \n\n# Holm's method\np_holm <- p.adjust(p_values, method = \"holm\")        \n\n# Benjamini-Hochberg (FDR)\np_fdr <- p.adjust(p_values, method = \"BH\")           \n\n# Compare significance rates\nsum(p_bonf < 0.05)  # Strictest correction\n#> [1] 3\nsum(p_holm < 0.05)  # Moderately strict\n#> [1] 3\nsum(p_fdr < 0.05)   # Most discoveries, controlled FDR\n#> [1] 5"},{"path":"hypothesis-testing.html","id":"sec-benjamini-yekutieli-procedure","chapter":"16 Hypothesis Testing","heading":"16.3.2 Benjamini-Yekutieli Procedure","text":"Benjamini-Yekutieli () method modifies Benjamini-Hochberg (BH) procedure account correlated test statistics (Benjamini Yekutieli 2001).key adjustment larger critical value significance, making conservative BH.Similar BH, procedure ranks \\(m\\) p-values ascending order:\\[\np_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}\n\\]Instead using \\(\\alpha \\frac{}{m}\\) BH, introduces correction factor:\\[\np_{()} \\leq \\frac{}{m C(m)} \\alpha\n\\]:\\[\nC(m) = \\sum_{j=1}^{m} \\frac{1}{j} \\approx \\ln(m) + 0.577\n\\]\\(C(m)\\) harmonic series correction (ensures control dependence).makes threshold larger BH, reducing false positives.Recommended tests positively correlated (e.g., fMRI, finance).can apply correction using built-p.adjust() function.Comparison: BH vs. BYBenjamini-Yekutieli conservative Benjamini-Hochberg.Benjamini-Yekutieli conservative Benjamini-Hochberg.BH identifies significant results , tests likely correlated.BH identifies significant results , tests likely correlated.","code":"\nset.seed(123)\n\n# Simulate 50 random p-values\np_values <- runif(50, 0, 0.1)\n\n# Apply BY correction\np_by <- p.adjust(p_values, method = \"BY\")\n\n# Compare raw and adjusted p-values\ndata.frame(Raw_p = round(p_values, 3), Adjusted_BY = round(p_by, 3))\n#>    Raw_p Adjusted_BY\n#> 1  0.029       0.430\n#> 2  0.079       0.442\n#> 3  0.041       0.430\n#> 4  0.088       0.442\n#> 5  0.094       0.442\n#> 6  0.005       0.342\n#> 7  0.053       0.442\n#> 8  0.089       0.442\n#> 9  0.055       0.442\n#> 10 0.046       0.430\n#> 11 0.096       0.442\n#> 12 0.045       0.430\n#> 13 0.068       0.442\n#> 14 0.057       0.442\n#> 15 0.010       0.429\n#> 16 0.090       0.442\n#> 17 0.025       0.430\n#> 18 0.004       0.342\n#> 19 0.033       0.430\n#> 20 0.095       0.442\n#> 21 0.089       0.442\n#> 22 0.069       0.442\n#> 23 0.064       0.442\n#> 24 0.099       0.447\n#> 25 0.066       0.442\n#> 26 0.071       0.442\n#> 27 0.054       0.442\n#> 28 0.059       0.442\n#> 29 0.029       0.430\n#> 30 0.015       0.429\n#> 31 0.096       0.442\n#> 32 0.090       0.442\n#> 33 0.069       0.442\n#> 34 0.080       0.442\n#> 35 0.002       0.342\n#> 36 0.048       0.430\n#> 37 0.076       0.442\n#> 38 0.022       0.430\n#> 39 0.032       0.430\n#> 40 0.023       0.430\n#> 41 0.014       0.429\n#> 42 0.041       0.430\n#> 43 0.041       0.430\n#> 44 0.037       0.430\n#> 45 0.015       0.429\n#> 46 0.014       0.429\n#> 47 0.023       0.430\n#> 48 0.047       0.430\n#> 49 0.027       0.430\n#> 50 0.086       0.442\np_bh <- p.adjust(p_values, method = \"BH\")  # BH correction\n\n# Visualize differences\ndata.frame(Raw_p = round(p_values, 3),\n           BH_Adjusted = round(p_bh, 3),\n           BY_Adjusted = round(p_by, 3)) |> head()\n#>   Raw_p BH_Adjusted BY_Adjusted\n#> 1 0.029       0.096       0.430\n#> 2 0.079       0.098       0.442\n#> 3 0.041       0.096       0.430\n#> 4 0.088       0.098       0.442\n#> 5 0.094       0.098       0.442\n#> 6 0.005       0.076       0.342"},{"path":"hypothesis-testing.html","id":"sec-storeys-q-value-approach","chapter":"16 Hypothesis Testing","heading":"16.3.3 Storey’s q-value Approach","text":"Storey’s q-value method directly estimates False Discovery Rate rather adjusting individual \\(p\\)-values (Benjamini Yekutieli 2001).Unlike BH/, assume fixed threshold (\\(\\alpha\\)) estimates FDR dynamically data.Define:\\(m_0\\): Number true null hypotheses.\\(m_0\\): Number true null hypotheses.\\(\\pi_0\\): Estimated proportion true nulls dataset.\\(\\pi_0\\): Estimated proportion true nulls dataset.Storey’s q-value adjusts \\(p\\)-values based : \\[\nq(p) = \\frac{\\pi_0 \\cdot m \\cdot p}{\\sum_{=1}^{m} 1_{p_i \\leq p}}\n\\] :\\(\\pi_0\\) estimated distribution large p-values.\\(\\pi_0\\) estimated distribution large p-values.Unlike BH/, Storey’s method dynamically estimates null proportion.Unlike BH/, Storey’s method dynamically estimates null proportion.Comparison: BH vs. vs. Storey","code":"\n# devtools::install_github(\"jdstorey/qvalue\")\nlibrary(qvalue)\n\n# Simulated data: 1000 hypothesis tests\nset.seed(123)\np_values <- runif(1000, 0, 0.1)  \n\n# Compute q-values\nqvals <- qvalue_truncp(p_values)$qvalues\n\n# Summary of q-values\nsummary(qvals)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.3126  0.9689  0.9771  0.9684  0.9847  1.0000\n# Apply multiple corrections\np_bh <- p.adjust(p_values, method = \"BH\")   # Benjamini-Hochberg\np_by <- p.adjust(p_values, method = \"BY\")   # Benjamini-Yekutieli\nq_vals <- qvalue_truncp(p_values)$qvalues   # Storey's q-value\n\n# Compare significance rates\ndata.frame(\n  Raw_p = round(p_values[1:10], 3),\n  BH = round(p_bh[1:10], 3),\n  BY = round(p_by[1:10], 3),\n  q_value = round(q_vals[1:10], 3)\n)\n#>    Raw_p    BH    BY q_value\n#> 1  0.029 0.097 0.725   0.969\n#> 2  0.079 0.098 0.737   0.985\n#> 3  0.041 0.097 0.725   0.969\n#> 4  0.088 0.099 0.740   0.989\n#> 5  0.094 0.100 0.746   0.998\n#> 6  0.005 0.094 0.700   0.936\n#> 7  0.053 0.098 0.737   0.985\n#> 8  0.089 0.099 0.740   0.989\n#> 9  0.055 0.098 0.737   0.985\n#> 10 0.046 0.098 0.731   0.977"},{"path":"hypothesis-testing.html","id":"summary-false-discovery-rate-methods","chapter":"16 Hypothesis Testing","heading":"16.3.4 Summary: False Discovery Rate Methods","text":"FDR control methods balance Type Type II errors, making powerful conservative Family-Wise Error Rate (FWER) methods like Bonferroni.","code":""},{"path":"hypothesis-testing.html","id":"comparison-of-testing-frameworks","chapter":"16 Hypothesis Testing","heading":"16.4 Comparison of Testing Frameworks","text":"","code":""},{"path":"sec-marginal-effects.html","id":"sec-marginal-effects","chapter":"17 Marginal Effects","heading":"17 Marginal Effects","text":"Marginal effects play fundamental role interpreting regression models, particularly analyzing impact explanatory variables outcome variable. effects provide precise measure small change independent variable influences dependent variable.concept marginal effect closely linked derivatives calculus. simple linear models, marginal effects correspond directly estimated regression coefficients. However, nonlinear models, computing marginal effects requires careful consideration, often involving either analytical differentiation numerical approximation.","code":""},{"path":"sec-marginal-effects.html","id":"definition-of-marginal-effects","chapter":"17 Marginal Effects","heading":"17.1 Definition of Marginal Effects","text":"Mathematically, marginal effect independent variable \\(X\\) expected value dependent variable \\(Y\\) given :\\[\n\\frac{\\partial E[Y|X]}{\\partial X}\n\\]represents instantaneous rate change \\(E[Y|X]\\) respect \\(X\\).linear regression model:\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\varepsilon\n\\]marginal effect \\(X_j\\) simply \\(\\beta_j\\). However, complex cases, nonlinear models, interaction effects, transformations, marginal effects directly given regression coefficients must computed explicitly.","code":""},{"path":"sec-marginal-effects.html","id":"sec-analytical-derivation-of-marginal-effects","chapter":"17 Marginal Effects","heading":"17.1.1 Analytical Derivation of Marginal Effects","text":"models \\(E[Y|X]\\) differentiable function \\(X\\), marginal effects computed using calculus. derivative function \\(f(x)\\) given :\\[\nf'(x) \\equiv \\lim_{h \\0} \\frac{f(x+h) - f(x)}{h}\n\\]Example: Quadratic FunctionConsider function:\\[\nf(x) = x^2.\n\\]marginal effect derived follows:\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h} \\\\\n&= \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\\n&= \\frac{2xh + h^2}{h} \\\\\n&= 2x + h.\n\\end{aligned}\n\\]\\(h \\0\\), marginal effect simplifies :\\[\nf'(x) = 2x.\n\\]Thus, small changes \\(x\\), effect \\(f(x)\\) depends \\(x\\) .","code":""},{"path":"sec-marginal-effects.html","id":"sec-numerical-approximation-of-marginal-effects","chapter":"17 Marginal Effects","heading":"17.1.2 Numerical Approximation of Marginal Effects","text":"practice, analytical differentiation may infeasible, particularly dealing complex functions, large datasets, models without closed-form derivatives. cases, numerical differentiation provides alternative.","code":""},{"path":"sec-marginal-effects.html","id":"sec-one-sided-numerical-approximation","chapter":"17 Marginal Effects","heading":"17.1.2.1 One-Sided Numerical Approximation","text":"simple way approximate derivative forward difference formula:\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h}  \\\\\n& \\approx \\frac{f(x+h) -f(x)}{h}\n\\end{aligned}\n\\]\\(h\\) small step size.","code":""},{"path":"sec-marginal-effects.html","id":"sec-two-sided-numerical-approximation","chapter":"17 Marginal Effects","heading":"17.1.2.2 Two-Sided Numerical Approximation","text":"accurate method central difference formula:\\[\nf'_2(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.\n\\]approach reduces numerical error generally preferred computational implementations.","code":""},{"path":"sec-marginal-effects.html","id":"choosing-an-appropriate-h","chapter":"17 Marginal Effects","heading":"17.1.2.3 Choosing an Appropriate \\(h\\)","text":"choice \\(h\\) critical (Gould, Pitblado, Poi 2010, chap. 1):small: Can lead numerical instability due floating-point precision limitations.large: Reduces accuracy approximation.common heuristic set \\(h = 10^{-5}\\) small fraction standard deviation \\(X\\).Comparison Analytical Numerical MethodsNumerical derivatives often preferred empirical applications, especially working complex models machine learning algorithms.","code":""},{"path":"sec-marginal-effects.html","id":"marginal-effects-in-different-contexts","chapter":"17 Marginal Effects","heading":"17.2 Marginal Effects in Different Contexts","text":"Linear Regression ModelsFor simple linear regression:\\[\nE[Y|X] = \\beta_0 + \\beta_1 X,\n\\]marginal effect constant equal \\(\\beta_1\\). makes interpretation straightforward.Logit Probit ModelsIn logistic regression, expected value \\(Y\\) modeled :\\[\nE[Y|X] = P(Y=1|X) = \\frac{1}{1 + e^{-\\beta_0 - \\beta_1 X}}.\n\\]marginal effect given :\\[\n\\frac{\\partial E[Y|X]}{\\partial X} = \\beta_1 P(Y=1|X) (1 - P(Y=1|X)).\n\\]Unlike linear models, effect varies \\(X\\), requiring evaluation specific values (e.g., means percentiles).Interaction Effects Nonlinear TermsWhen models include interactions (e.g., \\(X_1 X_2\\)) transformations (e.g., \\(\\log(X)\\)), marginal effects become complex. example, :\\[\nE[Y|X] = \\beta_0 + \\beta_1 X + \\beta_2 X^2,\n\\]marginal effect \\(X\\) :\\[\n\\frac{\\partial E[Y|X]}{\\partial X} = \\beta_1 + 2\\beta_2 X.\n\\]means marginal effect depends value \\(X\\).","code":""},{"path":"sec-marginal-effects.html","id":"marginal-effects-interpretation","chapter":"17 Marginal Effects","heading":"17.3 Marginal Effects Interpretation","text":"interpretation marginal effects differs depending whether \\(X\\) continuous discrete:example, binary variable case (e.g., dummy variable gender), marginal effect :\\[\nE[Y|X=1] - E[Y|X=0].\n\\]quantifies expected change \\(Y\\) switching \\(X = 0\\) \\(X = 1\\).","code":""},{"path":"sec-marginal-effects.html","id":"sec-delta-method","chapter":"17 Marginal Effects","heading":"17.4 Delta Method","text":"Delta Method statistical technique approximating mean variance function random variables. particularly useful regression analysis estimating standard errors nonlinear functions estimated coefficients, :Marginal effects nonlinear models (e.g., logistic regression)Elasticities risk measures (e.g., finance)Transformation regression coefficients (e.g., log transformations)method based first-order Taylor Series approximation, allows us estimate variance transformed parameter without requiring explicit distributional assumptions.Let \\(G(\\beta)\\) function estimated parameters \\(\\beta\\), \\(\\beta\\) follows asymptotically normal distribution:\\[\n\\beta \\sim N(\\hat{\\beta}, \\text{Var}(\\hat{\\beta})).\n\\]Using first-order Taylor expansion, approximate \\(G(\\beta)\\) around expectation:\\[\nG(\\beta) \\approx G(\\hat{\\beta}) + \\nabla G(\\beta) (\\beta - \\hat{\\beta}),\n\\]\\(\\nabla G(\\beta)\\) gradient (also known Jacobian) \\(G(\\beta)\\), .e., vector partial derivatives:\\[\n\\nabla G(\\beta) = \\left( \\frac{\\partial G}{\\partial \\beta_1}, \\frac{\\partial G}{\\partial \\beta_2}, \\dots, \\frac{\\partial G}{\\partial \\beta_k} \\right).\n\\]variance \\(G(\\beta)\\) approximated :\\[\n\\text{Var}(G(\\beta)) \\approx \\nabla G(\\beta) \\, \\text{Cov}(\\beta) \\, \\nabla G(\\beta)'.\n\\]:\\(\\nabla G(\\beta)\\) gradient vector \\(G(\\beta)\\).\\(\\nabla G(\\beta)\\) gradient vector \\(G(\\beta)\\).\\(\\text{Cov}(\\beta)\\) variance-covariance matrix \\(\\hat{\\beta}\\).\\(\\text{Cov}(\\beta)\\) variance-covariance matrix \\(\\hat{\\beta}\\).expression \\(\\nabla G(\\beta)'\\) denotes transpose gradient.expression \\(\\nabla G(\\beta)'\\) denotes transpose gradient.Key Properties Delta MethodSemi-parametric approach: require full knowledge distribution \\(G(\\beta)\\).Widely applicable: Useful computing standard errors regression models.Alternative approaches:\nAnalytical derivation: Directly deriving probability function margin.\nSimulation/Bootstrapping: Using Monte Carlo methods approximate standard errors.\nAnalytical derivation: Directly deriving probability function margin.Simulation/Bootstrapping: Using Monte Carlo methods approximate standard errors.","code":""},{"path":"sec-marginal-effects.html","id":"comparison-delta-method-vs.-alternative-approaches","chapter":"17 Marginal Effects","heading":"17.5 Comparison: Delta Method vs. Alternative Approaches","text":"Use Delta Method:need quick approximation standard errors.need quick approximation standard errors.function \\(G(\\beta)\\) smooth differentiable.function \\(G(\\beta)\\) smooth differentiable.working large sample sizes, asymptotic normality holds.working large sample sizes, asymptotic normality holds.deeper exploration, refer excellent resources:Advanced: modmarg package documentation – covers implementation Delta Method R.Advanced: modmarg package documentation – covers implementation Delta Method R.Intermediate: UCLA Statistical Consulting – practical FAQ Delta Method.Intermediate: UCLA Statistical Consulting – practical FAQ Delta Method.","code":""},{"path":"sec-marginal-effects.html","id":"example-applying-the-delta-method-in-a-logistic-regression","chapter":"17 Marginal Effects","heading":"17.5.1 Example: Applying the Delta Method in a logistic regression","text":"illustrate, let’s apply Delta Method compute standard error nonlinear transformation regression coefficients.logistic regression, estimated coefficient \\(\\hat{\\beta}\\) represents log-odds change one-unit increase \\(X\\). However, often want odds ratio, :\\[\nG(\\beta) = e^{\\beta}.\n\\]Delta Method, variance \\(e^{\\beta}\\) :\\[\n\\text{Var}(e^{\\beta}) \\approx e^{2\\beta} \\cdot \\text{Var}(\\beta).\n\\]Odds Ratio Computation\nodds ratio exponentiated coefficient \\(e^{\\hat{\\beta}}\\), represents multiplicative change odds \\(Y = 1\\) one-unit increase \\(X\\).\n\\(\\hat{\\beta} = 0.8\\), \\(e^{0.8} \\approx 2.23\\), meaning one-unit increase \\(X\\) increases odds \\(Y = 1\\) 123%.\nOdds Ratio ComputationThe odds ratio exponentiated coefficient \\(e^{\\hat{\\beta}}\\), represents multiplicative change odds \\(Y = 1\\) one-unit increase \\(X\\).odds ratio exponentiated coefficient \\(e^{\\hat{\\beta}}\\), represents multiplicative change odds \\(Y = 1\\) one-unit increase \\(X\\).\\(\\hat{\\beta} = 0.8\\), \\(e^{0.8} \\approx 2.23\\), meaning one-unit increase \\(X\\) increases odds \\(Y = 1\\) 123%.\\(\\hat{\\beta} = 0.8\\), \\(e^{0.8} \\approx 2.23\\), meaning one-unit increase \\(X\\) increases odds \\(Y = 1\\) 123%.Standard Error via Delta Method\nSince \\(\\beta\\) follows normal distribution, transformation \\(e^\\beta\\) normally distributed rather follows log-normal shape.\nDelta Method approximates standard error \\(e^\\beta\\) using: \\[\nSE(e^\\beta) = \\sqrt{e^{2\\beta} \\cdot \\text{Var}(\\beta)}\n\\]\nStandard Error via Delta MethodSince \\(\\beta\\) follows normal distribution, transformation \\(e^\\beta\\) normally distributed rather follows log-normal shape.Since \\(\\beta\\) follows normal distribution, transformation \\(e^\\beta\\) normally distributed rather follows log-normal shape.Delta Method approximates standard error \\(e^\\beta\\) using: \\[\nSE(e^\\beta) = \\sqrt{e^{2\\beta} \\cdot \\text{Var}(\\beta)}\n\\]Delta Method approximates standard error \\(e^\\beta\\) using: \\[\nSE(e^\\beta) = \\sqrt{e^{2\\beta} \\cdot \\text{Var}(\\beta)}\n\\]Confidence Intervals\nconfidence interval obtained : \\[\n[e^{\\beta - 1.96 \\cdot SE}, e^{\\beta + 1.96 \\cdot SE}]\n\\]\nhelps interpret uncertainty around odds ratio estimate.\nConfidence IntervalsThe confidence interval obtained : \\[\n[e^{\\beta - 1.96 \\cdot SE}, e^{\\beta + 1.96 \\cdot SE}]\n\\]helps interpret uncertainty around odds ratio estimate.Visualization\nhistogram simulated odds ratios shows transformation affects variance:\nred dashed line represents estimated odds ratio.\nblue dotted lines show confidence interval bounds.\nright-skewed distribution reflects non-linear transformation, meaning higher uncertainty larger values.\n\nVisualizationThe histogram simulated odds ratios shows transformation affects variance:\nred dashed line represents estimated odds ratio.\nblue dotted lines show confidence interval bounds.\nright-skewed distribution reflects non-linear transformation, meaning higher uncertainty larger values.\nhistogram simulated odds ratios shows transformation affects variance:red dashed line represents estimated odds ratio.red dashed line represents estimated odds ratio.blue dotted lines show confidence interval bounds.blue dotted lines show confidence interval bounds.right-skewed distribution reflects non-linear transformation, meaning higher uncertainty larger values.right-skewed distribution reflects non-linear transformation, meaning higher uncertainty larger values.","code":"\n# Load necessary packages\nlibrary(ggplot2)\nlibrary(margins)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Simulate data\nset.seed(123)\nn <- 100\nX <- rnorm(n)  # Simulate independent variable\n# Generate binary outcome using logistic model\nY <-\n    rbinom(n, 1, plogis(0.5 + 0.8 * X))  \n\n# Logistic regression\nlogit_model <- glm(Y ~ X, family = binomial(link = \"logit\"))\n\n# Extract coefficient and variance\nbeta_hat <- coef(logit_model)[\"X\"]   # Estimated coefficient\nvar_beta_hat <- vcov(logit_model)[\"X\", \"X\"]  # Variance of beta_hat\n\n# Apply Delta Method\nodds_ratio <- exp(beta_hat)  # Transform beta to odds ratio\nse_odds_ratio <-\n    sqrt(odds_ratio ^ 2 * var_beta_hat)  # Delta Method SE\n\n# Compute 95% Confidence Interval\nlower_CI <- exp(beta_hat - 1.96 * sqrt(var_beta_hat))\nupper_CI <- exp(beta_hat + 1.96 * sqrt(var_beta_hat))\n\n# Display results\nresults <- data.frame(\n    Term = \"X\",\n    Odds_Ratio = odds_ratio,\n    SE = se_odds_ratio,\n    Lower_CI = lower_CI,\n    Upper_CI = upper_CI\n)\n\nprint(results)\n#>   Term Odds_Ratio        SE Lower_CI Upper_CI\n#> X    X   2.655431 0.7677799 1.506669 4.680069\n\n# ---- VISUALIZATION 1: Distribution of Simulated Odds Ratios ----\nset.seed(123)\n# Simulate beta estimates\nsimulated_betas <-\n    rnorm(1000, mean = beta_hat, sd = sqrt(var_beta_hat))  \nsimulated_odds_ratios <-\n    exp(simulated_betas)  # Apply transformation\n\nggplot(data.frame(Odds_Ratio = simulated_odds_ratios),\n       aes(x = Odds_Ratio)) +\n    geom_histogram(\n        color = \"black\",\n        fill = \"skyblue\",\n        bins = 50,\n        alpha = 0.7\n    ) +\n    geom_vline(\n        xintercept = odds_ratio,\n        color = \"red\",\n        linetype = \"dashed\",\n        linewidth = 1.2\n    ) +\n    geom_vline(\n        xintercept = lower_CI,\n        color = \"blue\",\n        linetype = \"dotted\",\n        linewidth = 1.2\n    ) +\n    geom_vline(\n        xintercept = upper_CI,\n        color = \"blue\",\n        linetype = \"dotted\",\n        linewidth = 1.2\n    ) +\n    labs(title = \"Distribution of Simulated Odds Ratios\",\n         x = \"Odds Ratio\",\n         y = \"Frequency\") +\n    theme_minimal()"},{"path":"sec-marginal-effects.html","id":"types-of-marginal-effect","chapter":"17 Marginal Effects","heading":"17.6 Types of Marginal Effect","text":"","code":""},{"path":"sec-marginal-effects.html","id":"sec-average-marginal-effect","chapter":"17 Marginal Effects","heading":"17.6.1 Average Marginal Effect","text":"Average Marginal Effect (AME) measures expected change predicted probability independent variable increases small amount holding variables constant. Unlike marginal effects mean (MEM), AMEs average marginal effects across observations, providing representative measure.Applications AMEsMarketing: much increasing ad spend change probability customer purchase?Finance: interest rate change impact probability loan approval?Econometrics: effect education probability employment?Since nonlinear models like logit probit constant marginal effects, AMEs require numerical differentiation. two common approaches:One-Sided Numerical Derivative: Uses small forward step \\(h\\) estimate derivative.Two-Sided Numerical Derivative: Takes forward step backward step improve accuracy.","code":""},{"path":"sec-marginal-effects.html","id":"sec-one-sided-numerical-derivative","chapter":"17 Marginal Effects","heading":"17.6.1.1 One-Sided Numerical Derivative","text":"estimate \\(\\frac{\\partial p(\\mathbf{X},\\beta)}{\\partial X}\\) numerically:AlgorithmEstimate model using logistic (probit) regression.observation \\(\\):\nCompute predicted probability using observed data: \\[\n\\hat{Y}_{i0} = p(\\mathbf{X}_i, \\hat{\\beta}).\n\\]\nIncrease \\(X\\) small step \\(h\\), :\n\\(X\\) continuous, choose:\\[ h = (|\\bar{X}| + 0.001) \\times 0.001. \\]\n\\(X\\) discrete, set \\(h = 1\\).\n\nCompute new predicted probability: \\[\n\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\n\\]\nCompute numerical derivative: \\[\n\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i0}}{h}.\n\\]\nCompute predicted probability using observed data: \\[\n\\hat{Y}_{i0} = p(\\mathbf{X}_i, \\hat{\\beta}).\n\\]Increase \\(X\\) small step \\(h\\), :\n\\(X\\) continuous, choose:\\[ h = (|\\bar{X}| + 0.001) \\times 0.001. \\]\n\\(X\\) discrete, set \\(h = 1\\).\n\\(X\\) continuous, choose:\\[ h = (|\\bar{X}| + 0.001) \\times 0.001. \\]\\(X\\) discrete, set \\(h = 1\\).Compute new predicted probability: \\[\n\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\n\\]Compute numerical derivative: \\[\n\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i0}}{h}.\n\\]Average across observations: \\[\nE\\left[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i0}}{h}\\right] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}.\n\\]AME average effect \\(X\\) probability \\(Y=1\\).AME average effect \\(X\\) probability \\(Y=1\\).Since logistic regression nonlinear, effect varies across observations.Since logistic regression nonlinear, effect varies across observations.method assumes small \\(h\\) provides good approximation.method assumes small \\(h\\) provides good approximation.","code":"\n# Load necessary packages\nlibrary(margins)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Simulate data\nset.seed(123)\nn <- 100\nX <- rnorm(n)\nY <- rbinom(n, 1, plogis(0.5 + 0.8 * X))  # Logistic function\n\n# Logistic regression\nlogit_model <- glm(Y ~ X, family = binomial(link = \"logit\"))\n\n# Define step size h for continuous variable\nX_mean <- mean(X)\nh <- (abs(X_mean) + 0.001) * 0.001\n\n# Compute predicted probabilities at original X\npred_Y0 <- predict(logit_model, type = \"response\")\n\n# Compute predicted probabilities at X + h\nX_new <- X + h\ndata_new <- data.frame(X = X_new)\npred_Y1 <-\n    predict(logit_model, newdata = data_new, type = \"response\")\n\n# Compute marginal effects\nmarginal_effects <- (pred_Y1 - pred_Y0) / h\n\n# Compute Average Marginal Effect (AME)\nAME_one_sided <- mean(marginal_effects)\n\n# Display results\ndata.frame(Method = \"One-Sided AME\", Estimate = AME_one_sided)\n#>          Method  Estimate\n#> 1 One-Sided AME 0.1921614"},{"path":"sec-marginal-effects.html","id":"sec-two-sided-numerical-derivative","chapter":"17 Marginal Effects","heading":"17.6.1.2 Two-Sided Numerical Derivative","text":"improve accuracy, use two-sided derivative:AlgorithmEstimate model using logistic (probit) regression.Estimate model using logistic (probit) regression.observation \\(\\):\nCompute original predicted probability:\\[\\hat{Y}_{i0} = p(\\mathbf{X}_i, \\hat{\\beta}).\\]\nCompute new predicted probabilities:\nIncrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\\]\nDecrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i2} = p(\\mathbf{X}_i - h, \\hat{\\beta}).\\]\n\nCompute numerical derivative: \\[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i2}}{2h}.\\]\nobservation \\(\\):Compute original predicted probability:\\[\\hat{Y}_{i0} = p(\\mathbf{X}_i, \\hat{\\beta}).\\]Compute original predicted probability:\\[\\hat{Y}_{i0} = p(\\mathbf{X}_i, \\hat{\\beta}).\\]Compute new predicted probabilities:\nIncrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\\]\nDecrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i2} = p(\\mathbf{X}_i - h, \\hat{\\beta}).\\]\nCompute new predicted probabilities:Increase \\(X\\) \\(h\\):\\[\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\\]Increase \\(X\\) \\(h\\):\\[\\hat{Y}_{i1} = p(\\mathbf{X}_i + h, \\hat{\\beta}).\\]Decrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i2} = p(\\mathbf{X}_i - h, \\hat{\\beta}).\\]Decrease \\(X\\) \\(h\\):\\[\\hat{Y}_{i2} = p(\\mathbf{X}_i - h, \\hat{\\beta}).\\]Compute numerical derivative: \\[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i2}}{2h}.\\]Compute numerical derivative: \\[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i2}}{2h}.\\]Average across observations: \\[\nE\\left[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i2}}{2h}\\right] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}.\n\\]Average across observations: \\[\nE\\left[\\frac{\\hat{Y}_{i1} - \\hat{Y}_{i2}}{2h}\\right] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}.\n\\]Comparison One-Sided vs. Two-Sided AMEOne-sided AME computationally simpler can introduce bias.One-sided AME computationally simpler can introduce bias.Two-sided AME reduces bias requires two function evaluations per observation.Two-sided AME reduces bias requires two function evaluations per observation.","code":"\n# Compute predicted probabilities at X - h\nX_new_minus <- X - h\ndata_new_minus <- data.frame(X = X_new_minus)\npred_Y2 <-\n    predict(logit_model, newdata = data_new_minus, type = \"response\")\n\n# Compute two-sided marginal effects\nmarginal_effects_2sided <- (pred_Y1 - pred_Y2) / (2 * h)\n\n# Compute Average Marginal Effect (AME) - Two-Sided\nAME_two_sided <- mean(marginal_effects_2sided)\n\n# Display results\ndata.frame(Method = \"Two-Sided AME\", Estimate = AME_two_sided)\n#>          Method  Estimate\n#> 1 Two-Sided AME 0.1921633"},{"path":"sec-marginal-effects.html","id":"sec-marginal-effects-at-the-mean","chapter":"17 Marginal Effects","heading":"17.6.2 Marginal Effects at the Mean","text":"Marginal effects nonlinear models constant, depend values independent variables. One way summarize computing Marginal Effects Mean (MEM), estimates marginal effects average values independent variables.MEM commonly used :Econometrics: Evaluating effect education wages average level experience.Econometrics: Evaluating effect education wages average level experience.Finance: Assessing impact credit scores loan approval probability typical applicant.Finance: Assessing impact credit scores loan approval probability typical applicant.Marketing: Estimating effect price purchase probability average customer.Marketing: Estimating effect price purchase probability average customer.Unlike Average Marginal Effect, averages marginal effects observations, MEM computes effect single point—mean explanatory variables.Let \\(p(\\mathbf{X}, \\beta)\\) predicted probability nonlinear model (e.g., logistic regression). MEM computed :\\[\n\\frac{\\partial p(\\bar{\\mathbf{X}}, \\beta)}{\\partial X}\n\\]\\(\\bar{\\mathbf{X}}\\) vector mean values explanatory variables.logistic regression model:\\[\nE[Y|X] = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]MEM continuous variable \\(X\\) :\\[\n\\frac{\\partial E[Y|X]}{\\partial X} \\Bigg|_{X = \\bar{X}} = \\beta_1 \\cdot p(\\bar{X}) \\cdot (1 - p(\\bar{X})).\n\\]MEM vs. AMEStep 1: Estimate ModelStep 2: Compute Marginal Effects MeanInterpretationThe MEM tells us effect \\(X\\) probability \\(Y=1\\) mean value \\(X\\).MEM tells us effect \\(X\\) probability \\(Y=1\\) mean value \\(X\\).provides simple interpretation may capture variability marginal effects across different values \\(X\\).provides simple interpretation may capture variability marginal effects across different values \\(X\\).third approach Marginal Effects Representative Values (MER), calculate marginal effects specific percentiles (e.g., median, quartiles). comparison:Use MethodUse MEM need quick, interpretable summary “average” individual.Use MEM need quick, interpretable summary “average” individual.Use AME marginal effects vary widely across individuals.Use AME marginal effects vary widely across individuals.Use MER need understand effects specific values interest.Use MER need understand effects specific values interest.","code":"\n# Load necessary packages\nlibrary(margins)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Simulate data\nset.seed(123)\nn <- 100\nX <- rnorm(n)\nY <- rbinom(n, 1, plogis(0.5 + 0.8 * X))  # Logistic function\n\n# Logistic regression\nlogit_model <- glm(Y ~ X, family = binomial(link = \"logit\"))\n# Compute mean of X\nX_mean <- mean(X)\n\n# Compute predicted probability at mean X\np_mean <-\n    predict(logit_model,\n            newdata = data.frame(X = X_mean),\n            type = \"response\")\n\n# Compute MEM for X\nMEM <- coef(logit_model)[\"X\"] * p_mean * (1 - p_mean)\n\n# Display result\ndata.frame(Method = \"MEM\", Estimate = MEM)\n#>   Method  Estimate\n#> X    MEM 0.2146628"},{"path":"sec-marginal-effects.html","id":"sec-marginal-effects-at-the-average","chapter":"17 Marginal Effects","heading":"17.6.3 Marginal Effects at the Average","text":"Marginal effects summarize independent variable influences probability outcome nonlinear models (e.g., logistic regression). already discussed:Marginal Effects Mean (MEM): Marginal effects computed mean values independent variables.Marginal Effects Mean (MEM): Marginal effects computed mean values independent variables.Average Marginal Effects (AME): mean marginal effects computed observation.Average Marginal Effects (AME): mean marginal effects computed observation.third approach Marginal Effects Average(MAE), first average independent variables across observations, compute marginal effect single averaged observation.Let \\(p(\\mathbf{X}, \\beta)\\) probability function model (e.g., logistic regression). Marginal Effect Average computed :\\[\n\\frac{\\partial p(\\bar{\\mathbf{X}}, \\beta)}{\\partial X}\n\\]\\(\\bar{\\mathbf{X}}\\) vector averaged independent variables across observations.Key Differences AME MAEAME answers general question: “\\(X\\) affect \\(Y\\) across entire dataset?”AME answers general question: “\\(X\\) affect \\(Y\\) across entire dataset?”MAE answers specific question: “\\(X\\) affect \\(Y\\) typical (average) person dataset?”MAE answers specific question: “\\(X\\) affect \\(Y\\) typical (average) person dataset?”MAE particularly relevant want single, interpretable effect representative individual.Use Cases MAEPolicy & Business Decision-Making: policymakers business leaders want know effect tax increase “typical” consumer, MAE gives effect single representative individual.Policy & Business Decision-Making: policymakers business leaders want know effect tax increase “typical” consumer, MAE gives effect single representative individual.Marketing Campaigns: marketing team wants know much increasing ad spend affects purchase probability “average” customer, MAE provides insight.Marketing Campaigns: marketing team wants know much increasing ad spend affects purchase probability “average” customer, MAE provides insight.Simplified Reporting: AMEs vary across individuals, can make reporting complex. MAE condenses everything one easy--interpret number.Simplified Reporting: AMEs vary across individuals, can make reporting complex. MAE condenses everything one easy--interpret number.Comparison: MAE vs. MEM vs. AMEIntuition Behind MAEInstead computing individual marginal effects (AME), MAE computes marginal effect single averaged observation.method somewhat similar MEM, instead taking mean independent variable separately, first computes single averaged observation derives marginal effect observation.Step 1: Estimate ModelStep 2: Compute MAEThe MAE \\(X_1\\) represents change probability increasing \\(X_1\\) average values \\(X_1\\) \\(X_2\\).MAE \\(X_1\\) represents change probability increasing \\(X_1\\) average values \\(X_1\\) \\(X_2\\).MAE \\(X_2\\) (binary variable) represents probability change switching \\(X_2 = 0\\) \\(X_2 = 1\\), holding variables average.MAE \\(X_2\\) (binary variable) represents probability change switching \\(X_2 = 0\\) \\(X_2 = 1\\), holding variables average.Quick interpretation reference point.individual means meaningful (e.g., symmetric data).Simple summary interactions exist.need single interpretable summary accounts interactionsWhen Use MAEWhen want single number summary reflects realistic scenario.want single number summary reflects realistic scenario.interaction effects, want account joint impact predictors.interaction effects, want account joint impact predictors.However, predictor distributions skewed, AME usually preferred.However, predictor distributions skewed, AME usually preferred.","code":"\n# Load necessary packages\nlibrary(margins)\n\n# Simulate data\nset.seed(123)\nn <- 100\nX1 <- rnorm(n)  # Continuous variable\nX2 <- rbinom(n, 1, 0.5)  # Binary variable\n# Logistic function\nY <-\n    rbinom(n, 1, plogis(0.5 + 0.8 * X1 - 0.5 * X2))  \n\n# Logistic regression\nlogit_model <- glm(Y ~ X1 + X2, family = binomial(link = \"logit\"))\n# Compute the average of independent variables\nX_mean <- data.frame(X1 = mean(X1), X2 = mean(X2))\n\n# Compute predicted probability at averaged X\np_mean <- predict(logit_model, newdata = X_mean, type = \"response\")\n\n# Compute MAE for X1\nMAE_X1 <- coef(logit_model)[\"X1\"] * p_mean * (1 - p_mean)\n\n# Compute MAE for X2\nMAE_X2 <- coef(logit_model)[\"X2\"] * p_mean * (1 - p_mean)\n\n# Display results\ndata.frame(\n    Method = \"MAE\",\n    Variable = c(\"X1\", \"X2\"),\n    Estimate = c(MAE_X1, MAE_X2)\n)\n#>    Method Variable    Estimate\n#> X1    MAE       X1  0.20280618\n#> X2    MAE       X2 -0.06286593"},{"path":"sec-marginal-effects.html","id":"packages-for-marginal-effects","chapter":"17 Marginal Effects","heading":"17.7 Packages for Marginal Effects","text":"Several R packages compute marginal effects regression models, different features functionalities. primary packages include:marginaleffects – modern, flexible, efficient package.margins – widely used package replicating Stata’s margins command.mfx – package tailored Generalized Linear Models (glm).tools help analyze small changes explanatory variables impact dependent variable.","code":""},{"path":"sec-marginal-effects.html","id":"marginaleffects-package-recommended","chapter":"17 Marginal Effects","heading":"17.7.1 marginaleffects Package (Recommended)","text":"marginaleffects package successor margins emtrends, offering faster, efficient, flexible approach estimating marginal effects.Use marginaleffects?Supports interaction effects complex modelsSupports interaction effects complex modelsComputes marginal effects, marginal means, counterfactualsComputes marginal effects, marginal means, counterfactualsIntegrates well ggplot2 visualizationIntegrates well ggplot2 visualizationWorks many model types (linear, logistic, Poisson, etc.)Works many model types (linear, logistic, Poisson, etc.)Limitation:built-function multiple comparisons correction, can use p.adjust() adjustment.Key DefinitionsMarginal Effects: partial derivative (slope) outcome respect variable.\nmargins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”\nmargins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”Marginal Means: expected outcome averaged grid predictor values.Computing Predictions Marginal EffectsComputing Marginal EffectsCounterfactual Comparisons","code":"\nlibrary(marginaleffects)\nlibrary(tidyverse)\ndata(mtcars)\n\n\n# Fit a regression model with interaction terms\nmod <- lm(mpg ~ hp * wt * am, data = mtcars)\n\n# Get predicted values\npredictions(mod) %>% head()\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      22.5      0.884 25.4   <0.001 471.7  20.8   24.2\n#>      20.8      1.194 17.4   <0.001 223.3  18.5   23.1\n#>      25.3      0.709 35.7   <0.001 922.7  23.9   26.7\n#>      20.3      0.704 28.8   <0.001 601.5  18.9   21.6\n#>      17.0      0.712 23.9   <0.001 416.2  15.6   18.4\n#>      19.7      0.875 22.5   <0.001 368.8  17.9   21.4\n#> \n#> Type:  response\n\n# Create a reference grid for prediction\nnewdata <- datagrid(am = 0,\n                    wt = c(2, 4),\n                    model = mod)\n\n# Plot predictions for 'hp' and 'wt'\nmarginaleffects::plot_predictions(mod, \n                                  newdata = newdata, \n                                  condition = c(\"hp\", \"wt\"))\n# Compute Average Marginal Effects (AME)\nmfx <- marginaleffects::slopes(mod, variables = c(\"hp\", \"wt\"))\nhead(mfx)\n#> \n#>  Estimate Std. Error     z Pr(>|z|)   S   2.5 %    97.5 %\n#>   -0.0369     0.0185 -2.00  0.04598 4.4 -0.0732 -0.000659\n#>   -0.0287     0.0156 -1.84  0.06630 3.9 -0.0593  0.001931\n#>   -0.0466     0.0226 -2.06  0.03945 4.7 -0.0909 -0.002249\n#>   -0.0423     0.0133 -3.18  0.00146 9.4 -0.0683 -0.016232\n#>   -0.0390     0.0134 -2.91  0.00363 8.1 -0.0653 -0.012730\n#>   -0.0387     0.0135 -2.87  0.00409 7.9 -0.0652 -0.012289\n#> \n#> Term: hp\n#> Type:  response \n#> Comparison: dY/dX\n\n# Compute Group-Average Marginal Effects\nhead(marginaleffects::slopes(mod, by = \"hp\", variables = \"am\"))\n#> \n#>  hp Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  52     3.98       5.20  0.764    0.445 1.2 -6.22  14.18\n#>  62    -2.77       2.51 -1.107    0.268 1.9 -7.68   2.14\n#>  65     3.00       4.13  0.725    0.468 1.1 -5.10  11.10\n#>  66     2.03       3.48  0.582    0.561 0.8 -4.80   8.85\n#>  91     1.86       2.76  0.674    0.500 1.0 -3.54   7.26\n#>  93     1.20       2.35  0.511    0.609 0.7 -3.40   5.80\n#> \n#> Term: am\n#> Type:  response \n#> Comparison: 1 - 0\n\n# Marginal Effects at Representative Values (MER)\nmarginaleffects::slopes(mod, newdata = datagrid(am = 0, wt = c(2, 4)))\n#> \n#>  Term Contrast am wt Estimate Std. Error      z Pr(>|z|)   S   2.5 %   97.5 %\n#>    am    1 - 0  0  2   2.5465     2.7860  0.914   0.3607 1.5 -2.9139  8.00694\n#>    am    1 - 0  0  4  -2.9661     3.0381 -0.976   0.3289 1.6 -8.9207  2.98852\n#>    hp    dY/dX  0  2  -0.0598     0.0283 -2.115   0.0344 4.9 -0.1153 -0.00439\n#>    hp    dY/dX  0  4  -0.0309     0.0187 -1.654   0.0981 3.4 -0.0676  0.00571\n#>    wt    dY/dX  0  2  -2.6762     1.4199 -1.885   0.0595 4.1 -5.4591  0.10676\n#>    wt    dY/dX  0  4  -2.6762     1.4206 -1.884   0.0596 4.1 -5.4605  0.10816\n#> \n#> Type:  response\n\n# Marginal Effects at the Mean (MEM)\nmarginaleffects::slopes(mod, newdata = \"mean\")\n#> \n#>  Term Contrast Estimate Std. Error      z Pr(>|z|)   S   2.5 %  97.5 %\n#>    am    1 - 0  -0.8086     1.5238 -0.531  0.59568 0.7 -3.7952  2.1781\n#>    hp    dY/dX  -0.0422     0.0133 -3.181  0.00147 9.4 -0.0683 -0.0162\n#>    wt    dY/dX  -2.6762     1.4193 -1.886  0.05935 4.1 -5.4579  0.1055\n#> \n#> Type:  response\n# Counterfactual comparison: Effect of changing 'am' from 0 to 1\ncomparisons(mod, variables = list(am = 0:1))\n#> \n#>  Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>     0.325       1.68  0.193    0.847 0.2 -2.97   3.62\n#>    -0.544       1.57 -0.347    0.729 0.5 -3.62   2.53\n#>     1.201       2.35  0.511    0.609 0.7 -3.40   5.80\n#>    -1.703       1.87 -0.912    0.362 1.5 -5.36   1.96\n#>    -0.615       1.68 -0.366    0.715 0.5 -3.91   2.68\n#> --- 22 rows omitted. See ?print.marginaleffects --- \n#>     4.081       3.94  1.037    0.300 1.7 -3.63  11.79\n#>     2.106       2.29  0.920    0.358 1.5 -2.38   6.59\n#>     0.895       1.64  0.544    0.586 0.8 -2.33   4.12\n#>     4.027       3.24  1.243    0.214 2.2 -2.32  10.38\n#>    -0.237       1.59 -0.149    0.881 0.2 -3.35   2.87\n#> Term: am\n#> Type:  response \n#> Comparison: 1 - 0"},{"path":"sec-marginal-effects.html","id":"margins-package","chapter":"17 Marginal Effects","heading":"17.7.2 margins Package","text":"margins package popular choice, designed replicate Stata’s margins command R. provides marginal effects variable model, including interaction terms. margins define:Average Partial Effects: contribution variable outcome scale, conditional variables involved link function transformation linear predictor.Average Marginal Effects: marginal contribution variable scale linear predictor.Average marginal effects represent mean unit-specific partial derivatives given sample.Key FeaturesComputes Average Partial Effects (APE).Computes Average Partial Effects (APE).Supports Marginal Effects Mean (MEM).Supports Marginal Effects Mean (MEM).Provides visualizations marginal effects.Provides visualizations marginal effects.Limitation:Slower marginaleffects, especially large datasets.Marginal Effects Representative Values","code":"\nlibrary(margins)\n\n# Fit a linear model with interactions\nmod <- lm(mpg ~ cyl * hp + wt, data = mtcars)\n\n# Compute marginal effects\nsummary(margins(mod))\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n\n# Equivalent function for summary\nmargins_summary(mod)\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n\n# Plot marginal effects\nplot(margins(mod))\n# Compute marginal effects when 'hp' = 150\nmargins(mod, at = list(hp = 150)) %>% summary()\n#>  factor       hp     AME     SE       z      p   lower   upper\n#>     cyl 150.0000  0.1009 0.6128  0.1647 0.8692 -1.1001  1.3019\n#>      hp 150.0000 -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt 150.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236"},{"path":"sec-marginal-effects.html","id":"mfx-package","chapter":"17 Marginal Effects","heading":"17.7.3 mfx Package","text":"mfx package specialized GLMs, computing marginal effects probit, logit, Poisson, count models.Limitation:Computes marginal effects variable, average marginal effect.Supported Models mfxExample: Poisson RegressionFor details, see mfx vignette.","code":"\nlibrary(mfx)\ndata(\"mtcars\")\n\n# Fit a Poisson model and compute marginal effects\npoissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> Call:\n#> poissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> \n#> Marginal Effects:\n#>                    dF/dx   Std. Err.       z  P>|z|\n#> mpg           1.4722e-03  8.7531e-03  0.1682 0.8664\n#> cyl           6.6420e-03  3.9263e-02  0.1692 0.8657\n#> disp          1.5899e-04  9.4555e-04  0.1681 0.8665\n#> mpg:cyl      -3.4698e-04  2.0564e-03 -0.1687 0.8660\n#> mpg:disp     -7.6794e-06  4.5545e-05 -0.1686 0.8661\n#> cyl:disp     -3.3837e-05  1.9919e-04 -0.1699 0.8651\n#> mpg:cyl:disp  1.6812e-06  9.8919e-06  0.1700 0.8650"},{"path":"sec-marginal-effects.html","id":"comparison-of-packages","chapter":"17 Marginal Effects","heading":"17.7.4 Comparison of Packages","text":"","code":""},{"path":"moderation.html","id":"moderation","chapter":"18 Moderation","heading":"18 Moderation","text":"Moderation analysis examines relationship independent variable (\\(X\\)) dependent variable (\\(Y\\)) changes depending third variable, moderator (\\(M\\)). regression terms, moderation represented interaction effect.","code":""},{"path":"moderation.html","id":"types-of-moderation-analyses","chapter":"18 Moderation","heading":"18.1 Types of Moderation Analyses","text":"two primary approaches analyzing moderation:1. Spotlight AnalysisAlso known Simple Slopes Analysis.Examines effect \\(X\\) \\(Y\\) specific values \\(M\\) (e.g., mean, \\(\\pm 1\\) SD, percentiles).Typically used categorical discretized moderators.2. Floodlight AnalysisExtends spotlight analysis examine moderation across entire range \\(M\\).Based Johnson-Neyman Intervals, identifying values \\(M\\) effect \\(X\\) \\(Y\\) statistically significant.Useful moderator continuous specific cutoffs predefined.","code":""},{"path":"moderation.html","id":"key-terminology","chapter":"18 Moderation","heading":"18.2 Key Terminology","text":"Main Effect: effect independent variable without considering interactions.Interaction Effect: combined effect \\(X\\) \\(M\\) \\(Y\\).Simple Slope: slope \\(X\\) \\(Y\\) specific value \\(M\\) (used \\(M\\) continuous).Simple Effect: effect \\(X\\) \\(Y\\) particular level \\(M\\) \\(X\\) categorical.","code":""},{"path":"moderation.html","id":"moderation-model","chapter":"18 Moderation","heading":"18.3 Moderation Model","text":"typical moderation model represented :\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 M + \\beta_3 X \\times M + \\varepsilon\n\\]:\\(\\beta_0\\): Intercept\\(\\beta_0\\): Intercept\\(\\beta_1\\): Main effect \\(X\\)\\(\\beta_1\\): Main effect \\(X\\)\\(\\beta_2\\): Main effect \\(M\\)\\(\\beta_2\\): Main effect \\(M\\)\\(\\beta_3\\): Interaction effect \\(X\\) \\(M\\)\\(\\beta_3\\): Interaction effect \\(X\\) \\(M\\)\\(\\beta_3\\) significant, suggests effect \\(X\\) \\(Y\\) depends \\(M\\).","code":""},{"path":"moderation.html","id":"types-of-interactions","chapter":"18 Moderation","heading":"18.4 Types of Interactions","text":"Continuous Continuous: \\(X\\) \\(M\\) continuous (e.g., age moderating effect income spending).Continuous Categorical: \\(X\\) continuous, \\(M\\) categorical (e.g., gender moderating effect education salary).Categorical Categorical: \\(X\\) \\(M\\) categorical (e.g., effect training program performance, moderated job role).","code":""},{"path":"moderation.html","id":"three-way-interactions","chapter":"18 Moderation","heading":"18.5 Three-Way Interactions","text":"models second moderator (\\(W\\)), examine:\\[\n\\begin{aligned}\nY &= \\beta_0 + \\beta_1 X + \\beta_2 M + \\beta_3 W\n+ \\beta_4 X \\times M \\\\\n&+ \\beta_5 X \\times W + \\beta_6 M \\times W + \\beta_7 X \\times M \\times W + \\varepsilon\n\\end{aligned}\n\\]interpret three-way interactions, slope difference test can used (Dawson Richter 2006).","code":""},{"path":"moderation.html","id":"additional-resources","chapter":"18 Moderation","heading":"18.6 Additional Resources","text":"Bayesian ANOVA models: BANOVAL package allows floodlight analysis.Structural Equation Modeling: cSEM package includes doFloodlightAnalysis.details, refer (Spiller et al. 2013).","code":""},{"path":"moderation.html","id":"application-2","chapter":"18 Moderation","heading":"18.7 Application","text":"","code":""},{"path":"moderation.html","id":"emmeans-package","chapter":"18 Moderation","heading":"18.7.1 emmeans Package","text":"emmeans package (Estimated Marginal Means) powerful tool post-hoc analysis linear models, enabling researchers explore interaction effects simple slopes estimated marginal means.install load package:dataset used section sourced UCLA Statistical Consulting Group, :gender (male, female) prog (exercise program: jogging, swimming, reading) categorical variables.gender (male, female) prog (exercise program: jogging, swimming, reading) categorical variables.loss represents weight loss, hours effort continuous predictors.loss represents weight loss, hours effort continuous predictors.","code":"\ninstall.packages(\"emmeans\")\nlibrary(tidyverse)\ndat <- readRDS(\"data/exercise.rds\") %>%\n    mutate(prog = factor(prog, labels = c(\"jog\", \"swim\", \"read\"))) %>%\n    mutate(gender = factor(gender, labels = c(\"male\", \"female\")))"},{"path":"moderation.html","id":"continuous-by-continuous-interaction","chapter":"18 Moderation","heading":"18.7.1.1 Continuous by Continuous Interaction","text":"begin interaction model two continuous variables: hours (exercise duration) effort (self-reported effort level).","code":"\ncontcont <- lm(loss ~ hours * effort, data = dat)\nsummary(contcont)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * effort, data = dat)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -29.52 -10.60  -1.78  11.13  34.51 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   7.79864   11.60362   0.672   0.5017  \n#> hours        -9.37568    5.66392  -1.655   0.0982 .\n#> effort       -0.08028    0.38465  -0.209   0.8347  \n#> hours:effort  0.39335    0.18750   2.098   0.0362 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 13.56 on 896 degrees of freedom\n#> Multiple R-squared:  0.07818,    Adjusted R-squared:  0.07509 \n#> F-statistic: 25.33 on 3 and 896 DF,  p-value: 9.826e-16"},{"path":"moderation.html","id":"simple-slopes-analysis-spotlight-analysis","chapter":"18 Moderation","heading":"18.7.1.1.1 Simple Slopes Analysis (Spotlight Analysis)","text":"Following Aiken West (2005), spotlight analysis examines effect hours loss three levels effort:Mean effort plus one standard deviationMean effort plus one standard deviationMean effortMean effortMean effort minus one standard deviationMean effort minus one standard deviationThe three p-values obtained correspond interaction term regression model.professional figure, refine visualization using ggplot2:","code":"\nlibrary(emmeans)\neffar <- round(mean(dat$effort) + sd(dat$effort), 1)\neffr  <- round(mean(dat$effort), 1)\neffbr <- round(mean(dat$effort) - sd(dat$effort), 1)\n\n\n\n# Define values for estimation\nmylist <- list(effort = c(effbr, effr, effar))\n\n# Compute simple slopes\nemtrends(contcont, ~ effort, var = \"hours\", at = mylist)\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.350 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.310 896    1.745     6.88\n#> \n#> Confidence level used: 0.95\n\n# Visualization of the interaction\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\nemmip(contcont, effort ~ hours, at = mylist, CIs = TRUE)\n\n# Test statistical differences in slopes\nemtrends(\n    contcont,\n    pairwise ~ effort,\n    var = \"hours\",\n    at = mylist,\n    adjust = \"none\"\n)\n#> $emtrends\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.350 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.310 896    1.745     6.88\n#> \n#> Results are averaged over the levels of: hours \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast                estimate    SE  df t.ratio p.value\n#>  effort24.5 - effort29.7    -2.05 0.975 896  -2.098  0.0362\n#>  effort24.5 - effort34.8    -4.05 1.930 896  -2.098  0.0362\n#>  effort29.7 - effort34.8    -2.01 0.956 896  -2.098  0.0362\n#> \n#> Results are averaged over the levels of: hours\nlibrary(ggplot2)\n\n# Prepare data for plotting\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\ncontcontdat <-\n    emmip(contcont,\n          effort ~ hours,\n          at = mylist,\n          CIs = TRUE,\n          plotit = FALSE)\n\n# Convert effort levels to factors\ncontcontdat$feffort <- factor(contcontdat$effort)\nlevels(contcontdat$feffort) <- c(\"low\", \"medium\", \"high\")\n\n# Generate plot\np  <-\n    ggplot(data = contcontdat,\n           aes(x = hours, y = yvar, color = feffort)) +\n    geom_line()\n\np1 <-\n    p +\n    geom_ribbon(aes(ymax = UCL, ymin = LCL, fill = feffort),\n                alpha = 0.4)\n\np1  + labs(x = \"Exercise Hours\",\n           y = \"Weight Loss\",\n           color = \"Effort\",\n           fill = \"Effort Level\")"},{"path":"moderation.html","id":"continuous-by-categorical-interaction","chapter":"18 Moderation","heading":"18.7.1.2 Continuous by Categorical Interaction","text":"Next, examine interaction hours (continuous) interacts gender (categorical). set “Female” reference category:Simple Slopes GenderSince test equivalent interaction term regression model, significant result confirms moderating effect gender.","code":"\ndat$gender <- relevel(dat$gender, ref = \"female\")\ncontcat <- lm(loss ~ hours * gender, data = dat)\nsummary(contcat)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * gender, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -27.118 -11.350  -1.963  10.001  42.376 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)         3.335      2.731   1.221    0.222  \n#> hours               3.315      1.332   2.489    0.013 *\n#> gendermale          3.571      3.915   0.912    0.362  \n#> hours:gendermale   -1.724      1.898  -0.908    0.364  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 14.06 on 896 degrees of freedom\n#> Multiple R-squared:  0.008433,   Adjusted R-squared:  0.005113 \n#> F-statistic:  2.54 on 3 and 896 DF,  p-value: 0.05523\n# Compute simple slopes for each gender\nemtrends(contcat, ~ gender, var = \"hours\")\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95\n\n# Test slope differences\nemtrends(contcat, pairwise ~ gender, var = \"hours\")\n#> $emtrends\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast      estimate  SE  df t.ratio p.value\n#>  female - male     1.72 1.9 896   0.908  0.3639\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               gender = c(\"female\", \"male\"))\nemmip(contcat, gender ~ hours, at = mylist, CIs = TRUE)"},{"path":"moderation.html","id":"categorical-by-categorical-interaction","chapter":"18 Moderation","heading":"18.7.1.3 Categorical by Categorical Interaction","text":"Now, examine interaction two categorical variables: gender (male, female) prog (exercise program). set “Read” reference category prog “Female” gender:Simple Effects Contrast AnalysisFor intuitive presentation, use bar graph error bars","code":"\ndat$prog   <- relevel(dat$prog, ref = \"read\")\ndat$gender <- relevel(dat$gender, ref = \"female\")\n\ncatcat <- lm(loss ~ gender * prog, data = dat)\nsummary(catcat)\n#> \n#> Call:\n#> lm(formula = loss ~ gender * prog, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -19.1723  -4.1894  -0.0994   3.7506  27.6939 \n#> \n#> Coefficients:\n#>                     Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          -3.6201     0.5322  -6.802 1.89e-11 ***\n#> gendermale           -0.3355     0.7527  -0.446    0.656    \n#> progjog               7.9088     0.7527  10.507  < 2e-16 ***\n#> progswim             32.7378     0.7527  43.494  < 2e-16 ***\n#> gendermale:progjog    7.8188     1.0645   7.345 4.63e-13 ***\n#> gendermale:progswim  -6.2599     1.0645  -5.881 5.77e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.519 on 894 degrees of freedom\n#> Multiple R-squared:  0.7875, Adjusted R-squared:  0.7863 \n#> F-statistic: 662.5 on 5 and 894 DF,  p-value: < 2.2e-16\n# Estimated marginal means for all combinations of gender and program\nemcatcat <- emmeans(catcat, ~ gender * prog)\n\n# Compare effects of gender within each program\ncontrast(emcatcat, \"revpairwise\", by = \"prog\", adjust = \"bonferroni\")\n#> prog = read:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -0.335 0.753 894  -0.446  0.6559\n#> \n#> prog = jog:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female    7.483 0.753 894   9.942  <.0001\n#> \n#> prog = swim:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -6.595 0.753 894  -8.762  <.0001\nemmip(catcat, prog ~ gender, CIs = TRUE)\n# Prepare data\ncatcatdat <- emmip(catcat,\n                   gender ~ prog,\n                   CIs = TRUE,\n                   plotit = FALSE)\n\n# Generate plot\np <-\n    ggplot(data = catcatdat,\n           aes(x = prog, y = yvar, fill = gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")\n\np1 <-\n    p + geom_errorbar(\n        position = position_dodge(.9),\n        width = .25,\n        aes(ymax = UCL, ymin = LCL),\n        alpha = 0.3\n    )\n\np1  + labs(x = \"Exercise Program\",\n           y = \"Weight Loss\",\n           fill = \"Gender\")"},{"path":"moderation.html","id":"probemod-package","chapter":"18 Moderation","heading":"18.7.2 probemod Package","text":"probemod package designed moderation analysis, particularly focusing Johnson-Neyman intervals simple slopes analysis. However, package recommended due known issues subscript handling formatting errors outputs.Johnson-Neyman technique identifies values moderator (gender) effect independent variable (hours) dependent variable (loss) statistically significant. method particularly useful moderator continuous can also applied categorical moderators.Example: J-N Analysis loss ~ hours * gender ModelThe jn() function computes Johnson-Neyman intervals, highlighting values gender relationship hours loss statistically significant.Pick--Point method tests simple effect hours specific values gender, akin spotlight analysis.","code":"\ninstall.packages(\"probemod\", dependencies = T)\nlibrary(probemod)\n\nmyModel <-\n    lm(loss ~ hours * gender, data = dat %>% \n           select(loss, hours, gender))\n\njnresults <- jn(myModel,\n                dv = 'loss',\n                iv = 'hours',\n                mod = 'gender')\npickapoint(\n    myModel,\n    dv = 'loss',\n    iv = 'hours',\n    mod = 'gender',\n    alpha = .01\n)\n\nplot(jnresults)"},{"path":"moderation.html","id":"interactions-package","chapter":"18 Moderation","heading":"18.7.3 interactions Package","text":"interactions package recommended tool visualizing interpreting interaction effects regression models. provides user-friendly functions interaction plots, simple slopes analysis, Johnson-Neyman intervals, making excellent choice moderation analysis.","code":"\ninstall.packages(\"interactions\")"},{"path":"moderation.html","id":"continuous-by-continuous-interaction-1","chapter":"18 Moderation","heading":"18.7.3.1 Continuous by Continuous Interaction","text":"section covers interactions least one two variables continuous.Example: Interaction Illiteracy MurderWe use state.x77 dataset explore Illiteracy Rate Murder Rate interact predict Income across U.S. states.continuous moderators, standard values chosen visualization :Mean + 1 SDMean + 1 SDMeanMeanMean - 1 SDMean - 1 SDThe interact_plot() function provides easy way visualize effects.model includes weights, can incorporated visualizationA partial effect plot shows effect one variable changes across different levels another variable controlling predictors.check whether interaction truly linear, can compare fitted lines based :whole sample (black line)whole sample (black line)Subsamples based moderator (red line)Subsamples based moderator (red line)","code":"\nstates <- as.data.frame(state.x77)\nfiti <- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)\nsummary(fiti)\n#> \n#> Call:\n#> lm(formula = Income ~ Illiteracy * Murder + `HS Grad`, data = states)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -916.27 -244.42   28.42  228.14 1221.16 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        1414.46     737.84   1.917  0.06160 .  \n#> Illiteracy          753.07     385.90   1.951  0.05724 .  \n#> Murder              130.60      44.67   2.923  0.00540 ** \n#> `HS Grad`            40.76      10.92   3.733  0.00053 ***\n#> Illiteracy:Murder   -97.04      35.86  -2.706  0.00958 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 459.5 on 45 degrees of freedom\n#> Multiple R-squared:  0.4864, Adjusted R-squared:  0.4407 \n#> F-statistic: 10.65 on 4 and 45 DF,  p-value: 3.689e-06\nlibrary(interactions)\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              \n              # Disable automatic mean-centering\n              centered = \"none\", \n              \n              # Exclude the mean value of the moderator\n              # modx.values = \"plus-minus\", \n              \n              # Divide the moderator's distribution into three groups\n              # modx.values = \"terciles\", \n              \n              plot.points = TRUE, # Overlay raw data\n              \n              # Different shapes for different levels of the moderator\n              point.shape = TRUE, \n              \n              # Jittering to prevent overplotting\n              jitter = 0.1, \n              \n              # Custom appearance\n              x.label = \"Illiteracy Rate (%)\", \n              y.label = \"Income ($)\", \n              main.title = \"Interaction Between Illiteracy and Murder Rate\",\n              legend.main = \"Murder Rate Levels\",\n              colors = \"blue\",\n              \n              # Confidence bands\n              interval = TRUE, \n              int.width = 0.9, \n              robust = TRUE # Use robust standard errors\n              ) \nfiti <- lm(Income ~ Illiteracy * Murder,\n           data = states,\n           weights = Population)\n\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              plot.points = TRUE)\nlibrary(ggplot2)\ndata(cars)\n\nfitc <- lm(cty ~ year + cyl * displ + class + fl + drv, \n           data = mpg)\nsummary(fitc)\n#> \n#> Call:\n#> lm(formula = cty ~ year + cyl * displ + class + fl + drv, data = mpg)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.9772 -0.7164  0.0018  0.7690  5.9314 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     -200.97599   47.00954  -4.275 2.86e-05 ***\n#> year               0.11813    0.02347   5.033 1.01e-06 ***\n#> cyl               -1.85648    0.27745  -6.691 1.86e-10 ***\n#> displ             -3.56467    0.65943  -5.406 1.70e-07 ***\n#> classcompact      -2.60177    0.92972  -2.798 0.005597 ** \n#> classmidsize      -2.62996    0.93273  -2.820 0.005253 ** \n#> classminivan      -4.40817    1.03844  -4.245 3.24e-05 ***\n#> classpickup       -4.37322    0.93416  -4.681 5.02e-06 ***\n#> classsubcompact   -2.38384    0.92943  -2.565 0.010997 *  \n#> classsuv          -4.27352    0.86777  -4.925 1.67e-06 ***\n#> fld                6.34343    1.69499   3.742 0.000233 ***\n#> fle               -4.57060    1.65992  -2.754 0.006396 ** \n#> flp               -1.91733    1.58649  -1.209 0.228158    \n#> flr               -0.78873    1.56551  -0.504 0.614901    \n#> drvf               1.39617    0.39660   3.520 0.000525 ***\n#> drvr               0.48740    0.46113   1.057 0.291694    \n#> cyl:displ          0.36206    0.07934   4.564 8.42e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.526 on 217 degrees of freedom\n#> Multiple R-squared:  0.8803, Adjusted R-squared:  0.8715 \n#> F-statistic: 99.73 on 16 and 217 DF,  p-value: < 2.2e-16\n\ninteract_plot(\n    fitc,\n    pred = displ,\n    modx = cyl,\n    \n    # Show observed data as partial residuals\n    partial.residuals = TRUE, \n    \n    # Specify moderator values manually\n    modx.values = c(4, 5, 6, 8)\n)\n# Generate synthetic data\nx_2 <- runif(n = 200, min = -3, max = 3)\nw   <- rbinom(n = 200, size = 1, prob = 0.5)\nerr <- rnorm(n = 200, mean = 0, sd = 4)\ny_2 <- 2.5 - x_2 ^ 2 - 5 * w + 2 * w * (x_2 ^ 2) + err\n\ndata_2 <- as.data.frame(cbind(x_2, y_2, w))\n\n# Fit interaction model\nmodel_2 <- lm(y_2 ~ x_2 * w, data = data_2)\nsummary(model_2)\n#> \n#> Call:\n#> lm(formula = y_2 ~ x_2 * w, data = data_2)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -14.554  -3.050   0.065   2.782  13.384 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.42495    0.42785   0.993   0.3218  \n#> x_2         -0.56419    0.25426  -2.219   0.0276 *\n#> w           -0.02538    0.66809  -0.038   0.9697  \n#> x_2:w        0.54260    0.39873   1.361   0.1751  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.641 on 196 degrees of freedom\n#> Multiple R-squared:  0.02454,    Adjusted R-squared:  0.009605 \n#> F-statistic: 1.643 on 3 and 196 DF,  p-value: 0.1807\n\n# Linearity check plot\ninteract_plot(\n    model_2,\n    pred = x_2,\n    modx = w,\n    linearity.check = TRUE,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"simple-slopes-analysis","chapter":"18 Moderation","heading":"18.7.3.1.1 Simple Slopes Analysis","text":"simple slopes analysis examines conditional effect independent variable (\\(X\\)) specific levels moderator (\\(M\\)).sim_slopes() Works:Continuous moderators: Analyzes effects mean ±1 SD.Continuous moderators: Analyzes effects mean ±1 SD.Categorical moderators: Uses factor levels.Categorical moderators: Uses factor levels.Mean-centers variables except predictor interest.Mean-centers variables except predictor interest.Example: Continuous Continuous InteractionWe can also visualize simple slopesFor publication-quality results, convert simple slopes analysis table using huxtable.","code":"\nsim_slopes(fiti,\n           pred = Illiteracy,\n           modx = Murder,\n           johnson_neyman = FALSE)\n#> SIMPLE SLOPES ANALYSIS\n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -17.43   250.08    -0.07   0.94\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -399.64   178.86    -2.23   0.03\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -781.85   189.11    -4.13   0.00\n# Store results\nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\n\n# Plot the slopes\nplot(ss)\nlibrary(huxtable)\n\nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\n\n# Convert to a formatted table\nas_huxtable(ss)"},{"path":"moderation.html","id":"johnson-neyman-intervals","chapter":"18 Moderation","heading":"18.7.3.1.2 Johnson-Neyman Intervals","text":"Johnson-Neyman technique identifies range moderator (\\(M\\)) effect predictor (\\(X\\)) dependent variable (\\(Y\\)) statistically significant. approach useful moderator continuous, allowing us determine effect exists rather arbitrarily choosing values.Although J-N approach widely used (P. O. Johnson Neyman 1936), known inflated Type error rates (Bauer Curran 2005). correction method proposed (Esarey Sumner 2018) address issues.Since J-N performs multiple comparisons across values moderator, inflates Type error. control , use False Discovery Rate correction.Example: Johnson-Neyman AnalysisTo visualize J-N intervalsThe y-axis represents conditional slope predictor (\\(X\\)).x-axis represents values moderator (\\(M\\)).x-axis represents values moderator (\\(M\\)).shaded region represents range effect \\(X\\) \\(Y\\) statistically significant.shaded region represents range effect \\(X\\) \\(Y\\) statistically significant.","code":"\nsim_slopes(\n    fiti,\n    pred = Illiteracy,\n    modx = Murder,\n    johnson_neyman = TRUE,\n    control.fdr = TRUE,  # Correction for Type I and II errors\n    \n    # Include conditional intercepts\n    # cond.int = TRUE, \n    \n    robust = \"HC3\",  # Use robust SE\n    \n    # Don't mean-center non-focal variables\n    # centered = \"none\",\n    \n    jnalpha = 0.05  # Significance level\n)\n#> JOHNSON-NEYMAN INTERVAL\n#> \n#> When Murder is OUTSIDE the interval [-7.87, 8.51], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.35 \n#> \n#> SIMPLE SLOPES ANALYSIS\n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -17.43   227.37    -0.08   0.94\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -399.64   158.77    -2.52   0.02\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -781.85   156.96    -4.98   0.00\njohnson_neyman(fiti,\n               pred = Illiteracy,\n               modx = Murder,\n               control.fdr = TRUE, # Corrects for Type I error\n               alpha = .05)\n#> JOHNSON-NEYMAN INTERVAL\n#> \n#> When Murder is OUTSIDE the interval [-22.57, 8.52], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.33"},{"path":"moderation.html","id":"three-way-interactions-1","chapter":"18 Moderation","heading":"18.7.3.1.3 Three-Way Interactions","text":"three-way interactions, effect \\(X\\) \\(Y\\) depends two moderators (\\(M_1\\) \\(M_2\\)). allows nuanced understanding moderation effects.Example: 3-Way Interaction Visualization","code":"\nlibrary(jtools)\n# Convert 'cyl' to factor\nmtcars$cyl <- factor(mtcars$cyl,\n                     labels = c(\"4 cylinder\", \"6 cylinder\", \"8 cylinder\"))\n\n# Fit the model\nfitc3 <- lm(mpg ~ hp * wt * cyl, data = mtcars)\n\n# Plot interaction\ninteract_plot(fitc3,\n              pred = hp,\n              modx = wt,\n              mod2 = cyl) +\n    theme_apa(legend.pos = \"bottomright\")"},{"path":"moderation.html","id":"johnson-neyman-for-three-way-interaction","chapter":"18 Moderation","heading":"18.7.3.1.4 Johnson-Neyman for Three-Way Interaction","text":"Johnson-Neyman technique can also applied three-way interaction contextTo present results publication-ready format, generate tables plots","code":"\nlibrary(survey)\ndata(api)\n\n# Define survey design\ndstrat <- svydesign(\n    id = ~ 1,\n    strata = ~ stype,\n    weights = ~ pw,\n    data = apistrat,\n    fpc = ~ fpc\n)\n\n# Fit 3-way interaction model\nregmodel3 <-\n    survey::svyglm(api00 ~ avg.ed * growth * enroll, design = dstrat)\n\n# Johnson-Neyman analysis with visualization\nsim_slopes(\n    regmodel3,\n    pred = growth,\n    modx = avg.ed,\n    mod2 = enroll,\n    jnplot = TRUE\n)\n#> ███████████████ While enroll (2nd moderator) =  153.0518 (- 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL\n#> \n#> When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS\n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   1.25   0.32     3.86   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.39   0.22     1.75   0.08\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.48   0.35    -1.37   0.17\n#> \n#> ████████████████ While enroll (2nd moderator) =  595.2821 (Mean) ███████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL\n#> \n#> When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS\n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.72   0.22     3.29   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.34   0.16     2.16   0.03\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.04   0.24    -0.16   0.87\n#> \n#> ███████████████ While enroll (2nd moderator) = 1037.5125 (+ 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL\n#> \n#> The Johnson-Neyman interval could not be found. Is the p value for your\n#> interaction term below the specified alpha?\n#> \n#> SIMPLE SLOPES ANALYSIS\n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.18   0.31     0.58   0.56\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.29   0.20     1.49   0.14\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.40   0.27     1.49   0.14\nss3 <-\n    sim_slopes(regmodel3,\n               pred = growth,\n               modx = avg.ed,\n               mod2 = enroll)\n\n# Plot results\nplot(ss3)\n\n# Convert results into a formatted table\nlibrary(huxtable)\nas_huxtable(ss3)"},{"path":"moderation.html","id":"categorical-interactions","chapter":"18 Moderation","heading":"18.7.3.2 Categorical Interactions","text":"Interactions categorical predictors can visualized using categorical plots.Example: Interaction cyl, fwd, autoLine Plot Categorical InteractionBar Plot Representation","code":"\nlibrary(ggplot2)\n\n# Convert variables to factors\nmpg2 <- mpg %>%\n    mutate(cyl = factor(cyl))\n\nmpg2[\"auto\"] <- \"auto\"\nmpg2$auto[mpg2$trans %in% c(\"manual(m5)\", \"manual(m6)\")] <- \"manual\"\nmpg2$auto <- factor(mpg2$auto)\n\nmpg2[\"fwd\"] <- \"2wd\"\nmpg2$fwd[mpg2$drv == \"4\"] <- \"4wd\"\nmpg2$fwd <- factor(mpg2$fwd)\n\n# Drop cars with 5 cylinders (since most have 4, 6, or 8)\nmpg2 <- mpg2[mpg2$cyl != \"5\",]\n\n# Fit the model\nfit3 <- lm(cty ~ cyl * fwd * auto, data = mpg2)\n\nlibrary(jtools) # For summ()\nsumm(fit3)\ncat_plot(fit3,\n         pred = cyl,\n         modx = fwd,\n         plot.points = TRUE)\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"line\",\n    point.shape = TRUE,\n    vary.lty = TRUE\n)\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"bar\",\n    interval = TRUE,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"interactionr-package","chapter":"18 Moderation","heading":"18.7.4 interactionR Package","text":"interactionR package designed publication-quality reporting interaction effects, particularly epidemiology social sciences. provides tools computing interaction measures, confidence intervals, statistical inference following well-established methodologies.Key Features:Publication-Ready Interaction AnalysisConfidence intervals calculated using:\nDelta method (Hosmer Lemeshow 1992)\nVariance recovery (“mover”) method (G. Y. Zou 2008)\nBootstrapping (Assmann et al. 1996)\nDelta method (Hosmer Lemeshow 1992)Variance recovery (“mover”) method (G. Y. Zou 2008)Bootstrapping (Assmann et al. 1996)Standardized reporting guidelines based (Knol VanderWeele 2012).","code":"\ninstall.packages(\"interactionR\", dependencies = T)"},{"path":"moderation.html","id":"sjplot-package","chapter":"18 Moderation","heading":"18.7.5 sjPlot Package","text":"sjPlot package highly recommended publication-quality visualizations interaction effects. provides enhanced aesthetics customizable interaction plots suitable academic journals.details: sjPlot interaction visualization","code":"\ninstall.packages(\"sjPlot\")"},{"path":"moderation.html","id":"summary-of-moderation-analysis-packages","chapter":"18 Moderation","heading":"18.7.6 Summary of Moderation Analysis Packages","text":"","code":""},{"path":"mediation.html","id":"mediation","chapter":"19 Mediation","heading":"19 Mediation","text":"","code":""},{"path":"mediation.html","id":"traditional-approach","chapter":"19 Mediation","heading":"19.1 Traditional Approach","text":"classical mediation analysis follows approach introduced Baron Kenny (1986), though limitations, particularly requiring first step (\\(X \\Y\\)) significant. Despite shortcomings, framework provides useful foundation.","code":""},{"path":"mediation.html","id":"steps-in-the-traditional-mediation-model","chapter":"19 Mediation","heading":"19.1.1 Steps in the Traditional Mediation Model","text":"Mediation typically assessed three regression models:Total Effect: \\(X \\Y\\)Path \\(\\): \\(X \\M\\)Path \\(b\\) Direct Effect (\\(c'\\)): \\(X + M \\Y\\):\\(X\\) = independent (causal) variable\\(Y\\) = dependent (outcome) variable\\(M\\) = mediating variableOriginally, Baron Kenny (1986) required direct path \\(X \\Y\\) significant. However, mediation can still occur even direct effect significant. example:effect \\(X\\) \\(Y\\) might fully absorbed \\(M\\).effect \\(X\\) \\(Y\\) might fully absorbed \\(M\\).Multiple mediators (\\(M_1, M_2\\)) opposing effects cancel , leading non-significant direct effect.Multiple mediators (\\(M_1, M_2\\)) opposing effects cancel , leading non-significant direct effect.","code":""},{"path":"mediation.html","id":"graphical-representation-of-mediation","chapter":"19 Mediation","heading":"19.1.2 Graphical Representation of Mediation","text":"","code":""},{"path":"mediation.html","id":"unmediated-model","chapter":"19 Mediation","heading":"19.1.2.1 Unmediated Model","text":", \\(c\\) represents total effect \\(X\\) \\(Y\\).","code":""},{"path":"mediation.html","id":"mediated-model","chapter":"19 Mediation","heading":"19.1.2.2 Mediated Model","text":":\\(c'\\) = direct effect (effect \\(X\\) \\(Y\\) accounting mediation)\\(ab\\) = indirect effect (mediation pathway)Thus, can express:\\[\n\\text{total effect} = \\text{direct effect} + \\text{indirect effect}\n\\],\\[\nc = c' + ab\n\\]equation holds standard linear models necessarily cases :Latent variable modelsLogistic regression (approximation)Multilevel models (Bauer, Preacher, Gil 2006)","code":""},{"path":"mediation.html","id":"measuring-mediation","chapter":"19 Mediation","heading":"19.1.3 Measuring Mediation","text":"Several approaches exist quantifying indirect effect (\\(ab\\)):Proportional Reduction Approach:\\[1 - \\frac{c'}{c}\\]recommended due high instability, especially \\(c\\) small (D. P. MacKinnon, Warsi, Dwyer 1995).Proportional Reduction Approach:\\[1 - \\frac{c'}{c}\\]recommended due high instability, especially \\(c\\) small (D. P. MacKinnon, Warsi, Dwyer 1995).Product Method:\\[\\times b\\]\ncommon approach.Product Method:\\[\\times b\\]\ncommon approach.Difference Method:\\[c - c'\\]\nConceptually similar product method less precise small samples.Difference Method:\\[c - c'\\]\nConceptually similar product method less precise small samples.","code":""},{"path":"mediation.html","id":"assumptions-in-linear-mediation-models","chapter":"19 Mediation","heading":"19.1.4 Assumptions in Linear Mediation Models","text":"valid mediation analysis, following assumptions hold:unmeasured confounders \\(X-Y\\), \\(X-M\\), \\(M-Y\\).reverse causality: \\(X\\) influenced confounder (\\(C\\)) also affects \\(M-Y\\).Measurement reliability: \\(M\\) measured without error (, consider errors--variables models).Regression Equations Mediation StepsStep 1: Total Effect \\(X\\) \\(Y\\)\\[\nY = \\beta_0 + cX + \\epsilon\n\\]significance \\(c\\) required mediation occur.Step 2: Effect \\(X\\) \\(M\\)\\[\nM = \\alpha_0 + aX + \\epsilon\n\\]coefficient \\(\\) must significant mediation analysis proceed.Step 3: Effect \\(M\\) \\(Y\\) (Including \\(X\\))\\[\nY = \\gamma_0 + c'X + bM + \\epsilon\n\\]\\(c'\\) becomes non-significant including \\(M\\), full mediation occurs.\\(c'\\) reduced remains significant, partial mediation present.Interpretation Mediation Outcomes","code":""},{"path":"mediation.html","id":"testing-for-mediation","chapter":"19 Mediation","heading":"19.1.5 Testing for Mediation","text":"Several statistical tests exist assess whether indirect effect (\\(ab\\)) significant:Sobel Test (Sobel 1982)\nBased standard error \\(ab\\).\nLimitation: Assumes normality \\(ab\\), may hold small samples.\nBased standard error \\(ab\\).Limitation: Assumes normality \\(ab\\), may hold small samples.Joint Significance Test\n\\(\\) \\(b\\) significant, mediation likely.\n\\(\\) \\(b\\) significant, mediation likely.Bootstrapping (Preferred) Shrout Bolger (2002)\nEstimates confidence interval \\(ab\\).\nassume normality.\nRecommended small--moderate sample sizes.\nEstimates confidence interval \\(ab\\).assume normality.Recommended small--moderate sample sizes.","code":""},{"path":"mediation.html","id":"additional-considerations","chapter":"19 Mediation","heading":"19.1.6 Additional Considerations","text":"Proximal mediation (path \\(\\) exceeds path \\(b\\)) can lead multicollinearity reduced statistical power. contrast, distal mediation (path \\(b\\) exceeds path \\(\\)) tends maximize power. fact, slightly distal mediators—\\(b\\) somewhat larger \\(\\)—often strike ideal balance power mediation analyses (Hoyle 1999).Tests direct effects (\\(c\\) \\(c'\\)) generally lower power tests indirect effect (\\(ab\\)). result, possible indirect effect (\\(ab\\)) statistically significant even direct effect (\\(c\\)) . situation can appear indicate “complete mediation,” yet lack statistically significant direct effect \\(X\\) \\(Y\\) (.e., \\(c'\\)) definitively rule possibilities (Kenny Judd 2014).testing \\(ab\\) essentially combines two tests, often provides power advantage testing \\(c'\\) alone. However, using non-significant \\(c'\\) sole criterion claiming complete mediation done cautiously——given importance adequate sample size power. Indeed, Hayes Scharkow (2013) recommend avoiding claims complete mediation based solely non-significant \\(c'\\), particularly partial mediation may still present.","code":""},{"path":"mediation.html","id":"assumptions-in-mediation-analysis","chapter":"19 Mediation","heading":"19.1.7 Assumptions in Mediation Analysis","text":"Valid mediation analysis requires several key assumptions, can categorized causal direction, interaction effects, measurement reliability, confounding control.","code":""},{"path":"mediation.html","id":"direction","chapter":"19 Mediation","heading":"19.1.7.1 Direction","text":"Causal Order VariablesCausal Order VariablesA simple weak solution measure \\(X\\) \\(M\\) \\(Y\\) prevent reverse causality (.e., \\(M\\) \\(Y\\) causing \\(X\\)). Similarly, measuring \\(M\\) \\(Y\\) avoids feedback effects \\(Y\\) \\(M\\).simple weak solution measure \\(X\\) \\(M\\) \\(Y\\) prevent reverse causality (.e., \\(M\\) \\(Y\\) causing \\(X\\)). Similarly, measuring \\(M\\) \\(Y\\) avoids feedback effects \\(Y\\) \\(M\\).However, causal feedback loops \\(M\\) \\(Y\\) may still exist.\nassume full mediation (\\(c' = 0\\)), models reciprocal causal effects \\(M\\) \\(Y\\) can estimated using instrumental variables (IV).\nE. R. Smith (1982) suggests treating \\(M\\) \\(Y\\) potential mediators , requiring distinct instrumental variables avoid cross-contamination causal effects.\nHowever, causal feedback loops \\(M\\) \\(Y\\) may still exist.assume full mediation (\\(c' = 0\\)), models reciprocal causal effects \\(M\\) \\(Y\\) can estimated using instrumental variables (IV).assume full mediation (\\(c' = 0\\)), models reciprocal causal effects \\(M\\) \\(Y\\) can estimated using instrumental variables (IV).E. R. Smith (1982) suggests treating \\(M\\) \\(Y\\) potential mediators , requiring distinct instrumental variables avoid cross-contamination causal effects.E. R. Smith (1982) suggests treating \\(M\\) \\(Y\\) potential mediators , requiring distinct instrumental variables avoid cross-contamination causal effects.","code":""},{"path":"mediation.html","id":"interaction-effects-in-mediation","chapter":"19 Mediation","heading":"19.1.7.2 Interaction Effects in Mediation","text":"\\(M\\) interacts \\(X\\) predicting \\(Y\\), \\(M\\) mediator moderator (Baron Kenny 1986).\\(M\\) interacts \\(X\\) predicting \\(Y\\), \\(M\\) mediator moderator (Baron Kenny 1986).interaction term \\(X \\times M\\) always included model account possible moderation effects.interaction term \\(X \\times M\\) always included model account possible moderation effects.interpreting interactions mediation models, see (T. VanderWeele 2015), provides framework moderated mediation analysis.interpreting interactions mediation models, see (T. VanderWeele 2015), provides framework moderated mediation analysis.","code":""},{"path":"mediation.html","id":"reliability","chapter":"19 Mediation","heading":"19.1.7.3 Reliability","text":"Measurement error three key variables (\\(X, M, Y\\)) can bias estimates mediation effects.Measurement Error Mediator (\\(M\\)):\nBiases \\(b\\) \\(c'\\).\nPotential solution: Model \\(M\\) latent variable (reduces bias may decrease statistical power) (Ledgerwood Shrout 2011).\nSpecific effects:\n\\(b\\) attenuated (biased toward 0).\n\\(c'\\) :\nOverestimated \\(ab > 0\\).\nUnderestimated \\(ab < 0\\).\n\n\nBiases \\(b\\) \\(c'\\).Potential solution: Model \\(M\\) latent variable (reduces bias may decrease statistical power) (Ledgerwood Shrout 2011).Specific effects:\n\\(b\\) attenuated (biased toward 0).\n\\(c'\\) :\nOverestimated \\(ab > 0\\).\nUnderestimated \\(ab < 0\\).\n\n\\(b\\) attenuated (biased toward 0).\\(c'\\) :\nOverestimated \\(ab > 0\\).\nUnderestimated \\(ab < 0\\).\nOverestimated \\(ab > 0\\).Underestimated \\(ab < 0\\).Measurement Error Treatment (\\(X\\)):\nBiases \\(\\) \\(b\\).\nSpecific effects:\n\\(\\) attenuated.\n\\(b\\) :\nOverestimated \\(ac' > 0\\).\nUnderestimated \\(ac' < 0\\).\n\n\nBiases \\(\\) \\(b\\).Specific effects:\n\\(\\) attenuated.\n\\(b\\) :\nOverestimated \\(ac' > 0\\).\nUnderestimated \\(ac' < 0\\).\n\n\\(\\) attenuated.\\(b\\) :\nOverestimated \\(ac' > 0\\).\nUnderestimated \\(ac' < 0\\).\nOverestimated \\(ac' > 0\\).Underestimated \\(ac' < 0\\).Measurement Error Outcome (\\(Y\\)):\nunstandardized, bias.\nstandardized, attenuation bias (reduced effect sizes due error variance).\nunstandardized, bias.standardized, attenuation bias (reduced effect sizes due error variance).","code":""},{"path":"mediation.html","id":"confounding-in-mediation-analysis","chapter":"19 Mediation","heading":"19.1.7.4 Confounding in Mediation Analysis","text":"Omitted variable bias can distort three core relationships (\\(X \\Y\\), \\(X \\M\\), \\(M \\Y\\)). Addressing confounding requires either design-based statistical solutions.Design-Based Strategies (Preferred Feasible)Randomization independent variable (\\(X\\)) reduces confounding bias.Randomization mediator (\\(M\\)), possible, strengthens causal claims.Controlling measured confounders, though addresses observable confounding.Statistical Strategies (Randomization Possible)Instrumental Variables Approach:\nUsed confounder affects \\(M\\) \\(Y\\).\nFront-door adjustment can applied exists third variable fully mediates effect \\(M\\) \\(Y\\) independent confounder.\nUsed confounder affects \\(M\\) \\(Y\\).Front-door adjustment can applied exists third variable fully mediates effect \\(M\\) \\(Y\\) independent confounder.Weighting Methods (e.g., Inverse Probability Weighting - IPW):\nCorrects confounding reweighting observations balance confounders across treatment groups.\nRequires strong ignorability assumption: confounders must measured correctly specified (Westfall Yarkoni 2016).\nassumption formally tested, sensitivity analyses can help assess robustness.\nSee Heiss R code implementing IPW mediation models.\nCorrects confounding reweighting observations balance confounders across treatment groups.Requires strong ignorability assumption: confounders must measured correctly specified (Westfall Yarkoni 2016).assumption formally tested, sensitivity analyses can help assess robustness.See Heiss R code implementing IPW mediation models.","code":""},{"path":"mediation.html","id":"indirect-effect-tests","chapter":"19 Mediation","heading":"19.1.8 Indirect Effect Tests","text":"Testing indirect effect (\\(ab\\)) crucial mediation analysis. Several methods exist, advantages limitations.","code":""},{"path":"mediation.html","id":"sobel-test-delta-method","chapter":"19 Mediation","heading":"19.1.8.1 Sobel Test (Delta Method)","text":"Developed Sobel (1982).Also known delta method.recommended assumes sampling distribution \\(ab\\) normal, often hold (D. P. MacKinnon, Warsi, Dwyer 1995).standard error (SE) indirect effect :\\[\nSE_{ab} = \\sqrt{\\hat{b}^2 s_{\\hat{}}^2 + \\hat{}^2 s_{\\hat{b}}^2}\n\\]Z-statistic testing whether \\(ab\\) significantly different 0 :\\[\nz = \\frac{\\hat{ab}}{\\sqrt{\\hat{b}^2 s_{\\hat{}}^2 + \\hat{}^2 s_{\\hat{b}}^2}}\n\\]DisadvantagesAssumes \\(\\) \\(b\\) independent.Assumes \\(ab\\) follows normal distribution.Poor performance small samples.Lower power conservative bootstrapping.Special Case: Inconsistent MediationMediation can occur even direct indirect effects opposite signs, known inconsistent mediation (D. P. MacKinnon, Fairchild, Fritz 2007).happens mediator acts suppressor variable, leading counteracting paths.","code":"\nlibrary(bda)\nlibrary(mediation)\ndata(\"boundsdata\")\n\n# Sobel Test for Mediation\nbda::mediation.test(boundsdata$med, boundsdata$ttt, boundsdata$out) |>\n    tibble::rownames_to_column() |>\n    causalverse::nice_tab(2)\n#>   rowname Sobel Aroian Goodman\n#> 1 z.value  4.05   4.03    4.07\n#> 2 p.value  0.00   0.00    0.00"},{"path":"mediation.html","id":"joint-significance-test","chapter":"19 Mediation","heading":"19.1.8.2 Joint Significance Test","text":"Tests indirect effect nonzero checking whether \\(\\) \\(b\\) statistically significant.Assumes \\(\\perp b\\) (independence paths).Performs similarly bootstrapping (Hayes Scharkow 2013).robust non-normality can sensitive heteroscedasticity (Fossum Montoya 2023).provide confidence intervals, making effect size interpretation harder.","code":""},{"path":"mediation.html","id":"bootstrapping-preferred-method","chapter":"19 Mediation","heading":"19.1.8.3 Bootstrapping (Preferred Method)","text":"First applied mediation Bollen Stine (1990).Uses resampling empirically estimate sampling distribution indirect effect.require normality assumptions \\(\\perp b\\) independence.Works well small samples.Can handle complex models.Bootstrapping Method?Percentile bootstrap preferred due better Type error rates (Tibbe Montoya 2022).Bias-corrected bootstrapping can liberal (inflates Type errors) (Fritz, Taylor, MacKinnon 2012).Special Case: Meta-Analytic BootstrappingBootstrapping can applied without raw data, using \\(, b, var(), var(b), cov(,b)\\) multiple studies.instrumental variable (IV) available, causal effect can estimated reliably. visual representations.Mediation Analysis Fixed Effects ModelsBootstrapped Mediation AnalysisAlternatively, use robmed package robust mediation analysis:","code":"\n# Meta-Analytic Bootstrapping for Mediation\nlibrary(causalverse)\n\nresult <- causalverse::med_ind(\n    a = 0.5, \n    b = 0.7, \n    var_a = 0.04, \n    var_b = 0.05, \n    cov_ab = 0.01\n)\nresult$plot\nlibrary(DiagrammeR)\n\n# Simple Treatment-Outcome Model\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label = 'Treatment']\n    Y [label = 'Outcome']\n  edge [minlen = 2]\n    X->Y\n  { rank = same; X; Y }\n}\")\n\n# Mediation Model with an Instrument\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label ='Treatment', shape = box]\n    Y [label ='Outcome', shape = box]\n    M [label ='Mediator', shape = box]\n    IV [label ='Instrument', shape = box]\n  edge [minlen = 2]\n    IV->X\n    X->M  \n    M->Y \n    X->Y \n  { rank = same; X; Y; M }\n}\")\nlibrary(mediation)\nlibrary(fixest)\n\ndata(\"boundsdata\")\n\n# Step 1: Total Effect (c)\nout1 <- feols(out ~ ttt, data = boundsdata)\n\n# Step 2: Indirect Effect (a)\nout2 <- feols(med ~ ttt, data = boundsdata)\n\n# Step 3: Direct & Indirect Effect (c' & b)\nout3 <- feols(out ~ med + ttt, data = boundsdata)\n\n# Proportion of Mediation\ncoef(out2)['ttt'] * coef(out3)['med'] / coef(out1)['ttt'] * 100\n#>      ttt \n#> 68.63609\nlibrary(boot)\nset.seed(1)\n\n# Define the bootstrapping function\nmediation_fn <- function(data, i) {\n    df <- data[i,]\n    \n    a_path <- feols(med ~ ttt, data = df)\n    a <- coef(a_path)['ttt']\n    \n    b_path <- feols(out ~ med + ttt, data = df)\n    b <- coef(b_path)['med']\n    \n    cp <- coef(b_path)['ttt']\n    \n    # Indirect Effect (a * b)\n    ind_ef <- a * b\n    total_ef <- a * b + cp\n    return(c(ind_ef, total_ef))\n}\n\n# Perform Bootstrapping\nboot_med <- boot(boundsdata, mediation_fn, R = 100, parallel = \"multicore\", ncpus = 2)\nboot_med \n#> \n#> ORDINARY NONPARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> boot(data = boundsdata, statistic = mediation_fn, R = 100, parallel = \"multicore\", \n#>     ncpus = 2)\n#> \n#> \n#> Bootstrap Statistics :\n#>       original        bias    std. error\n#> t1* 0.04112035  0.0006346725 0.009539903\n#> t2* 0.05991068 -0.0004462572 0.029556611\n\n# Summary and Confidence Intervals\nsummary(boot_med) |> causalverse::nice_tab()\n#>     R original bootBias bootSE bootMed\n#> 1 100     0.04        0   0.01    0.04\n#> 2 100     0.06        0   0.03    0.06\n\n# Confidence Intervals (percentile bootstrap preferred)\nboot.ci(boot_med, type = c(\"norm\", \"perc\"))\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#> Based on 100 bootstrap replicates\n#> \n#> CALL : \n#> boot.ci(boot.out = boot_med, type = c(\"norm\", \"perc\"))\n#> \n#> Intervals : \n#> Level      Normal             Percentile     \n#> 95%   ( 0.0218,  0.0592 )   ( 0.0249,  0.0623 )  \n#> Calculations and Intervals on Original Scale\n#> Some percentile intervals may be unstable\n\n# Point Estimates (Indirect and Total Effects)\ncolMeans(boot_med$t)\n#> [1] 0.04175502 0.05946442\nlibrary(robmed)"},{"path":"mediation.html","id":"power-analysis-for-mediation","chapter":"19 Mediation","heading":"19.1.9 Power Analysis for Mediation","text":"assess whether study sufficient power detect mediation effects, use:interactive power analysis, see Kenny’s Mediation Power App.Summary Indirect Effect Tests","code":"\nlibrary(pwr2ppl)\n\n# Power analysis for the indirect effect (ab path)\nmedjs(\n    rx1m1 = .3,  # Correlation: X → M (path a)\n    rx1y  = .1,  # Correlation: X → Y (path c')\n    rym1  = .3,  # Correlation: M → Y (path b)\n    n     = 100, # Sample size\n    alpha = 0.05,\n    mvars = 1,   # Number of mediators\n    rep   = 1000 # Replications (use 10,000 for accuracy)\n)"},{"path":"mediation.html","id":"multiple-mediation-analysis","chapter":"19 Mediation","heading":"19.1.10 Multiple Mediation Analysis","text":"cases, single mediator (\\(M\\)) fully capture indirect effect \\(X\\) \\(Y\\). Multiple mediation models extend traditional mediation including two mediators, allowing us examine multiple pathways contribute outcome.Several R packages handle multiple mediation models:manymome: flexible package multiple mediation modeling.\nVignette\nVignettemma: Used multiple mediator models.\nPackage PDF\nVignette\nmma: Used multiple mediator models.Package PDFPackage PDFVignetteVignette","code":"\nlibrary(manymome)\nlibrary(mma)"},{"path":"mediation.html","id":"multiple-mediators-structural-equation-modeling-approach","chapter":"19 Mediation","heading":"19.1.10.1 Multiple Mediators: Structural Equation Modeling Approach","text":"popular method estimating multiple mediation models Structural Equation Modeling using lavaan.test multiple mediation, first simulate data two mediators (\\(M_1\\) \\(M_2\\)) contribute outcome (\\(Y\\)).analyze indirect effects mediators (\\(M_1\\) \\(M_2\\)).Correctly Modeling Correlated Mediators2. Incorrectly Ignoring Correlation MediatorsComparison Model FitsTo check whether modeling correlation matters, compare AIC RMSEA.AIC RMSEA lower correlated model, suggests accounting correlated errors improves fit.fitting model, assess:Direct Effect: effect \\(X\\) \\(Y\\) accounting mediators (\\(c'\\)).Direct Effect: effect \\(X\\) \\(Y\\) accounting mediators (\\(c'\\)).Indirect Effects:\n\\(a_1 \\times b_1\\): Effect \\(X \\M_1 \\Y\\).\n\\(a_2 \\times b_2\\): Effect \\(X \\M_2 \\Y\\).\nIndirect Effects:\\(a_1 \\times b_1\\): Effect \\(X \\M_1 \\Y\\).\\(a_1 \\times b_1\\): Effect \\(X \\M_1 \\Y\\).\\(a_2 \\times b_2\\): Effect \\(X \\M_2 \\Y\\).\\(a_2 \\times b_2\\): Effect \\(X \\M_2 \\Y\\).Total Effect: Sum direct indirect effects.Total Effect: Sum direct indirect effects.\\(c'\\) reduced (still significant), partial mediation. \\(c' \\approx 0\\), suggests full mediation.","code":"\n# Load required packages\nlibrary(MASS)  # For mvrnorm (generating correlated errors)\nlibrary(lavaan)\n\n# Function to generate synthetic data\ngenerate_data <- function(n = 10000, a1 = 0.5, a2 = -0.35, \n                          b1 = 0.7, b2 = 0.48, \n                          corr = TRUE, correlation_value = 0.7) {\n    set.seed(12345)\n    X <- rnorm(n)  # Independent variable\n    \n    # Generate correlated errors for mediators\n    if (corr) {\n        Sigma <- matrix(c(1, correlation_value, correlation_value, 1), nrow = 2)\n        errors <- mvrnorm(n, mu = c(0, 0), Sigma = Sigma) \n    } else {\n        errors <- mvrnorm(n, mu = c(0, 0), Sigma = diag(2)) \n    }\n    \n    M1 <- a1 * X + errors[, 1]\n    M2 <- a2 * X + errors[, 2]\n    Y  <- b1 * M1 + b2 * M2 + rnorm(n)  # Outcome variable\n\n    return(data.frame(X = X, M1 = M1, M2 = M2, Y = Y))\n}\n# Generate data with correlated mediators\nData_corr <- generate_data(n = 10000, corr = TRUE, correlation_value = 0.7)\n\n# Define SEM model for multiple mediation\nmodel_corr <- '\n  Y ~ b1 * M1 + b2 * M2 + c * X\n  M1 ~ a1 * X\n  M2 ~ a2 * X\n  M1 ~~ M2  # Correlated mediators (modeling correlation correctly)\n'\n\n# Fit SEM model\nfit_corr <- sem(model_corr, data = Data_corr)\n\n# Extract parameter estimates\nparameterEstimates(fit_corr)[, c(\"lhs\", \"rhs\", \"est\", \"se\", \"pvalue\")]\n#>    lhs rhs    est    se pvalue\n#> 1    Y  M1  0.700 0.014  0.000\n#> 2    Y  M2  0.487 0.014  0.000\n#> 3    Y   X -0.009 0.015  0.545\n#> 4   M1   X  0.519 0.010  0.000\n#> 5   M2   X -0.340 0.010  0.000\n#> 6   M1  M2  0.677 0.012  0.000\n#> 7    Y   Y  0.975 0.014  0.000\n#> 8   M1  M1  0.973 0.014  0.000\n#> 9   M2  M2  0.982 0.014  0.000\n#> 10   X   X  1.000 0.000     NA\n# Define SEM model without modeling mediator correlation\nmodel_uncorr <- '\n  Y ~ b1 * M1 + b2 * M2 + c * X\n  M1 ~ a1 * X\n  M2 ~ a2 * X\n'\n\n# Fit incorrect model\nfit_uncorr <- sem(model_uncorr, data = Data_corr)\n\n# Compare parameter estimates\nparameterEstimates(fit_uncorr)[, c(\"lhs\", \"rhs\", \"est\", \"se\", \"pvalue\")]\n#>   lhs rhs    est    se pvalue\n#> 1   Y  M1  0.700 0.010  0.000\n#> 2   Y  M2  0.487 0.010  0.000\n#> 3   Y   X -0.009 0.012  0.443\n#> 4  M1   X  0.519 0.010  0.000\n#> 5  M2   X -0.340 0.010  0.000\n#> 6   Y   Y  0.975 0.014  0.000\n#> 7  M1  M1  0.973 0.014  0.000\n#> 8  M2  M2  0.982 0.014  0.000\n#> 9   X   X  1.000 0.000     NA\n# Extract model fit statistics\nfit_measures <- function(fit) {\n  fitMeasures(fit, c(\"aic\", \"bic\", \"rmsea\", \"chisq\"))\n}\n\n# Compare model fits\nfit_measures(fit_corr)  # Correct model (correlated mediators)\n#>      aic      bic    rmsea    chisq \n#> 77932.45 77997.34     0.00     0.00\nfit_measures(fit_uncorr)  # Incorrect model (ignores correlation)\n#>       aic       bic     rmsea     chisq \n#> 84453.208 84510.891     0.808  6522.762\n# Extract indirect and direct effects\nparameterEstimates(fit_corr, standardized = TRUE)\n#>    lhs op rhs label    est    se       z pvalue ci.lower ci.upper std.lv\n#> 1    Y  ~  M1    b1  0.700 0.014  50.489  0.000    0.673    0.727  0.700\n#> 2    Y  ~  M2    b2  0.487 0.014  35.284  0.000    0.460    0.514  0.487\n#> 3    Y  ~   X     c -0.009 0.015  -0.606  0.545   -0.038    0.020 -0.009\n#> 4   M1  ~   X    a1  0.519 0.010  52.563  0.000    0.499    0.538  0.519\n#> 5   M2  ~   X    a2 -0.340 0.010 -34.314  0.000   -0.360   -0.321 -0.340\n#> 6   M1 ~~  M2        0.677 0.012  56.915  0.000    0.654    0.700  0.677\n#> 7    Y ~~   Y        0.975 0.014  70.711  0.000    0.948    1.002  0.975\n#> 8   M1 ~~  M1        0.973 0.014  70.711  0.000    0.946    1.000  0.973\n#> 9   M2 ~~  M2        0.982 0.014  70.711  0.000    0.955    1.010  0.982\n#> 10   X ~~   X        1.000 0.000      NA     NA    1.000    1.000  1.000\n#>    std.all std.nox\n#> 1    0.528   0.528\n#> 2    0.345   0.345\n#> 3   -0.006  -0.006\n#> 4    0.465   0.465\n#> 5   -0.325  -0.325\n#> 6    0.692   0.692\n#> 7    0.447   0.447\n#> 8    0.784   0.784\n#> 9    0.895   0.895\n#> 10   1.000   1.000\n# Load required packages\nlibrary(MASS)  # for mvrnorm\nlibrary(lavaan)\n\n# Function to generate synthetic data with correctly correlated errors for mediators\ngenerate_data <-\n  function(n = 10000,\n           a1 = 0.5,\n           a2 = -0.35,\n           b1 = 0.7,\n           b2 = 0.48,\n           corr = TRUE,\n           correlation_value = 0.7) {\n    set.seed(12345)\n    X <- rnorm(n)\n    \n    # Generate correlated errors using a multivariate normal distribution\n    if (corr) {\n        Sigma <- matrix(c(1, correlation_value, correlation_value, 1), nrow = 2)  # Higher covariance matrix for errors\n        errors <- mvrnorm(n, mu = c(0, 0), Sigma = Sigma)  # Generate correlated errors\n    } else {\n        errors <- mvrnorm(n, mu = c(0, 0), Sigma = diag(2))  # Independent errors\n    }\n    \n    M1 <- a1 * X + errors[, 1]\n    M2 <- a2 * X + errors[, 2]\n    Y <- b1 * M1 + b2 * M2 + rnorm(n)  # Y depends on M1 and M2\n    \n    data.frame(X = X, M1 = M1, M2 = M2, Y = Y)\n}\n\n# Ground truth for comparison\nground_truth <- data.frame(Parameter = c(\"b1\", \"b2\"), GroundTruth = c(0.7, 0.48))\n\n# Function to extract relevant estimates, standard errors, and model fit\nextract_estimates_b1_b2 <- function(fit) {\n    estimates <- parameterEstimates(fit)\n    estimates <- estimates[estimates$lhs == \"Y\" & estimates$rhs %in% c(\"M1\", \"M2\"), c(\"rhs\", \"est\", \"se\")]\n    estimates$Parameter <- ifelse(estimates$rhs == \"M1\", \"b1\", \"b2\")\n    estimates <- estimates[, c(\"Parameter\", \"est\", \"se\")]\n    fit_stats <- fitMeasures(fit, c(\"aic\", \"bic\", \"rmsea\", \"chisq\"))\n    return(list(estimates = estimates, fit_stats = fit_stats))\n}\n\n# Case 1: Correlated errors for mediators (modeled correctly)\nData_corr <- generate_data(n = 10000, corr = TRUE, correlation_value = 0.7)\nmodel_corr <- '\n  Y ~ b1 * M1 + b2 * M2 + c * X\n  M1 ~ a1 * X\n  M2 ~ a2 * X\n  M1 ~~ M2  # Correlated mediators (errors)\n'\nfit_corr <- sem(model = model_corr, data = Data_corr)\nresults_corr <- extract_estimates_b1_b2(fit_corr)\n\n# Case 2: Uncorrelated errors for mediators (modeled correctly)\nData_uncorr <- generate_data(n = 10000, corr = FALSE)\nmodel_uncorr <- '\n  Y ~ b1 * M1 + b2 * M2 + c * X\n  M1 ~ a1 * X\n  M2 ~ a2 * X\n'\nfit_uncorr <- sem(model = model_uncorr, data = Data_uncorr)\nresults_uncorr <- extract_estimates_b1_b2(fit_uncorr)\n\n# Case 3: Correlated errors, but not modeled as correlated\nfit_corr_incorrect <- sem(model = model_uncorr, data = Data_corr)\nresults_corr_incorrect <- extract_estimates_b1_b2(fit_corr_incorrect)\n\n# Case 4: Uncorrelated errors, but modeled as correlated\nfit_uncorr_incorrect <- sem(model = model_corr, data = Data_uncorr)\nresults_uncorr_incorrect <- extract_estimates_b1_b2(fit_uncorr_incorrect)\n\n# Combine all estimates for comparison\nestimates_combined <- list(\n    \"Correlated (Correct)\" = results_corr$estimates,\n    \"Uncorrelated (Correct)\" = results_uncorr$estimates,\n    \"Correlated (Incorrect)\" = results_corr_incorrect$estimates,\n    \"Uncorrelated (Incorrect)\" = results_uncorr_incorrect$estimates\n)\n\n# Combine all into a single table\ncomparison_table <- do.call(rbind, lapply(names(estimates_combined), function(case) {\n    df <- estimates_combined[[case]]\n    df$Case <- case\n    df\n}))\n\n# Merge with ground truth for final comparison\ncomparison_table <- merge(comparison_table, ground_truth, by = \"Parameter\")\n\n# Display the comparison table\ncomparison_table\n#>   Parameter       est          se                     Case GroundTruth\n#> 1        b1 0.7002984 0.013870433     Correlated (Correct)        0.70\n#> 2        b1 0.6973612 0.009859426   Uncorrelated (Correct)        0.70\n#> 3        b1 0.7002984 0.010010367   Correlated (Incorrect)        0.70\n#> 4        b1 0.6973612 0.009859634 Uncorrelated (Incorrect)        0.70\n#> 5        b2 0.4871118 0.013805615     Correlated (Correct)        0.48\n#> 6        b2 0.4868318 0.010009908   Uncorrelated (Correct)        0.48\n#> 7        b2 0.4871118 0.009963588   Correlated (Incorrect)        0.48\n#> 8        b2 0.4868318 0.010010119 Uncorrelated (Incorrect)        0.48\n\n# Display model fit statistics for each case\nfit_stats_combined <- list(\n    \"Correlated (Correct)\" = results_corr$fit_stats,\n    \"Uncorrelated (Correct)\" = results_uncorr$fit_stats,\n    \"Correlated (Incorrect)\" = results_corr_incorrect$fit_stats,\n    \"Uncorrelated (Incorrect)\" = results_uncorr_incorrect$fit_stats\n)\n\nfit_stats_combined\n#> $`Correlated (Correct)`\n#>      aic      bic    rmsea    chisq \n#> 77932.45 77997.34     0.00     0.00 \n#> \n#> $`Uncorrelated (Correct)`\n#>       aic       bic     rmsea     chisq \n#> 84664.312 84721.995     0.000     0.421 \n#> \n#> $`Correlated (Incorrect)`\n#>       aic       bic     rmsea     chisq \n#> 84453.208 84510.891     0.808  6522.762 \n#> \n#> $`Uncorrelated (Incorrect)`\n#>      aic      bic    rmsea    chisq \n#> 84665.89 84730.78     0.00     0.00"},{"path":"mediation.html","id":"multiple-treatments-in-mediation","chapter":"19 Mediation","heading":"19.1.11 Multiple Treatments in Mediation","text":"cases, multiple independent variables (\\(X_1\\), \\(X_2\\)) influence mediators. called multiple treatments mediation (Hayes Preacher 2014).example PROCESS (SPSS/R), see:Process Mediation Multiple Treatments.","code":""},{"path":"mediation.html","id":"causal-inference-approach-to-mediation","chapter":"19 Mediation","heading":"19.2 Causal Inference Approach to Mediation","text":"Traditional mediation models assume regression-based estimates provide valid causal inference. However, causal mediation analysis (CMA) extends beyond traditional models explicitly defining mediation terms potential outcomes counterfactuals.","code":""},{"path":"mediation.html","id":"sec-example-traditional-mediation-analysis","chapter":"19 Mediation","heading":"19.2.1 Example: Traditional Mediation Analysis","text":"begin classic three-step mediation approach.Total Effect: \\(\\hat{c} = 0.3961\\) → effect \\(X\\) \\(Y\\) without controlling \\(M\\).Direct Effect (ADE): \\(\\hat{c'} = 0.0396\\) → effect \\(X\\) \\(Y\\) accounting \\(M\\).ACME (Average Causal Mediation Effect):\nACME = \\(\\hat{c} - \\hat{c'} = 0.3961 - 0.0396 = 0.3565\\)\nEquivalent product paths: \\(\\hat{} \\times \\hat{b} = 0.56102 \\times 0.6355 = 0.3565\\).\nACME = \\(\\hat{c} - \\hat{c'} = 0.3961 - 0.0396 = 0.3565\\)ACME = \\(\\hat{c} - \\hat{c'} = 0.3961 - 0.0396 = 0.3565\\)Equivalent product paths: \\(\\hat{} \\times \\hat{b} = 0.56102 \\times 0.6355 = 0.3565\\).Equivalent product paths: \\(\\hat{} \\times \\hat{b} = 0.56102 \\times 0.6355 = 0.3565\\).calculations rely strong causal assumptions. causal interpretation, need rigorous framework.","code":"\n# Load data\nmyData <- read.csv(\"data/mediationData.csv\")\n\n# Step 1 (Total Effect: X → Y) [No longer required]\nmodel.0 <- lm(Y ~ X, data = myData)\nsummary(model.0)\n#> \n#> Call:\n#> lm(formula = Y ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.0262 -1.2340 -0.3282  1.5583  5.1622 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.8572     0.6932   4.122 7.88e-05 ***\n#> X             0.3961     0.1112   3.564 0.000567 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.929 on 98 degrees of freedom\n#> Multiple R-squared:  0.1147, Adjusted R-squared:  0.1057 \n#> F-statistic:  12.7 on 1 and 98 DF,  p-value: 0.0005671\n\n# Step 2 (Effect of X on M)\nmodel.M <- lm(M ~ X, data = myData)\nsummary(model.M)\n#> \n#> Call:\n#> lm(formula = M ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3046 -0.8656  0.1344  1.1344  4.6954 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.49952    0.58920   2.545   0.0125 *  \n#> X            0.56102    0.09448   5.938 4.39e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.639 on 98 degrees of freedom\n#> Multiple R-squared:  0.2646, Adjusted R-squared:  0.2571 \n#> F-statistic: 35.26 on 1 and 98 DF,  p-value: 4.391e-08\n\n# Step 3 (Effect of M on Y, controlling for X)\nmodel.Y <- lm(Y ~ X + M, data = myData)\nsummary(model.Y)\n#> \n#> Call:\n#> lm(formula = Y ~ X + M, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.7631 -1.2393  0.0308  1.0832  4.0055 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.9043     0.6055   3.145   0.0022 ** \n#> X             0.0396     0.1096   0.361   0.7187    \n#> M             0.6355     0.1005   6.321 7.92e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.631 on 97 degrees of freedom\n#> Multiple R-squared:  0.373,  Adjusted R-squared:  0.3601 \n#> F-statistic: 28.85 on 2 and 97 DF,  p-value: 1.471e-10\n\n# Step 4: Bootstrapping for ACME\nlibrary(mediation)\nresults <-\n    mediate(\n        model.M,\n        model.Y,\n        treat = 'X',\n        mediator = 'M',\n        boot = TRUE,\n        sims = 500\n    )\nsummary(results)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the Percentile Method\n#> \n#>                Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME             0.3565       0.2119         0.51  <2e-16 ***\n#> ADE              0.0396      -0.1750         0.28   0.760    \n#> Total Effect     0.3961       0.1743         0.64   0.004 ** \n#> Prop. Mediated   0.9000       0.5042         1.94   0.004 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 100 \n#> \n#> \n#> Simulations: 500"},{"path":"mediation.html","id":"two-approaches-in-causal-mediation-analysis","chapter":"19 Mediation","heading":"19.2.2 Two Approaches in Causal Mediation Analysis","text":"mediation package Imai, Keele, Yamamoto (2010) enables causal mediation analysis. supports two inference types:Model-Based Inference\nAssumptions:\nTreatment randomized (approximated via matching).\nSequential Ignorability: unobserved confounding :\nTreatment → Mediator\nTreatment → Outcome\nMediator → Outcome\n\nassumption hard justify observational studies.\n\nModel-Based InferenceAssumptions:\nTreatment randomized (approximated via matching).\nSequential Ignorability: unobserved confounding :\nTreatment → Mediator\nTreatment → Outcome\nMediator → Outcome\n\nassumption hard justify observational studies.\nAssumptions:Treatment randomized (approximated via matching).Treatment randomized (approximated via matching).Sequential Ignorability: unobserved confounding :\nTreatment → Mediator\nTreatment → Outcome\nMediator → Outcome\nSequential Ignorability: unobserved confounding :Treatment → MediatorTreatment → MediatorTreatment → OutcomeTreatment → OutcomeMediator → OutcomeMediator → OutcomeThis assumption hard justify observational studies.assumption hard justify observational studies.Design-Based Inference\nRelies experimental design isolate causal mechanism.\nDesign-Based InferenceRelies experimental design isolate causal mechanism.NotationWe follow standard potential outcomes framework:\\(M_i(t)\\) = mediator treatment condition \\(t\\)\\(M_i(t)\\) = mediator treatment condition \\(t\\)\\(T_i \\{0,1}\\) = treatment assignment\\(T_i \\{0,1}\\) = treatment assignment\\(Y_i(t, m)\\) = outcome treatment \\(t\\) mediator value \\(m\\)\\(Y_i(t, m)\\) = outcome treatment \\(t\\) mediator value \\(m\\)\\(X_i\\) = observed pre-treatment covariates\\(X_i\\) = observed pre-treatment covariatesThe treatment effect individual \\(\\): \\[\n\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\n\\] decomposes :Causal Mediation Effect (ACME):\\[\n\\delta_i (t) = Y_i (t,M_i(1)) - Y_i(t,M_i(0))\n\\]Direct Effect (ADE):\\[\n\\zeta_i (t) = Y_i (1, M_i(1)) - Y_i(0, M_i(0))\n\\]Summing :\\[\n\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\n\\]Sequential Ignorability AssumptionFor CMA valid, assume:\\[\n\\begin{aligned}\n\\{ Y_i (t', m), M_i (t) \\} &\\perp T_i |X_i = x\\\\\nY_i(t',m) &\\perp M_i(t) | T_i = t, X_i = x\n\\end{aligned}\n\\]First condition standard strong ignorability condition treatment assignment random conditional pre-treatment confounders.First condition standard strong ignorability condition treatment assignment random conditional pre-treatment confounders.Second condition stronger mediators also random given observed treatment pre-treatment confounders. condition satisfied unobserved pre-treatment confounders, post-treatment confounders, multiple mediators correlated.Second condition stronger mediators also random given observed treatment pre-treatment confounders. condition satisfied unobserved pre-treatment confounders, post-treatment confounders, multiple mediators correlated.Key Challenge⚠️ Sequential Ignorability testable. Researchers conduct sensitivity analysis.now fit causal mediation model using mediation.Alternative: Nonparametric BootstrapIf suspect moderation, include interaction term.Since sequential ignorability untestable, examine unmeasured confounding affects ACME estimates.ACME confidence intervals contain 0, effect robust confounding.Alternatively, using \\(R^2\\) interpretation, need specify direction confounder affects mediator outcome variables plot using sign.prod = \"positive\" (.e., direction) sign.prod = \"negative\" (.e., opposite direction).Summary: Causal Mediation vs. Traditional Mediation","code":"\nlibrary(mediation)\nset.seed(2014)\ndata(\"framing\", package = \"mediation\")\n\n# Step 1: Fit mediator model (M ~ T, X)\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\n\n# Step 2: Fit outcome model (Y ~ M, T, X)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\n\n# Step 3: Causal Mediation Analysis (Quasi-Bayesian)\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )  # Use sims = 10000 in practice\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0791       0.0351         0.15  <2e-16 ***\n#> ACME (treated)             0.0804       0.0367         0.16  <2e-16 ***\n#> ADE (control)              0.0206      -0.0976         0.12    0.70    \n#> ADE (treated)              0.0218      -0.1053         0.12    0.70    \n#> Total Effect               0.1009      -0.0497         0.23    0.14    \n#> Prop. Mediated (control)   0.6946      -6.3109         3.68    0.14    \n#> Prop. Mediated (treated)   0.7118      -5.7936         3.50    0.14    \n#> ACME (average)             0.0798       0.0359         0.15  <2e-16 ***\n#> ADE (average)              0.0212      -0.1014         0.12    0.70    \n#> Prop. Mediated (average)   0.7032      -6.0523         3.59    0.14    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        boot = TRUE,\n        treat = \"treat\",\n        mediator = \"emo\",\n        sims = 100,\n        boot.ci.type = \"bca\"\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the BCa Method\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0848       0.0424         0.14  <2e-16 ***\n#> ACME (treated)             0.0858       0.0410         0.14  <2e-16 ***\n#> ADE (control)              0.0117      -0.0726         0.13    0.58    \n#> ADE (treated)              0.0127      -0.0784         0.14    0.58    \n#> Total Effect               0.0975       0.0122         0.25    0.06 .  \n#> Prop. Mediated (control)   0.8698       1.7460       151.20    0.06 .  \n#> Prop. Mediated (treated)   0.8804       1.6879       138.91    0.06 .  \n#> ACME (average)             0.0853       0.0434         0.14  <2e-16 ***\n#> ADE (average)              0.0122      -0.0756         0.14    0.58    \n#> Prop. Mediated (average)   0.8751       1.7170       145.05    0.06 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\n\nout.fit <-\n    glm(\n        cong_mesg ~ emo * treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\n\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\n\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                           Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.07417      0.02401         0.14  <2e-16 ***\n#> ACME (treated)             0.09496      0.02702         0.16  <2e-16 ***\n#> ADE (control)             -0.01353     -0.11855         0.11    0.76    \n#> ADE (treated)              0.00726     -0.11007         0.11    0.90    \n#> Total Effect               0.08143     -0.05646         0.19    0.26    \n#> Prop. Mediated (control)   0.64510    -14.31243         3.13    0.26    \n#> Prop. Mediated (treated)   0.98006    -17.83202         4.01    0.26    \n#> ACME (average)             0.08457      0.02738         0.15  <2e-16 ***\n#> ADE (average)             -0.00314     -0.11457         0.12    1.00    \n#> Prop. Mediated (average)   0.81258    -16.07223         3.55    0.26    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\ntest.TMint(med.out, conf.level = .95)  # Tests for interaction effect\n#> \n#>  Test of ACME(1) - ACME(0) = 0\n#> \n#> data:  estimates from med.out\n#> ACME(1) - ACME(0) = 0.020796, p-value = 0.3\n#> alternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.01757310  0.07110837\n# Load required package\nlibrary(mediation)\n\n# Simulate some example data\nset.seed(123)\nn <- 100\ndata <- data.frame(\n  treat = rbinom(n, 1, 0.5),       # Binary treatment\n  med = rnorm(n),                   # Continuous mediator\n  outcome = rnorm(n)                 # Continuous outcome\n)\n\n# Fit the mediator model (med ~ treat)\nmed_model <- lm(med ~ treat, data = data)\n\n# Fit the outcome model (outcome ~ treat + med)\noutcome_model <- lm(outcome ~ treat + med, data = data)\n\n# Perform mediation analysis\nmed_out <- mediate(med_model, \n                   outcome_model, \n                   treat = \"treat\", \n                   mediator = \"med\", \n                   sims = 100)\n\n# Conduct sensitivity analysis\nsens_out <- medsens(med_out, sims = 100)\n\n# Print and plot results\nsummary(sens_out)\n#> \n#> Mediation Sensitivity Analysis for Average Causal Mediation Effect\n#> \n#> Sensitivity Region\n#> \n#>        Rho    ACME 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n#>  [1,] -0.9 -0.6194      -1.3431       0.1043         0.81       0.7807\n#>  [2,] -0.8 -0.3898      -0.8479       0.0682         0.64       0.6168\n#>  [3,] -0.7 -0.2790      -0.6096       0.0516         0.49       0.4723\n#>  [4,] -0.6 -0.2067      -0.4552       0.0418         0.36       0.3470\n#>  [5,] -0.5 -0.1525      -0.3406       0.0355         0.25       0.2409\n#>  [6,] -0.4 -0.1083      -0.2487       0.0321         0.16       0.1542\n#>  [7,] -0.3 -0.0700      -0.1723       0.0323         0.09       0.0867\n#>  [8,] -0.2 -0.0354      -0.1097       0.0389         0.04       0.0386\n#>  [9,] -0.1 -0.0028      -0.0648       0.0591         0.01       0.0096\n#> [10,]  0.0  0.0287      -0.0416       0.0990         0.00       0.0000\n#> [11,]  0.1  0.0603      -0.0333       0.1538         0.01       0.0096\n#> [12,]  0.2  0.0928      -0.0317       0.2173         0.04       0.0386\n#> [13,]  0.3  0.1275      -0.0333       0.2882         0.09       0.0867\n#> [14,]  0.4  0.1657      -0.0369       0.3684         0.16       0.1542\n#> [15,]  0.5  0.2100      -0.0422       0.4621         0.25       0.2409\n#> [16,]  0.6  0.2642      -0.0495       0.5779         0.36       0.3470\n#> [17,]  0.7  0.3364      -0.0601       0.7329         0.49       0.4723\n#> [18,]  0.8  0.4473      -0.0771       0.9717         0.64       0.6168\n#> [19,]  0.9  0.6768      -0.1135       1.4672         0.81       0.7807\n#> \n#> Rho at which ACME = 0: -0.1\n#> R^2_M*R^2_Y* at which ACME = 0: 0.01\n#> R^2_M~R^2_Y~ at which ACME = 0: 0.0096\nplot(sens_out)\nplot(sens.out, sens.par = \"R2\", r.type = \"total\", sign.prod = \"positive\")"},{"path":"prediction-and-estimation.html","id":"prediction-and-estimation","chapter":"20 Prediction and Estimation","heading":"20 Prediction and Estimation","text":"modern statistics, econometrics, machine learning, two primary goals often motivate data analysis:Prediction: build function \\(\\hat{f}\\) accurately predicts outcome \\(Y\\) observed features (predictors) \\(X\\).Prediction: build function \\(\\hat{f}\\) accurately predicts outcome \\(Y\\) observed features (predictors) \\(X\\).Estimation Causal Inference: uncover quantify relationship (often causal) \\(X\\) \\(Y\\), typically estimating parameters like \\(\\beta\\) model \\(Y = g(X; \\beta)\\).Estimation Causal Inference: uncover quantify relationship (often causal) \\(X\\) \\(Y\\), typically estimating parameters like \\(\\beta\\) model \\(Y = g(X; \\beta)\\).goals, superficially similar, rest distinct philosophical mathematical foundations. , explore difference detail, illustrating key ideas formal definitions, theorems, proofs (relevant), references seminal works.","code":""},{"path":"prediction-and-estimation.html","id":"conceptual-framing","chapter":"20 Prediction and Estimation","heading":"20.1 Conceptual Framing","text":"","code":""},{"path":"prediction-and-estimation.html","id":"predictive-modeling","chapter":"20 Prediction and Estimation","heading":"20.1.1 Predictive Modeling","text":"Predictive modeling focuses building function \\(\\hat{f}: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) maps inputs \\(X\\) outputs \\(Y\\). simplicity, assume:\\(X \\\\mathbb{R}^p\\) (though practice \\(X\\) can images, text, time series, etc.).\\(Y \\\\mathbb{R}\\) regression \\(Y \\\\{0, 1\\}\\) (finite set) classification.yardstick success function’s accuracy --sample predictions, often measured loss function \\(L(\\hat{y}, y)\\). typically choose \\(\\hat{f}\\) minimize expected loss:\\[\n\\text{(Predictive Problem)} \\quad \\hat{f} = \\arg \\min_{f \\\\mathcal{F}} \\mathbb{E}[L(f(X), Y)],\n\\]\\(\\mathcal{F}\\) class functions (models) \\(\\mathbb{E}[\\cdot]\\) taken joint distribution \\((X, Y)\\).","code":""},{"path":"prediction-and-estimation.html","id":"estimation-or-causal-inference","chapter":"20 Prediction and Estimation","heading":"20.1.2 Estimation or Causal Inference","text":"contrast, estimation causal inference generally aims uncover underlying mechanism: \\(X\\) (particular component \\(T \\subseteq X\\)) cause changes \\(Y\\)? canonical problem estimate parameters \\(\\beta\\) model \\(m_\\beta(x)\\) :\\[\nY = m_\\beta(X) + \\varepsilon,\n\\], linear form,\\[\nY = X\\beta + \\varepsilon.\n\\]variety statistical properties—consistency, unbiasedness, efficiency, confidence intervals, hypothesis tests—relevant . Causal interpretations usually require assumptions beyond typical ..d. sampling: unconfoundedness, exogeneity, random assignment, \\(\\beta\\) indeed captures changes \\(X\\) cause changes \\(Y\\).Key Distinction:Prediction require parameters used \\(\\hat{f}\\) reflect real-world mechanism. long --sample predictive performance good, model deemed successful—even ’s “black box.”Causal inference demands interpretability terms structural exogenous relationships. main objective consistent estimation true (theoretically defined) parameter \\(\\beta\\), economic, biomedical, policy interpretation.","code":""},{"path":"prediction-and-estimation.html","id":"mathematical-setup","chapter":"20 Prediction and Estimation","heading":"20.2 Mathematical Setup","text":"","code":""},{"path":"prediction-and-estimation.html","id":"probability-space-and-data","chapter":"20 Prediction and Estimation","heading":"20.2.1 Probability Space and Data","text":"posit probability space \\((\\Omega, \\mathcal{F}, P)\\) random variables \\((X, Y)\\) . typically ..d. sample \\(\\{(X_i, Y_i)\\}_{=1}^n\\) true distribution \\(\\mathcal{D}\\). Let:\\[\n(X, Y) \\sim \\mathcal{D}, \\quad (X_i, Y_i) \\overset{\\text{..d.}}{\\sim} \\mathcal{D}.\n\\]prediction, train \\(\\{(X_i, Y_i)\\}_{=1}^n\\) obtain \\(\\hat{f}\\), evaluate test point \\((\\tilde{X}, \\tilde{Y})\\) drawn \\(\\mathcal{D}\\). causal inference, scrutinize data generating process carefully, ensuring can identify causal effect. example, may require:Potential outcomes \\(\\{Y_i(0), Y_i(1)\\}\\) treatment effect settings.Unconfoundedness randomization assumptions.","code":""},{"path":"prediction-and-estimation.html","id":"loss-functions-and-risk","chapter":"20 Prediction and Estimation","heading":"20.2.2 Loss Functions and Risk","text":"general framework tasks risk minimization approach. function \\(f\\), define:population (expected) risk: \\[\n\\mathcal{R}(f) = \\mathbb{E}[L(f(X), Y)].\n\\]empirical risk (sample size \\(n\\)): \\[\n\\hat{\\mathcal{R}}_n(f) = \\frac{1}{n} \\sum_{=1}^n L(f(X_i), Y_i).\n\\]Prediction: often solve empirical risk minimization (ERM) problem:\\[\n\\hat{f} = \\arg \\min_{f \\\\mathcal{F}} \\hat{\\mathcal{R}}_n(f),\n\\]possibly regularization. measure success \\(\\mathcal{R}(\\hat{f})\\), .e., well \\(\\hat{f}\\) generalizes beyond training sample.Causal/Parameter Estimation: might define \\(M\\)-estimator \\(\\beta\\) (Newey McFadden 1994). Consider function \\(\\psi(\\beta; X, Y)\\) true parameter \\(\\beta_0\\) satisfies:\\[\n\\mathbb{E}[\\psi(\\beta_0; X, Y)] = 0.\n\\]empirical \\(M\\)-estimator solves\\[\n\\hat{\\beta} = \\arg \\min_\\beta \\left\\| \\frac{1}{n} \\sum_{=1}^n \\psi(\\beta; X_i, Y_i) \\right\\|,\n\\]equivalently sets zero method--moments sense:\\[\n\\frac{1}{n} \\sum_{=1}^n \\psi(\\hat{\\beta}; X_i, Y_i) = 0.\n\\]Properties like consistency (\\(\\hat{\\beta} \\overset{p}{\\} \\beta_0\\)) asymptotic normality (\\(\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\overset{d}{\\} N(0, \\Sigma)\\)) central. emphasis uncovering true \\(\\beta_0\\) rather purely predictive accuracy.","code":""},{"path":"prediction-and-estimation.html","id":"prediction-in-detail","chapter":"20 Prediction and Estimation","heading":"20.3 Prediction in Detail","text":"","code":""},{"path":"prediction-and-estimation.html","id":"empirical-risk-minimization-and-generalization","chapter":"20 Prediction and Estimation","heading":"20.3.1 Empirical Risk Minimization and Generalization","text":"supervised learning, goal find function \\(f\\) class candidate models \\(\\mathcal{F}\\) (e.g., linear models, neural networks, tree-based models) accurately predicts outcome \\(Y\\) given input \\(X\\). typically formulated Empirical Risk Minimization problem, seek minimize average loss training data:\\[\n\\hat{f} = \\arg \\min_{f \\\\mathcal{F}} \\frac{1}{n} \\sum_{=1}^n L(f(X_i), Y_i).\n\\]\\(L(\\cdot, \\cdot)\\) loss function quantifies error predictions actual values. Common choices include:Squared Error (Regression): \\(L(\\hat{y}, y) = (\\hat{y} - y)^2\\).Absolute Error (Regression): \\(L(\\hat{y}, y) = |\\hat{y} - y|\\).Logistic Loss (Classification): \\(L(\\hat{p}, y) = -[y \\log \\hat{p} + (1 - y) \\log(1 - \\hat{p})]\\).minimizing empirical risk, find function \\(\\hat{f}\\) best fits observed data. However, minimizing training error guarantee good generalization—ability \\(\\hat{f}\\) perform well unseen data.","code":""},{"path":"prediction-and-estimation.html","id":"overfitting-and-regularization","chapter":"20 Prediction and Estimation","heading":"20.3.1.1 Overfitting and Regularization","text":"\\(\\mathcal{F}\\) large expressive (e.g., deep neural networks millions parameters), \\(\\hat{f}\\) can become complex, learning patterns exist training set generalize new data. called overfitting.mitigate overfitting, introduce regularization, modifying optimization objective penalize complex models:\\[\n\\hat{f}_\\lambda = \\arg \\min_{f \\\\mathcal{F}} \\left\\{ \\hat{\\mathcal{R}}_n(f) + \\lambda \\Omega(f) \\right\\}.\n\\]:\\(\\hat{\\mathcal{R}}_n(f) = \\frac{1}{n} \\sum_{=1}^{n} L(f(X_i), Y_i)\\) empirical risk.\\(\\hat{\\mathcal{R}}_n(f) = \\frac{1}{n} \\sum_{=1}^{n} L(f(X_i), Y_i)\\) empirical risk.\\(\\Omega(f)\\) complexity penalty discourages overly flexible models.\\(\\Omega(f)\\) complexity penalty discourages overly flexible models.\\(\\lambda\\) controls strength regularization.\\(\\lambda\\) controls strength regularization.Common choices \\(\\Omega(f)\\) include:LASSO penalty: \\(\\|\\beta\\|_1\\) (sparsity constraint linear models).LASSO penalty: \\(\\|\\beta\\|_1\\) (sparsity constraint linear models).Ridge penalty: \\(\\|\\beta\\|_2^2\\) (shrinking coefficients reduce variance).Ridge penalty: \\(\\|\\beta\\|_2^2\\) (shrinking coefficients reduce variance).Neural network weight decay: \\(\\sum w^2\\) (prevents exploding weights).Neural network weight decay: \\(\\sum w^2\\) (prevents exploding weights).Regularization encourages simpler models, likely generalize well.","code":""},{"path":"prediction-and-estimation.html","id":"generalization-and-statistical-learning-theory","chapter":"20 Prediction and Estimation","heading":"20.3.1.2 Generalization and Statistical Learning Theory","text":"fundamental question machine learning : well \\(\\hat{f}\\) perform unseen data? captured expected risk:\\[\nR(f) = \\mathbb{E}[L(f(X), Y)].\n\\]Ideally, want minimize gap true risk \\(R(\\hat{f})\\) best possible risk \\(R(f^*)\\) within \\(\\mathcal{F}\\):\\[\nR(\\hat{f}) - \\min_{f \\\\mathcal{F}} R(f).\n\\]difference, called excess risk, measures well \\(\\hat{f}\\) generalizes beyond training sample. Statistical Learning Theory provides theoretical tools analyze gap (Vapnik 2013; Hastie, Tibshirani, Friedman 2009). particular, establishes generalization bounds depend capacity function class \\(\\mathcal{F}\\).","code":""},{"path":"prediction-and-estimation.html","id":"complexity-measures","chapter":"20 Prediction and Estimation","heading":"20.3.1.3 Complexity Measures","text":"Two important ways quantify complexity \\(\\mathcal{F}\\) areVC DimensionVC DimensionRademacher ComplexityRademacher Complexity","code":""},{"path":"prediction-and-estimation.html","id":"vc-dimension","chapter":"20 Prediction and Estimation","heading":"20.3.1.3.1 VC Dimension","text":"VC dimension measures ability hypothesis class \\(\\mathcal{F}\\) fit arbitrary labels. Formally, VC dimension \\(\\mathcal{F}\\), denoted \\(\\operatorname{VC}(\\mathcal{F})\\), largest number points can shattered function \\(\\mathcal{F}\\).set points shattered \\(\\mathcal{F}\\) , every possible labeling points, exists function \\(f \\\\mathcal{F}\\) perfectly classifies .Example 1: Linear Classifiers 2DConsider set points \\(\\mathbb{R}^2\\) (plane).Consider set points \\(\\mathbb{R}^2\\) (plane).\\(\\mathcal{F}\\) consists linear decision boundaries, can shatter three points general position (single line can separate way).\\(\\mathcal{F}\\) consists linear decision boundaries, can shatter three points general position (single line can separate way).However, four points always shattered (e.g., arranged XOR pattern). - Thus, VC dimension linear classifiers \\(\\mathbb{R}^2\\) 3.However, four points always shattered (e.g., arranged XOR pattern). - Thus, VC dimension linear classifiers \\(\\mathbb{R}^2\\) 3.Key Property:higher VC dimension means expressive model class (higher capacity).higher VC dimension means expressive model class (higher capacity).\\(\\operatorname{VC}(\\mathcal{F})\\) large, model can memorize training set, leading poor generalization.\\(\\operatorname{VC}(\\mathcal{F})\\) large, model can memorize training set, leading poor generalization.","code":""},{"path":"prediction-and-estimation.html","id":"rademacher-complexity","chapter":"20 Prediction and Estimation","heading":"20.3.1.3.2 Rademacher Complexity","text":"VC dimension combinatorial measure, Rademacher complexity refined, data-dependent measure function class flexibility.Intuition: Rademacher complexity quantifies well functions \\(\\mathcal{F}\\) can correlate random noise. function class can fit random labels well, flexible likely overfit.Definition:\nGiven \\(n\\) training samples, let \\(\\sigma_1, \\dots, \\sigma_n\\) independent Rademacher variables (.e., random variables taking values \\(\\pm1\\) equal probability). empirical Rademacher complexity \\(\\mathcal{F}\\) :\\[\n\\hat{\\mathcal{R}}_n(\\mathcal{F}) = \\mathbb{E}_{\\sigma} \\left[ \\sup_{f \\\\mathcal{F}} \\frac{1}{n} \\sum_{=1}^{n} \\sigma_i f(X_i) \\right].\n\\]Interpretation:\\(\\hat{\\mathcal{R}}_n(\\mathcal{F})\\) large, \\(\\mathcal{F}\\) can fit random noise well \\(\\Rightarrow\\) high risk overfitting.\\(\\hat{\\mathcal{R}}_n(\\mathcal{F})\\) large, \\(\\mathcal{F}\\) can fit random noise well \\(\\Rightarrow\\) high risk overfitting.\\(\\hat{\\mathcal{R}}_n(\\mathcal{F})\\) small, \\(\\mathcal{F}\\) stable \\(\\Rightarrow\\) better generalization.\\(\\hat{\\mathcal{R}}_n(\\mathcal{F})\\) small, \\(\\mathcal{F}\\) stable \\(\\Rightarrow\\) better generalization.Example 2: Linear Models Bounded NormSuppose \\(\\mathcal{F}\\) consists linear models \\(f(X) = w^\\top X\\), \\(\\|w\\| \\leq C\\).Suppose \\(\\mathcal{F}\\) consists linear models \\(f(X) = w^\\top X\\), \\(\\|w\\| \\leq C\\).Rademacher complexity class scales \\(\\mathcal{O}(C/\\sqrt{n})\\).Rademacher complexity class scales \\(\\mathcal{O}(C/\\sqrt{n})\\).suggests controlling norm \\(w\\) (e.g., via Ridge Regression) improves generalization.suggests controlling norm \\(w\\) (e.g., via Ridge Regression) improves generalization.","code":""},{"path":"prediction-and-estimation.html","id":"bias-variance-decomposition","chapter":"20 Prediction and Estimation","heading":"20.3.2 Bias-Variance Decomposition","text":"regression problem squared-error loss, classic decomposition :\\[\n\\mathbb{E}_{\\text{train}}[(\\hat{f}(X) - Y)^2] = \\underbrace{(\\mathbb{E}[\\hat{f}(X)] - f^*(X))^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}[(\\hat{f}(X) - \\mathbb{E}[\\hat{f}(X)])^2]}_{\\text{Variance}} + \\underbrace{\\sigma_\\varepsilon^2}_{\\text{Irreducible Error}}\n\\]\\(f^*(X) = \\mathbb{E}[Y \\mid X]\\). Minimizing sum bias\\(^2\\) variance key.prediction, small increase bias often acceptable yields large reduction variance—can improve --sample performance. However, causal inference, added bias problematic distorts interpretation parameters.","code":""},{"path":"prediction-and-estimation.html","id":"example-linear-regression-for-prediction","chapter":"20 Prediction and Estimation","heading":"20.3.3 Example: Linear Regression for Prediction","text":"Consider linear predictor:\\[\n\\hat{y} = x^\\top \\hat{\\beta}.\n\\]choose \\(\\hat{\\beta}\\) minimize:\\[\n\\sum_{=1}^n (y_i - x_i^\\top \\beta)^2 \\quad \\text{penalty:} \\quad \\sum_{=1}^n (y_i - x_i^\\top \\beta)^2 + \\lambda \\|\\beta\\|_2^2.\n\\]Goal: Achieve minimal prediction error unseen data \\((\\tilde{x}, \\tilde{y})\\).estimated \\(\\hat{\\beta}\\) might biased use regularization (e.g., ridge). purely predictive lens, bias can advantageous lowers variance substantially thus lowers expected prediction error.","code":""},{"path":"prediction-and-estimation.html","id":"applications-in-economics","chapter":"20 Prediction and Estimation","heading":"20.3.4 Applications in Economics","text":"economics (related social sciences), prediction plays increasingly prominent role (Mullainathan Spiess 2017; Athey Imbens 2019):Measure Variables: Predicting missing proxy variables (e.g., predicting income observable covariates, predicting individual preferences online behaviors).Embed Prediction Tasks Within Parameter Estimation Treatment Effects: Sometimes, first-stage prediction (e.g., imputing missing data generating prognostic scores) used input subsequent causal analyses.Control Observed Confounders: Machine learning methods—LASSO, random forests, neural nets—can used control high-dimensional \\(X\\) partial-adjustments residualizing outcomes (Belloni, Chernozhukov, Hansen 2014; Chernozhukov et al. 2018).","code":""},{"path":"prediction-and-estimation.html","id":"parameter-estimation-and-causal-inference","chapter":"20 Prediction and Estimation","heading":"20.4 Parameter Estimation and Causal Inference","text":"","code":""},{"path":"prediction-and-estimation.html","id":"estimation-in-parametric-models","chapter":"20 Prediction and Estimation","heading":"20.4.1 Estimation in Parametric Models","text":"simple parametric form:\\[\nY = X\\beta + \\varepsilon, \\quad \\mathbb{E}[\\varepsilon \\mid X] = 0, \\quad \\text{Var}(\\varepsilon \\mid X) = \\sigma^2 .\n\\]Ordinary Least Squares estimator :\\[\n\\hat{\\beta}_{\\text{OLS}} = \\arg \\min_\\beta \\|Y - X\\beta\\|_2^2 = (X^\\top X)^{-1} X^\\top Y.\n\\]classical assumptions (e.g., perfect collinearity, homoskedastic errors), \\(\\hat{\\beta}_{\\text{OLS}}\\) BLUE—Best Linear Unbiased Estimator.general form, parameter estimation, denoted \\(\\hat{\\beta}\\), focuses estimating relationship \\(y\\) \\(x\\), often view toward causality. many econometric statistical settings, write:\\[ y = x^\\top \\beta + \\varepsilon, \\]generally \\(y = g\\bigl(x;\\beta\\bigr) + \\varepsilon,\\) \\(\\beta\\) encodes structural causal parameters wish recover.core aim consistency—, large \\(n\\), want \\(\\hat{\\beta}\\) converge true \\(\\beta\\) defines underlying relationship. words:\\[ \\hat{\\beta}  \\xrightarrow{p}  \\beta, \\quad \\text{} n \\\\infty. \\]texts phrase informally requiring \\[ \\mathbb{E}\\bigl[\\hat{f}\\bigr] = f, \\]meaning estimator (asymptotically) unbiased true function parameters.However, consistency alone may suffice scientific inference. One often also examines:Asymptotic Normality: \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\;\\;\\xrightarrow{d}\\;\\; \\mathcal{N}(0,\\Sigma).\\)Confidence Intervals: \\(\\hat{\\beta}_j \\;\\pm\\; z_{\\alpha/2}\\,\\mathrm{SE}\\bigl(\\hat{\\beta}_j\\bigr).\\)Hypothesis Tests: \\(H_0\\colon \\beta_j = 0 \\quad\\text{vs.}\\quad H_1\\colon \\beta_j \\neq 0.\\)","code":""},{"path":"prediction-and-estimation.html","id":"causal-inference-fundamentals","chapter":"20 Prediction and Estimation","heading":"20.4.2 Causal Inference Fundamentals","text":"interpret \\(\\beta\\) \\(Y = X\\beta + \\varepsilon\\) “causal,” typically require changes \\(X\\) (least one component \\(X\\)) lead changes \\(Y\\) confounded omitted variables simultaneity. prototypical potential-outcomes framework (binary treatment \\(D\\)):\\(Y_i(1)\\): outcome unit \\(\\) receives treatment \\(D = 1\\).\\(Y_i(0)\\): outcome unit \\(\\) receives treatment \\(D = 0\\).observed outcome \\(Y_i\\) \\[\nY_i = D_i Y_i(1) + (1 - D_i) Y_i(0).\n\\]Average Treatment Effect (ATE) :\\[\n\\tau = \\mathbb{E}[Y(1) - Y(0)].\n\\]Identification \\(\\tau\\) requires assumption like unconfoundedness:\\[\n\\{Y(0), Y(1)\\} \\perp D \\mid X,\n\\].e., conditioning \\(X\\), treatment assignment -random. Estimation strategies revolve around properly adjusting \\(X\\).assumptions necessary raw prediction \\(Y\\): black-box function can yield \\(\\hat{Y} \\approx Y\\) without ensuring \\(\\hat{Y}(1) - \\hat{Y}(0)\\) unbiased estimate \\(\\tau\\).","code":""},{"path":"prediction-and-estimation.html","id":"role-of-identification","chapter":"20 Prediction and Estimation","heading":"20.4.3 Role of Identification","text":"Identification means parameter interest (\\(\\beta\\) \\(\\tau\\)) uniquely pinned distribution observables (assumptions). \\(\\beta\\) identified (e.g., endogeneity insufficient variation \\(X\\)), matter large sample, estimate \\(\\beta\\) consistently.prediction, “identification” usually main concern. function \\(\\hat{f}(x)\\) complicated ensemble method just fits well, without guaranteeing structural causal interpretation parameters.","code":""},{"path":"prediction-and-estimation.html","id":"challenges-1","chapter":"20 Prediction and Estimation","heading":"20.4.4 Challenges","text":"High-Dimensional Spaces: large \\(p\\) (number predictors), covariance among variables (multicollinearity) can hamper classical estimation. setting well-known bias-variance tradeoff (Hastie, Tibshirani, Friedman 2009; Bishop 2006).Endogeneity: \\(x\\) correlated error term \\(\\varepsilon\\), ordinary least squares (OLS) biased. Causal inference demands identifying exogenous variation \\(x\\), requires additional assumptions designs (e.g., randomization).Model Misspecification: functional form \\(g\\bigl(x;\\beta\\bigr)\\) incorrect, parameter estimates can systematically deviate capturing true underlying mechanism.","code":""},{"path":"prediction-and-estimation.html","id":"causation-versus-prediction","chapter":"20 Prediction and Estimation","heading":"20.5 Causation versus Prediction","text":"Understanding relationship causation prediction crucial statistical modeling. Building Kleinberg et al. (2015) Mullainathan Spiess (2017), consider scenario \\(Y\\) outcome variable dependent \\(X\\), want manipulate \\(X\\) maximize payoff function \\(\\pi(X,Y)\\). Formally:\\[\n\\pi(X,Y)\n=\n\\mathbb{E}\\bigl[\\,U(X,Y)\\bigr]\n\\quad\n\\text{objective measure}.\n\\]decision \\(X\\) depends changes \\(X\\) influence \\(\\pi\\). Taking derivative:\\[\n\\frac{d\\,\\pi(X,Y)}{dX}\n=\n\\frac{\\partial \\pi}{\\partial X}(Y)\n+\n\\frac{\\partial \\pi}{\\partial Y}\\,\\frac{\\partial Y}{\\partial X}.\n\\]can interpret terms:\\(\\displaystyle \\frac{\\partial \\pi}{\\partial X}\\): direct dependence payoff \\(X\\), can predicted can forecast \\(\\pi\\) changes \\(X\\).\\(\\displaystyle \\frac{\\partial Y}{\\partial X}\\): causal effect \\(X\\) \\(Y\\), essential understanding interventions \\(X\\) shift \\(Y\\).\\(\\displaystyle \\frac{\\partial \\pi}{\\partial Y}\\): marginal effect \\(Y\\) payoff.Hence, Kleinberg et al. (2015) frames distinction one predicting \\(Y\\) effectively (instance, “observe \\(X\\), can guess \\(Y\\)?”) versus managing causing \\(Y\\) change via interventions \\(X\\). Empirically:predict \\(Y\\), model \\(\\mathbb{E}\\bigl[Y\\mid X\\bigr]\\).infer causality, require identification strategies isolate exogenous variation \\(X\\).Empirical work economics, social science often aims estimate partial derivatives structural reduced-form equations:\\(\\displaystyle \\frac{\\partial Y}{\\partial X}\\): causal derivative; tells us \\(Y\\) changes intervene \\(X\\).\\(\\displaystyle \\frac{\\partial \\pi}{\\partial X}\\): effect \\(X\\) payoff, partially mediated changes \\(Y\\).Without proper identification (e.g., randomization, instrumental variables, difference--differences, quasi-experimental designs), risk conflating association (\\(\\hat{f}\\) predicts \\(Y\\)) causation (\\(\\hat{\\beta}\\) truly captures \\(X\\) shifts \\(Y\\)).illustrate concepts, consider following directed acyclic graph (DAG):","code":"\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(ggplot2)\n\n\n# Define the DAG structure with custom coordinates\ndag <- dagitty('\ndag {\n  X0 [pos=\"0,1\"]\n  X [pos=\"1,2\"]\n  Y [pos=\"1,1\"]\n  II [pos=\"1,0\"]\n\n  X0 -> Y\n  X0 -> II\n  X -> Y\n  Y -> II\n}\n')\n\n# Convert to ggdag format with manual layout\ndag_plot <- ggdag(dag) +\n    theme_void() +\n    geom_text(aes(x = 0.5, y = 1.2, label = \"Causation\"), size = 4) +\n    geom_text(aes(x = 0.3, y = 0.5, label = \"Prediction\"), size = 4) \n\n# Display the DAG\ndag_plot"},{"path":"prediction-and-estimation.html","id":"illustrative-equations-and-mathematical-contrasts","chapter":"20 Prediction and Estimation","heading":"20.6 Illustrative Equations and Mathematical Contrasts","text":", showcase derivations highlight predictive modeling vs. causal inference differ mathematical structure interpretation.","code":""},{"path":"prediction-and-estimation.html","id":"risk-minimization-vs.-consistency","chapter":"20 Prediction and Estimation","heading":"20.6.1 Risk Minimization vs. Consistency","text":"Consider real-valued outcome \\(Y\\) predictors \\(X\\). Let \\(\\ell(y, \\hat{y})\\) loss function, define Bayes regressor \\(f^*\\) :\\[\nf^* = \\arg \\min_f \\mathbb{E}[\\ell(Y, f(X))].\n\\]squared error loss, Bayes regressor \\(f^*(x) = \\mathbb{E}[Y \\mid X = x]\\).learning algorithm tries approximate \\(f^*\\). parametrize \\(f_\\beta(x) = x^\\top \\beta\\) empirical risk minimization large enough sample, \\(\\beta\\) converges minimizer :\\[\n\\beta^* = \\arg \\min_\\beta \\mathbb{E}[(Y - X^\\top \\beta)^2].\n\\]Note \\(\\beta^*\\) solution \\(\\mathbb{E}[XX^\\top] \\beta = \\mathbb{E}[XY]\\). \\(\\text{Cov}(X, X)\\) invertible, \\[\n\\beta^* = \\text{Cov}(X, X)^{-1} \\text{Cov}(X, Y).\n\\]\\(\\beta^*\\) necessarily “true” \\(\\beta_0\\) structural equation \\(Y = X\\beta_0 + \\varepsilon\\) unless \\(\\mathbb{E}[\\varepsilon \\mid X] = 0\\).predictive standpoint, \\(\\beta^*\\) best linear predictor sense mean squared error. causal standpoint, want \\(\\beta_0\\) \\(\\varepsilon\\) mean-independent \\(X\\). fails, \\(\\beta^* \\neq \\beta_0\\).","code":""},{"path":"prediction-and-estimation.html","id":"partial-derivatives-vs.-predictions","chapter":"20 Prediction and Estimation","heading":"20.6.2 Partial Derivatives vs. Predictions","text":"powerful way see difference compare:\\(\\frac{\\partial}{\\partial x} f^*(x)\\) – partial derivative best predictor w.r.t. \\(x\\). model’s prediction changes \\(x\\).\\(\\frac{\\partial}{\\partial x} m_\\beta(x)\\) – partial derivative structural function \\(m_\\beta(\\cdot)\\). true outcome \\(Y\\) changes \\(x\\), .e., causal effect \\(m_\\beta\\) indeed structural.Unless model identified assumptions hold (exogeneity, omitted variables, etc.), partial derivative purely predictive model represent causal effect.short: “slopes” black-box predictive model guaranteed reflect interventions \\(X\\) shift \\(Y\\).","code":""},{"path":"prediction-and-estimation.html","id":"example-high-dimensional-regularization","chapter":"20 Prediction and Estimation","heading":"20.6.3 Example: High-Dimensional Regularization","text":"Suppose large number predictors \\(p\\), possibly \\(p \\gg n\\). common approach prediction inference LASSO:\\[\n\\hat{\\beta}_{\\text{LASSO}} = \\arg \\min_\\beta \\left\\{ \\frac{1}{n} \\sum_{=1}^n (y_i - x_i^\\top \\beta)^2 + \\lambda \\|\\beta\\|_1 \\right\\}.\n\\]Prediction: Choose \\(\\lambda\\) optimize --sample MSE. bias introduced \\(\\hat{\\beta}\\), final model might predict extremely well, especially many true coefficients near zero.Causal Estimation: must worry whether LASSO shrinking zeroing confounders. crucial confounder’s coefficient set zero, resulting estimate treatment variable’s coefficient biased. Therefore, special procedures (like double/debiased machine learning approach (Chernozhukov et al. 2018)) introduced correct selection bias post-selection inference (Belloni, Chernozhukov, Hansen 2014).mathematics “best subset” prediction vs. valid coverage intervals parameters diverges significantly.","code":""},{"path":"prediction-and-estimation.html","id":"potential-outcomes-notation","chapter":"20 Prediction and Estimation","heading":"20.6.4 Potential Outcomes Notation","text":"Let \\(D \\\\{0, 1\\}\\) treatment indicator, define potential outcomes:\\[\nY_i(0), Y_i(1).\n\\]observed outcome :\\[\nY_i = D_i Y_i(1) + (1 - D_i) Y_i(0).\n\\]Prediction: One might train model \\(\\hat{Y} = \\hat{f}(X, D)\\) guess \\(Y\\) \\((X, D)\\). model black box guarantee \\(\\hat{Y}(1) - \\hat{Y}(0)\\) unbiased estimate \\(Y_i(1) - Y_i(0)\\).Causal Inference: want estimate \\(\\mathbb{E}[Y(1) - Y(0)]\\) \\(\\mathbb{E}[Y(1) - Y(0) \\mid X = x]\\). Identification typically requires \\(\\{Y(0), Y(1)\\} \\perp D \\mid X\\), .e., conditioning \\(X\\), treatment assignment -random. assumption, difference \\(\\hat{f}(x, 1) - \\hat{f}(x, 0)\\) can interpreted causal effect.","code":""},{"path":"prediction-and-estimation.html","id":"extended-mathematical-points","chapter":"20 Prediction and Estimation","heading":"20.7 Extended Mathematical Points","text":"now delve deeper mathematical nuances especially relevant distinguishing predictive vs. causal modeling.","code":""},{"path":"prediction-and-estimation.html","id":"m-estimation-and-asymptotic-theory","chapter":"20 Prediction and Estimation","heading":"20.7.1 M-Estimation and Asymptotic Theory","text":"\\(M\\)-Estimators unify many approaches: maximum likelihood, method moments, generalized method moments, quasi-likelihood estimators. Let \\(\\beta_0\\) true parameter define population criterion function:\\[\nQ(\\beta) = \\mathbb{E}[m(\\beta; X, Y)],\n\\]function \\(m\\). M-estimator \\(\\hat{\\beta}\\) solves:\\[\n\\hat{\\beta} = \\arg \\max_{\\beta \\\\Theta} \\frac{1}{n} \\sum_{=1}^n m(\\beta; X_i, Y_i).\n\\](\\(\\arg \\min\\), depending convention.)regularity conditions (Newey McFadden 1994; White 1980), :Consistency: \\(\\hat{\\beta} \\overset{p}{\\} \\beta_0\\).Asymptotic Normality: \\(\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\overset{d}{\\} N(0, \\Sigma)\\),\\(\\Sigma\\) derived derivatives \\(m(\\cdot; \\cdot, \\cdot)\\) distribution \\((X, Y)\\).prediction, classical asymptotic properties may less interest unless want build confidence intervals around predictions. causal inference, entire enterprise revolves around properties ensure valid inference \\(\\beta_0\\).","code":""},{"path":"prediction-and-estimation.html","id":"the-danger-of-omitted-variables","chapter":"20 Prediction and Estimation","heading":"20.7.2 The Danger of Omitted Variables","text":"Consider structural equation:\\[\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon,\n\\quad\n\\mathbb{E}[\\varepsilon \\mid X_1, X_2] = 0.\n\\]ignore \\(X_2\\) regress \\(Y\\) \\(X_1\\) , resulting \\(\\hat{\\beta}_1\\) can severely biased:\\[\n\\hat{\\beta}_1\n=\n\\arg\\min_{b} \\sum_{=1}^n \\bigl(y_i - b\\,x_{i1}\\bigr)^2.\n\\]expected value \\(\\hat{\\beta}_1\\) large samples :\\[\n\\beta_1\n\\;+\\;\n\\beta_2 \\,\\frac{\\mathrm{Cov}(X_1, X_2)}{\\mathrm{Var}(X_1)}.\n\\]extra term \\(\\displaystyle \\beta_2 \\,\\frac{\\mathrm{Cov}(X_1, X_2)}{\\mathrm{Var}(X_1)}\\) omitted variables bias. prediction, omitting \\(X_2\\) might sometimes acceptable \\(X_2\\) little incremental predictive value care accuracy domain. However, inference \\(\\beta_1\\), ignoring \\(X_2\\) invalidates causal interpretation.","code":""},{"path":"prediction-and-estimation.html","id":"cross-validation-vs.-statistical-testing","chapter":"20 Prediction and Estimation","heading":"20.7.3 Cross-Validation vs. Statistical Testing","text":"Cross-Validation: Predominantly used prediction tasks. split data training validation sets, measure --sample error, select hyperparameters minimize CV error.Cross-Validation: Predominantly used prediction tasks. split data training validation sets, measure --sample error, select hyperparameters minimize CV error.Statistical Testing: Predominantly used inference tasks. compute test statistics (e.g., \\(t\\)-test, Wald test), form confidence intervals, test hypotheses parameters (\\(H_0: \\beta_j = 0\\)).Statistical Testing: Predominantly used inference tasks. compute test statistics (e.g., \\(t\\)-test, Wald test), form confidence intervals, test hypotheses parameters (\\(H_0: \\beta_j = 0\\)).serve different objectives:CV predictive model selection.Testing scientific policy conclusions whether \\(\\beta_j\\) differs zero (.e., “particular variable causal effect?”).","code":""},{"path":"prediction-and-estimation.html","id":"putting-it-all-together-comparing-objectives","chapter":"20 Prediction and Estimation","heading":"20.8 Putting It All Together: Comparing Objectives","text":"overarching illustration, let \\(\\hat{f}\\) trained predictor (ML model, regression, etc.) let \\(\\hat{\\beta}\\) parameter estimator structural causal model. respective tasks differ:Form Output\n\\(\\hat{f}\\) function \\(\\mathcal{X} \\\\mathcal{Y}\\).\n\\(\\hat{\\beta}\\) vector parameters theoretical meaning.\n\\(\\hat{f}\\) function \\(\\mathcal{X} \\\\mathcal{Y}\\).\\(\\hat{\\beta}\\) vector parameters theoretical meaning.Criterion\nPrediction: Minimizes predictive loss \\(\\mathbb{E}[L(Y,\\hat{f}(X))]\\).\nCausal Inference: Seeks \\(\\beta\\) \\(Y = m_\\beta(X)\\) correct structural representation. Minimizes bias \\(\\beta\\), satisfies orthogonality conditions method--moments style, etc.\nPrediction: Minimizes predictive loss \\(\\mathbb{E}[L(Y,\\hat{f}(X))]\\).Causal Inference: Seeks \\(\\beta\\) \\(Y = m_\\beta(X)\\) correct structural representation. Minimizes bias \\(\\beta\\), satisfies orthogonality conditions method--moments style, etc.Validity\nPrediction: Usually validated --sample experiments cross-validation.\nEstimation: Validated theoretical identification arguments, assumptions exogeneity, randomization, omitted confounders.\nPrediction: Usually validated --sample experiments cross-validation.Estimation: Validated theoretical identification arguments, assumptions exogeneity, randomization, omitted confounders.Interpretation\nPrediction: “\\(\\hat{f}(x)\\) best guess \\(Y\\) new \\(x\\).”\nCausal Inference: “\\(\\beta\\) measures \\(Y\\) changes intervene \\(X\\).”\nPrediction: “\\(\\hat{f}(x)\\) best guess \\(Y\\) new \\(x\\).”Causal Inference: “\\(\\beta\\) measures \\(Y\\) changes intervene \\(X\\).”","code":""},{"path":"prediction-and-estimation.html","id":"conclusion","chapter":"20 Prediction and Estimation","heading":"20.9 Conclusion","text":"Prediction Estimation/Causal Inference serve distinctly different roles data analysis:Prediction: emphasis predictive accuracy. final model \\(\\hat{f}\\) may uninterpretable parameters (e.g., deep neural networks) yet excel forecasting \\(Y\\). Bias parameter estimates necessarily problematic reduces variance improves --sample performance.Prediction: emphasis predictive accuracy. final model \\(\\hat{f}\\) may uninterpretable parameters (e.g., deep neural networks) yet excel forecasting \\(Y\\). Bias parameter estimates necessarily problematic reduces variance improves --sample performance.Estimation/Causal Inference: emphasis obtaining consistent unbiased estimates parameters (\\(\\beta\\), treatment effect \\(\\tau\\)). impose stronger assumptions data collection relationship \\(X\\) \\(\\varepsilon\\). success criterion whether \\(\\hat{\\beta}\\approx\\beta_0\\) formal sense, valid confidence intervals robust identification strategies.Estimation/Causal Inference: emphasis obtaining consistent unbiased estimates parameters (\\(\\beta\\), treatment effect \\(\\tau\\)). impose stronger assumptions data collection relationship \\(X\\) \\(\\varepsilon\\). success criterion whether \\(\\hat{\\beta}\\approx\\beta_0\\) formal sense, valid confidence intervals robust identification strategies.Key Takeaway:question “predict \\(Y\\) new \\(X\\) accurately possible?”, prioritize prediction.question “predict \\(Y\\) new \\(X\\) accurately possible?”, prioritize prediction.question “changing \\(X\\) (assigning treatment \\(D\\)) affect \\(Y\\) causal sense?”, focus estimation fully developed identification strategy.question “changing \\(X\\) (assigning treatment \\(D\\)) affect \\(Y\\) causal sense?”, focus estimation fully developed identification strategy.","code":""},{"path":"sec-causal-inference.html","id":"sec-causal-inference","chapter":"21 Causal Inference","heading":"21 Causal Inference","text":"Throughout journey statistical concepts, ’ve uncovered patterns, relationships, trends data. now, arrive one profound questions research decision-making: truly causes ?’ve heard phrase—correlation causation.Correlation causation.Just two things move together doesn’t mean one pulling strings . Ice cream sales drowning incidents rise summer, ice cream isn’t blame.exactly causation? Let’s explore.One insightful books topic Book Judea Pearl (Pearl Mackenzie 2018), explains nuances causal reasoning beautifully. concise summary key ideas Pearl’s work, supplemented insights econometrics statistics.Understanding causal relationships essential research, particularly fields like economics, finance, marketing, medicine. statistical methods traditionally focused associational reasoning, causal inference allows us answer -questions make decisions based interventions.However, one must aware limitations statistical methods. discussed throughout book, relying solely data without incorporating domain expertise can lead misleading conclusions. establish causality, often need expert judgment, prior research, rigorous experimental design.may come across amusing examples spurious correlations—famous Tyler Vigen collection, shows absurd relationships (e.g., “number Nicholas Cage movies correlates drowning accidents”). highlight danger mistaking correlation causation.Historically, one earliest attempts infer causation using regression analysis Yule (1899), investigated effect relief policies poverty. Unfortunately, analysis suggested relief policies increased poverty—misleading conclusion due unaccounted confounders.long time, statistics largely causality-free discipline. field began addressing causation 1920s, Sewall Wright introduced path analysis, graphical approach representing causal relationships. However, wasn’t Judea Pearl’s Causal Revolution (1990s) gained formal calculus causation.Pearl’s framework introduced two key innovations:Causal Diagrams (Directed Acyclic Graphs) – graphical representation cause--effect relationships.Symbolic Language: -Operator (\\((X)\\)) – mathematical notation interventions.Traditional statistics deals conditional probabilities:\\[\nP(Y | X)\n\\]formula tells us probability event \\(Y\\) occurring given event \\(X\\) occurred. context observed data, \\(P(Y \\mid X)\\) reflects association \\(X\\) \\(Y\\), showing likely \\(Y\\) \\(X\\) happens.However, causal inference requires different concept:\\[\nP(Y | (X))\n\\]describes happens actively intervene set \\(X\\). crucial distinction :\\[\nP(Y | X) \\neq P(Y | (X))\n\\]general, passively observing \\(X\\) actively manipulating .make causal claims, need answer counterfactual questions:happened done \\(X\\)?concept essential fields like policy evaluation, medicine, business decision-making.build intelligent systems can reason causally, need inference engine:Pearl outlines three levels cognitive ability required causal learning:Seeing – Observing associations data.– Understanding interventions predicting outcomes.Imagining – Reasoning counterfactuals.levels correspond Ladder Causation.","code":""},{"path":"sec-causal-inference.html","id":"sec-the-ladder-of-causation","chapter":"21 Causal Inference","heading":"21.1 The Ladder of Causation","text":"Pearl’s Ladder Causation describes three hierarchical levels causal reasoning:(Adapted (Pearl 2019), p. 57)level requires cognitive ability data. Classical statistics operates Level 1 (association), causal inference enables us reach Levels 2 3.","code":""},{"path":"sec-causal-inference.html","id":"the-formal-notation-of-causality","chapter":"21 Causal Inference","heading":"21.2 The Formal Notation of Causality","text":"common mistake defining causation using probability:\\[\nX \\text{ causes } Y \\text{ } P(Y | X) > P(Y).\n\\]Seeing \\(X\\) (1st level) doesn’t mean probability Y increases.either \\(X\\) causes Y, \\(Z\\) affects \\(X\\) \\(Y\\). might able use control variables - \\(P(Y|X, Z = z) > P(Y|Z = z)\\). question becomes\nchoose \\(Z\\)?\nchoose enough \\(Z\\)?\nchoose right \\(Z\\)?\nchoose \\(Z\\)?choose enough \\(Z\\)?choose right \\(Z\\)?Hence, previous statement incorrect. correct causal statement :\\[\nP(Y | (X)) > P(Y).\n\\]causal diagrams -calculus, can formally express interventions answer questions 2nd level (Intervention).","code":""},{"path":"sec-causal-inference.html","id":"the-7-tools-of-structural-causal-models","chapter":"21 Causal Inference","heading":"21.3 The 7 Tools of Structural Causal Models","text":"Pearl’s Structural Causal Model (SCM) framework provides tools causal inference (Pearl 2019):Encoding Causal Assumptions – Using causal graphs transparency testability.-Calculus – Controlling confounding using backdoor criterion.Algorithmization Counterfactuals – Modeling “?” scenarios.Mediation Analysis – Understanding direct vs. indirect effects.External Validity & Adaptability – Addressing selection bias domain adaptation.Handling Missing Data – Using causal methods infer missing information.Causal Discovery – Learning causal relationships data using:\nd-separation\nFunctional decomposition (Hoyer et al. 2008)\nSpontaneous local changes (Pearl 2014)\nd-separationFunctional decomposition (Hoyer et al. 2008)Spontaneous local changes (Pearl 2014)","code":""},{"path":"sec-causal-inference.html","id":"simpsons-paradox","chapter":"21 Causal Inference","heading":"21.4 Simpson’s Paradox","text":"Simpson’s Paradox one striking examples causality matters simple statistical associations can misleading.","code":""},{"path":"sec-causal-inference.html","id":"what-is-simpsons-paradox","chapter":"21 Causal Inference","heading":"21.4.1 What is Simpson’s Paradox?","text":"core, Simpson’s Paradox occurs :trend observed overall population reverses population divided subgroups.means statistical associations raw data can misleading important confounding variables ignored.","code":""},{"path":"sec-causal-inference.html","id":"why-is-this-important","chapter":"21 Causal Inference","heading":"21.4.2 Why is this Important?","text":"Understanding Simpson’s Paradox critical causal inference :highlights danger naive data analysis – Just looking overall trends can lead incorrect conclusions.emphasizes importance confounding variables – must control relevant factors making causal claims.demonstrates causal reasoning necessary – Simply relying statistical associations (\\(P(Y | X)\\)) without considering structural relationships can lead paradoxical results.","code":""},{"path":"sec-causal-inference.html","id":"comparison-between-simpsons-paradox-and-omitted-variable-bias","chapter":"21 Causal Inference","heading":"21.4.3 Comparison between Simpson’s Paradox and Omitted Variable Bias","text":"Simpson’s Paradox occurs trend overall dataset reverses broken subgroups. happens due data aggregation issues, differences subgroup sizes distort overall trend.often resembles omitted variable bias (OVB)—missing confounders lead misleading conclusions—Simpson’s Paradox just causal inference problem. mathematical phenomenon can arise purely improper weighting data, even descriptive statistics.Similarities Simpson’s Paradox OVB:involve missing variable:Simpson’s Paradox, key confounding variable (e.g., customer segment) hidden aggregate data, leading misleading conclusions.Simpson’s Paradox, key confounding variable (e.g., customer segment) hidden aggregate data, leading misleading conclusions.OVB, relevant variable (e.g., seasonality) missing regression model, causing bias.OVB, relevant variable (e.g., seasonality) missing regression model, causing bias.distort causal conclusions:OVB biases effect estimates failing control confounding.OVB biases effect estimates failing control confounding.Simpson’s Paradox flips statistical relationships controlling confounder.Simpson’s Paradox flips statistical relationships controlling confounder.Differences Simpson’s Paradox OVB:OVB cases show Simpson’s Paradox:OVB generally causes bias, doesn’t always create reversal trends.OVB generally causes bias, doesn’t always create reversal trends.Example: seasonality increases ad spend sales, omitting inflates ad spend → sales relationship necessarily reverse .Example: seasonality increases ad spend sales, omitting inflates ad spend → sales relationship necessarily reverse .Simpson’s Paradox can occur even without causal inference:Simpson’s Paradox mathematical/statistical phenomenon can arise even purely observational data, just causal inference.Simpson’s Paradox mathematical/statistical phenomenon can arise even purely observational data, just causal inference.results data weighting issues, even causality considered.results data weighting issues, even causality considered.OVB model specification issue; Simpson’s Paradox data aggregation issue:OVB occurs regression models fail include relevant predictors.OVB occurs regression models fail include relevant predictors.Simpson’s Paradox arises incorrect data aggregation groups properly analyzed separately.Simpson’s Paradox arises incorrect data aggregation groups properly analyzed separately.Right Way Think ItSimpson’s Paradox often caused omitted variable bias, thing.Simpson’s Paradox often caused omitted variable bias, thing.OVB problem causal inference models; Simpson’s Paradox problem raw data interpretation.OVB problem causal inference models; Simpson’s Paradox problem raw data interpretation.Fix Issues?OVB: Use causal diagrams, add control variables, use regression adjustments.OVB: Use causal diagrams, add control variables, use regression adjustments.Simpson’s Paradox: Always analyze subgroup-level trends making conclusions based aggregate data.Simpson’s Paradox: Always analyze subgroup-level trends making conclusions based aggregate data.Bottom line: Simpson’s Paradox often caused omitted variable bias, just OVB—fundamental issue misleading data aggregation.Bottom line: Simpson’s Paradox often caused omitted variable bias, just OVB—fundamental issue misleading data aggregation.","code":""},{"path":"sec-causal-inference.html","id":"illustrating-simpsons-paradox-marketing-campaign-success-rates","chapter":"21 Causal Inference","heading":"21.4.4 Illustrating Simpson’s Paradox: Marketing Campaign Success Rates","text":"Let’s explore paradox using practical business example.","code":""},{"path":"sec-causal-inference.html","id":"scenario-marketing-campaign-performance","chapter":"21 Causal Inference","heading":"21.4.4.1 Scenario: Marketing Campaign Performance","text":"Imagine company running two marketing campaigns, Campaign Campaign B, attract new customers. analyze campaign higher conversion rate.","code":""},{"path":"sec-causal-inference.html","id":"step-1-creating-the-data","chapter":"21 Causal Inference","heading":"21.4.4.2 Step 1: Creating the Data","text":"simulate conversion rates two different customer segments: High-Value customers (typically convert higher rate) Low-Value customers (convert lower rate).Interpreting DataCampaign B High-Value segment: \\(\\frac{180}{300} = 60\\%\\)Campaign B High-Value segment: \\(\\frac{180}{300} = 60\\%\\)Campaign High-Value segment: \\(\\frac{290}{500} = 58\\%\\)\n=> B better High-Value segment (60% vs 58%).Campaign High-Value segment: \\(\\frac{290}{500} = 58\\%\\)=> B better High-Value segment (60% vs 58%).Campaign B Low-Value segment: \\(\\frac{270}{3000} = 9\\%\\)Campaign B Low-Value segment: \\(\\frac{270}{3000} = 9\\%\\)Campaign Low-Value segment: \\(\\frac{170}{2000} = 8.5\\%\\)\n=> B better Low-Value segment (9% vs 8.5%).Campaign Low-Value segment: \\(\\frac{170}{2000} = 8.5\\%\\)=> B better Low-Value segment (9% vs 8.5%).Thus, B outperforms individual segment.","code":"\n# Load necessary libraries\nlibrary(dplyr)\n\n# Create a dataset where:\n#  - B is better than A in each individual segment.\n#  - A turns out better when we look at the overall (aggregated) data.\n\nmarketing_data <- data.frame(\n  Campaign = c(\"A\", \"A\", \"B\", \"B\"),\n  Segment  = c(\"High-Value\", \"Low-Value\", \"High-Value\", \"Low-Value\"),\n  Visitors = c(500, 2000, 300, 3000),    # total visitors in each segment\n  Conversions = c(290, 170, 180, 270)   # successful conversions\n)\n\n# Compute segment-level conversion rate\nmarketing_data <- marketing_data %>%\n  mutate(Conversion_Rate = Conversions / Visitors)\n\n# Print the data\nprint(marketing_data)\n#>   Campaign    Segment Visitors Conversions Conversion_Rate\n#> 1        A High-Value      500         290           0.580\n#> 2        A  Low-Value     2000         170           0.085\n#> 3        B High-Value      300         180           0.600\n#> 4        B  Low-Value     3000         270           0.090"},{"path":"sec-causal-inference.html","id":"step-2-aggregating-data-ignoring-customer-segments","chapter":"21 Causal Inference","heading":"21.4.4.3 Step 2: Aggregating Data (Ignoring Customer Segments)","text":"Now, let’s calculate overall conversion rate campaign without considering customer segments.","code":"\n# Compute overall conversion rates for each campaign\noverall_rates <- marketing_data %>%\n  group_by(Campaign) %>%\n  summarise(\n    Total_Visitors     = sum(Visitors),\n    Total_Conversions  = sum(Conversions),\n    Overall_Conversion_Rate = Total_Conversions / Total_Visitors\n  )\n\n# Print overall conversion rates\nprint(overall_rates)\n#> # A tibble: 2 × 4\n#>   Campaign Total_Visitors Total_Conversions Overall_Conversion_Rate\n#>   <chr>             <dbl>             <dbl>                   <dbl>\n#> 1 A                  2500               460                   0.184\n#> 2 B                  3300               450                   0.136"},{"path":"sec-causal-inference.html","id":"step-3-observing-simpsons-paradox","chapter":"21 Causal Inference","heading":"21.4.4.4 Step 3: Observing Simpson’s Paradox","text":"Let’s determine campaign appears higher conversion rate.Even though Campaign B better segment, see Campaign higher aggregated (overall) conversion rate!","code":"\n# Identify the campaign with the higher overall conversion rate\nbest_campaign_overall <- overall_rates %>%\n    filter(Overall_Conversion_Rate == max(Overall_Conversion_Rate)) %>%\n    select(Campaign, Overall_Conversion_Rate)\n\nprint(best_campaign_overall)\n#> # A tibble: 1 × 2\n#>   Campaign Overall_Conversion_Rate\n#>   <chr>                      <dbl>\n#> 1 A                          0.184"},{"path":"sec-causal-inference.html","id":"step-4-conversion-rates-within-customer-segments","chapter":"21 Causal Inference","heading":"21.4.4.5 Step 4: Conversion Rates Within Customer Segments","text":"now analyze conversion rates separately high-value low-value customers.High-Value, B > .High-Value, B > .Low-Value, B > .Low-Value, B > .Yet, overall, > B.reversal hallmark Simpson’s Paradox.","code":"\n# Compute conversion rates by customer segment\nby_segment <- marketing_data %>%\n  select(Campaign, Segment, Conversion_Rate) %>%\n  arrange(Segment)\n\nprint(by_segment)\n#>   Campaign    Segment Conversion_Rate\n#> 1        A High-Value           0.580\n#> 2        B High-Value           0.600\n#> 3        A  Low-Value           0.085\n#> 4        B  Low-Value           0.090"},{"path":"sec-causal-inference.html","id":"step-5-visualizing-the-paradox","chapter":"21 Causal Inference","heading":"21.4.4.6 Step 5: Visualizing the Paradox","text":"make clearer, let’s visualize results.bar chart reveals segments, B’s bar taller (.e., B’s conversion rate higher). examined segment-level data, conclude B superior campaign.However, aggregate data (ignore segments), get opposite conclusion — better overall.","code":"\nlibrary(ggplot2)\n\n# Plot conversion rates by campaign and segment\nggplot(marketing_data,\n       aes(x = Segment,\n           y = Conversion_Rate,\n           fill = Campaign)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Simpson’s Paradox in Marketing\",\n    x     = \"Customer Segment\",\n    y     = \"Conversion Rate\"\n  ) +\n  theme_minimal()"},{"path":"sec-causal-inference.html","id":"why-does-this-happen","chapter":"21 Causal Inference","heading":"21.4.5 Why Does This Happen?","text":"paradox arises confounding variable — case, distribution visitors across segments.Campaign traffic High-Value segment (conversions generally high).Campaign traffic High-Value segment (conversions generally high).Campaign B many visitors Low-Value segment.Campaign B many visitors Low-Value segment.volume Low-Value visitors B extremely large (3000 vs. 2000 ), weighs B’s overall average , allowing ’s overall rate exceed B’s.","code":""},{"path":"sec-causal-inference.html","id":"how-does-causal-inference-solve-this","chapter":"21 Causal Inference","heading":"21.4.6 How Does Causal Inference Solve This?","text":"avoid Simpson’s Paradox, need move beyond association use causal analysis:Use causal diagrams (DAGs) model relationships\nmarketing campaign choice confounded customer segment.\nmust control confounding variable.\nUse causal diagrams (DAGs) model relationshipsThe marketing campaign choice confounded customer segment.marketing campaign choice confounded customer segment.must control confounding variable.must control confounding variable.Use stratification regression adjustment\nInstead comparing raw conversion rates, compare rates within customer segment.\nensures confounding factors distort results.\nUse stratification regression adjustmentInstead comparing raw conversion rates, compare rates within customer segment.Instead comparing raw conversion rates, compare rates within customer segment.ensures confounding factors distort results.ensures confounding factors distort results.Use -operator simulate interventions\nInstead asking \\(P(\\text{Conversion} \\mid \\text{Campaign})\\), ask: \\(P(\\text{Conversion} \\mid (\\text{Campaign}))\\)\nestimates happen randomly assigned campaigns (removing confounding bias).\nUse -operator simulate interventionsInstead asking \\(P(\\text{Conversion} \\mid \\text{Campaign})\\), ask: \\(P(\\text{Conversion} \\mid (\\text{Campaign}))\\)Instead asking \\(P(\\text{Conversion} \\mid \\text{Campaign})\\), ask: \\(P(\\text{Conversion} \\mid (\\text{Campaign}))\\)estimates happen randomly assigned campaigns (removing confounding bias).estimates happen randomly assigned campaigns (removing confounding bias).","code":""},{"path":"sec-causal-inference.html","id":"correcting-simpsons-paradox-with-regression-adjustment","chapter":"21 Causal Inference","heading":"21.4.7 Correcting Simpson’s Paradox with Regression Adjustment","text":"Let’s adjust confounding variable using logistic regression.model includes Campaign Segment predictors, giving clearer picture true effect campaign conversion, controlling differences segment composition.","code":"\n# Logistic regression adjusting for the Segment\nmodel <- glm(\n  cbind(Conversions, Visitors - Conversions) ~ Campaign + Segment,\n  family = binomial(),\n  data   = marketing_data\n)\n\nsummary(model)\n#> \n#> Call:\n#> glm(formula = cbind(Conversions, Visitors - Conversions) ~ Campaign + \n#>     Segment, family = binomial(), data = marketing_data)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       0.32783    0.07839   4.182 2.89e-05 ***\n#> CampaignB         0.06910    0.08439   0.819    0.413    \n#> SegmentLow-Value -2.70806    0.08982 -30.151  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 977.473003  on 3  degrees of freedom\n#> Residual deviance:   0.012337  on 1  degrees of freedom\n#> AIC: 32.998\n#> \n#> Number of Fisher Scoring iterations: 3"},{"path":"sec-causal-inference.html","id":"key-takeaways-3","chapter":"21 Causal Inference","heading":"21.4.8 Key Takeaways","text":"Simpson’s Paradox demonstrates causal inference essential.\nAggregated statistics can misleading due hidden confounding.\nBreaking data subgroups can reverse conclusions.\nSimpson’s Paradox demonstrates causal inference essential.Aggregated statistics can misleading due hidden confounding.Aggregated statistics can misleading due hidden confounding.Breaking data subgroups can reverse conclusions.Breaking data subgroups can reverse conclusions.Causal reasoning helps identify correct paradoxes.\nUsing causal graphs, -calculus, adjustment techniques, can find true causal effect.\nCausal reasoning helps identify correct paradoxes.Using causal graphs, -calculus, adjustment techniques, can find true causal effect.Naïve data analysis can lead bad business decisions.\ncompany allocated budget Campaign B based overall conversion rates, might investing wrong strategy!\nNaïve data analysis can lead bad business decisions.company allocated budget Campaign B based overall conversion rates, might investing wrong strategy!","code":""},{"path":"sec-causal-inference.html","id":"additional-resources-1","chapter":"21 Causal Inference","heading":"21.5 Additional Resources","text":"explore causal inference R, check CRAN Task View Causal Inference:reading:Book – Judea Pearl (Pearl Mackenzie 2018)Causal Inference Statistics: Primer – Pearl, Glymour, JewellCausality: Models, Reasoning, Inference – Judea Pearl","code":""},{"path":"sec-causal-inference.html","id":"experimental-vs.-quasi-experimental-designs","chapter":"21 Causal Inference","heading":"21.6 Experimental vs. Quasi-Experimental Designs","text":"Experimental quasi-experimental designs differ approach causal inference. table summarizes key distinctions:","code":""},{"path":"sec-causal-inference.html","id":"criticisms-of-quasi-experimental-designs","chapter":"21 Causal Inference","heading":"21.6.1 Criticisms of Quasi-Experimental Designs","text":"Quasi-experimental methods always approximate experimental results accurately. instance, LaLonde (1986) demonstrates commonly used methods :Matching MethodsDifference--differences[Tobit-2] (Heckman-type models)often fail replicate experimental estimates reliably. finding cast serious doubt credibility observational studies estimating causal effects, igniting ongoing debate econometrics statistics reliability nonexperimental evaluations.LaLonde’s critical assessment served catalyst significant methodological practical advancements causal inference. decades since publication, field evolved considerably, introducing theoretical innovations empirical practices aimed addressing limitations exposed (G. Imbens Xu 2024). Among advances :Emphasis estimators based unconfoundedness (selection observables):\nModern causal inference frameworks frequently adopt unconfoundedness conditional independence assumption. premise, treatment assignment assumed independent potential outcomes, conditional observed covariates. theoretical foundation underpins many widely used estimation techniques, matching methods, inverse probability weighting, regression adjustment.Emphasis estimators based unconfoundedness (selection observables):\nModern causal inference frameworks frequently adopt unconfoundedness conditional independence assumption. premise, treatment assignment assumed independent potential outcomes, conditional observed covariates. theoretical foundation underpins many widely used estimation techniques, matching methods, inverse probability weighting, regression adjustment.Focus covariate overlap (common support):\nResearchers now recognize critical importance overlap, also referred common support, distributions covariates across treatment control groups. Without sufficient overlap, comparisons treated untreated units rely extrapolation, weakens causal claims. Modern methods explicitly assess often impose restrictions ensure overlap proceeding estimation.Focus covariate overlap (common support):\nResearchers now recognize critical importance overlap, also referred common support, distributions covariates across treatment control groups. Without sufficient overlap, comparisons treated untreated units rely extrapolation, weakens causal claims. Modern methods explicitly assess often impose restrictions ensure overlap proceeding estimation.Introduction propensity score-based methods doubly robust estimators:\nintroduction propensity score methods (Rosenbaum Rubin 1983) breakthrough, offering way reduce dimensionality covariate space balancing observed characteristics across groups. recently, doubly robust estimators emerged, combining propensity score weighting outcome regression. estimators provide consistent treatment effect estimates either propensity score model outcome model correctly specified, offering greater robustness practice.Introduction propensity score-based methods doubly robust estimators:\nintroduction propensity score methods (Rosenbaum Rubin 1983) breakthrough, offering way reduce dimensionality covariate space balancing observed characteristics across groups. recently, doubly robust estimators emerged, combining propensity score weighting outcome regression. estimators provide consistent treatment effect estimates either propensity score model outcome model correctly specified, offering greater robustness practice.Greater emphasis validation exercises bolster credibility:\nModern studies increasingly incorporate validation techniques evaluate credibility findings. Placebo tests, falsification exercises, sensitivity analyses commonly employed assess whether estimated effects may driven unobserved confounding model misspecification. practices go beyond traditional goodness--fit statistics, directly interrogating assumptions underlying causal inference.Greater emphasis validation exercises bolster credibility:\nModern studies increasingly incorporate validation techniques evaluate credibility findings. Placebo tests, falsification exercises, sensitivity analyses commonly employed assess whether estimated effects may driven unobserved confounding model misspecification. practices go beyond traditional goodness--fit statistics, directly interrogating assumptions underlying causal inference.Methods estimating exploiting treatment effect heterogeneity:\nBeyond estimating average treatment effects, contemporary research frequently explores heterogeneous treatment effects. methods identify subgroups may experience different causal impacts, particular relevance fields like personalized marketing, targeted interventions, policy design.Methods estimating exploiting treatment effect heterogeneity:\nBeyond estimating average treatment effects, contemporary research frequently explores heterogeneous treatment effects. methods identify subgroups may experience different causal impacts, particular relevance fields like personalized marketing, targeted interventions, policy design.illustrate practical lessons methodological advances, G. Imbens Xu (2024) reexamine two canonical datasets:LaLonde’s National Supported Work Demonstration dataThe Imbens-Rubin-Sacerdote draft lottery dataApplying modern causal inference methods datasets demonstrates , sufficient covariate overlap exists, robust estimates adjusted differences treatment control groups can achieved. However, critical underscore robustness estimation equate validity. Without direct validation exercises, placebo tests, even well-behaved estimates may misleading.G. Imbens Xu (2024) highlight several key lessons practitioners working nonexperimental data estimate causal effects:Careful examination assignment process essential.\nUnderstanding mechanisms units assigned treatment control conditions informs plausibility unconfoundedness assumption.Careful examination assignment process essential.\nUnderstanding mechanisms units assigned treatment control conditions informs plausibility unconfoundedness assumption.Inspection covariate overlap non-negotiable.\nWithout sufficient overlap, causal effect estimation may rely heavily model extrapolation, undermining credibility.Inspection covariate overlap non-negotiable.\nWithout sufficient overlap, causal effect estimation may rely heavily model extrapolation, undermining credibility.Validation exercises indispensable.\nPlacebo tests falsification strategies help ensure estimated treatment effects artifacts modeling choices unobserved confounding.Validation exercises indispensable.\nPlacebo tests falsification strategies help ensure estimated treatment effects artifacts modeling choices unobserved confounding.methodological advances substantially improved tools available causal inference observational data, effective application requires rigorous attention underlying assumptions diligent validation support credible causal claims.","code":""},{"path":"sec-causal-inference.html","id":"hierarchical-ordering-of-causal-tools","chapter":"21 Causal Inference","heading":"21.7 Hierarchical Ordering of Causal Tools","text":"Causal inference tools can categorized based methodological rigor, randomized controlled trials (RCTs) considered gold standard.Experimental Design: Randomized Control Trials (Gold standard)Experimental Design: Randomized Control Trials (Gold standard)Quasi-experimental\nRegression Discontinuity\nSynthetic Difference--Differences\nDifference--Differences\nSynthetic Control\nEvent Studies\nFixed Effects Estimator\nEndogenous Treatment: mostly Instrumental Variables\nMatching Methods\nInterrupted Time Series\nEndogenous Sample Selection\nQuasi-experimentalRegression DiscontinuityRegression DiscontinuitySynthetic Difference--DifferencesSynthetic Difference--DifferencesDifference--DifferencesDifference--DifferencesSynthetic ControlSynthetic ControlEvent StudiesEvent StudiesFixed Effects EstimatorFixed Effects EstimatorEndogenous Treatment: mostly Instrumental VariablesEndogenous Treatment: mostly Instrumental VariablesMatching MethodsMatching MethodsInterrupted Time SeriesInterrupted Time SeriesEndogenous Sample SelectionEndogenous Sample Selection","code":""},{"path":"sec-causal-inference.html","id":"types-of-validity-in-research","chapter":"21 Causal Inference","heading":"21.8 Types of Validity in Research","text":"Validity research includes:Measurement Validity (e.g., construct, content, criterion, face validity)Measurement Validity (e.g., construct, content, criterion, face validity)Internal ValidityInternal ValidityExternal ValidityExternal ValidityEcological ValidityEcological ValidityStatistical Conclusion ValidityStatistical Conclusion ValidityBy examining , can ensure study’s measurements accurate, findings reliably causal, conclusions generalize broader contexts.","code":""},{"path":"sec-causal-inference.html","id":"sec-measurement-validity","chapter":"21 Causal Inference","heading":"21.8.1 Measurement Validity","text":"Measurement validity pertains whether instrument method use truly measures ’s intended measure. Within umbrella, several sub-types:","code":""},{"path":"sec-causal-inference.html","id":"face-validity","chapter":"21 Causal Inference","heading":"21.8.1.1 Face Validity","text":"Definition: extent measurement test appears measure supposed measure, face value (.e., “look” right experts users?).Definition: extent measurement test appears measure supposed measure, face value (.e., “look” right experts users?).Importance: often considered less rigorous form validity, ’s useful ensuring test instrument intuitively acceptable stakeholders, participants, experts field.Importance: often considered less rigorous form validity, ’s useful ensuring test instrument intuitively acceptable stakeholders, participants, experts field.Example: questionnaire measuring “anxiety” questions nervousness, worries, stress good face validity obviously seems address anxiety.Example: questionnaire measuring “anxiety” questions nervousness, worries, stress good face validity obviously seems address anxiety.","code":""},{"path":"sec-causal-inference.html","id":"content-validity","chapter":"21 Causal Inference","heading":"21.8.1.2 Content Validity","text":"Definition: extent test measurement covers relevant facets construct aims measure.Definition: extent test measurement covers relevant facets construct aims measure.Importance: Especially critical fields like education psychological testing, want ensure entire domain subject/construct properly sampled.Importance: Especially critical fields like education psychological testing, want ensure entire domain subject/construct properly sampled.Example: math test includes questions algebra, geometry, calculus might high content validity comprehensive math skill assessment. tested algebra, content validity low.Example: math test includes questions algebra, geometry, calculus might high content validity comprehensive math skill assessment. tested algebra, content validity low.","code":""},{"path":"sec-causal-inference.html","id":"sec-construct-validity","chapter":"21 Causal Inference","heading":"21.8.2 Construct Validity","text":"Definition: degree test measurement tool accurately represents theoretical construct intends measure (e.g., intelligence, motivation, self-esteem).Definition: degree test measurement tool accurately represents theoretical construct intends measure (e.g., intelligence, motivation, self-esteem).Types Evidence:\nConvergent Validity: Demonstrated measures supposed related (theoretically) observed correlate.\nDiscriminant (Divergent) Validity: Demonstrated measures supposed unrelated theoretically correlate.\nTypes Evidence:Convergent Validity: Demonstrated measures supposed related (theoretically) observed correlate.Convergent Validity: Demonstrated measures supposed related (theoretically) observed correlate.Discriminant (Divergent) Validity: Demonstrated measures supposed unrelated theoretically correlate.Discriminant (Divergent) Validity: Demonstrated measures supposed unrelated theoretically correlate.Example: new questionnaire “job satisfaction” correlate established job satisfaction questionnaires (convergent validity) correlate strongly unrelated constructs like “physical health” (discriminant validity).Example: new questionnaire “job satisfaction” correlate established job satisfaction questionnaires (convergent validity) correlate strongly unrelated constructs like “physical health” (discriminant validity).","code":""},{"path":"sec-causal-inference.html","id":"sec-criterion-validity","chapter":"21 Causal Inference","heading":"21.8.3 Criterion Validity","text":"Definition: extent measurement predicts correlates outcome criterion. words, scores measure relate external standard “criterion”?Definition: extent measurement predicts correlates outcome criterion. words, scores measure relate external standard “criterion”?Types:\nPredictive Validity: measure predicts future outcome (e.g., entrance exam predicting college success).\nConcurrent Validity: measure correlates existing, accepted measure taken time (e.g., new depression scale compared gold-standard clinical interview).\nTypes:Predictive Validity: measure predicts future outcome (e.g., entrance exam predicting college success).Predictive Validity: measure predicts future outcome (e.g., entrance exam predicting college success).Concurrent Validity: measure correlates existing, accepted measure taken time (e.g., new depression scale compared gold-standard clinical interview).Concurrent Validity: measure correlates existing, accepted measure taken time (e.g., new depression scale compared gold-standard clinical interview).Example: new test driving skills high criterion validity people score highly perform better actual road tests (predictive validity).Example: new test driving skills high criterion validity people score highly perform better actual road tests (predictive validity).","code":""},{"path":"sec-causal-inference.html","id":"sec-internal-validity","chapter":"21 Causal Inference","heading":"21.8.4 Internal Validity","text":"Internal validity refers extent study can establish cause--effect relationship. High internal validity means can confident observed effects due treatment intervention due confounding factors alternative explanations. validity economists applied scientists largely care .","code":""},{"path":"sec-causal-inference.html","id":"major-threats-to-internal-validity","chapter":"21 Causal Inference","heading":"21.8.4.1 Major Threats to Internal Validity","text":"Selection Bias: Systematic differences groups exist treatment applied.Selection Bias: Systematic differences groups exist treatment applied.History Effects: External events occurring study can affect outcomes (e.g., economic downturn job-training study).History Effects: External events occurring study can affect outcomes (e.g., economic downturn job-training study).Maturation: Participants might change time simply due aging, learning, fatigue, etc., independent treatment.Maturation: Participants might change time simply due aging, learning, fatigue, etc., independent treatment.Testing Effects: Taking test can influence participants’ responses (practice effect).Testing Effects: Taking test can influence participants’ responses (practice effect).Instrumentation: Changes measurement instrument observers can lead inconsistencies data collection.Instrumentation: Changes measurement instrument observers can lead inconsistencies data collection.Regression Mean: Extreme pre-test scores tend move closer average subsequent tests.Regression Mean: Extreme pre-test scores tend move closer average subsequent tests.Attrition (Mortality): Participants dropping study ways systematically related treatment outcomes.Attrition (Mortality): Participants dropping study ways systematically related treatment outcomes.","code":""},{"path":"sec-causal-inference.html","id":"strategies-to-improve-internal-validity","chapter":"21 Causal Inference","heading":"21.8.4.2 Strategies to Improve Internal Validity","text":"Random Assignment: Ensures , average, groups equivalent known unknown variables.Random Assignment: Ensures , average, groups equivalent known unknown variables.Control Groups: Provide baseline comparison isolate effect intervention.Control Groups: Provide baseline comparison isolate effect intervention.Blinding (Single-, Double-, Triple-blind): Reduces biases participants, researchers, analysts.Blinding (Single-, Double-, Triple-blind): Reduces biases participants, researchers, analysts.Standardized Procedures Protocols: Minimizes variability interventions measurements administered.Standardized Procedures Protocols: Minimizes variability interventions measurements administered.Matching Stratification: randomization possible, matching participants key characteristics can reduce selection bias.Matching Stratification: randomization possible, matching participants key characteristics can reduce selection bias.Pretest-Posttest Designs: Compare participant performance intervention (though watch testing effects).Pretest-Posttest Designs: Compare participant performance intervention (though watch testing effects).","code":""},{"path":"sec-causal-inference.html","id":"sec-external-validity","chapter":"21 Causal Inference","heading":"21.8.5 External Validity","text":"External validity addresses generalizability findings beyond specific context study. study high external validity can applied populations, settings, times. hand, localness can affect external validity.","code":""},{"path":"sec-causal-inference.html","id":"subtypes-or-related-concepts-of-external-validity","chapter":"21 Causal Inference","heading":"21.8.5.1 Subtypes (or Related Concepts) of External Validity","text":"Population Validity: degree study findings can generalized larger population sample drawn.Population Validity: degree study findings can generalized larger population sample drawn.Ecological Validity (sometimes considered separately): Whether findings obtained controlled conditions can applied real-world settings.Ecological Validity (sometimes considered separately): Whether findings obtained controlled conditions can applied real-world settings.Temporal Validity: Whether results study hold true time. Changing societal norms, technologies, economic conditions might render findings obsolete.Temporal Validity: Whether results study hold true time. Changing societal norms, technologies, economic conditions might render findings obsolete.","code":""},{"path":"sec-causal-inference.html","id":"threats-to-external-validity","chapter":"21 Causal Inference","heading":"21.8.5.2 Threats to External Validity","text":"Unrepresentative Samples: sample reflect wider population (demographics, culture, etc.), generalization limited.Unrepresentative Samples: sample reflect wider population (demographics, culture, etc.), generalization limited.Artificial Research Environments: Highly controlled lab settings may capture real-world complexities.Artificial Research Environments: Highly controlled lab settings may capture real-world complexities.Treatment-Setting Interaction: effect treatment might depend unique conditions setting (e.g., particular school, hospital, region).Treatment-Setting Interaction: effect treatment might depend unique conditions setting (e.g., particular school, hospital, region).Treatment-Selection Interaction: Certain characteristics selected participants might interact treatment (e.g., results specialized population apply general public).Treatment-Selection Interaction: Certain characteristics selected participants might interact treatment (e.g., results specialized population apply general public).","code":""},{"path":"sec-causal-inference.html","id":"strategies-to-improve-external-validity","chapter":"21 Causal Inference","heading":"21.8.5.3 Strategies to Improve External Validity","text":"Use Diverse Representative Samples: Recruit participants mirror larger population.Use Diverse Representative Samples: Recruit participants mirror larger population.Field Studies Naturalistic Settings: Conduct experiments real-world environments rather artificial labs.Field Studies Naturalistic Settings: Conduct experiments real-world environments rather artificial labs.Replication Multiple Contexts: Replicate study across different settings, geographic locations, populations.Replication Multiple Contexts: Replicate study across different settings, geographic locations, populations.Longitudinal Studies: Evaluate whether relationships hold extended periods.Longitudinal Studies: Evaluate whether relationships hold extended periods.","code":""},{"path":"sec-causal-inference.html","id":"sec-ecological-validity","chapter":"21 Causal Inference","heading":"21.8.6 Ecological Validity","text":"Ecological validity often discussed subcategory external validity. specifically focuses realism study environment tasks:Definition: degree study findings can generalized real-life settings people actually live, work, interact.Definition: degree study findings can generalized real-life settings people actually live, work, interact.Key Idea: Even lab experiment shows particular behavior, people behave way daily lives everyday distractions, social pressures, contextual factors?Key Idea: Even lab experiment shows particular behavior, people behave way daily lives everyday distractions, social pressures, contextual factors?","code":""},{"path":"sec-causal-inference.html","id":"enhancing-ecological-validity","chapter":"21 Causal Inference","heading":"21.8.6.1 Enhancing Ecological Validity","text":"Naturalistic Observation: Conduct observations experiments participants’ usual environments.Naturalistic Observation: Conduct observations experiments participants’ usual environments.Realistic Tasks: Use tasks closely mimic real-world challenges behaviors.Realistic Tasks: Use tasks closely mimic real-world challenges behaviors.Minimal Interference: Researchers strive reduce artificiality setting, allowing participants behave naturally possible.Minimal Interference: Researchers strive reduce artificiality setting, allowing participants behave naturally possible.","code":""},{"path":"sec-causal-inference.html","id":"sec-statistical-conclusion-validity","chapter":"21 Causal Inference","heading":"21.8.7 Statistical Conclusion Validity","text":"Though often discussed alongside internal validity, statistical conclusion validity focuses whether statistical tests used study appropriate, powerful enough, applied correctly.","code":""},{"path":"sec-causal-inference.html","id":"threats-to-statistical-conclusion-validity","chapter":"21 Causal Inference","heading":"21.8.7.1 Threats to Statistical Conclusion Validity","text":"Low Statistical Power: sample size small, study may fail detect real effect (Type II error).Low Statistical Power: sample size small, study may fail detect real effect (Type II error).Violations Statistical Assumptions: Incorrect application statistical tests can lead spurious conclusions (e.g., using parametric tests non-normal data without appropriate adjustments).Violations Statistical Assumptions: Incorrect application statistical tests can lead spurious conclusions (e.g., using parametric tests non-normal data without appropriate adjustments).Fishing Error Rate Problem: Running many statistical tests without correction increases chance Type error (finding false positive).Fishing Error Rate Problem: Running many statistical tests without correction increases chance Type error (finding false positive).Reliability Measures: measurement instruments unreliable, statistical correlations differences may undervalued overstated.Reliability Measures: measurement instruments unreliable, statistical correlations differences may undervalued overstated.","code":""},{"path":"sec-causal-inference.html","id":"improving-statistical-conclusion-validity","chapter":"21 Causal Inference","heading":"21.8.7.2 Improving Statistical Conclusion Validity","text":"Adequate Sample Size: Conduct power analysis determine necessary size detect meaningful effects.Adequate Sample Size: Conduct power analysis determine necessary size detect meaningful effects.Appropriate Statistical Techniques: Ensure chosen analysis matches nature data research question.Appropriate Statistical Techniques: Ensure chosen analysis matches nature data research question.Multiple Testing Corrections: Use methods like Bonferroni false discovery rate corrections conducting multiple comparisons.Multiple Testing Corrections: Use methods like Bonferroni false discovery rate corrections conducting multiple comparisons.High-Quality Measurements: Use reliable valid measures reduce measurement error.High-Quality Measurements: Use reliable valid measures reduce measurement error.","code":""},{"path":"sec-causal-inference.html","id":"putting-it-all-together","chapter":"21 Causal Inference","heading":"21.8.8 Putting It All Together","text":"Face Validity: look like measures ?Face Validity: look like measures ?Content Validity: cover facets construct?Content Validity: cover facets construct?Construct Validity: truly reflect theoretical concept?Construct Validity: truly reflect theoretical concept?Criterion Validity: correlate predict relevant outcomes?Criterion Validity: correlate predict relevant outcomes?Internal Validity: relationship treatment outcome truly causal?Internal Validity: relationship treatment outcome truly causal?External Validity: Can findings generalized populations, settings, times?External Validity: Can findings generalized populations, settings, times?Ecological Validity: findings applicable real-world scenarios?Ecological Validity: findings applicable real-world scenarios?Statistical Conclusion Validity: statistical inferences correct robust?Statistical Conclusion Validity: statistical inferences correct robust?Researchers typically need strike balance among different validities:highly controlled lab study might excel internal validity might limited external ecological validity.highly controlled lab study might excel internal validity might limited external ecological validity.broad, naturalistic field study might stronger external ecological validity weaker internal validity due less control confounding variables.broad, naturalistic field study might stronger external ecological validity weaker internal validity due less control confounding variables.single study can maximize validity types simultaneously, replication, triangulation (using multiple methods), transparent reporting crucial strategies bolster overall credibility.","code":""},{"path":"sec-causal-inference.html","id":"types-of-subjects-in-a-treatment-setting","chapter":"21 Causal Inference","heading":"21.9 Types of Subjects in a Treatment Setting","text":"conducting causal inference, particularly randomized experiments quasi-experimental settings, individuals study can classified four distinct groups based response treatment assignment. groups differ react assigned receive receive treatment.","code":""},{"path":"sec-causal-inference.html","id":"non-switchers","chapter":"21 Causal Inference","heading":"21.9.1 Non-Switchers","text":"Non-switchers individuals whose treatment status change regardless whether assigned treatment control group. individuals provide useful causal information behavior remains unchanged. divided :Always-Takers: individuals always receive treatment, even assigned control group.Never-Takers: individuals never receive treatment, even assigned treatment group.Since behavior independent assignment, always-takers never-takers contribute identifying causal effects standard randomized experiments. Instead, presence can introduce bias treatment effect estimation, particularly intention--treat analysis.","code":""},{"path":"sec-causal-inference.html","id":"switchers","chapter":"21 Causal Inference","heading":"21.9.2 Switchers","text":"Switchers individuals whose treatment status depends assignment. individuals primary focus causal inference provide meaningful information effect treatment. classified :Compliers: Individuals follow assigned treatment protocol.\nassigned treatment group, accept receive treatment.\nassigned control group, receive treatment.\ncompliers important?\ngroup treatment assignment affects actual treatment receipt.\nCausal effect estimates (local average treatment effect, LATE) typically identified using compliers.\ndataset contains compliers, intention--treat effect (ITT) equal treatment effect.\n\nassigned treatment group, accept receive treatment.assigned control group, receive treatment.compliers important?\ngroup treatment assignment affects actual treatment receipt.\nCausal effect estimates (local average treatment effect, LATE) typically identified using compliers.\ndataset contains compliers, intention--treat effect (ITT) equal treatment effect.\ngroup treatment assignment affects actual treatment receipt.Causal effect estimates (local average treatment effect, LATE) typically identified using compliers.dataset contains compliers, intention--treat effect (ITT) equal treatment effect.Defiers: Individuals opposite assigned.\nassigned treatment group, refuse treatment.\nassigned control group, seek receive treatment anyway.\ndefiers typically ignored?\nstudies, defiers assumed small negligible group.\nStandard causal inference frameworks assume monotonicity, meaning one behaves defier.\ndefiers exist large numbers, estimating causal effects becomes significantly complex.\n\nassigned treatment group, refuse treatment.assigned control group, seek receive treatment anyway.defiers typically ignored?\nstudies, defiers assumed small negligible group.\nStandard causal inference frameworks assume monotonicity, meaning one behaves defier.\ndefiers exist large numbers, estimating causal effects becomes significantly complex.\nstudies, defiers assumed small negligible group.Standard causal inference frameworks assume monotonicity, meaning one behaves defier.defiers exist large numbers, estimating causal effects becomes significantly complex.","code":""},{"path":"sec-causal-inference.html","id":"classification-of-individuals-based-on-treatment-assignment","chapter":"21 Causal Inference","heading":"21.9.3 Classification of Individuals Based on Treatment Assignment","text":"following table summarizes different types individuals respond treatment control assignments:Key Takeaways:Compliers group allows us estimate causal effects using randomized quasi-experimental designs.Always-Takers Never-Takers provide meaningful variation treatment status, making less useful causal inference.Defiers typically violate assumption monotonicity, presence complicates causal estimation.dataset consists compliers, intention--treat effect equal treatment effect.correctly identifying accounting different subject types, researchers can ensure accurate causal inference minimize biases estimating treatment effects.","code":""},{"path":"sec-causal-inference.html","id":"types-of-treatment-effects","chapter":"21 Causal Inference","heading":"21.10 Types of Treatment Effects","text":"evaluating causal impact intervention, different estimands (quantities interest) can used measure treatment effects, depending study design assumptions compliance.Terminology:Estimands: causal effect parameters seek measure.Estimators: statistical procedures used estimate parameters.Sources Bias (L. Keele Grieve 2025):\\[\n\\begin{aligned}\n&\\text{Estimator - True Causal Effect} \\\\\n&= \\underbrace{\\textbf{Hidden bias}}_{\\text{Due design}}\n+ \\underbrace{\\textbf{Misspecification bias}}_{\\text{Due modeling}}\n+\\underbrace{\\textbf{Statistical noise}}_{\\text{Due finite sample}}\n\\end{aligned}\n\\]Hidden Bias (Due Design)Arises unobserved confounders measurement error remain conditioning observed covariates.“hidden” true magnitude direction directly observed.Violations conditional exchangeability (also called unobserved confounding) imply presence hidden bias.Misspecification Bias (Due Modeling)Occurs assumed model outcome treatment assignment reflect true data-generating process.Persists even perfect exchangeability (.e., hidden bias).Can viewed -specification (omitting essential terms functional forms) -specification (including unnecessary parameters).Statistical Noise (Due Finite Sample)Even perfect design correct model specification, finite samples lead randomness estimates.Standard errors, confidence intervals, p-values reflect uncertainty.practice, three sources bias uncertainty can coexist varying degrees.","code":""},{"path":"sec-causal-inference.html","id":"sec-average-treatment-effect","chapter":"21 Causal Inference","heading":"21.10.1 Average Treatment Effect","text":"Average Treatment Effect (ATE) expected difference outcomes individuals receive treatment .DefinitionLet:\\(Y_i(1)\\) outcome individual \\(\\) treatment.\\(Y_i(1)\\) outcome individual \\(\\) treatment.\\(Y_i(0)\\) outcome individual \\(\\) control.\\(Y_i(0)\\) outcome individual \\(\\) control.individual treatment effect :\\[\n\\tau_i = Y_i(1) - Y_i(0)\n\\]Since observe \\(Y_i(1)\\) \\(Y_i(0)\\) individual (fundamental problem causal inference), estimate ATE across population:\\[\nATE = E[Y(1)] - E[Y(0)]\n\\]Identification RandomizationIf treatment assignment randomized (Experimental Design), observed difference means treatment control groups provides unbiased estimator ATE:\\[\nATE = \\frac{1}{N} \\sum_{=1}^{N} \\tau_i = \\frac{\\sum_1^N Y_i(1)}{N} - \\frac{\\sum_i^N Y_i(0)}{N}\n\\]randomization, assume:\\[\nE[Y(1) | D = 1] = E[Y(1) | D = 0] = E[Y(1)]\n\\]\\[\nE[Y(0) | D = 1] = E[Y(0) | D = 0] = E[Y(0)]\n\\]Thus, difference observed means treated control groups provides unbiased estimate ATE.\\[\nATE = E[Y(1)] - E[Y(0)]\n\\]Alternatively, can express potential outcomes framework regression form, allows us connect causal inference concepts standard regression analysis.Instead writing treatment effects potential outcomes, can define observed outcome \\(Y_i\\) terms regression equation:\\[\nY_i = Y_i(0)  + [Y_i (1) - Y_i(0)] D_i\n\\]:\\(Y_i(0)\\) outcome individual \\(\\) receive treatment.\\(Y_i(0)\\) outcome individual \\(\\) receive treatment.\\(Y_i(1)\\) outcome individual \\(\\) receive treatment.\\(Y_i(1)\\) outcome individual \\(\\) receive treatment.\\(D_i\\) binary indicator treatment assignment:\\(D_i\\) binary indicator treatment assignment:\\(D_i = 1\\) individual \\(\\) receives treatment.\\(D_i = 1\\) individual \\(\\) receives treatment.\\(D_i = 0\\) individual \\(\\) control group.\\(D_i = 0\\) individual \\(\\) control group.can redefine equation using regression notation:\\[\nY_i = \\beta_{0i} + \\beta_{1i} D_i\n\\]:\\(\\beta_{0i} = Y_i(0)\\) represents baseline (control group) outcome.\\(\\beta_{0i} = Y_i(0)\\) represents baseline (control group) outcome.\\(\\beta_{1i} = Y_i(1) - Y_i(0)\\) represents individual treatment effect.\\(\\beta_{1i} = Y_i(1) - Y_i(0)\\) represents individual treatment effect.Thus, ideal setting, coefficient \\(D_i\\) regression gives us treatment effect.observational studies, treatment assignment \\(D_i\\) often random, leading endogeneity. means error term regression equation might correlated \\(D_i\\), violating one key assumptions Ordinary Least Squares estimator.formalize issue, can express outcome equation :\\[\n\\begin{aligned}\nY_i &= \\beta_{0i} + \\beta_{1i} D_i \\\\\n&= ( \\bar{\\beta}_{0} + \\epsilon_{0i} ) + (\\bar{\\beta}_{1} + \\epsilon_{1i} )D_i \\\\\n&=  \\bar{\\beta}_{0} + \\epsilon_{0i} + \\bar{\\beta}_{1} D_i + \\epsilon_{1i} D_i\n\\end{aligned}\n\\]:\\(\\bar{\\beta}_{0}\\) average baseline outcome.\\(\\bar{\\beta}_{0}\\) average baseline outcome.\\(\\bar{\\beta}_{1}\\) average treatment effect.\\(\\bar{\\beta}_{1}\\) average treatment effect.\\(\\epsilon_{0i}\\) captures individual-specific deviations control group outcomes.\\(\\epsilon_{0i}\\) captures individual-specific deviations control group outcomes.\\(\\epsilon_{1i}\\) captures heterogeneous treatment effects.\\(\\epsilon_{1i}\\) captures heterogeneous treatment effects.treatment assignment truly random, :\\[\nE[\\epsilon_{0i}] = E[\\epsilon_{1i}] = 0\n\\]ensures:selection bias: \\(D_i \\perp \\epsilon_{0i}\\) (.e., treatment assignment independent baseline error).selection bias: \\(D_i \\perp \\epsilon_{0i}\\) (.e., treatment assignment independent baseline error).Treatment effect independent assignment: \\(D_i \\perp \\epsilon_{1i}\\).Treatment effect independent assignment: \\(D_i \\perp \\epsilon_{1i}\\).However, observational studies, assumptions often fail. leads :Selection bias: individuals self-select treatment based unobserved characteristics, \\(D_i\\) correlates \\(\\epsilon_{0i}\\).Heterogeneous treatment effects: treatment effect varies across individuals, \\(D_i\\) correlates \\(\\epsilon_{1i}\\).issues violate exogeneity assumption OLS regression, leading biased estimates \\(\\beta_1\\).estimating treatment effects using OLS regression, need aware potential estimation issues.OLS Estimator Difference--MeansUnder random assignment, OLS estimator \\(\\beta_1\\) simplifies difference means estimator:\\[\n\\hat{\\beta}_1^{OLS} = \\bar{Y}_{\\text{treated}} - \\bar{Y}_{\\text{control}}\n\\]unbiased estimator Average Treatment Effect.However, treatment assignment random, OLS estimates may biased due unobserved confounders.Heteroskedasticity Robust Standard ErrorsIf treatment effects vary across individuals (.e., treatment effect heterogeneity), error term contains interaction:\\[\n\\epsilon_i = \\epsilon_{0i} + D_i \\epsilon_{1i}\n\\]leads heteroskedasticity (.e., variance errors depends \\(D_i\\) possibly covariates \\(X_i\\)).address , use heteroskedasticity-robust standard errors, ensure valid inference even variance constant across observations.","code":""},{"path":"sec-causal-inference.html","id":"sec-conditional-average-treatment-effect-","chapter":"21 Causal Inference","heading":"21.10.2 Conditional Average Treatment Effect","text":"Treatment effects may vary across different subgroups population. Conditional Average Treatment Effect (CATE) captures heterogeneity treatment effects across subpopulations.DefinitionFor subgroup characterized covariates \\(X_i\\):\\[\nCATE = E[Y(1) - Y(0) | X_i]\n\\]CATE Useful?Heterogeneous Treatment Effects: Certain groups may benefit treatment others.Policy Targeting: Understanding benefits allows better resource allocation.ExamplePolicy Intervention: job training program may different effects younger vs. older workers.Medical Treatments: Drug effectiveness may differ gender, age, genetic factors.Estimating CATE allows policymakers researchers identify benefits intervention.","code":""},{"path":"sec-causal-inference.html","id":"sec-intention-to-treat-effect","chapter":"21 Causal Inference","heading":"21.10.3 Intention-to-Treat Effect","text":"key issue empirical research non-compliance, individuals always follow assigned treatment (.e., either people supposed receive treatment don’t receive , people supposed control group receive treatment). Intention--Treat (ITT) effect measures impact offering treatment, regardless whether individuals actually receive .DefinitionThe ITT effect observed difference means groups assigned treatment control:\\[\nITT = E[Y | D = 1] - E[Y | D = 0]\n\\]Use ITT?Policy Evaluation: ITT reflects real-world effectiveness intervention, accounting incomplete take-.Randomized Trials: ITT preserves randomization, even compliance imperfect.Example: VaccinationA government offers vaccine (ITT), everyone actually takes .true treatment effect depends receive vaccine, differs effect measured ITT.Since non-compliance common real-world settings, ITT effects often smaller true treatment effects. case, difference observed means treatment control groups [Average Treatment Effects], Intention--Treat Effect.","code":""},{"path":"sec-causal-inference.html","id":"sec-local-average-treatment-effects","chapter":"21 Causal Inference","heading":"21.10.4 Local Average Treatment Effects","text":"many empirical settings, individuals assigned treatment actually receive (non-compliance). Instead estimating treatment effect everyone assigned treatment (.e., Intention--Treat Effects), often want estimate effect treatment actually comply assignment.known Local Average Treatment Effect, also referred Complier Average Causal Effect (CACE).LATE treatment effect subgroup compliers—take treatment assigned .Unlike Conditional Average Treatment Effects, describes heterogeneity across observable subgroups, LATE focuses compliance behavior.typically recover LATE using Instrumental Variables, leveraging random treatment assignment instrument.","code":""},{"path":"sec-causal-inference.html","id":"estimating-late-using-instrumental-variables","chapter":"21 Causal Inference","heading":"21.10.4.1 Estimating LATE Using Instrumental Variables","text":"Instrumental variable estimation allows us isolate effect treatment compliers using random treatment assignment instrument actual treatment receipt.instrumental variables perspective, LATE estimated :\\[\nLATE = \\frac{ITT}{\\text{Share Compliers}}\n\\]:ITT (Intention--Treat Effect) effect assigned treatment.ITT (Intention--Treat Effect) effect assigned treatment.Share Compliers proportion individuals actually take treatment assigned .Share Compliers proportion individuals actually take treatment assigned .","code":""},{"path":"sec-causal-inference.html","id":"key-properties-of-late","chapter":"21 Causal Inference","heading":"21.10.4.2 Key Properties of LATE","text":"proportion compliers increases, LATE converges ITT.LATE always larger ITT, since ITT averages compliers non-compliers.Standard error rule thumb:\nstandard error LATE given :\n\\[\nSE(LATE) = \\frac{SE(ITT)}{\\text{Share Compliers}}\n\\]\nstandard error LATE given :\n\\[\nSE(LATE) = \\frac{SE(ITT)}{\\text{Share Compliers}}\n\\]standard error LATE given :\\[\nSE(LATE) = \\frac{SE(ITT)}{\\text{Share Compliers}}\n\\]LATE can also estimated using pure placebo group (Gerber et al. 2010).Partial compliance difficult studyThe IV/2SLS estimator biased small samples, requiring Bayesian methods correction (Long, Little, Lin 2010; Jin Rubin 2009, 2008).","code":""},{"path":"sec-causal-inference.html","id":"one-sided-noncompliance","chapter":"21 Causal Inference","heading":"21.10.4.3 One-Sided Noncompliance","text":"One-sided noncompliance occurs observe compliers never-takers sample (.e., always-takers).Key assumptions:Exclusion Restriction (Excludability): Never-takers outcomes regardless assignment (.e., treatment effect never receive ).Exclusion Restriction (Excludability): Never-takers outcomes regardless assignment (.e., treatment effect never receive ).Random Assignment Ensures Balance: number never-takers expected equal treatment control groups.Random Assignment Ensures Balance: number never-takers expected equal treatment control groups.Estimation LATE one-sided noncompliance:\\[\nLATE = \\frac{ITT}{\\text{Share Compliers}}\n\\]Since never-takers receive treatment, simplifies estimation.","code":""},{"path":"sec-causal-inference.html","id":"two-sided-noncompliance","chapter":"21 Causal Inference","heading":"21.10.4.4 Two-Sided Noncompliance","text":"Two-sided noncompliance occurs observe compliers, never-takers, always-takers sample.Key assumptions:Exclusion Restriction (Excludability): Never-takers always-takers outcome regardless treatment assignment.Exclusion Restriction (Excludability): Never-takers always-takers outcome regardless treatment assignment.Monotonicity Assumption (Defiers):\ndefiers, meaning individuals systematically avoid treatment assigned .\nassumption standard practical studies.\nMonotonicity Assumption (Defiers):defiers, meaning individuals systematically avoid treatment assigned .defiers, meaning individuals systematically avoid treatment assigned .assumption standard practical studies.assumption standard practical studies.Estimation LATE two-sided noncompliance:\\[\nLATE = \\frac{ITT}{\\text{Share Compliers}}\n\\]Since always-takers receive treatment regardless assignment, presence bias LATE long monotonicity holds.practice, monotonicity often reasonable, defiers rare.","code":""},{"path":"sec-causal-inference.html","id":"population-vs.-sample-average-treatment-effects","chapter":"21 Causal Inference","heading":"21.10.5 Population vs. Sample Average Treatment Effects","text":"experimental observational studies, often estimate Sample Average Treatment Effect (SATE) using finite sample. However, Population Average Treatment Effect (PATE) parameter interest making broader generalizations.Key Issue:\nSATE necessarily equal PATE due sample selection bias treatment imbalance.See (Imai, King, Stuart 2008) -depth discussion SATE diverges PATE.Consider finite population size \\(N\\) observe sample size \\(n\\) (\\(N \\gg n\\)). Half sample receives treatment, half assigned control.Define following indicators:Sampling Indicator:\\[\n  I_i =\n  \\begin{cases}\n  1, & \\text{unit } \\text{ sample} \\\\\n  0, & \\text{otherwise}\n  \\end{cases}\n  \\]Treatment Assignment Indicator:\\[\nT_i =\n\\begin{cases}\n1, & \\text{unit } \\text{ treatment group} \\\\\n0, & \\text{unit } \\text{ control group}\n\\end{cases}\n\\]Potential Outcomes Framework:\\[\nY_i =\n\\begin{cases}\nY_i(1), & \\text{} T_i = 1 \\text{ (Treated)} \\\\\nY_i(0), & \\text{} T_i = 0 \\text{ (Control)}\n\\end{cases}\n\\]Observed Outcome:\nSince can never observe potential outcomes unit, observed outcome :\n\\[\nY_i | I_i = 1 = T_i Y_i(1) + (1 - T_i) Y_i(0)\n\\]Observed Outcome:\nSince can never observe potential outcomes unit, observed outcome :\\[\nY_i | I_i = 1 = T_i Y_i(1) + (1 - T_i) Y_i(0)\n\\]True Individual Treatment Effect:\nindividual-level treatment effect :\n\\[\nTE_i = Y_i(1) - Y_i(0)\n\\]True Individual Treatment Effect:\nindividual-level treatment effect :\\[\nTE_i = Y_i(1) - Y_i(0)\n\\]However, since observe one \\(Y_i(1)\\) \\(Y_i(0)\\), \\(TE_i\\) never directly observed.","code":""},{"path":"sec-causal-inference.html","id":"definitions-of-sate-and-pate","chapter":"21 Causal Inference","heading":"21.10.5.1 Definitions of SATE and PATE","text":"Sample Average Treatment Effect (SATE): \\[\nSATE = \\frac{1}{n} \\sum_{\\\\{I_i = 1\\}} TE_i\n\\] SATE average treatment effect within sample.Sample Average Treatment Effect (SATE): \\[\nSATE = \\frac{1}{n} \\sum_{\\\\{I_i = 1\\}} TE_i\n\\] SATE average treatment effect within sample.Population Average Treatment Effect (PATE): \\[\nPATE = \\frac{1}{N} \\sum_{=1}^N TE_i\n\\] PATE represents true treatment effect across entire population.Population Average Treatment Effect (PATE): \\[\nPATE = \\frac{1}{N} \\sum_{=1}^N TE_i\n\\] PATE represents true treatment effect across entire population.Since observe subset population, SATE may equal PATE.","code":""},{"path":"sec-causal-inference.html","id":"decomposing-estimation-error","chapter":"21 Causal Inference","heading":"21.10.5.2 Decomposing Estimation Error","text":"baseline estimator SATE PATE difference observed means:\\[\n\\begin{aligned}\nD &= \\frac{1}{n/2} \\sum_{\\(I_i = 1, T_i = 1)} Y_i - \\frac{1}{n/2} \\sum_{\\(I_i = 1 , T_i = 0)} Y_i \\\\\n&= \\text{(Mean Treated Group)} - \\text{(Mean Control Group)}\n\\end{aligned}\n\\]Define \\(\\Delta\\) estimation error (.e., deviation truth), additive model:\\[\nY_i(t) = g_t(X_i) + h_t(U_i)\n\\]estimation error decomposed \\[\n\\begin{aligned}\nPATE - D = \\Delta &= \\Delta_S + \\Delta_T \\\\\n&= (PATE - SATE) + (SATE - D)\\\\\n&= \\text{Sample Selection Bias} + \\text{Treatment Imbalance} \\\\\n&= (\\Delta_{S_X} + \\Delta_{S_U}) + (\\Delta_{T_X} + \\Delta_{T_U}) \\\\\n&= (\\text{Selection Observables} + \\text{Selection Unobservables}) \\\\\n&+ (\\text{Treatment Imbalance Observables} + \\text{Treatment Imbalance Unobservables})\n\\end{aligned}\n\\]illustrate , begin explicitly defining total discrepancy \\(PATE - D\\) separates different components.Step 1: \\(PATE - D\\) \\(\\Delta_S + \\Delta_T\\)\\[\n\\underbrace{PATE - D}_{\\Delta}\n\\;=\\;\n\\underbrace{(PATE - SATE)}_{\\Delta_S}\n\\; +\\;\n\\underbrace{(SATE - D)}_{\\Delta_T}.\n\\]\\(PATE - D\\): total discrepancy true population treatment effect estimate \\(D\\).\\(\\Delta_S = PATE - SATE\\): Sample Selection Bias – much sample ATE differs population ATE.\\(\\Delta_T = SATE - D\\): Treatment Imbalance – much estimated treatment effect deviates sample ATE.Step 2: Breaking Bias Observables UnobservablesEach bias term can decomposed observed (\\(X\\)) unobserved (\\(U\\)) factors:\\[\n\\Delta_S\n= \\underbrace{\\Delta_{S_X}}_{\\text{Selection Observables}}\n+ \\underbrace{\\Delta_{S_U}}_{\\text{Selection Unobservables}}\n\\]\\[\n\\Delta_T\n= \\underbrace{\\Delta_{T_X}}_{\\text{Treatment Imbalance Observables}}\n+ \\underbrace{\\Delta_{T_U}}_{\\text{Treatment Imbalance Unobservables}}\n\\]Thus, final expression:\\[\n\\begin{aligned}\nPATE - D &= \\underbrace{(PATE - SATE)}_{\\Delta_S:\\,\\text{Sample Selection Bias}}\n\\;+\\;\n\\underbrace{(SATE - D)}_{\\Delta_T:\\,\\text{Treatment Imbalance}} \\\\\n&= \\underbrace{(\\Delta_{S_X} + \\Delta_{S_U})}_{\\text{Selection }X + \\text{ Selection }U}\n\\;+\\;\n\\underbrace{(\\Delta_{T_X} + \\Delta_{T_U})}_{\\text{Imbalance }X + \\text{ Imbalance }U}.\n\\end{aligned}\n\\]decomposition clarifies sources error estimating true effect, distinguishing sample representativeness (selection bias) treatment assignment differences (treatment imbalance), separating observable unobservable components.","code":""},{"path":"sec-causal-inference.html","id":"sample-selection-bias-delta_s","chapter":"21 Causal Inference","heading":"21.10.5.2.1 Sample Selection Bias ( \\(\\Delta_S\\) )","text":"Also called sample selection error, arises sample representative population.\\[\n\\Delta_S = PATE - SATE = \\frac{N - n}{N}(NATE - SATE)\n\\]:NATE (Non-Sample Average Treatment Effect) average treatment effect part population included sample:\\[\nNATE = \\sum_{\\(I_i = 0)} \\frac{TE_i}{N-n}\n\\]eliminate sample selection bias (\\(\\Delta_S = 0\\)):Redefine sample entire population (\\(N = n\\)).Ensure \\(NATE = SATE\\) (e.g., treatment effects must homogeneous across sampled non-sampled units).However, treatment effects vary across individuals, random sampling warrants sample selection bias sample eliminate error.","code":""},{"path":"sec-causal-inference.html","id":"treatment-imbalance-error-delta_t","chapter":"21 Causal Inference","heading":"21.10.5.2.2 Treatment Imbalance Error ( \\(\\Delta_T\\) )","text":"Also called treatment imbalance bias, occurs empirical distribution treated control units differs.\\[\n\\Delta_T = SATE - D\n\\]Key insight:\\(\\Delta_T \\0\\) treatment control groups balanced across observables (\\(X\\)) unobservables (\\(U\\)).Since directly adjust unobservables, imbalance correction methods focus observables.","code":""},{"path":"sec-causal-inference.html","id":"adjusting-for-observable-treatment-imbalance","chapter":"21 Causal Inference","heading":"21.10.5.3 Adjusting for (Observable) Treatment Imbalance","text":"However, real-world studies:can adjust observables \\(X\\), unobservables \\(U\\).can adjust observables \\(X\\), unobservables \\(U\\).Residual imbalance unobservables may still introduce bias adjustment.Residual imbalance unobservables may still introduce bias adjustment.address treatment imbalance, researchers commonly use:BlockingMatching MethodsEliminates imbalance observables (\\(\\Delta_{T_X} = 0\\)).Effect unobservables uncertain (may help unobservables correlate observables).Reduces model dependence, bias, variance, mean-squared error (MSE).Matching balances observables, effect unobservables unknown.","code":""},{"path":"sec-causal-inference.html","id":"average-treatment-effects-on-the-treated-and-control","chapter":"21 Causal Inference","heading":"21.10.6 Average Treatment Effects on the Treated and Control","text":"many empirical studies, researchers interested treatment affects specific subpopulations rather entire population. Two commonly used treatment effect measures :Average Treatment Effect Treated (ATT): effect treatment individuals actually received treatment.Average Treatment Effect Control (ATC): effect treatment individuals treated.Understanding distinction ATT, ATC, ATE crucial determining external validity designing targeted policies.","code":""},{"path":"sec-causal-inference.html","id":"sec-average-treatment-effect-on-the-treated","chapter":"21 Causal Inference","heading":"21.10.6.1 Average Treatment Effect on the Treated","text":"ATT measures expected treatment effect actually treated:\\[\n\\begin{aligned}\nATT &= E[Y_i(1) - Y_i(0) | D_i = 1] \\\\\n&= E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 1]\n\\end{aligned}\n\\]Key Interpretation:ATT tells us much better (worse) treated individuals compared hypothetical counterfactual outcome (treated).ATT tells us much better (worse) treated individuals compared hypothetical counterfactual outcome (treated).useful evaluating effectiveness interventions self-select treatment.useful evaluating effectiveness interventions self-select treatment.","code":""},{"path":"sec-causal-inference.html","id":"sec-average-treatment-effect-on-the-control","chapter":"21 Causal Inference","heading":"21.10.6.2 Average Treatment Effect on the Control","text":"ATC measures expected treatment effect treated:\\[\n\\begin{aligned}\nATC &= E[Y_i(1) - Y_i(0) | D_i = 0] \\\\\n&= E[Y_i(1) | D_i = 0] - E[Y_i(0) | D_i = 0]\n\\end{aligned}\n\\]Key Interpretation:ATC answers question: “effect treatment given treated?”ATC answers question: “effect treatment given treated?”important understanding intervention might generalize untreated populations.important understanding intervention might generalize untreated populations.","code":""},{"path":"sec-causal-inference.html","id":"relationship-between-att-atc-and-ate","chapter":"21 Causal Inference","heading":"21.10.6.3 Relationship Between ATT, ATC, and ATE","text":"random assignment full compliance, :\\[\nATE = ATT = ATC\n\\]?Randomization ensures treated untreated groups statistically identical treatment.Randomization ensures treated untreated groups statistically identical treatment.Thus, treatment effects across groups, leading ATT = ATC = ATE.Thus, treatment effects across groups, leading ATT = ATC = ATE.However, observational settings, selection bias treatment heterogeneity may cause ATT ATC diverge ATE.","code":""},{"path":"sec-causal-inference.html","id":"sec-sample-average-treatment-effect-on-the-treated","chapter":"21 Causal Inference","heading":"21.10.6.4 Sample Average Treatment Effect on the Treated","text":"Sample ATT (SATT) empirical estimate ATT finite sample:\\[\nSATT = \\frac{1}{n} \\sum_{\\D_i = 1} TE_i\n\\]:\\(TE_i = Y_i(1) - Y_i(0)\\) treatment effect unit \\(\\).\\(TE_i = Y_i(1) - Y_i(0)\\) treatment effect unit \\(\\).\\(n\\) number treated units sample.\\(n\\) number treated units sample.summation taken treated units sample.summation taken treated units sample.","code":""},{"path":"sec-causal-inference.html","id":"sec-population-average-treatment-effect-on-the-treated","chapter":"21 Causal Inference","heading":"21.10.6.5 Population Average Treatment Effect on the Treated","text":"Population ATT (PATT) generalizes ATT entire treated population:\\[\nPATT = \\frac{1}{N} \\sum_{\\D_i = 1} TE_i\n\\]:\\(TE_i = Y_i(1) - Y_i(0)\\) treatment effect unit \\(\\).\\(TE_i = Y_i(1) - Y_i(0)\\) treatment effect unit \\(\\).\\(N\\) total number treated units population.\\(N\\) total number treated units population.summation taken treated individuals population.summation taken treated individuals population.sample randomly drawn, \\(SATT \\approx PATT\\), sample representative, \\(SATT\\) may overestimate underestimate \\(PATT\\).","code":""},{"path":"sec-causal-inference.html","id":"when-att-and-atc-diverge-from-ate","chapter":"21 Causal Inference","heading":"21.10.6.6 When ATT and ATC Diverge from ATE","text":"real-world studies, ATT ATC often differ ATE due treatment effect heterogeneity selection bias.","code":""},{"path":"sec-causal-inference.html","id":"selection-bias-in-att","chapter":"21 Causal Inference","heading":"21.10.6.6.1 Selection Bias in ATT","text":"individuals self-select treatment, treated group may systematically different control group.Example:\nSuppose job training program voluntary.\nIndividuals enroll might motivated better skills .\nresult, treatment effect (ATT) may generalize untreated group (ATC).\nSuppose job training program voluntary.Individuals enroll might motivated better skills .result, treatment effect (ATT) may generalize untreated group (ATC).implies:\\[\nATT \\neq ATC\n\\]unless treatment assignment random.","code":""},{"path":"sec-causal-inference.html","id":"treatment-effect-heterogeneity","chapter":"21 Causal Inference","heading":"21.10.6.6.2 Treatment Effect Heterogeneity","text":"treatment effects vary across individuals, :ATT may larger smaller ATE, depending treatment effects differ across subgroups.ATC may larger smaller ATT, untreated group responded differently treatment.Example:scholarship program may beneficial students lower-income families students wealthier backgrounds.scholarship program may beneficial students lower-income families students wealthier backgrounds.lower-income students likely apply scholarship, ATT > ATE.lower-income students likely apply scholarship, ATT > ATE.However, wealthier students (receive scholarship) benefited less , ATC < ATE.However, wealthier students (receive scholarship) benefited less , ATC < ATE.Thus, may observe:\\[\nATE \\neq ATT \\neq ATC\n\\]","code":""},{"path":"sec-causal-inference.html","id":"sec-quantile-average-treatment-effects","chapter":"21 Causal Inference","heading":"21.10.7 Quantile Average Treatment Effects","text":"Instead focusing mean effect (ATE), Quantile Treatment Effects (QTE) help us understand treatment shifts entire distribution outcome variable.Quantile Treatment Effect quantile \\(\\tau\\) defined :\\[\nQTE_{\\tau} = Q_{\\tau} (Y_1) - Q_{\\tau} (Y_0)\n\\]:\\(Q_{\\tau} (Y_1)\\) \\(\\tau\\)-th quantile outcome distribution treatment.\\(Q_{\\tau} (Y_1)\\) \\(\\tau\\)-th quantile outcome distribution treatment.\\(Q_{\\tau} (Y_0)\\) \\(\\tau\\)-th quantile outcome distribution control.\\(Q_{\\tau} (Y_0)\\) \\(\\tau\\)-th quantile outcome distribution control.Use QTE?Heterogeneous Treatment Effects: treatment effects differ across individuals, ATE may misleading.Policy Targeting: Policymakers may care low-income individuals (e.g., bottom 25%) rather average effect.Distributional Changes: QTE allows us assess whether treatment increases inequality (e.g., benefits rich poor).Estimation QTEQTE can estimated using:Quantile Regression: Extends linear regression estimate effects different quantiles.Quantile Regression: Extends linear regression estimate effects different quantiles.Instrumental Variables QTE: Requires additional assumptions estimate causal effects presence endogeneity (Abadie, Angrist, Imbens 2002; Chernozhukov Hansen 2005).Instrumental Variables QTE: Requires additional assumptions estimate causal effects presence endogeneity (Abadie, Angrist, Imbens 2002; Chernozhukov Hansen 2005).Example: Wage Policy ImpactSuppose minimum wage increase introduced.ATE might show small positive effect earnings.However, QTE might reveal:\neffect bottom quantiles (workers lose jobs).\npositive effect median.\nstrong positive effect top quantiles (experienced workers benefit ).\neffect bottom quantiles (workers lose jobs).positive effect median.strong positive effect top quantiles (experienced workers benefit ).Thus, QTE provides detailed view treatment effect across entire income distribution.","code":""},{"path":"sec-causal-inference.html","id":"sec-log-odds-treatment-effects-for-binary-outcomes","chapter":"21 Causal Inference","heading":"21.10.8 Log-Odds Treatment Effects for Binary Outcomes","text":"outcome variable binary (e.g., success/failure, employed/unemployed, survived/died), often useful measure treatment effect log-odds form.binary outcome \\(Y\\), define probability success :\\[\nP(Y = 1 | D = d)\n\\]log-odds success treatment control :\\[\n\\text{Log-odds}(Y | D = 1) = \\log \\left( \\frac{P(Y = 1 | D = 1)}{1 - P(Y = 1 | D = 1)} \\right)\n\\]\\[\n\\text{Log-odds}(Y | D = 0) = \\log \\left( \\frac{P(Y = 1 | D = 0)}{1 - P(Y = 1 | D = 0)} \\right)\n\\]Log-Odds Treatment Effect (LOTE) :\\[\nLOTE = \\text{Log-odds}(Y | D = 1) - \\text{Log-odds}(Y | D = 0)\n\\]captures treatment affects relative likelihood success nonlinear way.Use Log-Odds Treatment Effects?Binary Outcomes: treatment outcome 0 1 (e.g., employed/unemployed).Nonlinear Treatment Effects: Log-odds help handle situations effects multiplicative rather additive.Rare Events: Useful cases outcome probability small large.Estimation Log-Odds Treatment EffectsLogistic Regression Treatment Indicator: \\[\n\\log \\left( \\frac{P(Y = 1 | D = 1)}{1 - P(Y = 1 | D = 1)} \\right) = \\beta_0 + \\beta_1 D\n\\] \\(\\beta_1\\) represents log-odds treatment effect.Logistic Regression Treatment Indicator: \\[\n\\log \\left( \\frac{P(Y = 1 | D = 1)}{1 - P(Y = 1 | D = 1)} \\right) = \\beta_0 + \\beta_1 D\n\\] \\(\\beta_1\\) represents log-odds treatment effect.Randomization-Based Estimation: Freedman (2008) provides framework randomized trials ensures consistent estimation.Randomization-Based Estimation: Freedman (2008) provides framework randomized trials ensures consistent estimation.Attributable Effects: Alternative methods, (Rosenbaum 2002), estimate proportion cases attributable treatment.Attributable Effects: Alternative methods, (Rosenbaum 2002), estimate proportion cases attributable treatment.","code":""},{"path":"sec-causal-inference.html","id":"summary-table-treatment-effect-estimands","chapter":"21 Causal Inference","heading":"21.10.9 Summary Table: Treatment Effect Estimands","text":"","code":""},{"path":"sec-experimental-design.html","id":"sec-experimental-design","chapter":"22 Experimental Design","heading":"22 Experimental Design","text":"Imagine ’re marketing manager trying decide whether new advertising campaign boost sales. perhaps ’re economist investigating impact tax incentives consumer spending. cases, need way determine causal effects—just correlations. experimental design becomes essential.core, experimental design ensures studies conducted efficiently valid conclusions can drawn. business applications, experiments help quantify effects pricing strategies, marketing tactics, financial interventions. well-designed experiment reduces bias, controls variability, maximizes accuracy causal inferences.","code":""},{"path":"sec-experimental-design.html","id":"principles-of-experimental-design","chapter":"22 Experimental Design","heading":"22.1 Principles of Experimental Design","text":"well-designed experiment follows three key principles:Randomization: Ensures subjects experimental units assigned different groups randomly, eliminating selection bias.Replication: Increases precision estimates repeating experiment multiple subjects.Control: Isolates effect treatments using control groups baseline conditions minimize confounding factors.addition , blocking factorial designs refine experimental accuracy.","code":""},{"path":"sec-experimental-design.html","id":"sec-the-gold-standard-randomized-controlled-trials","chapter":"22 Experimental Design","heading":"22.2 The Gold Standard: Randomized Controlled Trials","text":"Randomized Controlled Trials (RCTs) holy grail causal inference. power comes random assignment, ensures differences treatment control groups arise due intervention.RCTs provide:Unbiased estimates treatment effectsElimination confounding factors average (although covariate imbalance can occur, necessitating techniques like [Rerandomization] achieve “platinum standard” set Tukey (1993))RCT consists two groups:Treatment group: Receives intervention (e.g., new marketing campaign, drug, financial incentive).Control group: receive intervention, serving baseline.Subjects population randomly assigned either group. randomization ensures observed differences outcomes due solely treatment—external factors.However, RCTs easier conduct hard sciences (e.g., medicine physics), treatments environments can tightly controlled. social sciences, challenges arise :Human behavior unpredictable.treatments impossible unethical introduce (e.g., assigning individuals poverty).Real-world environments difficult control.address challenges, social scientists use Quasi-Experimental Methods approximate experimental conditions.RCTs establish internal validity, meaning observed treatment effects causally interpretable. Even though random assignment ceteris paribus (holding everything else constant), achieves similar effect: average, treatment control groups comparable.","code":""},{"path":"sec-experimental-design.html","id":"selection-problem","chapter":"22 Experimental Design","heading":"22.3 Selection Problem","text":"fundamental challenge causal inference never observe potential outcomes individual—one . creates selection problem, formalize .Assume :binary treatment variable:\\(D_i \\\\{0,1\\}\\), :\n\\(D_i = 1\\) indicates individual \\(\\) receives treatment.\n\\(D_i = 0\\) indicates individual \\(\\) receive treatment.\n\\(D_i = 1\\) indicates individual \\(\\) receives treatment.\\(D_i = 0\\) indicates individual \\(\\) receive treatment.outcome interest:\\(Y_i\\), depends whether individual treated :\n\\(Y_{0i}\\): outcome treated.\n\\(Y_{1i}\\): outcome treated.\n\\(Y_{0i}\\): outcome treated.\\(Y_{1i}\\): outcome treated.Thus, potential outcomes framework defined :\\[\n\\text{Potential Outcome} =\n\\begin{cases}\nY_{1i}, & \\text{} D_i = 1 \\quad (\\text{Treated}) \\\\\nY_{0i}, & \\text{} D_i = 0 \\quad (\\text{Untreated})\n\\end{cases}\n\\]However, observe one outcome per individual:\\[\nY_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i\n\\]means given person, either observe \\(Y_{1i}\\) \\(Y_{0i}\\), never . Since observe counterfactuals (unless invent time machine), must rely statistical inference estimate treatment effects.","code":""},{"path":"sec-experimental-design.html","id":"the-observed-difference-in-outcomes","chapter":"22 Experimental Design","heading":"22.3.1 The Observed Difference in Outcomes","text":"goal estimate difference expected outcomes treated untreated individuals:\\[\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0]\n\\]Expanding equation:\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) \\\\\n&+ (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n&= (E[Y_{1i}-Y_{0i}|D_i = 1] ) \\\\\n&+ (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0])\n\\end{aligned}\n\\]equation decomposes observed difference two components:Treatment Effect Treated: \\(E[Y_{1i} - Y_{0i} |D_i = 1]\\), represents causal impact treatment treated.Treatment Effect Treated: \\(E[Y_{1i} - Y_{0i} |D_i = 1]\\), represents causal impact treatment treated.Selection Bias:\\(E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]\\), captures systematic differences treated untreated groups even absence treatment.Selection Bias:\\(E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]\\), captures systematic differences treated untreated groups even absence treatment.Thus, observed difference outcomes :\\[\n\\text{Observed Difference} = \\text{ATT} + \\text{Selection Bias}\n\\]","code":""},{"path":"sec-experimental-design.html","id":"eliminating-selection-bias-with-random-assignment","chapter":"22 Experimental Design","heading":"22.3.2 Eliminating Selection Bias with Random Assignment","text":"random assignment treatment, \\(D_i\\) independent potential outcomes:\\[\nE[Y_i | D_i = 1] - E[Y_i|D_i = 0] = E[Y_{1i} - Y_{0i}]\n\\]works , true randomization:\\[\nE[Y_{0i} | D_i = 1] = E[Y_{0i} | D_i = 0]\n\\]eliminates selection bias. Consequently, observed difference now directly estimates true causal effect:\\[\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0] = E[Y_{1i} - Y_{0i}]\n\\]Thus, randomized controlled trials provide unbiased estimate average treatment effect.","code":""},{"path":"sec-experimental-design.html","id":"another-representation-under-regression","chapter":"22 Experimental Design","heading":"22.3.3 Another Representation Under Regression","text":"far, framed selection problem using expectations potential outcomes. Another way represent treatment effects regression models, provide practical framework estimation.Suppose treatment effect constant across individuals:\\[\nY_{1i} - Y_{0i} = \\rho\n\\]implies treated individual experiences treatment effect (\\(\\rho\\)), though baseline outcomes (\\(Y_{0i}\\)) may vary.Since observe one potential outcomes, observed outcome can expressed :\\[\n\\begin{aligned}\nY_i &= E(Y_{0i}) + (Y_{1i} - Y_{0i}) D_i + [Y_{0i} - E(Y_{0i})] \\\\\n&= \\alpha + \\rho D_i + \\eta_i\n\\end{aligned}\n\\]:\\(\\alpha = E(Y_{0i})\\), expected outcome untreated individuals.\\(\\rho\\) represents causal treatment effect.\\(\\eta_i = Y_{0i} - E(Y_{0i})\\), capturing individual deviations mean untreated outcome.Thus, regression model provides intuitive way express treatment effects.","code":""},{"path":"sec-experimental-design.html","id":"conditional-expectations-and-selection-bias","chapter":"22 Experimental Design","heading":"22.3.3.1 Conditional Expectations and Selection Bias","text":"Taking expectations conditional treatment status:\\[\n\\begin{aligned}\nE[Y_i |D_i = 1] &= \\alpha + \\rho + E[\\eta_i |D_i = 1] \\\\\nE[Y_i |D_i = 0] &= \\alpha + E[\\eta_i |D_i = 0]\n\\end{aligned}\n\\]observed difference means treated untreated groups :\\[\nE[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \\rho + E[\\eta_i |D_i = 1] - E[\\eta_i |D_i = 0]\n\\], term \\(E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\\) represents selection bias—correlation regression error term (\\(\\eta_i\\)) treatment variable (\\(D_i\\)).random assignment, assume potential outcomes independent treatment (\\(D_i\\)):\\[\nE[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0] = 0\n\\]Thus, true randomization, selection bias disappears, observed difference directly estimates causal effect \\(\\rho\\).","code":""},{"path":"sec-experimental-design.html","id":"controlling-for-additional-variables","chapter":"22 Experimental Design","heading":"22.3.3.2 Controlling for Additional Variables","text":"many real-world scenarios, random assignment imperfect, selection bias may still exist. mitigate , introduce control variables (\\(X_i\\)), demographic characteristics, firm size, prior purchasing behavior.\\(X_i\\) uncorrelated treatment (\\(D_i\\)), including regression model bias estimate \\(\\rho\\) two advantages:reduces residual variance (\\(\\eta_i\\)), improving precision \\(\\rho\\).accounts additional sources variability, making model robust.Thus, regression model extends :\\[\nY_i = \\alpha + \\rho D_i + X_i'\\gamma + \\eta_i\n\\]:\\(X_i\\) represents vector control variables.\\(\\gamma\\) captures effect \\(X_i\\) outcome.","code":""},{"path":"sec-experimental-design.html","id":"example-racial-discrimination-in-hiring","chapter":"22 Experimental Design","heading":"22.3.3.3 Example: Racial Discrimination in Hiring","text":"famous study Bertrand Mullainathan (2004) examined racial discrimination hiring randomly assigning Black- White-sounding names identical job applications. ensuring names assigned randomly, authors eliminated confounding factors like education experience, allowing estimate causal effect race callback rates.","code":""},{"path":"sec-experimental-design.html","id":"classical-experimental-designs","chapter":"22 Experimental Design","heading":"22.4 Classical Experimental Designs","text":"Experimental designs provide structured frameworks conducting experiments, ensuring results statistically valid practically applicable. choice design depends research question, nature treatment, potential sources variability. -depth statistical understanding designs, revisit Analysis Variance.","code":""},{"path":"sec-experimental-design.html","id":"completely-randomized-design","chapter":"22 Experimental Design","heading":"22.4.1 Completely Randomized Design","text":"Completely Randomized Design (CRD), experimental unit randomly assigned treatment group. simplest form experimental design effective confounding factors present.Example: Email Marketing ExperimentA company tests three different email marketing strategies (, B, C) measure effect customer engagement (click-rate). Customers randomly assigned receive one three emails.Mathematical Model\\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]:\\(Y_{ij}\\) response variable (e.g., click-rate).\\(\\mu\\) overall mean response.\\(\\tau_i\\) effect treatment \\(\\).\\(\\epsilon_{ij}\\) random error term, assumed normally distributed: \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\).p-value ANOVA summary less 0.05, reject null hypothesis conclude least one email strategy significantly affects engagement.","code":"\nset.seed(123)\n\n# Simulated dataset for email marketing experiment\ndata <- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 10),\n  response = c(rnorm(10, mean=50, sd=5),\n               rnorm(10, mean=55, sd=5),\n               rnorm(10, mean=60, sd=5))\n)\n\n# ANOVA to test for differences among groups\nanova_result <- aov(response ~ group, data = data)\nsummary(anova_result)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> group        2  306.1  153.04   6.435 0.00518 **\n#> Residuals   27  642.1   23.78                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-experimental-design.html","id":"randomized-block-design","chapter":"22 Experimental Design","heading":"22.4.2 Randomized Block Design","text":"Randomized Block Design (RBD) used experimental units can grouped homogeneous blocks based known confounding factor. Blocking helps reduce unwanted variation, increasing precision estimated treatment effects.Example: Store Layout ExperimentA retailer tests three store layouts (, B, C) sales performance. Since store location (Urban, Suburban, Rural) might influence sales, use blocking control effect.Mathematical Model \\[\nY_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\n\\] :\\(Y_{ij}\\) sales outcome store \\(\\) location \\(j\\).\\(Y_{ij}\\) sales outcome store \\(\\) location \\(j\\).\\(\\mu\\) overall mean sales.\\(\\mu\\) overall mean sales.\\(\\tau_i\\) effect layout \\(\\).\\(\\tau_i\\) effect layout \\(\\).\\(\\beta_j\\) represents block effect (location).\\(\\beta_j\\) represents block effect (location).\\(\\epsilon_{ij}\\) random error.\\(\\epsilon_{ij}\\) random error.including block model, account location effects, leading accurate treatment comparisons.","code":"\nset.seed(123)\n\n# Simulated dataset for store layout experiment\ndata <- data.frame(\n  block = rep(c(\"Urban\", \"Suburban\", \"Rural\"), each = 6),\n  layout = rep(c(\"A\", \"B\", \"C\"), times = 6),\n  sales = c(rnorm(6, mean=200, sd=20),\n            rnorm(6, mean=220, sd=20),\n            rnorm(6, mean=210, sd=20))\n)\n\n# ANOVA with blocking factor\nanova_block <- aov(sales ~ layout + block, data = data)\nsummary(anova_block)\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> layout       2     71    35.7   0.071  0.931\n#> block        2    328   164.1   0.328  0.726\n#> Residuals   13   6500   500.0"},{"path":"sec-experimental-design.html","id":"factorial-design","chapter":"22 Experimental Design","heading":"22.4.3 Factorial Design","text":"Factorial Design evaluates effects two factors simultaneously, allowing study interactions variables.Example: Pricing Advertising ExperimentA company tests two pricing strategies (High, Low) two advertising methods (TV, Social Media) sales.Mathematical Model \\[\nY_{ijk} = \\mu + \\tau_i + \\gamma_j + (\\tau\\gamma)_{ij} + \\epsilon_{ijk}\n\\]:\\(\\tau_i\\) effect price level \\(\\).\\(\\tau_i\\) effect price level \\(\\).\\(\\gamma_j\\) effect advertising method \\(j\\).\\(\\gamma_j\\) effect advertising method \\(j\\).\\((\\tau\\gamma)_{ij}\\) interaction effect price advertising.\\((\\tau\\gamma)_{ij}\\) interaction effect price advertising.\\(\\epsilon_{ijk}\\) random error term.\\(\\epsilon_{ijk}\\) random error term.","code":"\nset.seed(123)\n\n# Simulated dataset\ndata <- expand.grid(\n  Price = c(\"High\", \"Low\"),\n  Advertising = c(\"TV\", \"Social Media\"),\n  Replicate = 1:10\n)\n\n# Generate response variable (sales)\ndata$Sales <- with(data, \n                   100 + \n                   ifelse(Price == \"Low\", 10, 0) + \n                   ifelse(Advertising == \"Social Media\", 15, 0) + \n                   ifelse(Price == \"Low\" & Advertising == \"Social Media\", 5, 0) +\n                   rnorm(nrow(data), sd=5))\n\n# Two-way ANOVA\nanova_factorial <- aov(Sales ~ Price * Advertising, data = data)\nsummary(anova_factorial)\n#>                   Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Price              1   1364    1364   66.60 1.05e-09 ***\n#> Advertising        1   3640    3640  177.67 1.72e-15 ***\n#> Price:Advertising  1     15      15    0.71    0.405    \n#> Residuals         36    738      20                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-experimental-design.html","id":"crossover-design","chapter":"22 Experimental Design","heading":"22.4.4 Crossover Design","text":"Crossover Design used subject receives multiple treatments sequential manner. design controls individual differences using subject control.Example: Drug TrialPatients receive Drug first period Drug B second period, vice versa.Mathematical Model \\[\nY_{ijk} = \\mu + \\tau_i + \\pi_j + \\beta_k + \\epsilon_{ijk}\n\\] :\\(\\tau_i\\) treatment effect.\\(\\tau_i\\) treatment effect.\\(\\pi_j\\) period effect (e.g., learning effects).\\(\\pi_j\\) period effect (e.g., learning effects).\\(\\beta_k\\) subject effect (individual baseline differences).\\(\\beta_k\\) subject effect (individual baseline differences).","code":"\nset.seed(123)\n\n# Simulated dataset\ndata <- data.frame(\n  Subject = rep(1:10, each = 2),\n  Period = rep(c(\"Period 1\", \"Period 2\"), times = 10),\n  Treatment = rep(c(\"A\", \"B\"), each = 10),\n  Response = c(rnorm(10, mean=50, sd=5), rnorm(10, mean=55, sd=5))\n)\n\n# Crossover ANOVA\nanova_crossover <-\n  aov(Response ~ Treatment + Period + Error(Subject / Period),\n      data = data)\nsummary(anova_crossover)\n#> \n#> Error: Subject\n#>           Df Sum Sq Mean Sq\n#> Treatment  1  63.94   63.94\n#> \n#> Error: Subject:Period\n#>        Df Sum Sq Mean Sq\n#> Period  1  21.92   21.92\n#> \n#> Error: Within\n#>           Df Sum Sq Mean Sq F value Pr(>F)  \n#> Treatment  1  134.9  134.91   5.231 0.0371 *\n#> Period     1    0.2    0.24   0.009 0.9237  \n#> Residuals 15  386.9   25.79                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-experimental-design.html","id":"split-plot-design","chapter":"22 Experimental Design","heading":"22.4.5 Split-Plot Design","text":"Split-Plot Design used one factor applied group (whole-plot) level another individual (sub-plot) level. design particularly useful factors harder expensive randomize others.Example: Farming ExperimentA farm testing two irrigation methods (Drip vs. Sprinkler) two soil types (Clay vs. Sand) crop yield. Since irrigation systems installed farm level difficult change, treated whole-plot factor. However, different soil types exist within farm can tested easily, making sub-plot factor.Mathematical ModelThe statistical model Split-Plot Design :\\[\nY_{ijk} = \\mu + \\alpha_i + B_k + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\n\\]:\\(Y_{ijk}\\) response (e.g., crop yield).\\(\\mu\\) overall mean.\\(\\alpha_i\\) whole-plot factor (Irrigation method).\\(B_k\\) random block effect (Farm-level variation).\\(\\beta_j\\) sub-plot factor (Soil type).\\((\\alpha\\beta)_{ij}\\) interaction effect Irrigation Soil type.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) represents random error term.key feature Split-Plot Design whole-plot factor (\\(\\alpha_i\\)) tested farm-level variance (\\(B_k\\)), sub-plot factor (\\(\\beta_j\\)) tested individual variance (\\(\\epsilon_{ijk}\\)).model Split-Plot Design using Mixed Effects Model, treating Farm random effect account variation whole-plot level.model:Irrigation (Whole-plot factor) tested Farm-level variance.Irrigation (Whole-plot factor) tested Farm-level variance.Soil type (Sub-plot factor) tested Residual variance.Soil type (Sub-plot factor) tested Residual variance.interaction Irrigation × Soil also evaluated.interaction Irrigation × Soil also evaluated.hierarchical structure accounts fact farms independent, improving precision estimates.","code":"\nset.seed(123)\n\n# Simulated dataset for a split-plot experiment\ndata <- data.frame(\n  Farm = rep(1:6, each = 4), # 6 farms (whole plots)\n  \n  # Whole-plot factor\n  Irrigation = rep(c(\"Drip\", \"Sprinkler\"), each = 12), \n  Soil = rep(c(\"Clay\", \"Sand\"), times = 12), # Sub-plot factor\n  \n  # Response variable\n  Yield = c(rnorm(12, mean=30, sd=5), rnorm(12, mean=35, sd=5)) \n)\n\n# Load mixed-effects model library\nlibrary(lme4)\n\n# Mixed-effects model: Whole-plot factor (Irrigation) as a random effect\nmodel_split <- lmer(Yield ~ Irrigation * Soil + (1 | Farm), data = data)\n\n# Summary of the model\nsummary(model_split)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: Yield ~ Irrigation * Soil + (1 | Farm)\n#>    Data: data\n#> \n#> REML criterion at convergence: 128.1\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -1.72562 -0.57572 -0.09767  0.60248  2.04346 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Farm     (Intercept)  0.00    0.000   \n#>  Residual             24.79    4.979   \n#> Number of obs: 24, groups:  Farm, 6\n#> \n#> Fixed effects:\n#>                              Estimate Std. Error t value\n#> (Intercept)                    31.771      2.033  15.629\n#> IrrigationSprinkler             2.354      2.875   0.819\n#> SoilSand                       -1.601      2.875  -0.557\n#> IrrigationSprinkler:SoilSand    1.235      4.066   0.304\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) IrrgtS SolSnd\n#> IrrgtnSprnk -0.707              \n#> SoilSand    -0.707  0.500       \n#> IrrgtnSp:SS  0.500 -0.707 -0.707\n#> optimizer (nloptwrap) convergence code: 0 (OK)\n#> boundary (singular) fit: see help('isSingular')"},{"path":"sec-experimental-design.html","id":"latin-square-design","chapter":"22 Experimental Design","heading":"22.4.6 Latin Square Design","text":"two potential confounding factors exist, Latin Square Designs provide structured way control variables. design common scheduling, manufacturing, supply chain experiments.Example: Assembly Line ExperimentA manufacturer wants test three assembly methods (, B, C) controlling work shifts workstations. Since shifts workstations may influence production time, Latin Square Design ensures method tested per shift per workstation.Mathematical ModelA Latin Square Design ensures treatment appears exactly row column: \\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\] :\\(Y_{ijk}\\) outcome (e.g., assembly time).\\(Y_{ijk}\\) outcome (e.g., assembly time).\\(\\mu\\) overall mean.\\(\\mu\\) overall mean.\\(\\alpha_i\\) treatment effect (Assembly Method).\\(\\alpha_i\\) treatment effect (Assembly Method).\\(\\beta_j\\) row effect (Work Shift).\\(\\beta_j\\) row effect (Work Shift).\\(\\gamma_k\\) column effect (Workstation).\\(\\gamma_k\\) column effect (Workstation).\\(\\epsilon_{ijk}\\) random error term.\\(\\epsilon_{ijk}\\) random error term.ensures treatment equally balanced across confounding factors.implement Latin Square Design treating Assembly Method primary factor, controlling Shifts Workstations.p-value “Method” significant, conclude different assembly methods impact production time.p-value “Method” significant, conclude different assembly methods impact production time.“Shift” “Workstation” significant, indicates systematic differences across variables.“Shift” “Workstation” significant, indicates systematic differences across variables.","code":"\nset.seed(123)\n\n# Define the Latin Square layout\nlatin_square <- data.frame(\n  Shift = rep(1:3, each = 3), # Rows\n  Workstation = rep(1:3, times = 3), # Columns\n  \n  # Treatments assigned in a balanced way\n  Method = c(\"A\", \"B\", \"C\", \"C\", \"A\", \"B\", \"B\", \"C\", \"A\"), \n  Time = c(rnorm(3, mean = 30, sd = 3),\n           rnorm(3, mean = 28, sd = 3),\n           rnorm(3, mean = 32, sd = 3)) # Assembly time\n)\n\n# ANOVA for Latin Square Design\nanova_latin <-\n  aov(Time ~ factor(Shift) + factor(Workstation) + factor(Method),\n      data = latin_square)\nsummary(anova_latin)\n#>                     Df Sum Sq Mean Sq F value Pr(>F)\n#> factor(Shift)        2  1.148   0.574   0.079  0.927\n#> factor(Workstation)  2 24.256  12.128   1.659  0.376\n#> factor(Method)       2 14.086   7.043   0.964  0.509\n#> Residuals            2 14.619   7.310"},{"path":"sec-experimental-design.html","id":"advanced-experimental-designs","chapter":"22 Experimental Design","heading":"22.5 Advanced Experimental Designs","text":"","code":""},{"path":"sec-experimental-design.html","id":"semi-random-experiments","chapter":"22 Experimental Design","heading":"22.5.1 Semi-Random Experiments","text":"semi-random experiments, participants fully randomized treatment control groups. Instead, structured randomization process ensures fairness allowing level causal inference.","code":""},{"path":"sec-experimental-design.html","id":"example-loan-assignment-fairness","chapter":"22 Experimental Design","heading":"22.5.1.1 Example: Loan Assignment Fairness","text":"bank wants evaluate new loan approval policy ensuring experiment unfairly exclude specific demographics.maintain fairness:Applicants first stratified based income credit history.Within stratum, random subset assigned receive new policy, others continue old policy.approach ensures income group fairly represented treatment control conditions.","code":"\nset.seed(123)\n\n# Create stratified groups\ndata <- data.frame(\n  income_group = rep(c(\"Low\", \"Medium\", \"High\"), each = 10),\n  \n  # Stratified randomization\n  treatment = sample(rep(c(\"New Policy\", \"Old Policy\"), each = 15)) \n)\n\n# Display the stratification results\ntable(data$income_group, data$treatment)\n#>         \n#>          New Policy Old Policy\n#>   High            6          4\n#>   Low             6          4\n#>   Medium          3          7"},{"path":"sec-experimental-design.html","id":"case-study-chicago-open-enrollment-program","chapter":"22 Experimental Design","heading":"22.5.1.2 Case Study: Chicago Open Enrollment Program","text":"well-known example semi-random assignment Chicago Open Enrollment Program (Cullen, Jacob, Levitt 2005), students apply choice schools.However, since many schools oversubscribed (.e., demand exceeded supply), used random lottery system allocate spots.Thus, enrollment fully random, lottery outcomes random, allowing researchers estimate Intent--Treat effect acknowledging students won lottery actually enrolled.situation presents classic case :School choice random: Families self-select applying certain schools.Lottery outcomes random: Among apply, winning losing lottery good random assignment.Let:\\(Enroll_{ij} = 1\\) student \\(\\) enrolls school \\(j\\), \\(0\\) otherwise.\\(Win_{ij} = 1\\) student \\(\\) wins lottery, \\(0\\) otherwise.\\(Apply_{ij} = 1\\) student \\(\\) applies school \\(j\\).define:\\[\n\\delta_j = E[Y_i | Enroll_{ij} = 1, Apply_{ij} = 1] - E[Y_i | Enroll_{ij} = 0, Apply_{ij} = 1]\n\\]\\[\n\\theta_j = E[Y_i | Win_{ij} = 1, Apply_{ij} = 1] - E[Y_i | Win_{ij} = 0, Apply_{ij} = 1]\n\\]:\\(\\delta_j\\) treatment effect (impact actual school enrollment).\\(\\theta_j\\) intent--treat effect (impact winning lottery).Since winners enroll, know :\\[\n\\delta_j \\neq \\theta_j\n\\]Thus, can estimate \\(\\theta_j\\) directly, need additional method recover \\(\\delta_j\\).distinction crucial simply comparing lottery winners losers measure true effect enrollment—effect given opportunity enroll.estimate treatment effect (\\(\\delta_j\\)), use Instrumental Variable approach:\\[\n\\delta_j = \\frac{E[Y_i | W_{ij} = 1, A_{ij} = 1] - E[Y_i | W_{ij} = 0, A_{ij} = 1]}{P(Enroll_{ij} = 1 | W_{ij} = 1, A_{ij} = 1) - P(Enroll_{ij} = 1 | W_{ij} = 0, A_{ij} = 1)}\n\\]:\\(P(Enroll_{ij} = 1 | W_{ij} = 1, A_{ij} = 1)\\) = probability enrolling winning lottery.\\(P(Enroll_{ij} = 1 | W_{ij} = 0, A_{ij} = 1)\\) = probability enrolling losing lottery.adjustment accounts fact lottery winners enroll, thus observed effect (\\(\\theta_j\\)) underestimates true treatment effect (\\(\\delta_j\\)).Instrumental Variable approach corrects selection bias leveraging randomized lottery assignment.Numerical ExampleAssume 10 students win lottery 10 students lose.Winners:Losers:Computing Intent--Treat EffectWe compute expected outcome won lost lottery.\\[\n\\begin{aligned}\nE[Y_i | W_{ij} = 1, A_{ij} = 1] &= \\frac{1(1.2) + 2(1) + 7(-0.1)}{10} = 0.25 \\\\\nE[Y_i | W_{ij} = 0, A_{ij} = 1] &= \\frac{1(1.2) + 2(0) + 7(-0.1)}{10} = 0.05\n\\end{aligned}\n\\]Thus, Intent--Treat Effect :\\[\n\\text{Intent--Treat Effect} = 0.25 - 0.05 = 0.2\n\\]Now, calculate probability enrollment lottery winners losers:\\[\n\\begin{aligned}\nP(Enroll_{ij} = 1 | W_{ij} = 1, A_{ij} = 1) &= \\frac{1+2}{10} = 0.3 \\\\\nP(Enroll_{ij} = 1 | W_{ij} = 0, A_{ij} = 1) &= \\frac{1}{10} = 0.1\n\\end{aligned}\n\\]Using formula treatment effect (\\(\\delta\\)):\\[\n\\text{Treatment Effect} = \\frac{0.2}{0.3 - 0.1} = 1\n\\]confirms true treatment effect 1 unit.account additional factors (\\(X_i\\)), extend model follows:\\[\nY_{ia} = \\delta W_{ia} + \\lambda L_{ia} + X_i \\theta + u_{ia}\n\\]:\\(\\delta\\) = Intent--Treat effect\\(\\lambda\\) = True treatment effect\\(W\\) = Whether student wins lottery\\(L\\) = Whether student enrolls school\\(X_i \\theta\\) = Control variables (.e., reweighting lottery), affect treatment effect \\(E(\\delta)\\)Since choosing apply lottery random, must consider following:\\[\nE(\\lambda) \\neq E(\\lambda_1)\n\\]demonstrates lottery-based assignment useful imperfect tool causal inference—winning lottery random, applies .","code":""},{"path":"sec-experimental-design.html","id":"re-randomization","chapter":"22 Experimental Design","heading":"22.5.2 Re-Randomization","text":"standard randomization, baseline covariates balanced average, meaning imbalance can still occur due random chance. Re-randomization eliminates bad randomizations checking balance experiment begins (Morgan Rubin 2012).Key Motivations Re-RandomizationRandomization guarantee balance:\nExample: 10 covariates, probability least one imbalance \\(\\alpha = 0.05\\) :\\[  \n1 - (1 - 0.05)^{10} = 0.40 = 40\\%  \n\\]\nmeans high chance imbalance across treatment groups.\nExample: 10 covariates, probability least one imbalance \\(\\alpha = 0.05\\) :\\[  \n1 - (1 - 0.05)^{10} = 0.40 = 40\\%  \n\\]means high chance imbalance across treatment groups.Re-randomization increases precision: covariates correlate outcome, improving covariate balance improves treatment effect estimationAccounting re-randomization inference: Since re-randomization filters bad assignments, equivalent increasing sample size must considered computing standard errors.Alternative balancing techniques:\nStratified randomization (Johansson Schultzberg 2022).\nMatched randomization (Greevy et al. 2004; Kapelner Krieger 2014).\nMinimization (Pocock Simon 1975).\nStratified randomization (Johansson Schultzberg 2022).Matched randomization (Greevy et al. 2004; Kapelner Krieger 2014).Minimization (Pocock Simon 1975).Example: Balancing Experimental GroupsAn online retailer testing two website designs (B) wants ensure key customer demographics (e.g., age) balanced across treatment groups.define balance criterion check mean age difference groups acceptable proceeding.","code":"\nset.seed(123)\n\n# Define balance criterion: Ensure mean age difference < 1 year\nbalance_criterion <- function(data) {\n  abs(mean(data$age[data$group == \"A\"]) - mean(data$age[data$group == \"B\"])) < 1\n}\n\n# Generate randomized groups, repeat until balance criterion is met\nrepeat {\n  data <- data.frame(\n    age = rnorm(100, mean = 35, sd = 10),\n    group = sample(c(\"A\", \"B\"), 100, replace = TRUE)\n  )\n  if (balance_criterion(data)) break\n}\n\n# Check final group means\ntapply(data$age, data$group, mean)\n#>        A        B \n#> 35.91079 35.25483"},{"path":"sec-experimental-design.html","id":"rerandomization-criterion","chapter":"22 Experimental Design","heading":"22.5.2.1 Rerandomization Criterion","text":"Re-randomization based function covariate matrix (\\(\\mathbf{X}\\)) treatment assignments (\\(\\mathbf{W}\\)).\\[\nW_i =\n\\begin{cases}\n1, & \\text{treated} \\\\\n0, & \\text{control}\n\\end{cases}\n\\]common approach use Mahalanobis Distance measure covariate balance treatment control groups:\\[\n\\begin{aligned}\nM &= (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' \\text{cov}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C) \\\\\n&= (\\frac{1}{n_T} + \\frac{1}{n_C})^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' \\text{cov}(\\mathbf{X})^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)\n\\end{aligned}\n\\]:\\(\\bar{\\mathbf{X}}_T\\) \\(\\bar{\\mathbf{X}}_C\\) mean covariate values treatment control groups.\\(\\text{cov}(\\mathbf{X})\\) covariance matrix covariates.\\(n_T\\) \\(n_C\\) sample sizes treatment control groups.sample size large randomization pure, \\(M\\) follows chi-squared distribution:\\[\nM \\sim \\chi^2_k\n\\]\\(k\\) number covariates balanced.Choosing Rerandomization Threshold (\\(M > \\))Define \\(p_a\\) probability accepting randomization:Smaller \\(p_a\\) → Stronger balance, longer computation time.Larger \\(p_a\\) → Faster randomization, weaker balance.rule thumb re-randomize whenever:\\[\nM > \n\\]\\(\\) chosen based acceptable balance thresholds.apply Mahalanobis Distance balance criterion ensure treatment control groups well-matched proceeding experiment.","code":"\nset.seed(123)\nlibrary(MASS)\n\n# Generate a dataset with two covariates\nn <- 100\nX <- mvrnorm(n, mu = c(0, 0), Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2))\ncolnames(X) <- c(\"Covariate1\", \"Covariate2\")\n\n# Balance function using Mahalanobis Distance\nbalance_criterion <- function(X, group) {\n  X_treat <- X[group == 1, ]\n  X_control <- X[group == 0, ]\n  \n  mean_diff <- colMeans(X_treat) - colMeans(X_control)\n  cov_inv <- solve(cov(X))\n  M <- t(mean_diff) %*% cov_inv %*% mean_diff\n  \n  \n  # Acceptable threshold \n  # (chi-squared critical value for k = 2, alpha = 0.05)\n  return(M < 3.84)  \n}\n\n# Repeat randomization until balance is met\nrepeat {\n  group <- sample(c(0, 1), n, replace = TRUE)\n  if (balance_criterion(X, group)) break\n}\n\n# Display final balance check\ntable(group)\n#> group\n#>  0  1 \n#> 50 50\ncolMeans(X[group == 1, ])  # Treatment group means\n#> Covariate1 Covariate2 \n#>  0.2469635  0.1918521\ncolMeans(X[group == 0, ])  # Control group means\n#>  Covariate1  Covariate2 \n#>  0.01717088 -0.14281124"},{"path":"sec-experimental-design.html","id":"two-stage-randomized-experiments","chapter":"22 Experimental Design","heading":"22.5.3 Two-Stage Randomized Experiments","text":"two-stage randomized experiment involves sequential interventions, treatment assignments depend earlier responses. design widely used :Adaptive Learning: Adjusting educational content based student progress.Personalized Advertising: Targeting follow-ads based engagement.Medical Trials: Adapting treatments based patient response.introducing second randomization stage, researchers can evaluate:effect initial treatments.effect follow-treatments.Potential interactions two stages.","code":""},{"path":"sec-experimental-design.html","id":"example-personalized-advertising-experiment","chapter":"22 Experimental Design","heading":"22.5.3.1 Example: Personalized Advertising Experiment","text":"company tests two initial ad campaigns (Ad vs. Ad B). observing customer engagement, apply second-stage intervention (e.g., Discount vs. Discount).two-stage experiment can modeled :\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk}\n\\]:\\(\\mu\\) = Overall mean outcome (e.g., conversion rate).\\(\\alpha_i\\) = Effect first-stage intervention (\\(\\) = Ad Ad B).\\(\\beta_{j()}\\) = Effect second-stage intervention, nested within first-stage groups.\\(\\epsilon_{ijk}\\) = Random error term.nested structure ensures second-stage treatment (\\(\\beta_{j()}\\)) assigned within first-stage treatment group.structure ensures:Fair assignment initial ads.Fair assignment initial ads.Adaptive targeting second stage based user engagement.Adaptive targeting second stage based user engagement.","code":"\nset.seed(123)\n\n# Generate first-stage randomization (Initial Ad)\ndata <- data.frame(\n  stage1 = sample(c(\"Ad A\", \"Ad B\"), 100, replace = TRUE),\n  stage2 = rep(NA, 100)  # Placeholder for second-stage randomization\n)\n\n# Second-stage assignment based on first-stage response\ndata$stage2[data$stage1 == \"Ad A\"] <-\n    sample(c(\"Discount\", \"No Discount\"),\n           sum(data$stage1 == \"Ad A\"),\n           replace = TRUE)\ndata$stage2[data$stage1 == \"Ad B\"] <-\n    sample(c(\"Discount\", \"No Discount\"),\n           sum(data$stage1 == \"Ad B\"),\n           replace = TRUE)\n\n# Display final randomization\ntable(data$stage1, data$stage2)\n#>       \n#>        Discount No Discount\n#>   Ad A       24          33\n#>   Ad B       22          21"},{"path":"sec-experimental-design.html","id":"two-stage-randomized-experiments-with-interference-and-noncompliance","chapter":"22 Experimental Design","heading":"22.5.4 Two-Stage Randomized Experiments with Interference and Noncompliance","text":"real-world experiments, interference noncompliance complicate analysis (Imai, Jiang, Malani 2021):Interference: treatment effects “spill ” one group another (e.g., social influence marketing).Interference: treatment effects “spill ” one group another (e.g., social influence marketing).Noncompliance: participants adhere assigned treatment (e.g., customer ignoring ad).Noncompliance: participants adhere assigned treatment (e.g., customer ignoring ad).handle noncompliance, define:\\(Z_{ik}\\) = Assigned treatment (e.g., Ad Ad B).\\(Z_{ik}\\) = Assigned treatment (e.g., Ad Ad B).\\(D_{ik}\\) = Actual treatment received (e.g., whether user actually saw ad).\\(D_{ik}\\) = Actual treatment received (e.g., whether user actually saw ad).\\(Y_{ik}\\) = Outcome (e.g., purchase).\\(Y_{ik}\\) = Outcome (e.g., purchase).two-stage Instrumental Variable model adjusts noncompliance:\\[\n\\begin{aligned}\nD_{ik} &= \\gamma_0 + \\gamma_1 Z_{ik} + v_{ik} \\\\\nY_{ik} &= \\beta_0 + \\beta_1 D_{ik} + \\epsilon_{ik}\n\\end{aligned}\n\\] :\\(\\gamma_1\\) measures effect assignment actual treatment received.\\(\\gamma_1\\) measures effect assignment actual treatment received.\\(\\beta_1\\) estimates treatment effect, adjusting noncompliance.\\(\\beta_1\\) estimates treatment effect, adjusting noncompliance.individuals influence ’s outcomes, traditional randomization biased. Solutions include:Cluster Randomization: Assigning treatments group level (e.g., entire social circles receive ad).Cluster Randomization: Assigning treatments group level (e.g., entire social circles receive ad).Partial Interference Models: Assume interference occurs within predefined groups.Partial Interference Models: Assume interference occurs within predefined groups.first-stage regression estimates strongly \\(Z\\) affects \\(D\\) (compliance).first-stage regression estimates strongly \\(Z\\) affects \\(D\\) (compliance).second-stage regression estimates true causal effect \\(D\\) \\(Y\\).second-stage regression estimates true causal effect \\(D\\) \\(Y\\).interference present, standard IV method may biased. Researchers explore network-based randomization spatial models.","code":"\nset.seed(123)\nlibrary(ivreg)  # Load Instrumental Variable Regression Package\n\n# Generate data for first-stage treatment assignment\nn <- 500\ndata <- data.frame(\n  Z = sample(c(0, 1), n, replace = TRUE),  # Initial assignment (randomized)\n  D = NA,  # Actual treatment received (affected by compliance)\n  Y = NA   # Outcome variable (e.g., purchase)\n)\n\n# Introduce noncompliance: 80% compliance rate\ndata$D <- ifelse(runif(n) < 0.8, data$Z, 1 - data$Z)\n\n# Generate outcome variable (Y) with true treatment effect\n# True effect of D on Y is 3\ndata$Y <- 5 + 3 * data$D + rnorm(n, mean = 0, sd = 2)  \n\n# Estimate Two-Stage Least Squares (2SLS)\niv_model <- ivreg(Y ~ D | Z, data = data)\nsummary(iv_model)\n#> \n#> Call:\n#> ivreg(formula = Y ~ D | Z, data = data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.497 -1.344  0.018  1.303  5.493 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   5.1285     0.1861  27.557   <2e-16 ***\n#> D             2.7487     0.3072   8.949   <2e-16 ***\n#> \n#> Diagnostic tests:\n#>                  df1 df2 statistic p-value    \n#> Weak instruments   1 498    263.54  <2e-16 ***\n#> Wu-Hausman         1 497      0.19   0.663    \n#> Sargan             0  NA        NA      NA    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.017 on 498 degrees of freedom\n#> Multiple R-Squared: 0.2997,  Adjusted R-squared: 0.2983 \n#> Wald test: 80.08 on 1 and 498 DF,  p-value: < 2.2e-16"},{"path":"sec-experimental-design.html","id":"emerging-research","chapter":"22 Experimental Design","heading":"22.6 Emerging Research","text":"Recent research highlights significant challenges measuring causal effects ad content online advertising experiments. Digital advertising platforms employ algorithmic targeting dynamic ad delivery mechanisms, can introduce systematic biases /B testing (Braun Schwartz 2025). Key concerns include:Nonrandom Exposure: Online advertising platforms optimize ad delivery dynamically, meaning users randomly assigned different ad variants. Instead, platforms use proprietary algorithms serve ads different, often highly heterogeneous, user groups.Nonrandom Exposure: Online advertising platforms optimize ad delivery dynamically, meaning users randomly assigned different ad variants. Instead, platforms use proprietary algorithms serve ads different, often highly heterogeneous, user groups.Divergent Delivery: optimization process can lead “divergent delivery,” differences user engagement patterns platform objectives result non-comparable treatment groups. confounds true effect ad content algorithmic biases exposure.Divergent Delivery: optimization process can lead “divergent delivery,” differences user engagement patterns platform objectives result non-comparable treatment groups. confounds true effect ad content algorithmic biases exposure.Bias Measured Effects: Algorithmic targeting, user heterogeneity, data aggregation can distort magnitude direction estimated ad effects. means traditional /B test results may accurately reflect true impact ad creatives.Bias Measured Effects: Algorithmic targeting, user heterogeneity, data aggregation can distort magnitude direction estimated ad effects. means traditional /B test results may accurately reflect true impact ad creatives.Limited Transparency: Platforms little incentive assist advertisers researchers disentangling ad content effects proprietary targeting mechanisms. result, experimenters must design careful identification strategies mitigate biases.Limited Transparency: Platforms little incentive assist advertisers researchers disentangling ad content effects proprietary targeting mechanisms. result, experimenters must design careful identification strategies mitigate biases.","code":""},{"path":"sec-experimental-design.html","id":"covariate-balancing-in-online-ab-testing-the-pigeonhole-design","chapter":"22 Experimental Design","heading":"22.6.1 Covariate Balancing in Online A/B Testing: The Pigeonhole Design","text":"J. Zhao Zhou (2024) address challenge covariate balancing online /B testing experimental subjects arrive sequentially. Traditional experimental designs struggle maintain balance real-time settings treatment assignments must made immediately. work introduces online blocking problem, subjects heterogeneous covariates must assigned treatment control groups dynamically, goal minimizing total discrepancy—quantified minimum weight perfect matching groups.improve covariate balance, authors propose pigeonhole design, randomized experimental design operates follows:covariate space partitioned smaller subspaces called “pigeonholes.”Within pigeonhole, design balances number treated control subjects, reducing systematic imbalances.Theoretical analysis demonstrates effectiveness design reducing discrepancy compared existing approaches. Specifically, pigeonhole design benchmarked :Matched-pair designCompletely Randomized DesignThe results highlight scenarios pigeonhole design outperforms alternatives maintaining covariate balance.Using Yahoo! data, authors validate pigeonhole design practice, demonstrating 10.2% reduction variance estimating Average Treatment Effect. improvement underscores design’s practical utility reducing noise improving precision experimental results.findings J. Zhao Zhou (2024) provide valuable insights practitioners conducting online /B tests:Ensuring real-time covariate balance improves reliability causal estimates.pigeonhole design offers structured yet flexible approach balancing covariates dynamically.reducing variance treatment effect estimation, enhances statistical efficiency large-scale online experiments.study contributes broader discussion randomized experimental design dynamic settings, providing compelling alternative traditional approaches.","code":""},{"path":"sec-experimental-design.html","id":"sec-handling-zero-valued-outcomes","chapter":"22 Experimental Design","heading":"22.6.2 Handling Zero-Valued Outcomes","text":"analyzing treatment effects, common issue arises outcome variable includes zero values.example, business applications, marketing intervention may effect customers (resulting zero sales). apply log transformation outcome variable, problematic :\\(\\log(0)\\) undefined.Log transformation sensitive outcome units, making interpretation difficult (J. Chen Roth 2024).Instead applying log transformation, use methods robust zero values:Percentage changes Average\nusing Poisson Quasi-Maximum Likelihood Estimation (QMLE), can interpret coefficients percentage changes mean outcome treatment group relative control group.\nusing Poisson Quasi-Maximum Likelihood Estimation (QMLE), can interpret coefficients percentage changes mean outcome treatment group relative control group.Extensive vs. Intensive Margins\napproach distinguishes :\nExtensive margin: likelihood moving zero positive outcome (e.g., increasing probability making sale).\nIntensive margin: increase outcome given already positive (e.g., increasing sales amount).\n\nestimate intensive-margin bounds, use Lee (2009), assuming treatment monotonic effect outcome.\napproach distinguishes :\nExtensive margin: likelihood moving zero positive outcome (e.g., increasing probability making sale).\nIntensive margin: increase outcome given already positive (e.g., increasing sales amount).\nExtensive margin: likelihood moving zero positive outcome (e.g., increasing probability making sale).Intensive margin: increase outcome given already positive (e.g., increasing sales amount).estimate intensive-margin bounds, use Lee (2009), assuming treatment monotonic effect outcome.first generate dataset simulate scenario outcome variable (e.g., sales, website clicks) many zeros, treatment affects extensive intensive margins.","code":"\nset.seed(123) # For reproducibility\nlibrary(tidyverse)\n\nn <- 1000 # Number of observations\np_treatment <- 0.5 # Probability of being treated\n\n# Step 1: Generate the treatment variable D\nD <- rbinom(n, 1, p_treatment)\n\n# Step 2: Generate potential outcomes\n# Untreated potential outcome (mostly zeroes)\nY0 <- rnorm(n, mean = 0, sd = 1) * (runif(n) < 0.3)\n\n# Treated potential outcome (affecting both extensive and intensive margins)\nY1 <- Y0 + rnorm(n, mean = 2, sd = 1) * (runif(n) < 0.7)\n\n# Step 3: Combine effects based on treatment assignment\nY_observed <- (1 - D) * Y0 + D * Y1\n\n# Ensure non-negative outcomes (modeling real-world situations)\nY_observed[Y_observed < 0] <- 0\n\n# Create a dataset with an additional control variable\ndata <- data.frame(\n    ID = 1:n,\n    Treatment = D,\n    Outcome = Y_observed,\n    X = rnorm(n) # Control variable\n) |>\n    dplyr::mutate(positive = Outcome > 0) # Indicator for extensive margin\n\n# View first few rows\nhead(data)\n#>   ID Treatment   Outcome          X positive\n#> 1  1         0 0.0000000  1.4783345    FALSE\n#> 2  2         1 2.2369379 -1.4067867     TRUE\n#> 3  3         0 0.0000000 -1.8839721    FALSE\n#> 4  4         1 3.2192276 -0.2773662     TRUE\n#> 5  5         1 0.6649693  0.4304278     TRUE\n#> 6  6         0 0.0000000 -0.1287867    FALSE\n\n# Plot distribution of outcomes\nhist(\n    data$Outcome,\n    breaks = 30,\n    main = \"Distribution of Outcomes\",\n    col = \"lightblue\"\n)"},{"path":"sec-experimental-design.html","id":"estimating-percentage-changes-in-the-average","chapter":"22 Experimental Design","heading":"22.6.2.1 Estimating Percentage Changes in the Average","text":"Since use log transformation, estimate percentage changes using Poisson QMLE, robust zero-valued outcomes.interpret results:coefficient Treatment represents log-percentage change mean outcome treatment group relative control group.coefficient Treatment represents log-percentage change mean outcome treatment group relative control group.compute proportional effect, exponentiate coefficient:compute proportional effect, exponentiate coefficient:Compute standard error:Thus, conclude treatment effect increases outcome 1215% treated group compared control group.","code":"\nlibrary(fixest)\n\n# Poisson Quasi-Maximum Likelihood Estimation (QMLE)\nres_pois <- fepois(\n    fml = Outcome ~ Treatment + X, \n    data = data, \n    vcov = \"hetero\"\n)\n\n# Display results\netable(res_pois)\n#>                           res_pois\n#> Dependent Var.:            Outcome\n#>                                   \n#> Constant        -2.223*** (0.1440)\n#> Treatment        2.579*** (0.1494)\n#> X                  0.0235 (0.0406)\n#> _______________ __________________\n#> S.E. type       Heteroskedas.-rob.\n#> Observations                 1,000\n#> Squared Cor.               0.33857\n#> Pseudo R2                  0.26145\n#> BIC                        1,927.9\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Compute the proportional effect of treatment\ntreatment_effect <- exp(coefficients(res_pois)[\"Treatment\"]) - 1\ntreatment_effect\n#> Treatment \n#>  12.17757\n# Compute standard error\nse_treatment <- exp(coefficients(res_pois)[\"Treatment\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment\", \"Treatment\"])\nse_treatment\n#> Treatment \n#>  1.968684"},{"path":"sec-experimental-design.html","id":"estimating-extensive-vs.-intensive-margins","chapter":"22 Experimental Design","heading":"22.6.2.2 Estimating Extensive vs. Intensive Margins","text":"now estimate treatment effects separately extensive margin (probability nonzero outcome) intensive margin (magnitude effect among nonzero outcome).First, use Lee Bounds estimate intensive-margin effect individuals always nonzero outcome.Since confidence interval includes zero, conclude treatment significant intensive-margin effect.","code":"\nlibrary(causalverse)\n\nres <- causalverse::lee_bounds(\n    df = data,\n    d = \"Treatment\",\n    m = \"positive\",\n    y = \"Outcome\",\n    numdraws = 10\n) |> \n    causalverse::nice_tab(2)\n\nprint(res)\n#>          term estimate std.error\n#> 1 Lower bound    -0.22      0.09\n#> 2 Upper bound     2.77      0.14"},{"path":"sec-experimental-design.html","id":"sensitivity-analysis-varying-the-effect-for-compliers","chapter":"22 Experimental Design","heading":"22.6.2.3 Sensitivity Analysis: Varying the Effect for Compliers","text":"investigate intensive-margin effect, consider sensitive results different assumptions compliers.assume expected outcome compliers \\(100 \\times c%\\) lower higher always-takers: \\[\nE(Y(1) \\| \\text{Complier}) = (1 - c) E(Y(1) \\| \\text{Always-taker})\n\\] compute Lee Bounds different values \\(c\\):assume \\(c = 0.1\\), meaning compliers’ outcomes 10% always-takers’, intensive-margin effect 6.6 units higher always-takers.\\(c = 0.5\\), meaning compliers’ outcomes 50% always-takers’, intensive-margin effect 2.54 units higher.results highlight assumptions compliers affect conclusions intensive margin.dealing zero-valued outcomes, log transformations appropriate. Instead:Poisson QMLE provides robust way estimate percentage changes outcome.Poisson QMLE provides robust way estimate percentage changes outcome.Extensive vs. Intensive Margins allow us distinguish :\nprobability nonzero outcome (extensive margin).\nmagnitude change among nonzero outcome (intensive margin).\nExtensive vs. Intensive Margins allow us distinguish :probability nonzero outcome (extensive margin).probability nonzero outcome (extensive margin).magnitude change among nonzero outcome (intensive margin).magnitude change among nonzero outcome (intensive margin).Lee Bounds provide method estimate intensive-margin effect, though results can sensitive assumptions always-takers compliers.Lee Bounds provide method estimate intensive-margin effect, though results can sensitive assumptions always-takers compliers.","code":"\nset.seed(1)\nc_values = c(0.1, 0.5, 0.7)\n\ncombined_res <- bind_rows(lapply(c_values, function(c) {\n    res <- causalverse::lee_bounds(\n        df = data,\n        d = \"Treatment\",\n        m = \"positive\",\n        y = \"Outcome\",\n        numdraws = 10,\n        c_at_ratio = c\n    )\n    \n    res$c_value <- as.character(c)\n    return(res)\n}))\n\ncombined_res |> \n    dplyr::select(c_value, everything()) |> \n    causalverse::nice_tab()\n#>   c_value           term estimate std.error\n#> 1     0.1 Point estimate     6.60      0.71\n#> 2     0.5 Point estimate     2.54      0.13\n#> 3     0.7 Point estimate     1.82      0.08"},{"path":"sampling.html","id":"sampling","chapter":"23 Sampling","heading":"23 Sampling","text":"Sampling allows us draw conclusions population without analyzing every individual . business applications—marketing research, financial forecasting—sampling enables efficient decision-making reducing costs effort.","code":""},{"path":"sampling.html","id":"population-and-sample","chapter":"23 Sampling","heading":"23.1 Population and Sample","text":"refresher terminology regarding sampling.Population (\\(N\\)): complete set elements study.Sample (\\(n\\)): subset population selected analysis.Parameter: numerical measure describes characteristic population (e.g., population mean \\(\\mu\\), population variance \\(\\sigma^2\\)).Statistic: numerical measure computed sample, used estimate population parameter (e.g., sample mean \\(\\bar{x}\\), sample variance \\(s^2\\)).well-chosen sample ensures results generalize population, reducing sampling bias.","code":""},{"path":"sampling.html","id":"probability-sampling","chapter":"23 Sampling","heading":"23.2 Probability Sampling","text":"Probability sampling methods ensure every element population known, nonzero probability selected. methods preferred inferential statistics since allow estimation sampling error.","code":""},{"path":"sampling.html","id":"sec-simple-random-sampling","chapter":"23 Sampling","heading":"23.2.1 Simple Random Sampling","text":"Simple Random Sampling (SRS) ensures every element population equal chance selected. can done replacement without replacement, impacting whether element can chosen .example drawing simple random sample without replacement population 100 elements:Advantages:Simple easy implementSimple easy implementEnsures unbiased selectionEnsures unbiased selectionDisadvantages:May represent subgroups well, especially heterogeneous populationsMay represent subgroups well, especially heterogeneous populationsRequires access complete list populationRequires access complete list population","code":"\nset.seed(123)\npopulation <- 1:100  # A population of 100 elements\nsample_srs <- sample(population, size = 10, replace = FALSE)\nsample_srs\n#>  [1] 31 79 51 14 67 42 50 43 97 25"},{"path":"sampling.html","id":"using-dplyr","chapter":"23 Sampling","heading":"23.2.1.1 Using dplyr","text":"sample_n() function dplyr allows simple random sampling dataset:","code":"\nlibrary(dplyr)\niris_df <- iris\nset.seed(1)\nsample_n(iris_df, 5)  # Randomly selects 5 rows from the iris dataset\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#> 1          5.8         2.7          4.1         1.0 versicolor\n#> 2          6.4         2.8          5.6         2.1  virginica\n#> 3          4.4         3.2          1.3         0.2     setosa\n#> 4          4.3         3.0          1.1         0.1     setosa\n#> 5          7.0         3.2          4.7         1.4 versicolor"},{"path":"sampling.html","id":"using-the-sampling-package","chapter":"23 Sampling","heading":"23.2.1.2 Using the sampling Package","text":"sampling package provides functions random sampling without replacement.","code":"\nlibrary(sampling)\n# Assign a unique ID to each row in the dataset\niris_df$id <- 1:nrow(iris_df)\n\n# Simple random sampling without replacement\nsrs_sample <- srswor(10, length(iris_df$id))  \n# srs_sample\n\n# Simple random sampling with replacement\nsrs_sample_wr <- srswr(10, length(iris_df$id))\n# srs_sample_wr"},{"path":"sampling.html","id":"using-the-sampler-package","chapter":"23 Sampling","heading":"23.2.1.3 Using the sampler Package","text":"sampler package provides additional functionality, oversampling account non-response.","code":"\nlibrary(sampler)\nrsamp(albania, n = 260, over = 0.1, rep = FALSE)"},{"path":"sampling.html","id":"handling-missing-data-in-sample-collection","chapter":"23 Sampling","heading":"23.2.1.4 Handling Missing Data in Sample Collection","text":"compare sample received (collected) data identify missing elements:","code":"\nalsample <- rsamp(df = albania, 544)  # Initial sample\nalreceived <- rsamp(df = alsample, 390)  # Collected data\nrmissing(sampdf = alsample, colldf = alreceived, col_name = qvKod)"},{"path":"sampling.html","id":"sec-stratified-sampling","chapter":"23 Sampling","heading":"23.2.2 Stratified Sampling","text":"Stratified sampling involves dividing population distinct strata based characteristic (e.g., age, income level, region). random sample drawn stratum, often proportion size within population. method ensures subgroups adequately represented, improving precision estimates.following example demonstrates stratified sampling individuals belong three different groups (, B, C), random sample drawn .Advantages:Ensures representation subgroupsEnsures representation subgroupsMore precise estimates compared Simple Random SamplingMore precise estimates compared Simple Random SamplingReduces sampling error accounting population variabilityReduces sampling error accounting population variabilityDisadvantages:Requires prior knowledge population strataRequires prior knowledge population strataMore complex implement SRSMore complex implement SRS","code":"\nlibrary(dplyr)\n\nset.seed(123)\ndata <- data.frame(\n  ID = 1:100,\n  Group = sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)\n)\n\n# Stratified random sampling: selecting 10 elements per group\nstratified_sample <- data %>%\n  group_by(Group) %>%\n  sample_n(size = 10)\n\n# stratified_sample"},{"path":"sampling.html","id":"using-dplyr-for-stratified-sampling","chapter":"23 Sampling","heading":"23.2.2.1 Using dplyr for Stratified Sampling","text":"Sampling Fixed Number RowsHere, extract 5 random observations species iris dataset.Sampling Fraction StratumInstead selecting fixed number, can sample 15% species:","code":"\nlibrary(dplyr)\n\nset.seed(123)\nsample_iris <- iris %>%\n  group_by(Species) %>%\n  sample_n(5)  # Selects 5 samples per species\n\n# sample_iris\nset.seed(123)\nsample_iris <- iris %>%\n  group_by(Species) %>%\n  sample_frac(size = 0.15)  # Selects 15% of each species\n\n# sample_iris"},{"path":"sampling.html","id":"using-the-sampler-package-1","chapter":"23 Sampling","heading":"23.2.2.2 Using the sampler Package","text":"sampler package allows stratified sampling proportional allocation:","code":"\nlibrary(sampler)\n\n# Stratified sample using proportional allocation without replacement\nssamp(df = albania, n = 360, strata = qarku, over = 0.1)"},{"path":"sampling.html","id":"handling-missing-data-in-stratified-sampling","chapter":"23 Sampling","heading":"23.2.2.3 Handling Missing Data in Stratified Sampling","text":"identify number missing values stratum initial sample collected data:","code":"\nalsample <- rsamp(df = albania, 544)  # Initial sample\nalreceived <- rsamp(df = alsample, 390)  # Collected data\n\nsmissing(\n  sampdf = alsample,\n  colldf = alreceived,\n  strata = qarku,   # Strata column\n  col_name = qvKod  # Column for checking missing values\n)"},{"path":"sampling.html","id":"systematic-sampling","chapter":"23 Sampling","heading":"23.2.3 Systematic Sampling","text":"Selects every \\(k\\)th element random starting point.Advantages:Simple implementSimple implementEnsures even coverageEnsures even coverageDisadvantages:data follows pattern, bias may introduced","code":"\nk <- 10  # Select every 10th element\nstart <- sample(1:k, 1)  # Random start point\nsample_systematic <- population[seq(start, length(population), by = k)]"},{"path":"sampling.html","id":"cluster-sampling","chapter":"23 Sampling","heading":"23.2.4 Cluster Sampling","text":"Instead selecting individuals, entire clusters (e.g., cities, schools) randomly chosen, members selected clusters included.Advantages:Cost-effective population largeCost-effective population largeUseful population naturally divided groupsUseful population naturally divided groupsDisadvantages:Higher variabilityHigher variabilityRisk unrepresentative clustersRisk unrepresentative clusters","code":"\ndata$Cluster <- sample(1:10, 100, replace = TRUE)  # Assign 10 clusters\nchosen_clusters <- sample(1:10, size = 3)  # Select 3 clusters\ncluster_sample <- filter(data, Cluster %in% chosen_clusters)"},{"path":"sampling.html","id":"non-probability-sampling","chapter":"23 Sampling","heading":"23.3 Non-Probability Sampling","text":"methods give elements known probability selection. used exploratory research suitable making formal statistical inferences.","code":""},{"path":"sampling.html","id":"convenience-sampling","chapter":"23 Sampling","heading":"23.3.1 Convenience Sampling","text":"Selecting individuals easiest reach (e.g., mall surveys).Pros: Quick inexpensiveCons: High risk bias, generalizable","code":""},{"path":"sampling.html","id":"quota-sampling","chapter":"23 Sampling","heading":"23.3.2 Quota Sampling","text":"Similar stratified sampling non-random.Pros: Ensures subgroup representationCons: Subject selection bias","code":""},{"path":"sampling.html","id":"snowball-sampling","chapter":"23 Sampling","heading":"23.3.3 Snowball Sampling","text":"Used hard--reach populations (e.g., networking referrals).Pros: Useful population unknownCons: High bias, dependency initial subjects","code":""},{"path":"sampling.html","id":"sec-unequal-probability-sampling","chapter":"23 Sampling","heading":"23.4 Unequal Probability Sampling","text":"Unequal probability sampling assigns different selection probabilities elements population. approach often used certain units important, higher variability, require higher precision estimation.Common methods unequal probability sampling include:Probability Proportional Size (PPS): Selection probability proportional given auxiliary variable (e.g., revenue, population size).Probability Proportional Size (PPS): Selection probability proportional given auxiliary variable (e.g., revenue, population size).Poisson Sampling: Independent selection unit given probability.Poisson Sampling: Independent selection unit given probability.Systematic Sampling Unequal Probabilities: Uses systematic approach ensuring different probabilities.Systematic Sampling Unequal Probabilities: Uses systematic approach ensuring different probabilities.following functions sampling package implement various unequal probability sampling methods:methods specific use cases theoretical justifications. example:Poisson sampling allows flexible control sample size may lead variable sample sizes.Poisson sampling allows flexible control sample size may lead variable sample sizes.Systematic sampling useful population elements arranged meaningful order.Systematic sampling useful population elements arranged meaningful order.Tillé’s method ensures better control sample’s second-order inclusion probabilities.Tillé’s method ensures better control sample’s second-order inclusion probabilities.","code":"\nlibrary(sampling)\n\n# Different methods for unequal probability sampling\nUPbrewer()         # Brewer's method\nUPmaxentropy()     # Maximum entropy method\nUPmidzuno()        # Midzuno’s method\nUPmidzunopi2()     # Midzuno’s method with second-order inclusion probabilities\nUPmultinomial()    # Multinomial method\nUPpivotal()        # Pivotal method\nUPrandompivotal()  # Randomized pivotal method\nUPpoisson()        # Poisson sampling\nUPsampford()       # Sampford’s method\nUPsystematic()     # Systematic sampling\nUPrandomsystematic() # Randomized systematic sampling\nUPsystematicpi2()  # Systematic sampling with second-order probabilities\nUPtille()          # Tillé’s method\nUPtillepi2()       # Tillé’s method with second-order inclusion probabilities"},{"path":"sampling.html","id":"sec-balanced-sampling","chapter":"23 Sampling","heading":"23.5 Balanced Sampling","text":"Balanced sampling ensures means auxiliary variables sample match population. method improves estimation efficiency reduces variability without introducing bias.Balanced sampling differs purposive selection still involves randomization, ensuring statistical validity.balancing equation given : \\[\n\\sum_{k \\S} \\frac{\\mathbf{x}_k}{\\pi_k} = \\sum_{k \\U} \\mathbf{x}_k\n\\] :\\(\\mathbf{x}_k\\) vector auxiliary variables (e.g., income, age, household size).\\(\\mathbf{x}_k\\) vector auxiliary variables (e.g., income, age, household size).\\(\\pi_k\\) inclusion probability unit \\(k\\).\\(\\pi_k\\) inclusion probability unit \\(k\\).\\(S\\) sample, \\(U\\) population.\\(S\\) sample, \\(U\\) population.ensures total weighted sum auxiliary variables sample matches total sum population.","code":""},{"path":"sampling.html","id":"cube-method-for-balanced-sampling","chapter":"23 Sampling","heading":"23.5.1 Cube Method for Balanced Sampling","text":"Cube Method widely used approach balanced sampling, consisting two phases:Flight Phase: Ensures initial balance auxiliary variables.Flight Phase: Ensures initial balance auxiliary variables.Landing Phase: Adjusts sample meet constraints keeping randomness.Landing Phase: Adjusts sample meet constraints keeping randomness.","code":"\nlibrary(sampling)\n\n# Cube method functions\nsamplecube()       # Standard cube method\nfastflightcube()   # Optimized flight phase\nlandingcube()      # Landing phase method"},{"path":"sampling.html","id":"balanced-sampling-with-stratification","chapter":"23 Sampling","heading":"23.5.2 Balanced Sampling with Stratification","text":"Stratification attempts replicate population structure sample preserving original multivariate histogram.additional method balanced stratification :method ensures within stratum, sample retains original proportions auxiliary variables.","code":"\nlibrary(survey)\ndata(\"api\")\n\n# Stratified design with proportional allocation\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        strata = ~stype,\n                        id = ~1)\nbalancedstratification()"},{"path":"sampling.html","id":"balanced-sampling-in-cluster-sampling","chapter":"23 Sampling","heading":"23.5.3 Balanced Sampling in Cluster Sampling","text":"Cluster sampling involves selecting entire groups (clusters) instead individual units. balanced approach ensures sampled clusters preserve overall distribution auxiliary variables.explicitly balanced cluster sampling:method ensures cluster-level characteristics sample match population.","code":"\nlibrary(survey)\ndata(\"api\")\n\n# Cluster sampling design\nsrs_design <- svydesign(data = apiclus1,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~dnum)\nbalancedcluster()"},{"path":"sampling.html","id":"balanced-sampling-in-two-stage-sampling","chapter":"23 Sampling","heading":"23.5.4 Balanced Sampling in Two-Stage Sampling","text":"Two-stage sampling first selects primary units (e.g., schools, cities) samples within . balanced approach ensures representative selection stages.explicitly balanced two-stage sampling:method ensures auxiliary variables remain balanced across selection stages, reducing variability maintaining randomness.","code":"\nlibrary(survey)\ndata(\"api\")\n\n# Two-stage sampling design\nsrs_design <- svydesign(data = apiclus2, \n                        fpc = ~fpc1 + fpc2, \n                        id = ~dnum + snum)\nbalancedtwostage()"},{"path":"sampling.html","id":"sample-size-determination","chapter":"23 Sampling","heading":"23.6 Sample Size Determination","text":"appropriate sample size depends margin error, confidence level, population variability. commonly used formula estimating required sample size proportion :\\[\nn = \\frac{Z^2 p (1 - p)}{E^2}\n\\] :\\(Z\\) Z-score corresponding confidence level\\(Z\\) Z-score corresponding confidence level\\(p\\) estimated proportion\\(p\\) estimated proportion\\(E\\) margin error\\(E\\) margin error","code":"\nz <- qnorm(0.975)  # 95% confidence level\np <- 0.5  # Estimated proportion\nE <- 0.05  # 5% margin of error\nn <- (z^2 * p * (1 - p)) / (E^2)\nceiling(n)  # Round up to nearest integer\n#> [1] 385"},{"path":"sec-analysis-of-variance-anova.html","id":"sec-analysis-of-variance-anova","chapter":"24 Analysis of Variance","heading":"24 Analysis of Variance","text":"Analysis Variance (ANOVA) shares underlying mechanism linear regression. However, ANOVA approaches analysis different perspective, making particularly useful studying qualitative variables designed experiments.Key TerminologyFactor: explanatory predictor variable studied experiment.Treatment (Factor Level): specific value category factor applied experimental unit.Experimental Unit: entity (e.g., person, animal, material) subjected treatments providing response.Single-Factor Experiment: experiment one explanatory variable.Multifactor Experiment: experiment involving multiple explanatory variables.Classification Factor: factor controlled experimenter (common observational studies).Experimental Factor: factor directly assigned experimenter.well-designed experiment requires careful planning following areas:Choice treatments: Selecting factor levels tested.Selection experimental units: Ensuring appropriate sample.Treatment assignment: Avoiding selection bias proper randomization.Measurement: Minimizing measurement bias considering blinding necessary.Advancements Experimental DesignFactorial Experiments:\nInvestigate multiple factors simultaneously.\nAllow study interactions factors.\nInvestigate multiple factors simultaneously.Allow study interactions factors.Replication:\nRepeating experiments increases statistical power.\nHelps estimate mean squared error.\nRepeating experiments increases statistical power.Helps estimate mean squared error.Randomization:\nIntroduced formally R.. Fisher early 1900s.\nEnsures treatment assignment systematically biased.\nHelps eliminate confounding effects due time, space, lurking variables.\nIntroduced formally R.. Fisher early 1900s.Ensures treatment assignment systematically biased.Helps eliminate confounding effects due time, space, lurking variables.Local Control (Blocking/Stratification):\nReduces experimental error controlling known sources variability.\nIncreases power grouping similar experimental units together randomizing treatments.\nReduces experimental error controlling known sources variability.Increases power grouping similar experimental units together randomizing treatments.Randomization also helps eliminate correlations due time space.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-completely-randomized-design","chapter":"24 Analysis of Variance","heading":"24.1 Completely Randomized Design","text":"Completely Randomized Design (CRD) simplest type experimental design, experimental units randomly assigned treatments.Consider treatment factor \\(\\) \\(\\geq 2\\) treatment levels. experimental unit randomly assigned one levels. number units group can :Balanced: groups equal sample sizes \\(n\\).Unbalanced: Groups different sample sizes \\(n_i\\) (\\(= 1, ..., \\)).total sample size given :\\[\nN = \\sum_{=1}^{} n_i\n\\]number possible assignments units treatments :\\[\nk = \\frac{N!}{n_1! n_2! \\dots n_a!}\n\\]assignment equal probability selected: \\(1/k\\). response experimental unit denoted \\(Y_{ij}\\), :\\(\\) indexes treatment group.\\(j\\) indexes individual unit within treatment \\(\\).Treatment Response TableWhere:\\[\n\\bar{Y_{.}} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} Y_{ij}\n\\]\\[\ns_i^2 = \\frac{1}{n_i - 1} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y_{.}})^2\n\\]grand mean :\\[\n\\bar{Y_{..}} = \\frac{1}{N} \\sum_{} \\sum_{j} Y_{ij}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-single-factor-fixed-effects-model","chapter":"24 Analysis of Variance","heading":"24.1.1 Single-Factor Fixed Effects ANOVA","text":"Also known One-Way ANOVA ANOVA Type Model.total variability response variable \\(Y_{ij}\\) can decomposed follows:\\[\n\\begin{aligned}\nY_{ij} - \\bar{Y_{..}} &= Y_{ij} - \\bar{Y}_{..} + \\bar{Y}_{.} - \\bar{Y}_{.} \\\\\n& = (\\bar{Y_{.}} - \\bar{Y_{..}}) + (Y_{ij} - \\bar{Y_{.}})\n\\end{aligned}\n\\]:first term represents -treatment variability (deviation treatment means grand mean).second term represents within-treatment variability (deviation observations treatment mean).Thus, partition total sum squares (SSTO) :\\[\n\\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y_{..}})^2 = \\sum_{} n_i (\\bar{Y_{.}} - \\bar{Y_{..}})^2 + \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y_{.}})^2\n\\]equivalently:\\[\nSSTO = SSTR + SSE\n\\]:SSTO (Total SS): Total variability data.SSTR (Treatment SS): Variability due differences treatment means.SSE (Error SS): Variability within treatments (unexplained variance).Degrees freedom (d.f.):\\[\n(N-1) = (-1) + (N-)\n\\]lose degree freedom total corrected SSTO estimation mean (\\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y}_{..} )= 0\\)) SSTR (\\(\\sum_i n_i (\\bar{Y}_{.} - \\bar{Y}_{..}) = 0\\))Mean squares:\\[\nMSTR = \\frac{SSTR}{-1}, \\quad MSR = \\frac{SSE}{N-}\n\\]ANOVA TableFor linear model interpretation ANOVA, eitherCell Means ModelTreatment Effect (Factor Effects Model)","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-cell-means-model","chapter":"24 Analysis of Variance","heading":"24.1.1.1 Cell Means Model","text":"cell means model describes response :\\[\nY_{ij} = \\mu_i + \\epsilon_{ij}\n\\]:\\(Y_{ij}\\): Response unit \\(j\\) treatment \\(\\).\\(\\mu_i\\): Fixed population mean treatment \\(\\).\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\): Independent errors.\\(E(Y_{ij}) = \\mu_i\\), \\(\\text{Var}(Y_{ij}) = \\sigma^2\\).observations assumed equal variance across treatments.Example: ANOVA \\(= 3\\) TreatmentsConsider case three treatments (\\(= 3\\)), treatment two replicates (\\(n_1 = n_2 = n_3 = 2\\)). response vector can expressed matrix form :\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\]:\\(X_{k,ij} = 1\\) \\(k\\)-th treatment applied unit \\((,j)\\).\\(X_{k,ij} = 0\\) otherwise.Note: intercept term model.least squares estimator \\(\\beta\\) given :Thus, estimated treatment means :\\[\n\\hat{\\mu}_i = \\bar{Y_i}, \\quad = 1,2,3\n\\]estimator \\(\\mathbf{b} = [\\bar{Y_1}, \\bar{Y_2}, \\bar{Y_3}]'\\) best linear unbiased estimator (BLUE) \\(\\beta\\) (.e., \\(E(\\mathbf{b}) = \\beta\\))Since \\(\\mathbf{b} \\sim N(\\beta, \\sigma^2 (\\mathbf{X'X})^{-1})\\), variance estimated treatment means :\\[\nvar(\\mathbf{b}) = \\sigma^2(\\mathbf{X'X})^{-1} = \\sigma^2\n\\left[\\begin{array}{ccc}\n1/n_1 & 0 & 0\\\\\n0 & 1/n_2 & 0\\\\\n0 & 0 & 1/n_3\\\\\n\\end{array}\\right]\n\\]Thus, variance estimated treatment mean :\\[\nvar(b_i) = var(\\hat{\\mu}_i) = \\frac{\\sigma^2}{n_i}, \\quad = 1,2,3\n\\]mean squared error (MSE) given :\\[\n\\begin{aligned}\nMSE\n&= \\frac{1}{N - } \\sum_{=1}^\\sum_{j=1}^{n_i} \\bigl(Y_{ij} - \\overline{Y}_{\\cdot}\\bigr)^2\n\\\\[6pt]\n&= \\frac{1}{N - }\n   \\sum_{=1}^\n   \\Bigl[\n     (n_i - 1) \\;\n     \\underbrace{\n       \\frac{1}{n_i - 1}\n       \\sum_{j=1}^{n_i}\n         \\bigl(Y_{ij} - \\overline{Y}_{\\cdot}\\bigr)^2\n     }_{=\\,s_i^2}\n   \\Bigr]\n\\\\[6pt]\n&= \\frac{1}{N - } \\sum_{=1}^(n_i - 1)\\, s_i^2.\n\\end{aligned}\n\\]\\(s_i^2\\) sample variance within \\(\\)-th treatment group.Since \\(E(s_i^2) = \\sigma^2\\), get:\\[\nE(MSE) = \\frac{1}{N-} \\sum_{} (n_i-1) \\sigma^2 = \\sigma^2\n\\]Thus, MSE unbiased estimator \\(\\sigma^2\\), regardless whether treatment means equal.expected mean square treatments (MSTR) :\\[\nE(MSTR) = \\sigma^2 + \\frac{\\sum_{} n_i (\\mu_i - \\mu_.)^2}{-1}\n\\]:\\[\n\\mu_. = \\frac{\\sum_{=1}^{} n_i \\mu_i}{\\sum_{=1}^{} n_i}\n\\]treatment means equal (\\(\\mu_1 = \\mu_2 = \\dots = \\mu_a = \\mu_.\\)), :\\[\nE(MSTR) = \\sigma^2\n\\]\\(F\\)-Test Equality Treatment MeansWe test null hypothesis:\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_a\n\\]alternative:\\[\nH_a: \\text{least one } \\mu_i \\text{ differs}\n\\]test statistic :\\[\nF = \\frac{MSTR}{MSE}\n\\]Large values \\(F\\) suggest rejecting \\(H_0\\) (since MSTR larger MSE \\(H_a\\) true).Values \\(F\\) near 1 suggest fail reject \\(H_0\\).Since \\(MSTR\\) \\(MSE\\) independent chi-square random variables scaled degrees freedom, \\(H_0\\):\\[\nF \\sim F_{(-1, N-)}\n\\]Decision Rule:\\(F \\leq F_{(-1, N-;1-\\alpha)}\\), fail reject \\(H_0\\).\\(F \\geq F_{(-1, N-;1-\\alpha)}\\), reject \\(H_0\\).two treatments (\\(= 2\\)), ANOVA \\(F\\)-test reduces two-sample \\(t\\)-test:\\[\nF = t^2\n\\]:\\[\nt = \\frac{\\bar{Y_1} - \\bar{Y_2}}{\\sqrt{MSE \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"treatment-effects-factor-effects","chapter":"24 Analysis of Variance","heading":"24.1.1.2 Treatment Effects (Factor Effects)","text":"Besides cell means model, another way formalize one-way ANOVA:\\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\]:\\(Y_{ij}\\) \\(j\\)-th response \\(\\)-th treatment.\\(\\tau_i\\) \\(\\)-th treatment effect.\\(\\mu\\) constant component common observations.\\(\\epsilon_{ij}\\) independent random errors, assumed normally distributed: \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\).example, \\(= 3\\) treatments \\(n_1 = n_2 = n_3 = 2\\) observations per treatment, model representation :However, matrix:\\[\n\\mathbf{X'X} =\n\\left(\n\\begin{array}\n{cccc}\n\\sum_{}n_i & n_1 & n_2 & n_3 \\\\\nn_1 & n_1 & 0 & 0 \\\\\nn_2 & 0 & n_2 & 0 \\\\\nn_3 & 0 & 0 & n_3 \\\\\n\\end{array}\n\\right)\n\\]singular, meaning \\(\\mathbf{X'X}\\) invertible. results infinite number possible solutions \\(\\mathbf{b}\\).resolve , impose restrictions parameters ensure \\(\\mathbf{X}\\) full rank. Regardless restriction used, expected value remains:\\[\nE(Y_{ij}) = \\mu + \\tau_i = \\mu_i = \\text{mean response $$-th treatment}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"restriction-on-sum-of-tau","chapter":"24 Analysis of Variance","heading":"24.1.1.2.1 Restriction on Sum of Treatment Effects","text":"One common restriction :\\[\n\\sum_{=1}^{} \\tau_i = 0\n\\]implies :\\[\n\\mu = \\frac{1}{} \\sum_{=1}^{} (\\mu + \\tau_i)\n\\]meaning \\(\\mu\\) represents grand mean (overall mean response across treatments).treatment effect can expressed :\\[\n\\begin{aligned}\n\\tau_i &= \\mu_i - \\mu \\\\\n&= \\text{treatment mean} - \\text{grand mean}\n\\end{aligned}\n\\]Since \\(\\sum_{} \\tau_i = 0\\), can solve \\(\\tau_a\\) :\\[\n\\tau_a = -(\\tau_1 + \\tau_2 + \\dots + \\tau_{-1})\n\\]Thus, mean \\(\\)-th treatment :\\[\n\\mu_a = \\mu + \\tau_a = \\mu - (\\tau_1 + \\tau_2 + \\dots + \\tau_{-1})\n\\]reduces number parameters \\(+ 1\\) just \\(\\), meaning estimate:\\[\n\\mu, \\tau_1, \\tau_2, ..., \\tau_{-1}\n\\]Rewriting Equation (24.2):\\(\\beta = [\\mu, \\tau_1, \\tau_2]'\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"restriction-on-first-tau","chapter":"24 Analysis of Variance","heading":"24.1.1.2.2 Restriction on the First \\(\\tau\\)","text":"R, default parameterization lm() one-way ANOVA model sets \\(\\tau_1 = 0\\). effectively chooses first treatment (group) baseline reference, making treatment effect \\(\\tau_1\\) equal zero.Consider last example three treatments, two observations, \\(\\,n_1 = n_2 = n_3 = 2\\). restriction \\(\\tau_1 = 0\\), treatment means can expressed :\\[\n\\begin{aligned}\n\\mu_1 &= \\mu + \\tau_1 \\;=\\; \\mu + 0 \\;=\\; \\mu, \\\\\n\\mu_2 &= \\mu + \\tau_2, \\\\\n\\mu_3 &= \\mu + \\tau_3.\n\\end{aligned}\n\\]Hence, \\(\\mu\\) becomes mean response first treatment.write observations vector form:\\[\n\\begin{aligned}\n\\mathbf{y}\n&= \\begin{pmatrix}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{pmatrix}\n=\n\\underbrace{\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n\\end{pmatrix}\n}_{\\mathbf{X}}\n\\begin{pmatrix}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3 \\\\\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{pmatrix} \\\\\n&= \\mathbf{X\\beta} + \\mathbf{\\epsilon},\n\\end{aligned}\n\\]\\[\n\\beta =\n\\begin{pmatrix}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3\n\\end{pmatrix}.\n\\]OLS estimator :\\[\n\\mathbf{b}\n=\n\\begin{pmatrix}\n\\hat{\\mu} \\\\\n\\hat{\\tau_2} \\\\\n\\hat{\\tau_3}\n\\end{pmatrix}\n= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{y}.\n\\]specific case equal sample sizes (\\(n_1=n_2=n_3=2\\)), \\((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) calculation yields:\\[\n\\begin{aligned}\n\\mathbf{b}\n& =     \\left[\\begin{array}{ccc}          \\sum_{}n_i & n_2 & n_3\\\\          n_2 & n_2 & 0\\\\          n_3 & 0 & n_3 \\\\          \\end{array}\\right]^{-1}\\left[\\begin{array}{c}      Y_{..}\\\\      Y_{2.}\\\\      Y_{3.}\\\\      \\end{array}\\right] \\\\\n&=\n\\begin{pmatrix}\n\\bar{Y}_{1\\cdot} \\\\\n\\bar{Y}_{2\\cdot} - \\bar{Y}_{1\\cdot} \\\\\n\\bar{Y}_{3\\cdot} - \\bar{Y}_{1\\cdot}\n\\end{pmatrix}\n\\end{aligned}\n\\] \\(\\bar{Y}_{1\\cdot}\\), \\(\\bar{Y}_{2\\cdot}\\), \\(\\bar{Y}_{3\\cdot}\\) sample means treatments 1, 2, 3, respectively.Taking expectation \\(\\mathbf{b}\\) confirms:\\[\nE(\\mathbf{b})\n=\n\\beta\n=\n\\begin{pmatrix}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2 - \\mu_1 \\\\\n\\mu_3 - \\mu_1\n\\end{pmatrix}.\n\\]Recall :\\[\n\\text{Var}(\\mathbf{b})\n=\n\\sigma^2\\,(\\mathbf{X}'\\mathbf{X})^{-1}.\n\\]Hence,\\[\n\\begin{aligned}\n\\text{Var}(\\hat{\\mu})\n&= \\text{Var}(\\bar{Y}_{1\\cdot})\n= \\frac{\\sigma^2}{n_1}, \\\\[6pt]\n\\text{Var}(\\hat{\\tau_2})\n&= \\text{Var}\\bigl(\\bar{Y}_{2\\cdot}-\\bar{Y}_{1\\cdot}\\bigr)\n= \\frac{\\sigma^2}{n_2} + \\frac{\\sigma^2}{n_1}, \\\\[6pt]\n\\text{Var}(\\hat{\\tau_3})\n&= \\text{Var}\\bigl(\\bar{Y}_{3\\cdot}-\\bar{Y}_{1\\cdot}\\bigr)\n= \\frac{\\sigma^2}{n_3} + \\frac{\\sigma^2}{n_1}.\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"equivalence-of-parameterizations","chapter":"24 Analysis of Variance","heading":"24.1.1.3 Equivalence of Parameterizations","text":"Despite different ways writing model, three parameterizations yield ANOVA table:Model 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\).Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\sum_i \\tau_i = 0\\).Model 3: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\tau_1 = 0\\).three lead fitted values, \\[\n\\mathbf{\\hat{Y}} = \\mathbf{X}\\bigl(\\mathbf{X}'\\mathbf{X}\\bigr)^{-1}\\mathbf{X}'\\mathbf{Y}\n= \\mathbf{P\\,Y}\n= \\mathbf{X\\,b}.\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"anova-table","chapter":"24 Analysis of Variance","heading":"24.1.1.4 ANOVA Table","text":"generic form ANOVA table :Error(within treatments)\\(\\mathbf{P}_1 = \\frac{1}{n}\\mathbf{J}\\), \\(n = \\sum_i n_i\\), \\(\\mathbf{J}\\) -ones matrix.\\(F\\)-statistic \\((-1, N-)\\) degrees freedom numeric value unchanged three parameterizations. slight difference lies state null hypothesis:\\[\n\\begin{aligned}\nH_0 &: \\mu_1 = \\mu_2 = \\dots = \\mu_a, \\\\\nH_0 &: \\mu + \\tau_1 = \\mu + \\tau_2 = \\dots = \\mu + \\tau_a, \\\\\nH_0 &: \\tau_1 = \\tau_2 = \\dots = \\tau_a.\n\\end{aligned}\n\\]\\(F\\)-test serves preliminary analysis, see difference different factors. -depth analysis, consider different testing treatment effects.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"testing-of-treatment-effects","chapter":"24 Analysis of Variance","heading":"24.1.1.5 Testing of Treatment Effects","text":"Single Treatment Mean \\(\\mu_i\\)Differences Treatment MeansContrast Among Treatment MeansLinear Combination Treatment Means","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-single-treatment-mean-anova","chapter":"24 Analysis of Variance","heading":"24.1.1.5.1 Single Treatment Mean","text":"single treatment group, sample mean serves estimate population mean:\\[\n\\hat{\\mu_i} = \\bar{Y}_{.}\n\\]:\\(E(\\bar{Y}_{.}) = \\mu_i\\), indicating unbiasedness.\\(var(\\bar{Y}_{.}) = \\frac{\\sigma^2}{n_i}\\), estimated \\(s^2(\\bar{Y}_{.}) = \\frac{MSE}{n_i}\\).Since standardized test statistic\\[\nT = \\frac{\\bar{Y}_{.} - \\mu_i}{s(\\bar{Y}_{.})}\n\\]follows \\(t\\)-distribution \\(N-\\) degrees freedom (\\(t_{N-}\\)), \\((1-\\alpha)100\\%\\) confidence interval \\(\\mu_i\\) :\\[\n\\bar{Y}_{.} \\pm t_{1-\\alpha/2;N-} s(\\bar{Y}_{.})\n\\]test whether \\(\\mu_i\\) equal constant \\(c\\), set hypothesis:\\[\n\\begin{aligned}\n&H_0: \\mu_i = c \\\\\n&H_1: \\mu_i \\neq c\n\\end{aligned}\n\\]test statistic:\\[\nT = \\frac{\\bar{Y}_{.} - c}{s(\\bar{Y}_{.})} \\sim t_{N-}\n\\]\\(H_0\\), reject \\(H_0\\) \\(\\alpha\\) level :\\[\n|T| > t_{1-\\alpha/2;N-}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-differences-between-treatment-means-anova","chapter":"24 Analysis of Variance","heading":"24.1.1.5.2 Differences Between Treatment Means","text":"difference two treatment means, also called pairwise comparison, given :\\[\nD = \\mu_i - \\mu_{'}\n\\]estimated :\\[\n\\hat{D} = \\bar{Y}_{.} - \\bar{Y}_{'.}\n\\]estimate unbiased since:\\[\nE(\\hat{D}) = \\mu_i - \\mu_{'}\n\\]Since \\(\\bar{Y}_{.}\\) \\(\\bar{Y}_{'.}\\) independent, variance \\(\\hat{D}\\) :\\[\nvar(\\hat{D}) = var(\\bar{Y}_{.}) + var(\\bar{Y}_{'.}) = \\sigma^2 \\left(\\frac{1}{n_i} + \\frac{1}{n_{'}}\\right)\n\\]estimated :\\[\ns^2(\\hat{D}) = MSE \\left(\\frac{1}{n_i} + \\frac{1}{n_{'}}\\right)\n\\]Using inference structure single treatment mean:\\[\n\\frac{\\hat{D} - D}{s(\\hat{D})} \\sim t_{N-}\n\\]\\((1-\\alpha)100\\%\\) confidence interval \\(D\\) :\\[\n\\hat{D} \\pm t_{1-\\alpha/2;N-} s(\\hat{D})\n\\]hypothesis testing:\\[\n\\begin{aligned}\n&H_0: \\mu_i = \\mu_{'} \\\\\n&H_a: \\mu_i \\neq \\mu_{'}\n\\end{aligned}\n\\]use test statistic:\\[\nT = \\frac{\\hat{D}}{s(\\hat{D})} \\sim t_{N-}\n\\]reject \\(H_0\\) \\(\\alpha\\) level :\\[\n|T| > t_{1-\\alpha/2;N-}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-contrast-among-treatment-means-anova","chapter":"24 Analysis of Variance","heading":"24.1.1.5.3 Contrast Among Treatment Means","text":"generalize comparison two means, introduce contrasts.contrast linear combination treatment means:\\[\nL = \\sum_{=1}^{} c_i \\mu_i\n\\]coefficients \\(c_i\\) non-random constants satisfy constraint:\\[\n\\sum_{=1}^{} c_i = 0\n\\]ensures contrasts focus relative comparisons rather absolute magnitudes.unbiased estimator \\(L\\) given :\\[\n\\hat{L} = \\sum_{=1}^{} c_i \\bar{Y}_{.}\n\\]Since expectation linear operator:\\[\nE(\\hat{L}) = \\sum_{=1}^{} c_i E(\\bar{Y}_{.}) = \\sum_{=1}^{} c_i \\mu_i = L\n\\]Thus, \\(\\hat{L}\\) unbiased estimator \\(L\\).Since sample means \\(\\bar{Y}_{.}\\) independent, variance \\(\\hat{L}\\) :\\[\n\\begin{aligned}\nvar(\\hat{L}) &= var\\left(\\sum_{=1}^c_i \\bar{Y}_{.} \\right) \\\\\n&= \\sum_{=1}^c_i^2 var(\\bar{Y}_{.}) \\\\\n&= \\sum_{=1}^c_i^2 \\frac{\\sigma^2}{n_i} \\\\\n&= \\sigma^2 \\sum_{=1}^{} \\frac{c_i^2}{n_i}\n\\end{aligned}\n\\]Since \\(\\sigma^2\\) unknown, estimate using mean squared error:\\[\ns^2(\\hat{L}) = MSE \\sum_{=1}^{} \\frac{c_i^2}{n_i}\n\\]Since \\(\\hat{L}\\) linear combination independent normal random variables, follows normal distribution:\\[\n\\hat{L} \\sim N\\left(L, \\sigma^2 \\sum_{=1}^{} \\frac{c_i^2}{n_i} \\right)\n\\]Since \\(SSE/\\sigma^2 \\sim \\chi^2_{N-}\\) \\(MSE = SSE/(N-)\\), use \\(t\\)-distribution:\\[\n\\frac{\\hat{L} - L}{s(\\hat{L})} \\sim t_{N-}\n\\]Thus, \\((1-\\alpha)100\\%\\) confidence interval \\(L\\) :\\[\n\\hat{L} \\pm t_{1-\\alpha/2; N-} s(\\hat{L})\n\\]test whether specific contrast equals zero:\\[\n\\begin{aligned}\n&H_0: L = 0 \\quad \\text{(difference contrast)} \\\\\n&H_a: L \\neq 0 \\quad \\text{(significant contrast)}\n\\end{aligned}\n\\]use test statistic:\\[\nT = \\frac{\\hat{L}}{s(\\hat{L})} \\sim t_{N-}\n\\]reject \\(H_0\\) \\(\\alpha\\) level :\\[\n|T| > t_{1-\\alpha/2;N-}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-linear-combination-of-treatment-means-anova","chapter":"24 Analysis of Variance","heading":"24.1.1.5.4 Linear Combination of Treatment Means","text":"linear combination treatment means extends idea contrast:\\[\nL = \\sum_{=1}^{} c_i \\mu_i\n\\]Unlike contrasts, restrictions coefficients \\(c_i\\) (.e., need sum zero).Since tests single treatment mean, pairwise differences, contrasts special cases general form, can express hypothesis test :\\[\n\\begin{aligned}\n&H_0: \\sum_{=1}^{} c_i \\mu_i = c \\\\\n&H_a: \\sum_{=1}^{} c_i \\mu_i \\neq c\n\\end{aligned}\n\\]test statistic follows \\(t\\)-distribution:\\[\nT = \\frac{\\hat{L} - c}{s(\\hat{L})} \\sim t_{N-}\n\\]Since squaring \\(t\\)-distributed variable results \\(F\\)-distributed variable,\\[\nF = T^2 \\sim F_{1,N-}\n\\]means tests can viewed single-degree--freedom \\(F\\)-tests, since numerator degrees freedom always 1.Multiple ContrastsWhen testing \\(k \\geq 2\\) contrasts simultaneously, test statistics \\(T_1, T_2, ..., T_k\\) follow multivariate \\(t\\)-distribution, since dependent (based data).Limitations Multiple ComparisonsInflation Type Error:\nconfidence coefficient \\((1-\\alpha)\\) applies single estimate, series estimates. Similarly, Type error rate \\(\\alpha\\) applies individual test, collection tests.\nExample: three \\(t\\)-tests performed \\(\\alpha = 0.05\\), independent (), :\n\\[\n(1 - 0.05)^3 = 0.857\n\\]\nmeaning overall Type error rate approximately \\(0.143\\), \\(0.05\\).Inflation Type Error:\nconfidence coefficient \\((1-\\alpha)\\) applies single estimate, series estimates. Similarly, Type error rate \\(\\alpha\\) applies individual test, collection tests.Example: three \\(t\\)-tests performed \\(\\alpha = 0.05\\), independent (), :\\[\n(1 - 0.05)^3 = 0.857\n\\]meaning overall Type error rate approximately \\(0.143\\), \\(0.05\\).Data Snooping Concern:\nsignificance level \\(\\alpha\\) valid test planned examining data.\nOften, experiment suggests relationships investigate.\nExploring effects based observed data known data snooping.\nData Snooping Concern:\nsignificance level \\(\\alpha\\) valid test planned examining data.Often, experiment suggests relationships investigate.Exploring effects based observed data known data snooping.address issues, use Multiple Comparison Procedures, :Tukey – pairwise comparisons treatment means.Scheffé – possible contrasts.Bonferroni – fixed number planned comparisons.","code":""},{},{},{},{},{},{},{},{"path":"sec-analysis-of-variance-anova.html","id":"sec-single-factor-random-effects-model","chapter":"24 Analysis of Variance","heading":"24.1.2 Single Factor Random Effects ANOVA","text":"Also known ANOVA Type II model, single factor random effects model assumes treatments randomly selected larger population. Thus, inference extends beyond observed treatments entire population treatments.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"random-cell-means-model","chapter":"24 Analysis of Variance","heading":"24.1.2.1 Random Cell Means Model","text":"model given :\\[\nY_{ij} = \\mu_i + \\epsilon_{ij}\n\\]:\\(\\mu_i \\sim N(\\mu, \\sigma^2_{\\mu})\\), independent across treatments.\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\), independent across observations.\\(\\mu_i\\) \\(\\epsilon_{ij}\\) mutually independent \\(= 1, \\dots, \\) \\(j = 1, \\dots, n\\).treatment sample sizes equal:\\[\n\\begin{aligned}\nE(Y_{ij}) &= E(\\mu_i) = \\mu \\\\\nvar(Y_{ij}) &= var(\\mu_i) + var(\\epsilon_{ij}) = \\sigma^2_{\\mu} + \\sigma^2\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"covariance-structure","chapter":"24 Analysis of Variance","heading":"24.1.2.1.1 Covariance Structure","text":"Since \\(Y_{ij}\\) independent, calculate covariances:treatment group (\\(\\) fixed, \\(j \\neq j'\\)):\\[\n\\begin{aligned}\ncov(Y_{ij}, Y_{ij'}) &= E(Y_{ij} Y_{ij'}) - E(Y_{ij}) E(Y_{ij'}) \\\\\n&= E(\\mu_i^2 + \\mu_i \\epsilon_{ij'} + \\mu_i \\epsilon_{ij} + \\epsilon_{ij} \\epsilon_{ij'}) - \\mu^2 \\\\\n&= \\sigma^2_{\\mu} + \\mu^2 - \\mu^2 \\\\\n&= \\sigma^2_{\\mu}\n\\end{aligned}\n\\]Different treatment groups (\\(\\neq '\\)):\\[\n\\begin{aligned}\ncov(Y_{ij}, Y_{'j'}) &= E(\\mu_i \\mu_{'} + \\mu_i \\epsilon_{'j'} + \\mu_{'} \\epsilon_{ij} + \\epsilon_{ij} \\epsilon_{'j'}) - \\mu^2 \\\\\n&= \\mu^2 - \\mu^2 = 0\n\\end{aligned}\n\\]Thus:observations variance: \\(var(Y_{ij}) = \\sigma^2_{\\mu} + \\sigma^2\\).Observations treatment covariance: \\(\\sigma^2_{\\mu}\\).Observations different treatments uncorrelated.intraclass correlation two responses treatment:\\[\n\\rho(Y_{ij}, Y_{ij'}) = \\frac{\\sigma^2_{\\mu}}{\\sigma^2_{\\mu} + \\sigma^2}, \\quad j \\neq j'\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"inference-for-random-effects-model","chapter":"24 Analysis of Variance","heading":"24.1.2.1.2 Inference for Random Effects Model","text":"Intraclass Correlation Coefficient:\\[\n\\frac{\\sigma^2_{\\mu}}{\\sigma^2 + \\sigma^2_{\\mu}}\n\\]measures proportion total variability \\(Y_{ij}\\) accounted treatment differences.test whether treatments contribute significantly variance:\\[\n\\begin{aligned}\n&H_0: \\sigma_{\\mu}^2 = 0 \\quad \\text{(treatment effect, $\\mu_i = \\mu$)} \\\\\n&H_a: \\sigma_{\\mu}^2 \\neq 0\n\\end{aligned}\n\\]\\(H_0\\), ANOVA F-test used:\\[\nF = \\frac{MSTR}{MSE}\n\\]:\\(MSTR\\) (Mean Square Treatments) captures variation treatments.\\(MSE\\) (Mean Square Error) captures variation within treatments.\\(H_0\\) true, :\\[\nF \\sim F_{(-1, (n-1))}\n\\]Reject \\(H_0\\) :\\[\nF > f_{(1-\\alpha; -1, (n-1))}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"comparison-fixed-effects-vs.-random-effects-models","chapter":"24 Analysis of Variance","heading":"24.1.2.1.3 Comparison: Fixed Effects vs. Random Effects Models","text":"Although ANOVA calculations fixed random effects models, interpretation results differs.\\(\\sigma^2_{\\mu} = 0\\), \\(E(MSTR) = E(MSE)\\), implying treatment effect.Otherwise, \\(E(MSTR) > E(MSE)\\), suggesting significant treatment variation.sample sizes equal, \\(F\\)-test remains valid, degrees freedom change :\\[\nF \\sim F_{(-1, N-)}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"estimation-of-mu","chapter":"24 Analysis of Variance","heading":"24.1.2.1.4 Estimation of \\(\\mu\\)","text":"unbiased estimator \\(E(Y_{ij}) = \\mu\\) grand mean:\\[\n\\hat{\\mu} = \\bar{Y}_{..} = \\frac{1}{n} \\sum_{=1}^{} \\sum_{j=1}^{n} Y_{ij}\n\\]variance estimator :\\[\n\\begin{aligned}\nvar(\\bar{Y}_{..}) &= var\\left(\\frac{1}{} \\sum_{=1}^{} \\bar{Y}_{.} \\right) \\\\\n&= \\frac{1}{^2} \\sum_{=1}^{} var(\\bar{Y}_{.}) \\\\\n&= \\frac{1}{^2} \\sum_{=1}^{} \\left(\\sigma^2_\\mu + \\frac{\\sigma^2}{n} \\right) \\\\\n&= \\frac{n \\sigma^2_{\\mu} + \\sigma^2}{n}\n\\end{aligned}\n\\]unbiased estimator variance :\\[\ns^2(\\bar{Y}_{..}) = \\frac{MSTR}{n}\n\\]Since:\\[\n\\frac{\\bar{Y}_{..} - \\mu}{s(\\bar{Y}_{..})} \\sim t_{-1}\n\\]\\((1-\\alpha)100\\%\\) confidence interval \\(\\mu\\) :\\[\n\\bar{Y}_{..} \\pm t_{1-\\alpha/2; -1} s(\\bar{Y}_{..})\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"estimation-of-intraclass-correlation-coefficient-fracsigma2_musigma2_musigma2","chapter":"24 Analysis of Variance","heading":"24.1.2.1.5 Estimation of Intraclass Correlation Coefficient \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_{\\mu}+\\sigma^2}\\)","text":"random fixed effects models, \\(MSTR\\) \\(MSE\\) independent.sample sizes equal (\\(n_i = n\\) \\(\\)), test statistic:\\[\n\\frac{\\frac{MSTR}{n\\sigma^2_\\mu + \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\sim F_{-1, (n-1)}\n\\]\\((1-\\alpha)100\\%\\) confidence interval \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\) follows :\\[\nP\\left(f_{\\alpha/2; -1, (n-1)} \\leq \\frac{\\frac{MSTR}{n\\sigma^2_\\mu + \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\leq f_{1-\\alpha/2; -1, (n-1)} \\right) = 1 - \\alpha\n\\]Defining:\\[\n\\begin{aligned}\nL &= \\frac{1}{n} \\left( \\frac{MSTR}{MSE} \\times \\frac{1}{f_{1-\\alpha/2; -1, (n-1)}} - 1 \\right) \\\\\nU &= \\frac{1}{n} \\left( \\frac{MSTR}{MSE} \\times \\frac{1}{f_{\\alpha/2; -1, (n-1)}} - 1 \\right)\n\\end{aligned}\n\\]lower upper confidence limits \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\) :\\[\n\\begin{aligned}\nL^* &= \\frac{L}{1+L} \\\\\nU^* &= \\frac{U}{1+U}\n\\end{aligned}\n\\]\\(L^*\\) negative, customarily set 0.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"estimation-of-sigma2","chapter":"24 Analysis of Variance","heading":"24.1.2.1.6 Estimation of \\(\\sigma^2\\)","text":"Since:\\[\n\\frac{(n-1) MSE}{\\sigma^2} \\sim \\chi^2_{(n-1)}\n\\]\\((1-\\alpha)100\\%\\) confidence interval \\(\\sigma^2\\) :\\[\n\\frac{(n-1) MSE}{\\chi^2_{1-\\alpha/2; (n-1)}} \\leq \\sigma^2 \\leq \\frac{(n-1) MSE}{\\chi^2_{\\alpha/2; (n-1)}}\n\\]sample sizes unequal, formula applies, degrees freedom change :\\[\ndf = N - \n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"estimation-of-sigma2_mu","chapter":"24 Analysis of Variance","heading":"24.1.2.1.7 Estimation of \\(\\sigma^2_\\mu\\)","text":"expectations:\\[\nE(MSE) = \\sigma^2, \\quad E(MSTR) = \\sigma^2 + n\\sigma^2_\\mu\n\\]solve \\(\\sigma^2_{\\mu}\\):\\[\n\\sigma^2_{\\mu} = \\frac{E(MSTR) - E(MSE)}{n}\n\\]unbiased estimator \\(\\sigma^2_\\mu\\) :\\[\ns^2_\\mu = \\frac{MSTR - MSE}{n}\n\\]\\(s^2_\\mu < 0\\), set \\(s^2_\\mu = 0\\) (since variances negative).sample sizes unequal, replace \\(n\\) effective sample size \\(n'\\):\\[\ns^2_\\mu = \\frac{MSTR - MSE}{n'}\n\\]:\\[\nn' = \\frac{1}{-1} \\left(\\sum_i n_i - \\frac{\\sum_i n_i^2}{\\sum_i n_i} \\right)\n\\]exact confidence intervals \\(\\sigma^2_\\mu\\), can approximate using Satterthwaite procedure.","code":""},{},{"path":"sec-analysis-of-variance-anova.html","id":"random-treatment-effects-model","chapter":"24 Analysis of Variance","heading":"24.1.2.2 Random Treatment Effects Model","text":"random effects model, treatment levels considered random samples larger population possible treatments. model accounts variability across potential treatments, just observed study.define random treatment effect :\\[\n\\tau_i = \\mu_i - E(\\mu_i) = \\mu_i - \\mu\n\\]\\(\\tau_i\\) represents deviation treatment mean \\(\\mu_i\\) overall mean \\(\\mu\\).Thus, rewrite treatment means :\\[\n\\mu_i = \\mu + \\tau_i\n\\]Substituting response model:\\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]:\\(\\mu\\) = common mean across observations.\\(\\tau_i \\sim N(0, \\sigma^2_\\tau)\\), random treatment effects, assumed independent.\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\), random error terms, also independent.\\(\\tau_{}\\) \\(\\epsilon_{ij}\\) mutually independent \\(= 1, \\dots, \\) \\(j = 1, \\dots, n\\).consider balanced single-factor ANOVA (equal sample sizes across treatments).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"diagnostic-measures-for-model-assumptions","chapter":"24 Analysis of Variance","heading":"24.1.2.3 Diagnostic Measures for Model Assumptions","text":"Checking assumptions crucial valid inference. Common issues include:","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"remedial-measures","chapter":"24 Analysis of Variance","heading":"24.1.2.4 Remedial Measures","text":"diagnostic checks indicate violations assumptions, possible solutions include:Weighted Least Squares – Adjusts heteroscedasticity.Variable Transformation – Log Box-Cox transformations may improve normality stabilize variance.Non-Parametric Procedures – Kruskal-Wallis test bootstrapping normality assumptions fail.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"key-notes-on-robustness","chapter":"24 Analysis of Variance","heading":"24.1.2.5 Key Notes on Robustness","text":"Fixed effects ANOVA relatively robust :\nNon-normality, particularly sample sizes moderate large.\nUnequal variances sample sizes roughly equal.\nF-test multiple comparisons remain valid mild violations.\nNon-normality, particularly sample sizes moderate large.Unequal variances sample sizes roughly equal.F-test multiple comparisons remain valid mild violations.Random effects ANOVA sensitive :\nLack independence, severely affects fixed random effects models.\nUnequal variances, particularly estimating variance components.\nLack independence, severely affects fixed random effects models.Unequal variances, particularly estimating variance components.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-two-factor-fixed-effects-anova","chapter":"24 Analysis of Variance","heading":"24.1.3 Two-Factor Fixed Effects ANOVA","text":"multi-factor experiment offers several advantages:Higher efficiency – precise estimates fewer observations.Increased information – Allows testing interactions factors.Greater validity – Reduces confounding controlling additional sources variation.Balanced Two-Factor ANOVA: AssumptionsEqual sample sizes treatment combinations.treatment means equal importance (weighting).Factors categorical chosen purposefully.assume:Factor \\(\\) levels Factor B \\(b\\) levels.\\(\\times b\\) factor level combinations included.treatment combination \\(n\\) replications.total number observations:\\[ N = abn \\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-cell-means-model-two-factor-anova","chapter":"24 Analysis of Variance","heading":"24.1.3.1 Cell Means Model","text":"response modeled :\\[\nY_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\n\\]:\\(\\mu_{ij}\\) fixed parameters (cell means).\\(= 1, \\dots, \\) represents levels Factor .\\(j = 1, \\dots, b\\) represents levels Factor B.\\(\\epsilon_{ijk} \\sim \\text{independent } N(0, \\sigma^2)\\) \\(, j, k\\).Expected values variance:\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{ij} \\\\\nvar(Y_{ijk}) &= var(\\epsilon_{ijk}) = \\sigma^2\n\\end{aligned}\n\\]Thus:\\[\nY_{ijk} \\sim \\text{independent } N(\\mu_{ij}, \\sigma^2)\n\\]can expressed matrix notation:\\[\n\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\n\\]:\\[\n\\begin{aligned}\nE(\\mathbf{Y}) &= \\mathbf{X} \\beta \\\\\nvar(\\mathbf{Y}) &= \\sigma^2 \\mathbf{}\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"interaction-effects","chapter":"24 Analysis of Variance","heading":"24.1.3.1.1 Interaction Effects","text":"Interaction measures whether effect one factor depends level factor. defined :\\[\n(\\alpha \\beta)_{ij} = \\mu_{ij} - (\\mu_{..} + \\alpha_i + \\beta_j)\n\\]:Grand mean:\\[ \\mu_{..} = \\frac{1}{ab} \\sum_i \\sum_j \\mu_{ij} \\]Main effect Factor (average effect level \\(\\)):\\[ \\alpha_i = \\mu_{.} - \\mu_{..} \\]Main effect Factor B (average effect level \\(j\\)):\\[ \\beta_j = \\mu_{.j} - \\mu_{..} \\]Interaction effect:\\[ (\\alpha \\beta)_{ij} = \\mu_{ij} - \\mu_{.} - \\mu_{.j} + \\mu_{..} \\]determine whether interactions exist:Check \\(\\mu_{ij}\\) can written sums \\(\\mu_{..} + \\alpha_i + \\beta_j\\)\n(.e., check interaction terms zero).Compare mean differences across levels Factor B level Factor .Compare mean differences across levels Factor level Factor B.Graphical method:\nPlot treatment means level Factor B.\nlines parallel, interaction exists.\nPlot treatment means level Factor B.lines parallel, interaction exists.interaction terms satisfy:level Factor B:\\[\n\\sum_i (\\alpha \\beta)_{ij} = \\sum_i \\left(\\mu_{ij} - \\mu_{..} - \\alpha_i - \\beta_j \\right)\n\\]Expanding:\\[\n\\begin{aligned}\n\\sum_i (\\alpha \\beta)_{ij} &= \\sum_i \\mu_{ij} - \\mu_{..} - \\sum_i \\alpha_i - \\beta_j \\\\\n&= \\mu_{.j} - \\mu_{..} - \\sum_i (\\mu_{.} - \\mu_{..}) - (\\mu_{.j} - \\mu_{..}) \\\\\n&= \\mu_{.j} - \\mu_{..} - \\mu_{..}+ \\mu_{..} - (\\mu_{.j} - \\mu_{..})  \\\\\n&= 0\n\\end{aligned}\n\\]Similarly:\\[\n\\sum_j (\\alpha \\beta)_{ij} = 0, \\quad = 1, \\dots, \n\\]:\\[\n\\sum_i \\sum_j (\\alpha \\beta)_{ij} = 0, \\quad \\sum_i \\alpha_i = 0, \\quad \\sum_j \\beta_j = 0\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-factor-effects-model-two-factor-anova","chapter":"24 Analysis of Variance","heading":"24.1.3.2 Factor Effects Model","text":"Factor Effects Model, express response :\\[\n\\begin{aligned}\n\\mu_{ij} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\end{aligned}\n\\]:\\(\\mu_{..}\\) grand mean.\\(\\alpha_i\\) main effects Factor , subject :\\[ \\sum_i \\alpha_i = 0 \\]\\(\\beta_j\\) main effects Factor B, subject :\\[ \\sum_j \\beta_j = 0 \\]\\((\\alpha \\beta)_{ij}\\) interaction effects, subject :\\[ \\sum_i (\\alpha \\beta)_{ij} = 0, \\quad j = 1, \\dots, b \\]\\[ \\sum_j (\\alpha \\beta)_{ij} = 0, \\quad = 1, \\dots, \\]\\(\\epsilon_{ijk} \\sim \\text{independent } N(0, \\sigma^2)\\) \\(k = 1, \\dots, n\\).Thus, :\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\\nvar(Y_{ijk}) &= \\sigma^2 \\\\\nY_{ijk} &\\sim N (\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}, \\sigma^2)\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"parameter-counting-and-restrictions","chapter":"24 Analysis of Variance","heading":"24.1.3.3 Parameter Counting and Restrictions","text":"Cell Means Model \\(ab\\) parameters corresponding combination factor levels.\nFactor Effects Model, imposed constraints reduce number estimable parameters:Thus, total number parameters:\\[\n1 + (-1) + (b-1) + (-1)(b-1) = ab\n\\]matches number parameters Cell Means Model.uniquely estimate parameters, apply constraints:\\[\n\\begin{aligned}\n\\alpha_a  &= -(\\alpha_1 + \\alpha_2 + \\dots + \\alpha_{-1}) \\\\\n\\beta_b &= -(\\beta_1 + \\beta_2 + \\dots + \\beta_{b-1}) \\\\\n(\\alpha \\beta)_{ib} &= -(\\alpha \\beta)_{i1} - (\\alpha \\beta)_{i2} - \\dots - (\\alpha \\beta)_{,b-1}, \\quad = 1, \\dots, \\\\\n(\\alpha \\beta)_{aj} &= -(\\alpha \\beta)_{1j} - (\\alpha \\beta)_{2j} - \\dots - (\\alpha \\beta)_{-1,j}, \\quad j = 1, \\dots, b\n\\end{aligned}\n\\]model can fitted using least squares maximum likelihood estimation.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"cell-means-model-estimation","chapter":"24 Analysis of Variance","heading":"24.1.3.3.1 Cell Means Model Estimation","text":"Minimizing:\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{ij})^2\n\\]yields estimators:\\[\n\\begin{aligned}\n\\hat{\\mu}_{ij} &= \\bar{Y}_{ij} \\\\\n\\hat{Y}_{ijk} &= \\bar{Y}_{ij} \\\\\ne_{ijk} &= Y_{ijk} - \\hat{Y}_{ijk} = Y_{ijk} - \\bar{Y}_{ij}\n\\end{aligned}\n\\]\\(e_{ijk} \\sim \\text{independent } N(0, \\sigma^2)\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"factor-effects-model-estimation","chapter":"24 Analysis of Variance","heading":"24.1.3.3.2 Factor Effects Model Estimation","text":"Minimizing:\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{..} - \\alpha_i - \\beta_j - (\\alpha \\beta)_{ij})^2\n\\]subject constraints:\\[\n\\begin{aligned}\n\\sum_i \\alpha_i &= 0 \\\\\n\\sum_j \\beta_j &= 0 \\\\\n\\sum_i (\\alpha \\beta)_{ij} &= 0, \\quad j = 1, \\dots, b \\\\\n\\sum_j (\\alpha \\beta)_{ij} &= 0, \\quad = 1, \\dots, \n\\end{aligned}\n\\]yields estimators:\\[\n\\begin{aligned}\n\\hat{\\mu}_{..} &= \\bar{Y}_{...} \\\\\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\beta}_j &= \\bar{Y}_{.j.} - \\bar{Y}_{...} \\\\\n(\\hat{\\alpha \\beta})_{ij} &= \\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.} + \\bar{Y}_{...}\n\\end{aligned}\n\\]fitted values :\\[\n\\hat{Y}_{ijk} = \\bar{Y}_{...} + (\\bar{Y}_{..} - \\bar{Y}_{...}) + (\\bar{Y}_{.j.} - \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.} + \\bar{Y}_{...})\n\\]simplifies :\\[\n\\hat{Y}_{ijk} = \\bar{Y}_{ij.}\n\\]residuals :\\[\ne_{ijk} = Y_{ijk} - \\bar{Y}_{ij.}\n\\]follow:\\[\ne_{ijk} \\sim \\text{independent } N(0, \\sigma^2)\n\\]variances estimated effects :\\[\n\\begin{aligned}\ns^2_{\\hat{\\mu}_{..}} &= \\frac{MSE}{nab} \\\\\ns^2_{\\hat{\\alpha}_i} &= MSE \\left(\\frac{1}{nb} - \\frac{1}{nab} \\right) \\\\\ns^2_{\\hat{\\beta}_j} &= MSE \\left(\\frac{1}{na} - \\frac{1}{nab} \\right) \\\\\ns^2_{(\\hat{\\alpha\\beta})_{ij}} &= MSE \\left(\\frac{1}{n} - \\frac{1}{na} - \\frac{1}{nb} + \\frac{1}{nab} \\right)\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"partitioning-the-total-sum-of-squares","chapter":"24 Analysis of Variance","heading":"24.1.3.3.3 Partitioning the Total Sum of Squares","text":"total deviation observation overall mean can decomposed :\\[\nY_{ijk} - \\bar{Y}_{...} = (\\bar{Y}_{ij.} - \\bar{Y}_{...}) + (Y_{ijk} - \\bar{Y}_{ij.})\n\\]:\\(Y_{ijk} - \\bar{Y}_{...}\\): Total deviation observation.\\(\\bar{Y}_{ij.} - \\bar{Y}_{...}\\): Deviation treatment mean overall mean.\\(Y_{ijk} - \\bar{Y}_{ij.}\\): Residual deviation observation treatment mean.Summing observations:\\[\n\\sum_i \\sum_j \\sum_k (Y_{ijk} - \\bar{Y}_{...})^2 = n \\sum_i \\sum_j (\\bar{Y}_{ij.} - \\bar{Y}_{...})^2 + \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\bar{Y}_{ij.})^2\n\\]Thus:\\[\nSSTO = SSTR + SSE\n\\]:\\(SSTO\\) = Total Sum Squares (Total variation).\\(SSTR\\) = Treatment Sum Squares (Variation due factor effects).\\(SSE\\) = Error Sum Squares (Residual variation).Since cross-product terms 0, model naturally partitions variance.factor effects model:\\[\n\\bar{Y}_{ij.} - \\bar{Y}_{...} = (\\bar{Y}_{..} - \\bar{Y}_{...}) + (\\bar{Y}_{.j.} - \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.} + \\bar{Y}_{...})\n\\]Squaring summing:\\[\n\\begin{aligned}\nn\\sum_i \\sum_j (\\bar{Y}_{ij.} - \\bar{Y}_{...})^2 &= nb\\sum_i (\\bar{Y}_{..} - \\bar{Y}_{...})^2 + na\\sum_j (\\bar{Y}_{.j.} - \\bar{Y}_{...})^2 \\\\\n&+ n\\sum_i \\sum_j (\\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.} + \\bar{Y}_{...})^2\n\\end{aligned}\n\\]Thus, treatment sum squares can partitioned :\\[\nSSTR = SSA + SSB + SSAB\n\\]:\\(SSA\\): Sum Squares Factor .\\(SSB\\): Sum Squares Factor B.\\(SSAB\\): Sum Squares Interaction.interaction term can also expressed :\\[\nSSAB = SSTO - SSE - SSA - SSB\n\\]equivalently:\\[\nSSAB = SSTR - SSA - SSB\n\\]:\\(SSA\\) measures variability estimated factor level means (\\(\\bar{Y}_{..}\\)). variable means, larger \\(SSA\\).\\(SSB\\) measures variability estimated factor B level means (\\(\\bar{Y}_{.j.}\\)).\\(SSAB\\) measures variability interaction effects.Two-Factor ANOVA, degrees freedom partitioning follows:Since:\\[\nSSTR = SSA + SSB + SSAB\n\\]treatment degrees freedom also partition :\\[\nab - 1 = (- 1) + (b - 1) + (- 1)(b - 1)\n\\]\\(df_{SSA} = - 1\\)\n(One degree freedom lost due constraint \\(\\sum (\\bar{Y}_{..} - \\bar{Y}_{...}) = 0\\)).\\(df_{SSB} = b - 1\\)\n(One degree freedom lost due constraint \\(\\sum (\\bar{Y}_{.j.} - \\bar{Y}_{...}) = 0\\)).\\(df_{SSAB} = (- 1)(b - 1)\\)\n(Due interaction constraints).Mean Squares obtained dividing Sum Squares corresponding degrees freedom:\\[\n\\begin{aligned}\nMSA &= \\frac{SSA}{- 1} \\\\\nMSB &= \\frac{SSB}{b - 1} \\\\\nMSAB &= \\frac{SSAB}{(- 1)(b - 1)}\n\\end{aligned}\n\\]expectations mean squares :\\[\n\\begin{aligned}\nE(MSE) &= \\sigma^2 \\\\\nE(MSA) &= \\sigma^2 + nb \\frac{\\sum \\alpha_i^2}{- 1} = \\sigma^2 + nb \\frac{\\sum (\\mu_{..} - \\mu_{..})^2}{- 1} \\\\\nE(MSB) &= \\sigma^2 + na \\frac{\\sum \\beta_j^2}{b - 1} = \\sigma^2 + na \\frac{\\sum (\\mu_{.j.} - \\mu_{..})^2}{b - 1} \\\\\nE(MSAB) &= \\sigma^2 + n \\frac{\\sum \\sum (\\alpha \\beta)^2_{ij}}{(-1)(b-1)} = \\sigma^2 + n \\frac{\\sum (\\mu_{ij} - \\mu_{..} - \\mu_{.j.} + \\mu_{..})^2}{(- 1)(b - 1)}\n\\end{aligned}\n\\]Factor effect (\\(\\mu_{..} = \\mu_{..}\\)), \\(MSA\\) \\(MSE\\) expectation.\nSimilarly, Factor B effect, \\(MSB = MSE\\).Thus, MSA > MSE MSB > MSE suggest presence factor effects.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"testing-for-interaction","chapter":"24 Analysis of Variance","heading":"24.1.3.4 Testing for Interaction","text":"Hypotheses:\\[\n\\begin{aligned}\nH_0: \\mu_{ij} - \\mu_{..} - \\mu_{.j.} + \\mu_{..} = 0 &\\quad \\text{(interaction)} \\\\\nH_a: \\mu_{ij} - \\mu_{..} - \\mu_{.j.} + \\mu_{..} \\neq 0 &\\quad \\text{(Interaction present)}\n\\end{aligned}\n\\]equivalently:\\[\n\\begin{aligned}\n&H_0: \\text{} (\\alpha \\beta)_{ij} = 0 \\\\\n&H_a: \\text{} (\\alpha \\beta)_{ij} = 0\n\\end{aligned}\n\\]F-statistic :\\[\nF = \\frac{MSAB}{MSE}\n\\]\\(H_0\\), \\(F \\sim F_{(-1)(b-1), ab(n-1)}\\). Reject \\(H_0\\) :\\[\nF > F_{1-\\alpha; (-1)(b-1), ab(n-1)}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"two-way-anova-summary-table","chapter":"24 Analysis of Variance","heading":"24.1.3.5 Two-Way ANOVA Summary Table","text":"Two-Way ANOVA table partitions total variation components:Interpreting Two-Way ANOVA ResultsWhen conducting Two-Way ANOVA, always check interaction effects first:interaction (\\(\\times B\\)) significant:\neffect one factor depends level factor.\nMain effects interpretable alone impact varies across levels second factor.\neffect one factor depends level factor.Main effects interpretable alone impact varies across levels second factor.interaction significant:\nfactors independent (additive) effects.\nMain effects can tested individually.\nfactors independent (additive) effects.Main effects can tested individually.Post-Hoc ComparisonsIf interaction significant, proceed main effect comparisons using:\nTukey\nScheffé\nBonferroni\nTukeySchefféBonferroniIf interaction significant, post-hoc tests examine simple effects (comparisons within level factor).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"contrasts-in-two-way-anova","chapter":"24 Analysis of Variance","heading":"24.1.3.5.1 Contrasts in Two-Way ANOVA","text":"Two-Way ANOVA, can define contrasts test specific hypotheses:\\[\nL = \\sum c_i \\mu_i, \\quad \\text{} \\sum c_i = 0\n\\]unbiased estimator \\(L\\):\\[\n\\hat{L} = \\sum c_i \\bar{Y}_{..}\n\\]variance:\\[\n\\sigma^2(\\hat{L}) = \\frac{\\sigma^2}{bn} \\sum c_i^2\n\\]variance estimate:\\[\n\\frac{MSE}{bn} \\sum c_i^2\n\\]","code":""},{},{},{"path":"sec-analysis-of-variance-anova.html","id":"unbalanced-two-way-anova","chapter":"24 Analysis of Variance","heading":"24.1.3.6 Unbalanced Two-Way ANOVA","text":"many practical situations, sample sizes may unequal across factor combinations, :Observational studies (e.g., real-world data missing values).Dropouts designed studies (e.g., clinical trials subject attrition).Larger sample sizes inexpensive treatments.Sample sizes chosen match population proportions.assume standard Two-Way ANOVA model:\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]sample sizes vary:\\[\n\\begin{aligned}\nn_{.} &= \\sum_j n_{ij} \\quad \\text{(Total factor level } ) \\\\\nn_{.j} &= \\sum_i n_{ij} \\quad \\text{(Total factor level } j) \\\\\nn_T &= \\sum_i \\sum_j n_{ij} \\quad \\text{(Total sample size)}\n\\end{aligned}\n\\]However, unbalanced designs, major issue arises:\\[\nSSTO \\neq SSA + SSB + SSAB + SSE\n\\]Unlike balanced case, design non-orthogonal, meaning sum--squares partitions add cleanly.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"indicator-variables-for-factor-levels","chapter":"24 Analysis of Variance","heading":"24.1.3.6.1 Indicator Variables for Factor Levels","text":"handle unbalanced data, use indicator (dummy) variables predictors.Factor (\\(= 1, \\dots, -1\\)):\\[\nu_i =\n\\begin{cases}\n+1 & \\text{observation level } \\text{ Factor } \\\\\n-1 & \\text{observation reference level (level } \\text{)} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (\\(j = 1, \\dots, b-1\\)):\\[\nv_j =\n\\begin{cases}\n+1 & \\text{observation level } j \\text{ Factor B} \\\\\n-1 & \\text{observation reference level (level } b \\text{)} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Rewriting ANOVA model using indicator variables:\\[\nY = \\mu_{..} + \\sum_{=1}^{-1} \\alpha_i u_i + \\sum_{j=1}^{b-1} \\beta_j v_j + \\sum_{=1}^{-1} \\sum_{j=1}^{b-1}(\\alpha \\beta)_{ij} u_i v_j + \\epsilon\n\\], unknown parameters :\\(\\mu_{..}\\) (grand mean),\\(\\mu_{..}\\) (grand mean),\\(\\alpha_i\\) (main effects Factor ),\\(\\alpha_i\\) (main effects Factor ),\\(\\beta_j\\) (main effects Factor B),\\(\\beta_j\\) (main effects Factor B),\\((\\alpha \\beta)_{ij}\\) (interaction effects).\\((\\alpha \\beta)_{ij}\\) (interaction effects).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"hypothesis-testing-using-extra-sum-of-squares","chapter":"24 Analysis of Variance","heading":"24.1.3.6.2 Hypothesis Testing Using Extra Sum of Squares","text":"unbalanced designs, use sequential (type ) adjusted (type III) sum squares test hypotheses.test interaction effects, test:\\[\n\\begin{aligned}\n&H_0: \\text{} (\\alpha \\beta)_{ij} = 0 \\quad \\text{(interaction)} \\\\\n&H_a: \\text{} (\\alpha \\beta)_{ij} = 0 \\quad \\text{(Interaction present)}\n\\end{aligned}\n\\]test whether Factor B effect:\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_b = 0 \\\\\n&H_a: \\text{least one } \\beta_j \\neq 0\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"factor-mean-analysis-and-contrasts","chapter":"24 Analysis of Variance","heading":"24.1.3.6.3 Factor Mean Analysis and Contrasts","text":"Factor means contrasts (e.g., pairwise comparisons) work similarly balanced case require adjustments due unequal sample sizes.variance estimate contrast:\\[\n\\sigma^2(\\hat{L}) = \\frac{\\sigma^2}{\\sum n_{ij}} \\sum c_i^2\n\\]modified :\\[\n\\frac{MSE}{\\sum n_{ij}} \\sum c_i^2\n\\]Orthogonal contrasts harder define unequal sample sizes break orthogonality.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"regression-approach-to-unbalanced-anova","chapter":"24 Analysis of Variance","heading":"24.1.3.6.4 Regression Approach to Unbalanced ANOVA","text":"alternative fit cell means model regression model:\\[\nY_{ij} = \\mu_{ij} + \\epsilon_{ij}\n\\]allows us analyze treatment mean separately.However, empty cells (factor combinations observations), regression approach fails, partial analyses can conducted.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-two-way-random-effects-anova","chapter":"24 Analysis of Variance","heading":"24.1.4 Two-Way Random Effects ANOVA","text":"Two-Way Random Effects ANOVA assumes Factor Factor B levels randomly sampled larger populations.model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]:\\(\\mu_{..}\\): Overall mean (constant).\\(\\alpha_i \\sim N(0, \\sigma^2_{\\alpha})\\) \\(= 1, \\dots, \\) (random effects Factor , independently distributed).\\(\\beta_j \\sim N(0, \\sigma^2_{\\beta})\\) \\(j = 1, \\dots, b\\) (random effects Factor B, independently distributed).\\((\\alpha \\beta)_{ij} \\sim N(0, \\sigma^2_{\\alpha \\beta})\\) \\(= 1, \\dots, \\), \\(j = 1, \\dots, b\\) (random interaction effects, independently distributed).\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) (random error, independently distributed).Additionally, random effects (\\(\\alpha_i, \\beta_j, (\\alpha \\beta)_{ij}\\)) error terms (\\(\\epsilon_{ijk}\\)) mutually independent.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"expectation","chapter":"24 Analysis of Variance","heading":"24.1.4.1 Expectation","text":"Taking expectations sides:\\[\nE(Y_{ijk}) = E(\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk})\n\\]Since random effects mean zero:\\[\nE(Y_{ijk}) = \\mu_{..}\n\\]Thus, mean response across factor levels \\(\\mu_{..}\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"variance-1","chapter":"24 Analysis of Variance","heading":"24.1.4.2 Variance","text":"total variance observations sum variance components:\\[\n\\begin{aligned}\nvar(Y_{ijk}) &= var(\\alpha_i) + var(\\beta_j) + var((\\alpha \\beta)_{ij}) + var(\\epsilon_{ijk}) \\\\\n&= \\sigma^2_{\\alpha} + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\end{aligned}\n\\]Thus:\\[\nY_{ijk} \\sim N(\\mu_{..}, \\sigma^2_{\\alpha} + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta} + \\sigma^2)\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"covariance-structure-1","chapter":"24 Analysis of Variance","heading":"24.1.4.3 Covariance Structure","text":"random effects models, observations correlated share factor levels.Case 1: factor , different factor BIf \\(\\) \\(j \\neq j'\\), :\\[\ncov(Y_{ijk}, Y_{ij'k'}) = var(\\alpha_i) = \\sigma^2_{\\alpha}\n\\]Case 2: factor B, different factor AIf \\(j\\) \\(\\neq '\\), :\\[\ncov(Y_{ijk}, Y_{'jk'}) = var(\\beta_j) = \\sigma^2_{\\beta}\n\\]Case 3: factor B, different replicationIf factor levels (\\(, j\\) fixed), different replication (\\(k \\neq k'\\)):\\[\ncov(Y_{ijk}, Y_{ijk'}) = var(\\alpha_i) + var(\\beta_j) + var((\\alpha \\beta)_{ij}) = \\sigma^2_{\\alpha} + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta}\n\\]Case 4: Completely different factor levelsIf neither factor B (\\(\\neq '\\), \\(j \\neq j'\\)), :\\[\ncov(Y_{ijk}, Y_{'j'k'}) = 0\n\\]since random effects independent across different factor levels.Summary Variance-Covariance Structure","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-two-way-mixed-effects-anova","chapter":"24 Analysis of Variance","heading":"24.1.5 Two-Way Mixed Effects ANOVA","text":"Two-Way Mixed Effects Model, one factor fixed, random.\noften referred mixed effects model simply mixed model.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"balanced","chapter":"24 Analysis of Variance","heading":"24.1.5.1 Balanced","text":"balanced design, restricted mixed model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]:\\(\\mu_{..}\\): Overall mean (constant).\\(\\alpha_i\\): Fixed effects Factor , subject constraint \\(\\sum \\alpha_i = 0\\).\\(\\beta_j \\sim N(0, \\sigma^2_\\beta)\\) (random effects Factor B).\\((\\alpha \\beta)_{ij} \\sim N(0, \\frac{-1}{} \\sigma^2_{\\alpha \\beta})\\)\n(interaction effects, constrained \\(\\sum_i (\\alpha \\beta)_{ij} = 0\\) \\(j\\)). variance written proportion convenience, makes expected mean squares simpler.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) (random error).\\(\\beta_j, (\\alpha \\beta)_{ij}, \\epsilon_{ijk}\\) pairwise independent.restriction interaction variance (\\(\\frac{-1}{} \\sigma^2_{\\alpha \\beta}\\)) simplifies expected mean squares, though sources assume \\(var((\\alpha \\beta)_{ij}) = \\sigma^2_{\\alpha \\beta}\\).unrestricted version model removes constraints interaction terms.Define:\\[\n\\begin{aligned}\n\\beta_j &= \\beta_j^* + (\\bar{\\alpha \\beta})_{ij}^* \\\\\n(\\alpha \\beta)_{ij} &= (\\alpha \\beta)_{ij}^* - (\\bar{\\alpha \\beta})_{ij}^*\n\\end{aligned}\n\\]\\(\\beta^*\\) \\((\\alpha \\beta)^*_{ij}\\) unrestricted random effects.consider restricted model general, use restricted form simplicity.Taking expectations:\\[\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i\n\\]total variance responses:\\[\nvar(Y_{ijk}) = \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\]Covariance StructureObservations sharing random factor (B) level correlated.Covariances Different CasesThus, observations become independent share random effect.advantage restricted mixed model 2 observations random factor (B) level can positively negatively correlated. unrestricted model, can positively correlated.Comparison Fixed, Random, Mixed Effects ModelsWhile SS df identical across models, expected mean squares differ, affecting test statistics.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"hypothesis-testing-in-mixed-anova","chapter":"24 Analysis of Variance","heading":"24.1.5.1.1 Hypothesis Testing in Mixed ANOVA","text":"random ANOVA, test:\\[\n\\begin{aligned}\nH_0: \\sigma^2 = 0 \\quad vs. \\quad H_a: \\sigma^2 > 0\n\\end{aligned}\n\\]using:\\[\nF = \\frac{MSA}{MSAB} \\sim F_{-1, (-1)(b-1)}\n\\]mixed models, test statistic used :\\[\nH_0: \\alpha_i = 0, \\quad \\forall \n\\]However, fixed effects models, test statistic differs.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"variance-component-estimation","chapter":"24 Analysis of Variance","heading":"24.1.5.1.2 Variance Component Estimation","text":"random mixed effects models, interested estimating variance components.estimate \\(\\sigma^2_\\beta\\):\\[\nE(\\sigma^2_\\beta) = \\frac{E(MSB) - E(MSE)}{na} = \\frac{\\sigma^2 + na \\sigma^2_\\beta - \\sigma^2}{na} = \\sigma^2_\\beta\n\\]estimated :\\[\n\\hat{\\sigma}^2_\\beta = \\frac{MSB - MSE}{na}\n\\]Confidence intervals variance components can approximated using:Satterthwaite procedure.Modified large-sample (MLS) method","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"estimating-fixed-effects-in-mixed-models","chapter":"24 Analysis of Variance","heading":"24.1.5.1.3 Estimating Fixed Effects in Mixed Models","text":"Fixed effects \\(\\alpha_i\\) estimated :\\[\n\\begin{aligned}\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\mu}_{.} &= \\bar{Y}_{...} + (\\bar{Y}_{..} - \\bar{Y}_{...}) = \\bar{Y}_{..}\n\\end{aligned}\n\\]variances:\\[\n\\begin{aligned}\n\\sigma^2(\\hat{\\alpha}_i) &= \\frac{\\sigma^2 + n \\sigma^2_{\\alpha \\beta}}{bn} = \\frac{E(MSAB)}{bn} \\\\\ns^2(\\hat{\\alpha}_i) &= \\frac{MSAB}{bn}\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"contrasts-on-fixed-effects","chapter":"24 Analysis of Variance","heading":"24.1.5.1.4 Contrasts on Fixed Effects","text":"contrast:\\[\nL = \\sum c_i \\alpha_i, \\quad \\text{} \\sum c_i = 0\n\\]Estimate:\\[\n\\hat{L} = \\sum c_i \\hat{\\alpha}_i\n\\]Variance:\\[\n\\sigma^2(\\hat{L}) = \\sum c^2_i \\sigma^2(\\hat{\\alpha}_i), \\quad s^2(\\hat{L}) = \\frac{MSAB}{bn} \\sum c^2_i\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"unbalanced-two-way-mixed-effects-anova","chapter":"24 Analysis of Variance","heading":"24.1.5.2 Unbalanced Two-Way Mixed Effects ANOVA","text":"unbalanced two-way mixed model (e.g., \\(= 2, b = 4\\)), model remains:\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]:\\(\\alpha_i\\): Fixed effects Factor .\\(\\beta_j \\sim N(0, \\sigma^2_\\beta)\\): Random effects Factor B.\\((\\alpha \\beta)_{ij} \\sim N(0, \\frac{\\sigma^2_{\\alpha \\beta}}{2})\\): Interaction effects.\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\): Residual error.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"variance-components","chapter":"24 Analysis of Variance","heading":"24.1.5.2.1 Variance Components","text":"variance components :\\[\n\\begin{aligned}\nvar(\\beta_j) &= \\sigma^2_\\beta \\\\\nvar((\\alpha \\beta)_{ij}) &= \\frac{2-1}{2} \\sigma^2_{\\alpha \\beta} = \\frac{\\sigma^2_{\\alpha \\beta}}{2} \\\\\nvar(\\epsilon_{ijk}) &= \\sigma^2\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"expectation-and-variance","chapter":"24 Analysis of Variance","heading":"24.1.5.2.2 Expectation and Variance","text":"Taking expectations:\\[\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i\n\\]Total variance:\\[\nvar(Y_{ijk}) = \\sigma^2_{\\beta} + \\frac{\\sigma^2_{\\alpha \\beta}}{2} + \\sigma^2\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"covariance-structure-2","chapter":"24 Analysis of Variance","heading":"24.1.5.2.3 Covariance Structure","text":"Observations sharing Factor B (random effect) correlated.Covariances Different CasesThus, observations within random factor level share dependence.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"matrix-representation","chapter":"24 Analysis of Variance","heading":"24.1.5.2.4 Matrix Representation","text":"Assume:\\[\n\\mathbf{Y} \\sim N(\\mathbf{X} \\beta, M)\n\\]:\\(\\mathbf{X}\\): Fixed effects design matrix.\\(\\beta\\): Fixed effect coefficients.\\(M\\): Block diagonal covariance matrix containing variance components.density function \\(\\mathbf{Y}\\) :\\[\nf(\\mathbf{Y}) = \\frac{1}{(2\\pi)^{N/2} |M|^{1/2}} \\exp \\left( -\\frac{1}{2} (\\mathbf{Y} - \\mathbf{X} \\beta)' M^{-1} (\\mathbf{Y} - \\mathbf{X} \\beta) \\right)\n\\]variance components known, use Generalized Least Squares:\\[\n\\hat{\\beta}_{GLS} = (\\mathbf{X}' M^{-1} \\mathbf{X})^{-1} \\mathbf{X}' M^{-1} \\mathbf{Y}\n\\]However, since variance components (\\(\\sigma^2, \\sigma^2_\\beta, \\sigma^2_{\\alpha \\beta}\\)) unknown, estimate using:Maximum LikelihoodRestricted Maximum LikelihoodMaximizing likelihood:\\[\n\\ln L = - \\frac{N}{2} \\ln (2\\pi) - \\frac{1}{2} \\ln |M| - \\frac{1}{2} (\\mathbf{Y} - \\mathbf{X} \\beta)' M^{-1} (\\mathbf{Y} - \\mathbf{X} \\beta)\n\\]:\\(|M|\\): Determinant variance-covariance matrix.\\((\\mathbf{Y} - \\mathbf{X} \\beta)' M^{-1} (\\mathbf{Y} - \\mathbf{X} \\beta)\\): Quadratic form likelihood.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-nonparametric-anova","chapter":"24 Analysis of Variance","heading":"24.2 Nonparametric ANOVA","text":"assumptions normality equal variance satisfied, use nonparametric ANOVA tests, rank data instead using raw values.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"kruskal-wallis-test-one-way-nonparametric-anova","chapter":"24 Analysis of Variance","heading":"24.2.1 Kruskal-Wallis Test (One-Way Nonparametric ANOVA)","text":"Kruskal-Wallis test generalization Wilcoxon rank-sum test two independent samples. alternative one-way ANOVA normality assumed.Setup\\(\\geq 2\\) independent treatments.\\(n_i\\) sample size \\(\\)-th treatment.\\(Y_{ij}\\) \\(j\\)-th observation \\(\\)-th treatment.assumption normality.Assume observations independent random samples continuous CDFs \\(F_1, F_2, \\dots, F_a\\).Hypotheses\\[\n\\begin{aligned}\n&H_0: F_1 = F_2 = \\dots = F_a \\quad \\text{(distributions identical)} \\\\\n&H_a: F_i < F_j \\text{ } \\neq j\n\\end{aligned}\n\\] data come location-scale family, hypothesis simplifies :\\[\nH_0: \\theta_1 = \\theta_2 = \\dots = \\theta_a\n\\]ProcedureRank \\(N = \\sum_{=1}^n_i\\) observations ascending order.\nLet \\(r_{ij} = rank(Y_{ij})\\)\nsum ranks must satisfy:\n\\[\n\\sum_i \\sum_j r_{ij} = \\frac{N(N+1)}{2}\n\\]Rank \\(N = \\sum_{=1}^n_i\\) observations ascending order.\nLet \\(r_{ij} = rank(Y_{ij})\\)\nsum ranks must satisfy:\\[\n\\sum_i \\sum_j r_{ij} = \\frac{N(N+1)}{2}\n\\]Compute rank sums averages: \\[\nr_{.} = \\sum_{j=1}^{n_i} r_{ij}, \\quad \\bar{r}_{.} = \\frac{r_{.}}{n_i}\n\\]Compute rank sums averages: \\[\nr_{.} = \\sum_{j=1}^{n_i} r_{ij}, \\quad \\bar{r}_{.} = \\frac{r_{.}}{n_i}\n\\]Calculate test statistic:\n\\[\n\\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}}\n\\]\n:\nTreatment Sum Squares: \\[\nSSTR = \\sum n_i (\\bar{r}_{.} - \\bar{r}_{..})^2\n\\]\nTotal Sum Squares: \\[\nSSTO = \\sum_i \\sum_j (r_{ij} - \\bar{r}_{..})^2\n\\]\nOverall Mean Rank: \\[\n\\bar{r}_{..} = \\frac{N+1}{2}\n\\]\nCalculate test statistic:\\[\n\\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}}\n\\]:Treatment Sum Squares: \\[\nSSTR = \\sum n_i (\\bar{r}_{.} - \\bar{r}_{..})^2\n\\]Total Sum Squares: \\[\nSSTO = \\sum_i \\sum_j (r_{ij} - \\bar{r}_{..})^2\n\\]Overall Mean Rank: \\[\n\\bar{r}_{..} = \\frac{N+1}{2}\n\\]Compare chi-square distribution:\nlarge \\(n_i\\) (\\(\\geq 5\\)), \\(\\chi^2_{KW} \\sim \\chi^2_{-1}\\).\nReject \\(H_0\\) : \\[\n\\chi^2_{KW} > \\chi^2_{(1-\\alpha; -1)}\n\\]\nCompare chi-square distribution:large \\(n_i\\) (\\(\\geq 5\\)), \\(\\chi^2_{KW} \\sim \\chi^2_{-1}\\).Reject \\(H_0\\) : \\[\n\\chi^2_{KW} > \\chi^2_{(1-\\alpha; -1)}\n\\]Exact Test Small Samples:\nCompute possible rank assignments:\\[\n\\frac{N!}{n_1! n_2! \\dots n_a!}\n\\]\nEvaluate Kruskal-Wallis statistic determine empirical p-value.\nExact Test Small Samples:Compute possible rank assignments:\\[\n\\frac{N!}{n_1! n_2! \\dots n_a!}\n\\]Evaluate Kruskal-Wallis statistic determine empirical p-value.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"friedman-test-nonparametric-two-way-anova","chapter":"24 Analysis of Variance","heading":"24.2.2 Friedman Test (Nonparametric Two-Way ANOVA)","text":"Friedman test distribution-free alternative two-way ANOVA data measured randomized complete block design normality assumed.Setup\\(Y_{ij}\\) represents responses \\(n\\) blocks \\(r\\) treatments.Assume normality homogeneity variance.Let \\(F_{ij}\\) CDF \\(Y_{ij}\\), corresponding observed values.Hypotheses\\[\n\\begin{aligned}\n&H_0: F_{i1} = F_{i2} = \\dots = F_{ir} \\quad \\forall \\quad \\text{(Identical distributions within block)} \\\\\n&H_a: F_{ij} < F_{ij'} \\text{ } j \\neq j' \\quad \\forall \n\\end{aligned}\n\\]location-scale families, hypothesis simplifies :\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = \\dots = \\tau_r \\\\\n&H_a: \\tau_j > \\tau_{j'} \\text{ } j \\neq j'\n\\end{aligned}\n\\]ProcedureRank observations within block separately (ascending order).\nties, assign average ranks.\nRank observations within block separately (ascending order).ties, assign average ranks.Compute test statistic:\n\\[\n\\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}}\n\\]\n:\nTreatment Sum Squares: \\[\nSSTR = n \\sum (\\bar{r}_{.j} - \\bar{r}_{..})^2\n\\]\nError Sum Squares: \\[\nSSE = \\sum_i \\sum_j (r_{ij} - \\bar{r}_{.j})^2\n\\]\nMean Ranks: \\[\n\\bar{r}_{.j} = \\frac{\\sum_i r_{ij}}{n}, \\quad \\bar{r}_{..} = \\frac{r+1}{2}\n\\]\nCompute test statistic:\\[\n\\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}}\n\\]:Treatment Sum Squares: \\[\nSSTR = n \\sum (\\bar{r}_{.j} - \\bar{r}_{..})^2\n\\]Error Sum Squares: \\[\nSSE = \\sum_i \\sum_j (r_{ij} - \\bar{r}_{.j})^2\n\\]Mean Ranks: \\[\n\\bar{r}_{.j} = \\frac{\\sum_i r_{ij}}{n}, \\quad \\bar{r}_{..} = \\frac{r+1}{2}\n\\]Alternative Formula Large Samples (Ties):\nties, Friedman’s statistic simplifies :\n\\[\n\\chi^2_F = \\left[\\frac{12}{nr(n+1)} \\sum_j r_{.j}^2\\right] - 3n(r+1)\n\\]Alternative Formula Large Samples (Ties):ties, Friedman’s statistic simplifies :\\[\n\\chi^2_F = \\left[\\frac{12}{nr(n+1)} \\sum_j r_{.j}^2\\right] - 3n(r+1)\n\\]Compare chi-square distribution:\nlarge \\(n\\), \\(\\chi^2_F \\sim \\chi^2_{r-1}\\).\nReject \\(H_0\\) : \\[\n\\chi^2_F > \\chi^2_{(1-\\alpha; r-1)}\n\\]\nCompare chi-square distribution:large \\(n\\), \\(\\chi^2_F \\sim \\chi^2_{r-1}\\).Reject \\(H_0\\) : \\[\n\\chi^2_F > \\chi^2_{(1-\\alpha; r-1)}\n\\]Exact Test Small Samples:\nCompute possible ranking permutations: \\[\n(r!)^n\n\\]\nEvaluate Friedman statistic determine empirical p-value.\nExact Test Small Samples:Compute possible ranking permutations: \\[\n(r!)^n\n\\]Evaluate Friedman statistic determine empirical p-value.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-randomized-block-designs","chapter":"24 Analysis of Variance","heading":"24.3 Randomized Block Designs","text":"improve precision treatment comparisons, can reduce variability among experimental units grouping blocks.\nblock contains homogeneous units, reducing impact nuisance variation.Key Principles BlockingWithin block, treatments randomly assigned units.number units per block multiple number factor combinations.Commonly, treatment appears per block.Benefits BlockingReduction variability treatment effect estimates\nImproved power t-tests F-tests.\nNarrower confidence intervals.\nSmaller mean square error (MSE).\nReduction variability treatment effect estimatesImproved power t-tests F-tests.Improved power t-tests F-tests.Narrower confidence intervals.Narrower confidence intervals.Smaller mean square error (MSE).Smaller mean square error (MSE).Allows comparison treatments across different conditions (captured blocks).Allows comparison treatments across different conditions (captured blocks).Potential Downsides BlockingIf blocks chosen well, degrees freedom wasted negligible block effects.blocks chosen well, degrees freedom wasted negligible block effects.reduces df t-tests F-tests without reducing MSE, causing small loss power.reduces df t-tests F-tests without reducing MSE, causing small loss power.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-random-block-effects-with-additive-effects","chapter":"24 Analysis of Variance","heading":"24.3.0.1 Random Block Effects with Additive Effects","text":"statistical model randomized block design:\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]:\\(= 1, 2, \\dots, n\\) (Blocks)\\(= 1, 2, \\dots, n\\) (Blocks)\\(j = 1, 2, \\dots, r\\) (Treatments)\\(j = 1, 2, \\dots, r\\) (Treatments)\\(\\mu_{..}\\): Overall mean response (averaged across blocks treatments).\\(\\mu_{..}\\): Overall mean response (averaged across blocks treatments).\\(\\rho_i\\): Block effect (average difference \\(\\)-th block), constrained :\n\\[\n\\sum_i \\rho_i = 0\n\\]\\(\\rho_i\\): Block effect (average difference \\(\\)-th block), constrained :\\[\n\\sum_i \\rho_i = 0\n\\]\\(\\tau_j\\): Treatment effect (average across blocks), constrained :\n\\[\n\\sum_j \\tau_j = 0\n\\]\\(\\tau_j\\): Treatment effect (average across blocks), constrained :\\[\n\\sum_j \\tau_j = 0\n\\]\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\): Random experimental error.\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\): Random experimental error.Interpretation ModelBlock treatment effects additive.Block treatment effects additive.difference average response two treatments within block:\n\\[\n(\\mu_{..} + \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j') = \\tau_j - \\tau_j'\n\\]difference average response two treatments within block:\\[\n(\\mu_{..} + \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j') = \\tau_j - \\tau_j'\n\\]ensures blocking affects variability, treatment comparisons.ensures blocking affects variability, treatment comparisons.Estimators Model ParametersOverall Mean:\n\\[\n\\hat{\\mu} = \\bar{Y}_{..}\n\\]Overall Mean:\\[\n\\hat{\\mu} = \\bar{Y}_{..}\n\\]Block Effects:\n\\[\n\\hat{\\rho}_i = \\bar{Y}_{.} - \\bar{Y}_{..}\n\\]Block Effects:\\[\n\\hat{\\rho}_i = \\bar{Y}_{.} - \\bar{Y}_{..}\n\\]Treatment Effects:\n\\[\n\\hat{\\tau}_j = \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\]Treatment Effects:\\[\n\\hat{\\tau}_j = \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\]Fitted Response: \\[\n\\hat{Y}_{ij} = \\bar{Y}_{..} + (\\bar{Y}_{.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j} - \\bar{Y}_{..})\n\\]\nSimplifies :\n\\[\n\\hat{Y}_{ij} = \\bar{Y}_{.} + \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\]Fitted Response: \\[\n\\hat{Y}_{ij} = \\bar{Y}_{..} + (\\bar{Y}_{.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j} - \\bar{Y}_{..})\n\\]Simplifies :\\[\n\\hat{Y}_{ij} = \\bar{Y}_{.} + \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\]Residuals:\n\\[\ne_{ij} = Y_{ij} - \\hat{Y}_{ij} = Y_{ij} - \\bar{Y}_{.} - \\bar{Y}_{.j} + \\bar{Y}_{..}\n\\]Residuals:\\[\ne_{ij} = Y_{ij} - \\hat{Y}_{ij} = Y_{ij} - \\bar{Y}_{.} - \\bar{Y}_{.j} + \\bar{Y}_{..}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"anova-table-for-randomized-block-design","chapter":"24 Analysis of Variance","heading":"24.3.0.2 ANOVA Table for Randomized Block Design","text":"ANOVA decomposition partitions total variability contributions blocks, treatments, residual error.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"f-tests-in-randomized-block-designs","chapter":"24 Analysis of Variance","heading":"24.3.0.3 F-tests in Randomized Block Designs","text":"test treatment effects, use F-test:fixed treatment effects:\\[\n\\begin{aligned}\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_r = 0 \\quad \\text{(treatment effect)} \\\\\nH_a: \\text{} \\tau_j = 0\n\\end{aligned}\n\\]random treatment effects:\\[\n\\begin{aligned}\nH_0: \\sigma^2_{\\tau} = 0 \\quad \\text{(variance treatment effects)} \\\\\nH_a: \\sigma^2_{\\tau} \\neq 0\n\\end{aligned}\n\\]cases, test statistic :\\[\nF = \\frac{MSTR}{MSE}\n\\]Reject \\(H_0\\) :\\[\nF > f_{(1-\\alpha; r-1, (n-1)(r-1))}\n\\]Use F-Test Blocks?test block effects :Blocks assumed different priori.Randomization occurs within block, ensuring treatments comparable.Efficiency Gain BlockingTo measure improvement precision, compare mean square error (MSE) completely randomized design vs. randomized block design.Estimated variance CRD:\\[\n\\hat{\\sigma}^2_{CR} = \\frac{(n-1)MSBL + n(r-1)MSE}{nr-1}\n\\]Estimated variance RBD:\\[\n\\hat{\\sigma}^2_{RB} = MSE\n\\]Relative efficiency:\\[\n\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}}\n\\]greater 1, blocking reduces experimental error.greater 1, blocking reduces experimental error.percentage reduction required sample size RBD:percentage reduction required sample size RBD:\\[\n\\left( \\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}} - 1 \\right) \\times 100\\%\n\\]Random Blocks Mixed ModelsIf blocks randomly selected, treated random effects.\n, experiment repeated, new set blocks selected, :\\[\n\\rho_1, \\rho_2, \\dots, \\rho_i \\sim N(0, \\sigma^2_\\rho)\n\\]model remains:\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]:\\(\\mu_{..}\\) fixed.\\(\\rho_i \\sim iid N(0, \\sigma^2_\\rho)\\) (random block effects).\\(\\tau_j\\) fixed (random, \\(\\sum \\tau_j = 0\\)).\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"variance-and-covariance-structure","chapter":"24 Analysis of Variance","heading":"24.3.0.4 Variance and Covariance Structure","text":"fixed treatment effects:\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) &= \\sigma^2_{\\rho} + \\sigma^2\n\\end{aligned}\n\\]Observations within block correlated:\\[\ncov(Y_{ij}, Y_{ij'}) = \\sigma^2_{\\rho}, \\quad j \\neq j'\n\\]Observations different blocks independent:\\[\ncov(Y_{ij}, Y_{'j'}) = 0, \\quad \\neq ', j \\neq j'\n\\]intra-block correlation:\\[\n\\frac{\\sigma^2_{\\rho}}{\\sigma^2 + \\sigma^2_{\\rho}}\n\\]Expected Mean Squares Fixed Treatments","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sec-random-block-effects-with-interaction","chapter":"24 Analysis of Variance","heading":"24.3.0.5 Random Block Effects with Interaction","text":"block-treatment interaction exists, modify model:\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + (\\rho \\tau)_{ij} + \\epsilon_{ij}\n\\]:\\(\\rho_i \\sim iid N(0, \\sigma^2_{\\rho})\\) (random).\\(\\rho_i \\sim iid N(0, \\sigma^2_{\\rho})\\) (random).\\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\)).\\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\)).\\((\\rho \\tau)_{ij} \\sim N(0, \\frac{r-1}{r} \\sigma^2_{\\rho \\tau})\\), constrained :\n\\[\n\\sum_j (\\rho \\tau)_{ij} = 0, \\quad \\forall \n\\]\\((\\rho \\tau)_{ij} \\sim N(0, \\frac{r-1}{r} \\sigma^2_{\\rho \\tau})\\), constrained :\\[\n\\sum_j (\\rho \\tau)_{ij} = 0, \\quad \\forall \n\\]Covariance interaction terms:\n\\[\ncov((\\rho \\tau)_{ij}, (\\rho \\tau)_{ij'}) = -\\frac{1}{r} \\sigma^2_{\\rho \\tau}, \\quad j \\neq j'\n\\]Covariance interaction terms:\\[\ncov((\\rho \\tau)_{ij}, (\\rho \\tau)_{ij'}) = -\\frac{1}{r} \\sigma^2_{\\rho \\tau}, \\quad j \\neq j'\n\\]\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\).\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\).Variance Covariance InteractionExpectation:\n\\[\nE(Y_{ij}) = \\mu_{..} + \\tau_j\n\\]Expectation:\\[\nE(Y_{ij}) = \\mu_{..} + \\tau_j\n\\]Total variance:\n\\[\nvar(Y_{ij}) = \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2\n\\]Total variance:\\[\nvar(Y_{ij}) = \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2\n\\]Within-block covariance:\n\\[\ncov(Y_{ij}, Y_{ij'}) = \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, \\quad j \\neq j'\n\\]Within-block covariance:\\[\ncov(Y_{ij}, Y_{ij'}) = \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, \\quad j \\neq j'\n\\]-block covariance:\n\\[\ncov(Y_{ij}, Y_{'j'}) = 0, \\quad \\neq ', j \\neq j'\n\\]-block covariance:\\[\ncov(Y_{ij}, Y_{'j'}) = 0, \\quad \\neq ', j \\neq j'\n\\]sum squares degrees freedom interaction model additive model. difference exists expected mean squares.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"anova-table-with-interaction-effects","chapter":"24 Analysis of Variance","heading":"24.3.0.6 ANOVA Table with Interaction Effects","text":"exact test possible block effects interaction present (important blocks used primarily reduce experimental error variability)\\(E(MSE) = \\sigma^2 + \\sigma^2_{\\rho \\tau}\\) error term variance interaction variance \\(\\sigma^2_{\\rho \\tau}\\). can’t estimate components separately model. two confounded.one observation per treatment block combination, one can consider interaction fixed block effects, called generalized randomized block designs (multifactor analysis).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"tukey-test-of-additivity","chapter":"24 Analysis of Variance","heading":"24.3.0.7 Tukey Test of Additivity","text":"Tukey’s 1-degree--freedom test additivity provides formal test interaction effects blocks treatments randomized block design.test can also used two-way ANOVA one observation per cell.randomized block design, additive model assumes:\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]:\\(\\mu_{..}\\) = overall mean\\(\\mu_{..}\\) = overall mean\\(\\rho_i\\) = block effect\\(\\rho_i\\) = block effect\\(\\tau_j\\) = treatment effect\\(\\tau_j\\) = treatment effect\\(\\epsilon_{ij}\\) = random error, \\(iid N(0, \\sigma^2)\\)\\(\\epsilon_{ij}\\) = random error, \\(iid N(0, \\sigma^2)\\)test interaction, introduce less restricted interaction term:\\[\n(\\rho \\tau)_{ij} = D \\rho_i \\tau_j\n\\]\\(D\\) constant measuring interaction strength.Thus, interaction model becomes:\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + D\\rho_i \\tau_j + \\epsilon_{ij}\n\\]least squares estimate (MLE) \\(D\\) :\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j \\rho_i \\tau_j Y_{ij}}{\\sum_i \\rho_i^2 \\sum_j \\tau_j^2}\n\\]Replacing \\(\\rho_i\\) \\(\\tau_j\\) estimates:\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j (\\bar{Y}_{.} - \\bar{Y}_{..})(\\bar{Y}_{.j} - \\bar{Y}_{..}) Y_{ij}}{\\sum_i (\\bar{Y}_{.} - \\bar{Y}_{..})^2 \\sum_j (\\bar{Y}_{.j} - \\bar{Y}_{..})^2}\n\\]sum squares interaction :\\[\nSS_{int} = \\sum_i \\sum_j \\hat{D}^2 (\\bar{Y}_{.} - \\bar{Y}_{..})^2 (\\bar{Y}_{.j} - \\bar{Y}_{..})^2\n\\]ANOVA DecompositionThe total sum squares (SSTO) decomposed :\\[\nSSTO = SSBL + SSTR + SS_{int} + SS_{Rem}\n\\]:\\(SSBL\\) = Sum squares due blocks\\(SSBL\\) = Sum squares due blocks\\(SSTR\\) = Sum squares due treatments\\(SSTR\\) = Sum squares due treatments\\(SS_{int}\\) = Interaction sum squares\\(SS_{int}\\) = Interaction sum squares\\(SS_{Rem}\\) = Remainder sum squares, computed :\\(SS_{Rem}\\) = Remainder sum squares, computed :\\[\nSS_{Rem} = SSTO - SSBL - SSTR - SS_{int}\n\\]test:\\[\n\\begin{aligned}\n&H_0: D = 0 \\quad \\text{(interaction present)} \\\\\n&H_a: D \\neq 0 \\quad \\text{(Interaction form $D \\rho_i \\tau_j$ present)}\n\\end{aligned}\n\\]\\(D = 0\\), \\(SS_{int}\\) \\(SS_{Rem}\\) independent follow:\\[\nSS_{int} \\sim \\chi^2_1, \\quad SS_{Rem} \\sim \\chi^2_{(rn-r-n)}\n\\]Thus, F-statistic testing interaction :\\[\nF = \\frac{SS_{int} / 1}{SS_{Rem} / (rn - r - n)}\n\\]follows \\(F\\)-distribution:\\[\nF \\sim F_{(1, nr - r - n)}\n\\]reject \\(H_0\\) :\\[\nF > f_{(1-\\alpha; 1, nr - r - n)}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"nested-designs","chapter":"24 Analysis of Variance","heading":"24.4 Nested Designs","text":"nested design occurs one factor entirely contained within another. differs crossed design, levels one factor present across levels another factor.Crossed Design: Factor B crossed Factor , level Factor B appears every level Factor .Nested Design: Factor B nested within Factor , level Factor B unique particular level Factor .Thus, Factor B nested within Factor :Level 1 B within = 1 nothing common withLevel 1 B within = 1 nothing common withLevel 1 B within = 2.Level 1 B within = 2.Types FactorsClassification Factors: Factors manipulated (e.g., geographical regions, subjects).Experimental Factors: Factors randomly assigned experiment.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"two-factor-nested-design","chapter":"24 Analysis of Variance","heading":"24.4.1 Two-Factor Nested Design","text":"consider nested two-factor model :Factor \\(\\) levels.Factor \\(\\) levels.Factor B nested within Factor , \\(b\\) levels per level .Factor B nested within Factor , \\(b\\) levels per level .factors fixed.factors fixed.treatment means equally important.treatment means equally important.mean response level \\(\\) Factor :\\[\n\\mu_{.} = \\frac{1}{b} \\sum_j \\mu_{ij}\n\\]main effect Factor :\\[\n\\alpha_i = \\mu_{.} - \\mu_{..}\n\\]:\\[\n\\mu_{..} = \\frac{1}{ab} \\sum_i \\sum_j \\mu_{ij} = \\frac{1}{} \\sum_i \\mu_{.}\n\\]constraint:\\[\n\\sum_i \\alpha_i = 0\n\\]nested effect Factor B within denoted \\(\\beta_{j()}\\), :\\[\n\\begin{aligned}\n\\beta_{j()} &= \\mu_{ij} - \\mu_{.} \\\\\n&= \\mu_{ij} - \\alpha_i - \\mu_{..}\n\\end{aligned}\n\\]restriction:\\[\n\\sum_j \\beta_{j()} = 0, \\quad \\forall = 1, \\dots, \n\\]Since \\(\\beta_{j()}\\) specific effect \\(j\\)-th level factor \\(B\\) nested within \\(\\)-th level factor \\(\\), full model can written :\\[\n\\mu_{ij} = \\mu_{..} + \\alpha_i + \\beta_{j()}\n\\]equivalently:\\[\n\\mu_{ij} = \\mu_{..} + (\\mu_{.} - \\mu_{..}) + (\\mu_{ij} - \\mu_{.})\n\\]statistical model two-factor nested design :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk}\n\\]:\\(Y_{ijk}\\) = response \\(k\\)-th observation :\nFactor level \\(\\).\nFactor B (nested within ) level \\(j\\).\n\\(Y_{ijk}\\) = response \\(k\\)-th observation :Factor level \\(\\).Factor level \\(\\).Factor B (nested within ) level \\(j\\).Factor B (nested within ) level \\(j\\).\\(\\mu_{..}\\) = overall mean.\\(\\mu_{..}\\) = overall mean.\\(\\alpha_i\\) = main effect Factor (subject : \\(\\sum_i \\alpha_i = 0\\)).\\(\\alpha_i\\) = main effect Factor (subject : \\(\\sum_i \\alpha_i = 0\\)).\\(\\beta_{j()}\\) = nested effect Factor B within (subject : \\(\\sum_j \\beta_{j()} = 0\\) \\(\\)).\\(\\beta_{j()}\\) = nested effect Factor B within (subject : \\(\\sum_j \\beta_{j()} = 0\\) \\(\\)).\\(\\epsilon_{ijk} \\sim iid N(0, \\sigma^2)\\) = random error.\\(\\epsilon_{ijk} \\sim iid N(0, \\sigma^2)\\) = random error.Thus, expected value variance :\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_{j()} \\\\\nvar(Y_{ijk}) &= \\sigma^2\n\\end{aligned}\n\\]Note: interaction term nested model, Factor B levels unique within level .least squares maximum likelihood estimates:residual error:\\[\ne_{ijk} = Y_{ijk} - \\bar{Y}_{ij.}\n\\]total sum squares (SSTO) partitioned :\\[\nSSTO = SSA + SSB() + SSE\n\\]:\\[\n\\begin{aligned}\n\\sum_i \\sum_j \\sum_k (Y_{ijk} - \\bar{Y}_{...})^2\n&= bn \\sum_i (\\bar{Y}_{..} - \\bar{Y}_{...})^2\n+ n \\sum_i \\sum_j (\\bar{Y}_{ij.} - \\bar{Y}_{..})^2 \\\\\n&+ \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\bar{Y}_{ij.})^2\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"anova-table-for-nested-designs","chapter":"24 Analysis of Variance","heading":"24.4.1.1 ANOVA Table for Nested Designs","text":"","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"tests-for-factor-effects","chapter":"24 Analysis of Variance","heading":"24.4.1.2 Tests For Factor Effects","text":"Factor :\n\\[\nF = \\frac{MSA}{MSB()} \\sim F_{(-1, (b-1))}\n\\]\nReject \\(H_0\\) \\(F > f_{(1-\\alpha; -1, (b-1))}\\).Factor :\\[\nF = \\frac{MSA}{MSB()} \\sim F_{(-1, (b-1))}\n\\]Reject \\(H_0\\) \\(F > f_{(1-\\alpha; -1, (b-1))}\\).Factor B within :\n\\[\nF = \\frac{MSB()}{MSE} \\sim F_{((b-1), ab(n-1))}\n\\]\nReject \\(H_0\\) \\(F > f_{(1-\\alpha; (b-1), ab(n-1))}\\).Factor B within :\\[\nF = \\frac{MSB()}{MSE} \\sim F_{((b-1), ab(n-1))}\n\\]Reject \\(H_0\\) \\(F > f_{(1-\\alpha; (b-1), ab(n-1))}\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"testing-factor-effect-contrasts","chapter":"24 Analysis of Variance","heading":"24.4.1.3 Testing Factor Effect Contrasts","text":"contrast linear combination factor level means:\\[\nL = \\sum c_i \\mu_i, \\quad \\text{} \\quad \\sum c_i = 0\n\\]estimated contrast:\\[\n\\hat{L} = \\sum c_i \\bar{Y}_{..}\n\\]confidence interval \\(L\\):\\[\n\\hat{L} \\pm t_{(1-\\alpha/2; df)} s(\\hat{L})\n\\]:\\[\ns^2(\\hat{L}) = \\sum c_i^2 s^2(\\bar{Y}_{..}), \\quad \\text{} \\quad s^2(\\bar{Y}_{..}) = \\frac{MSE}{bn}, \\quad df = ab(n-1)\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"testing-treatment-means","chapter":"24 Analysis of Variance","heading":"24.4.1.4 Testing Treatment Means","text":"treatment means, similar approach applies:\\[\nL = \\sum c_i \\mu_{.j}, \\quad \\hat{L} = \\sum c_i \\bar{Y}_{ij}\n\\]confidence limits \\(L\\):\\[\n\\hat{L} \\pm t_{(1-\\alpha/2; (n-1)ab)} s(\\hat{L})\n\\]:\\[\ns^2(\\hat{L}) = \\frac{MSE}{n} \\sum c_i^2\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"unbalanced-nested-two-factor-designs","chapter":"24 Analysis of Variance","heading":"24.4.2 Unbalanced Nested Two-Factor Designs","text":"Factor B different levels different levels Factor , design unbalanced.\\[\n\\begin{aligned}\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk} \\\\\n\\sum_{=1}^2 \\alpha_i &= 0, \\quad\n\\sum_{j=1}^3 \\beta_{j(1)} = 0, \\quad\n\\sum_{j=1}^2 \\beta_{j(2)} = 0\n\\end{aligned}\n\\]:Factor : \\(= 1, 2\\).Factor : \\(= 1, 2\\).Factor B (nested ): \\(j = 1, \\dots, b_i\\).Factor B (nested ): \\(j = 1, \\dots, b_i\\).Observations: \\(k = 1, \\dots, n_{ij}\\).Observations: \\(k = 1, \\dots, n_{ij}\\).Example case:\\(b_1 = 3, b_2 = 2\\) (Factor B different levels ).\\(b_1 = 3, b_2 = 2\\) (Factor B different levels ).\\(n_{11} = n_{13} = 2, n_{12} = 1, n_{21} = n_{22} = 2\\).\\(n_{11} = n_{13} = 2, n_{12} = 1, n_{21} = n_{22} = 2\\).Parameters: \\(\\alpha_1, \\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\).Parameters: \\(\\alpha_1, \\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\).Constraints:\\[\n\\alpha_2 = -\\alpha_1, \\quad\n\\beta_{3(1)} = -\\beta_{1(1)} - \\beta_{2(1)}, \\quad\n\\beta_{2(2)} = -\\beta_{1(2)}\n\\]unbalanced design can modeled using indicator variables:Factor (School Level): \\[\nX_1 =\n\\begin{cases}\n1 & \\text{observation school 1} \\\\\n-1 & \\text{observation school 2}\n\\end{cases}\n\\]Factor (School Level): \\[\nX_1 =\n\\begin{cases}\n1 & \\text{observation school 1} \\\\\n-1 & \\text{observation school 2}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_2 =\n\\begin{cases}\n1 & \\text{observation instructor 1 school 1} \\\\\n-1 & \\text{observation instructor 3 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_2 =\n\\begin{cases}\n1 & \\text{observation instructor 1 school 1} \\\\\n-1 & \\text{observation instructor 3 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_3 =\n\\begin{cases}\n1 & \\text{observation instructor 2 school 1} \\\\\n-1 & \\text{observation instructor 3 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_3 =\n\\begin{cases}\n1 & \\text{observation instructor 2 school 1} \\\\\n-1 & \\text{observation instructor 3 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_4 =\n\\begin{cases}\n1 & \\text{observation instructor 1 school 1} \\\\\n-1 & \\text{observation instructor 2 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Factor B (Instructor within School 1): \\[\nX_4 =\n\\begin{cases}\n1 & \\text{observation instructor 1 school 1} \\\\\n-1 & \\text{observation instructor 2 school 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Using indicator variables, full regression model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_1 X_{ijk1} + \\beta_{1(1)} X_{ijk2} + \\beta_{2(1)} X_{ijk3} + \\beta_{1(2)} X_{ijk4} + \\epsilon_{ijk}\n\\]\\(X_1, X_2, X_3, X_4\\) represent different factor effects.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"random-factor-effects","chapter":"24 Analysis of Variance","heading":"24.4.3 Random Factor Effects","text":"factors random:\\[\n\\begin{aligned}\n\\alpha_1 &\\sim iid N(0, \\sigma^2_\\alpha) \\\\\n\\beta_{j()} &\\sim iid N(0, \\sigma^2_\\beta)\n\\end{aligned}\n\\]Expected Mean Squares Random EffectsF-Tests Factor EffectsAnother way increase precision treatment comparisons adjusting covariates using regression models. called Analysis Covariance (ANCOVA).use ANCOVA?Reduces variability accounting covariate effects.Reduces variability accounting covariate effects.Increases statistical power removing nuisance variation.Increases statistical power removing nuisance variation.Combines ANOVA regression precise comparisons.Combines ANOVA regression precise comparisons.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"sample-size-planning-for-anova","chapter":"24 Analysis of Variance","heading":"24.5 Sample Size Planning for ANOVA","text":"","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"balanced-designs","chapter":"24 Analysis of Variance","heading":"24.5.1 Balanced Designs","text":"Choosing appropriate sample size ANOVA study requires ensuring sufficient power balancing practical constraints.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"single-factor-studies","chapter":"24 Analysis of Variance","heading":"24.5.2 Single Factor Studies","text":"","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"fixed-cell-means-model","chapter":"24 Analysis of Variance","heading":"24.5.2.1 Fixed Cell Means Model","text":"probability rejecting \\(H_0\\) false (power) given :\\[\nP(F > f_{(1-\\alpha; -1, N-)} | \\phi) = 1 - \\beta\n\\]:\\(\\phi\\) non-centrality parameter (measuring inequality among treatment means \\(\\mu_i\\)):\n\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{} \\sum_{} (\\mu_i - \\mu_.)^2}, \\quad (n_i \\equiv n)\n\\]\\(\\phi\\) non-centrality parameter (measuring inequality among treatment means \\(\\mu_i\\)):\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{} \\sum_{} (\\mu_i - \\mu_.)^2}, \\quad (n_i \\equiv n)\n\\]\\(\\mu_.\\) overall mean:\n\\[\n\\mu_. = \\frac{\\sum \\mu_i}{}\n\\]\\(\\mu_.\\) overall mean:\\[\n\\mu_. = \\frac{\\sum \\mu_i}{}\n\\]determine power, use non-central F distribution.Using Power TablesPower tables can used directly :effects fixed.effects fixed.design balanced.design balanced.minimum range factor level means \\(\\Delta\\) known:\n\\[\n\\Delta = \\max(\\mu_i) - \\min(\\mu_i)\n\\]minimum range factor level means \\(\\Delta\\) known:\\[\n\\Delta = \\max(\\mu_i) - \\min(\\mu_i)\n\\]Thus, required inputs :Significance level (\\(\\alpha\\))Minimum range means (\\(\\Delta\\))Error standard deviation (\\(\\sigma\\))Power (\\(1 - \\beta\\))Notes Sample Size SensitivityWhen \\(\\Delta/\\sigma\\) small, sample size requirements increase dramatically.Lowering \\(\\alpha\\) \\(\\beta\\) increases required sample sizes.Errors estimating \\(\\sigma\\) can significantly impact sample size calculations.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"multi-factor-studies","chapter":"24 Analysis of Variance","heading":"24.5.3 Multi-Factor Studies","text":"noncentral \\(F\\) tables apply multi-factor models.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"two-factor-fixed-effects-model","chapter":"24 Analysis of Variance","heading":"24.5.3.1 Two-Factor Fixed Effects Model","text":"","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"test-for-interaction-effects","chapter":"24 Analysis of Variance","heading":"24.5.3.1.1 Test for Interaction Effects","text":"non-centrality parameter:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum_i \\sum_j (\\alpha \\beta)_{ij}^2}{(-1)(b-1)+1}}\n\\]equivalently:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum_i \\sum_j (\\mu_{ij} - \\mu_{.} - \\mu_{.j} + \\mu_{..})^2}{(-1)(b-1)+1}}\n\\]degrees freedom :\\[\n\\begin{aligned}\n\\upsilon_1 &= (-1)(b-1) \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"test-for-factor-a-main-effects","chapter":"24 Analysis of Variance","heading":"24.5.3.1.2 Test for Factor \\(A\\) Main Effects","text":"non-centrality parameter:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum \\alpha_i^2}{}}\n\\]equivalently:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum (\\mu_{.} - \\mu_{..})^2}{}}\n\\]degrees freedom :\\[\n\\begin{aligned}\n\\upsilon_1 &= -1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"test-for-factor-b-main-effects","chapter":"24 Analysis of Variance","heading":"24.5.3.1.3 Test for Factor \\(B\\) Main Effects","text":"non-centrality parameter:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum \\beta_j^2}{b}}\n\\]equivalently:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum (\\mu_{.j} - \\mu_{..})^2}{b}}\n\\]degrees freedom :\\[\n\\begin{aligned}\n\\upsilon_1 &= b-1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"procedure-for-sample-size-selection","chapter":"24 Analysis of Variance","heading":"24.5.4 Procedure for Sample Size Selection","text":"Specify minimum range Factor \\(\\) means.Obtain sample size power tables using \\(r = \\).\nresulting sample size \\(bn\\), \\(n\\) can derived.\nresulting sample size \\(bn\\), \\(n\\) can derived.Repeat steps 1-2 Factor \\(B\\).Choose larger sample size calculations Factors \\(\\) \\(B\\).","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"randomized-block-experiments","chapter":"24 Analysis of Variance","heading":"24.5.5 Randomized Block Experiments","text":"Analogous completely randomized designs . power F-test treatment effects randomized block design uses non-centrality parameter completely randomized design:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{r} \\sum (\\mu_i - \\mu_.)^2}\n\\]However, power level different randomized block design becauseerror variance \\(\\sigma^2\\) differentdf(MSE) different.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"single-factor-covariance-model","chapter":"24 Analysis of Variance","heading":"24.6 Single Factor Covariance Model","text":"single-factor covariance model (Analysis Covariance, ANCOVA) accounts treatment effects continuous covariate:\\[\nY_{ij} = \\mu_{.} + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) + \\epsilon_{ij}\n\\]\\(= 1, \\dots, r\\) (treatments) \\(j = 1, \\dots, n_i\\) (observations per treatment).\\(\\mu_{.}\\): Overall mean response.\\(\\tau_i\\): Fixed treatment effects (\\(\\sum \\tau_i = 0\\)).\\(\\gamma\\): Fixed regression coefficient (relationship covariate \\(X\\) response \\(Y\\)).\\(X_{ij}\\): Observed covariate (fixed, random).\\(\\epsilon_{ij} \\sim iid N(0, \\sigma^2)\\): Independent random errors.use \\(\\gamma X_{ij}\\) directly (without centering), \\(\\mu_{.}\\) longer overall mean. Thus, centering covariate necessary maintain interpretability.Expectation Variance\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_. + \\tau_i + \\gamma(X_{ij}-\\bar{X}_{..}) \\\\\nvar(Y_{ij}) &= \\sigma^2\n\\end{aligned}\n\\]Since \\(Y_{ij} \\sim N(\\mu_{ij},\\sigma^2)\\), express:\\[\n\\mu_{ij} = \\mu_. + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..})\n\\]\\(\\sum \\tau_i = 0\\). mean response \\(\\mu_{ij}\\) regression line intercept \\(\\mu_. + \\tau_i\\) slope \\(\\gamma\\) treatment \\(\\).Key AssumptionsAll treatments share slope (\\(\\gamma\\)).interaction treatment covariate (parallel regression lines).slopes differ, ANCOVA appropriate → use separate regressions per treatment.general model allows multiple covariates:\\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma_1(X_{ij1}-\\bar{X}_{..1}) + \\gamma_2(X_{ij2}-\\bar{X}_{..2}) + \\epsilon_{ij}\n\\]Using indicator variables treatments:treatment \\(= 1\\): \\[\nl_1 =\n\\begin{cases}\n1 & \\text{case belongs treatment 1} \\\\\n-1 & \\text{case belongs treatment $r$} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]treatment \\(= r-1\\): \\[\nl_{r-1} =\n\\begin{cases}\n1 & \\text{case belongs treatment $r-1$} \\\\\n-1 & \\text{case belongs treatment $r$} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Defining \\(x_{ij} = X_{ij}- \\bar{X}_{..}\\), regression model :\\[\nY_{ij} = \\mu_. + \\tau_1 l_{ij,1} + \\dots + \\tau_{r-1} l_{ij,r-1} + \\gamma x_{ij} + \\epsilon_{ij}\n\\]\\(I_{ij,1}\\) indicator variable \\(l_1\\) \\(j\\)-th case treatment \\(\\).treatment effects (\\(\\tau_i\\)) simply regression coefficients indicator variables.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"statistical-inference-for-treatment-effects","chapter":"24 Analysis of Variance","heading":"24.6.1 Statistical Inference for Treatment Effects","text":"test treatment effects:\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = \\dots = 0 \\\\\n&H_a: \\text{} \\tau_i = 0\n\\end{aligned}\n\\]Full Model (treatment effects): \\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} + \\epsilon_{ij}\n\\]Full Model (treatment effects): \\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} + \\epsilon_{ij}\n\\]Reduced Model (without treatment effects): \\[\nY_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij}\n\\]Reduced Model (without treatment effects): \\[\nY_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij}\n\\]F-Test Treatment EffectsThe test statistic :\\[\nF = \\frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} \\Big/ \\frac{SSE(F)}{N-(r+1)}\n\\]:\\(SSE(R)\\): Sum squared errors reduced model.\\(SSE(R)\\): Sum squared errors reduced model.\\(SSE(F)\\): Sum squared errors full model.\\(SSE(F)\\): Sum squared errors full model.\\(N\\): Total number observations.\\(N\\): Total number observations.\\(r\\): Number treatment groups.\\(r\\): Number treatment groups.\\(H_0\\), statistic follows \\(F\\)-distribution:\\[\nF \\sim F_{(r-1, N-(r+1))}\n\\]Comparisons Treatment EffectsFor \\(r = 3\\), estimate:","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"testing-for-parallel-slopes","chapter":"24 Analysis of Variance","heading":"24.6.2 Testing for Parallel Slopes","text":"check slopes differ across treatments, use model:\\[\nY_{ij} = \\mu_{.} + \\tau_1 I_{ij,1} + \\tau_2 I_{ij,2} + \\gamma X_{ij} + \\beta_1 I_{ij,1}X_{ij} + \\beta_2 I_{ij,2}X_{ij} + \\epsilon_{ij}\n\\]:\\(\\beta_1, \\beta_2\\): Interaction coefficients (slope differences across treatments).Hypothesis Test\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0 \\quad (\\text{Slopes equal}) \\\\\n&H_a: \\text{least one } \\beta \\neq 0 \\quad (\\text{Slopes differ})\n\\end{aligned}\n\\]\\(F\\)-test fails reject \\(H_0\\), assume parallel slopes.","code":""},{"path":"sec-analysis-of-variance-anova.html","id":"adjusted-means","chapter":"24 Analysis of Variance","heading":"24.6.3 Adjusted Means","text":"adjusted treatment means account covariate effects:\\[\nY_{.}(\\text{adj}) = \\bar{Y}_{.} - \\hat{\\gamma}(\\bar{X}_{.} - \\bar{X}_{..})\n\\]:\\(\\bar{Y}_{.}\\): Observed mean response treatment \\(\\).\\(\\bar{Y}_{.}\\): Observed mean response treatment \\(\\).\\(\\hat{\\gamma}\\): Estimated regression coefficient.\\(\\hat{\\gamma}\\): Estimated regression coefficient.\\(\\bar{X}_{.}\\): Mean covariate value treatment \\(\\).\\(\\bar{X}_{.}\\): Mean covariate value treatment \\(\\).\\(\\bar{X}_{..}\\): Overall mean covariate value.\\(\\bar{X}_{..}\\): Overall mean covariate value.provides estimated treatment means controlling covariate effects.","code":""},{"path":"sec-multivariate-methods.html","id":"sec-multivariate-methods","chapter":"25 Multivariate Methods","heading":"25 Multivariate Methods","text":"previous section ANOVA, examined compare means across multiple groups. However, ANOVA primarily deals single response variable. many business financial applications, often need analyze multiple interrelated variables simultaneously. instance:marketing, customer purchase behavior, brand perception, loyalty scores often studied together.finance, portfolio risk assessment involves analyzing correlations different asset returns.handle cases, use multivariate methods, extend classical statistical techniques multiple dependent variables. core multivariate analysis lies covariance matrix, captures relationships multiple random variables.","code":""},{"path":"sec-multivariate-methods.html","id":"basic-understanding","chapter":"25 Multivariate Methods","heading":"25.1 Basic Understanding","text":"","code":""},{"path":"sec-multivariate-methods.html","id":"multivariate-random-vectors","chapter":"25 Multivariate Methods","heading":"25.1.1 Multivariate Random Vectors","text":"Let \\(y_1, \\dots, y_p\\) random variables, possibly correlated, means \\(\\mu_1, \\dots, \\mu_p\\). define random vector:\\[\n\\mathbf{y} =\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_p\n\\end{bmatrix}\n\\]expected value (mean vector) :\\[\nE(\\mathbf{y}) =\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix}\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"sec-covariance-matrix-multivariate","chapter":"25 Multivariate Methods","heading":"25.1.2 Covariance Matrix","text":"covariance two variables \\(y_i\\) \\(y_j\\) :\\[\n\\sigma_{ij} = \\text{cov}(y_i, y_j) = E[(y_i - \\mu_i)(y_j - \\mu_j)]\n\\]leads variance-covariance matrix, also called dispersion matrix:\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij}) =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n\\end{bmatrix}\n\\]\\(\\sigma_{ii} = \\text{Var}(y_i)\\) represents variance \\(y_i\\). Since covariance symmetric, :\\[\n\\sigma_{ij} = \\sigma_{ji}, \\quad \\forall , j.\n\\]consider two random vectors \\(\\mathbf{u}_{p \\times 1}\\) \\(\\mathbf{v}_{q \\times 1}\\) means \\(\\mu_u\\) \\(\\mu_v\\), cross-covariance matrix :\\[\n\\mathbf{\\Sigma}_{uv} = \\text{cov}(\\mathbf{u}, \\mathbf{v}) = E[(\\mathbf{u} - \\mu_u)(\\mathbf{v} - \\mu_v)']\n\\]\\(\\mathbf{\\Sigma}_{uv} \\neq \\mathbf{\\Sigma}_{vu}\\), satisfy:\\[\n\\mathbf{\\Sigma}_{uv} = \\mathbf{\\Sigma}_{vu}'.\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"properties-of-covariance-matrices","chapter":"25 Multivariate Methods","heading":"25.1.2.1 Properties of Covariance Matrices","text":"valid covariance matrix \\(\\mathbf{\\Sigma}\\) satisfies following properties:Symmetry:\\[\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}.\\]Symmetry:\\[\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}.\\]Non-negative definiteness:\\[\\mathbf{}'\\mathbf{\\Sigma} \\mathbf{} \\geq 0, \\quad \\forall \\mathbf{} \\\\mathbb{R}^p,\\] implies eigenvalues \\(\\lambda_1, \\dots, \\lambda_p\\) satisfy: \\[\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0.\\]Non-negative definiteness:\\[\\mathbf{}'\\mathbf{\\Sigma} \\mathbf{} \\geq 0, \\quad \\forall \\mathbf{} \\\\mathbb{R}^p,\\] implies eigenvalues \\(\\lambda_1, \\dots, \\lambda_p\\) satisfy: \\[\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0.\\]Generalized variance (determinant \\(\\mathbf{\\Sigma}\\)):\\[|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 \\dots \\lambda_p \\geq 0.\\]Generalized variance (determinant \\(\\mathbf{\\Sigma}\\)):\\[|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 \\dots \\lambda_p \\geq 0.\\]Total variance (trace \\(\\mathbf{\\Sigma}\\)):\\[\\text{tr}(\\mathbf{\\Sigma}) = \\sum_{=1}^{p} \\lambda_i = \\sum_{=1}^{p} \\sigma_{ii}.\\]Total variance (trace \\(\\mathbf{\\Sigma}\\)):\\[\\text{tr}(\\mathbf{\\Sigma}) = \\sum_{=1}^{p} \\lambda_i = \\sum_{=1}^{p} \\sigma_{ii}.\\]Positive definiteness (common assumption multivariate analysis):\neigenvalues \\(\\mathbf{\\Sigma}\\) strictly positive.\n\\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), satisfying: \\[\\mathbf{\\Sigma}^{-1} \\mathbf{\\Sigma} = \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma} \\mathbf{\\Sigma}^{-1}.\\]\nPositive definiteness (common assumption multivariate analysis):eigenvalues \\(\\mathbf{\\Sigma}\\) strictly positive.\\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), satisfying: \\[\\mathbf{\\Sigma}^{-1} \\mathbf{\\Sigma} = \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma} \\mathbf{\\Sigma}^{-1}.\\]","code":""},{"path":"sec-multivariate-methods.html","id":"correlation-matrices-1","chapter":"25 Multivariate Methods","heading":"25.1.2.2 Correlation Matrices","text":"correlation matrix provides standardized measure linear relationships variables. correlation two variables \\(y_i\\) \\(y_j\\) defined :\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n\\]\\(\\sigma_{ij}\\) covariance \\(\\sigma_{ii}\\) \\(\\sigma_{jj}\\) variances.Thus, correlation matrix \\(\\mathbf{R}\\) :\\[\n\\mathbf{R} =\n\\begin{bmatrix}\n\\rho_{11} & \\rho_{12} & \\dots & \\rho_{1p} \\\\\n\\rho_{21} & \\rho_{22} & \\dots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\dots & \\rho_{pp}\n\\end{bmatrix}\n\\]\\(\\rho_{ii} = 1\\) \\(\\).Alternatively, correlation matrix can expressed :\\[\n\\mathbf{R} = [\\text{diag}(\\mathbf{\\Sigma})]^{-1/2} \\mathbf{\\Sigma} [\\text{diag}(\\mathbf{\\Sigma})]^{-1/2}\n\\]:\\(\\text{diag}(\\mathbf{\\Sigma})\\) diagonal matrix elements \\(\\sigma_{ii}\\) diagonal zeros elsewhere.\\(\\mathbf{}^{1/2}\\) (square root symmetric matrix) symmetric matrix satisfying \\(\\mathbf{} = \\mathbf{}^{1/2} \\mathbf{}^{1/2}\\).","code":""},{"path":"sec-multivariate-methods.html","id":"equalities-in-expectation-and-variance","chapter":"25 Multivariate Methods","heading":"25.1.3 Equalities in Expectation and Variance","text":"Let:\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) covariance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants, \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants.following properties hold:Expectation transformations: \\[\nE(\\mathbf{Ay + c}) = \\mathbf{} \\mu_y + \\mathbf{c}\n\\]Expectation transformations: \\[\nE(\\mathbf{Ay + c}) = \\mathbf{} \\mu_y + \\mathbf{c}\n\\]Variance transformations: \\[\n\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{} \\text{Var}(\\mathbf{y}) \\mathbf{}' = \\mathbf{\\Sigma_y '}\n\\]Variance transformations: \\[\n\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{} \\text{Var}(\\mathbf{y}) \\mathbf{}' = \\mathbf{\\Sigma_y '}\n\\]Covariance linear transformations: \\[\n\\text{Cov}(\\mathbf{Ay + c}, \\mathbf{+ d}) = \\mathbf{\\Sigma_y B'}\n\\]Covariance linear transformations: \\[\n\\text{Cov}(\\mathbf{Ay + c}, \\mathbf{+ d}) = \\mathbf{\\Sigma_y B'}\n\\]Expectation combined variables: \\[\nE(\\mathbf{Ay + Bx + c}) = \\mathbf{} \\mu_y + \\mathbf{B} \\mu_x + \\mathbf{c}\n\\]Expectation combined variables: \\[\nE(\\mathbf{Ay + Bx + c}) = \\mathbf{} \\mu_y + \\mathbf{B} \\mu_x + \\mathbf{c}\n\\]Variance combined variables: \\[\n\\text{Var}(\\mathbf{Ay + Bx + c}) =\n\\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx} B' + B\\Sigma'_{yx}'}\n\\]Variance combined variables: \\[\n\\text{Var}(\\mathbf{Ay + Bx + c}) =\n\\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx} B' + B\\Sigma'_{yx}'}\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"multivariate-normal-distribution-1","chapter":"25 Multivariate Methods","heading":"25.1.4 Multivariate Normal Distribution","text":"multivariate normal distribution (MVN) fundamental multivariate analysis. Let \\(\\mathbf{y}\\) multivariate normal random variable mean \\(\\mu\\) covariance matrix \\(\\mathbf{\\Sigma}\\). probability density function (PDF) :\\[\nf(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2} |\\mathbf{\\Sigma}|^{1/2}}\n\\exp \\left(-\\frac{1}{2} (\\mathbf{y} - \\mu)' \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mu) \\right).\n\\]denote distribution :\\[\n\\mathbf{y} \\sim N_p(\\mu, \\mathbf{\\Sigma}).\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"properties-of-the-multivariate-normal-distribution","chapter":"25 Multivariate Methods","heading":"25.1.4.1 Properties of the Multivariate Normal Distribution","text":"multivariate normal distribution several important properties fundamental multivariate statistical methods.Linear Transformations:\nLet \\(\\mathbf{}_{r \\times p}\\) fixed matrix. :\n\\[\n\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu}, \\mathbf{\\Sigma '})\n\\]\n\\(r \\leq p\\). Additionally, \\(\\mathbf{\\Sigma '}\\) non-singular, rows \\(\\mathbf{}\\) must linearly independent.Linear Transformations:\nLet \\(\\mathbf{}_{r \\times p}\\) fixed matrix. :\\[\n\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu}, \\mathbf{\\Sigma '})\n\\]\\(r \\leq p\\). Additionally, \\(\\mathbf{\\Sigma '}\\) non-singular, rows \\(\\mathbf{}\\) must linearly independent.Standardization using Precision Matrix:\nLet \\(\\mathbf{G}\\) matrix :\n\\[\n\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\n\\]\n:\n\\[\n\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu}, \\mathbf{})\n\\]\n:\n\\[\n\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{}).\n\\]\ntransformation whitens data, converting identity covariance structure.Standardization using Precision Matrix:\nLet \\(\\mathbf{G}\\) matrix :\\[\n\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\n\\]:\\[\n\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu}, \\mathbf{})\n\\]:\\[\n\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{}).\n\\]transformation whitens data, converting identity covariance structure.Linear Combinations:\nfixed linear combination \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c'y}\\), follows:\n\\[\n\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu}, \\mathbf{c' \\Sigma c}).\n\\]Linear Combinations:\nfixed linear combination \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c'y}\\), follows:\\[\n\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu}, \\mathbf{c' \\Sigma c}).\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"partitioning-the-mvn-distribution","chapter":"25 Multivariate Methods","heading":"25.1.4.2 Partitioning the MVN Distribution","text":"Consider partitioned random vector:\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n\\mathbf{y}_1 \\\\\n\\mathbf{y}_2\n\\end{bmatrix}\n\\sim\nN_p\n\\left(\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{bmatrix},\n\\begin{bmatrix}\n\\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\\n\\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22}\n\\end{bmatrix}\n\\right).\n\\]:\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\),\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(p_1 + p_2 = p\\),\\(p_1, p_2 \\geq 1\\).marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) :\\[\n\\mathbf{y}_1 \\sim N_{p_1}(\\mathbf{\\mu_1}, \\mathbf{\\Sigma_{11}})\n\\quad \\text{} \\quad\n\\mathbf{y}_2 \\sim N_{p_2}(\\mathbf{\\mu_2}, \\mathbf{\\Sigma_{22}}).\n\\]component \\(y_i\\) follows:\\[\ny_i \\sim N_1(\\mu_i, \\sigma_{ii}).\n\\]conditional distribution \\(\\mathbf{y}_1\\) given \\(\\mathbf{y}_2\\) also normal:\\[\n\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p_1} \\Big(\n\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2)},\n\\mathbf{\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}}\n\\Big).\n\\]equation shows knowing \\(\\mathbf{y}_2\\) adjusts mean \\(\\mathbf{y}_1\\), variance reduced.\nSimilarly, conditional distribution \\(\\mathbf{y}_2\\) given \\(\\mathbf{y}_1\\) follows structure.\\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independent :\n\\[\n\\mathbf{\\Sigma}_{12} = 0.\n\\]\\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independent :\\[\n\\mathbf{\\Sigma}_{12} = 0.\n\\]\\(\\mathbf{y} \\sim N(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, :\\[\n(\\mathbf{y} - \\mu)' \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mu) \\sim \\chi^2_p.\n\\]property essential hypothesis testing Mahalanobis distance calculations.","code":""},{"path":"sec-multivariate-methods.html","id":"summation-of-independent-mvn-variables","chapter":"25 Multivariate Methods","heading":"25.1.4.3 Summation of Independent MVN Variables","text":"\\(\\mathbf{y}_i\\) independent random vectors following:\\[\n\\mathbf{y}_i \\sim N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i),\n\\]fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), sum:\\[\n\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i\n\\]follows:\\[\n\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m \\Big(\n\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i'\n\\Big).\n\\]property underpins multivariate regression linear discriminant analysis.","code":""},{"path":"sec-multivariate-methods.html","id":"multiple-regression","chapter":"25 Multivariate Methods","heading":"25.1.4.4 Multiple Regression","text":"multivariate analysis, multiple regression extends simple regression cases multiple predictor variables influence response variable. Suppose:\\[\n\\left(\n\\begin{array}\n{c}\nY \\\\\n\\mathbf{x}\n\\end{array}\n\\right)\n\\sim\nN_{p+1}\n\\left(\n\\left[\n\\begin{array}\n{c}\n\\mu_y \\\\\n\\mathbf{\\mu}_x\n\\end{array}\n\\right]\n,\n\\left[\n\\begin{array}\n{cc}\n\\sigma^2_Y & \\mathbf{\\Sigma}_{yx} \\\\\n\\mathbf{\\Sigma}_{yx} & \\mathbf{\\Sigma}_{xx}\n\\end{array}\n\\right]\n\\right)\n\\]:\\(Y\\) scalar response variable.\\(\\mathbf{x}\\) \\(p \\times 1\\) vector predictors.\\(\\mu_y\\) \\(\\mathbf{\\mu}_x\\) respective means.\\(\\sigma_Y^2\\) variance \\(Y\\).\\(\\mathbf{\\Sigma}_{xx}\\) covariance matrix \\(\\mathbf{x}\\).\\(\\mathbf{\\Sigma}_{yx}\\) covariance vector \\(Y\\) \\(\\mathbf{x}\\).properties multivariate normal distribution, conditional expectation \\(Y\\) given \\(\\mathbf{x}\\) :\\[\n\\begin{aligned}\nE(Y| \\mathbf{x}) &= \\mu_y + \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} (\\mathbf{x}- \\mathbf{\\mu}_x) \\\\\n&= \\mu_y - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\mu}_x + \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{x} \\\\\n&= \\beta_0 + \\mathbf{\\beta' x},\n\\end{aligned}\n\\]:\\(\\beta_0 = \\mu_y - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\mu}_x\\) (intercept).\\(\\mathbf{\\beta} = (\\beta_1, \\dots, \\beta_p)' = \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{yx}'\\) (regression coefficients).resembles least squares estimator:\\[\n\\mathbf{\\beta} = (\\mathbf{x'x})^{-1} \\mathbf{x'y},\n\\]differs considering theoretical covariance relationships rather empirical estimates.conditional variance \\(Y\\) given \\(\\mathbf{x}\\) :\\[\n\\text{Var}(Y | \\mathbf{x}) = \\sigma^2_Y - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma'}_{yx}.\n\\]shows knowing \\(\\mathbf{x}\\) reduces uncertainty predicting \\(Y\\).","code":""},{"path":"sec-multivariate-methods.html","id":"samples-from-multivariate-normal-populations","chapter":"25 Multivariate Methods","heading":"25.1.4.5 Samples from Multivariate Normal Populations","text":"Suppose random sample size \\(n\\), denoted :\\[\n\\mathbf{y}_1, \\dots, \\mathbf{y}_n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}).\n\\]:Sample Mean: sample mean given :\n\\[\n\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{=1}^n \\mathbf{y}_i.\n\\]\nSince \\(\\mathbf{y}_i\\) independent identically distributed (iid), follows :\n\\[\n\\bar{\\mathbf{y}} \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma} / n).\n\\]\nimplies \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\).Sample Mean: sample mean given :\\[\n\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{=1}^n \\mathbf{y}_i.\n\\]Since \\(\\mathbf{y}_i\\) independent identically distributed (iid), follows :\\[\n\\bar{\\mathbf{y}} \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma} / n).\n\\]implies \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\).Sample Covariance Matrix: \\(p \\times p\\) sample variance-covariance matrix :\n\\[\n\\mathbf{S} = \\frac{1}{n-1} \\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})'.\n\\]\nExpanding :\n\\[\n\\mathbf{S} = \\frac{1}{n-1} \\left( \\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}} \\bar{\\mathbf{y}}' \\right).\n\\]\n\\(\\mathbf{S}\\) symmetric.\n\\(\\mathbf{S}\\) unbiased estimator \\(\\mathbf{\\Sigma}\\).\n\\(\\mathbf{S}\\) contains \\(p(p+1)/2\\) unique random variables.\nSample Covariance Matrix: \\(p \\times p\\) sample variance-covariance matrix :\\[\n\\mathbf{S} = \\frac{1}{n-1} \\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})'.\n\\]Expanding :\\[\n\\mathbf{S} = \\frac{1}{n-1} \\left( \\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}} \\bar{\\mathbf{y}}' \\right).\n\\]\\(\\mathbf{S}\\) symmetric.\\(\\mathbf{S}\\) unbiased estimator \\(\\mathbf{\\Sigma}\\).\\(\\mathbf{S}\\) contains \\(p(p+1)/2\\) unique random variables.Wishart Distribution: scaled sample covariance matrix follows Wishart distribution:\n\\[\n(n-1) \\mathbf{S} \\sim W_p(n-1, \\mathbf{\\Sigma}).\n\\]\n:\n\\(W_p(n-1, \\mathbf{\\Sigma})\\) Wishart distribution \\(n-1\\) degrees freedom.\n\\(E[(n-1) \\mathbf{S}] = (n-1) \\mathbf{\\Sigma}\\).\nWishart distribution multivariate generalization chi-square distribution.Wishart Distribution: scaled sample covariance matrix follows Wishart distribution:\\[\n(n-1) \\mathbf{S} \\sim W_p(n-1, \\mathbf{\\Sigma}).\n\\]:\\(W_p(n-1, \\mathbf{\\Sigma})\\) Wishart distribution \\(n-1\\) degrees freedom.\\(E[(n-1) \\mathbf{S}] = (n-1) \\mathbf{\\Sigma}\\).Wishart distribution multivariate generalization chi-square distribution.Independence \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\): sample mean \\(\\bar{\\mathbf{y}}\\) sample covariance matrix \\(\\mathbf{S}\\) independent:\n\\[\n\\bar{\\mathbf{y}} \\perp \\mathbf{S}.\n\\]\nresult crucial inference multivariate hypothesis testing.Independence \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\): sample mean \\(\\bar{\\mathbf{y}}\\) sample covariance matrix \\(\\mathbf{S}\\) independent:\\[\n\\bar{\\mathbf{y}} \\perp \\mathbf{S}.\n\\]result crucial inference multivariate hypothesis testing.Sufficiency \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\): pair \\((\\bar{\\mathbf{y}}, \\mathbf{S})\\) sufficient statistics \\((\\mathbf{\\mu}, \\mathbf{\\Sigma})\\).\n, information \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) sample contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\), regardless sample size.Sufficiency \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\): pair \\((\\bar{\\mathbf{y}}, \\mathbf{S})\\) sufficient statistics \\((\\mathbf{\\mu}, \\mathbf{\\Sigma})\\).\n, information \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) sample contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\), regardless sample size.","code":""},{"path":"sec-multivariate-methods.html","id":"large-sample-properties-1","chapter":"25 Multivariate Methods","heading":"25.1.4.6 Large Sample Properties","text":"Consider random sample \\(\\mathbf{y}_1, \\dots, \\mathbf{y}_n\\) drawn population mean \\(\\mathbf{\\mu}\\) variance-covariance matrix \\(\\mathbf{\\Sigma}\\).Key PropertiesConsistency Estimators:\nsample mean \\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mathbf{\\mu}\\).\nsample covariance matrix \\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\).\nConsistency Estimators:sample mean \\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mathbf{\\mu}\\).sample covariance matrix \\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\).Multivariate Central Limit Theorem:\nSimilar univariate case, sample mean follows approximately:\n\\[\n\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0}, \\mathbf{\\Sigma})\n\\]\napproximation holds sample size large relative number variables (\\(n \\geq 25p\\)).\nEquivalently, sample mean follows:\n\\[\n\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma} / n).\n\\]\nMultivariate Central Limit Theorem:Similar univariate case, sample mean follows approximately:\n\\[\n\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0}, \\mathbf{\\Sigma})\n\\]\napproximation holds sample size large relative number variables (\\(n \\geq 25p\\)).Similar univariate case, sample mean follows approximately:\\[\n\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0}, \\mathbf{\\Sigma})\n\\]approximation holds sample size large relative number variables (\\(n \\geq 25p\\)).Equivalently, sample mean follows:\n\\[\n\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma} / n).\n\\]Equivalently, sample mean follows:\\[\n\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma} / n).\n\\]Wald’s Theorem:\n\\(n\\) large relative \\(p\\):\n\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu})' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mathbf{\\mu}) \\sim \\chi^2_p.\n\\]\nuseful hypothesis testing \\(\\mathbf{\\mu}\\).Wald’s Theorem:\\(n\\) large relative \\(p\\):\n\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu})' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mathbf{\\mu}) \\sim \\chi^2_p.\n\\]\\(n\\) large relative \\(p\\):\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu})' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mathbf{\\mu}) \\sim \\chi^2_p.\n\\]useful hypothesis testing \\(\\mathbf{\\mu}\\).","code":""},{"path":"sec-multivariate-methods.html","id":"maximum-likelihood-estimation-for-mvn","chapter":"25 Multivariate Methods","heading":"25.1.4.7 Maximum Likelihood Estimation for MVN","text":"Suppose \\(\\mathbf{y}_1, \\dots, \\mathbf{y}_n\\) iid random vectors :\\[\n\\mathbf{y}_i \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}).\n\\]likelihood function sample :\\[\n\\begin{aligned}\nL(\\mathbf{\\mu}, \\mathbf{\\Sigma}) &= \\prod_{j=1}^n \\left[ \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}\n\\exp \\left(-\\frac{1}{2} (\\mathbf{y}_j - \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{y}_j - \\mathbf{\\mu}) \\right) \\right] \\\\\n&= \\frac{1}{(2\\pi)^{np/2}|\\mathbf{\\Sigma}|^{n/2}}\n\\exp \\left(-\\frac{1}{2} \\sum_{j=1}^n (\\mathbf{y}_j - \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{y}_j - \\mathbf{\\mu}) \\right).\n\\end{aligned}\n\\]Taking log-likelihood function differentiating respect \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) leads maximum likelihood estimators:MLE mean simply sample mean:\\[\n\\hat{\\mathbf{\\mu}} = \\bar{\\mathbf{y}}.\n\\]MLE covariance matrix :\\[\n\\hat{\\mathbf{\\Sigma}} = \\frac{n-1}{n} \\mathbf{S}.\n\\]:\\[\n\\mathbf{S} = \\frac{1}{n-1} \\sum_{j=1}^n (\\mathbf{y}_j - \\bar{\\mathbf{y}})(\\mathbf{y}_j - \\bar{\\mathbf{y}})'.\n\\]differs \\(\\mathbf{S}\\) factor \\(\\frac{n-1}{n}\\), making \\(\\hat{\\mathbf{\\Sigma}}\\) biased estimator \\(\\mathbf{\\Sigma}\\).","code":""},{"path":"sec-multivariate-methods.html","id":"properties-of-maximum-likelihood-estimators","chapter":"25 Multivariate Methods","heading":"25.1.4.7.1 Properties of Maximum Likelihood Estimators","text":"MLEs several important theoretical properties:Invariance:\n\\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE function \\(h(\\theta)\\) :\n\\[\nh(\\hat{\\theta}).\n\\]\n\\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE function \\(h(\\theta)\\) :\n\\[\nh(\\hat{\\theta}).\n\\]\\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE function \\(h(\\theta)\\) :\\[\nh(\\hat{\\theta}).\n\\]Consistency:\nMLEs consistent estimators, meaning converge true parameter values \\(n \\\\infty\\).\nHowever, can biased finite samples.\nMLEs consistent estimators, meaning converge true parameter values \\(n \\\\infty\\).However, can biased finite samples.Efficiency:\nMLEs asymptotically efficient, meaning achieve Cramér-Rao lower bound variance large samples.\nestimator smaller variance asymptotically.\nMLEs asymptotically efficient, meaning achieve Cramér-Rao lower bound variance large samples.estimator smaller variance asymptotically.Asymptotic Normality:\nSuppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based \\(n\\) independent observations.\n, large \\(n\\):\n\\[\n\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1}),\n\\]\n\\(\\mathbf{H}\\) Fisher Information Matrix, defined :\n\\[\n\\mathbf{H}_{ij} = -E\\left(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j}\\right).\n\\]\nFisher Information Matrix measures amount information data \\(\\theta\\).\ncan estimated evaluating second derivatives log-likelihood function \\(\\hat{\\theta}_n\\).\n\nSuppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based \\(n\\) independent observations.Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based \\(n\\) independent observations., large \\(n\\):\n\\[\n\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1}),\n\\]\n\\(\\mathbf{H}\\) Fisher Information Matrix, defined :\n\\[\n\\mathbf{H}_{ij} = -E\\left(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j}\\right).\n\\]\nFisher Information Matrix measures amount information data \\(\\theta\\).\ncan estimated evaluating second derivatives log-likelihood function \\(\\hat{\\theta}_n\\).\n, large \\(n\\):\\[\n\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1}),\n\\]\\(\\mathbf{H}\\) Fisher Information Matrix, defined :\\[\n\\mathbf{H}_{ij} = -E\\left(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j}\\right).\n\\]Fisher Information Matrix measures amount information data \\(\\theta\\).can estimated evaluating second derivatives log-likelihood function \\(\\hat{\\theta}_n\\).","code":""},{"path":"sec-multivariate-methods.html","id":"likelihood-ratio-testing","chapter":"25 Multivariate Methods","heading":"25.1.4.7.2 Likelihood Ratio Testing","text":"MLEs allow us construct likelihood ratio tests hypothesis testing.Suppose test null hypothesis \\(H_0\\):\n\\[\nH_0: \\mathbf{\\theta} \\\\Theta_0 \\quad \\text{vs.} \\quad H_A: \\mathbf{\\theta} \\\\Theta.\n\\]Suppose test null hypothesis \\(H_0\\):\\[\nH_0: \\mathbf{\\theta} \\\\Theta_0 \\quad \\text{vs.} \\quad H_A: \\mathbf{\\theta} \\\\Theta.\n\\]likelihood ratio statistic :\n\\[\n\\Lambda = \\frac{\\max_{\\theta \\\\Theta_0} L(\\mathbf{\\mu}, \\mathbf{\\Sigma} | \\mathbf{Y})}\n{\\max_{\\theta \\\\Theta} L(\\mathbf{\\mu}, \\mathbf{\\Sigma} | \\mathbf{Y})}.\n\\]likelihood ratio statistic :\\[\n\\Lambda = \\frac{\\max_{\\theta \\\\Theta_0} L(\\mathbf{\\mu}, \\mathbf{\\Sigma} | \\mathbf{Y})}\n{\\max_{\\theta \\\\Theta} L(\\mathbf{\\mu}, \\mathbf{\\Sigma} | \\mathbf{Y})}.\n\\]large sample conditions, use Wilks’ theorem, states:\n\\[\n-2 \\log \\Lambda \\sim \\chi^2_v,\n\\]\n:\n\\(v\\) difference number parameters unrestricted restricted models.\nallows us approximate distribution \\(-2 \\log \\Lambda\\) using chi-square distribution.\nlarge sample conditions, use Wilks’ theorem, states:\\[\n-2 \\log \\Lambda \\sim \\chi^2_v,\n\\]:\\(v\\) difference number parameters unrestricted restricted models.allows us approximate distribution \\(-2 \\log \\Lambda\\) using chi-square distribution.","code":""},{"path":"sec-multivariate-methods.html","id":"test-of-multivariate-normality","chapter":"25 Multivariate Methods","heading":"25.1.5 Test of Multivariate Normality","text":"Assessing multivariate normality essential many statistical techniques, including multivariate regression, principal component analysis, MANOVA. key methods testing MVN.","code":""},{"path":"sec-multivariate-methods.html","id":"univariate-normality-checks","chapter":"25 Multivariate Methods","heading":"25.1.5.1 Univariate Normality Checks","text":"testing multivariate normality, useful check univariate normality variable separately:Normality Assessment: Visual statistical tests can used check normality.Key Property: univariate distribution normal, joint multivariate distribution normal.Important Caveat: Even univariate distributions normal, guarantee multivariate normality.Thus, univariate normality necessary sufficient condition MVN.","code":""},{"path":"sec-multivariate-methods.html","id":"sec-mardias-test-for-multivariate-normality","chapter":"25 Multivariate Methods","heading":"25.1.5.2 Mardia’s Test for Multivariate Normality","text":"Mardia (1970) proposed two measures assessing MVN:1. Multivariate SkewnessDefined :\\[\n\\beta_{1,p} = E[(\\mathbf{y} - \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3,\n\\]\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent identically distributed.2. Multivariate KurtosisDefined :\\[\n\\beta_{2,p} = E[(\\mathbf{y} - \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2.\n\\]true multivariate normal distribution:\\[\n\\beta_{1,p} = 0, \\quad \\beta_{2,p} = p(p+2).\n\\]Sample EstimatesFor random sample size \\(n\\), estimate:\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2} \\sum_{=1}^{n} \\sum_{j=1}^{n} g^2_{ij},\n\\]\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^{n} g^2_{ii},\n\\]:\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\),\\(g_{ii} = d_i^2\\), Mahalanobis distance.Mardia (1970) derived following large-sample approximations:\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6},\n\\]\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1).\n\\]Interpretation\\(\\kappa_1\\) \\(\\kappa_2\\) test statistics null hypothesis MVN.Non-normality means associated skewness (\\(\\beta_{1,p}\\)).Non-normality covariance associated kurtosis (\\(\\beta_{2,p}\\)).","code":""},{"path":"sec-multivariate-methods.html","id":"doornik-hansen-test","chapter":"25 Multivariate Methods","heading":"25.1.5.3 Doornik-Hansen Test","text":"test transforms variables approximate normality using skewness kurtosis corrections (Doornik Hansen 2008).Recommended sample sizes small.","code":""},{"path":"sec-multivariate-methods.html","id":"chi-square-q-q-plot","chapter":"25 Multivariate Methods","heading":"25.1.5.4 Chi-Square Q-Q Plot","text":"Chi-Square Q-Q plot graphical method assessing MVN:Compute Mahalanobis distances:\n\\[\nd_i^2 = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_i - \\bar{\\mathbf{y}}).\n\\]Compute Mahalanobis distances:\\[\nd_i^2 = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_i - \\bar{\\mathbf{y}}).\n\\]transformed variables:\n\\[\n\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu})\n\\]\niid \\(N_p(\\mathbf{0}, \\mathbf{})\\), thus:\n\\[\nd_i^2 \\sim \\chi^2_p.\n\\]transformed variables:\\[\n\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu})\n\\]iid \\(N_p(\\mathbf{0}, \\mathbf{})\\), thus:\\[\nd_i^2 \\sim \\chi^2_p.\n\\]Plot ordered \\(d_i^2\\) values theoretical quantiles \\(\\chi^2_p\\) distribution.Plot ordered \\(d_i^2\\) values theoretical quantiles \\(\\chi^2_p\\) distribution.InterpretationIf data MVN, plot resemble straight line 45°.Deviations suggest non-normality, especially tails.LimitationsRequires large sample size.Even data truly MVN, tails may deviate.","code":""},{"path":"sec-multivariate-methods.html","id":"handling-non-normality","chapter":"25 Multivariate Methods","heading":"25.1.5.5 Handling Non-Normality","text":"data fail multivariate normality tests, possible approaches include:Ignoring non-normality (acceptable large samples due Central Limit Theorem).Using nonparametric methods (e.g., permutation tests).Applying approximate models (e.g., Generalized Linear Mixed Models).Transforming data (e.g., log, Box-Cox, rank transformations 12).","code":"\n# Load necessary libraries\nlibrary(heplots)      # Multivariate hypothesis tests\nlibrary(ICSNP)        # Multivariate tests\nlibrary(MVN)          # Multivariate normality tests\nlibrary(tidyverse)    # Data wrangling & visualization\n\n\n# Load dataset\ntrees <- read.table(\"images/trees.dat\")\nnames(trees) <-\n    c(\"Nitrogen\", \"Phosphorous\", \"Potassium\", \"Ash\", \"Height\")\n\n# Structure of dataset\nstr(trees)\n#> 'data.frame':    26 obs. of  5 variables:\n#>  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...\n#>  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...\n#>  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...\n#>  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...\n#>  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...\n\n# Summary statistics\nsummary(trees)\n#>     Nitrogen      Phosphorous       Potassium           Ash        \n#>  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  \n#>  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  \n#>  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  \n#>  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  \n#>  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  \n#>  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  \n#>      Height     \n#>  Min.   : 65.0  \n#>  1st Qu.:122.5  \n#>  Median :181.0  \n#>  Mean   :196.6  \n#>  3rd Qu.:276.0  \n#>  Max.   :373.0\n\n# Pearson correlation matrix\ncor(trees, method = \"pearson\")\n#>              Nitrogen Phosphorous Potassium       Ash    Height\n#> Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641\n#> Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656\n#> Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683\n#> Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771\n#> Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000\n\n# Q-Q plots for each variable\ngg <- trees %>%\n    pivot_longer(everything(), names_to = \"Var\", values_to = \"Value\") %>%\n    ggplot(aes(sample = Value)) +\n    geom_qq() +\n    geom_qq_line() +\n    facet_wrap( ~ Var, scales = \"free\")\n\nprint(gg)\n\n# Shapiro-Wilk test for univariate normality\nsw_tests <- apply(trees, MARGIN = 2, FUN = shapiro.test)\nsw_tests\n#> $Nitrogen\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.96829, p-value = 0.5794\n#> \n#> \n#> $Phosphorous\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.93644, p-value = 0.1104\n#> \n#> \n#> $Potassium\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.95709, p-value = 0.3375\n#> \n#> \n#> $Ash\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.92071, p-value = 0.04671\n#> \n#> \n#> $Height\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.94107, p-value = 0.1424\n\n# Kolmogorov-Smirnov test for normality\nks_tests <- map(trees, ~ ks.test(scale(.x), \"pnorm\"))\nks_tests\n#> $Nitrogen\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.12182, p-value = 0.8351\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Phosphorous\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.17627, p-value = 0.3944\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Potassium\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.10542, p-value = 0.9348\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Ash\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.14503, p-value = 0.6449\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Height\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.1107, p-value = 0.9076\n#> alternative hypothesis: two-sided\n\n# Mardia's test for multivariate normality\nmardia_test <-\n    mvn(\n        trees,\n        mvnTest = \"mardia\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nmardia_test$multivariateNormality\n#>              Test         Statistic            p value Result\n#> 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES\n#> 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281    YES\n#> 3             MVN              <NA>               <NA>    YES\n\n# Doornik-Hansen test\ndh_test <-\n    mvn(\n        trees,\n        mvnTest = \"dh\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\ndh_test$multivariateNormality\n#>             Test        E df      p value MVN\n#> 1 Doornik-Hansen 161.9446 10 1.285352e-29  NO\n\n# Henze-Zirkler test\nhz_test <-\n    mvn(\n        trees,\n        mvnTest = \"hz\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nhz_test$multivariateNormality\n#>            Test        HZ   p value MVN\n#> 1 Henze-Zirkler 0.7591525 0.6398905 YES\n\n# Royston's test (only for 3 < obs < 5000)\nroyston_test <-\n    mvn(\n        trees,\n        mvnTest = \"royston\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nroyston_test$multivariateNormality\n#>      Test        H    p value MVN\n#> 1 Royston 9.064631 0.08199215 YES\n\n# Energy test\nestat_test <-\n    mvn(\n        trees,\n        mvnTest = \"energy\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nestat_test$multivariateNormality\n#>          Test Statistic p value MVN\n#> 1 E-statistic  1.091101   0.554 YES"},{"path":"sec-multivariate-methods.html","id":"mean-vector-inference","chapter":"25 Multivariate Methods","heading":"25.1.6 Mean Vector Inference","text":"","code":""},{"path":"sec-multivariate-methods.html","id":"univariate-case","chapter":"25 Multivariate Methods","heading":"25.1.6.1 Univariate Case","text":"univariate normal distribution, test:\\[\nH_0: \\mu = \\mu_0\n\\]using t-test statistic:\\[\nT = \\frac{\\bar{y} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}.\n\\]Decision RuleIf \\(H_0\\) true, \\(T\\) follows t-distribution \\(n-1\\) degrees freedom.\\(H_0\\) true, \\(T\\) follows t-distribution \\(n-1\\) degrees freedom.reject \\(H_0\\) :\n\\[\n|T| > t_{(1-\\alpha/2, n-1)}\n\\]\nextreme value suggests observing \\(\\bar{y}\\) \\(H_0\\) unlikely.reject \\(H_0\\) :\\[\n|T| > t_{(1-\\alpha/2, n-1)}\n\\]extreme value suggests observing \\(\\bar{y}\\) \\(H_0\\) unlikely.Alternative FormulationSquaring \\(T\\), obtain:\\[\nT^2 = \\frac{(\\bar{y} - \\mu_0)^2}{s^2/n} = n(\\bar{y} - \\mu_0) (s^2)^{-1} (\\bar{y} - \\mu_0).\n\\]\\(H_0\\):\\[\nT^2 \\sim f_{(1,n-1)}.\n\\]formulation allows direct extension multivariate case.","code":""},{"path":"sec-multivariate-methods.html","id":"multivariate-generalization-hotellings-t2-test","chapter":"25 Multivariate Methods","heading":"25.1.6.2 Multivariate Generalization: Hotelling’s \\(T^2\\) Test","text":"p-dimensional mean vector, test:\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0, \\\\\n&H_a: \\mathbf{\\mu} \\neq \\mathbf{\\mu}_0.\n\\end{aligned}\n\\]Define Hotelling’s \\(T^2\\) test statistic:\\[\nT^2 = n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0).\n\\]:\\(\\bar{\\mathbf{y}}\\) sample mean vector,\\(\\bar{\\mathbf{y}}\\) sample mean vector,\\(\\mathbf{S}\\) sample covariance matrix,\\(\\mathbf{S}\\) sample covariance matrix,\\(T^2\\) can interpreted generalized squared distance \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{\\mu}_0\\).\\(T^2\\) can interpreted generalized squared distance \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{\\mu}_0\\).multivariate normality, test statistic follows F-distribution:\\[\nF = \\frac{n-p}{(n-1)p} T^2 \\sim f_{(p, n-p)}.\n\\]reject \\(H_0\\) :\\[\nF > f_{(1-\\alpha, p, n-p)}.\n\\]Key Properties Hotelling’s \\(T^2\\) TestInvariance Measurement Scale:\napply linear transformation data:\n\\[\n\\mathbf{z} = \\mathbf{C} \\mathbf{y} + \\mathbf{d},\n\\]\n\\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), :\n\\[\nT^2(\\mathbf{z}) = T^2(\\mathbf{y}).\n\\]\nensures unit changes (e.g., inches centimeters) affect test results.\napply linear transformation data:\n\\[\n\\mathbf{z} = \\mathbf{C} \\mathbf{y} + \\mathbf{d},\n\\]\n\\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), :\n\\[\nT^2(\\mathbf{z}) = T^2(\\mathbf{y}).\n\\]\nensures unit changes (e.g., inches centimeters) affect test results.apply linear transformation data:\\[\n\\mathbf{z} = \\mathbf{C} \\mathbf{y} + \\mathbf{d},\n\\]\\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), :\\[\nT^2(\\mathbf{z}) = T^2(\\mathbf{y}).\n\\]ensures unit changes (e.g., inches centimeters) affect test results.Likelihood Ratio Test:\n\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0\\).\n\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0\\).","code":"\n# Load required packages\nlibrary(MASS)    # For multivariate analysis\nlibrary(ICSNP)   # For Hotelling's T^2 test\n\n# Simulated dataset (5 variables, 30 observations)\nset.seed(123)\nn <- 30  # Sample size\np <- 5   # Number of variables\nmu <- rep(0, p)  # Population mean vector\nSigma <- diag(p) # Identity covariance matrix\n\n# Generate multivariate normal data\ndata <- mvrnorm(n, mu, Sigma)\ncolnames(data) <- paste0(\"V\", 1:p)\n\n# Compute sample mean and covariance\nsample_mean <- colMeans(data)\nsample_cov  <- cov(data)\n\n# Perform Hotelling's T^2 test (testing against mu_0 = rep(0, p))\nhotelling_test <- HotellingsT2(data, mu = rep(0, p))\n\n# Print results\nprint(hotelling_test)\n#> \n#>  Hotelling's one sample T2-test\n#> \n#> data:  data\n#> T.2 = 0.43475, df1 = 5, df2 = 25, p-value = 0.82\n#> alternative hypothesis: true location is not equal to c(0,0,0,0,0)"},{"path":"sec-multivariate-methods.html","id":"confidence-intervals","chapter":"25 Multivariate Methods","heading":"25.1.6.3 Confidence Intervals","text":"","code":""},{"path":"sec-multivariate-methods.html","id":"confidence-region-for-the-mean-vector","chapter":"25 Multivariate Methods","heading":"25.1.6.3.1 Confidence Region for the Mean Vector","text":"exact \\(100(1-\\alpha)\\%\\) confidence region population mean vector \\(\\mathbf{\\mu}\\) set vectors \\(\\mathbf{v}\\) “close enough” observed mean vector \\(\\bar{\\mathbf{y}}\\) :\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\leq \\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)}.\n\\]InterpretationThe confidence region consists mean vectors \\(\\mathbf{\\mu}_0\\) fail reject \\(H_0\\) Hotelling’s \\(T^2\\) test.\\(p = 2\\), confidence region forms hyper-ellipsoid.Use Confidence Regions?provide joint assessment plausible values \\(\\mathbf{\\mu}\\).However, practice, often prefer individual confidence intervals mean component.","code":""},{"path":"sec-multivariate-methods.html","id":"simultaneous-confidence-intervals","chapter":"25 Multivariate Methods","heading":"25.1.6.3.2 Simultaneous Confidence Intervals","text":"want simultaneous confidence statements, ensuring individual confidence intervals hold simultaneously high probability.Simultaneous Confidence Intervals (General Form)projecting confidence region onto coordinate axes, obtain simultaneous confidence intervals:\\[\n\\bar{y}_{} \\pm \\sqrt{\\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)} \\frac{s_{ii}}{n}}, \\quad \\text{} = 1, \\dots, p.\n\\]intervals conservative, meaning actual confidence level least \\(100(1 - \\alpha)\\%\\).Simultaneous Confidence Intervals Linear CombinationFor arbitrary linear combination \\(\\mathbf{'\\mu}\\):\\[\n\\mathbf{'\\bar{y}} \\pm \\sqrt{\\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)} \\frac{\\mathbf{'Sa}}{n}}.\n\\]:\\(\\mathbf{'\\mu} = a_1 \\mu_1 + \\dots + a_p \\mu_p\\) projection onto axis direction \\(\\mathbf{}\\).\\(\\mathbf{'\\mu} = a_1 \\mu_1 + \\dots + a_p \\mu_p\\) projection onto axis direction \\(\\mathbf{}\\).probability least one interval fails contain corresponding \\(\\mathbf{'\\mu}\\) \\(\\alpha\\).probability least one interval fails contain corresponding \\(\\mathbf{'\\mu}\\) \\(\\alpha\\).intervals useful “data snooping” (similar Scheffé’s method ANOVA\n24.1.1.5.4.2).intervals useful “data snooping” (similar Scheffé’s method ANOVA24.1.1.5.4.2).","code":""},{"path":"sec-multivariate-methods.html","id":"one-at-a-time-confidence-intervals","chapter":"25 Multivariate Methods","heading":"25.1.6.3.3 One-at-a-Time Confidence Intervals","text":"simpler alternative construct separate confidence intervals mean component individually:\\[\n\\bar{y}_i \\pm t_{(1 - \\alpha/2, n-1)} \\sqrt{\\frac{s_{ii}}{n}}.\n\\]LimitationsEach interval probability \\(1-\\alpha\\) covering corresponding \\(\\mu_i\\).ignore covariance structure \\(p\\) variables.Bonferroni Correction Multiple ComparisonsIf care \\(k\\) specific intervals, can adjust multiple comparisons using Bonferroni correction:\\[\n\\bar{y}_i \\pm t_{(1 - \\alpha/(2k), n-1)} \\sqrt{\\frac{s_{ii}}{n}}.\n\\]ensures overall confidence level remains \\(100(1 - \\alpha)\\%\\).method becomes conservative number comparisons \\(k\\) increases.","code":"\n# Load necessary libraries\nlibrary(MASS)    # For multivariate analysis\nlibrary(ICSNP)   # For Hotelling's T2 test\nlibrary(tidyverse)  # Data manipulation and plotting\n\n# Simulated dataset (5 variables, 30 observations)\nset.seed(123)\nn <- 30  # Sample size\np <- 5   # Number of variables\nalpha <- 0.05  # Significance level\n\n# Population mean and covariance\nmu <- rep(0, p)  \nSigma <- diag(p)  \n\n# Generate multivariate normal data\ndata <- mvrnorm(n, mu, Sigma)\ncolnames(data) <- paste0(\"V\", 1:p)\n\n# Compute sample mean and covariance\nsample_mean <- colMeans(data)\nsample_cov  <- cov(data)\n\n# Hotelling's T^2 statistic\nT2 <-\n    n * t(sample_mean - mu) %*% solve(sample_cov) %*% (sample_mean - mu)\n\n# Critical value for Hotelling's T^2 test\nF_crit <- ((n - 1) * p / (n - p)) * qf(1 - alpha, p, n - p)\n\n# Confidence region check\nT2 <= F_crit  # If TRUE, mean vector is within the confidence region\n#>      [,1]\n#> [1,] TRUE\n\n# Simultaneous confidence intervals\nCI_limits <-\n    sqrt(((n - 1) * p) / (n - p) * qf(1 - alpha, p, n - p) * diag(sample_cov) / n)\n\n# Construct confidence intervals\nsimultaneous_CI <- data.frame(\n  Variable = colnames(data),\n  Lower = sample_mean - CI_limits,\n  Upper = sample_mean + CI_limits\n)\n\nprint(simultaneous_CI)\n#>    Variable      Lower     Upper\n#> V1       V1 -0.9983080 0.6311472\n#> V2       V2 -0.7372215 0.5494437\n#> V3       V3 -0.5926088 0.6414496\n#> V4       V4 -0.4140990 0.7707756\n#> V5       V5 -0.7430441 0.6488366\n\n# Bonferroni-corrected one-at-a-time confidence intervals\nt_crit <- qt(1 - alpha / (2 * p), n - 1)\n\nbonferroni_CI <- data.frame(\n  Variable = colnames(data),\n  Lower = sample_mean - t_crit * sqrt(diag(sample_cov) / n),\n  Upper = sample_mean + t_crit * sqrt(diag(sample_cov) / n)\n)\n\nprint(bonferroni_CI)\n#>    Variable      Lower     Upper\n#> V1       V1 -0.7615465 0.3943857\n#> V2       V2 -0.5502678 0.3624900\n#> V3       V3 -0.4132989 0.4621397\n#> V4       V4 -0.2419355 0.5986122\n#> V5       V5 -0.5408025 0.4465950"},{"path":"sec-multivariate-methods.html","id":"general-hypothesis-testing","chapter":"25 Multivariate Methods","heading":"25.1.7 General Hypothesis Testing","text":"","code":""},{"path":"sec-multivariate-methods.html","id":"sec-one-sample-multivariate-tests","chapter":"25 Multivariate Methods","heading":"25.1.7.1 One-Sample Multivariate Tests","text":"consider testing hypothesis:\\[\nH_0: \\mathbf{C \\mu} = 0\n\\]:\\(\\mathbf{C}\\) \\(c \\times p\\) contrast matrix rank \\(c\\), \\(c \\leq p\\).\\(\\mathbf{C}\\) \\(c \\times p\\) contrast matrix rank \\(c\\), \\(c \\leq p\\).\\(\\mathbf{\\mu}\\) \\(p \\times 1\\) population mean vector.\\(\\mathbf{\\mu}\\) \\(p \\times 1\\) population mean vector.test statistic hypothesis :\\[\nF = \\frac{n - c}{(n-1)c} T^2\n\\]:\\[\nT^2 = n(\\mathbf{C\\bar{y}})' (\\mathbf{CSC'})^{-1} (\\mathbf{C\\bar{y}}).\n\\]follows F-distribution:\\[\nF \\sim f_{(c, n-c)}.\n\\]Example: Testing Equal Means Across VariablesWe test whether mean components equal:\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_p.\n\\]can rewritten :\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0, \\\\\n\\mu_2 - \\mu_3 &= 0, \\\\\n&\\vdots \\\\\n\\mu_{p-1} - \\mu_p &= 0.\n\\end{aligned}\n\\]Since testing \\(p-1\\) constraints, contrast matrix \\(\\mathbf{C}\\) \\((p-1) \\times p\\) matrix:\\[\n\\mathbf{C} =\n\\begin{bmatrix}\n1 & -1 & 0 & \\dots & 0 \\\\\n0 & 1 & -1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1 & -1\n\\end{bmatrix}.\n\\]Alternatively, can compare means first mean:\\[\nH_0: \\mu_1 - \\mu_2 = 0, \\quad \\mu_1 - \\mu_3 = 0, \\quad \\dots, \\quad \\mu_1 - \\mu_p = 0.\n\\]contrast matrix \\(\\mathbf{C}\\) becomes:\\[\n\\mathbf{C} =\n\\begin{bmatrix}\n-1 & 1 & 0 & \\dots & 0 \\\\\n-1 & 0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n-1 & 0 & \\dots & 0 & 1\n\\end{bmatrix}.\n\\]Key PropertyThe value \\(T^2\\) invariant different choices \\(\\mathbf{C}\\).Application: Repeated Measures DesignRepeated measures designs involve measuring subject multiple times different conditions time points.Let:\\(y_{ij}\\) response subject \\(\\) time \\(j\\), \\(= 1, \\dots, n\\) \\(j = 1, \\dots, T\\).\\(y_{ij}\\) response subject \\(\\) time \\(j\\), \\(= 1, \\dots, n\\) \\(j = 1, \\dots, T\\).\\(\\mathbf{y}_i = (y_{i1}, ..., y_{})'\\) random sample :\\(\\mathbf{y}_i = (y_{i1}, ..., y_{})'\\) random sample :\\[\nN_T (\\mathbf{\\mu}, \\mathbf{\\Sigma}).\n\\]Example: Testing Equal Means TimeSuppose :\\(n = 8\\) subjects,\\(n = 8\\) subjects,\\(T = 6\\) time points.\\(T = 6\\) time points.test:\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_6.\n\\]equivalent :\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0, \\\\\n\\mu_2 - \\mu_3 &= 0, \\\\\n&\\dots, \\\\\n\\mu_5 - \\mu_6 &= 0.\n\\end{aligned}\n\\]corresponding contrast matrix :\\[\n\\mathbf{C} =\n\\begin{bmatrix}\n1 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -1\n\\end{bmatrix}.\n\\]measurements occur equally spaced time points, can test trend effects using orthogonal polynomials.example, testing whether quadratic cubic trends jointly zero, use:\\[\n\\mathbf{C} =\n\\begin{bmatrix}\n1 & -1 & -1 & 1 \\\\\n-1 & 3 & -3 & 1\n\\end{bmatrix}.\n\\]","code":"\n# Load necessary libraries\nlibrary(MASS)    # For multivariate normal data\nlibrary(ICSNP)   # For Hotelling's T^2 test\n\n# Simulated dataset (6 variables, 8 subjects)\nset.seed(123)\nn <- 8   # Number of subjects\np <- 6   # Number of time points\n\n# Generate sample data\nmu <- rep(5, p)  # Population mean\nSigma <- diag(p)  # Identity covariance matrix\n\ndata <- mvrnorm(n, mu, Sigma)\ncolnames(data) <- paste0(\"Time\", 1:p)\n\n# Compute sample mean and covariance\nsample_mean <- colMeans(data)\nsample_cov  <- cov(data)\n\n# Define contrast matrix for equal means hypothesis\nC <- matrix(0, nrow = p - 1, ncol = p)\nfor (i in 1:(p - 1)) {\n  C[i, i] <- 1\n  C[i, i + 1] <- -1\n}\n\n# Compute Hotelling's T^2 statistic\nT2 <-\n    n * t(C %*% sample_mean) %*% solve(C %*% sample_cov %*% t(C)) %*% (C %*% sample_mean)\n\n# Compute F statistic\nc <- nrow(C)\nF_stat <- ((n - c) / ((n - 1) * c)) * T2\n\n# Critical value\nF_crit <- qf(0.95, c, n - c)\n\n# Decision rule\ndecision <- F_stat > F_crit\n\n# Print results\nlist(\n  T2_statistic = T2,\n  F_statistic = F_stat,\n  F_critical_value = F_crit,\n  Reject_H0 = decision\n)\n#> $T2_statistic\n#>          [,1]\n#> [1,] 22.54896\n#> \n#> $F_statistic\n#>          [,1]\n#> [1,] 1.932768\n#> \n#> $F_critical_value\n#> [1] 9.013455\n#> \n#> $Reject_H0\n#>       [,1]\n#> [1,] FALSE"},{"path":"sec-multivariate-methods.html","id":"sec-two-sample-multivariate-tests","chapter":"25 Multivariate Methods","heading":"25.1.7.2 Two-Sample Multivariate Tests","text":"Consider testing equality two multivariate population means. Suppose two independent random samples:\\[\n\\begin{aligned}\n\\mathbf{y}_{1i} &\\sim N_p (\\mathbf{\\mu}_1, \\mathbf{\\Sigma}), \\quad = 1, \\dots, n_1, \\\\\n\\mathbf{y}_{2j} &\\sim N_p (\\mathbf{\\mu}_2, \\mathbf{\\Sigma}), \\quad j = 1, \\dots, n_2.\n\\end{aligned}\n\\]assume:Multivariate normality populations.Multivariate normality populations.Equal variance-covariance matrices: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\).Equal variance-covariance matrices: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\).Independence samples.Independence samples.summarize data using sufficient statistics:Sample means: \\(\\mathbf{\\bar{y}}_1\\), \\(\\mathbf{\\bar{y}}_2\\).Sample means: \\(\\mathbf{\\bar{y}}_1\\), \\(\\mathbf{\\bar{y}}_2\\).Sample covariance matrices: \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\).Sample covariance matrices: \\(\\mathbf{S}_1\\), \\(\\mathbf{S}_2\\).Sample sizes: \\(n_1, n_2\\).Sample sizes: \\(n_1, n_2\\).Since assume equal variance-covariance matrices, compute pooled estimator:\\[\n\\mathbf{S} = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2 - 1)\\mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}\n\\]\\(n_1 + n_2 - 2\\) degrees freedom.test:\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2, \\\\\n&H_a: \\mathbf{\\mu}_1 \\neq \\mathbf{\\mu}_2.\n\\end{aligned}\n\\], check whether least one element \\(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2\\) different.use:\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2\\).\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2\\).\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\).\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\).Since two populations independent, covariance :\\[\n\\text{Cov}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = \\text{Var}(\\mathbf{\\bar{y}}_1) + \\text{Var}(\\mathbf{\\bar{y}}_2) = \\mathbf{\\Sigma} \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right).\n\\]Hotelling’s \\(T^2\\) statistic :\\[\nT^2 = (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\left\\{ \\mathbf{S} \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right\\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2).\n\\]simplifies :\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\mathbf{S}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2).\n\\]Reject \\(H_0\\) :\\[\nT^2 \\geq \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 - p - 1} f_{(1- \\alpha, p, n_1 + n_2 - p - 1)}\n\\]equivalently, using F-statistic:\\[\nF = \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2.\n\\]Reject \\(H_0\\) :\\[\nF \\geq f_{(1- \\alpha, p , n_1 + n_2 - p -1)}.\n\\]\\(100(1-\\alpha)\\%\\) confidence region \\(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2\\) consists vectors \\(\\mathbf{\\delta}\\) satisfying:\\[\n\\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta})' \\mathbf{S}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta}) \\leq \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 - p - 1} f_{(1-\\alpha, p, n_1 + n_2 - p -1)}.\n\\]linear combinations \\(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2\\), simultaneous confidence intervals:\\[\n\\mathbf{'}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\pm \\sqrt{\\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1} f_{(1-\\alpha, p, n_1 + n_2 - p -1)} \\times \\mathbf{'Sa} \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}.\n\\]\\(k\\) pairwise comparisons, Bonferroni intervals :\\[\n(\\bar{y}_{1i} - \\bar{y}_{2i}) \\pm t_{(1-\\alpha/2k, n_1 + n_2 - 2)} \\sqrt{\\left(\\frac{1}{n_1}  + \\frac{1}{n_2}\\right) s_{ii}}.\n\\]","code":"\n# Load necessary libraries\nlibrary(MASS)    # For multivariate analysis\nlibrary(ICSNP)   # For Hotelling's T^2 test\n\n# Simulated dataset (p = 4 variables, two groups)\nset.seed(123)\nn1 <- 20  # Sample size for group 1\nn2 <- 25  # Sample size for group 2\np <- 4    # Number of variables\n\n# Generate data for both groups\nmu1 <- rep(0, p)  # Mean vector for group 1\nmu2 <- rep(1, p)  # Mean vector for group 2\nSigma <- diag(p)  # Identity covariance matrix\n\ndata1 <- mvrnorm(n1, mu1, Sigma)\ndata2 <- mvrnorm(n2, mu2, Sigma)\n\n# Compute sample means and covariance matrices\ny1_bar <- colMeans(data1)\ny2_bar <- colMeans(data2)\nS1 <- cov(data1)\nS2 <- cov(data2)\n\n# Compute pooled covariance matrix\nS_pooled <- ((n1 - 1) * S1 + (n2 - 1) * S2) / (n1 + n2 - 2)\n\n# Compute Hotelling's T^2 statistic\nT2 <- (y1_bar - y2_bar) %*% solve(S_pooled * (1/n1 + 1/n2)) %*% (y1_bar - y2_bar)\n\n# Convert to F-statistic\nF_stat <- ((n1 + n2 - p - 1) / ((n1 + n2 - 2) * p)) * T2\nF_crit <- qf(0.95, p, n1 + n2 - p - 1)\n\n# Decision rule\ndecision <- F_stat > F_crit\n\n# Print results\nlist(\n  T2_statistic = T2,\n  F_statistic = F_stat,\n  F_critical_value = F_crit,\n  Reject_H0 = decision\n)\n#> $T2_statistic\n#>          [,1]\n#> [1,] 51.90437\n#> \n#> $F_statistic\n#>          [,1]\n#> [1,] 12.07078\n#> \n#> $F_critical_value\n#> [1] 2.605975\n#> \n#> $Reject_H0\n#>      [,1]\n#> [1,] TRUE"},{"path":"sec-multivariate-methods.html","id":"model-assumptions-in-multivariate-tests","chapter":"25 Multivariate Methods","heading":"25.1.7.3 Model Assumptions in Multivariate Tests","text":"","code":""},{"path":"sec-multivariate-methods.html","id":"effects-of-unequal-covariance-matrices","chapter":"25 Multivariate Methods","heading":"25.1.7.3.1 Effects of Unequal Covariance Matrices","text":"assume two population covariance matrices equal (\\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2\\)), reality, assumption may hold.Impact Type Error PowerIf \\(n_1 = n_2\\) (large samples), impact Type error rate power minimal.\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) less 1, Type error inflated.\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error small, reducing power.","code":""},{"path":"sec-multivariate-methods.html","id":"effects-of-non-normality","chapter":"25 Multivariate Methods","heading":"25.1.7.3.2 Effects of Non-Normality","text":"Multivariate tests often assume normality, real-world data may follow normal distribution.Impact Test PerformanceTwo-sample Hotelling’s \\(T^2\\) test robust moderate departures normality populations similar distributions.One-sample Hotelling’s \\(T^2\\) test sensitive lack normality, especially distribution skewed.IntuitionA one-sample test depends distribution individual variables, making sensitive normality violations.two-sample test depends distribution differences, may less sensitive non-normality groups similar distributions.SolutionsTransform data (e.g., log Box-Cox transformation 12) improve normality.Transform data (e.g., log Box-Cox transformation 12) improve normality.Use large samples rely Central Limit Theorem.Use large samples rely Central Limit Theorem.Use alternative tests assume normality:\nWald’s Test (Chi-square-based test), require:\nNormality,\nEqual sample sizes,\nEqual covariance matrices.\n\nTest:\n\\[\nH_0: \\mathbf{\\mu}_1 - \\mathbf{\\mu}_2 = 0\n\\]\nusing:\n\\[\n(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\left( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2} \\mathbf{S}_2 \\right)^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_p.\n\\]\nUse alternative tests assume normality:Wald’s Test (Chi-square-based test), require:\nNormality,\nEqual sample sizes,\nEqual covariance matrices.\nWald’s Test (Chi-square-based test), require:Normality,Equal sample sizes,Equal covariance matrices.Test:\n\\[\nH_0: \\mathbf{\\mu}_1 - \\mathbf{\\mu}_2 = 0\n\\]\nusing:\n\\[\n(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\left( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2} \\mathbf{S}_2 \\right)^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_p.\n\\]Test:\\[\nH_0: \\mathbf{\\mu}_1 - \\mathbf{\\mu}_2 = 0\n\\]using:\\[\n(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\left( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2} \\mathbf{S}_2 \\right)^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_p.\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"testing-equality-of-covariance-matrices","chapter":"25 Multivariate Methods","heading":"25.1.7.3.3 Testing Equality of Covariance Matrices","text":"\\(k\\) independent groups, \\(p\\)-dimensional vector, test:\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\dots = \\mathbf{\\Sigma}_k = \\mathbf{\\Sigma}, \\\\\n&H_a: \\text{least two different}.\n\\end{aligned}\n\\]\\(H_0\\) holds, use pooled covariance estimate:\\[\n\\mathbf{S} = \\frac{\\sum_{=1}^k (n_i -1)\\mathbf{S}_i}{\\sum_{=1}^k (n_i - 1)}\n\\]\\(\\sum_{=1}^k (n_i -1)\\) degrees freedom.","code":""},{"path":"sec-multivariate-methods.html","id":"bartletts-test-for-equal-covariances","chapter":"25 Multivariate Methods","heading":"25.1.7.3.4 Bartlett’s Test for Equal Covariances","text":"Bartlett’s test likelihood ratio test equality covariance matrices.Define:\\[\nN = \\sum_{=1}^k n_i.\n\\]Compute:\\[\nM = (N - k) \\log|\\mathbf{S}| - \\sum_{=1}^k (n_i - 1) \\log|\\mathbf{S}_i|.\n\\]Correction factor:\\[\nC^{-1} = 1 - \\frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \\left\\{ \\sum_{=1}^k \\left(\\frac{1}{n_i - 1}\\right) - \\frac{1}{N-k} \\right\\}.\n\\]Reject \\(H_0\\) :\\[\nMC^{-1} > \\chi^2_{1- \\alpha, (k-1)p(p+1)/2}.\n\\]LimitationsSensitive non-normality: data normal, \\(MC^{-1}\\) often follows right-skewed distribution (.e., shifted right nomial \\(\\chi^2\\) distriubtion), increasing false positives.Best practice: Check univariate multivariate normality first using Bartlett’s test.","code":"\n# Load required packages\nlibrary(MASS)    # For multivariate normal data\nlibrary(ICSNP)   # Multivariate tests\nlibrary(car)     # Homogeneity of variance tests\n\n# Simulated dataset (three groups, p = 4 variables)\nset.seed(123)\nn1 <- 20  # Group 1 sample size\nn2 <- 25  # Group 2 sample size\nn3 <- 30  # Group 3 sample size\np <- 4    # Number of variables\n\n# Generate data from different covariance structures\nmu1 <- rep(0, p)  \nmu2 <- rep(1, p)  \nmu3 <- rep(2, p)  \n\nSigma1 <- diag(p)         # Identity covariance for group 1\nSigma2 <- 2 * diag(p)     # Scaled identity for group 2\nSigma3 <- matrix(0.5, p, p) + diag(0.5, p)  # Structured covariance for group 3\n\ndata1 <- mvrnorm(n1, mu1, Sigma1)\ndata2 <- mvrnorm(n2, mu2, Sigma2)\ndata3 <- mvrnorm(n3, mu3, Sigma3)\n\n# Create a combined dataset\ngroup_labels <- c(rep(\"Group1\", n1), rep(\"Group2\", n2), rep(\"Group3\", n3))\ndata <- data.frame(Group = group_labels, rbind(data1, data2, data3))\n\n# Compute covariance matrices\nS1 <- cov(data1)\nS2 <- cov(data2)\nS3 <- cov(data3)\n\n# Bartlett's Test for Equal Covariances\nbartlett_test <- bartlett.test(data[,-1], g = data$Group)\nprint(bartlett_test)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  data[, -1]\n#> Bartlett's K-squared = 0.99333, df = 3, p-value = 0.8029\n\n# Box’s M test (alternative for multivariate homogeneity)\nbox_test <- boxM(data[,-1], data$Group)\nprint(box_test)\n#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  data[, -1]\n#> Chi-Sq (approx.) = 51.039, df = 20, p-value = 0.000157"},{"path":"sec-multivariate-methods.html","id":"two-sample-repeated-measures-analysis","chapter":"25 Multivariate Methods","heading":"25.1.7.4 Two-Sample Repeated Measures Analysis","text":"Define \\(\\mathbf{y}_{hi}\\) \\(t\\)-dimensional response vector subject \\(\\) group \\(h\\):\\[\n\\mathbf{y}_{hi} = (y_{hi1}, y_{hi2}, ..., y_{hit})'\n\\]Assume:Group 1: \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1} \\sim N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) (.e., iid common distribution).Group 1: \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1} \\sim N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) (.e., iid common distribution).Group 2: \\(\\mathbf{y}_{21}, ..., \\mathbf{y}_{2n_2} \\sim N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\).Group 2: \\(\\mathbf{y}_{21}, ..., \\mathbf{y}_{2n_2} \\sim N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\).test whether mean response vectors equal across groups:\\[\nH_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c.\n\\]:\\(\\mathbf{C}\\) contrast matrix dimensions \\(c \\times t\\) (rank \\(c\\), \\(c \\leq t\\)).\\(\\mathbf{C}\\) contrast matrix dimensions \\(c \\times t\\) (rank \\(c\\), \\(c \\leq t\\)).\\(H_0\\) true, two groups mean structure.\\(H_0\\) true, two groups mean structure.Hotelling’s \\(T^2\\) statistic repeated measures :\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\mathbf{C}' (\\mathbf{CSC'})^{-1} \\mathbf{C} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2).\n\\]\\(\\mathbf{S}\\) pooled covariance matrix. corresponding F-statistic follows:\\[\nF = \\frac{n_1 + n_2 - c - 1}{(n_1 + n_2 - 2)c} T^2 \\sim f_{(c, n_1 + n_2 - c - 1)}.\n\\]null hypothesis.reject \\(H_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2\\), may test whether profiles parallel:\\[\n\\begin{aligned}\n\\mu_{11} - \\mu_{21} &= \\mu_{12} - \\mu_{22}, \\\\\n&\\vdots \\\\\n\\mu_{1t-1} - \\mu_{2t-1} &= \\mu_{1t} - \\mu_{2t}.\n\\end{aligned}\n\\]expressed :\\[\nH_0: \\mathbf{C}(\\mu_1 - \\mu_2) = \\mathbf{0}_c,\n\\]:\\(c = t - 1\\) (one fewer number time points).contrast matrix \\(\\mathbf{C}\\) :\\[\n\\mathbf{C} =\n\\begin{bmatrix}\n1 & -1 & 0 & \\dots & 0 \\\\\n0 & 1 & -1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & -1\n\\end{bmatrix}_{(t-1) \\times t}.\n\\]One-Sample Hotelling’s \\(T^2\\) TestIf p-value large, fail reject \\(H_0\\) conclude hypothesized mean vector plausible.p-value large, fail reject \\(H_0\\) conclude hypothesized mean vector plausible.p-value small, reject \\(H_0\\) infer sample mean significantly differs hypothesized values.p-value small, reject \\(H_0\\) infer sample mean significantly differs hypothesized values.Paired-Sample Hotelling’s \\(T^2\\) TestUsed subject two sets paired measurements.Reject \\(H_0\\): Measurements two labs significantly differ.Reject \\(H_0\\): Measurements two labs significantly differ.Fail reject \\(H_0\\): significant difference two labs.Fail reject \\(H_0\\): significant difference two labs.Independent-Sample Hotelling’s \\(T^2\\) Test Bartlett’s TestUsed comparing two independent groups.Reject \\(H_0\\): two temperature groups significantly different mean vectors.Reject \\(H_0\\): two temperature groups significantly different mean vectors.Fail reject \\(H_0\\): significant difference groups.Fail reject \\(H_0\\): significant difference groups.Summary Repeated Measures Hypothesis Testing","code":"\n# Load necessary libraries\nlibrary(ICSNP)\nlibrary(dplyr)\n\n# Data: Measurements on 3 variables\nplants <- data.frame(\n    y1 = c(2.11, 2.36, 2.13, 2.78, 2.17),\n    y2 = c(10.1, 35.0, 2.0, 6.0, 2.0),\n    y3 = c(3.4, 4.1, 1.9, 3.8, 1.7)\n)\n\n# Center the data with hypothesized means\nplants_ctr <- plants %>%\n    transmute(y1_ctr = y1 - 2.85,\n              y2_ctr = y2 - 15.0,\n              y3_ctr = y3 - 6.0) %>%\n    as.matrix()\n\n# Perform Wilks' Lambda test for one-sample Hotelling's T^2\nonesamp_fit <- anova(lm(plants_ctr ~ 1), test = \"Wilks\")\nprint(onesamp_fit)\n#> Analysis of Variance Table\n#> \n#>             Df    Wilks approx F num Df den Df  Pr(>F)  \n#> (Intercept)  1 0.054219   11.629      3      2 0.08022 .\n#> Residuals    4                                          \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Data: Commercial vs. State Lab Waste Analysis\nwaste <- data.frame(\n    case = 1:11,\n    com_y1 = c(6, 6, 18, 8, 11, 34, 28, 71, 43, 33, 20),\n    com_y2 = c(27, 23, 64, 44, 30, 75, 26, 124, 54, 30, 14),\n    state_y1 = c(25, 28, 36, 35, 15, 44, 42, 54, 34, 29, 39),\n    state_y2 = c(15, 13, 22, 29, 31, 64, 30, 64, 56, 20, 21)\n)\n\n# Compute differences between commercial and state labs\nwaste_diff <- waste %>%\n    transmute(y1_diff = com_y1 - state_y1,\n              y2_diff = com_y2 - state_y2)\n\n# Perform Paired Hotelling’s T^2 test\npaired_fit <- HotellingsT2(waste_diff)\nprint(paired_fit)\n#> \n#>  Hotelling's one sample T2-test\n#> \n#> data:  waste_diff\n#> T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083\n#> alternative hypothesis: true location is not equal to c(0,0)\n# Read steel strength data\nsteel <- read.table(\"images/steel.dat\")\nnames(steel) <- c(\"Temp\", \"Yield\", \"Strength\")\n\n# Scatter plot of Yield vs Strength\nlibrary(ggplot2)\nggplot(steel, aes(x = Yield, y = Strength)) +\n    geom_text(aes(label = Temp), size = 5) +\n    geom_segment(aes(x = 33, y = 57.5, xend = 42, yend = 65), col = \"red\")\n\n# Bartlett's test for equality of covariances\nbart_test <- boxM(steel[, -1], steel$Temp)\nprint(bart_test)  # If p > 0.05, fail to reject equal covariances\n#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  steel[, -1]\n#> Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n\n# Multivariate analysis of variance (MANOVA) using Wilks' Lambda\ntwosamp_fit <-\n    anova(lm(cbind(Yield, Strength) ~ factor(Temp), data = steel), \n          test = \"Wilks\")\nprint(twosamp_fit)\n#> Analysis of Variance Table\n#> \n#>              Df    Wilks approx F num Df den Df    Pr(>F)    \n#> (Intercept)   1 0.001177   3818.1      2      9 6.589e-14 ***\n#> factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** \n#> Residuals    10                                              \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Independent-Sample Hotelling's T^2 Test\ntwosamp_fit2 <- HotellingsT2(cbind(steel$Yield, steel$Strength) ~ factor(steel$Temp))\nprint(twosamp_fit2)\n#> \n#>  Hotelling's two sample T2-test\n#> \n#> data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)\n#> T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\n#> alternative hypothesis: true location difference is not equal to c(0,0)"},{"path":"sec-multivariate-methods.html","id":"sec-multivariate-analysis-of-variance","chapter":"25 Multivariate Methods","heading":"25.2 Multivariate Analysis of Variance","text":"Multivariate Analysis Variance (MANOVA) extension univariate Analysis Variance (ANOVA) allows researchers examine multiple dependent variables simultaneously. Unlike ANOVA, evaluates differences means single dependent variable across groups, MANOVA assesses whether statistically significant differences among groups across two correlated dependent variables.considering multiple dependent variables , MANOVA accounts interdependencies , reducing likelihood Type errors may arise conducting multiple separate ANOVA tests. particularly useful fields psychology, marketing, social sciences, multiple outcome measures often interrelated.technique commonly applied experimental observational studies researchers seek determine impact categorical independent variables multiple continuous dependent variables.","code":""},{"path":"sec-multivariate-methods.html","id":"one-way-manova","chapter":"25 Multivariate Methods","heading":"25.2.1 One-Way MANOVA","text":"One-way MANOVA extends univariate one-way ANOVA multiple dependent variables. used compare treatment means across \\(h\\) different populations response consists multiple correlated variables.Let populations indexed \\(= 1, 2, \\dots, h\\), observations within population indexed \\(j = 1, 2, \\dots, n_i\\). assume:Population 1: \\(\\mathbf{y}_{11}, \\mathbf{y}_{12}, \\dots, \\mathbf{y}_{1n_1} \\sim \\text{..d. } N_p (\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})\\)\\(\\vdots\\)Population \\(h\\): \\(\\mathbf{y}_{h1}, \\mathbf{y}_{h2}, \\dots, \\mathbf{y}_{hn_h} \\sim \\text{..d. } N_p (\\boldsymbol{\\mu}_h, \\boldsymbol{\\Sigma})\\):\\(\\mathbf{y}_{ij}\\) \\(p\\)-dimensional response vector \\(j\\)th observation \\(\\)th group.\\(\\mathbf{y}_{ij}\\) \\(p\\)-dimensional response vector \\(j\\)th observation \\(\\)th group.\\(\\boldsymbol{\\mu}_i\\) population mean vector \\(\\)th group.\\(\\boldsymbol{\\mu}_i\\) population mean vector \\(\\)th group.\\(\\boldsymbol{\\Sigma}\\) common covariance matrix across groups.\\(\\boldsymbol{\\Sigma}\\) common covariance matrix across groups.AssumptionsIndependence: Observations within across groups independent.Multivariate Normality: population follows \\(p\\)-variate normal distribution.Homogeneity Covariance Matrices: covariance matrix \\(\\boldsymbol{\\Sigma}\\) groups.group \\(\\), can compute:Sample mean vector: \\(\\mathbf{\\bar{y}}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} \\mathbf{y}_{ij}\\)Sample mean vector: \\(\\mathbf{\\bar{y}}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} \\mathbf{y}_{ij}\\)Sample covariance matrix: \\(\\mathbf{S}_i = \\frac{1}{n_i - 1} \\sum_{j=1}^{n_i} (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)(\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)'\\)Sample covariance matrix: \\(\\mathbf{S}_i = \\frac{1}{n_i - 1} \\sum_{j=1}^{n_i} (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)(\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)'\\)Pooled covariance matrix: \\[\n  \\mathbf{S} = \\frac{1}{\\sum_{=1}^{h} (n_i - 1)} \\sum_{=1}^{h} (n_i - 1) \\mathbf{S}_i\n  \\]Pooled covariance matrix: \\[\n  \\mathbf{S} = \\frac{1}{\\sum_{=1}^{h} (n_i - 1)} \\sum_{=1}^{h} (n_i - 1) \\mathbf{S}_i\n  \\]","code":""},{"path":"sec-multivariate-methods.html","id":"effects-model-formulation","chapter":"25 Multivariate Methods","heading":"25.2.1.1 Effects Model Formulation","text":"Similar univariate one-way ANOVA, effects model can written :\\[\n\\boldsymbol{\\mu}_i = \\boldsymbol{\\mu} + \\boldsymbol{\\tau}_i\n\\]:\\(\\boldsymbol{\\mu}_i\\) mean vector group \\(\\).\\(\\boldsymbol{\\mu}_i\\) mean vector group \\(\\).\\(\\boldsymbol{\\mu}\\) overall mean effect.\\(\\boldsymbol{\\mu}\\) overall mean effect.\\(\\boldsymbol{\\tau}_i\\) treatment effect group \\(\\).\\(\\boldsymbol{\\tau}_i\\) treatment effect group \\(\\).observational model :\\[\n\\mathbf{y}_{ij} = \\boldsymbol{\\mu} + \\boldsymbol{\\tau}_i + \\boldsymbol{\\epsilon}_{ij}\n\\]\\(\\boldsymbol{\\epsilon}_{ij} \\sim N_p(\\mathbf{0}, \\boldsymbol{\\Sigma})\\) represents residual variation.Since model overparameterized, impose constraint:\\[\n\\sum_{=1}^h n_i \\boldsymbol{\\tau}_i = \\mathbf{0}\n\\]equivalently, may set \\(\\boldsymbol{\\tau}_h = \\mathbf{0}\\).Analogous univariate ANOVA, total variability partitioned :\\[\n\\sum_{= 1}^h \\sum_{j = 1}^{n_i} (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{y}_{ij} - \\mathbf{\\bar{y}})' =\n\\sum_{= 1}^h n_i (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})' +\n\\sum_{=1}^h \\sum_{j = 1}^{n_i} (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)(\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i)'\n\\]:LHS: Total corrected sums squares cross-products (SSCP) matrix.LHS: Total corrected sums squares cross-products (SSCP) matrix.RHS:\nFirst term: -groups SSCP matrix (denoted \\(\\mathbf{H}\\)).\nSecond term: Within-groups (residual) SSCP matrix (denoted \\(\\mathbf{E}\\)).\nRHS:First term: -groups SSCP matrix (denoted \\(\\mathbf{H}\\)).First term: -groups SSCP matrix (denoted \\(\\mathbf{H}\\)).Second term: Within-groups (residual) SSCP matrix (denoted \\(\\mathbf{E}\\)).Second term: Within-groups (residual) SSCP matrix (denoted \\(\\mathbf{E}\\)).total within-group variation :\\[\n\\mathbf{E} = (n_1 - 1)\\mathbf{S}_1  + \\dots + (n_h -1) \\mathbf{S}_h = (\\sum_{=1}^h n_i - h) \\mathbf{S}\n\\]MANOVA Table","code":""},{"path":"sec-multivariate-methods.html","id":"hypothesis-testing-1","chapter":"25 Multivariate Methods","heading":"25.2.1.1.1 Hypothesis Testing","text":"null hypothesis states:\\[\nH_0: \\boldsymbol{\\tau}_1 = \\boldsymbol{\\tau}_2 = \\dots = \\boldsymbol{\\tau}_h = \\mathbf{0}\n\\]implies group mean vectors equal.test \\(H_0\\), assess relative sizes \\(\\mathbf{E}\\) \\(\\mathbf{H + E}\\) using Wilks’ Lambda:Wilks’ Lambda defined :\\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H + E}|}\n\\]Properties:univariate case, Wilks’ Lambda reduces F-statistic.exact distribution \\(\\Lambda^*\\) known special cases.large samples, reject \\(H_0\\) :\\[\n-\\left( \\sum_{=1}^h n_i - 1 - \\frac{p+h}{2} \\right) \\log(\\Lambda^*) > \\chi^2_{(1-\\alpha, p(h-1))}\n\\]","code":"\n# Load dataset\ndata(iris)\n\n# Fit MANOVA model\nmanova_fit <-\n    manova(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Species,\n           data = iris)\n\n# Summary of the MANOVA test\nsummary(manova_fit, test = \"Wilks\")\n#>            Df    Wilks approx F num Df den Df    Pr(>F)    \n#> Species     2 0.023439   199.15      8    288 < 2.2e-16 ***\n#> Residuals 147                                              \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-multivariate-methods.html","id":"testing-general-hypotheses-in-manova","chapter":"25 Multivariate Methods","heading":"25.2.1.2 Testing General Hypotheses in MANOVA","text":"consider \\(h\\) different treatments, \\(\\)-th treatment applied \\(n_i\\) subjects, observed \\(p\\) repeated measures. results \\(p\\)-dimensional observation vector subject random sample \\(h\\) different treatment populations.general MANOVA model can written :\\[\n\\mathbf{y}_{ij} = \\boldsymbol{\\mu} + \\boldsymbol{\\tau}_i + \\boldsymbol{\\epsilon}_{ij}, \\quad = 1, \\dots, h; \\quad j = 1, \\dots, n_i\n\\]Equivalently, matrix notation:\\[\n\\mathbf{Y} = \\mathbf{XB} + \\boldsymbol{\\epsilon}\n\\]:\\(\\mathbf{Y}_{(n \\times p)}\\) matrix response variables.\\(\\mathbf{Y}_{(n \\times p)}\\) matrix response variables.\\(\\mathbf{X}_{(n \\times h)}\\) design matrix. - \\(\\mathbf{B}_{(h \\times p)}\\) contains treatment effects.\\(\\mathbf{X}_{(n \\times h)}\\) design matrix. - \\(\\mathbf{B}_{(h \\times p)}\\) contains treatment effects.\\(\\boldsymbol{\\epsilon}_{(n \\times p)}\\) error matrix.\\(\\boldsymbol{\\epsilon}_{(n \\times p)}\\) error matrix.response matrix:\\[\n\\mathbf{Y}_{(n \\times p)} =\n\\begin{bmatrix}\n\\mathbf{y}_{11}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{1n_1}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{hn_h}'\n\\end{bmatrix}\n\\]coefficient matrix:\\[\n\\mathbf{B}_{(h \\times p)} =\n\\begin{bmatrix}\n\\boldsymbol{\\mu}' \\\\\n\\boldsymbol{\\tau}_1' \\\\\n\\vdots \\\\\n\\boldsymbol{\\tau}_{h-1}'\n\\end{bmatrix}\n\\]error matrix:\\[\n\\boldsymbol{\\epsilon}_{(n \\times p)} =\n\\begin{bmatrix}\n\\boldsymbol{\\epsilon}_{11}' \\\\\n\\vdots \\\\\n\\boldsymbol{\\epsilon}_{1n_1}' \\\\\n\\vdots \\\\\n\\boldsymbol{\\epsilon}_{hn_h}'\n\\end{bmatrix}\n\\]design matrix \\(\\mathbf{X}\\) encodes treatment assignments:\\[\n\\mathbf{X}_{(n \\times h)} =\n\\begin{bmatrix}\n1 & 1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n1 & 1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n1 & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 0 & 0 & \\dots & 0\n\\end{bmatrix}\n\\]","code":""},{"path":"sec-multivariate-methods.html","id":"estimation-1","chapter":"25 Multivariate Methods","heading":"25.2.1.2.1 Estimation","text":"least squares estimate \\(\\mathbf{B}\\) given :\\[\n\\hat{\\mathbf{B}} = (\\mathbf{X'X})^{-1} \\mathbf{X'Y}\n\\]Since rows \\(\\mathbf{Y}\\) independent, assume:\\[\n\\operatorname{Var}(\\mathbf{Y}) = \\mathbf{}_n \\otimes \\boldsymbol{\\Sigma}\n\\]\\(\\otimes\\) denotes Kronecker product, resulting \\(np \\times np\\) covariance matrix.","code":""},{"path":"sec-multivariate-methods.html","id":"hypothesis-testing-2","chapter":"25 Multivariate Methods","heading":"25.2.1.2.2 Hypothesis Testing","text":"general hypothesis MANOVA can written :\\[\n\\begin{aligned}\n&H_0: \\mathbf{LBM} = 0 \\\\\n&H_a: \\mathbf{LBM} \\neq 0\n\\end{aligned}\n\\]:\\(\\mathbf{L}\\) \\((g \\times h)\\) matrix full row rank (\\(g \\le h\\)), specifying comparisons across groups.\\(\\mathbf{L}\\) \\((g \\times h)\\) matrix full row rank (\\(g \\le h\\)), specifying comparisons across groups.\\(\\mathbf{M}\\) \\((p \\times u)\\) matrix full column rank (\\(u \\le p\\)), specifying comparisons across traits.\\(\\mathbf{M}\\) \\((p \\times u)\\) matrix full column rank (\\(u \\le p\\)), specifying comparisons across traits.evaluate effect treatments MANOVA framework, compute treatment corrected sums squares cross-product (SSCP) matrix:\\[\n\\mathbf{H} = \\mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}\n\\]testing null hypothesis:\\[\nH_0: \\mathbf{LBM} = \\mathbf{D}\n\\]corresponding SSCP matrix :\\[\n\\mathbf{H} = (\\mathbf{\\hat{LBM}} - \\mathbf{D})'[\\mathbf{X(X'X)^{-1}L}]^{-1}(\\mathbf{\\hat{LBM}} - \\mathbf{D})\n\\]Similarly, residual (error) SSCP matrix given :\\[\n\\mathbf{E} = \\mathbf{M'Y'[- X(X'X)^{-1}X']Y M}\n\\]can also expressed :\\[\n\\mathbf{E} = \\mathbf{M'[Y'Y - \\hat{B}'(X'X)^{-1} \\hat{B}]M}\n\\]matrices, \\(\\mathbf{H}\\) \\(\\mathbf{E}\\), serve basis assessing relative treatment effect multivariate setting.","code":""},{"path":"sec-multivariate-methods.html","id":"test-statistics-in-manova","chapter":"25 Multivariate Methods","heading":"25.2.1.2.3 Test Statistics in MANOVA","text":"test whether treatment effects significantly impact multivariate response, examine eigenvalues \\(\\mathbf{}^{-1}\\), leading several common test statistics:Wilks’ Lambda: \\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\n\\] smaller \\(\\Lambda^*\\) indicates greater difference among group mean vectors. degrees freedom depend ranks \\(\\mathbf{L}, \\mathbf{M},\\) \\(\\mathbf{X}\\).Wilks’ Lambda: \\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\n\\] smaller \\(\\Lambda^*\\) indicates greater difference among group mean vectors. degrees freedom depend ranks \\(\\mathbf{L}, \\mathbf{M},\\) \\(\\mathbf{X}\\).Lawley-Hotelling Trace: \\[\nU = \\operatorname{tr}(\\mathbf{}^{-1})\n\\] statistic sums eigenvalues \\(\\mathbf{}^{-1}\\), capturing overall treatment effect.Lawley-Hotelling Trace: \\[\nU = \\operatorname{tr}(\\mathbf{}^{-1})\n\\] statistic sums eigenvalues \\(\\mathbf{}^{-1}\\), capturing overall treatment effect.Pillai’s Trace: \\[\nV = \\operatorname{tr}(\\mathbf{H}(\\mathbf{H} + \\mathbf{E})^{-1})\n\\] test known robustness violations MANOVA assumptions.Pillai’s Trace: \\[\nV = \\operatorname{tr}(\\mathbf{H}(\\mathbf{H} + \\mathbf{E})^{-1})\n\\] test known robustness violations MANOVA assumptions.Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\). focuses strongest treatment effect present data.Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\). focuses strongest treatment effect present data.large \\(n\\), \\(H_0\\):\\[\n-\\left(n - 1 - \\frac{p + h}{2} \\right) \\ln \\Lambda^* \\sim \\chi^2_{p(h-1)}\n\\]certain cases, specific values \\(p\\) \\(h\\) allow exact \\(F\\)-distribution \\(H_0\\).Since Wilks’ Lambda test results small \\(p\\)-value, reject null hypothesis equal multivariate mean vectors among three admission groups.Repeated Measures MANOVAChoosing Correct ModelIf time (three levels) treated independent variable, use univariate ANOVA (requires sphericity assumption, meaning variances differences must equal).time (three levels) treated independent variable, use univariate ANOVA (requires sphericity assumption, meaning variances differences must equal).time point treated separate variable, use MANOVA (require sphericity assumption).time point treated separate variable, use MANOVA (require sphericity assumption).results indicate reject null hypothesis sphericity, meaning univariate ANOVA also appropriate. linear time effect significant, quadratic time effect .Polynomial Contrasts Time EffectsTo explore effect time, examine polynomial contrasts.results confirm significant linear trend time quadratic trend.MANOVA Drug TreatmentsWe now analyze multivariate response different drug treatments.Since obtain small \\(p\\)-value, reject null hypothesis difference means treatments.Contrasts Treatment GroupsTo investigate group differences, define contrast matrices.Hypothesis Testing Contrast MatricesInstead setting contrasts heart$drug, use contrast matrix \\(\\mathbf{M}\\)Comparing Drug bww9 vs ControlSince \\(p\\)-value significant, conclude significant difference control bww9 drug treatment.Comparing Drug ax23 vs RestSince obtain significant \\(p\\)-value, conclude ax23 drug treatment significantly differs rest treatments.","code":"\n## One-Way MANOVA\n\nlibrary(car)\nlibrary(emmeans)\nlibrary(profileR)\nlibrary(tidyverse)\n\n# Read in the data\ngpagmat <- read.table(\"images/gpagmat.dat\")\n\n# Change the variable names\nnames(gpagmat) <- c(\"y1\", \"y2\", \"admit\")\n\n# Check the structure of the dataset\nstr(gpagmat)\n#> 'data.frame':    85 obs. of  3 variables:\n#>  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...\n#>  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...\n#>  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...\n\n# Plot the data\ngg <- ggplot(gpagmat, aes(x = y1, y = y2)) +\n    geom_text(aes(label = admit, col = as.character(admit))) +\n    scale_color_discrete(name = \"Admission\",\n                         labels = c(\"Admit\", \"Do not admit\", \"Borderline\")) +\n    scale_x_continuous(name = \"GPA\") +\n    scale_y_continuous(name = \"GMAT\")\n\n# Fit a one-way MANOVA model\noneway_fit <- manova(cbind(y1, y2) ~ admit, data = gpagmat)\n\n# MANOVA test using Wilks' Lambda\nsummary(oneway_fit, test = \"Wilks\")\n#>           Df  Wilks approx F num Df den Df    Pr(>F)    \n#> admit      1 0.6126   25.927      2     82 1.881e-09 ***\n#> Residuals 83                                            \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Create dataset for repeated measures example\nstress <- data.frame(\n    subject = 1:8,\n    begin = c(3, 2, 5, 6, 1, 5, 1, 5),\n    middle = c(3, 4, 3, 7, 4, 7, 1, 2),\n    final = c(6, 7, 4, 7, 6, 7, 3, 5)\n)\n# Fit the MANOVA model for repeated measures\nstress_mod <- lm(cbind(begin, middle, final) ~ 1, data = stress)\n\n# Define the within-subject factor\nidata <- data.frame(time = factor(\n    c(\"begin\", \"middle\", \"final\"),\n    levels = c(\"begin\", \"middle\", \"final\")\n))\n\n# Perform repeated measures MANOVA\nrepeat_fit <- Anova(\n    stress_mod,\n    idata = idata,\n    idesign = ~ time,\n    icontrasts = \"contr.poly\"\n)\n\n# Summarize results\nsummary(repeat_fit) \n#> \n#> Type III Repeated Measures MANOVA Tests:\n#> \n#> ------------------------------------------\n#>  \n#> Term: (Intercept) \n#> \n#>  Response transformation matrix:\n#>        (Intercept)\n#> begin            1\n#> middle           1\n#> final            1\n#> \n#> Sum of squares and products for the hypothesis:\n#>             (Intercept)\n#> (Intercept)        1352\n#> \n#> Multivariate Tests: (Intercept)\n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.896552 60.66667      1      7 0.00010808 ***\n#> Wilks             1  0.103448 60.66667      1      7 0.00010808 ***\n#> Hotelling-Lawley  1  8.666667 60.66667      1      7 0.00010808 ***\n#> Roy               1  8.666667 60.66667      1      7 0.00010808 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> ------------------------------------------\n#>  \n#> Term: time \n#> \n#>  Response transformation matrix:\n#>               time.L     time.Q\n#> begin  -7.071068e-01  0.4082483\n#> middle -7.850462e-17 -0.8164966\n#> final   7.071068e-01  0.4082483\n#> \n#> Sum of squares and products for the hypothesis:\n#>           time.L   time.Q\n#> time.L 18.062500 6.747781\n#> time.Q  6.747781 2.520833\n#> \n#> Multivariate Tests: time\n#>                  Df test stat approx F num Df den Df   Pr(>F)  \n#> Pillai            1 0.7080717 7.276498      2      6 0.024879 *\n#> Wilks             1 0.2919283 7.276498      2      6 0.024879 *\n#> Hotelling-Lawley  1 2.4254992 7.276498      2      6 0.024879 *\n#> Roy               1 2.4254992 7.276498      2      6 0.024879 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n#> \n#>             Sum Sq num Df Error SS den Df F value    Pr(>F)    \n#> (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***\n#> time         20.58      2    24.75     14  5.8215 0.0144578 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> \n#> Mauchly Tests for Sphericity\n#> \n#>      Test statistic p-value\n#> time         0.7085 0.35565\n#> \n#> \n#> Greenhouse-Geisser and Huynh-Feldt Corrections\n#>  for Departure from Sphericity\n#> \n#>       GG eps Pr(>F[GG])  \n#> time 0.77429    0.02439 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>         HF eps Pr(>F[HF])\n#> time 0.9528433 0.01611634\n# Check the reference for the marginal means\nref_grid(stress_mod, mult.name = \"time\")\n#> 'emmGrid' object with variables:\n#>     1 = 1\n#>     time = multivariate response levels: begin, middle, final\n\n# Compute marginal means for time levels\ncontr_means <- emmeans(stress_mod, ~ time, mult.name = \"time\")\n\n# Test for polynomial trends\ncontrast(contr_means, method = \"poly\")\n#>  contrast  estimate    SE df t.ratio p.value\n#>  linear        2.12 0.766  7   2.773  0.0276\n#>  quadratic     1.38 0.944  7   1.457  0.1885\n# Read in the dataset\nheart <- read.table(\"images/heart.dat\")\n\n# Assign variable names\nnames(heart) <- c(\"drug\", \"y1\", \"y2\", \"y3\", \"y4\")\n\n# Create a subject ID nested within drug groups\nheart <- heart %>%\n    group_by(drug) %>%\n    mutate(subject = row_number()) %>%\n    ungroup()\n\n# Check dataset structure\nstr(heart)\n#> tibble [24 × 6] (S3: tbl_df/tbl/data.frame)\n#>  $ drug   : chr [1:24] \"ax23\" \"ax23\" \"ax23\" \"ax23\" ...\n#>  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...\n#>  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...\n#>  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...\n#>  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...\n#>  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...\n\n# Create means summary for a profile plot\nheart_means <- heart %>%\n    group_by(drug) %>%\n    summarize_at(vars(starts_with(\"y\")), mean) %>%\n    ungroup() %>%\n    pivot_longer(-drug, names_to = \"time\", values_to = \"mean\") %>%\n    mutate(time = as.numeric(as.factor(time)))\n\n# Generate the profile plot\ngg_profile <- ggplot(heart_means, aes(x = time, y = mean)) +\n    geom_line(aes(col = drug)) +\n    geom_point(aes(col = drug)) +\n    ggtitle(\"Profile Plot\") +\n    scale_y_continuous(name = \"Response\") +\n    scale_x_discrete(name = \"Time\")\n\ngg_profile\n\n# Fit the MANOVA model\nheart_mod <- lm(cbind(y1, y2, y3, y4) ~ drug, data = heart)\n\n# Perform MANOVA test\nman_fit <- car::Anova(heart_mod)\n\n# Summarize results\nsummary(man_fit)\n#> \n#> Type II MANOVA Tests:\n#> \n#> Sum of squares and products for error:\n#>        y1      y2      y3     y4\n#> y1 641.00 601.750 535.250 426.00\n#> y2 601.75 823.875 615.500 534.25\n#> y3 535.25 615.500 655.875 555.25\n#> y4 426.00 534.250 555.250 674.50\n#> \n#> ------------------------------------------\n#>  \n#> Term: drug \n#> \n#> Sum of squares and products for the hypothesis:\n#>        y1       y2       y3    y4\n#> y1 567.00 335.2500  42.7500 387.0\n#> y2 335.25 569.0833 404.5417 367.5\n#> y3  42.75 404.5417 391.0833 171.0\n#> y4 387.00 367.5000 171.0000 316.0\n#> \n#> Multivariate Tests: drug\n#>                  Df test stat  approx F num Df den Df     Pr(>F)    \n#> Pillai            2  1.283456  8.508082      8     38 1.5010e-06 ***\n#> Wilks             2  0.079007 11.509581      8     36 6.3081e-08 ***\n#> Hotelling-Lawley  2  7.069384 15.022441      8     34 3.9048e-09 ***\n#> Roy               2  6.346509 30.145916      4     19 5.4493e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Convert drug variable to a factor\nheart$drug <- factor(heart$drug)\n\n# Define contrast matrix L\nL <- matrix(c(0, 2,\n              1, -1,-1, -1), nrow = 3, byrow = TRUE)\n\ncolnames(L) <- c(\"bww9:ctrl\", \"ax23:rest\")\nrownames(L) <- unique(heart$drug)\n\n# Assign contrasts\ncontrasts(heart$drug) <- L\ncontrasts(heart$drug)\n#>      bww9:ctrl ax23:rest\n#> ax23         0         2\n#> bww9         1        -1\n#> ctrl        -1        -1\n# Define contrast matrix M for further testing\nM <- matrix(c(1, -1, 0, 0,\n              0, 1, -1, 0,\n              0, 0, 1, -1), nrow = 4)\n\n# Update model for contrast testing\nheart_mod2 <- update(heart_mod)\n\n# Display model coefficients\ncoef(heart_mod2)\n#>                  y1         y2        y3    y4\n#> (Intercept)   75.00 78.9583333 77.041667 74.75\n#> drugbww9:ctrl  4.50  5.8125000  3.562500  4.25\n#> drugax23:rest -2.25  0.7708333  1.979167 -0.75\nbww9vctrl <-\n    car::linearHypothesis(heart_mod2,\n                          hypothesis.matrix = c(0, 1, 0),\n                          P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\n\nbww9vctrl <-\n    car::linearHypothesis(heart_mod,\n                          hypothesis.matrix = c(0, 1,-1),\n                          P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\naxx23vrest <-\n    car::linearHypothesis(heart_mod2,\n                          hypothesis.matrix = c(0, 0, 1),\n                          P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  438.0208  175.20833 -395.7292\n#> [2,]  175.2083   70.08333 -158.2917\n#> [3,] -395.7292 -158.29167  357.5208\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.855364 37.45483      3     19 3.5484e-08 ***\n#> Wilks             1  0.144636 37.45483      3     19 3.5484e-08 ***\n#> Hotelling-Lawley  1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> Roy               1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naxx23vrest <-\n    car::linearHypothesis(heart_mod,\n                          hypothesis.matrix = c(2,-1, 1),\n                          P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  402.5208  127.41667 -390.9375\n#> [2,]  127.4167   40.33333 -123.7500\n#> [3,] -390.9375 -123.75000  379.6875\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.842450 33.86563      3     19 7.9422e-08 ***\n#> Wilks             1  0.157550 33.86563      3     19 7.9422e-08 ***\n#> Hotelling-Lawley  1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> Roy               1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-multivariate-methods.html","id":"sec-profile-analysis","chapter":"25 Multivariate Methods","heading":"25.2.2 Profile Analysis","text":"Profile analysis multivariate extension repeated measures ANOVA used examine similarities treatment effects longitudinal data. null hypothesis states treatments average effect:\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_h\n\\]equivalent testing:\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h\n\\]analysis helps determine exact nature similarities differences treatments sequentially testing following hypotheses:profiles parallel? interaction treatment time.profiles coincidental? Identical profiles across treatments.profiles horizontal? differences across time points.parallelism assumption rejected, can investigate:Differences among groups specific time points.Differences among groups specific time points.Differences among time points within particular group.Differences among time points within particular group.Differences within subsets time points across groups.Differences within subsets time points across groups.Example: Profile Analysis SetupConsider example :4 time points (\\(p = 4\\))4 time points (\\(p = 4\\))3 treatments (\\(h = 3\\))3 treatments (\\(h = 3\\))analyze whether profiles treatment group identical except mean shift (.e., parallel).","code":""},{"path":"sec-multivariate-methods.html","id":"sec-parallel-profile-hypothesis","chapter":"25 Multivariate Methods","heading":"25.2.2.1 Parallel Profile Hypothesis","text":"test whether differences treatment means remain constant across time:\\[\n\\begin{aligned}\nH_0: \\mu_{11} - \\mu_{21} &= \\mu_{12} - \\mu_{22} = \\dots = \\mu_{1t} - \\mu_{2t} \\\\\n\\mu_{11} - \\mu_{31} &= \\mu_{12} - \\mu_{32} = \\dots = \\mu_{1t} - \\mu_{3t} \\\\\n&\\vdots\n\\end{aligned}\n\\]\\(h-1\\) equations. Equivalently, matrix form:\\[\nH_0: \\mathbf{LBM} = 0\n\\]cell means parameterization \\(\\mathbf{B}\\) :\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{ccc}\n1 & -1 & 0 \\\\\n1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n\\mu_{11} & \\dots & \\mu_{14} \\\\\n\\mu_{21} & \\dots & \\mu_{24} \\\\\n\\mu_{31} & \\dots & \\mu_{34}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \\mathbf{0}\n\\]:first matrix compares treatments time point.first matrix compares treatments time point.second matrix contains mean responses treatment across time.second matrix contains mean responses treatment across time.third matrix compares time points within treatment.third matrix compares time points within treatment.first multiplication, \\(\\mathbf{LB}\\), results :\\[\n\\left[\n\\begin{array}\n{cccc}\n\\mu_{11} - \\mu_{21} & \\mu_{12} - \\mu_{22} & \\mu_{13} - \\mu_{23} & \\mu_{14} - \\mu_{24}\\\\\n\\mu_{11} - \\mu_{31} & \\mu_{12} - \\mu_{32} & \\mu_{13} - \\mu_{33} & \\mu_{14} - \\mu_{34}\n\\end{array}\n\\right]\n\\]represents differences treatment means time point.Multiplying \\(\\mathbf{M}\\) compares differences across time:\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{21}) - (\\mu_{12} - \\mu_{22}) & (\\mu_{11} - \\mu_{21}) - (\\mu_{13} - \\mu_{23}) & (\\mu_{11} - \\mu_{21}) - (\\mu_{14} - \\mu_{24}) \\\\\n(\\mu_{11} - \\mu_{31}) - (\\mu_{12} - \\mu_{32}) & (\\mu_{11} - \\mu_{31}) - (\\mu_{13} - \\mu_{33}) & (\\mu_{11} - \\mu_{31}) - (\\mu_{14} - \\mu_{34})\n\\end{array}\n\\right]\n\\]matrix captures changes treatment differences time.Instead using cell means parameterization, can express model terms effects:\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{cccc}\n0 & 1 & -1 & 0 \\\\\n0 & 1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\mu' \\\\\n\\tau'_1 \\\\\n\\tau_2' \\\\\n\\tau_3'\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \\mathbf{0}\n\\]:first matrix compares treatment effects.first matrix compares treatment effects.second matrix contains overall mean treatment effects.second matrix contains overall mean treatment effects.third matrix compares time points within treatment.third matrix compares time points within treatment.parameterizations:\\(\\operatorname{rank}(\\mathbf{L}) = h-1\\)\\(\\operatorname{rank}(\\mathbf{L}) = h-1\\)\\(\\operatorname{rank}(\\mathbf{M}) = p-1\\)\\(\\operatorname{rank}(\\mathbf{M}) = p-1\\)indicating testing differences across treatments (\\(\\mathbf{L}\\)) time points (\\(\\mathbf{M}\\)).Different choices \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) still lead hypothesis:\\[\n\\mathbf{L} =\n\\left[\n\\begin{array}\n{cccc}\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} =\n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]choice yields profile comparison structure.","code":""},{"path":"sec-multivariate-methods.html","id":"sec-coincidental-profiles","chapter":"25 Multivariate Methods","heading":"25.2.2.2 Coincidental Profiles","text":"establish profiles parallel (.e., fail reject parallel profile test), next ask:profiles identical?sums components \\(\\boldsymbol{\\mu}_i\\) equal across treatments, profiles coincidental (.e., identical across groups).Hypothesis Coincidental Profiles\\[\nH_0: \\mathbf{1'}_p \\boldsymbol{\\mu}_1 = \\mathbf{1'}_p \\boldsymbol{\\mu}_2 = \\dots = \\mathbf{1'}_p \\boldsymbol{\\mu}_h\n\\]Equivalently, matrix notation:\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\], cell means parameterization:\\[\n\\mathbf{L} =\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\\[\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & 1 & 1 & 1\n\\end{bmatrix}'\n\\]Multiplying \\(\\mathbf{LBM}\\) yields:\\[\n\\begin{bmatrix}\n(\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\\\\n(\\mu_{21} + \\mu_{22} + \\mu_{23} + \\mu_{24}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}\n\\]Thus, rejecting hypothesis suggests treatments identical effects time.previous cases, different choices \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) can yield result.","code":""},{"path":"sec-multivariate-methods.html","id":"sec-horizontal-profiles","chapter":"25 Multivariate Methods","heading":"25.2.2.3 Horizontal Profiles","text":"fail reject null hypothesis \\(h\\) profiles (.e., confirm parallel coincidental profiles), ask:elements common profile equal across time?question tests whether profiles horizontal, meaning differences time points.Hypothesis Horizontal Profiles\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]\\[\n\\mathbf{L} =\n\\begin{bmatrix}\n1 & 0 & 0\n\\end{bmatrix}\n\\]\\[\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{bmatrix}\n\\]Multiplication yields:\\[\n\\begin{bmatrix}\n(\\mu_{11} - \\mu_{12}) & (\\mu_{12} - \\mu_{13}) & (\\mu_{13} - \\mu_{14})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 0 & 0\n\\end{bmatrix}\n\\]Thus, rejecting hypothesis suggests least one time point differs significantly others common profile.","code":""},{"path":"sec-multivariate-methods.html","id":"summary-of-profile-tests","chapter":"25 Multivariate Methods","heading":"25.2.2.4 Summary of Profile Tests","text":"fail reject three hypotheses, conclude :significant differences treatments.significant differences treatments.significant differences time points.significant differences time points.three profile tests correspond well-known univariate ANOVA components:","code":"\n# Profile analysis for heart dataset\nprofile_fit <- pbg(\n    data = as.matrix(heart[, 2:5]),   # Response variables\n    group = as.matrix(heart[, 1]),    # Treatment group\n    original.names = TRUE,            # Keep original variable names\n    profile.plot = FALSE              # Disable automatic plotting\n)\n\n# Summary of profile analysis results\nsummary(profile_fit)\n#> Call:\n#> pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, \n#>     1]), original.names = TRUE, profile.plot = FALSE)\n#> \n#> Hypothesis Tests:\n#> $`Ho: Profiles are parallel`\n#>   Multivariate.Test Statistic  Approx.F num.df den.df      p.value\n#> 1             Wilks 0.1102861 12.737599      6     38 7.891497e-08\n#> 2            Pillai 1.0891707  7.972007      6     40 1.092397e-05\n#> 3  Hotelling-Lawley 6.2587852 18.776356      6     36 9.258571e-10\n#> 4               Roy 5.9550887 39.700592      3     20 1.302458e-08\n#> \n#> $`Ho: Profiles have equal levels`\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> group        2  328.7  164.35   5.918 0.00915 **\n#> Residuals   21  583.2   27.77                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> $`Ho: Profiles are flat`\n#>          F df1 df2      p-value\n#> 1 14.30928   3  19 4.096803e-05"},{"path":"sec-multivariate-methods.html","id":"statistical-test-selection-for-comparing-means","chapter":"25 Multivariate Methods","heading":"25.3 Statistical Test Selection for Comparing Means","text":"","code":""},{"path":"sec-quasi-experimental.html","id":"sec-quasi-experimental","chapter":"26 Quasi-Experimental Methods","heading":"26 Quasi-Experimental Methods","text":"Quasi-experimental methods widely used causal inference randomized experiments feasible. Typically, methods rely pre- post-intervention data attempt identify exogenous variation can leveraged estimate causal effects.internal validity ensures credible causal effect, external validity assesses whether findings generalize beyond sample.Key considerations:Representativeness SampleLimitations DesignUsing Quasi-Experimental Results Structural Models\nSee (J. E. Anderson, Larch, Yotov 2015), (Einav, Finkelstein, Levin 2010), (Chung, Steenburgh, Sudhir 2014) applications.\nSee (J. E. Anderson, Larch, Yotov 2015), (Einav, Finkelstein, Levin 2010), (Chung, Steenburgh, Sudhir 2014) applications.","code":""},{"path":"sec-quasi-experimental.html","id":"identification-strategy-in-quasi-experiments","chapter":"26 Quasi-Experimental Methods","heading":"26.1 Identification Strategy in Quasi-Experiments","text":"Unlike randomized experiments, quasi-experiments lack formal statistical proof causality. Instead, researchers must build plausible argument supported empirical evidence.Key components identification strategy:Source Exogenous Variation\nJustify exogenous variation originates.\nUse institutional knowledge theoretical arguments support claim.\nJustify exogenous variation originates.Use institutional knowledge theoretical arguments support claim.Exclusion Restriction\nProvide evidence variation exogenous shock affects outcome proposed mechanism.\nrequires ruling confounding factors.\nProvide evidence variation exogenous shock affects outcome proposed mechanism.requires ruling confounding factors.Stable Unit Treatment Value Assumption\ntreatment unit \\(\\) affect outcome unit \\(\\).\nspillovers interference treatment control groups.\ntreatment unit \\(\\) affect outcome unit \\(\\).spillovers interference treatment control groups.Every quasi-experimental method involves tradeoff statistical power support exogeneity assumption. means researchers often discard variation data meet exogeneity assumption.Important Notes:\\(R^2\\) reliable metric causal inference can misleading model comparison (Ebbes, Papies, Van Heerde 2011).\\(R^2\\) reliable metric causal inference can misleading model comparison (Ebbes, Papies, Van Heerde 2011).Clustering determined based study design, just expectations correlation (Abadie et al. 2023).Clustering determined based study design, just expectations correlation (Abadie et al. 2023).small samples, use wild bootstrap procedure correct downward bias (Cameron, Gelbach, Miller 2008). See also (Cai et al. 2022) assumptions.small samples, use wild bootstrap procedure correct downward bias (Cameron, Gelbach, Miller 2008). See also (Cai et al. 2022) assumptions.","code":""},{"path":"sec-quasi-experimental.html","id":"robustness-checks","chapter":"26 Quasi-Experimental Methods","heading":"26.2 Robustness Checks","text":"Robustness checks essential demonstrate findings driven model specification choices.Recommended robustness checks (Goldfarb, Tucker, Wang 2022):Alternative Control Sets\nShow results without controls.\nExamine estimate interest changes.\nUse Rosenbaum bounds formal sensitivity analysis (Altonji, Elder, Taber 2005).\nmarketing applications, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012).\nShow results without controls.Examine estimate interest changes.Use Rosenbaum bounds formal sensitivity analysis (Altonji, Elder, Taber 2005).marketing applications, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012).Different Functional Forms\nCheck whether results hold different model specifications (e.g., linear vs. non-linear models).\nCheck whether results hold different model specifications (e.g., linear vs. non-linear models).Varying Time Windows\nlongitudinal settings, test different time frames ensure robustness.\nlongitudinal settings, test different time frames ensure robustness.Alternative Dependent Variables\nUse related outcomes different measures dependent variable.\nUse related outcomes different measures dependent variable.Varying Control Group Size\nCompare results using matched vs. unmatched samples assess sensitivity sample selection.\nCompare results using matched vs. unmatched samples assess sensitivity sample selection.Placebo Tests\nConduct placebo tests ensure effect spurious.\nappropriate placebo test depends specific quasi-experimental method used (examples provided later sections).\nConduct placebo tests ensure effect spurious.appropriate placebo test depends specific quasi-experimental method used (examples provided later sections).","code":""},{"path":"sec-quasi-experimental.html","id":"establishing-mechanisms","chapter":"26 Quasi-Experimental Methods","heading":"26.3 Establishing Mechanisms","text":"causal effect established, next step investigate effect operates.Mediation Analysis\nTest whether intermediate variable explains effect treatment.\nTest whether intermediate variable explains effect treatment.Moderation Analysis\nEstimate model separately different subgroups.\nTest three-way interactions (e.g., interaction treatment, time, group membership Difference--Differences settings).\nEstimate model separately different subgroups.Test three-way interactions (e.g., interaction treatment, time, group membership Difference--Differences settings).","code":""},{"path":"sec-quasi-experimental.html","id":"limitations-of-quasi-experiments","chapter":"26 Quasi-Experimental Methods","heading":"26.4 Limitations of Quasi-Experiments","text":"Researchers explicitly discuss limitations quasi-experimental approach.Key Questions Address:identification assumptions?\nClearly state assumptions required causal inference.\nClearly state assumptions required causal inference.threats validity?\nConsider potential confounders, measurement errors, violations SUTVA.\nConsider potential confounders, measurement errors, violations SUTVA.address threats?\nDescribe robustness checks alternative specifications.\nSuggest directions future research improve causal identification.\nDescribe robustness checks alternative specifications.Suggest directions future research improve causal identification.","code":""},{"path":"sec-quasi-experimental.html","id":"assumptions-for-identifying-treatment-effects","chapter":"26 Quasi-Experimental Methods","heading":"26.5 Assumptions for Identifying Treatment Effects","text":"identify causal effects non-randomized studies, rely three key assumptions:Stable Unit Treatment Value Assumption (SUTVA)Conditional Ignorability (Unconfoundedness) AssumptionOverlap (Positivity) AssumptionThese assumptions ensure can properly define estimate causal effects, mitigating biases confounders selection effects.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-sutva","chapter":"26 Quasi-Experimental Methods","heading":"26.5.1 Stable Unit Treatment Value Assumption","text":"SUTVA consists two key components:Consistency Assumption: treatment indicator \\(Z \\\\{0,1\\}\\) adequately represents versions treatment.Interference Assumption: subject’s outcome depends treatment status affected treatment assignments subjects.assumption ensures potential outcomes well-defined independent external influences, forming foundation Rubin’s Causal Model (RCM). Violations SUTVA can lead biased estimators incorrect standard errors.Let \\(Y_i(Z)\\) denote potential outcome unit \\(\\) treatment assignment \\(Z\\), \\(Z \\\\{0,1\\}\\) represents binary treatment.SUTVA states :\\[\nY_i(Z) = Y_i(Z, \\mathbf{Z}_{-})\n\\]\\(\\mathbf{Z}_{-}\\) denotes treatment assignments units except \\(\\). SUTVA holds, :\\[\nY_i(Z) = Y_i(Z, \\mathbf{Z}_{-}) \\quad \\forall \\mathbf{Z}_{-}.\n\\]implies unit \\(\\)’s outcome depends treatment status unaffected treatment others.","code":""},{"path":"sec-quasi-experimental.html","id":"implications-of-sutva","chapter":"26 Quasi-Experimental Methods","heading":"26.5.1.1 Implications of SUTVA","text":"SUTVA holds, Average Treatment Effect well-defined :\\[\n\\text{ATE} = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)].\n\\]However, SUTVA violated, standard causal inference methods may fail. Common violations include:Interference (Spillover Effects): treatment one unit influences another’s outcome.\nExample: marketing campaign product influences treated untreated customers word--mouth effects.\nSolution: Use spatial econometrics network-based causal inference models account spillovers.\nExample: marketing campaign product influences treated untreated customers word--mouth effects.Solution: Use spatial econometrics network-based causal inference models account spillovers.Treatment Inconsistency: Multiple versions treatment exist explicitly modeled.\nExample: Different incentive levels sales promotion may different effects.\nSolution: Explicitly define distinguish different treatment versions using principal stratification.\nExample: Different incentive levels sales promotion may different effects.Solution: Explicitly define distinguish different treatment versions using principal stratification.Violating SUTVA introduces significant challenges causal inference, leading :Thus, SUTVA unlikely hold, researchers must adopt flexible methodologies ensure valid inferences.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-no-interference","chapter":"26 Quasi-Experimental Methods","heading":"26.5.1.2 No Interference","text":"interference component Stable Unit Treatment Value Assumption states one unit’s treatment assignment influence another unit’s outcome. However, many real-world scenarios, assumption violated due spillover effects, :Epidemiology: vaccine studies, individual’s health status may depend vaccination status social network.Marketing Experiments: online advertising, one consumer’s exposure ad campaign may influence peers’ purchasing behavior.Mathematically, interference occurs unit \\(\\)’s outcome depends treatment assignments units within neighborhood function \\(\\mathcal{N}()\\):\\[\nY_i(Z, \\mathbf{Z}_{\\mathcal{N}()}),\n\\]\\(\\mathbf{Z}_{\\mathcal{N}()}\\) represents treatment assignments neighboring units. \\(\\mathcal{N}() \\neq \\emptyset\\), SUTVA violated, necessitating alternative modeling approaches spatial econometrics, network-based causal inference, graph-based treatment effect estimation.Special Cases InterferenceSeveral forms interference can arise applied settings:Complete Interference: unit’s outcome depends treatment assignments units (e.g., fully connected social network).Partial Interference: Interference occurs within subgroups (e.g., students within classrooms across schools).Network Interference: Treatment effects propagate social spatial network, requiring models graph-based causal inference spatial econometrics.cases necessitates adjustments standard causal inference techniques.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-no-hidden-variations-in-treatment","chapter":"26 Quasi-Experimental Methods","heading":"26.5.1.3 No Hidden Variations in Treatment","text":"second component SUTVA ensures treatment uniquely defined, meaning unaccounted variations treatment administration. multiple versions treatment exist (e.g., different dosages drug), causal effect may well-defined.Mathematically, different treatment versions \\(v\\) exist given treatment \\(Z\\), potential outcome indexed accordingly:\\[\nY_i(Z, v).\n\\]treatment variations lead different responses:\\[\nY_i(Z, v_1) \\neq Y_i(Z, v_2),\n\\]SUTVA violated. cases, methods instrumental variables latent variable models can help adjust treatment heterogeneity.","code":""},{"path":"sec-quasi-experimental.html","id":"strategies-to-address-sutva-violations","chapter":"26 Quasi-Experimental Methods","heading":"26.5.1.4 Strategies to Address SUTVA Violations","text":"Several approaches help mitigate effects SUTVA violations:Randomized Saturation Designs: Assign treatment varying intensities across clusters estimate spillover effects.Network-Based Causal Models: Utilize graph theory adjacency matrices account interference.Instrumental Variables: multiple versions treatment exist, use IV isolate single version.Stratified Analysis: treatment variations known, analyze subgroups separately.Difference--Differences Spatial Controls: Incorporate spatial lag terms model geographic spillovers.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-conditional-ignorability-assumption","chapter":"26 Quasi-Experimental Methods","heading":"26.5.2 Conditional Ignorability Assumption","text":"Next, must assume treatment assignment independent potential outcomes conditional observed covariates. assumption several equivalent names causal inference literature, includingConditional IgnorabilityConditional IgnorabilityConditional ExchangeabilityConditional ExchangeabilityNo Unobserved ConfoundingNo Unobserved ConfoundingNo Omitted VariablesNo Omitted VariablesIn language causal diagrams, assumption ensures backdoor paths treatment outcome blocked observed covariates.Formally, assume treatment assignment \\(Z\\) independent potential outcomes \\(Y(Z)\\) given set observed covariates \\(X\\):\\[\nY(1), Y(0) \\perp\\!\\!\\!\\perp Z \\mid X.\n\\]means conditioning \\(X\\), probability receiving treatment unrelated potential outcomes, ensuring comparisons treated untreated units unbiased.causal inference, treatment assignment said ignorable , conditional observed covariates \\(X\\), treatment indicator \\(Z\\) independent potential outcomes:\\[\nP(Y(1), Y(0) \\mid Z, X) = P(Y(1), Y(0) \\mid X).\n\\]Equivalently, terms conditional probability:\\[\nP(Z = 1 \\mid Y(1), Y(0), X) = P(Z = 1 \\mid X).\n\\]ensures treatment assignment good random control \\(X\\), meaning probability receiving treatment depend unmeasured confounders.direct consequence can estimate Average Treatment Effect using observational data:\\[\n\\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}[\\mathbb{E}[Y \\mid Z=1, X] - \\mathbb{E}[Y \\mid Z=0, X]].\n\\]ignorability holds, standard regression models, matching, weighting techniques (e.g., propensity score weighting) can provide unbiased causal estimates.","code":""},{"path":"sec-quasi-experimental.html","id":"the-role-of-causal-diagrams-and-backdoor-paths","chapter":"26 Quasi-Experimental Methods","heading":"26.5.2.1 The Role of Causal Diagrams and Backdoor Paths","text":"causal diagrams (DAGs), confounding arises backdoor path exists treatment \\(Z\\) outcome \\(Y\\). backdoor path non-causal path creates spurious associations \\(Z\\) \\(Y\\). conditional ignorability assumption requires paths blocked conditioning sufficient set covariates \\(X\\).Consider simple causal diagram:, \\(X\\) common cause \\(Z\\) \\(Y\\), creating backdoor path \\(Z \\leftarrow X \\rightarrow Y\\). fail control \\(X\\), estimated effect \\(Z\\) \\(Y\\) biased. However, condition \\(X\\), block backdoor path obtain unbiased estimate treatment effect.satisfy conditional ignorability assumption, researchers must identify sufficient set confounders block backdoor paths. often done using domain knowledge causal structure learning algorithms.Minimal Sufficient Adjustment Set: smallest set covariates \\(X\\) , conditioned upon, satisfies ignorability.Propensity Score Methods: Instead adjusting directly \\(X\\), one can estimate probability treatment \\(P(Z=1 \\mid X)\\) use inverse probability weighting matching.","code":""},{"path":"sec-quasi-experimental.html","id":"violations-of-the-ignorability-assumption","chapter":"26 Quasi-Experimental Methods","heading":"26.5.2.2 Violations of the Ignorability Assumption","text":"ignorability hold, treatment assignment depends unobserved confounders, introducing omitted variable bias. Mathematically, exists unmeasured variable \\(U\\) :\\[\nY(1), Y(0) \\\\perp\\!\\!\\!\\perp Z \\mid X,\n\\]estimates treatment effect biased.Consequences ViolationsConfounded Estimates: estimated treatment effect captures causal effect bias unobserved confounders.Selection Bias: treatment assignment related factors also influence outcome, sample may representative.Overestimation Underestimation: Ignoring important confounders can lead inflated deflated estimates treatment effects.Example ConfoundingConsider observational study smoking lung cancer:, genetics unmeasured confounder affecting smoking lung cancer. control genetics, estimated effect smoking lung cancer biased.","code":""},{"path":"sec-quasi-experimental.html","id":"strategies-to-address-violations","chapter":"26 Quasi-Experimental Methods","heading":"26.5.2.3 Strategies to Address Violations","text":"ignorability violated due unobserved confounding, several techniques can used mitigate bias:Instrumental Variables:\nUse variable \\(W\\) affects treatment \\(Z\\) direct effect \\(Y\\), ensuring exogeneity.\nExample: Randomized incentives encourage treatment uptake.\nUse variable \\(W\\) affects treatment \\(Z\\) direct effect \\(Y\\), ensuring exogeneity.Example: Randomized incentives encourage treatment uptake.Difference--Differences:\nCompare changes outcomes treatment treated vs. control group.\nRequires parallel trends assumption.\nCompare changes outcomes treatment treated vs. control group.Requires parallel trends assumption.Regression Discontinuity:\nExploit cutoff-based treatment assignment.\nExample: Scholarship eligibility certain GPA threshold.\nExploit cutoff-based treatment assignment.Example: Scholarship eligibility certain GPA threshold.Propensity Score Methods:\nEstimate probability treatment given \\(X\\).\nUse matching, inverse probability weighting (IPW), stratification balance treatment groups.\nEstimate probability treatment given \\(X\\).Use matching, inverse probability weighting (IPW), stratification balance treatment groups.Sensitivity Analysis:\nQuantify much unobserved confounding needed alter conclusions.\nExample: Rosenbaum’s sensitivity bounds.\nQuantify much unobserved confounding needed alter conclusions.Example: Rosenbaum’s sensitivity bounds.","code":""},{"path":"sec-quasi-experimental.html","id":"practical-considerations-5","chapter":"26 Quasi-Experimental Methods","heading":"26.5.2.4 Practical Considerations","text":"Select Covariates \\(X\\)?Domain Knowledge: Consult experts identify potential confounders.Causal Discovery Methods: Use Bayesian networks structure learning infer relationships.Statistical Tests: Examine balance pre-treatment characteristics.Trade-Offs Covariate SelectionToo Covariates → Risk omitted variable bias.Many Covariates → Overfitting, loss efficiency estimation.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-overlap-positivity-assumption","chapter":"26 Quasi-Experimental Methods","heading":"26.5.3 Overlap (Positivity) Assumption","text":"overlap assumption, also known common support positivity, ensures probability receiving treatment strictly 0 1 values observed covariates \\(X_i\\). Mathematically, expressed :\\[\n0 < P(Z_i = 1 \\mid X_i) < 1, \\quad \\forall X_i.\n\\]condition ensures every possible value \\(X_i\\), nonzero probability receiving treatment (\\(Z_i = 1\\)) control (\\(Z_i = 0\\)). , covariate distributions treated control units must overlap.overlap limited, Average Treatment Effect may identifiable. cases, Average Treatment Effect Treated remains identifiable, extreme violations overlap can make even ATT estimation problematic. alternative estimand, Average Treatment Effect Overlap Population (ATO), may used instead, focusing subpopulation treatment assignment deterministic (F. Li, Morgan, Zaslavsky 2018).ATO offers practical solution presence limited overlap, researchers may still prefer conventional estimands like ATT interpretability policy relevance. cases, balancing weights present valuable alternative inverse probability weights. Unlike Inverse Probability Weighting, can yield extreme weights poor overlap, balancing weights directly address covariate imbalance estimation process. Ben-Michael Keele (2023) demonstrate balancing weights can recover ATT even inverse probability weighting fails due limited overlap. Although overlap weights remain important tool reducing bias, balancing weights enable researchers target familiar causal estimands like ATT, offering compelling compromise statistical robustness interpretability.overlap assumption prevents cases treatment assignment deterministic, ensuring :\\[\n0 < P(Z_i = 1 \\mid X_i) < 1, \\quad \\forall X_i.\n\\]implies two key properties:Positivity Condition: Every unit nonzero probability receiving treatment.Deterministic Treatment Assignment: \\(P(Z_i = 1 \\mid X_i) = 0\\) \\(P(Z_i = 1 \\mid X_i) = 1\\) \\(X_i\\), causal effect identifiable values.subpopulations always receive treatment (\\(P(Z_i = 1 \\mid X_i) = 1\\)) never receive treatment (\\(P(Z_i = 1 \\mid X_i) = 0\\)), counterfactual available, making causal inference impossible groups.","code":""},{"path":"sec-quasi-experimental.html","id":"implications-of-violating-the-overlap-assumption","chapter":"26 Quasi-Experimental Methods","heading":"26.5.3.1 Implications of Violating the Overlap Assumption","text":"overlap assumption violated, identifying causal effects becomes challenging. Key implications include:Example: Education Intervention Socioeconomic StatusSuppose study education intervention aimed improving student performance. high-income students received intervention (\\(P(Z = 1 \\mid X) = 1\\) high income) low-income students received (\\(P(Z = 1 \\mid X) = 0\\) low income), common support. result, estimate valid treatment effect low-income students.","code":""},{"path":"sec-quasi-experimental.html","id":"diagnosing-overlap-violations","chapter":"26 Quasi-Experimental Methods","heading":"26.5.3.2 Diagnosing Overlap Violations","text":"estimating causal effects, crucial assess overlap using diagnostic tools :Propensity Score DistributionEstimate propensity score \\(e(X) = P(Z = 1 \\mid X)\\) visualize distribution:Good Overlap (Well-Mixed Propensity Score Distributions) → Treated control groups similar propensity score distributions.Poor Overlap (Separated Propensity Score Distributions) → Clear separation propensity score distributions suggests limited common support.Standardized Mean DifferencesCompare covariate distributions treated control groups. Large imbalances suggest weak overlap.Kernel Density PlotsVisualize density propensity scores treatment group. Non-overlapping regions indicate poor support.","code":""},{"path":"sec-quasi-experimental.html","id":"strategies-to-address-overlap-violations","chapter":"26 Quasi-Experimental Methods","heading":"26.5.3.3 Strategies to Address Overlap Violations","text":"overlap weak, several strategies can help:Trimming Non-Overlapping Units\nExclude units extreme propensity scores (e.g., \\(P(Z = 1 \\mid X) \\approx 0\\) \\(P(Z = 1 \\mid X) \\approx 1\\)).\nImproves internal validity reduces sample size.\nExclude units extreme propensity scores (e.g., \\(P(Z = 1 \\mid X) \\approx 0\\) \\(P(Z = 1 \\mid X) \\approx 1\\)).Improves internal validity reduces sample size.Reweighting Approaches\nUse overlap weights focus population treatment plausibly assignable.\nAverage Treatment Effect Overlap Population (ATO) estimates effects units \\(P(Z = 1 \\mid X)\\) moderate (e.g., close 0.5).\nUse overlap weights focus population treatment plausibly assignable.Average Treatment Effect Overlap Population (ATO) estimates effects units \\(P(Z = 1 \\mid X)\\) moderate (e.g., close 0.5).Matching Propensity Score\nRemove units lack suitable matches opposite treatment group.\nImproves balance cost excluding observations.\nRemove units lack suitable matches opposite treatment group.Improves balance cost excluding observations.Covariate Balancing Techniques\nUse entropy balancing inverse probability weighting adjust limited overlap.\nUse entropy balancing inverse probability weighting adjust limited overlap.Sensitivity Analysis\nAssess overlap violations affect causal conclusions.\nExample: Rosenbaum’s sensitivity bounds quantify impact unmeasured confounding.\nAssess overlap violations affect causal conclusions.Example: Rosenbaum’s sensitivity bounds quantify impact unmeasured confounding.","code":""},{"path":"sec-quasi-experimental.html","id":"average-treatment-effect-for-the-overlap-population","chapter":"26 Quasi-Experimental Methods","heading":"26.5.3.4 Average Treatment Effect for the Overlap Population","text":"overlap weak, ATO offers alternative estimand focuses subpopulation treatment deterministic.Instead estimating ATE (applies entire population) ATT (applies treated population), ATO focuses units treatment control plausible options.ATO estimated using overlap weights:\\[\nW_i = P(Z_i = 1 \\mid X_i) (1 - P(Z_i = 1 \\mid X_i)).\n\\]weights:Downweight extreme propensity scores (\\(P(Z = 1 \\mid X)\\) close 0 1).Focus inference subpopulation overlap.Improve robustness cases limited common support.Comparison EstimandsPractical Applications ATOPolicy Evaluation: treatment assignment highly structured, ATO ensures conclusions relevant feasible intervention group.Observational Studies: Avoids extrapolation bias estimating treatment effects subpopulations common support.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-natural-experiments","chapter":"26 Quasi-Experimental Methods","heading":"26.6 Natural Experiments","text":"natural experiment observational study exogenous event, policy change, external factor creates -random variation treatment assignment across units. Unlike randomized controlled trials (RCTs)—researchers actively manipulate treatment assignment—natural experiments leverage naturally occurring circumstances approximate randomization.many fields, including economics, marketing, political science, epidemiology, natural experiments provide indispensable tool causal inference, particularly RCTs impractical, unethical, prohibitively expensive.Key Characteristics Natural ExperimentsExogenous Shock: Treatment assignment determined external event, policy, regulation rather researchers.-Randomization: event must create variation plausibly unrelated unobserved confounders, mimicking RCT.Comparability Treatment Control Groups: study design ensure treated untreated units comparable except exposure intervention.Examples Natural Experiments Economics MarketingMinimum Wage Policy EmploymentA classic example comes Card Krueger (1993) study minimum wage. New Jersey increased minimum wage neighboring Pennsylvania , created natural experiment. comparing fast-food employment trends states, study estimated causal effect minimum wage increase employment.Advertising Bans Consumer BehaviorSuppose country bans advertising particular product, tobacco alcohol, similar neighboring country . policy creates natural experiment: researchers can compare sales trends ban countries estimate causal impact advertising restrictions consumer demand.Facebook Outage Natural ExperimentIn October 2021, Facebook experienced global outage, making advertising platform temporarily unavailable. businesses relied Facebook ads, outage created exogenous shock digital marketing strategies. Researchers compare advertisers’ sales website traffic , , outage assess impact social media advertising.Lottery-Based Admission SchoolsIn many cities, students apply competitive schools via lottery-based admissions. Since admission randomly assigned, creates natural experiment studying causal effect elite schooling future earnings, college attendance, academic performance.examples illustrate natural experiments leveraged estimate causal effects randomization infeasible.Natural Experiments Important?Natural experiments powerful tools identifying causal relationships often eliminate selection bias—major issue observational studies. However, also present challenges:Treatment Assignment Always Perfectly Random: Unlike RCTs, natural experiments rely assumptions -randomness treatment.Potential Confounding: Even treatment appears random, hidden factors might still bias results.Repeated Use Natural Experiment: researchers analyze natural experiment multiple times, increases risk false discoveries due multiple hypothesis testing.statistical challenges, especially risks false positives, crucial understand address.","code":""},{"path":"sec-quasi-experimental.html","id":"the-problem-of-reusing-natural-experiments","chapter":"26 Quasi-Experimental Methods","heading":"26.6.1 The Problem of Reusing Natural Experiments","text":"Recent simulations demonstrate number estimated outcomes far exceeds number true effects (\\(N_{\\text{Outcome}} \\gg N_{\\text{True effect}}\\)), proportion false positive findings can exceed 50% (Heath et al. 2023, 2331). problem arises due :Data Snooping: multiple hypotheses tested dataset, probability finding least one statistically significant result purely chance increases.Researcher Degrees Freedom: flexibility defining outcomes, selecting models, specifying robustness checks can lead p-hacking publication bias.Dependence Across Tests: Many estimated outcomes correlated, meaning traditional multiple testing corrections may adequately control Type errors.problem exacerbated :Studies use policy change, regulatory event, shock across different settings.Studies use policy change, regulatory event, shock across different settings.Multiple subgroups model specifications tested without proper corrections.Multiple subgroups model specifications tested without proper corrections.P-values interpreted without adjusting multiple testing bias.P-values interpreted without adjusting multiple testing bias.","code":""},{"path":"sec-quasi-experimental.html","id":"statistical-challenges-in-reusing-natural-experiments","chapter":"26 Quasi-Experimental Methods","heading":"26.6.2 Statistical Challenges in Reusing Natural Experiments","text":"natural experiment analyzed multiple studies, even within single study across many different outcomes, probability obtaining spurious significant results increases. Key statistical challenges include:Family-Wise Error Rate (FWER) InflationEach additional hypothesis tested increases probability least one false rejection null hypothesis (Type error). test \\(m\\) independent hypotheses nominal significance level \\(\\alpha\\), probability making least one Type error :\\[\nP(\\text{least one false positive}) = 1 - (1 - \\alpha)^m.\n\\]example, \\(\\alpha = 0.05\\) \\(m = 20\\) tests:\\[\nP(\\text{least one false positive}) = 1 - (0.95)^{20} \\approx 0.64.\n\\]means even null hypotheses true, expect 64% chance falsely rejecting least one.False Discovery Rate (FDR) Dependent TestsFWER corrections Bonferroni conservative may stringent outcomes correlated. cases researchers test multiple related hypotheses, False Discovery Rate (FDR) control provides alternative limiting expected proportion false discoveries among rejected hypotheses.Multiple Testing Sequential ExperimentsIn many longitudinal rolling studies, results reported time data becomes available. Chronological testing introduces additional biases:Repeated interim analyses increase probability stopping early false positives.Outcomes tested different times require corrections adjust sequential dependence.address issues, researchers must apply multiple testing corrections.","code":""},{"path":"sec-quasi-experimental.html","id":"solutions-multiple-testing-corrections","chapter":"26 Quasi-Experimental Methods","heading":"26.6.3 Solutions: Multiple Testing Corrections","text":"mitigate risks false positives natural experiment research, various statistical corrections can applied.Family-Wise Error Rate (FWER) ControlThe conservative approach controls probability least one false positive:Bonferroni Correction:\n\\[\np^*_i = m \\cdot p_i,\n\\]\n\\(p^*_i\\) adjusted p-value \\(m\\) total number hypotheses tested.Bonferroni Correction:\\[\np^*_i = m \\cdot p_i,\n\\]\\(p^*_i\\) adjusted p-value \\(m\\) total number hypotheses tested.Holm-Bonferroni Method (Holm 1979): Less conservative Bonferroni, adjusting significance thresholds stepwise fashion.Holm-Bonferroni Method (Holm 1979): Less conservative Bonferroni, adjusting significance thresholds stepwise fashion.Sidak Correction (Šidák 1967): Accounts multiple comparisons assuming independent tests.Sidak Correction (Šidák 1967): Accounts multiple comparisons assuming independent tests.Romano-Wolf Stepwise Correction (Romano Wolf 2005, 2016): Recommended natural experiments controls dependence across tests.Romano-Wolf Stepwise Correction (Romano Wolf 2005, 2016): Recommended natural experiments controls dependence across tests.Hochberg’s Sharper FWER Control (Hochberg 1988): step-procedure, powerful Holm’s method.Hochberg’s Sharper FWER Control (Hochberg 1988): step-procedure, powerful Holm’s method.False Discovery Rate (FDR) ControlFDR-controlling methods less conservative FWER-based approaches, allowing false positives controlling expected proportion.Benjamini-Hochberg (BH) Procedure (Benjamini Hochberg 1995): Limits expected proportion false discoveries among rejected hypotheses.Adaptive Benjamini-Hochberg (Benjamini Hochberg 2000): Adjusts situations true proportion null hypotheses unknown.Benjamini-Yekutieli () Correction (Benjamini Yekutieli 2001): Accounts arbitrary dependence tests.Two-Stage Benjamini-Hochberg (Benjamini, Krieger, Yekutieli 2006): powerful version BH, particularly useful large-scale studies.Sequential Approaches Multiple TestingTwo major frameworks exist applying multiple testing corrections time:Chronological SequencingOutcomes ordered date first reported.Outcomes ordered date first reported.Multiple testing corrections applied sequentially, progressively increasing statistical significance threshold time.Multiple testing corrections applied sequentially, progressively increasing statistical significance threshold time.Best Foot Forward PolicyOutcomes ranked least likely rejected based experimental data.Outcomes ranked least likely rejected based experimental data.Frequently used clinical trials primary outcomes given priority.Frequently used clinical trials primary outcomes given priority.New outcomes added linked primary treatment effects.New outcomes added linked primary treatment effects.Alternatively, refer rules thumb Table AI (Heath et al. 2023, 2356).approaches ensure p-value correction consistent temporal structure natural experiments.Romano-Wolf CorrectionThe Romano-Wolf correction highly recommended handling multiple testing natural experiments:General Multiple Testing AdjustmentsFor multiple testing adjustments, use multtest package:","code":"# Install required packages\n# install.packages(\"fixest\")\n# install.packages(\"wildrwolf\")\n\nlibrary(fixest)\nlibrary(wildrwolf)\n\n# Load example data\ndata(iris)\n\n# Fit multiple regression models\nfit1 <- feols(Sepal.Width ~ Sepal.Length, data = iris)\nfit2 <- feols(Petal.Length ~ Sepal.Length, data = iris)\nfit3 <- feols(Petal.Width ~ Sepal.Length, data = iris)\n\n# Apply Romano-Wolf stepwise correction\nres <- rwolf(\n  models = list(fit1, fit2, fit3), \n  param = \"Sepal.Length\",  \n  B = 500\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n#> \n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\nres\n#>   model   Estimate Std. Error   t value     Pr(>|t|) RW Pr(>|t|)\n#> 1     1 -0.0618848 0.04296699 -1.440287    0.1518983 0.145708583\n#> 2     2   1.858433 0.08585565  21.64602 1.038667e-47 0.001996008\n#> 3     3  0.7529176 0.04353017  17.29645 2.325498e-37 0.001996008\n# Install package if necessary\n# BiocManager::install(\"multtest\")\n\nlibrary(multtest)\n\n# Define multiple correction procedures\nprocs <-\n    c(\"Bonferroni\",\n      \"Holm\",\n      \"Hochberg\",\n      \"SidakSS\",\n      \"SidakSD\",\n      \"BH\",\n      \"BY\",\n      \"ABH\",\n      \"TSBH\")\n\n# Generate random p-values for demonstration\np_values <- runif(10)\n\n# Apply multiple testing corrections\nadj_pvals <- mt.rawp2adjp(p_values, procs)\n\n# Print results in a readable format\nadj_pvals |> causalverse::nice_tab()\n#>    adjp.rawp adjp.Bonferroni adjp.Holm adjp.Hochberg adjp.SidakSS adjp.SidakSD\n#> 1       0.12               1         1          0.75         0.72         0.72\n#> 2       0.22               1         1          0.75         0.92         0.89\n#> 3       0.24               1         1          0.75         0.94         0.89\n#> 4       0.29               1         1          0.75         0.97         0.91\n#> 5       0.36               1         1          0.75         0.99         0.93\n#> 6       0.38               1         1          0.75         0.99         0.93\n#> 7       0.44               1         1          0.75         1.00         0.93\n#> 8       0.59               1         1          0.75         1.00         0.93\n#> 9       0.65               1         1          0.75         1.00         0.93\n#> 10      0.75               1         1          0.75         1.00         0.93\n#>    adjp.BH adjp.BY adjp.ABH adjp.TSBH_0.05 index h0.ABH h0.TSBH\n#> 1     0.63       1     0.63           0.63     2     10      10\n#> 2     0.63       1     0.63           0.63     6     10      10\n#> 3     0.63       1     0.63           0.63     8     10      10\n#> 4     0.63       1     0.63           0.63     3     10      10\n#> 5     0.63       1     0.63           0.63    10     10      10\n#> 6     0.63       1     0.63           0.63     1     10      10\n#> 7     0.63       1     0.63           0.63     7     10      10\n#> 8     0.72       1     0.72           0.72     9     10      10\n#> 9     0.72       1     0.72           0.72     5     10      10\n#> 10    0.75       1     0.75           0.75     4     10      10\n\n# adj_pvals$adjp"},{"path":"sec-quasi-experimental.html","id":"design-vs.-model-based-approaches","chapter":"26 Quasi-Experimental Methods","heading":"26.7 Design vs. Model-Based Approaches","text":"Quasi-experimental methods exist along spectrum design-based model-based approaches, rather fitting neatly one category . one end, design-based perspective emphasizes study structure, aiming approximate randomization external sources variation, policy changes natural experiments. end, model-based perspective relies heavily statistical assumptions functional forms estimate causal relationships.quasi-experimental methods blend elements perspectives, differing degree prioritize design validity statistical modeling. section explores continuum approaches, highlighting fundamental differences implications empirical research.illustrate , generate visual spectrum positioning common quasi-experimental methods along continuum design-based model-based.Design-Based Causal Inference:Focuses using design study experiment establish causal relationships.Focuses using design study experiment establish causal relationships.Relies heavily randomization, natural experiments, quasi-experimental designs (e.g., Difference--Differences, Regression Discontinuity, Instrumental Variables).Relies heavily randomization, natural experiments, quasi-experimental designs (e.g., Difference--Differences, Regression Discontinuity, Instrumental Variables).Assumes well-designed studies natural experiments produce plausibly exogenous variation treatment, minimizing reliance strong modeling assumptions.Assumes well-designed studies natural experiments produce plausibly exogenous variation treatment, minimizing reliance strong modeling assumptions.Emphasizes transparency treatment assignment mechanism isolates causal effects.Emphasizes transparency treatment assignment mechanism isolates causal effects.Model-Based Causal Inference:Relies explicit statistical models infer causality, typically grounded theory assumptions data-generating process.Relies explicit statistical models infer causality, typically grounded theory assumptions data-generating process.Commonly uses structural equation models, propensity score models, frameworks like Bayesian inference.Commonly uses structural equation models, propensity score models, frameworks like Bayesian inference.Assumptions crucial (e.g., omitted variable bias, correct model specification, ignorability).Assumptions crucial (e.g., omitted variable bias, correct model specification, ignorability).May employed experimental quasi-experimental designs feasible.May employed experimental quasi-experimental designs feasible.Comparison Two PerspectivesBoth streams complementary, often, researchers combine elements robustly estimate causal effects (.e., multi-method studies). example, design-based approach might supplemented model-based framework address specific limitations like imperfect randomization.","code":""},{"path":"sec-quasi-experimental.html","id":"sec-design-based","chapter":"26 Quasi-Experimental Methods","heading":"26.7.1 Design-Based Perspective","text":"design-based perspective causal inference emphasizes exploiting natural sources variation approximate randomization. Unlike structural models, rely extensive theoretical assumptions, design-based methods leverage exogenous events, policy changes, assignment rules infer causal effects minimal modeling assumptions.methods particularly useful :Randomized controlled trials infeasible, unethical, impractical.Randomized controlled trials infeasible, unethical, impractical.Treatment assignment plausibly exogenous due policy, threshold, external shock.Treatment assignment plausibly exogenous due policy, threshold, external shock.Quasi-random variation exists (e.g., firms, consumers, individuals assigned treatment based factors outside control).Quasi-random variation exists (e.g., firms, consumers, individuals assigned treatment based factors outside control).","code":""},{"path":"sec-quasi-experimental.html","id":"sec-model-based-perspective","chapter":"26 Quasi-Experimental Methods","heading":"26.7.2 Model-Based Perspective","text":"model-based perspective causal inference relies statistical modeling techniques estimate treatment effects, rather exploiting exogenous sources variation design-based approaches. framework, researchers explicitly specify relationships variables, using mathematical models control confounding estimate counterfactual outcomes.Unlike quasi-experimental designs leverage external assignment mechanisms (e.g., cutoff, policy change, natural experiment), model-based methods assume confounding can adequately adjusted using statistical techniques alone. makes highly flexible also vulnerable model misspecification omitted variable bias.Key Characteristics Model-Based ApproachesDependence Correct Model Specification\nvalidity causal estimates hinges correct functional form model.\nmodel misspecified (e.g., incorrect interactions, omitted variables), bias can arise.\nDependence Correct Model SpecificationThe validity causal estimates hinges correct functional form model.validity causal estimates hinges correct functional form model.model misspecified (e.g., incorrect interactions, omitted variables), bias can arise.model misspecified (e.g., incorrect interactions, omitted variables), bias can arise.Need Exogenous Variation\nUnlike design-based methods, model-based approaches rely policy shocks, thresholds, instrumental variables.\nInstead, estimate causal effects entirely statistical modeling.\nNeed Exogenous VariationUnlike design-based methods, model-based approaches rely policy shocks, thresholds, instrumental variables.Unlike design-based methods, model-based approaches rely policy shocks, thresholds, instrumental variables.Instead, estimate causal effects entirely statistical modeling.Instead, estimate causal effects entirely statistical modeling.Assumption Ignorability\nmethods assume relevant confounders observed properly included model.\nassumption untestable may violated practice, leading biased estimates.\nAssumption IgnorabilityThese methods assume relevant confounders observed properly included model.methods assume relevant confounders observed properly included model.assumption untestable may violated practice, leading biased estimates.assumption untestable may violated practice, leading biased estimates.Flexibility Generalizability\nModel-based methods allow researchers estimate treatment effects even exogenous variation unavailable.\ncan applied wide range settings policy-driven variation exist.\nFlexibility GeneralizabilityModel-based methods allow researchers estimate treatment effects even exogenous variation unavailable.Model-based methods allow researchers estimate treatment effects even exogenous variation unavailable.can applied wide range settings policy-driven variation exist.can applied wide range settings policy-driven variation exist.Key Model-Based Methods Causal Inference","code":""},{"path":"sec-quasi-experimental.html","id":"propensity-score-matching-and-weighting","chapter":"26 Quasi-Experimental Methods","heading":"26.7.2.1 Propensity Score Matching and Weighting","text":"Propensity Score Matching estimates probability treatment assignment based observed characteristics, matches treated control units similar probabilities mimic randomization.Key AssumptionsCorrect Model Specification: propensity score model (typically logistic regression) must correctly capture relevant covariates affecting treatment assignment.Correct Model Specification: propensity score model (typically logistic regression) must correctly capture relevant covariates affecting treatment assignment.Ignorability: unmeasured confounders—relevant differences treated control groups accounted .Ignorability: unmeasured confounders—relevant differences treated control groups accounted .’s Model-BasedUnlike design-based methods, exploit external sources variation, PSM depends entirely statistical model balance covariates.Unlike design-based methods, exploit external sources variation, PSM depends entirely statistical model balance covariates.mis-specification propensity score model can introduce bias.mis-specification propensity score model can introduce bias.ExamplesMarketing: Evaluating impact targeted advertising purchase behavior matching customers exposed ad unexposed customers similar browsing histories.Marketing: Evaluating impact targeted advertising purchase behavior matching customers exposed ad unexposed customers similar browsing histories.Economics: Estimating effect job training programs income matching participants non-participants based demographic employment history.Economics: Estimating effect job training programs income matching participants non-participants based demographic employment history.","code":""},{"path":"sec-quasi-experimental.html","id":"structural-causal-models-and-directed-acyclic-graphs","chapter":"26 Quasi-Experimental Methods","heading":"26.7.2.2 Structural Causal Models and Directed Acyclic Graphs","text":"SCMs provide mathematical framework represent causal relationships using structural equations directed acyclic graphs (DAGs). models explicitly encode assumptions variables influence .Key AssumptionsCorrect DAG Specification: researcher must correctly specify causal graph structure.Correct DAG Specification: researcher must correctly specify causal graph structure.Exclusion Restrictions: variables must affect outcomes specified causal pathways.Exclusion Restrictions: variables must affect outcomes specified causal pathways.’s Model-BasedSCMs require explicit causal assumptions, unlike design-based approaches rely external assignments.SCMs require explicit causal assumptions, unlike design-based approaches rely external assignments.Identification depends structural equations, can misspecified.Identification depends structural equations, can misspecified.ExamplesMarketing: Modeling causal impact pricing strategies sales accounting advertising, competitor actions, seasonal effects.Marketing: Modeling causal impact pricing strategies sales accounting advertising, competitor actions, seasonal effects.Economics: Estimating effect education wages, considering impact family background, school quality, job market conditions.Economics: Estimating effect education wages, considering impact family background, school quality, job market conditions.Covariate Adjustment (Regression-Based Approaches)Regression models estimate causal effects controlling observed confounders. includes linear regression, logistic regression, flexible models like generalized additive models (GAMs).Key AssumptionsLinear Nonlinear Functional Form Must Correct.Linear Nonlinear Functional Form Must Correct.Omitted Variable Bias: relevant confounders must measured included.Omitted Variable Bias: relevant confounders must measured included.’s Model-BasedEstimates depend entirely correctness model form covariates included.Estimates depend entirely correctness model form covariates included.Unlike design-based methods, exogenous source variation.Unlike design-based methods, exogenous source variation.ExamplesMarketing: Estimating effect social media ads conversions controlling past purchase behavior customer demographics.Marketing: Estimating effect social media ads conversions controlling past purchase behavior customer demographics.Economics: Evaluating impact financial aid student performance, adjusting prior academic records.Economics: Evaluating impact financial aid student performance, adjusting prior academic records.","code":""},{"path":"sec-quasi-experimental.html","id":"machine-learning-for-causal-inference","chapter":"26 Quasi-Experimental Methods","heading":"26.7.2.3 Machine Learning for Causal Inference","text":"Machine learning (ML) techniques, causal forests, Bayesian Additive Regression Trees (BART), double machine learning (DML), provide flexible methods estimating treatment effects without strict functional form assumptions.Key AssumptionsSufficient Training Data: ML models require large datasets capture treatment heterogeneity.Sufficient Training Data: ML models require large datasets capture treatment heterogeneity.Algorithm Interpretability: Unlike standard regression, ML-based causal inference can harder interpret.Algorithm Interpretability: Unlike standard regression, ML-based causal inference can harder interpret.’s Model-BasedRelies statistical learning algorithms rather external assignment mechanisms.Relies statistical learning algorithms rather external assignment mechanisms.Identification depends data-driven feature selection, policy design.Identification depends data-driven feature selection, policy design.ExamplesMarketing: Predicting individualized treatment effects ad targeting using causal forests.Marketing: Predicting individualized treatment effects ad targeting using causal forests.Economics: Estimating heterogeneous effects tax incentives business investment.Economics: Estimating heterogeneous effects tax incentives business investment.","code":""},{"path":"sec-quasi-experimental.html","id":"placing-methods-along-a-spectrum","chapter":"26 Quasi-Experimental Methods","heading":"26.7.3 Placing Methods Along a Spectrum","text":"reality, method balances two philosophies differently:Strongly Design-Based: Randomized Controlled Trials, clear natural experiments (e.g., lotteries), near-random assignment rules (RD near threshold).Strongly Design-Based: Randomized Controlled Trials, clear natural experiments (e.g., lotteries), near-random assignment rules (RD near threshold).Hybrid / Semi-Design Approaches: Difference--Differences, Synthetic Control, many matching approaches (rely credible assumptions designs often still need modeling choices, e.g., functional form trends).Hybrid / Semi-Design Approaches: Difference--Differences, Synthetic Control, many matching approaches (rely credible assumptions designs often still need modeling choices, e.g., functional form trends).Strongly Model-Based: Structural equation models (SCMs), certain propensity score approaches, Bayesian causal inference fully specified likelihood, etc.Strongly Model-Based: Structural equation models (SCMs), certain propensity score approaches, Bayesian causal inference fully specified likelihood, etc.Hence, key question method :“much identification relying ‘-good--random’ design versus much structural statistical modeling assumptions?”can argue variation treatment practically random (near-random) design standpoint, ’re leaning toward design-based camp.can argue variation treatment practically random (near-random) design standpoint, ’re leaning toward design-based camp.identification crucially depends specifying correct model strong assumptions data-generating process, ’re model-based camp.identification crucially depends specifying correct model strong assumptions data-generating process, ’re model-based camp.real-world applications lie somewhere middle, combining aspects approaches.Model-Based vs. Design-Based Causal Inference: Key Differences","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-regression-discontinuity","chapter":"27 Regression Discontinuity","heading":"27 Regression Discontinuity","text":"Regression Discontinuity (RD) quasi-experimental design exploits discrete jump treatment assignment based continuous (approximately continuous) variable, often called running variable, forcing variable, assignment variable.common application occurs intervention assigned based whether running variable exceeds predefined cutoff.approach provides localized causal inference around cutoff, akin randomized experiment local region.Historical Background: First introduced Thistlethwaite Campbell (1960) context merit awards effect future academic outcomes.Key References:Foundational papers: (G. Imbens Lemieux 2008; Lee Lemieux 2010)","code":""},{"path":"sec-regression-discontinuity.html","id":"conceptual-framework","chapter":"27 Regression Discontinuity","heading":"27.1 Conceptual Framework","text":"Regression discontinuity best understood localized experiment threshold:Internal Validity: Strong within narrow bandwidth around cutoff.Internal Validity: Strong within narrow bandwidth around cutoff.External Validity: Limited—results may generalize beyond bandwidth.External Validity: Limited—results may generalize beyond bandwidth.Comparison Randomized Experiments: Empirical evidence suggests RD randomized controlled trials yield similar estimates ((Chaplin et al. 2018), Mathematica).Comparison Randomized Experiments: Empirical evidence suggests RD randomized controlled trials yield similar estimates ((Chaplin et al. 2018), Mathematica).RD connected causal inference methods:Randomized Experiment: ’s local randomization.Randomized Experiment: ’s local randomization.Instrumental Variables: RD can viewed structural IV model (J. D. Angrist Lavy 1999).Instrumental Variables: RD can viewed structural IV model (J. D. Angrist Lavy 1999).Matching Methods: RD special case matching occurs single threshold (J. J. Heckman, LaLonde, Smith 1999).Matching Methods: RD special case matching occurs single threshold (J. J. Heckman, LaLonde, Smith 1999).Interrupted Time Series (): RD preferable running variable finely measured. However, highly discrete time data (e.g., quarterly annual), may appropriate. Hence, RD always better data infinite (substantially large).Interrupted Time Series (): RD preferable running variable finely measured. However, highly discrete time data (e.g., quarterly annual), may appropriate. Hence, RD always better data infinite (substantially large).","code":""},{"path":"sec-regression-discontinuity.html","id":"types-of-regression-discontinuity-designs","chapter":"27 Regression Discontinuity","heading":"27.1.1 Types of Regression Discontinuity Designs","text":"Sharp RD: Treatment probability jumps 0 1 cutoff.Fuzzy RD: Treatment probability changes discontinuously reach 1.Kink RD: Discontinuity occurs slope rather level running variable (Nielsen, Sørensen, Taber 2010) (see applications Böckerman, Kanninen, Suoniemi (2018) theoretical foundations Card et al. (2015)).Regression Discontinuity Time (.e., Interrupted Time Series): running variable time.Additional variations:Multiple Cutoffs: Different thresholds across subgroups.Multiple Cutoffs: Different thresholds across subgroups.Multiple Scores: one running variable.Multiple Scores: one running variable.Geographic RD: Cutoff spatially defined.Geographic RD: Cutoff spatially defined.Dynamic Treatments: Treatment effects evolve time.Dynamic Treatments: Treatment effects evolve time.Continuous Treatments: Instead binary treatment, intensity varies.Continuous Treatments: Instead binary treatment, intensity varies.","code":""},{"path":"sec-regression-discontinuity.html","id":"assumptions-for-rd-validity","chapter":"27 Regression Discontinuity","heading":"27.1.2 Assumptions for RD Validity","text":"Independent Assignment: treatment assigned solely based running variable.Independent Assignment: treatment assigned solely based running variable.Continuity Conditional Expectations: expected outcomes without treatment continuous cutoff:\n\\[\nE[Y(0)|X=x] \\text{ } E[Y(1)|X=x] \\text{ continuous } x = c.\n\\]Continuity Conditional Expectations: expected outcomes without treatment continuous cutoff:\\[\nE[Y(0)|X=x] \\text{ } E[Y(1)|X=x] \\text{ continuous } x = c.\n\\]Exogeneity Cutoff: cutoff manipulable. confounding interventions cutoff.Exogeneity Cutoff: cutoff manipulable. confounding interventions cutoff.Discontinuity Confounding Variables: covariates smooth threshold. common test check jumps covariates unrelated treatment.Discontinuity Confounding Variables: covariates smooth threshold. common test check jumps covariates unrelated treatment.","code":""},{"path":"sec-regression-discontinuity.html","id":"threats-to-rd-validity","chapter":"27 Regression Discontinuity","heading":"27.1.3 Threats to RD Validity","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"model-estimation-strategies","chapter":"27 Regression Discontinuity","heading":"27.2 Model Estimation Strategies","text":"regression discontinuity framework, researchers can choose parametric nonparametric models. choice depends assumptions functional form, data availability, trade-flexibility interpretability.","code":""},{"path":"sec-regression-discontinuity.html","id":"parametric-models-polynomial-regression","chapter":"27 Regression Discontinuity","heading":"27.2.1 Parametric Models: Polynomial Regression","text":"Parametric models, linear regression, assume specific functional form relationship dependent variable predictors. One way relax strict linearity assumption incorporating polynomial functions forcing variable (Lee Lemieux 2010). choice polynomial degree determined based data characteristics.However, using high-order polynomials comes challenges. Gelman Imbens (2019) highlight three key issues associated global high-degree polynomials:Imprecise Estimates Due Noise: Higher-degree polynomials can overfit, capturing noise instead meaningful patterns.Sensitivity Polynomial Degree: Estimates can vary significantly depending chosen degree, making model less stable.Inadequate Confidence Interval Coverage: Confidence intervals tend misleading using high-degree polynomials, leading incorrect inference.reasons, researchers often advised avoid global high-order polynomials instead rely alternative approaches (.e., nonparametric version).","code":""},{"path":"sec-regression-discontinuity.html","id":"nonparametric-models-local-regression","chapter":"27 Regression Discontinuity","heading":"27.2.2 Nonparametric Models: Local Regression","text":"Nonparametric models, local polynomial regression, offer greater flexibility avoiding strong assumptions functional form. Instead fitting single equation entire dataset, methods estimate relationships locally—often using linear quadratic polynomials within neighborhood data point.Uses weighted observations near cutoff.flexible global polynomial regression.Local regression methods, local linear regression, address shortcomings high-order global polynomials :Reducing overfitting, adapt local patterns without excessive complexity.Providing stable estimates, since results less sensitive arbitrary choices polynomial degree.Improving inference, ensuring reliable confidence intervals.balancing flexibility robustness, local regression techniques often preferred global polynomial models applied research.Best Practice: Use multiple model specifications check consistency results.","code":""},{"path":"sec-regression-discontinuity.html","id":"formal-definition","chapter":"27 Regression Discontinuity","heading":"27.3 Formal Definition","text":"Let \\(X_i\\) running variable, \\(c\\) cutoff, \\(D_i\\) treatment indicator:\\[ D_i = 1_{X_i > c} \\]equivalently,\\[ D_i =  \\begin{cases} 1, & X_i > c \\\\ 0, & X_i < c \\end{cases} \\]:\\(D_i\\): Treatment assignment\\(D_i\\): Treatment assignment\\(X_i\\): Running variable (continuous)\\(X_i\\): Running variable (continuous)\\(c\\): Cutoff value\\(c\\): Cutoff value","code":""},{"path":"sec-regression-discontinuity.html","id":"identification-assumptions","chapter":"27 Regression Discontinuity","heading":"27.3.1 Identification Assumptions","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"continuity-based-identification","chapter":"27 Regression Discontinuity","heading":"27.3.1.1 Continuity-Based Identification","text":"RD estimates [Local Average Treatment Effect] cutoff:\\[ \\begin{aligned} \\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\\\ &= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c] \\\\ &= \\lim_{x \\c^+} E[Y_{1i}|X_i = x] - \\lim_{x \\c^-} E[Y_{0i}|X_i = x] \\end{aligned} \\]relies assumption , absence treatment, conditional expectation potential outcomes continuous threshold \\(c\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"local-randomization-based-identification","chapter":"27 Regression Discontinuity","heading":"27.3.1.2 Local Randomization-Based Identification","text":"Alternatively, identification can achieved using local randomization within small bandwidth \\(W\\) (.e., neighborhood around cutoff). [Local Average Treatment Effect] case :\\[ \\begin{aligned} \\alpha_{LR} &= E[Y_{1i} - Y_{0i}|X_i \\W] \\\\ &= \\frac{1}{N_1} \\sum_{X_i \\W, D_i = 1} Y_i - \\frac{1}{N_0} \\sum_{X_i \\W, D_i = 0} Y_i \\end{aligned} \\]Since RD estimates local, may generalize entire population. However, many applications, internal validity primary concern (rather external validity).","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-and-inference","chapter":"27 Regression Discontinuity","heading":"27.4 Estimation and Inference","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"local-randomization-based-approach","chapter":"27 Regression Discontinuity","heading":"27.4.1 Local Randomization-Based Approach","text":"local randomization approach additionally assumes within chosen window \\(W = [c - w, c + w]\\), treatment assignment good random. requires:joint probability distribution running variable values inside \\(W\\) known.Potential outcomes independent running variable within \\(W\\).stronger assumption continuity-based identification, requires regression functions smooth \\(c\\) remain unaffected \\(X_i\\) within \\(W\\).Since researchers can choose window \\(W\\) (random assignment plausibly holds), sample size can often small.selection \\(W\\) can based :Pre-treatment covariate balance: Ensure covariates similar across threshold.Independent tests: Check independence outcome running variable.Domain knowledge: Use theoretical empirical justification window choice.inference, researchers can use:(Fisher) randomization inference(Neyman) design-based methods","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-continuity-based-approach","chapter":"27 Regression Discontinuity","heading":"27.4.2 Continuity-Based Approach","text":"Also known local polynomial regression method, approach estimates treatment effects fitting polynomial model locally around cutoff. Global polynomial regression recommended due issues :Lack robustnessOverfittingRunge’s phenomenon (oscillatory behavior boundaries)Steps Local Polynomial EstimationChoose polynomial order weighting schemeSelect optimal bandwidth (minimizing MSE coverage error)Estimate parameter interestPerform robust bias-corrected inferenceThis method ensures estimation remains local, capturing treatment effect precisely cutoff.","code":""},{"path":"sec-regression-discontinuity.html","id":"specification-checks","chapter":"27 Regression Discontinuity","heading":"27.5 Specification Checks","text":"validate credibility RD design, researchers perform several specification checks:Balance ChecksSorting, Bunching, ManipulationPlacebo TestsSensitivity Bandwidth ChoiceManipulation-Robust Regression Discontinuity Bounds","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-balance-checks","chapter":"27 Regression Discontinuity","heading":"27.5.1 Balance Checks","text":"Also known checking discontinuities average covariates, test examines whether covariates affected treatment exhibit discontinuity cutoff.Null Hypothesis (\\(H_0\\)): average effect covariates pseudo-outcomes (.e., influenced treatment) zero.rejected, raises serious doubts RD design, necessitating strong justification.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-sorting-bunching-and-manipulation","chapter":"27 Regression Discontinuity","heading":"27.5.2 Sorting, Bunching, and Manipulation","text":"test, also known checking discontinuities distribution forcing variable, detects whether subjects manipulate running variable sort treatment.individuals can manipulate running variable—especially cutoff known advance—can lead bunching behavior (.e., clustering just cutoff).treatment desirable, individuals try sort treatment, leading gap just cutoff.treatment undesirable, individuals try avoid , leading gap just cutoff.RD, assume manipulation running variable. However, bunching behavior, firms individuals strategically manipulate position, violates assumption.address issue, bunching approach estimates counterfactual distribution—density individuals absence manipulation.address issue, bunching approach estimates counterfactual distribution—density individuals absence manipulation.fraction individuals engaged manipulation calculated comparing observed distribution counterfactual distribution.fraction individuals engaged manipulation calculated comparing observed distribution counterfactual distribution.standard RD framework, step unnecessary assumes observed distribution counterfactual (manipulation-free) distribution , implying manipulationIn standard RD framework, step unnecessary assumes observed distribution counterfactual (manipulation-free) distribution , implying manipulation","code":""},{"path":"sec-regression-discontinuity.html","id":"mccrary-sorting-test","chapter":"27 Regression Discontinuity","heading":"27.5.2.1 McCrary Sorting Test","text":"widely used formal test McCrary density test (McCrary 2008), later refined Cattaneo, Idrobo, Titiunik (2019).Null Hypothesis (\\(H_0\\)): density running variable continuous cutoff.Alternative Hypothesis (\\(H_a\\)): discontinuity (jump) density function cutoff, suggesting manipulation.Interpretation:\nsignificant discontinuity suggests manipulation, violating RD assumptions.\nfailure reject \\(H_0\\) necessarily confirm validity, forms manipulation may remain undetected.\ntwo-sided manipulation, test fail detect .\nsignificant discontinuity suggests manipulation, violating RD assumptions.failure reject \\(H_0\\) necessarily confirm validity, forms manipulation may remain undetected.two-sided manipulation, test fail detect .","code":""},{"path":"sec-regression-discontinuity.html","id":"guidelines-for-assessing-manipulation","chapter":"27 Regression Discontinuity","heading":"27.5.2.2 Guidelines for Assessing Manipulation","text":"J. L. Zhang Rubin (2003), Lee (2009), Aronow, Baron, Pinson (2019) provide criteria evaluating manipulation risks.Knowing research design inside crucial anticipating possible manipulation attempts.Manipulation often one-sided, meaning subjects shift one direction relative cutoff. rare cases, two-sided manipulation may occur often cancels .also observe partial manipulation reality (e.g., subjects can imperfectly manipulate). However, since typically treat like fuzzy RD, encounter identification problems. contrast, complete manipulation lead serious identification issues.Bunching MethodologyBunching occurs individuals self-select specific values running variable (e.g., policy thresholds). See Kleven (2016) review.method helps estimate counterfactual distribution—density without manipulation.fraction individuals manipulated can estimated comparing observed densities counterfactual.running variable outcome simultaneously determined, modified RD estimator can used consistent estimation (Bajari et al. 2011):One-sided manipulation: Individuals shift one direction relative cutoff (similar monotonicity assumption instrumental variables).Bounded manipulation (regularity assumption): density individuals far threshold remains unaffected (Blomquist et al. 2021; Bertanha, McCallum, Seegert 2021).","code":""},{"path":"sec-regression-discontinuity.html","id":"steps-for-bunching-analysis","chapter":"27 Regression Discontinuity","heading":"27.5.2.3 Steps for Bunching Analysis","text":"Identify window bunching occurs (based Bosch, Dekker, Strohmaier (2020)). Perform robustness checks varying manipulation window.Estimate manipulation-free counterfactual distribution.Standard errors inference can calculated (following Chetty, Hendren, Katz (2016)), bootstrap resampling residuals used estimating counts individuals within bins. However, step may unnecessary large datasets.bunching test fails detect manipulation, proceed Placebo Test.McCrary Density Test (Discontinuity Forcing Variable)Cattaneo Density Test (Improved Version)","code":"\nlibrary(rdd)\n\nset.seed(1)\n\n# Simulated data without discontinuity\nx <- runif(100, -1, 1)\nDCdensity(x, 0)  # No discontinuity#> [1] 0.06355195\n\n# Simulated data with discontinuity\nx <- runif(1000, -1, 1)\nx <- x + 2 * (runif(1000, -1, 1) > 0 & x < 0)\nDCdensity(x, 0)  # Discontinuity detected#> [1] 0.001936782\nlibrary(rddensity)\n\n# Simulated continuous density\nset.seed(1)\nx   <- rnorm(100, mean = -0.5)\nrdd <- rddensity(X = x, vce = \"jackknife\")\nsummary(rdd)\n#> \n#> Manipulation testing using local polynomial density estimation.\n#> \n#> Number of obs =       100\n#> Model =               unrestricted\n#> Kernel =              triangular\n#> BW method =           estimated\n#> VCE method =          jackknife\n#> \n#> c = 0                 Left of c           Right of c          \n#> Number of obs         66                  34                  \n#> Eff. Number of obs    38                  25                  \n#> Order est. (p)        2                   2                   \n#> Order bias (q)        3                   3                   \n#> BW est. (h)           0.922               0.713               \n#> \n#> Method                T                   P > |T|             \n#> Robust                0.6715              0.5019              \n#> \n#> \n#> P-values of binomial tests (H0: p=0.5).\n#> \n#> Window Length              <c     >=c    P>|T|\n#> 0.563     + 0.563          26      20    0.4614\n#> 0.603     + 0.580          27      20    0.3817\n#> 0.643     + 0.596          30      20    0.2026\n#> 0.683     + 0.613          32      21    0.1690\n#> 0.722     + 0.630          32      22    0.2203\n#> 0.762     + 0.646          33      22    0.1770\n#> 0.802     + 0.663          33      23    0.2288\n#> 0.842     + 0.680          35      24    0.1925\n#> 0.882     + 0.696          36      24    0.1550\n#> 0.922     + 0.713          38      25    0.1299\n\n# Plot requires customization (refer to package documentation)"},{"path":"sec-regression-discontinuity.html","id":"placebo-tests","chapter":"27 Regression Discontinuity","heading":"27.5.3 Placebo Tests","text":"Placebo tests, also known falsification checks, assess whether discontinuities appear points treatment cutoff. helps verify observed effects causal rather artifacts method data.jumps outcome values cutoff (\\(X_i < c\\) \\(X_i \\geq c\\)).test involves shifting cutoff along running variable using bandwidth check discontinuities conditional mean outcome.approach similar balance checks experimental design, ensuring pre-existing differences. Remember, can test observables, unobservables.valid RD design, matching methods unnecessary. Just randomized experiments, balance naturally occur across threshold. adjustments required, suggests RD assumptions may invalid.","code":""},{"path":"sec-regression-discontinuity.html","id":"applications-of-placebo-tests","chapter":"27 Regression Discontinuity","heading":"27.5.3.1 Applications of Placebo Tests","text":"Testing Discontinuity Predetermined Covariates: Covariates affected treatment exhibit jump cutoff.Testing Discontinuities: Checking discontinuities arbitrary points along running variable.Using Placebo Outcomes: outcome variable affected treatment shows significant discontinuity, raises concerns RD validity.Assessing Sensitivity Covariates: RD estimates highly sensitive inclusion exclusion covariates.","code":""},{"path":"sec-regression-discontinuity.html","id":"mathematical-specification","chapter":"27 Regression Discontinuity","heading":"27.5.3.2 Mathematical Specification","text":"balance observable characteristics sides threshold can tested using:\\[\nZ_i = \\alpha_0 + \\alpha_1 f(X_i) + (X_i \\geq c) \\alpha_2 + [f(X_i) \\times (X_i \\geq c)]\\alpha_3 + u_i\n\\]:\\(X_i\\) = running variable\\(X_i\\) = running variable\\(Z_i\\) = predetermined characteristics (e.g., age, education, etc.)\\(Z_i\\) = predetermined characteristics (e.g., age, education, etc.)\\(\\alpha_2\\) zero \\(Z_i\\) unaffected treatment.\\(\\alpha_2\\) zero \\(Z_i\\) unaffected treatment.multiple covariates \\(Z_i\\) tested simultaneously, simulating joint distribution avoids false positives due multiple comparisons. step unnecessary covariates independent, independence unlikely practice.","code":""},{"path":"sec-regression-discontinuity.html","id":"sensitivity-to-bandwidth-choice","chapter":"27 Regression Discontinuity","heading":"27.5.4 Sensitivity to Bandwidth Choice","text":"choice bandwidth crucial RD estimation. Different bandwidth selection methods exist:Ad-hoc Substantively Driven: Based theoretical empirical reasoning.Data-Driven Selection (Cross-Validation): Optimizes bandwidth minimize prediction error.Conservative Approach: Uses robust optimal bandwidth selection methods (e.g., (Calonico, Cattaneo, Farrell 2020)).objective minimize mean squared error (MSE) estimated actual treatment effects.","code":""},{"path":"sec-regression-discontinuity.html","id":"assessing-sensitivity","chapter":"27 Regression Discontinuity","heading":"27.5.5 Assessing Sensitivity","text":"Results consistent across reasonable bandwidth choices.optimal bandwidth estimating treatment effects may differ optimal bandwidth testing covariates fairly close.","code":"\n# Load required package\nlibrary(rdd)\n\n# Simulate some data\nset.seed(123)\nn <- 100  # Sample size\n\n# Running variable centered around 0\nrunning_var <- runif(n, -1, 1)  \n\n# Treatment assigned at cutpoint 0\ntreatment   <- ifelse(running_var >= 0, 1, 0)  \n\n# Outcome variable\noutcome_var <- 2 * running_var + treatment * 1.5 + rnorm(n)  \n\n# Compute the optimal Imbens-Kalyanaraman bandwidth\nbandwidth <-\n    IKbandwidth(running_var,\n                outcome_var,\n                cutpoint = 0,\n                kernel = \"triangular\")\n\n# Print the bandwidth\nprint(bandwidth)\n#> [1] 0.4374142"},{"path":"sec-regression-discontinuity.html","id":"manipulation-robust-regression-discontinuity-bounds","chapter":"27 Regression Discontinuity","heading":"27.5.6 Manipulation-Robust Regression Discontinuity Bounds","text":"Regression Discontinuity designs rely assumption running variable \\(X_i\\) manipulable agents study. However, McCrary (2008) showed discontinuity density \\(X_i\\) cutoff may indicate manipulation, potentially invalidating RD estimates. common approach handling detected manipulation :manipulation detected, proceed RD analysis.manipulation detected, use “doughnut-hole” method (.e., excluding near-cutoff observations), contradicts RD principles.However, strict adherence rule can lead two problems:False Negatives: small sample size might fail detect manipulation, leading biased estimates manipulation still affects running variable.Loss Informative Data: Even manipulation detected, data may still contain valuable information causal inference.address challenges, Gerard, Rokkanen, Rothe (2020) introduce framework accounts manipulated observations rather discarding . approach:Identifies extent manipulation.Computes worst-case bounds treatment effects.Provides systematic way incorporate manipulated observations maintaining credibility RD analysis.manipulation believed unlikely, alternative approach conduct sensitivity analysis :Testing different hypothetical values \\(\\tau\\) affect bounds.Testing different hypothetical values \\(\\tau\\) affect bounds.Comparing results across various \\(\\tau\\) assumptions assess robustness.Comparing results across various \\(\\tau\\) assumptions assess robustness.Consider independent observations \\((X_i, Y_i, D_i)\\):\\(X_i\\): Running variable.\\(Y_i\\): Outcome variable.\\(D_i\\): Treatment indicator (\\(D_i = 1\\) \\(X_i \\geq c\\), \\(D_i = 0\\) otherwise).sharp RD design satisfies \\(D_i = (X_i \\geq c)\\), fuzzy RD design allows probabilistic treatment assignment.population consists two types units:Potentially-Assigned Units (\\(M_i = 0\\)): units follow standard RD framework. potential outcomes \\(Y_i(d)\\) potential treatment states \\(D_i(x)\\).Always-Assigned Units (\\(M_i = 1\\)): units always appear one side cutoff require potential outcomes.always-assigned units exist (\\(M_i = 1\\) units), standard RD model holds.","code":""},{"path":"sec-regression-discontinuity.html","id":"key-assumptions","chapter":"27 Regression Discontinuity","heading":"27.5.6.1 Key Assumptions","text":"Local Independence Continuity:\nTreatment probability jumps \\(c\\) among potentially-assigned units: \\[\nP(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0).\n\\]\ndefiers: \\(P(D^+ \\geq D^- | X = c, M = 0) = 1\\).\nPotential outcomes treatment states continuous \\(c\\).\ndensity running variable among potentially-assigned units, \\(F_{X|M=0}(x)\\), differentiable \\(c\\).\nTreatment probability jumps \\(c\\) among potentially-assigned units: \\[\nP(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0).\n\\]defiers: \\(P(D^+ \\geq D^- | X = c, M = 0) = 1\\).Potential outcomes treatment states continuous \\(c\\).density running variable among potentially-assigned units, \\(F_{X|M=0}(x)\\), differentiable \\(c\\).Smoothness Running Variable among Potentially-Assigned Units:\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).Restrictions Always-Assigned Units:\nAlways-assigned units satisfy \\(P(X \\geq c|M = 1) = 1\\).\ndensity running variable among always-assigned units, \\(F_{X|M=1}(x)\\), right-differentiable \\(c\\).\none-sided manipulation assumption allows identification proportion always-assigned units.\nAlways-assigned units satisfy \\(P(X \\geq c|M = 1) = 1\\).density running variable among always-assigned units, \\(F_{X|M=1}(x)\\), right-differentiable \\(c\\).one-sided manipulation assumption allows identification proportion always-assigned units.always-assigned units exist, RD design effectively becomes fuzzy, since: 1. potentially-assigned units receive treatment others . 2. Always-assigned units always treated (always untreated).","code":""},{"path":"sec-regression-discontinuity.html","id":"estimating-treatment-effects","chapter":"27 Regression Discontinuity","heading":"27.5.6.2 Estimating Treatment Effects","text":"potentially-assigned units, causal effect interest :\\[\n\\Gamma = E[Y(1) - Y(0) | X = c, D^+ > D^-, M = 0].\n\\]parameter represents local average treatment effect potentially-assigned compliers, .e., whose treatment status affected running variable crossing cutoff.","code":""},{"path":"sec-regression-discontinuity.html","id":"bounding-treatment-effects","chapter":"27 Regression Discontinuity","heading":"27.5.6.3 Bounding Treatment Effects","text":"approach bounding treatment effects consists two key steps:Estimating Proportion Always-Assigned Units:\ndone measuring discontinuity density \\(X\\) \\(c\\).\nlarger discontinuity, greater fraction always-assigned units.\ndone measuring discontinuity density \\(X\\) \\(c\\).larger discontinuity, greater fraction always-assigned units.Computing Worst-Case Bounds Treatment Effects:\nmanipulation exists, treatment effects must inferred using extreme-case scenarios.\nsharp RD designs, bounds estimated trimming extreme outcomes near cutoff.\nfuzzy RD designs, additional adjustments required account presence always-assigned units.\nmanipulation exists, treatment effects must inferred using extreme-case scenarios.sharp RD designs, bounds estimated trimming extreme outcomes near cutoff.fuzzy RD designs, additional adjustments required account presence always-assigned units.Extensions approach use covariates economic behavior assumptions refine bounds .Comparison Doughnut-Hole RD Designs","code":""},{"path":"sec-regression-discontinuity.html","id":"identification-challenges","chapter":"27 Regression Discontinuity","heading":"27.5.6.4 Identification Challenges","text":"central challenge manipulation-robust RD designs inability directly distinguish always-assigned potentially-assigned units. result, LATE \\(\\Gamma\\) point identified. Instead, establish sharp bounds \\(\\Gamma\\).bounds leverage stochastic dominance potential outcome cumulative distribution functions observed distributions. allows us infer treatment effects without making strong parametric assumptions.formalize population structure, define five types units:Potentially-Assigned Units:\n\\(C_0\\) (Compliers): Receive treatment \\(X \\geq c\\).\n\\(A_0\\) (Always-Takers): Always receive treatment, regardless \\(X\\).\n\\(N_0\\) (Never-Takers): Never receive treatment, regardless \\(X\\).\n\\(C_0\\) (Compliers): Receive treatment \\(X \\geq c\\).\\(A_0\\) (Always-Takers): Always receive treatment, regardless \\(X\\).\\(N_0\\) (Never-Takers): Never receive treatment, regardless \\(X\\).Always-Assigned Units:\n\\(T_1\\) (Treated Always-Assigned Units): Always appear cutoff receive treatment.\n\\(U_1\\) (Untreated Always-Assigned Units): Always appear cutoff receive treatment.\n\\(T_1\\) (Treated Always-Assigned Units): Always appear cutoff receive treatment.\\(U_1\\) (Untreated Always-Assigned Units): Always appear cutoff receive treatment.measure \\(\\tau\\), representing proportion always-assigned units near cutoff, point-identified using discontinuity density running variable \\(f_X\\) \\(c\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"identification-in-sharp-rd","chapter":"27 Regression Discontinuity","heading":"27.5.6.4.1 Identification in Sharp RD","text":"sharp RD design:Units left cutoff potentially-assigned units.observed distribution untreated outcomes, \\(Y(0)\\), among units corresponds outcomes potentially-assigned compliers (\\(C_0\\)) cutoff.estimate sharp bounds \\(\\Gamma\\), need assess distribution treated outcomes (\\(Y(1)\\)) compliers.However, information treated outcomes (\\(Y(1)\\)) cutoff available treated subpopulation, includes:Potentially-assigned compliers (\\(C_0\\))Potentially-assigned compliers (\\(C_0\\))Always-assigned treated units (\\(T_1\\))Always-assigned treated units (\\(T_1\\))Since \\(\\tau\\) point-identified, can construct sharp bounds \\(\\Gamma\\) adjusting presence \\(T_1\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"identification-in-fuzzy-rd","chapter":"27 Regression Discontinuity","heading":"27.5.6.4.2 Identification in Fuzzy RD","text":"fuzzy RD, treatment assignment deterministic. subpopulations observed treatment-status group cutoff :Source: Table page 848 Gerard, Rokkanen, Rothe (2020)Key Takeaways:Unit Types Combinations: five distinct unit types four combinations treatment assignments decisions relevant analysis. distinctions important affect potential outcomes analyzed bounded.Outcome Distributions: analysis involves estimating distribution potential outcomes (treated untreated) among potentially-assigned compliers cutoff.goal estimate distribution potential outcomes (treated untreated) potentially-assigned compliers cutoff.","code":""},{"path":"sec-regression-discontinuity.html","id":"three-step-process-for-bounding-treatment-effects","chapter":"27 Regression Discontinuity","heading":"27.5.6.5 Three-Step Process for Bounding Treatment Effects","text":"method obtain sharp bounds \\(\\Gamma\\) follows three steps:Bounding Potential Outcomes Treatment:\nUse observed treated outcomes estimate upper lower bounds \\(F_{Y(1)}(y | X = c, M = 0)\\).\nUse observed treated outcomes estimate upper lower bounds \\(F_{Y(1)}(y | X = c, M = 0)\\).Bounding Potential Outcomes Non-Treatment:\nUse observed untreated outcomes estimate upper lower bounds \\(F_{Y(0)}(y | X = c, M = 0)\\).\nUse observed untreated outcomes estimate upper lower bounds \\(F_{Y(0)}(y | X = c, M = 0)\\).Deriving Bounds \\(\\Gamma\\):\nUsing bounds Steps 1 2, compute sharp upper lower bounds local average treatment effect.\nUsing bounds Steps 1 2, compute sharp upper lower bounds local average treatment effect.Extreme Value Consideration:bounds account worst-case scenarios considering extreme assumptions distribution potential outcomes.bounds sharp (.e., tightened without additional assumptions) remain empirically relevant.","code":""},{"path":"sec-regression-discontinuity.html","id":"extensions","chapter":"27 Regression Discontinuity","heading":"27.5.6.6 Extensions","text":"Quantile Treatment Effects (QTEs)alternative average treatment effects quantile treatment effect (QTE), focuses different percentiles outcome distribution. Advantages QTE bounds include:Less Sensitivity Extreme Values: Unlike ATE bounds, QTE bounds less affected outliers outcome distribution.Informative Policy Analysis: Helps determine whether effects concentrated certain segments population.QTE vs. ATE Manipulation:ATE inference highly sensitive manipulation, confidence intervals widening significantly assumed manipulation increases.QTE inference remains meaningful even substantial manipulation.Discrete Outcome Extensions: framework applies continuous outcomes also discrete outcome variables.Role Behavioral AssumptionsMaking behavioral assumptions high treatment likelihood among always-assigned units can refine bounds.example, assuming always-assigned units treated allows narrower bounds treatment effects.Incorporation CovariatesIncluding pre-treatment covariates can refine treatment effect bounds.Covariates help:\nDistinguish potentially-assigned always-assigned units.\nImprove inference treatment effect heterogeneity.\nGuide policy targeting identifying unit types based observed characteristics.\nDistinguish potentially-assigned always-assigned units.Improve inference treatment effect heterogeneity.Guide policy targeting identifying unit types based observed characteristics.","code":"\ndevtools::install_github(\"francoisgerard/rdbounds/R\")\nlibrary(formattable)\nlibrary(data.table)\nlibrary(rdbounds)\nset.seed(123)\ndf <- rdbounds_sampledata(1000, covs = FALSE)\n#> [1] \"True tau: 0.117999815082062\"\n#> [1] \"True treatment effect on potentially-assigned: 2\"\n#> [1] \"True treatment effect on right side of cutoff: 2.35399944524618\"\nhead(df)\n#>            x         y treatment\n#> 1 -1.2532616  2.684827         0\n#> 2 -0.5146925  5.845219         0\n#> 3  3.4853777  6.166070         0\n#> 4  0.1576616  3.227139         0\n#> 5  0.2890962  7.031685         1\n#> 6  3.8350019 10.238570         1\n\nrdbounds_est <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        \n        # for median effect use \n        # type = \"qte\", \n        # percentiles = .5, \n        \n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.38047\"\n#> [1] \"2025-05-14 20:33:47.909864 Estimating CDFs for point estimates\"\n#> [1] \"2025-05-14 20:33:48.466955 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2025-05-14 20:33:50.700413 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2025-05-14 20:33:50.765021 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2025-05-14 20:33:54.245449 Beginning parallelized output by bootstrap..\"\n#> [1] \"2025-05-14 20:33:59.711629 Computing Confidence Intervals\"\n#> [1] \"2025-05-14 20:34:16.710439 Time taken:0.48 minutes\"\nrdbounds_summary(rdbounds_est, title_prefix = \"Sample Data Results\")\n#> [1] \"Time taken: 0.48 minutes\"\n#> [1] \"Sample size: 1000\"\n#> [1] \"Local Average Treatment Effect:\"\n#> $tau_hat\n#> [1] 0.3804665\n#> \n#> $tau_hat_CI\n#> [1] 0.4180576 1.4333618\n#> \n#> $takeup_increase\n#> [1] 0.6106973\n#> \n#> $takeup_increase_CI\n#> [1] 0.4332132 0.7881814\n#> \n#> $TE_SRD_naive\n#> [1] 1.589962\n#> \n#> $TE_SRD_naive_CI\n#> [1] 1.083977 2.095948\n#> \n#> $TE_SRD_bounds\n#> [1] 0.7165801 2.3454868\n#> \n#> $TE_SRD_CI\n#> [1] -1.200992  7.733214\n#> \n#> $TE_SRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_SRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_FRD_naive\n#> [1] 2.583271\n#> \n#> $TE_FRD_naive_CI\n#> [1] 1.506573 3.659969\n#> \n#> $TE_FRD_bounds\n#> [1] 1.273896 3.684860\n#> \n#> $TE_FRD_CI\n#> [1] -1.266341 10.306116\n#> \n#> $TE_FRD_bounds_refinementA\n#> [1] 1.273896 3.684860\n#> \n#> $TE_FRD_refinementA_CI\n#> [1] -1.266341 10.306116\n#> \n#> $TE_FRD_bounds_refinementB\n#> [1] 1.420594 3.677963\n#> \n#> $TE_FRD_refinementB_CI\n#> [1] NA NA\n#> \n#> $TE_FRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_FRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_SRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_FRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_SRD_right_bounds\n#> [1] -2.056178  3.650820\n#> \n#> $TE_SRD_right_CI\n#> [1] -11.044937   9.282217\n#> \n#> $TE_FRD_right_bounds\n#> [1] -2.900703  5.272370\n#> \n#> $TE_FRD_right_CI\n#> [1] -14.75421  15.58640\nrdbounds_est_tau <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        potential_taus = c(.025, .05, .1, .2),\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.38047\"\n#> [1] \"2025-05-14 20:34:18.345701 Estimating CDFs for point estimates\"\n#> [1] \"2025-05-14 20:34:18.580208 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2025-05-14 20:34:21.070091 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2025-05-14 20:34:21.121132 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2025-05-14 20:34:24.568454 Beginning parallelized output by bootstrap..\"\n#> [1] \"2025-05-14 20:34:29.367159 Estimating CDFs with fixed tau value of: 0.025\"\n#> [1] \"2025-05-14 20:34:29.434238 Estimating CDFs with fixed tau value of: 0.05\"\n#> [1] \"2025-05-14 20:34:29.531261 Estimating CDFs with fixed tau value of: 0.1\"\n#> [1] \"2025-05-14 20:34:29.598908 Estimating CDFs with fixed tau value of: 0.2\"\n#> [1] \"2025-05-14 20:34:30.932877 Beginning parallelized output by bootstrap x fixed tau..\"\n#> [1] \"2025-05-14 20:34:33.544657 Computing Confidence Intervals\"\n#> [1] \"2025-05-14 20:34:50.413887 Time taken:0.54 minutes\"\ncausalverse::plot_rd_aa_share(rdbounds_est_tau) # For SRD (default)\n# causalverse::plot_rd_aa_share(rdbounds_est_tau, rd_type = \"FRD\")  # For FRD"},{"path":"sec-regression-discontinuity.html","id":"sec-fuzzy-regression-discontinuity-design","chapter":"27 Regression Discontinuity","heading":"27.6 Fuzzy Regression Discontinuity Design","text":"Fuzzy Regression Discontinuity Design occurs assignment rule cutoff perfectly determine treatment status instead causes discontinuity probability treatment. Unlike Sharp RD Design, crossing threshold fully determines treatment, fuzzy RD, individuals sides threshold may may receive treatment.treatment strictly assigned cutoff, usual RD estimator (assumes deterministic assignment) valid. Instead, use cutoff instrumental variable estimate treatment effect compliers, .e., individuals whose treatment status depends whether cross threshold.Define indicator variable \\(Z_i\\) (.e., instrument treatment assignment) captures whether individual cutoff:\\[\nZ_i =\n\\begin{cases}\n1 & \\text{} X_i \\geq c \\\\\n0 & \\text{} X_i < c\n\\end{cases}\n\\]variable \\(Z_i\\) serves instrument treatment variable \\(D_i\\) :strongly correlates treatment (\\(D_i\\)).strongly correlates treatment (\\(D_i\\)).exogenous, meaning affects outcome effect treatment.exogenous, meaning affects outcome effect treatment.","code":""},{"path":"sec-regression-discontinuity.html","id":"compliance-types","chapter":"27 Regression Discontinuity","heading":"27.6.1 Compliance Types","text":"Since treatment assignment longer deterministic, individuals can classified four groups based respond cutoff:Compliers (\\(C_0\\)): Individuals receive treatment \\(X_i \\geq c\\).Always-Takers (\\(A_0\\)): Individuals always receive treatment, regardless whether \\(X_i \\geq c\\).Never-Takers (\\(N_0\\)): Individuals never receive treatment, even \\(X_i \\geq c\\).Defiers (violating monotonicity, assumed zero): Individuals receive treatment \\(X_i < c\\) \\(X_i \\geq c\\).Fuzzy RD estimator identifies treatment effect compliers, treatment status depends \\(Z_i\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"estimating-the-local-average-treatment-effect","chapter":"27 Regression Discontinuity","heading":"27.6.2 Estimating the Local Average Treatment Effect","text":"estimate LATE using ratio discontinuities:\\[\n\\text{LATE} = \\frac{\\lim\\limits_{x \\downarrow c}E[Y|X = x] - \\lim\\limits_{x \\uparrow c}E[Y|X = x]}{\\lim\\limits_{x \\downarrow c } E[D |X = x] - \\lim\\limits_{x \\uparrow c}E[D |X=x]}\n\\]Intuitively, formula represents:\\[\n\\text{LATE} = \\frac{\\text{Discontinuity } E[Y|X]}{\\text{Discontinuity } E[D|X]}\n\\]:numerator captures jump expected outcome cutoff.numerator captures jump expected outcome cutoff.denominator captures jump probability treatment cutoff.denominator captures jump probability treatment cutoff.ratio valid three key assumptions:Continuity potential outcomes: \\(E[Y(d)|X]\\) continuous \\(X = c\\) \\(d \\\\{0,1\\}\\).Monotonicity: defiers (\\(P(D^+ \\geq D^- | X = c) = 1\\)).First-stage relevance: discontinuity \\(P(D = 1 | X)\\) \\(X = c\\).conditions hold, fuzzy RD estimator gives valid estimate causal effect treatment compliers.","code":""},{"path":"sec-regression-discontinuity.html","id":"equivalent-representation-using-expectations","chapter":"27 Regression Discontinuity","heading":"27.6.3 Equivalent Representation Using Expectations","text":"can also define LATE terms conditional expectations treatment outcome:\\[\n\\lim_{\\epsilon \\0} \\frac{E[Y |Z = 1] - E[Y |Z=0]}{E[D|Z = 1] - E[D|Z = 0]}\n\\]\\(Z\\) instrument (indicator cutoff). approach highlights IV nature fuzzy RD.","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-strategies","chapter":"27 Regression Discontinuity","heading":"27.6.4 Estimation Strategies","text":"two equivalent ways estimate LATE practice:Approach 1: Two-Step EstimationEstimate Sharp RD Outcome \\(Y\\):\nRegress \\(Y\\) \\(X\\) using local linear regression either side \\(c\\).\nEstimate discontinuity \\(E[Y|X]\\) \\(c\\).\nRegress \\(Y\\) \\(X\\) using local linear regression either side \\(c\\).Estimate discontinuity \\(E[Y|X]\\) \\(c\\).Estimate Sharp RD Treatment \\(D\\):\nRegress \\(D\\) \\(X\\) using local linear regression either side \\(c\\).\nEstimate discontinuity \\(E[D|X]\\) \\(c\\).\nRegress \\(D\\) \\(X\\) using local linear regression either side \\(c\\).Estimate discontinuity \\(E[D|X]\\) \\(c\\).Compute Ratio:\nDivide estimated discontinuity \\(E[Y|X]\\) estimated discontinuity \\(E[D|X]\\).\nDivide estimated discontinuity \\(E[Y|X]\\) estimated discontinuity \\(E[D|X]\\).Mathematically:\\[\n\\widehat{\\text{LATE}} = \\frac{\\widehat{E[Y | X = c^+]} - \\widehat{E[Y | X = c^-]}}{\\widehat{E[D | X = c^+]} - \\widehat{E[D | X = c^-]}}.\n\\]Approach 2: Instrumental Variables RegressionSubset data observations close \\(c\\).Subset data observations close \\(c\\).Use \\(Z_i\\) (/cutoff indicator) instrument \\(D_i\\) two-stage least squares regression:\nFirst-stage regression (predicting treatment using cutoff indicator):\n\\[\nD_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i\n\\]\ncaptures effect cutoff treatment assignment.\n\nSecond-stage regression (estimating treatment effect using predicted \\(D_i\\)):\n\\[\nY_i = \\delta + \\tau \\widehat{D}_i + \\lambda X_i + \\nu_i\n\\]\ncoefficient \\(\\tau\\) gives LATE estimate.\n\nUse \\(Z_i\\) (/cutoff indicator) instrument \\(D_i\\) two-stage least squares regression:First-stage regression (predicting treatment using cutoff indicator):\n\\[\nD_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i\n\\]\ncaptures effect cutoff treatment assignment.\nFirst-stage regression (predicting treatment using cutoff indicator):\\[\nD_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i\n\\]captures effect cutoff treatment assignment.Second-stage regression (estimating treatment effect using predicted \\(D_i\\)):\n\\[\nY_i = \\delta + \\tau \\widehat{D}_i + \\lambda X_i + \\nu_i\n\\]\ncoefficient \\(\\tau\\) gives LATE estimate.\nSecond-stage regression (estimating treatment effect using predicted \\(D_i\\)):\\[\nY_i = \\delta + \\tau \\widehat{D}_i + \\lambda X_i + \\nu_i\n\\]coefficient \\(\\tau\\) gives LATE estimate.","code":""},{"path":"sec-regression-discontinuity.html","id":"practical-considerations-6","chapter":"27 Regression Discontinuity","heading":"27.6.5 Practical Considerations","text":"Bandwidth Selection: observations near cutoff used. Methods like cross-validation Calonico, Cattaneo, Farrell (2020) optimal bandwidth selection can help.Polynomial Order: local linear model typically preferred, higher-order polynomials may used cautiously.Robust Inference: Standard errors computed using heteroskedasticity-robust clustered standard errors necessary.Strong first-stage (e.g., F-stat \\(\\ge\\) 16); exclusion restriction violations; model outcome treatment take-.","code":""},{"path":"sec-regression-discontinuity.html","id":"steps-for-fuzzy-rd","chapter":"27 Regression Discontinuity","heading":"27.6.6 Steps for Fuzzy RD","text":"1. VisualizationGraph Outcome Variable:\nCompute average outcome within bins running variable \\(X_i\\).\nChoose bins large enough display smooth trends small enough reveal discontinuities cutoff.\nOverlay smoothed regression line either side cutoff visualize jumps.\nCompute average outcome within bins running variable \\(X_i\\).Choose bins large enough display smooth trends small enough reveal discontinuities cutoff.Overlay smoothed regression line either side cutoff visualize jumps.Graph Probability Treatment:\nCompute average treatment probability within bins.\nPlot \\(E[D|X]\\) check discontinuity \\(X = c\\), confirming first-stage relevance instrument.\nCompute average treatment probability within bins.Plot \\(E[D|X]\\) check discontinuity \\(X = c\\), confirming first-stage relevance instrument.2. Estimation Treatment EffectUse Two-Stage Least Squares estimate Local Average Treatment Effect:First Stage (Predict Treatment Using Cutoff Indicator \\(Z_i\\)): \\[\nD_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i\n\\]\nregression captures treatment probability changes cutoff.\ncoefficient \\(\\beta\\) measures jump treatment probability \\(X = c\\).\nregression captures treatment probability changes cutoff.coefficient \\(\\beta\\) measures jump treatment probability \\(X = c\\).Second Stage (Estimate Outcome Using Predicted Treatment): \\[\nY_i = \\delta + \\tau \\widehat{D}_i + \\lambda X_i + \\nu_i\n\\]\ncoefficient \\(\\tau\\) gives LATE, estimates treatment effect compliers.\ncoefficient \\(\\tau\\) gives LATE, estimates treatment effect compliers.3. Robustness ChecksAssess Possible Jumps Covariates:\nCheck whether pre-determined covariates (e.g., age, income) exhibit discontinuities cutoff.\ncovariates jump, may indicate endogenous sorting omitted variable bias.\nCheck whether pre-determined covariates (e.g., age, income) exhibit discontinuities cutoff.covariates jump, may indicate endogenous sorting omitted variable bias.Hypothesis Testing Bunching (McCrary Test):\nTest manipulation running variable examining whether density \\(X_i\\) changes discontinuously \\(c\\).\nsignificant density jump suggests sorting behavior, invalidate RD assumptions.\nTest manipulation running variable examining whether density \\(X_i\\) changes discontinuously \\(c\\).significant density jump suggests sorting behavior, invalidate RD assumptions.Placebo Tests:\nRepeat analysis fake cutoffs (values \\(X\\) intervention occurs).\ntreatment effect appears placebo cutoff, suggests spurious RD effect.\nRepeat analysis fake cutoffs (values \\(X\\) intervention occurs).treatment effect appears placebo cutoff, suggests spurious RD effect.Varying Bandwidth Sensitivity:\nRe-run analysis using different bandwidths around cutoff.\nCheck whether estimates remain stable window narrows expands.\nRe-run analysis using different bandwidths around cutoff.Check whether estimates remain stable window narrows expands.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-sharp-regression-discontinuity-design","chapter":"27 Regression Discontinuity","heading":"27.7 Sharp Regression Discontinuity Design","text":"Sharp Regression Discontinuity Design occurs treatment assignment follows strict rule known cutoff. , units receive treatment running variable \\(X_i\\) crosses threshold \\(c\\):\\[\nD_i =\n\\begin{cases}\n1 & \\text{} X_i \\geq c \\\\\n0 & \\text{} X_i < c\n\\end{cases}\n\\]Unlike Fuzzy RD, treatment probability changes discontinuously deterministic, Sharp RD ensures perfect compliance cutoff rule.key idea units just just cutoff nearly identical expectation, except treatment status. mimics randomized experiments local neighborhood around \\(X = c\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"assumptions-for-identification","chapter":"27 Regression Discontinuity","heading":"27.7.1 Assumptions for Identification","text":"valid Sharp RD design, assume:Continuity Conditional Expectation Potential Outcomes\nexpected outcome given \\(X\\) smooth \\(c\\) absence treatment: \\[\n\\lim_{x \\uparrow c} E[Y(0) | X = x] = \\lim_{x \\downarrow c} E[Y(0) | X = x].\n\\]\nensures observed discontinuity \\(E[Y | X]\\) \\(X = c\\) due treatment, pre-existing differences.\nexpected outcome given \\(X\\) smooth \\(c\\) absence treatment: \\[\n\\lim_{x \\uparrow c} E[Y(0) | X = x] = \\lim_{x \\downarrow c} E[Y(0) | X = x].\n\\]ensures observed discontinuity \\(E[Y | X]\\) \\(X = c\\) due treatment, pre-existing differences.Manipulation Running Variable\nAgents perfectly sort around cutoff (e.g., students manipulating test scores qualify scholarship).\ntypically checked using McCrary density test detect discontinuities density \\(X\\) \\(c\\).\nAgents perfectly sort around cutoff (e.g., students manipulating test scores qualify scholarship).typically checked using McCrary density test detect discontinuities density \\(X\\) \\(c\\).Local Randomization\nNear cutoff, individuals good randomly assigned treatment control.\nNear cutoff, individuals good randomly assigned treatment control.conditions hold, Sharp RD estimator provides unbiased estimate causal effect treatment.","code":""},{"path":"sec-regression-discontinuity.html","id":"estimating-the-local-average-treatment-effect-1","chapter":"27 Regression Discontinuity","heading":"27.7.2 Estimating the Local Average Treatment Effect","text":"treatment effect cutoff given :\\[\n\\tau = \\lim_{x \\downarrow c}E[Y | X = x] - \\lim_{x \\uparrow c} E[Y | X = x].\n\\]represents jump expected outcome cutoff.","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-methods","chapter":"27 Regression Discontinuity","heading":"27.7.3 Estimation Methods","text":"Local Linear RegressionA common approach estimate separate linear regressions side cutoff:observations cutoff \\((X < c)\\):\\[\nY_i = \\alpha + \\beta (X_i - c) + \\epsilon_i.\n\\]observations cutoff \\((X \\geq c)\\):\\[\nY_i = \\gamma + \\delta (X_i - c) + \\tau D_i + \\nu_i.\n\\], coefficient \\(\\tau\\) captures treatment effect \\(X = c\\).practice, estimate:\\[\n\\hat{\\tau} = \\hat{E}[Y | X = c^+] - \\hat{E}[Y | X = c^-].\n\\]can implemented using Weighted Least Squares observations near cutoff receiving higher weights.Global Polynomial RegressionAn alternative approach use polynomial regression:\\[\nY_i = \\alpha + \\sum_{k=1}^{K} \\beta_k (X_i - c)^k + \\tau D_i + \\epsilon_i.\n\\]:Higher-order terms \\((X_i - c)^k\\) capture nonlinear relationships.Higher-order terms \\((X_i - c)^k\\) capture nonlinear relationships.Typical choices \\(K\\) 2 3, higher orders may lead overfitting.Typical choices \\(K\\) 2 3, higher orders may lead overfitting.Nonparametric Local RegressionInstead assuming linear polynomial relationship, local regression (kernel-based) method estimates:\\[\nE[Y | X = x] = \\sum_{=1}^{n} K_h (X_i - x) Y_i.\n\\]\\(K_h\\) kernel function (e.g., Epanechnikov), \\(h\\) bandwidth.smaller \\(h\\) captures local variation increases variance.larger \\(h\\) smooths noise risks bias.","code":""},{"path":"sec-regression-discontinuity.html","id":"steps-for-sharp-rd","chapter":"27 Regression Discontinuity","heading":"27.7.4 Steps for Sharp RD","text":"VisualizationGraph outcome variable:\nCompute binned averages \\(Y_i\\) intervals \\(X\\).\nChoose bin sizes balance smoothness clarity.\nOverlay smoothed regression line side \\(c\\).\nCompute binned averages \\(Y_i\\) intervals \\(X\\).Choose bin sizes balance smoothness clarity.Overlay smoothed regression line side \\(c\\).Graph running variable’s density:\nUse histograms check manipulation.\nConduct McCrary density test detect discontinuities.\nUse histograms check manipulation.Conduct McCrary density test detect discontinuities.Estimation Treatment EffectRun local linear regression separately sides cutoff.Use nonparametric methods (e.g., kernel regression) robustness.Estimate treatment effect using: \\[\n\\hat{\\tau} = \\hat{E}[Y | X = c^+] - \\hat{E}[Y | X = c^-].\n\\]Robustness ChecksCheck Jumps CovariatesIf pre-determined covariate jumps cutoff, suggests sorting omitted variable bias.pre-determined covariate jumps cutoff, suggests sorting omitted variable bias.Run RD regressions covariate:\n\\[\nW_i = \\alpha + \\beta (X_i - c) + \\gamma D_i + \\epsilon_i.\n\\]Run RD regressions covariate:\\[\nW_i = \\alpha + \\beta (X_i - c) + \\gamma D_i + \\epsilon_i.\n\\]significant \\(\\gamma\\) suggests violation continuity assumptions.significant \\(\\gamma\\) suggests violation continuity assumptions.McCrary Density Test (Checking Manipulation)Run McCrary test examine whether density \\(X_i\\) exhibits discontinuity \\(c\\).significant jump indicates sorting behavior, can invalidate RD design.Placebo TestsPerform fake cutoff tests estimating RD effects arbitrary points \\(c^*\\).significant effects appear non-cutoff points, RD design may picking spurious trends.Varying BandwidthRe-run RD analysis using different bandwidths \\(h\\).results change dramatically, treatment effects may highly sensitive bandwidth choice.Use data-driven bandwidth selection methods (e.g., G. Imbens Kalyanaraman (2012)).","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-regression-kink-design","chapter":"27 Regression Discontinuity","heading":"27.8 Regression Kink Design","text":"Regression Kink Design (RKD) extends logic RD exploiting changes slope treatment intensity function known threshold rather discontinuous jump treatment assignment.Instead RD jump treatment probability \\(X = c\\), treatment function \\(b(X)\\) exhibits kink cutoff:Sharp RKD, kink deterministic, meaning treatment function \\(b(X)\\) changes slope exactly \\(X = c\\).Fuzzy RKD, treatment assignment remains probabilistic, requiring instrumental variable approach similar Fuzzy RD.Example: Unemployment BenefitsConsider unemployment insurance program benefits increase diminishing rate prior earnings increase. function governing benefits, \\(b(X)\\), exhibits kink threshold \\(X = c\\). RKD framework allows us estimate marginal causal effect additional benefits employment duration.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-identification-in-sharp-regression-kink-design","chapter":"27 Regression Discontinuity","heading":"27.8.1 Identification in Sharp Regression Kink Design","text":"Sharp RKD, treatment intensity function \\(b(X)\\) exhibits known change slope \\(X = c\\), formally:\\[\nD_i = b(X_i), \\quad \\text{} b(X) \\text{ kink } X = c.\n\\]key identification assumption potential outcome function \\(E[Y(d) | X]\\) smooth \\(X\\). Thus, observed change slope \\(E[Y | X]\\) \\(X = c\\) can attributed change \\(b(X)\\).causal effect interest :\\[\n\\alpha_{KRD} = \\frac{\\lim\\limits_{x \\downarrow c} \\frac{d}{dx}E[Y |X = x]- \\lim\\limits_{x \\uparrow c} \\frac{d}{dx}E[Y |X = x]}{\\lim\\limits_{x \\downarrow c} \\frac{d}{dx}b(x) - \\lim\\limits_{x \\uparrow c} \\frac{d}{dx}b(x)}.\n\\]:\\(b(X)\\) known function determining treatment intensity.\\(b(X)\\) known function determining treatment intensity.numerator captures discontinuous change slope expected outcome \\(X = c\\).numerator captures discontinuous change slope expected outcome \\(X = c\\).denominator captures change slope treatment function \\(X = c\\).denominator captures change slope treatment function \\(X = c\\).\\(b(X)\\) known deterministic, denominator non-random, allowing precise estimation \\(\\alpha_{KRD}\\).Assumptions IdentificationContinuity Potential Outcomes\nexpected potential outcomes \\(E[Y(d)|X]\\) smooth \\(X\\) (jumps).\nexpected potential outcomes \\(E[Y(d)|X]\\) smooth \\(X\\) (jumps).Manipulation Running Variable\ndensity \\(X\\) continuous \\(X = c\\), implying agents sort based kink.\ndensity \\(X\\) continuous \\(X = c\\), implying agents sort based kink.First-Stage Validity\nslope \\(b(X)\\) must change \\(X = c\\) (.e., kink must exist).\nslope \\(b(X)\\) must change \\(X = c\\) (.e., kink must exist).assumptions hold, \\(\\alpha_{KRD}\\) represents marginal causal effect treatment intensity outcome.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-identification-in-fuzzy-regression-kink-design","chapter":"27 Regression Discontinuity","heading":"27.8.2 Identification in Fuzzy Regression Kink Design","text":"Fuzzy RKD, treatment function \\(D_i\\) directly follow deterministic function \\(b(X)\\) instead exhibits kink probability distribution:\\[\nE[D | X] \\text{ kink } X = c.\n\\]treatment intensity function unknown, requiring instrumental variable (IV) strategy, analogous Fuzzy RD.causal effect given :\\[\n\\alpha_{KRD} = \\frac{\\lim\\limits_{x \\downarrow c} \\frac{d}{dx}E[Y |X = x]- \\lim\\limits_{x \\uparrow c} \\frac{d}{dx}E[Y |X = x]}{\\lim\\limits_{x \\downarrow c} \\frac{d}{dx}E[D |X = x]- \\lim\\limits_{x \\uparrow c} \\frac{d}{dx}E[D |X = x]}.\n\\]:numerator measures kink expected outcome.numerator measures kink expected outcome.denominator measures kink probability treatment.denominator measures kink probability treatment.ratio provides local instrumental variable estimate causal effect compliers.ratio provides local instrumental variable estimate causal effect compliers.Identification AssumptionsIn addition Sharp RKD assumptions, Fuzzy RKD requires:Monotonicity\nindividuals decrease treatment intensity others increase kink (analogous Fuzzy RD monotonicity).\nindividuals decrease treatment intensity others increase kink (analogous Fuzzy RD monotonicity).Relevance Kink\nmust statistically significant slope change \\(E[D | X]\\) \\(X = c\\).\nmust statistically significant slope change \\(E[D | X]\\) \\(X = c\\).assumptions hold, Fuzzy RKD estimator identifies local treatment effect.","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-of-rkd-effects","chapter":"27 Regression Discontinuity","heading":"27.8.3 Estimation of RKD Effects","text":"RKD estimation involves three main steps:Step 1: Estimating Kink Outcome FunctionEstimate left- right-hand derivatives \\(E[Y | X]\\):\\[\n\\frac{d}{dx}E[Y | X] = \\lim_{h \\0} \\frac{E[Y | X = c + h] - E[Y | X = c - h]}{h}.\n\\]can done using:Local linear regression either side kink.Local linear regression either side kink.Higher-order polynomial regression improved flexibility.Higher-order polynomial regression improved flexibility.Step 2: Estimating Kink Treatment FunctionFor Sharp RKD, kink \\(b(X)\\) known.Fuzzy RKD, estimate kink \\(E[D | X]\\):\\[\n\\frac{d}{dx}E[D | X] = \\lim_{h \\0} \\frac{E[D | X = c + h] - E[D | X = c - h]}{h}.\n\\]Use local regression piecewise polynomials estimate slope.Step 3: Compute RKD EstimatorFor Sharp RKD:\\[\n\\hat{\\alpha}_{KRD} = \\frac{\\hat{\\tau}_Y}{\\tau_b},\n\\]:\\(\\hat{\\tau}_Y\\) estimated kink \\(E[Y | X]\\).\\(\\hat{\\tau}_Y\\) estimated kink \\(E[Y | X]\\).\\(\\tau_b\\) known slope change \\(b(X)\\).\\(\\tau_b\\) known slope change \\(b(X)\\).Fuzzy RKD:\\[\n\\hat{\\alpha}_{KRD} = \\frac{\\hat{\\tau}_Y}{\\hat{\\tau}_D}.\n\\]:\\(\\hat{\\tau}_D\\) estimated kink \\(E[D | X]\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"robustness-checks-1","chapter":"27 Regression Discontinuity","heading":"27.8.4 Robustness Checks","text":"Assess Covariate Smoothness\nVerify pre-determined covariates (e.g., age, education) exhibit kinks \\(X = c\\).\nVerify pre-determined covariates (e.g., age, education) exhibit kinks \\(X = c\\).Check Manipulation Running Variable\nPerform McCrary test ensure density \\(X\\) continuous \\(X = c\\).\nPerform McCrary test ensure density \\(X\\) continuous \\(X = c\\).Placebo Kinks\nTest spurious kinks arbitrary values \\(X\\).\nTest spurious kinks arbitrary values \\(X\\).Bandwidth Sensitivity\nEstimate RKD effects varying bandwidths check consistency.\nEstimate RKD effects varying bandwidths check consistency.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-multi-cutoff-regression-discontinuity-design","chapter":"27 Regression Discontinuity","heading":"27.9 Multi-Cutoff Regression Discontinuity Design","text":"Multi-Cutoff Regression Discontinuity Design extends standard RD framework allowing multiple cutoff points across different groups geographic regions. Instead single threshold \\(c\\), different subgroups assigned different cutoffs \\(C_i\\). framework allows heterogeneous treatment effect function:\\[\n\\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c].\n\\]Use Multi-Cutoff RD?Policy Variation: Policies often implement different cutoffs across regions institutions (e.g., different states setting different minimum test scores scholarship eligibility).Generalizability: Allows estimation treatment effects across multiple populations instead relying single threshold.Improved Precision: Leveraging multiple thresholds can enhance statistical power compared single-cutoff RD.multi-cutoff RD framework provides several advantages:Estimation Local Heterogeneous Effects\nUnlike standard RD, estimates single treatment effect, multi-cutoff RD allows heterogeneity effects across groups.\nUnlike standard RD, estimates single treatment effect, multi-cutoff RD allows heterogeneity effects across groups.Improved Precision\nobservations across different thresholds can increase statistical power.\nobservations across different thresholds can increase statistical power.Policy Implications\nUseful settings policy thresholds vary (e.g., different states setting different income eligibility limits welfare programs).\nUseful settings policy thresholds vary (e.g., different states setting different income eligibility limits welfare programs).","code":""},{"path":"sec-regression-discontinuity.html","id":"identification","chapter":"27 Regression Discontinuity","heading":"27.9.1 Identification","text":"potential outcomes framework, unit \\(\\) :running variable \\(X_i\\).running variable \\(X_i\\).cutoff specific group \\(C_i\\).cutoff specific group \\(C_i\\).binary treatment indicator:binary treatment indicator:\\[\nD_i = (X_i \\geq C_i).\n\\]observed outcome :\\[\nY_i = D_i Y_{1i} + (1 - D_i) Y_{0i}.\n\\]treatment effect expected difference potential outcomes:\\[\n\\tau(x, c) = E[Y_{1i} - Y_{0i} | X_i = x, C_i = c].\n\\]","code":""},{"path":"sec-regression-discontinuity.html","id":"key-assumptions-1","chapter":"27 Regression Discontinuity","heading":"27.9.2 Key Assumptions","text":"ensure causal identification, extend standard RD assumptions:Continuity Potential Outcomes\nexpected potential outcomes \\(E[Y(0)|X]\\) \\(E[Y(1)|X]\\) smooth functions \\(X\\) cutoff \\(C_i\\).\nFormally: \\[\n\\lim_{x \\uparrow C_i} E[Y(0)|X=x, C_i=c] = \\lim_{x \\downarrow C_i} E[Y(0)|X=x, C_i=c].\n\\]\nexpected potential outcomes \\(E[Y(0)|X]\\) \\(E[Y(1)|X]\\) smooth functions \\(X\\) cutoff \\(C_i\\).Formally: \\[\n\\lim_{x \\uparrow C_i} E[Y(0)|X=x, C_i=c] = \\lim_{x \\downarrow C_i} E[Y(0)|X=x, C_i=c].\n\\]Manipulation Running Variable\ndensity \\(X_i\\) must continuous \\(C_i\\), ensuring individuals selectively sort assigned cutoff.\ndensity \\(X_i\\) must continuous \\(C_i\\), ensuring individuals selectively sort assigned cutoff.Local Randomization\nNear cutoff, units -good--randomly assigned treatment control.\nNear cutoff, units -good--randomly assigned treatment control.Independence Across Cutoffs\ncutoff assignment rule exogenous correlated unobserved determinants \\(Y\\).\ncutoff assignment rule exogenous correlated unobserved determinants \\(Y\\).assumptions hold, cutoff provides valid local treatment effect estimate.","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-approaches","chapter":"27 Regression Discontinuity","heading":"27.9.3 Estimation Approaches","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"pooling-cutoffs-with-fixed-effects","chapter":"27 Regression Discontinuity","heading":"27.9.3.1 Pooling Cutoffs with Fixed Effects","text":"straightforward way estimate multi-cutoff RD include cutoff fixed effects:\\[\nY_i = \\alpha + \\beta (X_i - C_i) + \\tau D_i + \\gamma C_i + \\epsilon_i.\n\\]:\\(\\tau\\) captures average treatment effect across cutoffs.\\(\\tau\\) captures average treatment effect across cutoffs.\\(C_i\\) included fixed effect account different intercepts across groups.\\(C_i\\) included fixed effect account different intercepts across groups.","code":""},{"path":"sec-regression-discontinuity.html","id":"separate-rd-estimation-for-each-cutoff","chapter":"27 Regression Discontinuity","heading":"27.9.3.2 Separate RD Estimation for Each Cutoff","text":"Instead pooling, can estimate separate RD effects \\(C_i\\):\\[\n\\tau_c = \\lim_{x \\downarrow C_i}E[Y|X = x, C_i = c] - \\lim_{x \\uparrow C_i} E[Y|X = x, C_i = c].\n\\]approach allows heterogeneous treatment effects.","code":""},{"path":"sec-regression-discontinuity.html","id":"interaction-model-for-heterogeneous-effects","chapter":"27 Regression Discontinuity","heading":"27.9.3.3 Interaction Model for Heterogeneous Effects","text":"estimate treatment effects vary \\(C_i\\), interact \\(D_i\\) \\(C_i\\):\\[\nY_i = \\alpha + \\beta (X_i - C_i) + \\tau D_i + \\lambda D_i C_i + \\epsilon_i.\n\\]:\\(\\lambda\\) captures treatment effect varies cutoff.\\(\\lambda\\) captures treatment effect varies cutoff.significant \\(\\lambda\\) implies \\(\\tau(x, c)\\) constant across cutoffs.significant \\(\\lambda\\) implies \\(\\tau(x, c)\\) constant across cutoffs.","code":""},{"path":"sec-regression-discontinuity.html","id":"nonparametric-local-estimation","chapter":"27 Regression Discontinuity","heading":"27.9.3.4 Nonparametric Local Estimation","text":"fully flexible approach estimates \\(\\tau(x, c)\\) separately cutoff using kernel-based methods:\\[\n\\hat{\\tau}(c) = \\frac{\\sum_{=1}^{n} K_h (X_i - C_i) D_i Y_i}{\\sum_{=1}^{n} K_h (X_i - C_i) D_i} - \\frac{\\sum_{=1}^{n} K_h (X_i - C_i) (1 - D_i) Y_i}{\\sum_{=1}^{n} K_h (X_i - C_i) (1 - D_i)}.\n\\]:\\(K_h(\\cdot)\\) kernel function (e.g., Epanechnikov).\\(K_h(\\cdot)\\) kernel function (e.g., Epanechnikov).\\(h\\) bandwidth, chosen via cross-validation.\\(h\\) bandwidth, chosen via cross-validation.","code":""},{"path":"sec-regression-discontinuity.html","id":"robustness-checks-2","chapter":"27 Regression Discontinuity","heading":"27.9.4 Robustness Checks","text":"Covariate Balance CutoffTest whether pre-treatment covariates show jumps \\(C_i\\).Test whether pre-treatment covariates show jumps \\(C_i\\).Run placebo RD regressions covariates:\n\\[\nW_i = \\alpha + \\beta (X_i - C_i) + \\gamma D_i + \\epsilon_i.\n\\]Run placebo RD regressions covariates:\\[\nW_i = \\alpha + \\beta (X_i - C_i) + \\gamma D_i + \\epsilon_i.\n\\]significant \\(\\gamma\\) suggests RD assumptions violated.significant \\(\\gamma\\) suggests RD assumptions violated.McCrary Density TestPerform McCrary test separately cutoff check manipulation:\n\\[\nf(X) \\text{ continuous } X = C_i.\n\\]Perform McCrary test separately cutoff check manipulation:\\[\nf(X) \\text{ continuous } X = C_i.\n\\]discontinuities exist, individuals may sorting around cutoffs.discontinuities exist, individuals may sorting around cutoffs.Placebo CutoffsImplement fake cutoffs re-estimate \\(\\tau(x, c)\\).significant effects appear, RD estimates may biased.Varying BandwidthsRe-estimate treatment effects using different bandwidths.\\(\\hat{\\tau}(x,c)\\) changes drastically, suggests sensitivity bandwidth choice.","code":""},{"path":"sec-regression-discontinuity.html","id":"sec-multi-score-regression-discontinuity-design","chapter":"27 Regression Discontinuity","heading":"27.10 Multi-Score Regression Discontinuity Design","text":"Multi-Score Regression Discontinuity Design extends standard single-score RD multi-cutoff RD introducing multiple running variables simultaneously determine treatment eligibility. Instead relying single threshold assignment, treatment now depends combination multiple continuous scores crossing predetermined cutoffs.Multi-score RD relevant policy eligibility based multiple criteria, :Education: Honors program admission based math English scores.Education: Honors program admission based math English scores.Healthcare: Medical trial eligibility based BMI blood pressure levels.Healthcare: Medical trial eligibility based BMI blood pressure levels.Taxation: Tax incentives based income level household size.Taxation: Tax incentives based income level household size.","code":""},{"path":"sec-regression-discontinuity.html","id":"general-framework","chapter":"27 Regression Discontinuity","heading":"27.10.1 General Framework","text":"individual \\(\\) :Two running variables, \\(X_{1i}\\) \\(X_{2i}\\).Two running variables, \\(X_{1i}\\) \\(X_{2i}\\).Two predetermined cutoffs, \\(C_1\\) \\(C_2\\).Two predetermined cutoffs, \\(C_1\\) \\(C_2\\).binary treatment indicator \\(D_i\\), assigned based whether individual’s scores exceed thresholds.binary treatment indicator \\(D_i\\), assigned based whether individual’s scores exceed thresholds.treatment effect defined :\\[\n\\tau (x_1, x_2) = E[Y_{1i} - Y_{0i} | X_{1i} = x_1, X_{2i} = x_2].\n\\]represents local average treatment effect two-dimensional RD setting.","code":""},{"path":"sec-regression-discontinuity.html","id":"identification-1","chapter":"27 Regression Discontinuity","heading":"27.10.2 Identification","text":"potential outcomes framework, individual \\(\\), define:\\(Y_{1i}\\): Potential outcome treatment.\\(Y_{1i}\\): Potential outcome treatment.\\(Y_{0i}\\): Potential outcome control.\\(Y_{0i}\\): Potential outcome control.\\(D_i\\): Treatment assignment rule.\\(D_i\\): Treatment assignment rule.observed outcome :\\[\nY_i = D_i Y_{1i} + (1 - D_i) Y_{0i}.\n\\]treatment assignment mechanism follows:\\[\nD_i =\n\\begin{cases}\n1 & \\text{} X_{1i} \\geq C_1 \\text{ } X_{2i} \\geq C_2, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]","code":""},{"path":"sec-regression-discontinuity.html","id":"key-assumptions-2","chapter":"27 Regression Discontinuity","heading":"27.10.3 Key Assumptions","text":"ensure valid causal inference, multi-score RD framework extends standard RD assumptions:Continuity Potential Outcomes Running Variables\nexpected potential outcomes \\(E[Y(0) | X_1, X_2]\\) \\(E[Y(1) | X_1, X_2]\\) smooth \\(X_1\\) \\(X_2\\).\nFormally: \\[\n\\lim_{(x_1, x_2) \\(C_1, C_2)^-} E[Y(0) | X_1 = x_1, X_2 = x_2] = \\lim_{(x_1, x_2) \\(C_1, C_2)^+} E[Y(0) | X_1 = x_1, X_2 = x_2].\n\\]\nEnsures observed discontinuity \\(E[Y | X_1, X_2]\\) attributable treatment.\nexpected potential outcomes \\(E[Y(0) | X_1, X_2]\\) \\(E[Y(1) | X_1, X_2]\\) smooth \\(X_1\\) \\(X_2\\).Formally: \\[\n\\lim_{(x_1, x_2) \\(C_1, C_2)^-} E[Y(0) | X_1 = x_1, X_2 = x_2] = \\lim_{(x_1, x_2) \\(C_1, C_2)^+} E[Y(0) | X_1 = x_1, X_2 = x_2].\n\\]Ensures observed discontinuity \\(E[Y | X_1, X_2]\\) attributable treatment.Manipulation Running Variables\ndensity \\((X_1, X_2)\\) must continuous \\((C_1, C_2)\\).\nagents able precisely manipulate scores cross threshold.\ndensity \\((X_1, X_2)\\) must continuous \\((C_1, C_2)\\).agents able precisely manipulate scores cross threshold.Local Randomization\nNear \\((C_1, C_2)\\), units good randomly assigned treatment control.\nNear \\((C_1, C_2)\\), units good randomly assigned treatment control.Interaction Effects Running Variables (optional)\nmodels, assume effect crossing \\(C_1\\) depend \\(C_2\\) vice versa.\nmodels, assume effect crossing \\(C_1\\) depend \\(C_2\\) vice versa.assumptions hold, treatment effect identified discontinuity \\(E[Y | X_1, X_2]\\) \\((C_1, C_2)\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"estimation-approaches-1","chapter":"27 Regression Discontinuity","heading":"27.10.4 Estimation Approaches","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"local-linear-regression-in-two-dimensions","chapter":"27 Regression Discontinuity","heading":"27.10.4.1 Local Linear Regression in Two Dimensions","text":"simplest approach estimate separate regressions side cutoff dimensions:observations threshold \\((C_1, C_2)\\):\\[\nY_i = \\alpha + \\beta_1 (X_{1i} - C_1) + \\beta_2 (X_{2i} - C_2) + \\epsilon_i.\n\\]observations threshold \\((C_1, C_2)\\):\\[\nY_i = \\gamma + \\delta_1 (X_{1i} - C_1) + \\delta_2 (X_{2i} - C_2) + \\tau D_i + \\nu_i.\n\\]treatment effect \\(\\tau\\) estimated :\\[\n\\hat{\\tau} = \\hat{E}[Y | X_1 = C_1^+, X_2 = C_2^+] - \\hat{E}[Y | X_1 = C_1^-, X_2 = C_2^-].\n\\]approach assumes local linearity, higher-order polynomials can used:\\[\nY_i = \\alpha + \\sum_{k=1}^{K} \\beta_k (X_{1i} - C_1)^k + \\sum_{k=1}^{K} \\gamma_k (X_{2i} - C_2)^k + \\tau D_i + \\epsilon_i.\n\\]","code":""},{"path":"sec-regression-discontinuity.html","id":"kernel-weighted-estimation","chapter":"27 Regression Discontinuity","heading":"27.10.4.2 Kernel-Weighted Estimation","text":"flexible approach estimates \\(\\tau(x_1, x_2)\\) using nonparametric local regression:\\[\n\\hat{\\tau}(x_1, x_2) = \\frac{\\sum_{=1}^{n} K_h (X_{1i} - x_1) K_h (X_{2i} - x_2) D_i Y_i}{\\sum_{=1}^{n} K_h (X_{1i} - x_1) K_h (X_{2i} - x_2) D_i}\n- \\frac{\\sum_{=1}^{n} K_h (X_{1i} - x_1) K_h (X_{2i} - x_2) (1 - D_i) Y_i}{\\sum_{=1}^{n} K_h (X_{1i} - x_1) K_h (X_{2i} - x_2) (1 - D_i)}.\n\\]:\\(K_h(\\cdot)\\) kernel function (e.g., Epanechnikov).\\(K_h(\\cdot)\\) kernel function (e.g., Epanechnikov).\\(h\\) bandwidth, selected via cross-validation.\\(h\\) bandwidth, selected via cross-validation.","code":""},{"path":"sec-regression-discontinuity.html","id":"interaction-model-for-heterogeneous-effects-1","chapter":"27 Regression Discontinuity","heading":"27.10.4.3 Interaction Model for Heterogeneous Effects","text":"assess interaction effects running variables, estimate:\\[\nY_i = \\alpha + \\beta_1 (X_{1i} - C_1) + \\beta_2 (X_{2i} - C_2) + \\tau D_i + \\lambda D_i (X_{1i} - C_1)(X_{2i} - C_2) + \\epsilon_i.\n\\]\\(\\lambda\\) captures whether treatment effect depends \\(X_1\\) \\(X_2\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"robustness-checks-3","chapter":"27 Regression Discontinuity","heading":"27.10.5 Robustness Checks","text":"Covariate Balance Dimensions\nTest whether pre-treatment covariates jump \\((C_1, C_2)\\).\nTest whether pre-treatment covariates jump \\((C_1, C_2)\\).McCrary Density Test Two Dimensions\nVerify density \\((X_1, X_2)\\) smooth \\((C_1, C_2)\\).\nVerify density \\((X_1, X_2)\\) smooth \\((C_1, C_2)\\).Placebo Cutoffs\nImplement fake cutoffs re-estimate \\(\\tau(x_1, x_2)\\).\nImplement fake cutoffs re-estimate \\(\\tau(x_1, x_2)\\).Varying Bandwidths\nRe-estimate using different bandwidths robustness.\nRe-estimate using different bandwidths robustness.","code":""},{"path":"sec-regression-discontinuity.html","id":"evaluation-of-a-regression-discontinuity-design","chapter":"27 Regression Discontinuity","heading":"27.11 Evaluation of a Regression Discontinuity Design","text":"estimating RD model, crucial evaluate whether assumptions hold whether results robust different specifications. key aspects RD evaluation include:Graphical formal evidence discontinuity treatment outcome variables.Validation RD assumptions, including:\nabsence discontinuities pre-treatment covariates.\nmanipulation assignment variable.\nabsence discontinuities pre-treatment covariates.manipulation assignment variable.Robustness checks functional form bandwidth choice.External validity: assessing whether results generalize beyond cutoff.well-implemented RD demonstrate clear treatment effect cutoff ensuring discontinuous changes confound effect.","code":""},{"path":"sec-regression-discontinuity.html","id":"graphical-and-formal-evidence","chapter":"27 Regression Discontinuity","heading":"27.11.1 Graphical and Formal Evidence","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"visual-inspection-of-the-rd-effect","chapter":"27 Regression Discontinuity","heading":"27.11.1.1 Visual Inspection of the RD Effect","text":"fundamental step RD analysis plot outcome variable running variable:Compute binned averages \\(Y_i\\) small intervals \\(X_i\\).Overlay smoothed polynomial regression separately \\(X < c\\) \\(X \\geq c\\).visible jump \\(X = c\\) provides initial evidence treatment effect.Additionally, plotting treatment probability \\(P(D = 1 | X)\\) ensures assignment follows expected RD rule.","code":""},{"path":"sec-regression-discontinuity.html","id":"no-discontinuity-in-pre-treatment-covariates","chapter":"27 Regression Discontinuity","heading":"27.11.1.2 No Discontinuity in Pre-Treatment Covariates","text":"rule omitted variable bias, check whether covariates (age, education, prior test scores, etc.) exhibit jumps cutoff.covariate \\(W_i\\), estimate:\\[\nW_i = \\alpha + f(X_i) \\beta + \\tau D_i + \\epsilon_i.\n\\]\\(\\tau\\) statistically significant, suggests violation RD assumptions.Covariate jumps imply factors treatment may driving observed outcome change.","code":""},{"path":"sec-regression-discontinuity.html","id":"manipulation-test-mccrary-density-test","chapter":"27 Regression Discontinuity","heading":"27.11.1.3 Manipulation Test (McCrary Density Test)","text":"critical assumption RD units precisely manipulate values \\(X_i\\). individuals can selectively sort around \\(c\\) (e.g., students altering test scores qualify scholarship), RD estimates become invalid.test manipulation, estimate density function \\(X\\) test discontinuity \\(c\\):\\[\n\\hat{f}(X) = \\lim_{x \\uparrow c} f(X) - \\lim_{x \\downarrow c} f(X).\n\\]significant difference suggests sorting behavior, violates RD assumptions.","code":""},{"path":"sec-regression-discontinuity.html","id":"functional-form-of-the-running-variable","chapter":"27 Regression Discontinuity","heading":"27.11.2 Functional Form of the Running Variable","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"general-rd-model","chapter":"27 Regression Discontinuity","heading":"27.11.2.1 General RD Model","text":"flexible RD specification includes:functional form \\(f(X_i)\\) account trends.functional form \\(f(X_i)\\) account trends.indicator treatment \\(D_i\\). - interaction term \\(D_i f(X_i)\\) allowing different slopes side.indicator treatment \\(D_i\\). - interaction term \\(D_i f(X_i)\\) allowing different slopes side.\\[\nY_i = \\alpha_0 + f(X_i) \\alpha_1 + (X_i \\geq c) \\alpha_2 + f(X_i) (X_i \\geq c) \\alpha_3 + u_i.\n\\]:\\(\\alpha_2\\) captures treatment effect \\(X = c\\).\\(\\alpha_2\\) captures treatment effect \\(X = c\\).\\(\\alpha_3\\) tests differences slopes across threshold.\\(\\alpha_3\\) tests differences slopes across threshold.","code":""},{"path":"sec-regression-discontinuity.html","id":"simple-case-linear-rd","chapter":"27 Regression Discontinuity","heading":"27.11.2.2 Simple Case: Linear RD","text":"\\(f(X_i)\\) linear function, estimate:\\[\nY_i = \\beta_0 + \\beta_1 X_i + (X_i \\geq c) \\beta_2 + \\epsilon_i.\n\\]RD gives \\(\\beta_2\\) (causal effect) \\(X\\) \\(Y\\) cutoff pointIn practice, everyone \\[ Y_i = \\alpha_0 + f(x) \\alpha _1 + [(x_i \\ge c)]\\alpha_2 + [f(x_i)\\times [(x_i \\ge c)]\\alpha_3 + u_i \\]estimate different slope different sides line. estimate \\(\\alpha_3\\) different 0 return simple case.","code":""},{"path":"sec-regression-discontinuity.html","id":"higher-order-polynomials","chapter":"27 Regression Discontinuity","heading":"27.11.2.3 Higher-Order Polynomials","text":"flexible specification allows nonlinear relationships:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3 + \\tau D_i + \\epsilon_i.\n\\]Higher-order polynomials reduce bias increase variance.Overfitting risk, especially limited data near \\(c\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"nonparametric-estimation","chapter":"27 Regression Discontinuity","heading":"27.11.2.4 Nonparametric Estimation","text":"polynomial models restrictive, use nonparametric local linear regression:\\[\n\\hat{E}[Y | X] = \\sum_{=1}^{n} K_h(X_i - c) Y_i.\n\\]:\\(K_h(X)\\) kernel function (e.g., Epanechnikov).\\(K_h(X)\\) kernel function (e.g., Epanechnikov).\\(h\\) bandwidth, chosen optimize bias-variance tradeoff.\\(h\\) bandwidth, chosen optimize bias-variance tradeoff.","code":""},{"path":"sec-regression-discontinuity.html","id":"bandwidth-selection-2","chapter":"27 Regression Discontinuity","heading":"27.11.3 Bandwidth Selection","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"tradeoff-between-bias-and-efficiency","chapter":"27 Regression Discontinuity","heading":"27.11.3.1 Tradeoff Between Bias and Efficiency","text":"Choosing appropriate bandwidth \\(h\\) crucial:Narrow \\(h\\) (close \\(c\\)): Lower bias, high variance.Narrow \\(h\\) (close \\(c\\)): Lower bias, high variance.Wider \\(h\\): efficient estimates potential bias.Wider \\(h\\): efficient estimates potential bias.","code":""},{"path":"sec-regression-discontinuity.html","id":"optimal-bandwidth-selection","chapter":"27 Regression Discontinuity","heading":"27.11.3.2 Optimal Bandwidth Selection","text":"Several methods exist selecting \\(h\\):Cross-validation: Minimizing mean squared error (MSE).G. Imbens Kalyanaraman (2012) bandwidth selection:\nBalances bias-variance tradeoff.\nOften reported RD studies.\nBalances bias-variance tradeoff.Often reported RD studies.Cattaneo, Idrobo, Titiunik (2019) robust bandwidth selection:\nFocuses valid inference, just point estimation.\nFocuses valid inference, just point estimation.","code":""},{"path":"sec-regression-discontinuity.html","id":"bandwidth-sensitivity-analysis","chapter":"27 Regression Discontinuity","heading":"27.11.3.3 Bandwidth Sensitivity Analysis","text":"standard robustness check estimating \\(\\tau\\) different \\(h\\) values:results change drastically, estimates may sensitive bandwidth choice.estimates remain stable, findings credible.","code":""},{"path":"sec-regression-discontinuity.html","id":"addressing-potential-confounders","chapter":"27 Regression Discontinuity","heading":"27.11.4 Addressing Potential Confounders","text":"","code":""},{"path":"sec-regression-discontinuity.html","id":"multiple-running-variables","chapter":"27 Regression Discontinuity","heading":"27.11.4.1 Multiple Running Variables","text":"multiple forcing variables influence treatment (e.g., math English scores honors eligibility), failing account may introduce confounding.solution extend RD multi-score framework:\\[\nY_i = \\alpha_0 + f(X_{1i}, X_{2i}) \\alpha_1 + (X_{1i} \\geq c_1, X_{2i} \\geq c_2) \\alpha_2 + \\epsilon_i.\n\\]Controls scores simultaneously.Allows interactions assignment variables.","code":""},{"path":"sec-regression-discontinuity.html","id":"bundling-of-institutions","chapter":"27 Regression Discontinuity","heading":"27.11.4.2 Bundling of Institutions","text":"cases policies change institutional levels (e.g., different states implementing varying minimum wages), treatment effects may reflect institutional bundling rather individual cutoff effects.One solution include institution fixed effects:\\[\nY_i = \\alpha + f(X_i) \\beta + \\tau D_i + \\gamma C_j + \\epsilon_i.\n\\]\\(C_j\\) categorical variable institution \\(j\\).","code":""},{"path":"sec-regression-discontinuity.html","id":"external-validity-in-rd","chapter":"27 Regression Discontinuity","heading":"27.11.5 External Validity in RD","text":"RD provides strong internal validity, generalizability often limited:Local Nature RD Estimates\nRD estimates apply cutoff.\nEffects may extrapolate values \\(X\\).\nRD estimates apply cutoff.Effects may extrapolate values \\(X\\).Heterogeneous Treatment Effects\n\\(\\tau\\) varies across subgroups, RD estimates may generalize.\n\\(\\tau\\) varies across subgroups, RD estimates may generalize.Spillover Effects\ntreatment effects extend beyond threshold (e.g., peer effects), RD assumptions may violated.\ntreatment effects extend beyond threshold (e.g., peer effects), RD assumptions may violated.","code":""},{"path":"sec-regression-discontinuity.html","id":"applications-of-rd-designs","chapter":"27 Regression Discontinuity","heading":"27.12 Applications of RD Designs","text":"Regression Discontinuity (RD) designs widespread applications empirical research across economics, political science, marketing, public policy. applications leverage threshold-based decision rules identify causal effects real-world settings randomized experiments infeasible.Key applications include:Marketing: Estimating causal effects promotions advertising intensity.Marketing: Estimating causal effects promotions advertising intensity.Education: Evaluating impact financial aid merit scholarships.Education: Evaluating impact financial aid merit scholarships.Healthcare: Assessing effect medical interventions assigned based eligibility thresholds.Healthcare: Assessing effect medical interventions assigned based eligibility thresholds.Labor Economics: Studying unemployment benefits wage policies.Labor Economics: Studying unemployment benefits wage policies.","code":""},{"path":"sec-regression-discontinuity.html","id":"applications-in-marketing","chapter":"27 Regression Discontinuity","heading":"27.12.1 Applications in Marketing","text":"RD widely applied marketing research estimate causal effects pricing, advertising, product positioning.Position Effects Advertising(Narayanan Kalyanam 2015) uses RD approach estimate causal impact advertisement placement search engines. Since ad placement follows auction-based system, discontinuity occurs top-ranked ad receives disproportionately clicks lower-ranked ads.Key insights:Higher-ranked ads generate click-throughs, always translate higher conversion rates.Higher-ranked ads generate click-throughs, always translate higher conversion rates.RD framework helps distinguish correlation causation leveraging rank cutoff.RD framework helps distinguish correlation causation leveraging rank cutoff.Identifying Causal Marketing Mix Effects(Hartmann, Nair, Narayanan 2011) presents nonparametric RD estimation approach identify causal effects marketing mix variables, :Advertising budgetsAdvertising budgetsPrice changesPrice changesPromotional campaignsPromotional campaignsBy comparing firms just expenditure threshold, RD framework isolates true causal impact marketing spending consumer behavior.","code":""},{"path":"sec-regression-discontinuity.html","id":"r-packages-for-rd-estimation","chapter":"27 Regression Discontinuity","heading":"27.12.2 R Packages for RD Estimation","text":"Key RD Packages RFor detailed comparison, see (Thoemmes, Liao, Jin 2017) (Table 1, p. 347).","code":""},{"path":"sec-regression-discontinuity.html","id":"specialized-rd-packages","chapter":"27 Regression Discontinuity","heading":"27.12.2.1 Specialized RD Packages","text":"rddensity: Tests discontinuities density running variable (useful manipulation/bunching detection).rdlocrand: Implements randomization-based inference RD.rdmulti: Extends RD multiple cutoffs multiple scores.rdpower: Conducts power calculations sample selection RD designs.Use rdrobust?Implements local polynomial regression, providing bias correction robust standard errors.Uses state---art bandwidth selection methods.Produces automatic RD plots visualization.","code":""},{"path":"sec-regression-discontinuity.html","id":"example-of-regression-discontinuity-in-education","chapter":"27 Regression Discontinuity","heading":"27.12.3 Example of Regression Discontinuity in Education","text":"illustrate Sharp RD using simulated example college GPA future career success.Setup:Students qualify prestigious internship GPA ≥ 3.5.Students qualify prestigious internship GPA ≥ 3.5.outcome variable future career success (e.g., salary).outcome variable future career success (e.g., salary).RD estimates causal impact internship program earnings.RD estimates causal impact internship program earnings.\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i\n\\]\\[\nX_i =\n\\begin{cases}\n1, W_i \\ge c \\\\\n0, W_i < c\n\\end{cases}\n\\]simulate 100 observations, GPA forcing variable career success depends GPA treatment effect.estimate Sharp RD treatment effect using local linear regression:Plot RD regression line binned observations:verify whether results hold different bandwidths functional forms:Varying BandwidthMcCrary Test ManipulationPolynomial Functional FormsWe compare linear quadratic specifications:","code":"\n# Set seed for reproducibility\nset.seed(42)\n\nn = 100\n\n# Simulate GPA scores (0-4 scale)\nGPA <- runif(n, 0, 4)\n\n# Generate future success with treatment effect at GPA >= 3.5\nfuture_success <- 10 + 2 * GPA + 10 * (GPA >= 3.5) + rnorm(n)\n\n# Load RD package\nlibrary(rddtools)\n\n# Format data for RD analysis\ndata <- rdd_data(future_success, GPA, cutpoint = 3.5)\n\n# Plot RD scatterplot\nplot(data, col = \"red\", cex = 0.5, xlab = \"GPA\", ylab = \"Future Success\")\n# Estimate the sharp RDD model\nrdd_mod <- rdd_reg_lm(rdd_object = data, slope = \"same\")\n\n# Display results\nsummary(rdd_mod)\n#> \n#> Call:\n#> lm(formula = y ~ ., data = dat_step1, weights = weights)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.08072 -0.54182  0.05352  0.54135  2.51267 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 17.21703    0.20060   85.83   <2e-16 ***\n#> D            9.79856    0.31716   30.89   <2e-16 ***\n#> x            2.14839    0.09914   21.67   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9284 on 97 degrees of freedom\n#> Multiple R-squared:  0.9746, Adjusted R-squared:  0.9741 \n#> F-statistic:  1863 on 2 and 97 DF,  p-value: < 2.2e-16\n# Plot RD regression\nplot(rdd_mod, cex = 0.5, col = \"red\", xlab = \"GPA\", ylab = \"Future Success\")\n# Using rdrobust for robust estimation\nlibrary(rdrobust)\n\n# Estimate RD with optimal bandwidth\nrd_out <- rdrobust(y = future_success, x = GPA, c = 3.5)\nsummary(rd_out)\n#> Sharp RD estimates using local polynomial regression.\n#> \n#> Number of Obs.                  100\n#> BW type                       mserd\n#> Kernel                   Triangular\n#> VCE method                       NN\n#> \n#> Number of Obs.                   83           17\n#> Eff. Number of Obs.               7           12\n#> Order est. (p)                    1            1\n#> Order bias  (q)                   2            2\n#> BW est. (h)                   0.361        0.361\n#> BW bias (b)                   0.670        0.670\n#> rho (h/b)                     0.540        0.540\n#> Unique Obs.                      83           17\n#> \n#> =============================================================================\n#>         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n#> =============================================================================\n#>   Conventional    12.836     2.495     5.144     0.000     [7.945 , 17.727]    \n#>         Robust         -         -     4.473     0.000     [7.677 , 19.653]    \n#> =============================================================================\nlibrary(rddensity)\n\n# Check for discontinuities in GPA distribution\nrddensity(GPA, c = 3.5)\n#> Call:\n#> rddensity.\n#> Sample size: 100. Cutoff: 3.5.\n#> Model: unrestricted. Kernel: triangular. VCE: jackknife\n# Estimate RD with a quadratic polynomial\nrdd_mod_quad <- rdd_reg_lm(rdd_object = data, slope = \"separate\", order = 2)\nsummary(rdd_mod_quad)\n#> \n#> Call:\n#> lm(formula = y ~ ., data = dat_step1, weights = weights)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.06894 -0.52799  0.01642  0.55260  2.45318 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 17.30296    0.32241  53.667  < 2e-16 ***\n#> D            9.14439    1.13752   8.039 2.65e-12 ***\n#> x            2.29753    0.41955   5.476 3.61e-07 ***\n#> `x^2`        0.04253    0.11228   0.379    0.706    \n#> x_right      2.51976    9.23566   0.273    0.786    \n#> `x^2_right` -1.61694   17.10035  -0.095    0.925    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9376 on 94 degrees of freedom\n#> Multiple R-squared:  0.9749, Adjusted R-squared:  0.9736 \n#> F-statistic:   731 on 5 and 94 DF,  p-value: < 2.2e-16"},{"path":"sec-regression-discontinuity.html","id":"example-of-occupational-licensing-and-market-efficiency","chapter":"27 Regression Discontinuity","heading":"27.12.4 Example of Occupational Licensing and Market Efficiency","text":"Occupational licensing form labor market regulation requires workers obtain licenses certifications allowed work specific professions. debate around occupational licensing revolves around two competing effects:Efficiency-enhancing effect:\nLicensing ensures workers meet minimum quality standards, reducing information asymmetries firms consumers.\nHigher-quality service providers selected labor market.\nLicensing ensures workers meet minimum quality standards, reducing information asymmetries firms consumers.Higher-quality service providers selected labor market.Barrier--entry effect:\nLicensing requirements impose costs potential entrants, reducing labor supply.\ncreates frictions market, potentially leading inefficiencies.\nLicensing requirements impose costs potential entrants, reducing labor supply.creates frictions market, potentially leading inefficiencies.Bowblis Smith (2021) investigates tradeoff using Fuzzy RD based threshold rule:Nursing homes 120 beds required hire certain fraction licensed/certified workers.Nursing homes 120 beds required hire certain fraction licensed/certified workers.study examines whether policy improves quality care merely restricts labor market competition.study examines whether policy improves quality care merely restricts labor market competition.","code":""},{"path":"sec-regression-discontinuity.html","id":"ols-estimation-naïve-approach-and-its-bias","chapter":"27 Regression Discontinuity","heading":"27.12.4.1 OLS Estimation: Naïve Approach and Its Bias","text":"simple Ordinary Least Squares regression can used estimate effect licensed workers quality service:\\[\nY_i = \\alpha_0 + X_i \\alpha_1 + LW_i \\alpha_2 + \\epsilon_i\n\\]:\\(Y_i\\) = Quality service facility \\(\\).\\(Y_i\\) = Quality service facility \\(\\).\\(LW_i\\) = Proportion licensed workers facility \\(\\).\\(LW_i\\) = Proportion licensed workers facility \\(\\).\\(X_i\\) = Facility characteristics (e.g., size, staffing levels).\\(X_i\\) = Facility characteristics (e.g., size, staffing levels).","code":""},{"path":"sec-regression-discontinuity.html","id":"potential-bias-in-alpha_2","chapter":"27 Regression Discontinuity","heading":"27.12.4.2 Potential Bias in \\(\\alpha_2\\)","text":"Mitigation-Based Bias:\nFacilities poor quality outcomes may hire licensed workers response bad performance.\ninduces negative correlation \\(LW_i\\) \\(Y_i\\), biasing \\(\\alpha_2\\) downward.\nFacilities poor quality outcomes may hire licensed workers response bad performance.induces negative correlation \\(LW_i\\) \\(Y_i\\), biasing \\(\\alpha_2\\) downward.Preference-Based Bias:\nHigher-quality facilities may greater incentives hire certified workers.\ninduces positive correlation \\(LW_i\\) \\(Y_i\\), biasing \\(\\alpha_2\\) upward.\nHigher-quality facilities may greater incentives hire certified workers.induces positive correlation \\(LW_i\\) \\(Y_i\\), biasing \\(\\alpha_2\\) upward.","code":""},{"path":"sec-regression-discontinuity.html","id":"fuzzy-rd-framework","chapter":"27 Regression Discontinuity","heading":"27.12.4.3 Fuzzy RD Framework","text":"OLS model endogenous, implement Fuzzy RD Design exploits discontinuity licensing requirements 120 beds.\\[\nY_{ist} = \\beta_0 + [(Bed \\geq 121)_{ist}]\\beta_1 + f(Size_{ist}) \\beta_2 + [f(Size_{ist}) \\times (Bed \\geq 121)_{ist}] \\beta_3 + X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\]:\\((Bed \\geq 121)\\) = Indicator treatment eligibility (least 120 beds).\\((Bed \\geq 121)\\) = Indicator treatment eligibility (least 120 beds).\\(f(Size_{ist})\\) = Flexible functional form facility size.\\(f(Size_{ist})\\) = Flexible functional form facility size.\\(X_{}\\) = control variables.\\(X_{}\\) = control variables.\\(\\gamma_s\\) = State fixed effects (accounts state-level regulations).\\(\\gamma_s\\) = State fixed effects (accounts state-level regulations).\\(\\theta_t\\) = Time fixed effects (accounts temporal shocks).\\(\\theta_t\\) = Time fixed effects (accounts temporal shocks).\\(\\beta_1\\) = Intent--treat (ITT) effect.\\(\\beta_1\\) = Intent--treat (ITT) effect.model estimates discontinuities quality outcomes 120-bed threshold.","code":""},{"path":"sec-regression-discontinuity.html","id":"why-this-rd-is-fuzzy","chapter":"27 Regression Discontinuity","heading":"27.12.4.4 Why This RD is Fuzzy","text":"Unlike Sharp RD, treatment fully determined cutoff:facilities fewer 120 beds voluntarily hire licensed workers.facilities fewer 120 beds voluntarily hire licensed workers.facilities 120 beds may comply fully.facilities 120 beds may comply fully.Thus, RD framework must modified:Fixed-Effect Considerations:\nstates sort differently near threshold, state fixed effects (\\(\\gamma_s\\)) may needed.\nHowever, sorting non-random, RD assumption fails.\nstates sort differently near threshold, state fixed effects (\\(\\gamma_s\\)) may needed.However, sorting non-random, RD assumption fails.Panel Data Implications:\nFixed-effects models preferred RD, lower causal inference hierarchy.\nRD typically excludes pre-treatment periods, whereas fixed-effects require --comparisons.\nFixed-effects models preferred RD, lower causal inference hierarchy.RD typically excludes pre-treatment periods, whereas fixed-effects require --comparisons.Variation Facility Size:\nHospitals rarely change bed capacity, including hospital-specific fixed effects removes variation necessary RD estimation.\nHospitals rarely change bed capacity, including hospital-specific fixed effects removes variation necessary RD estimation.","code":""},{"path":"sec-regression-discontinuity.html","id":"instrumental-variable-approach","chapter":"27 Regression Discontinuity","heading":"27.12.4.5 Instrumental Variable Approach","text":"correct endogeneity, use two-stage least squares IV approach, running variable serves instrument treatment intensity.Stage 1: First-Stage RegressionEstimate probability hiring licensed workers based 120-bed threshold:\\[\nQSW_{ist} = \\alpha_0 + [(Bed \\geq 121)_{ist}]\\alpha_1 + f(Size_{ist}) \\alpha_2 + [f(Size_{ist}) \\times (Bed \\geq 121)_{ist}] \\alpha_3 + X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\]:\\(QSW_{ist}\\) = Predicted proportion licensed workers facility \\(\\) state \\(s\\) time \\(t\\).Stage 2: Second-Stage RegressionEstimate causal effect hiring licensed workers quality service:\\[\nY_{ist} = \\gamma_0 + \\gamma_1 \\hat{QWS}_{ist} + f(Size_{ist}) \\delta_2 + [f(Size_{ist}) \\times (Bed \\geq 121)] \\delta_3 + X_{} \\lambda + \\eta_s + \\tau_t + u_{ist}\n\\]:\\(\\hat{QWS}_{ist}\\) = Instrumented proportion licensed workers.\\(\\hat{QWS}_{ist}\\) = Instrumented proportion licensed workers.\\(\\gamma_1\\) = Causal effect certified staffing quality outcomes.\\(\\gamma_1\\) = Causal effect certified staffing quality outcomes.Key ObservationsThe larger discontinuity 120 beds, closer \\(\\gamma_1 \\approx \\beta_1\\).ITT effect (\\(\\beta_1\\)) always weaker local average treatment effect (\\(\\gamma_1\\)).","code":""},{"path":"sec-regression-discontinuity.html","id":"empirical-challenges","chapter":"27 Regression Discontinuity","heading":"27.12.4.6 Empirical Challenges","text":"Bunching Round Numbers:\nFigure 1 shows facilities clustering every 5-bed increment.\nmanipulation occurs, expect underrepresentation 130 beds.\nFigure 1 shows facilities clustering every 5-bed increment.manipulation occurs, expect underrepresentation 130 beds.Clustering Standard Errors:\nDue limited unique mass points (facilities specific sizes), errors clustered mass point instead individual facilities.\nDue limited unique mass points (facilities specific sizes), errors clustered mass point instead individual facilities.Noncompliance Selection Bias:\nFuzzy RD requires drop non-compliers (introduce selection bias).\nHowever, can drop manipulators clear evidence behavioral bias (e.g., preference round numbers).\nFuzzy RD requires drop non-compliers (introduce selection bias).However, can drop manipulators clear evidence behavioral bias (e.g., preference round numbers).","code":""},{"path":"sec-regression-discontinuity.html","id":"replicating-carpenter2009effect","chapter":"27 Regression Discontinuity","heading":"27.12.5 Replicating (Carpenter and Dobkin 2009)","text":"well-known RD study (Carpenter Dobkin 2009) investigates causal effect legal drinking age mortality rates.Replication available : Philipp Leppert.Data available : OpenICPSR.","code":""},{"path":"sec-regression-discontinuity.html","id":"additional-rd-applications","chapter":"27 Regression Discontinuity","heading":"27.12.6 Additional RD Applications","text":"comprehensive empirical RD application, see (Thoemmes, Liao, Jin 2017), :Multiple RD packages (rdd, rdrobust, rddtools) tested.Multiple RD packages (rdd, rdrobust, rddtools) tested.Robustness checks performed across bandwidth selection polynomial orders.Robustness checks performed across bandwidth selection polynomial orders.","code":""},{"path":"temporal-discontinuity-designs.html","id":"temporal-discontinuity-designs","chapter":"28 Temporal Discontinuity Designs","heading":"28 Temporal Discontinuity Designs","text":"evaluating causal impact policy, intervention, treatment begins known time, two popular quasi-experimental methods :Regression Discontinuity TimeInterrupted Time SeriesBoth leverage fact intervention policy starts specific point time, differ model outcome around point assumptions require.use timing intervention key identifying causal effect.assume major changes coincide exactly intervention.Unlike standard RD, exploits variation cross-sectional dimension (e.g., eligibility thresholds), RDiT leverages variation time dimension estimate causal effects.RDiT particularly useful :policy implemented fixed date subjects (e.g., national tax reform).policy implemented different times different subjects (e.g., state-level policy rollouts).suitable cross-sectional control group, making difference--differences infeasible.","code":""},{"path":"temporal-discontinuity-designs.html","id":"sec-regression-discontinuity-in-time","chapter":"28 Temporal Discontinuity Designs","heading":"28.1 Regression Discontinuity in Time","text":"Regression Discontinuity Time special case Regression Discontinuity design “forcing variable” time . exact cutoff time \\(T^*\\), policy intervention implemented. compare observations just just \\(T^*\\) estimate causal effect.Key assumptions RDiT:Sharp assignment: intervention precisely begins time \\(T^*\\).Local continuity: Units just \\(T^*\\) comparable except treatment status.Continuity Time-Varying Confounders\nfundamental assumption RDiT unobserved factors affecting outcome evolve smoothly time.\nunobserved confounder changes discontinuously cutoff date, RDiT attribute effect intervention incorrectly.\nfundamental assumption RDiT unobserved factors affecting outcome evolve smoothly time.unobserved confounder changes discontinuously cutoff date, RDiT attribute effect intervention incorrectly.confounding interventions begin exactly \\(T^*\\).Manipulation Running Variable (Time)\nUnlike standard RD, subjects may manipulate assignment variable (e.g., test scores), time directly manipulated.\nHowever, strategic anticipation policy (e.g., firms adjusting behavior tax increase) can create bias.\nUnlike standard RD, subjects may manipulate assignment variable (e.g., test scores), time directly manipulated.However, strategic anticipation policy (e.g., firms adjusting behavior tax increase) can create bias.Using local polynomial approach near \\(T^*\\):\\[\nY_t = \\alpha_0 + \\alpha_1 \\bigl(T_t - T^*\\bigr) \\;+\\; \\tau\\,D_t \\;+\\; \\alpha_2 \\bigl(T_t - T^*\\bigr) D_t \\;+\\; \\epsilon_t,\n\\quad \\text{} |T_t - T^*| < h,\n\\]:\\(Y_t\\) outcome time \\(t\\).\\(T_t\\) time index (running variable).\\(T^*\\) cutoff (intervention) time.\\(D_t = 1\\) \\(t \\ge T^*\\) 0 otherwise.\\(h\\) chosen bandwidth observations close \\(T^*\\) used.\\(\\tau\\) represents treatment effect (discontinuity \\(T^*\\)).RDiT focuses local window around \\(T^*\\), best expect immediate jump cutoff observations near cutoff likely similar except treatment.RDiT Used Instead Traditional RD","code":""},{"path":"temporal-discontinuity-designs.html","id":"estimation-and-model-selection","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.1 Estimation and Model Selection","text":"Regression Discontinuity Time, model selection critical accurately estimate causal effect cutoff \\(T^*\\). Unlike Interrupted Time Series, models long-term trends intervention, RDiT relies local comparisons around cutoff. means :narrow bandwidth (\\(h\\)) chosen focus observations just \\(T^*\\).Polynomial order selection guided Bayesian Information Criterion avoid overfitting.Higher-order polynomials can introduce spurious curvature, local linear quadratic models preferred.RDiT uses time-series data, essential correct serial correlation errors:Clustered Standard Errors: Adjusts within-time correlation, ensuring valid inference.Newey-West HAC Standard Errors: Corrects heteroskedasticity serial correlation time.serial dependence exists \\(\\epsilon_{}\\) (error term), straightforward fix—introducing lagged dependent variable may mis-specify model.serial dependence exists \\(y_{}\\) (outcome variable):\nlong windows, identifying precise treatment effect becomes challenging.\nIncluding lagged dependent variable can help, though bias may still arise time-varying treatment effects -fitting.\nlong windows, identifying precise treatment effect becomes challenging.Including lagged dependent variable can help, though bias may still arise time-varying treatment effects -fitting.Baseline Local Linear Model (Preferred RDiT)\\[\nY_t = \\alpha_0 + \\alpha_1 (T_t - T^*) + \\tau D_t + \\alpha_2 (T_t - T^*) D_t + \\epsilon_t, \\quad \\text{} |T_t - T^*| < h\n\\]:\\(Y_t\\) outcome interest time \\(t\\).\\(Y_t\\) outcome interest time \\(t\\).\\(T_t\\) time forcing variable.\\(T_t\\) time forcing variable.\\(T^*\\) cutoff time policy/intervention occurs.\\(T^*\\) cutoff time policy/intervention occurs.\\(D_t\\) treatment indicator:\\(D_t\\) treatment indicator:\\(D_t = 1\\) \\(t \\geq T^*\\) (post-intervention).\\(D_t = 1\\) \\(t \\geq T^*\\) (post-intervention).\\(D_t = 0\\) \\(t < T^*\\) (pre-intervention).\\(D_t = 0\\) \\(t < T^*\\) (pre-intervention).\\(h\\) bandwidth, restricting analysis observations close \\(T^*\\).\\(h\\) bandwidth, restricting analysis observations close \\(T^*\\).\\(\\tau\\) treatment effect, measuring discontinuity \\(T^*\\).\\(\\tau\\) treatment effect, measuring discontinuity \\(T^*\\).\\(\\alpha_1 (T_t - T^*)\\) allows smooth time trend sides cutoff.\\(\\alpha_1 (T_t - T^*)\\) allows smooth time trend sides cutoff.\\(\\alpha_2 (T_t - T^*) D_t\\) captures differential time trends post-treatment.\\(\\alpha_2 (T_t - T^*) D_t\\) captures differential time trends post-treatment.model ensures treatment effect identified discontinuity \\(T^*\\), rather long-term trends.Quadratic Local Model (Allowing Nonlinear Trends)outcome variable exhibits curvature time, quadratic term can added:\\[\nY_t = \\alpha_0 + \\alpha_1 (T_t - T^*) + \\alpha_2 (T_t - T^*)^2 + \\tau D_t + \\alpha_3 (T_t - T^*) D_t + \\alpha_4 (T_t - T^*)^2 D_t + \\epsilon_t, \\quad \\text{} |T_t - T^*| < h\n\\]\\(\\alpha_2 (T_t - T^*)^2\\) accounts nonlinear pre-treatment trends.\\(\\alpha_4 (T_t - T^*)^2 D_t\\) allows nonlinear post-treatment effects.model useful visual inspection suggests curved relationship near cutoff.Augmented Local Linear Model (Robust Control Confounders)Following C. Hausman Rapson (2018), augmented approach helps control omitted variables:First-stage regression: Estimate outcome relevant control variables compute residuals.\n\\[\nY_t = \\delta_0 + \\sum_{j} \\delta_j X_{jt} + \\nu_t\n\\]\n\\(X_{jt}\\) observed covariates influence \\(Y_t\\).First-stage regression: Estimate outcome relevant control variables compute residuals.\\[\nY_t = \\delta_0 + \\sum_{j} \\delta_j X_{jt} + \\nu_t\n\\]\\(X_{jt}\\) observed covariates influence \\(Y_t\\).Second-stage RDiT model: Use residuals first stage standard local linear RDiT model:\n\\[\n\\hat{\\nu}_t = \\beta_0 + \\beta_1 (T_t - T^*) + \\tau D_t + \\beta_2 (T_t - T^*) D_t + \\epsilon_t, \\quad \\text{} |T_t - T^*| < h\n\\]Second-stage RDiT model: Use residuals first stage standard local linear RDiT model:\\[\n\\hat{\\nu}_t = \\beta_0 + \\beta_1 (T_t - T^*) + \\tau D_t + \\beta_2 (T_t - T^*) D_t + \\epsilon_t, \\quad \\text{} |T_t - T^*| < h\n\\]approach removes variation explained covariates estimating treatment effect.Bootstrap methods used correct first-stage estimation variance.","code":""},{"path":"temporal-discontinuity-designs.html","id":"strengths-of-rdit","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.2 Strengths of RDiT","text":"One key advantages Regression Discontinuity Time ability handle cases standard Difference--Differences approaches infeasible. typically occurs treatment implementation lacks cross-sectional variation—meaning units receive treatment time, leaving untreated control group comparison. cases, RDiT provides viable alternative exploiting temporal discontinuities treatment assignment.Notably, studies combine RDiT strengthen identification provide additional insights. instance,Auffhammer Kellogg (2011) applies methods examine treatment effects vary across individuals geographic space.Auffhammer Kellogg (2011) applies methods examine treatment effects vary across individuals geographic space.Gallego, Montero, Salas (2013) contrasts RDiT estimates validity control group uncertain, helping assess potential biases.Gallego, Montero, Salas (2013) contrasts RDiT estimates validity control group uncertain, helping assess potential biases.Beyond alternative , RDiT also offers advantages simpler pre/post comparisons. Unlike naive --analyses, RDiT can incorporate flexible controls time trends, reducing risk spurious results due temporal confounders.Event study methods, particularly modern implementations, improved significantly, allowing researchers study treatment effects long time horizons. However, RDiT still holds certain advantages:Longer time horizons: Unlike traditional event studies, RDiT restricted short-term dynamics can capture effects unfold gradually extended periods.Higher-order time controls: RDiT allows flexible modeling time trends, including use higher-order polynomials, may provide better approximations underlying time dynamics.Comparison Methods","code":""},{"path":"temporal-discontinuity-designs.html","id":"limitations-and-challenges-of-rdit","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3 Limitations and Challenges of RDiT","text":"Despite strengths, RDiT comes several methodological challenges researchers must carefully address.","code":""},{"path":"temporal-discontinuity-designs.html","id":"selection-bias-at-the-time-threshold","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3.1 Selection Bias at the Time Threshold","text":"major concern RDiT bias selecting observations close threshold. Unlike cross-sectional RD designs, observations either side cutoff assumed comparable, time-based designs introduce complications:data-generating process may exhibit time-dependent structure.data-generating process may exhibit time-dependent structure.Unobserved shocks occurring near threshold can confound estimates.Unobserved shocks occurring near threshold can confound estimates.Seasonal cyclical trends may drive changes discontinuity rather treatment .Seasonal cyclical trends may drive changes discontinuity rather treatment .","code":""},{"path":"temporal-discontinuity-designs.html","id":"inapplicability-of-the-mccrary-test","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3.2 Inapplicability of the McCrary Test","text":"key diagnostic tool standard RD designs McCrary test (McCrary 2008), checks discontinuities density running variable detect manipulation. Unfortunately, test feasible RDiT time uniformly distributed. limitation makes challenging rule sorting, anticipation, forms manipulation (27.5.2) around threshold.","code":""},{"path":"temporal-discontinuity-designs.html","id":"potential-discontinuities-in-unobservables","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3.3 Potential Discontinuities in Unobservables","text":"Even treatment assigned exogenously specific time, time-varying unobserved factors can still introduce discontinuities dependent variable. unobservable factors coincide threshold, may mistakenly attributed treatment effect, leading biased conclusions.","code":""},{"path":"temporal-discontinuity-designs.html","id":"challenges-in-modeling-time-varying-treatment-effects","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3.4 Challenges in Modeling Time-Varying Treatment Effects","text":"RDiT naturally accommodate time-varying treatment effects, can lead specification issues. choosing time window:narrow window improves local approximation may reduce statistical power.narrow window improves local approximation may reduce statistical power.broader window provides data increases risk bias additional confounders.broader window provides data increases risk bias additional confounders.address concerns, researchers must assume:model correctly specified, meaning includes relevant confounders polynomial approximation accurately captures time trends.treatment effect correctly specified, whether assumed smooth, constant, varying time.Additionally, two assumptions must interact—words, polynomial control correlated unobserved variation treatment effect. condition fails, bias misspecification treatment heterogeneity can compound (C. Hausman Rapson 2018, 544).","code":""},{"path":"temporal-discontinuity-designs.html","id":"sorting-and-anticipation-effects","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.3.5 Sorting and Anticipation Effects","text":"Unlike traditional RD designs, individuals manipulate assignment treatment, time-based cutoffs introduce potential sorting, anticipation, avoidance behaviors.McCrary test applied detect manipulation, researchers can perform robustness checks:Check discontinuities covariates: Ideally, covariates smooth around threshold.Check discontinuities covariates: Ideally, covariates smooth around threshold.Test placebo discontinuities: significant jumps appear , randomly chosen thresholds, raises concerns validity estimated treatment effect.Test placebo discontinuities: significant jumps appear , randomly chosen thresholds, raises concerns validity estimated treatment effect.difficulty RDiT even treatment effect detected, may reflect just causal effect intervention. Anticipatory behavior, adaptation, strategic avoidance may contribute observed discontinuities, making harder isolate true causal effect.Thus, researchers relying RDiT must make strong case behaviors drive results. often requires additional robustness tests, alternative specifications, comparisons methods rule alternative explanations.","code":""},{"path":"temporal-discontinuity-designs.html","id":"recommendations-for-robustness-checks","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.4 Recommendations for Robustness Checks","text":"ensure validity RDiT estimates, researchers conduct series robustness checks detect potential biases overfitting, time-varying treatment effects, model misspecification. following strategies, based (C. Hausman Rapson 2018, 549), provide comprehensive framework assessing reliability results.Visual Inspection: Raw Data ResidualsBefore applying complex statistical adjustments, start simple visualization raw data residuals (removing confounders time trends). results sensitive choice polynomial order local linear controls, signal time-varying treatment effects.well-behaved RDiT exhibit clear consistent discontinuity threshold, regardless specification used. discontinuity shifts fades different model choices, suggests sensitivity polynomial approximation, potentially indicating bias.Sensitivity Polynomial Order Bandwidth ChoiceA common concern RDiT overfitting due high-order global polynomials. diagnose issue:Estimate model different polynomial orders check whether results remain consistent.Estimate model different polynomial orders check whether results remain consistent.Compare global polynomial estimates local linear specifications using different bandwidths.Compare global polynomial estimates local linear specifications using different bandwidths.findings remain stable across specifications, estimates likely robust. However, results fluctuate significantly, suggests potential overfitting sensitivity bandwidth choice.findings remain stable across specifications, estimates likely robust. However, results fluctuate significantly, suggests potential overfitting sensitivity bandwidth choice.Placebo TestsTo strengthen causal claims, conduct placebo tests estimating RDiT model conditions treatment effect exist. two primary approaches:Estimate RD different location population receive treatment. discontinuity detected, suggests estimated effect may driven factors intervention.Use alternative time threshold intervention took place. model still detects significant effect, implies discontinuity may artifact method rather treatment.placebo tests reveal significant discontinuities, reinforces credibility primary RDiT estimate.Discontinuity Continuous ControlsAnother useful diagnostic plot RD discontinuity continuous control variables affected treatment.covariates exhibit significant jump threshold, raises concerns time-varying confounders may driving observed effect.covariates exhibit significant jump threshold, raises concerns time-varying confounders may driving observed effect.Ideally, covariates remain smooth across threshold, confirming discontinuity outcome due unobserved factors.Ideally, covariates remain smooth across threshold, confirming discontinuity outcome due unobserved factors.Donut RD: Excluding Observations Near CutoffTo assess whether strategic behavior anticipation effects influencing estimates, researchers can use donut RD approach (Barreca et al. 2011).involves removing observations immediately around threshold check whether results remain consistent.involves removing observations immediately around threshold check whether results remain consistent.avoiding selection close cutoff significantly alters findings, suggests sorting, anticipation, measurement error may affecting estimates.avoiding selection close cutoff significantly alters findings, suggests sorting, anticipation, measurement error may affecting estimates.results stable even excluding observations, strengthens confidence identification strategy.results stable even excluding observations, strengthens confidence identification strategy.Testing AutoregressionBecause RDiT operates time-series framework, serial dependence residuals can distort standard errors bias inference. diagnose :Use pre-treatment data test autoregression dependent variable.Use pre-treatment data test autoregression dependent variable.autoregression detected, consider including lagged dependent variable account serial correlation.autoregression detected, consider including lagged dependent variable account serial correlation.However, cautious—introducing lagged outcome may create dynamic bias treatment effect influences lag.However, cautious—introducing lagged outcome may create dynamic bias treatment effect influences lag.Augmented Local Linear ApproachInstead relying global polynomials, risk overfitting, reliable alternative augmented local linear approach, avoids excessive reliance high-order time polynomials. procedure involves two key steps:Use full sample control key predictors, ensuring model accounts important covariates may confound treatment effect.Estimate conditioned second-stage model narrower bandwidth refine local approximation maintaining robustness overfitting.","code":""},{"path":"temporal-discontinuity-designs.html","id":"applications-of-rdit","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.5 Applications of RDiT","text":"Regression Discontinuity Time widely applied across various disciplines, particularly economics marketing, policy changes, regulations, market shifts often provide natural time-based discontinuities. , summarize key studies employed RDiT estimate causal effects.","code":""},{"path":"temporal-discontinuity-designs.html","id":"applications-in-economics-1","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.5.1 Applications in Economics","text":"RDiT used extensively environmental economics, transportation policy, public health, regulations interventions introduced well-defined points time. notable studies include:Environmental Regulations Air QualitySeveral studies exploit sudden policy changes emission regulations estimate impact air pollution:Davis (2008), Auffhammer Kellogg (2011), H. Chen et al. (2018) examine impact environmental regulations air quality.Davis (2008), Auffhammer Kellogg (2011), H. Chen et al. (2018) examine impact environmental regulations air quality.Gallego, Montero, Salas (2013) compares RDiT Difference--Differences () estimates assess reliability control groups air pollution studies.Gallego, Montero, Salas (2013) compares RDiT Difference--Differences () estimates assess reliability control groups air pollution studies.leveraging RDiT, studies isolate immediate changes pollution levels policy thresholds controlling underlying trends.Traffic Transportation PoliciesRDiT used assess impact transportation policies congestion, accidents, public transit usage:Bento et al. (2014) M. L. Anderson (2014) investigate new subway openings transportation policies affect traffic congestion.Bento et al. (2014) M. L. Anderson (2014) investigate new subway openings transportation policies affect traffic congestion.De Paola, Scoppa, Falcone (2013) Burger, Kaffine, Yu (2014) analyze deterrent effects traffic safety regulations car accidents, using policy enactment dates discontinuity.De Paola, Scoppa, Falcone (2013) Burger, Kaffine, Yu (2014) analyze deterrent effects traffic safety regulations car accidents, using policy enactment dates discontinuity.studies demonstrate RDiT can effectively measure behavioral responses transportation interventions, isolating immediate impacts broader secular trends.COVID-19 Lockdowns Well-beingBrodeur et al. (2021) employs RDiT assess impact COVID-19 lockdowns psychological well-, economic activity, public health outcomes.sudden implementation lockdown policies provides sharp time-based discontinuity, making RDiT natural method evaluate effects.","code":""},{"path":"temporal-discontinuity-designs.html","id":"applications-in-marketing-1","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.5.2 Applications in Marketing","text":"marketing, RDiT applied analyze consumer behavior, pricing strategies, promotional effectiveness response abrupt policy market changes.Vehicle Pricing Demand ShocksSeveral studies leverage RDiT study consumers firms respond price changes automotive market:M. Busse, Silva-Risso, Zettelmeyer (2006), M. R. Busse, Simester, Zettelmeyer (2010), M. R. Busse et al. (2013) explore impact promotional campaigns policy changes vehicle prices.M. Busse, Silva-Risso, Zettelmeyer (2006), M. R. Busse, Simester, Zettelmeyer (2010), M. R. Busse et al. (2013) explore impact promotional campaigns policy changes vehicle prices.Davis Kahn (2010) investigates changes international trade policies influence vehicle pricing.Davis Kahn (2010) investigates changes international trade policies influence vehicle pricing.identifying sharp price discontinuities time policy changes, studies provide causal insights price elasticity consumer demand.Customer Satisfaction Learning EffectsX. Chen et al. (2009) applies RDiT measure customer satisfaction evolves firms make abrupt service changes.study demonstrates RDiT can capture immediate consumer reactions, distinguishing short-term dissatisfaction long-term adaptation.","code":""},{"path":"temporal-discontinuity-designs.html","id":"empirical-example","chapter":"28 Temporal Discontinuity Designs","heading":"28.1.6 Empirical Example","text":"Generating synthetic time-series data known cutoff T_starVisualizing entire dataset, including jump T_starFitting local linear RDiT model (baseline)Adding local polynomial (quadratic) allow nonlinear trendsDemonstrating robust (sandwich) standard errors account serial correlationPerforming “donut” RDiT excluding observations near T_starConducting placebo test fake cutoffDemonstrating augmented approach confounderPlotting local data fitted RDiT regression lines‘D’ local jump T_star (.e., treatment effect, tau).‘t_centered:D’ indicates slope differs post-cutoff.placebo test, D_placebo ideally insignificant.donut model, large differences baseline may suggest local anomalies anticipation near T_star.augmented model, controlling X can change tau X correlated Y time.","code":"\n# -------------------------------------------------------------------\n# 0. Libraries\n# -------------------------------------------------------------------\nif(!require(\"sandwich\")) install.packages(\"sandwich\", quiet=TRUE)\nif(!require(\"lmtest\"))   install.packages(\"lmtest\",   quiet=TRUE)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# -------------------------------------------------------------------\n# 1. Generate Synthetic Data\n# -------------------------------------------------------------------\nset.seed(123)\nn      <- 200                # total number of time points\nT_star <- 100                # cutoff (policy/intervention) time\nt_vals <- seq_len(n)         # time index: 1, 2, ..., n\n\n# Outcome Y has a linear pre-trend, a mild quadratic component, \n# and a jump of +5 at T_star\n# plus a small sinusoidal seasonality and random noise.\nY <- 0.4 * t_vals +          # baseline slope\n     0.002 * (t_vals^2) +    # mild curvature\n     ifelse(t_vals >= T_star, 5, 0) +  # jump at T_star\n     2*sin(t_vals / 8) +              # mild seasonal pattern\n     rnorm(n, sd = 2)                 # random noise\n\n# Optional confounder X that also increases with time\nX <- 1.5 * t_vals + rnorm(n, sd=5)\n\n# Store everything in a single data frame\ndf_full <- data.frame(t = t_vals, Y = Y, X = X)\n\n# -------------------------------------------------------------------\n# 2. Plot the Entire Dataset & Highlight the Cutoff\n# -------------------------------------------------------------------\npar(mfrow=c(1,2))  # We'll produce two plots side by side\n\n# Plot 1: Full data\nplot(df_full$t, df_full$Y, pch=16,\n     xlab=\"Time (t)\", ylab=\"Outcome (Y)\",\n     main=\"Full Time Series with True Jump at T_star\")\nabline(v=T_star, lwd=2)  # vertical line at the cutoff\n\n# -------------------------------------------------------------------\n# 3. Restrict to a Local Bandwidth (h) Around T_star\n# -------------------------------------------------------------------\nh <- 10\ndf_local <- subset(df_full, abs(t - T_star) < h)\n\n# Create variables for local regression\ndf_local$D           <- ifelse(df_local$t >= T_star, 1, 0)\ndf_local$t_centered  <- df_local$t - T_star\n\n# -------------------------------------------------------------------\n# 4. Baseline Local Linear RDiT Model\n# -------------------------------------------------------------------\n# Model:\n#   Y = alpha_0 + alpha_1*(t - T_star) + tau*D + alpha_2*(t - T_star)*D + error\nmod_rdit_linear <- lm(Y ~ t_centered + D + t_centered:D, data = df_local)\n# Robust (HC) standard errors for potential serial correlation\nres_rdit_linear <- coeftest(mod_rdit_linear, vcov = vcovHC(mod_rdit_linear, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 5. Local Polynomial (Quadratic) RDiT Model\n# -------------------------------------------------------------------\ndf_local$t_centered2 <- df_local$t_centered^2\nmod_rdit_quad <- lm(Y ~ t_centered + t_centered2 + D + t_centered:D + t_centered2:D,\n                    data = df_local)\nres_rdit_quad <- coeftest(mod_rdit_quad, vcov = vcovHC(mod_rdit_quad, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 6. Donut Approach: Excluding Observations Near the Cutoff\n# -------------------------------------------------------------------\ndf_donut <- subset(df_local, abs(t - T_star) > 1)  # remove t_star +/- 1 unit\nmod_rdit_donut <- lm(Y ~ t_centered + D + t_centered:D, data = df_donut)\nres_rdit_donut <- coeftest(mod_rdit_donut, vcov = vcovHC(mod_rdit_donut, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 7. Placebo Test: Fake Cutoff\n# -------------------------------------------------------------------\nT_fake <- 120\ndf_placebo <- subset(df_full, abs(t - T_fake) < h)\ndf_placebo$D_placebo          <- ifelse(df_placebo$t >= T_fake, 1, 0)\ndf_placebo$t_centered_placebo <- df_placebo$t - T_fake\n\nmod_rdit_placebo <- lm(Y ~ t_centered_placebo + D_placebo + t_centered_placebo:D_placebo,\n                       data = df_placebo)\nres_rdit_placebo <- coeftest(mod_rdit_placebo, vcov = vcovHC(mod_rdit_placebo, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 8. Augmented RDiT Model (Controlling for X)\n# -------------------------------------------------------------------\nmod_rdit_aug <- lm(Y ~ X + t_centered + D + t_centered:D, data = df_local)\nres_rdit_aug <- coeftest(mod_rdit_aug, vcov = vcovHC(mod_rdit_aug, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 9. Plot the Local Data and Fitted RDiT Lines\n# -------------------------------------------------------------------\nplot(df_local$t, df_local$Y, pch=16,\n     xlab=\"Time (t)\", ylab=\"Outcome (Y)\",\n     main=\"Local Window Around T_star\")\n\n# Sort data by centered time for a smooth line\ndf_local_sorted <- df_local[order(df_local$t_centered), ]\npred_linear     <- predict(mod_rdit_linear, newdata=df_local_sorted)\nlines(df_local_sorted$t, pred_linear, lwd=2)\n\n# Add a vertical line at T_star for reference\nabline(v=T_star, lwd=2)\n\n# -------------------------------------------------------------------\n# Print Summaries & Brief Interpretation\n# -------------------------------------------------------------------\n\ncat(\" Local Linear RDiT (Baseline):\\n\")\n#>  Local Linear RDiT (Baseline):\nprint(res_rdit_linear)\n#> \n#> t test of coefficients:\n#> \n#>               Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)  61.213434   1.625008 37.6696 2.853e-16 ***\n#> t_centered    1.032094   0.231380  4.4606 0.0004579 ***\n#> D             2.853971   1.772639  1.6100 0.1282338    \n#> t_centered:D -0.076391   0.271405 -0.2815 0.7821991    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n Local Quadratic RDiT:\\n\")\n#> \n#>  Local Quadratic RDiT:\nprint(res_rdit_quad)\n#> \n#> t test of coefficients:\n#> \n#>                Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)   61.827393   3.038967 20.3449 3.061e-11 ***\n#> t_centered     1.366981   1.342431  1.0183    0.3271    \n#> t_centered2    0.033489   0.124970  0.2680    0.7929    \n#> D              1.577410   3.103945  0.5082    0.6198    \n#> t_centered:D   0.085673   1.397606  0.0613    0.9521    \n#> t_centered2:D -0.088706   0.134490 -0.6596    0.5210    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n Donut RDiT (Excluding Observations Near T_star):\\n\")\n#> \n#>  Donut RDiT (Excluding Observations Near T_star):\nprint(res_rdit_donut)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)  62.52140    1.87095 33.4169 3.273e-13 ***\n#> t_centered    1.22829    0.27153  4.5235 0.0006975 ***\n#> D             2.97980    2.01021  1.4823 0.1640306    \n#> t_centered:D -0.49288    0.32020 -1.5393 0.1496810    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n Placebo Test (Fake Cutoff at T_fake=120):\\n\")\n#> \n#>  Placebo Test (Fake Cutoff at T_fake=120):\nprint(res_rdit_placebo)\n#> \n#> t test of coefficients:\n#> \n#>                               Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)                  82.658533   0.868852 95.1354 < 2.2e-16 ***\n#> t_centered_placebo            0.768238   0.185830  4.1341 0.0008831 ***\n#> D_placebo                    -0.377333   1.188411 -0.3175 0.7552328    \n#> t_centered_placebo:D_placebo -0.013586   0.245019 -0.0555 0.9565112    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n Augmented RDiT (Controlling for X):\\n\")\n#> \n#>  Augmented RDiT (Controlling for X):\nprint(res_rdit_aug)\n#> \n#> t test of coefficients:\n#> \n#>               Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  50.960390  13.226193  3.8530 0.001757 **\n#> X             0.068836   0.087887  0.7832 0.446540   \n#> t_centered    0.984583   0.249801  3.9415 0.001476 **\n#> D             2.922905   1.935753  1.5100 0.153291   \n#> t_centered:D -0.141496   0.318509 -0.4442 0.663655   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"temporal-discontinuity-designs.html","id":"sec-interrupted-time-series","chapter":"28 Temporal Discontinuity Designs","heading":"28.2 Interrupted Time Series","text":"Interrupted Time Series () powerful quasi-experimental method used assess intervention affects level /trend outcome time. analyzing long-term pre- post-intervention data, estimates happened absence intervention—assuming pre-existing trend continued unchanged.particularly useful policy, treatment, intervention implemented distinct point time, affecting entire population group simultaneously. differs RDiT typically models abrupt gradual changes time rather exploiting sharp discontinuity.well-specified model account :Seasonal trends: outcomes exhibit cyclical patterns (e.g., sales, disease prevalence), must adjusted .Seasonal trends: outcomes exhibit cyclical patterns (e.g., sales, disease prevalence), must adjusted .Concurrent events: changes occurring around time intervention may confound estimates, making difficult attribute observed changes solely intervention.Concurrent events: changes occurring around time intervention may confound estimates, making difficult attribute observed changes solely intervention.appropriate :Longitudinal data available: outcome must observed time, multiple data points intervention.population-wide intervention occurs specific time: intervention affect units simultaneously structured way allows stacking based intervention timing.NotesFor analyzing subgroup effects (heterogeneity treatment impact), see (Harper Bruckner 2017).interpreting results control variables, see (Bottomley, Scott, Isham 2019).Possible Threats Validity Analysis (Baicker Svoronos 2019)Delayed effects (Rodgers, John, Coleman 2005)\nimpact intervention may manifest time introduction. immediate post-intervention period assessed, key effects missed.Delayed effects (Rodgers, John, Coleman 2005)\nimpact intervention may manifest time introduction. immediate post-intervention period assessed, key effects missed.confounding events (Linden Yarnold 2016; Linden 2017)\nConcurrent policy changes external shocks overlap intervention period can obscure inflate apparent intervention effect.confounding events (Linden Yarnold 2016; Linden 2017)\nConcurrent policy changes external shocks overlap intervention period can obscure inflate apparent intervention effect.Intervention introduced later withdrawn (Linden 2015)\nintervention remain place, time series may reflect multiple shifts trends levels, complicating interpretation single “interrupted” period.Intervention introduced later withdrawn (Linden 2015)\nintervention remain place, time series may reflect multiple shifts trends levels, complicating interpretation single “interrupted” period.Autocorrelation\nTime series data often exhibit autocorrelation, can lead underestimated standard errors properly accounted , thus overstating statistical significance intervention effect.Autocorrelation\nTime series data often exhibit autocorrelation, can lead underestimated standard errors properly accounted , thus overstating statistical significance intervention effect.Regression mean\nshort-term shock, outcomes may revert toward prior average levels. Interpreting natural reversion intervention effect can misleading.Regression mean\nshort-term shock, outcomes may revert toward prior average levels. Interpreting natural reversion intervention effect can misleading.Selection bias\ncertain individuals settings receive intervention, pre-existing differences may confound results. Designs multiple groups comparison series can help mitigate bias.Selection bias\ncertain individuals settings receive intervention, pre-existing differences may confound results. Designs multiple groups comparison series can help mitigate bias.intervention, outcome can exhibit four distinct patterns:Key assumptions :pre-intervention trend remain stable intervention never occurred (.e., major time-varying confounders coinciding intervention).Data available multiple time points intervention estimate trends.following model integrates immediate sustained intervention effects (.e., segmented regression):\\[\nY_t = \\beta_0 + \\beta_1 T_t + \\beta_2 D_t + \\beta_3 (T_t \\times D_t) + \\beta_4 P_t + \\epsilon_t\n\\]:\\(Y_t\\): Outcome variable time \\(t\\).\\(T_t\\): Time index (continuous).\n\\(\\beta_1\\): Baseline slope (trend intervention).\n\\(\\beta_1\\): Baseline slope (trend intervention).\\(D_t\\): Intervention dummy (\\(D_t = 1\\) \\(t \\geq T^*\\), otherwise 0).\n\\(\\beta_2\\): Immediate effect (level change intervention).\n\\(\\beta_2\\): Immediate effect (level change intervention).\\((T_t \\times D_t)\\): Interaction term capturing change slope post-intervention.\n\\(\\beta_3\\): Difference slope intervention compared .\n\\(\\beta_3\\): Difference slope intervention compared .\\(P_t\\): Time since intervention (0 intervention, increments ).\n\\(\\beta_4\\): Sustained effect time.\n\\(\\beta_4\\): Sustained effect time.\\(\\epsilon_t\\): Error term (assumed normally distributed).model allows us :Measure pre-intervention trend (\\(\\beta_1\\)).Capture immediate effect intervention (\\(\\beta_2\\)).Identify slope changes post-intervention (\\(\\beta_3\\)).Examine long-term effects using \\(P_t\\) (\\(\\beta_4\\)).require purely immediate discontinuous effect; effect can gradual delayed, can captured additional terms (e.g., lags non-linear structures).","code":""},{"path":"temporal-discontinuity-designs.html","id":"advantages-of-its","chapter":"28 Temporal Discontinuity Designs","heading":"28.2.1 Advantages of ITS","text":"offers several benefits, particularly public policy, health research, economics. According (Penfold Zhang 2013), key advantages include:Controls long-term trends: Unlike simple pre/post comparisons, explicitly models pre-existing trajectories, reducing bias underlying trends.Controls long-term trends: Unlike simple pre/post comparisons, explicitly models pre-existing trajectories, reducing bias underlying trends.Applicable population-wide interventions: entire group region affected simultaneously, provides strong alternative traditional experimental methods.Applicable population-wide interventions: entire group region affected simultaneously, provides strong alternative traditional experimental methods.","code":""},{"path":"temporal-discontinuity-designs.html","id":"limitations-of-its","chapter":"28 Temporal Discontinuity Designs","heading":"28.2.2 Limitations of ITS","text":"valuable tool, key limitations:Requires sufficient number observations: least 8 data points 8 intervention typically recommended reliable estimation.Requires sufficient number observations: least 8 data points 8 intervention typically recommended reliable estimation.Challenging multiple overlapping events: several interventions occur close together time, can difficult isolate individual effects.Challenging multiple overlapping events: several interventions occur close together time, can difficult isolate individual effects.","code":""},{"path":"temporal-discontinuity-designs.html","id":"empirical-example-1","chapter":"28 Temporal Discontinuity Designs","heading":"28.2.3 Empirical Example","text":"Generating synthetic time-series data known intervention time T_starModeling Interrupted Time Series () immediate sustained effectsAccounting pre-intervention trend, immediate jump, slope change, post-intervention timeDemonstrating robust (sandwich) standard errors handle possible autocorrelationVisualizing data fitted linesBrief interpretation coefficientsWe’ll simulate time-series :baseline slope pre-interventionA baseline slope pre-interventionAn immediate jump \\(T^*\\)immediate jump \\(T^*\\)change slope \\(T^*\\)change slope \\(T^*\\)mild seasonal patternA mild seasonal patternRandom noiseRandom noise\\(T_t\\): time index (’ll just use ‘time’ )\\(T_t\\): time index (’ll just use ‘time’ )\\(D_t\\): indicator post-intervention (1 \\(t >= T^*\\), else 0)\\(D_t\\): indicator post-intervention (1 \\(t >= T^*\\), else 0)\\(P_t\\): time since intervention (0 \\(T^*\\), increments )\\(P_t\\): time since intervention (0 \\(T^*\\), increments )Model: \\[Y_t = \\beta_0 + \\beta_1*T + \\beta_2*D + \\beta_3*(T*D) + \\beta_4*P + \\epsilon_t\\] :\\(\\beta_0\\): baseline level\\(\\beta_0\\): baseline level\\(\\beta_1\\): pre-intervention slope\\(\\beta_1\\): pre-intervention slope\\(\\beta_2\\): immediate jump T_star\\(\\beta_2\\): immediate jump T_star\\(\\beta_3\\): change slope post-intervention\\(\\beta_3\\): change slope post-intervention\\(\\beta_4\\): sustained effect time since intervention\\(\\beta_4\\): sustained effect time since intervention\\(\\epsilon_t\\): error term\\(\\epsilon_t\\): error term(Intercept) = \\(\\beta_0\\): Baseline level \\(T=0\\).T = \\(\\beta_1\\): Baseline slope (pre-intervention trend).D = \\(\\beta_2\\): Immediate jump (level change) T_star.T:D = \\(\\beta_3\\): Slope change post-intervention.P = \\(\\beta_4\\): Sustained (additional) effect time since \\(T^*\\).compare observed data counterfactual (assuming treatment) (Lee Rodgers, Beasley, Schuelke 2014).Notes Real-World Usage:Consider checking seasonality explicitly (e.g., Fourier terms), covariates might confound outcome.Consider checking seasonality explicitly (e.g., Fourier terms), covariates might confound outcome.Assess autocorrelation (e.g., Durbin-Watson test, Box-Jenkins approach).Assess autocorrelation (e.g., Durbin-Watson test, Box-Jenkins approach).practice, also run diagnostics conduct robustness checks, e.g., removing overlapping interventions investigating delayed effects.practice, also run diagnostics conduct robustness checks, e.g., removing overlapping interventions investigating delayed effects.","code":"\n# -------------------------------------------------------------------\n# 0. Libraries\n# -------------------------------------------------------------------\nif(!require(\"sandwich\")) install.packages(\"sandwich\", quiet=TRUE)\nif(!require(\"lmtest\"))   install.packages(\"lmtest\",   quiet=TRUE)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# -------------------------------------------------------------------\n# 1. Generate Synthetic Data\n# -------------------------------------------------------------------\nset.seed(456)\nn       <- 50               # total number of time points\nT_star  <- 25               # intervention time\nt_vals  <- seq_len(n)       # time index: 1, 2, ..., n\nY <- 1.0 * t_vals +                      # baseline slope\n  ifelse(t_vals >= T_star, 10, 0) +   # immediate jump of +10 at T_star\n  \n  # additional slope post-intervention\n  ifelse(t_vals >= T_star, 0.5 * (t_vals - T_star), 0) +\n  5 * sin(t_vals / 6) +                # mild seasonal pattern\n  rnorm(n, sd = 3)                   # random noise\n\n# Combine into a data frame\ndf_its <- data.frame(time = t_vals, Y = Y)\n\n# -------------------------------------------------------------------\n# 2. Define Key ITS Variables\n# -------------------------------------------------------------------\ndf_its$D  <- ifelse(df_its$time >= T_star, 1, 0)\ndf_its$T  <- df_its$time\ndf_its$P  <- ifelse(df_its$time >= T_star, df_its$time - T_star, 0)\n\n# -------------------------------------------------------------------\n# 3. Plot the Entire Dataset & Highlight the Intervention\n# -------------------------------------------------------------------\nplot(\n    df_its$T,\n    df_its$Y,\n    pch = 16,\n    xlab = \"Time (T)\",\n    ylab = \"Outcome (Y)\",\n    main = \"Full Series with Intervention at T_star\"\n)\nabline(v = T_star, lwd = 2)  # vertical line for the intervention\n# -------------------------------------------------------------------\n# 4. Fit the Comprehensive ITS Model (Segmented Regression)\n# -------------------------------------------------------------------\nmod_its <- lm(Y ~ T + D + I(T*D) + P, data = df_its)\n\n# Use robust standard errors to account for potential autocorrelation\nres_its <- coeftest(mod_its, vcov = vcovHC(mod_its, type=\"HC1\"))\n\n# -------------------------------------------------------------------\n# 5. Create Fitted Values for Plotting\n# -------------------------------------------------------------------\ndf_its$pred_its <- predict(mod_its)\n\n# -------------------------------------------------------------------\n# 6. Plot the Observed Data & Fitted ITS Lines\n# -------------------------------------------------------------------\nplot(df_its$T, df_its$Y, pch=16,\n     xlab=\"Time (T)\", ylab=\"Outcome (Y)\",\n     main=\"ITS: Observed vs. Fitted\")\nabline(v=T_star, lwd=2)\nlines(df_its$T, df_its$pred_its, lwd=2)\n\n# -------------------------------------------------------------------\n# 7. Summaries & Brief Interpretation\n# -------------------------------------------------------------------\n\nprint(res_its)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept)   5.42261    2.06039  2.6318   0.01152 *  \n#> T             0.78737    0.16204  4.8589 1.409e-05 ***\n#> D           -28.27162    3.95530 -7.1478 5.472e-09 ***\n#> I(T * D)      1.25703    0.18351  6.8498 1.531e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nplot(\n    df_its$T,\n    df_its$Y,\n    pch = 16,\n    col = \"gray\",\n    xlab = \"Time (T)\",\n    ylab = \"Outcome (Y)\",\n    main = \"ITS: Observed vs. Fitted vs. Counterfactual\"\n)\n\n# Add vertical line indicating intervention point\nabline(v = T_star, lwd = 2, col = \"black\")\n\n# Add fitted ITS model trend\nlines(df_its$T,\n      df_its$pred_its,\n      lwd = 2,\n      col = \"blue\")\n\n# Add counterfactual trend (what would have happened without intervention)\nlines(\n    df_its$T,\n    df_its$pred_counterfactual,\n    lwd = 2,\n    col = \"red\",\n    lty = 2\n)\n\n# Add legend\nlegend(\n    \"topleft\",\n    legend = c(\"Observed Data\", \"Fitted ITS\", \"Counterfactual\"),\n    col = c(\"gray\", \"blue\", \"red\"),\n    lty = c(NA, 1, 2),\n    pch = c(16, NA, NA),\n    lwd = c(NA, 2, 2)\n)"},{"path":"temporal-discontinuity-designs.html","id":"combining-both-rdit-and-its","chapter":"28 Temporal Discontinuity Designs","heading":"28.3 Combining both RDiT and ITS","text":"Combining Regression Discontinuity Time Interrupted Time Series single regression framework standard, hybrid augmented approaches can capture :sharp local discontinuity cutoff (\\(T^*\\)).Longer-term trend changes intervention.conceptual strategies merging RDiT ideas.","code":""},{"path":"temporal-discontinuity-designs.html","id":"augment-an-its-model-with-a-local-discontinuity-term","chapter":"28 Temporal Discontinuity Designs","heading":"28.3.1 Augment an ITS Model with a Local Discontinuity Term","text":"comprehensive can written using following form:\\[\nY_t = \\beta_0 + \\beta_1 T_t + \\beta_2 D_t + \\beta_3 \\bigl(T_t \\times D_t\\bigr) + \\beta_4 P_t + \\epsilon_t,\n\\]:\\(T_t\\) continuous time index\n\\(\\beta_1\\) baseline slope (pre-intervention).\n\\(\\beta_1\\) baseline slope (pre-intervention).\\(D_t = 1\\) \\(t \\geq T^*\\) (0 otherwise)\n\\(\\beta_2\\) immediate jump (level change) \\(T^*\\).\n\\(\\beta_2\\) immediate jump (level change) \\(T^*\\).\\(\\bigl(T_t \\times D_t\\bigr)\\) interaction term allows slope differ \\(T^*\\)\n\\(\\beta_3\\) difference slope post-\\(T^*\\).\n\\(\\beta_3\\) difference slope post-\\(T^*\\).\\(P_t\\) time elapsed since \\(T^*\\) (0 \\(T^*\\))\n\\(\\beta_4\\) captures sustained effect intervention.\n\\(\\beta_4\\) captures sustained effect intervention.\\(\\epsilon_t\\) error term.embed RDiT-style local discontinuity, define bandwidth \\(h\\) around \\(T^*\\) add local polynomial terms (e.g., linear) inside window:\\[\n\\begin{aligned}\nY_t =\\;& \\beta_0\n       + \\beta_1 T_t\n       + \\beta_2 D_t\n       + \\beta_3 \\bigl(T_t \\times D_t\\bigr)\n       + \\beta_4 P_t \\\\\n     &\\quad + \\alpha_1 \\bigl(T_t - T^*\\bigr)\\mathbf{1}\\bigl(\\lvert T_t - T^* \\rvert < h\\bigr)\n       + \\alpha_2 \\bigl(T_t - T^*\\bigr) D_t \\,\\mathbf{1}\\bigl(\\lvert T_t - T^* \\rvert < h\\bigr)\n       + \\epsilon_t.\n\\end{aligned}\n\\]\\(\\mathbf{1}(\\cdot)\\) indicator function 1 time index within \\(h\\) \\(T^*\\) (0 otherwise).\\(\\beta\\) terms capture global pre/post trends, level changes, sustained effects.\\(\\alpha\\) terms capture local curvature sharper jump within \\(\\pm h\\) region \\(T^*\\).approach leverages entire time series trend estimation (), also including local (RDiT-like) estimation immediate discontinuity around \\(T^*\\).Cautions:need sufficient data globally () locally near \\(T^*\\) (RDiT terms).Extra parameters can lead complexity potential overfitting.","code":""},{"path":"temporal-discontinuity-designs.html","id":"two-stage-or-multi-stage-modeling","chapter":"28 Temporal Discontinuity Designs","heading":"28.3.2 Two-Stage (or Multi-Stage) Modeling","text":"ad hoc sometimes practical workflow :Stage 1: Local RDiTFocus chosen bandwidth around \\(T^*\\).Fit local polynomial capture immediate jump \\(T^*\\).Estimate \\(\\hat{\\tau}\\) jump.Stage 2: Full SeriesUse entire time series segmented (interrupted) regression framework.Incorporate \\(\\hat{\\tau}\\) explicitly known offset treat prior estimate jump.practice, adjust outcome estimated jump:\\[\nY_t^* = Y_t - \\hat{\\tau}\\,D_t,\n\\]fit augmented model:\\[\nY_t^* = \\beta_0 + \\beta_1 T_t + \\beta_2 D_t + \\beta_3 \\bigl(T_t \\times D_t\\bigr) + \\beta_4 P_t + \\nu_t.\n\\], \\(Y_t^*\\) outcome removing locally estimated jump Stage 1. single unified regression, two-step approach combining local (RDiT) global () analyses.","code":""},{"path":"temporal-discontinuity-designs.html","id":"hierarchical-or-multi-level-modeling","chapter":"28 Temporal Discontinuity Designs","heading":"28.3.3 Hierarchical or Multi-Level Modeling","text":"unified, Bayesian hierarchical approach can also combine RDiT :\\[\n\\begin{aligned}\nY_{t} &= \\underbrace{\\beta_0\n           + \\beta_1 T_t\n           + \\beta_2 D_t\n           + \\beta_3 \\bigl(T_t \\times D_t\\bigr)\n           + \\beta_4 P_t}_{\\text{Global component}}\n           \\\\\n       &\\quad\\;+\\;\n       \\underbrace{\\alpha_1 \\bigl(T_t - T^*\\bigr)\\mathbf{1}\\bigl(\\lvert T_t - T^* \\rvert < h\\bigr)\n           + \\alpha_2 \\bigl(T_t - T^*\\bigr) D_t\\,\\mathbf{1}\\bigl(\\lvert T_t - T^* \\rvert < h\\bigr)}_{\\text{Local RDiT component}}\n           \\;+\\; \\epsilon_t.\n\\end{aligned}\n\\]Global component captures overall level shifts, slope changes, sustained effects.Local RDiT component captures sharp jump local polynomial shape around \\(T^*\\).Additional hierarchical layers (e.g., group-level effects) can handle multiple groups multiple cutoffs.Using one strategies ensures capture global pre/post-intervention trends () local discontinuities near \\(T^*\\) (RDiT).","code":""},{"path":"temporal-discontinuity-designs.html","id":"empirical-example-2","chapter":"28 Temporal Discontinuity Designs","heading":"28.3.4 Empirical Example","text":"Generating synthetic data global pattern (immediate jump, slope change, sustained effect) plus localized “extra” jump around \\(T^* \\pm h\\) (RDiT-style).Fitting single unified model includes:\nGlobal terms (beta coefficients)\nLocal RDiT terms (alpha coefficients) within bandwidth h\nGlobal terms (beta coefficients)Local RDiT terms (alpha coefficients) within bandwidth hShowing two-stage approach first estimate local jump adjust outcome.Visualizing results.’ll create dataset :Baseline slope (pre-intervention)Baseline slope (pre-intervention)Immediate jump \\(T^*\\)Immediate jump \\(T^*\\)Post-intervention slope change (style)Post-intervention slope change (style)Sustained effectSustained effectPLUS local polynomial “extra jump” around \\(T^* \\pm h=5\\) (RDiT style)PLUS local polynomial “extra jump” around \\(T^* \\pm h=5\\) (RDiT style)Create outcome \\(Y\\) :baseline slope: \\(0.3 * t\\)baseline slope: \\(0.3 * t\\)immediate jump \\(T^*\\): +5immediate jump \\(T^*\\): +5slope change \\(T^*\\): +0.2 per unit time (beyond baseline)slope change \\(T^*\\): +0.2 per unit time (beyond baseline)sustained effect: \\(0.1*( \\text{time since } T^* )\\)sustained effect: \\(0.1*( \\text{time since } T^* )\\)local polynomial “extra jump” region \\([T^* \\pm h]\\): \\((t - T^*) * 2\\) active within \\(\\pm h\\) \\(T^*\\)local polynomial “extra jump” region \\([T^* \\pm h]\\): \\((t - T^*) * 2\\) active within \\(\\pm h\\) \\(T^*\\)random noiserandom noiseBeta terms (\\(T_c, D, T_c:D, P\\)) capture global components:\nBaseline slope (\\(\\beta_1\\))\nImmediate jump (\\(\\beta_2\\))\nPost-intervention slope change (\\(\\beta_3\\))\nSustained effect (\\(\\beta_4\\))\nBaseline slope (\\(\\beta_1\\))Immediate jump (\\(\\beta_2\\))Post-intervention slope change (\\(\\beta_3\\))Sustained effect (\\(\\beta_4\\))Alpha terms (t_centered * local_indicator, etc.) capture local RDiT effect.two-stage approach, \\(\\hat{\\tau}\\) local jump. Subtracting (Stage 1) yields ‘cleaned’ \\(Y^*\\), fit simpler (Stage 2).","code":"\n# -------------------------------------------------------------------\n# 0. Libraries\n# -------------------------------------------------------------------\nif(!require(\"sandwich\")) install.packages(\"sandwich\", quiet=TRUE)\nif(!require(\"lmtest\"))   install.packages(\"lmtest\",   quiet=TRUE)\nlibrary(sandwich)\nlibrary(lmtest)\n\n# -------------------------------------------------------------------\n# 1. Generate Synthetic Data\n# -------------------------------------------------------------------\nset.seed(111)\nn       <- 150             # total number of time points\nT_star  <- 80              # cutoff/intervention time\nt_vals  <- seq_len(n)      # time index: 1, 2, ..., n\nh <- 5  # bandwidth for local discontinuity\nwithin_h <- abs(t_vals - T_star) < h  # indicator for local region\n\nY <- 0.3 * t_vals +                                        # baseline slope\n     ifelse(t_vals >= T_star, 5, 0) +                      # immediate jump\n     ifelse(t_vals >= T_star, 0.2*(t_vals - T_star), 0) +  # slope change\n     ifelse(t_vals >= T_star, 0.1*(t_vals - T_star), 0) +  # sustained effect\n     ifelse(within_h, 2*(t_vals - T_star), 0) +            # local polynomial jump\n     rnorm(n, sd=2)                                        # noise\n\n# Put it in a data frame\ndf <- data.frame(t = t_vals, Y = Y)\n\n# -------------------------------------------------------------------\n# 2. Define Variables for a Single Unified Model\n# -------------------------------------------------------------------\ndf$D   <- ifelse(df$t >= T_star, 1, 0)          # intervention dummy\ndf$T_c <- df$t                                 # rename time to T_c for clarity\ndf$P   <- ifelse(df$t >= T_star, df$t - T_star, 0)  # time since intervention\n\n# Indicator for local region (± h around T_star)\ndf$local_indicator <- ifelse(abs(df$t - T_star) < h, 1, 0)\n\n# Center time around T_star for local polynomial\ndf$t_centered <- df$t - T_star\n\n# -------------------------------------------------------------------\n# 3. Fit the Unified \"Augmented ITS + RDiT\" Model\n# -------------------------------------------------------------------\n# Y_t = beta_0\n#       + beta_1 * T_c\n#       + beta_2 * D\n#       + beta_3 * (T_c * D)\n#       + beta_4 * P\n#       + alpha_1 * (t_centered)* local_indicator\n#       + alpha_2 * (t_centered)* D * local_indicator\n#       + epsilon_t\n#\nmod_hybrid <- lm(\n  Y ~ T_c + D + I(T_c*D) + P +\n       I(t_centered * local_indicator) +\n       I(t_centered * D * local_indicator),\n  data = df\n)\n\n# Robust standard errors\nres_hybrid <- coeftest(mod_hybrid, vcov = vcovHC(mod_hybrid, type = \"HC1\"))\n\n# -------------------------------------------------------------------\n# 4. A Two-Stage Approach for Illustrative Purposes\n# -------------------------------------------------------------------\n# STAGE 1: Local RDiT to estimate extra local jump around T_star ± h\ndf_local <- subset(df, abs(t - T_star) < h)\nmod_local <- lm(Y ~ t_centered*D, data = df_local)\n\n# estimate of the jump at T_star from local model\ntau_hat <- coef(mod_local)[\"D\"]  \n\n# Adjust outcome by subtracting local jump * D\ndf$Y_star <- df$Y - tau_hat * df$D\n\n# STAGE 2: Fit a standard ITS on the adjusted outcome Y_star\nmod_its_adjusted <- lm(Y_star ~ T_c + D + I(T_c*D) + P, data = df)\nres_its_adjusted <-\n    coeftest(mod_its_adjusted, vcov = vcovHC(mod_its_adjusted, type = \"HC1\"))\n\n# -------------------------------------------------------------------\n# 5. Plot the Data and Fitted Lines\n# -------------------------------------------------------------------\npar(mfrow=c(1,2))\n\n# Plot 1: Observed data vs. fitted \"Hybrid Model\"\nplot(\n  df$t,\n  df$Y,\n  pch = 16,\n  xlab = \"Time (t)\",\n  ylab = \"Outcome (Y)\",\n  main = \"Hybrid RDiT+ITS Model\"\n)\nabline(v = T_star, lwd = 2, lty = 2)  # cutoff\nlines(df$t,\n      predict(mod_hybrid),\n      col = \"blue\",\n      lwd = 2)\n\n# Plot 2: Adjusted Data (Two-Stage) vs. Fitted ITS\nplot(\n  df$t,\n  df$Y_star,\n  pch = 16,\n  xlab = \"Time (t)\",\n  ylab = \"Outcome (Y*)\",\n  main = \"Two-Stage Approach: Adjusted Y*\"\n)\nabline(v = T_star, lwd = 2, lty = 2)\nlines(df$t,\n      predict(mod_its_adjusted),\n      col = \"red\",\n      lwd = 2)\n\nprint(res_hybrid)\n#> \n#> t test of coefficients:\n#> \n#>                                       Estimate Std. Error  t value  Pr(>|t|)\n#> (Intercept)                          -1.066743   0.453019  -2.3547   0.01989\n#> T_c                                   0.328522   0.009090  36.1409 < 2.2e-16\n#> D                                   -19.072974   1.557761 -12.2438 < 2.2e-16\n#> I(T_c * D)                            0.282215   0.015636  18.0488 < 2.2e-16\n#> I(t_centered * local_indicator)       2.197553   0.323646   6.7900 2.726e-10\n#> I(t_centered * D * local_indicator)  -0.317783   0.399128  -0.7962   0.42723\n#>                                        \n#> (Intercept)                         *  \n#> T_c                                 ***\n#> D                                   ***\n#> I(T_c * D)                          ***\n#> I(t_centered * local_indicator)     ***\n#> I(t_centered * D * local_indicator)    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n Two-Stage Approach:\\n\")\n#> \n#>  Two-Stage Approach:\ncat(\"   (1) Local RDiT => estimated local jump = \", round(tau_hat,2), \"\\n\")\n#>    (1) Local RDiT => estimated local jump =  3.27\ncat(\"   (2) Standard ITS on adjusted outcome:\\n\")\n#>    (2) Standard ITS on adjusted outcome:\nprint(res_its_adjusted)\n#> \n#> t test of coefficients:\n#> \n#>               Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)  -0.553196   0.507846  -1.0893   0.2778    \n#> T_c           0.308729   0.012701  24.3075   <2e-16 ***\n#> D           -20.268345   1.889371 -10.7276   <2e-16 ***\n#> I(T_c * D)    0.281836   0.019730  14.2844   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"temporal-discontinuity-designs.html","id":"practical-guidance","chapter":"28 Temporal Discontinuity Designs","heading":"28.3.5 Practical Guidance","text":"Data Requirements: must enough data local window around \\(T^*\\) RDiT enough pre- post-intervention observations .Avoid -Parameterization: Adding many interaction polynomial terms can quickly increase complexity.Interpretation: single unified model merges local jumps global trends may difficult interpret. Distinguish clearly parameters capture sharp discontinuity versus long-run trend.practice, hybrid approach can work truly believe local immediate jump longer-run trend change standard alone may fully capture. However, weigh added complexity quality richness data.","code":""},{"path":"temporal-discontinuity-designs.html","id":"case-crossover-study-design","chapter":"28 Temporal Discontinuity Designs","heading":"28.4 Case-Crossover Study Design","text":"case-crossover study observational epidemiological method designed primarily assess transient effects acute exposures risk sudden-onset outcomes. Introduced Maclure (1991), design utilizes individuals controls, thus inherently adjusting stable, within-subject confounders.example, consider assessing whether vigorous exercise increases risk myocardial infarction (MI) (Mittleman et al. 1993):Hazard Period: 1-hour preceding MI onset.Control Periods: Identical 1-hour windows days.Applying conditional logistic regression, estimate odds ratio () indicating acute exercise influences MI risk.Implementation R: Z. Zhang (2016) season packageAdvantagesSelf-Matching: Controls individual-level confounding, including genetics, demographics, chronic health status (Maclure 1991).Efficiency: Requires fewer subjects due within-person matching (Mittleman, Maclure, Robins 1995).Applicability: Highly suitable studying acute triggers (e.g., air pollution, physical exertion, drug use) (Lumley Levy 2000).LimitationsCarryover Effects: Difficulties arise exposure prolonged effects (Maclure Mittleman 2000).Time-Varying Confounding: inherently control confounders changing within short timeframes unless explicitly modeled (Navidi 1998).Selection Bias: Incorrect selection control periods can bias results (Lumley Levy 2000).","code":""},{"path":"temporal-discontinuity-designs.html","id":"mathematical-foundations","chapter":"28 Temporal Discontinuity Designs","heading":"28.4.1 Mathematical Foundations","text":"case-crossover design compares exposure status “hazard” “case” period immediately preceding event exposure status “control” periods event occur (Maclure Mittleman 2000).Formally, let:\\(X_{,t}\\) denote exposure status (binary continuous) individual \\(\\) time \\(t\\).\\(Y_{,t}\\) indicate occurrence (1) absence (0) acute event time \\(t\\).probability event occurring time \\(t\\) given exposure modeled conditional logistic regression:\\[ \\text{logit}(P(Y_{,t}=1|X_{,t})) = \\alpha_i + \\beta X_{,t} \\]:\\(\\alpha_i\\) subject-specific fixed effect controls time-invariant confounding within subjects.\\(\\alpha_i\\) subject-specific fixed effect controls time-invariant confounding within subjects.\\(\\beta\\) log-odds ratio quantifying exposure effect.\\(\\beta\\) log-odds ratio quantifying exposure effect.conditional logistic regression explicitly considers matched strata (individual cases) compares distribution exposure case control periods within individual:\\[ L(\\beta) = \\prod_{=1}^{n} \\frac{\\exp(\\beta X_{,case})}{\\exp(\\beta X_{,case}) + \\sum_{j \\C_i} \\exp(\\beta X_{,j})} \\]\\(X_{,case}\\) represents exposure hazard period case \\(\\).\\(X_{,j}\\) represents exposure control periods \\(C_i\\) case \\(\\).","code":""},{"path":"temporal-discontinuity-designs.html","id":"selection-of-control-periods","chapter":"28 Temporal Discontinuity Designs","heading":"28.4.2 Selection of Control Periods","text":"","code":""},{"path":"temporal-discontinuity-designs.html","id":"bidirectional","chapter":"28 Temporal Discontinuity Designs","heading":"28.4.2.1 Bidirectional","text":"Bidirectional (symmetrical) selection involves choosing control periods event equally (Navidi 1998):\\[ \\text{Control Periods} = \\{t - k, t + k\\}, k \\\\mathbb{N} \\]method mitigates biases related temporal trends exposure.","code":""},{"path":"temporal-discontinuity-designs.html","id":"unidirectional","chapter":"28 Temporal Discontinuity Designs","heading":"28.4.2.2 Unidirectional","text":"Unidirectional selection (either exclusively prior subsequent event) might introduce biases exposure trends strong (Lumley Levy 2000):\\[ \\text{Control Periods} = \\{t - k\\}, k \\\\mathbb{N} \\]","code":""},{"path":"temporal-discontinuity-designs.html","id":"assumptions-1","chapter":"28 Temporal Discontinuity Designs","heading":"28.4.3 Assumptions","text":"Transient Effect Assumption: Exposure effects acute short-lived (Maclure 1991).Exchangeability Hazard Control Periods: Conditional individual fixed effects, control periods accurately represent exposure probabilities hazard periods (Mittleman, Maclure, Robins 1995).Carryover Effects: Exposure one period affect subsequent periods (Maclure Mittleman 2000).","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"sec-synthetic-difference-in-differences","chapter":"29 Synthetic Difference-in-Differences","heading":"29 Synthetic Difference-in-Differences","text":"Understanding impact policy interventions fundamental challenge empirical research. Analysts frequently rely panel data, tracks multiple units time, assess outcomes change policy implemented. However, estimating causal effects setting complicated fact policies often adopted non-randomly—certain units may likely receive treatment based characteristics, prior trends, external factors. treatment assignment correlates unit-specific time-specific factors, selection bias can invalidate causal conclusions, even observed covariates accounted (G. W. Imbens Rubin 2015).well-established approach dealing challenge Difference--Differences () method, compares changes outcomes treated untreated groups. However, relies crucial parallel trends assumption—idea , absence treatment, treated untreated units followed similar trajectories. assumption violated, can produce biased estimates.alternative approach, Synthetic Control (SC), particularly useful units receive treatment. Instead assuming parallel trends, SC constructs weighted combination untreated units best approximates pre-treatment behavior treated group. method improves comparability limitations, including sensitivity extrapolation instability many control units.address shortcomings SC, Synthetic Difference--Differences (SDID) introduced (Arkhangelsky et al. 2021). SDID weighted double-differencing estimator :Combines features SC improve causal inference.Adjusts differences pre-treatment trends reweighting (like SC).Remains invariant additive unit-level shifts valid large panels (like ).key advantage SDID achieves double robustness: performs well assumptions hold also corrects deviations parallel trends using synthetic weights. makes SDID particularly useful situations treatment assignment correlated latent factors affect outcomes time.SDID explicitly accounts systematic unit-level effects influence treatment assignment.especially valuable treatment non-randomly assigned based persistent unit characteristics.Even purely random treatment assignment, three methods (, SC, SDID) unbiased—SDID achieves smallest standard error (SE).Recent research applied SDID evaluate marketing interventions policy changes:TV Advertising & Online Sales (Lambrecht, Tucker, Zhang 2024): SDID used estimate effect TV ad campaigns consumer behavior, accounting time-varying confounders.Soda Tax & Marketing Effectiveness (Keller, Guyt, Grewal 2024): SDID helped measure soda tax policies influenced marketing strategies consumer demand.Key Differences SCAttractive Features SDIDStatistical Properties\nProvides consistent asymptotically normal estimates.\nAchieves double robustness, similar augmented inverse probability weighting estimators Scharfstein, Rotnitzky, Robins (1999).\nProvides consistent asymptotically normal estimates.Achieves double robustness, similar augmented inverse probability weighting estimators Scharfstein, Rotnitzky, Robins (1999).Performance Comparison\nsettings valid, SDID performs least well better.\nUnlike , SDID remains valid treatment assignment correlates latent unit-level time-varying factors.\nsettings SC used, SDID performs equally well better.\ntreatment randomly assigned, methods remain unbiased, SDID tends higher precision.\nsettings valid, SDID performs least well better.Unlike , SDID remains valid treatment assignment correlates latent unit-level time-varying factors.settings SC used, SDID performs equally well better.treatment randomly assigned, methods remain unbiased, SDID tends higher precision.Bias Reduction\nParticularly effective treatment assignment uniformly random.\nOffers robustness violations standard SC assumptions.\nParticularly effective treatment assignment uniformly random.Offers robustness violations standard SC assumptions.Related Augmented SC\nShares methodological similarities augmented SC estimators (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112).\nShares methodological similarities augmented SC estimators (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112).Recommended Use Cases\nSDID particularly effective :\nnumber control units (\\(N_{ctr}\\)) similar number pre-treatment periods (\\(T_{pre}\\)).\nnumber post-treatment periods (\\(T_{post}\\)) small.\nnumber treated units (\\(N_{tr}\\)) satisfies:\\[ N_{tr} < \\sqrt{N_{ctr}} \\]\nnumber control units (\\(N_{ctr}\\)) similar number pre-treatment periods (\\(T_{pre}\\)).number post-treatment periods (\\(T_{post}\\)) small.number treated units (\\(N_{tr}\\)) satisfies:\\[ N_{tr} < \\sqrt{N_{ctr}} \\]","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"understanding","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1 Understanding","text":"formally define SDID estimator, begin considering balanced panel dataset \\(N\\) units observed \\(T\\) time periods. goal estimate causal effect treatment intervention accounting unit-specific time-specific confounders.Let:\\(Y_{}\\) outcome variable unit \\(\\) time \\(t\\).\\(W_{} \\\\{0,1\\}\\) binary indicator treatment, \\(W_{} = 1\\) unit \\(\\) treated time \\(t\\) \\(0\\) otherwise.panel consists :\n\\(N_c\\) control units (never treated).\n\\(N_t\\) treated units, exposed treatment period \\(T_{pre}\\).\n\\(N_c\\) control units (never treated).\\(N_t\\) treated units, exposed treatment period \\(T_{pre}\\).","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"steps-in-sdid-estimation","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.1 Steps in SDID Estimation","text":"SDID combines ideas SC introducing unit weights time weights:Find unit weights \\(\\hat{w}_i^{sdid}\\) ensure pre-treatment outcomes weighted control group match pre-treatment outcomes treated units:\\[\n\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} Y_{} \\approx \\frac{1}{N_t} \\sum_{= N_c + 1}^{N} Y_{}, \\quad \\forall t = 1, \\dots, T_{pre}\n\\]\nensures pre-treatment trends treated control units similar, just SC.Find unit weights \\(\\hat{w}_i^{sdid}\\) ensure pre-treatment outcomes weighted control group match pre-treatment outcomes treated units:\\[\n\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} Y_{} \\approx \\frac{1}{N_t} \\sum_{= N_c + 1}^{N} Y_{}, \\quad \\forall t = 1, \\dots, T_{pre}\n\\]\nensures pre-treatment trends treated control units similar, just SC.Find time weights \\(\\hat{\\lambda}_t^{sdid}\\) balance post-treatment deviations pre-treatment outcomes, stabilizing inference.Find time weights \\(\\hat{\\lambda}_t^{sdid}\\) balance post-treatment deviations pre-treatment outcomes, stabilizing inference.Estimate treatment effect \\(\\hat{\\tau}^{sdid}\\) solving following minimization problem:\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\sum_{=1}^{N} \\sum_{t=1}^{T} (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid} \\hat{\\lambda}_t^{sdid}\n\\]\n:\n\\(\\mu\\) global intercept.\n\\(\\alpha_i\\) captures unit-specific fixed effects.\n\\(\\beta_t\\) captures time-specific fixed effects.\n\\(\\tau\\) represents treatment effect.\nEstimate treatment effect \\(\\hat{\\tau}^{sdid}\\) solving following minimization problem:\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\sum_{=1}^{N} \\sum_{t=1}^{T} (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid} \\hat{\\lambda}_t^{sdid}\n\\]\n:\\(\\mu\\) global intercept.\\(\\alpha_i\\) captures unit-specific fixed effects.\\(\\beta_t\\) captures time-specific fixed effects.\\(\\tau\\) represents treatment effect.Unlike standard SC estimators, SDID incorporates unit time weights, making less sensitive violations parallel trends.solves:\\[\n(\\hat{\\tau}^{}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\sum_{=1}^{N} \\sum_{t=1}^{T} (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2\n\\]\nHowever, use unit time weights, making unreliable treatment assignment correlates unobserved factors.SC minimizes:\\[\n(\\hat{\\tau}^{sc}, \\hat{\\mu}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\beta} \\sum_{=1}^{N} \\sum_{t=1}^{T} (Y_{} - \\mu - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid}\n\\]\nSC include unit fixed effects (\\(\\alpha_i\\)) time weights (\\(\\hat{\\lambda}_t\\)), can introduce bias unmeasured confounders vary time.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"comparison-of-methods-1","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.2 Comparison of Methods","text":"table summarizes key differences , SC, SDID:alternative formulation SDID treatment effect :\\[\n\\hat{\\tau} = \\hat{\\delta}_t - \\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} \\hat{\\delta}_i\n\\]\n:\\(\\hat{\\delta}_t = \\frac{1}{N_t} \\sum_{= N_c + 1}^{N} \\hat{\\delta}_i\\) represents average deviation treated units post-treatment.\\(\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} \\hat{\\delta}_i\\) adjusts differences using weighted control unit deviations.key innovation Synthetic Difference--Differences use unit time weights refine causal estimates. applying weights, SDID essentially localizes standard two-way fixed effects regression, making robust precise.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"why-use-weights","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.3 Why Use Weights?","text":"Unit weights (\\(\\hat{w}_i^{sdid}\\)) emphasize control units similar treated units based pre-treatment trends.Time weights (\\(\\hat{\\lambda}_t^{sdid}\\)) prioritize time periods comparable post-treatment period.approach improves parallel trends validity without requiring perfect match raw data.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"benefits-of-localization-in-sdid","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.4 Benefits of Localization in SDID","text":"Robustness\nfocusing comparable units time periods, SDID reduces bias dissimilar observations.\nfocusing comparable units time periods, SDID reduces bias dissimilar observations.Improved Precision\nSDID eliminates predictable variation outcomes, reducing standard errors (SEs) compared SC\nSEs SDID smaller SC.\nCaveat: outcome heterogeneity minimal, unequal weighting might slightly reduce precision relative standard .\nSDID eliminates predictable variation outcomes, reducing standard errors (SEs) compared SCSEs SDID smaller SC.Caveat: outcome heterogeneity minimal, unequal weighting might slightly reduce precision relative standard .","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"designing-sdid-weights","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.5 Designing SDID Weights","text":"","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"unit-weights-balancing-pre-treatment-trends","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.5.1 Unit Weights: Balancing Pre-Treatment Trends","text":"Unit weights \\(\\hat{w}_i^{sdid}\\) ensure weighted control group mimics treated group’s pre-treatment trends, similar SC greater flexibility:\\[\n\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} Y_{} \\approx \\frac{1}{N_t} \\sum_{= N_c + 1}^{N} Y_{}, \\quad \\forall t = 1, \\dots, T_{pre}\n\\]\nhelps achieve parallel pre-treatment trends rather requiring exact match levels.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"time-weights-stabilizing-post-treatment-inference","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.5.2 Time Weights: Stabilizing Post-Treatment Inference","text":"Time weights \\(\\hat{\\lambda}_t^{sdid}\\) ensure post-treatment deviations balanced relative pre-treatment trends. minimizes bias -weighting time periods vastly different post-treatment period.Unlike unit weights, time weights require regularization outcomes within time period highly correlated across units.Time weights improve precision SDID estimates preventing certain periods dominating estimation process.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"how-sdid-enhances-dids-plausibility","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.6 How SDID Enhances DID’s Plausibility","text":"assumes parallel trends, raw data often violates assumption. SDID corrects non-parallel trends weighting units time periods.Similar techniques used adjust assumptions, controlling covariates selecting specific time periods (Callaway Sant’Anna 2021).SDID automates process, applying systematic weighting approach units time periods.Including unit fixed effects (\\(\\alpha_i\\)) SDID two main advantages:Flexibility:\nAllows systematic differences across units preserving parallel trends reweighting.\nAllows systematic differences across units preserving parallel trends reweighting.Enhanced Precision:\nExplains large fraction variation outcomes, reducing noise improving estimation accuracy.\nExplains large fraction variation outcomes, reducing noise improving estimation accuracy.Unit Fixed Effects SC WeightingUnder ideal conditions, SC reweighting alone account unit fixed effects—weighted average control unit outcomes perfectly matched treated unit’s pre-treatment trajectory.However, rarely happens reality, making unit fixed effects necessary robust inference.use fixed effects synthetic control regressions (SC intercept) first proposed Doudchenko Imbens (2016) Ferman Pinto (2021), referred DIFP (Difference--Fixed-Effects Prediction).","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"choosing-sdid-weights","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.7 Choosing SDID Weights","text":"Choosing Unit WeightsRegularization Parameter:penalty term calibrated based typical one-period change control unit outcomes pre-treatment period.value multiplied scaling factor (Arkhangelsky et al. 2021, 4092).Relation Synthetic Control Weights:SDID weights resemble used Abadie, Diamond, Hainmueller (2010) two key modifications:Inclusion Intercept Term\nUnlike SC, SDID force control pre-trends exactly match treated pre-trends—ensures parallel.\nflexibility arises unit fixed effects, absorb systematic level differences.\nUnlike SC, SDID force control pre-trends exactly match treated pre-trends—ensures parallel.flexibility arises unit fixed effects, absorb systematic level differences.Regularization Penalty\nBorrowed Doudchenko Imbens (2016).\nEnsures dispersion weights, preventing -reliance control units.\nguarantees unique solution unit weights.\nBorrowed Doudchenko Imbens (2016).Ensures dispersion weights, preventing -reliance control units.guarantees unique solution unit weights.SDID Compares Weights:weights special case SC weights without intercept regularization penalty.applies unit weights SC one treated unit.Choosing Time WeightsLike unit weights, time weights include intercept term account overall time effects.regularization applied time weights within-period correlations across units expected.design allows SDID minimize bias stabilizing inference.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"accounting-for-time-varying-covariates-in-weight-estimation","chapter":"29 Synthetic Difference-in-Differences","heading":"29.1.8 Accounting for Time-Varying Covariates in Weight Estimation","text":"refine estimation process, SDID can incorporate time-varying covariates adjusting outcome variable:\\[\nY_{}^{res} = Y_{} - X_{} \\hat{\\beta}\n\\]\n\\(\\hat{\\beta}\\) comes regression:\\[\nY_{} = X_{} \\beta + \\varepsilon_{}\n\\]\nresidualized outcome (\\(Y_{}^{res}\\)) ensures weighting process accounts time-varying confounders, improving validity causal estimates.","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"application-3","chapter":"29 Synthetic Difference-in-Differences","heading":"29.2 Application","text":"SDID AlgorithmCompute regularization parameter \\(\\zeta\\)\\[\n\\zeta = (N_{t}T_{post})^{1/4} \\hat{\\sigma}\n\\]\\[\n\\hat{\\sigma}^2 = \\frac{1}{N_c(T_{pre}- 1)} \\sum_{= 1}^{N_c} \\sum_{t = 1}^{T_{re}-1}(\\Delta_{} - \\hat{\\Delta})^2\n\\]\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)Compute unit weights \\(\\hat{w}^{sdid}\\)\\[\n(\\hat{w}_0, \\hat{w}^{sidid}) = \\arg \\min_{w_0 \\R, w \\\\Omega}l_{unit}(w_0, w)\n\\]\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)Compute time weights \\(\\hat{\\lambda}^{sdid}\\)\\[\n(\\hat{\\lambda}_0 , \\hat{\\lambda}^{sdid}) = \\arg \\min_{\\lambda_0 \\R, \\lambda \\\\Lambda} l_{time}(\\lambda_0, \\lambda)\n\\]\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)Compute SDID estimator\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta}\\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid}\\hat{\\lambda}_t^{sdid}\n\\]SE EstimationUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUsing asymptotic variance, conventional confidence intervals can applied SDID.Using asymptotic variance, conventional confidence intervals can applied SDID.\\[\n\\tau \\\\hat{\\tau}^{sdid} \\pm z_{\\alpha/2}\\sqrt{\\hat{V}_\\tau}\n\\]3 approaches variance estimation confidence intervals:\nClustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\n\nJackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\n\nPlacebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\n\n3 approaches variance estimation confidence intervals:Clustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\nClustered Bootstrap (Efron 1992):Independently resample units.Independently resample units.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Disadvantage: Computationally expensive.Disadvantage: Computationally expensive.Jackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\nJackknife (Miller 1974):Applied weighted SDID regression fixed weights.Applied weighted SDID regression fixed weights.Generally conservative precise treated control units sufficiently similar.Generally conservative precise treated control units sufficiently similar.recommended methods, like SC estimator, due potential biases.recommended methods, like SC estimator, due potential biases.Appropriate jackknifing without random weights.Appropriate jackknifing without random weights.Placebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\nPlacebo Variance Estimation:Can used cases one treated unit large panels.Can used cases one treated unit large panels.Placebo evaluations swap treated unit untreated ones estimate noise.Placebo evaluations swap treated unit untreated ones estimate noise.Relies homoskedasticity across units.Relies homoskedasticity across units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).algorithms Arkhangelsky et al. (2021), p. 4109:Bootstrap Variance EstimationFor \\(b\\) \\(1 \\B\\):\nSample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.\nsample lacks treated control units, resample.\nCalculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).\n\\(b\\) \\(1 \\B\\):Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.sample lacks treated control units, resample.sample lacks treated control units, resample.Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Jackknife Variance EstimationFor \\(\\) \\(1 \\N\\):\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)Calculate: \\(\\hat{V}_{\\tau} = (N - 1) N^{-1} \\sum_{= 1}^N (\\hat{\\tau}^{(-)} - \\hat{\\tau})^2\\)Placebo Variance EstimationFor \\(b\\) \\(1 \\B\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatment\nConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controls\nCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatmentConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controlsCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)Calculate \\(\\hat{V}_\\tau = \\frac{1}{B}\\sum_{b = 1}^B (\\hat{\\tau}^b - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)","code":""},{"path":"sec-synthetic-difference-in-differences.html","id":"block-treatment","chapter":"29 Synthetic Difference-in-Differences","heading":"29.2.1 Block Treatment","text":"Code provided synthdid package","code":"\nlibrary(synthdid)\nlibrary(tidyverse)\n\n# Estimate the effect of California Proposition 99 on cigarette consumption\ndata('california_prop99')\n\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\ntau.hat = synthdid::synthdid_estimate(setup$Y, setup$N0, setup$T0)\n\n# se = sqrt(vcov(tau.hat, method = 'placebo'))\n\nplot(tau.hat) + causalverse::ama_theme()\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\n# Run for specific estimators\nresults_selected = causalverse::panel_estimate(setup,\n                                               selected_estimators = c(\"synthdid\", \"did\", \"sc\"))\n\nresults_selected\n#> $synthdid\n#> $synthdid$estimate\n#> synthdid: -15.604 +- NA. Effective N0/N0 = 16.4/38~0.4. Effective T0/T0 = 2.8/19~0.1. N1,T1 = 1,12. \n#> \n#> $synthdid$std.error\n#> [1] 10.05324\n#> \n#> \n#> $did\n#> $did$estimate\n#> synthdid: -27.349 +- NA. Effective N0/N0 = 38.0/38~1.0. Effective T0/T0 = 19.0/19~1.0. N1,T1 = 1,12. \n#> \n#> $did$std.error\n#> [1] 15.81479\n#> \n#> \n#> $sc\n#> $sc$estimate\n#> synthdid: -19.620 +- NA. Effective N0/N0 = 3.8/38~0.1. Effective T0/T0 = Inf/19~Inf. N1,T1 = 1,12. \n#> \n#> $sc$std.error\n#> [1] 11.16422\n\n# to access more details in the estimate object\nsummary(results_selected$did$estimate)\n#> $estimate\n#> [1] -27.34911\n#> \n#> $se\n#>      [,1]\n#> [1,]   NA\n#> \n#> $controls\n#>                estimate 1\n#> Wyoming             0.026\n#> Wisconsin           0.026\n#> West Virginia       0.026\n#> Virginia            0.026\n#> Vermont             0.026\n#> Utah                0.026\n#> Texas               0.026\n#> Tennessee           0.026\n#> South Dakota        0.026\n#> South Carolina      0.026\n#> Rhode Island        0.026\n#> Pennsylvania        0.026\n#> Oklahoma            0.026\n#> Ohio                0.026\n#> North Dakota        0.026\n#> North Carolina      0.026\n#> New Mexico          0.026\n#> New Hampshire       0.026\n#> Nevada              0.026\n#> Nebraska            0.026\n#> Montana             0.026\n#> Missouri            0.026\n#> Mississippi         0.026\n#> Minnesota           0.026\n#> Maine               0.026\n#> Louisiana           0.026\n#> Kentucky            0.026\n#> Kansas              0.026\n#> Iowa                0.026\n#> Indiana             0.026\n#> Illinois            0.026\n#> Idaho               0.026\n#> Georgia             0.026\n#> Delaware            0.026\n#> Connecticut         0.026\n#> \n#> $periods\n#>      estimate 1\n#> 1988      0.053\n#> 1987      0.053\n#> 1986      0.053\n#> 1985      0.053\n#> 1984      0.053\n#> 1983      0.053\n#> 1982      0.053\n#> 1981      0.053\n#> 1980      0.053\n#> 1979      0.053\n#> 1978      0.053\n#> 1977      0.053\n#> 1976      0.053\n#> 1975      0.053\n#> 1974      0.053\n#> 1973      0.053\n#> 1972      0.053\n#> 1971      0.053\n#> \n#> $dimensions\n#>           N1           N0 N0.effective           T1           T0 T0.effective \n#>            1           38           38           12           19           19\n\ncausalverse::process_panel_estimate(results_selected)\n#>     Method Estimate    SE\n#> 1 SYNTHDID   -15.60 10.05\n#> 2      DID   -27.35 15.81\n#> 3       SC   -19.62 11.16"},{"path":"sec-synthetic-difference-in-differences.html","id":"staggered-adoption","chapter":"29 Synthetic Difference-in-Differences","heading":"29.2.2 Staggered Adoption","text":"apply staggered adoption settings using SDID estimator (see examples Arkhangelsky et al. (2021), p. 4115 similar Ben-Michael, Feller, Rothstein (2022)), can:Apply SDID estimator repeatedly, every adoption date.Apply SDID estimator repeatedly, every adoption date.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Create multiple samples splitting data time periods. sample consistent adoption date.Create multiple samples splitting data time periods. sample consistent adoption date.formal note special case, see Porreca (2022). compares outcomes using SynthDiD estimators:Two-Way Fixed Effects (TWFE),Two-Way Fixed Effects (TWFE),group time average treatment effect estimator Callaway Sant’Anna (2021),group time average treatment effect estimator Callaway Sant’Anna (2021),partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.findings reveal SynthDiD produces different estimate average treatment effect compared methods.\nSimulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.\nfindings reveal SynthDiD produces different estimate average treatment effect compared methods.Simulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.explore heterogeneity treatment effect, can subgroup analysis (Berman Israeli 2022, 1092)Split data separate subsets subgroup.Compute synthetic effects subset.Use control group consisting non-adopters balanced panel cohort analysis.Switch treatment units subgroup analyzed.Perform synthdid analysis.Use data estimate synthetic control weights.Compute treatment effects using treated subgroup units treatment units.Plot different estimators","code":"\nlibrary(tidyverse)\ndf <- fixest::base_stagg |>\n   dplyr::mutate(treatvar = if_else(time_to_treatment >= 0, 1, 0)) |>\n   dplyr::mutate(treatvar = as.integer(if_else(year_treated > (5 + 2), 0, treatvar)))\n\n\nest <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\"\n)\n#> Adoption Cohort: 5 \n#> Treated units: 5 Control units: 65 \n#> Adoption Cohort: 6 \n#> Treated units: 5 Control units: 60 \n#> Adoption Cohort: 7 \n#> Treated units: 5 Control units: 55\n\ndata.frame(\n    Period = names(est$TE_mean_w),\n    ATE    = est$TE_mean_w,\n    SE     = est$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2 -0.05 0.22\n#> 2      -1  0.05 0.22\n#> 3       0 -5.07 0.80\n#> 4       1 -4.68 0.51\n#> 5       2 -3.70 0.79\n#> 6 cumul.0 -5.07 0.80\n#> 7 cumul.1 -4.87 0.55\n#> 8 cumul.2 -4.48 0.53\n\n\ncausalverse::synthdid_plot_ate(est)\nest_sub <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\",\n  # a vector of subgroup id (from unit id)\n  subgroup           =  c(\n    # some are treated\n    \"11\", \"30\", \"49\" ,\n    # some are control within this period\n    \"20\", \"25\", \"21\")\n)\n#> Adoption Cohort: 5 \n#> Treated units: 3 Control units: 65 \n#> Adoption Cohort: 6 \n#> Treated units: 0 Control units: 60 \n#> Adoption Cohort: 7 \n#> Treated units: 0 Control units: 55\n\ndata.frame(\n    Period = names(est_sub$TE_mean_w),\n    ATE = est_sub$TE_mean_w,\n    SE = est_sub$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2  0.32 0.44\n#> 2      -1 -0.32 0.44\n#> 3       0 -4.29 1.68\n#> 4       1 -4.00 1.52\n#> 5       2 -3.44 2.90\n#> 6 cumul.0 -4.29 1.68\n#> 7 cumul.1 -4.14 1.52\n#> 8 cumul.2 -3.91 1.82\n\ncausalverse::synthdid_plot_ate(est)\nlibrary(causalverse)\nmethods <- c(\"synthdid\", \"did\", \"sc\", \"sc_ridge\", \"difp\", \"difp_ridge\")\n\nestimates <- lapply(methods, function(method) {\n  synthdid_est_ate(\n    data               = df,\n    adoption_cohorts   = 5:7,\n    lags               = 2,\n    leads              = 2,\n    time_var           = \"year\",\n    unit_id_var        = \"id\",\n    treated_period_var = \"year_treated\",\n    treat_stat_var     = \"treatvar\",\n    outcome_var        = \"y\",\n    method = method\n  )\n})\n\nplots <- lapply(seq_along(estimates), function(i) {\n  causalverse::synthdid_plot_ate(estimates[[i]],\n                                 title = methods[i],\n                                 theme = causalverse::ama_theme(base_size = 6))\n})\n\ngridExtra::grid.arrange(grobs = plots, ncol = 2)"},{"path":"sec-difference-in-differences.html","id":"sec-difference-in-differences","chapter":"30 Difference-in-Differences","heading":"30 Difference-in-Differences","text":"Difference--Differences () widely used causal inference method estimating effect policy interventions exogenous shocks randomized experiments feasible. key idea behind compare changes outcomes time treated control groups, assumption —absent treatment—groups followed parallel trends.analysis can go beyond simple treatment effects exploring causal mechanisms using mediation moderation analyses:Mediation : Examines intermediate variables (e.g., consumer sentiment, brand perception) mediate treatment effect (Habel, Alavi, Linsenmayer 2021).Moderation Analysis: Studies treatment effects vary across different groups (e.g., high vs. low brand loyalty) (Goldfarb Tucker 2011).","code":""},{"path":"sec-difference-in-differences.html","id":"empirical-studies","chapter":"30 Difference-in-Differences","heading":"30.1 Empirical Studies","text":"","code":""},{"path":"sec-difference-in-differences.html","id":"applications-of-did-in-marketing","chapter":"30 Difference-in-Differences","heading":"30.1.1 Applications of DID in Marketing","text":"extensively applied marketing business research measure impact policy changes, advertising campaigns, competitive actions. several notable examples:TV Advertising & Online Shopping (Liaukonyte, Teixeira, Wilbur 2015): Examines TV ads influence consumer behavior online shopping.Political Advertising & Voting Behavior (Wang, Lewis, Schweidel 2018): Uses geographic discontinuities state borders analyze ad sources tone affect voter turnout.Music Streaming & Consumption (Datta, Knox, Bronnenberg 2018): Investigates adopting music streaming service affects total music consumption.Data Breaches & Customer Spending (Janakiraman, Lim, Rishika 2018): Analyzes customer spending changes firm announces data breach.Price Monitoring & Policy Enforcement (Israeli 2018): Studies effect digital monitoring minimum advertised price policy enforcement.Foreign Direct Investment & Firm Responses (Ramani Srinivasan 2019): Examines firms India responded FDI liberalization reforms 1991.Paywalls & Readership (Pattabhiramaiah, Sriram, Manchanda 2019): Investigates implementing paywalls affects online news consumption.Aggregators & Airline Business (Akca Rao 2020): Evaluates online aggregators impact airline ticket sales.Nutritional Labels & Competitive Response (Lim et al. 2020): Analyzes whether nutrition labels affect nutritional quality competing brands.Payment Disclosure & Physician Behavior (Guo, Sriram, Manchanda 2020): Studies payment disclosure laws impact prescription behavior.Fake Reviews & Sales (S. , Hollenbeck, Proserpio 2022): Uses Amazon policy change measure effect fake reviews sales ratings.Data Protection Regulations & Website Usage (Peukert et al. 2022): Assesses impact GDPR regulations website usage online business models.","code":""},{"path":"sec-difference-in-differences.html","id":"applications-of-did-in-economics","chapter":"30 Difference-in-Differences","heading":"30.1.2 Applications of DID in Economics","text":"also extensively applied economics, particularly policy evaluation, labor economics, macroeconomics:Natural Experiments Development Economics (Rosenzweig Wolpin 2000)Instrumental Variables & Natural Experiments (J. D. Angrist Krueger 2001)Macroeconomic Policy Analysis (Fuchs-Schündeln Hassan 2016)","code":""},{"path":"sec-difference-in-differences.html","id":"sec-visualization-did","chapter":"30 Difference-in-Differences","heading":"30.2 Visualization","text":"","code":"\nlibrary(panelView)\nlibrary(fixest)\nlibrary(tidyverse)\nbase_stagg <- fixest::base_stagg |>\n    # treatment status\n    dplyr::mutate(treat_stat = dplyr::if_else(time_to_treatment < 0, 0, 1)) |> \n    select(id, year, treat_stat, y)\n\nhead(base_stagg)\n#>   id year treat_stat           y\n#> 2 90    1          0  0.01722971\n#> 3 89    1          0 -4.58084528\n#> 4 88    1          0  2.73817174\n#> 5 87    1          0 -0.65103066\n#> 6 86    1          0 -5.33381664\n#> 7 85    1          0  0.49562631\n\npanelView::panelview(\n    y ~ treat_stat,\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# alternatively specification\npanelView::panelview(\n    Y = \"y\",\n    D = \"treat_stat\",\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# Average outcomes for each cohort\npanelView::panelview(\n    data = base_stagg, \n    Y = \"y\",\n    D = \"treat_stat\",\n    index = c(\"id\", \"year\"),\n    by.timing = T,\n    display.all = F,\n    type = \"outcome\", \n    by.cohort = T\n)\n#> Number of unique treatment histories: 10"},{"path":"sec-difference-in-differences.html","id":"sec-simple-difference-in-differences","chapter":"30 Difference-in-Differences","heading":"30.3 Simple Difference-in-Differences","text":"Difference--Differences originated tool analyze natural experiments, applications extend far beyond . built Fixed Effects Estimator, making fundamental approach policy evaluation causal inference observational studies.leverages inter-temporal variation groups:Cross-sectional comparison: Helps avoid omitted variable bias due common trends.Time-series comparison: Helps mitigate omitted variable bias due cross-sectional heterogeneity.","code":""},{"path":"sec-difference-in-differences.html","id":"basic-setup-of-did","chapter":"30 Difference-in-Differences","heading":"30.3.1 Basic Setup of DID","text":"Consider simple setting :Treatment Group (\\(D_i = 1\\))Control Group (\\(D_i = 0\\))Pre-Treatment Period (\\(T = 0\\))Post-Treatment Period (\\(T = 1\\))fundamental challenge: observe \\(E[Y_{0i}(1)|D_i = 1]\\)—.e., counterfactual outcome treated group received treatment.estimates Average Treatment Effect Treated using following formula:\\[\n\\begin{aligned}\nE[Y_1(1) - Y_0(1) | D = 1] &= \\{E[Y(1)|D = 1] - E[Y(1)|D = 0] \\} \\\\\n&- \\{E[Y(0)|D = 1] - E[Y(0)|D = 0] \\}\n\\end{aligned}\n\\]formulation differences time-invariant unobserved factors, assuming parallel trends assumption holds.treated group, isolate difference treated treated.control group experienced different trajectory, estimate may biased.Since observe treatment variation control group, infer treatment effect group.table organizes mean outcomes four cells:Control Group, Pre-period (\\(\\bar{Y}_{00}\\)): Mean outcome control group intervention.Control Group, Pre-period (\\(\\bar{Y}_{00}\\)): Mean outcome control group intervention.Control Group, Post-period (\\(\\bar{Y}_{01}\\)): Mean outcome control group intervention.Control Group, Post-period (\\(\\bar{Y}_{01}\\)): Mean outcome control group intervention.Treated Group, Pre-period (\\(\\bar{Y}_{10}\\)): Mean outcome treated group intervention.Treated Group, Pre-period (\\(\\bar{Y}_{10}\\)): Mean outcome treated group intervention.Treated Group, Post-period (\\(\\bar{Y}_{11}\\)): Mean outcome treated group intervention.Treated Group, Post-period (\\(\\bar{Y}_{11}\\)): Mean outcome treated group intervention.treatment effect calculated simple formula averages identical estimate OLS regression interaction term.treatment effect calculated :\\(\\text{} = (\\bar{Y}_{11} - \\bar{Y}_{10}) - (\\bar{Y}_{01} - \\bar{Y}_{00})\\)Compute manually:\\((\\bar{Y}_{11} - \\bar{Y}_{10}) - (\\bar{Y}_{01} - \\bar{Y}_{00})\\)Use OLS regression:\\(Y_{} = \\beta_0 + \\beta_1 \\text{treated}_i + \\beta_2 \\text{time}_t + \\beta_3 (\\text{treated}_i \\cdot \\text{time}_t) + \\epsilon_{}\\)Using simulated table:\\(\\text{} = (14 - 8) - (7 - 5) = 6 - 2 = 4\\)matches interaction term coefficient (\\(\\beta_3 = 4\\)) OLS regression.methods give result!","code":"\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nset.seed(1)\n\n# Simulated dataset for illustration\ndata <- data.frame(\n  time = rep(c(0, 1), each = 50),  # Pre (0) and Post (1)\n  treated = rep(c(0, 1), times = 50), # Control (0) and Treated (1)\n  error = rnorm(100)\n)\n\n# Generate outcome variable\ndata$outcome <-\n    5 + 3 * data$treated + 2 * data$time + \n    4 * data$treated * data$time + data$error\n\n# Compute averages for 2x2 table\ntable_means <- data %>%\n  group_by(treated, time) %>%\n  summarize(mean_outcome = mean(outcome), .groups = \"drop\") %>%\n  mutate(\n    group = paste0(ifelse(treated == 1, \"Treated\", \"Control\"), \", \", \n                   ifelse(time == 1, \"Post\", \"Pre\"))\n  )\n\n# Display the 2x2 table\ntable_2x2 <- table_means %>%\n  select(group, mean_outcome) %>%\n  tidyr::spread(key = group, value = mean_outcome)\n\nprint(\"2x2 Table of Mean Outcomes:\")\n#> [1] \"2x2 Table of Mean Outcomes:\"\nprint(table_2x2)\n#> # A tibble: 1 × 4\n#>   `Control, Post` `Control, Pre` `Treated, Post` `Treated, Pre`\n#>             <dbl>          <dbl>           <dbl>          <dbl>\n#> 1            7.19           5.20            14.0           8.00\n\n# Calculate Diff-in-Diff manually\n\n# Treated, Post\nY11 <- table_means$mean_outcome[table_means$group == \"Treated, Post\"]  \n\n# Treated, Pre\nY10 <- table_means$mean_outcome[table_means$group == \"Treated, Pre\"]   \n\n# Control, Post\nY01 <- table_means$mean_outcome[table_means$group == \"Control, Post\"]  \n\n# Control, Pre\nY00 <- table_means$mean_outcome[table_means$group == \"Control, Pre\"]   \n\ndiff_in_diff_formula <- (Y11 - Y10) - (Y01 - Y00)\n\n# Estimate DID using OLS\nmodel <- lm(outcome ~ treated * time, data = data)\nols_estimate <- coef(model)[\"treated:time\"]\n\n# Print results\nresults <- data.frame(\n  Method = c(\"Diff-in-Diff Formula\", \"OLS Estimate\"),\n  Estimate = c(diff_in_diff_formula, ols_estimate)\n)\n\nprint(\"Comparison of DID Estimates:\")\n#> [1] \"Comparison of DID Estimates:\"\nprint(results)\n#>                            Method Estimate\n#>              Diff-in-Diff Formula 4.035895\n#> treated:time         OLS Estimate 4.035895\n\n# Visualization\nggplot(data,\n       aes(\n           x = as.factor(time),\n           y = outcome,\n           color = as.factor(treated),\n           group = treated\n       )) +\n    stat_summary(fun = mean, geom = \"point\", size = 3) +\n    stat_summary(fun = mean,\n                 geom = \"line\",\n                 linetype = \"dashed\") +\n    labs(\n        title = \"Difference-in-Differences Visualization\",\n        x = \"Time (0 = Pre, 1 = Post)\",\n        y = \"Outcome\",\n        color = \"Group\"\n    ) +\n    scale_color_manual(labels = c(\"Control\", \"Treated\"),\n                       values = c(\"blue\", \"red\")) +\n    causalverse::ama_theme()"},{"path":"sec-difference-in-differences.html","id":"extensions-of-did","chapter":"30 Difference-in-Differences","heading":"30.3.2 Extensions of DID","text":"","code":""},{"path":"sec-difference-in-differences.html","id":"did-with-more-than-two-groups-or-time-periods","chapter":"30 Difference-in-Differences","heading":"30.3.2.1 DID with More Than Two Groups or Time Periods","text":"can extended multiple treatments, multiple controls, two periods:\\[\nY_{igt} = \\alpha_g + \\gamma_t + \\beta I_{gt} + \\delta X_{igt} + \\epsilon_{igt}\n\\]:\\(\\alpha_g\\) = Group-Specific Fixed Effects (e.g., firm, region).\\(\\alpha_g\\) = Group-Specific Fixed Effects (e.g., firm, region).\\(\\gamma_t\\) = Time-Specific Fixed Effects (e.g., year, quarter).\\(\\gamma_t\\) = Time-Specific Fixed Effects (e.g., year, quarter).\\(\\beta\\) = Effect.\\(\\beta\\) = Effect.\\(I_{gt}\\) = Interaction Terms (Treatment × Post-Treatment).\\(I_{gt}\\) = Interaction Terms (Treatment × Post-Treatment).\\(\\delta X_{igt}\\) = Additional Covariates.\\(\\delta X_{igt}\\) = Additional Covariates.known Two-Way Fixed Effects model. However, TWFE performs poorly staggered treatment adoption, different groups receive treatment different times.","code":""},{"path":"sec-difference-in-differences.html","id":"examining-long-term-effects-dynamic-did","chapter":"30 Difference-in-Differences","heading":"30.3.2.2 Examining Long-Term Effects (Dynamic DID)","text":"examine dynamic treatment effects (rollout/staggered design), can create centered time variable.Last pre-treatment period right treatment period(Baseline/Reference Group)Dynamic Treatment Model SpecificationBy interacting factor variable, can examine dynamic effect treatment (.e., whether ’s fading intensifying):\\[\n\\begin{aligned}\nY &= \\alpha_0 + \\alpha_1 Group + \\alpha_2 Time  \\\\\n&+ \\beta_{-T_1} Treatment + \\beta_{-(T_1 -1)} Treatment + \\dots + \\beta_{-1} Treatment \\\\\n&+ \\beta_1 + \\dots + \\beta_{T_2} Treatment\n\\end{aligned}\n\\]:\\(\\beta_0\\) (Baseline Period) reference group (.e., drop model).\\(\\beta_0\\) (Baseline Period) reference group (.e., drop model).\\(T_1\\) = Pre-Treatment Period.\\(T_1\\) = Pre-Treatment Period.\\(T_2\\) = Post-Treatment Period.\\(T_2\\) = Post-Treatment Period.Treatment coefficients (\\(\\beta_t\\)) measure effect time.Treatment coefficients (\\(\\beta_t\\)) measure effect time.Key Observations:Pre-treatment coefficients close zero (\\(\\beta_{-T_1}, \\dots, \\beta_{-1} \\approx 0\\)), ensuring pre-trend bias.Pre-treatment coefficients close zero (\\(\\beta_{-T_1}, \\dots, \\beta_{-1} \\approx 0\\)), ensuring pre-trend bias.Post-treatment coefficients significantly different zero (\\(\\beta_1, \\dots, \\beta_{T_2} \\neq 0\\)), measuring treatment effect time.Post-treatment coefficients significantly different zero (\\(\\beta_1, \\dots, \\beta_{T_2} \\neq 0\\)), measuring treatment effect time.Higher standard errors interactions: Including many lags can reduce precision.Higher standard errors interactions: Including many lags can reduce precision.","code":""},{"path":"sec-difference-in-differences.html","id":"did-on-relationships-not-just-levels","chapter":"30 Difference-in-Differences","heading":"30.3.2.3 DID on Relationships, Not Just Levels","text":"can also applied relationships variables rather just outcome levels.example, can used estimate treatment effects regression coefficients comparing relationships policy change.","code":""},{"path":"sec-difference-in-differences.html","id":"goals-of-did","chapter":"30 Difference-in-Differences","heading":"30.3.3 Goals of DID","text":"Pre-Treatment Coefficients Insignificant\nEnsure \\(\\beta_{-T_1}, \\dots, \\beta_{-1} = 0\\) (similar Placebo Test).\nEnsure \\(\\beta_{-T_1}, \\dots, \\beta_{-1} = 0\\) (similar Placebo Test).Post-Treatment Coefficients Significant\nVerify \\(\\beta_1, \\dots, \\beta_{T_2} \\neq 0\\).\nExamine whether trend post-treatment coefficients increasing decreasing time.\nVerify \\(\\beta_1, \\dots, \\beta_{T_2} \\neq 0\\).Examine whether trend post-treatment coefficients increasing decreasing time.","code":"\nlibrary(tidyverse)\nlibrary(fixest)\n\nod <- causaldata::organ_donations %>%\n    \n    # Treatment variable\n    dplyr::mutate(California = State == 'California') %>%\n    # centered time variable\n    dplyr::mutate(center_time = as.factor(Quarter_Num - 3))  \n# where 3 is the reference period precedes the treatment period\n\nclass(od$California)\n#> [1] \"logical\"\nclass(od$State)\n#> [1] \"character\"\n\ncali <- feols(Rate ~ i(center_time, California, ref = 0) |\n                  State + center_time,\n              data = od)\n\netable(cali)\n#>                                              cali\n#> Dependent Var.:                              Rate\n#>                                                  \n#> California x center_time = -2    -0.0029 (0.0051)\n#> California x center_time = -1   0.0063** (0.0023)\n#> California x center_time = 1  -0.0216*** (0.0050)\n#> California x center_time = 2  -0.0203*** (0.0045)\n#> California x center_time = 3    -0.0222* (0.0100)\n#> Fixed-Effects:                -------------------\n#> State                                         Yes\n#> center_time                                   Yes\n#> _____________________________ ___________________\n#> S.E.: Clustered                         by: State\n#> Observations                                  162\n#> R2                                        0.97934\n#> Within R2                                 0.00979\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\niplot(cali, pt.join = T)\ncoefplot(cali)"},{"path":"sec-difference-in-differences.html","id":"empirical-research-walkthrough","chapter":"30 Difference-in-Differences","heading":"30.4 Empirical Research Walkthrough","text":"","code":""},{"path":"sec-difference-in-differences.html","id":"example-the-unintended-consequences-of-ban-the-box-policies","chapter":"30 Difference-in-Differences","heading":"30.4.1 Example: The Unintended Consequences of “Ban the Box” Policies","text":"Doleac Hansen (2020) examine unintended effects “Ban Box” (BTB) policies, prevent employers asking criminal records hiring process. intended goal BTB increase job access individuals criminal records. However, study found employers, unable observe criminal history, resorted statistical discrimination based race, leading unintended negative consequences.Three Types “Ban Box” Policies:Public employers onlyPrivate employers government contractsAll employersIdentification StrategyIf county within Metropolitan Statistical Area (MSA) adopts BTB, entire MSA considered treated.state passes law banning BTB, counties state treated.basic model :\\[\nY_{} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treat}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treat}_i) + \\epsilon_{}\n\\]:\\(Y_{}\\) = employment outcome individual \\(\\) time \\(t\\)\\(\\text{Post}_t\\) = indicator post-treatment period\\(\\text{Treat}_i\\) = indicator treated MSAs\\(\\beta_3\\) = coefficient, capturing effect BTB employment\\(\\epsilon_{}\\) = error termLimitations: different locations adopt BTB different times, model valid due staggered treatment timing.settings different MSAs adopt BTB different times, use staggered approach:\\[\n\\begin{aligned}\nE_{imrt} &= \\alpha + \\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt} \\\\\n&+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + \\delta_m \\times f(t) \\beta_7 + e_{imrt}\n\\end{aligned}\n\\]:\\(\\) = individual, \\(m\\) = MSA, \\(r\\) = region (e.g., Midwest, South), \\(t\\) = year\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(BTB_{imt}\\) = Ban Box policy indicator\\(\\delta_m\\) = MSA fixed effect\\(D_{imt}\\) = individual-level controls\\(\\lambda_{rt}\\) = region--time fixed effect\\(\\delta_m \\times f(t)\\) = linear time trend within MSAFixed Effects Considerations:Including \\(\\lambda_r\\) \\(\\lambda_t\\) separately gives broader fixed effects.Including \\(\\lambda_r\\) \\(\\lambda_t\\) separately gives broader fixed effects.Using \\(\\lambda_{rt}\\) provides granular controls regional time trends.Using \\(\\lambda_{rt}\\) provides granular controls regional time trends.estimate effects Black men specifically, model simplifies :\\[\nE_{imrt} = \\alpha + BTB_{mt} \\beta_1 + \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt}\n\\]check pre-trends dynamic effects, estimate:\\[\n\\begin{aligned}\nE_{imrt} &= \\alpha + BTB_{m (t - 3)} \\theta_1 + BTB_{m (t - 2)} \\theta_2 + BTB_{m (t - 1)} \\theta_3 \\\\\n&+ BTB_{mt} \\theta_4 + BTB_{m (t + 1)} \\theta_5 + BTB_{m (t + 2)} \\theta_6 + BTB_{m (t + 3)} \\theta_7 \\\\\n&+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{r} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt}\n\\end{aligned}\n\\]Key points:Leave \\(BTB_{m (t - 1)} \\theta_3\\) reference category (avoid perfect collinearity).\\(\\theta_2\\) significantly different \\(\\theta_3\\), suggests pre-trend issues, indicate anticipatory effects BTB implementation.Substantively, Shoag Veuger (2021) show Ban--box policies increased employment high-crime neighborhoods 4%, especially public sector low-wage jobs. first nationwide evidence laws improve job access areas many ex-offenders.","code":""},{"path":"sec-difference-in-differences.html","id":"example-minimum-wage-and-employment","chapter":"30 Difference-in-Differences","heading":"30.4.2 Example: Minimum Wage and Employment","text":"Card Krueger (1993) famously studied effect increase minimum wage employment, challenging traditional economic view higher wages reduce employment.Philipp Leppert provides R-based replication.Original datasets available David Card’s website.SettingTreatment group: New Jersey (NJ), increased minimum wage.Control group: Pennsylvania (PA), change minimum wage.Outcome variable: Employment levels fast-food restaurants.study used Difference--Differences approach estimate impact::\\(- B\\) captures treatment effect plus general time trends.\\(C - D\\) captures general time trends.\\((- B) - (C - D)\\) isolates causal effect minimum wage increase.estimator valid, following conditions must hold:Parallel Trends Assumption\nemployment trends NJ PA absence policy change.\nPre-treatment employment trends similar two states.\nemployment trends NJ PA absence policy change.Pre-treatment employment trends similar two states.“Switchers”\npolicy must induce restaurants switch locations NJ PA (e.g., restaurant relocating across border).\npolicy must induce restaurants switch locations NJ PA (e.g., restaurant relocating across border).PA Valid Counterfactual\nPA represents NJ looked like changed minimum wage.\nstudy focuses bordering counties increase comparability.\nPA represents NJ looked like changed minimum wage.study focuses bordering counties increase comparability.main regression specification :\\[\nY_{jt} = \\beta_0 + NJ_j \\beta_1 + POST_t \\beta_2 + (NJ_j \\times POST_t)\\beta_3+ X_{jt}\\beta_4 + \\epsilon_{jt}\n\\]:\\(Y_{jt}\\) = Employment restaurant \\(j\\) time \\(t\\)\\(NJ_j\\) = 1 restaurant NJ, 0 PA\\(POST_t\\) = 1 post-policy period, 0 pre-policy\\((NJ_j \\times POST_t)\\) = interaction term, capturing causal effect NJ’s minimum wage increase\\(X_{jt}\\) = Additional controls (optional)\\(\\epsilon_{jt}\\) = Error termNotes Model Specification\\(\\beta_3\\) (coefficient) key parameter interest, representing causal impact policy.\\(\\beta_3\\) (coefficient) key parameter interest, representing causal impact policy.\\(\\beta_4\\) (controls \\(X_{jt}\\)) necessary unbiasedness improves efficiency.\\(\\beta_4\\) (controls \\(X_{jt}\\)) necessary unbiasedness improves efficiency.difference pre-period (\\(\\Delta Y_{jt} = Y_{j,Post} - Y_{j,Pre}\\)), can simplify model:\n\\[\n\\Delta Y_{jt} = \\alpha + NJ_j \\beta_1 + \\epsilon_{jt}\n\\]\n, longer need \\(\\beta_2\\) post-treatment period.difference pre-period (\\(\\Delta Y_{jt} = Y_{j,Post} - Y_{j,Pre}\\)), can simplify model:\\[\n\\Delta Y_{jt} = \\alpha + NJ_j \\beta_1 + \\epsilon_{jt}\n\\], longer need \\(\\beta_2\\) post-treatment period.alternative specification uses high-wage NJ restaurants control group, arguing affected minimum wage increase. However:approach eliminates cross-state differences, butIt may harder interpret causality, control group entirely untreated.common misconception treatment control groups must baseline levels dependent variable (e.g., employment levels). However:requires parallel trends, meaning slopes employment changes pre-treatment.pre-treatment trends diverge, threatens validity.post-treatment trends converge, may suggest policy effects rather pre-trend violations.Parallel Trends Necessary Sufficient Condition?sufficient: Even pre-trends parallel, confounders affect results.necessary: Parallel trends may emerge treatment, depending behavioral responses.Thus, prove valid—can present evidence supports assumptions.","code":""},{"path":"sec-difference-in-differences.html","id":"example-the-effects-of-grade-policies-on-major-choice","chapter":"30 Difference-in-Differences","heading":"30.4.3 Example: The Effects of Grade Policies on Major Choice","text":"Butcher, McEwan, Weerapana (2014) investigate grading policies influence students’ major choices. central theory grading standards vary discipline, affects students’ decisions.highest-achieving students often major hard sciences?Grading Practices Differ Across Majors\nSTEM fields, grading often stricter, meaning professors less likely give students benefit doubt.\ncontrast, softer disciplines (e.g., humanities) may lenient grading, making students’ experiences pleasant.\nSTEM fields, grading often stricter, meaning professors less likely give students benefit doubt.contrast, softer disciplines (e.g., humanities) may lenient grading, making students’ experiences pleasant.Labor Market Incentives\nDegrees lower market value (e.g., humanities) might compensate offering pleasant academic experience.\nSTEM degrees tend rigorous provide higher job market returns.\nDegrees lower market value (e.g., humanities) might compensate offering pleasant academic experience.STEM degrees tend rigorous provide higher job market returns.examine grades influence major selection, study first estimates OLS model:\\[\nE_{ij} = \\beta_0 + X_i \\beta_1 + G_j \\beta_2 + \\epsilon_{ij}\n\\]:\\(E_{ij}\\) = Indicator whether student \\(\\) chooses major \\(j\\).\\(X_i\\) = Student-level attributes (e.g., SAT scores, demographics).\\(G_j\\) = Average grade major \\(j\\).\\(\\beta_2\\) = Key coefficient, capturing grading standards influence major choice.Potential Biases \\(\\hat{\\beta}_2\\):Negative Bias:\nDepartments lower enrollment rates may offer higher grades attract students.\nendogenous response leads downward bias OLS estimate.\nDepartments lower enrollment rates may offer higher grades attract students.endogenous response leads downward bias OLS estimate.Positive Bias:\nSTEM majors attract best students, grades naturally higher ability controlled.\nability fully accounted , \\(\\hat{\\beta}_2\\) may upward biased.\nSTEM majors attract best students, grades naturally higher ability controlled.ability fully accounted , \\(\\hat{\\beta}_2\\) may upward biased.address potential endogeneity OLS, study uses difference--differences approach:\\[\nY_{idt} = \\beta_0 + POST_t \\beta_1 + Treat_d \\beta_2 + (POST_t \\times Treat_d)\\beta_3 + X_{idt} + \\epsilon_{idt}\n\\]:\\(Y_{idt}\\) = Average grade department \\(d\\) time \\(t\\) student \\(\\).\\(POST_t\\) = 1 post-policy period, 0 otherwise.\\(Treat_d\\) = 1 department treated (.e., grade policy change), 0 otherwise.\\((POST_t \\times Treat_d)\\) = interaction term, capturing causal effect grade policy changes major choice.\\(X_{idt}\\) = Additional student controls.Difference--Differences TableThe average pre-period outcome control group given \\(\\beta_0\\).key coefficient interest \\(\\beta_3\\), captures difference post-treatment effect treated control groups.flexible specification includes fixed effects:\\[\nY_{idt} = \\alpha_0 + (POST_t \\times Treat_d) \\alpha_1 + \\theta_d + \\delta_t + X_{idt} + u_{idt}\n\\]:\\(\\theta_d\\) = Department fixed effects (absorbing \\(Treat_d\\)).\\(\\delta_t\\) = Time fixed effects (absorbing \\(POST_t\\)).\\(\\alpha_1\\) = Effect policy change (equivalent \\(\\beta_3\\) simpler model).Use Fixed Effects?flexible specification:\nInstead assuming uniform treatment effect across groups, model allows department-specific differences (\\(\\theta_d\\)) time-specific shocks (\\(\\delta_t\\)).\nInstead assuming uniform treatment effect across groups, model allows department-specific differences (\\(\\theta_d\\)) time-specific shocks (\\(\\delta_t\\)).Higher degrees freedom:\nFixed effects absorb variation otherwise attributed \\(POST_t\\) \\(Treat_d\\), making estimation efficient.\nFixed effects absorb variation otherwise attributed \\(POST_t\\) \\(Treat_d\\), making estimation efficient.Interpretation ResultsIf \\(\\alpha_1 > 0\\), policy increased grades treated departments.\\(\\alpha_1 < 0\\), policy decreased grades treated departments.","code":""},{"path":"sec-difference-in-differences.html","id":"one-difference","chapter":"30 Difference-in-Differences","heading":"30.5 One Difference","text":"regression formula follows Liaukonytė, Tuchman, Zhu (2023):\\[\ny_{ut} = \\beta \\text{Post}_t + \\gamma_u + \\gamma_w(t) + \\gamma_l + \\gamma_g(u)p(t) + \\epsilon_{ut}\n\\]\\(y_{ut}\\): Outcome interest unit u time t.\\(\\text{Post}_t\\): Dummy variable representing specific post-event period.\\(\\beta\\): Coefficient measuring average change outcome event relative pre-period.\\(\\gamma_u\\): Fixed effects unit.\\(\\gamma_w(t)\\): Time-specific fixed effects account periodic variations.\\(\\gamma_l\\): Dummy variable specific significant period (e.g., major event change).\\(\\gamma_g(u)p(t)\\): Group x period fixed effects flexible trends may vary across different categories (e.g., geographical regions) periods.\\(\\epsilon_{ut}\\): Error term.model can used analyze impact event outcome interest controlling various fixed effects time-specific variations, using units pre-treatment controls.","code":""},{"path":"sec-difference-in-differences.html","id":"sec-two-way-fixed-effects","chapter":"30 Difference-in-Differences","heading":"30.6 Two-Way Fixed Effects","text":"generalization Difference--Differences model two-way fixed effects (TWFE) model, accounts multiple groups multiple time periods including unit time fixed effects. practice, TWFE frequently used estimate causal effects panel data settings. However, design-based, non-parametric causal estimator (Imai Kim 2021), can suffer severe biases treatment effect heterogeneous across units time.applying TWFE datasets multiple treatment groups staggered treatment timing, estimated causal coefficient weighted average possible two-group, two-period comparisons. Crucially, weights can negative (Goodman-Bacon 2021), leads potential biases. weighting scheme depends :Group sizesVariation treatment timingPlacement middle panel (units middle tend get highest weight)","code":""},{"path":"sec-difference-in-differences.html","id":"canonical-twfe-model","chapter":"30 Difference-in-Differences","heading":"30.6.1 Canonical TWFE Model","text":"canonical TWFE model typically written :\\[\nY_{} = \\alpha_i + \\lambda_t + \\tau W_{} + \\beta X_{} + \\epsilon_{},\n\\]:\\(Y_{}\\) = Outcome unit \\(\\) time \\(t\\)\\(Y_{}\\) = Outcome unit \\(\\) time \\(t\\)\\(\\alpha_i\\) = Unit fixed effect\\(\\alpha_i\\) = Unit fixed effect\\(\\lambda_t\\) = Time fixed effect\\(\\lambda_t\\) = Time fixed effect\\(\\tau\\) = Causal effect treatment\\(\\tau\\) = Causal effect treatment\\(W_{}\\) = Treatment indicator (\\(1\\) treated, \\(0\\) otherwise)\\(W_{}\\) = Treatment indicator (\\(1\\) treated, \\(0\\) otherwise)\\(X_{}\\) = Covariates\\(X_{}\\) = Covariates\\(\\epsilon_{}\\) = Error term\\(\\epsilon_{}\\) = Error termAn illustrative TWFE event-study model (Stevenson Wolfers 2006):\\[ \\begin{aligned} Y_{} &= \\sum_{k} \\beta_{k} \\cdot Treatment_{}^{k} \\;+\\; \\eta_{} \\;+\\; \\lambda_{t} \\;+\\; Controls_{} \\;+\\; \\epsilon_{}, \\end{aligned} \\]:\\(Treatment_{}^k\\): Indicator whether unit \\(\\) \\(k\\)-th year relative treatment time \\(t\\).\\(Treatment_{}^k\\): Indicator whether unit \\(\\) \\(k\\)-th year relative treatment time \\(t\\).\\(\\eta_i\\): Unit fixed effects, controlling time-invariant unobserved heterogeneity.\\(\\eta_i\\): Unit fixed effects, controlling time-invariant unobserved heterogeneity.\\(\\lambda_t\\): Time fixed effects, capturing overall macro shocks.\\(\\lambda_t\\): Time fixed effects, capturing overall macro shocks.Standard Errors: Typically clustered group cohort level.Standard Errors: Typically clustered group cohort level.Usually, researchers drop period immediately treatment (\\(k=-1\\)) avoid collinearity. However, dropping another period inappropriately can shift bias estimates.two time periods \\((T=2)\\), TWFE simplifies traditional model. homogeneous treatment effects parallel trends assumption holds, \\(\\hat{\\tau}_{OLS}\\) unbiased. Specifically, model assumes (Imai Kim 2021):Homogeneous treatment effects across units time periods, meaning:\ndynamic treatment effects (.e., treatment effects evolve time).\ntreatment effect constant across units (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021).\ndynamic treatment effects (.e., treatment effects evolve time).treatment effect constant across units (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021).Parallel trends assumptionLinear additive effects valid (Imai Kim 2021).However, practice, treatment effects often heterogeneous. effects vary cohort time, standard TWFE estimates can biased—particularly staggered adoption dynamic treatment effects (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021). Hence, use TWFE, actually argue effects homogeneous justify TWFE use:Assess treatment heterogeneity: heterogeneity exists, TWFE may produce biased estimates. Researchers :\nPlot treatment timing across units.\nDecompose treatment effect using Goodman-Bacon decomposition identify negative weights.\nCheck proportion never-treated observations: 80% sample never treated, TWFE bias negligible.\nBeware bias worsening long-run effects.\nPlot treatment timing across units.Decompose treatment effect using Goodman-Bacon decomposition identify negative weights.Check proportion never-treated observations: 80% sample never treated, TWFE bias negligible.Beware bias worsening long-run effects.Dropping relative time periods:\nunits eventually receive treatment, two relative time periods must dropped avoid multicollinearity.\nsoftware packages drop periods randomly; post-treatment period dropped, bias may result.\nstandard approach drop periods -1 -2.\nunits eventually receive treatment, two relative time periods must dropped avoid multicollinearity.software packages drop periods randomly; post-treatment period dropped, bias may result.standard approach drop periods -1 -2.Sources treatment heterogeneity:\nDelayed treatment effects: impact treatment may take time manifest.\nEvolving effects: Treatment effects can increase change time (e.g., phase-effects).\nDelayed treatment effects: impact treatment may take time manifest.Evolving effects: Treatment effects can increase change time (e.g., phase-effects).TWFE compares different types treatment/control groups:Valid comparisons:\nNewly treated units vs. control units\nNewly treated units vs. -yet treated units\nNewly treated units vs. control unitsNewly treated units vs. -yet treated unitsProblematic comparisons:\nNewly treated units vs. already treated units (since already treated units represent correct counterfactual).\nStrict exogeneity violations:\nPresence time-varying confounders\nFeedback past outcomes treatment (Imai Kim 2019)\n\nFunctional form restrictions:\nAssumes treatment effect homogeneity.\ncarryover effects anticipation effects (Imai Kim 2019).\n\nNewly treated units vs. already treated units (since already treated units represent correct counterfactual).Strict exogeneity violations:\nPresence time-varying confounders\nFeedback past outcomes treatment (Imai Kim 2019)\nPresence time-varying confoundersFeedback past outcomes treatment (Imai Kim 2019)Functional form restrictions:\nAssumes treatment effect homogeneity.\ncarryover effects anticipation effects (Imai Kim 2019).\nAssumes treatment effect homogeneity.carryover effects anticipation effects (Imai Kim 2019).","code":""},{"path":"sec-difference-in-differences.html","id":"limitations-of-twfe","chapter":"30 Difference-in-Differences","heading":"30.6.2 Limitations of TWFE","text":"TWFE valid strong assumptions treatment effect vary across units time. reality, almost always see form treatment heterogeneity:dynamic treatment effects: model requires treatment effect evolve time.unit-level differences: treatment effect must constant across units.Linear additive effects: TWFE assumes underlying data-generating process captured additive fixed effects plus constant treatment effect (Imai Kim 2021).assumptions violated, TWFE can produce biased estimates. Specifically:Negative Weights & Biased Estimates: multiple groups staggered timing, TWFE estimate becomes complicated average “two-group, two-period” comparisons, can receive negative weights (Goodman-Bacon 2021).Potential Bias Dropping Relative Time Periods: units eventually get treated, software often drops reference period (periods) avoid multicollinearity. dropped period post-treatment, bias can worsen. Researchers often drop relative time \\(-1\\) \\(-2\\).Delayed Evolving Treatment Effects: effect treatment takes time manifest changes time, TWFE’s single coefficient \\(\\tau\\) can misleading.two time periods exist, TWFE collapses back traditional model, making problems far less severe. soon one moves beyond single treatment period variation treatment timing, issues become critical.Several authors (L. Sun Abraham 2021; Callaway Sant’Anna 2021; Goodman-Bacon 2021) raised concerns TWFE regressions staggered adoption:Mixes Cohorts: May unintentionally compare newly treated units already treated units, conflating post-treatment behavior early adopters pre-treatment trends later adopters.Negative Weights: group comparisons receive negative weights, can reverse sign overall estimate.Pre-Treatment Leads: Leads may appear non-zero earlier-treated groups remain sample later adopters still untreated.Long-Run Effects: Heterogeneity lagged (long-run) effects can exacerbate bias.fields like finance accounting, newer estimators often reveal null much smaller effects standard TWFE bias properly accounted (Baker, Larcker, Wang 2022).","code":""},{"path":"sec-difference-in-differences.html","id":"diagnosing-and-addressing-bias-in-twfe","chapter":"30 Difference-in-Differences","heading":"30.6.3 Diagnosing and Addressing Bias in TWFE","text":"Researchers can identify mitigate biases arising heterogeneous treatment effects diagnostic checks alternative estimators:Goodman-Bacon DecompositionPurpose: Decomposes TWFE estimate sum two-group, two-period comparisons.Insight: Reveals comparisons negative weights much comparison contributes overall estimate (Goodman-Bacon 2021).Implementation: Identify subgroups treatment timing, examine group–time pair see contributes aggregate TWFE coefficient.Plotting Treatment TimingVisual Inspection: Always plot distribution treatment timing across units.High Risk Bias: treatment staggered many units differ adoption times, standard TWFE often biased.Assessing Treatment Heterogeneity DirectlyCheck Variation Effects: theoretical empirical reason believe treatment effects differ subgroup time, TWFE might appropriate.Size Never-Treated Sample: 80% sample never treated, potential bias TWFE smaller. However, large shares treated units varied adoption times raise red flags.Long-Run Effects: Bias can worsen treatment effect accumulates changes time.","code":""},{"path":"sec-difference-in-differences.html","id":"sec-goodman-bacon-decomposition","chapter":"30 Difference-in-Differences","heading":"30.6.3.1 Goodman-Bacon Decomposition","text":"Goodman-Bacon decomposition (Goodman-Bacon 2021) powerful diagnostic tool understanding TWFE estimator settings staggered treatment adoption. approach clarifies TWFE estimate weighted average many 2×2 difference--differences comparisons groups treated different times (never treated).Key TakeawaysA pairwise estimate (\\(\\tau\\)) receives weight :\ntreatment happens closer midpoint observation window.\ncomparison involves observations (e.g., units years).\ntreatment happens closer midpoint observation window.comparison involves observations (e.g., units years).Comparisons early-treated later-treated groups can produce negative weights, potentially biasing aggregate TWFE estimate.illustrate decomposition using castle dataset bacondecomp package:Running Goodman-Bacon DecompositionComparing TWFE EstimateInterpretation: TWFE estimate (approx. 0.08) equals weighted average Bacon decomposition estimates, confirming decomposition’s validity.Visualizing DecompositionInsight: plot shows contribution 2×2 comparison, highlighting estimates large weights dominate overall TWFE coefficient.Interpretation Practical ImplicationsPurpose: Decomposes TWFE estimate sum two-group, two-period comparisons.Insight: Reveals much comparison contributes overall estimate whether negative misleading effects.Implementation:\nIdentify subgroups treatment timing.\nCompute 2×2 comparison (early vs. late, late vs. never, etc.).\nEvaluate contribute final TWFE estimate.\nIdentify subgroups treatment timing.Compute 2×2 comparison (early vs. late, late vs. never, etc.).Evaluate contribute final TWFE estimate.time-varying covariates included allow identification within treatment timing groups, certain problematic comparisons (like “early vs. late”) may longer influence TWFE estimator directly. scenarios may collapse simpler within-group estimates, improving identification.Summary Table: Goodman-Bacon Comparison Types","code":"\nlibrary(bacondecomp)\nlibrary(tidyverse)\n\n# Load and inspect the castle dataset\ncastle <- bacondecomp::castle %>% \n  dplyr::select(l_homicide, post, state, year)\nhead(castle)\n#>   l_homicide post   state year\n#> 1   2.027356    0 Alabama 2000\n#> 2   2.164867    0 Alabama 2001\n#> 3   1.936334    0 Alabama 2002\n#> 4   1.919567    0 Alabama 2003\n#> 5   1.749841    0 Alabama 2004\n#> 6   2.130440    0 Alabama 2005\n# Apply Goodman-Bacon decomposition\ndf_bacon <- bacon(\n  formula = l_homicide ~ post,\n  data = castle,\n  id_var = \"state\",\n  time_var = \"year\"\n)\n#>                       type  weight  avg_est\n#> 1 Earlier vs Later Treated 0.05976 -0.00554\n#> 2 Later vs Earlier Treated 0.03190  0.07032\n#> 3     Treated vs Untreated 0.90834  0.08796\n\n# Display weighted average of the decomposition\nweighted_avg <- sum(df_bacon$estimate * df_bacon$weight)\nweighted_avg\n#> [1] 0.08181162\nlibrary(broom)\n\n# Fit a TWFE model\nfit_tw <- lm(l_homicide ~ post + factor(state) + factor(year), data = castle)\ntidy(fit_tw)\n#> # A tibble: 61 × 5\n#>    term                     estimate std.error statistic   p.value\n#>    <chr>                       <dbl>     <dbl>     <dbl>     <dbl>\n#>  1 (Intercept)                1.95      0.0624    31.2   2.84e-118\n#>  2 post                       0.0818    0.0317     2.58  1.02e-  2\n#>  3 factor(state)Alaska       -0.373     0.0797    -4.68  3.77e-  6\n#>  4 factor(state)Arizona       0.0158    0.0797     0.198 8.43e-  1\n#>  5 factor(state)Arkansas     -0.118     0.0810    -1.46  1.44e-  1\n#>  6 factor(state)California   -0.108     0.0810    -1.34  1.82e-  1\n#>  7 factor(state)Colorado     -0.696     0.0810    -8.59  1.14e- 16\n#>  8 factor(state)Connecticut  -0.785     0.0810    -9.68  2.08e- 20\n#>  9 factor(state)Delaware     -0.547     0.0810    -6.75  4.18e- 11\n#> 10 factor(state)Florida      -0.251     0.0798    -3.14  1.76e-  3\n#> # ℹ 51 more rows\nlibrary(ggplot2)\n\nggplot(df_bacon) +\n  aes(\n    x = weight,\n    y = estimate,\n    color = type\n  ) +\n  geom_point() +\n  labs(\n    x = \"Weight\",\n    y = \"Estimate\",\n    color = \"Comparison Type\"\n  ) +\n  causalverse::ama_theme()"},{"path":"sec-difference-in-differences.html","id":"remedies-for-twfes-shortcomings","chapter":"30 Difference-in-Differences","heading":"30.6.4 Remedies for TWFE’s Shortcomings","text":"section outlines alternative estimators design-based approaches explicitly handle heterogeneous treatment effects, staggered adoption (Baker, Larcker, Wang 2022), dynamic treatment effects better standard TWFE (e.g., Modern Estimators Staggered Adoption).Group-Time Average Treatment EffectsCallaway Sant’Anna (2021) propose two-step approach:Group-time treatment effects: time period, estimate effect cohort first received treatment period (compared never-treated group).Aggregate: Use bootstrap procedure account autocorrelation clustering, aggregate across groups.Advantages: Allows heterogeneous treatment effects across groups time; compares treated groups never-treated units (well-chosen controls).Implementation: package R.Event-Study Design Cohort-Specific EstimatesL. Sun Abraham (2021) build Callaway Sant’Anna (2021) handle event-study settings:Lags Leads: Capture dynamic treatment effects including time lags leads relative event (treatment).Cohort-Specific Estimates: Estimate separate paths outcomes cohort, controlling cohorts carefully.Interaction-Weighted Estimator: Adjusts differences treatment began.Implementation: fixest package R.Panel Match Estimator --Treatment ConditionsImai Kim (2021) develop methods allowing units switch treatment:Matching create weighted version TWFE, addressing bias heterogeneous effects.Implementation: wfe PanelMatch R packages.Two-Stage Difference--Differences (DiD2S)Gardner (2022) propose two-stage :Idea: Partial fixed effects first, perform second-stage regression focuses within-group/time variation.Strength: Handles heterogeneous treatment effects well, especially never-treated units present.Implementation: did2s R package.Switching EstimatorIf study never-treated units, Clément De Chaisemartin d’Haultfoeuille (2020) suggest switching estimator recover average treatment effect.Caveat: approach still fails detect heterogeneity treatment effects vary exposure length (L. Sun Shapiro 2022).Matrix Completion EstimatorReshaped Inverse Probability Weighting–TWFE EstimatorDesign-Based Approaches: Arkhangelsky et al. (2024) offer refinements incorporate inverse probability weighting.Goal: Improve balance reduce bias non-random treatment timing.Stacked (Simpler Biased)\nBuild stacked datasets treatment cohort, running separate regressions “event window.”\napproach simpler can still carry biases underlying assumptions violated (Gormley Matsa 2011; Cengiz et al. 2019; Deshpande Li 2019).\nBuild stacked datasets treatment cohort, running separate regressions “event window.”approach simpler can still carry biases underlying assumptions violated (Gormley Matsa 2011; Cengiz et al. 2019; Deshpande Li 2019).Doubly Robust Difference--Differences Estimators (DR-) (Sant’Anna Zhao 2020)\nDR-estimators combine outcome regression propensity score weighting identify treatment effects, remaining consistent either model correctly specified.\nachieve local efficiency joint correctness can applied panel repeated cross-section data.\nDR-estimators combine outcome regression propensity score weighting identify treatment effects, remaining consistent either model correctly specified.achieve local efficiency joint correctness can applied panel repeated cross-section data.Nonlinear Difference--Differences","code":""},{"path":"sec-difference-in-differences.html","id":"best-practices-and-recommendations","chapter":"30 Difference-in-Differences","heading":"30.6.5 Best Practices and Recommendations","text":"practical guidelines deciding use TWFE diagnose address potential bias.TWFE Appropriate?\nSingle Treatment Period: TWFE works well one treatment period treated units (variation timing).\nHomogeneous Effects: strong theoretical empirical reasons suggest constant treatment effects across cohorts time, TWFE remains reasonable choice.\nSingle Treatment Period: TWFE works well one treatment period treated units (variation timing).Homogeneous Effects: strong theoretical empirical reasons suggest constant treatment effects across cohorts time, TWFE remains reasonable choice.Diagnosing Addressing Bias Staggered Adoption\nPlot Treatment Timing: Examine distribution treatment timing across units. treatment adoption highly staggered, TWFE likely produce biased estimates.\nDecomposition Methods: Use Goodman-Bacon Decomposition (Goodman-Bacon 2021) see TWFE pools comparisons (whether negative weights emerge). decomposition infeasible (e.g., unbalanced panels), share never-treated units can indicate potential bias severity.\nDecomposes TWFE estimate two-group, two-period comparisons.\nIdentifies comparisons receive negative weights, can lead biased estimates.\nHelps determine influence specific groups overall estimate.\n\nDiscuss Heterogeneity: Explicitly state likelihood treatment effect heterogeneity; incorporate research design.\nPlot Treatment Timing: Examine distribution treatment timing across units. treatment adoption highly staggered, TWFE likely produce biased estimates.Decomposition Methods: Use Goodman-Bacon Decomposition (Goodman-Bacon 2021) see TWFE pools comparisons (whether negative weights emerge). decomposition infeasible (e.g., unbalanced panels), share never-treated units can indicate potential bias severity.\nDecomposes TWFE estimate two-group, two-period comparisons.\nIdentifies comparisons receive negative weights, can lead biased estimates.\nHelps determine influence specific groups overall estimate.\nDecomposes TWFE estimate two-group, two-period comparisons.Identifies comparisons receive negative weights, can lead biased estimates.Helps determine influence specific groups overall estimate.Discuss Heterogeneity: Explicitly state likelihood treatment effect heterogeneity; incorporate research design.Event-Study Specifications within TWFE\nAvoid Arbitrary Binning: collapse multiple time periods single bin unless can justify homogeneous effects within bin.\nFull Relative-Time Indicators: Include flexible event-time indicators, carefully choosing reference period (commonly \\(-1\\), year treatment). Specifically, Include fully flexible relative time indicators, justify reference period (usually \\(l = -1\\) period just prior treatment).\nBeware Multicollinearity: Including leads lags can cause multicollinearity artificially produce significant “pre-trends.”\nDrop Right Periods: units eventually get treated, dropping post-treatment periods accidentally can bias results.\nAvoid Arbitrary Binning: collapse multiple time periods single bin unless can justify homogeneous effects within bin.Full Relative-Time Indicators: Include flexible event-time indicators, carefully choosing reference period (commonly \\(-1\\), year treatment). Specifically, Include fully flexible relative time indicators, justify reference period (usually \\(l = -1\\) period just prior treatment).Beware Multicollinearity: Including leads lags can cause multicollinearity artificially produce significant “pre-trends.”Drop Right Periods: units eventually get treated, dropping post-treatment periods accidentally can bias results.Consider Alternative Estimators","code":""},{"path":"sec-difference-in-differences.html","id":"sec-multiple-periods-and-variation-in-treatment-timing","chapter":"30 Difference-in-Differences","heading":"30.7 Multiple Periods and Variation in Treatment Timing","text":"TWFE extended beyond simple setup multiple periods staggered adoption (treatment occurs different times different units). designs common applied economics, public policy, longitudinal research. However, standard TWFE regressions can biased contexts treatment effects heterogeneous across groups time.","code":""},{"path":"sec-difference-in-differences.html","id":"sec-staggered-difference-in-differences","chapter":"30 Difference-in-Differences","heading":"30.7.1 Staggered Difference-in-Differences","text":"staggered treatment adoption (also called event-study dynamic ):Different units adopt treatment different time periods.Standard TWFE often produces biased estimates “pools” treated units (regardless started treatment) together, implicitly comparing newly treated units already treated ones.Treatments occurred earlier may contaminate counterfactual later adopters model properly handle dynamic heterogeneous effects (Wing et al. 2024; Baker, Larcker, Wang 2022).applied guidance, see (Wing et al. 2024) recommendations (Baker, Larcker, Wang 2022).Researchers aware standard TWFE can mix treatment effects early adopters (long-exposed) later adopters (newly exposed), potentially assigning negative weights particular group comparisons (Goodman-Bacon 2021).using staggered adoption, following assumptions critical:Rollout Exogeneity\nTreatment assignment timing uncorrelated potential outcomes.\nEvidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)\nEvidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\n\nRollout Exogeneity\nTreatment assignment timing uncorrelated potential outcomes.Evidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)Evidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .Treatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .Confounding Events\nEnsure policies shocks coincide staggered treatment rollout.Confounding Events\nEnsure policies shocks coincide staggered treatment rollout.Exclusion Restrictions\nAnticipation: Treatment timing affect outcomes prior treatment.\nInvariance History: Treatment duration shouldn’t matter; treated status matters (often violated).\nExclusion RestrictionsNo Anticipation: Treatment timing affect outcomes prior treatment.Invariance History: Treatment duration shouldn’t matter; treated status matters (often violated).Standard Assumptions\nParallel Trends (Conditional Unconditional)\nRandom Sampling\nOverlap (Common Support)\nEffect Additivity\nStandard AssumptionsParallel Trends (Conditional Unconditional)Random SamplingOverlap (Common Support)Effect Additivity","code":""},{"path":"sec-difference-in-differences.html","id":"sec-modern-estimators-for-staggered-adoption","chapter":"30 Difference-in-Differences","heading":"30.8 Modern Estimators for Staggered Adoption","text":"","code":""},{"path":"sec-difference-in-differences.html","id":"sec-group-time-average-treatment-effects-callaway2021difference","chapter":"30 Difference-in-Differences","heading":"30.8.1 Group-Time Average Treatment Effects (Callaway and Sant’Anna 2021)","text":"Notation Recap\\(Y_{}(0)\\): Potential outcome unit \\(\\) time \\(t\\) absence treatment.\\(Y_{}(0)\\): Potential outcome unit \\(\\) time \\(t\\) absence treatment.\\(Y_{}(g)\\): Potential outcome unit \\(\\) time \\(t\\) first treated period \\(g\\).\\(Y_{}(g)\\): Potential outcome unit \\(\\) time \\(t\\) first treated period \\(g\\).\\(Y_{}\\): Observed outcome unit \\(\\) time \\(t\\).\n\\[\nY_{} =\n\\begin{cases}\nY_{}(0), & \\text{unit } \\text{ never treated ( } C_i = 1 \\text{)} \\\\\n1\\{G_i > t\\} Y_{}(0) + 1\\{G_i \\le t\\} Y_{}(G_i), & \\text{otherwise}\n\\end{cases}\n\\]\\(Y_{}\\): Observed outcome unit \\(\\) time \\(t\\).\\[\nY_{} =\n\\begin{cases}\nY_{}(0), & \\text{unit } \\text{ never treated ( } C_i = 1 \\text{)} \\\\\n1\\{G_i > t\\} Y_{}(0) + 1\\{G_i \\le t\\} Y_{}(G_i), & \\text{otherwise}\n\\end{cases}\n\\]\\(G_i\\): Group assignment, .e., time period unit \\(\\) first receives treatment.\\(G_i\\): Group assignment, .e., time period unit \\(\\) first receives treatment.\\(C_i = 1\\): Indicator unit \\(\\) never receives treatment (never-treated group).\\(C_i = 1\\): Indicator unit \\(\\) never receives treatment (never-treated group).\\(D_{} = 1\\{G_i \\le t\\}\\): Indicator unit \\(\\) treated time \\(t\\).\\(D_{} = 1\\{G_i \\le t\\}\\): Indicator unit \\(\\) treated time \\(t\\).AssumptionsThe following assumptions typically imposed identify treatment effects staggered adoption settings.Staggered Treatment Adoption\ntreated, unit remains treated subsequent periods.\nFormally, \\(D_{}\\) non-decreasing \\(t\\).Staggered Treatment Adoption\ntreated, unit remains treated subsequent periods.\nFormally, \\(D_{}\\) non-decreasing \\(t\\).Parallel Trends Assumptions (Conditional Unconditional Covariates)\nTwo common variants:\nParallel trends based never-treated units: \\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | C_i = 1]\n\\] Interpretation:\naverage potential outcome trends treated group (\\(G_i = g\\)) never-treated group, absent treatment.\n\nParallel trends based -yet-treated units: \\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | D_{} = 0, G_i \\ne g]\n\\] Interpretation:\nUnits yet treated time \\(s\\) (\\(D_{} = 0\\)) can serve controls units first treated \\(g\\).\n\nassumptions can also conditional covariates \\(X\\), :\n\\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | X_i, G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | X_i, C_i = 1]\n\\]Parallel Trends Assumptions (Conditional Unconditional Covariates)Two common variants:Parallel trends based never-treated units: \\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | C_i = 1]\n\\] Interpretation:\naverage potential outcome trends treated group (\\(G_i = g\\)) never-treated group, absent treatment.\naverage potential outcome trends treated group (\\(G_i = g\\)) never-treated group, absent treatment.Parallel trends based -yet-treated units: \\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | D_{} = 0, G_i \\ne g]\n\\] Interpretation:\nUnits yet treated time \\(s\\) (\\(D_{} = 0\\)) can serve controls units first treated \\(g\\).\nUnits yet treated time \\(s\\) (\\(D_{} = 0\\)) can serve controls units first treated \\(g\\).assumptions can also conditional covariates \\(X\\), :\\[\n\\mathbb{E}[Y_t(0) - Y_{t-1}(0) | X_i, G_i = g] = \\mathbb{E}[Y_t(0) - Y_{t-1}(0) | X_i, C_i = 1]\n\\]Random Sampling\nUnits sampled independently identically population.Random Sampling\nUnits sampled independently identically population.Irreversibility Treatment\ntreated, units revert untreated status.Irreversibility Treatment\ntreated, units revert untreated status.Overlap (Positivity)\ngroup \\(g\\), propensity receiving treatment \\(g\\) lies strictly within \\((0, 1)\\): \\[\n0 < \\mathbb{P}(G_i = g | X_i) < 1\n\\]Overlap (Positivity)\ngroup \\(g\\), propensity receiving treatment \\(g\\) lies strictly within \\((0, 1)\\): \\[\n0 < \\mathbb{P}(G_i = g | X_i) < 1\n\\]Group-Time ATT, \\(ATT(g, t)\\), measures average treatment effect units first treated period \\(g\\), evaluated time \\(t\\).\\[\nATT(g, t) = \\mathbb{E}[Y_t(g) - Y_t(0) | G_i = g]\n\\]Interpretation:\\(g\\) indexes group first receives treatment.\\(g\\) indexes group first receives treatment.\\(t\\) time period effect evaluated.\\(t\\) time period effect evaluated.\\(ATT(g, t)\\) captures treatment effects evolve time, following adoption time \\(g\\).\\(ATT(g, t)\\) captures treatment effects evolve time, following adoption time \\(g\\).Identification \\(ATT(g, t)\\)Using Never-Treated Units Controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | C_i = 1], \\quad \\forall t \\ge g\n\\]Using Never-Treated Units Controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | C_i = 1], \\quad \\forall t \\ge g\n\\]Using -Yet-Treated Units Controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | D_{} = 0, G_i \\ne g], \\quad \\forall t \\ge g\n\\]Using -Yet-Treated Units Controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | D_{} = 0, G_i \\ne g], \\quad \\forall t \\ge g\n\\]Conditional Parallel Trends (Covariates):\ntreatment assignment depends covariates \\(X_i\\), adjust parallel trends assumption:\nNever-treated controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | X_i, G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | X_i, C_i = 1], \\quad \\forall t \\ge g\n\\]\n-yet-treated controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | X_i, G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | X_i, D_{} = 0, G_i \\ne g], \\quad \\forall t \\ge g\n\\]\nConditional Parallel Trends (Covariates):\ntreatment assignment depends covariates \\(X_i\\), adjust parallel trends assumption:Never-treated controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | X_i, G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | X_i, C_i = 1], \\quad \\forall t \\ge g\n\\]-yet-treated controls: \\[\nATT(g, t) = \\mathbb{E}[Y_t - Y_{g-1} | X_i, G_i = g] - \\mathbb{E}[Y_t - Y_{g-1} | X_i, D_{} = 0, G_i \\ne g], \\quad \\forall t \\ge g\n\\]Aggregating \\(ATT(g, t)\\): Common Parameters InterestAverage Treatment Effect per Group (\\(\\theta_S(g)\\)):\nAverage effect periods treatment group \\(g\\): \\[\n\\theta_S(g) = \\frac{1}{\\tau - g + 1} \\sum_{t = g}^{\\tau} ATT(g, t)\n\\]\n\\(\\tau\\): Last time period panel.\nAverage Treatment Effect per Group (\\(\\theta_S(g)\\)):\nAverage effect periods treatment group \\(g\\): \\[\n\\theta_S(g) = \\frac{1}{\\tau - g + 1} \\sum_{t = g}^{\\tau} ATT(g, t)\n\\]\\(\\tau\\): Last time period panel.Overall Average Treatment Effect Treated (ATT) (\\(\\theta_S^O\\)):\nWeighted average \\(\\theta_S(g)\\) across groups \\(g\\), weighted group size: \\[\n\\theta_S^O = \\sum_{g=2}^{\\tau} \\theta_S(g) \\cdot \\mathbb{P}(G_i = g)\n\\]Overall Average Treatment Effect Treated (ATT) (\\(\\theta_S^O\\)):\nWeighted average \\(\\theta_S(g)\\) across groups \\(g\\), weighted group size: \\[\n\\theta_S^O = \\sum_{g=2}^{\\tau} \\theta_S(g) \\cdot \\mathbb{P}(G_i = g)\n\\]Dynamic Treatment Effects (\\(\\theta_D(e)\\)):\nAverage effect \\(e\\) periods treatment exposure: \\[\n\\theta_D(e) = \\sum_{g=2}^{\\tau} \\mathbb{1}\\{g + e \\le \\tau\\} \\cdot ATT(g, g + e) \\cdot \\mathbb{P}(G_i = g | g + e \\le \\tau)\n\\]Dynamic Treatment Effects (\\(\\theta_D(e)\\)):\nAverage effect \\(e\\) periods treatment exposure: \\[\n\\theta_D(e) = \\sum_{g=2}^{\\tau} \\mathbb{1}\\{g + e \\le \\tau\\} \\cdot ATT(g, g + e) \\cdot \\mathbb{P}(G_i = g | g + e \\le \\tau)\n\\]Calendar Time Treatment Effects (\\(\\theta_C(t)\\)):\nAverage treatment effect time \\(t\\) across groups treated \\(t\\): \\[\n\\theta_C(t) = \\sum_{g=2}^{\\tau} \\mathbb{1}\\{g \\le t\\} \\cdot ATT(g, t) \\cdot \\mathbb{P}(G_i = g | g \\le t)\n\\]Calendar Time Treatment Effects (\\(\\theta_C(t)\\)):\nAverage treatment effect time \\(t\\) across groups treated \\(t\\): \\[\n\\theta_C(t) = \\sum_{g=2}^{\\tau} \\mathbb{1}\\{g \\le t\\} \\cdot ATT(g, t) \\cdot \\mathbb{P}(G_i = g | g \\le t)\n\\]Average Calendar Time Treatment Effect (\\(\\theta_C\\)):\nAverage \\(\\theta_C(t)\\) across post-treatment periods: \\[\n\\theta_C = \\frac{1}{\\tau - 1} \\sum_{t=2}^{\\tau} \\theta_C(t)\n\\]Average Calendar Time Treatment Effect (\\(\\theta_C\\)):\nAverage \\(\\theta_C(t)\\) across post-treatment periods: \\[\n\\theta_C = \\frac{1}{\\tau - 1} \\sum_{t=2}^{\\tau} \\theta_C(t)\n\\]staggered() function offers several estimands, defining different way aggregating group-time average treatment effects single overall treatment effect:Simple: Equally weighted across groups.Simple: Equally weighted across groups.Cohort: Weighted group sizes (.e., treated cohorts).Cohort: Weighted group sizes (.e., treated cohorts).Calendar: Weighted number observations calendar time.Calendar: Weighted number observations calendar time.visualize treatment dynamics around time adoption, event study specification estimates dynamic treatment effects relative time treatment.staggered package also includes direct implementations alternative estimators:staggered_cs() implements Callaway Sant’Anna (2021) estimator.staggered_cs() implements Callaway Sant’Anna (2021) estimator.staggered_sa() implements L. Sun Abraham (2021) estimator, adjusts bias comparisons involving already-treated units.staggered_sa() implements L. Sun Abraham (2021) estimator, adjusts bias comparisons involving already-treated units.assess statistical significance sharp null hypothesis \\(H_0: \\text{TE} = 0\\), staggered package includes option Fisher’s randomization (permutation) test. approach tests whether observed estimate plausibly occur random reallocation treatment timings.test provides non-parametric method inference particularly useful number groups small standard errors unreliable due clustering heteroskedasticity.","code":"\nlibrary(staggered) \nlibrary(fixest)\ndata(\"base_stagg\")\n\n# Simple weighted average ATT\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245\n\n# Cohort weighted ATT (i.e., by treatment cohort size)\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"cohort\"\n)\n#>    estimate        se se_neyman\n#> 1 -2.724242 0.2701093 0.2701745\n\n# Calendar weighted ATT (i.e., by year)\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"calendar\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.5861831 0.1768297 0.1770729\nres <- staggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"eventstudy\", \n    eventTime = -9:8\n)\n\n# Plotting the event study with pointwise confidence intervals\nlibrary(ggplot2)\nlibrary(dplyr)\n\nggplot(\n    res |> mutate(\n        ymin_ptwise = estimate - 1.96 * se,\n        ymax_ptwise = estimate + 1.96 * se\n    ),\n    aes(x = eventTime, y = estimate)\n) +\n    geom_pointrange(aes(ymin = ymin_ptwise, ymax = ymax_ptwise)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    xlab(\"Event Time\") +\n    ylab(\"ATT Estimate\") +\n    ggtitle(\"Event Study: Dynamic Treatment Effects\") +\n    causalverse::ama_theme()\n# Callaway and Sant’Anna estimator\nstaggered_cs(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7994889 0.4484987 0.4486122\n\n# Sun and Abraham estimator\nstaggered_sa(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7551901 0.4407818 0.4409525\n# Fisher Randomization Test\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\",\n    compute_fisher = TRUE,\n    num_fisher_permutations = 100\n)\n#>     estimate        se se_neyman fisher_pval fisher_pval_se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245           0                     0\n#>   num_fisher_permutations\n#> 1                     100"},{"path":"sec-difference-in-differences.html","id":"sec-cohort-average-treatment-effects-sun2021estimating","chapter":"30 Difference-in-Differences","heading":"30.8.2 Cohort Average Treatment Effects (L. Sun and Abraham 2021)","text":"L. Sun Abraham (2021) propose solution TWFE problem staggered adoption settings introducing interaction-weighted estimator dynamic treatment effects. estimator based concept Cohort Average Treatment Effects Treated (CATT), accounts variation treatment timing dynamic treatment responses.Traditional TWFE estimators implicitly assume homogeneous treatment effects often rely treated units serving controls later-treated units. treatment effects vary time across groups, leads contaminated comparisons, especially event-study specifications.L. Sun Abraham (2021) address issue :Estimating cohort-specific treatment effects relative time since treatment.Estimating cohort-specific treatment effects relative time since treatment.Using never-treated units controls, absence, last-treated cohort.Using never-treated units controls, absence, last-treated cohort.","code":""},{"path":"sec-difference-in-differences.html","id":"defining-the-parameter-of-interest-catt","chapter":"30 Difference-in-Differences","heading":"30.8.2.1 Defining the Parameter of Interest: CATT","text":"Let \\(E_i = e\\) denote period unit \\(\\) first receives treatment. cohort-specific average treatment effect treated (CATT) defined : \\[\nCATT_{e, l} = \\mathbb{E}[Y_{, e + l} - Y_{, e + l}^\\infty \\mid E_i = e]\n\\] :\\(l\\) relative period (e.g., \\(l = -1\\) one year treatment, \\(l = 0\\) treatment year).\\(l\\) relative period (e.g., \\(l = -1\\) one year treatment, \\(l = 0\\) treatment year).\\(Y_{, e + l}^\\infty\\) potential outcome without treatment.\\(Y_{, e + l}^\\infty\\) potential outcome without treatment.\\(Y_{, e + l}\\) observed outcome.\\(Y_{, e + l}\\) observed outcome.formulation allows one trace dynamic effect treatment cohort, relative treatment start time.L. Sun Abraham (2021) extend interaction-weighted idea panel settings, originally introduced Gibbons, Suárez Serrato, Urbancic (2018) cross-sectional context.propose regressing outcome :Relative time indicators constructed interacting treatment cohort (\\(E_i\\)) time (\\(t\\)).Relative time indicators constructed interacting treatment cohort (\\(E_i\\)) time (\\(t\\)).Unit time fixed effects.Unit time fixed effects.method explicitly estimates \\(CATT_{e, l}\\) terms, avoiding contaminating influence already-treated units TWFE models often suffer .Relative Period Bin Indicator\\[ D_{}^l = \\mathbb{1}(t - E_i = l) \\]\\(E_i\\): time period unit \\(\\) first receives treatment.\\(l\\): relative time period—many periods passed since treatment began.Static Specification\\[ Y_{} = \\alpha_i + \\lambda_t + \\mu_g \\sum_{l \\ge 0} D_{}^l + \\epsilon_{} \\]\\(\\alpha_i\\): Unit fixed effects.\\(\\lambda_t\\): Time fixed effects.\\(\\mu_g\\): Effect group \\(g\\).Excludes periods prior treatment.Dynamic Specification\\[ Y_{} = \\alpha_i + \\lambda_t + \\sum_{\\substack{l = -K \\\\ l \\neq -1}}^{L} \\mu_l D_{}^l + \\epsilon_{} \\]Includes leads lags treatment indicators \\(D_{}^l\\).Excludes one period (typically \\(l = -1\\)) avoid perfect collinearity.Tests pre-treatment parallel trends rely leads (\\(l < 0\\)).","code":""},{"path":"sec-difference-in-differences.html","id":"identifying-assumptions","chapter":"30 Difference-in-Differences","heading":"30.8.2.2 Identifying Assumptions","text":"Parallel TrendsFor identification, assumed untreated potential outcomes follow parallel trends across cohorts absence treatment: \\[\n\\mathbb{E}[Y_{}^\\infty - Y_{, t-1}^\\infty \\mid E_i = e] = \\text{constant across } e\n\\] allows us use never-treated -yet-treated units valid counterfactuals.Anticipatory EffectsTreatment influence outcomes implemented. : \\[\nCATT_{e, l} = 0 \\quad \\text{} l < 0\n\\] ensures pre-trends driven behavioral changes anticipation treatment.Treatment Effect Homogeneity (Optional)treatment effect consistent across cohorts relative period. adoption cohort path treatment effects. words, trajectory treatment cohort similar.Although L. Sun Abraham (2021) allow treatment effect heterogeneity, settings may assume homogeneous effects within cohorts periods:cohort pattern response time.cohort pattern response time.relaxed design assumed simpler TWFE settings.relaxed design assumed simpler TWFE settings.","code":""},{"path":"sec-difference-in-differences.html","id":"comparison-to-other-designs","chapter":"30 Difference-in-Differences","heading":"30.8.2.3 Comparison to Other Designs","text":"Different designs make distinct assumptions treatment effects vary:Homogeneous across cohortsHeterogeneity timeRestricts one dimensionHeterogeneity either “vary across units time” “vary time across units”.","code":""},{"path":"sec-difference-in-differences.html","id":"sources-of-treatment-effect-heterogeneity","chapter":"30 Difference-in-Differences","heading":"30.8.2.4 Sources of Treatment Effect Heterogeneity","text":"Several forces can generate heterogeneous treatment effects:Calendar Time Effects: Macro events (e.g., recessions, policy changes) affect cohorts differently.Calendar Time Effects: Macro events (e.g., recessions, policy changes) affect cohorts differently.Selection Timing: Units self-select early/late treatment based anticipated effects.Selection Timing: Units self-select early/late treatment based anticipated effects.Composition Differences: Adoption cohorts may differ observed unobserved ways.Composition Differences: Adoption cohorts may differ observed unobserved ways.heterogeneity can bias TWFE estimates, often average effects across incomparable groups.","code":""},{"path":"sec-difference-in-differences.html","id":"technical-issues","chapter":"30 Difference-in-Differences","heading":"30.8.2.5 Technical Issues","text":"using event-study TWFE regression estimate dynamic treatment effects staggered adoption settings, one must exclude certain relative time indicators avoid perfect multicollinearity. arises relative period indicators linearly dependent due presence unit time fixed effects.Specifically, following two terms must addressed:period immediately treatment (\\(l = -1\\)): period typically omitted serves baseline comparison. normalization standard practice event study regressions prior L. Sun Abraham (2021) .period immediately treatment (\\(l = -1\\)): period typically omitted serves baseline comparison. normalization standard practice event study regressions prior L. Sun Abraham (2021) .distant post-treatment period (e.g., \\(l = +5\\) \\(l = +10\\)): L. Sun Abraham (2021) clarified addition baseline period, least one relative time indicator—typically far tail post-treatment distribution—must dropped, binned, trimmed avoid multicollinearity among relative time dummies. issue emerges fixed effects absorb much within-unit within-time variation, reducing effective rank design matrix.distant post-treatment period (e.g., \\(l = +5\\) \\(l = +10\\)): L. Sun Abraham (2021) clarified addition baseline period, least one relative time indicator—typically far tail post-treatment distribution—must dropped, binned, trimmed avoid multicollinearity among relative time dummies. issue emerges fixed effects absorb much within-unit within-time variation, reducing effective rank design matrix.Dropping certain relative periods (especially pre-treatment periods) introduces implicit normalization: estimates included periods now interpreted relative omitted periods. treatment effects present omitted periods—say, due anticipation early effects—contaminate estimates included relative periods.avoid contamination, researchers often assume pre-treatment periods zero treatment effect, .e.,\\[\nCATT_{e, l} = 0 \\quad \\text{} l < 0\n\\]assumption ensures excluded pre-treatment periods form valid counterfactual, estimates \\(l \\geq 0\\) biased due normalization.L. Sun Abraham (2021) resolve issues weighting aggregation using clean weighting scheme avoids contamination excluded periods. method produces weighted average cohort- time-specific treatment effects (\\(CATT_{e, l}\\)), weights :Non-negativeSum oneInterpretable fraction treated units observed \\(l\\) periods treatment, normalized number available periods \\(g\\)interaction-weighted estimator ensures estimated average treatment effect reflects convex combination dynamic treatment effects different cohorts times.way, aggregation logic closely mirrors Callaway Sant’Anna (2021), also construct average treatment effects group-time ATTs using interpretable weights align sampling structure.Use iplot() visualize estimated dynamic treatment effects across relative time:can summarize results using different aggregation options:fwlplot package provides diagnostics much variation explained fixed effects covariates:","code":"\nlibrary(fixest)\ndata(\"base_stagg\")\n\n# Estimate Sun & Abraham interaction-weighted model\nres_sa20 <- feols(\n  y ~ x1 + sunab(year_treated, year) | id + year,\n  data = base_stagg\n)\niplot(res_sa20)\n# Overall average ATT\nsummary(res_sa20, agg = \"att\")\n#> OLS estimation, Dep. Var.: y\n#> Observations: 950\n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>      Estimate Std. Error  t value  Pr(>|t|)    \n#> x1   0.994678   0.018378 54.12293 < 2.2e-16 ***\n#> ATT -1.133749   0.205070 -5.52858 2.882e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n# Aggregation across post-treatment periods (excluding leads)\nsummary(res_sa20, agg = c(\"att\" = \"year::[^-]\"))\n#> OLS estimation, Dep. Var.: y\n#> Observations: 950\n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>                      Estimate Std. Error   t value   Pr(>|t|)    \n#> x1                   0.994678   0.018378 54.122928  < 2.2e-16 ***\n#> year::-9:cohort::10  0.351766   0.359073  0.979649 3.2977e-01    \n#> year::-8:cohort::9   0.033914   0.471437  0.071937 9.4281e-01    \n#> year::-8:cohort::10 -0.191932   0.352896 -0.543876 5.8781e-01    \n#> year::-7:cohort::8  -0.589387   0.736910 -0.799809 4.2584e-01    \n#> year::-7:cohort::9   0.872995   0.493427  1.769249 8.0096e-02 .  \n#> year::-7:cohort::10  0.019512   0.603411  0.032336 9.7427e-01    \n#> year::-6:cohort::7  -0.042147   0.865736 -0.048683 9.6127e-01    \n#> year::-6:cohort::8  -0.657571   0.573257 -1.147078 2.5426e-01    \n#> year::-6:cohort::9   0.877743   0.533331  1.645775 1.0315e-01    \n#> year::-6:cohort::10 -0.403635   0.347412 -1.161832 2.4825e-01    \n#> year::-5:cohort::6  -0.658034   0.913407 -0.720418 4.7306e-01    \n#> year::-5:cohort::7  -0.316974   0.697939 -0.454158 6.5076e-01    \n#> year::-5:cohort::8  -0.238213   0.469744 -0.507113 6.1326e-01    \n#> year::-5:cohort::9   0.301477   0.604201  0.498968 6.1897e-01    \n#> year::-5:cohort::10 -0.564801   0.463214 -1.219308 2.2578e-01    \n#> year::-4:cohort::5  -0.983453   0.634492 -1.549984 1.2451e-01    \n#> year::-4:cohort::6   0.360407   0.858316  0.419900 6.7552e-01    \n#> year::-4:cohort::7  -0.430610   0.661356 -0.651102 5.1657e-01    \n#> year::-4:cohort::8  -0.895195   0.374901 -2.387816 1.8949e-02 *  \n#> year::-4:cohort::9  -0.392478   0.439547 -0.892914 3.7418e-01    \n#> year::-4:cohort::10  0.519001   0.597880  0.868069 3.8757e-01    \n#> year::-3:cohort::4   0.591288   0.680169  0.869324 3.8688e-01    \n#> year::-3:cohort::5  -1.000650   0.971741 -1.029749 3.0577e-01    \n#> year::-3:cohort::6   0.072188   0.652641  0.110609 9.1216e-01    \n#> year::-3:cohort::7  -0.836820   0.804275 -1.040465 3.0079e-01    \n#> year::-3:cohort::8  -0.783148   0.701312 -1.116691 2.6697e-01    \n#> year::-3:cohort::9   0.811285   0.564470  1.437251 1.5397e-01    \n#> year::-3:cohort::10  0.527203   0.320051  1.647250 1.0285e-01    \n#> year::-2:cohort::3   0.036941   0.673771  0.054828 9.5639e-01    \n#> year::-2:cohort::4   0.832250   0.859544  0.968246 3.3541e-01    \n#> year::-2:cohort::5  -1.574086   0.525563 -2.995051 3.5076e-03 ** \n#> year::-2:cohort::6   0.311758   0.832095  0.374666 7.0875e-01    \n#> year::-2:cohort::7  -0.558631   0.871993 -0.640638 5.2332e-01    \n#> year::-2:cohort::8   0.429591   0.305270  1.407250 1.6265e-01    \n#> year::-2:cohort::9   1.201899   0.819186  1.467188 1.4566e-01    \n#> year::-2:cohort::10 -0.002429   0.682087 -0.003562 9.9717e-01    \n#> att                 -1.133749   0.205070 -5.528584 2.8820e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n# Aggregate post-treatment effects from l = 0 to 8\nsummary(res_sa20, agg = c(\"att\" = \"year::[012345678]\")) |> \n  etable(digits = 2)\n#>                         summary(res_..\n#> Dependent Var.:                      y\n#>                                       \n#> x1                      0.99*** (0.02)\n#> year = -9 x cohort = 10    0.35 (0.36)\n#> year = -8 x cohort = 9     0.03 (0.47)\n#> year = -8 x cohort = 10   -0.19 (0.35)\n#> year = -7 x cohort = 8    -0.59 (0.74)\n#> year = -7 x cohort = 9    0.87. (0.49)\n#> year = -7 x cohort = 10    0.02 (0.60)\n#> year = -6 x cohort = 7    -0.04 (0.87)\n#> year = -6 x cohort = 8    -0.66 (0.57)\n#> year = -6 x cohort = 9     0.88 (0.53)\n#> year = -6 x cohort = 10   -0.40 (0.35)\n#> year = -5 x cohort = 6    -0.66 (0.91)\n#> year = -5 x cohort = 7    -0.32 (0.70)\n#> year = -5 x cohort = 8    -0.24 (0.47)\n#> year = -5 x cohort = 9     0.30 (0.60)\n#> year = -5 x cohort = 10   -0.56 (0.46)\n#> year = -4 x cohort = 5    -0.98 (0.63)\n#> year = -4 x cohort = 6     0.36 (0.86)\n#> year = -4 x cohort = 7    -0.43 (0.66)\n#> year = -4 x cohort = 8   -0.90* (0.37)\n#> year = -4 x cohort = 9    -0.39 (0.44)\n#> year = -4 x cohort = 10    0.52 (0.60)\n#> year = -3 x cohort = 4     0.59 (0.68)\n#> year = -3 x cohort = 5     -1.0 (0.97)\n#> year = -3 x cohort = 6     0.07 (0.65)\n#> year = -3 x cohort = 7    -0.84 (0.80)\n#> year = -3 x cohort = 8    -0.78 (0.70)\n#> year = -3 x cohort = 9     0.81 (0.56)\n#> year = -3 x cohort = 10    0.53 (0.32)\n#> year = -2 x cohort = 3     0.04 (0.67)\n#> year = -2 x cohort = 4     0.83 (0.86)\n#> year = -2 x cohort = 5   -1.6** (0.53)\n#> year = -2 x cohort = 6     0.31 (0.83)\n#> year = -2 x cohort = 7    -0.56 (0.87)\n#> year = -2 x cohort = 8     0.43 (0.31)\n#> year = -2 x cohort = 9      1.2 (0.82)\n#> year = -2 x cohort = 10  -0.002 (0.68)\n#> att                     -1.1*** (0.21)\n#> Fixed-Effects:          --------------\n#> id                                 Yes\n#> year                               Yes\n#> _______________________ ______________\n#> S.E.: Clustered                 by: id\n#> Observations                       950\n#> R2                             0.90982\n#> Within R2                      0.87641\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(fwlplot)\n\n# Simple FWL plot\nfwl_plot(y ~ x1, data = base_stagg)\n\n\n# With fixed effects\nfwl_plot(y ~ x1 | id + year,\n         data = base_stagg,\n         n_sample = 100)\n\n# Splitting by treatment status\nfwl_plot(\n    y ~ x1 |\n        id + year,\n    data = base_stagg,\n    n_sample = 100,\n    fsplit = ~ treated\n)"},{"path":"sec-difference-in-differences.html","id":"sec-stacked-difference-in-differences","chapter":"30 Difference-in-Differences","heading":"30.8.3 Stacked Difference-in-Differences","text":"Stacked approach addresses key limitations standard TWFE models staggered adoption designs, particularly treatment effect heterogeneity timing variations. constructing sub-experiments around treatment event, researchers can isolate cleaner comparisons reduce contamination improperly specified control groups.Basic TWFE Specification\\[\nY_{} = \\beta_{FE} D_{} + A_i + B_t + \\epsilon_{}\n\\]\\(Y_{}\\): Outcome unit \\(\\) time \\(t\\).\\(D_{}\\): Treatment indicator (1 treated, 0 otherwise).\\(A_i\\): Unit (group) fixed effects.\\(B_t\\): Time period fixed effects.\\(\\epsilon_{}\\): Idiosyncratic error term.Steps Stacked Procedure","code":""},{"path":"sec-difference-in-differences.html","id":"choose-an-event-window","chapter":"30 Difference-in-Differences","heading":"30.8.3.1 Choose an Event Window","text":"Define:\\(\\kappa_a\\): Number pre-treatment periods include event window (lead periods).\\(\\kappa_b\\): Number post-treatment periods include event window (lag periods).Implication:\nevents sufficient pre- post-treatment periods exist included (.e., excluding events meet criteria).","code":""},{"path":"sec-difference-in-differences.html","id":"enumerate-sub-experiments","chapter":"30 Difference-in-Differences","heading":"30.8.3.2 Enumerate Sub-Experiments","text":"Define:\\(T_1\\): First period panel.\\(T_T\\): Last period panel.\\(\\Omega_A\\): set treatment adoption periods fit within event window:\\[\n\\Omega_A = \\left\\{ A_i \\;\\middle|\\; T_1 + \\kappa_a \\le A_i \\le T_T - \\kappa_b \\right\\}\n\\]\\(A_i\\) represents adoption period unit \\(\\) enough time sides event.Let \\(d = 1, \\dots, D\\) index sub-experiments \\(\\Omega_A\\).\\(\\omega_d\\): event (adoption) date \\(d\\)-th sub-experiment.","code":""},{"path":"sec-difference-in-differences.html","id":"define-inclusion-criteria","chapter":"30 Difference-in-Differences","heading":"30.8.3.3 Define Inclusion Criteria","text":"Valid Treated UnitsIn sub-experiment \\(d\\), treated units adoption date exactly equal \\(\\omega_d\\).unit may treated one sub-experiment avoid duplication.Clean Control UnitsControls units \\(A_i > \\omega_d + \\kappa_b\\), .e.,\nnever treated, \ntreated far future (beyond post-event window).\nnever treated, orThey treated far future (beyond post-event window).control unit can appear multiple sub-experiments, requires correcting standard errors (see ).Valid Time PeriodsOnly observations \\[\n\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\n\\]\nincluded.ensures analysis centered event window.","code":""},{"path":"sec-difference-in-differences.html","id":"specify-estimating-equation","chapter":"30 Difference-in-Differences","heading":"30.8.3.4 Specify Estimating Equation","text":"Basic Specification Stacked Dataset\\[\nY_{itd} = \\beta_0 + \\beta_1 T_{id}  + \\beta_2 P_{td} + \\beta_3 (T_{id} \\times P_{td}) + \\epsilon_{itd}\n\\]:\\(\\): Unit index\\(\\): Unit index\\(t\\): Time index\\(t\\): Time index\\(d\\): Sub-experiment index\\(d\\): Sub-experiment index\\(T_{id}\\): Indicator treated units sub-experiment \\(d\\)\\(T_{id}\\): Indicator treated units sub-experiment \\(d\\)\\(P_{td}\\): Indicator post-treatment periods sub-experiment \\(d\\)\\(P_{td}\\): Indicator post-treatment periods sub-experiment \\(d\\)\\(\\beta_3\\): Captures estimate treatment effect.\\(\\beta_3\\): Captures estimate treatment effect.Equivalent Form Fixed Effects\\[\nY_{itd} = \\beta_3 (T_{id} \\times P_{td}) + \\theta_{id} + \\gamma_{td} + \\epsilon_{itd}\n\\]\\(\\theta_{id}\\): Unit--sub-experiment fixed effect.\\(\\theta_{id}\\): Unit--sub-experiment fixed effect.\\(\\gamma_{td}\\): Time--sub-experiment fixed effect.\\(\\gamma_{td}\\): Time--sub-experiment fixed effect.Note:\\(\\beta_3\\) summarizes average treatment effect across sub-experiments allow dynamic effects time since treatment.","code":""},{"path":"sec-difference-in-differences.html","id":"stacked-event-study-specification","chapter":"30 Difference-in-Differences","heading":"30.8.3.5 Stacked Event Study Specification","text":"Define Time Since Event (\\(YSE_{td}\\)):\\[\nYSE_{td} = t- \\omega_d\n\\]whereMeasures time since event (relative time) sub-experiment \\(d\\).Measures time since event (relative time) sub-experiment \\(d\\).\\(YSE_{td} \\[-\\kappa_a, \\dots, 0, \\dots, \\kappa_b]\\) every sub-experiment.\\(YSE_{td} \\[-\\kappa_a, \\dots, 0, \\dots, \\kappa_b]\\) every sub-experiment.Event-Study Regression (Sub-Experiment Level)\\[\nY_{}^d = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j^d . 1 (YSE_{td} = j) + \\sum_{j = -\\kappa_a}^{\\kappa_b} \\delta_j^d (T_{id} . 1 (YSE_{td} = j)) + \\theta_i^d + \\epsilon_{}^d\n\\]whereSeparate coefficients sub-experiment \\(d\\).Separate coefficients sub-experiment \\(d\\).\\(\\delta_j^d\\): Captures treatment effects relative time \\(j\\) within sub-experiment \\(d\\).\\(\\delta_j^d\\): Captures treatment effects relative time \\(j\\) within sub-experiment \\(d\\).Pooled Stacked Event-Study Regression\\[\nY_{itd} = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j \\cdot \\mathbb{1}(YSE_{td} = j) + \\sum_{j = -\\kappa_a}^{\\kappa_b} \\delta_j \\left( T_{id} \\cdot \\mathbb{1}(YSE_{td} = j) \\right) + \\theta_{id} + \\epsilon_{itd}\n\\]Pooled coefficients \\(\\delta_j\\) reflect average treatment effects event time \\(j\\) across sub-experiments.","code":""},{"path":"sec-difference-in-differences.html","id":"clustering-in-stacked-did","chapter":"30 Difference-in-Differences","heading":"30.8.3.6 Clustering in Stacked DID","text":"Cluster Unit × Sub-Experiment Level (Cengiz et al. 2019): Accounts units appearing multiple times across sub-experiments.Cluster Unit × Sub-Experiment Level (Cengiz et al. 2019): Accounts units appearing multiple times across sub-experiments.Cluster Unit Level (Deshpande Li 2019): Appropriate units uniquely identified appear multiple sub-experiments.Cluster Unit Level (Deshpande Li 2019): Appropriate units uniquely identified appear multiple sub-experiments.","code":"\nlibrary(did)\nlibrary(tidyverse)\nlibrary(fixest)\n\n# Load example data\ndata(base_stagg)\n\n# Get treated cohorts (exclude never-treated units coded as 10000)\ncohorts <- base_stagg %>%\n    filter(year_treated != 10000) %>%\n    distinct(year_treated) %>%\n    pull()\n\n# Function to generate data for each sub-experiment\ngetdata <- function(j, window) {\n    base_stagg %>%\n        filter(\n            year_treated == j |               # treated units in cohort j\n            year_treated > j + window         # controls not treated soon after\n        ) %>%\n        filter(\n            year >= j - window &\n            year <= j + window                # event window bounds\n        ) %>%\n        mutate(df = j)                        # sub-experiment indicator\n}\n\n# Generate the stacked dataset\nstacked_data <- map_df(cohorts, ~ getdata(., window = 5)) %>%\n    mutate(\n        rel_year = if_else(df == year_treated, time_to_treatment, NA_real_)\n    ) %>%\n    fastDummies::dummy_cols(\"rel_year\", ignore_na = TRUE) %>%\n    mutate(across(starts_with(\"rel_year_\"), ~ replace_na(., 0)))\n\n# Estimate fixed effects regression on the stacked data\nstacked_result <- feols(\n    y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` + `rel_year_-2` +\n        rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 +\n        rel_year_4 + rel_year_5 |\n        id ^ df + year ^ df,\n    data = stacked_data\n)\n\n# Extract coefficients and standard errors\nstacked_coeffs <- stacked_result$coefficients\nstacked_se <- stacked_result$se\n\n# Insert zero for the omitted period (usually -1)\nstacked_coeffs <- c(stacked_coeffs[1:4], 0, stacked_coeffs[5:10])\nstacked_se <- c(stacked_se[1:4], 0, stacked_se[5:10])\n# Plotting estimates from three methods: Callaway & Sant'Anna, Sun & Abraham, and Stacked DiD\n\ncs_out <- att_gt(\n    yname = \"y\",\n    data = base_stagg,\n    gname = \"year_treated\",\n    idname = \"id\",\n    # xformla = \"~x1\",\n    tname = \"year\"\n)\ncs <-\n    aggte(\n        cs_out,\n        type = \"dynamic\",\n        min_e = -5,\n        max_e = 5,\n        bstrap = FALSE,\n        cband = FALSE\n    )\n\n\n\nres_sa20 = feols(y ~ sunab(year_treated, year) |\n                     id + year, base_stagg)\nsa = tidy(res_sa20)[5:14, ] %>% pull(estimate)\nsa = c(sa[1:4], 0, sa[5:10])\n\nsa_se = tidy(res_sa20)[6:15, ] %>% pull(std.error)\nsa_se = c(sa_se[1:4], 0, sa_se[5:10])\n\ncompare_df_est = data.frame(\n    period = -5:5,\n    cs = cs$att.egt,\n    sa = sa,\n    stacked = stacked_coeffs\n)\n\ncompare_df_se = data.frame(\n    period = -5:5,\n    cs = cs$se.egt,\n    sa = sa_se,\n    stacked = stacked_se\n)\n\ncompare_df_longer <- compare_df_est %>%\n    pivot_longer(!period, names_to = \"estimator\", values_to = \"est\") %>%\n    full_join(compare_df_se %>%\n                  pivot_longer(!period, names_to = \"estimator\", values_to = \"se\")) %>%\n    mutate(upper = est +  1.96 * se,\n           lower = est - 1.96 * se)\n\nggplot(compare_df_longer) +\n    geom_ribbon(aes(\n        x = period,\n        ymin = lower,\n        ymax = upper,\n        group = estimator\n    ), alpha = 0.2) +\n    geom_line(aes(\n        x = period,\n        y = est,\n        group = estimator,\n        color = estimator\n    ),\n    linewidth = 1.2) +\n    \n    labs(\n        title = \"Comparison of Dynamic Treatment Effects\",\n        x = \"Event Time (Periods since Treatment)\",\n        y = \"Estimated ATT\",\n        color = \"Estimator\"\n    ) + \n    causalverse::ama_theme()"},{"path":"sec-difference-in-differences.html","id":"sec-panel-match-did-estimator-with-in-and-out-treatment-conditions","chapter":"30 Difference-in-Differences","heading":"30.8.4 Panel Match DiD Estimator with In-and-Out Treatment Conditions","text":"noted Imai Kim (2021), TWFE regression model widely used fundamentally relies strong modeling assumptions, particularly linearity additivity. constitute fully nonparametric estimation method may yield biased results model misspecification.","code":""},{"path":"sec-difference-in-differences.html","id":"limitations-of-twfe-1","chapter":"30 Difference-in-Differences","heading":"30.8.4.1 Limitations of TWFE","text":"Researchers often prefer TWFE due ability control unit- time-specific unobserved confounders:\\(\\alpha_i = h(\\mathbf{U}_i)\\) accounts unit-level confounders.\\(\\gamma_t = f(\\mathbf{V}_t)\\) adjusts time-level confounders.functional forms \\(h(\\cdot)\\) \\(f(\\cdot)\\) left unspecified, additivity separability assumed. TWFE based model:\\[\nY_{} = \\alpha_i + \\gamma_t + \\beta X_{} + \\epsilon_{}\n\\]\\(= 1, \\dots, N\\), \\(t = 1, \\dots, T\\). However, formulation requires linear specification treatment effect \\(\\beta\\). Contrary popular belief, model require functional form assumptions validity (Imai Kim 2021, 406; 2019).","code":""},{"path":"sec-difference-in-differences.html","id":"matching-and-the-panel-match-did-estimator","chapter":"30 Difference-in-Differences","heading":"30.8.4.2 Matching and the Panel Match DiD Estimator","text":"mitigate model dependence improve causal inference validity, Imai Kim (2021) propose matching-based framework panel data. method implemented via wfe PanelMatch R packages offers design-based identification relaxed assumptions.setting generalizes staggered adoption, allowing units transition treatment. core idea construct matched control groups share treatment history treated units apply Difference--Differences logic. better synthetic controls (e.g., Xu (2017)) requires less data achieve good performance can adapt contexts units switch treatment status multiple times.Key Properties PM-(Imai, Kim, Wang 2021)Designed multiple treatment switches time.Addresses issues carryover, reversal, attenuation bias.Allows estimation short-term long-term causal effects, accounting time dynamics.Key Findings (Imai, Kim, Wang 2021)Even favorable conditions OLS, PM-robust model misspecification omitted lags.Even favorable conditions OLS, PM-robust model misspecification omitted lags.robustness comes cost: reduced efficiency (larger variance).robustness comes cost: reduced efficiency (larger variance).Reflects classic bias-variance tradeoff flexible parametric estimators.Reflects classic bias-variance tradeoff flexible parametric estimators.Data Software RequirementsTreatment variable: binary (0 = control, 1 = treated).Treatment variable: binary (0 = control, 1 = treated).Unit time variables: integer/numeric ordered.Unit time variables: integer/numeric ordered.Input data must data.frame format.Input data must data.frame format.Examples:Scheve Stasavage (2012)Scheve Stasavage (2012)Acemoglu et al. (2019)Acemoglu et al. (2019)","code":""},{"path":"sec-difference-in-differences.html","id":"two-way-matching-interpretation-of-twfe","chapter":"30 Difference-in-Differences","heading":"30.8.4.3 Two-Way Matching Interpretation of TWFE","text":"least squares estimate \\(\\beta\\) TWFE model can re-expressed matching estimator compares treated unit observations within:unit (within-unit match),time period (within-time match),Adjusted third set observations neither group.leads mismatches—treated observations compared units treatment status, causes attenuation bias.adjustment factor \\(K\\) corrects weighting matches appropriately. However, even weighted TWFE estimator contains mismatches relies comparisons across units differ key characteristics.simple two-period, two-group setting, TWFE estimators coincide. However, multi-period treatment reversals, equivalence breaks (Imai Kim 2021).unweighted TWFE equivalent multi-period .multi-period equivalent weighted TWFE, weights negative—problematic feature design-based perspective.means justifying TWFE via logic incorrect unless linearity assumption satisfied.","code":""},{"path":"sec-difference-in-differences.html","id":"estimation-using-panel-match-did","chapter":"30 Difference-in-Differences","heading":"30.8.4.4 Estimation Using Panel Match DiD","text":"Core Estimation Steps (Imai, Kim, Wang 2021):Match treated observations control observations time period identical treatment histories past \\(L\\) periods.Use standard matching weighting methods refine control sets (e.g., Mahalanobis distance, propensity score).Apply estimator compute treatment effects time \\(t + F\\).Evaluate match quality using covariate balance diagnostics (Ho et al. 2007).Causal EstimandLet \\(F\\) number leads (future periods) \\(L\\) number lags (past treatment periods). Define average treatment effect :\\[\n\\delta(F, L) = \\mathbb{E}\\left[Y_{, t+F}(1) - Y_{, t+F}(0) \\mid \\text{treatment history } t-L \\text{ } t\\right]\n\\]\\(F = 0\\): contemporaneous effect (short-run ATT)\\(F > 0\\): future outcomes (long-run ATT)\\(L\\): adjusts potential carryover effectsThe estimator also allows estimation Average Reversal Treatment Effect (ART) treatment status switches 1 0.","code":""},{"path":"sec-difference-in-differences.html","id":"model-assumptions","chapter":"30 Difference-in-Differences","heading":"30.8.4.5 Model Assumptions","text":"spillover effects across units (.e., SUTVA holds)spillover effects across units (.e., SUTVA holds)Carryover effects allowed \\(L\\) periods.Carryover effects allowed \\(L\\) periods.\\(L\\) lags, prior treatments assumed effect \\(Y_{,t+F}\\).\\(L\\) lags, prior treatments assumed effect \\(Y_{,t+F}\\).potential outcome \\(t + F\\) independent treatment assignments beyond \\(t - L\\).potential outcome \\(t + F\\) independent treatment assignments beyond \\(t - L\\).key identifying assumption conditional parallel trends assumption. Outcome trends assumed parallel across treated matched control units, conditional :\nPast treatment,\nCovariate histories,\nLagged outcomes (excluding recent).\nUnlike standard TWFE, strong ignorability required.key identifying assumption conditional parallel trends assumption. Outcome trends assumed parallel across treated matched control units, conditional :Past treatment,Past treatment,Covariate histories,Covariate histories,Lagged outcomes (excluding recent).Lagged outcomes (excluding recent).Unlike standard TWFE, strong ignorability required.","code":""},{"path":"sec-difference-in-differences.html","id":"covariate-balance-assessment","chapter":"30 Difference-in-Differences","heading":"30.8.4.6 Covariate Balance Assessment","text":"Assessing balance estimating ATT critical:Compute mean standardized difference treated matched control units.Check balance across covariates lagged outcomes \\(L\\) pretreatment periods.Imbalanced covariates may indicate violations parallel trends assumption.","code":""},{"path":"sec-difference-in-differences.html","id":"implementing-the-panel-match-did-estimator","chapter":"30 Difference-in-Differences","heading":"30.8.4.7 Implementing the Panel Match DiD Estimator","text":"Treatment Variation PlotVisualizing variation treatment across space time essential assess whether treatment sufficient heterogeneity support credible causal identification.plot aids identifying whether treatment broadly distributed concentrated among units time periods.plot aids identifying whether treatment broadly distributed concentrated among units time periods.Insufficient treatment variation may weaken identification reduce precision estimated effects.Insufficient treatment variation may weaken identification reduce precision estimated effects.","code":"\nlibrary(PanelMatch)\nDisplayTreatment(\n  panel.data = PanelData(\n    panel.data = dem,\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    treatment = \"dem\",\n    outcome = \"y\"\n  ),\n  legend.position = \"none\",\n  xlab = \"year\",\n  ylab = \"Country Code\"\n)"},{"path":"sec-difference-in-differences.html","id":"setting-parameters-f-and-l","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.1 Setting Parameters \\(F\\) and \\(L\\)","text":"Select \\(F\\): number leads, time periods treatment, effect measured.\\(F = 0\\): contemporaneous (short-term) treatment effect.\\(F = 0\\): contemporaneous (short-term) treatment effect.\\(F > 0\\): long-term cumulative effects.\\(F > 0\\): long-term cumulative effects.Select \\(L\\): number lags (prior treatment periods) used matching adjust carryover effects.Increasing \\(L\\) enhances credibility reduces match quality sample size.Increasing \\(L\\) enhances credibility reduces match quality sample size.selection reflects bias-variance tradeoff.selection reflects bias-variance tradeoff.","code":""},{"path":"sec-difference-in-differences.html","id":"causal-quantity-of-interest","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.2 Causal Quantity of Interest","text":"ATT defined :\\[\n\\delta(F, L) = \\mathbb{E} \\left[ Y_{,t+F}(1) - Y_{,t+F}(0) \\mid X_{,t} = 1, \\text{History}_{,t-L:t-1} \\right]\n\\]estimator accounts carryover history (via \\(L\\)) post-treatment dynamics (via \\(F\\)).estimator accounts carryover history (via \\(L\\)) post-treatment dynamics (via \\(F\\)).also robust treatment reversals, .e., treatment switching back control.also robust treatment reversals, .e., treatment switching back control.related estimand, Average Reversal Treatment Effect (ART), measures causal effect switching treatment control.","code":""},{"path":"sec-difference-in-differences.html","id":"choosing-f-and-l","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.3 Choosing \\(F\\) and \\(L\\)","text":"Large \\(L\\):\nImproves identification causal effect accounting long-term treatment confounding.\nReduces sample size due stricter matching requirements.\nLarge \\(L\\):Improves identification causal effect accounting long-term treatment confounding.Improves identification causal effect accounting long-term treatment confounding.Reduces sample size due stricter matching requirements.Reduces sample size due stricter matching requirements.Large \\(F\\):\nEnables analysis delayed effects.\nComplicates interpretation units switch treatment \\(t + F\\).\nLarge \\(F\\):Enables analysis delayed effects.Enables analysis delayed effects.Complicates interpretation units switch treatment \\(t + F\\).Complicates interpretation units switch treatment \\(t + F\\).Researchers select \\(F\\) \\(L\\) based substantive context, theoretical considerations, sensitivity analysis.","code":""},{"path":"sec-difference-in-differences.html","id":"constructing-and-refining-matched-sets","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.4 Constructing and Refining Matched Sets","text":"Initial MatchingEach treated observation matched control units units time period.treated observation matched control units units time period.Matching based exact treatment histories \\(t - L\\) \\(t - 1\\).Matching based exact treatment histories \\(t - L\\) \\(t - 1\\).PurposeControls carryover effects.Controls carryover effects.Ensures matched units similar latent propensities treatment.Ensures matched units similar latent propensities treatment.Refinement ProcessRefined matched sets additionally adjust pre-treatment covariates lagged outcomes.Refined matched sets additionally adjust pre-treatment covariates lagged outcomes.Matching strategies:\nMahalanobis distance.\nPropensity score.\nMatching strategies:Mahalanobis distance.Mahalanobis distance.Propensity score.Propensity score.\\(J\\) best matches per treated unit may used.WeightingAssigns weights matched controls emphasize similarity.Assigns weights matched controls emphasize similarity.Weighting often done via inverse propensity scores, balance-enhancing metrics.Weighting often done via inverse propensity scores, balance-enhancing metrics.Can considered generalization traditional matching.Can considered generalization traditional matching.","code":""},{"path":"sec-difference-in-differences.html","id":"difference-in-differences-estimation","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.5 Difference-in-Differences Estimation","text":"matched sets constructed:counterfactual treated unit weighted average outcomes matched control set.counterfactual treated unit weighted average outcomes matched control set.estimate ATT :estimate ATT :\\[\n\\widehat{\\delta}_{\\text{ATT}} = \\frac{1}{|T_1|} \\sum_{(,t) \\T_1} \\left[ Y_{,t+F} - \\sum_{j \\\\mathcal{C}_{}} w_{ijt} Y_{j,t+F} \\right]\n\\]\\(T_1\\) set treated observations, \\(\\mathcal{C}_{}\\) matched control set, \\(w_{ijt}\\) normalized weights.Considerations \\(F > 0\\):Matched controls may switch treatment \\(t + F\\).Matched controls may switch treatment \\(t + F\\).treated units may revert control.treated units may revert control.","code":""},{"path":"sec-difference-in-differences.html","id":"checking-covariate-balance","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.6 Checking Covariate Balance","text":"One main advantages matching-based estimators ability diagnose balance:covariate lag, compute:\\[\n\\text{Standardized Difference} = \\frac{\\bar{X}_{\\text{treated}} - \\bar{X}_{\\text{control}}}{\\text{SD}_{\\text{treated}}}\n\\]Aggregate treated observations time periods.Aggregate treated observations time periods.Examine balance :\nTime-varying covariates,\nLagged outcomes,\nBaseline covariates\nExamine balance :Time-varying covariates,Time-varying covariates,Lagged outcomes,Lagged outcomes,Baseline covariatesBaseline covariatesBalance checks provide indirect validation parallel trends assumption.","code":""},{"path":"sec-difference-in-differences.html","id":"standard-error-estimation","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.7 Standard Error Estimation","text":"Analogous conditional variance seen regression models.Analogous conditional variance seen regression models.Standard errors calculated conditional matching weights (G. W. Imbens Rubin 2015).Standard errors calculated conditional matching weights (G. W. Imbens Rubin 2015).SE measure sampling uncertainty given matched design.SE measure sampling uncertainty given matched design.Note: incorporate uncertainty matching procedure (Ho et al. 2007).","code":""},{"path":"sec-difference-in-differences.html","id":"matching-on-treatment-history","chapter":"30 Difference-in-Differences","heading":"30.8.4.7.8 Matching on Treatment History","text":"goal compare treated units transitioning treatment control units comparable treatment histories.goal compare treated units transitioning treatment control units comparable treatment histories.Set qoi =:\n\"att\": Average Treatment Treated,\n\"atc\": Average Treatment Controls,\n\"art\": Average Reversal Treatment Effect,\n\"ate\": Average Treatment Effect.\nSet qoi =:\"att\": Average Treatment Treated,\"att\": Average Treatment Treated,\"atc\": Average Treatment Controls,\"atc\": Average Treatment Controls,\"art\": Average Reversal Treatment Effect,\"art\": Average Reversal Treatment Effect,\"ate\": Average Treatment Effect.\"ate\": Average Treatment Effect.Control units treated unit identical treatment histories lag window (1988-1991)set limited first one, can still see exact past histories.Refining Matched Sets\nRefinement involves assigning weights control units.\nUsers must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\n\nRefining Matched SetsRefinement involves assigning weights control units.Refinement involves assigning weights control units.Users must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\nUsers must:Specify method calculating unit similarity/distance.Specify method calculating unit similarity/distance.Choose variables similarity/distance calculations.Choose variables similarity/distance calculations.Select Refinement Method\nUsers determine refinement method via refinement.method argument.\nOptions include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\n\nMethods “match” name Mahalanobis assign equal weights similar control units.\n“Weighting” methods give higher weights control units similar treated units.\nSelect Refinement MethodUsers determine refinement method via refinement.method argument.Users determine refinement method via refinement.method argument.Options include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\nOptions include:mahalanobismahalanobisps.matchps.matchCBPS.matchCBPS.matchps.weightps.weightCBPS.weightCBPS.weightps.msm.weightps.msm.weightCBPS.msm.weightCBPS.msm.weightnonenoneMethods “match” name Mahalanobis assign equal weights similar control units.Methods “match” name Mahalanobis assign equal weights similar control units.“Weighting” methods give higher weights control units similar treated units.“Weighting” methods give higher weights control units similar treated units.Variable Selection\nUsers need define covariates used covs.formula argument, one-sided formula object.\nVariables right side formula used calculations.\n“Lagged” versions variables can included using format: (lag(name..var, 0:n)).\nVariable SelectionUsers need define covariates used covs.formula argument, one-sided formula object.Users need define covariates used covs.formula argument, one-sided formula object.Variables right side formula used calculations.Variables right side formula used calculations.“Lagged” versions variables can included using format: (lag(name..var, 0:n)).“Lagged” versions variables can included using format: (lag(name..var, 0:n)).Understanding PanelMatch matched.set objects\nPanelMatch function returns PanelMatch object.\ncrucial element within PanelMatch object matched.set object.\nWithin PanelMatch object, matched.set object names like att, art, atc.\nqoi = ate, two matched.set objects: att atc.\nUnderstanding PanelMatch matched.set objectsThe PanelMatch function returns PanelMatch object.PanelMatch function returns PanelMatch object.crucial element within PanelMatch object matched.set object.crucial element within PanelMatch object matched.set object.Within PanelMatch object, matched.set object names like att, art, atc.Within PanelMatch object, matched.set object names like att, art, atc.qoi = ate, two matched.set objects: att atc.qoi = ate, two matched.set objects: att atc.Matched.set Object Details\nmatched.set named list added attributes.\nAttributes include:\nLag\nNames treatment\nUnit time variables\n\nlist entry represents matched set treated control units.\nNaming follows structure: [id variable].[time variable].\nlist element vector control unit ids match treated unit mentioned element name.\nSince ’s matching method, weights given size.match similar control units based distance calculations.\nMatched.set Object Detailsmatched.set named list added attributes.matched.set named list added attributes.Attributes include:\nLag\nNames treatment\nUnit time variables\nAttributes include:LagLagNames treatmentNames treatmentUnit time variablesUnit time variablesEach list entry represents matched set treated control units.list entry represents matched set treated control units.Naming follows structure: [id variable].[time variable].Naming follows structure: [id variable].[time variable].list element vector control unit ids match treated unit mentioned element name.list element vector control unit ids match treated unit mentioned element name.Since ’s matching method, weights given size.match similar control units based distance calculations.Since ’s matching method, weights given size.match similar control units based distance calculations.Visualizing Matched Sets plot methodUsers can visualize distribution matched set sizes.Users can visualize distribution matched set sizes.red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).Plot adjustments can made using graphics::plot.Plot adjustments can made using graphics::plot.Comparing Methods RefinementUsers encouraged :\nUse substantive knowledge experimentation evaluation.\nConsider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\n\nNote: Large numbers small matched sets can lead larger standard errors estimation stage.\nCovariates aren’t well balanced can lead undesirable comparisons treated control units.\nAspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\n\nUsers encouraged :Use substantive knowledge experimentation evaluation.Use substantive knowledge experimentation evaluation.Consider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\nConsider following configuring PanelMatch:number matched sets.number matched sets.number controls matched treated unit.number controls matched treated unit.Achieving covariate balance.Achieving covariate balance.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Aspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\nAspects consider include:Refinement method.Refinement method.Variables weight calculation.Variables weight calculation.Size lag window.Size lag window.Procedures addressing missing data (refer match.missing listwise.delete arguments).Procedures addressing missing data (refer match.missing listwise.delete arguments).Maximum size matched sets (matching methods).Maximum size matched sets (matching methods).Supportive Features:\nprint, plot, summary methods assist understanding matched sets sizes.\nget_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\n\nSupportive Features:print, plot, summary methods assist understanding matched sets sizes.print, plot, summary methods assist understanding matched sets sizes.get_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\nget_covariate_balance helps evaluate covariate balance:Lower values covariate balance calculations preferred.PanelEstimateStandard Error Calculation Methods\ndifferent methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\n\nqoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\n\nStandard Error Calculation MethodsThere different methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\ndifferent methods available:Bootstrap (default method 1000 iterations).Bootstrap (default method 1000 iterations).Conditional: Assumes independence across units, time.Conditional: Assumes independence across units, time.Unconditional: Doesn’t make assumptions independence across units time.Unconditional: Doesn’t make assumptions independence across units time.qoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\nqoi values set att, art, atc (Imai, Kim, Wang 2021):can use analytical methods calculating standard errors, include “conditional” “unconditional” methods.Moderating VariablesIn study, closely aligned research Acemoglu et al. (2019), two key effects democracy economic growth estimated: impact democratization authoritarian reversal. treatment variable, \\(X_{}\\), defined one country \\(\\) democratic year \\(t\\), zero otherwise.Average Treatment Effect Treated (ATT) democratization formulated follows:\\[\n\\begin{aligned}\n\\delta(F, L) &= \\mathbb{E} \\left\\{ Y_{, t + F} (X_{} = 1, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 0, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) | X_{} = 1, X_{, t - 1} = 0 \\right\\}\n\\end{aligned}\n\\]framework, treated observations countries transition authoritarian regime \\(X_{-1} = 0\\) democratic one \\(X_{} = 1\\). variable \\(F\\) represents number leads, denoting time periods following treatment, \\(L\\) signifies number lags, indicating time periods preceding treatment.ATT authoritarian reversal given :\\[\n\\begin{aligned}\n&\\mathbb{E} \\left[ Y_{, t + F} (X_{} = 0, X_{, t - 1} = 1, \\{ X_{, t - l}\\}_{l=2}^L ) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 1, X_{-1} = 1, \\{X_{, t - l} \\}_{l=2}^L ) | X_{} = 0, X_{, t - 1} = 1 \\right]\n\\end{aligned}\n\\]ATT calculated conditioning 4 years lags (\\(L = 4\\)) 4 years following policy change \\(F = 1, 2, 3, 4\\). Matched sets treated observation constructed based treatment history, number matched control units generally decreasing considering 4-year treatment history compared 1-year history.enhance quality matched sets, methods Mahalanobis distance matching, propensity score matching, propensity score weighting utilized. approaches enable us evaluate effectiveness refinement method. process matching, employ --five --ten matching investigate sensitive empirical results maximum number allowed matches. information refinement process, please see Web AppendixThe Mahalanobis distance expressed specific formula. aim pair treated unit maximum \\(J\\) control units, permitting replacement, denoted \\(| \\mathcal{M}_{} \\le J|\\). average Mahalanobis distance treated control unit time computed :\\[ S_{} (') = \\frac{1}{L} \\sum_{l = 1}^L \\sqrt{(\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})^T \\mathbf{\\Sigma}_{, t - l}^{-1} (\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})} \\]matched control unit \\(' \\\\mathcal{M}_{}\\), \\(\\mathbf{V}_{'}\\) represents time-varying covariates adjust , \\(\\mathbf{\\Sigma}_{'}\\) sample covariance matrix \\(\\mathbf{V}_{'}\\). Essentially, calculate standardized distance using time-varying covariates average across different time intervals.context propensity score matching, employ logistic regression model balanced covariates derive propensity score. Defined conditional likelihood treatment given pre-treatment covariates (Rosenbaum Rubin 1983), propensity score estimated first creating data subset comprised treated matched control units year. logistic regression model fitted follows:\\[ \\begin{aligned} & e_{} (\\{\\mathbf{U}_{, t - l} \\}^L_{l = 1}) \\\\ &= Pr(X_{} = 1| \\mathbf{U}_{, t -1}, \\ldots, \\mathbf{U}_{, t - L}) \\\\ &= \\frac{1}{1 = \\exp(- \\sum_{l = 1}^L \\beta_l^T \\mathbf{U}_{, t - l})} \\end{aligned} \\]\\(\\mathbf{U}_{'} = (X_{'}, \\mathbf{V}_{'}^T)^T\\). Given model, estimated propensity score treated matched control units computed. enables adjustment lagged covariates via matching calculated propensity score, resulting following distance measure:\\[ S_{} (') = | \\text{logit} \\{ \\hat{e}_{} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\} - \\text{logit} \\{ \\hat{e}_{'t}( \\{ \\mathbf{U}_{', t - l} \\}^L_{l = 1})\\} | \\], \\(\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\) represents estimated propensity score matched control unit \\(' \\\\mathcal{M}_{}\\).distance measure \\(S_{} (')\\) determined control units original matched set, fine-tune set selecting \\(J\\) closest control units, meet researcher-defined caliper constraint \\(C\\). control units receive zero weight. results refined matched set treated unit \\((, t)\\):\\[ \\mathcal{M}_{}^* = \\{' : ' \\\\mathcal{M}_{}, S_{} (') < C, S_{} \\le S_{}^{(J)}\\} \\]\\(S_{}^{(J)}\\) \\(J\\)th smallest distance among control units original set \\(\\mathcal{M}_{}\\).refinement using weighting, weight assigned control unit \\('\\) matched set corresponding treated unit \\((, t)\\), greater weight accorded similar units. utilize inverse propensity score weighting, based propensity score model mentioned earlier:\\[ w_{}^{'} \\propto \\frac{\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )}{1 - \\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )} \\]model, \\(\\sum_{' \\\\mathcal{M}_{}} w_{}^{'} = 1\\) \\(w_{}^{'} = 0\\) \\(' \\notin \\mathcal{M}_{}\\). model fitted complete sample treated matched control units.Checking Covariate Balance distinct advantage proposed methodology regression methods ability offers researchers inspect covariate balance treated matched control observations. facilitates evaluation whether treated matched control observations comparable regarding observed confounders. investigate mean difference covariate (e.g., \\(V_{'j}\\), representing \\(j\\)-th variable \\(\\mathbf{V}_{'}\\)) treated observation matched control observation pre-treatment time period (.e., \\(t' < t\\)), standardize difference. given pretreatment time period, adjust standard deviation covariate across treated observations dataset. Thus, mean difference quantified terms standard deviation units. Formally, treated observation \\((,t)\\) \\(D_{} = 1\\), define covariate balance variable \\(j\\) pretreatment time period \\(t - l\\) : \\[\\begin{equation}\nB_{}(j, l) = \\frac{V_{, t- l,j}- \\sum_{' \\\\mathcal{M}_{}}w_{}^{'}V_{', t-l,j}}{\\sqrt{\\frac{1}{N_1 - 1} \\sum_{'=1}^N \\sum_{t' = L+1}^{T-F}D_{'t'}(V_{', t'-l, j} - \\bar{V}_{t' - l, j})^2}}\n\\label{eq:covbalance}\n\\end{equation}\\] \\(N_1 = \\sum_{'= 1}^N \\sum_{t' = L+1}^{T-F} D_{'t'}\\) denotes total number treated observations \\(\\bar{V}_{t-l,j} = \\sum_{=1}^N D_{,t-l,j}/N\\). aggregate covariate balance measure across treated observations covariate pre-treatment time period:Lastly, evaluate balance lagged outcome variables several pre-treatment periods time-varying covariates. examination aids assessing validity parallel trend assumption integral estimator justification.Figure balance scatter, demonstrate enhancement covariate balance thank refinement matched sets. scatter plot contrasts absolute standardized mean difference, detailed Equation @ref(eq: ), (horizontal axis) (vertical axis) refinement. Points 45-degree line indicate improved standardized mean balance certain time-varying covariates post-refinement. majority variables benefit refinement process. Notably, propensity score weighting (bottom panel) shows significant improvement, whereas Mahalanobis matching (top panel) yields modest improvement.can either sequentiallyor parallelNote: Scatter plots display standardized mean difference covariate \\(j\\) lag year \\(l\\) defined Equation (??) (x-axis) (y-axis) matched set refinement. plot includes varying numbers possible matches matching method. Rows represent different matching/weighting methods, columns indicate adjustments various lag lengths.exportNote: graph displays standardized mean difference, outlined Equation (??), plotted vertical axis across pre-treatment duration four years represented horizontal axis. leftmost column illustrates balance prior refinement, subsequent three columns depict covariate balance post application distinct refinement techniques. individual line signifies balance specific variable pre-treatment phase.red line tradewb blue line lagged outcome variable.Figure ??, observe marked improvement covariate balance due implemented matching procedures pre-treatment period. analysis prioritizes methods adjust time-varying covariates span four years preceding treatment initiation. two rows delineate standardized mean balance treatment modalities, individual lines representing balance covariate.Across scenarios, refinement attributed matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances confounders. degree imbalance remains evident Mahalanobis distance propensity score matching techniques, standardized mean difference lagged outcome remains stable throughout pre-treatment phase. consistency lends credence validity proposed estimator.Estimation ResultsWe now detail estimated ATTs derived matching techniques. Figure offers visual representations impacts treatment initiation (upper panel) treatment reversal (lower panel) outcome variable duration 5 years post-transition, specifically, (\\(F = 0, 1, …, 4\\)). Across five methods (columns), becomes evident point estimates effects associated treatment initiation consistently approximate zero 5-year window. contrast, estimated outcomes treatment reversal notably negative maintain statistical significance refinement techniques initial year transition 1 4 years follow, provided treatment reversal permissible. effects notably pronounced, pointing estimated reduction roughly X% outcome variable.Collectively, findings indicate transition treated state absence doesn’t invariably lead heightened outcome. Instead, transition treated state back absence exerts considerable negative effect outcome variable short intermediate terms. Hence, positive effect treatment (use traditional ) actually driven negative effect treatment reversal.export","code":"\nlibrary(PanelMatch)\n# All examples follow the package's vignette\n# Create the matched sets\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"none\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# visualize the treated unit and matched controls\nDisplayTreatment(\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    matched.set = PM.results.none$att[1],\n    # highlight the particular set\n    show.set.only = TRUE\n)\nDisplayTreatment(\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    matched.set = PM.results.none$att[2],\n    # highlight the particular set\n    show.set.only = TRUE\n)\n# PanelMatch without any refinement\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"none\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# Extract the matched.set object\nmsets.none <- PM.results.none$att\n\n# PanelMatch with refinement\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"mahalanobis\", # use Mahalanobis distance\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = TRUE,\n        covs.formula = ~ tradewb,\n        size.match = 5,\n        qoi = \"att\" ,\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nmsets.maha <- PM.results.maha$att\n# these 2 should be identical because weights are not shown\nmsets.none |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\nmsets.maha |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\n# summary(msets.none)\n# summary(msets.maha)\nplot(\n  msets.none,\n  panel.data = PanelData(\n    panel.data = dem,\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    treatment = \"dem\",\n    outcome = \"y\"\n  )\n)\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"none\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"mahalanobis\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# listwise deletion used for missing data\nPM.results.listwise <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"mahalanobis\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# propensity score based weighting method\nPM.results.ps.weight <-\n    PanelMatch(\n        lag = 4,\n        refinement.method = \"ps.weight\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE\n    )\n\nget_covariate_balance(\n    PM.results.none,\n    panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    covariates = c(\"tradewb\", \"y\")\n)\n#> $att\n#>         tradewb            y\n#> t_4 -0.07245466  0.291871990\n#> t_3 -0.20930129  0.208654876\n#> t_2 -0.24425207  0.107736647\n#> t_1 -0.10806125 -0.004950238\n#> t_0 -0.09493854 -0.015198483\n# Compare covariate balance to refined sets\n# See large improvement in balance\nget_covariate_balance(\n    PM.results.ps.weight,\n     panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    covariates = c(\"tradewb\", \"y\")\n)\n#> $att\n#>         tradewb          y\n#> t_4 0.014362590 0.04035905\n#> t_3 0.005529734 0.04188731\n#> t_2 0.009410044 0.04195008\n#> t_1 0.027907540 0.03975173\n#> t_0 0.040272235 0.04167921\nPE.results <- PanelEstimate(\n    sets              = PM.results.ps.weight,\n    panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    se.method         = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level  = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#> NULL\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.6126167 1.0374472 1.4424743 1.8057148 2.1835808\n\n\n# use conditional method\nPE.results <- PanelEstimate(\n    sets             = PM.results.ps.weight,\n    panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n    se.method        = \"conditional\",\n    confidence.level = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#> NULL\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143\n\nsummary(PE.results)\n#>      estimate std.error       2.5%    97.5%\n#> t+0 0.2609565 0.4844805 -0.6886078 1.210521\n#> t+1 0.9630847 0.8170604 -0.6383243 2.564494\n#> t+2 1.2851017 1.1171942 -0.9045586 3.474762\n#> t+3 1.7370930 1.4116879 -1.0297644 4.503950\n#> t+4 1.4871846 1.7172143 -1.8784937 4.852863\n\nplot(PE.results)\n# moderating variable\ndem$moderator <- 0\ndem$moderator <- ifelse(dem$wbcode2 > 100, 1, 2)\n\nPM.results <-\n    PanelMatch(\n        lag                          = 4,\n        # time.id                      = \"year\",\n        # unit.id                      = \"wbcode2\",\n        # treatment                    = \"dem\",\n        refinement.method            = \"mahalanobis\",\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing                = TRUE,\n        covs.formula                 = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match                   = 5,\n        qoi                          = \"att\",\n        # outcome.var                  = \"y\",\n        lead                         = 0:4,\n        forbid.treatment.reversal    = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPE.results <-\n    PanelEstimate(sets      = PM.results,\n                  panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n                  moderator = \"moderator\")\n\n# Each element in the list corresponds to a level in the moderator\nplot(PE.results[[1]])\n\nplot(PE.results[[2]])\nlibrary(PanelMatch)\nlibrary(causalverse)\n\nrunPanelMatch <- function(method, lag, size.match=NULL, qoi=\"att\") {\n    \n    # Default parameters for PanelMatch\n    common.args <- list(\n        lag = lag,\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi = qoi,\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        size.match = size.match  # setting size.match here for all methods\n    )\n    \n    if(method == \"mahalanobis\") {\n        common.args$refinement.method <- \"mahalanobis\"\n        common.args$match.missing <- TRUE\n        common.args$use.diagonal.variance.matrix <- TRUE\n    } else if(method == \"ps.match\") {\n        common.args$refinement.method <- \"ps.match\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    } else if(method == \"ps.weight\") {\n        common.args$refinement.method <- \"ps.weight\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    }\n    \n    return(do.call(PanelMatch, common.args))\n}\n\nmethods <- c(\"mahalanobis\", \"ps.match\", \"ps.weight\")\nlags <- c(1, 4)\nsizes <- c(5, 10)\nres_pm <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm[[name]] <- runPanelMatch(method, lag, size)\n        }\n    }\n}\n\n# Now, you can access res_pm using res_pm[[\"mahalanobis.1lag.5m\"]] etc.\n\n# for treatment reversal\nres_pm_rev <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm_rev[[name]] <- runPanelMatch(method, lag, size, qoi = \"art\")\n        }\n    }\n}\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(cores = 4)\n# Initialize an empty list to store results\nres_pm <- list()\n\n# Replace nested for-loops with foreach\nresults <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <- runPanelMatch(method, lag, size)\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results)) {\n  res_pm[[name]] <- results[[name]]\n}\n\n# Treatment reversal\n# Initialize an empty list to store results\nres_pm_rev <- list()\n\n# Replace nested for-loops with foreach\nresults_rev <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <-\n          runPanelMatch(method, lag, size, qoi = \"art\")\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results_rev)) {\n  res_pm_rev[[name]] <- results_rev[[name]]\n}\n\n\nstopImplicitCluster()\nlibrary(gridExtra)\n\n# Updated plotting function\ncreate_balance_plot <- function(method, lag, sizes, res_pm, dem) {\n    matched_set_lists <- lapply(sizes, function(size) {\n        res_pm[[paste0(method, \".\", lag, \"lag.\", size, \"m\")]]$att\n    })\n    \n    return(\n        balance_scatter_custom(\n            matched_set_list = matched_set_lists,\n            legend.title = \"Possible Matches\",\n            set.names = as.character(sizes),\n            legend.position = c(0.2, 0.8),\n            \n            # for compiled plot, you don't need x,y, or main labs\n            x.axis.label = \"\",\n            y.axis.label = \"\",\n            main = \"\",\n            data = dem,\n            dot.size = 5,\n            # show.legend = F,\n            them_use = causalverse::ama_theme(base_size = 32),\n            covariates = c(\"y\", \"tradewb\")\n        )\n    )\n}\n\nplots <- list()\n\nfor (method in methods) {\n    for (lag in lags) {\n        plots[[paste0(method, \".\", lag, \"lag\")]] <-\n            create_balance_plot(method, lag, sizes, res_pm, dem)\n    }\n}\n\n# # Arranging plots in a 3x2 grid\n# grid.arrange(plots[[\"mahalanobis.1lag\"]],\n#              plots[[\"mahalanobis.4lag\"]],\n#              plots[[\"ps.match.1lag\"]],\n#              plots[[\"ps.match.4lag\"]],\n#              plots[[\"ps.weight.1lag\"]],\n#              plots[[\"ps.weight.4lag\"]],\n#              ncol=2, nrow=3)\n\n\n# Standardized Mean Difference of Covariates\nlibrary(gridExtra)\nlibrary(grid)\n\n# Create column and row labels using textGrob\ncol_labels <- c(\"1-year Lag\", \"4-year Lag\")\nrow_labels <- c(\"Maha Matching\", \"PS Matching\", \"PS Weigthing\")\n\nmajor.axes.fontsize = 40\nminor.axes.fontsize = 30\n\npng(\n    file.path(getwd(), \"images\", \"did_balance_scatter.png\"),\n    width = 1200,\n    height = 1000\n)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(textGrob(\n        row_labels[1],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"mahalanobis.1lag\"]], plots[[\"mahalanobis.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[2],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.match.1lag\"]], plots[[\"ps.match.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[3],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.weight.1lag\"]], plots[[\"ps.weight.4lag\"]])\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\ngrid.arrange(\n    grobs = grobs,\n    ncol = 3,\n    nrow = 4,\n    widths = c(0.15, 0.42, 0.42),\n    heights = c(0.15, 0.28, 0.28, 0.28)\n)\n\ngrid.text(\n    \"Before Refinement\",\n    x = 0.5,\n    y = 0.03,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"After Refinement\",\n    x = 0.03,\n    y = 0.5,\n    rot = 90,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ndev.off()\n# Step 1: Define configurations\nconfigurations <- list(\n    list(refinement.method = \"none\", qoi = \"att\"),\n    list(refinement.method = \"none\", qoi = \"art\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"att\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"art\"),\n    list(refinement.method = \"ps.match\", qoi = \"att\"),\n    list(refinement.method = \"ps.match\", qoi = \"art\"),\n    list(refinement.method = \"ps.weight\", qoi = \"att\"),\n    list(refinement.method = \"ps.weight\", qoi = \"art\")\n)\n\n# Step 2: Use lapply or loop to generate results\nresults <- lapply(configurations, function(config) {\n    PanelMatch(\n        lag                       = 4,\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        match.missing             = FALSE,\n        listwise.delete           = TRUE,\n        size.match                = 5,\n        lead                      = 0:4,\n        forbid.treatment.reversal = FALSE,\n        refinement.method         = config$refinement.method,\n        covs.formula              = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi                       = config$qoi\n    )\n})\n\n# Step 3: Get covariate balance and plot\nplots <- mapply(function(result, config) {\n    df <- get_covariate_balance(\n        if (config$qoi == \"att\")\n            result$att\n        else\n            result$art,\n        panel.data = PanelData(panel.data = dem, \n              unit.id = \"wbcode2\", \n              time.id = \"year\", \n              treatment = \"dem\", \n              outcome = \"y\"),\n        covariates = c(\"tradewb\", \"y\"),\n        plot = F\n    )\n    causalverse::plot_covariate_balance_pretrend(df, main = \"\", show_legend = F)\n}, results, configurations, SIMPLIFY = FALSE)\n\n# Set names for plots\nnames(plots) <- sapply(configurations, function(config) {\n    paste(config$qoi, config$refinement.method, sep = \".\")\n})\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <-\n    c(\"None\",\n      \"Mahalanobis\",\n      \"Propensity Score Matching\",\n      \"Propensity Score Weighting\")\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_covariate_balance.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(\n        textGrob(\n            row_labels[1],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$att.none,\n        plots$att.mahalanobis,\n        plots$att.ps.match,\n        plots$att.ps.weight\n    ),\n    \n    list(\n        textGrob(\n            row_labels[2],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$art.none,\n        plots$art.mahalanobis,\n        plots$art.ps.match,\n        plots$art.ps.weight\n    )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n    grobs   = grobs,\n    ncol    = 5,\n    nrow    = 3,\n    widths  = c(0.1, 0.225, 0.225, 0.225, 0.225),\n    heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n    \"Refinement Methods\",\n    x  = 0.5,\n    y  = 0.01,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"Quantities of Interest\",\n    x   = 0.02,\n    y   = 0.5,\n    rot = 90,\n    gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_covariate_balance.png\"))\n# sequential\n# Step 1: Apply PanelEstimate function\n\n# Initialize an empty list to store results\nres_est <- vector(\"list\", length(res_pm))\n\n# Iterate over each element in res_pm\nfor (i in 1:length(res_pm)) {\n  res_est[[i]] <- PanelEstimate(\n    res_pm[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est list\n  names(res_est)[i] <- names(res_pm)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function\n\n# Initialize an empty list to store plot results\nres_est_plot <- vector(\"list\", length(res_est))\n\n# Iterate over each element in res_est\nfor (i in 1:length(res_est)) {\n    res_est_plot[[i]] <-\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n    # Transfer the name of the current element to the res_est_plot list\n    names(res_est_plot)[i] <- names(res_est)[i]\n}\n\n# check results\n# res_est_plot$mahalanobis.1lag.5m\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev\n\n# Initialize an empty list to store results\nres_est_rev <- vector(\"list\", length(res_pm_rev))\n\n# Iterate over each element in res_pm_rev\nfor (i in 1:length(res_pm_rev)) {\n  res_est_rev[[i]] <- PanelEstimate(\n    res_pm_rev[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est_rev list\n  names(res_est_rev)[i] <- names(res_pm_rev)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev\n\n# Initialize an empty list to store plot results\nres_est_plot_rev <- vector(\"list\", length(res_est_rev))\n\n# Iterate over each element in res_est_rev\nfor (i in 1:length(res_est_rev)) {\n    res_est_plot_rev[[i]] <-\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n  # Transfer the name of the current element to the res_est_plot_rev list\n  names(res_est_plot_rev)[i] <- names(res_est_rev)[i]\n}\n# parallel\nlibrary(doParallel)\nlibrary(foreach)\n\n# Detect the number of cores to use for parallel processing\nnum_cores <- 4\n\n# Register the parallel backend\ncl <- makeCluster(num_cores)\nregisterDoParallel(cl)\n\n# Step 1: Apply PanelEstimate function in parallel\nres_est <-\n    foreach(i = 1:length(res_pm), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm to res_est\nnames(res_est) <- names(res_pm)\n\n# Step 2: Apply plot_PanelEstimate function in parallel\nres_est_plot <-\n    foreach(\n        i = 1:length(res_est),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est to res_est_plot\nnames(res_est_plot) <- names(res_est)\n\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev in parallel\nres_est_rev <-\n    foreach(i = 1:length(res_pm_rev), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm_rev[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm_rev to res_est_rev\nnames(res_est_rev) <- names(res_pm_rev)\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev in parallel\nres_est_plot_rev <-\n    foreach(\n        i = 1:length(res_est_rev),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est_rev to res_est_plot_rev\nnames(res_est_plot_rev) <- names(res_est_rev)\n\n# Stop the cluster\nstopCluster(cl)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <- c(\"Mahalanobis 5m\", \n                \"Mahalanobis 10m\", \n                \"PS Matching 5m\", \n                \"PS Matching 10m\", \n                \"PS Weighting 5m\")\n\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n  list(\n    nullGrob(),\n    textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[5], gp = gpar(fontsize = minor.axes.fontsize))\n  ),\n  \n  list(\n    textGrob(row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot$mahalanobis.1lag.5m,\n    res_est_plot$mahalanobis.1lag.10m,\n    res_est_plot$ps.match.1lag.5m,\n    res_est_plot$ps.match.1lag.10m,\n    res_est_plot$ps.weight.1lag.5m\n  ),\n  \n  list(\n    textGrob(row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot_rev$mahalanobis.1lag.5m,\n    res_est_plot_rev$mahalanobis.1lag.10m,\n    res_est_plot_rev$ps.match.1lag.5m,\n    res_est_plot_rev$ps.match.1lag.10m,\n    res_est_plot_rev$ps.weight.1lag.5m\n  )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n  grobs   = grobs,\n  ncol    = 6,\n  nrow    = 3,\n  widths  = c(0.1, 0.18, 0.18, 0.18, 0.18, 0.18),\n  heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n  \"Methods\",\n  x  = 0.5,\n  y  = 0.02,\n  gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n  \"\",\n  x   = 0.02,\n  y   = 0.5,\n  rot = 90,\n  gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"))"},{"path":"sec-difference-in-differences.html","id":"counterfactual-estimators","chapter":"30 Difference-in-Differences","heading":"30.8.5 Counterfactual Estimators","text":"Also known imputation approach (Liu, Wang, Xu 2024)class estimator consider observation treatment missing data. Models built using data control units impute conterfactuals treated observations.’s called counterfactual estimators predict outcomes treated observations received treatment.Advantages:\nAvoids negative weights biases using treated observations modeling applying uniform weights.\nSupports various models, including may relax strict exogeneity assumptions.\nAvoids negative weights biases using treated observations modeling applying uniform weights.Supports various models, including may relax strict exogeneity assumptions.Methods including\nFixed-effects conterfactual estimator (FEct) (special case):\nBased [Two-way Fixed-effects], assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\n\nInteractive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\n\nMatrix Completion Estimator (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\n\nSynthetic Control (case studies)\nFixed-effects conterfactual estimator (FEct) (special case):\nBased [Two-way Fixed-effects], assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\nBased [Two-way Fixed-effects], assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).Interactive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.Generalized Synthetic Controls subset IFEct treatments don’t revert.Matrix Completion Estimator (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.latent factors (unobservables) strong sparse, IFEct outperforms MC.Synthetic Control (case studies)Identifying Assumptions:Function Form: Additive separability observables, unobservables, idiosyncratic error term.\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).Strict Exogeneity: Conditional observables unobservables, potential outcomes independent treatment assignment (.e., baseline quasi-randomization)\n, unobservables = unit + time FEs, assumption parallel trends assumption\n, unobservables = unit + time FEs, assumption parallel trends assumptionLow-dimensional Decomposition (Feasibility Assumption): Unobservable effects can decomposed low-dimension.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.Estimation Procedure:Using control observations, estimate functions observable unobservable variables (relying Assumptions 1 3).Predict counterfactual outcomes treated unit using obtained functions.Calculate difference treatment effect treated individual.averaging treated individuals, can obtain Average Treatment Effect Treated (ATT).Notes:Use jackknife number treated units small (Liu, Wang, Xu 2024, 166).","code":""},{"path":"sec-difference-in-differences.html","id":"imputation-method","chapter":"30 Difference-in-Differences","heading":"30.8.5.0.1 Imputation Method","text":"Liu, Wang, Xu (2024) can also account treatment reversals heterogeneous treatment effects.imputation estimators include(Gardner 2022; Borusyak, Jaravel, Spiess 2021)(Gardner 2022; Borusyak, Jaravel, Spiess 2021)(N. Brown, Butts, Westerlund 2023)(N. Brown, Butts, Westerlund 2023)F-test \\(H_0\\): residual averages pre-treatment periods = 0To see treatment reversal effects","code":"\nlibrary(fect)\n\nPanelMatch::dem\n\nmodel.fect <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        parallel = TRUE,\n        seed = 1234,\n        # twfe\n        force = \"two-way\"\n    )\nprint(model.fect$est.avg)\n\nplot(model.fect)\n\nplot(model.fect, stats = \"F.p\")\nplot(model.fect, stats = \"F.p\", type = 'exit')"},{"path":"sec-difference-in-differences.html","id":"placebo-test","chapter":"30 Difference-in-Differences","heading":"30.8.5.0.2 Placebo Test","text":"selecting part data excluding observations within specified range improve model fitting, evaluate whether estimated Average Treatment Effect (ATT) within range significantly differs zero. approach helps us analyze periods treatment.test fails, either functional form strict exogeneity assumption problematic.","code":"\nout.fect.p <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        placeboTest = TRUE,\n        # using 3 periods\n        placebo.period = c(-2, 0)\n    )\nplot(out.fect.p, proportion = 0.1, stats = \"placebo.p\")"},{"path":"sec-difference-in-differences.html","id":"no-carryover-effects-test","chapter":"30 Difference-in-Differences","heading":"30.8.5.0.3 (No) Carryover Effects Test","text":"placebo test can adapted assess carryover effects masking several post-treatment periods instead pre-treatment ones. carryover effects present, average prediction error approximate zero. carryover test, set carryoverTest = TRUE. Specify post-treatment period range carryover.period exclude observations model fitting, evaluate estimated ATT significantly deviates zero.Even carryover effects, cases staggered adoption setting, researchers interested cumulative effects, aggregated treatment effects, ’s okay.evidence carryover effects.","code":"\nout.fect.c <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        carryoverTest = TRUE,\n        # how many periods of carryover\n        carryover.period = c(1, 3)\n    )\nplot(out.fect.c,  stats = \"carryover.p\")"},{"path":"sec-difference-in-differences.html","id":"sec-matrix-completion-estimator","chapter":"30 Difference-in-Differences","heading":"30.8.6 Matrix Completion Estimator","text":"Matrix completion methods become increasingly influential causal inference panel data, particularly estimating average treatment effects business settings marketing experiments, customer behavior modeling, pricing interventions. settings often feature staggered adoptionof treatments across units time, leading structured missing data. Athey et al. (2021) develop matrix completion framework subsumes methods based unconfoundedness synthetic controls, leveraging low-rank structure potential outcomes matrices.important empirical context consumer choice data marketing, missing outcomes can arise due intermittent treatment, e.g., promotional campaigns delivered varying times across different stores consumer segments. One illustrative application provided Bronnenberg, Dubé, Sanders (2020), investigates consumer response targeted marketing campaigns using panel data naturally contains missing counterfactual outcomes treated units.Two key literatures historically addressed problem imputing missing potential outcomes:Unconfoundedness Framework (G. W. Imbens Rubin 2015):\nAssumes selection observables.\nImputes missing control outcomes matching regression using untreated units similar characteristics histories.\nAssumes time patterns stable across units.\nAssumes selection observables.Imputes missing control outcomes matching regression using untreated units similar characteristics histories.Assumes time patterns stable across units.Synthetic Control (Abadie, Diamond, Hainmueller 2010):\nConstructs counterfactual outcomes weighted averages control units.\nAssumes unit patterns stable time.\nParticularly suited single treated unit settings.\nConstructs counterfactual outcomes weighted averages control units.Assumes unit patterns stable time.Particularly suited single treated unit settings.methods can unified matrix completion framework, interprets panel outcomes low-rank matrix plus noise, allowing flexible imputation without strong parametric assumptions.Contributions Athey et al. (2021)Accommodates structured missingness, including staggered adoption.Adjusts unit (\\(\\mu_i\\)) time (\\(\\lambda_t\\)) fixed effects prior low-rank estimation.Exhibits strong performance across unbalanced panels varying dimensions:\n\\(T \\gg N\\): unconfoundedness struggles.\n\\(N \\gg T\\): synthetic control performs poorly.\n\\(T \\gg N\\): unconfoundedness struggles.\\(N \\gg T\\): synthetic control performs poorly.Advantages Matrix CompletionUtilizes units periods, even treated ones, learn latent factors.Handles complex missingness patterns autocorrelated errors.Accommodates covariates heterogeneous treatment effects.Can apply weighted loss functions account non-random assignment missingness.","code":""},{"path":"sec-difference-in-differences.html","id":"matrix-completion-core-assumptions","chapter":"30 Difference-in-Differences","heading":"30.8.6.1 Matrix Completion Core Assumptions","text":"matrix completion approach built assumption complete outcome matrix \\(\\mathbf{Y}\\) satisfies:Low-rank structure: \\[\n\\mathbf{Y} = \\mathbf{U} \\mathbf{V}^T + \\mathbf{E}\n\\] \\(\\mathbf{U} \\\\mathbb{R}^{N \\times R}\\), \\(\\mathbf{V} \\\\mathbb{R}^{T \\times R}\\), \\(\\mathbf{E}\\) noise matrix.Missing Completely Random (MCAR): pattern missing data independent unobserved outcomes, conditional observables.Unlike prior approaches, matrix completion impose specific factorization, rather regularizes estimator, e.g., via nuclear norm minimization.identify causal estimand, matrix completion relies :SUTVA (Stable Unit Treatment Value Assumption): \\(Y_{}(w)\\) depends \\(W_{}\\), units’ treatments.SUTVA (Stable Unit Treatment Value Assumption): \\(Y_{}(w)\\) depends \\(W_{}\\), units’ treatments.dynamic treatment effects: treatment time \\(t\\) influence outcomes periods.dynamic treatment effects: treatment time \\(t\\) influence outcomes periods.","code":""},{"path":"sec-difference-in-differences.html","id":"causal-estimand","chapter":"30 Difference-in-Differences","heading":"30.8.6.2 Causal Estimand","text":"Let \\(Y_{}(0)\\) \\(Y_{}(1)\\) denote potential outcomes control treatment. observe treated outcomes, aim impute unobserved control outcomes:\\[\n\\tau = \\frac{\\sum_{(,t): W_{} = 1} \\left[ Y_{}(1) - Y_{}(0) \\right]}{\\sum_{,t} W_{}}\n\\]Let \\(\\mathcal{M}\\) set indices \\((, t)\\) \\(W_{} = 1\\) (treated, hence \\(Y_{}(0)\\) missing), \\(\\mathcal{O}\\) set \\(W_{} = 0\\) (control, hence \\(Y_{}(0)\\) observed).conceptualize data 2 \\(N \\times T\\) matrices:\\[\n\\mathbf{Y} =\n\\begin{pmatrix}\nY_{11} & Y_{12} & ? & \\cdots & Y_{1T} \\\\\n? & ? & Y_{23} & \\cdots & ? \\\\\nY_{31} & ? & Y_{33} & \\cdots & ? \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nY_{N1} & ? & Y_{N3} & \\cdots & ?\n\\end{pmatrix},\n\\quad\n\\mathbf{W} =\n\\begin{pmatrix}\n0 & 0 & 1 & \\cdots & 0 \\\\\n1 & 1 & 0 & \\cdots & 1 \\\\\n0 & 1 & 0 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 1 & 0 & \\cdots & 1\n\\end{pmatrix}\n\\]Matrix ShapeSpecial Patterns MissingnessBlock structures:\nSingle-period treatment (horizontal regression) (G. W. Imbens Rubin 2015)\nSingle-unit treatment (vertical regression) (Abadie, Diamond, Hainmueller 2010)\nSingle-period treatment (horizontal regression) (G. W. Imbens Rubin 2015)Single-unit treatment (vertical regression) (Abadie, Diamond, Hainmueller 2010)Staggered adoption: Treatments occur different times across units, many business interventions.","code":""},{"path":"sec-difference-in-differences.html","id":"unified-low-rank-model","chapter":"30 Difference-in-Differences","heading":"30.8.6.3 Unified Low-Rank Model","text":"Matrix completion generalizes approaches using low-rank plus noise model:\\[\n\\mathbf{Y} = \\mathbf{U} \\mathbf{V}^T + \\mathbf{E}\n\\]\\(R = \\text{rank}(\\mathbf{Y})\\) typically low relative \\(N\\) \\(T\\).TWFE assumes additivity: \\(\\mathbf{Y}_{} = \\mu_i + \\lambda_t + \\epsilon_{}\\).Interactive Fixed Effects use \\(R\\) factors: \\(\\mathbf{Y}_{} = \\sum_{r=1}^R \\alpha_{ir} \\gamma_{rt} + \\epsilon_{}\\). estimate number factors \\(R\\), see Bai Ng (2002) Moon Weidner (2015).Matrix Completion estimates \\(\\mathbf{Y}\\) via regularization, avoiding need explicitly choose \\(R\\).practical settings (e.g., marketing campaigns), ’s beneficial incorporate unit-level time-varying covariates:\\[\nY_{} = L_{} + \\sum_{p=1}^{P} \\sum_{q=1}^{Q} X_{ip} H_{pq} Z_{qt} + \\mu_i + \\lambda_t + V_{} \\beta + \\epsilon_{}\n\\]\\(X_{ip}\\): Unit covariates (matrix \\(p\\) variables unit \\(\\))\\(Z_{qt}\\): Time covariates (matrix \\(q\\) variables time \\(t\\))\\(V_{}\\): Time-varying covariates\\(H\\): Interaction effects. Lasso-type \\(l_1\\) norm (\\(||H|| = \\sum_{p = 1}^p \\sum_{q = 1}^Q |H_{pq}|\\)) used shrink \\(H \\0\\).several options regularize \\(L\\):Frobenius Norm(.e., Ridge)Nuclear Norm(.e., Lasso)","code":""},{"path":"sec-difference-in-differences.html","id":"sec-reshaped-inverse-probability-weighting-twfe-estimator","chapter":"30 Difference-in-Differences","heading":"30.8.7 Reshaped Inverse Probability Weighting - TWFE Estimator","text":"Reshaped Inverse Probability Weighting (RIPW) estimator extends classic TWFE regression framework account arbitrary, time- unit-varying treatment assignment mechanisms. approach leverages explicit model treatment assignment achieve design robustness, maintaining consistency even traditional fixed-effects outcome models misspecified.RIPW-TWFE framework particularly relevant panel data settings general treatment patternsstaggered adoptionstaggered adoptiontransient treatments.transient treatments.Setting NotationPanel data \\(n\\) units observed \\(T\\) time periods.Panel data \\(n\\) units observed \\(T\\) time periods.Potential outcomes: unit \\(\\\\{1, \\dots, n\\}\\) time \\(t \\\\{1, \\dots, T\\}\\):\n\\[\nY_{}(1), \\quad Y_{}(0)\n\\]Potential outcomes: unit \\(\\\\{1, \\dots, n\\}\\) time \\(t \\\\{1, \\dots, T\\}\\):\\[\nY_{}(1), \\quad Y_{}(0)\n\\]Observed outcomes:\n\\[\nY_{} = W_{} Y_{}(1) + (1 - W_{}) Y_{}(0)\n\\]Observed outcomes:\\[\nY_{} = W_{} Y_{}(1) + (1 - W_{}) Y_{}(0)\n\\]Treatment assignment path unit \\(\\):\n\\[\n\\mathbf{W}_i = (W_{i1}, \\dots, W_{}) \\\\{0,1\\}^T\n\\]Treatment assignment path unit \\(\\):\\[\n\\mathbf{W}_i = (W_{i1}, \\dots, W_{}) \\\\{0,1\\}^T\n\\]Generalized Propensity Score (GPS): unit \\(\\), probability distribution treatment paths:\n\\[\n\\mathbf{W}_i \\sim \\pi_i(\\cdot)\n\\]\n\\(\\pi_i(w)\\) known estimated.Generalized Propensity Score (GPS): unit \\(\\), probability distribution treatment paths:\\[\n\\mathbf{W}_i \\sim \\pi_i(\\cdot)\n\\]\\(\\pi_i(w)\\) known estimated.AssumptionsBinary Treatment: \\(W_{} \\\\{0,1\\}\\) \\(\\) \\(t\\).Binary Treatment: \\(W_{} \\\\{0,1\\}\\) \\(\\) \\(t\\).Dynamic Effects: Current outcomes depend current treatment, past treatments.Dynamic Effects: Current outcomes depend current treatment, past treatments.Overlap Condition (Assumption 2.2 Arkhangelsky et al. (2024)):\nexists subset \\(S^* \\subseteq \\{0,1\\}^T\\), \\(|S^*| \\ge 2\\) \\(S^* \\\\subseteq \\{0_T, 1_T\\}\\), :\n\\[\n\\pi_i(w) > c > 0, \\quad \\forall w \\S^*, \\forall \\\\{1, \\dots, n\\}\n\\]Overlap Condition (Assumption 2.2 Arkhangelsky et al. (2024)):exists subset \\(S^* \\subseteq \\{0,1\\}^T\\), \\(|S^*| \\ge 2\\) \\(S^* \\\\subseteq \\{0_T, 1_T\\}\\), :\\[\n\\pi_i(w) > c > 0, \\quad \\forall w \\S^*, \\forall \\\\{1, \\dots, n\\}\n\\]Maximal Correlation Decay (Assumption 2.1): Dependence units decays rate \\(n^{-q}\\) \\(q \\(0,1]\\).Maximal Correlation Decay (Assumption 2.1): Dependence units decays rate \\(n^{-q}\\) \\(q \\(0,1]\\).Bounded Second Moments (Assumption 2.3): \\(\\sup_{,t,w} \\mathbb{E}[Y_{}^2(w)] < M < \\infty\\).Bounded Second Moments (Assumption 2.3): \\(\\sup_{,t,w} \\mathbb{E}[Y_{}^2(w)] < M < \\infty\\).Key Quantities InterestUnit-Time Specific Treatment Effect:\n\\[\n\\tau_{} = Y_{}(1) - Y_{}(0)\n\\]Unit-Time Specific Treatment Effect:\\[\n\\tau_{} = Y_{}(1) - Y_{}(0)\n\\]Time-Specific Average Treatment Effect:\n\\[\n\\tau_t = \\frac{1}{n} \\sum_{=1}^n \\tau_{}\n\\]Time-Specific Average Treatment Effect:\\[\n\\tau_t = \\frac{1}{n} \\sum_{=1}^n \\tau_{}\n\\]Doubly Averaged Treatment Effect (DATE):\n\\[\n\\tau(\\xi) = \\sum_{t=1}^T \\xi_t \\tau_t = \\sum_{t=1}^T \\xi_t \\left( \\frac{1}{n} \\sum_{=1}^n \\tau_{} \\right)\n\\]\n\\(\\xi = (\\xi_1, \\dots, \\xi_T)\\) vector non-negative weights \\(\\sum_{t=1}^T \\xi_t = 1\\).Doubly Averaged Treatment Effect (DATE):\\[\n\\tau(\\xi) = \\sum_{t=1}^T \\xi_t \\tau_t = \\sum_{t=1}^T \\xi_t \\left( \\frac{1}{n} \\sum_{=1}^n \\tau_{} \\right)\n\\]\\(\\xi = (\\xi_1, \\dots, \\xi_T)\\) vector non-negative weights \\(\\sum_{t=1}^T \\xi_t = 1\\).Special Case: Equally weighted DATE:\n\\[\n\\tau_{\\text{eq}} = \\frac{1}{nT} \\sum_{t=1}^T \\sum_{=1}^n \\tau_{}\n\\]Special Case: Equally weighted DATE:\\[\n\\tau_{\\text{eq}} = \\frac{1}{nT} \\sum_{t=1}^T \\sum_{=1}^n \\tau_{}\n\\]Inverse Probability Weighting (IPW) methods widely used correct selection bias treatment assignment reweighting observations according probability receiving given treatment. panel data settings TWFE regression, IPW approach can incorporated address non-random treatment assignments time across units.begin classic TWFE regression objective, show IPW modifies , finally generalize Reshaped IPW (RIPW) estimator.unweighted TWFE estimator minimizes following objective function:\\[\n\\min_{\\tau, \\mu, \\{\\alpha_i\\}, \\{\\lambda_t\\}} \\sum_{=1}^{n} \\sum_{t=1}^{T} \\left( Y_{} - \\mu - \\alpha_i - \\lambda_t - W_{} \\tau \\right)^2\n\\]\\(n\\): Total number units (e.g., individuals, firms, regions).\\(T\\): Total number time periods.\\(Y_{}\\): Observed outcome unit \\(\\) time \\(t\\).\\(W_{}\\): Binary treatment indicator unit \\(\\) time \\(t\\).\n\\(W_{} = 1\\) unit \\(\\) treated time \\(t\\); \\(0\\) otherwise.\n\\(W_{} = 1\\) unit \\(\\) treated time \\(t\\); \\(0\\) otherwise.\\(\\tau\\): Parameter interest, representing Average Treatment Effect TWFE model.\\(\\mu\\): Common intercept, capturing overall average outcome level across units times.\\(\\alpha_i\\): Unit-specific fixed effects, controlling time-invariant heterogeneity across units.\\(\\lambda_t\\): Time-specific fixed effects, controlling shocks common trends affect units time period \\(t\\).standard TWFE regression assumes parallel trends across units absence treatment ignores treatment assignment mechanism.IPW-TWFE estimator modifies classic TWFE regression reweighting contribution observation according inverse probability entire treatment path unit \\(\\).weighted objective function :\\[\n\\min_{\\tau, \\mu, \\{\\alpha_i\\}, \\{\\lambda_t\\}} \\sum_{=1}^{n} \\sum_{t=1}^{T} \\left( Y_{} - \\mu - \\alpha_i - \\lambda_t - W_{} \\tau \\right)^2 \\cdot \\frac{1}{\\pi_i(\\mathbf{W}_i)}\n\\]\\(\\pi_i(\\mathbf{W}_i)\\): generalized propensity score (GPS) unit \\(\\).\njoint probability unit \\(\\) follows entire treatment assignment path \\(\\mathbf{W}_i = (W_{i1}, W_{i2}, \\dots, W_{})\\).\nrepresents assignment mechanism, may known (experimental designs) estimated (observational studies).\njoint probability unit \\(\\) follows entire treatment assignment path \\(\\mathbf{W}_i = (W_{i1}, W_{i2}, \\dots, W_{})\\).represents assignment mechanism, may known (experimental designs) estimated (observational studies).weighting squared residual unit-time observation \\(\\frac{1}{\\pi_i(\\mathbf{W}_i)}\\), IPW-TWFE estimator adjusts non-random treatment assignment, similar role IPW cross-sectional data.Reshaped IPW (RIPW) estimator generalizes IPW approach introducing user-specified reshaped design distribution, denoted \\(\\Pi\\), space treatment assignment paths.RIPW-TWFE estimator minimizes following weighted objective:\\[\n\\hat{\\tau}_{RIPW}(\\Pi) = \\arg \\min_{\\tau, \\mu, \\{\\alpha_i\\}, \\{\\lambda_t\\}} \\sum_{=1}^{n} \\sum_{t=1}^{T} \\left( Y_{} - \\mu - \\alpha_i - \\lambda_t - W_{} \\tau \\right)^2 \\cdot \\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)}\n\\]\\(\\Pi(\\mathbf{W}_i)\\): user-specified reshaped distribution treatment assignment paths \\(\\mathbf{W}_i\\).\ndescribes alternative “design” researcher wants emulate, possibly reflecting hypothetical target assignment mechanisms.\ndescribes alternative “design” researcher wants emulate, possibly reflecting hypothetical target assignment mechanisms.weight \\(\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)}\\) can interpreted likelihood ratio:\n\\(\\pi_i(\\cdot)\\) true assignment distribution, reweighting \\(\\Pi(\\cdot)\\) effectively shifts sampling design \\(\\pi_i\\) \\(\\Pi\\).\n\\(\\pi_i(\\cdot)\\) true assignment distribution, reweighting \\(\\Pi(\\cdot)\\) effectively shifts sampling design \\(\\pi_i\\) \\(\\Pi\\).ratio \\(\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)}\\) adjusts differences observed assignment mechanism target design.Support \\(\\mathbf{W}_i\\)support treatment assignment paths defined :\\[\n\\mathbb{S} = \\bigcup_{=1}^{n} \\text{Supp}(\\mathbf{W}_i)\n\\]\\(\\text{Supp}(\\mathbf{W}_i)\\): support random variable \\(\\mathbf{W}_i\\), .e., set treatment paths positive probability \\(\\pi_i(\\cdot)\\).\\(\\mathbb{S}\\) represents combined support across units \\(= 1, \\dots, n\\).\\(\\Pi(\\cdot)\\) support contained within \\(\\mathbb{S}\\), ensure valid reweighting.Special Cases RIPW EstimatorThe choice \\(\\Pi(\\cdot)\\) determines behavior interpretation RIPW estimator. Several special cases noteworthy:Uniform Reshaped Design:\n\\[\n\\Pi(\\cdot) \\sim \\text{Uniform}(\\mathbb{S})\n\\]\n, \\(\\Pi\\) places equal probability mass possible treatment path \\(\\mathbb{S}\\).\nweight becomes:\n\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = \\frac{1 / |\\mathbb{S}|}{\\pi_i(\\mathbf{W}_i)}\n\\]\nreduces RIPW standard IPW-TWFE estimator, target uniform treatment assignment design.\nUniform Reshaped Design:\\[\n\\Pi(\\cdot) \\sim \\text{Uniform}(\\mathbb{S})\n\\], \\(\\Pi\\) places equal probability mass possible treatment path \\(\\mathbb{S}\\)., \\(\\Pi\\) places equal probability mass possible treatment path \\(\\mathbb{S}\\).weight becomes:\n\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = \\frac{1 / |\\mathbb{S}|}{\\pi_i(\\mathbf{W}_i)}\n\\]weight becomes:\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = \\frac{1 / |\\mathbb{S}|}{\\pi_i(\\mathbf{W}_i)}\n\\]reduces RIPW standard IPW-TWFE estimator, target uniform treatment assignment design.reduces RIPW standard IPW-TWFE estimator, target uniform treatment assignment design.Reshaped Design Equals True Assignment:\n\\[\n\\Pi(\\cdot) = \\pi_i(\\cdot)\n\\]\nweight simplifies :\n\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = 1\n\\]\nRIPW estimator reduces unweighted TWFE regression, consistent experiment assignment mechanism \\(\\pi_i\\) known correctly specified.\nReshaped Design Equals True Assignment:\\[\n\\Pi(\\cdot) = \\pi_i(\\cdot)\n\\]weight simplifies :\n\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = 1\n\\]weight simplifies :\\[\n\\frac{\\Pi(\\mathbf{W}_i)}{\\pi_i(\\mathbf{W}_i)} = 1\n\\]RIPW estimator reduces unweighted TWFE regression, consistent experiment assignment mechanism \\(\\pi_i\\) known correctly specified.RIPW estimator reduces unweighted TWFE regression, consistent experiment assignment mechanism \\(\\pi_i\\) known correctly specified.ensure \\(\\hat{\\tau}_{RIPW}(\\Pi)\\) consistently estimates DATE \\(\\tau(\\xi)\\), solve DATE Equation:\\[\n\\mathbb{E}_{\\mathbf{W} \\sim \\Pi} \\left[ \\left( \\text{diag}(\\mathbf{W}) - \\xi \\mathbf{W}^\\top \\right) J \\left( \\mathbf{W} - \\mathbb{E}_{\\Pi}[\\mathbf{W}] \\right) \\right] = 0\n\\]\\(J = I_T - \\frac{1}{T} \\mathbf{1}_T \\mathbf{1}_T^\\top\\) projection matrix removing mean.Solving equation ensures consistency RIPW estimator \\(\\tau(\\xi)\\).Choosing Reshaped Distribution \\(\\Pi\\)support \\(\\mathbb{S}\\) \\(\\pi_i(\\cdot)\\) known, \\(\\Pi\\) can specified directly.Closed-form solutions \\(\\Pi\\) available settings staggered adoption designs.closed-form solutions unavailable, optimization algorithms (e.g., BFGS) can employed solve DATE equation numerically.PropertiesThe RIPW estimator provides design-robustness:\ncan correct misspecified outcome models properly reweighting according assignment mechanism.\naccommodates complex treatment assignment processes, staggered adoption non-random assignment.\ncan correct misspecified outcome models properly reweighting according assignment mechanism.accommodates complex treatment assignment processes, staggered adoption non-random assignment.flexibility choose \\(\\Pi(\\cdot)\\) allows researchers target estimands represent specific policy interventions hypothetical designs.RIPW estimator double robustness property:\\(\\hat{\\tau}_{RIPW}(\\Pi)\\) consistent either:\nassignment model \\(\\pi_i(\\cdot)\\) correctly specified \noutcome regression (TWFE) model correctly specified.\n\\(\\hat{\\tau}_{RIPW}(\\Pi)\\) consistent either:assignment model \\(\\pi_i(\\cdot)\\) correctly specified orThe assignment model \\(\\pi_i(\\cdot)\\) correctly specified orThe outcome regression (TWFE) model correctly specified.outcome regression (TWFE) model correctly specified.feature particularly valuable quasi-experimental designs parallel trends assumption may hold globally.Design-Robustness: RIPW corrects negative weighting issues identified TWFE literature (e.g., Goodman-Bacon (2021); Clement De Chaisemartin D’haultfœuille (2023)).Unlike conventional TWFE regressions, can yield biased estimands heterogeneity, RIPW explicitly targets user-specified weighted averages (DATE).randomized experiments, RIPW ensures effective estimand interpretable population-level average, determined design \\(\\Pi\\).","code":""},{"path":"sec-difference-in-differences.html","id":"gardner2022two-and-borusyak2024revisiting","chapter":"30 Difference-in-Differences","heading":"30.8.8 Gardner (2022) and Borusyak, Jaravel, and Spiess (2024)","text":"Estimate time unit fixed effects separatelyEstimate time unit fixed effects separatelyKnown imputation method (Borusyak, Jaravel, Spiess 2024) two-stage (Gardner 2022)Known imputation method (Borusyak, Jaravel, Spiess 2024) two-stage (Gardner 2022)Borusyak, Jaravel, Spiess (2024) didimputationThis version currently working","code":"\n# remotes::install_github(\"kylebutts/did2s\")\nlibrary(did2s)\nlibrary(ggplot2)\nlibrary(fixest)\nlibrary(tidyverse)\ndata(base_stagg)\n\n\nest <- did2s(\n    data = base_stagg |> mutate(treat = if_else(time_to_treatment >= 0, 1, 0)),\n    yname = \"y\",\n    first_stage = ~ x1 | id + year,\n    second_stage = ~ i(time_to_treatment, ref = c(-1,-1000)),\n    treatment = \"treat\" ,\n    cluster_var = \"id\"\n)\n\nfixest::esttable(est)\n#>                                       est\n#> Dependent Var.:                         y\n#>                                          \n#> time_to_treatment = -9  0.3518** (0.1332)\n#> time_to_treatment = -8  -0.3130* (0.1213)\n#> time_to_treatment = -7    0.0894 (0.2367)\n#> time_to_treatment = -6    0.0312 (0.2176)\n#> time_to_treatment = -5   -0.2079 (0.1519)\n#> time_to_treatment = -4   -0.1152 (0.1438)\n#> time_to_treatment = -3   -0.0127 (0.1483)\n#> time_to_treatment = -2    0.1503 (0.1440)\n#> time_to_treatment = 0  -5.139*** (0.3680)\n#> time_to_treatment = 1  -3.480*** (0.3784)\n#> time_to_treatment = 2  -2.021*** (0.3055)\n#> time_to_treatment = 3   -0.6965. (0.3947)\n#> time_to_treatment = 4    1.070** (0.3501)\n#> time_to_treatment = 5   2.173*** (0.4456)\n#> time_to_treatment = 6   4.449*** (0.3680)\n#> time_to_treatment = 7   4.864*** (0.3698)\n#> time_to_treatment = 8   6.187*** (0.2702)\n#> ______________________ __________________\n#> S.E. type                          Custom\n#> Observations                          950\n#> R2                                0.62486\n#> Adj. R2                           0.61843\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfixest::iplot(\n    est,\n    main = \"Event study\",\n    xlab = \"Time to treatment\",\n    ref.line = -1\n)\n\ncoefplot(est)\nmult_est <- did2s::event_study(\n    data = fixest::base_stagg |>\n        dplyr::mutate(year_treated = dplyr::if_else(year_treated == 10000, 0, year_treated)),\n    gname = \"year_treated\",\n    idname = \"id\",\n    tname = \"year\",\n    yname = \"y\",\n    estimator = \"all\"\n)\n#> Error in purrr::map(., function(y) { : ℹ In index: 1.\n#> ℹ With name: y.\n#> Caused by error in `.subset2()`:\n#> ! no such index at level 1\ndid2s::plot_event_study(mult_est)\nlibrary(didimputation)\nlibrary(fixest)\ndata(\"base_stagg\")\n\ndid_imputation(\n    data = base_stagg,\n    yname = \"y\",\n    gname = \"year_treated\",\n    tname = \"year\",\n    idname = \"id\"\n)"},{"path":"sec-difference-in-differences.html","id":"dynamic-treatment-effect-estimation-with-interactive-fixed-effects-and-short-panels","chapter":"30 Difference-in-Differences","heading":"30.8.9 Dynamic Treatment Effect Estimation with Interactive Fixed Effects and Short Panels","text":"N. L. Brown Butts (2025)","code":""},{"path":"sec-difference-in-differences.html","id":"sec-switching-difference-in-differences-estimator-de2020two","chapter":"30 Difference-in-Differences","heading":"30.8.10 Switching Difference-in-Differences Estimator (Clément De Chaisemartin and d’Haultfoeuille 2020)","text":"TWFE hinges restrictive assumptions — notably, homogeneity treatment effects across time groups. assumption violated, TWFE can yield misleading results, including estimates signs opposite underlying effects.consider standard panel data setup \\(G\\) groups \\(T\\) time periods. observation belongs group-period cell \\((g, t)\\), treatment \\(D_{g,t} \\\\{0,1\\}\\) assigned group-time level.Let \\(Y_{,g,t}\\) outcome individual \\(\\) group \\(g\\) time \\(t\\), potential outcomes \\(Y_{,g,t}(1)\\) \\(Y_{,g,t}(0)\\). Observed outcomes satisfy:\\[\nY_{,g,t} = D_{g,t} \\cdot Y_{,g,t}(1) + (1 - D_{g,t}) \\cdot Y_{,g,t}(0)\n\\]canonical TWFE regression :\\[\nY_{,g,t} = \\alpha_g + \\lambda_t + \\beta^{fe} D_{g,t} + \\varepsilon_{,g,t}\n\\]\\(\\alpha_g\\) group fixed effects, \\(\\lambda_t\\) time fixed effects, \\(\\beta^{fe}\\) interpreted treatment effect homogeneous treatment effects.","code":""},{"path":"sec-difference-in-differences.html","id":"heterogeneous-treatment-effects-and-weighting-bias","chapter":"30 Difference-in-Differences","heading":"30.8.10.1 Heterogeneous Treatment Effects and Weighting Bias","text":"treatment effects vary across \\((g,t)\\) cells, TWFE estimator \\(\\hat{\\beta}^{fe}\\) longer simple average. Instead, can decomposed weighted sum cell-specific average treatment effects:\\[\n\\beta^{fe} = \\mathbb{E} \\left[ \\sum_{(g,t): D_{g,t}=1} w_{g,t} \\Delta_{g,t} \\right]\n\\]:\\(\\Delta_{g,t} = \\mathbb{E}[Y_{g,t}(1) - Y_{g,t}(0)]\\) average treatment effect cell \\((g,t)\\)\\(w_{g,t}\\) weights can negative sum one.weights negative TWFE implicitly compares outcomes across treated untreated groups, even “controls” treated. comparisons can subtract treatment effects, leading negative weights.","code":""},{"path":"sec-difference-in-differences.html","id":"illustration-negative-weights-can-flip-signs","chapter":"30 Difference-in-Differences","heading":"30.8.10.2 Illustration: Negative Weights Can Flip Signs","text":"example, suppose:Group 1 treated period 3: \\(\\Delta_{1,3} = 1\\)Group 2 treated periods 2 3:\n\\(\\Delta_{2,2} = 1\\)\n\\(\\Delta_{2,3} = 4\\)\n\\(\\Delta_{2,2} = 1\\)\\(\\Delta_{2,3} = 4\\)TWFE produces:\\[\n\\beta^{fe} = \\frac{1}{2} \\Delta_{1,3} + \\Delta_{2,2} - \\frac{1}{2} \\Delta_{2,3} = \\frac{1}{2} + 1 - 2 = -0.5\n\\]\\(\\Delta_{g,t}\\)s positive, TWFE estimate negative.assess extent TWFE may misleading, compute robustness ratio:\\[\n\\sigma^{fe}_\\_ = \\frac{|\\hat{\\beta}^{fe}|}{\\hat{\\sigma}(w)}\n\\]:\\(\\hat{\\sigma}(w)\\) standard deviation TWFE weights across treated cells.small \\(\\sigma^{fe}_\\_\\) indicates minor heterogeneity can reverse sign estimate.can estimated directly data helps determine whether TWFE reliable context.","code":""},{"path":"sec-difference-in-differences.html","id":"did_m-estimator-a-robust-alternative","chapter":"30 Difference-in-Differences","heading":"30.8.10.3 DID_M Estimator: A Robust Alternative","text":"Clément De Chaisemartin d’Haultfoeuille (2020) propose DID_M estimator, valid heterogeneous treatment effects. focuses switching groups, using non-switchers controls local difference--differences design.Let \\(S\\) denote set \\((g,t)\\) cells treatment status changes \\(t-1\\) \\(t\\). DID_M estimator :\\[\n\\text{}_M = \\sum_{t=2}^{T} \\left( \\frac{N_{1,0,t}}{N_S} \\cdot \\text{}^+_t + \\frac{N_{0,1,t}}{N_S} \\cdot \\text{}^-_t \\right)\n\\]:\\(\\text{}^+_t\\) compares joiners stable untreated groups\\(\\text{}^-_t\\) compares leavers stable treated groups\\(N_S\\) total number observations switching cellsDID_M requires:Common trends treated untreated potential outcomesExistence stable groups every \\(t\\) (.e., groups don’t change treatment status)Ashenfelter dip (treatment triggered negative shocks)assumptions weaker required TWFE unbiased.Clément De Chaisemartin d’Haultfoeuille (2020) also propose placebo version DID_M using pre-treatment periods. pre-treatment differences exist switchers non-switchers, indicates violation parallel trends assumption. test analogous pre-trend checks event-study designs.Estimate TWFE ModelDecompose Weights TwoWayFEWeightsEstimate DID_M (Switching Estimator) DIDmultiplegt","code":"\n# Load required packages\nlibrary(fixest)            # For TWFE model and dataset\nlibrary(TwoWayFEWeights)   # For decomposing TWFE weights\nlibrary(DIDmultiplegt)     # For robust SDID estimator\n\n# Load the sample staggered adoption dataset\ndata(\"base_stagg\")\n\n# Preview the data\nhead(base_stagg)\n#>   id year year_treated time_to_treatment treated treatment_effect_true\n#> 2 90    1            2                -1       1                     0\n#> 3 89    1            3                -2       1                     0\n#> 4 88    1            4                -3       1                     0\n#> 5 87    1            5                -4       1                     0\n#> 6 86    1            6                -5       1                     0\n#> 7 85    1            7                -6       1                     0\n#>           x1           y\n#> 2 -1.0947021  0.01722971\n#> 3 -3.7100676 -4.58084528\n#> 4  2.5274402  2.73817174\n#> 5 -0.7204263 -0.65103066\n#> 6 -3.6711678 -5.33381664\n#> 7 -0.3152137  0.49562631\n# Run standard TWFE using fixest\ntwfe <- feols(y ~ treatment | id + year,\n                    data = base_stagg |>\n                        dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)))\nsummary(twfe)\n#> OLS estimation, Dep. Var.: y\n#> Observations: 950\n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>           Estimate Std. Error t value   Pr(>|t|)    \n#> treatment -3.46761    0.47672 -7.2739 1.0326e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 2.471     Adj. R2: 0.272265\n#>               Within R2: 0.111912\ntwfe_weights <- twowayfeweights(\n    data = base_stagg |> dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n    Y = \"y\",\n    G = \"year_treated\",\n    T = \"year\",\n    D = \"treatment\", \n    summary_measures = T\n)\n\n# Show summary\ntwfe_weights\n#> \n#> Under the common trends assumption,\n#> the TWFE coefficient beta, equal to -3.4676, estimates a weighted sum of 45 ATTs.\n#> 41 ATTs receive a positive weight, and 4 receive a negative weight.\n#> \n#> ────────────────────────────────────────── \n#> Treat. var: treatment    ATTs    Σ weights \n#> ────────────────────────────────────────── \n#> Positive weights           41       1.0238 \n#> Negative weights            4      -0.0238 \n#> ────────────────────────────────────────── \n#> Total                      45            1 \n#> ──────────────────────────────────────────\n#> \n#> Summary Measures:\n#>   TWFE Coefficient (β_fe): -3.4676\n#>   min σ(Δ) compatible with β_fe and Δ_TR = 0: 4.8357\n#>   min σ(Δ) compatible with treatment effect of opposite sign than β_fe in all (g,t) cells: 36.1549\n#>   Reference: Corollary 1, de Chaisemartin, C and D'Haultfoeuille, X (2020a)\n#> \n#> The development of this package was funded by the European Union (ERC, REALLYCREDIBLE,GA N. 101043899).\n# Estimate robust SDID estimator (DID_M)\ndid_m <- did_multiplegt(\n        mode = \"dyn\",\n        df = base_stagg |>\n            dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n        outcome = \"y\",\n        group = \"year_treated\",\n        time = \"year\",\n        treatment = \"treatment\",\n        effects = 5,\n        # controls = c(\"x1\"),\n        placebo = 2\n    )\n\nsummary(did_m)\n#> \n#> ----------------------------------------------------------------------\n#>        Estimation of treatment effects: Event-study effects\n#> ----------------------------------------------------------------------\n#>              Estimate SE      LB CI    UB CI    N  Switchers\n#> Effect_1     -5.04943 0.03256 -5.11324 -4.98562 54 9        \n#> Effect_2     -3.25734 0.06712 -3.38889 -3.12579 44 8        \n#> Effect_3     -2.17826 0.08866 -2.35204 -2.00449 35 7        \n#> Effect_4     -0.03749 0.12841 -0.28916 0.21418  27 6        \n#> Effect_5     1.31986  0.12128 1.08216  1.55756  20 5        \n#> \n#> Test of joint nullity of the effects : p-value = 0.0000\n#> ----------------------------------------------------------------------\n#>     Average cumulative (total) effect per treatment unit\n#> ----------------------------------------------------------------------\n#>  Estimate        SE     LB CI     UB CI         N Switchers \n#>  -2.29649   0.07360  -2.44074  -2.15223        80        35 \n#> Average number of time periods over which a treatment effect is accumulated: 2.7143\n#> \n#> ----------------------------------------------------------------------\n#>      Testing the parallel trends and no anticipation assumptions\n#> ----------------------------------------------------------------------\n#>              Estimate SE      LB CI    UB CI    N  Switchers\n#> Placebo_1    0.27204  0.04917 0.17567  0.36841  44 8        \n#> Placebo_2    -0.72910 0.06790 -0.86219 -0.59601 27 6        \n#> \n#> Test of joint nullity of the placebos : p-value = 0.0000\n#> \n#> \n#> The development of this package was funded by the European Union.\n#> ERC REALLYCREDIBLE - GA N. 101043899"},{"path":"sec-difference-in-differences.html","id":"augmentedforward-did","chapter":"30 Difference-in-Differences","heading":"30.8.11 Augmented/Forward DID","text":"Methods Limited Pre-Treatment Periods:Augmented (K. T. Li Van den Bulte 2023)Forward (K. T. Li 2024)","code":""},{"path":"sec-difference-in-differences.html","id":"doubly-robust-difference-in-differences-estimators","chapter":"30 Difference-in-Differences","heading":"30.8.12 Doubly Robust Difference-in-Differences Estimators","text":"simplest “canonical” form, compares --outcomes one group eventually receives treatment (“treated” group) --outcomes another group never receives treatment (“comparison” group). parallel trends assumption, can recover average treatment effect treated (ATT).Practitioners often enrich analyses conditioning observed covariates mitigate violations unconditional parallel trends assumption. conditioning covariates, framework remains attractive, provided conditional parallel trends hold.Historically, two main approaches emerged estimation presence covariates:Outcome Regression () (J. J. Heckman, Ichimura, Todd 1997). Model outcome evolution comparison group (possibly treated group), plug fitted outcome equations formula.Inverse Probability Weighting (IPW) (Abadie 2005). Model probability treatment conditional covariates (“propensity score”) use inverse-probability reweighting reconstruct appropriate counterfactuals treated group.key insight semiparametric causal inference one can combine two approaches—modeling outcome regression propensity score tandem—form estimator remains consistent either outcome-regression equations correctly specified propensity-score equation correctly specified. estimator called doubly robust (Sant’Anna Zhao 2020).section covers cases () panel data (observe unit pre- post-treatment periods) (ii) repeated cross-section data (, period, observe new random sample units).suitable conditions, proposed doubly robust estimators exhibit desirable robustness properties misspecification can also attain semiparametric efficiency bound, making locally efficient working models correct.","code":""},{"path":"sec-difference-in-differences.html","id":"notation-and-set-up","chapter":"30 Difference-in-Differences","heading":"30.8.12.1 Notation and set-up","text":"Two-time-period design. consider setting two time periods: \\(t = 0\\) (pre-treatment) \\(t = 1\\) (post-treatment). subset units receives treatment \\(t = 1\\). Hence unit \\(\\):\n\\(D_i = D_{i1} \\\\{0, 1\\}\\) indicator receiving treatment time 1 (\\(D_{i0} = 0\\) \\(\\)).\n\\(Y_{}\\) observed outcome time \\(t\\).\nTwo-time-period design. consider setting two time periods: \\(t = 0\\) (pre-treatment) \\(t = 1\\) (post-treatment). subset units receives treatment \\(t = 1\\). Hence unit \\(\\):\\(D_i = D_{i1} \\\\{0, 1\\}\\) indicator receiving treatment time 1 (\\(D_{i0} = 0\\) \\(\\)).\\(Y_{}\\) observed outcome time \\(t\\).Potential outcomes. adopt potential outcomes framework. Let \\[\n   Y_{}(1) \\;=\\; \\text{potential outcome unit }\\text{ time } t \\text{ treated,}\n\\] \\[\n   Y_{}(0) \\;=\\; \\text{potential outcome unit }\\text{ time } t \\text{ treated.}\n\\] observed outcome satisfies \\(Y_{} = D_i \\, Y_{}(1) + (1-D_i)\\, Y_{}(0)\\).Potential outcomes. adopt potential outcomes framework. Let \\[\n   Y_{}(1) \\;=\\; \\text{potential outcome unit }\\text{ time } t \\text{ treated,}\n\\] \\[\n   Y_{}(0) \\;=\\; \\text{potential outcome unit }\\text{ time } t \\text{ treated.}\n\\] observed outcome satisfies \\(Y_{} = D_i \\, Y_{}(1) + (1-D_i)\\, Y_{}(0)\\).Covariates. vector observed pre-treatment covariates denoted \\(X_i\\). Throughout, assume first component \\(X_i\\) constant (intercept).Covariates. vector observed pre-treatment covariates denoted \\(X_i\\). Throughout, assume first component \\(X_i\\) constant (intercept).Data structures.\nPanel data. observe \\(\\{(Y_{i0}, Y_{i1}, D_i, X_i)\\}_{=1}^n\\), sample size \\(n\\) drawn ..d. underlying population.\nRepeated cross-sections. period \\(t\\), observe fresh random sample units. Let \\(T_i\\\\{0,1\\}\\) indicator whether observation drawn post-treatment period \\((T_i=1)\\) pre-treatment period \\((T_i=0)\\). Write \\(\\{(Y_i, D_i, X_i, T_i)\\}_{=1}^n\\). , \\(T_i=1\\), \\(Y_i \\equiv Y_{i1}\\); \\(T_i=0\\), \\(Y_i \\equiv Y_{i0}\\). typically assume stationarity condition, namely distribution \\((D, X)\\) stable across two periods.\nData structures.Panel data. observe \\(\\{(Y_{i0}, Y_{i1}, D_i, X_i)\\}_{=1}^n\\), sample size \\(n\\) drawn ..d. underlying population.Repeated cross-sections. period \\(t\\), observe fresh random sample units. Let \\(T_i\\\\{0,1\\}\\) indicator whether observation drawn post-treatment period \\((T_i=1)\\) pre-treatment period \\((T_i=0)\\). Write \\(\\{(Y_i, D_i, X_i, T_i)\\}_{=1}^n\\). , \\(T_i=1\\), \\(Y_i \\equiv Y_{i1}\\); \\(T_i=0\\), \\(Y_i \\equiv Y_{i0}\\). typically assume stationarity condition, namely distribution \\((D, X)\\) stable across two periods.Let \\(n_1\\) \\(n_0\\) respective sample sizes post- pre-treatment periods, \\(n_1 + n_0 = n\\). Often let \\(\\lambda = P(T=1)\\approx n_1/n.\\)focus ATT: \\[\n    \\tau \\;=\\; \\mathbb{E}\\bigl[Y_{i1}(1) - Y_{i1}(0)\\,\\big|\\; D_i=1\\bigr].\n\\] observe \\(Y_{i1}(1)\\) treated group, central challenge recover \\(\\mathbb{E}[Y_{i1}(0)\\mid D_i=1]\\). standard assumptions, identify \\(\\tau\\) comparing treated group’s evolution outcomes comparison group’s evolution outcomes.require two key assumptions:Conditional parallel trends\n\\(t=0,1\\), let \\[\n    \\mathbb{E}[\\,Y_{1}(0) - Y_{0}(0)\\,\\mid D=1,\\, X]\n    \\;=\\;\n    \\mathbb{E}[\\,Y_{1}(0) - Y_{0}(0)\\,\\mid D=0,\\, X].\n\\] means —conditional \\(X\\)—absence treatment, treated comparison groups parallel outcome evolutions.Conditional parallel trends\n\\(t=0,1\\), let \\[\n    \\mathbb{E}[\\,Y_{1}(0) - Y_{0}(0)\\,\\mid D=1,\\, X]\n    \\;=\\;\n    \\mathbb{E}[\\,Y_{1}(0) - Y_{0}(0)\\,\\mid D=0,\\, X].\n\\] means —conditional \\(X\\)—absence treatment, treated comparison groups parallel outcome evolutions.Overlap\nexists \\(\\varepsilon>0\\) \\[\n  P(D=1)\\;>\\;\\varepsilon,\n  \\quad\\text{}\\quad\n  P(D=1\\,\\vert\\,X)\\;\\le\\;1-\\varepsilon\n\\] , require nontrivial fraction population treated, \\(X\\), nontrivial probability untreated group (\\(D=0\\)).Overlap\nexists \\(\\varepsilon>0\\) \\[\n  P(D=1)\\;>\\;\\varepsilon,\n  \\quad\\text{}\\quad\n  P(D=1\\,\\vert\\,X)\\;\\le\\;1-\\varepsilon\n\\] , require nontrivial fraction population treated, \\(X\\), nontrivial probability untreated group (\\(D=0\\)).assumptions, can identify \\(\\mathbb{E}[\\,Y_{1}(0)\\mid D=1]\\) semiparametric fashion, either modeling outcome regressions (approach) modeling propensity score (IPW approach).","code":""},{"path":"sec-difference-in-differences.html","id":"two-traditional-did-approaches","chapter":"30 Difference-in-Differences","heading":"30.8.12.2 Two Traditional DID Approaches","text":"briefly outline classical estimators rely solely either outcome regressions inverse probability weighting, motivate doubly robust idea.","code":""},{"path":"sec-difference-in-differences.html","id":"outcome-regression-or-approach","chapter":"30 Difference-in-Differences","heading":"30.8.12.2.1 Outcome-regression (OR) approach","text":"Define \\[\n   m_{d,t}(x) \\;=\\; \\mathbb{E}[\\,Y_t \\,\\mid\\,D=d,\\; X=x\\,].\n\\] conditional parallel trends assumption, \\[\n  \\mathbb{E}[\\,Y_{1}(0)\\mid D=1 \\,]\n  \\;=\\;\n   \\mathbb{E}[\\,Y_{0}(0)\\mid D=1\\,]\n   \\;+\\;\n   \\mathbb{E}[\\,m_{0,1}(X) - m_{0,0}(X)\\,\\big\\vert\\,D=1\\,].\n\\] Hence -based estimator (panel repeated cross-section) typically looks like \\[\n  \\hat{\\tau}^{\\mathrm{reg}}\n  \\;=\\;\n  \\Bigl(\\overline{Y}_{1,1}\\Bigr)\n  \\;-\\;\n  \\Bigl(\\,\\overline{Y}_{1,0}\n        \\;+\\;\n        \\frac{1}{n_{\\mathrm{treat}}}\n        \\sum_{:D_i=1}\n         \\bigl[\\,\n             \\hat{m}_{0,1}(X_i) \\;-\\; \\hat{m}_{0,0}(X_i)\n         \\bigr]\n   \\Bigr),\n\\]\\(\\overline{Y}_{d,t}\\) sample mean \\(Y_t\\) among units \\(D=d\\), \\(\\hat{m}_{0,t}\\) fitted model (e.g., linear semiparametric) \\(\\mathbb{E}[\\,Y_t \\mid D=0,\\,X\\,]\\).estimator consistent () correctly specified two outcome-regression functions \\(m_{0,1}(x)\\) \\(m_{0,0}(x)\\). regressions misspecified, estimator generally biased.","code":""},{"path":"sec-difference-in-differences.html","id":"ipw-approach","chapter":"30 Difference-in-Differences","heading":"30.8.12.2.2 IPW approach","text":"alternative model propensity score \\[\n   p(x)\\;=\\; P(D=1\\mid X=x),\n\\] use Horvitz–Thompson-type reweighting (Horvitz Thompson 1952) reconstruct “happened” treated group treatment. panel-data case, Abadie (2005) show ATT can identified via \\[\n  \\tau\n  \\;=\\;\n  \\frac{1}{\\mathbb{E}[D]}\n  \\,\\mathbb{E}\\Bigl[\\,\n    \\frac{D - p(X)}{1 - p(X)}\n    \\Bigl(Y_1 - Y_0\\Bigr)\n  \\Bigr].\n\\] Hence IPW estimator panel data \\[\n  \\hat{\\tau}^{\\mathrm{ipw,p}}\n  \\;=\\;\n  \\frac{1}{\\overline{D}}\n  \\sum_{=1}^n\n    \\Bigl[\n       \\frac{D_i - \\hat{p}(X_i)}{1-\\hat{p}(X_i)}\n    \\Bigr]\n    \\,\\frac{1}{n}\\bigl(Y_{i1} - Y_{i0}\\bigr),\n\\] \\(\\hat{p}(\\cdot)\\) fitted propensity score model. Similar expressions exist repeated cross-sections, small modifications handle fact observe \\(Y_1\\) \\(T=1\\), etc.IPW estimator consistent () propensity score correctly specified, .e., \\(\\hat{p}(x)\\approx p(x)\\). propensity-score model incorrect, estimator may severely biased.","code":""},{"path":"sec-difference-in-differences.html","id":"doubly-robust-did-main-identification","chapter":"30 Difference-in-Differences","heading":"30.8.12.3 Doubly Robust DID: Main Identification","text":"doubly robust (DR) idea combine IPW approaches resulting estimator consistent either model correct propensity-score model correct. Formally, consider two generic “working” models:\\(\\pi(X)\\) \\(p(X)\\), .e., model propensity score,\\(\\pi(X)\\) \\(p(X)\\), .e., model propensity score,\\(\\mu_{0,t}(X)\\) outcome regressions \\(m_{0,t}(X)=\\mathbb{E}[Y_t \\mid D=0,X]\\).\\(\\mu_{0,t}(X)\\) outcome regressions \\(m_{0,t}(X)=\\mathbb{E}[Y_t \\mid D=0,X]\\).define two “DR moments” data structure.","code":""},{"path":"sec-difference-in-differences.html","id":"dr-estimand-for-panel-data","chapter":"30 Difference-in-Differences","heading":"30.8.12.3.1 DR estimand for panel data","text":"panel data available, define \\[\n   \\Delta Y \\;=\\; Y_1 \\,-\\, Y_0,\n   \\quad\n   \\mu_{0,\\Delta}(X)\\;=\\;\\mu_{0,1}(X)\\;-\\;\\mu_{0,0}(X).\n\\] DR moment \\(\\tau\\) : \\[\n   \\tau^{\\mathrm{dr,p}}\n   \\;=\\;\n   \\mathbb{E}\\Bigl[\n    \\Bigl(\\,w_{1}^{\\mathrm{p}}(D)\\,-\\,w_{0}^{\\mathrm{p}}(D,X;\\,\\pi)\\Bigr)\\,\n    \\Bigl(\\,\\Delta Y \\;-\\;\\mu_{0,\\Delta}(X)\\Bigr)\n   \\Bigr],\n\\] \\[\n   w_{1}^{\\mathrm{p}}(D)\\;=\\;\\frac{D}{\\mathbb{E}[D]},\n   \\quad\\quad\n   w_{0}^{\\mathrm{p}}(D,X;\\,\\pi)\\;=\\;\\frac{\\pi(X)\\,(1-D)}{(1-\\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)}{1-\\pi(X)}\\bigr]}.\n\\] can shown \\(\\tau^{\\mathrm{dr,p}} = \\tau\\) provided either \\(\\pi(x)=p(x)\\) almost surely (.s.) \\(\\mu_{0,\\Delta}(x)=m_{0,\\Delta}(x)\\) .s. (latter meaning least regression comparison group correct).","code":""},{"path":"sec-difference-in-differences.html","id":"dr-estimands-for-repeated-cross-sections","chapter":"30 Difference-in-Differences","heading":"30.8.12.3.2 DR estimands for repeated cross-sections","text":"repeated cross-sections, DR construction must account fact \\(Y_0, Y_1\\) observed jointly individuals. Let \\(\\lambda = P(T=1)\\). two valid DR estimands :\\[\n  \\tau^{\\mathrm{dr,rc}}_{1}\n  \\;=\\;\n  \\mathbb{E}\\Bigl[\n   \\bigl(\\,w_{1}^{\\mathrm{rc}}(D,T)\\,-\\,w_{0}^{\\mathrm{rc}}(D,T,X;\\,\\pi)\\bigr)\\,\n   \\bigl(Y \\;-\\;\\mu_{0,Y}(T,X)\\bigr)\n  \\Bigr],\n\\] \\[\n  w_{1}^{\\mathrm{rc}}(D,T)\n  \\;=\\;\n  \\frac{D\\,1\\{T=1\\}}\n       {\\mathbb{E}[D\\,1\\{T=1\\}]}\n  \\;-\\;\n  \\frac{D\\,1\\{T=0\\}}\n       {\\mathbb{E}[D\\,1\\{T=0\\}]},\n\\] \\[\n  w_{0}^{\\mathrm{rc}}(D,T,X;\\,\\pi)\n  \\;=\\;\n  \\frac{\\pi(X)\\,(1-D)\\,1\\{T=1\\}}{(1-\\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)\\,1\\{T=1\\}}{(1-\\pi(X))}\\bigr]}\n  \\;-\\;\n  \\frac{\\pi(X)\\,(1-D)\\,1\\{T=0\\}}{(1-\\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)\\,1\\{T=0\\}}{(1-\\pi(X))}\\bigr]},\n\\] \\(\\mu_{0,Y}(T,X)=T\\cdot\\mu_{0,1}(X)+(1-T)\\cdot\\mu_{0,0}(X)\\).\\[\n  \\tau^{\\mathrm{dr,rc}}_{1}\n  \\;=\\;\n  \\mathbb{E}\\Bigl[\n   \\bigl(\\,w_{1}^{\\mathrm{rc}}(D,T)\\,-\\,w_{0}^{\\mathrm{rc}}(D,T,X;\\,\\pi)\\bigr)\\,\n   \\bigl(Y \\;-\\;\\mu_{0,Y}(T,X)\\bigr)\n  \\Bigr],\n\\] \\[\n  w_{1}^{\\mathrm{rc}}(D,T)\n  \\;=\\;\n  \\frac{D\\,1\\{T=1\\}}\n       {\\mathbb{E}[D\\,1\\{T=1\\}]}\n  \\;-\\;\n  \\frac{D\\,1\\{T=0\\}}\n       {\\mathbb{E}[D\\,1\\{T=0\\}]},\n\\] \\[\n  w_{0}^{\\mathrm{rc}}(D,T,X;\\,\\pi)\n  \\;=\\;\n  \\frac{\\pi(X)\\,(1-D)\\,1\\{T=1\\}}{(1-\\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)\\,1\\{T=1\\}}{(1-\\pi(X))}\\bigr]}\n  \\;-\\;\n  \\frac{\\pi(X)\\,(1-D)\\,1\\{T=0\\}}{(1-\\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)\\,1\\{T=0\\}}{(1-\\pi(X))}\\bigr]},\n\\] \\(\\mu_{0,Y}(T,X)=T\\cdot\\mu_{0,1}(X)+(1-T)\\cdot\\mu_{0,0}(X)\\).\\[\n\\tau^{\\mathrm{dr,rc}}_{2} \\;=\\; \\tau^{\\mathrm{dr,rc}}_{1}\\;+\\;\\Bigl[    \\mathbb{E}\\bigl(\\mu_{1,1}(X) - \\mu_{0,1}(X)\\,\\big|\\,D=1\\bigr)    \\;-\\;    \\mathbb{E}\\bigl(\\mu_{1,1}(X) - \\mu_{0,1}(X)\\,\\big|\\,D=1,\\,T=1\\bigr)    \\Bigr]\\\\\\;-\\;\\Bigl[    \\mathbb{E}\\bigl(\\mu_{1,0}(X) - \\mu_{0,0}(X)\\,\\big|\\,D=1\\bigr)    \\;-\\;    \\mathbb{E}\\bigl(\\mu_{1,0}(X) - \\mu_{0,0}(X)\\,\\big|\\,D=1,\\,T=0\\bigr)    \\Bigr]\n\\] \\(\\mu_{d,t}(x)\\) model \\(m_{d,t}(x)=\\mathbb{E}[Y \\mid D=d,\\,T=t,\\,X=x]\\).\\[\n\\tau^{\\mathrm{dr,rc}}_{2} \\;=\\; \\tau^{\\mathrm{dr,rc}}_{1}\\;+\\;\\Bigl[    \\mathbb{E}\\bigl(\\mu_{1,1}(X) - \\mu_{0,1}(X)\\,\\big|\\,D=1\\bigr)    \\;-\\;    \\mathbb{E}\\bigl(\\mu_{1,1}(X) - \\mu_{0,1}(X)\\,\\big|\\,D=1,\\,T=1\\bigr)    \\Bigr]\\\\\\;-\\;\\Bigl[    \\mathbb{E}\\bigl(\\mu_{1,0}(X) - \\mu_{0,0}(X)\\,\\big|\\,D=1\\bigr)    \\;-\\;    \\mathbb{E}\\bigl(\\mu_{1,0}(X) - \\mu_{0,0}(X)\\,\\big|\\,D=1,\\,T=0\\bigr)    \\Bigr]\n\\] \\(\\mu_{d,t}(x)\\) model \\(m_{d,t}(x)=\\mathbb{E}[Y \\mid D=d,\\,T=t,\\,X=x]\\).One can show \\(\\tau^{\\mathrm{dr,rc}}_1 = \\tau^{\\mathrm{dr,rc}}_2 = \\tau\\) long stationarity \\((D,X)\\) across time holds either propensity score model \\(\\pi(x)=p(x)\\) correct comparison-group outcome regressions correct (Sant’Anna Zhao 2020). Notably, \\(\\tau^{\\mathrm{dr,rc}}_2\\) also includes explicit modeling treated group’s outcomes. However, terms identification alone, \\(\\tau^{\\mathrm{dr,rc}}_1\\) \\(\\tau^{\\mathrm{dr,rc}}_2\\) share double-robust property.","code":""},{"path":"sec-difference-in-differences.html","id":"semiparametric-efficiency-bounds-and-local-efficiency","chapter":"30 Difference-in-Differences","heading":"30.8.12.4 Semiparametric Efficiency Bounds and Local Efficiency","text":"important concept semiparametric inference semiparametric efficiency bound, infimum asymptotic variance across regular estimators exploit imposed assumptions (parallel trends, overlap, stationarity). Equivalently, one can think variance “efficient influence function” (EIF).highlight key results:Efficient influence function panel dataUnder mentioned assumptions (..d. data generating process, overlap, conditional parallel trends) without imposing parametric constraints \\((m_{d,t},p)\\), one can derive EIF \\(\\tau\\) panel-data setting \\[\n\\eta^{e,\\mathrm{p}}(Y_1, Y_0, D, X) \\;=\\;\\frac{D}{\\mathbb{E}[D]}\\Bigl[m_{1,\\Delta}(X) - m_{0,\\Delta}(X) - \\tau\\Bigr]\\\\\\;+\\;\\frac{D}{\\mathbb{E}[D]}\\Bigl[\\Delta Y - m_{1,\\Delta}(X)\\Bigr]\\\\\\;-\\;\\frac{\\pi(X)\\,(1-D)}{(1 - \\pi(X))\\,\\mathbb{E}\\bigl[\\tfrac{\\pi(X)\\,(1-D)}{1 - \\pi(X)}\\bigr]}\\Bigl[\\Delta Y - m_{0,\\Delta}(X)\\Bigr]\n\\]\\(\\Delta Y = Y_1 - Y_0\\) \\(m_{d,\\Delta}(X) \\equiv m_{d,1}(X) - m_{d,0}(X)\\).associated semiparametric efficiency bound \\[\n   \\mathbb{E}\\bigl[\\,\\eta^{e,\\mathrm{p}}(Y_1,Y_0,D,X)^2\\bigr].\n\\] can shown DR estimator can attain bound (1) propensity score correctly modeled, (2) comparison-group outcome regressions correctly modeled.Efficient influence function repeated cross-sectionsSimilarly, repeated cross-sections available, EIF becomes\\[\n\\eta^{e,\\mathrm{rc}}(Y,D,T,X) \\;=\\;\\frac{D}{\\mathbb{E}[D]}\\Bigl[m_{1,\\Delta}(X) - m_{0,\\Delta}(X) - \\tau\\Bigr]\\\\\\;+\\;\\Bigl[    w_{1,1}^{\\mathrm{rc}}(D,T)\\bigl(Y - m_{1,1}(X)\\bigr)    \\;-\\;    w_{1,0}^{\\mathrm{rc}}(D,T)\\bigl(Y - m_{1,0}(X)\\bigr)    \\Bigr]\\\\\\;-\\;\\Bigl[    w_{0,1}^{\\mathrm{rc}}(D,T,X;p)\\bigl(Y - m_{0,1}(X)\\bigr)    \\;-\\;    w_{0,0}^{\\mathrm{rc}}(D,T,X;p)\\bigl(Y - m_{0,0}(X)\\bigr)    \\Bigr]\n\\]\\(\\,m_{d,\\Delta}(X)=m_{d,1}(X)-m_{d,0}(X)\\). resulting efficiency bound \\(\\mathbb{E}\\bigl[\\eta^{e,\\mathrm{rc}}(Y,D,T,X)^2\\bigr]\\).One can show access panel data can strictly informative (.e., yields smaller semiparametric variance bound) repeated cross-sections. difference can grow pre- post-treatment samples highly unbalanced.","code":""},{"path":"sec-difference-in-differences.html","id":"construction-of-doubly-robust-did-estimators","chapter":"30 Difference-in-Differences","heading":"30.8.12.5 Construction of Doubly Robust DID Estimators","text":"Generic two-step approachBuilding upon DR moment expressions, natural approach estimation :First-stage modeling (nuisance parameters).\nEstimate \\(\\hat{\\pi}(X)\\) propensity score, e.g. via logistic regression parametric semiparametric methods.\nEstimate \\(\\hat{m}_{0,t}(X)\\) \\(t=0,1\\). One might also estimate \\(\\hat{m}_{1,t}(X)\\) using second DR estimator repeated cross-sections.\nEstimate \\(\\hat{\\pi}(X)\\) propensity score, e.g. via logistic regression parametric semiparametric methods.Estimate \\(\\hat{m}_{0,t}(X)\\) \\(t=0,1\\). One might also estimate \\(\\hat{m}_{1,t}(X)\\) using second DR estimator repeated cross-sections.Plug DR moment.\nReplace \\(p\\) \\(\\hat{\\pi}\\) \\(m_{d,t}\\) \\(\\hat{m}_{d,t}\\) chosen DR formula (panel repeated cross-sections).estimators DR, either propensity score well specified outcome regressions comparison group well specified, consistency assured.Improving efficiency inference: special parametric choicesIt sometimes desirable construct DR estimators also “DR inference,” meaning asymptotic variance depend portion model correct. Achieving typically requires carefully choosing first-stage estimators extra “estimation effect” vanishes influence-function calculations. Concretely:Propensity score: Use logistic regression (special inverse probability tilting estimator) proposed Graham, Xavier Pinto, Egel (2012).Outcome regressions: Use linear regressions specific weights (OLS treated group).One obtains:panel data: \\[\n    \\hat{\\tau}^{\\mathrm{dr,p}}_{\\mathrm{imp}}\n    \\;=\\;\n    \\frac1n \\sum_{=1}^n\n      \\Bigl[\n         w_{1}^{\\mathrm{p}}(D_i) \\;-\\; w_{0}^{\\mathrm{p}}\\bigl(D_i,X_i;\\,\\hat{\\gamma}^{\\mathrm{ipt}}\\bigr)\n      \\Bigr]\n      \\Bigl[\n        \\bigl(Y_{i1}-Y_{i0}\\bigr)\n        \\;-\\;\n        \\hat{\\mu}^{\\mathrm{lin,p}}_{0,\\Delta}\\bigl(X_i;\\,\\hat{\\beta}^{\\mathrm{wls,p}}_{0,\\Delta}\\bigr)\n      \\Bigr],\n  \\] \\(\\hat{\\gamma}^{\\mathrm{ipt}}\\) “inverse probability tilting” estimate logit propensity score, \\(\\hat{\\beta}^{\\mathrm{wls,p}}_{0,\\Delta}\\) weighted least squares estimate difference regressions comparison group. suitable regularity conditions, estimator:\nRemains consistent either logit model linear outcome model \\(\\Delta Y\\) control group correct.\nasymptotic distribution depend model correct (thus simplifying inference).\nAchieves semiparametric efficiency bound models correct.\npanel data: \\[\n    \\hat{\\tau}^{\\mathrm{dr,p}}_{\\mathrm{imp}}\n    \\;=\\;\n    \\frac1n \\sum_{=1}^n\n      \\Bigl[\n         w_{1}^{\\mathrm{p}}(D_i) \\;-\\; w_{0}^{\\mathrm{p}}\\bigl(D_i,X_i;\\,\\hat{\\gamma}^{\\mathrm{ipt}}\\bigr)\n      \\Bigr]\n      \\Bigl[\n        \\bigl(Y_{i1}-Y_{i0}\\bigr)\n        \\;-\\;\n        \\hat{\\mu}^{\\mathrm{lin,p}}_{0,\\Delta}\\bigl(X_i;\\,\\hat{\\beta}^{\\mathrm{wls,p}}_{0,\\Delta}\\bigr)\n      \\Bigr],\n  \\] \\(\\hat{\\gamma}^{\\mathrm{ipt}}\\) “inverse probability tilting” estimate logit propensity score, \\(\\hat{\\beta}^{\\mathrm{wls,p}}_{0,\\Delta}\\) weighted least squares estimate difference regressions comparison group. suitable regularity conditions, estimator:Remains consistent either logit model linear outcome model \\(\\Delta Y\\) control group correct.Remains consistent either logit model linear outcome model \\(\\Delta Y\\) control group correct.asymptotic distribution depend model correct (thus simplifying inference).asymptotic distribution depend model correct (thus simplifying inference).Achieves semiparametric efficiency bound models correct.Achieves semiparametric efficiency bound models correct.repeated cross-sections, one can analogously use logistic-based tilting propensity score weighted/ordinary least squares control/treated outcome regressions. second DR estimator \\(\\hat{\\tau}^{\\mathrm{dr,rc}}_{2}\\) models treated group’s outcomes well can, correct specification models, achieve local efficiency.","code":""},{"path":"sec-difference-in-differences.html","id":"large-sample-properties-2","chapter":"30 Difference-in-Differences","heading":"30.8.12.6 Large-Sample Properties","text":"Assume mild regularity conditions consistency asymptotic normality (e.g., overlapping support, identifiability pseudo-true parameters first-step models, suitable rates convergence) (Sant’Anna Zhao 2020).Double Robust Consistency.\nproposed DR estimator consistent \\(\\tau\\) either () \\(\\hat{p}(X)\\) consistent \\(p(X)\\), (b) relevant outcome regressions \\(\\hat{m}_{0,t}(X)\\) consistent \\(m_{0,t}(X)\\). Thus, say estimator doubly robust misspecification.Double Robust Consistency.\nproposed DR estimator consistent \\(\\tau\\) either () \\(\\hat{p}(X)\\) consistent \\(p(X)\\), (b) relevant outcome regressions \\(\\hat{m}_{0,t}(X)\\) consistent \\(m_{0,t}(X)\\). Thus, say estimator doubly robust misspecification.Asymptotic Normality.\\[\n    \\sqrt{n}\\bigl(\\,\\hat{\\tau}^{\\mathrm{dr}} \\,-\\, \\tau \\bigr)\n    \\;\\;\\xrightarrow{d}\\;\\;\n    N\\bigl(\\,0,\\;\\mathrm{Var}(\\text{})\\bigr),\n\\] \\(\\mathrm{Var}(\\text{})\\) depends part(s) nuisance models consistently estimated. general, one must account variance contribution first-stage estimation. certain special constructions (“improved DR” approaches inverse probability tilting specialized weighting), first-stage contribute additional variance, making inference simpler.Asymptotic Normality.\\[\n    \\sqrt{n}\\bigl(\\,\\hat{\\tau}^{\\mathrm{dr}} \\,-\\, \\tau \\bigr)\n    \\;\\;\\xrightarrow{d}\\;\\;\n    N\\bigl(\\,0,\\;\\mathrm{Var}(\\text{})\\bigr),\n\\] \\(\\mathrm{Var}(\\text{})\\) depends part(s) nuisance models consistently estimated. general, one must account variance contribution first-stage estimation. certain special constructions (“improved DR” approaches inverse probability tilting specialized weighting), first-stage contribute additional variance, making inference simpler.Local Semiparametric Efficiency.\npropensity score model outcome-regression models correct, estimator’s influence function matches efficient influence function, hence achieving semiparametric efficiency bound. repeated cross-sections, DR estimator also models treated group’s outcomes (namely \\(\\tau^{\\mathrm{dr,rc}}_2\\)) one can achieve local efficiency.Local Semiparametric Efficiency.\npropensity score model outcome-regression models correct, estimator’s influence function matches efficient influence function, hence achieving semiparametric efficiency bound. repeated cross-sections, DR estimator also models treated group’s outcomes (namely \\(\\tau^{\\mathrm{dr,rc}}_2\\)) one can achieve local efficiency.","code":""},{"path":"sec-difference-in-differences.html","id":"practical-implementation","chapter":"30 Difference-in-Differences","heading":"30.8.12.7 Practical Implementation","text":"practice, recommended workflow :Specify (estimate) flexible working model propensity score. logistic regression inverse-probability-tilting approach often good default, simplifies subsequent steps one wants DR inference.Model outcome comparison group time. panel data, one can directly model \\(\\Delta Y\\). repeated cross-sections, one typically models \\(\\{m_{0,t}(X)\\}_{t=0,1}\\).(Optional) Model outcome treated group seeking local-efficiency version DR repeated cross-sections.Form DR estimator plugging fitted models steps (1)–(3) chosen DR moment expression.Inference can often carried taking empirical variance estimated influence function: \\[\n   \\hat{V} \\;=\\; \\frac{1}{n}\\sum_{=1}^n \\hat{\\eta}_i^2,\n\\] \\(\\hat{\\eta}_i\\) evaluator’s best estimate influence function observation \\(\\). certain “improved DR” constructions, \\(\\hat{\\eta}_i\\) works regardless part model correct.","code":"\ndata('base_stagg')\n\nlibrary(did)\n\ndrdid_result <- att_gt(\n    yname = \"y\",\n    tname = \"year\",\n    idname = \"id\",\n    gname = \"year_treated\",\n    xformla = ~ x1,\n    data = base_stagg\n)\n\n\naggte(drdid_result, type = \"simple\")\n#> \n#> Call:\n#> aggte(MP = drdid_result, type = \"simple\")\n#> \n#> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n#> \n#> \n#>      ATT    Std. Error     [ 95%  Conf. Int.] \n#>  -0.8636        0.5869     -2.014      0.2867 \n#> \n#> \n#> ---\n#> Signif. codes: `*' confidence band does not cover 0\n#> \n#> Control Group:  Never Treated,  Anticipation Periods:  0\n#> Estimation Method:  Doubly Robust\nagg.es <- aggte(drdid_result, type = \"dynamic\")\n\nggdid(agg.es)\n\nagg.gs <- aggte(drdid_result, type = \"group\")\n\nggdid(agg.gs)"},{"path":"sec-difference-in-differences.html","id":"nonlinear-difference-in-differences","chapter":"30 Difference-in-Differences","heading":"30.8.13 Nonlinear Difference-in-Differences","text":"Traditional Difference--Differences methods typically rely linear models strong assumptions like constant treatment effects homogeneous trends across treatment groups. assumptions often fail real-world data — especially outcomes binary, fractional, counts, :Employment status (binary),Proportion customers churned (fraction),Number crimes neighborhood (count).cases, linear parallel trends assumption may inappropriate. section develops advanced, flexible framework nonlinear estimation staggered interventions (Wooldridge 2023).","code":""},{"path":"sec-difference-in-differences.html","id":"overview-of-framework","chapter":"30 Difference-in-Differences","heading":"30.8.13.1 Overview of Framework","text":"consider panel dataset units observed \\(T\\) time periods. Units become treated various times (staggered rollout), goal estimate Average Treatment Effect Treated (ATT) different times.Let \\(Y_{}(g)\\) denote potential outcome time \\(t\\) unit \\(\\) first treated period \\(g\\), \\(g = q, \\ldots, T\\) \\(g = \\infty\\) (never treated). Define ATT cohort \\(g\\) time \\(r \\geq g\\) :\\[\n\\tau_{gr} = \\mathbb{E}\\left[Y_{ir}(g) - Y_{ir}(\\infty) \\mid D_g = 1\\right]\n\\], \\(D_g = 1\\) indicates unit \\(\\) first treated period \\(g\\).Rather assuming linear conditional expectations untreated outcomes, posit nonlinear conditional mean using known, strictly increasing function \\(G(\\cdot)\\):\\[\n\\mathbb{E}[Y_{}(0) \\mid D, X] = G\\left( \\alpha + \\sum_{g=q}^{T} \\beta_g D_g + X \\kappa + \\sum_{g=q}^{T} (D_g \\cdot X)\\eta_g + \\gamma_t + X \\pi_t \\right)\n\\]formulation nests logistic Poisson mean structures, allows us handle various limited dependent variables.","code":""},{"path":"sec-difference-in-differences.html","id":"assumptions-2","chapter":"30 Difference-in-Differences","heading":"30.8.13.2 Assumptions","text":"require following identification assumptions:Conditional Anticipation: \\[\n\\mathbb{E}[Y_{}(g) \\mid D_g = 1, X] = \\mathbb{E}[Y_{}(\\infty) \\mid D_g = 1, X], \\quad \\forall t < g\n\\]Conditional Anticipation: \\[\n\\mathbb{E}[Y_{}(g) \\mid D_g = 1, X] = \\mathbb{E}[Y_{}(\\infty) \\mid D_g = 1, X], \\quad \\forall t < g\n\\]Conditional Index Parallel Trends: untreated mean trends parallel transformed index space: \\[\n\\mathbb{E}[Y_{}(\\infty) \\mid D, X] = G(\\text{linear index } D, X, t)\n\\]\n\\(G(\\cdot)\\) known, strictly increasing function (e.g., \\(\\exp(\\cdot)\\) Poisson)\nConditional Index Parallel Trends: untreated mean trends parallel transformed index space: \\[\n\\mathbb{E}[Y_{}(\\infty) \\mid D, X] = G(\\text{linear index } D, X, t)\n\\]\\(G(\\cdot)\\) known, strictly increasing function (e.g., \\(\\exp(\\cdot)\\) Poisson)assumptions weaker realistic linear Parallel Trends, especially outcomes constrained.","code":""},{"path":"sec-difference-in-differences.html","id":"estimation-2","chapter":"30 Difference-in-Differences","heading":"30.8.13.3 Estimation","text":"Step 1: Imputation EstimatorEstimate Parameters Using Untreated Observations :Use \\((,t)\\) unit \\(\\) untreated \\(t\\) (.e., \\(W_{} = 0\\)). Fit nonlinear regression model: \\[ Y_{} = G\\left(\\alpha + \\sum_g \\beta_g D_g + X_i \\kappa + D_g X_i \\eta_g + \\gamma_t + X_i \\pi_t\\right) + \\varepsilon_{} \\]Impute Counterfactual Outcomes Treated Observations:treated observations \\((,t)\\) \\(W_{}=1\\), predict \\(\\widehat{Y}_{}(0)\\) using model Step 1.Compute ATT Cohort \\(g\\) Time \\(r\\):\\[ \\hat{\\tau}_{gr} = \\frac{1}{N_{gr}} \\sum_{: D_g=1} \\left( Y_{ir} - \\widehat{Y}_{ir}(0) \\right) \\]Step 2: Pooled QMLE Estimator (Equivalent Using Canonical Link)Fit Model Using Observations:Fit pooled nonlinear model across units time: \\[ Y_{} = G\\left(\\alpha + \\sum_g \\beta_g D_g + X_i \\kappa + D_g X_i \\eta_g + \\gamma_t + X_i \\pi_t + \\delta_r \\cdot W_{} + W_{} X_i \\xi \\right) + \\varepsilon_{} \\]:\\(W_{} = 1\\) unit \\(\\) treated time \\(t\\)\\(W_{} = 1\\) unit \\(\\) treated time \\(t\\)\\(W_{} = 0\\) otherwise\\(W_{} = 0\\) otherwiseEstimate \\(\\delta_r\\) ATT cohort \\(g\\) period \\(r\\):\\(\\delta_r\\) interpreted event-time ATTThis estimator consistent \\(G^{-1}(\\cdot)\\) canonical link (e.g., log link Poisson)Average Partial Effect (APE) ATT:\\[ \\hat{\\tau}_{gr} = \\frac{1}{N_g} \\sum_{: D_g=1} \\left[ G\\left( X_i\\beta + \\delta_r + \\ldots \\right) - G\\left( X_i\\beta + \\ldots \\right) \\right] \\]Canonical Links Practice","code":""},{"path":"sec-difference-in-differences.html","id":"inference-2","chapter":"30 Difference-in-Differences","heading":"30.8.13.4 Inference","text":"Standard errors can obtained via delta method bootstrapCluster-robust standard errors unit preferredWhen using QMLE, estimates valid correct mean specification, regardless higher momentsWhen Imputation Pooled Methods Match?numerically identical :\nEstimating canonical link function\nModel correctly specified\ndata used (.e., W_it = 0 pooled)\nEstimating canonical link functionModel correctly specifiedSame data used (.e., W_it = 0 pooled)","code":""},{"path":"sec-difference-in-differences.html","id":"application-using-etwfe","chapter":"30 Difference-in-Differences","heading":"30.8.13.5 Application Using etwfe","text":"etwfe package provides unified, user-friendly interface estimating staggered treatment effects using generalized linear models. particularly well-suited nonlinear outcomes, binary, fractional, count data.’ll now demonstrate apply etwfe estimate Average Treatment Effect Treated (ATT) nonlinear framework using Poisson model. aligns exponential conditional mean assumption discussed earlier.Install load packagesSimulate known data-generating processImagine multi-period business panel “unit” regional store branch large retail chain. Half stores eventually receive new marketing analytics platform known time, principle changes performance metric (e.g., weekly log sales). half never receive platform, functioning “never-treated” control group.\\(N=200\\) stores (half eventually treated, half never treated).\\(N=200\\) stores (half eventually treated, half never treated).store observed \\(T=10\\) time periods (e.g., quarters years).store observed \\(T=10\\) time periods (e.g., quarters years).true “treatment effect treated” constant \\(\\delta = -0.05\\) post-treatment times. (Interpretation: new marketing platform reduced log-sales 5 percent, though real life one might expect positive effect!)true “treatment effect treated” constant \\(\\delta = -0.05\\) post-treatment times. (Interpretation: new marketing platform reduced log-sales 5 percent, though real life one might expect positive effect!)stores “staggered” sense adopt different periods. ’ll randomly draw adoption date \\(\\{4,5,6\\}\\). Others never adopt .stores “staggered” sense adopt different periods. ’ll randomly draw adoption date \\(\\{4,5,6\\}\\). Others never adopt .include store-level intercepts, time intercepts, idiosyncratic noise make realistic.include store-level intercepts, time intercepts, idiosyncratic noise make realistic.business setting, can imagine logY natural log revenue, sales, another KPI, xvar log local population, number competitor stores region, similar.Estimate etwfeWe want test whether new marketing analytics platform changed log outcome. use etwfe:fml = logY ~ xvar says logY outcome, xvar control.fml = logY ~ xvar says logY outcome, xvar control.tvar = time time variable.tvar = time time variable.gvar = adopt_time group/cohort variable (“first treatment time” 0 never).gvar = adopt_time group/cohort variable (“first treatment time” 0 never).vcov = ~id clusters standard errors store level.vcov = ~id clusters standard errors store level.cgroup = \"never\": specify never-treated units form comparison group. ensures can see pre-treatment post-treatment dynamic effects event-study plot.cgroup = \"never\": specify never-treated units form comparison group. ensures can see pre-treatment post-treatment dynamic effects event-study plot.Nothing fancy appear raw coefficient list ’s fully “saturated” interactions. real prize aggregated treatment effects, ’ll obtain next.Recover ATTHere’s single-number estimate overall average effect treated, across times cohorts:see estimate near true \\(-0.05\\).Recover event-study pattern dynamic effectsTo check pre- post-treatment dynamics, ask event study via type = \"event\". shows outcome evolves around adoption time. Negative “event” values correspond pre-treatment, nonnegative “event” values post-treatment.Event studyBy default, return events (roughly) earliest pre-treatment period maximum possible post-treatment period data, using never-treated comparison group.default, return events (roughly) earliest pre-treatment period maximum possible post-treatment period data, using never-treated comparison group.Inspect estimates confidence intervals. Ideally, pre-treatment estimates near 0, post-treatment estimates near \\(-0.05\\).Inspect estimates confidence intervals. Ideally, pre-treatment estimates near 0, post-treatment estimates near \\(-0.05\\).Plot estimated event-study vs. true effectIn business marketing study, useful final step chart showing point estimates (confidence bands) plus known true effect reference.Construct “true” dynamic effect curvePre-treatment periods: effect = 0Pre-treatment periods: effect = 0Post-treatment periods: effect = \\(\\delta=-0.05\\)Post-treatment periods: effect = \\(\\delta=-0.05\\):Extract estimated event effects mod_es.Extract estimated event effects mod_es.Build reference dataset event times.Build reference dataset event times.Plot figure.Plot figure.Solid line shaded region: ETWFE point estimates 95% confidence intervals, event time relative adoption.Solid line shaded region: ETWFE point estimates 95% confidence intervals, event time relative adoption.Dashed red line: true effect built DGP.Dashed red line: true effect built DGP.estimation works well (sample big enough), estimated event-study effects hover near dashed red line post-treatment, near zero pre-treatment.Alternatively, also plot function produce quick plot.Double-check regression table (optional)like see clean numeric summary dynamic estimates period, can pipe event-study object modelsummary:","code":"\n# --- 1) Load packages ---\n# install.packages(\"fixest\")\n# install.packages(\"marginaleffects\")\n# install.packages(\"etwfe\")\n# install.packages(\"ggplot2\")\n# install.packages(\"modelsummary\")\n\nlibrary(etwfe)\nlibrary(fixest)\nlibrary(marginaleffects)\nlibrary(ggplot2)\nlibrary(modelsummary)\nset.seed(12345)\n# --- 2) Simulate Data ---\nN <- 200   # number of stores\nT <- 10    # number of time periods\nid   <- rep(1:N, each = T)\ntime <- rep(1:T, times = N)\n\n# Mark half of them as eventually treated, half never\ntreated_ids <- sample(1:N, size = N/2, replace = FALSE)\nis_treated  <- id %in% treated_ids\n\n# Among the treated, pick an adoption time 4,5, or 6 at random\nadopt_time_vec <- sample(c(4,5,6), size = length(treated_ids), replace = TRUE)\nadopt_time     <- rep(0, N) # 0 means \"never\"\nadopt_time[treated_ids] <- adopt_time_vec\n\n# Store effects, time effects, control variable, noise\nalpha_i <- rnorm(N, mean = 2, sd = 0.5)[id]\ngamma_t <- rnorm(T, mean = 0, sd = 0.2)[time]\nxvar    <- rnorm(N*T, mean = 1, sd = 0.3)\nbeta    <- 0.10\nnoise   <- rnorm(N*T, mean = 0, sd = 0.1)\n\n# True treatment effect = -0.05 for time >= adopt_time\ntrue_ATT <- -0.05\nD_it     <- as.numeric((adopt_time[id] != 0) & (time >= adopt_time[id]))\n\n# Final outcome in logs:\ny <- alpha_i + gamma_t + beta*xvar + true_ATT*D_it + noise\n\n# Put it all in a data frame\nsimdat <- data.frame(\n    id         = id,\n    time       = time,\n    adopt_time = adopt_time[id],\n    treat      = D_it,\n    xvar       = xvar,\n    logY       = y\n)\n\nhead(simdat)\n#>   id time adopt_time treat      xvar     logY\n#> 1  1    1          6     0 1.4024608 1.317343\n#> 2  1    2          6     0 0.5226395 2.273805\n#> 3  1    3          6     0 1.3357914 1.517705\n#> 4  1    4          6     0 1.2101680 1.759481\n#> 5  1    5          6     0 0.9953143 1.771928\n#> 6  1    6          6     1 0.8893066 1.439206\n# --- 3) Estimate with etwfe ---\nmod <- etwfe(\n    fml    = logY ~ xvar,\n    tvar   = time,\n    gvar   = adopt_time,\n    data   = simdat,\n    # xvar   = moderator, # Heterogenous Treatment Effects\n    vcov   = ~id,\n    cgroup = \"never\"  # so that never-treated are the baseline\n)\n# --- 4) Single-number ATT ---\nATT_est <- emfx(mod, type = \"simple\")\nprint(ATT_est)\n#> \n#>  .Dtreat Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 %\n#>     TRUE  -0.0707     0.0178 -3.97   <0.001 13.8 -0.106 -0.0358\n#> \n#> Term: .Dtreat\n#> Type:  response \n#> Comparison: TRUE - FALSE\n# --- 5) Event-study estimates ---\nmod_es <- emfx(mod, type = \"event\")\nmod_es\n#> \n#>  event Estimate Std. Error      z Pr(>|z|)    S   2.5 %   97.5 %\n#>     -5 -0.04132     0.0467 -0.885  0.37628  1.4 -0.1329  0.05022\n#>     -4 -0.01120     0.0282 -0.396  0.69180  0.5 -0.0666  0.04416\n#>     -3  0.01747     0.0226  0.772  0.43999  1.2 -0.0269  0.06180\n#>     -2 -0.00912     0.0193 -0.472  0.63686  0.7 -0.0470  0.02873\n#>     -1  0.00000         NA     NA       NA   NA      NA       NA\n#>      0 -0.08223     0.0206 -3.995  < 0.001 13.9 -0.1226 -0.04188\n#>      1 -0.06108     0.0209 -2.926  0.00344  8.2 -0.1020 -0.02016\n#>      2 -0.07094     0.0225 -3.159  0.00158  9.3 -0.1150 -0.02692\n#>      3 -0.07383     0.0189 -3.906  < 0.001 13.4 -0.1109 -0.03679\n#>      4 -0.07330     0.0334 -2.196  0.02808  5.2 -0.1387 -0.00788\n#>      5 -0.06527     0.0285 -2.294  0.02178  5.5 -0.1210 -0.00951\n#>      6 -0.05661     0.0402 -1.407  0.15953  2.6 -0.1355  0.02227\n#> \n#> Term: .Dtreat\n#> Type:  response \n#> Comparison: TRUE - FALSE\n\n\n# Renaming function to replace \".Dtreat\" with something more meaningful\nrename_fn = function(old_names) {\n  new_names = gsub(\".Dtreat\", \"Period post treatment =\", old_names)\n  setNames(new_names, old_names)\n}\n\nmodelsummary(\n  list(mod_es),\n  shape       = term:event:statistic ~ model,\n  coef_rename = rename_fn,\n  gof_omit    = \"Adj|Within|IC|RMSE\",\n  stars       = TRUE,\n  title       = \"Event study\",\n  notes       = \"Std. errors are clustered at the id level\"\n)\n# --- 6) Plot results vs. known effect ---\nest_df <- as.data.frame(mod_es)\n\nrange_of_event <- range(est_df$event)\nevent_breaks   <- seq(range_of_event[1], range_of_event[2], by = 1)\ntrue_fun <- function(e) ifelse(e < 0, 0, -0.05)\nevent_grid <- seq(range_of_event[1], range_of_event[2], by = 1)\ntrue_df <- data.frame(\n    event       = event_grid,\n    true_effect = sapply(event_grid, true_fun)\n)\n\nggplot() +\n    # Confidence interval ribbon (put it first so it's behind everything)\n    geom_ribbon(\n        data = est_df,\n        aes(x = event, ymin = conf.low, ymax = conf.high),\n        fill = \"grey60\",   # light gray fill\n        alpha = 0.3\n    ) +\n    # Estimated effect line\n    geom_line(\n        data = est_df,\n        aes(x = event, y = estimate),\n        color = \"black\",\n        size = 1\n    ) +\n    # Estimated effect points\n    geom_point(\n        data = est_df,\n        aes(x = event, y = estimate),\n        color = \"black\",\n        size = 2\n    ) +\n    # Known true effect (dashed red line)\n    geom_line(\n        data = true_df,\n        aes(x = event, y = true_effect),\n        color = \"red\",\n        linetype = \"dashed\",\n        linewidth = 1\n    ) +\n    # Horizontal zero line\n    geom_hline(yintercept = 0, linetype = \"dotted\") +\n    # Vertical line at event = 0 for clarity\n    geom_vline(xintercept = 0, color = \"gray40\", linetype = \"dashed\") +\n    # Make sure x-axis breaks are integers\n    scale_x_continuous(breaks = event_breaks) +\n    labs(\n        title = \"Event-Study Plot vs. Known True Effect\",\n        subtitle = \"Business simulation with new marketing platform adoption\",\n        x = \"Event time (periods relative to adoption)\",\n        y = \"Effect on log-outcome (ATT)\",\n        caption = \"Dashed red line = known true effect; Shaded area = 95% CI\"\n    ) +\n    causalverse::ama_theme()\nplot(\n    mod_es,\n    type = \"ribbon\",\n    # col  = \"\",# color\n    xlab = \"\",\n    main = \"\",\n    sub  = \"\",\n    # file = \"event-study.png\", width = 8, height = 5. # save file\n)\n# --- 7) Optional table for dynamic estimates ---\nmodelsummary(\n    list(\"Event-Study\" = mod_es),\n    shape     = term + statistic ~ model + event,\n    gof_map   = NA,\n    coef_map  = c(\".Dtreat\" = \"ATT\"),\n    title     = \"ETWFE Event-Study by Relative Adoption Period\",\n    notes     = \"Std. errors are clustered by store ID\"\n)"},{"path":"sec-difference-in-differences.html","id":"multiple-treatments","chapter":"30 Difference-in-Differences","heading":"30.9 Multiple Treatments","text":"settings, researchers encounter two () treatments rather single treatment control group. complicates standard estimation, properly structured model ensures accurate identification.Additional References(Fricke 2017): Discusses identification challenges multiple treatment settings.(Clement De Chaisemartin D’haultfœuille 2023): Provides video tutorial (YouTube) code (Google Drive) implementing multiple-treatment models.Key Principles Dealing Multiple TreatmentsAlways include treatment groups single regression model.\nensures proper identification treatment-specific effects maintaining clear comparison control group.\nAlways include treatment groups single regression model.ensures proper identification treatment-specific effects maintaining clear comparison control group.Never use one treated group control .\nRunning separate regressions treatment group can lead biased estimates treatment group may differ systematically control group ways separate model fully capture.\nNever use one treated group control .Running separate regressions treatment group can lead biased estimates treatment group may differ systematically control group ways separate model fully capture.Compare significance treatment effects (\\(\\delta_1\\) vs. \\(\\delta_2\\)).\nInstead assuming equal effects, formally test whether effects two treatments statistically different using F-test Wald test:\n\\[\nH_0: \\delta_1 = \\delta_2\n\\]\nreject \\(H_0\\), conclude two treatments significantly different effects.\nCompare significance treatment effects (\\(\\delta_1\\) vs. \\(\\delta_2\\)).Instead assuming equal effects, formally test whether effects two treatments statistically different using F-test Wald test:\\[\nH_0: \\delta_1 = \\delta_2\n\\]reject \\(H_0\\), conclude two treatments significantly different effects.","code":""},{"path":"sec-difference-in-differences.html","id":"multiple-treatment-groups-model-specification","chapter":"30 Difference-in-Differences","heading":"30.9.1 Multiple Treatment Groups: Model Specification","text":"properly specified regression model two treatments takes following form:\\[\n\\begin{aligned}\nY_{} &= \\alpha + \\gamma_1 Treat1_{} + \\gamma_2 Treat2_{} + \\lambda Post_t  \\\\\n&+ \\delta_1(Treat1_i \\times Post_t) + \\delta_2(Treat2_i \\times Post_t) + \\epsilon_{}\n\\end{aligned}\n\\]:\\(Y_{}\\) = Outcome variable individual \\(\\) time \\(t\\).\\(Treat1_i\\) = 1 individual \\(\\) Treatment Group 1, 0 otherwise.\\(Treat2_i\\) = 1 individual \\(\\) Treatment Group 2, 0 otherwise.\\(Post_t\\) = 1 post-treatment period, 0 otherwise.coefficients:\n\\(\\delta_1\\) = Effect Treatment 1.\n\\(\\delta_2\\) = Effect Treatment 2.\n\\(\\delta_1\\) = Effect Treatment 1.\\(\\delta_2\\) = Effect Treatment 2.\\(\\epsilon_{}\\) = Error term.","code":""},{"path":"sec-difference-in-differences.html","id":"understanding-the-control-group-in-multiple-treatment-did","chapter":"30 Difference-in-Differences","heading":"30.9.2 Understanding the Control Group in Multiple Treatment DiD","text":"One common concern multiple-treatment models properly define control group. well-specified model ensures :control group consists untreated individuals, individuals another treatment group.reference category regression represents control group (.e., individuals \\(Treat1_i = 0\\) \\(Treat2_i = 0\\)).\\(Treat1_i = 1\\), \\(Treat2_i = 0\\) vice versa.Failing correctly specify control group lead incorrect estimates treatment effects. example, omitting one treatment indicators unintentionally redefine control group mix treated untreated individuals.","code":""},{"path":"sec-difference-in-differences.html","id":"alternative-approaches-separate-regressions-vs.-one-model","chapter":"30 Difference-in-Differences","heading":"30.9.3 Alternative Approaches: Separate Regressions vs. One Model","text":"common question whether run one large regression including treatment groups run separate models subsets data. approach implications:One Model Approach (Preferred)Running one comprehensive regression allows direct comparison treatment effects statistically valid way.interaction terms (\\(\\delta_1, \\delta_2\\)) ensure treatment effect estimated relative common control group.F-test (Wald test) enables formal test whether two treatments significantly different effects.Separate Regressions ApproachRunning separate models treatment group can still valid, :\nestimated treatment effects less efficient come separate samples.\nComparisons become less straightforward, rely confidence interval overlap rather direct hypothesis testing.\nhomoscedasticity holds (.e., equal error variances across groups), separate regressions approach unnecessary. combined model efficient.\nestimated treatment effects less efficient come separate samples.Comparisons become less straightforward, rely confidence interval overlap rather direct hypothesis testing.homoscedasticity holds (.e., equal error variances across groups), separate regressions approach unnecessary. combined model efficient.Thus, unless strong justification separate regressions (e.g., significant heterogeneity error variance), one-model approach preferred.","code":""},{"path":"sec-difference-in-differences.html","id":"handling-treatment-intensity","chapter":"30 Difference-in-Differences","heading":"30.9.4 Handling Treatment Intensity","text":"cases, treatments differ just type, also intensity (e.g., low vs. high treatment exposure). observe different levels treatment intensity, can model using single categorical variable rather multiple treatment dummies:Rather coding separate dummies treatment group, define multi-valued treatment variable:\\[\nY_{} = \\alpha + \\sum_{j=1}^{J} \\beta_j (Treatment_j \\times Post_t) + \\lambda Post_t + \\epsilon_{}\n\\]:\\(Treatment_j\\) categorical variable indicating whether individual belongs control group, low-intensity treatment, high-intensity treatment.approach allows cleaner implementation avoids excessive interaction terms.approach advantage :Automatically setting control group reference category.Automatically setting control group reference category.Ensuring correct interpretation coefficients different treatment levels.Ensuring correct interpretation coefficients different treatment levels.","code":""},{"path":"sec-difference-in-differences.html","id":"considerations-when-individuals-can-move-between-treatment-groups","chapter":"30 Difference-in-Differences","heading":"30.9.5 Considerations When Individuals Can Move Between Treatment Groups","text":"One potential complication multiple-treatment settings individuals can switch treatment groups time (e.g., moving low-intensity high-intensity treatment policy implementation).movement rare, may significantly affect estimates.movement rare, may significantly affect estimates.movement frequent, creates challenge causal identification treatment effects might confounded self-selection.movement frequent, creates challenge causal identification treatment effects might confounded self-selection.possible solution use intention--treat (ITT) approach, treatment assignment based initially assigned group, regardless whether individuals later switch.","code":""},{"path":"sec-difference-in-differences.html","id":"parallel-trends-assumption-in-multiple-treatment-did","chapter":"30 Difference-in-Differences","heading":"30.9.6 Parallel Trends Assumption in Multiple-Treatment DiD","text":"Just standard , key assumption multiple-treatment models treatment control groups followed parallel trends absence treatment.Just standard , key assumption multiple-treatment models treatment control groups followed parallel trends absence treatment.multiple treatments, must check pre-trends separately treated group control group.multiple treatments, must check pre-trends separately treated group control group.pre-treatment trends parallel, may need adopt alternative methods synthetic control models event study analyses.pre-treatment trends parallel, may need adopt alternative methods synthetic control models event study analyses.","code":""},{"path":"sec-difference-in-differences.html","id":"mediation-under-did","chapter":"30 Difference-in-Differences","heading":"30.10 Mediation Under DiD","text":"Mediation analysis helps determine whether treatment affects outcome directly intermediate variable (mediator). framework, allows us separate:Direct effects: effect treatment outcome independent mediator.Indirect (mediated) effects: effect treatment operates mediator.useful treatment consists multiple components want understand mechanisms behind observed effect.","code":""},{"path":"sec-difference-in-differences.html","id":"mediation-model-in-did","chapter":"30 Difference-in-Differences","heading":"30.10.1 Mediation Model in DiD","text":"incorporate mediation, estimate two equations:Step 1: Effect Treatment Mediator\\[\nM_{} = \\alpha + \\gamma Treat_i + \\lambda Post_t + \\delta (Treat_i \\times Post_t) + \\epsilon_{}\n\\] :\\(M_{}\\) = Mediator variable (e.g., job search intensity, firm investment, police presence).\\(\\delta\\) = Effect treatment mediator (capturing treatment changes \\(M\\)).Step 2: Effect Treatment Mediator Outcome\\[\nY_{} = \\alpha' + \\gamma' Treat_i + \\lambda' Post_t + \\delta' (Treat_i \\times Post_t) + \\theta M_{} + \\epsilon'_{}\n\\] :\\(Y_{}\\) = Outcome variable (e.g., employment, crime rate, firm performance).\\(\\theta\\) = Effect mediator outcome.\\(\\delta'\\) = Direct effect treatment (controlling mediator).","code":""},{"path":"sec-difference-in-differences.html","id":"interpreting-the-results","chapter":"30 Difference-in-Differences","heading":"30.10.2 Interpreting the Results","text":"\\(\\theta\\) statistically significant, suggests mediation occurring—, treatment affects outcome partly mediator.\\(\\delta'\\) smaller \\(\\delta\\), indicates part treatment effect explained mediator. remaining portion \\(\\delta'\\) represents direct effect.Thus, can decompose total treatment effect :\\[\n\\text{Total Effect} = \\delta' + (\\theta \\times \\delta)\n\\]:\\(\\delta'\\) = Direct effect (holding mediator constant).\\(\\delta'\\) = Direct effect (holding mediator constant).\\(\\theta \\times \\delta\\) = Indirect (mediated) effect.\\(\\theta \\times \\delta\\) = Indirect (mediated) effect.","code":""},{"path":"sec-difference-in-differences.html","id":"challenges-in-mediation-analysis-for-did","chapter":"30 Difference-in-Differences","heading":"30.10.3 Challenges in Mediation Analysis for DiD","text":"Mediation setting introduces several challenges require careful consideration:Potential Confounding MediatorA key assumption unmeasured confounders affect mediator outcome.confounders exist, estimates \\(\\theta\\) may biased.Mediator-Outcome EndogeneityIf mediator influenced unobserved factors correlated outcome, introduces endogeneity, making direct OLS estimates \\(\\theta\\) problematic.example, crime policy evaluation:\nnumber police officers (mediator) may influenced crime rates (outcome), leading reverse causality.\nnumber police officers (mediator) may influenced crime rates (outcome), leading reverse causality.Interaction Multiple MediatorsIf multiple mediators (e.g., policy increases police presence surveillance cameras), may interact .useful test regress mediator treatment mediators. mediator predicts another, effects independent, complicating interpretation.","code":""},{"path":"sec-difference-in-differences.html","id":"alternative-approach-instrumental-variables-for-mediation","chapter":"30 Difference-in-Differences","heading":"30.10.4 Alternative Approach: Instrumental Variables for Mediation","text":"One way address mediator endogeneity use instrumental variables, treatment serves instrument mediator:Two-Stage Estimation:First Stage: Predict Mediator Using Treatment \\[\nM_{} = \\alpha + \\pi Treat_i + \\lambda Post_t + \\delta (Treat_i \\times Post_t) + \\nu_{}\n\\]Second Stage: Predict Outcome Using Instrumented Mediator \\[\nY_{} = \\alpha' + \\gamma' Treat_i + \\lambda' Post_t + \\phi \\hat{M}_{} + \\epsilon'_{}\n\\], \\(\\hat{M}_{}\\) (predicted values first stage) replaces \\(M_{}\\), eliminating endogeneity concerns exclusion restriction holds (.e., treatment affects \\(Y\\) \\(M\\)).Key Limitation IV ApproachThe IV strategy assumes treatment affects outcome mediator, may strong assumption complex policy settings.","code":""},{"path":"sec-difference-in-differences.html","id":"assumptions-3","chapter":"30 Difference-in-Differences","heading":"30.11 Assumptions","text":"Parallel Trends AssumptionThe Difference--Differences estimator relies key identifying assumption: parallel trends assumption. assumption states , absence treatment, average outcome treated group evolved time way control group.Let:\\(Y_{}(0)\\) denote potential outcome without treatment unit \\(\\) time \\(t\\)\\(Y_{}(0)\\) denote potential outcome without treatment unit \\(\\) time \\(t\\)\\(D_i = 1\\) unit \\(\\) treatment group, \\(D_i = 0\\) control group\\(D_i = 1\\) unit \\(\\) treatment group, \\(D_i = 0\\) control groupThen, parallel trends assumption can written :\\[\nE[Y_{}(0) \\mid D_i = 1] - E[Y_{}(0) \\mid D_i = 0] = \\Delta_0 \\quad \\text{} t,\n\\]\\(\\Delta_0\\) constant difference time untreated potential outcomes two groups. assumption require levels outcomes groups—difference remains constant time.words, gap treatment control groups absence treatment must remain stable. holds, deviation stable difference treatment attributed causal effect treatment.important understand parallel trends assumption compares , stronger assumptions:untreated levels across groups:\nstronger assumption require treated control groups identical untreated outcomes times:\n\\[\nE[Y_{}(0) \\mid D_i = 1] = E[Y_{}(0) \\mid D_i = 0] \\quad \\text{} t\n\\]\noften unrealistic observational settings, baseline characteristics typically differ groups.untreated levels across groups:\nstronger assumption require treated control groups identical untreated outcomes times:\\[\nE[Y_{}(0) \\mid D_i = 1] = E[Y_{}(0) \\mid D_i = 0] \\quad \\text{} t\n\\]often unrealistic observational settings, baseline characteristics typically differ groups.change untreated outcomes time:\nAnother strong assumption untreated outcomes remain constant time, groups:\n\\[\nE[Y_{}(0)] = E[Y_{,t'}(0)] \\quad \\text{} \\text{ times } t, t'\n\\]\nimplies secular trends, rarely plausible real-world applications outcomes (e.g., sales, earnings, health metrics) naturally evolve time.change untreated outcomes time:\nAnother strong assumption untreated outcomes remain constant time, groups:\\[\nE[Y_{}(0)] = E[Y_{,t'}(0)] \\quad \\text{} \\text{ times } t, t'\n\\]implies secular trends, rarely plausible real-world applications outcomes (e.g., sales, earnings, health metrics) naturally evolve time.parallel trends assumption weaker generally defensible, especially supported pre-treatment data.appropriate :pre-treatment post-treatment outcome dataYou pre-treatment post-treatment outcome dataYou clearly defined treatment control groupsYou clearly defined treatment control groupsThe parallel trends assumption plausibleThe parallel trends assumption plausibleAvoid using :Treatment assignment random quasi-randomTreatment assignment random quasi-randomUnobserved confounders may cause groups evolve differently timeUnobserved confounders may cause groups evolve differently timeTesting Parallel Trends: Prior Parallel Trends Test.Anticipation Effect (Pre-Treatment Exogeneity)Individuals groups change behavior treatment implemented expectation treatment.Individuals groups change behavior treatment implemented expectation treatment.units anticipate treatment adjust behavior beforehand, can introduce bias estimates.units anticipate treatment adjust behavior beforehand, can introduce bias estimates.Exogenous Treatment AssignmentTreatment assigned based potential outcomes.Ideally, assignment good random, conditional observables.Stable Composition Groups (Attrition Spillover)Treatment control groups remain stable time.selective attrition (individuals enter/leave due treatment).spillover effects: Control units indirectly affected treatment.Simultaneous Confounding Events (Exogeneity Shocks)major shocks affect treatment/control groups differently time treatment implementation.Limitations Common IssuesFunctional Form DependenceIf response treatment nonlinear, compare high- vs. low-intensity groups.Selection (Time-Varying) UnobservablesUse Rosenbaum Bounds check sensitivity estimates unobserved confounders.Long-Term EffectsParallel trends reliable short time windows.long periods, confounding factors may emerge.Heterogeneous EffectsTreatment intensity (e.g., different doses) may vary across groups, leading different effects.Ashenfelter’s Dip (Ashenfelter 1978)Participants job training programs often experience earnings drops enrolling, making systematically different nonparticipants.Fix: Compute long-run differences, excluding periods around treatment, test sustained impact (Proserpio Zervas 2017; J. J. Heckman, LaLonde, Smith 1999; Jepsen, Troske, Coomes 2014).Lagged Treatment EffectsIf effects immediate, using lagged dependent variable \\(Y_{-1}\\) may appropriate (Blundell Bond 1998).Bias Unobserved Factors Affecting TrendsIf external shocks influence treatment control groups differently, biases estimates.Correlated ObservationsStandard errors clustered appropriately.Incidental Parameters Problem (Lancaster 2000)Always prefer individual time fixed effects reduce bias.Treatment Timing Negative WeightsIf treatment timing varies across units, negative weights can arise standard estimators treatment effects heterogeneous (Athey Imbens 2022; Borusyak, Jaravel, Spiess 2024; Goodman-Bacon 2021).Fix: Use estimators Callaway Sant’Anna (2021) Clément De Chaisemartin d’Haultfoeuille (2020) (package).expecting lags leads, see L. Sun Abraham (2021).Treatment Effect Heterogeneity Across GroupsIf treatment effects vary across groups interact treatment variance, standard estimators may invalid (Gibbons, Suárez Serrato, Urbancic 2018).Endogenous TimingIf timing units can influenced strategic decisions analysis, instrumental variable approach control function can used control endogeneity timing.Questionable CounterfactualsIn situations control units may serve reliable counterfactual treated units, matching methods propensity score matching generalized random forest can utilized. Additional methods can found Matching Methods.","code":""},{"path":"sec-difference-in-differences.html","id":"prior-parallel-trends-test","chapter":"30 Difference-in-Differences","heading":"30.11.1 Prior Parallel Trends Test","text":"parallel trends assumption ensures , absent treatment, treated control groups followed similar outcome trajectories. Testing assumption involves visualization statistical analysis.Marcus Sant’Anna (2021) discuss pre-trend testing staggered .Visual Inspection: Outcome Trends Treatment RolloutPlot raw outcome trends groups treatment.Use event-study plots check pre-trend violations anticipation effects.Visualization tools like ggplot2 panelView help illustrate treatment timing trends.Event-Study RegressionsA formal test pre-trends uses event-study model:\\[ Y_{} = \\alpha + \\sum_{k=-K}^{K} \\beta_k 1(T = k) + X_{} \\gamma + \\lambda_i + \\delta_t + \\epsilon_{} \\]:\\(1(T = k)\\) time dummies periods treatment.\\(1(T = k)\\) time dummies periods treatment.\\(\\beta_k\\) captures deviations outcomes treatment; statistically indistinguishable zero parallel trends hold.\\(\\beta_k\\) captures deviations outcomes treatment; statistically indistinguishable zero parallel trends hold.\\(\\lambda_i\\) \\(\\delta_t\\) unit time fixed effects.\\(\\lambda_i\\) \\(\\delta_t\\) unit time fixed effects.\\(X_{}\\) optional covariates.\\(X_{}\\) optional covariates.Violation parallel trends occurs pre-treatment coefficients (\\(\\beta_k\\) \\(k < 0\\)) statistically significant.Statistical Test Pre-Treatment Trend DifferencesUsing pre-treatment data, estimate:\\[\nY = \\alpha_g + \\beta_1 T + \\beta_2 (T \\times G) + \\epsilon\n\\]:\\(\\beta_2\\) measures differences time trends groups.\\(\\beta_2\\) measures differences time trends groups.\\(\\beta_2 = 0\\), trends parallel treatment.\\(\\beta_2 = 0\\), trends parallel treatment.Considerations:Alternative functional forms (e.g., polynomials nonlinear trends) can tested.\\(\\beta_2 \\neq 0\\), potential explanations include:\nLarge sample size driving statistical significance.\nSmall deviations one period disrupting otherwise stable trend.\nLarge sample size driving statistical significance.Small deviations one period disrupting otherwise stable trend.time fixed effects can partially address violations parallel trends (commonly used modern research), may also absorb part treatment effect, especially treatment effects vary time (Wolfers 2003).Debate Parallel TrendsLevels vs. Trends: Kahn-Lang Lang (2020) argue similarity levels also crucial. treatment control groups start different levels, assume trends ?\nSolution:\nPlot time series treated control groups.\nUse matched samples improve comparability (Ryan et al. 2019) (useful parallel trends assumption questionable).\n\nlevels differ significantly, functional form assumptions become critical must justified.\nSolution:\nPlot time series treated control groups.\nUse matched samples improve comparability (Ryan et al. 2019) (useful parallel trends assumption questionable).\nPlot time series treated control groups.Use matched samples improve comparability (Ryan et al. 2019) (useful parallel trends assumption questionable).levels differ significantly, functional form assumptions become critical must justified.Power Pre-Trend Tests:\nPre-trend tests often lack statistical power, making false negatives common (Roth 2022).\nSee: PretrendsPower pretrends (adjustments).\nPre-trend tests often lack statistical power, making false negatives common (Roth 2022).See: PretrendsPower pretrends (adjustments).Outcome Transformations Matter:\nparallel trends assumption specific transformation units outcome variable (Roth Sant’Anna 2023).\nConduct falsification tests check whether assumption holds different functional forms.\nparallel trends assumption specific transformation units outcome variable (Roth Sant’Anna 2023).Conduct falsification tests check whether assumption holds different functional forms.alarming since one periods significantly different 0, means parallel trends assumption plausible.cases parallel trends assumption questionable, researchers consider methods assessing addressing potential violations. key approaches discussed Rambachan Roth (2023):Imposing Restrictions: Constrain different post-treatment violations parallel trends can relative pre-treatment deviations.Imposing Restrictions: Constrain different post-treatment violations parallel trends can relative pre-treatment deviations.Partial Identification: Rather assuming single causal effect, derive bounds ATT.Partial Identification: Rather assuming single causal effect, derive bounds ATT.Sensitivity Analysis: Evaluate sensitive results potential deviations parallel trends.Sensitivity Analysis: Evaluate sensitive results potential deviations parallel trends.implement approaches, HonestDiD package Rambachan Roth (2023) provides robust statistical tools:Alternatively, Ban Kedagni (2022) propose method incorporates pre-treatment covariates information set makes assumption selection bias post-treatment period. Specifically, assume selection bias lies within convex hull pre-treatment selection biases. assumption:identify set possible ATT values.identify set possible ATT values.stronger assumption selection bias—grounded policymakers’ perspectives—can estimate point estimate ATT.stronger assumption selection bias—grounded policymakers’ perspectives—can estimate point estimate ATT.Another useful tool assessing parallel trends pretrends package Roth (2022), provides formal pre-trend tests:","code":"\nlibrary(tidyverse)\nlibrary(fixest)\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    filter(Quarter_Num <= 3) %>% \n    # Treatment variable\n    dplyr::mutate(California = State == 'California')\n\n# use my package\ncausalverse::plot_par_trends(\n    data = od,\n    metrics_and_names = list(\"Rate\" = \"Rate\"),\n    treatment_status_var = \"California\",\n    time_var = list(Quarter_Num = \"Time\"),\n    display_CI = F\n)\n#> [[1]]\n\n# do it manually\n# always good but plot the dependent out\nod |>\n    # group by treatment status and time\n    dplyr::group_by(California, Quarter) |>\n    dplyr::summarize_all(mean) |>\n    dplyr::ungroup() |>\n    # view()\n    \n    ggplot2::ggplot(aes(x = Quarter_Num, y = Rate, color = California)) +\n    ggplot2::geom_line() +\n    causalverse::ama_theme()\n\n\n# but it's also important to use statistical test\nprior_trend <- fixest::feols(Rate ~ i(Quarter_Num, California) |\n                                 State + Quarter,\n                             data = od)\n\nfixest::coefplot(prior_trend, grid = F)\nfixest::iplot(prior_trend, grid = F)\n# https://github.com/asheshrambachan/HonestDiD\n# remotes::install_github(\"asheshrambachan/HonestDiD\")\n# library(HonestDiD)\n# Install and load the pretrends package\n# install.packages(\"pretrends\")\n# library(pretrends)"},{"path":"sec-difference-in-differences.html","id":"sec-placebo-test-did","chapter":"30 Difference-in-Differences","heading":"30.11.2 Placebo Test","text":"placebo test diagnostic tool used Difference--Differences analysis assess whether estimated treatment effect driven pre-existing trends rather treatment . idea estimate treatment effect scenario actual treatment occurred. significant effect found, suggests parallel trends assumption may hold, casting doubt validity causal inference.Types Placebo TestsGroup-Based Placebo TestAssign treatment group never actually treated rerun model.estimated treatment effect statistically significant, suggests differences groups—treatment—driving results.test helps rule possibility estimated effect artifact unobserved systematic differences.valid treatment effect consistent across different reasonable control groups. assess :Rerun model using alternative comparable control group.Rerun model using alternative comparable control group.Compare estimated treatment effects across multiple control groups.Compare estimated treatment effects across multiple control groups.results vary significantly, suggests choice control group may influencing estimated effect, indicating potential selection bias unobserved confounding.results vary significantly, suggests choice control group may influencing estimated effect, indicating potential selection bias unobserved confounding.Time-Based Placebo TestConduct using pre-treatment data, pretending treatment occurred earlier period.significant estimated treatment effect implies differences pre-existing trends—treatment—responsible observed post-treatment effects.test particularly useful concerns exist unobserved shocks anticipatory effects.Random Reassignment TreatmentKeep treatment control periods randomly assign treatment units actually treated.significant effect still emerges, suggests presence biases, unobserved confounding, systematic differences groups violate parallel trends assumption.Procedure Placebo TestUsing Pre-Treatment Data OnlyA robust placebo test often involves analyzing pre-treatment periods check whether spurious treatment effects appear. procedure includes:Restricting sample pre-treatment periods .Restricting sample pre-treatment periods .Assigning fake treatment period actual intervention.Assigning fake treatment period actual intervention.Testing sequence placebo cutoffs time examine whether different assumed treatment timings yield significant effects.Testing sequence placebo cutoffs time examine whether different assumed treatment timings yield significant effects.Generating random treatment periods using randomization inference assess sampling distribution placebo effect.Generating random treatment periods using randomization inference assess sampling distribution placebo effect.Estimating model using fake post-treatment period (post_time = 1).Estimating model using fake post-treatment period (post_time = 1).Interpretation: estimated treatment effect statistically significant, indicates pre-existing trends (treatment) might influencing results, violating parallel trends assumption.Interpretation: estimated treatment effect statistically significant, indicates pre-existing trends (treatment) might influencing results, violating parallel trends assumption.Using Control Groups Placebo TestIf multiple control groups available, placebo test can also conducted :Dropping actual treated group analysis.Dropping actual treated group analysis.Assigning one control groups fake treated group.Assigning one control groups fake treated group.Estimating model checking whether significant effect detected.Estimating model checking whether significant effect detected.Interpretation:\nplacebo effect appears (.e., estimated treatment effect significant), suggests even among control groups, systematic differences exist time.\nHowever, result necessarily disqualifying. methods, Synthetic Control, explicitly model differences maintaining credibility.\nInterpretation:placebo effect appears (.e., estimated treatment effect significant), suggests even among control groups, systematic differences exist time.placebo effect appears (.e., estimated treatment effect significant), suggests even among control groups, systematic differences exist time.However, result necessarily disqualifying. methods, Synthetic Control, explicitly model differences maintaining credibility.However, result necessarily disqualifying. methods, Synthetic Control, explicitly model differences maintaining credibility.like “supposed” insignificant.","code":"\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(causaldata)\n\n# Load the dataset\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    dplyr::filter(Quarter_Num <= 3) %>%\n    \n    # Create fake (placebo) treatment variables\n    dplyr::mutate(\n        FakeTreat1 = as.integer(State == 'California' &\n                                    Quarter %in% c('Q12011', 'Q22011')),\n        FakeTreat2 = as.integer(State == 'California' &\n                                    Quarter == 'Q22011')\n    )\n\n# Estimate the placebo effects using fixed effects regression\nclfe1 <- fixest::feols(Rate ~ FakeTreat1 | State + Quarter, data = od)\nclfe2 <- fixest::feols(Rate ~ FakeTreat2 | State + Quarter, data = od)\n\n# Display the regression results\nfixest::etable(clfe1, clfe2)\n#>                           clfe1            clfe2\n#> Dependent Var.:            Rate             Rate\n#>                                                 \n#> FakeTreat1      0.0061 (0.0051)                 \n#> FakeTreat2                      -0.0017 (0.0028)\n#> Fixed-Effects:  --------------- ----------------\n#> State                       Yes              Yes\n#> Quarter                     Yes              Yes\n#> _______________ _______________ ________________\n#> S.E.: Clustered       by: State        by: State\n#> Observations                 81               81\n#> R2                      0.99377          0.99376\n#> Within R2               0.00192          0.00015\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Extract coefficients and confidence intervals\ncoef_df <- tibble(\n    Model = c(\"FakeTreat1\", \"FakeTreat2\"),\n    Estimate = c(coef(clfe1)[\"FakeTreat1\"], coef(clfe2)[\"FakeTreat2\"]),\n    SE = c(summary(clfe1)$coeftable[\"FakeTreat1\", \"Std. Error\"], \n           summary(clfe2)$coeftable[\"FakeTreat2\", \"Std. Error\"]),\n    Lower = Estimate - 1.96 * SE,\n    Upper = Estimate + 1.96 * SE\n)\n\n# Plot the placebo effects\nggplot(coef_df, aes(x = Model, y = Estimate)) +\n    geom_point(size = 3, color = \"blue\") +\n    geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, color = \"blue\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(\n        title = \"Placebo Treatment Effects\",\n        y = \"Estimated Effect on Organ Donation Rate\",\n        x = \"Placebo Treatment\"\n    )"},{"path":"sec-difference-in-differences.html","id":"robustness-checks-4","chapter":"30 Difference-in-Differences","heading":"30.12 Robustness Checks","text":"well-executed Difference--Differences analysis requires robustness checks verify validity estimated treatment effects best practices ensure methodological rigor.","code":""},{"path":"sec-difference-in-differences.html","id":"robustness-checks-to-strengthen-causal-interpretation","chapter":"30 Difference-in-Differences","heading":"30.12.1 Robustness Checks to Strengthen Causal Interpretation","text":"parallel trends assumption assessed, additional robustness tests ensure treatment effects driven confounding factors modeling choices.Varying Time WindowShorter time windows reduce exposure long-term confounders risk losing statistical power.Longer time windows capture persistent effects may introduce unrelated policy changes.Solution: Estimate model across different time horizons check results stable.Higher-Order Polynomial Time TrendsStandard models assume linear time trend.trends nonlinear, assumption may restrictive.Solution: Introduce quadratic cubic time trends verify whether results hold.Testing Alternative Dependent VariablesThe treatment affect expected dependent variable.robustness check involves running unrelated dependent variables.treatment effects appear , signals possible identification problem.Triple-Difference (DDD) StrategyA Triple-Difference (DDD) model adds additional comparison group address remaining biases:\\[\n\\begin{aligned}\nY_{ijt} &= \\alpha + \\gamma Treat_{} + \\lambda Post_t + \\theta Group_j + \\delta_1 (Treat_i \\times Post_t) \\\\\n&+ \\delta_2 (Treat_i \\times Group_j) + \\delta_3 (Post_t \\times Group_j) \\\\\n&+ \\delta_4 (Treat_i \\times Post_t \\times Group_j) + \\epsilon_{ijt}\n\\end{aligned}\n\\]:\\(Group_j\\) represents subgroup within treatment/control (e.g., high- vs. low-intensity exposure).\\(Group_j\\) represents subgroup within treatment/control (e.g., high- vs. low-intensity exposure).\\(\\delta_4\\) captures DDD effect, removes residual biases present standard model.\\(\\delta_4\\) captures DDD effect, removes residual biases present standard model.","code":""},{"path":"sec-difference-in-differences.html","id":"best-practices-for-reliable-did-implementation","chapter":"30 Difference-in-Differences","heading":"30.12.2 Best Practices for Reliable DiD Implementation","text":"improve credibility transparency estimates, researchers adhere following best practices:Documenting Treatment CohortsClearly report number treated control units time.treatment staggered, adjust different exposure durations.Checking Covariate Balance & OverlapVerify whether distribution covariates similar across treatment control groups.treatment control groups differ significantly, consider using matching methods.Conducting Sensitivity Analyses Parallel TrendsApply alternative weighting schemes (e.g., entropy balancing) reduce dependence model assumptions.Use honestDiD test robustness different parallel trends violations.","code":""},{"path":"sec-difference-in-differences.html","id":"concerns-in-did","chapter":"30 Difference-in-Differences","heading":"30.13 Concerns in DID","text":"","code":""},{"path":"sec-difference-in-differences.html","id":"matching-methods-in-did","chapter":"30 Difference-in-Differences","heading":"30.13.1 Matching Methods in DID","text":"Matching methods often used causal inference balance treated control units based pre-treatment observables. context Difference--Differences, matching helps:Reduce selection bias ensuring treated control units comparable treatment.Improve parallel trends validity selecting control units similar pre-treatment trajectories.Enhance robustness treatment assignment non-random across groups.Key Considerations MatchingStandard Errors Need Adjustment\nStandard errors account fact matching reduces variance (J. J. Heckman, Ichimura, Todd 1997).\nrobust alternative Doubly Robust (Sant’Anna Zhao 2020), either matching regression suffices unbiased treatment effect identification.\nStandard errors account fact matching reduces variance (J. J. Heckman, Ichimura, Todd 1997).robust alternative Doubly Robust (Sant’Anna Zhao 2020), either matching regression suffices unbiased treatment effect identification.Group Fixed Effects Alone Eliminate Selection Bias\nFixed effects absorb time-invariant heterogeneity, correct selection treatment.\nMatching helps close “backdoor path” :\nPropensity treated\nDynamics outcome evolution post-treatment\n\nFixed effects absorb time-invariant heterogeneity, correct selection treatment.Matching helps close “backdoor path” :\nPropensity treated\nDynamics outcome evolution post-treatment\nPropensity treatedDynamics outcome evolution post-treatmentMatching Time-Varying Covariates\nBeware regression mean: extreme pre-treatment outcomes may artificially bias post-treatment estimates (Daw Hatfield 2018).\nissue less concerning time-invariant covariates.\nBeware regression mean: extreme pre-treatment outcomes may artificially bias post-treatment estimates (Daw Hatfield 2018).issue less concerning time-invariant covariates.Comparing Matching vs. Performance\nMatching use pre-treatment outcomes mitigate selection bias.\nSimulations (Chabé-Ferret 2015) show :\nMatching tends underestimate true treatment effect improves pre-treatment periods.\nselection bias symmetric, Symmetric (equal pre- post-treatment periods) performs well.\nselection bias asymmetric, generally outperforms Matching.\n\nMatching use pre-treatment outcomes mitigate selection bias.Simulations (Chabé-Ferret 2015) show :\nMatching tends underestimate true treatment effect improves pre-treatment periods.\nselection bias symmetric, Symmetric (equal pre- post-treatment periods) performs well.\nselection bias asymmetric, generally outperforms Matching.\nMatching tends underestimate true treatment effect improves pre-treatment periods.selection bias symmetric, Symmetric (equal pre- post-treatment periods) performs well.selection bias asymmetric, generally outperforms Matching.Forward Control Unit Selection Algorithm\nefficient way select control units Forward (K. T. Li 2024).\nefficient way select control units Forward (K. T. Li 2024).","code":""},{"path":"sec-difference-in-differences.html","id":"control-variables-in-did","chapter":"30 Difference-in-Differences","heading":"30.13.2 Control Variables in DID","text":"Always report results without controls:\ncontrols fixed within groups time periods, absorbed fixed effects.\ncontrols vary across groups time, suggests parallel trends assumption questionable.\ncontrols fixed within groups time periods, absorbed fixed effects.controls vary across groups time, suggests parallel trends assumption questionable.\\(R^2\\) crucial causal inference:\nUnlike predictive models, causal models prioritize explanatory power (\\(R^2\\)), rather unbiased identification treatment effects.\nUnlike predictive models, causal models prioritize explanatory power (\\(R^2\\)), rather unbiased identification treatment effects.","code":""},{"path":"sec-difference-in-differences.html","id":"did-for-count-data-fixed-effects-poisson-model","chapter":"30 Difference-in-Differences","heading":"30.13.3 DID for Count Data: Fixed-Effects Poisson Model","text":"count data, one can use fixed-effects Poisson pseudo-maximum likelihood estimator (PPML) (Athey Imbens 2006; Puhani 2012). Applications method can found management (Burtch, Carnahan, Greenwood 2018) marketing (C. et al. 2021).approach offers robust standard errors -dispersion (Wooldridge 1999) particularly useful dealing excess zeros data.Key advantages PPML:Handles zero-inflated data better log-OLS: log-OLS regression may produce biased estimates (O’Hara Kotze 2010) heteroskedasticity present (Silva Tenreyro 2006), especially datasets many zeros (Silva Tenreyro 2011).Avoids limitations negative binomial fixed effects: Unlike Poisson, widely accepted fixed-effects estimator negative binomial model (Allison Waterman 2002).","code":""},{"path":"sec-difference-in-differences.html","id":"handling-zero-valued-outcomes-in-did","chapter":"30 Difference-in-Differences","heading":"30.13.4 Handling Zero-Valued Outcomes in DID","text":"dealing zero-valued outcomes, crucial separate intensive margin effect (e.g., outcome changes 10 11) extensive margin effect (e.g., outcome changes 0 1).common issue treatment coefficient log-transformed regression directly interpreted percentage change zeros present (J. Chen Roth 2024). address , can consider two alternative approaches:Proportional Treatment EffectsWe define percentage change treated group’s post-treatment outcome :\\[\n\\theta_{ATT\\%} = \\frac{E[Y_{}(1) \\mid D_i = 1, Post_t = 1] - E[Y_{}(0) \\mid D_i = 1, Post_t = 1]}{E[Y_{}(0) \\mid D_i = 1, Post_t = 1]}\n\\]Instead assuming parallel trends levels, can rely parallel trends assumption ratios (Wooldridge 2023).Poisson QMLE model :\\[\nY_{} = \\exp(\\beta_0 + \\beta_1 D_i \\times Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\n\\]treatment effect estimated :\\[\n\\hat{\\theta}_{ATT\\%} = \\exp(\\hat{\\beta}_1) - 1\n\\]validate parallel trends ratios assumption, can estimate dynamic Poisson QMLE model:\\[\nY_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r))\n\\]assumption holds, expect:\\[\n\\exp(\\hat{\\beta}_r) - 1 = 0 \\quad \\text{} \\quad r < 0.\n\\]Even pre-treatment estimates appear close zero, still conduct sensitivity analysis (Rambachan Roth 2023) assess robustness (see Prior Parallel Trends Test).example, coefficient significant. However, say ’s significant, can interpret coefficient 3 percent increase post-treatment period due treatment.parallel trend “ratio” version Wooldridge (2023) :\\[\n\\frac{E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) |D_i = 1, Post_t = 0)} = \\frac{E(Y_{}(0) |D_i = 0, Post_t = 1)}{E(Y_{}(0) |D_i =0, Post_t = 0)}\n\\]means without treatment, average percentage change mean outcome treated group identical control group.Log Effects Calibrated Extensive-Margin ValueA potential limitation proportional treatment effects may well-suited heavy-tailed outcomes. cases, may prefer explicitly model extensive margin effect.Following (J. Chen Roth 2024, 39), can calibrate weight placed intensive vs. extensive margin ensure meaningful interpretation treatment effect.want study treatment effect concave transformation outcome less influenced distribution’s tail, can perform analysis.Steps:Normalize outcomes 1 represents minimum non-zero positive value (.e., divide outcome minimum non-zero positive value).Estimate treatment effects new outcome\\[\nm(y) =\n\\begin{cases}\n\\log(y) & \\text{} y >0 \\\\\n-x & \\text{} y = 0\n\\end{cases}\n\\]choice \\(x\\) depends researcher interested :dynamic treatment effects different hypothesized extensive-margin value \\(x \\(0, .1, .5, 1, 3, 5)\\)first column zero-valued outcome equal \\(y_{min, y>0}\\) (.e., different minimum outcome zero outcome - \\(x = 0\\))particular example, extensive margin increases, see increase effect magnitude. second column assume extensive-margin change 0 \\(y_{min, y >0}\\) equivalent 10 (.e., \\(0.1 \\times 100\\)) log point change along intensive margin.","code":"\nset.seed(123) # For reproducibility\n\nn <- 500 # Number of observations per group (treated and control)\n# Generating IDs for a panel setup\nID <- rep(1:n, times = 2)\n\n# Defining groups and periods\nGroup <- rep(c(\"Control\", \"Treated\"), each = n)\nTime <- rep(c(\"Before\", \"After\"), times = n)\nTreatment <- ifelse(Group == \"Treated\", 1, 0)\nPost <- ifelse(Time == \"After\", 1, 0)\n\n# Step 1: Generate baseline outcomes with a zero-inflated model\nlambda <- 20 # Average rate of occurrence\nzero_inflation <- 0.5 # Proportion of zeros\nY_baseline <-\n    ifelse(runif(2 * n) < zero_inflation, 0, rpois(2 * n, lambda))\n\n# Step 2: Apply DiD treatment effect on the treated group in the post-treatment period\nTreatment_Effect <- Treatment * Post\nY_treatment <-\n    ifelse(Treatment_Effect == 1, rpois(n, lambda = 2), 0)\n\n# Incorporating a simple time trend, ensuring outcomes are non-negative\nTime_Trend <- ifelse(Time == \"After\", rpois(2 * n, lambda = 1), 0)\n\n# Step 3: Combine to get the observed outcomes\nY_observed <- Y_baseline + Y_treatment + Time_Trend\n\n# Ensure no negative outcomes after the time trend\nY_observed <- ifelse(Y_observed < 0, 0, Y_observed)\n\n# Create the final dataset\ndata <-\n    data.frame(\n        ID = ID,\n        Treatment = Treatment,\n        Period = Post,\n        Outcome = Y_observed\n    )\n\n# Viewing the first few rows of the dataset\nhead(data)\n#>   ID Treatment Period Outcome\n#> 1  1         0      0       0\n#> 2  2         0      1      25\n#> 3  3         0      0       0\n#> 4  4         0      1      20\n#> 5  5         0      0      19\n#> 6  6         0      1       0\nlibrary(fixest)\nres_pois <-\n    fepois(Outcome ~ Treatment + Period + Treatment * Period,\n           data = data,\n           vcov = \"hetero\")\netable(res_pois)\n#>                             res_pois\n#> Dependent Var.:              Outcome\n#>                                     \n#> Constant           2.249*** (0.0717)\n#> Treatment           0.1743. (0.0932)\n#> Period               0.0662 (0.0960)\n#> Treatment x Period   0.0314 (0.1249)\n#> __________________ _________________\n#> S.E. type          Heteroskeda.-rob.\n#> Observations                   1,000\n#> Squared Cor.                 0.01148\n#> Pseudo R2                    0.00746\n#> BIC                         15,636.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Average percentage change\nexp(coefficients(res_pois)[\"Treatment:Period\"]) - 1\n#> Treatment:Period \n#>       0.03191643\n\n# SE using delta method\nexp(coefficients(res_pois)[\"Treatment:Period\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment:Period\", \"Treatment:Period\"])\n#> Treatment:Period \n#>        0.1288596\nlibrary(fixest)\n\nbase_did_log0 <- base_did |> \n    mutate(y = if_else(y > 0, y, 0))\n\nres_pois_es <-\n    fepois(y ~ x1 + i(period, treat, 5) | id + period,\n           data = base_did_log0,\n           vcov = \"hetero\")\n\netable(res_pois_es)\n#>                            res_pois_es\n#> Dependent Var.:                      y\n#>                                       \n#> x1                  0.1895*** (0.0108)\n#> treat x period = 1    -0.2769 (0.3545)\n#> treat x period = 2    -0.2699 (0.3533)\n#> treat x period = 3     0.1737 (0.3520)\n#> treat x period = 4    -0.2381 (0.3249)\n#> treat x period = 6     0.3724 (0.3086)\n#> treat x period = 7    0.7739* (0.3117)\n#> treat x period = 8    0.5028. (0.2962)\n#> treat x period = 9   0.9746** (0.3092)\n#> treat x period = 10  1.310*** (0.3193)\n#> Fixed-Effects:      ------------------\n#> id                                 Yes\n#> period                             Yes\n#> ___________________ __________________\n#> S.E. type           Heteroskedas.-rob.\n#> Observations                     1,080\n#> Squared Cor.                   0.51131\n#> Pseudo R2                      0.34836\n#> BIC                            5,868.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\niplot(res_pois_es)\nlibrary(fixest)\nbase_did_log0_cali <- base_did_log0 |> \n    # get min \n    mutate(min_y = min(y[y > 0])) |> \n    \n    # normalized the outcome \n    mutate(y_norm = y / min_y)\n\nmy_regression <-\n    function(x) {\n        base_did_log0_cali <-\n            base_did_log0_cali %>% mutate(my = ifelse(y_norm == 0,-x,\n                                                      log(y_norm)))\n        my_reg <-\n            feols(\n                fml = my ~ x1 + i(period, treat, 5) | id + period,\n                data = base_did_log0_cali,\n                vcov = \"hetero\"\n            )\n        \n        return(my_reg)\n    }\n\nxvec <- c(0, .1, .5, 1, 3)\nreg_list <- purrr::map(.x = xvec, .f = my_regression)\n\n\niplot(reg_list, \n      pt.col =  1:length(xvec),\n      pt.pch = 1:length(xvec))\nlegend(\"topleft\", \n       col = 1:length(xvec),\n       pch = 1:length(xvec),\n       legend = as.character(xvec))\n\n\netable(\n    reg_list,\n    headers = list(\"Extensive-margin value (x)\" = as.character(xvec)),\n    digits = 2,\n    digits.stats = 2\n)\n#>                                   model 1        model 2        model 3\n#> Extensive-margin value (x)              0            0.1            0.5\n#> Dependent Var.:                        my             my             my\n#>                                                                        \n#> x1                         0.43*** (0.02) 0.44*** (0.02) 0.46*** (0.03)\n#> treat x period = 1           -0.92 (0.67)   -0.94 (0.69)    -1.0 (0.73)\n#> treat x period = 2           -0.41 (0.66)   -0.42 (0.67)   -0.43 (0.71)\n#> treat x period = 3           -0.34 (0.67)   -0.35 (0.68)   -0.38 (0.73)\n#> treat x period = 4            -1.0 (0.67)    -1.0 (0.68)    -1.1 (0.73)\n#> treat x period = 6            0.44 (0.66)    0.44 (0.67)    0.45 (0.72)\n#> treat x period = 7            1.1. (0.64)    1.1. (0.65)    1.2. (0.70)\n#> treat x period = 8            1.1. (0.64)    1.1. (0.65)     1.1 (0.69)\n#> treat x period = 9           1.7** (0.65)   1.7** (0.66)    1.8* (0.70)\n#> treat x period = 10         2.4*** (0.62)  2.4*** (0.63)  2.5*** (0.68)\n#> Fixed-Effects:             -------------- -------------- --------------\n#> id                                    Yes            Yes            Yes\n#> period                                Yes            Yes            Yes\n#> __________________________ ______________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080          1,080\n#> R2                                   0.43           0.43           0.43\n#> Within R2                            0.26           0.26           0.25\n#> \n#>                                   model 4        model 5\n#> Extensive-margin value (x)              1              3\n#> Dependent Var.:                        my             my\n#>                                                         \n#> x1                         0.49*** (0.03) 0.62*** (0.04)\n#> treat x period = 1            -1.1 (0.79)     -1.5 (1.0)\n#> treat x period = 2           -0.44 (0.77)   -0.51 (0.99)\n#> treat x period = 3           -0.43 (0.78)    -0.60 (1.0)\n#> treat x period = 4            -1.2 (0.78)     -1.5 (1.0)\n#> treat x period = 6            0.45 (0.77)     0.46 (1.0)\n#> treat x period = 7             1.2 (0.75)     1.3 (0.97)\n#> treat x period = 8             1.2 (0.74)     1.3 (0.96)\n#> treat x period = 9            1.8* (0.75)    2.1* (0.97)\n#> treat x period = 10         2.7*** (0.73)  3.2*** (0.94)\n#> Fixed-Effects:             -------------- --------------\n#> id                                    Yes            Yes\n#> period                                Yes            Yes\n#> __________________________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080\n#> R2                                   0.42           0.41\n#> Within R2                            0.25           0.24\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-difference-in-differences.html","id":"standard-errors-1","chapter":"30 Difference-in-Differences","heading":"30.13.5 Standard Errors","text":"One major statistical challenges estimation serial correlation error terms. issue particularly problematic can lead underestimated standard errors, inflating likelihood Type errors (false positives). discussed Bertrand, Duflo, Mullainathan (2004), serial correlation arises settings due several factors:Long time series: Many studies use multiple time periods, increasing risk correlated errors.Highly positively serially correlated outcomes: Many economic business variables (e.g., GDP, sales, employment rates) exhibit strong persistence time.Minimal within-group variation treatment timing: example, state-level policy change, individuals state receive treatment time, leading correlation within state-time clusters.correct serial correlation, various methods can employed. However, approaches work better others:Avoid standard parametric corrections: common approach model error term using autoregressive (AR) process. However, Bertrand, Duflo, Mullainathan (2004) show often fails settings fully account within-group correlation.Nonparametric solutions (preferred number groups large):\nBlock bootstrap: Resampling entire groups (e.g., states) rather individual observations maintains correlation structure provides robust standard errors.\nBlock bootstrap: Resampling entire groups (e.g., states) rather individual observations maintains correlation structure provides robust standard errors.Collapsing data two periods (Pre vs. Post):\nAggregating data single pre-treatment single post-treatment period can mitigate serial correlation issues. approach particularly useful number groups small (Donald Lang 2007).\nNote: reduces power analysis discarding variation across time, ensures standard errors artificially deflated.\nAggregating data single pre-treatment single post-treatment period can mitigate serial correlation issues. approach particularly useful number groups small (Donald Lang 2007).Note: reduces power analysis discarding variation across time, ensures standard errors artificially deflated.Variance-covariance matrix corrections:\nEmpirical corrections (e.g., cluster-robust standard errors) arbitrary variance-covariance matrix adjustments (e.g., Newey-West) can work well, reliable large samples.\nEmpirical corrections (e.g., cluster-robust standard errors) arbitrary variance-covariance matrix adjustments (e.g., Newey-West) can work well, reliable large samples.Overall, selecting appropriate correction method depends sample size structure data. possible, block bootstrapping collapsing data pre/post periods among effective approaches.","code":""},{"path":"sec-difference-in-differences.html","id":"sec-partial-identification-did","chapter":"30 Difference-in-Differences","heading":"30.13.6 Partial Identification","text":"Classical delivers point‑identified estimate average treatment effect treated (ATT) parallel‑trends assumption exact. Yet practice pre‑trends often deviate post‑treatment shocks differ across groups (e.g., famous Ashenfelter (1978)). Partial‑identification (PI) methods shift question “estimate unbiased?” “much identifying assumption fail reverse conclusion?”PI approaches DiDManski Pepper (2018) propose bounding treatment effect parameter \\(\\delta\\) across time space presence violations parallel trends assumption.L. J. Keele et al. (2019) introduce Rosenbaum-style sensitivity parameter quantifies magnitude hidden bias necessary reverse sign estimated effect.L. Keele, Hasegawa, Small (2019) extends framework settings treated group may simultaneously exposed another event, using Rosenbaum-style sensitivity analysis assess robustness causal claims compound treatments.Ye et al. (2024) utilize two control groups subjected negatively correlated shocks non-parametrically bracket counterfactual trend, offering bounds treatment effects without relying parallel trends assumption.Leavitt (2020) adopts Empirical Bayes approach places hierarchical prior \\(\\delta\\), shrinking extreme violations parallel trends providing posterior credible intervals explicitly account trend heterogeneity.Two bounding strategies Rambachan Roth (2023) —() Relative magnitude bounds: \\(|\\delta| \\leq M \\cdot \\max\\) pre-period jumps; (ii) Smoothness bounds: allow \\(\\delta\\)’s slope shift \\(M\\) per period.","code":""},{"path":"sec-difference-in-differences.html","id":"canonical-did-notation-and-the-source-of-bias","chapter":"30 Difference-in-Differences","heading":"30.13.6.1 Canonical DiD notation and the source of bias","text":"Let\\(Y_{}^{1}\\) \\(Y_{}^{0}\\) potential outcomes unit \\(\\) period \\(t\\) treatment treatment;\\(D_i\\\\{0,1\\}\\) indicate ever‑treated units;\\(\\texttt{Post}_t\\) mark periods policy shock.two periods (pre, post) two groups (T = treated, U = untreated) simple estimator \\[\n\\hat\\beta_{DD}= \\Bigl[E(Y_T^{1}\\mid \\texttt{Post})-E(Y_T^{0}\\mid \\texttt{Pre})\\Bigr]\\;-\\;\\Bigl[E(Y_U^{0}\\mid \\texttt{Post})-E(Y_U^{0}\\mid \\texttt{Pre})\\Bigr].\n\\]Add–subtract counterfactual trend \\(E(Y_T^{0}\\mid \\texttt{Post})\\) rearrange:\\[\n\\begin{aligned}\n\\hat\\beta_{DD} &= \\underbrace{E(Y_T^{1}\\mid\\texttt{Post})-E(Y_T^{0}\\mid\\texttt{Post})}_{\\text{ATT}} \\\\\n&+\n\\underbrace{\\bigl[E(Y_T^{0}\\mid\\texttt{Post})-E(Y_T^{0}\\mid\\texttt{Pre})\\bigr]\n-\\bigl[E(Y_U^{0}\\mid\\texttt{Post})-E(Y_U^{0}\\mid\\texttt{Pre})\\bigr]}_{\\delta}\n\\end{aligned}\n\\]Hence\\[\n\\hat\\beta_{DD}= \\text{ATT}+\\delta .\n\\]parallel trends hold, \\(\\delta=0\\); violation translates one‑‑one bias (Frake et al. 2025).","code":""},{"path":"sec-difference-in-differences.html","id":"bounding-the-att-when-deltaneq-0","chapter":"30 Difference-in-Differences","heading":"30.13.6.2 Bounding the ATT when \\(\\delta\\neq 0\\)","text":"core PI idea simple: derive credible bounds \\(\\delta\\), translate bounds ATT.Two families restrictions—operationalized Rambachan Roth (2023) implemented honestdid package—now widely used:Relative‑magnitude (RM) bounds: post‑treatment gap \\(|\\delta|\\) exceed \\(M\\) times largest period‑‑period deviation observed pre‑treatment event‑study.Smoothness (SM) bounds: slope treated‑minus‑control gap can change \\(M\\) units period treatment.\\(M\\) researcher‑chosen, inference reported across grid plausible \\(M\\) values, revealing breakdown threshold confidence set first touches zero (Frake et al. 2025, 14).","code":""},{"path":"sec-difference-in-differences.html","id":"relativemagnitude-restriction-sec-relativemagnitude-restriction-honestdid","chapter":"30 Difference-in-Differences","heading":"30.13.6.3 Relative‑Magnitude Restriction {#sec-relative‑magnitude-restriction-honestdid}}","text":"Let \\(\\hat g_t\\) event‑study coefficient time \\(t\\) (normalized zero \\(t=-1\\)).Step 1 — calibrate worst pre‑trend:\\(M_0 = \\max_{s<0} |\\hat g_s-\\hat g_{s-1}|\\).Step 1 — calibrate worst pre‑trend:\\(M_0 = \\max_{s<0} |\\hat g_s-\\hat g_{s-1}|\\).Step 2 — allow proportional violations post‑treatment:\nImpose \\(|\\delta_t|\\le M\\cdot M_0\\) every \\(t\\ge 0\\).Step 2 — allow proportional violations post‑treatment:\nImpose \\(|\\delta_t|\\le M\\cdot M_0\\) every \\(t\\ge 0\\).Step 3 — construct “honest” confidence sets ATT period‑specific effects solving constrained least‑squares problem (Rambachan Roth 2023).Step 3 — construct “honest” confidence sets ATT period‑specific effects solving constrained least‑squares problem (Rambachan Roth 2023).","code":""},{"path":"sec-difference-in-differences.html","id":"sec-smoothness-restriction-honestdid","chapter":"30 Difference-in-Differences","heading":"30.13.6.4 Smoothness Restriction","text":"violations evolve gradually—e.g., treated units drift upward time—SM bound credible.Step 1 — estimate pre‑treatment slope:\\(s_0 = \\hat g_{-1}-\\hat g_{-2}\\).Step 1 — estimate pre‑treatment slope:\\(s_0 = \\hat g_{-1}-\\hat g_{-2}\\).Step 2 — restrict post‑treatment slopes:\ncounterfactual slope period \\(k\\) may vary within\\([s_0-M,\\; s_0+M]\\).Step 2 — restrict post‑treatment slopes:\ncounterfactual slope period \\(k\\) may vary within\\([s_0-M,\\; s_0+M]\\).Step 3 — propagate forward obtain admissible path \\(\\delta_t\\); compute least‑favorable path, derive confidence sets (Rambachan Roth 2023).Step 3 — propagate forward obtain admissible path \\(\\delta_t\\); compute least‑favorable path, derive confidence sets (Rambachan Roth 2023).","code":""},{"path":"sec-difference-in-differences.html","id":"stepbystep-empirical-workflow","chapter":"30 Difference-in-Differences","heading":"30.13.6.5 Step‑by‑step Empirical Workflow","text":"See Frake et al. (2025), p. 15 decision tree guides choice method.Graph event‑study. pre‑trends visible? Quantify magnitude /slope.Choose restriction family (RM episodic shocks, SM trending deviations). Justify choice theoretically (e.g., anticipation effects vs. gradual selection).Select grid \\(M\\). Always display break‑even \\(M^{*}\\) CI first covers zero; interpret \\(M^{*}\\) real‑world units.Report full set, “robust” interval. Transparency trumps opportunistic suppression.Complement falsification tests (placebo policy dates, placebo outcomes) discipline plausible range \\(M\\).Document software code. honestdid (R/Stata) includes convenient wrappers restrictions automatic figures.","code":""},{"path":"sec-difference-in-differences.html","id":"worked-example-stylized","chapter":"30 Difference-in-Differences","heading":"30.13.6.6 Worked Example (Stylized)","text":"Suppose pre‑policy trends exhibit 0.8‑point swing. Setting \\(M\\\\{0,0.5,1,1.5\\}\\):Thus causal claim survives unless unobserved counterfactual deviations \\(\\ge 1 \\times\\) worst pre‑treatment kink—interpretation far clearer single \\(p\\)‑value.","code":""},{"path":"sec-difference-in-differences.html","id":"common-pitfalls-and-best-practices","chapter":"30 Difference-in-Differences","heading":"30.13.6.7 Common Pitfalls and Best Practices","text":"Mistaking absence significant pre‑trends proof parallel‑trends. ‑powered tests can miss economically large differences.Choosing \\(M\\) post‑hoc. Decide grid looking results.Ignoring sign. theory predicts treated counterfactual fallen, impose one‑sided bound; otherwise wastes power.Forgetting heterogeneous timing staggered adoption. Apply PI group‑time ATT estimates modern estimators aggregation.Partial‑identification transforms “take‑‑‑leave‑” enterprise transparent sensitivity analysis. explicitly parameterizing—visually communicating—extent permissible departures parallel trends, researchers allow readers map priors onto empirical conclusions. Used thoughtfully, PI techniques enhance credibility without demanding impossible.","code":""},{"path":"sec-changes-in-changes.html","id":"sec-changes-in-changes","chapter":"31 Changes-in-Changes","heading":"31 Changes-in-Changes","text":"Changes--Changes (CiC) estimator, introduced Athey Imbens (2006), alternative Difference--Differences strategy. traditional estimates Average Treatment Effect Treated (ATT), CiC focuses Quantile Treatment Effect Treated (QTT).Policymakers analysts often look beyond average program impacts understand benefits distributed across different subgroups. QTT approach particularly useful cases :Policy decisions depend distributional effects:\ninstance, consider two job training programs negative average effect.\none program harms high-income earners benefiting low-income earners, might still considered valuable, whereas program negatively affects low-income earners might rejected.\ninstance, consider two job training programs negative average effect.one program harms high-income earners benefiting low-income earners, might still considered valuable, whereas program negatively affects low-income earners might rejected.Limitations traditional methods:\nMethods linear regression assume uniform treatment effects across population, may mask important distributional differences.\nMethods linear regression assume uniform treatment effects across population, may mask important distributional differences.Advantages QTE methods:\nQuantile treatment effects (QTEs) allow detailed examination treatment effects vary across different segments population.\nQTEs provide distributional insights, can also used recover ATEs weaker assumptions.\nQuantile treatment effects (QTEs) allow detailed examination treatment effects vary across different segments population.QTEs provide distributional insights, can also used recover ATEs weaker assumptions.References\nAthey Imbens (2006)\nFrölich Melly (2013): IV-based\nCallaway Li (2019): panel data\nM. Huber, Schelker, Strittmatter (2022)\nReferencesAthey Imbens (2006)Frölich Melly (2013): IV-basedCallaway Li (2019): panel dataM. Huber, Schelker, Strittmatter (2022)Additional Resources\nCode examples available Stata.\nAdditional ResourcesCode examples available Stata.","code":""},{"path":"sec-changes-in-changes.html","id":"key-concepts","chapter":"31 Changes-in-Changes","heading":"31.1 Key Concepts","text":"Quantile Treatment Effect Treated (QTT):\nMeasures difference quantiles potential outcome distributions treated units.\nMeasures difference quantiles potential outcome distributions treated units.Rank Preservation:\nAssumes rank individual remains unchanged across different potential outcome distributions.\nstrong assumption considered carefully empirical applications.\nAssumes rank individual remains unchanged across different potential outcome distributions.strong assumption considered carefully empirical applications.Counterfactual Distribution:\nmain estimation challenge CiC constructing counterfactual distribution outcomes treated units period 1.\nmain estimation challenge CiC constructing counterfactual distribution outcomes treated units period 1.","code":""},{"path":"sec-changes-in-changes.html","id":"estimating-qtt-with-cic","chapter":"31 Changes-in-Changes","heading":"31.2 Estimating QTT with CiC","text":"CiC relies four distributions 2 × 2 Difference--Differences () setup:\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.Quantile Treatment Effect Treated (QTT) quantile \\(\\theta\\) :\\[\n\\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta)\n\\]estimate counterfactual CDF:\\[\n\\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right)\n\\]leads estimation inverse counterfactual CDF:\\[\n\\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right)\n\\]Finally, treatment effect estimate :\\[\n\\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta)\n\\]Alternatively, CiC can expressed difference two QTE estimates:\\[\n\\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta',0}\n\\]:\\(\\Delta^{QTT}_{\\theta,1}\\) represents change time quantile \\(\\theta\\) treated group (\\(D=1\\)).\\(\\Delta^{QTU}_{\\theta',0}\\) represents change time quantile \\(\\theta'\\) control group (\\(D=0\\)).\nquantile \\(\\theta'\\) selected match value \\(y\\) quantile \\(\\theta\\) treated group’s period 0 distribution.\nquantile \\(\\theta'\\) selected match value \\(y\\) quantile \\(\\theta\\) treated group’s period 0 distribution.Marketing ExampleSuppose company introduces new online marketing strategy aimed improving customer retention rates. goal analyze strategy affects retention different quantiles customer base.QTT Interpretation:\nInstead looking average effect marketing strategy, CiC allows company examine retention rates change across different quantiles (e.g., low vs. high-retention customers).\nInstead looking average effect marketing strategy, CiC allows company examine retention rates change across different quantiles (e.g., low vs. high-retention customers).Rank Preservation Assumption:\napproach assumes customers’ rank retention distribution remains unchanged, regardless whether received new strategy.\napproach assumes customers’ rank retention distribution remains unchanged, regardless whether received new strategy.Counterfactual Distribution:\nCiC helps estimate retention rates evolved without new strategy, comparing trends control group.\nCiC helps estimate retention rates evolved without new strategy, comparing trends control group.","code":""},{"path":"sec-changes-in-changes.html","id":"application-4","chapter":"31 Changes-in-Changes","heading":"31.3 Application","text":"","code":""},{"path":"sec-changes-in-changes.html","id":"ecic-package","chapter":"31 Changes-in-Changes","heading":"31.3.1 ECIC package","text":"","code":"\nlibrary(ecic)\ndata(dat, package = \"ecic\")\nmod =\n  ecic(\n    yvar  = lemp,         # dependent variable\n    gvar  = first.treat,  # group indicator\n    tvar  = year,         # time indicator\n    ivar  = countyreal,   # unit ID\n    dat   = dat,          # dataset\n    boot  = \"weighted\",   # bootstrap proceduce (\"no\", \"normal\", or \"weighted\")\n    nReps = 3            # number of bootstrap runs\n    )\nmod_res <- summary(mod)\nmod_res\n#>   perc    coefs          se\n#> 1  0.1 1.206140 0.021351711\n#> 2  0.2 1.316599 0.009225026\n#> 3  0.3 1.449963 0.001859468\n#> 4  0.4 1.583415 0.015296156\n#> 5  0.5 1.739932 0.011240454\n#> 6  0.6 1.915558 0.013060348\n#> 7  0.7 2.114966 0.014482208\n#> 8  0.8 2.363105 0.005173865\n#> 9  0.9 2.779202 0.020831180\n\necic_plot(mod_res)"},{"path":"sec-changes-in-changes.html","id":"qte-package","chapter":"31 Changes-in-Changes","heading":"31.3.2 QTE package","text":"QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.-like modelsWith distributional difference--differences assumption Callaway Li (2019), extension parallel trends assumption, can estimate QTET.2 periods, distributional assumption can partially identify QTET bounds (Fan Yu 2012)restrictive assumption difference quantiles distribution potential outcomes treated untreated groups values quantiles, can mean modelOn top distributional assumption, need copula stability assumption (.e., , treatment, units highest outcomes improving , expect see improving current period .) models:","code":"\nlibrary(qte)\ndata(lalonde)\n\n# randomized setting\n# qte is identical to qtet\njt.rand <-\n    ci.qtet(\n        re78 ~ treat,\n        data = lalonde.exp,\n        iters = 10\n    )\nsummary(jt.rand)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05    0.00    0.00\n#> 0.1     0.00    0.00\n#> 0.15    0.00    0.00\n#> 0.2     0.00  172.22\n#> 0.25  338.65  412.28\n#> 0.3   846.40  495.49\n#> 0.35 1451.51  850.06\n#> 0.4  1177.72 1324.94\n#> 0.45 1396.08 1284.24\n#> 0.5  1123.55 1165.53\n#> 0.55 1181.54 1278.94\n#> 0.6  1466.51 1305.18\n#> 0.65 2115.04 1214.72\n#> 0.7  1795.12 1223.99\n#> 0.75 2347.49 1263.86\n#> 0.8  2278.12 2035.94\n#> 0.85 2178.28 1748.55\n#> 0.9  3239.60 2943.38\n#> 0.95 3979.62 2956.72\n#> \n#> Average Treatment Effect:    1794.34\n#>   Std. Error:        1231.65\nggqte(jt.rand)\n# conditional independence assumption (CIA)\njt.cia <- ci.qte(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.cia)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1       0.00      153.44\n#> 0.15  -4433.18     1247.76\n#> 0.2   -8219.15      567.13\n#> 0.25 -10435.74      890.72\n#> 0.3  -12232.03      780.23\n#> 0.35 -12428.30     1046.95\n#> 0.4  -14195.24     1255.86\n#> 0.45 -14248.66     1375.50\n#> 0.5  -15538.67     1957.68\n#> 0.55 -16550.71     2333.33\n#> 0.6  -15595.02     2586.83\n#> 0.65 -15827.52     2623.48\n#> 0.7  -16090.32     2082.43\n#> 0.75 -16091.49     1644.88\n#> 0.8  -17864.76     1929.23\n#> 0.85 -16756.71     2383.14\n#> 0.9  -17914.99     2205.49\n#> 0.95 -23646.22     2246.10\n#> \n#> Average Treatment Effect:    -13435.40\n#>   Std. Error:        953.59\nggqte(jt.cia)\n\njt.ciat <- ci.qtet(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.ciat)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1   -1018.15      621.26\n#> 0.15  -3251.00     1647.68\n#> 0.2   -7240.86     1075.73\n#> 0.25  -8379.94      646.57\n#> 0.3   -8758.82      655.46\n#> 0.35  -9897.44      837.77\n#> 0.4  -10239.57      864.00\n#> 0.45 -10751.39     1305.32\n#> 0.5  -10570.14      901.01\n#> 0.55 -11348.96     1165.84\n#> 0.6  -11550.84      985.99\n#> 0.65 -12203.56     1017.18\n#> 0.7  -13277.72     1005.26\n#> 0.75 -14011.74      535.50\n#> 0.8  -14373.95      694.64\n#> 0.85 -14499.18      840.33\n#> 0.9  -15008.63     1745.99\n#> 0.95 -15954.05     1689.42\n#> \n#> Average Treatment Effect:    4266.19\n#>   Std. Error:        447.40\nggqte(jt.ciat)\n# distributional DiD assumption\njt.pqtet <- panel.qtet(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tmin2 = 1974,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.pqtet)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05  4779.21     1381.99\n#> 0.1   1987.35      580.24\n#> 0.15   842.95      597.66\n#> 0.2  -7366.04     4554.93\n#> 0.25 -8449.96     1941.26\n#> 0.3  -7992.15      840.84\n#> 0.35 -7429.21      860.91\n#> 0.4  -6597.37      804.03\n#> 0.45 -5519.45      917.33\n#> 0.5  -4702.88      883.83\n#> 0.55 -3904.52     1206.82\n#> 0.6  -2741.80     1282.21\n#> 0.65 -1507.31     1266.61\n#> 0.7   -771.12     1403.72\n#> 0.75   707.81     1255.73\n#> 0.8    580.00      972.56\n#> 0.85   821.75      921.04\n#> 0.9   -250.77     1597.59\n#> 0.95 -1874.54     1775.63\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        354.00\nggqte(jt.pqtet)\nres_bound <-\n    bounds(\n        re ~ treat,\n        t = 1978,\n        tmin1 = 1975,\n        data = lalonde.psid.panel,\n        idname = \"id\",\n        tname = \"year\"\n    )\nsummary(res_bound)\n#> \n#> Bounds on the Quantile Treatment Effect on the Treated:\n#>      \n#> tau  Lower Bound Upper Bound\n#>         tau  Lower Bound Upper Bound\n#>        0.05       -51.72           0\n#>         0.1     -1220.84           0\n#>        0.15      -1881.9           0\n#>         0.2     -2601.32           0\n#>        0.25     -2916.38      485.23\n#>         0.3     -3080.16      943.05\n#>        0.35     -3327.89     1505.98\n#>         0.4     -3240.59     2133.59\n#>        0.45     -2982.51     2616.84\n#>         0.5     -3108.01      2566.2\n#>        0.55     -3342.66     2672.82\n#>         0.6      -3491.4      3065.7\n#>        0.65     -3739.74     3349.74\n#>         0.7     -4647.82     2992.03\n#>        0.75     -4826.78     3219.32\n#>         0.8      -5801.7     2702.33\n#>        0.85     -6588.61     2499.41\n#>         0.9     -8953.84     2020.84\n#>        0.95    -14283.61      397.04\n#> \n#> Average Treatment Effect on the Treated: 2326.51\nplot(res_bound)\njt.mdid <- ddid2(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.mdid)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05 10616.61      964.99\n#> 0.1   5019.83      375.72\n#> 0.15  2388.12      349.21\n#> 0.2   1033.23      337.01\n#> 0.25   485.23      513.72\n#> 0.3    943.05      493.07\n#> 0.35   931.45      704.90\n#> 0.4    945.35      907.41\n#> 0.45  1205.88      852.24\n#> 0.5   1362.11      797.66\n#> 0.55  1279.05     1008.63\n#> 0.6   1618.13     1133.52\n#> 0.65  1834.30     1233.78\n#> 0.7   1326.06     1024.53\n#> 0.75  1586.35      735.59\n#> 0.8   1256.09      944.51\n#> 0.85   723.10      960.34\n#> 0.9    251.36     1766.75\n#> 0.95 -1509.92     2264.59\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        599.15\nplot(jt.mdid)\njt.qdid <- QDiD(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)\n\njt.cic <- CiC(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)"},{"path":"sec-synthetic-control.html","id":"sec-synthetic-control","chapter":"32 Synthetic Control","heading":"32 Synthetic Control","text":"Synthetic Control Method (SCM) powerful causal inference tool used one treated unit exists, researchers must construct synthetic comparison group donor pool untreated units. Introduced Abadie Gardeazabal (2003) later extended Abadie, Diamond, Hainmueller (2010), SCM become widely used method policy evaluation business analytics. See Abadie (2021) review method.Use SCM?data-driven method constructing comparable control group (.e., “black box” approach).Avoids large bias covariate distributions match treated control groups.Natural alternative Difference--Differences :\nperfect untreated comparison group exists.\nTreatment applied single unit small number units.\nperfect untreated comparison group exists.Treatment applied single unit small number units.SCM recommended settings :major policy social event evaluated (e.g., minimum wage laws, tax reforms, advertising campaigns).one treated case exists, several potential controls (e.g., state enacts new law, others ).","code":""},{"path":"sec-synthetic-control.html","id":"marketing-applications","chapter":"32 Synthetic Control","heading":"32.1 Marketing Applications","text":"SCM applied various marketing business contexts:Impact Offline TV Advertising Online Chatter (Tirunillai Tellis 2017)Effect Mobile Hailing Technology Adoption Drivers’ Hourly Earnings (Wang, Wu, Zhu 2019)Impact Payment Disclosure Laws Physician Prescription Behavior (Massachusetts Open Payment Law) (Guo, Sriram, Manchanda 2020)Effect Mandatory GMO Labels Consumer Demand (Vermont case study) (Adalja et al. 2023)","code":""},{"path":"sec-synthetic-control.html","id":"key-features-of-scm","chapter":"32 Synthetic Control","heading":"32.2 Key Features of SCM","text":"SCM estimates asymptotically normal parameters linear panel models pre-treatment period sufficiently long, making natural alternative Difference--Differences model (Arkhangelsky Hirshberg 2023).SCM superior Matching Methods, matches pre-treatment covariates also pre-treatment outcomes. SCM differs Matching Methods :\nMatching methods focus covariates.\nSCM constructs synthetic unit matching pre-treatment outcomes.\nMatching methods focus covariates.SCM constructs synthetic unit matching pre-treatment outcomes.SCM can implemented Bayesian framework (Bayesian Synthetic Control) avoid restrictive priors (S. Kim, Lee, Gupta 2020).","code":""},{"path":"sec-synthetic-control.html","id":"advantages-of-scm","chapter":"32 Synthetic Control","heading":"32.3 Advantages of SCM","text":"","code":""},{"path":"sec-synthetic-control.html","id":"compared-to-did","chapter":"32 Synthetic Control","heading":"32.3.1 Compared to DiD","text":"Maximizes similarity control treated units (including unobservables).Useful untreated unit closely matches treated unit.Objective selection control units, reducing researcher bias.","code":""},{"path":"sec-synthetic-control.html","id":"compared-to-linear-regression","chapter":"32 Synthetic Control","heading":"32.3.2 Compared to Linear Regression","text":"Avoids extrapolation (regression weights outside [0,1]).Provides transparent weights, explicitly showing control unit contributions.require post-treatment outcomes control group (reducing risk p-hacking).","code":""},{"path":"sec-synthetic-control.html","id":"additional-advantages","chapter":"32 Synthetic Control","heading":"32.3.3 Additional Advantages","text":"Selection criteria provide insights relative importance donor unit.Prevents overfitting, since post-intervention outcomes used constructing synthetic control.Enhances interpretability, since synthetic unit constructed using observable pre-treatment characteristics.","code":""},{"path":"sec-synthetic-control.html","id":"disadvantages-of-scm","chapter":"32 Synthetic Control","heading":"32.4 Disadvantages of SCM","text":"Interpretability Weights: can difficult justify exact weights assigned control units.Strong Assumptions: Assumes counterfactual outcome treated unit can expressed linear combination control units.Limited Small Samples: Requires large set pre-treatment periods sufficiently large donor pool.","code":""},{"path":"sec-synthetic-control.html","id":"assumptions-4","chapter":"32 Synthetic Control","heading":"32.5 Assumptions","text":"Good Donor Pool Matching\nsynthetic control closely resemble treated unit pre-treatment periods.\ngap pre-treatment outcomes treated unit synthetic control zero (small).\nsynthetic control closely resemble treated unit pre-treatment periods.gap pre-treatment outcomes treated unit synthetic control zero (small).Contamination\ntreated unit experiences intervention.\nControl units donor pool receive treatment.\ntreated unit experiences intervention.Control units donor pool receive treatment.Major Changes\ntreatment significant event affecting treated unit.\ntreatment significant event affecting treated unit.Linearity Assumption\ncounterfactual outcome treated unit can constructed weighted sum control units.\ncounterfactual outcome treated unit can constructed weighted sum control units.Exlusion Restriction (Identification Assumption):\nexclusion restriction holds conditional pre-treatment outcomes.\nexclusion restriction holds conditional pre-treatment outcomes.Synth package provides algorithm determine optimal weight control unit synthetic control best resembles treated unit treatment.","code":""},{"path":"sec-synthetic-control.html","id":"estimation-3","chapter":"32 Synthetic Control","heading":"32.6 Estimation","text":"observe \\(J + 1\\) units \\(T\\) time periods.first unit (\\(= 1\\)) treated starting time \\(T_0 + 1\\).remaining \\(J\\) units serve donor pool (potential controls).Define:\n\\(Y_{}^\\): Outcome unit \\(\\) treatment (\\(=1\\), \\(t \\geq T_0 + 1\\)).\n\\(Y_{}^N\\): Outcome unit \\(\\) absence treatment (counterfactual).\n\\(Y_{}^\\): Outcome unit \\(\\) treatment (\\(=1\\), \\(t \\geq T_0 + 1\\)).\\(Y_{}^N\\): Outcome unit \\(\\) absence treatment (counterfactual).goal estimate treatment effect:\\[\n\\tau_{1t} = Y_{1t}^- Y_{1t}^N\n\\]observe:\\[\nY_{1t}^= Y_{1t}\n\\]\\(Y_{1t}^N\\) unobserved must estimated using synthetic control.","code":""},{"path":"sec-synthetic-control.html","id":"constructing-the-synthetic-control","chapter":"32 Synthetic Control","heading":"32.6.1 Constructing the Synthetic Control","text":"estimate counterfactual outcome, create synthetic control unit, weighted combination untreated donor units. assign weights \\(\\mathbf{W} = (w_2, \\dots, w_{J+1})'\\) satisfy:Non-negativity constraint:\\[ w_j \\geq 0, \\quad \\forall j = 2, \\dots, J+1 \\]Sum--one constraint:\\[ w_2 + w_3 + \\dots + w_{J+1} = 1 \\]optimal weights found solving:\\[\n\\min_{\\mathbf{W}} ||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}||\n\\]:\\(\\mathbf{X}_1\\) \\(k \\times 1\\) vector pre-treatment characteristics treated unit.\\(\\mathbf{X}_0\\) \\(k \\times J\\) matrix pre-treatment characteristics donor units.common approach minimize weighted sum:\\[\n\\min_{\\mathbf{W}} \\sum_{h=1}^{k} v_h (X_{h1} - w_2 X_{h2} - \\dots - w_{J+1} X_{hJ+1})^2\n\\]:\\(v_h\\) represents predictive power \\(k\\)-dimensional pre-treatment characteristic \\(Y_{1t}^N\\).weights \\(v_h\\) can chosen either:\nExplicitly researcher, \nData-driven via optimization.\nExplicitly researcher, orData-driven via optimization.","code":""},{"path":"sec-synthetic-control.html","id":"penalized-synthetic-control","chapter":"32 Synthetic Control","heading":"32.6.2 Penalized Synthetic Control","text":"reduce interpolation bias, penalized synthetic control method (Abadie L’hour 2021) modifies optimization problem:\\[\n\\min_{\\mathbf{W}} ||\\mathbf{X}_1 - \\sum_{j=2}^{J+1}W_j \\mathbf{X}_j ||^2 + \\lambda \\sum_{j=2}^{J+1} W_j ||\\mathbf{X}_1 - \\mathbf{X}_j||^2\n\\]:\\(\\lambda > 0\\) controls trade-fit regularization:\n\\(\\lambda \\0\\): Standard synthetic control (unpenalized).\n\\(\\lambda \\\\infty\\): Nearest-neighbor matching (strong penalization).\n\\(\\lambda \\0\\): Standard synthetic control (unpenalized).\\(\\lambda \\\\infty\\): Nearest-neighbor matching (strong penalization).method ensures:\nSparse unique solutions weights.\nExclusion dissimilar control units (reducing interpolation bias).\nSparse unique solutions weights.Exclusion dissimilar control units (reducing interpolation bias).final synthetic control estimator :\\[\n\\hat{\\tau}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt}\n\\]\\(Y_{jt}\\) outcome unit \\(j\\) time \\(t\\).","code":""},{"path":"sec-synthetic-control.html","id":"theoretical-considerations","chapter":"32 Synthetic Control","heading":"32.7 Theoretical Considerations","text":"SCM assumes counterfactual outcome follows factor model (Abadie, Diamond, Hainmueller 2010):\\[\nY_{}^N = \\mathbf{\\theta}_t \\mathbf{Z}_i + \\mathbf{\\lambda}_t \\mathbf{\\mu}_i + \\epsilon_{}\n\\]:\\(\\mathbf{Z}_i\\) = Observed characteristics.\\(\\mathbf{\\mu}_i\\) = Unobserved factors.\\(\\epsilon_{}\\) = Transitory shocks (random noise).ensure valid synthetic control, weights \\(\\mathbf{W}^*\\) must satisfy:\\[\n\\sum_{j=2}^{J+1} w_j^* \\mathbf{Z}_j  = \\mathbf{Z}_1\n\\]\\[\n\\sum_{j=2}^{J+1} w_j^* Y_{j1} = Y_{11}, \\quad \\dots, \\quad \\sum_{j=2}^{J+1} w_j^* Y_{jT_0} = Y_{1T_0}\n\\]guarantees synthetic control closely matches treated unit pre-treatment periods.Bias Considerations:accuracy SCM depends ratio transitory shocks (\\(\\epsilon_{}\\)) pre-treatment periods (\\(T_0\\)). words, good fit \\(Y_{1t}\\) pre-treatment period (.e., \\(T_0\\) large small variance \\(\\epsilon_{}\\))Good fit pre-treatment periods (large \\(T_0\\)) crucial.pre-treatment fit poor, bias correction methods required Ben-Michael, Feller, Rothstein (2020).","code":""},{"path":"sec-synthetic-control.html","id":"inference-in-scm","chapter":"32 Synthetic Control","heading":"32.8 Inference in SCM","text":"Unlike traditional methods, SCM rely standard statistical inference due :Undefined sampling mechanism (e.g., one treated unit).SCM deterministic, making p-values difficult interpret.","code":""},{"path":"sec-synthetic-control.html","id":"permutation-placebo-inference","chapter":"32 Synthetic Control","heading":"32.8.1 Permutation (Placebo) Inference","text":"perform inference:Iteratively reassign treatment units donor pool.Estimate placebo treatment effects synthetic control.Compare actual treatment effect placebo distribution.treatment effect considered statistically significant extreme relative placebo distribution.","code":""},{"path":"sec-synthetic-control.html","id":"one-sided-inference","chapter":"32 Synthetic Control","heading":"32.8.2 One-Sided Inference","text":"Recommended due limited number treated cases.permutation test robust standard p-values.benchmark distributions (e.g., uniform permutation distributions), see (Firpo Possebom 2018).","code":""},{"path":"sec-synthetic-control.html","id":"sec-augmented-synthetic-control","chapter":"32 Synthetic Control","heading":"32.9 Augmented Synthetic Control Method","text":"Augmented Synthetic Control Method (ASCM), introduced Ben-Michael, Feller, Rothstein (2021), extends Synthetic Control Method cases perfect pre-treatment fit infeasible. ASCM combines SCM weighting bias correction outcome model, improving estimates SCM alone fails match pre-treatment outcomes precisely.Key Idea:Standard SCM requires synthetic control closely matches treated unit pre-treatment periods.possible, ASCM adjusts bias using outcome modeling, similar bias correction matching estimators.ASCM can seen trade-SCM regression-based approaches, incorporating synthetic control weighting outcome modeling.ASCM builds SCM relaxes strong convex hull assumption. Key assumptions:Interference: Treatment affects treated unit.Interference: Treatment affects treated unit.Unobserved Time-Varying Confounders: Changes time correlated treatment assignment.Unobserved Time-Varying Confounders: Changes time correlated treatment assignment.Regularization Controls Extrapolation Bias: Ridge penalty prevents overfitting.Regularization Controls Extrapolation Bias: Ridge penalty prevents overfitting.ASCM recommended :SCM alone provide good pre-treatment fit.SCM alone provide good pre-treatment fit.one treated unit available.one treated unit available.Auxiliary covariates need incorporated.Auxiliary covariates need incorporated.Advantages ASCMHandles Poor Pre-Treatment Fit\nStandard SCM fails treated unit lies outside convex hull donor units.\nASCM allows negative weights (via ridge regression) improve fit.\nStandard SCM fails treated unit lies outside convex hull donor units.ASCM allows negative weights (via ridge regression) improve fit.Balances Bias Variance\nRidge penalty controls extrapolation, reducing overfitting.\nRidge penalty controls extrapolation, reducing overfitting.Flexible Estimation Framework\nWorks auxiliary covariates, extending beyond pure pre-treatment matching.\nWorks auxiliary covariates, extending beyond pure pre-treatment matching.Let:\\(J + 1\\) units observed \\(T\\) time periods.\\(J + 1\\) units observed \\(T\\) time periods.first unit (\\(=1\\)) treated periods \\(T_0 + 1, \\dots, T\\). - remaining \\(J\\) units donor pool (potential controls).first unit (\\(=1\\)) treated periods \\(T_0 + 1, \\dots, T\\). - remaining \\(J\\) units donor pool (potential controls).Define:\n\\(Y_{}^\\): Outcome unit \\(\\) treatment.\n\\(Y_{}^N\\): Outcome unit \\(\\) absence treatment (counterfactual).\nDefine:\\(Y_{}^\\): Outcome unit \\(\\) treatment.\\(Y_{}^\\): Outcome unit \\(\\) treatment.\\(Y_{}^N\\): Outcome unit \\(\\) absence treatment (counterfactual).\\(Y_{}^N\\): Outcome unit \\(\\) absence treatment (counterfactual).treatment effect interest:\\[\n\\tau_{1t} = Y_{1t}^- Y_{1t}^N\n\\]:\\[\nY_{1t}^= Y_{1t}\n\\]\\(Y_{1t}^N\\) unobserved must estimated.ASCM improves SCM incorporating outcome model correct poor pre-treatment fit. counterfactual outcome estimated :\\[\n\\hat{Y}^{\\text{aug}}_{1T}(0) = \\sum_{=2}^{J+1} w_i Y_{} + \\left( m_1 - \\sum_{=2}^{J+1} w_i m_i \\right)\n\\]:\\(w_i\\) SCM weights chosen best match pre-treatment outcomes.\\(w_i\\) SCM weights chosen best match pre-treatment outcomes.\\(m_i\\) outcome model prediction unit \\(\\).\\(m_i\\) outcome model prediction unit \\(\\).SCM achieves perfect pre-treatment fit, \\(m_1 - \\sum w_i m_i \\approx 0\\), ASCM reduces standard SCM.SCM achieves perfect pre-treatment fit, \\(m_1 - \\sum w_i m_i \\approx 0\\), ASCM reduces standard SCM.common implementation, Ridge ASCM, uses ridge regression estimate \\(m_i\\), leading :\\[\n\\hat{Y}^{\\text{aug}}_{1T}(0) = \\sum_{=2}^{J+1} w_i Y_{} + \\left( X_1 - \\sum w_i X_i \\right) \\beta\n\\]\\(\\beta\\) estimated using ridge regression post-treatment outcomes pre-treatment outcomes.","code":""},{"path":"sec-synthetic-control.html","id":"synthetic-control-with-staggered-adoption","chapter":"32 Synthetic Control","heading":"32.10 Synthetic Control with Staggered Adoption","text":"traditional SCM focuses cases single treated unit, many real-world policies exhibit staggered adoption, different units receive treatment different times.staggered adoption designs, policy implementation occurs multiple time periods across different units (e.g., states, companies, regions). presents challenges:Traditional SCM limitations: SCM designed single treated unit naturally accommodate multiple adoption times.Heterogeneous treatment effects: impact intervention may vary time across units.Estimation bias: Common approaches Two-Way Fixed Effects can yield biased results treatment effects heterogeneous.","code":""},{"path":"sec-synthetic-control.html","id":"partially-pooled-synthetic-control","chapter":"32 Synthetic Control","heading":"32.10.1 Partially Pooled Synthetic Control","text":"Ben-Michael, Feller, Rothstein (2022) propose partially pooled SCM approach, balancing trade-offs separate SCM unit fully pooled approach estimates single synthetic control treated units. key ideas include:Two imbalance measures:\nIndividual unit-level imbalance.\nAggregate imbalance average treated unit.\nIndividual unit-level imbalance.Aggregate imbalance average treated unit.Optimization framework:\nWeights chosen minimize weighted sum two imbalance measures.\nmethod reduces bias compared estimating separate SCM models unit​.\nWeights chosen minimize weighted sum two imbalance measures.method reduces bias compared estimating separate SCM models unit​.Mathematically, let:\\(Y_{}\\) outcome interest unit \\(\\) time \\(t\\).\\(Y_{}\\) outcome interest unit \\(\\) time \\(t\\).\\(T_i\\) treatment adoption time unit \\(\\).\\(T_i\\) treatment adoption time unit \\(\\).\\(W_i\\) synthetic control weight assigned unit \\(\\).\\(W_i\\) synthetic control weight assigned unit \\(\\)., treated unit \\(j\\), estimated counterfactual :\\[ \\hat{Y}_{jT_j+k} = \\sum_{=1}^{N} \\hat{W}_{ij} Y_{iT_j+k} \\]objective function combines individual aggregate balance constraints:\\[ \\min_W \\lambda \\sum_{j} ||Y_{j,\\text{pre}} - \\sum_i W_{ij} Y_{,\\text{pre}}||^2 + (1-\\lambda) ||Y_{\\text{treated, pre}} - \\sum_i W_i Y_{,\\text{pre}}||^2 \\]\\(\\lambda\\) tuning parameter controls trade-individual pooled balance​.","code":"\n# Load necessary library\nlibrary(augsynth)"},{"path":"sec-synthetic-control.html","id":"generalized-synthetic-control","chapter":"32 Synthetic Control","heading":"32.11 Generalized Synthetic Control","text":"Generalized Synthetic Control (GSC) Method extends synthetic control approach accommodate multiple treated units heterogeneous treatment effects relaxing parallel trends assumption required difference--differences. Originally developed Xu (2017), GSC method integrates interactive fixed effects models, improving efficiency robustness time-series cross-sectional (TSCS) data.","code":""},{"path":"sec-synthetic-control.html","id":"the-problem-with-traditional-methods","chapter":"32 Synthetic Control","heading":"32.11.1 The Problem with Traditional Methods","text":"Traditional causal inference methods require parallel trends assumption: \\[\nE[Y_{}(0) | D_i = 1] - E[Y_{}(0) | D_i = 0] = \\text{constant}\n\\] states absence treatment, difference outcomes treated control units remained constant time. However, assumption often fails due :Time-varying unobserved confounders affecting treatment assignment outcomes.Heterogeneous treatment effects across units time.Multiple treatment periods different units adopt treatment different times.address limitations, GSC builds interactive fixed effects model, allows unit-specific time-specific latent factors can capture unobserved confounding trends.","code":""},{"path":"sec-synthetic-control.html","id":"generalized-synthetic-control-model","chapter":"32 Synthetic Control","heading":"32.11.2 Generalized Synthetic Control Model","text":"Let \\(Y_{}\\) represent observed outcome unit \\(\\) time \\(t\\), define potential outcomes framework: \\[\nY_{}(d) = \\mu_{} + \\delta_{} d + \\varepsilon_{}, \\quad d \\\\{0,1\\}\n\\] :\\(\\mu_{}\\) represents latent factor structure untreated outcomes.\\(\\mu_{}\\) represents latent factor structure untreated outcomes.\\(\\delta_{}\\) treatment effect.\\(\\delta_{}\\) treatment effect.\\(\\varepsilon_{}\\) idiosyncratic error term.\\(\\varepsilon_{}\\) idiosyncratic error term.interactive fixed effects model, assume untreated outcome follows: \\[\n\\mu_{} = X_{} \\beta + \\lambda_i' f_t\n\\] :\\(X_{}\\) vector observed covariates.\\(X_{}\\) vector observed covariates.\\(\\beta\\) vector unknown coefficients.\\(\\beta\\) vector unknown coefficients.\\(\\lambda_i\\) represents unit-specific factor loadings.\\(\\lambda_i\\) represents unit-specific factor loadings.\\(f_t\\) represents time-specific factors.\\(f_t\\) represents time-specific factors.presence \\(\\lambda_i' f_t\\) allows GSC control unobserved confounders vary across time units, key advantage traditional SCM.","code":""},{"path":"sec-synthetic-control.html","id":"identification-and-estimation","chapter":"32 Synthetic Control","heading":"32.11.3 Identification and Estimation","text":"estimate Average Treatment Effect Treated, define: \\[\n\\text{ATT}_t = \\frac{1}{N_T} \\sum_{\\T} \\left[ Y_{}(1) - Y_{}(0) \\right]\n\\] \\(N_T\\) number treated units. challenge \\(Y_{}(0)\\) treated units counterfactual must estimated.Step 1: Estimating Factor Loadings Latent FactorsUsing control units, estimate latent factors factor loadings: \\[\nY_{} = X_{} \\beta + \\lambda_i' f_t + \\varepsilon_{}, \\quad \\C\n\\] can rewritten matrix form: \\[\nY_C = X_C \\beta + \\Lambda_C F' + E_C.\n\\] key assumption factor loadings latent factors apply treated control units, ensuring valid counterfactual estimation.Step 2: Imputing Counterfactual OutcomesFor treated units, estimate: \\[\n\\hat{\\lambda}_i = (F_0'F_0)^{-1} F_0' (Y_{,0} - X_{,0} \\beta)\n\\] \\(F_0\\) \\(Y_{,0}\\) denote pre-treatment data. imputed counterfactuals :\\[\n\\hat{Y}_{}(0) = X_{} \\beta + \\hat{\\lambda}_i' \\hat{f}_t.\n\\]","code":""},{"path":"sec-synthetic-control.html","id":"bootstrap-procedure-for-standard-errors","chapter":"32 Synthetic Control","heading":"32.11.4 Bootstrap Procedure for Standard Errors","text":"key issue statistical inference GSC estimation uncertainty. standard nonparametric bootstrap biased due dependent structures panel data adopt parametric bootstrap (K. T. Li Sonnier 2023) correct bias.Corrected Bootstrap Algorithm:Estimate IFE Model using control units.Resample residuals \\(\\hat{\\varepsilon}_{}\\) fitted model.Generate new synthetic datasets using: \\[\nY_{}^* = X_{} \\hat{\\beta} + \\hat{\\lambda}_i' \\hat{f}_t + \\varepsilon_{}^*\n\\]Re-estimate model resampled data compute bootstrap confidence intervals.approach ensures correct coverage probabilities avoids bias standard error estimation.","code":"# Load required package\nlibrary(gsynth)\n\n# Example data\ndata(\"gsynth\")\n\n# Fit Generalized Synthetic Control Model\ngsc_model <-\n    gsynth(\n        Y ~ D + X1 + X2,\n        data = simdata,\n        parallel = FALSE,\n        index = c(\"id\", \"time\"),\n        force = \"two-way\",\n        CV = TRUE,\n        r = c(0, 5),\n        se = T\n    )\n#> Cross-validating ... \n#>  r = 0; sigma2 = 1.84865; IC = 1.02023; PC = 1.74458; MSPE = 2.37280\n#>  r = 1; sigma2 = 1.51541; IC = 1.20588; PC = 1.99818; MSPE = 1.71743\n#>  r = 2; sigma2 = 0.99737; IC = 1.16130; PC = 1.69046; MSPE = 1.14540*\n#>  r = 3; sigma2 = 0.94664; IC = 1.47216; PC = 1.96215; MSPE = 1.15032\n#>  r = 4; sigma2 = 0.89411; IC = 1.76745; PC = 2.19241; MSPE = 1.21397\n#>  r = 5; sigma2 = 0.85060; IC = 2.05928; PC = 2.40964; MSPE = 1.23876\n#> \n#>  r* = 2\n#> \n#> \nBootstrapping ...\n#> ..\n\n# Summary of results\nsummary(gsc_model)\n#>              Length Class   Mode     \n#> Y.dat         1500  -none-  numeric  \n#> Y                1  -none-  character\n#> D                1  -none-  character\n#> X                2  -none-  character\n#> W                0  -none-  NULL     \n#> index            2  -none-  character\n#> id              50  -none-  numeric  \n#> time            30  -none-  numeric  \n#> obs.missing   1500  -none-  numeric  \n#> id.tr            5  -none-  numeric  \n#> id.co           45  -none-  numeric  \n#> D.tr           150  -none-  numeric  \n#> I.tr           150  -none-  numeric  \n#> Y.tr           150  -none-  numeric  \n#> Y.ct           150  -none-  numeric  \n#> Y.co          1350  -none-  numeric  \n#> eff            150  -none-  numeric  \n#> Y.bar           90  -none-  numeric  \n#> att             30  -none-  numeric  \n#> att.avg          1  -none-  numeric  \n#> force            1  -none-  numeric  \n#> sameT0           1  -none-  logical  \n#> T                1  -none-  numeric  \n#> N                1  -none-  numeric  \n#> p                1  -none-  numeric  \n#> Ntr              1  -none-  numeric  \n#> Nco              1  -none-  numeric  \n#> T0               5  -none-  numeric  \n#> tr              50  -none-  logical  \n#> pre            150  -none-  logical  \n#> post           150  -none-  logical  \n#> r.cv             1  -none-  numeric  \n#> IC               1  -none-  numeric  \n#> PC               1  -none-  numeric  \n#> beta             2  -none-  numeric  \n#> est.co          13  -none-  list     \n#> mu               1  -none-  numeric  \n#> validX           1  -none-  numeric  \n#> sigma2           1  -none-  numeric  \n#> res.co        1350  -none-  numeric  \n#> MSPE             1  -none-  numeric  \n#> CV.out          30  -none-  numeric  \n#> niter            1  -none-  numeric  \n#> factor          60  -none-  numeric  \n#> lambda.co       90  -none-  numeric  \n#> lambda.tr       10  -none-  numeric  \n#> wgt.implied    225  -none-  numeric  \n#> alpha.tr         5  -none-  numeric  \n#> alpha.co        45  -none-  numeric  \n#> xi              30  -none-  numeric  \n#> inference        1  -none-  character\n#> est.att        180  -none-  numeric  \n#> est.avg          5  -none-  numeric  \n#> att.avg.boot   200  -none-  numeric  \n#> att.boot      6000  -none-  numeric  \n#> eff.boot     30000  -none-  numeric  \n#> Dtr.boot     30000  -none-  numeric  \n#> Itr.boot     30000  -none-  numeric  \n#> beta.boot      400  -none-  numeric  \n#> est.beta        10  -none-  numeric  \n#> call             9  -none-  call     \n#> formula          3  formula call\n\n# Visualization\nplot(gsc_model)"},{"path":"sec-synthetic-control.html","id":"bayesian-synthetic-control","chapter":"32 Synthetic Control","heading":"32.12 Bayesian Synthetic Control","text":"Bayesian Synthetic Control (BSC) approach introduces probabilistic alternative traditional synthetic control methods. Unlike standard SCM, estimates single point estimate treatment effects using convex combination control units, BSC incorporates posterior predictive distributions, allowing proper uncertainty quantification probabilistic inference​.Bayesian methods offer several advantages frequentist SCM:Probabilistic Treatment Effects: Instead single deterministic estimate, Bayesian SCM provides distribution possible treatment effects.Probabilistic Treatment Effects: Instead single deterministic estimate, Bayesian SCM provides distribution possible treatment effects.Regularization via Priors: Bayesian approaches allow incorporation shrinkage priors improve estimation stability high-dimensional settings.Regularization via Priors: Bayesian approaches allow incorporation shrinkage priors improve estimation stability high-dimensional settings.Flexibility: Bayesian framework can accommodate dynamic latent factor models, addressing issues like time-varying heterogeneity​.Flexibility: Bayesian framework can accommodate dynamic latent factor models, addressing issues like time-varying heterogeneity​.Two major Bayesian approaches SCM:Dynamic Multilevel Factor Models (Pang, Liu, Xu 2022)Bayesian Sparse Synthetic Control (S. Kim, Lee, Gupta 2020)","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-causal-inference-framework","chapter":"32 Synthetic Control","heading":"32.12.1 Bayesian Causal Inference Framework","text":"traditional SCM, estimate counterfactual outcome \\(Y_{}(0)\\) treated unit \\(\\) time \\(t\\) using weighted sum control units:\\[\n\\hat{Y}_{}(0) = \\sum_{j \\neq } w_j Y_{jt}.\n\\]However, deterministic approach quantify uncertainty estimation. Bayesian SCM instead models counterfactual outcome posterior predictive distribution:\\[\nP(Y_{}(0) | Y_{\\text{obs}}, \\theta),\n\\]\\(\\theta\\) represents parameters model (e.g., factor loadings, regression coefficients, latent variables). Bayesian approach estimates full posterior distributions, allowing us compute credible intervals instead relying solely point estimates.","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-dynamic-multilevel-factor-model","chapter":"32 Synthetic Control","heading":"32.12.2 Bayesian Dynamic Multilevel Factor Model","text":"Dynamic Multilevel Factor Model (DM-LFM), proposed Pang, Liu, Xu (2022), extends SCM incorporating latent factor models correct unit-specific time trends.","code":""},{"path":"sec-synthetic-control.html","id":"model-specification-4","chapter":"32 Synthetic Control","heading":"32.12.2.1 Model Specification","text":"Let \\(Y_{}\\) observed outcome unit \\(\\) time \\(t\\). potential untreated outcome follows:\\[\nY_{}(0) = X_{} \\beta + \\lambda_i' f_t + \\varepsilon_{},\n\\]:\\(X_{}\\) observed covariates,\\(X_{}\\) observed covariates,\\(\\beta\\) regression coefficients,\\(\\beta\\) regression coefficients,\\(\\lambda_i\\) unit-specific factor loadings,\\(\\lambda_i\\) unit-specific factor loadings,\\(f_t\\) common latent time factors,\\(f_t\\) common latent time factors,\\(\\varepsilon_{} \\sim \\mathcal{N}(0, \\sigma^2)\\) noise term.\\(\\varepsilon_{} \\sim \\mathcal{N}(0, \\sigma^2)\\) noise term.treatment effect defined :\\[\n\\tau_{} = Y_{}(1) - Y_{}(0).\n\\]latent ignorability, assume:\\[\nP(T_i | X_i, U_i) = P(T_i | X_i),\n\\]\\(U_i\\) latent variables extracted outcome data​.","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-inference-procedure","chapter":"32 Synthetic Control","heading":"32.12.2.2 Bayesian Inference Procedure","text":"estimate treatment effects, follow steps:Estimate \\(\\lambda_i\\) \\(f_t\\) using control units.Predict counterfactuals treated units: \\[\n\\hat{Y}_{}(0) = X_{} \\hat{\\beta} + \\hat{\\lambda}_i' \\hat{f}_t.\n\\]Obtain posterior predictive distribution treatment effects.Bayesian approach enables proper credible intervals causal effects.","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-sparse-synthetic-control","chapter":"32 Synthetic Control","heading":"32.12.3 Bayesian Sparse Synthetic Control","text":"S. Kim, Lee, Gupta (2020) propose alternative Bayesian framework removes restrictive constraints imposed standard SCM​.","code":""},{"path":"sec-synthetic-control.html","id":"relaxing-scm-constraints","chapter":"32 Synthetic Control","heading":"32.12.3.1 Relaxing SCM Constraints","text":"Traditional SCM imposes:Nonnegative weights: \\(w_j \\geq 0\\).Convex combination: \\(\\sum_j w_j = 1\\).BSCM relaxes allowing negative weights regularization priors. models control unit weights using Bayesian shrinkage priors:\\[\nw_j \\sim \\mathcal{N}(0, \\tau^2),\n\\]\\(\\tau^2\\) regularization parameter. allows flexible weight selection preventing overfitting.","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-shrinkage-priors","chapter":"32 Synthetic Control","heading":"32.12.3.2 Bayesian Shrinkage Priors","text":"BSCM incorporates horseshoe priors spike--slab priors select relevant control units:Horseshoe Prior: \\[\n  w_j \\sim \\mathcal{N}(0, \\lambda_j^2), \\quad \\lambda_j \\sim C^+(0,1).\n  \\]Horseshoe Prior: \\[\n  w_j \\sim \\mathcal{N}(0, \\lambda_j^2), \\quad \\lambda_j \\sim C^+(0,1).\n  \\]Spike--Slab Prior: \\[\n  w_j \\sim \\gamma_j \\mathcal{N}(0, \\sigma_1^2) + (1-\\gamma_j) \\mathcal{N}(0, \\sigma_0^2),\n  \\] \\(\\gamma_j \\sim \\text{Bernoulli}(\\pi)\\) determines whether control unit included.Spike--Slab Prior: \\[\n  w_j \\sim \\gamma_j \\mathcal{N}(0, \\sigma_1^2) + (1-\\gamma_j) \\mathcal{N}(0, \\sigma_0^2),\n  \\] \\(\\gamma_j \\sim \\text{Bernoulli}(\\pi)\\) determines whether control unit included.priors ensure robust weight selection controlling overfitting.","code":""},{"path":"sec-synthetic-control.html","id":"bayesian-inference-and-mcmc-estimation","chapter":"32 Synthetic Control","heading":"32.12.4 Bayesian Inference and MCMC Estimation","text":"DM-LFM BSCM estimated using Markov Chain Monte Carlo (MCMC). Given observed data \\(Y_{\\text{obs}}\\), sample posterior:\\[\nP(\\theta | Y_{\\text{obs}}) \\propto P(Y_{\\text{obs}} | \\theta) P(\\theta),\n\\]\\(P(\\theta)\\) encodes prior beliefs parameters.Common MCMC techniques used:Gibbs Sampling latent factors regression coefficients.Gibbs Sampling latent factors regression coefficients.Hamiltonian Monte Carlo (HMC) high-dimensional posteriors.Hamiltonian Monte Carlo (HMC) high-dimensional posteriors.","code":"\n# Load necessary libraries\nlibrary(rstan)\nlibrary(bayesplot)\n\n# Define Bayesian SCM model in Stan\nscm_model <- \"\ndata {\n  int<lower=0> N;  // Number of observations\n  int<lower=0> T;  // Time periods\n  matrix[N, T] Y;  // Outcome matrix\n}\nparameters {\n  vector[T] f;  // Latent factors\n  vector[N] lambda; // Factor loadings\n  real<lower=0> sigma; // Noise variance\n}\nmodel {\n  // Priors\n  f ~ normal(0, 1);\n  lambda ~ normal(0, 1);\n\n  // Likelihood\n  for (i in 1:N)\n    Y[i, ] ~ normal(lambda[i] * f, sigma);\n}\n\"\n\n# Compile and fit the model\nfit <-\n  stan(model_code = scm_model, data = list(\n    N = 50,\n    T = 20,\n    Y = matrix(rnorm(1000), 50, 20)\n  ))\n\n# Summarize results\nprint(fit)"},{"path":"sec-synthetic-control.html","id":"using-multiple-outcomes-to-improve-the-synthetic-control-method","chapter":"32 Synthetic Control","heading":"32.13 Using Multiple Outcomes to Improve the Synthetic Control Method","text":"Typically, SCM constructs weighted combination untreated control units approximate counterfactual outcome treated unit. However, standard SCM limited single outcome variable, can lead biased estimates multiple correlated outcomes available.work, L. Sun, Ben-Michael, Feller (2023) propose novel extension SCM leverages multiple outcome variables improve causal inference :Using common set synthetic control weights across outcomes rather estimating separate weights outcome.Reducing bias using low-rank factor model, exploits shared latent structures across outcomes.","code":""},{"path":"sec-synthetic-control.html","id":"standard-synthetic-control-method","chapter":"32 Synthetic Control","heading":"32.13.1 Standard Synthetic Control Method","text":"Let \\(Y_{itk}\\) denote observed outcome unit \\(\\) time \\(t\\) outcome \\(k\\), \\(= 1, \\dots, N\\), \\(t = 1, \\dots, T\\), \\(k = 1, \\dots, K\\). potential outcomes framework assumes:\\[\nY_{itk}(d) = \\mu_{itk} + \\delta_{itk} d + \\varepsilon_{itk}, \\quad d \\\\{0,1\\}\n\\]:\\(\\mu_{itk}\\) represents latent structure untreated outcomes.\\(\\mu_{itk}\\) represents latent structure untreated outcomes.\\(\\delta_{itk}\\) treatment effect.\\(\\delta_{itk}\\) treatment effect.\\(\\varepsilon_{itk} \\sim \\mathcal{N}(0, \\sigma^2)\\) random noise.\\(\\varepsilon_{itk} \\sim \\mathcal{N}(0, \\sigma^2)\\) random noise.unit \\(=1\\) (treated unit), observed outcome follows:\\[\nY_{1tk} = Y_{1tk}(0) + D_{1t} \\delta_{1tk}\n\\]\\(D_{1t}\\) indicator treatment time \\(t\\). challenge estimate counterfactual outcome \\(Y_{1tk}(0)\\), unobserved post-treatment.SCM estimates \\(Y_{1tk}(0)\\) weighted combination control units:\\[\n\\hat{Y}_{1tk}(0) = \\sum_{=2}^{N} w_i Y_{itk}\n\\]weights \\(w_i\\) chosen minimize pre-treatment imbalance.","code":""},{"path":"sec-synthetic-control.html","id":"using-multiple-outcomes-for-bias-reduction","chapter":"32 Synthetic Control","heading":"32.13.2 Using Multiple Outcomes for Bias Reduction","text":"Instead estimating separate weights \\(w_k\\) outcome \\(k\\), L. Sun, Ben-Michael, Feller (2023) propose single set weights \\(w\\) across outcomes. approach justified low-rank factor model, assumes multiple outcomes share common latent factors.","code":""},{"path":"sec-synthetic-control.html","id":"low-rank-factor-model","chapter":"32 Synthetic Control","heading":"32.13.2.1 Low-Rank Factor Model","text":"Assume untreated potential outcome follows linear factor structure:\\[\nY_{itk}(0) = X_{} \\beta_k + \\lambda_i' f_{tk} + \\varepsilon_{itk}\n\\]:\\(X_{}\\) observed covariates.\\(X_{}\\) observed covariates.\\(\\beta_k\\) outcome-specific coefficients.\\(\\beta_k\\) outcome-specific coefficients.\\(\\lambda_i\\) unit-specific factor loadings.\\(\\lambda_i\\) unit-specific factor loadings.\\(f_{tk}\\) time--outcome-specific latent factors.\\(f_{tk}\\) time--outcome-specific latent factors.outcomes share latent factor structure, bias synthetic control estimation can reduced factor \\(1 / \\sqrt{K}\\) number outcomes \\(K\\) increases.","code":""},{"path":"sec-synthetic-control.html","id":"estimation-methods-1","chapter":"32 Synthetic Control","heading":"32.13.3 Estimation Methods","text":"L. Sun, Ben-Michael, Feller (2023) propose two methods constructing common synthetic control:Concatenated Outcome Weights: Estimate weights minimizing imbalance across outcomes simultaneously:\n\\[\n\\hat{w} = \\arg\\min_w \\sum_{k=1}^{K} || Y_{1,\\text{pre},k} - \\sum_{=2}^{N} w_i Y_{,\\text{pre},k} ||^2\n\\]Concatenated Outcome Weights: Estimate weights minimizing imbalance across outcomes simultaneously:\\[\n\\hat{w} = \\arg\\min_w \\sum_{k=1}^{K} || Y_{1,\\text{pre},k} - \\sum_{=2}^{N} w_i Y_{,\\text{pre},k} ||^2\n\\]Averaged Outcome Weights: Estimate weights based linear combination (e.g., average) outcomes:\n\\[\n\\hat{w} = \\arg\\min_w || \\frac{1}{K} \\sum_{k=1}^{K} Y_{1,\\text{pre},k} - \\sum_{=2}^{N} w_i \\frac{1}{K} \\sum_{k=1}^{K} Y_{,\\text{pre},k} ||^2\n\\]Averaged Outcome Weights: Estimate weights based linear combination (e.g., average) outcomes:\\[\n\\hat{w} = \\arg\\min_w || \\frac{1}{K} \\sum_{k=1}^{K} Y_{1,\\text{pre},k} - \\sum_{=2}^{N} w_i \\frac{1}{K} \\sum_{k=1}^{K} Y_{,\\text{pre},k} ||^2\n\\]methods improve SCM performance reducing variance overfitting noise.","code":""},{"path":"sec-synthetic-control.html","id":"empirical-application-flint-water-crisis","chapter":"32 Synthetic Control","heading":"32.13.4 Empirical Application: Flint Water Crisis","text":"illustrate benefits multiple outcome SCM, L. Sun, Ben-Michael, Feller (2023) re-analyze Flint water crisis, led lead contamination drinking water, potentially affecting student performance.Four key educational outcomes studied:Math AchievementReading AchievementSpecial Needs StatusDaily AttendanceBy applying common weights across outcomes, SCM results showed:Reduced bias improved robustness compared separate SCM fits.Reduced bias improved robustness compared separate SCM fits.Better pre-treatment fit educational outcomes.Better pre-treatment fit educational outcomes.Stronger evidence educational impacts following crisis.Stronger evidence educational impacts following crisis.","code":"\n# Load necessary libraries\nlibrary(augsynth)\n\n# Fit SCM using a common set of weights across multiple outcomes\nsynth_model <- augsynth_multiout()"},{"path":"sec-synthetic-control.html","id":"applications","chapter":"32 Synthetic Control","heading":"32.14 Applications","text":"","code":""},{"path":"sec-synthetic-control.html","id":"synthetic-control-estimation","chapter":"32 Synthetic Control","heading":"32.14.1 Synthetic Control Estimation","text":"example simulates data 10 states 30 years, State receives treatment year 15. implement two approaches:Standard Synthetic Control using Synth.Generalized Synthetic Control using gsynth, incorporates iterative fixed effects multiple treated units.Load Required LibrariesWe construct panel dataset outcome variable \\(Y\\) influenced two covariates (\\(X_1, X_2\\)). Treatment (\\(T\\)) applied State year 15 onwards, artificial treatment effect +20.Step 1: Estimating Synthetic Control SynthThe Synth package creates synthetic control matching pre-treatment trends treated unit (State ) weighted combination donor states.specify predictors, dependent variable, donor pool.Fit Synthetic Control ModelPlot: Observed vs. Synthetic State APlot: Treatment Effect (Gaps Plot)gaps plot shows difference State synthetic control time.Step 2: Estimating Generalized Synthetic Control gsynthUnlike Synth, gsynth package supports:Iterative Fixed Effects (IFE) unobserved heterogeneity.Iterative Fixed Effects (IFE) unobserved heterogeneity.Multiple Treated Units Staggered Adoption.Multiple Treated Units Staggered Adoption.Bootstrapped Standard Errors inference.Bootstrapped Standard Errors inference.Fit Generalized Synthetic Control ModelWe use two-way fixed effects, cross-validation (CV = TRUE), bootstrapped standard errors.","code":"\n# install.packages(\"Synth\")\n# install.packages(\"gsynth\")\nlibrary(\"Synth\")\nlibrary(\"gsynth\")\n# Simulate panel data\nset.seed(1)\n\ndf <- data.frame(\n  year      = rep(1:30, 10),          # 30 years for 10 states\n  state     = rep(LETTERS[1:10], each = 30),  # States A to J\n  X1        = round(rnorm(300, 2, 1), 2),     # Continuous covariate\n  X2        = round(rbinom(300, 1, 0.5) + rnorm(300), 2),  # Binary + noise\n  state.num = rep(1:10, each = 30)    # Numeric identifier for states\n)\n\n# Outcome variable: Y = 1 + 2 * X1 + noise\ndf$Y <- round(1 + 2 * df$X1 + rnorm(300), 2)\n\n# Assign treatment: Only State A (state == \"A\") from year 15 onward\ndf$T <- as.integer(df$state == \"A\" & df$year >= 15)\n\n# Apply treatment effect: Increase Y by 20 where treatment is applied\ndf$Y[df$T == 1] <- df$Y[df$T == 1] + 20\n\n# View data structure\nstr(df)\n#> 'data.frame':    300 obs. of  7 variables:\n#>  $ year     : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ state    : chr  \"A\" \"A\" \"A\" \"A\" ...\n#>  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...\n#>  $ X2       : num  1.96 0.4 -0.75 -0.56 -0.45 1.06 0.51 -2.1 0 0.54 ...\n#>  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...\n#>  $ T        : int  0 0 0 0 0 0 0 0 0 0 ...\ndataprep.out <- dataprep(\n  df,\n  predictors = c(\"X1\", \"X2\"),\n  dependent = \"Y\",\n  unit.variable = \"state.num\",\n  time.variable = \"year\",\n  unit.names.variable = \"state\",\n  treatment.identifier = 1,\n  controls.identifier = 2:10,\n  time.predictors.prior = 1:14,\n  time.optimize.ssr = 1:14,\n  time.plot = 1:30\n)\nsynth.out <- synth(dataprep.out)\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 9.831789 \n#> \n#> solution.v:\n#>  0.3888387 0.6111613 \n#> \n#> solution.w:\n#>  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853\n\n# Display synthetic control weights and balance table\nprint(synth.tab(dataprep.res = dataprep.out, synth.res = synth.out))\n#> $tab.pred\n#>    Treated Synthetic Sample Mean\n#> X1   2.028     2.028       2.017\n#> X2   0.513     0.513       0.394\n#> \n#> $tab.v\n#>    v.weights\n#> X1 0.389    \n#> X2 0.611    \n#> \n#> $tab.w\n#>    w.weights unit.names unit.numbers\n#> 2      0.112          B            2\n#> 3      0.183          C            3\n#> 4      0.103          D            4\n#> 5      0.312          E            5\n#> 6      0.061          F            6\n#> 7      0.035          G            7\n#> 8      0.059          H            8\n#> 9      0.057          I            9\n#> 10     0.078          J           10\n#> \n#> $tab.loss\n#>            Loss W   Loss V\n#> [1,] 9.761708e-12 9.831789\npath.plot(\n  synth.out,\n  dataprep.out,\n  Ylab = \"Y\",\n  Xlab = \"Year\",\n  Legend = c(\"State A\", \"Synthetic State A\"),\n  Legend.position = \"topleft\"\n)\n\n# Mark the treatment start year\nabline(v = 15, lty = 2)  \ngaps.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Gap\"),\n          Xlab         = c(\"Year\"),\n          Ylim         = c(-30, 30),\n          Main         = \"\"\n)\n\nabline(v   = 15, lty = 2)gsynth.out <- gsynth(\n  Y ~ T + X1 + X2,\n  data = df,\n  index = c(\"state\", \"year\"),\n  force = \"two-way\",\n  CV = TRUE,\n  r = c(0, 5),\n  se = TRUE,\n  inference = \"parametric\",\n  nboots = 100\n)\n#> Parallel computing ...\n#> Cross-validating ... \n#>  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502\n#>  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375\n#>  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*\n#>  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319\n#>  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301\n#>  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596\n#> \n#>  r* = 2\n#> \n#> \nSimulating errors ...\nBootstrapping ...\n#> \n# Plot Estimated Treatment Effects\nplot(gsynth.out)\n\n# Plot Counterfactual Trends\nplot(gsynth.out, type = \"counterfactual\")\n\n# Show Estimations for Control Cases\nplot(gsynth.out, type = \"counterfactual\", raw = \"all\") "},{"path":"sec-synthetic-control.html","id":"the-basque-country-policy-change","chapter":"32 Synthetic Control","heading":"32.14.2 The Basque Country Policy Change","text":"Basque Country Spain implemented major policy change 1975, impacting GDP per capita growth. example uses SCM estimate counterfactual economic performance policy implemented.:Construct synthetic control using economic predictors. 2Evaluate policy’s impact comparing real synthetic Basque GDP.basque dataset Synth contains economic indicators Spain’s regions 1955 1997.Step 1: Preparing Data SynthWe define predictors specify pre-treatment (1960–1969) post-treatment (1975–1997) periods.Step 2: Estimating Synthetic ControlWe now estimate synthetic control weights using synth(). solves weights best match pre-treatment economic indicators Basque Country.Step 3: Evaluating Treatment EffectsCalculate GDP Gap Real Synthetic Basque CountryThe synth.tab() function provides pre-treatment fit diagnostics predictor weights.Importance Control Region Synthetic Basque CountryStep 4: Visualizing ResultsPlot Real vs. Synthetic GDP Per CapitaPlot Treatment Effect (GDP Gap)gap plot shows difference actual Basque GDP synthetic control time. large post-treatment deviation suggests policy impact.","code":"\nlibrary(Synth)\ndata(\"basque\")\ndim(basque)  # 774 observations, 17 variables\n#> [1] 774  17\nhead(basque)\n#>   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry\n#> 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA\n#> 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA\n#> 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA\n#> 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA\n#> 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA\n#> 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA\n#>   sec.construction sec.services.venta sec.services.nonventa school.illit\n#> 1               NA                 NA                    NA           NA\n#> 2               NA                 NA                    NA           NA\n#> 3               NA                 NA                    NA           NA\n#> 4               NA                 NA                    NA           NA\n#> 5               NA                 NA                    NA           NA\n#> 6               NA                 NA                    NA           NA\n#>   school.prim school.med school.high school.post.high popdens invest\n#> 1          NA         NA          NA               NA      NA     NA\n#> 2          NA         NA          NA               NA      NA     NA\n#> 3          NA         NA          NA               NA      NA     NA\n#> 4          NA         NA          NA               NA      NA     NA\n#> 5          NA         NA          NA               NA      NA     NA\n#> 6          NA         NA          NA               NA      NA     NA\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op = \"mean\",\n    time.predictors.prior = 1964:1969,\n    # Pre-treatment period\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969, \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969, \"mean\")\n    ),\n    dependent = \"gdpcap\",\n    unit.variable = \"regionno\",\n    unit.names.variable = \"regionname\",\n    time.variable = \"year\",\n    treatment.identifier = 17,\n    # Basque region\n    controls.identifier = c(2:16, 18),\n    # Control regions\n    time.optimize.ssr = 1960:1969,\n    time.plot = 1955:1997\n)\nsynth.out = synth(data.prep.obj = dataprep.out, method = \"BFGS\")\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 0.008864606 \n#> \n#> solution.v:\n#>  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n#> \n#> solution.w:\n#>  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps = dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w)\ngaps[1:3,1]  # First three differences\n#>       1955       1956       1957 \n#> 0.15023473 0.09168035 0.03716475\nsynth.tables = synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nnames(synth.tables)\n#> [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\n\n# Predictor Balance Table\nsynth.tables$tab.pred[1:13,]\n#>                                          Treated Synthetic Sample Mean\n#> school.illit                              39.888   256.337     170.786\n#> school.prim                             1031.742  2730.104    1127.186\n#> school.med                                90.359   223.340      76.260\n#> school.high                               25.728    63.437      24.235\n#> school.post.high                          13.480    36.153      13.478\n#> invest                                    24.647    21.583      21.424\n#> special.gdpcap.1960.1969                   5.285     5.271       3.581\n#> special.sec.agriculture.1961.1969          6.844     6.179      21.353\n#> special.sec.energy.1961.1969               4.106     2.760       5.310\n#> special.sec.industry.1961.1969            45.082    37.636      22.425\n#> special.sec.construction.1961.1969         6.150     6.952       7.276\n#> special.sec.services.venta.1961.1969      33.754    41.104      36.528\n#> special.sec.services.nonventa.1961.1969    4.072     5.371       7.111\nsynth.tables$tab.w[8:14, ]\n#>    w.weights            unit.names unit.numbers\n#> 9      0.000    Castilla-La Mancha            9\n#> 10     0.851              Cataluna           10\n#> 11     0.000  Comunidad Valenciana           11\n#> 12     0.000           Extremadura           12\n#> 13     0.000               Galicia           13\n#> 14     0.149 Madrid (Comunidad De)           14\n#> 15     0.000    Murcia (Region de)           15\npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"Real Per-Capita GDP (1986 USD, thousand)\",\n    Xlab = \"Year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque Country\", \"Synthetic Basque Country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"Gap in Real Per-Capita GDP (1986 USD, thousand)\",\n    Xlab = \"Year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)"},{"path":"sec-synthetic-control.html","id":"micro-synthetic-control-with-microsynth","chapter":"32 Synthetic Control","heading":"32.14.3 Micro-Synthetic Control with microsynth","text":"microsynth package extends standard SCM allowing: 1Matching predictors time-variant outcomes. 2Permutation tests jackknife resampling placebo tests.Multiple follow-periods track long-term treatment effects.Omnibus test statistics multiple outcome variables.example evaluates Seattle Drug Market Initiative, policing intervention aimed reducing drug-related crime. dataset seattledmi contains crime statistics across different city zones.Step 1: Load Required Package DataStep 2: Define Predictors Outcome VariablesWe specify:Predictors (time-invariant): Population, race distribution, household data.Predictors (time-invariant): Population, race distribution, household data.Outcomes (time-variant): Crime rates across different offense categories.Outcomes (time-variant): Crime rates across different offense categories.Step 3: Fit Micro-Synthetic Control ModelWe first estimate treatment effects using single follow-period.Step 4: Summarize ResultsThis output provides:Weighted unit contributions synthetic control.Weighted unit contributions synthetic control.Treatment effect estimates confidence intervals.Treatment effect estimates confidence intervals.p-values permutation tests.p-values permutation tests.Step 5: Visualize Treatment EffectWe generate treatment effect plot comparing actual vs. synthetic trends.Step 6: Incorporating Multiple Follow-PeriodsWe extend analysis adding multiple post-treatment periods (end.post = c(14, 16)) permutation-based placebo tests (perm = 250, jack = TRUE).Step 7: Placebo Testing Robustness ChecksPermutation jackknife tests assess whether observed treatment effects statistically significant due random fluctuations.Key robustness checks:Permutation Tests (perm = 250)Jackknife Resampling (jack = TRUE)","code":"\nlibrary(microsynth)\ndata(\"seattledmi\")\ncov.var <- c(\n    \"TotalPop\",\n    \"BLACK\",\n    \"HISPANIC\",\n    \"Males_1521\",\n    \"HOUSEHOLDS\",\n    \"FAMILYHOUS\",\n    \"FEMALE_HOU\",\n    \"RENTER_HOU\",\n    \"VACANT_HOU\"\n)\nmatch.out <- c(\"i_felony\", \"i_misdemea\", \"i_drugs\", \"any_crime\")\nsea1 <- microsynth(\n    seattledmi,\n    idvar       = \"ID\",          # Unique unit identifier\n    timevar     = \"time\",        # Time variable\n    intvar      = \"Intervention\",# Treatment assignment\n    start.pre   = 1,             # Start of pre-treatment period\n    end.pre     = 12,            # End of pre-treatment period\n    end.post    = 16,            # End of post-treatment period\n    match.out   = match.out,     # Outcomes to match on\n    match.covar = cov.var,       # Predictors to match on\n    result.var  = match.out,     # Variables for treatment effect estimates\n    omnibus.var = match.out,     # Variables for omnibus p-values\n    test        = \"lower\",       # One-sided test (decrease in crime)\n    n.cores     = min(parallel::detectCores() - 4, 2) # Parallel processing\n)\nsummary(sea1)\nplot_microsynth(sea1)\nsea2 <- microsynth(\n    seattledmi,\n    idvar       = \"ID\",\n    timevar     = \"time\",\n    intvar      = \"Intervention\",\n    start.pre   = 1,\n    end.pre     = 12,\n    end.post    = c(14, 16), # Two follow-up periods\n    match.out   = match.out,\n    match.covar = cov.var,\n    result.var  = match.out,\n    omnibus.var = match.out,\n    test        = \"lower\",\n    perm        = 250, # Permutation placebo tests\n    jack        = TRUE, # Jackknife resampling\n    n.cores     = min(parallel::detectCores() - 4, 2)\n)1.  Reassigns treatment randomly and estimates placebo effects\n\n2.  If true effects exceed placebo effects, they are likely causal.1.  Removes one control unit at a time and re-estimates effects.\n\n2.  Ensures no single control dominates the synthetic match.\nsummary(sea2)  # Compare estimates with multiple follow-up periods\nplot_microsynth(sea2)  # Visualize placebo-adjusted treatment effects"},{"path":"sec-event-studies.html","id":"sec-event-studies","chapter":"33 Event Studies","heading":"33 Event Studies","text":"event study methodology widely used finance, marketing, management measure impact specific events stock prices. foundation methodology Efficient Markets Hypothesis proposed Fama (1970), asserts asset prices reflect available information. assumption, stock prices immediately react new, unexpected information, making event studies useful tool assessing economic impact firm- non-firm-initiated activities.first event study conducted Dolley (1933), Campbell et al. (1998) formalized methodology modern applications. Later, Dubow Monteiro (2006) developed metric assess market transparency (.e., way gauge “clean” market ) tracking unusual stock price movements major regulatory announcements. study found abnormal price shifts announcements indicate insider trading, prices reacted leaked information official disclosures.Advantages Event StudiesMore Reliable Accounting-Based Measures: Unlike financial metrics (e.g., profits), managers can manipulate, stock prices harder alter reflect real-time investor sentiment (Benston 1985).Reliable Accounting-Based Measures: Unlike financial metrics (e.g., profits), managers can manipulate, stock prices harder alter reflect real-time investor sentiment (Benston 1985).Easy Conduct: Event studies require stock price data simple econometric models, making widely accessible researchers.Easy Conduct: Event studies require stock price data simple econometric models, making widely accessible researchers.Types Events Event Studies","code":""},{"path":"sec-event-studies.html","id":"review-of-event-studies-across-disciplines","chapter":"33 Event Studies","heading":"33.1 Review of Event Studies Across Disciplines","text":"Event studies applied extensively management, marketing, finance assess different corporate external events influence shareholder value.","code":""},{"path":"sec-event-studies.html","id":"finance-applications","chapter":"33 Event Studies","heading":"33.1.1 Finance Applications","text":"(Fama et al. 1969): Stock split.","code":""},{"path":"sec-event-studies.html","id":"management-applications","chapter":"33 Event Studies","heading":"33.1.2 Management Applications","text":"(McWilliams Siegel 1997): Comprehensive review event studies management research.","code":""},{"path":"sec-event-studies.html","id":"marketing-applications-1","chapter":"33 Event Studies","heading":"33.1.3 Marketing Applications","text":"Event studies useful marketing evaluating effects firm decisions (e.g., advertising campaigns, brand changes) non-firm-initiated activities (e.g., regulatory decisions, third-party reviews).","code":""},{"path":"sec-event-studies.html","id":"firm-initiated-activities","chapter":"33 Event Studies","heading":"33.1.3.1 Firm-Initiated Activities","text":"Geyskens, Gielens, Dekimpe (2002): Internet channel (newspapers)Geyskens, Gielens, Dekimpe (2002): Internet channel (newspapers)Boyd, Chandy, Cunha Jr (2010): new CMO appointmentsBoyd, Chandy, Cunha Jr (2010): new CMO appointments","code":""},{"path":"sec-event-studies.html","id":"non-firm-initiated-activities","chapter":"33 Event Studies","heading":"33.1.3.2 Non-Firm-Initiated Activities","text":"Event studies also used examine impact external events firm value, including regulatory decisions, media coverage, economic shocks, unexpected crises.Event studies can extended new types events, :Advertising Campaigns – major ad campaigns affect stock prices?Advertising Campaigns – major ad campaigns affect stock prices?Market Entry – entering new market increase firm value?Market Entry – entering new market increase firm value?Product Failures & Recalls – recalls affect brand equity?Product Failures & Recalls – recalls affect brand equity?Patent Announcements – new patents generate abnormal returns?Patent Announcements – new patents generate abnormal returns?","code":""},{"path":"sec-event-studies.html","id":"key-assumptions-3","chapter":"33 Event Studies","heading":"33.2 Key Assumptions","text":"Efficient Market Hypothesis: Stock prices fully reflect available information (Fama 1970).Efficient Market Hypothesis: Stock prices fully reflect available information (Fama 1970).Stock Market Proxy Firm Value: Shareholders primary stakeholders.Stock Market Proxy Firm Value: Shareholders primary stakeholders.Sharp Event Effect: event must cause immediate stock price reaction.Sharp Event Effect: event must cause immediate stock price reaction.Proper Calculation Expected Returns: Requires appropriate benchmark model.Proper Calculation Expected Returns: Requires appropriate benchmark model.","code":""},{"path":"sec-event-studies.html","id":"steps-for-conducting-an-event-study","chapter":"33 Event Studies","heading":"33.3 Steps for Conducting an Event Study","text":"","code":""},{"path":"sec-event-studies.html","id":"step-1-event-identification","chapter":"33 Event Studies","heading":"33.3.1 Step 1: Event Identification","text":"event study examines particular event affects firm’s stock price, assuming stock markets incorporate new information efficiently. event must influence either firm’s expected cash flows discount rate (. Sorescu, Warren, Ertekin 2017, 191).Common Types Events AnalyzedTo systematically identify events, researchers use WRDS S&P Capital IQ Key Developments, tracks U.S. international corporate events.","code":""},{"path":"sec-event-studies.html","id":"step-2-define-the-event-and-estimation-windows","chapter":"33 Event Studies","heading":"33.3.2 Step 2: Define the Event and Estimation Windows","text":"","code":""},{"path":"sec-event-studies.html","id":"a-estimation-window-t_0-to-t_1","chapter":"33 Event Studies","heading":"33.3.2.1 (A) Estimation Window (\\(T_0 \\to T_1\\))","text":"estimation window used compute normal (expected) returns event.Leakage Concern: avoid biases information leaking event, researchers check broad news sources (e.g., LexisNexis, Factiva, RavenPack) pre-event rumors.","code":""},{"path":"sec-event-studies.html","id":"b-event-window-t_1-to-t_2","chapter":"33 Event Studies","heading":"33.3.2.2 (B) Event Window (\\(T_1 \\to T_2\\))","text":"event window captures market’s reaction event. selection appropriate window length depends event type information speed.","code":""},{"path":"sec-event-studies.html","id":"c-post-event-window-t_2-to-t_3","chapter":"33 Event Studies","heading":"33.3.2.3 (C) Post-Event Window (\\(T_2 \\to T_3\\))","text":"Used assess long-term effects stock prices.","code":""},{"path":"sec-event-studies.html","id":"step-3-compute-normal-vs.-abnormal-returns","chapter":"33 Event Studies","heading":"33.3.3 Step 3: Compute Normal vs. Abnormal Returns","text":"abnormal return measures much stock price deviates expected return:\\[\n\\epsilon_{}^* = \\frac{P_{} - E(P_{})}{P_{-1}} = R_{} - E(R_{} | X_t)\n\\]:\\(\\epsilon_{}^*\\) = abnormal return\\(\\epsilon_{}^*\\) = abnormal return\\(R_{}\\) = realized return\\(R_{}\\) = realized return\\(P_{}\\) = dividend-adjusted stock price\\(P_{}\\) = dividend-adjusted stock price\\(E(R_{} | X_t)\\) = expected return\\(E(R_{} | X_t)\\) = expected return","code":""},{"path":"sec-event-studies.html","id":"a-statistical-models-for-expected-returns","chapter":"33 Event Studies","heading":"33.3.3.1 (A) Statistical Models for Expected Returns","text":"models assume jointly normal independently distributed returns.Constant Mean Return Model\\[ E(R_{}) = \\frac{1}{T} \\sum_{t=T_0}^{T_1} R_{} \\]Market Model\\[ R_{} = \\alpha_i + \\beta_i R_{mt} + \\epsilon_{} \\]Adjusted Market Return Model\\[ E(R_{}) = R_{mt} \\]","code":""},{"path":"sec-event-studies.html","id":"b-economic-models-for-expected-returns","chapter":"33 Event Studies","heading":"33.3.3.2 (B) Economic Models for Expected Returns","text":"Capital Asset Pricing Model (CAPM)\\[ E(R_{}) = R_f + \\beta (R_m - R_f) \\]Arbitrage Pricing Theory (APT)\\[ R_{} = \\lambda_0 + \\lambda_1 F_1 + \\lambda_2 F_2 + ... + \\lambda_n F_n + \\epsilon_{} \\]","code":""},{"path":"sec-event-studies.html","id":"step-4-compute-cumulative-abnormal-returns","chapter":"33 Event Studies","heading":"33.3.4 Step 4: Compute Cumulative Abnormal Returns","text":"abnormal returns computed, aggregate event window:\\[\nCAR_{} = \\sum_{t=T_{\\text{event, start}}}^{T_{\\text{event, end}}} AR_{}\n\\]multiple firms, compute Average Cumulative Abnormal Return (ACAR):\\[\nACAR = \\frac{1}{N} \\sum_{=1}^{N} CAR_{}\n\\]","code":""},{"path":"sec-event-studies.html","id":"step-5-statistical-tests-for-significance","chapter":"33 Event Studies","heading":"33.3.5 Step 5: Statistical Tests for Significance","text":"determine abnormal returns statistically significant, use:T-Test Abnormal Returns \\[ t = \\frac{\\bar{CAR}}{\\sigma(CAR)} \\]Bootstrap & Monte Carlo Simulations\nUsed returns non-normally distributed.\nUsed returns non-normally distributed.","code":""},{"path":"sec-event-studies.html","id":"event-studies-in-marketing","chapter":"33 Event Studies","heading":"33.4 Event Studies in Marketing","text":"key challenge marketing-related event studies determining appropriate dependent variable (Skiera, Bayer, Schöler 2017). Traditional event studies finance use cumulative abnormal returns (CAR) shareholder value (\\(CAR^{SHV}\\)). However, marketing events primarily affect firm’s operating business, rather total shareholder value, leading potential distortions financial leverage ignored.According valuation theory, firm’s shareholder value (\\(SHV\\)) consists three components (Schulze, Skiera, Wiesel 2012):\\[\nSHV = \\text{Operating Business Value} + \\text{Non-Operating Assets} - \\text{Debt}\n\\]Many marketing-related events primarily impact operating business value (e.g., brand perception, customer satisfaction, advertising efficiency), non-operating assets debt remain largely unaffected.Ignoring firm-specific leverage effects event studies can cause:Inflated impact firms high debt.Deflated impact firms large non-operating assets.Thus, recommended \\(CAR^{OB}\\) \\(CAR^{SHV}\\) reported, justification appropriate.event studies explicitly controlled financial structure. Exceptions include:(Gielens et al. 2008): Studied marketing spending shocks accounting leverage.(Chaney, Devinney, Winer 1991): Examined advertising expenses firm value, controlling financial structure.","code":""},{"path":"sec-event-studies.html","id":"definition","chapter":"33 Event Studies","heading":"33.4.1 Definition","text":"Cumulative Abnormal Return Shareholder Value (\\(CAR^{SHV}\\))\\[\nCAR^{SHV} = \\frac{\\sum \\text{Abnormal Returns}}{SHV}\n\\]Shareholder Value (\\(SHV\\)): Market capitalization, defined :\n\\[\nSHV = \\text{Share Price} \\times \\text{Shares Outstanding}\n\\]Shareholder Value (\\(SHV\\)): Market capitalization, defined :\\[\nSHV = \\text{Share Price} \\times \\text{Shares Outstanding}\n\\]Cumulative Abnormal Return Operating Business (\\(CAR^{OB}\\))correct leverage effects, \\(CAR^{OB}\\) calculated :\\[\nCAR^{OB} = \\frac{CAR^{SHV}}{\\text{Leverage Effect}}\n\\]:\\[\n\\text{Leverage Effect} = \\frac{\\text{Operating Business Value}}{\\text{Shareholder Value}}\n\\]Key Relationships:Operating Business Value = \\(SHV -\\) Non-Operating Assets \\(+\\) Debt.Leverage Effect (\\(LE\\)) measures 1% change operating business value translates shareholder value movement.Leverage Effect vs. Leverage RatioLeverage Effect (\\(LE\\)) leverage ratio, typically:\\[\n\\text{Leverage Ratio} = \\frac{\\text{Debt}}{\\text{Firm Size}}\n\\]firm size can :Book value equityBook value equityMarket capitalizationMarket capitalizationTotal assetsTotal assetsDebt + EquityDebt + Equity","code":""},{"path":"sec-event-studies.html","id":"when-can-marketing-events-affect-non-operating-assets-or-debt","chapter":"33 Event Studies","heading":"33.4.2 When Can Marketing Events Affect Non-Operating Assets or Debt?","text":"marketing events impact operating business value, rare cases also influence non-operating assets debt:exceptions highlight controlling financial structure crucial event studies.","code":""},{"path":"sec-event-studies.html","id":"calculating-the-leverage-effect","chapter":"33 Event Studies","heading":"33.4.3 Calculating the Leverage Effect","text":"can express leverage effect (\\(LE\\)) :\\[\n\\begin{aligned}\nLE &= \\frac{\\text{Operating Business Value}}{\\text{Shareholder Value}} \\\\\n&= \\frac{(\\text{SHV} - \\text{Non-Operating Assets} + \\text{Debt})}{\\text{SHV}} \\\\\n&= \\frac{prcc_f \\times csho - ivst + dd1 + dltt + pstk}{prcc_f \\times csho}\n\\end{aligned}\n\\]:\\(prcc_f\\) = Share price\\(prcc_f\\) = Share price\\(csho\\) = Common shares outstanding\\(csho\\) = Common shares outstanding\\(ivst\\) = Short-term investments (Non-Operating Assets)\\(ivst\\) = Short-term investments (Non-Operating Assets)\\(dd1\\) = Long-term debt due one year\\(dd1\\) = Long-term debt due one year\\(dltt\\) = Long-term debt\\(dltt\\) = Long-term debt\\(pstk\\) = Preferred stock\\(pstk\\) = Preferred stock","code":""},{"path":"sec-event-studies.html","id":"computing-leverage-effect-from-compustat-data","chapter":"33 Event Studies","heading":"33.4.4 Computing Leverage Effect from Compustat Data","text":"","code":"\n# Load required libraries\nlibrary(tidyverse)\n\n\n# Load dataset\ndf_leverage_effect <- read.csv(\"data/leverage_effect.csv.gz\") %>%\n    \n    # Filter active firms\n    filter(costat == \"A\") %>%\n    \n    # Drop missing values\n    drop_na() %>%\n    \n    # Compute Shareholder Value (SHV)\n    mutate(shv = prcc_f * csho) %>%\n    \n    # Compute Operating Business Value (OBV)\n    mutate(obv = shv - ivst + dd1 + dltt + pstk) %>%\n    \n    # Compute Leverage Effect\n    mutate(leverage_effect = obv / shv) %>%\n    \n    # Remove infinite values and non-positive leverage effects\n    filter(is.finite(leverage_effect), leverage_effect > 0) %>%\n    \n    # Compute within-firm statistics\n    group_by(gvkey) %>%\n    mutate(\n        within_mean_le = mean(leverage_effect, na.rm = TRUE),\n        within_sd_le = sd(leverage_effect, na.rm = TRUE)\n    ) %>%\n    ungroup()\n\n# Summary statistics\nmean_le <- mean(df_leverage_effect$leverage_effect, na.rm = TRUE)\nmax_le <- max(df_leverage_effect$leverage_effect, na.rm = TRUE)\n\n# Plot histogram of leverage effect\nhist(\n    df_leverage_effect$leverage_effect,\n    main = \"Distribution of Leverage Effect\",\n    xlab = \"Leverage Effect\",\n    col = \"blue\",\n    breaks = 30\n)\n\n# Compute coefficient of variation (CV)\ncv_le <-\n    sd(df_leverage_effect$leverage_effect, na.rm = TRUE) / mean_le * 100\n\n# Plot within-firm coefficient of variation histogram\ndf_leverage_effect %>%\n    group_by(gvkey) %>%\n    slice(1) %>%\n    ungroup() %>%\n    mutate(cv = within_sd_le / within_mean_le) %>%\n    pull(cv) %>%\n    hist(\n        main = \"Within-Firm Coefficient of Variation\",\n        xlab = \"CV\",\n        col = \"red\",\n        breaks = 30\n    )"},{"path":"sec-event-studies.html","id":"economic-significance","chapter":"33 Event Studies","heading":"33.5 Economic Significance","text":"total wealth gain (loss) resulting marketing event given :\\[\n\\Delta W_t = CAR_t \\times MKTVAL_0\n\\]:\\(\\Delta W_t\\) = Change firm value (gain loss).\\(CAR_t\\) = Cumulative abnormal return date \\(t\\).\\(MKTVAL_0\\) = Market value firm event window.Interpretation:\\(\\Delta W_t > 0\\): event increased firm value.\\(\\Delta W_t < 0\\): event decreased firm value.magnitude \\(\\Delta W_t\\) reflects economic impact marketing event dollar terms.computing \\(\\Delta W_t\\), researchers can translate stock market reactions tangible financial implications, helping assess real-world significance marketing decisions.","code":"\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Simulated dataset of event study results\ndf_event_study <- tibble(\n    firm_id = 1:100,\n    # 100 firms\n    CAR_t = rnorm(100, mean = 0.02, sd = 0.05),\n    # Simulated CAR values\n    MKTVAL_0 = runif(100, min = 1e8, max = 5e9)  # Market value in dollars\n)\n\n# Compute total wealth gain/loss\ndf_event_study <- df_event_study %>%\n    mutate(wealth_change = CAR_t * MKTVAL_0)\n\n# Summary statistics of economic impact\nsummary(df_event_study$wealth_change)\n#>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n#> -452754614  -17135099   32402378   47805094   97481255  518034141\n\n# Histogram of total wealth gain/loss\nhist(\n    df_event_study$wealth_change,\n    main = \"Distribution of Wealth Change from Event\",\n    xlab = \"Wealth Change ($)\",\n    col = \"blue\",\n    breaks = 30\n)"},{"path":"sec-event-studies.html","id":"testing-in-event-studies","chapter":"33 Event Studies","heading":"33.6 Testing in Event Studies","text":"","code":""},{"path":"sec-event-studies.html","id":"statistical-power-in-event-studies","chapter":"33 Event Studies","heading":"33.6.1 Statistical Power in Event Studies","text":"Statistical power refers ability detect true effect (.e., identify significant abnormal returns) one exists.Power increases :firms sample → reduces variance increases reliability.Fewer days event window → avoids contamination confounding factors.Trade-:longer event window captures delayed market reactions risks contamination unrelated events.shorter event window reduces noise may miss slow adjustments stock prices.Thus, optimal event window balances precision (avoiding confounds) completeness (capturing true market reaction).","code":""},{"path":"sec-event-studies.html","id":"parametric-tests","chapter":"33 Event Studies","heading":"33.6.2 Parametric Tests","text":"S. J. Brown Warner (1985) provide evidence parametric tests perform well even non-normality, long sample includes least five securities. distribution abnormal returns converges normality sample size increases.","code":""},{"path":"sec-event-studies.html","id":"power-of-parametric-tests","chapter":"33 Event Studies","heading":"33.6.2.1 Power of Parametric Tests","text":"Kothari Warner (1997) highlights power detect significant abnormal returns depends :Sample size: firms improve statistical power.Magnitude abnormal returns: Larger effects easier detect.Variance abnormal returns across firms: Lower variance increases power.","code":""},{"path":"sec-event-studies.html","id":"t-test-for-abnormal-returns","chapter":"33 Event Studies","heading":"33.6.2.2 T-Test for Abnormal Returns","text":"applying Central Limit Theorem, can use t-test abnormal returns:\\[\n\\begin{aligned}\nt_{CAR} &= \\frac{\\bar{CAR_{}}}{\\sigma (CAR_{})/\\sqrt{n}} \\\\\nt_{BHAR} &= \\frac{\\bar{BHAR_{}}}{\\sigma (BHAR_{})/\\sqrt{n}}\n\\end{aligned}\n\\]Assumptions:Abnormal returns follow normal distribution.Abnormal returns follow normal distribution.Variance equal across firms.Variance equal across firms.cross-sectional correlation abnormal returns.cross-sectional correlation abnormal returns.assumptions hold, t-test misspecified, leading unreliable inference.Misspecification may occur due :Heteroskedasticity (unequal variance across firms).Heteroskedasticity (unequal variance across firms).Cross-sectional dependence (correlation abnormal returns across firms).Cross-sectional dependence (correlation abnormal returns across firms).Non-normality abnormal returns (though event study design often forces normality).Non-normality abnormal returns (though event study design often forces normality).address concerns, Patell Standardized Residuals provide robust alternative.","code":""},{"path":"sec-event-studies.html","id":"patell-standardized-residual-psr","chapter":"33 Event Studies","heading":"33.6.2.3 Patell Standardized Residual","text":"Patell (1976) developed Patell Standardized Residuals (PSR), standardizes abnormal returns correct estimation errors.Since market model relies observations outside event window, introduces prediction errors beyond true residuals. PSR corrects :\\[\nAR_{} = \\frac{\\hat{u}_{}}{s_i \\sqrt{C_{}}}\n\\]:\\(\\hat{u}_{}\\) = estimated residual market model.\\(s_i\\) = standard deviation residuals estimation period.\\(C_{}\\) = correction factor accounting estimation period variation.correction factor (\\(C_{}\\)) :\\[\nC_{} = 1 + \\frac{1}{T} + \\frac{(R_{mt} - \\bar{R}_m)^2}{\\sum_t (R_{mt} - \\bar{R}_m)^2}\n\\]:\\(T\\) = number observations estimation period.\\(R_{mt}\\) = market return time \\(t\\).\\(\\bar{R}_m\\) = mean market return.correction ensures abnormal returns properly scaled, reducing bias estimation errors.","code":""},{"path":"sec-event-studies.html","id":"non-parametric-tests-1","chapter":"33 Event Studies","heading":"33.6.3 Non-Parametric Tests","text":"Non-parametric tests assume specific return distribution, making robust non-normality heteroskedasticity.","code":""},{"path":"sec-event-studies.html","id":"sign-test-1","chapter":"33 Event Studies","heading":"33.6.3.1 Sign Test","text":"Sign Test assumes symmetric abnormal returns around zero.Null hypothesis (\\(H_0\\)): Equal probability positive negative abnormal returns.Alternative hypothesis (\\(H_A\\)): positive (negative) abnormal returns expected.","code":"\n# Perform a sign test using binomial test\nbinom.test(x = sum(CAR > 0), n = length(CAR), p = 0.5)"},{"path":"sec-event-studies.html","id":"wilcoxon-signed-rank-test-1","chapter":"33 Event Studies","heading":"33.6.3.2 Wilcoxon Signed-Rank Test","text":"Wilcoxon Signed-Rank Test allows non-symmetry returns.Use case: Detects shifts distribution abnormal returns.Use case: Detects shifts distribution abnormal returns.powerful sign test return magnitudes matter.powerful sign test return magnitudes matter.","code":"\n# Perform Wilcoxon Signed-Rank Test\nwilcox.test(CAR, mu = 0)"},{"path":"sec-event-studies.html","id":"generalized-sign-test","chapter":"33 Event Studies","heading":"33.6.3.3 Generalized Sign Test","text":"advanced sign test, comparing proportion positive abnormal returns historical norms.","code":""},{"path":"sec-event-studies.html","id":"corrado-rank-test","chapter":"33 Event Studies","heading":"33.6.3.4 Corrado Rank Test","text":"Corrado Rank Test rank-based test abnormal returns.Advantage: Accounts cross-sectional dependence.Advantage: Accounts cross-sectional dependence.robust t-test non-normality.robust t-test non-normality.","code":"\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Simulate abnormal returns (CAR)\nset.seed(123)\ndf_returns <- tibble(\n    firm_id = 1:100,  # 100 firms\n    CAR = rnorm(100, mean = 0.02, sd = 0.05)  # Simulated CAR values\n)\n\n# Parametric T-Test for CAR\nt_test_result <- t.test(df_returns$CAR, mu = 0)\n\n# Non-parametric tests\nsign_test_result <- binom.test(sum(df_returns$CAR > 0), n = nrow(df_returns), p = 0.5)\nwilcox_test_result <- wilcox.test(df_returns$CAR, mu = 0)\n\n# Print results\nlist(\n    T_Test = t_test_result,\n    Sign_Test = sign_test_result,\n    Wilcoxon_Test = wilcox_test_result\n)\n#> $T_Test\n#> \n#>  One Sample t-test\n#> \n#> data:  df_returns$CAR\n#> t = 5.3725, df = 99, p-value = 5.159e-07\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  0.01546417 0.03357642\n#> sample estimates:\n#> mean of x \n#> 0.0245203 \n#> \n#> \n#> $Sign_Test\n#> \n#>  Exact binomial test\n#> \n#> data:  sum(df_returns$CAR > 0) and nrow(df_returns)\n#> number of successes = 70, number of trials = 100, p-value = 7.85e-05\n#> alternative hypothesis: true probability of success is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.6001853 0.7875936\n#> sample estimates:\n#> probability of success \n#>                    0.7 \n#> \n#> \n#> $Wilcoxon_Test\n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  df_returns$CAR\n#> V = 3917, p-value = 1.715e-06\n#> alternative hypothesis: true location is not equal to 0"},{"path":"sec-event-studies.html","id":"sample-in-event-studies","chapter":"33 Event Studies","heading":"33.7 Sample in Event Studies","text":"Event studies marketing finance often use relatively small samples, can still yield meaningful results.Examples sample sizes prior studies:(Wiles, Morgan, Rego 2012): 572 acquisition announcements, 308 disposal announcements.(Wiles, Morgan, Rego 2012): 572 acquisition announcements, 308 disposal announcements.(Markovitch Golder 2008): Smallest sample 71 events.(Markovitch Golder 2008): Smallest sample 71 events.(Borah Tellis 2014): Largest sample 3,552 events.(Borah Tellis 2014): Largest sample 3,552 events.Thus, larger samples improve power, meaningful results can still obtained smaller datasets.","code":""},{"path":"sec-event-studies.html","id":"confounders-in-event-studies","chapter":"33 Event Studies","heading":"33.8 Confounders in Event Studies","text":"major challenge event studies controlling confounding events, bias estimation abnormal returns.","code":""},{"path":"sec-event-studies.html","id":"types-of-confounding-events","chapter":"33 Event Studies","heading":"33.8.1 Types of Confounding Events","text":"(McWilliams Siegel 1997) suggest excluding firms experience major events within two-day window around focal event. include:Financial announcements: Earnings reports, stock buybacks, dividend changes, IPOs.Corporate actions: Mergers, acquisitions, spin-offs, stock splits, debt defaults.Executive changes: CEO/CFO resignations appointments.Operational changes: Layoffs, restructurings, lawsuits, joint ventures.Fornell et al. (2006) recommend:One-day event period: date Wall Street Journal publishes ACSI announcement.Five-day window (event) rule news (PR Newswires, Dow Jones, Business Wires).Events controlled include:M&, spin-offs, stock splits.M&, spin-offs, stock splits.CEO CFO changes.CEO CFO changes.Layoffs, restructurings, lawsuits.Layoffs, restructurings, lawsuits.useful data source identifying confounding events Capital IQ’s Key Developments, captures almost important corporate events.","code":""},{"path":"sec-event-studies.html","id":"should-we-exclude-confounded-observations","chapter":"33 Event Studies","heading":"33.8.2 Should We Exclude Confounded Observations?","text":". Sorescu, Warren, Ertekin (2017) investigated confounding events short-term event windows using:RavenPack dataset (2000-2013).RavenPack dataset (2000-2013).3-day event windows 3,982 US publicly traded firms.3-day event windows 3,982 US publicly traded firms.Key Findings:difference full sample sample without confounded events statistically insignificant.Conclusion: Excluding confounded observations may necessary short-term event studies.?Selection bias risk: Researchers may selectively exclude events, introducing bias.Increasing exclusions time: time progresses, events need excluded, reducing statistical power.Short-term windows minimize confounder effects.","code":""},{"path":"sec-event-studies.html","id":"simulation-study-should-we-exclude-correlated-and-uncorrelated-events","chapter":"33 Event Studies","heading":"33.8.3 Simulation Study: Should We Exclude Correlated and Uncorrelated Events?","text":"illustrate impact correlated uncorrelated events, let’s conduct simulation study.consider three event types:Focal events (events interest).Correlated events (events often co-occur focal events).Uncorrelated events (random events might coincide focal events).analyze impact including vs. excluding correlated uncorrelated events.depicted plot, inclusion correlated events demonstrates minimal impact estimation focal events. Conversely, excluding correlated events can diminish statistical power. true cases pronounced correlation.However, consequences excluding unrelated events notably significant. becomes evident omitting around 40 unrelated events study, lose ability accurately identify true effects focal events. reality within research, often rely Key Developments database, excluding 150 events, practice can substantially impair capacity ascertain authentic impact focal events.little experiment really drives home point – better darn good reason exclude event study!","code":"\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tidyverse)\n\n# Parameters\nn                  <- 100000         # Number of observations\nn_focal            <- round(n * 0.2) # Number of focal events\noverlap_correlated <- 0.5            # Overlapping percentage between focal and correlated events\n\n# Function to compute mean and confidence interval\nmean_ci <- function(x) {\n    m <- mean(x)\n    ci <- qt(0.975, length(x)-1) * sd(x) / sqrt(length(x)) # 95% confidence interval\n    list(mean = m, lower = m - ci, upper = m + ci)\n}\n\n\n# Simulate data\nset.seed(42)\ndata <- tibble(\n    date       = seq.Date(\n        from = as.Date(\"2010-01-01\"),\n        by = \"day\",\n        length.out = n\n    ),\n    # Date sequence\n    focal      = rep(0, n),\n    correlated = rep(0, n),\n    ab_ret     = rnorm(n)\n)\n\n\n# Define focal events\nfocal_idx <- sample(1:n, n_focal)\ndata$focal[focal_idx] <- 1\n\ntrue_effect <- 0.25\n\n# Adjust the ab_ret for the focal events to have a mean of true_effect\ndata$ab_ret[focal_idx] <-\n    data$ab_ret[focal_idx] - mean(data$ab_ret[focal_idx]) + true_effect\n\n\n\n# Determine the number of correlated events that overlap with focal and those that don't\nn_correlated_overlap <-\n    round(length(focal_idx) * overlap_correlated)\nn_correlated_non_overlap <- n_correlated_overlap\n\n# Sample the overlapping correlated events from the focal indices\ncorrelated_idx <- sample(focal_idx, size = n_correlated_overlap)\n\n# Get the remaining indices that are not part of focal\nremaining_idx <- setdiff(1:n, focal_idx)\n\n# Check to ensure that we're not attempting to sample more than the available remaining indices\nif (length(remaining_idx) < n_correlated_non_overlap) {\n    stop(\"Not enough remaining indices for non-overlapping correlated events\")\n}\n\n# Sample the non-overlapping correlated events from the remaining indices\ncorrelated_non_focal_idx <-\n    sample(remaining_idx, size = n_correlated_non_overlap)\n\n# Combine the two to get all correlated indices\nall_correlated_idx <- c(correlated_idx, correlated_non_focal_idx)\n\n# Set the correlated events in the data\ndata$correlated[all_correlated_idx] <- 1\n\n\n# Inflate the effect for correlated events to have a mean of\ncorrelated_non_focal_idx <-\n    setdiff(all_correlated_idx, focal_idx) # Fixing the selection of non-focal correlated events\ndata$ab_ret[correlated_non_focal_idx] <-\n    data$ab_ret[correlated_non_focal_idx] - mean(data$ab_ret[correlated_non_focal_idx]) + 1\n\n\n# Define the numbers of uncorrelated events for each scenario\nnum_uncorrelated <- c(5, 10, 20, 30, 40)\n\n# Define uncorrelated events\nfor (num in num_uncorrelated) {\n    for (i in 1:num) {\n        data[paste0(\"uncorrelated_\", i)] <- 0\n        uncorrelated_idx <- sample(1:n, round(n * 0.1))\n        data[uncorrelated_idx, paste0(\"uncorrelated_\", i)] <- 1\n    }\n}\n\n\n# Define uncorrelated columns and scenarios\nunc_cols <- paste0(\"uncorrelated_\", 1:num_uncorrelated)\nresults <- tibble(\n    Scenario = c(\n        \"Include Correlated\",\n        \"Correlated Effects\",\n        \"Exclude Correlated\",\n        \"Exclude Correlated and All Uncorrelated\"\n    ),\n    MeanEffect = c(\n        mean_ci(data$ab_ret[data$focal == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 0 |\n                                data$correlated == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0 &\n                                rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$mean\n    ),\n    LowerCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 0 |\n                                data$correlated == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0 &\n                                rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$lower\n    ),\n    UpperCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 0 |\n                                data$correlated == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 &\n                                data$correlated == 0 &\n                                rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$upper\n    )\n)\n\n# Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated\nfor (num in num_uncorrelated) {\n    unc_cols <- paste0(\"uncorrelated_\", 1:num)\n    results <- results %>%\n        add_row(\n            Scenario = paste(\"Exclude\", num, \"Uncorrelated\"),\n            MeanEffect = mean_ci(data$ab_ret[data$focal == 1 &\n                                                 data$correlated == 0 &\n                                                 rowSums(data[, unc_cols]) == 0])$mean,\n            LowerCI = mean_ci(data$ab_ret[data$focal == 1 &\n                                              data$correlated == 0 &\n                                              rowSums(data[, unc_cols]) == 0])$lower,\n            UpperCI = mean_ci(data$ab_ret[data$focal == 1 &\n                                              data$correlated == 0 &\n                                              rowSums(data[, unc_cols]) == 0])$upper\n        )\n}\n\n\nggplot(results,\n       aes(\n           x = factor(Scenario, levels = Scenario),\n           y = MeanEffect,\n           ymin = LowerCI,\n           ymax = UpperCI\n       )) +\n    geom_pointrange() +\n    coord_flip() +\n    ylab(\"Mean Effect\") +\n    xlab(\"Scenario\") +\n    ggtitle(\"Mean Effect of Focal Events under Different Scenarios\") +\n    geom_hline(yintercept = true_effect,\n               linetype = \"dashed\",\n               color = \"red\") "},{"path":"sec-event-studies.html","id":"biases-in-event-studies","chapter":"33 Event Studies","heading":"33.9 Biases in Event Studies","text":"Event studies subject several biases can affect estimation abnormal returns, validity test statistics, interpretation results. section discusses key biases recommended corrections.","code":""},{"path":"sec-event-studies.html","id":"timing-bias-different-market-closing-times","chapter":"33 Event Studies","heading":"33.9.1 Timing Bias: Different Market Closing Times","text":"Campbell et al. (1998) highlight differences market closing times across exchanges can obscure abnormal return calculations, especially firms traded multiple time zones.Solution:Use synchronized market closing prices possible.Adjust event windows based firm’s primary trading exchange.","code":""},{"path":"sec-event-studies.html","id":"upward-bias-in-cumulative-abnormal-returns","chapter":"33 Event Studies","heading":"33.9.2 Upward Bias in Cumulative Abnormal Returns","text":"aggregation CARs can introduce upward bias due use transaction prices (.e., bid ask prices).Issue: Prices can jump due liquidity constraints, leading artificially inflated CARs.Solution:Use volume-weighted average prices (VWAP) instead raw transaction prices.Apply robust standard errors mitigate bias.","code":""},{"path":"sec-event-studies.html","id":"cross-sectional-dependence-bias","chapter":"33 Event Studies","heading":"33.9.3 Cross-Sectional Dependence Bias","text":"Cross-sectional dependence returns biases standard deviation estimates downward, leading inflated test statistics multiple firms experience event date.(MacKinlay 1997): bias particularly problematic firms industry market share event dates.(Wiles, Morgan, Rego 2012): Events concentrated industries amplify cross-sectional dependence, inflating test statistics.Solution:Use Calendar-Time Portfolio Abnormal Returns (CTARs) (Jaffe 1974).Apply time-series standard deviation test statistic (S. J. Brown Warner 1980) correct standard errors.","code":"\n# Load required libraries\nlibrary(sandwich)  # For robust standard errors\nlibrary(lmtest)    # For hypothesis testing\n\n# Simulated dataset\nset.seed(123)\ndf_returns <- data.frame(\n    event_id = rep(1:100, each = 10),\n    firm_id = rep(1:10, times = 100),\n    abnormal_return = rnorm(1000, mean = 0.02, sd = 0.05)\n)\n\n# Cross-sectional dependence adjustment using clustered standard errors\nmodel <- lm(abnormal_return ~ 1, data = df_returns)\ncoeftest(model, vcov = vcovCL(model, cluster = ~event_id))\n#> \n#> t test of coefficients:\n#> \n#>              Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept) 0.0208064  0.0014914  13.951 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-event-studies.html","id":"sample-selection-bias","chapter":"33 Event Studies","heading":"33.9.4 Sample Selection Bias","text":"Event studies often suffer self-selection bias, firms self-select treatment (event) based private information. similar omitted variable bias, omitted variable private information led firm take action.","code":""},{"path":"sec-event-studies.html","id":"corrections-for-sample-selection-bias","chapter":"33 Event Studies","heading":"33.9.5 Corrections for Sample Selection Bias","text":"Heckman Two-Stage Model (Acharya 1993)\nProblem: Hard find strong instrument meets exclusion restriction.\nSolution: Estimate Mills ratio (\\(\\lambda\\)) account private information firm decisions.\nHeckman Two-Stage Model (Acharya 1993)Problem: Hard find strong instrument meets exclusion restriction.Problem: Hard find strong instrument meets exclusion restriction.Solution: Estimate Mills ratio (\\(\\lambda\\)) account private information firm decisions.Solution: Estimate Mills ratio (\\(\\lambda\\)) account private information firm decisions.Counterfactual ObservationsCounterfactual ObservationsHeckman Selection ModelA Heckman selection model can used private information influences event participation abnormal returns.Examples: Y. Chen, Ganesan, Liu (2009); Wiles, Morgan, Rego (2012); Fang, Lee, Yang (2015)Steps:First Stage (Selection Equation): Model firm’s probability experiencing event using Probit regression.First Stage (Selection Equation): Model firm’s probability experiencing event using Probit regression.Second Stage (Outcome Equation): Model abnormal returns, controlling estimated Mills ratio (\\(\\lambda\\)).Second Stage (Outcome Equation): Model abnormal returns, controlling estimated Mills ratio (\\(\\lambda\\)).InterpretationIf Mills ratio (\\(\\lambda\\)) significant, indicates private information affects CARs.Mills ratio (\\(\\lambda\\)) significant, indicates private information affects CARs.Weak instruments can lead multicollinearity, making second-stage estimates unreliable.Weak instruments can lead multicollinearity, making second-stage estimates unreliable.Propensity Score MatchingPSM matches event firms similar non-event firms, controlling selection bias.Examples PSM Finance Marketing:Finance: Masulis Nahata (2011).Finance: Masulis Nahata (2011).Marketing: Cao Sorescu (2013).Marketing: Cao Sorescu (2013).Advantages PSMControls observable differences event non-event firms.Controls observable differences event non-event firms.Reduces selection bias maintaining valid control group.Reduces selection bias maintaining valid control group.Switching RegressionA Switching Regression Model accounts selection unobservables using instrumental variables.Example: Cao Sorescu (2013) applied switching regression compare two outcomes correcting selection bias.","code":"-   **Propensity Score Matching**: Matches firms experiencing an event with similar firms that did not.\n\n-    **Switching Regression**: Compares outcomes across two groups while accounting for unobserved heterogeneity.\n# Load required libraries\nlibrary(sampleSelection)\n\n# Simulated dataset for Heckman model\nset.seed(123)\ndf_heckman <- data.frame(\n    firm_id = 1:500,\n    event = rbinom(500, 1, 0.3),  # Event occurrence (selection)\n    firm_size = runif(500, 1, 10), # Firm characteristic\n    abnormal_return = rnorm(500, mean = 0.02, sd = 0.05)\n)\n\n# Introduce selection bias by correlating firm_size with event occurrence\ndf_heckman$event[df_heckman$firm_size > 7] <- 1\n\n# Heckman Selection Model\nheckman_model <- selection(\n    selection = event ~ firm_size,  # Selection equation\n    outcome = abnormal_return ~ firm_size,  # Outcome equation\n    data = df_heckman\n)\n\n# Summary of Heckman model\nsummary(heckman_model)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 6 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: 165.4579 \n#> 500 observations (239 censored and 261 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1.75936    0.15793  -11.14   <2e-16 ***\n#> firm_size    0.33933    0.02776   12.22   <2e-16 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 0.006025   0.040359   0.149    0.881\n#> firm_size   0.001311   0.004205   0.312    0.755\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma 0.049048   0.002836  17.297   <2e-16 ***\n#> rho   0.188195   0.421944   0.446    0.656    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n# Load required libraries\nlibrary(MatchIt)\n\n# Simulated dataset\nset.seed(123)\ndf_psm <- data.frame(\n    firm_id = 1:1000,\n    event = rbinom(1000, 1, 0.5),  # 50% of firms experience an event\n    firm_size = runif(1000, 1, 10),\n    market_cap = runif(1000, 100, 10000)\n)\n\n# Propensity score matching (PSM)\nmatch_model <- matchit(event ~ firm_size + market_cap, data = df_psm, method = \"nearest\")\n\n# Summary of matched sample\nsummary(match_model)\n#> \n#> Call:\n#> matchit(formula = event ~ firm_size + market_cap, data = df_psm, \n#>     method = \"nearest\")\n#> \n#> Summary of Balance for All Data:\n#>            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance          0.4987        0.4875          0.2093     1.0656    0.0602\n#> firm_size         5.2627        5.6998         -0.1683     1.0530    0.0494\n#> market_cap     5208.5283     4868.5828          0.1163     1.0483    0.0359\n#>            eCDF Max\n#> distance     0.1152\n#> firm_size    0.0902\n#> market_cap   0.0713\n#> \n#> Summary of Balance for Matched Data:\n#>            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance          0.4987        0.4898          0.1668     1.1170    0.0489\n#> firm_size         5.2627        5.6182         -0.1369     1.0693    0.0404\n#> market_cap     5208.5283     4949.8521          0.0885     1.0594    0.0283\n#>            eCDF Max Std. Pair Dist.\n#> distance     0.1034          0.1673\n#> firm_size    0.0872          0.6549\n#> market_cap   0.0649          0.9168\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           507     493\n#> Matched       493     493\n#> Unmatched      14       0\n#> Discarded       0       0\n\n# Extract matched data\nmatched_data <- match.data(match_model)"},{"path":"sec-event-studies.html","id":"long-run-event-studies","chapter":"33 Event Studies","heading":"33.10 Long-run Event Studies","text":"Long-horizon event studies analyze long-term impact corporate events stock prices. studies commonly assume distribution abnormal returns mean zero (. Sorescu, Warren, Ertekin 2017, 192). Moreover, . Sorescu, Warren, Ertekin (2017) provide evidence samples without confounding events yield similar results.However, long-run event studies face several methodological challenges:Systematic biases time: Estimation errors can accumulate long periods.Sensitivity model specification: choice asset pricing models can influence results.Long-run event studies typically use event windows 12 60 months (Loughran Ritter 1995; Brav Gompers 1997).three primary methods measuring long-term abnormal stock returns:Buy--Hold Abnormal Returns (BHAR)Long-term Cumulative Abnormal Returns (LCARs)Calendar-time Portfolio Abnormal Returns (CTARs), also known Jensen’s Alpha, better handles cross-sectional dependence less sensitive asset pricing model misspecification.Types Events Analyzed Long-run StudiesUnexpected changes firm-specific variables\nevents typically announced, may immediately visible investors, impact firm value complex. Examples include:\neffect customer satisfaction scores firm value (Jacobson Mizik 2009).\nUnexpected changes marketing expenditures potential mispricing effects (M. Kim McAlister 2011).\neffect customer satisfaction scores firm value (Jacobson Mizik 2009).Unexpected changes marketing expenditures potential mispricing effects (M. Kim McAlister 2011).Events complex consequences\nInvestors may take time fully incorporate information stock prices. example:\nlong-term impact acquisitions depends post-merger integration (. B. Sorescu, Chandy, Prabhu 2007).\nlong-term impact acquisitions depends post-merger integration (. B. Sorescu, Chandy, Prabhu 2007).example using crseEventStudy package, calculates standardized abnormal returns:","code":"\nlibrary(crseEventStudy)\n\n# Example using demo data from the package\ndata(demo_returns)\n\nSAR <- sar(event = demo_returns$EON,\n           control = demo_returns$RWE,\n           logret = FALSE)\n\nmean(SAR)\n#> [1] 0.006870196"},{"path":"sec-event-studies.html","id":"sec-buy-and-hold-abnormal-returns-bhar","chapter":"33 Event Studies","heading":"33.10.1 Buy-and-Hold Abnormal Returns (BHAR)","text":"BHAR one widely used methods long-term event studies. involves constructing portfolio benchmark stocks closely match event firms period comparing returns.Key References(Loughran Ritter 1995)(Loughran Ritter 1995)(Barber Lyon 1997)(Barber Lyon 1997)(Lyon, Barber, Tsai 1999)(Lyon, Barber, Tsai 1999)BHAR measures returns :Buying stocks event firms.Buying stocks event firms.Shorting stocks similar non-event firms.Shorting stocks similar non-event firms.Since cross-sectional correlations can inflate t-statistics, BHAR’s rank order remains reliable even absolute significance levels affected (Markovitch Golder 2008; . B. Sorescu, Chandy, Prabhu 2007).construct benchmark portfolio, firms matched based :SizeSizeBook--market ratioBook--market ratioMomentumMomentumMatching strategies vary across studies. two common procedures:(Barber Lyon 1997) approachEach July, common stocks CRSP database classified ten deciles based market capitalization previous June.July, common stocks CRSP database classified ten deciles based market capitalization previous June.Within size decile, firms grouped five quintiles based book--market ratios prior December.Within size decile, firms grouped five quintiles based book--market ratios prior December.benchmark portfolio consists non-event firms fit criteria.benchmark portfolio consists non-event firms fit criteria.(Wiles et al. 2010) approachFirms two-digit SIC code market values 50% 150% focal firm selected.Firms two-digit SIC code market values 50% 150% focal firm selected.subset, 10 firms closest book--market ratios form benchmark portfolio.subset, 10 firms closest book--market ratios form benchmark portfolio.Abnormal return firm \\(\\) time \\(t\\):\\[\nAR_{} = R_{} - E(R_{}|X_t)\n\\]Cumulative Abnormal Return (CAR):\\[\nCAR_{} = \\sum_{t=1}^T (R_{} - E(R_{}))\n\\]Buy--Hold Abnormal Return (BHAR):\\[\nBHAR_{t=1}^{T} = \\prod_{t=1}^{T} (1 + R_{}) - \\prod_{t=1}^{T} (1 + E(R_{}))\n\\]Unlike CAR, arithmetic, BHAR geometric.short-term studies, differences CAR BHAR minimal.short-term studies, differences CAR BHAR minimal.long-term studies, discrepancy significant. instance, Barber Lyon (1997) show annual BHAR exceeds 28%, dramatically surpasses CAR.long-term studies, discrepancy significant. instance, Barber Lyon (1997) show annual BHAR exceeds 28%, dramatically surpasses CAR.avoid favoring recent events, researchers cross-sectional event studies typically treat events equally assessing impact stock market time. approach helps identifying abnormal changes stock prices, particularly analyzing series unplanned events.However, long-run event studies face several biases can distort abnormal return calculations:Construct Benchmark Portfolios Fixed ConstituentsOne recommended approach form benchmark portfolios change constituent firms time (Mitchell Stafford 2000). helps mitigate following biases:New Listing Bias\nNewly public companies often underperform relative balanced market index (Ritter 1991). Including firms event studies may distort long-term return expectations. issue, termed new listing bias, first identified (Barber Lyon 1997).New Listing Bias\nNewly public companies often underperform relative balanced market index (Ritter 1991). Including firms event studies may distort long-term return expectations. issue, termed new listing bias, first identified (Barber Lyon 1997).Rebalancing Bias\nRegularly rebalancing equal-weighted portfolio can lead overestimated long-term returns. process systematically sells winning stocks buys underperformers, tends skew buy--hold abnormal returns downward (Barber Lyon 1997).Rebalancing Bias\nRegularly rebalancing equal-weighted portfolio can lead overestimated long-term returns. process systematically sells winning stocks buys underperformers, tends skew buy--hold abnormal returns downward (Barber Lyon 1997).Value-Weight Bias\nValue-weighted portfolios, assign higher weights larger market capitalization stocks, may overestimate BHARs. approach mimics active strategy continuously buys winners sells underperformers, inflates long-run return estimates.Value-Weight Bias\nValue-weighted portfolios, assign higher weights larger market capitalization stocks, may overestimate BHARs. approach mimics active strategy continuously buys winners sells underperformers, inflates long-run return estimates.Buy--Hold Without Annual RebalancingAnother method involves holding initial portfolio fixed throughout investment period. approach, returns compounded, average calculated across securities:\\[\n\\Pi_{t = s}^{T} (1 + E(R_{})) = \\sum_{=s}^{n_t} \\left( w_{} \\prod_{t=1}^{T} (1 + R_{}) \\right)\n\\]:\\(T\\) = total investment period,\\(T\\) = total investment period,\\(R_{}\\) = return security \\(\\) time \\(t\\),\\(R_{}\\) = return security \\(\\) time \\(t\\),\\(n_t\\) = number securities portfolio,\\(n_t\\) = number securities portfolio,\\(w_{}\\) = initial weight firm \\(\\) portfolio period \\(s\\) (either equal-weighted value-weighted).\\(w_{}\\) = initial weight firm \\(\\) portfolio period \\(s\\) (either equal-weighted value-weighted).Key Characteristics ApproachNo Monthly Adjustments\nportfolio remains fixed based stocks available time \\(s\\), meaning:\nnew stocks added period \\(s\\).\nrebalancing occurs period.\nMonthly Adjustments\nportfolio remains fixed based stocks available time \\(s\\), meaning:new stocks added period \\(s\\).rebalancing occurs period.Avoids Rebalancing Bias\nSince forced buying selling, distortions due rebalancing minimized.Avoids Rebalancing Bias\nSince forced buying selling, distortions due rebalancing minimized.Market-Weight Adjustment Required\nSince value-weighted portfolios favor larger firms, adjustments may necessary prevent recently listed firms exerting excessive influence portfolio returns.Market-Weight Adjustment Required\nSince value-weighted portfolios favor larger firms, adjustments may necessary prevent recently listed firms exerting excessive influence portfolio returns.choice equal-weighted value-weighted portfolios affects results:\nEqual-weighted portfolios ensure firm contributes equally.\nValue-weighted portfolios reflect real-world investment scenarios may skewed toward larger firms.\nEqual-weighted portfolios ensure firm contributes equally.Value-weighted portfolios reflect real-world investment scenarios may skewed toward larger firms.Researchers define minimum inclusion criteria (e.g., stocks must trade least 12 months post-event) filter firms insufficient return data.empirical research, Wharton Research Data Services (WRDS) provides automated tool computing Buy--Hold Abnormal Returns. tool allows researchers generate types BHAR measures based different weighting rebalancing approaches:Equal-weighted vs. Value-weighted portfoliosWith vs. Without annual rebalancingThe WRDS platform enables users upload event data apply methodologies efficiently. details can found WRDS Long-Run Event Study.WRDS tool provides several options customizing event study settings:firm’s monthly returns missing selected event window, matching portfolio returns used fill gaps. ensures BHAR calculations remain consistent even individual firm data incomplete.","code":""},{"path":"sec-event-studies.html","id":"long-term-cumulative-abnormal-returns-lcars","chapter":"33 Event Studies","heading":"33.10.2 Long-term Cumulative Abnormal Returns (LCARs)","text":"Long-term Cumulative Abnormal Returns (LCARs) measure total abnormal return event firm extended period post-event. Unlike Buy--Hold Abnormal Returns, use compounding, LCARs sum abnormal returns time.method widely used long-run event studies particularly useful examining event’s impact evolves gradually rather instantaneously.LCAR firm \\(\\) post-event horizon \\((1,T)\\) given (. B. Sorescu, Chandy, Prabhu 2007):\\[\nLCAR_{} = \\sum_{t = 1}^{T} (R_{} - R_{pt})\n\\]:\\(R_{}\\) = Rate return stock \\(\\) month \\(t\\).\\(R_{}\\) = Rate return stock \\(\\) month \\(t\\).\\(R_{pt}\\) = Rate return counterfactual (benchmark) portfolio month \\(t\\).\\(R_{pt}\\) = Rate return counterfactual (benchmark) portfolio month \\(t\\).LCARs aggregate monthly abnormal returns capture cumulative effect event time.","code":""},{"path":"sec-event-studies.html","id":"key-considerations-in-using-lcars","chapter":"33 Event Studies","heading":"33.10.2.1 Key Considerations in Using LCARs","text":"Benchmark Portfolio SelectionThe choice counterfactual portfolio \\(R_{pt}\\) critical, serves reference point detecting abnormal performance. Common benchmarks include:Size book--market matched portfolios\nFirms grouped based market capitalization book--market ratio control firm characteristics.Size book--market matched portfolios\nFirms grouped based market capitalization book--market ratio control firm characteristics.Industry-matched portfolios\nFirms within industry (e.g., 2-digit SIC code) provide relevant comparison.Industry-matched portfolios\nFirms within industry (e.g., 2-digit SIC code) provide relevant comparison.Market model expectations\nExpected returns estimated using asset pricing models CAPM Fama-French 3-factor model.Market model expectations\nExpected returns estimated using asset pricing models CAPM Fama-French 3-factor model.Event Window LengthLong-term event studies use windows ranging 12 60 months (Loughran Ritter 1995; Brav Gompers 1997). longer window captures full market reaction increases risk contamination unrelated events.Statistical Significance IssuesSince LCARs use simple summation abnormal returns, can suffer :Cross-sectional dependence: Abnormal returns across firms may correlated, inflating t-statistics.Variance drift: standard deviation cumulative returns grows time, complicating inference.correct biases, researchers often use:Bootstrapping methodsCalendar-time portfolio approaches (e.g., Jensen’s Alpha)Skewness-adjusted t-tests (Lyon, Barber, Tsai 1999)short-term studies, LCAR BHAR tend yield similar results, long-term studies, BHAR amplifies impact extreme returns, whereas LCAR provides linear view.","code":"\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n# Simulate stock returns and benchmark portfolio returns\nset.seed(123)\nmonths <- 60  # 5-year event window\nfirms <- 50   # Number of event firms\n\n# Generate random stock returns (normally distributed)\nstock_returns <-\n    matrix(rnorm(months * firms, mean = 0.01, sd = 0.05),\n           nrow = months,\n           ncol = firms)\n\n# Generate benchmark portfolio returns\nbenchmark_returns <- rnorm(months, mean = 0.009, sd = 0.03)\n\n# Compute LCAR for each firm\nLCARs <-\n    apply(stock_returns, 2, function(stock)\n        cumsum(stock - benchmark_returns))\n\n# Convert to data frame for visualization\nLCAR_df <- as.data.frame(LCARs) %>%\n    mutate(Month = 1:months) %>%\n    pivot_longer(-Month, names_to = \"Firm\", values_to = \"LCAR\")\n\n# Plot LCAR trajectories\nggplot(LCAR_df, aes(x = Month, y = LCAR, group = Firm)) +\n    geom_line(alpha = 0.3) +\n    theme_minimal() +\n    labs(\n        title = \"Long-term Cumulative Abnormal Returns (LCARs)\",\n        x = \"Months Since Event\",\n        y = \"Cumulative Abnormal Return\",\n        caption = \"Each line represents an event firm's LCAR trajectory.\"\n    )"},{"path":"sec-event-studies.html","id":"calendar-time-portfolio-abnormal-returns-ctars","chapter":"33 Event Studies","heading":"33.10.3 Calendar-time Portfolio Abnormal Returns (CTARs)","text":"Calendar-time Portfolio Abnormal Returns (CTARs) method, also known Jensen’s Alpha approach, widely used long-run event studies address cross-sectional dependence among firms experiencing similar events. Unlike BHAR LCAR, focus individual stock returns, CTARs evaluate portfolio-level abnormal returns time.method follows strict procedure outlined Wiles et al. (2010) key advantages:Controls cross-sectional correlation aggregating event firms portfolios.Controls cross-sectional correlation aggregating event firms portfolios.Reduces model misspecification biases relying time-series regressions instead individual firm-level return calculations.Reduces model misspecification biases relying time-series regressions instead individual firm-level return calculations.","code":""},{"path":"sec-event-studies.html","id":"constructing-the-calendar-time-portfolio","chapter":"33 Event Studies","heading":"33.10.3.1 Constructing the Calendar-time Portfolio","text":"Portfolio Formation\nportfolio constructed every day calendar time (including firms experience event day).\nSecurities portfolio equally weighted avoid bias firm size differences.\nportfolio constructed every day calendar time (including firms experience event day).Securities portfolio equally weighted avoid bias firm size differences.Compute Average Abnormal Return PortfolioFor given portfolio \\(P\\) day \\(t\\):\\[\nAAR_{Pt} = \\frac{\\sum_{=1}^S AR_i}{S}\n\\]:\\(S\\) = Number stocks portfolio \\(P\\).\\(S\\) = Number stocks portfolio \\(P\\).\\(AR_i\\) = Abnormal return stock \\(\\) portfolio.\\(AR_i\\) = Abnormal return stock \\(\\) portfolio.Calculate Standard Deviation AAR Preceding \\(k\\) Days\ntime-series standard deviation \\(AAR_{Pt}\\), denoted \\(SD(AAR_{Pt})\\), calculated using preceding \\(k\\) days (rolling window), assuming independence time.Calculate Standard Deviation AAR Preceding \\(k\\) DaysThe time-series standard deviation \\(AAR_{Pt}\\), denoted \\(SD(AAR_{Pt})\\), calculated using preceding \\(k\\) days (rolling window), assuming independence time.Standardize Average Abnormal ReturnStandardize Average Abnormal Return\\[\nSAAR_{Pt} = \\frac{AAR_{Pt}}{SD(AAR_{Pt})}\n\\]Compute Average Standardized Abnormal Return (ASAAR)standardized residuals across portfolios averaged across full calendar time:\\[\nASAAR = \\frac{1}{n} \\sum_{t=1}^{255} SAAR_{Pt} \\times D_t\n\\]:\\(D_t = 1\\) least one security portfolio \\(P_t\\), otherwise \\(D_t = 0\\).\\(D_t = 1\\) least one security portfolio \\(P_t\\), otherwise \\(D_t = 0\\).\\(n\\) number days least one firm portfolio, defined :\\(n\\) number days least one firm portfolio, defined :\\[\nn = \\sum_{t=1}^{255} D_t\n\\]Compute Cumulative Average Standardized Abnormal Return (CASSAR)cumulative impact events time horizon \\(S_1\\) \\(S_2\\) given :\\[\nCASSAR_{S_1, S_2} = \\sum_{t=S_1}^{S_2} ASAAR\n\\]Compute Test StatisticIf ASAAR values independent time, standard deviation cumulative metric :\\[\n\\sqrt{S_2 - S_1 + 1}\n\\]Thus, test statistic assessing statistical significance :\\[\nt = \\frac{CASSAR_{S_1, S_2}}{\\sqrt{S_2 - S_1 + 1}}\n\\]","code":""},{"path":"sec-event-studies.html","id":"limitations-of-the-ctar-method","chapter":"33 Event Studies","heading":"33.10.3.2 Limitations of the CTAR Method","text":"CTAR offers robust cross-sectional controls, notable limitations:Examine Individual Stock Differences\nCTAR evaluates portfolio-level differences, masking firm-level variations.\nworkaround construct multiple portfolios based relevant firm characteristics (e.g., size, book--market, industry) compare intercepts.\nCTAR evaluates portfolio-level differences, masking firm-level variations.workaround construct multiple portfolios based relevant firm characteristics (e.g., size, book--market, industry) compare intercepts.Low Statistical Power\nCTAR criticized low power (.e., high Type II error rates) (Loughran Ritter 2000).\nDetecting significant abnormal returns requires large number event firms sufficiently long time-series.\nCTAR criticized low power (.e., high Type II error rates) (Loughran Ritter 2000).Detecting significant abnormal returns requires large number event firms sufficiently long time-series.","code":""},{"path":"sec-event-studies.html","id":"aggregation","chapter":"33 Event Studies","heading":"33.11 Aggregation","text":"","code":""},{"path":"sec-event-studies.html","id":"over-time","chapter":"33 Event Studies","heading":"33.11.1 Over Time","text":"assess impact events stock performance time, calculate Cumulative Abnormal Return (CAR) event windows.Hypotheses:\\(H_0\\): standardized cumulative abnormal return (SCAR) stock \\(\\) 0 (.e., event effect stock performance).\\(H_1\\): SCAR 0 (.e., event effect stock performance).","code":""},{"path":"sec-event-studies.html","id":"across-firms-and-over-time","chapter":"33 Event Studies","heading":"33.11.2 Across Firms and Over Time","text":"addition evaluating CAR individual stocks, may want aggregate results across multiple firms determine whether events systematically affect stock prices.Additional Assumptions:Uncorrelated Abnormal Returns: abnormal returns different stocks assumed uncorrelated. strong assumption, holds reasonably well event windows different stocks overlap.Overlapping Event Windows: event windows overlap, follow methodology proposed Bernard (1987) Schipper Thompson (1983), Schipper Smith (1983).Hypotheses:\\(H_0\\): mean abnormal return across firms 0 (.e., systematic effect event).\\(H_1\\): mean abnormal return across firms different 0 (.e., event systematic effect).","code":""},{"path":"sec-event-studies.html","id":"statistical-tests-1","chapter":"33 Event Studies","heading":"33.11.3 Statistical Tests","text":"Two broad categories statistical tests can applied: parametric non-parametric.","code":""},{"path":"sec-event-studies.html","id":"parametric-tests-1","chapter":"33 Event Studies","heading":"33.11.3.1 Parametric Tests","text":"tests assume abnormal returns normally distributed. Empirical evidence suggests either following approaches can work well:Aggregate CAR stocks\nUse method true abnormal return variance greater stocks higher variance.\nUse method true abnormal return variance greater stocks higher variance.Aggregate SCAR stocks\nUse method true abnormal return constant across stocks.\nUse method true abnormal return constant across stocks.","code":""},{"path":"sec-event-studies.html","id":"non-parametric-tests-2","chapter":"33 Event Studies","heading":"33.11.3.2 Non-Parametric Tests","text":"Non-parametric tests provide robustness avoiding assumptions distribution abnormal returns.Sign Test\nAssumes abnormal returns CAR independent across stocks.\n\\(H_0\\), expect 50% stocks positive abnormal returns 50% negative abnormal returns.\nsystematic relationship event abnormal returns, observe significant deviation 50-50 split.\nalternative hypothesis suggests negative relationship, null hypothesis must adjusted accordingly.\nImportant Note: presence skewed distributions (common daily stock return data), size test may reliable. cases, rank-based test preferred.\nAssumes abnormal returns CAR independent across stocks.\\(H_0\\), expect 50% stocks positive abnormal returns 50% negative abnormal returns.systematic relationship event abnormal returns, observe significant deviation 50-50 split.alternative hypothesis suggests negative relationship, null hypothesis must adjusted accordingly.Important Note: presence skewed distributions (common daily stock return data), size test may reliable. cases, rank-based test preferred.Rank Test\nrobust sign test distributions skewed.\nNull Hypothesis (\\(H_0\\)): abnormal return event window.\nAlternative Hypothesis (\\(H_1\\)): abnormal return associated event.\nrobust sign test distributions skewed.Null Hypothesis (\\(H_0\\)): abnormal return event window.Alternative Hypothesis (\\(H_1\\)): abnormal return associated event.","code":""},{"path":"sec-event-studies.html","id":"heterogeneity-in-the-event-effect","chapter":"33 Event Studies","heading":"33.12 Heterogeneity in the Event Effect","text":"impact event stock performance can vary significantly across firms due firm-specific event-specific characteristics. model heterogeneity using following regression framework:\\[\ny = X \\theta + \\eta\n\\]:\\(y\\) = Cumulative Abnormal Return (CAR) given event window.\\(X\\) = Matrix firm- event-specific characteristics explain heterogeneity event effect.\\(\\theta\\) = Vector coefficients capturing impact characteristics abnormal returns.\\(\\eta\\) = Error term, capturing unobserved factors.Selection bias can arise firm characteristics influence likelihood experiencing event magnitude abnormal returns. One common issue investor anticipation:Example: Larger firms might benefit event, leading investors preemptively price expected effect, potentially distorting CAR measurements.Example: Larger firms might benefit event, leading investors preemptively price expected effect, potentially distorting CAR measurements.can result endogeneity problem, expected returns systematically related firm characteristics.can result endogeneity problem, expected returns systematically related firm characteristics.correct issue, White’s heteroskedasticity-consistent \\(t\\)-statistics used. provides lower bounds true significance coefficient estimates accounting heteroskedasticity regression residuals.Key Point: Even average CAR significantly different zero, analyzing heterogeneity remains essential, particularly CAR variance high (Boyd, Chandy, Cunha Jr 2010).","code":""},{"path":"sec-event-studies.html","id":"common-variables-affecting-car-in-marketing-and-finance","chapter":"33 Event Studies","heading":"33.12.1 Common Variables Affecting CAR in Marketing and Finance","text":"Event effects stock returns can influenced various firm-specific market-specific factors. following variables commonly examined event studies, summarized . Sorescu, Warren, Ertekin (2017) (Table 4).Firm-Specific CharacteristicsFirm Size\nFinance Literature: Typically negatively correlated abnormal returns.\nMarketing Literature: Results mixed, suggesting different dynamics.\nInterpretation: Large firms may less information asymmetry, leading smaller stock reactions.\nFinance Literature: Typically negatively correlated abnormal returns.Marketing Literature: Results mixed, suggesting different dynamics.Interpretation: Large firms may less information asymmetry, leading smaller stock reactions.Number Event Occurrences\nfirm frequently experiences similar events may see diminishing stock market reactions time.\nfirm frequently experiences similar events may see diminishing stock market reactions time.R&D Expenditure\nHigher R&D investment often signals long-term innovation potential may also increase risk, affecting abnormal returns.\nHigher R&D investment often signals long-term innovation potential may also increase risk, affecting abnormal returns.Advertising Expense\nCan enhance brand equity consumer perception, leading stronger stock price response events.\nCan enhance brand equity consumer perception, leading stronger stock price response events.Marketing Investment (SG&- Selling, General & Administrative Expenses)\nproxy strategic spending market development.\nHigh marketing investment may drive higher abnormal returns perceived value-enhancing.\nproxy strategic spending market development.High marketing investment may drive higher abnormal returns perceived value-enhancing.Financial Leverage (Debt--Equity Ratio)\nHigh leverage can amplify risk, leading pronounced market reactions events.\nHigh leverage can amplify risk, leading pronounced market reactions events.Book--Market Ratio\nfundamental indicator valuation.\nHigh book--market firms (value stocks) may respond differently events compared low book--market firms (growth stocks).\nfundamental indicator valuation.High book--market firms (value stocks) may respond differently events compared low book--market firms (growth stocks).Return Assets (ROA)\nmeasure firm profitability.\nHigher ROA firms may less susceptible negative shocks.\nmeasure firm profitability.Higher ROA firms may less susceptible negative shocks.Free Cash Flow\nHigh free cash flow can signal financial flexibility, potentially mitigating negative event impacts.\nHigh free cash flow can signal financial flexibility, potentially mitigating negative event impacts.Sales Growth\nproxy firm momentum.\nHigher growth firms may exhibit stronger abnormal returns following positive events.\nproxy firm momentum.Higher growth firms may exhibit stronger abnormal returns following positive events.Firm Age\nYounger firms may experience higher abnormal returns due greater investor uncertainty information asymmetry.\nYounger firms may experience higher abnormal returns due greater investor uncertainty information asymmetry.Industry-Specific & Market-Level CharacteristicsIndustry Concentration (Herfindahl-Hirschman Index - HHI, Number Competitors)\nHigh industry concentration (fewer competitors) can reduce competitive pressure, leading stronger abnormal returns.\nHigh industry concentration (fewer competitors) can reduce competitive pressure, leading stronger abnormal returns.Market Share\nFirms higher market share may experience weaker abnormal returns due already-established dominance.\nFirms higher market share may experience weaker abnormal returns due already-established dominance.Market Size (Total Sales Volume within Firm’s SIC Code)\nmeasure industry attractiveness.\nEvents occurring larger markets may muted effects due broader investor diversification.\nmeasure industry attractiveness.Events occurring larger markets may muted effects due broader investor diversification.Marketing Capability\nFirms stronger marketing capabilities may better leverage events long-term brand revenue growth, influencing CAR.\nFirms stronger marketing capabilities may better leverage events long-term brand revenue growth, influencing CAR.","code":""},{"path":"sec-event-studies.html","id":"expected-return-calculation","chapter":"33 Event Studies","heading":"33.13 Expected Return Calculation","text":"Expected return models essential estimating abnormal returns event studies. models help separate normal stock price movements caused specific events.","code":""},{"path":"sec-event-studies.html","id":"statistical-models-for-expected-returns","chapter":"33 Event Studies","heading":"33.13.1 Statistical Models for Expected Returns","text":"Statistical models rely assumptions behavior returns, often assuming stable distributions (Owen Rabinovitch 1983). models impose economic constraints instead focus statistical properties returns.","code":""},{"path":"sec-event-studies.html","id":"constant-mean-return-model","chapter":"33 Event Studies","heading":"33.13.1.1 Constant Mean Return Model","text":"simplest statistical model assumes stock’s expected return simply historical mean return:\\[\nRa_{} = R_{} - \\bar{R}_i\n\\]:\\(R_{}\\) = observed return stock \\(\\) period \\(t\\)\\(\\bar{R}_i\\) = mean return stock \\(\\) estimation period\\(Ra_{}\\) = abnormal return period \\(t\\) (.e., deviation historical average)Assumptions:Returns revert mean time (.e., follow stable mean-reverting process).assumption questionable, market conditions evolve dynamically.Empirical Note:\nconstant mean return model typically delivers similar results complex models since variance abnormal returns substantially reduced using sophisticated statistical approaches (S. J. Brown Warner 1985).","code":""},{"path":"sec-event-studies.html","id":"market-model","chapter":"33 Event Studies","heading":"33.13.1.2 Market Model","text":"widely used alternative constant mean model market model, assumes stock returns linearly related market returns:\\[\nR_{} = \\alpha_i + \\beta_i R_{mt} + \\epsilon_{}\n\\]:\\(R_{}\\) = return stock \\(\\) period \\(t\\)\\(R_{mt}\\) = market return period \\(t\\) (e.g., S&P 500 index)\\(\\alpha_i\\) = stock-specific intercept (capturing average return explained market)\\(\\beta_i\\) = systematic risk (market beta) stock \\(\\)\\(\\epsilon_{}\\) = zero-mean error term variance \\(\\sigma^2\\), capturing idiosyncratic riskNotes Implementation:market return (\\(R_{mt}\\)) typically proxied using:\nS&P 500 index\nCRSP value-weighted index\nCRSP equal-weighted index\nS&P 500 indexCRSP value-weighted indexCRSP equal-weighted indexIf \\(\\beta_i = 0\\), market model reduces constant mean return model.Key Insight:\nbetter fit market model, lower variance abnormal returns, making easier detect event effects.Robust Estimation:account heteroskedasticity autocorrelation, recommended use Generalized Method Moments (GMM) estimation.","code":""},{"path":"sec-event-studies.html","id":"fama-french-multifactor-models","chapter":"33 Event Studies","heading":"33.13.1.3 Fama-French Multifactor Models","text":"Fama-French family models extends market model incorporating additional factors capture systematic risks beyond market exposure.Key Considerations:distinction using total return excess return dependent variable.correct specification involves excess returns individual stocks market portfolio (Fama French 2010, 1917).Interpretation \\(\\alpha_i\\):\\(\\alpha_i\\) represents abnormal return, .e., return unexplained model.","code":""},{"path":"sec-event-studies.html","id":"fama-french-three-factor-model-ff3","chapter":"33 Event Studies","heading":"33.13.1.3.1 Fama-French Three-Factor Model (FF3)","text":"(Fama French 1993)\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} &= \\alpha_i + \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t\n\\end{aligned}\n\\]:\\(r_{ft}\\) = risk-free rate (e.g., 3-month Treasury bill)\\(R_{mt}\\) = market return (e.g., S&P 500)\\(SML_t\\) = size factor (returns small-cap stocks minus large-cap stocks)\\(HML_t\\) = value factor (returns high book--market stocks minus low book--market stocks)","code":""},{"path":"sec-event-studies.html","id":"fama-french-four-factor-model-ff4","chapter":"33 Event Studies","heading":"33.13.1.3.2 Fama-French Four-Factor Model (FF4)","text":"(Carhart 1997) extends FF3 adding momentum factor:\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} &= \\alpha_i + \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t\n\\end{aligned}\n\\]:\\(UMD_t\\) = momentum factor (returns high past-return stocks minus low past-return stocks)Practical Application Marketing:(. Sorescu, Warren, Ertekin 2017, 195) recommends:Market Model short-term event windows.Market Model short-term event windows.Fama-French Model long-term windows.Fama-French Model long-term windows.However, statistical properties FF model daily event studies remain untested.However, statistical properties FF model daily event studies remain untested.","code":""},{"path":"sec-event-studies.html","id":"economic-models-for-expected-returns","chapter":"33 Event Studies","heading":"33.13.2 Economic Models for Expected Returns","text":"Economic models impose theoretical constraints expected returns based equilibrium asset pricing theory. two widely used models :","code":""},{"path":"sec-event-studies.html","id":"capital-asset-pricing-model-capm","chapter":"33 Event Studies","heading":"33.13.2.1 Capital Asset Pricing Model (CAPM)","text":"CAPM derived modern portfolio theory assumes expected returns determined solely market risk:\\[\nE(R_i) = R_f + \\beta_i (E(R_m) - R_f)\n\\]:\\(E(R_i)\\) = expected return stock \\(\\)\\(R_f\\) = risk-free rate\\(E(R_m) - R_f\\) = market risk premium (excess return market portfolio)\\(\\beta_i\\) = firm-specific market beta (systematic risk measure)Key Assumption:\nInvestors hold market portfolio, systematic risk (beta) matters.","code":""},{"path":"sec-event-studies.html","id":"arbitrage-pricing-theory-apt","chapter":"33 Event Studies","heading":"33.13.2.2 Arbitrage Pricing Theory (APT)","text":"APT generalizes CAPM allowing multiple risk factors drive expected returns:\\[\nR = R_f + \\Lambda f + \\epsilon\n\\]:\\(\\Lambda\\) = factor loadings (sensitivities risk factors)\\(f \\sim N(\\mu, \\Omega)\\) = vector risk factors\n\\(\\mu\\) = expected risk premiums\n\\(\\Omega\\) = factor covariance matrix\n\\(\\mu\\) = expected risk premiums\\(\\Omega\\) = factor covariance matrix\\(\\epsilon \\sim N(0, \\Psi)\\) = idiosyncratic error termAPT vs. CAPM:CAPM assumes single factor (market risk).CAPM assumes single factor (market risk).APT allows multiple systematic factors, making flexible empirical applications.APT allows multiple systematic factors, making flexible empirical applications.Summary: Model Comparison","code":""},{"path":"sec-event-studies.html","id":"application-of-event-study","chapter":"33 Event Studies","heading":"33.14 Application of Event Study","text":"Several R packages facilitate event studies. commonly used ones:install packages, run:","code":"\ninstall.packages(\n    c(\n        \"eventstudies\",\n        \"erer\",\n        \"EventStudy\",\n        \"AbnormalReturns\",\n        \"PerformanceAnalytics\",\n        \"tidyquant\",\n        \"tidyverse\"\n    )\n)"},{"path":"sec-event-studies.html","id":"sorting-portfolios-for-expected-returns","chapter":"33 Event Studies","heading":"33.14.1 Sorting Portfolios for Expected Returns","text":"common approach finance sort stocks portfolios based firm characteristics size book--market (B/M) ratio. method helps control possibility standard models (e.g., Fama-French) may correctly specified.Sorting ProcessSort stock returns 10 deciles based size (market capitalization).Sort stock returns 10 deciles based size (market capitalization).Within size decile, sort returns 10 deciles based B/M ratio.Within size decile, sort returns 10 deciles based B/M ratio.Calculate average return portfolio period (.e., expected return stocks given characteristics).Calculate average return portfolio period (.e., expected return stocks given characteristics).Compare stock’s return corresponding portfolio.Compare stock’s return corresponding portfolio.Important Notes:Sorting often leads conservative estimates compared Fama-French models.Sorting often leads conservative estimates compared Fama-French models.event study results change depending sorting order (e.g., sorting B/M first vs. size first), suggests findings robust.event study results change depending sorting order (e.g., sorting B/M first vs. size first), suggests findings robust.","code":""},{"path":"sec-event-studies.html","id":"erer-package","chapter":"33 Event Studies","heading":"33.14.2 erer Package","text":"erer package provides straightforward implementation event studies.Step 1: Load Required LibrariesStep 2: Load Sample DataThe package includes example dataset, daEsa, contains stock returns event dates.Step 3: Compute Abnormal ReturnsWe define estimation window (250 days event) event window (\\(\\pm5\\) days around event):Step 4: Visualizing Results","code":"\nlibrary(erer)\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(daEsa)\nhead(daEsa)\n#>       date       tb3m    sp500     bbc     bow     csk      gp      ip     kmb\n#> 1 19900102  0.3973510  1.76420  2.5352  1.3575  0.6289  4.1237  1.3274  1.8707\n#> 2 19900103  0.6596306 -0.25889  0.2747  0.8929  6.2500  0.9901 -0.2183 -0.3339\n#> 3 19900104 -0.5242464 -0.86503 -1.3699 -0.4425 -2.3529  0.7353 -0.4376 -0.1675\n#> 4 19900105 -0.6587615 -0.98041 -0.5556 -0.4444  1.2048  0.0000  0.2198 -0.6711\n#> 5 19900108  0.0000000  0.45043 -1.3966 -0.8929 -1.1905  0.4866 -0.2193  1.0135\n#> 6 19900109  0.1326260 -1.18567  0.2833 -0.4505 -2.4096 -0.2421 -2.1978 -2.1739\n#>       lpx     mwv     pch     pcl      pop     tin     wpp      wy\n#> 1  1.7341  1.6529  4.0816  1.5464  2.43525 -1.0791  2.9197  2.7149\n#> 2  0.8523  2.0325  0.0000  0.5076  1.41509 -1.4545  0.7092 -2.2026\n#> 3 -0.2817  0.3984  0.3268 -0.5051 -0.93023 -0.1845  2.1127 -0.9009\n#> 4 -0.8475 -0.3968 -0.6515 -0.5076  0.00000  0.5545 -0.6897 -0.4545\n#> 5 -0.5698 -0.3984  0.3279  1.0204 -0.93897 -0.5515 -0.6944  0.0000\n#> 6 -0.2865 -1.6000  0.3268 -2.5253 -3.79147 -2.4030  0.6993 -1.8265\nhh <- evReturn(\n    y = daEsa,      \n    firm = \"wpp\",   \n    y.date = \"date\",\n    index = \"sp500\", \n    est.win = 250,   \n    event.date = 19990505, \n    event.win = 5    \n)\nplot(hh)"},{"path":"sec-event-studies.html","id":"eventus","chapter":"33 Event Studies","heading":"33.14.3 Eventus","text":"2 types output:Basic Event Study\nUsing different estimation methods (e.g., market model calendar-time approach)\ninclude event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.\nBasic Event StudyUsing different estimation methods (e.g., market model calendar-time approach)Using different estimation methods (e.g., market model calendar-time approach)include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.Cross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)\nSince stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.\nCross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)Since stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.","code":""},{"path":"sec-event-studies.html","id":"basic-event-study","chapter":"33 Event Studies","heading":"33.14.3.1 Basic Event Study","text":"Input text file contains firm identifier (e.g., PERMNO, CUSIP) event dateChoose market indices: equally weighted value weighted index (.e., weighted market capitalization). check Fama-French Carhart factors.Estimation options\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.\nUse “autodate” options: first trading event date used event falls weekend holiday\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.Estimation period: ESTLEN = 100 convention estimation impacted outliers.Use “autodate” options: first trading event date used event falls weekend holidayUse “autodate” options: first trading event date used event falls weekend holidayAbnormal returns window: depends specific eventChoose test: either parametric (including Patell Standardized Residual (PSR)) non-parametric","code":""},{"path":"sec-event-studies.html","id":"cross-sectional-analysis-of-eventus","chapter":"33 Event Studies","heading":"33.14.3.2 Cross-sectional Analysis of Eventus","text":"Similar Basic Event Study, now can event-specific abnormal returns.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-instrumental-variables","chapter":"34 Instrumental Variables","heading":"34 Instrumental Variables","text":"many empirical settings, seek estimate causal effect explanatory variable \\(X\\) outcome variable \\(Y\\). common starting point Ordinary Least Squares regression:\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon.\n\\]OLS provide unbiased consistent estimate \\(\\beta_1\\), explanatory variable \\(X\\) must satisfy exogeneity condition:\\[ \\mathbb{E}[\\varepsilon \\mid X] = 0. \\]However, \\(X\\) correlated error term \\(\\varepsilon\\), assumption violated, leading endogeneity. result, OLS estimator biased inconsistent. Common causes endogeneity include:Omitted Variable Bias (OVB): relevant variable omitted regression, leading correlation \\(X\\) \\(\\varepsilon\\).Simultaneity: \\(X\\) \\(Y\\) jointly determined, supply--demand models.Measurement Error: Errors measuring \\(X\\) introduce bias estimation.\nAttenuation Bias Errors--Variables: measurement error independent variable leads underestimate true effect (biasing coefficient toward zero).\nAttenuation Bias Errors--Variables: measurement error independent variable leads underestimate true effect (biasing coefficient toward zero).Instrumental Variables (IV) estimation addresses endogeneity introducing instrument \\(Z\\) affects \\(Y\\) \\(X\\). Similar RCT, try introduce randomization (random assignment treatment) treatment variable using variation instrument.Logic using instrument:Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)See relationship treatment outcome terms residual variations exogenous omitted variables.See relationship treatment outcome terms residual variations exogenous omitted variables.instrument \\(Z\\) valid, must satisfy two conditions:Relevance Condition: instrument \\(Z\\) must correlated endogenous variable \\(X\\): \\[ \\text{Cov}(Z, X) \\neq 0. \\]Relevance Condition: instrument \\(Z\\) must correlated endogenous variable \\(X\\): \\[ \\text{Cov}(Z, X) \\neq 0. \\]Exogeneity Condition (Exclusion Restriction): instrument \\(Z\\) must uncorrelated error term \\(\\varepsilon\\) affect \\(Y\\) \\(X\\): \\[ \\text{Cov}(Z, \\varepsilon) = 0. \\]Exogeneity Condition (Exclusion Restriction): instrument \\(Z\\) must uncorrelated error term \\(\\varepsilon\\) affect \\(Y\\) \\(X\\): \\[ \\text{Cov}(Z, \\varepsilon) = 0. \\]conditions ensure \\(Z\\) provides exogenous variation \\(X\\), allowing us isolate causal effect \\(X\\) \\(Y\\).conditions ensure \\(Z\\) provides exogenous variation \\(X\\), allowing us estimate causal effect \\(X\\) \\(Y\\). Random assignment \\(Z\\) helps ensure exogeneity, must also confirm \\(Z\\) influences \\(Y\\) \\(X\\) satisfy exclusion restriction.IV approach dates back early econometric research 1920s 1930s, significant role Cowles Commission studies simultaneous equations. Key contributions include:Wright (1928): One earliest applications, studying supply demand pig iron.J. Angrist Imbens (1991): Popularized IV methods using quarter--birth instrument education.credibility revolution econometrics (1990s–2000s) led widespread use IVs applied research, particularly economics, political science, epidemiology.","code":""},{"path":"sec-instrumental-variables.html","id":"challenges-with-instrumental-variables","chapter":"34 Instrumental Variables","heading":"34.1 Challenges with Instrumental Variables","text":"IVs can provide solution endogeneity, several challenges arise:Exclusion Restriction Violations: \\(Z\\) affects \\(Y\\) channel \\(X\\), IV estimate biased.Repeated Use Instruments: Common instruments, weather policy changes, may invalid due widespread application across studies (Gallen 2020). One needs test invalid instruments (Hausman-like test).\nnotable example Mellon (2021), documents 289 social sciences studies used weather instrument 195 variables, raising concerns exclusion violations.\nnotable example Mellon (2021), documents 289 social sciences studies used weather instrument 195 variables, raising concerns exclusion violations.Heterogeneous Treatment Effects: Local Average Treatment Effect (LATE) estimated IV applies compliers—units whose treatment status affected instrument.Weak Instruments: little correlation endogenous regressor yields unstable estimates.Invalid Instruments: instrument violates exogeneity, results inconsistent.Interpretation Mistakes: IV identifies effect “marginal” units whose treatment status driven instrument.","code":""},{"path":"sec-instrumental-variables.html","id":"framework-for-instrumental-variables","chapter":"34 Instrumental Variables","heading":"34.2 Framework for Instrumental Variables","text":"consider binary treatment framework :\\(D_i \\sim Bernoulli(p)\\) dummy treatment variable.\\(D_i \\sim Bernoulli(p)\\) dummy treatment variable.\\((Y_{0i}, Y_{1i})\\) potential outcomes control treatment.\\((Y_{0i}, Y_{1i})\\) potential outcomes control treatment.observed outcome : \\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i. \\]observed outcome : \\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i. \\]introduce instrumental variable \\(Z_i\\) satisfying: \\[ Z_i \\perp (Y_{0i}, Y_{1i}, D_{0i}, D_{1i}). \\]\nmeans \\(Z_i\\) independent potential outcomes potential treatment status.\n\\(Z_i\\) must also correlated \\(D_i\\) satisfy relevance condition.\nintroduce instrumental variable \\(Z_i\\) satisfying: \\[ Z_i \\perp (Y_{0i}, Y_{1i}, D_{0i}, D_{1i}). \\]means \\(Z_i\\) independent potential outcomes potential treatment status.\\(Z_i\\) must also correlated \\(D_i\\) satisfy relevance condition.","code":""},{"path":"sec-instrumental-variables.html","id":"constant-treatment-effect-model","chapter":"34 Instrumental Variables","heading":"34.2.1 Constant-Treatment-Effect Model","text":"constant treatment effect assumption (.e., treatment effect individuals),\\[\n\\begin{aligned}\nY_{0i} &= \\alpha + \\eta_i, \\\\\nY_{1i} - Y_{0i} &= \\rho, \\\\\nY_i &= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\\\\n    &= \\alpha + \\eta_i  + D_i \\rho \\\\\n    &= \\alpha + \\rho D_i + \\eta_i.\n\\end{aligned}\n\\]:\\(\\eta_i\\) captures individual-level heterogeneity.\\(\\rho\\) constant treatment effect.problem OLS estimation \\(D_i\\) may correlated \\(\\eta_i\\), leading endogeneity bias.","code":""},{"path":"sec-instrumental-variables.html","id":"instrumental-variable-solution","chapter":"34 Instrumental Variables","heading":"34.2.2 Instrumental Variable Solution","text":"valid instrument \\(Z_i\\) allows us estimate causal effect \\(\\rho\\) via:\\[\n\\begin{aligned}\n\\rho &= \\frac{\\text{Cov}(Y_i, Z_i)}{\\text{Cov}(D_i, Z_i)} \\\\\n     &= \\frac{\\text{Cov}(Y_i, Z_i) / V(Z_i) }{\\text{Cov}(D_i, Z_i) / V(Z_i)} \\\\\n     &= \\frac{\\text{Reduced form estimate}}{\\text{First-stage estimate}} \\\\\n     &= \\frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]}.\n\\end{aligned}\n\\]ratio measures treatment effect \\(Z_i\\) valid instrument.","code":""},{"path":"sec-instrumental-variables.html","id":"heterogeneous-treatment-effects-and-the-late-framework","chapter":"34 Instrumental Variables","heading":"34.2.3 Heterogeneous Treatment Effects and the LATE Framework","text":"general framework treatment effects vary across individuals,Define potential outcomes : \\[ Y_i(d,z) = \\text{outcome unit } \\text{ given } D_i = d, Z_i = z. \\]Define potential outcomes : \\[ Y_i(d,z) = \\text{outcome unit } \\text{ given } D_i = d, Z_i = z. \\]Define treatment status based \\(Z_i\\): \\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}). \\]\n:\n\\(D_{1i}\\) treatment status \\(Z_i = 1\\).\n\\(D_{0i}\\) treatment status \\(Z_i = 0\\).\n\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\).\nDefine treatment status based \\(Z_i\\): \\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}). \\]:\\(D_{1i}\\) treatment status \\(Z_i = 1\\).\\(D_{0i}\\) treatment status \\(Z_i = 0\\).\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\).","code":""},{"path":"sec-instrumental-variables.html","id":"assumptions-for-late-identification","chapter":"34 Instrumental Variables","heading":"34.2.4 Assumptions for LATE Identification","text":"","code":""},{"path":"sec-instrumental-variables.html","id":"independence-instrument-randomization","chapter":"34 Instrumental Variables","heading":"34.2.4.1 Independence (Instrument Randomization)","text":"instrument must good randomly assigned:\\[ [\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\perp Z_i. \\]ensures \\(Z_i\\) uncorrelated potential outcomes potential treatment status.assumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)\\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &= E[D_{1i} - D_{0i}] \\end{aligned} \\]assumption also sufficient causal interpretation reduced form, see effect instrument \\(Z_i\\) outcome \\(Y_i\\):\\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \\]","code":""},{"path":"sec-instrumental-variables.html","id":"exclusion-restriction","chapter":"34 Instrumental Variables","heading":"34.2.4.2 Exclusion Restriction","text":"also known existence instrument assumption (G. W. Imbens Angrist 1994). instrument affect \\(Y_i\\) \\(D_i\\) (.e., treatment \\(D_i\\) fully mediates effect \\(Z_i\\) \\(Y_i\\)):\\[\n\\begin{aligned}\nY_{1i} &= Y_i (1,1) = Y_i (1,0)\\\\\nY_{0i} &= Y_i (0,1) = Y_i (0,0)\n\\end{aligned}\n\\]assumption (assume \\(Y_{1i, Y_{0i}}\\) already satisfy independence assumption), observed outcome \\(Y_i\\) can rewritten :\\[\n\\begin{aligned}\n  Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\\n      &= Y_{0i} + (Y_{1i} - Y_{0i}) D_i.\n  \\end{aligned}\n\\]assumption let us go reduced-form causal effects treatment effects (J. D. Angrist Imbens 1995).","code":""},{"path":"sec-instrumental-variables.html","id":"monotonicity-no-defiers","chapter":"34 Instrumental Variables","heading":"34.2.4.3 Monotonicity (No Defiers)","text":"assume \\(Z_i\\) affects \\(D_i\\) monotonic way:\\[ D_{1i} \\geq D_{0i}, \\quad \\forall . \\]assumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\). implies \\(Z_i\\) moves individuals toward treatment, never away. rules “defiers” (.e., individuals taken treatment assigned refuse assigned).assumption used solve problem shifts participation status back non-participation status.\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.\nthird solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.Alternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).monotonicity,\\[\n\\begin{aligned}\n  E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}].\n  \\end{aligned}\n\\]","code":""},{"path":"sec-instrumental-variables.html","id":"local-average-treatment-effect-theorem","chapter":"34 Instrumental Variables","heading":"34.2.5 Local Average Treatment Effect Theorem","text":"Given Independence, Exclusion, Monotonicity, obtain LATE result (J. D. Angrist Pischke 2009, 4.4.1):\\[\n\\begin{aligned}\n\\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} > D_{0i}].\n\\end{aligned}\n\\]states IV estimator recovers causal effect compliers—units whose treatment status changes due \\(Z_i\\).IV identifies treatment effects switchers (compliers):IV estimates nothing always-takers never-takers since treatment status unaffected \\(Z_i\\) (Similar fixed-effects models).","code":""},{"path":"sec-instrumental-variables.html","id":"iv-in-randomized-trials-noncompliance","chapter":"34 Instrumental Variables","heading":"34.2.6 IV in Randomized Trials (Noncompliance)","text":"randomized trials, compliance imperfect (.e., compliance voluntary), individuals treatment group always take treatment (e.g., selection bias), intention--treat (ITT) estimates valid contaminated noncompliance.IV estimation using random assignment (\\(Z_i\\)) instrument actual treatment received (\\(D_i\\)) recovers LATE.\\[\n\\begin{aligned}\n\\frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \\frac{\\text{Intent--Treat Effect}}{\\text{Compliance Rate}} = E[Y_{1i} - Y_{0i} |D_i = 1].\n\\end{aligned}\n\\]full compliance, LATE = Treatment Effect Treated (TOT).","code":""},{"path":"sec-instrumental-variables.html","id":"sec-estimation","chapter":"34 Instrumental Variables","heading":"34.3 Estimation","text":"","code":""},{"path":"sec-instrumental-variables.html","id":"sec-two-stage-least-squares-estimation","chapter":"34 Instrumental Variables","heading":"34.3.1 Two-Stage Least Squares Estimation","text":"Two-Stage Least Squares (2SLS) widely used IV estimator ’s special case IV-GMM. Consider structural equation:\\[\nY_i = X_i \\beta + \\varepsilon_i,\n\\]\\(X_i\\) endogenous. introduce instrument \\(Z_i\\) satisfying:Relevance: \\(Z_i\\) correlated \\(X_i\\).Exogeneity: \\(Z_i\\) uncorrelated \\(\\varepsilon_i\\).2SLS StepsFirst-Stage Regression: Predict \\(X_i\\) using instrument: \\[\nX_i = \\pi_0 + \\pi_1 Z_i + v_i.\n\\]\nObtain fitted values \\(\\hat{X}_i = \\pi_0 + \\pi_1 Z_i\\).\nFirst-Stage Regression: Predict \\(X_i\\) using instrument: \\[\nX_i = \\pi_0 + \\pi_1 Z_i + v_i.\n\\]Obtain fitted values \\(\\hat{X}_i = \\pi_0 + \\pi_1 Z_i\\).Second-Stage Regression: Use \\(\\hat{X}_i\\) place \\(X_i\\): \\[\nY_i = \\beta_0 + \\beta_1 \\hat{X}_i + \\varepsilon_i.\n\\]\nestimated \\(\\hat{\\beta}_1\\) IV estimator.\nSecond-Stage Regression: Use \\(\\hat{X}_i\\) place \\(X_i\\): \\[\nY_i = \\beta_0 + \\beta_1 \\hat{X}_i + \\varepsilon_i.\n\\]estimated \\(\\hat{\\beta}_1\\) IV estimator.Diagnostic TestsTo assess instrument validity:set default printingTo see results different stages","code":"\nlibrary(fixest)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\n# IV Estimation\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nsummary(est_iv)\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\nfitstat(est_iv, type = c(\"n\", \"f\", \"ivf\", \"ivf1\", \"ivf2\", \"ivwald\", \"cd\"))\n#>                 Observations: 150\n#>                       F-test: stat = 132.0    , p < 2.2e-16 , on 3 and 146 DoF.\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>           F-test (2nd stage): stat = 194.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> Wald (1st stage), x_endo_1  : stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: IID.\n#> Wald (1st stage), x_endo_2  : stat =   3.25828, p = 0.041268, on 2 and 146 DoF, VCOV: IID.\n#>                 Cragg-Donald: 3.11162\n# always add second-stage Wald test\nsetFixest_print(fitstat = ~ . + ivwald2)\nest_iv\n# first-stage\nsummary(est_iv, stage = 1)\n\n# second-stage\nsummary(est_iv, stage = 2)\n\n# both stages\netable(summary(est_iv, stage = 1:2), fitstat = ~ . + ivfall + ivwaldall.p)\netable(summary(est_iv, stage = 2:1), fitstat = ~ . + ivfall + ivwaldall.p)\n# .p means p-value, not statistic\n# `all` means IV only"},{"path":"sec-instrumental-variables.html","id":"iv-gmm","chapter":"34 Instrumental Variables","heading":"34.3.2 IV-GMM","text":"Generalized Method Moments (GMM) provides flexible estimation framework generalizes Instrumental Variables (IV) approach, including 2SLS special case. key idea behind GMM use moment conditions derived economic models estimate parameters efficiently, even presence endogeneity.Consider standard linear regression model:\\[\nY = X\\beta + u, \\quad u \\sim (0, \\Omega)\n\\]:\\(Y\\) \\(N \\times 1\\) vector dependent variable.\\(X\\) \\(N \\times k\\) matrix endogenous regressors.\\(\\beta\\) \\(k \\times 1\\) vector coefficients.\\(u\\) \\(N \\times 1\\) vector error terms.\\(\\Omega\\) variance-covariance matrix \\(u\\).address endogeneity \\(X\\), introduce \\(N \\times l\\) matrix instruments, \\(Z\\), \\(l \\geq k\\). moment conditions given :\\[\nE[Z_i' u_i] = E[Z_i' (Y_i - X_i \\beta)] = 0.\n\\]practice, expectations replaced sample analogs. empirical moment conditions given :\\[\n\\bar{g}(\\beta) = \\frac{1}{N} \\sum_{=1}^{N} Z_i' (Y_i - X_i \\beta) = \\frac{1}{N} Z' (Y - X\\beta).\n\\]GMM estimates \\(\\beta\\) minimizing quadratic function sample moments.","code":""},{"path":"sec-instrumental-variables.html","id":"iv-and-gmm-estimators","chapter":"34 Instrumental Variables","heading":"34.3.2.1 IV and GMM Estimators","text":"Exactly Identified Case (\\(l = k\\))number instruments equals number endogenous regressors (\\(l = k\\)), moment conditions uniquely determine \\(\\beta\\). case, IV estimator :\\[\n\\hat{\\beta}_{IV} = (Z'X)^{-1}Z'Y.\n\\]equivalent classical 2SLS estimator.Overidentified Case (\\(l > k\\))instruments endogenous variables (\\(l > k\\)), system moment conditions parameters. case, project \\(X\\) onto instrument space:\\[\n\\hat{X} = Z(Z'Z)^{-1} Z' X = P_Z X.\n\\]2SLS estimator given :\\[\n\\begin{aligned}\n\\hat{\\beta}_{2SLS} &= (\\hat{X}'X)^{-1} \\hat{X}' Y \\\\\n&= (X'P_Z X)^{-1} X' P_Z Y.\n\\end{aligned}\n\\]However, 2SLS optimally weight instruments \\(l > k\\). IV-GMM approach resolves issue.","code":""},{"path":"sec-instrumental-variables.html","id":"iv-gmm-estimation","chapter":"34 Instrumental Variables","heading":"34.3.2.2 IV-GMM Estimation","text":"GMM estimator obtained minimizing objective function:\\[\nJ (\\hat{\\beta}_{GMM} ) = N \\bar{g}(\\hat{\\beta}_{GMM})' W \\bar{g} (\\hat{\\beta}_{GMM}),\n\\]\\(W\\) \\(l \\times l\\) symmetric weighting matrix.IV-GMM estimator, solving first-order conditions yields:\\[\n\\hat{\\beta}_{GMM} = (X'ZWZ' X)^{-1} X'ZWZ'Y.\n\\]weighting matrix \\(W\\), consistent estimator. optimal choice \\(W\\) \\(S^{-1}\\), \\(S\\) covariance matrix moment conditions:\\[\nS = E[Z' u u' Z] = \\lim_{N \\\\infty} N^{-1} [Z' \\Omega Z].\n\\]feasible estimator replaces \\(S\\) sample estimate 2SLS residuals:\\[\n\\hat{\\beta}_{FEGMM} = (X'Z \\hat{S}^{-1} Z' X)^{-1} X'Z \\hat{S}^{-1} Z'Y.\n\\]\\(\\Omega\\) satisfies standard assumptions:Errors independently identically distributed.\\(S = \\sigma_u^2 I_N\\).optimal weighting matrix proportional identity matrix., IV-GMM estimator simplifies standard IV (2SLS) estimator.Comparison 2SLS IV-GMMKey Takeaways:Use IV-GMM whenever overidentification concern (.e., \\(l > k\\)).2SLS special case IV-GMM weighting matrix proportional identity matrix.IV-GMM improves efficiency optimally weighting moment conditions.","code":"\n# Standard approach\n\nlibrary(gmm)\ngmm_model <- gmm(y ~ x1, ~ x_inst_1 + x_inst_2, data = base)\nsummary(gmm_model)\n#> \n#> Call:\n#> gmm(g = y ~ x1, x = ~x_inst_1 + x_inst_2, data = base)\n#> \n#> \n#> Method:  twoStep \n#> \n#> Kernel:  Quadratic Spectral(with bw =  0.72368 )\n#> \n#> Coefficients:\n#>              Estimate     Std. Error   t value      Pr(>|t|)   \n#> (Intercept)   1.4385e+01   1.8960e+00   7.5871e+00   3.2715e-14\n#> x1           -2.7506e+00   6.2101e-01  -4.4292e+00   9.4584e-06\n#> \n#> J-Test: degrees of freedom is 1 \n#>                 J-test     P-value  \n#> Test E(g)=0:    7.9455329  0.0048206\n#> \n#> Initial values of the coefficients\n#> (Intercept)          x1 \n#>   16.117875   -3.360622"},{"path":"sec-instrumental-variables.html","id":"overidentification-test-hansens-j-statistic","chapter":"34 Instrumental Variables","heading":"34.3.2.3 Overidentification Test: Hansen’s \\(J\\)-Statistic","text":"key advantage IV-GMM allows testing instrument validity Hansen \\(J\\)-test (also known GMM distance test Hayashi’s C-statistic). test statistic :\\[\nJ = N \\bar{g}(\\hat{\\beta}_{GMM})' \\hat{S}^{-1} \\bar{g} (\\hat{\\beta}_{GMM}),\n\\]follows \\(\\chi^2\\) distribution degrees freedom equal number overidentifying restrictions (\\(l - k\\)). significant \\(J\\)-statistic suggests instruments may valid.","code":""},{"path":"sec-instrumental-variables.html","id":"cluster-robust-standard-errors","chapter":"34 Instrumental Variables","heading":"34.3.2.4 Cluster-Robust Standard Errors","text":"empirical applications, errors often exhibit heteroskedasticity intra-group correlation (clustering), violating assumption independently identically distributed errors. Standard IV-GMM estimators remain consistent may efficient clustering ignored.address , adjust GMM weighting matrix incorporating cluster-robust variance estimation. Specifically, covariance matrix moment conditions \\(S\\) estimated :\\[\n\\hat{S} = \\frac{1}{N} \\sum_{c=1}^{C} \\left( \\sum_{\\c} Z_i' u_i \\right) \\left( \\sum_{\\c} Z_i' u_i \\right)',\n\\]:\\(C\\) number clusters,\\(C\\) number clusters,\\(\\c\\) represents observations belonging cluster \\(c\\),\\(\\c\\) represents observations belonging cluster \\(c\\),\\(u_i\\) residual observation \\(\\),\\(u_i\\) residual observation \\(\\),\\(Z_i\\) vector instruments.\\(Z_i\\) vector instruments.Using robust weighting matrix, compute clustered GMM estimator remains consistent improves inference clustering present.","code":"\n# Load required packages\nlibrary(gmm)\nlibrary(dplyr)\nlibrary(MASS)  # For generalized inverse if needed\n\n# General IV-GMM function with clustering\ngmmcl <- function(formula, instruments, data, cluster_var, lambda = 1e-6) {\n  \n  # Ensure cluster_var exists in data\n  if (!(cluster_var %in% colnames(data))) {\n    stop(\"Error: Cluster variable not found in data.\")\n  }\n  \n  # Step 1: Initial GMM estimation (identity weighting matrix)\n  initial_gmm <- gmm(formula, instruments, data = data, vcov = \"TrueFixed\", \n                      weightsMatrix = diag(ncol(model.matrix(instruments, data))))\n  \n  # Extract residuals\n  u_hat <- residuals(initial_gmm)\n  \n  # Matrix of instruments\n  Z <- model.matrix(instruments, data)\n  \n  # Ensure clusters are treated as a factor\n  data[[cluster_var]] <- as.factor(data[[cluster_var]])\n  \n  # Compute clustered weighting matrix\n  cluster_groups <- split(seq_along(u_hat), data[[cluster_var]])\n  \n  # Remove empty clusters (if any)\n  cluster_groups <- cluster_groups[lengths(cluster_groups) > 0]\n  \n  # Initialize cluster-based covariance matrix\n  S_cluster <- matrix(0, ncol(Z), ncol(Z))  # Zero matrix\n  \n  # Compute clustered weight matrix\n  for (indices in cluster_groups) {\n    if (length(indices) > 0) {  # Ensure valid clusters\n      u_cluster <- matrix(u_hat[indices], ncol = 1)  # Convert to column matrix\n      Z_cluster <- Z[indices, , drop = FALSE]        # Keep matrix form\n      S_cluster <- S_cluster + t(Z_cluster) %*% (u_cluster %*% t(u_cluster)) %*% Z_cluster\n    }\n  }\n  \n  # Normalize by sample size\n  S_cluster <- S_cluster / nrow(data)\n  \n  # Ensure S_cluster is invertible\n  S_cluster <- S_cluster + lambda * diag(ncol(S_cluster))  # Regularization\n\n  # Compute inverse or generalized inverse if needed\n  if (qr(S_cluster)$rank < ncol(S_cluster)) {\n    S_cluster_inv <- ginv(S_cluster)  # Use generalized inverse (MASS package)\n  } else {\n    S_cluster_inv <- solve(S_cluster)\n  }\n\n  # Step 2: GMM estimation using clustered weighting matrix\n  final_gmm <- gmm(formula, instruments, data = data, vcov = \"TrueFixed\", \n                    weightsMatrix = S_cluster_inv)\n  \n  return(final_gmm)\n}\n\n# Example: Simulated Data for IV-GMM with Clustering\nset.seed(123)\nn <- 200   # Total observations\nC <- 50    # Number of clusters\ndata <- data.frame(\n  cluster = rep(1:C, each = n / C),  # Cluster variable\n  z1 = rnorm(n),\n  z2 = rnorm(n),\n  x1 = rnorm(n),\n  y1 = rnorm(n)\n)\ndata$x1 <- data$z1 + data$z2 + rnorm(n)  # Endogenous regressor\ndata$y1 <- data$x1 + rnorm(n)            # Outcome variable\n\n# Run standard IV-GMM (without clustering)\ngmm_results_standard <- gmm(y1 ~ x1, ~ z1 + z2, data = data)\n\n# Run IV-GMM with clustering\ngmm_results_clustered <- gmmcl(y1 ~ x1, ~ z1 + z2, data = data, cluster_var = \"cluster\")\n\n# Display results for comparison\nsummary(gmm_results_standard)\n#> \n#> Call:\n#> gmm(g = y1 ~ x1, x = ~z1 + z2, data = data)\n#> \n#> \n#> Method:  twoStep \n#> \n#> Kernel:  Quadratic Spectral(with bw =  1.09893 )\n#> \n#> Coefficients:\n#>              Estimate     Std. Error   t value      Pr(>|t|)   \n#> (Intercept)   4.4919e-02   6.5870e-02   6.8193e-01   4.9528e-01\n#> x1            9.8409e-01   4.4215e-02   2.2257e+01  9.6467e-110\n#> \n#> J-Test: degrees of freedom is 1 \n#>                 J-test  P-value\n#> Test E(g)=0:    1.6171  0.2035 \n#> \n#> Initial values of the coefficients\n#> (Intercept)          x1 \n#>  0.05138658  0.98580796\nsummary(gmm_results_clustered)\n#> \n#> Call:\n#> gmm(g = formula, x = instruments, vcov = \"TrueFixed\", weightsMatrix = S_cluster_inv, \n#>     data = data)\n#> \n#> \n#> Method:  One step GMM with fixed W \n#> \n#> Kernel:  Quadratic Spectral\n#> \n#> Coefficients:\n#>              Estimate    Std. Error  t value     Pr(>|t|)  \n#> (Intercept)  4.9082e-02  7.0878e-05  6.9249e+02  0.0000e+00\n#> x1           9.8238e-01  5.2798e-05  1.8606e+04  0.0000e+00\n#> \n#> J-Test: degrees of freedom is 1 \n#>                 J-test   P-value\n#> Test E(g)=0:    1247099        0"},{"path":"sec-instrumental-variables.html","id":"limited-information-maximum-likelihood","chapter":"34 Instrumental Variables","heading":"34.3.3 Limited Information Maximum Likelihood","text":"LIML alternative 2SLS performs better instruments weak.solves: \\[\n\\min_{\\lambda} \\left| \\begin{bmatrix} Y - X\\beta \\\\ \\lambda (D - X\\gamma) \\end{bmatrix} \\right|\n\\] \\(\\lambda\\) eigenvalue.","code":""},{"path":"sec-instrumental-variables.html","id":"jackknife-iv","chapter":"34 Instrumental Variables","heading":"34.3.4 Jackknife IV","text":"JIVE reduces small-sample bias leaving observation estimating first-stage fitted values:\\[\n\\begin{aligned}\n\\hat{X}_i^{(-)} &= Z_i (Z_{-}'Z_{-})^{-1} Z_{-}'X_{-}. \\\\\n\\hat{\\beta}_{JIVE} &= (X^{(-)'}X^{(-)})^{-1}X^{(-)'} Y\n\\end{aligned}\n\\]","code":"\nlibrary(AER)\njive_model = ivreg(y ~ x_endo_1 | x_inst_1, data = base, method = \"jive\")\nsummary(jive_model)\n#> \n#> Call:\n#> ivreg(formula = y ~ x_endo_1 | x_inst_1, data = base, method = \"jive\")\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.2390 -0.3022 -0.0206  0.2772  1.0039 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  4.34586    0.08096   53.68   <2e-16 ***\n#> x_endo_1     0.39848    0.01964   20.29   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4075 on 148 degrees of freedom\n#> Multiple R-Squared: 0.7595,  Adjusted R-squared: 0.7578 \n#> Wald test: 411.6 on 1 and 148 DF,  p-value: < 2.2e-16"},{"path":"sec-instrumental-variables.html","id":"sec-control-function-approach","chapter":"34 Instrumental Variables","heading":"34.3.5 Control Function Approach","text":"Control Function (CF) approach, also known two-stage residual inclusion (2SRI), method used address endogeneity regression models. approach particularly suited models nonadditive errors, discrete choice models cases endogenous variable outcome binary.control function approach particularly useful :Binary outcome binary endogenous variable models:\nrare events, second stage typically uses logistic model (E. Tchetgen Tchetgen 2014).\nnon-rare events, risk ratio regression often appropriate.\nrare events, second stage typically uses logistic model (E. Tchetgen Tchetgen 2014).non-rare events, risk ratio regression often appropriate.Marketing applications:\nUsed consumer choice models account endogeneity demand estimation (Petrin Train 2010).\nUsed consumer choice models account endogeneity demand estimation (Petrin Train 2010).general model setup :\\[\nY = g(X) + U  \n\\]\\[\nX = \\pi(Z) + V  \n\\]key assumptions:Conditional mean independence:\\[E(U |Z,V) = E(U|V)\\]\nimplies control \\(V\\), instrumental variable \\(Z\\) directly affect \\(U\\).Conditional mean independence:\\[E(U |Z,V) = E(U|V)\\]\nimplies control \\(V\\), instrumental variable \\(Z\\) directly affect \\(U\\).Instrument relevance:\\[E(V|Z) = 0\\]\nensures \\(Z\\) valid instrument \\(X\\).Instrument relevance:\\[E(V|Z) = 0\\]\nensures \\(Z\\) valid instrument \\(X\\).control function approach, expectation \\(Y\\) conditional \\((Z,V)\\) can rewritten :\\[\nE(Y|Z,V) = g(X) + E(U|Z,V) = g(X) + E(U|V) = g(X) + h(V).\n\\], \\(h(V)\\) control function captures endogeneity first-stage residuals.","code":""},{"path":"sec-instrumental-variables.html","id":"implementation","chapter":"34 Instrumental Variables","heading":"34.3.5.1 Implementation","text":"Rather replacing endogenous variable \\(X_i\\) predicted value \\(\\hat{X}_i\\), CF approach explicitly incorporates residuals first-stage regression:Stage 1: Estimate First-Stage ResidualsEstimate endogenous variable using instrumental variables:\\[\nX_i = Z_i \\pi + v_i.\n\\]Obtain residuals:\\[\n\\hat{v}_i = X_i - Z_i \\hat{\\pi}.\n\\]Stage 2: Include Residuals Outcome EquationRegress outcome variable \\(X_i\\) first-stage residuals:\\[\nY_i = X_i \\beta + \\gamma \\hat{v}_i + \\varepsilon_i.\n\\]endogeneity present, \\(\\gamma \\neq 0\\); otherwise, endogenous regressor \\(X\\) exogenous.","code":""},{"path":"sec-instrumental-variables.html","id":"comparison-to-two-stage-least-squares","chapter":"34 Instrumental Variables","heading":"34.3.5.2 Comparison to Two-Stage Least Squares","text":"control function method differs 2SLS depending whether model linear nonlinear:Linear Endogenous Variables:\n\\(X\\) \\(Y\\) continuous, CF approach equivalent 2SLS.\n\\(X\\) \\(Y\\) continuous, CF approach equivalent 2SLS.Nonlinear Endogenous Variables:\n\\(X\\) nonlinear (e.g., binary treatment), CF differs 2SLS often performs better.\n\\(X\\) nonlinear (e.g., binary treatment), CF differs 2SLS often performs better.Nonlinear Parameters:\nmodels \\(g(X)\\) nonlinear (e.g., logit/probit models), CF typically superior 2SLS explicitly models endogeneity via control function \\(h(V)\\).\nmodels \\(g(X)\\) nonlinear (e.g., logit/probit models), CF typically superior 2SLS explicitly models endogeneity via control function \\(h(V)\\).Linear parameter linear endogenous variableNonlinear endogenous variableNonlinear parameters","code":"\nlibrary(fixest)\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Set the seed for reproducibility\nset.seed(123)\nn = 1000\n# Generate the exogenous variable from a normal distribution\nexogenous <- rnorm(n, mean = 5, sd = 1)\n\n# Generate the omitted variable as a function of the exogenous variable\nomitted <- rnorm(n, mean = 2, sd = 1)\n\n# Generate the endogenous variable as a function of the omitted variable and the exogenous variable\nendogenous <- 5 * omitted + 2 * exogenous + rnorm(n, mean = 0, sd = 1)\n\n# nonlinear endogenous variable\nendogenous_nonlinear <- 5 * omitted^2 + 2 * exogenous + rnorm(100, mean = 0, sd = 1)\n\nunrelated <- rexp(n, rate = 1)\n\n# Generate the response variable as a function of the endogenous variable and the omitted variable\nresponse <- 4 +  3 * endogenous + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear <- 4 +  3 * endogenous_nonlinear + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear_para <- 4 +  3 * endogenous ^ 2 + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\n\n# Combine the variables into a data frame\nmy_data <-\n    data.frame(\n        exogenous,\n        omitted,\n        endogenous,\n        response,\n        unrelated,\n        response,\n        response_nonlinear,\n        response_nonlinear_para\n    )\n\n# View the first few rows of the data frame\n# head(my_data)\n\nwo_omitted <- feols(response ~ endogenous + sw0(unrelated), data = my_data)\nw_omitted  <- feols(response ~ endogenous + omitted + unrelated, data = my_data)\n\n\n# ivreg::ivreg(response ~ endogenous + unrelated | exogenous, data = my_data)\niv <- feols(response ~ 1 + sw0(unrelated) | endogenous ~ exogenous, data = my_data)\n\netable(\n    wo_omitted,\n    w_omitted,\n    iv, \n    digits = 2\n    # vcov = list(\"each\", \"iid\", \"hetero\")\n)\n#>                   wo_omitted.1   wo_omitted.2     w_omitted          iv.1\n#> Dependent Var.:       response       response      response      response\n#>                                                                          \n#> Constant        -3.8*** (0.30) -3.6*** (0.31) 3.9*** (0.16) 12.0*** (1.4)\n#> endogenous       4.0*** (0.01)  4.0*** (0.01) 3.0*** (0.01) 3.2*** (0.07)\n#> unrelated                       -0.14. (0.08)  -0.02 (0.03)              \n#> omitted                                       6.0*** (0.08)              \n#> _______________ ______________ ______________ _____________ _____________\n#> S.E. type                  IID            IID           IID           IID\n#> Observations             1,000          1,000         1,000         1,000\n#> R2                     0.98756        0.98760       0.99817       0.94976\n#> Adj. R2                0.98755        0.98757       0.99816       0.94971\n#> \n#>                          iv.2\n#> Dependent Var.:      response\n#>                              \n#> Constant        12.2*** (1.4)\n#> endogenous      3.2*** (0.07)\n#> unrelated       -0.28. (0.16)\n#> omitted                      \n#> _______________ _____________\n#> S.E. type                 IID\n#> Observations            1,000\n#> R2                    0.95010\n#> Adj. R2               0.95000\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# manual\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -68.126 -14.949   0.608  15.099  73.842 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     11.9910     5.7671   2.079   0.0379 *  \n#> new_endogenous   3.2097     0.2832  11.335   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 21.49 on 998 degrees of freedom\n#> Multiple R-squared:  0.1141, Adjusted R-squared:  0.1132 \n#> F-statistic: 128.5 on 1 and 998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response ~ endogenous + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response ~ endogenous + residual, data = new_data_cf)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.1039 -1.0065  0.0247  0.9480  4.2521 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 11.99102    0.39849   30.09   <2e-16 ***\n#> endogenous   3.20974    0.01957  164.05   <2e-16 ***\n#> residual     0.95036    0.02159   44.02   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.485 on 997 degrees of freedom\n#> Multiple R-squared:  0.9958, Adjusted R-squared:  0.9958 \n#> F-statistic: 1.175e+05 on 2 and 997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous_nonlinear ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous_nonlinear = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -101.26  -53.01  -13.50   39.33  376.16 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               11.7539    21.6478   0.543    0.587    \n#> new_endogenous_nonlinear   3.1253     0.5993   5.215 2.23e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 70.89 on 998 degrees of freedom\n#> Multiple R-squared:  0.02653,    Adjusted R-squared:  0.02555 \n#> F-statistic:  27.2 on 1 and 998 DF,  p-value: 2.234e-07\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ endogenous_nonlinear + residual, \n#>     data = new_data_cf)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -12.8559  -0.8337   0.4429   1.3432   4.3147 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          11.75395    0.67012  17.540  < 2e-16 ***\n#> endogenous_nonlinear  3.12525    0.01855 168.469  < 2e-16 ***\n#> residual              0.13577    0.01882   7.213 1.08e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.194 on 997 degrees of freedom\n#> Multiple R-squared:  0.9991, Adjusted R-squared:  0.9991 \n#> F-statistic: 5.344e+05 on 2 and 997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear_para ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1402.34  -462.21   -64.22   382.35  3090.62 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    -1137.875    173.811  -6.547  9.4e-11 ***\n#> new_endogenous   122.525      8.534  14.357  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 647.7 on 998 degrees of freedom\n#> Multiple R-squared:  0.1712, Adjusted R-squared:  0.1704 \n#> F-statistic: 206.1 on 1 and 998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear_para ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ endogenous_nonlinear + \n#>     residual, data = new_data_cf)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -904.77 -154.35  -20.41  143.24  953.04 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          492.2494    32.3530   15.21  < 2e-16 ***\n#> endogenous_nonlinear  23.5991     0.8741   27.00  < 2e-16 ***\n#> residual              30.5914     3.7397    8.18 8.58e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 245.9 on 997 degrees of freedom\n#> Multiple R-squared:  0.8806, Adjusted R-squared:  0.8804 \n#> F-statistic:  3676 on 2 and 997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))"},{"path":"sec-instrumental-variables.html","id":"fuller-and-bias-reduced-iv","chapter":"34 Instrumental Variables","heading":"34.3.6 Fuller and Bias-Reduced IV","text":"Fuller adjusts LIML bias reduction.","code":"\nfuller_model = ivreg(y ~ x_endo_1 | x_inst_1, data = base, method = \"fuller\", k = 1)\nsummary(fuller_model)\n#> \n#> Call:\n#> ivreg(formula = y ~ x_endo_1 | x_inst_1, data = base, method = \"fuller\", \n#>     k = 1)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.2390 -0.3022 -0.0206  0.2772  1.0039 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  4.34586    0.08096   53.68   <2e-16 ***\n#> x_endo_1     0.39848    0.01964   20.29   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4075 on 148 degrees of freedom\n#> Multiple R-Squared: 0.7595,  Adjusted R-squared: 0.7578 \n#> Wald test: 411.6 on 1 and 148 DF,  p-value: < 2.2e-16"},{"path":"sec-instrumental-variables.html","id":"asymptotic-properties-of-the-iv-estimator","chapter":"34 Instrumental Variables","heading":"34.4 Asymptotic Properties of the IV Estimator","text":"IV estimation provides consistent asymptotically normal estimates structural parameters specific set assumptions. Understanding asymptotic properties IV estimator requires clarity identification conditions large-sample behavior estimator.Consider linear structural model:\\[\nY = X \\beta + u\n\\]:\\(Y\\) dependent variable (\\(n \\times 1\\))\\(Y\\) dependent variable (\\(n \\times 1\\))\\(X\\) matrix endogenous regressors (\\(n \\times k\\))\\(X\\) matrix endogenous regressors (\\(n \\times k\\))\\(u\\) error term\\(u\\) error term\\(\\beta\\) parameter vector interest (\\(k \\times 1\\))\\(\\beta\\) parameter vector interest (\\(k \\times 1\\))Suppose matrix instruments \\(Z\\) (\\(n \\times m\\)), \\(m \\ge k\\).IV estimator \\(\\beta\\) :\\[\n\\hat{\\beta}_{IV} = (Z'X)^{-1} Z'Y\n\\]Alternatively, using 2SLS, equivalent :\\[\n\\hat{\\beta}_{2SLS} = (X'P_ZX)^{-1} X'P_ZY\n\\]:\\(P_Z = Z (Z'Z)^{-1} Z'\\) projection matrix onto column space \\(Z\\).","code":""},{"path":"sec-instrumental-variables.html","id":"sec-consistency-iv","chapter":"34 Instrumental Variables","heading":"34.4.1 Consistency","text":"\\(\\hat{\\beta}_{IV}\\) consistent, following conditions must hold \\(n \\\\infty\\):Instrument Exogeneity\\[\n\\mathbb{E}[Z'u] = 0\n\\]Instruments must uncorrelated structural error term.Instruments must uncorrelated structural error term.Guarantees instrument validity.Guarantees instrument validity.Instrument Relevance\\[\n\\mathrm{rank}(\\mathbb{E}[Z'X]) = k\n\\]Instruments must correlated endogenous regressors.Instruments must correlated endogenous regressors.Ensures identification \\(\\beta\\).Ensures identification \\(\\beta\\).fails, model underidentified, \\(\\hat{\\beta}_{IV}\\) converge true \\(\\beta\\).fails, model underidentified, \\(\\hat{\\beta}_{IV}\\) converge true \\(\\beta\\).Random Sampling (IID Observations)\\(\\{(Y_i, X_i, Z_i)\\}_{=1}^n\\) independent identically distributed (..d.).general settings, stationarity mixing conditions can relax .Finite Moments\\(\\mathbb{E}[||Z||^2] < \\infty\\) \\(\\mathbb{E}[||u||^2] < \\infty\\)Ensures Law Large Numbers applies sample moments.conditions satisfied: \\[\n\\hat{\\beta}_{IV} \\overset{p}{\\} \\beta\n\\] means IV estimator consistent.","code":""},{"path":"sec-instrumental-variables.html","id":"asymptotic-normality-1","chapter":"34 Instrumental Variables","heading":"34.4.2 Asymptotic Normality","text":"addition consistency conditions, require:Homoskedasticity (Optional Simplifying)\\[\n\\mathbb{E}[u u' | Z] = \\sigma^2 \n\\]Simplifies variance estimation.Simplifies variance estimation.violated, heteroskedasticity-robust variance estimators must used.violated, heteroskedasticity-robust variance estimators must used.Central Limit Theorem ConditionsSample moments must satisfy CLT: \\[\n\\sqrt{n} \\left( \\frac{1}{n} \\sum_{=1}^n Z_i u_i \\right) \\overset{d}{\\} N(0, \\Omega)\n\\] \\(\\Omega = \\mathbb{E}[Z_i Z_i' u_i^2]\\).conditions: \\[\n\\sqrt{n}(\\hat{\\beta}_{IV} - \\beta) \\overset{d}{\\} N(0, V)\n\\]asymptotic variance-covariance matrix \\(V\\) : \\[\nV = (Q_{ZX})^{-1} Q_{Zuu} (Q_{ZX}')^{-1}\n\\] :\\(Q_{ZX} = \\mathbb{E}[Z_i X_i']\\)\\(Q_{ZX} = \\mathbb{E}[Z_i X_i']\\)\\(Q_{Zuu} = \\mathbb{E}[Z_i Z_i' u_i^2]\\)\\(Q_{Zuu} = \\mathbb{E}[Z_i Z_i' u_i^2]\\)","code":""},{"path":"sec-instrumental-variables.html","id":"asymptotic-efficiency-1","chapter":"34 Instrumental Variables","heading":"34.4.3 Asymptotic Efficiency","text":"Optimal Instrument ChoiceAmong IV estimators, 2SLS efficient instrument matrix \\(Z\\) contains relevant information.Generalized Method Moments (GMM) can deliver efficiency gains presence heteroskedasticity, optimally weighting moment conditions.GMM Estimator\\[\n\\hat{\\beta}_{GMM} = \\arg \\min_{\\beta} \\left( \\frac{1}{n} \\sum_{=1}^n Z_i (Y_i - X_i' \\beta) \\right)' W \\left( \\frac{1}{n} \\sum_{=1}^n Z_i (Y_i - X_i' \\beta) \\right)\n\\]\\(W\\) optimal weighting matrix, typically:\\[\nW = \\Omega^{-1}\n\\]ResultIf \\(Z\\) overidentified (\\(m > k\\)), GMM can efficient 2SLS.instruments exactly identified (\\(m = k\\)), IV, 2SLS, GMM coincide.Summary Table Conditions","code":""},{"path":"sec-instrumental-variables.html","id":"sec-inference-iv","chapter":"34 Instrumental Variables","heading":"34.5 Inference","text":"Inference IV models, particularly instruments weak, presents serious challenges can undermine standard testing confidence interval procedures. section, explore core issues IV inference weak instruments, discuss standard alternative approaches, outline practical guidelines applied research.Consider just-identified linear IV model:\\[\nY = \\beta X + u\n\\]:\\(X\\) endogenous: \\(\\text{Cov}(X, u) \\neq 0\\).\\(X\\) endogenous: \\(\\text{Cov}(X, u) \\neq 0\\).\\(Z\\) instrumental variable satisfying:\nRelevance: \\(\\text{Cov}(Z, X) \\neq 0\\).\nExogeneity: \\(\\text{Cov}(Z, u) = 0\\).\n\\(Z\\) instrumental variable satisfying:Relevance: \\(\\text{Cov}(Z, X) \\neq 0\\).Relevance: \\(\\text{Cov}(Z, X) \\neq 0\\).Exogeneity: \\(\\text{Cov}(Z, u) = 0\\).Exogeneity: \\(\\text{Cov}(Z, u) = 0\\).IV estimator \\(\\beta\\) consistent assumptions.commonly used approach inference t-ratio method, constructing 95% confidence interval :\\[\n\\hat{\\beta} \\pm 1.96 \\sqrt{\\hat{V}_N(\\hat{\\beta})}\n\\]However, approach invalid instruments weak. Specifically:t-ratio follow standard normal distribution weak instruments.t-ratio follow standard normal distribution weak instruments.Confidence intervals based method can severely -cover true parameter.Confidence intervals based method can severely -cover true parameter.Hypothesis tests can -reject, even large samples.Hypothesis tests can -reject, even large samples.problem first systematically identified Staiger Stock (1997) Dufour (1997). Weak instruments create distortions finite-sample distribution \\(\\hat{\\beta}\\).Common Practices MisinterpretationsOverreliance t-Ratio TestsPopular problematic instruments weak.Known -reject null hypotheses -cover confidence intervals.Documented extensively Nelson Startz (1990), Bound, Jaeger, Baker (1995), Dufour (1997), Lee et al. (2022).Weak Instrument DiagnosticsFirst-Stage F-Statistic:\nRule thumb: \\(F > 10\\) often used simplistic misleading.\naccurate critical values provided Stock Yogo (2005).\n95% coverage, \\(F > 16.38\\) often cited (Staiger Stock 1997).\nRule thumb: \\(F > 10\\) often used simplistic misleading.accurate critical values provided Stock Yogo (2005).95% coverage, \\(F > 16.38\\) often cited (Staiger Stock 1997).Misinterpretations PitfallsMistakenly interpreting \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{SE}\\) 95% CI instrument weak, Staiger Stock (1997) show \\(F > 16.38\\), nominal 95% CI may offer 85% coverage.Pretesting weak instruments can exacerbate inference problems (. R. Hall, Rudebusch, Wilcox 1996).Selective model specification based weak instrument diagnostics may introduce additional distortions (. Andrews, Stock, Sun 2019).","code":""},{"path":"sec-instrumental-variables.html","id":"sec-weak-instruments-problem","chapter":"34 Instrumental Variables","heading":"34.5.1 Weak Instruments Problem","text":"alternative statistic accounts weak instrument issues adjusting standard Anderson-Rubin (AR) test:\\[\n\\hat{t}^2 = \\hat{t}^2_{AR} \\times \\frac{1}{1 - \\hat{\\rho} \\frac{\\hat{t}_{AR}}{\\hat{f}} + \\frac{\\hat{t}^2_{AR}}{\\hat{f}^2}}\n\\]:\\(\\hat{t}^2_{AR} \\sim \\chi^2(1)\\) null, even weak instruments (T. W. Anderson Rubin 1949).\\(\\hat{t}^2_{AR} \\sim \\chi^2(1)\\) null, even weak instruments (T. W. Anderson Rubin 1949).\\(\\hat{t}_{AR} = \\dfrac{\\hat{\\pi}(\\hat{\\beta} - \\beta_0)}{\\sqrt{\\hat{V}_N (\\hat{\\pi} (\\hat{\\beta} - \\beta_0))}} \\sim N(0,1)\\).\\(\\hat{t}_{AR} = \\dfrac{\\hat{\\pi}(\\hat{\\beta} - \\beta_0)}{\\sqrt{\\hat{V}_N (\\hat{\\pi} (\\hat{\\beta} - \\beta_0))}} \\sim N(0,1)\\).\\(\\hat{f} = \\dfrac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\) measures instrument strength (first-stage F-stat).\\(\\hat{f} = \\dfrac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\) measures instrument strength (first-stage F-stat).\\(\\hat{\\pi}\\) coefficient first-stage regression \\(X\\) \\(Z\\).\\(\\hat{\\pi}\\) coefficient first-stage regression \\(X\\) \\(Z\\).\\(\\hat{\\rho} = \\text{Cov}(Zv, Zu)\\) captures correlation first-stage residuals \\(u\\).\\(\\hat{\\rho} = \\text{Cov}(Zv, Zu)\\) captures correlation first-stage residuals \\(u\\).ImplicationsEven large samples, \\(\\hat{t}^2 \\neq \\hat{t}^2_{AR}\\) adjustment term converge zero unless instruments strong \\(\\rho = 0\\).distribution \\(\\hat{t}\\) match standard normal follows complex distribution described Staiger Stock (1997) Stock Yogo (2005).divergence \\(\\hat{t}^2\\) \\(\\hat{t}^2_{AR}\\) depends :Instrument Strength (\\(\\pi\\)): Higher correlation \\(Z\\) \\(X\\) mitigates problem.First-Stage F-statistic (\\(E(F)\\)): weak first-stage regression increases bias distortion.Endogeneity Level (\\(|\\rho|\\)): Greater correlation \\(X\\) \\(u\\) exacerbates inference errors.","code":""},{"path":"sec-instrumental-variables.html","id":"solutions-and-approaches-for-valid-inference","chapter":"34 Instrumental Variables","heading":"34.5.2 Solutions and Approaches for Valid Inference","text":"Assume Problem Away (Risky Assumptions)\nHigh First-Stage F-statistic:\nRequire \\(E(F) > 142.6\\) near-validity (Lee et al. 2022).\nfirst-stage \\(F\\) observable, threshold high often impractical.\n\nLow Endogeneity:\nAssume \\(|\\rho| < 0.565\\) Lee et al. (2022). words, assume endogeneity less moderat level.\nundermines motivation IV first place, exists precisely suspected endogeneity.\n\nHigh First-Stage F-statistic:\nRequire \\(E(F) > 142.6\\) near-validity (Lee et al. 2022).\nfirst-stage \\(F\\) observable, threshold high often impractical.\nRequire \\(E(F) > 142.6\\) near-validity (Lee et al. 2022).first-stage \\(F\\) observable, threshold high often impractical.Low Endogeneity:\nAssume \\(|\\rho| < 0.565\\) Lee et al. (2022). words, assume endogeneity less moderat level.\nundermines motivation IV first place, exists precisely suspected endogeneity.\nAssume \\(|\\rho| < 0.565\\) Lee et al. (2022). words, assume endogeneity less moderat level.undermines motivation IV first place, exists precisely suspected endogeneity.Confront Problem Directly (Robust Methods)\nAnderson-Rubin (AR) Test (T. W. Anderson Rubin 1949):\nValid weak instruments.\nTests whether \\(Z\\) explains variation \\(Y - \\beta_0 X\\).\n\ntF Procedure (Lee et al. 2022):\nCombines t-statistics F-statistics unified testing framework.\nOffers valid inference presence weak instruments.\n\nAndrews-Kolesár (AK) Procedure (J. Angrist Kolesár 2023):\nProvides uniformly valid confidence intervals \\(\\beta\\).\nAllows weak instruments arbitrary heteroskedasticity.\nEspecially useful overidentified settings.\n\nAnderson-Rubin (AR) Test (T. W. Anderson Rubin 1949):\nValid weak instruments.\nTests whether \\(Z\\) explains variation \\(Y - \\beta_0 X\\).\nValid weak instruments.Tests whether \\(Z\\) explains variation \\(Y - \\beta_0 X\\).tF Procedure (Lee et al. 2022):\nCombines t-statistics F-statistics unified testing framework.\nOffers valid inference presence weak instruments.\nCombines t-statistics F-statistics unified testing framework.Offers valid inference presence weak instruments.Andrews-Kolesár (AK) Procedure (J. Angrist Kolesár 2023):\nProvides uniformly valid confidence intervals \\(\\beta\\).\nAllows weak instruments arbitrary heteroskedasticity.\nEspecially useful overidentified settings.\nProvides uniformly valid confidence intervals \\(\\beta\\).Allows weak instruments arbitrary heteroskedasticity.Especially useful overidentified settings.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-anderson-rubin-approach","chapter":"34 Instrumental Variables","heading":"34.5.3 Anderson-Rubin Approach","text":"Anderson-Rubin (AR) test, originally proposed T. W. Anderson Rubin (1949), remains one robust inferential tools context instrumental variable estimation, particularly instruments weak endogenous regressors exhibit complex error structures.AR test directly evaluates joint null hypothesis :\\[\nH_0: \\beta = \\beta_0\n\\]testing whether instruments explain variation residuals \\(Y - \\beta_0 X\\). null, model becomes:\\[\nY - \\beta_0 X = u\n\\]Given \\(\\text{Cov}(Z, u) = 0\\) (IV exogeneity assumption), test regresses \\((Y - \\beta_0 X)\\) \\(Z\\). test statistic constructed :\\[\nAR(\\beta_0) = \\frac{(Y - \\beta_0 X)' P_Z (Y - \\beta_0 X)}{\\hat{\\sigma}^2}\n\\]\\(P_Z\\) projection matrix onto column space \\(Z\\): \\(P_Z = Z (Z'Z)^{-1} Z'\\).\\(\\hat{\\sigma}^2\\) estimate error variance (homoskedasticity).\\(H_0\\), statistic follows chi-squared distribution:\\[\nAR(\\beta_0) \\sim \\chi^2(q)\n\\]\\(q\\) number instruments (1 just-identified model).Key Properties AR TestRobust Weak Instruments:\nAR test rely strength instruments.\ndistribution null hypothesis remains valid even instruments weak (Staiger Stock 1997).\nAR test rely strength instruments.distribution null hypothesis remains valid even instruments weak (Staiger Stock 1997).Robust Non-Normality Homoskedastic Errors:\nMaintains correct Type error rates even non-normal errors (Staiger Stock 1997).\nOptimality properties homoskedastic errors established D. W. Andrews, Moreira, Stock (2006) M. J. Moreira (2009).\nMaintains correct Type error rates even non-normal errors (Staiger Stock 1997).Optimality properties homoskedastic errors established D. W. Andrews, Moreira, Stock (2006) M. J. Moreira (2009).Robust Heteroskedasticity, Clustering, Autocorrelation:\nAR test generalized account heteroskedasticity, clustered errors, autocorrelation (Stock Wright 2000; H. Moreira Moreira 2019).\nValid inference possible combined heteroskedasticity-robust variance estimators cluster-robust techniques.\nAR test generalized account heteroskedasticity, clustered errors, autocorrelation (Stock Wright 2000; H. Moreira Moreira 2019).Valid inference possible combined heteroskedasticity-robust variance estimators cluster-robust techniques.AR test relatively simple implement available econometric software. ’s intuitive step--step breakdown:Specify null hypothesis value \\(\\beta_0\\).Compute residual \\(u = Y - \\beta_0 X\\).Regress \\(u\\) \\(Z\\) obtain \\(R^2\\) regression.Compute test statistic:\\[\nAR(\\beta_0) = \\frac{R^2 \\cdot n}{q}\n\\](just-identified model single instrument, \\(q=1\\).)Compare \\(AR(\\beta_0)\\) \\(\\chi^2(q)\\) distribution determine significance.","code":"\nlibrary(ivDiag)\n\n# AR test (robust to weak instruments)\n# example by the package's authors\nivDiag::AR_test(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    # treatment\n    D = \"lm_pob_mesa\",\n    # instruments\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    CI = FALSE\n)\n#> $Fstat\n#>         F       df1       df2         p \n#>   48.4768    1.0000 4350.0000    0.0000\n\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$AR\n#> $Fstat\n#>         F       df1       df2         p \n#>   48.4768    1.0000 4350.0000    0.0000 \n#> \n#> $ci.print\n#> [1] \"[-1.2626, -0.7073]\"\n#> \n#> $ci\n#> [1] -1.2626 -0.7073\n#> \n#> $bounded\n#> [1] TRUE\nivDiag::plot_coef(g)"},{"path":"sec-instrumental-variables.html","id":"sec-tf-procedure","chapter":"34 Instrumental Variables","heading":"34.5.4 tF Procedure","text":"Lee et al. (2022) introduce tF procedure, inference method specifically designed just-identified IV models (single endogenous regressor single instrument). addresses shortcomings traditional 2SLS \\(t\\)-tests weak instruments offers solution conceptually familiar researchers trained standard econometric practices.Unlike Anderson-Rubin test, inverts hypothesis tests form confidence sets, tF procedure adjusts standard \\(t\\)-statistics standard errors directly, making intuitive extension traditional hypothesis testing.tF procedure widely applicable settings just-identified IV models arise, including:Randomized controlled trials imperfect compliance\n(e.g., Local Average Treatment Effects G. W. Imbens Angrist (1994)).Randomized controlled trials imperfect compliance\n(e.g., Local Average Treatment Effects G. W. Imbens Angrist (1994)).Fuzzy Regression Discontinuity Designs\n(e.g., Lee Lemieux (2010)).Fuzzy Regression Discontinuity Designs\n(e.g., Lee Lemieux (2010)).Fuzzy Regression Kink Designs\n(e.g., (Card et al. 2015)).Fuzzy Regression Kink Designs\n(e.g., (Card et al. 2015)).comparison AR approach tF procedure can found . Andrews, Stock, Sun (2019).\\(F > 3.84\\), AR test’s expected interval length infinite, whereas tF procedure guarantees finite intervals, making superior practical applications weak instruments.tF procedure adjusts conventional 2SLS \\(t\\)-ratio first-stage F-statistic strength. Instead relying pre-testing threshold (e.g., \\(F > 10\\)), tF approach provides smooth adjustment standard errors.Key Features:Adjusts 2SLS \\(t\\)-ratio based observed first-stage F-statistic.Applies different adjustment factors different significance levels (e.g., 95% 99%).Remains valid even instrument weak, offering finite confidence intervals even first-stage F-statistic low.Advantages tF ProcedureSmooth Adjustment First-Stage StrengthThe tF procedure smoothly adjusts inference based observed first-stage F-statistic, avoiding need arbitrary pre-testing thresholds (e.g., \\(F > 10\\)).tF procedure smoothly adjusts inference based observed first-stage F-statistic, avoiding need arbitrary pre-testing thresholds (e.g., \\(F > 10\\)).produces finite usable confidence intervals even first-stage F-statistic low:\n\\[\nF > 3.84\n\\]produces finite usable confidence intervals even first-stage F-statistic low:\\[\nF > 3.84\n\\]threshold aligns critical value 3.84 95% Anderson-Rubin confidence interval, crucial advantage:\nAR interval becomes unbounded (.e., infinite length) \\(F \\le 3.84\\).\ntF procedure, contrast, still provides finite confidence interval, making practical weak instrument cases.\nthreshold aligns critical value 3.84 95% Anderson-Rubin confidence interval, crucial advantage:AR interval becomes unbounded (.e., infinite length) \\(F \\le 3.84\\).tF procedure, contrast, still provides finite confidence interval, making practical weak instrument cases.Clear Interpretable Confidence LevelsThe tF procedure offers transparent confidence intervals :\nDirectly incorporate impact first-stage instrument strength critical values used inference.\nMirror distortion-free properties robust methods like Anderson-Rubin test, remain closer spirit conventional \\(t\\)-based inference.\ntF procedure offers transparent confidence intervals :Directly incorporate impact first-stage instrument strength critical values used inference.Directly incorporate impact first-stage instrument strength critical values used inference.Mirror distortion-free properties robust methods like Anderson-Rubin test, remain closer spirit conventional \\(t\\)-based inference.Mirror distortion-free properties robust methods like Anderson-Rubin test, remain closer spirit conventional \\(t\\)-based inference.Researchers can interpret tF-based 95% 99% confidence intervals using familiar econometric tools, without needing invert hypothesis tests construct confidence sets.Researchers can interpret tF-based 95% 99% confidence intervals using familiar econometric tools, without needing invert hypothesis tests construct confidence sets.Robustness Common Error StructuresThe tF procedure remains robust presence :\nHeteroskedasticity\nClustering\nAutocorrelation\ntF procedure remains robust presence :HeteroskedasticityClusteringAutocorrelationNo additional adjustments necessary beyond use robust variance estimator :\nfirst-stage regression\nsecond-stage IV regression\nadditional adjustments necessary beyond use robust variance estimator :first-stage regressionThe second-stage IV regressionAs long robust variance estimator applied consistently, tF adjustment maintains valid inference without imposing additional computational complexity.long robust variance estimator applied consistently, tF adjustment maintains valid inference without imposing additional computational complexity.Applicability Published ResearchOne powerful features tF procedure flexibility re-evaluating published studies:\nResearchers need reported first-stage F-statistic standard errors 2SLS estimates.\naccess original data required recalculate confidence intervals test statistical significance using tF adjustment.\nOne powerful features tF procedure flexibility re-evaluating published studies:Researchers need reported first-stage F-statistic standard errors 2SLS estimates.Researchers need reported first-stage F-statistic standard errors 2SLS estimates.access original data required recalculate confidence intervals test statistical significance using tF adjustment.access original data required recalculate confidence intervals test statistical significance using tF adjustment.makes tF procedure particularly valuable meta-analyses, replications, robustness checks published IV studies, :\nRaw data may unavailable, \nReplication costs high.\nmakes tF procedure particularly valuable meta-analyses, replications, robustness checks published IV studies, :Raw data may unavailable, orReplication costs high.Consider linear IV model additional covariates \\(W\\):\\[\nY = X \\beta + W \\gamma + u\n\\]\\[\nX = Z \\pi + W \\xi + \\nu\n\\]:\\(Y\\): Outcome variable.\\(Y\\): Outcome variable.\\(X\\): Endogenous regressor interest.\\(X\\): Endogenous regressor interest.\\(Z\\): Instrumental variable (single instrument case).\\(Z\\): Instrumental variable (single instrument case).\\(W\\): Vector exogenous controls, possibly including intercept.\\(W\\): Vector exogenous controls, possibly including intercept.\\(u\\), \\(\\nu\\): Error terms.\\(u\\), \\(\\nu\\): Error terms.Key Statistics:\\(t\\)-ratio IV estimator:\n\\[\n\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\n\\]\\(t\\)-ratio IV estimator:\\[\n\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\n\\]\\(t\\)-ratio first-stage coefficient:\n\\[\n\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\n\\]\\(t\\)-ratio first-stage coefficient:\\[\n\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\n\\]First-stage F-statistic:\n\\[\n\\hat{F} = \\hat{f}^2\n\\]First-stage F-statistic:\\[\n\\hat{F} = \\hat{f}^2\n\\]\\(\\hat{\\beta}\\): Instrumental variable estimator.\\(\\hat{V}_N (\\hat{\\beta})\\): Estimated variance \\(\\hat{\\beta}\\), possibly robust deal non-iid errors.\\(\\hat{t}\\): \\(t\\)-ratio null hypothesis.\\(\\hat{f}\\): \\(t\\)-ratio null hypothesis \\(\\pi=0\\).traditional asymptotics large samples, \\(t\\)-ratio statistic follows:\\[\n\\hat{t}^2 \\^d t^2\n\\]critical values:\\(\\pm 1.96\\) 5% significance test.\\(\\pm 1.96\\) 5% significance test.\\(\\pm 2.58\\) 1% significance test.\\(\\pm 2.58\\) 1% significance test.However, IV settings (particularly weak instruments):distribution \\(t\\)-statistic distorted (.e., \\(t\\)-distribution might normal), even large samples.distribution \\(t\\)-statistic distorted (.e., \\(t\\)-distribution might normal), even large samples.distortion arises strength instrument (\\(F\\)) degree endogeneity (\\(\\rho\\)) affect \\(t\\)-distribution.distortion arises strength instrument (\\(F\\)) degree endogeneity (\\(\\rho\\)) affect \\(t\\)-distribution.Stock Yogo (2005) provide formula quantify distortion (just-identified case) Wald test statistics using 2SLS.:\\[\nt^2 = f + t_{AR} + \\rho f t_{AR}\n\\]:\\(\\hat{f} \\^d f\\)\\(\\hat{f} \\^d f\\)\\(\\bar{f} = \\dfrac{\\pi}{\\sqrt{\\dfrac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\\(\\bar{f} = \\dfrac{\\pi}{\\sqrt{\\dfrac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\\(t_{AR}\\) asymptotically standard normal (\\(AR = t^2_{AR}\\))\\(t_{AR}\\) asymptotically standard normal (\\(AR = t^2_{AR}\\))\\(\\rho\\) measures correlation (degree endogeneity) \\(Zu\\) \\(Z\\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\)).\\(\\rho\\) measures correlation (degree endogeneity) \\(Zu\\) \\(Z\\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\)).Implications:low \\(\\rho\\) (\\(\\rho \\[0, 0.5]\\)), rejection probabilities can nominal levels.high \\(\\rho\\) (\\(\\rho = 0.8\\)), rejection rates can inflated, e.g., 13% rejection nominal 5% significance level.Reliance standard \\(t\\)-ratios leads incorrect test sizes invalid confidence intervals.tF procedure corrects distortions adjusting standard error 2SLS estimator based observed first-stage F-statistic.Steps:Estimate \\(\\hat{\\beta}\\) conventional SE 2SLS.Compute first-stage \\(\\hat{F}\\).Multiply conventional SE adjustment factor, depends \\(\\hat{F}\\) desired confidence level.Compute new \\(t\\)-ratios construct confidence intervals using standard critical values (e.g., \\(\\pm 1.96\\) 95% CI).Lee et al. (2022) refer adjusted standard errors “0.05 tF SE” (5% significance level) “0.01 tF SE” (1%).Lee et al. (2022) conducted review recent single-instrument studies American Economic Review.Key Findings:least 25% examined specifications:\ntF-adjusted confidence intervals 49% longer 5% level.\ntF-adjusted confidence intervals 136% longer 1% level.\ntF-adjusted confidence intervals 49% longer 5% level.tF-adjusted confidence intervals 136% longer 1% level.Even among specifications \\(F > 10\\) \\(t > 1.96\\):\nApproximately 25% became statistically insignificant 5% level applying tF adjustment.\nApproximately 25% became statistically insignificant 5% level applying tF adjustment.Takeaway:tF procedure can substantially alter inference conclusions.Published studies can re-evaluated tF method using reported first-stage F-statistics, without requiring access underlying microdata.","code":"\nlibrary(ivDiag)\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$tF\n#>         F        cF      Coef        SE         t    CI2.5%   CI97.5%   p-value \n#> 8598.3264    1.9600   -0.9835    0.1424   -6.9071   -1.2626   -0.7044    0.0000\n# example in fixest package\nlibrary(fixest)\nlibrary(tidyverse)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nest_iv\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\n\nres_est_iv <- est_iv$coeftable |> \n    rownames_to_column()\n\n\ncoef_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Estimate\"]\nse_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Std. Error\"]\nfstat_1st <- fitstat(est_iv, type = \"ivf1\")[[1]]$stat\n\n# To get the correct SE based on 1st-stage F-stat (This result is similar without adjustment since F is large)\n# the results are the new CIS and p.value\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = fstat_1st) |> \n    causalverse::nice_tab(5)\n#>          F   cF  Coef     SE       t CI2.5. CI97.5. p.value\n#> 1 903.1628 1.96 0.445 0.0221 20.1474 0.4017  0.4883       0\n\n# We can try to see a different 1st-stage F-stat and how it changes the results\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = 2) |> \n    causalverse::nice_tab(5)\n#>   F    cF  Coef     SE       t CI2.5. CI97.5. p.value\n#> 1 2 18.66 0.445 0.0221 20.1474 0.0329  0.8571  0.0343"},{"path":"sec-instrumental-variables.html","id":"sec-ak-approach","chapter":"34 Instrumental Variables","heading":"34.5.5 AK Approach","text":"J. Angrist Kolesár (2024) offer reappraisal just-identified IV models, focusing finite-sample properties conventional inference cases single instrument used single endogenous variable. findings challenge pessimistic views weak instruments inference distortions microeconometric applications.Rather propose new estimator test, Angrist Kolesár provide framework rationale supporting validity traditional just-ID IV inference many practical settings. insights clarify conventional t-tests confidence intervals can trusted, offer practical guidance first-stage pretesting, bias reduction, endogeneity considerations.AK apply framework three canonical studies:J. D. Angrist Krueger (1991) - Education returnsJ. D. Angrist Evans (1998) - Family size female labor supplyJ. D. Angrist Lavy (1999) - Class size effectsFindings:Endogeneity (\\(\\rho\\)) studies moderate (typically \\(|\\rho| < 0.47\\)).Endogeneity (\\(\\rho\\)) studies moderate (typically \\(|\\rho| < 0.47\\)).Conventional t-tests confidence intervals work reasonably well.Conventional t-tests confidence intervals work reasonably well.many micro applications, theoretical bounds causal effects plausible OVB scenarios limit \\(\\rho\\), supporting validity conventional inference.many micro applications, theoretical bounds causal effects plausible OVB scenarios limit \\(\\rho\\), supporting validity conventional inference.Key Contributions AK ApproachReassessing Bias Coverage:\nAK demonstrate conventional IV estimates t-tests just-ID IV models often perform better theory might suggest—provided degree endogeneity (\\(\\rho\\)) moderate, first-stage F-statistic extremely weak.Reassessing Bias Coverage:\nAK demonstrate conventional IV estimates t-tests just-ID IV models often perform better theory might suggest—provided degree endogeneity (\\(\\rho\\)) moderate, first-stage F-statistic extremely weak.First-Stage Sign Screening:\npropose sign screening simple, costless strategy halve median bias IV estimators.\nScreening sign estimated first-stage coefficient (.e., using samples first-stage estimate correct sign) improves finite-sample performance just-ID IV estimates without degrading confidence interval coverage.\nFirst-Stage Sign Screening:propose sign screening simple, costless strategy halve median bias IV estimators.Screening sign estimated first-stage coefficient (.e., using samples first-stage estimate correct sign) improves finite-sample performance just-ID IV estimates without degrading confidence interval coverage.Bias-Minimizing Screening Rule:\nAK show setting first-stage t-statistic threshold \\(c = 0\\), .e., requiring correct sign first-stage estimate, minimizes median bias preserving conventional coverage properties.\nBias-Minimizing Screening Rule:AK show setting first-stage t-statistic threshold \\(c = 0\\), .e., requiring correct sign first-stage estimate, minimizes median bias preserving conventional coverage properties.Practical Implication:\nargue conventional just-ID IV inference, including t-tests confidence intervals, likely valid microeconometric applications, especially theory institutional knowledge suggests direction first-stage relationship.\nPractical Implication:argue conventional just-ID IV inference, including t-tests confidence intervals, likely valid microeconometric applications, especially theory institutional knowledge suggests direction first-stage relationship.","code":""},{"path":"sec-instrumental-variables.html","id":"model-setup-and-notation","chapter":"34 Instrumental Variables","heading":"34.5.5.1 Model Setup and Notation","text":"AK adopt reduced-form first-stage specification just-ID IV models:\\[\nY_i = Z_i \\delta + X_i' \\psi_1 + u_i \\\\\nD_i = Z_i \\pi + X_i' \\psi_2 + v_i\n\\]\\(Y_i\\): Outcome variable\\(D_i\\): Endogenous treatment variable\\(Z_i\\): Instrumental variable (single instrument)\\(X_i\\): Control variables\\(u_i, v_i\\): Error termsParameter Interest:\\[\n\\beta = \\frac{\\delta}{\\pi}\n\\]","code":""},{"path":"sec-instrumental-variables.html","id":"endogeneity-and-instrument-strength","chapter":"34 Instrumental Variables","heading":"34.5.5.2 Endogeneity and Instrument Strength","text":"AK characterize two key parameters governing finite-sample inference:Instrument Strength:\\[ E[F] = \\frac{\\pi^2}{\\sigma^2_{\\hat{\\pi}}} + 1 \\]\n(Expected value first-stage F-statistic.)Instrument Strength:\\[ E[F] = \\frac{\\pi^2}{\\sigma^2_{\\hat{\\pi}}} + 1 \\]\n(Expected value first-stage F-statistic.)Endogeneity:\\[ \\rho = \\text{cor}(\\hat{\\delta} - \\hat{\\pi} \\beta, \\hat{\\pi}) \\]\nMeasures degree correlation reduced-form first-stage residuals (\\(u\\) \\(v\\) homoskedasticity).Endogeneity:\\[ \\rho = \\text{cor}(\\hat{\\delta} - \\hat{\\pi} \\beta, \\hat{\\pi}) \\]\nMeasures degree correlation reduced-form first-stage residuals (\\(u\\) \\(v\\) homoskedasticity).Key Insight:\\(\\rho < 0.76\\), coverage conventional 95% confidence intervals distorted less 5%, regardless first-stage F-statistic.","code":""},{"path":"sec-instrumental-variables.html","id":"first-stage-sign-screening","chapter":"34 Instrumental Variables","heading":"34.5.5.3 First-Stage Sign Screening","text":"AK argue pre-screening based sign first-stage estimate (\\(\\hat{\\pi}\\)) offers bias reduction without compromising confidence interval coverage.Screening Rule:Screen \\(\\hat{\\pi} > 0\\)\n(\\(\\hat{\\pi} < 0\\) theoretical sign negative).Results:Halves median bias IV estimator.degradation confidence interval coverage.screening approach:Avoids pitfalls pre-testing based first-stage F-statistics (can exacerbate bias distort inference).Avoids pitfalls pre-testing based first-stage F-statistics (can exacerbate bias distort inference).Provides “free lunch”: bias reduction coverage cost.Provides “free lunch”: bias reduction coverage cost.","code":""},{"path":"sec-instrumental-variables.html","id":"rejection-rates-and-confidence-interval-coverage","chapter":"34 Instrumental Variables","heading":"34.5.5.4 Rejection Rates and Confidence Interval Coverage","text":"Rejection rates conventional t-tests stay close nominal level (5%) \\(|\\rho| < 0.76\\), independent instrument strength.\\(|\\rho| < 0.565\\), conventional t-tests exhibit -rejection, aligning findings Lee et al. (2022).Comparison AR tF Procedures:","code":""},{"path":"sec-instrumental-variables.html","id":"testing-assumptions","chapter":"34 Instrumental Variables","heading":"34.6 Testing Assumptions","text":"interested estimating causal effect endogenous regressor \\(X_2\\) outcome variable \\(Y\\), using instrumental variables \\(Z\\) address endogeneity.structural model interest :\\[\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\\(X_1\\): Exogenous regressors\\(X_2\\): Endogenous regressor(s)\\(Z\\): Instrumental variablesIf \\(Z\\) satisfies relevance exogeneity assumptions, can identify \\(\\beta_2\\) :\\[\n\\beta_2 = \\frac{Cov(Z, Y)}{Cov(Z, X_2)}\n\\]Alternatively, terms reduced form first stage estimates:Reduced Form (effect \\(Z\\) \\(Y\\)):\\[\n\\rho = \\frac{Cov(Y, Z)}{Var(Z)}\n\\]First Stage (effect \\(Z\\) \\(X_2\\)):\\[\n\\pi = \\frac{Cov(X_2, Z)}{Var(Z)}\n\\]IV Estimate:\\[\n\\beta_2 = \\frac{Cov(Y,Z)}{Cov(X_2, Z)} = \\frac{\\rho}{\\pi}\n\\]interpret \\(\\beta_2\\) causal effect \\(X_2\\) \\(Y\\), following assumptions must hold:","code":""},{"path":"sec-instrumental-variables.html","id":"sec-relevance-assumption","chapter":"34 Instrumental Variables","heading":"34.6.1 Relevance Assumption","text":"IV estimation, instrument relevance ensures instrument(s) \\(Z\\) can explain sufficient variation endogenous regressor(s) \\(X_2\\) identify structural equation:\\[\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]relevance condition requires instrument(s) \\(Z\\) correlated endogenous variable(s) \\(X_2\\), conditional covariates \\(X_1\\). Formally:\\[\nCov(Z, X_2) \\ne 0\n\\], matrix notation multiple instruments regressors, matrix correlations (generally, projection matrix) \\(Z\\) \\(X_2\\) must full column rank. guarantees \\(Z\\) non-trivial explanatory power \\(X_2\\).equivalent condition terms population moment conditions :\\[\nE[Z' (X_2 - E[X_2 | Z])] \\ne 0\n\\]condition ensures identification \\(\\beta_2\\). Without , IV estimator undefined due division zero ratio form:\\[\n\\hat{\\beta}_2^{IV} = \\frac{Cov(Z, Y)}{Cov(Z, X_2)}\n\\]first-stage regression operationalizes relevance assumption:\\[\nX_2 = Z \\pi + X_1 \\gamma + u\n\\]\\(\\pi\\): Vector first-stage coefficients, measuring effect instruments endogenous regressor(s).\\(u\\): First-stage residual.Identification \\(\\beta_2\\) requires \\(\\pi \\ne 0\\). \\(\\pi = 0\\), instrument explanatory power \\(X_2\\), IV procedure collapses.","code":""},{"path":"sec-instrumental-variables.html","id":"weak-instruments","chapter":"34 Instrumental Variables","heading":"34.6.1.1 Weak Instruments","text":"Even \\(Cov(Z, X_2) \\ne 0\\), weak instruments pose serious problem finite samples:Bias: IV estimator becomes biased direction OLS estimator.Size distortion: Hypothesis tests can inflated Type error rates.Variance: Estimates become highly variable unreliable.Asymptotic vs. Finite Sample ProblemsIV estimators consistent \\(n \\\\infty\\) relevance condition holds.weak instruments, convergence can slow finite-sample behavior practically indistinguishable inconsistency.Boundaries relevance strength thus critical applied work.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-first-stage-f-statistic","chapter":"34 Instrumental Variables","heading":"34.6.1.2 First-Stage F-Statistic","text":"single endogenous regressor case, first-stage F-statistic standard test instrument strength.First-Stage Regression:\\[\nX_2 = Z \\pi + X_1 \\gamma + u\n\\]test:\\[\n\\begin{aligned}\nH_0&: \\pi = 0 \\quad \\text{(Instruments explanatory power)}  \\\\\nH_1&: \\pi \\ne 0 \\quad \\text{(Instruments explain variation $X_2$)}\n\\end{aligned}\n\\]F-Statistic Formula:\\[\nF = \\frac{(SSR_r - SSR_{ur}) / q}{SSR_{ur} / (n - k - 1)}\n\\]\\(SSR_r\\): Sum squared residuals restricted model (instruments).\\(SSR_{ur}\\): Sum squared residuals unrestricted model (instruments).\\(q\\): Number excluded instruments (restrictions tested).\\(n\\): Number observations.\\(k\\): Number control variables.Interpretation:rule thumb (Staiger Stock 1997): \\(F < 10\\), instruments weak.However, Lee et al. (2022) criticizes threshold, advocating model-specific diagnostics.M. J. Moreira (2003) proposes Conditional Likelihood Ratio test inference weak instruments (D. W. Andrews, Moreira, Stock 2008).Use linearHypothesis() R test instrument relevance.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-cragg-donald-test","chapter":"34 Instrumental Variables","heading":"34.6.1.3 Cragg-Donald Test","text":"Cragg-Donald statistic essentially Wald statistic joint significance instruments first stage (Cragg Donald 1993), ’s used specifically multiple endogenous regressors. ’s calculated :\\[\nCD = n \\times (R_{ur}^2 - R_r^2)\n\\]:\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(n\\) number observations.\\(n\\) number observations.one endogenous variable, Cragg-Donald test results align closely Stock Yogo. Anderson canonical correlation test, likelihood ratio test, also works similar conditions, contrasting Cragg-Donald’s Wald statistic approach. valid one endogenous variable least one instrument.Large CD statistic implies instruments strong, case . judge critical value, look Stock-Yogo","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\n\ncragg_donald(\n    # control variables\n    X = ~ 1, \n    # endogeneous variables\n    D = ~ x, \n    # instrument variables \n    Z = ~ z, \n    data = WeakInstrument\n)\n#> Cragg-Donald test for weak instruments:\n#> \n#>      Data:                        WeakInstrument \n#>      Controls:                    ~1 \n#>      Treatments:                  ~x \n#>      Instruments:                 ~z \n#> \n#>      Cragg-Donald Statistic:        4.566136 \n#>      Df:                                 198"},{"path":"sec-instrumental-variables.html","id":"stock-yogo","chapter":"34 Instrumental Variables","heading":"34.6.1.4 Stock-Yogo","text":"Stock-Yogo test directly compute statistic like F-test Cragg-Donald, rather uses pre-computed critical values assess strength instruments. often uses eigenvalues derived concentration matrix:\\[\nS = \\frac{1}{n} (Z' X) (X'Z)\n\\]\\(Z\\) matrix instruments \\(X\\) matrix endogenous regressors.Stock Yogo provide critical values different scenarios (bias, size distortion) given number instruments endogenous regressors, based smallest eigenvalue \\(S\\). test compares eigenvalues critical values correspond thresholds permissible bias size distortion 2SLS estimator.Critical Values Test Conditions: critical values derived Stock Yogo depend level acceptable bias, number endogenous regressors, number instruments. example, 5% maximum acceptable bias, one endogenous variable, three instruments, critical value sufficient first stage F-statistic 13.91. Note framework requires least two overidentifying degree freedom. Stock Yogo (2002) set critical values bias less 10% (default)\\(H_0:\\) Instruments weak\\(H_1:\\) Instruments weak","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\n\nstock_yogo_test(\n    # control variables\n    X =  ~ Sepal.Length,\n    # endogeneous variables\n    D =  ~ Sepal.Width,\n    # instrument variables\n    Z =  ~ Petal.Length + Petal.Width + Species,\n    size_bias = \"bias\",\n    data = iris\n)\n#> Results of Stock and Yogo test for weak instruments:\n#> \n#>      Null Hypothesis:             Instruments are weak \n#>      Alternative Hypothesis:      Instruments are not weak \n#> \n#>      Data:                        iris       \n#>      Controls:                    ~Sepal.Length \n#>      Treatments:                  ~Sepal.Width \n#>      Instruments:                 ~Petal.Length + Petal.Width + Species \n#> \n#>      Alpha:                             0.05 \n#>      Acceptable level of bias:    5% relative to OLS.\n#>      Critical Value:                   16.85 \n#> \n#>      Cragg-Donald Statistic:        61.30973 \n#>      Df:                                 144"},{"path":"sec-instrumental-variables.html","id":"sec-anderson-rubin-test","chapter":"34 Instrumental Variables","heading":"34.6.1.5 Anderson-Rubin Test","text":"Anderson-Rubin (AR) test addresses issues weak instruments providing test structural parameter (\\(\\beta\\)) robust weak instruments (T. W. Anderson Rubin 1949). rely strength instruments control size, making valuable tool inference instrument relevance questionable.Consider following linear IV model:\\[\nY = X \\beta + u\n\\]\\(Y\\): Dependent variable (\\(n \\times 1\\))\\(X\\): Endogenous regressor (\\(n \\times k\\))\\(Z\\): Instrument matrix (\\(n \\times m\\)), assumed satisfy:\nInstrument Exogeneity: \\(\\mathbb{E}[Z'u] = 0\\)\nInstrument Relevance: \\(\\mathrm{rank}(\\mathbb{E}[Z'X]) = k\\)\nInstrument Exogeneity: \\(\\mathbb{E}[Z'u] = 0\\)Instrument Relevance: \\(\\mathrm{rank}(\\mathbb{E}[Z'X]) = k\\)relevance assumption ensures \\(Z\\) contains valid information predicting \\(X\\).Relevance typically assessed first-stage regression:\\[\nX = Z \\Pi + V\n\\]\\(Z\\) weakly correlated \\(X\\), \\(\\Pi\\) close zero, violating relevance assumption.AR test Wald-type test null hypothesis:\\[\nH_0: \\beta = \\beta_0\n\\]constructed examining whether residuals imposing \\(\\beta_0\\) orthogonal instruments \\(Z\\). Specifically:Compute reduced-form residuals \\(H_0\\):\\[\nr(\\beta_0) = Y - X \\beta_0\n\\]AR test statistic :\\[\nAR(\\beta_0) = \\frac{r(\\beta_0)' P_Z r(\\beta_0)}{\\hat{\\sigma}^2}\n\\]\\(P_Z = Z (Z'Z)^{-1} Z'\\) projection matrix onto column space \\(Z\\).\\(\\hat{\\sigma}^2 = \\frac{r(\\beta_0)' M_Z r(\\beta_0)}{n - m}\\) residual variance estimator, \\(M_Z = - P_Z\\).\\(H_0\\), assuming homoskedasticity, AR statistic follows F-distribution:\\[\nAR(\\beta_0) \\sim F(m, n - m)\n\\]Alternatively, large \\(n\\), AR statistic can approximated chi-squared distribution:\\[\nAR(\\beta_0) \\sim \\chi^2_m\n\\]InterpretationIf \\(AR(\\beta_0)\\) exceeds critical value \\(F\\) (\\(\\chi^2\\)) distribution, reject \\(H_0\\).test assesses whether \\(r(\\beta_0)\\) orthogonal \\(Z\\). , \\(H_0\\) inconsistent moment conditions.key advantage AR test size correct even instruments weak. AR statistic depend strength instruments (.e., magnitude \\(\\Pi\\)), making valid weak identification.contrasts standard 2SLS-based Wald tests, whose distribution depends first-stage relevance can severely distorted \\(Z\\) weakly correlated \\(X\\).relevance assumption necessary point identification consistent estimation IV.instruments weak, point estimates \\(\\beta\\) 2SLS can biased.AR test allows valid hypothesis testing, even instrument relevance weak.However, instruments completely irrelevant (.e., \\(Z'X = 0\\)), IV model unidentified, AR test lacks power (.e., reject \\(H_0\\) \\(\\beta_0\\)).AR test can inverted form confidence sets \\(\\beta\\):Compute \\(AR(\\beta)\\) grid \\(\\beta\\) values.Include \\(\\beta\\) values \\(AR(\\beta)\\) reject \\(H_0\\) chosen significance level.confidence sets robust weak instruments can disconnected unbounded identification weak.","code":"\nset.seed(123)\n\n# Simulate data\nn <- 500\nZ <- cbind(1, rnorm(n)) # Instrument (include constant)\nX <- 0.1 * Z[,2] + rnorm(n) # Weak first-stage relationship\nbeta_true <- 1\nu <- rnorm(n)\nY <- X * beta_true + u\n\n\nlibrary(ivmodel)\nivmodel(Y = Y, D = X, Z = Z)\n#> \n#> Call:\n#> ivmodel(Y = Y, D = X, Z = Z)\n#> sample size: 500\n#> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n#> \n#> First Stage Regression Result:\n#> \n#> F=0.684842, df1=2, df2=497, p-value is 0.50464\n#> R-squared=0.002748329,   Adjusted R-squared=-0.001264756\n#> Residual standard error: 1.01117 on 499 degrees of freedom\n#> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n#> \n#> Sargan Test Result:\n#> \n#> Sargan Test Statistics=0.007046677, df=1, p-value is 0.9331\n#> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n#> \n#> Coefficients of k-Class Estimators:\n#> \n#>              k Estimate Std. Error t value Pr(>|t|)    \n#> OLS    0.00000  1.05559    0.04396  24.015   <2e-16 ***\n#> Fuller 0.99800  1.81913    0.80900   2.249   0.0250 *  \n#> TSLS   1.00000  2.37533    1.40555   1.690   0.0917 .  \n#> LIML   1.00001  2.38211    1.41382   1.685   0.0926 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n#> \n#> Alternative tests for the treatment effect under H_0: beta=0.\n#> \n#> Anderson-Rubin test (under F distribution):\n#> F=1.874305, df1=2, df2=497, p-value is 0.15454\n#> 95 percent confidence interval:\n#>  Whole Real Line\n#> \n#> Conditional Likelihood Ratio test (under Normal approximation):\n#> Test Stat=3.741629, p-value is 0.14881\n#> 95 percent confidence interval:\n#>  Whole Real Line"},{"path":"sec-instrumental-variables.html","id":"sec-stock-wright-test","chapter":"34 Instrumental Variables","heading":"34.6.1.6 Stock-Wright Test","text":"Anderson-Rubin test offers one solution constructing test statistics valid regardless instrument strength, another complementary approach Stock-Wright test, sometimes referred S-test Score test. test belongs broader class conditional likelihood ratio tests proposed M. J. Moreira (2003) Stock Wright (2000), plays important role constructing weak-instrument robust confidence regions.Stock-Wright test exploits conditional score function IV model test hypotheses structural parameters, offering robustness weak identification.Consider linear IV model:\\[\nY = X \\beta + u\n\\]\\(Y\\): Outcome variable (\\(n \\times 1\\))\\(X\\): Endogenous regressor (\\(n \\times k\\))\\(Z\\): Instrument matrix (\\(n \\times m\\)), \\(m \\ge k\\)exogeneity relevance assumptions \\(Z\\) :\\(\\mathbb{E}[Z'u] = 0\\) (Exogeneity)\\(\\mathrm{rank}(\\mathbb{E}[Z'X]) = k\\) (Relevance)Weak instruments imply matrix \\(\\mathbb{E}[Z'X]\\) close rank-deficient nearly zero, can invalidate standard inference.goal test null hypothesis:\\[\nH_0: \\beta = \\beta_0\n\\]Stock-Wright test provides robust way perform hypothesis test constructing score statistic robust weak instruments.score test (Lagrange Multiplier test) evaluates whether score (gradient log-likelihood function respect \\(\\beta\\)) close zero null hypothesis.IV context, conditional score evaluated reduced-form equations. SW test uses fact \\(H_0\\), moment condition:\\[\n\\mathbb{E}[Z'(Y - X \\beta_0)] = 0\n\\]hold. Deviations condition can tested using score statistic.reduced-form equations :\\[\n\\begin{aligned}\nY &= Z \\pi_Y + \\epsilon_Y \\\\\nX &= Z \\pi_X + \\epsilon_X\n\\end{aligned}\n\\]system:\\(\\epsilon_Y\\) \\(\\epsilon_X\\) jointly normally distributed covariance matrix \\(\\Sigma\\).\\(\\epsilon_Y\\) \\(\\epsilon_X\\) jointly normally distributed covariance matrix \\(\\Sigma\\).structural model implies restriction \\(\\pi_Y\\): \\(\\pi_Y = \\pi_X \\beta\\).structural model implies restriction \\(\\pi_Y\\): \\(\\pi_Y = \\pi_X \\beta\\).Stock-Wright test statistic given :\\[\nS(\\beta_0) = (Z'(Y - X \\beta_0))' \\left[ \\hat{\\Omega}^{-1} \\right] (Z'(Y - X \\beta_0))\n\\]:\\(\\hat{\\Omega}\\) estimator covariance matrix moment condition \\(Z'u\\), often estimated \\(Z'Z\\) times estimator \\(\\mathrm{Var}(u)\\).\\(\\hat{\\Omega}\\) estimator covariance matrix moment condition \\(Z'u\\), often estimated \\(Z'Z\\) times estimator \\(\\mathrm{Var}(u)\\).homoskedastic settings, \\(\\hat{\\Omega} = \\hat{\\sigma}^2 Z'Z\\), \\(\\hat{\\sigma}^2\\) estimated residuals \\(H_0\\):homoskedastic settings, \\(\\hat{\\Omega} = \\hat{\\sigma}^2 Z'Z\\), \\(\\hat{\\sigma}^2\\) estimated residuals \\(H_0\\):\\[\n\\hat{\\sigma}^2 = \\frac{(Y - X \\beta_0)' (Y - X \\beta_0)}{n}\n\\]null hypothesis \\(H_0\\), test statistic \\(S(\\beta_0)\\) follows chi-squared distribution \\(m\\) degrees freedom:\\[\nS(\\beta_0) \\sim \\chi^2_m\n\\]Stock-Wright test closely related Anderson-Rubin test. However:AR test focuses orthogonality reduced-form residuals \\(Z\\).SW test focuses conditional score, derived likelihood framework.robust weak instruments, different power properties depending data-generating process.Geometric IntuitionThe SW test can seen testing whether orthogonality condition \\(Z\\) \\(u\\) holds, using score function.effectively checks whether directional derivative likelihood (evaluated \\(\\beta_0\\)) zero, offering generalized method moments interpretation.Stock-Wright S-test can inverted form confidence regions:grid \\(\\beta\\) values, compute \\(S(\\beta)\\).confidence region consists \\(\\beta\\) values \\(S(\\beta)\\) reject \\(H_0\\) chosen significance level.regions robust weak instruments, can disconnected unbounded instruments weak deliver informative inference.","code":"\nset.seed(42)\n\n# Simulate data\nn <- 500\nZ <- cbind(1, rnorm(n)) # Instrument (include constant)\nX <- 0.1 * Z[,2] + rnorm(n) # Weak first-stage relationship\nbeta_true <- 1\nu <- rnorm(n)\nY <- X * beta_true + u\n\n# Null hypothesis to test\nbeta_0 <- 0\n\n# Residuals under H0\nr_beta0 <- Y - X * beta_0\n\n# Estimate variance of residuals under H0\nsigma2_hat <- mean(r_beta0^2)\n\n# Compute Omega matrix\nOmega_hat <- sigma2_hat * t(Z) %*% Z\n\n# Compute the Stock-Wright S-statistic\nS_stat <- t(t(Z) %*% r_beta0) %*% solve(Omega_hat) %*% (t(Z) %*% r_beta0)\n\n# p-value from chi-squared distribution\ndf <- ncol(Z) # degrees of freedom\np_val <- 1 - pchisq(S_stat, df = df)\n\n# Output\ncat(\"Stock-Wright S-Statistic:\", round(S_stat, 4), \"\\n\")\n#> Stock-Wright S-Statistic: 5.0957\ncat(\"p-value:\", round(p_val, 4), \"\\n\")\n#> p-value: 0.0783"},{"path":"sec-instrumental-variables.html","id":"sec-kleibergen-paap-rk-statistic","chapter":"34 Instrumental Variables","heading":"34.6.1.7 Kleibergen-Paap rk Statistic","text":"Traditional diagnostics instrument relevance, :first-stage \\(F\\)-statistic (single endogenous variables homoskedastic errors)first-stage \\(F\\)-statistic (single endogenous variables homoskedastic errors)Cragg-Donald statistic (multiple endogenous regressors homoskedasticity)Cragg-Donald statistic (multiple endogenous regressors homoskedasticity)valid errors exhibit heteroskedasticity non-..d. behavior.Kleibergen-Paap (KP) rk statistic addresses limitations providing robust test underidentification weak identification IV models, even presence heteroskedasticity.Consider linear IV model:\\[\nY = X \\beta + u\n\\]\\(Y\\): Dependent variable (\\(n \\times 1\\))\\(X\\): Matrix endogenous regressors (\\(n \\times k\\))\\(Z\\): Instrument matrix (\\(n \\times m\\)), \\(m \\ge k\\)\\(u\\): Structural error termThe moment conditions exogeneity :\\[\n\\mathbb{E}[Z'u] = 0\n\\]relevance assumption requires:\\[\n\\mathrm{rank}(\\mathbb{E}[Z'X]) = k\n\\]condition fails, model underidentified, consistent estimation \\(\\beta\\) impossible.Kleibergen-Paap rk statistic performs two key functions:Test underidentification (whether instruments identify equation)Weak identification diagnostics, analogous Cragg-Donald statistic, robust heteroskedasticity.“rk”?“rk” stands rank—statistic tests whether matrix reduced-form coefficients full rank, necessary identification.KP rk statistic builds generalized method moments framework canonical correlations \\(X\\) \\(Z\\).reduced-form \\(X\\) :\\[\nX = Z \\Pi + V\n\\]\\(\\Pi\\): Matrix reduced-form coefficients (\\(m \\times k\\))\\(V\\): First-stage residuals (\\(n \\times k\\))null hypothesis underidentification, matrix \\(\\Pi\\) rank deficient, meaning \\(\\Pi\\) full rank \\(k\\).Kleibergen-Paap rk statistic tests null hypothesis:\\[\nH_0: \\mathrm{rank}(\\mathbb{E}[Z'X]) < k\n\\]alternative:\\[\nH_A: \\mathrm{rank}(\\mathbb{E}[Z'X]) = k\n\\]Kleibergen-Paap rk statistic Lagrange Multiplier test statistic derived first-stage canonical correlations \\(X\\) \\(Z\\), adjusted heteroskedasticity.Computation OutlineCompute first-stage residuals endogenous regressor.Estimate covariance matrix residuals, allowing heteroskedasticity.Calculate rank test statistic, asymptotic chi-squared distribution \\(k(m - k)\\) degrees freedom.\\(H_0\\), KP rk statistic follows:\\[\nKP_{rk} \\sim \\chi^2_{k(m - k)}\n\\]Intuition Behind KP rk StatisticThe statistic examines whether moment conditions based \\(Z\\) provide enough information identify \\(\\beta\\).\\(Z\\) fails explain sufficient variation \\(X\\), instruments relevant, model underidentified.KP rk test necessary condition relevance, though sufficient measure instrument strength.Practical UsageA rejection \\(H_0\\) suggests instruments relevant enough identification.failure reject \\(H_0\\) implies underidentification, IV estimates valid.KP rk statistic tests underidentification, directly assess weak instruments. However, often reported alongside Kleibergen-Paap LM Wald statistics, address weak instrument diagnostics heteroskedastic settings.Comparison: Kleibergen-Paap rk vs Cragg-Donald StatisticInterpretationThe Kleibergen-Paap rk statistic reported alongside LM Wald weak identification tests.Kleibergen-Paap rk statistic reported alongside LM Wald weak identification tests.p-value rk statistic tells us whether equation identified.p-value rk statistic tells us whether equation identified.test rejects, proceed evaluate weak instrument strength using Wald LM statistics.test rejects, proceed evaluate weak instrument strength using Wald LM statistics.","code":"\n# Load necessary packages\nlibrary(sandwich)  # For robust covariance estimators\nlibrary(lmtest)    # For testing\nlibrary(AER)       # For IV estimation (optional)\n\n# Simulate data\nset.seed(123)\nn <- 500\nZ1 <- rnorm(n)\nZ2 <- rnorm(n)\nZ <- cbind(1, Z1, Z2) # Instruments (include intercept)\n\n# Weak instrument case\nX1 <- 0.1 * Z1 + 0.1 * Z2 + rnorm(n)\nX <- cbind(X1)\nbeta_true <- 1\nu <- rnorm(n)\nY <- X %*% beta_true + u\n\n# First-stage regression: X ~ Z\nfirst_stage <- lm(X1 ~ Z1 + Z2)\nV <- residuals(first_stage) # First-stage residuals\n\n# Calculate robust covariance matrix of residuals\n# Note: sandwich package already computes heteroskedasticity-consistent covariances\n\n# Kleibergen-Paap rk test via sandwich estimators (conceptual)\n# In practice, Kleibergen-Paap rk statistics are provided by ivreg2 (Stata) or via custom functions\n\n# For illustration, using ivreg and summary statistics\niv_model <- ivreg(Y ~ X1 | Z1 + Z2)\n\n# Kleibergen-Paap rk statistic (reported by summary under diagnostics)\nsummary(iv_model, diagnostics = TRUE)\n#> \n#> Call:\n#> ivreg(formula = Y ~ X1 | Z1 + Z2)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -3.2259666 -0.6433047  0.0004169  0.8384112  3.4831350 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.05389    0.04810   1.120    0.263    \n#> X1           1.16437    0.22129   5.262 2.12e-07 ***\n#> \n#> Diagnostic tests:\n#>                  df1 df2 statistic  p-value    \n#> Weak instruments   2 497    11.793 9.91e-06 ***\n#> Wu-Hausman         1 497     2.356  0.12541    \n#> Sargan             1  NA     6.934  0.00846 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.066 on 498 degrees of freedom\n#> Multiple R-Squared: 0.3593,  Adjusted R-squared: 0.358 \n#> Wald test: 27.69 on 1 and 498 DF,  p-value: 2.123e-07\n\n# Interpretation:\n# Weak instruments are flagged if the KP rk statistic does not reject underidentification."},{"path":"sec-instrumental-variables.html","id":"comparison-of-weak-instrument-tests","chapter":"34 Instrumental Variables","heading":"34.6.1.8 Comparison of Weak Instrument Tests","text":"mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors independently identically distributed. assumption violated, Kleinbergen-Paap test robust violations iid assumption can applied even single endogenous variable instrument, provided model properly identified (Baum Lewbel 2019).","code":""},{"path":"sec-instrumental-variables.html","id":"independence-unconfoundedness","chapter":"34 Instrumental Variables","heading":"34.6.2 Independence (Unconfoundedness)","text":"\\(Z\\) independent factors affecting \\(Y\\), apart \\(X_2\\).Formally: \\(Z \\perp \\epsilon\\).stronger exclusion restriction typically requires randomized assignment \\(Z\\) strong theoretical justification.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-monotonicity-assumption","chapter":"34 Instrumental Variables","heading":"34.6.3 Monotonicity Assumption","text":"Relevant identifying Local Average Treatment Effects (G. W. Imbens Angrist 1994)Assumes defiers: instrument \\(Z\\) cause treatment \\(X_2\\) increase units decreasing others.\\[\nX_2(Z = 1) \\ge X_2(Z = 0) \\quad \\text{individuals}\n\\]Ensures identifying [Local Average Treatment Effect] (LATE) group compliers—individuals whose treatment status responds instrument.assumption particularly important empirical applications involving binary instruments heterogeneous treatment effects.business settings, instruments often arise policy changes, eligibility cutoffs, randomized marketing campaigns. instance:firm rolls new loyalty program (\\(Z = 1\\)) selected regions encourage purchases (\\(X_2\\)). monotonicity assumption implies customer reduces purchases program—increases leaves unchanged.assumption rules defiers—individuals respond instrument opposite direction—otherwise bias IV estimate introducing effects attributable compliers. Violations monotonicity make IV estimate difficult interpret, may average compliers defiers, yielding non-causal ambiguous LATE.monotonicity assumption unobserved counterfactuals thus directly testable, several empirical strategies can provide suggestive evidence:First-Stage RegressionEstimate impact instrument treatment. strong, consistent sign across subgroups supports monotonicity.positive significant coefficient \\(Z\\) supports monotonic relationship.positive significant coefficient \\(Z\\) supports monotonic relationship.coefficient near zero flips sign subgroups, may signal violations.coefficient near zero flips sign subgroups, may signal violations.Density Plot First-Stage ResidualsA unimodal residual distribution supports monotonicity.unimodal residual distribution supports monotonicity.bimodal heavily skewed pattern suggest mixture compliers defiers.bimodal heavily skewed pattern suggest mixture compliers defiers.Subgroup AnalysisSplit sample subgroups (e.g., market segment region) compare first-stage coefficients.groups show coefficient sign, supports monotonicity.groups show coefficient sign, supports monotonicity.Opposing signs raise concerns individuals may respond instrument.Opposing signs raise concerns individuals may respond instrument.","code":"\nset.seed(123)\n\n# Sample size\nn <- 1000\n\n# Generate instrument (Z), treatment (D), and outcome (Y)\nZ <- rbinom(n, 1, 0.5)  # Binary instrument (e.g., policy change)\nU <- rnorm(n)           # Unobserved confounder\nD <- 0.8 * Z + 0.3 * U + rnorm(n)  # Treatment variable\nY <- 2 * D + 0.5 * U + rnorm(n)    # Outcome variable\n\n# Create a data frame\ndf <- data.frame(Z, D, Y)\n\n# First-stage regression\nfirst_stage <- lm(D ~ Z, data = df)\nsummary(first_stage)\n#> \n#> Call:\n#> lm(formula = D ~ Z, data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.2277 -0.7054  0.0105  0.7047  3.3846 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02910    0.04651   0.626    0.532    \n#> Z            0.74286    0.06623  11.216   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.047 on 998 degrees of freedom\n#> Multiple R-squared:  0.1119, Adjusted R-squared:  0.111 \n#> F-statistic: 125.8 on 1 and 998 DF,  p-value: < 2.2e-16\n\n# Check F-statistic\nfs_f_stat <- summary(first_stage)$fstatistic[1]\nfs_f_stat\n#>    value \n#> 125.7911\n# Extract residuals\nresiduals_first_stage <- residuals(first_stage)\n\n# Plot density\nlibrary(ggplot2)\nggplot(data.frame(residuals = residuals_first_stage),\n       aes(x = residuals)) +\n    geom_density(fill = \"blue\", alpha = 0.5) +\n    ggtitle(\"Density of First-Stage Residuals\") +\n    xlab(\"Residuals from First-Stage Regression\") +\n    ylab(\"Density\") +\n    causalverse::ama_theme()\n# Create two random subgroups\ndf$subgroup <- ifelse(runif(n) > 0.5, \"Group A\", \"Group B\")\n\n# First-stage regression in subgroups\nfirst_stage_A <- lm(D ~ Z, data = df[df$subgroup == \"Group A\", ])\nfirst_stage_B <- lm(D ~ Z, data = df[df$subgroup == \"Group B\", ])\n\n# Compare coefficients\ncoef_A <- coef(first_stage_A)[\"Z\"]\ncoef_B <- coef(first_stage_B)[\"Z\"]\n\ncat(\"First-stage coefficient for Group A:\", coef_A, \"\\n\")\n#> First-stage coefficient for Group A: 0.6645617\ncat(\"First-stage coefficient for Group B:\", coef_B, \"\\n\")\n#> First-stage coefficient for Group B: 0.8256711"},{"path":"sec-instrumental-variables.html","id":"homogeneous-treatment-effects-optional","chapter":"34 Instrumental Variables","heading":"34.6.4 Homogeneous Treatment Effects (Optional)","text":"Assumes causal effect \\(X_2\\) \\(Y\\) constant across individuals.Without , IV estimates local rather global average treatment effect (LATE vs ATE).","code":""},{"path":"sec-instrumental-variables.html","id":"sec-linearity-and-additivity","chapter":"34 Instrumental Variables","heading":"34.6.5 Linearity and Additivity","text":"functional form linear parameters:\\[\nY = X \\beta + \\epsilon\n\\]interactions non-linearities unless explicitly modeled.Additivity error term \\(\\epsilon\\) implies heteroskedasticity classic IV models (though robust standard errors can relax ).","code":""},{"path":"sec-instrumental-variables.html","id":"instrument-exogeneity-exclusion-restriction","chapter":"34 Instrumental Variables","heading":"34.6.6 Instrument Exogeneity (Exclusion Restriction)","text":"\\(E[Z \\epsilon] = 0\\): Instruments must correlated error term.\\(Z\\) influences \\(Y\\) \\(X_2\\).omitted variable bias unobserved confounders correlated \\(Z\\).Key Interpretation:\\(Z\\) direct effect \\(Y\\).pathway \\(Z\\) \\(Y\\) must operate exclusively \\(X_2\\).","code":""},{"path":"sec-instrumental-variables.html","id":"sec-exogeneity-assumption","chapter":"34 Instrumental Variables","heading":"34.6.7 Exogeneity Assumption","text":"local average treatment effect (LATE) defined :\\[\n\\text{LATE} = \\frac{\\text{reduced form}}{\\text{first stage}} = \\frac{\\rho}{\\phi}\n\\]implies reduced form (\\(\\rho\\)) product first stage (\\(\\phi\\)) LATE:\\[\n\\rho = \\phi \\times \\text{LATE}\n\\]Thus, first stage (\\(\\phi\\)) 0, reduced form (\\(\\rho\\)) also 0.statistically significant reduced form estimate without corresponding first stage indicates issue, suggesting alternative channel linking instruments outcomes direct effect IV outcome.Direct Effect: direct effect 0 first stage 0, reduced form 0.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.Direct Effect: direct effect IV outcome, reduced form can significantly different 0, even first stage 0.\nviolates exogeneity assumption, IV affect outcome treatment variable.\nviolates exogeneity assumption, IV affect outcome treatment variable.test validity exogeneity assumption, can use sanity test:Identify groups effects instruments treatment variable small significantly different 0. reduced form estimate groups also 0. “-first-stage samples” provide evidence whether exogeneity assumption violated.","code":"\n# Load necessary libraries\nlibrary(shiny)\nlibrary(AER)  # for ivreg\nlibrary(ggplot2)  # for visualization\nlibrary(dplyr)  # for data manipulation\n\n# Function to simulate the dataset\nsimulate_iv_data <- function(n, beta, phi, direct_effect) {\n  Z <- rnorm(n)\n  epsilon_x <- rnorm(n)\n  epsilon_y <- rnorm(n)\n  X <- phi * Z + epsilon_x\n  Y <- beta * X + direct_effect * Z + epsilon_y\n  data <- data.frame(Y = Y, X = X, Z = Z)\n  return(data)\n}\n\n# Function to run the simulations and calculate the effects\nrun_simulation <- function(n, beta, phi, direct_effect) {\n  # Simulate the data\n  simulated_data <- simulate_iv_data(n, beta, phi, direct_effect)\n  \n  # Estimate first-stage effect (phi)\n  first_stage <- lm(X ~ Z, data = simulated_data)\n  phi <- coef(first_stage)[\"Z\"]\n  phi_ci <- confint(first_stage)[\"Z\", ]\n  \n  # Estimate reduced-form effect (rho)\n  reduced_form <- lm(Y ~ Z, data = simulated_data)\n  rho <- coef(reduced_form)[\"Z\"]\n  rho_ci <- confint(reduced_form)[\"Z\", ]\n  \n  # Estimate LATE using IV regression\n  iv_model <- ivreg(Y ~ X | Z, data = simulated_data)\n  iv_late <- coef(iv_model)[\"X\"]\n  iv_late_ci <- confint(iv_model)[\"X\", ]\n  \n  # Calculate LATE as the ratio of reduced-form and first-stage coefficients\n  calculated_late <- rho / phi\n  calculated_late_se <- sqrt(\n    (rho_ci[2] - rho)^2 / phi^2 + (rho * (phi_ci[2] - phi) / phi^2)^2\n  )\n  calculated_late_ci <- c(calculated_late - 1.96 * calculated_late_se, \n                          calculated_late + 1.96 * calculated_late_se)\n  \n  # Return a list of results\n  list(phi = phi, \n       phi_ci = phi_ci,\n       rho = rho, \n       rho_ci = rho_ci,\n       direct_effect = direct_effect,\n       direct_effect_ci = c(direct_effect, direct_effect),  # Placeholder for direct effect CI\n       iv_late = iv_late, \n       iv_late_ci = iv_late_ci,\n       calculated_late = calculated_late, \n       calculated_late_ci = calculated_late_ci,\n       true_effect = beta,\n       true_effect_ci = c(beta, beta))  # Placeholder for true effect CI\n}\n\n# Define UI for the sliders\nui <- fluidPage(\n  titlePanel(\"IV Model Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"beta\", \"True Effect of X on Y (beta):\", min = 0, max = 1.0, value = 0.5, step = 0.1),\n      sliderInput(\"phi\", \"First Stage Effect (phi):\", min = 0, max = 1.0, value = 0.7, step = 0.1),\n      sliderInput(\"direct_effect\", \"Direct Effect of Z on Y:\", min = -0.5, max = 0.5, value = 0, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"dotPlot\")\n    )\n  )\n)\n\n# Define server logic to run the simulation and generate the plot\nserver <- function(input, output) {\n  output$dotPlot <- renderPlot({\n    # Run simulation\n    results <- run_simulation(n = 1000, beta = input$beta, phi = input$phi, direct_effect = input$direct_effect)\n    \n    # Prepare data for plotting\n    plot_data <- data.frame(\n      Effect = c(\"First Stage (phi)\", \"Reduced Form (rho)\", \"Direct Effect\", \"LATE (Ratio)\", \"LATE (IV)\", \"True Effect\"),\n      Value = c(results$phi, results$rho, results$direct_effect, results$calculated_late, results$iv_late, results$true_effect),\n      CI_Lower = c(results$phi_ci[1], results$rho_ci[1], results$direct_effect_ci[1], results$calculated_late_ci[1], results$iv_late_ci[1], results$true_effect_ci[1]),\n      CI_Upper = c(results$phi_ci[2], results$rho_ci[2], results$direct_effect_ci[2], results$calculated_late_ci[2], results$iv_late_ci[2], results$true_effect_ci[2])\n    )\n    \n    # Create dot plot with confidence intervals\n    ggplot(plot_data, aes(x = Effect, y = Value)) +\n      geom_point(size = 3) +\n      geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +\n      labs(title = \"IV Model Effects\",\n           y = \"Coefficient Value\") +\n      coord_cartesian(ylim = c(-1, 1)) +  # Limits the y-axis to -1 to 1 but allows CI beyond\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"},{"path":"sec-instrumental-variables.html","id":"overid-tests","chapter":"34 Instrumental Variables","heading":"34.6.7.1 Overid Tests","text":"Wald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenous\nPeople might prefer Wald test Hausman test.\nWald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenousPeople might prefer Wald test Hausman test.Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)tests can provide evidence fo validity -identifying restrictions sufficient necessary validity moment conditions (.e., assumption tested). (Deaton 2010; Parente Silva 2012)-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)overid tests hold value/info?Overidentifying restrictions valid irrespective instruments’ validity\nWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)\nOveridentifying restrictions valid irrespective instruments’ validityWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)Overidentifying restriction invalid instrument valid\neffect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)\nOveridentifying restriction invalid instrument validWhen effect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)tests (overidentifying restrictions) used check whether different instruments identify parameters interest, check validity (J. . Hausman 1983; Parente Silva 2012)","code":""},{"path":"sec-instrumental-variables.html","id":"wald-test","chapter":"34 Instrumental Variables","heading":"34.6.7.1.1 Wald Test","text":"Assuming \\(Z\\) exogenous (valid instrument), want know whether \\(X_2\\) exogenous1st stage:\\[\nX_2 = \\hat{\\alpha} Z + \\hat{\\epsilon}\n\\]2nd stage:\\[\nY = \\delta_0 X_1 + \\delta_1 X_2 + \\delta_2 \\hat{\\epsilon} + u\n\\]\\(\\hat{\\epsilon}\\) residuals 1st stageThe Wald test exogeneity assumes\\[\nH_0: \\delta_2 = 0 \\\\\nH_1: \\delta_2 \\neq 0\n\\]one endogenous variable one instrument, \\(\\delta_2\\) vector residuals first-stage equations. null hypothesis jointly equal 0.reject hypothesis, means \\(X_2\\) endogenous. Hence, test, want reject null hypothesis.test sacrificially significant, might just don’t enough information reject null.valid instrument \\(Z\\), whether \\(X_2\\) endogenous exogenous, coefficient estimates \\(X_2\\) still consistent. \\(X_2\\) exogenous, 2SLS inefficient (.e., larger standard errors).Intuition:\\(\\hat{\\epsilon}\\) supposed endogenous part \\(X_2\\), regress \\(Y\\) \\(\\hat{\\epsilon}\\) observe coefficient different 0. means exogenous part \\(X_2\\) can explain well impact \\(Y\\), endogenous part.","code":""},{"path":"sec-instrumental-variables.html","id":"hausmans-test","chapter":"34 Instrumental Variables","heading":"34.6.7.1.2 Hausman’s Test","text":"Similar Wald Test identical Wald Test homoskedasticity (.e., homogeneity variances). assumption, ’s used less often Wald Test","code":""},{"path":"sec-instrumental-variables.html","id":"hansens-j","chapter":"34 Instrumental Variables","heading":"34.6.7.1.3 Hansen’s J","text":"(L. P. Hansen 1982)(L. P. Hansen 1982)J-test (-identifying restrictions test): test whether additional instruments exogenous\nCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\nAssume least one instrument within \\(Z\\) exogenous\nJ-test (-identifying restrictions test): test whether additional instruments exogenousCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\\(dim(Z) > dim(X_2)\\)Assume least one instrument within \\(Z\\) exogenousProcedure IV-GMM:Obtain residuals 2SLS estimationRegress residuals instruments exogenous variables.Test joint hypothesis coefficients residuals across instruments 0 (.e., true instruments exogenous).\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).\nexogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).Compute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.reject hypothesis, can \nfirst sets instruments invalid\nsecond sets instruments invalid\nsets instruments invalid\nfirst sets instruments invalidThe first sets instruments invalidThe second sets instruments invalidThe second sets instruments invalidBoth sets instruments invalidBoth sets instruments invalidNote: test true residuals homoskedastic.heteroskedasticity-robust \\(J\\)-statistic, see (Carrasco Doukali 2022; H. Li et al. 2022)","code":""},{"path":"sec-instrumental-variables.html","id":"sargan-test","chapter":"34 Instrumental Variables","heading":"34.6.7.1.4 Sargan Test","text":"(Sargan 1958)Similar Hansen’s J, assumes homoskedasticityHave careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).one heteroskedasticity design, Sargan test invalid (Pitt 2011})one heteroskedasticity design, Sargan test invalid (Pitt 2011})","code":""},{"path":"sec-instrumental-variables.html","id":"standard-interpretation-of-the-j-test-for-overidentifying-restrictions-is-misleading","chapter":"34 Instrumental Variables","heading":"34.6.7.2 Standard Interpretation of the J-Test for Overidentifying Restrictions Is Misleading","text":"IV estimation—particularly overidentified models number instruments \\(m\\) exceeds number endogenous regressors \\(k\\)—standard practice conduct J-test (Sargan 1958; L. P. Hansen 1982). Commonly, J-test (Sargan-Hansen test) described method test whether instruments valid. misleading. J-test establish instrument validity merely “fails reject” null; best, can uncover evidence validity.J-Test Actually DoesLet \\(Z\\) denote \\(n \\times m\\) matrix instruments, let \\(u\\) structural error term IV model. J-test evaluates following moment conditions implied instrument exogeneity:\\[\n\\begin{aligned}\nH_0 &: \\mathbb{E}[Z'u] = 0 \\quad \\text{(moment conditions hold simultaneously)},\\\\\nH_A &: \\mathbb{E}[Z'u] \\neq 0 \\quad \\text{(least one moment condition fails)}.\n\\end{aligned}\n\\]Reject \\(H_0\\): least one instrument invalid, model otherwise misspecified.Fail reject \\(H_0\\): sample evidence instruments invalid—mean necessarily valid.J-statistic can written (Generalized Method Moments context) :\\[\nJ = n \\,\\hat{g}'\\, W \\,\\hat{g},\n\\]\\(\\hat{g} = \\frac{1}{n} \\sum_{=1}^n z_i \\hat{u}_i\\) sample average instrument–residual covariances (residuals \\(\\hat{u}_i\\)), \\(W\\) appropriate weighting matrix (often inverse variance matrix \\(\\hat{g}\\)).null, \\(J\\) asymptotically \\(\\chi^2_{m - k}\\). large \\(J\\) (relative \\(\\chi^2\\) critical value) indicates rejection.Key Insight: Failing reject J-test null confirm validity. just means test detect evidence invalid instruments. test low power (e.g., small samples weak instruments), may see “rejection” even instruments truly invalid.“J-Test Validity Test” Wrong Way Think ItThe Null Hypothesis Almost Always Strong\nEconomic models approximations; strict exogeneity rarely holds perfectly.\nEven instruments “plausibly” exogenous, population moment \\(\\mathbb{E}[Z'u]\\) may approximately hold.\nJ-test requires instruments perfectly valid. Failing reject \\(H_0\\) prove .\nEconomic models approximations; strict exogeneity rarely holds perfectly.Even instruments “plausibly” exogenous, population moment \\(\\mathbb{E}[Z'u]\\) may approximately hold.J-test requires instruments perfectly valid. Failing reject \\(H_0\\) prove .Weak Instruments Lead Weak Power\nJ-test can low power instruments weak.\nmay fail reject even invalid instruments test detect violations.\nJ-test can low power instruments weak.may fail reject even invalid instruments test detect violations.Rejection Pinpoint Instrument Invalid\nlearn one instruments (entire model) problematic; J-test doesn’t tell ones.\nlearn one instruments (entire model) problematic; J-test doesn’t tell ones.Model Specification Error Confounds Interpretation\nJ-test rejection can stem instrument invalidity broader model mis-specification (e.g., incorrect functional form).\ntest distinguish sources.\nJ-test rejection can stem instrument invalidity broader model mis-specification (e.g., incorrect functional form).test distinguish sources.Overidentification Guarantee Validity Check\nJ-test available \\(m > k\\). \\(m = k\\) (exact identification), J-test possible.\nIronically, exactly-identified models often go “unquestioned” run J-test—yet mean valid.\nJ-test available \\(m > k\\). \\(m = k\\) (exact identification), J-test possible.Ironically, exactly-identified models often go “unquestioned” run J-test—yet mean valid.Think J-Test InsteadA Diagnostic, ProofRejection: Suggests problem—invalid instruments mis-specification.rejection: Implies detected evidence invalidity—proof validity.Analogy: rejecting J-test null like blood test detect virus. guarantee patient healthy; test may insensitive sample might small.Contextual Evaluation KeySubstantive/Theoretical Knowledge: Instrument validity ultimately hinges whether can justify \\(Z\\) uncorrelated error term theory.J-test merely complementary, substitute compelling arguments instruments exogenous.Practical Implications RecommendationsDon’t Rely Solely J-Test\nUse screening tool, always provide theoretical institutional justification instrument exogeneity.\nUse screening tool, always provide theoretical institutional justification instrument exogeneity.Assess Instrument Strength Separately\nJ-test says nothing relevance.\nWeak instruments reduce power J-test.\nCheck first-stage \\(F\\)-statistics Kleibergen-Paap rk statistics.\nJ-test says nothing relevance.Weak instruments reduce power J-test.Check first-stage \\(F\\)-statistics Kleibergen-Paap rk statistics.Sensitivity Robustness Analysis\nTest different subsets instruments alternative specifications.\nPerform leave-one-analyses see whether dropping particular instrument changes conclusions.\nTest different subsets instruments alternative specifications.Perform leave-one-analyses see whether dropping particular instrument changes conclusions.Use Weak-Instrument-Robust Tests\nConsider Anderson-Rubin, Stock-Wright, Conditional Likelihood Ratio tests.\ncan remain valid robust presence weak instruments model misspecification.\nConsider Anderson-Rubin, Stock-Wright, Conditional Likelihood Ratio tests.can remain valid robust presence weak instruments model misspecification.Summary Table: Common Misinterpretations vs. RealityInterpretation Output:large (non-significant) J-test statistic (large p-value) means reject hypothesis \\(\\hat{u} = 0\\). prove instruments valid—suggests sample provide evidence validity.large (non-significant) J-test statistic (large p-value) means reject hypothesis \\(\\hat{u} = 0\\). prove instruments valid—suggests sample provide evidence validity.Always pair theory-based justifications \\(Z\\).Always pair theory-based justifications \\(Z\\).","code":"\n# Load packages\nlibrary(AER)  # Provides ivreg function\n\n# Simulate data for a small demonstration\nset.seed(42)\nn <- 500\nZ1 <- rnorm(n)\nZ2 <- rnorm(n)\nZ  <- cbind(Z1, Z2)\n\n# Construct a (potentially) weak first stage\nX <- 0.2 * Z1 + 0.1 * Z2 + rnorm(n)\nu <- rnorm(n)\nY <- 1.5 * X + u\n\n# Fit IV (overidentified) using both Z1 and Z2 as instruments\niv_model <- ivreg(Y ~ X | Z1 + Z2)\n\n# Summary with diagnostics, including Sargan-Hansen J-test\nsummary(iv_model, diagnostics = TRUE)\n#> \n#> Call:\n#> ivreg(formula = Y ~ X | Z1 + Z2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.62393 -0.68911 -0.01314  0.69803  3.53553 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02941    0.04579   0.642    0.521    \n#> X            1.47136    0.17654   8.335 7.63e-16 ***\n#> \n#> Diagnostic tests:\n#>                  df1 df2 statistic  p-value    \n#> Weak instruments   2 497    17.289 5.51e-08 ***\n#> Wu-Hausman         1 497     0.003    0.959    \n#> Sargan             1  NA     0.131    0.717    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.005 on 498 degrees of freedom\n#> Multiple R-Squared: 0.6794,  Adjusted R-squared: 0.6787 \n#> Wald test: 69.47 on 1 and 498 DF,  p-value: 7.628e-16\n\n# Interpretation:\n# - If the J-test p-value is large, do NOT conclude \"valid instruments.\"\n# - Check the first-stage F-stat or other measures of strength."},{"path":"sec-instrumental-variables.html","id":"j-test-rejects-even-with-valid-instruments-heterogeneous-treatment-effects","chapter":"34 Instrumental Variables","heading":"34.6.7.3 J-Test Rejects Even with Valid Instruments (Heterogeneous Treatment Effects)","text":"subtle point: J-test can reject even instruments truly exogenous treatment effect heterogeneous—.e., instrument identifies different local average treatment effect.Core Issue: Inconsistent LATEsThe J-test implicitly assumes single (homogeneous) treatment effect \\(\\beta\\).J-test implicitly assumes single (homogeneous) treatment effect \\(\\beta\\).different instruments identify different segments population, instrument can yield distinct causal effect.different instruments identify different segments population, instrument can yield distinct causal effect.discrepancy can trigger J-test rejection, instruments invalid, don’t agree single parameter value.discrepancy can trigger J-test rejection, instruments invalid, don’t agree single parameter value.Expected ResultsIV via Z1 yield estimate \\(\\approx 1.0\\).IV via Z1 yield estimate \\(\\approx 1.0\\).IV via Z2 yield \\(\\approx 2.0\\).IV via Z2 yield \\(\\approx 2.0\\).IV via Z1 Z2 together yields 2SLS estimate weighted average 1.0 2.0.IV via Z1 Z2 together yields 2SLS estimate weighted average 1.0 2.0.J-test often rejects data support single \\(\\beta\\) across instruments.J-test often rejects data support single \\(\\beta\\) across instruments.Key TakeawaysThe J-Test Assumes HomogeneityIf treatment effects vary subpopulation instrument induces treatment, J-test can reject even instruments exogenous.Rejection May Signal Heterogeneity, InvalidityThe test distinguish invalid exogeneity different underlying causal parameters.Practical ImplicationsBe aware [Local Average Treatment Effect] interpretation instruments.aware [Local Average Treatment Effect] interpretation instruments.multiple instruments target different “complier” groups, standard J-test lumps one homogenous \\(\\beta\\).multiple instruments target different “complier” groups, standard J-test lumps one homogenous \\(\\beta\\).Consider reporting separate IV estimates using methods explicitly account treatment-effect heterogeneity (e.g., Marginal Treatment Effects advanced frameworks).Consider reporting separate IV estimates using methods explicitly account treatment-effect heterogeneity (e.g., Marginal Treatment Effects advanced frameworks).","code":"\nlibrary(AER)\nset.seed(123)\n\nn  <- 5000\nZ1 <- rbinom(n, 1, 0.5)\nZ2 <- rbinom(n, 1, 0.5)\n\n# Assign \"complier type\" for illustration\n# (This is just one way to simulate different subpopulations responding differently.)\ncomplier_type <- ifelse(Z1 == 1 & Z2 == 0, \"Z1_only\",\n                 ifelse(Z2 == 1 & Z1 == 0, \"Z2_only\", \"Both\"))\n\n# True LATEs differ by instrument-induced compliance\nbeta_Z1 <- 1.0\nbeta_Z2 <- 2.0\n\n# Generate endogenous X with partial influence from Z1 and Z2\npropensity <- 0.2 + 0.5 * Z1 + 0.5 * Z2\nX <- rbinom(n, 1, propensity)\nu <- rnorm(n)\n\n# Outcome with heterogeneous effects\nY <- ifelse(complier_type == \"Z1_only\", beta_Z1 * X,\n     ifelse(complier_type == \"Z2_only\", beta_Z2 * X, 1.5 * X)) + u\n\ndf <- data.frame(Y, X, Z1, Z2)\n\n# IV using Z1 only\niv_Z1 <- ivreg(Y ~ X | Z1, data = df)\nsummary(iv_Z1)\n#> \n#> Call:\n#> ivreg(formula = Y ~ X | Z1, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -4.50464 -1.08224 -0.01943  1.08215  4.27937 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.1026     0.1101   10.01  < 2e-16 ***\n#> X            -0.5259     0.1977   -2.66  0.00785 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.466 on 3802 degrees of freedom\n#> Multiple R-Squared: -0.2743, Adjusted R-squared: -0.2746 \n#> Wald test: 7.076 on 1 and 3802 DF,  p-value: 0.007847\n\n# IV using Z2 only\niv_Z2 <- ivreg(Y ~ X | Z2, data = df)\nsummary(iv_Z2)\n#> \n#> Call:\n#> ivreg(formula = Y ~ X | Z2, data = df)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -4.5025384 -1.1458187  0.0002382  1.1747011  5.0622955 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -1.2145     0.1279  -9.499   <2e-16 ***\n#> X             3.7344     0.2306  16.195   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.533 on 3802 degrees of freedom\n#> Multiple R-Squared: -0.3948, Adjusted R-squared: -0.3952 \n#> Wald test: 262.3 on 1 and 3802 DF,  p-value: < 2.2e-16\n\n# Overidentified model (Z1 + Z2)\niv_both <- ivreg(Y ~ X | Z1 + Z2, data = df)\nsummary(iv_both, diagnostics = TRUE)\n#> \n#> Call:\n#> ivreg(formula = Y ~ X | Z1 + Z2, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.46090 -0.71661 -0.01045  0.71687  3.82014 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02763    0.04423   0.625    0.532    \n#> X            1.45057    0.07494  19.356   <2e-16 ***\n#> \n#> Diagnostic tests:\n#>                   df1  df2 statistic p-value    \n#> Weak instruments    2 3801   510.265  <2e-16 ***\n#> Wu-Hausman          1 3801     0.751   0.386    \n#> Sargan              1   NA   264.175  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.059 on 3802 degrees of freedom\n#> Multiple R-Squared: 0.3345,  Adjusted R-squared: 0.3343 \n#> Wald test: 374.7 on 1 and 3802 DF,  p-value: < 2.2e-16"},{"path":"sec-instrumental-variables.html","id":"cautions-in-iv","chapter":"34 Instrumental Variables","heading":"34.7 Cautions in IV","text":"","code":""},{"path":"sec-instrumental-variables.html","id":"negative-r2-in-iv","chapter":"34 Instrumental Variables","heading":"34.7.1 Negative \\(R^2\\) in IV","text":"IV estimation, particularly 2SLS 3SLS, common problematic encounter negative \\(R^2\\) values second stage regression. Unlike Ordinary Least Squares, \\(R^2\\) often used assess fit model, IV regression primary concern consistency unbiased estimation coefficients interest, goodness--fit.Look Instead \\(R^2\\) IV?Instrument Relevance (First-stage \\(F\\)-statistics, Partial \\(R^2\\))Weak Instrument Tests (Kleibergen-Paap, Anderson-Rubin tests)Validity Instruments (Overidentification tests like Sargan/Hansen J-test)Endogeneity Tests (Durbin-Wu-Hausman test endogeneity)Confidence Intervals Standard Errors, focusing inference \\(\\hat{\\beta}\\).Geometric IntuitionIn OLS, fitted values \\(\\hat{y}\\) orthogonal projection \\(y\\) onto column space \\(X\\).2SLS, \\(\\hat{y}\\) projection onto space spanned \\(Z\\), \\(X\\).result, angle \\(y\\) \\(\\hat{y}\\) may minimize residual variance, RSS can larger OLS.Recall formula coefficient determination (\\(R^2\\)) regression model:\\[\nR^2 = 1 - \\frac{RSS}{TSS} = \\frac{MSS}{TSS}\n\\]:\\(TSS\\) Total Sum Squares: \\[\nTSS = \\sum_{=1}^n (y_i - \\bar{y})^2\n\\]\\(MSS\\) Model Sum Squares: \\[\nMSS = \\sum_{=1}^n (\\hat{y}_i - \\bar{y})^2\n\\]\\(RSS\\) Residual Sum Squares: \\[\nRSS = \\sum_{=1}^n (y_i - \\hat{y}_i)^2\n\\]OLS, \\(R^2\\) measures proportion variance \\(Y\\) explained regressors \\(X\\).Key Properties OLS:\\(R^2 \\[0, 1]\\)Adding regressors (even irrelevant ones) never decreases \\(R^2\\).\\(R^2\\) measures -sample goodness--fit, causal interpretation.","code":""},{"path":"sec-instrumental-variables.html","id":"why-does-r2-lose-its-meaning-in-iv-regression","chapter":"34 Instrumental Variables","heading":"34.7.1.1 Why Does \\(R^2\\) Lose Its Meaning in IV Regression?","text":"IV regression, second stage regression replaces endogenous variable \\(X_2\\) predicted values first stage:Stage 1:\\[\nX_2 = Z \\pi + v\n\\]Stage 2:\\[\nY = X_1 \\beta_1 + \\hat{X}_2 \\beta_2 + \\epsilon\n\\]\\(\\hat{X}_2\\) observed \\(X_2\\), proxy constructed \\(Z\\).\\(\\hat{X}_2\\) isolates exogenous variation \\(X_2\\) independent \\(\\epsilon\\).reduces bias, comes cost:\nvariation \\(\\hat{X}_2\\) typically less \\(X_2\\).\npredicted values \\(\\hat{y}_i\\) second stage necessarily close \\(y_i\\).\nvariation \\(\\hat{X}_2\\) typically less \\(X_2\\).predicted values \\(\\hat{y}_i\\) second stage necessarily close \\(y_i\\).","code":""},{"path":"sec-instrumental-variables.html","id":"why-r2-can-be-negative","chapter":"34 Instrumental Variables","heading":"34.7.1.2 Why \\(R^2\\) Can Be Negative:","text":"\\(R^2\\) calculated using: \\[\nR^2 = 1 - \\frac{RSS}{TSS}\n\\] IV:predicted values \\(Y\\) chosen minimize RSS, IV minimizing residuals second stage.Unlike OLS, 2SLS chooses \\(\\hat{\\beta}\\) satisfy moment conditions rather minimizing sum squared errors.possible (common IV) residual sum squares greater total sum squares: \\[\nRSS > TSS\n\\] makes: \\[\nR^2 = 1 - \\frac{RSS}{TSS} < 0\n\\]possible (common IV) residual sum squares greater total sum squares: \\[\nRSS > TSS\n\\] makes: \\[\nR^2 = 1 - \\frac{RSS}{TSS} < 0\n\\]happens :\npredicted values \\(\\hat{y}_i\\) IV optimized fit observed \\(y_i\\).\nresiduals can larger, IV focuses identifying causal effects, prediction.\nhappens :predicted values \\(\\hat{y}_i\\) IV optimized fit observed \\(y_i\\).residuals can larger, IV focuses identifying causal effects, prediction.example, assume :\\(TSS = 100\\)\\(TSS = 100\\)\\(RSS = 120\\)\\(RSS = 120\\): \\[ R^2 = 1 - \\frac{120}{100} = -0.20 \\]happens IV procedure minimize RSS. prioritizes solving endogeneity problem explaining variance \\(Y\\).","code":""},{"path":"sec-instrumental-variables.html","id":"why-we-dont-care-about-r2-in-iv","chapter":"34 Instrumental Variables","heading":"34.7.1.3 Why We Don’t Care About \\(R^2\\) in IV","text":"IV Estimates Focus Consistency, PredictionThe goal IV obtain consistent estimate \\(\\beta_2\\).IV sacrifices fit (higher variance \\(\\hat{y}_i\\)) remove endogeneity bias.\\(R^2\\) Reflect Quality IV EstimatorA high \\(R^2\\) IV may misleading (instance, instruments weak invalid).negative \\(R^2\\) imply bad IV estimator assumptions instrument validity met.IV Regression Identification, -Sample FitIV relies relevance exogeneity instruments, residual minimization.","code":""},{"path":"sec-instrumental-variables.html","id":"technical-details-on-r2","chapter":"34 Instrumental Variables","heading":"34.7.1.4 Technical Details on \\(R^2\\)","text":"OLS: \\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1} X'Y\n\\] Minimizes: \\[\nRSS = (Y - X \\hat{\\beta}^{OLS})'(Y - X \\hat{\\beta}^{OLS})\n\\]IV: \\[\n\\hat{\\beta}^{IV} = (X'P_Z X)^{-1} X'P_Z Y\n\\]:\\(P_Z = Z (Z'Z)^{-1} Z'\\) projection matrix onto \\(Z\\).\\(P_Z = Z (Z'Z)^{-1} Z'\\) projection matrix onto \\(Z\\).IV estimator solves: \\[\nZ'(Y - X\\hat{\\beta}) = 0\n\\]IV estimator solves: \\[\nZ'(Y - X\\hat{\\beta}) = 0\n\\]guarantee minimizes RSS.guarantee minimizes RSS.Residuals:\\[\ne^{IV} = Y - X \\hat{\\beta}^{IV}\n\\]norm \\(e^{IV}\\) typically larger OLS IV uses fewer effective degrees freedom (constrained variation via \\(Z\\)).Note \\(R^2\\) 3SLS GMMIn 3SLS GMM IV, \\(R^2\\) can similarly misleading.methods often operate moment conditions system estimation, residual minimization.","code":""},{"path":"sec-instrumental-variables.html","id":"sec-many-instruments-bias","chapter":"34 Instrumental Variables","heading":"34.7.2 Many-Instruments Bias","text":"IV powerful, also delicate. One critical issue arises many-instruments problem, also known many-IV bias.Consider structural model:\\(y_i = \\beta x_i + u_i\\)\\(x_i\\) endogenous: \\(\\mathbb{E}[x_i u_i] \\neq 0\\). address , introduce instruments \\(z_i\\) :Relevance: \\(\\mathbb{E}[z_i x_i] \\neq 0\\)Exogeneity: \\(\\mathbb{E}[z_i u_i] = 0\\)standard 2SLS estimator given :\\(\\hat{\\beta}_{2SLS} = (X'P_ZX)^{-1} X'P_Zy\\)\\(P_Z = Z(Z'Z)^{-1}Z'\\) projection matrix onto column space \\(Z\\).many-IV problem arises number instruments \\(L\\) large relative number observations \\(n\\). particular, issue becomes severe \\(L/n \\\\alpha > 0\\), leading several problems:Bias Toward OLS: 2SLS estimator becomes increasingly biased toward OLS estimator.Overfitting: first-stage regression overfits endogenous variable, capturing noise rather true variation.Inflated Variance: second-stage estimates become imprecise, leading misleading inference.Traditional IV asymptotics assume \\(L\\) fixed \\(n \\\\infty\\). Bekker (1994) proposed alternative framework :\\(L/n \\\\alpha \\(0, \\infty) \\quad \\text{} n \\\\infty\\)Bekker asymptotics:2SLS biased inconsistent unless instruments strong.bias grows \\(\\alpha\\), approaching OLS.formalized intuition adding instruments—especially weak ones—help, can actually harm estimation.","code":""},{"path":"sec-instrumental-variables.html","id":"sources-of-many-iv-bias","chapter":"34 Instrumental Variables","heading":"34.7.2.1 Sources of Many-IV Bias","text":"Weak InstrumentsMany instruments individually weak (.e., contribute little explaining \\(x\\)), collectively, inflate projection without improving identification.Overfitting First StageWith many instruments, first-stage regression captures random noise, leading poor --sample performance contamination second stage.Endogeneity LeakageOverfit first-stage predictions may reintroduce endogeneity due incidental correlation structural error term.","code":""},{"path":"sec-instrumental-variables.html","id":"diagnostic-tools","chapter":"34 Instrumental Variables","heading":"34.7.2.2 Diagnostic Tools","text":"First-Stage F-StatisticA weak instrument test: rule thumb \\(F > 10\\) single instrument; stringent thresholds apply many instruments.Overidentification TestsSargan Test: Assumes homoskedastic errorsHansen’s J-Test: Robust heteroskedasticityEigenvalue DiagnosticsKleibergen-Paap rk statistic (generalized clustered heteroskedastic settings)","code":""},{"path":"sec-instrumental-variables.html","id":"remedies-and-alternatives","chapter":"34 Instrumental Variables","heading":"34.7.2.3 Remedies and Alternatives","text":"Instrument SelectionLasso IV / Post-Double Selection: Use regularization select valid instruments.Factor-Based Methods: Project instruments onto principal components.Shrinkage EstimatorsLimited Information Maximum Likelihood (LIML): robust many-IV bias.Jackknife IV Estimator (JIVE): Adjusts overfitting bias first stage.Grouped Aggregated InstrumentsCollapse multiple instruments smaller number aggregated measures.","code":""},{"path":"sec-instrumental-variables.html","id":"practical-guidelines","chapter":"34 Instrumental Variables","heading":"34.7.2.4 Practical Guidelines","text":"Avoid Including Possible Instruments: Parsimony matters volume.Always Check First-Stage Strength: Even \\(R^2\\) high, individual instrument strength matters.Report Robustness Alternative Estimators: LIML JIVE can serve robustness checks.Test Overidentification: interpret results cautiously \\(L\\) large.","code":""},{"path":"sec-instrumental-variables.html","id":"heterogeneous-effects-in-iv-estimation","chapter":"34 Instrumental Variables","heading":"34.7.3 Heterogeneous Effects in IV Estimation","text":"","code":""},{"path":"sec-instrumental-variables.html","id":"constant-vs.-heterogeneous-treatment-effects","chapter":"34 Instrumental Variables","heading":"34.7.3.1 Constant vs. Heterogeneous Treatment Effects","text":"standard instrumental variables framework assumes causal effect endogenous regressor \\(D_i\\) outcome \\(Y_i\\) constant across individuals, .e.:\\[\nY_i = \\beta_0 + \\beta_1 D_i + u_i\n\\]implies homogeneous treatment effects, \\(\\beta_1\\) structural parameter applies uniformly individuals \\(\\) population. refer homogeneous treatment effects model, underlies traditional IV assumptions:Linearity constant effect \\(\\beta_1\\).Linearity constant effect \\(\\beta_1\\).Instrument relevance: \\(\\mathrm{Cov}(Z_i, D_i) \\ne 0\\).Instrument relevance: \\(\\mathrm{Cov}(Z_i, D_i) \\ne 0\\).Instrument exogeneity: \\(\\mathrm{Cov}(Z_i, u_i) = 0\\).Instrument exogeneity: \\(\\mathrm{Cov}(Z_i, u_i) = 0\\).assumptions, IV estimator \\(\\hat{\\beta}_1^{IV}\\) consistently estimates causal effect \\(\\beta_1\\).","code":""},{"path":"sec-instrumental-variables.html","id":"heterogeneous-treatment-effects-and-the-problem-for-iv","chapter":"34 Instrumental Variables","heading":"34.7.3.2 Heterogeneous Treatment Effects and the Problem for IV","text":"practice, treatment effects often vary across individuals. , effect \\(D_i\\) \\(Y_i\\) depends individual’s characteristics unobserved factors:\\[\nY_i = \\beta_{1i} D_i + u_i\n\\], \\(\\beta_{1i}\\) represents individual-specific causal effect, population Average Treatment Effect :\\[\nATE = \\mathbb{E}[\\beta_{1i}]\n\\]presence treatment effect heterogeneity, IV estimator \\(\\hat{\\beta}_1^{IV}\\) , general, estimate ATE. Instead, estimates weighted average heterogeneous treatment effects, weights determined instrumental variation data.distinction critical:OLS estimates weighted average treatment effect, weights depending variance \\(D_i\\).OLS estimates weighted average treatment effect, weights depending variance \\(D_i\\).IV estimates [Local Average Treatment Effect] (LATE), depending instrument \\(Z_i\\).IV estimates [Local Average Treatment Effect] (LATE), depending instrument \\(Z_i\\).one endogenous regressor \\(D_i\\) one instrument \\(Z_i\\), binary variables, can interpret IV estimator [Local Average Treatment Effect] specific assumptions. setup :\\[\nY_i = \\beta_0 + \\beta_{1i} D_i + u_i\n\\]\\(D_i \\\\{0, 1\\}\\): treatment indicator.\\(Z_i \\\\{0, 1\\}\\): binary instrument.Assumptions LATE InterpretationInstrument Exogeneity\\[\nZ_i \\perp (u_i, v_i)\n\\] - instrument good randomly assigned, independent structural error term \\(u_i\\) unobserved determinants \\(v_i\\) affect treatment selection.Relevance\\[\n\\mathbb{P}(D_i = 1 | Z_i = 1) \\ne \\mathbb{P}(D_i = 1 | Z_i = 0)\n\\] - instrument must affect likelihood receiving treatment \\(D_i\\).Monotonicity (G. W. Imbens Angrist 1994)\\[\nD_i(1) \\ge D_i(0) \\quad \\forall \n\\]defiers: individual takes treatment \\(Z_i = 0\\) take \\(Z_i = 1\\).defiers: individual takes treatment \\(Z_i = 0\\) take \\(Z_i = 1\\).Monotonicity testable, must defended theoretical grounds.Monotonicity testable, must defended theoretical grounds.assumptions, \\(\\hat{\\beta}_1^{IV}\\) estimates [Local Average Treatment Effect]:\\[\nLATE = \\mathbb{E}[\\beta_{1i} | \\text{Compliers}]\n\\]Compliers individuals receive treatment \\(Z_i = 1\\), \\(Z_i = 0\\).Local refers fact estimate pertains specific subpopulation compliers.Implications:LATE ATE, unless treatment effects homogeneous, complier subpopulation representative entire population.Different instruments define different complier groups, leading different LATEs.","code":""},{"path":"sec-instrumental-variables.html","id":"multiple-instruments-and-multiple-lates","chapter":"34 Instrumental Variables","heading":"34.7.3.3 Multiple Instruments and Multiple LATEs","text":"multiple instruments \\(Z_i^{(1)}, Z_i^{(2)}, \\dots, Z_i^{(m)}\\), can induce different complier groups:instrument LATE, corresponding group compliers.instrument LATE, corresponding group compliers.heterogeneous treatment effects exist, LATEs may differ.heterogeneous treatment effects exist, LATEs may differ.overidentified model, \\(m > k\\), 2SLS estimator imposes assumption instruments identify causal effect \\(\\beta_1\\). leads moment conditions:\\[\n\\mathbb{E}[Z_i^{(j)}(Y_i - D_i \\beta_1)] = 0 \\quad \\forall j = 1, \\dots, m\n\\]instruments identify different LATEs:moment conditions can inconsistent one another.moment conditions can inconsistent one another.Sargan-Hansen J-test may reject, even though instrument valid (.e., exogenous relevant).Sargan-Hansen J-test may reject, even though instrument valid (.e., exogenous relevant).Key Insight: J-test rejects homogeneity assumption violated—instruments invalid exogeneity sense.","code":""},{"path":"sec-instrumental-variables.html","id":"illustration-multiple-instruments-different-lates","chapter":"34 Instrumental Variables","heading":"34.7.3.4 Illustration: Multiple Instruments, Different LATEs","text":"Consider following example:\\(Z_i^{(1)}\\) identifies LATE 1.0.\\(Z_i^{(1)}\\) identifies LATE 1.0.\\(Z_i^{(2)}\\) identifies LATE 2.0.\\(Z_i^{(2)}\\) identifies LATE 2.0.instruments included overidentified IV model, 2SLS estimator tries reconcile LATEs identifying \\(\\beta_1\\), leading :\naverage LATEs.\npossible rejection overidentification restrictions via J-test.\ninstruments included overidentified IV model, 2SLS estimator tries reconcile LATEs identifying \\(\\beta_1\\), leading :average LATEs.average LATEs.possible rejection overidentification restrictions via J-test.possible rejection overidentification restrictions via J-test.scenario common :Labor economics (e.g., different instruments education identify different populations).Labor economics (e.g., different instruments education identify different populations).Marketing pricing experiments (e.g., different price instruments impact different customer segments).Marketing pricing experiments (e.g., different price instruments impact different customer segments).","code":""},{"path":"sec-instrumental-variables.html","id":"practical-implications-for-empirical-research","chapter":"34 Instrumental Variables","heading":"34.7.3.5 Practical Implications for Empirical Research","text":"Clear Whose Effect ’re EstimatingDifferent instruments often imply different complier groups.Understanding compliers essential policy implications.Interpret J-Test CarefullyA rejection may indicate treatment effect heterogeneity, necessarily instrument invalidity.Supplement J-test :\nSubgroup analysis.\nSensitivity analysis.\nLocal Instrumental Variable Marginal Treatment Effects frameworks.\nSubgroup analysis.Sensitivity analysis.Local Instrumental Variable Marginal Treatment Effects frameworks.Use Structural Models NeededIf need ATE, consider parametric semi-parametric structural models explicitly model heterogeneity.Don’t Assume LATE = ATEBe cautious generalizing LATE estimates beyond complier subpopulation.","code":""},{"path":"sec-instrumental-variables.html","id":"beyond-late","chapter":"34 Instrumental Variables","heading":"34.7.3.6 Beyond LATE","text":"presence heterogeneous treatment effects (\\(\\beta_{1i}\\) varying across individuals) raises fundamental challenge causal inference using IV methods. seen, traditional IV estimator identifies [Local Average Treatment Effect] (LATE) certain assumptions. However, approach implicitly adopts reverse engineering strategy: uses classical linear IV estimators designed homogeneity, acknowledges likely misspecification presence unobserved heterogeneity, interprets resulting estimate terms LATE.strategy highly influential remains central empirical work. Nevertheless, comes limitations:interpretation depends critically specific instrument used (.e., definition complier group).recover Average Treatment Effect (ATE) policy-relevant parameters unless strong additional assumptions hold.","code":""},{"path":"sec-instrumental-variables.html","id":"forward-engineering-the-marginal-treatment-effect","chapter":"34 Instrumental Variables","heading":"34.7.3.7 Forward Engineering: The Marginal Treatment Effect","text":"contrast, recent work—including Mogstad Torgovitsky (2024) —emphasizes forward engineering approach. Rather adapting estimators designed homogeneity, strategy builds models estimators explicitly allow unobserved heterogeneity treatment effects outset.key framework approach Marginal Treatment Effect (MTE), originally developed context selection models (Gronau 1974; J. J. Heckman 1979). idea model treatment decision result latent index:\\[\nD_i = \\mathbb{1}[v_i \\leq Z_i'\\pi + \\eta_i]\n\\]let treatment effect vary unobserved selection variables. MTE defined :\\[\n\\text{MTE}(u) = \\mathbb{E}[Y_i(1) - Y_i(0) | U_i = u]\n\\]\\(U_i\\) latent variable governing treatment selection. function traces treatment effect varies across individuals different propensities receive treatment, underlies average effects :ATE: \\(\\int_0^1 \\text{MTE}(u) \\, du\\)LATE: average MTE complier marginTT TUT: weighted averages MTEComparison IV, LATE, MTE ApproachesThe MTE framework also connects :Control function methods: account selection via inclusion latent variables (e.g., residuals) outcome equations.Partial identification / bounding methods: avoid strong parametric assumptions seek informative bounds treatment effects.newer strategies reflect shift modern econometrics: away treating unobserved heterogeneity nuisance, toward modeling directly richer causal inference.Understanding two strategies helps practitioners choose appropriate methods based :identifying assumptions.richness instruments.target estimand (e.g., ATE, LATE, MTE).willingness model selection process.Researchers cautious interpreting IV estimates general causal effects, especially heterogeneous treatment effects likely choice instrument strongly influences complier population.","code":""},{"path":"sec-instrumental-variables.html","id":"zero-valued-outcomes","chapter":"34 Instrumental Variables","heading":"34.7.4 Zero-Valued Outcomes","text":"outcomes take zero values, log transformations can introduce interpretation issues. Specifically, coefficient log-transformed outcome directly represent percentage change (J. Chen Roth 2024). distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change. cases, researchers use alternative methods:","code":""},{"path":"sec-instrumental-variables.html","id":"proportional-late-estimation","chapter":"34 Instrumental Variables","heading":"34.7.4.1 Proportional LATE Estimation","text":"dealing zero-valued outcomes, direct log transformations can lead interpretation issues. obtain interpretable percentage change outcome due treatment among compliers, estimate proportional Local Average Treatment Effect (LATE), denoted \\(\\theta_{ATE\\%}\\).Steps Estimate Proportional LATE:Estimate LATE using 2SLS:\nfirst estimate treatment effect using standard Two-Stage Least Squares regression: \\[ Y_i = \\beta D_i + X_i + \\epsilon_i, \\] :\n\\(D_i\\) endogenous treatment variable.\n\\(X_i\\) includes exogenous controls.\n\\(\\beta\\) represents LATE levels mean control group’s compliers.\nEstimate LATE using 2SLS:first estimate treatment effect using standard Two-Stage Least Squares regression: \\[ Y_i = \\beta D_i + X_i + \\epsilon_i, \\] :\\(D_i\\) endogenous treatment variable.\\(X_i\\) includes exogenous controls.\\(\\beta\\) represents LATE levels mean control group’s compliers.Estimate control complier mean (\\(\\beta_{cc}\\)):\nUsing 2SLS setup, estimate control mean compliers transforming outcome variable (Abadie, Angrist, Imbens 2002): \\[ Y_i^{CC} = -(D_i - 1) Y_i. \\] estimated coefficient regression, \\(\\beta_{cc}\\), captures mean outcome compliers control group.Estimate control complier mean (\\(\\beta_{cc}\\)):Using 2SLS setup, estimate control mean compliers transforming outcome variable (Abadie, Angrist, Imbens 2002): \\[ Y_i^{CC} = -(D_i - 1) Y_i. \\] estimated coefficient regression, \\(\\beta_{cc}\\), captures mean outcome compliers control group.Compute proportional LATE:\nestimated proportional LATE given : \\[ \\theta_{ATE\\%} = \\frac{\\hat{\\beta}}{\\hat{\\beta}_{cc}}, \\] provides direct percentage change interpretation outcome among compliers induced instrument.Compute proportional LATE:estimated proportional LATE given : \\[ \\theta_{ATE\\%} = \\frac{\\hat{\\beta}}{\\hat{\\beta}_{cc}}, \\] provides direct percentage change interpretation outcome among compliers induced instrument.Obtain standard errors via non-parametric bootstrap:\nSince \\(\\theta_{ATE\\%}\\) ratio estimated coefficients, standard errors best obtained using non-parametric bootstrap methods.Obtain standard errors via non-parametric bootstrap:Since \\(\\theta_{ATE\\%}\\) ratio estimated coefficients, standard errors best obtained using non-parametric bootstrap methods.Special case: Binary instrument\ninstrument binary, \\(\\theta_{ATE\\%}\\) intensive margin compliers can directly estimated using Poisson IV regression (ivpoisson Stata).Special case: Binary instrumentIf instrument binary, \\(\\theta_{ATE\\%}\\) intensive margin compliers can directly estimated using Poisson IV regression (ivpoisson Stata).","code":""},{"path":"sec-instrumental-variables.html","id":"bounds-on-intensive-margin-effects","chapter":"34 Instrumental Variables","heading":"34.7.4.2 Bounds on Intensive-Margin Effects","text":"Lee (2009) proposed bounding approach intensive-margin effects, assuming compliers always positive outcomes regardless treatment (.e., intensive-margin effect). bounds help estimate treatment effects without relying log transformations. However, requires monotonicity assumption compliers still positive outcome regardless treatment status.","code":""},{"path":"sec-instrumental-variables.html","id":"types-of-iv","chapter":"34 Instrumental Variables","heading":"34.8 Types of IV","text":"","code":""},{"path":"sec-instrumental-variables.html","id":"treatment-intensity","chapter":"34 Instrumental Variables","heading":"34.8.1 Treatment Intensity","text":"Two-Stage Least Squares powerful method estimating average causal effect treatment intensity varies across units. Rather simple binary treatment (treated vs. untreated), many empirical applications involve treatments can take range values. TSLS can identify causal effects settings, capturing “weighted average per-unit treatment effects along length causal response function” (J. D. Angrist Imbens 1995, 431).Common examples treatment intensity include:Drug dosage (e.g., milligrams administered)Hours exam preparation test performance (Powers Swinton 1984)Cigarette consumption (e.g., packs per day) infant birth weights (Permutt Hebel 1989)Years education effect earningsClass size impact student test scores (J. D. Angrist Lavy 1999)Sibship size later-life earnings (J. Angrist, Lavy, Schlosser 2010)Social media adoption intensity (e.g., time spent, number platforms)average causal effect refers conditional expectation difference outcomes treated unit (given treatment intensity) happened counterfactual scenario (different treatment intensity). Importantly:Linearity required relationships dependent variable, treatment intensities, instruments. TSLS can accommodate nonlinear causal response functions, provided assumptions method hold.","code":""},{"path":"sec-instrumental-variables.html","id":"example-schooling-and-earnings","chapter":"34 Instrumental Variables","heading":"34.8.1.1 Example: Schooling and Earnings","text":"seminal paper, J. D. Angrist Imbens (1995) estimate causal effect years schooling earnings, using quarter birth instrumental variable. intuition individuals born different quarters subject different compulsory schooling laws, affect educational attainment plausibly unrelated unobserved ability motivation (typical omitted variables context).structural outcome equation :\\[\nY = \\gamma_0 + \\gamma_1 X_1 + \\rho S + \\varepsilon\n\\]:\\(Y\\) log earnings (dependent variable)\\(S\\) years schooling (endogenous regressor)\\(X_1\\) vector (matrix) exogenous covariates (e.g., demographic characteristics)\\(\\rho\\) causal return schooling wish estimate\\(\\varepsilon\\) error term, capturing unobserved factorsBecause schooling \\(S\\) may endogenous (e.g., correlated \\(\\varepsilon\\)), model first-stage relationship exogenous variables instruments:\\[\nS = \\delta_0 + X_1 \\delta_1 + X_2 \\delta_2 + \\eta\n\\]:\\(X_2\\) represents instrumental variables (e.g., quarter birth)\\(\\delta_2\\) coefficient instrument\\(\\eta\\) first-stage error termThe Two-Stage ProcedureFirst Stage Regression\nRegress \\(S\\) \\(X_1\\) \\(X_2\\) obtain predicted (fitted) values \\(\\hat{S}\\).\\[\n\\hat{S} = \\widehat{\\delta_0} + X_1 \\widehat{\\delta_1} + X_2 \\widehat{\\delta_2}\n\\]Second Stage Regression\nReplace \\(S\\) \\(\\hat{S}\\) structural equation estimate:\\[\nY = \\gamma_0 + \\gamma_1 X_1 + \\rho \\hat{S} + \\nu\n\\]\\(\\nu\\) new error term (different \\(\\varepsilon\\) \\(\\hat{S}\\) constructed exogenous).standard IV assumptions, \\(\\rho\\) consistent estimator causal effect schooling earnings.","code":""},{"path":"sec-instrumental-variables.html","id":"causal-interpretation-of-rho","chapter":"34 Instrumental Variables","heading":"34.8.1.2 Causal Interpretation of \\(\\rho\\)","text":"\\(\\rho\\) valid causal interpretation, two key assumptions essential:SUTVA (Stable Unit Treatment Value Assumption)\npotential outcomes individual affected treatment assignments units.\nhidden variations treatment; “one year schooling” means treatment type across individuals.\nimportant, SUTVA often assumed without extensive defense empirical work, though violations (e.g., spillovers education settings) acknowledged plausible.\npotential outcomes individual affected treatment assignments units.hidden variations treatment; “one year schooling” means treatment type across individuals.important, SUTVA often assumed without extensive defense empirical work, though violations (e.g., spillovers education settings) acknowledged plausible.[Local Average Treatment Effect] (LATE)\nTSLS identifies weighted average marginal effects points instrument induces variation treatment intensity.\nFormally, \\(\\rho\\) converges probability weighted average causal increments:\nTSLS identifies weighted average marginal effects points instrument induces variation treatment intensity.Formally, \\(\\rho\\) converges probability weighted average causal increments:\\[\n\\text{plim } \\hat{\\rho} = \\sum_j w_j \\cdot E[Y_j - Y_{j-1} \\mid \\text{Compliers level } j]\n\\]\\(w_j\\) weights determined distribution instrument treatment intensity.LATE interpretation means TSLS estimates apply compliers whose treatment intensity changes response instrument. heterogeneity treatment effects across units, interpretation \\(\\rho\\) becomes instrument-dependent may generalize entire population.","code":""},{"path":"sec-instrumental-variables.html","id":"decision-maker-iv","chapter":"34 Instrumental Variables","heading":"34.8.2 Decision-Maker IV","text":"Examiner designs, judge IV designs, leniency IV refer family instrumental variable strategies exploit quasi-random assignment decision-makers (judges examiners) observational units. designs used identify causal effects settings controlled experiments feasible.Examiner/judge IV design approach instrument identity behavior assigned decision-maker (“examiner” judge). classic setup arises courts: cases typically assigned judges manner good random (often conditional timing location), different judges systematically different propensities rule harshly leniently. means , purely luck draw, otherwise-similar individuals may receive different treatments (e.g. longer vs. shorter sentence) depending judge happen get. design, judge assignment (function ) serves instrumental variable treatment interest (like sentence length). key insight examiner/judge can treated exogenous shock influences treatment (ideally) unrelated person’s characteristics.term judge IV design specifically refers using judges legal settings instruments. approach rose prominence studies criminal justice system; well-known early example Kling (2006), used randomly assigned judges instrument incarceration length studying effect later earnings. generally, literature often calls “judge leniency” design, leverages differences judges’ leniency/harshness. Importantly, idea extends beyond literal judges. Examiners various administrative medical contexts can play analogous role. instance, bureaucrats evaluating benefit claims, patent examiners reviewing applications, physicians making discretionary treatment decisions can act like “judges” whose assignment -good--random whose leniency varies. non-court contexts, researchers sometimes use term examiner design general label, essentially IV strategy. summary, whether say examiner design, judge IV, judge leniency IV, usually referring identification strategy – using quasi-random assignment decision-maker varying tendencies instrument.design structured: practice, one can implement IV couple ways. One method include dummy variables judge/examiner instruments (since judge distinct source variation). Another common approach construct leniency measure decision-maker – example, judge’s historical rate granting treatment – use single continuous instrument. latter approach (using summary measure leniency) popular reduces dimensionality mitigates weak-instrument concerns many judges. instance, instead 50 separate judge dummies, one can calculate judge’s leave-one-approval sentencing rate use number instrument. “leave-one-” jackknife approach ensures measure judge calculated excluding case question (avoiding mechanical endogeneity). Overall, examiner/judge IV design turns naturally occurring randomness examiner assignment source exogenous variation: randomly assigned becomes instrument treatment received.","code":""},{"path":"sec-instrumental-variables.html","id":"achieving-identification-with-a-leniency-iv","chapter":"34 Instrumental Variables","heading":"34.8.2.1 Achieving Identification with a Leniency IV","text":"examiner/judge design powerful way achieve identification observational data. rests core requirements valid instrumental variable:Quasi-Random Assignment (Exogeneity): examiners judges assigned cases essentially random (often rotation, scheduling, lottery), particular decision-maker individual gets independent individual’s characteristics. approximates randomness experiment. long assignment truly random (-good--random conditioning known factors like time location), examiner identity uncorrelated unobserved confounders. words, judge draw direct bearing outcome except judge’s decision. satisfies exogeneity condition IV.\nDiscretion Binary Treatment: decision maker discretionary authority treatment variable \\(D_i\\), typically binary (e.g., pretrial release vs. detention).\nHeterogeneity Behavior: Decision makers differ systematically propensity assign treatment, allowing us use differences instrumental variation.\nQuasi-Random Assignment (Exogeneity): examiners judges assigned cases essentially random (often rotation, scheduling, lottery), particular decision-maker individual gets independent individual’s characteristics. approximates randomness experiment. long assignment truly random (-good--random conditioning known factors like time location), examiner identity uncorrelated unobserved confounders. words, judge draw direct bearing outcome except judge’s decision. satisfies exogeneity condition IV.Discretion Binary Treatment: decision maker discretionary authority treatment variable \\(D_i\\), typically binary (e.g., pretrial release vs. detention).Discretion Binary Treatment: decision maker discretionary authority treatment variable \\(D_i\\), typically binary (e.g., pretrial release vs. detention).Heterogeneity Behavior: Decision makers differ systematically propensity assign treatment, allowing us use differences instrumental variation.Heterogeneity Behavior: Decision makers differ systematically propensity assign treatment, allowing us use differences instrumental variation.Instrument Relevance: Different examiners different propensities deliver treatment. judges severe (likely incarcerate give long sentences), others lenient; doctors likely prescribe intensive treatment, etc. translates substantial variation probability treatment based solely case assigned . example, patent context, assigned lenient patent examiner vs. strict one can significantly change probability patent grant (Farre-Mensa, Hegde, Ljungqvist 2020).Instrument Relevance: Different examiners different propensities deliver treatment. judges severe (likely incarcerate give long sentences), others lenient; doctors likely prescribe intensive treatment, etc. translates substantial variation probability treatment based solely case assigned . example, patent context, assigned lenient patent examiner vs. strict one can significantly change probability patent grant (Farre-Mensa, Hegde, Ljungqvist 2020).Exclusion Restriction: IV assumption assigned examiner affects outcome treatment . judge design, means “type judge assigned” impact defendant’s future outcomes solely via judge’s decision (e.g. incarceration release), channel. instance, harsh judge might send prison; lenient judge might – difference can affect future, assume ’s incarceration matters future outcome, direct effect interacting harsh vs. nice judge per se. exclusion restriction plausible decision-maker direct interaction individual beyond making decision. Researchers take care argue conditional controls treatment , identity examiner independent effect outcomes. conditions (relevance exogeneity/exclusion) hold, variation treatment induced examiner assignment can used consistently estimate causal effect treatment.Exclusion Restriction: IV assumption assigned examiner affects outcome treatment . judge design, means “type judge assigned” impact defendant’s future outcomes solely via judge’s decision (e.g. incarceration release), channel. instance, harsh judge might send prison; lenient judge might – difference can affect future, assume ’s incarceration matters future outcome, direct effect interacting harsh vs. nice judge per se. exclusion restriction plausible decision-maker direct interaction individual beyond making decision. Researchers take care argue conditional controls treatment , identity examiner independent effect outcomes. conditions (relevance exogeneity/exclusion) hold, variation treatment induced examiner assignment can used consistently estimate causal effect treatment.meeting conditions, examiner/judge IV designs create natural experiment. Essentially, compare outcomes individuals , random luck, received different treatment assignments (e.g. one incarcerated, another ) due differing examiner leniency, despite individuals comparable expectation. helps isolate causal impact treatment confounding factors. Notably, estimates designs often correspond local average treatment effect (LATE) cases whose treatment status swayed examiner’s leniency – example, “marginal” defendants incarcerated strict judge released lenient judge. sum, designs allow researchers mimic randomized experiment within observational data leveraging institutional randomness (gets assigned ) instrument.","code":""},{"path":"sec-instrumental-variables.html","id":"leniency-iv-clarifying-the-terminology","chapter":"34 Instrumental Variables","heading":"34.8.2.2 Leniency IV: Clarifying the Terminology","text":"term leniency IV refers instrumental variable strategy, emphasizing role examiner’s leniency (strictness). many studies, instrument literally measure lenient assigned judge examiner tends . example, Social Security Disability study, researchers “exploit variation examiners’ allowance rates instrument benefit receipt.” (Maestas, Mullen, Strand 2013). , examiner’s allowance rate (fraction cases approve) direct quantification leniency, serves instrumental variable. Similarly, one can define judge’s leniency percentage past defendants judge jailed average sentence length give, use IV. phrase “leniency design” leniency instrument simply underscores ’s lenient vs. strict tendencies decision-maker provide exogenous variation.leniency IV design typically involves constructing instrument like “leave-mean decision rate assigned examiner.” , instance, fraction previous similar cases examiner approved (excluding current case). number captures lenient strict generally . assignment random, individuals get high-leniency examiner others low-leniency examiner, creating exogenous variation treatment. comparing outcomes across , one can identify causal effect treatment. term “leniency” highlights ’s discretionary toughness examiner ’re leveraging.","code":""},{"path":"sec-instrumental-variables.html","id":"examples-in-economics","chapter":"34 Instrumental Variables","heading":"34.8.2.3 Examples in Economics","text":"Many influential studies across economics related fields employed examiner judge IV designs answer causal questions. several prominent examples illustrating range applications findings:Criminal Sentencing Recidivism: seminal study, Kling (2006) examined effect incarceration length ex-prisoners’ labor market outcomes. used random assignment judges instrument, capitalizing fact judges harsher (give longer sentences) others lenient. judge IV strategy since used extensively study prison time impacts future criminal behavior employment.Criminal Sentencing Recidivism: seminal study, Kling (2006) examined effect incarceration length ex-prisoners’ labor market outcomes. used random assignment judges instrument, capitalizing fact judges harsher (give longer sentences) others lenient. judge IV strategy since used extensively study prison time impacts future criminal behavior employment.Pre-Trial Detention Decisions: leniency design also applied bail pre-trial release. Dobbie, Goldin, Yang (2018) use fact arraignment judges vary tendency set bail (versus release defendants) instrument study impact pre-trial detention defendants’ case outcomes future behavior. defendants quasi-randomly assigned bail judges, approach isolates jailed trial causally affects outcomes like conviction re-offense. authors others find, example, lenient bail judge (releases pre-trial) leads better long-run outcomes compared strict judge, indicating pre-trial detention can harmful causal effects.Pre-Trial Detention Decisions: leniency design also applied bail pre-trial release. Dobbie, Goldin, Yang (2018) use fact arraignment judges vary tendency set bail (versus release defendants) instrument study impact pre-trial detention defendants’ case outcomes future behavior. defendants quasi-randomly assigned bail judges, approach isolates jailed trial causally affects outcomes like conviction re-offense. authors others find, example, lenient bail judge (releases pre-trial) leads better long-run outcomes compared strict judge, indicating pre-trial detention can harmful causal effects.Juvenile Incarceration Life Outcomes: related vein, Aizer Doyle Jr (2015) studied effect juvenile detention high school completion adult crime. leveraged random assignment juvenile court judges, judges likely incarcerate young offenders others. judge IV design revealed large negative causal impacts juvenile incarceration educational attainment increase adult crime, evidence sentencing leniency youth can dramatically alter life trajectories (results consistent general pattern found judge IV studies incarceration). application illustrates judicial decisions youth treated natural experiments.Juvenile Incarceration Life Outcomes: related vein, Aizer Doyle Jr (2015) studied effect juvenile detention high school completion adult crime. leveraged random assignment juvenile court judges, judges likely incarcerate young offenders others. judge IV design revealed large negative causal impacts juvenile incarceration educational attainment increase adult crime, evidence sentencing leniency youth can dramatically alter life trajectories (results consistent general pattern found judge IV studies incarceration). application illustrates judicial decisions youth treated natural experiments.Disability Insurance Labor Supply: realm social insurance, Maestas, Mullen, Strand (2013) used examiner design determine whether receiving disability benefits discourages work. disability claim assigned disability examiner, examiners approve benefits higher rates others. using quasi-random examiner assignment instrument, found applicants margin eligibility, receiving Disability Insurance caused significant reduction employment compared denied. report 23% applicants affected examiner get, allowed benefits due lenient examiner substantially higher employment rates instead assigned stricter examiner (thus denied). study prime example using medical administrative examiner assignments identify policy’s effect.Disability Insurance Labor Supply: realm social insurance, Maestas, Mullen, Strand (2013) used examiner design determine whether receiving disability benefits discourages work. disability claim assigned disability examiner, examiners approve benefits higher rates others. using quasi-random examiner assignment instrument, found applicants margin eligibility, receiving Disability Insurance caused significant reduction employment compared denied. report 23% applicants affected examiner get, allowed benefits due lenient examiner substantially higher employment rates instead assigned stricter examiner (thus denied). study prime example using medical administrative examiner assignments identify policy’s effect.Patent Grants Innovation: Examiner designs limited courts social programs; applied innovation economics well. Farre-Mensa, Hegde, Ljungqvist (2020) analyze value obtaining patent startups exploiting U.S. Patent Office’s quasi-random assignment applications patent examiners. patent examiners much lenient (likely grant patent) others, effectively creating “patent lottery”. authors use examiner leniency instrument whether startup’s patent approved. find striking results: startups “won” lottery drawing lenient examiner 55% higher employment growth 80% higher sales five years later average, compared similar startups ended strict examiner thus didn’t get patent. suggests patent grants large causal impact firm growth. study showcases examiner design regulatory/innovation setting – term leniency IV case refers examiner’s propensity allow patents.Patent Grants Innovation: Examiner designs limited courts social programs; applied innovation economics well. Farre-Mensa, Hegde, Ljungqvist (2020) analyze value obtaining patent startups exploiting U.S. Patent Office’s quasi-random assignment applications patent examiners. patent examiners much lenient (likely grant patent) others, effectively creating “patent lottery”. authors use examiner leniency instrument whether startup’s patent approved. find striking results: startups “won” lottery drawing lenient examiner 55% higher employment growth 80% higher sales five years later average, compared similar startups ended strict examiner thus didn’t get patent. suggests patent grants large causal impact firm growth. study showcases examiner design regulatory/innovation setting – term leniency IV case refers examiner’s propensity allow patents.Business Accelerators Firm Growth: entrepreneurial finance context, González-Uribe Reyes (2021) evaluate impact getting accepted business accelerator. Admission accelerator determined panels judges scoring startup applicants, judges’ scoring leniency varied randomly across groups. researchers exploit constructing instrument based generosity judges’ scores applicant. find participating accelerator dramatic effect: startups just made (thanks generous-scoring judges) grew 166% revenue just missed cutoff. example “judge leniency” design outside courtroom – “judges” competition evaluators, leniency scoring provided exogenous variation program entry. demonstrates examiner/judge IV approach can applied settings like business program evaluations scenario selection committees.Business Accelerators Firm Growth: entrepreneurial finance context, González-Uribe Reyes (2021) evaluate impact getting accepted business accelerator. Admission accelerator determined panels judges scoring startup applicants, judges’ scoring leniency varied randomly across groups. researchers exploit constructing instrument based generosity judges’ scores applicant. find participating accelerator dramatic effect: startups just made (thanks generous-scoring judges) grew 166% revenue just missed cutoff. example “judge leniency” design outside courtroom – “judges” competition evaluators, leniency scoring provided exogenous variation program entry. demonstrates examiner/judge IV approach can applied settings like business program evaluations scenario selection committees.examples illustrate examiner/judge (leniency) IV designs used wide array empirical settings: judicial decisions bail, sentencing, juvenile detention, administrative adjudications disability bankruptcy, regulatory approvals like patents, evaluation panels business education contexts. case, randomness assignment differing “strictness” decision-makers create natural experiment researchers harness estimate causal effects.designs valuable? allow analysts address problem unobserved heterogeneity selection bias observational data. Normally, people receive treatment (go prison, get benefit, win award) may differ systematically don’t, confounding simple comparisons. outside examiner’s quasi-random decision determines gets treatment, credible instrument break link. one article notes, approach become quite popular reliable way recover causal effects, even many attempted instruments face skepticism. trade-one must context random examiner assignment occurs must carefully check assumptions (e.g. truly random assignment, direct effect examiner outcomes aside via treatment). conditions met, examiner judge IV designs provide compelling evidence causal relationships hard identify otherwise.","code":""},{"path":"sec-instrumental-variables.html","id":"examples-in-marketing","chapter":"34 Instrumental Variables","heading":"34.8.2.4 Examples in Marketing","text":"marketing research, analogous setups can constructed identifying quasi-random sources variation decision-makers’ behaviors—sales representatives, regional managers, customer service agents—differ systematically tendency approve discounts, upgrade customers, resolve complaints favorably. agents’ “leniency” can serve instrument treatment assignment, enabling researchers isolate causal effects observational data randomization infeasible.analogical use judge leniency introduces powerful framework addressing endogeneity business contexts, allowing us disentangle effect marketing actions (e.g., discounts, loyalty offers) confounding influence customer selection targeting bias.","code":""},{"path":"sec-instrumental-variables.html","id":"formal-setup-and-notation","chapter":"34 Instrumental Variables","heading":"34.8.2.5 Formal Setup and Notation","text":"define setup formally follows:Let \\(= 1, \\dots, n\\) index individuals.individual two outcomes interest:\n\\(D_i\\): treatment decision, made decision maker (e.g., bail granted ).\n\\(Y_i\\): final outcome, potentially affected \\(D_i\\) (e.g., rearrest, discharge success).\n\\(D_i\\): treatment decision, made decision maker (e.g., bail granted ).\\(Y_i\\): final outcome, potentially affected \\(D_i\\) (e.g., rearrest, discharge success).individual randomly assigned one \\(K\\) decision makers: \\(Q_i \\\\{0, 1, \\dots, K - 1\\}\\).define potential treatment outcomes :\n\\(D_i(q)\\): treatment decision individual \\(\\) assigned decision maker \\(q\\).\n\\(D_i(q)\\): treatment decision individual \\(\\) assigned decision maker \\(q\\).observe realized \\(D_i = D_i(Q_i)\\).Note: meaningful ordering \\(Q_i\\). variation purely categorical, ordinal.also define potential final outcomes:\\(Y_i(D_i(q), q)\\): final outcome individual \\(\\) assigned decision maker \\(q\\), receives treatment \\(D_i(q)\\).conducting IV, focus reduced form first-stage variation induced \\(Q_i\\), seek shut direct effect \\(Q_i\\) \\(Y_i\\), conditioning \\(D_i\\).first consider variation \\(D_i\\) across \\(Q_i\\), potentially conditioning observed covariates \\(W_i\\).Assume conditional ignorability: assignment \\(Q_i\\) good random given \\(W_i\\).simplest case, \\(W_i\\) contains constant (.e., unadjusted analysis).can define:\\[\n\\tau_{q, q'} = \\mathbb{E}[D_i \\mid Q_i = q] - \\mathbb{E}[D_i \\mid Q_i = q']\n\\]relative effect decision maker \\(q\\) vs. \\(q'\\) treatment assignment.Define:\\[\n\\mu_D(q) = \\mathbb{E}[D_i \\mid Q_i = q]\n\\]judge \\(q\\)’s average leniency (.e., often assign treatment).Estimated via simple regression:\\[\nD_i = Q_i \\mu_D + u_i\n\\]\\(Q_i\\) vector \\(K\\) dummies. \\(\\hat{\\mu}_D(q)\\) fitted means.isolate judge-level variation adjusting baseline differences (e.g., location), control \\(W_i\\):\\[\nD_i = Q_i \\mu_D + W_i \\gamma + u_i\n\\], \\(\\mu_D(q)\\) reflects conditional leniency, net \\(W_i\\).Define predicted value:\\[\nZ_i = \\hat{D}_i^\\perp\n\\]residualized leniency score, representing lenient decision maker \\(Q_i\\) , beyond expected covariates \\(W_i\\).Interpreting Instrument \\(Z_i\\)\\(Z_i\\) reflects within-location variation decision maker behavior.isolates judge-specific variation confounded observable case-level court-level factors.Mechanically, define residualized predicted treatment :\\[\n\\hat{D}_i^\\perp = \\frac{1}{n} \\left( \\sum_i \\underbrace{1(Q_i = q) D_i}_{\\text{judge mean}} - \\sum_i \\underbrace{1(W_i = w) D_i}_{\\text{location mean}} \\right)\n\\]judge mean captures average treatment assigned judge \\(q\\).location mean captures average treatment assigned across individuals observable characteristics \\(W_i = w\\) (e.g., court, time window).Subtracting location mean residualizes judge effect removing location-specific variation.goal extract variation across judges orthogonal systematic location-level treatment patterns.Re-centering Necessary?raw leniency score \\(\\mu_D(q) = \\mathbb{E}[D_i \\mid Q_i = q]\\) may reflect:Actual judge discretion, alsoSystematic differences case mix, depending location time dayThis can bias instrument judges assigned uniformly across contexts.centering judge-level means relative mean outcome location, obtain meaningful instrument:now reflects lenient judge relative local peer group, controlling observable confounding.apply residualization:obtain “recentered” leniency measure, reflects within-location differences decision-maker behavior.measure commonly used empirical IV applications (e.g., criminal justice, bankruptcy courts, asylum decisions).aligns real-world practice empirical work, often uses leave-one-versions \\(\\mu_D(Q_i)\\) avoid mechanical endogeneity.use leave-one-?Including individual’s outcome estimation instrument can induce endogeneity (mechanical correlation).leave-one-leniency score judge \\(q\\) :\\[\n\\tilde{\\mu}_D^{(-)}(q) = \\frac{1}{n_q - 1} \\sum_{j \\ne , Q_j = q} D_j\n\\]\\(n_q\\) number individuals assigned judge \\(q\\).ensures instrument uncorrelated individual-level shocks, key IV requirement.can compute residualized outcomes subtracting location-level variation, just like leniency:\\[\n\\hat{Y}_i^\\perp = \\hat{Y}_i - \\mathbb{E}[Y_i \\mid W_i]\n\\]helps us distinguish much variation outcomes due judge effects, rather location-level factors.Even controlling \\(W_i\\), often still observe meaningful variation \\(Y_i\\) across judges.implies judge assignment induces variation outcomes, even holding location case mix fixed.","code":""},{"path":"sec-instrumental-variables.html","id":"assumptions-5","chapter":"34 Instrumental Variables","heading":"34.8.2.6 Assumptions","text":"Relevance:\ninstrument \\(Z_i\\) (e.g., judge leniency) must strongly correlated \\(D_i\\).\n, \\(\\mathbb{E}[D_i \\mid Z_i]\\) must vary meaningfully across values \\(Z_i\\).\ninstrument \\(Z_i\\) (e.g., judge leniency) must strongly correlated \\(D_i\\)., \\(\\mathbb{E}[D_i \\mid Z_i]\\) must vary meaningfully across values \\(Z_i\\).Exclusion:\ninstrument must affect outcome \\(D_i\\).\n, \\(Q_i\\) direct effect \\(Y_i\\) control \\(D_i\\).\ninstrument must affect outcome \\(D_i\\)., \\(Q_i\\) direct effect \\(Y_i\\) control \\(D_i\\).Monotonicity (G. W. Imbens Angrist 1994):\neffect instrument “flip signs” across units.\njudge \\(q\\) lenient \\(q'\\), lenient everyone.\neffect instrument “flip signs” across units.judge \\(q\\) lenient \\(q'\\), lenient everyone.Note: exclusion monotonicity assumptions directly testable can controversial. ’ll return depth later.","code":""},{"path":"sec-instrumental-variables.html","id":"one-vs.-many-instruments","chapter":"34 Instrumental Variables","heading":"34.8.2.7 One vs. Many Instruments","text":"many applied papers, researchers use leniency score \\(Z_i = \\hat{D}_i^\\perp\\) rather judge dummies directly.?Computational Simplicity:\nUsing \\(Z_i\\) avoids estimating -identified system (.e., one instrument per judge).\nspeeds computation, especially large datasets.\nUsing \\(Z_i\\) avoids estimating -identified system (.e., one instrument per judge).speeds computation, especially large datasets.Ease Visualization:\nResearchers often plot reduced-form first-stage coefficients leniency.\noffers intuitive visual evidence instrument strength outcome response.\nResearchers often plot reduced-form first-stage coefficients leniency.offers intuitive visual evidence instrument strength outcome response.First Stage Power:\njust-identified model, checking strength instrument (e.g., F-statistic) straightforward.\ncontrast, many-instrument settings can obscure weak instrument issues.\njust-identified model, checking strength instrument (e.g., F-statistic) straightforward.contrast, many-instrument settings can obscure weak instrument issues.Recall 2SLS estimator matrix form:\\[\n\\hat{\\beta}_{2SLS} = \\frac{D'Q(Q'Q)^{-1}Q'Y}{D'Q(Q'Q)^{-1}Q'D} = \\frac{\\hat{D}'Y}{\\hat{D}'D}\n\\]highlights key point:Using many instruments (judge dummies) projecting onto ,using predicted value \\(\\hat{D}_i\\) single instrument,algebraically equivalent absence controls.Adding controls just residualizes \\(D_i\\), \\(Y_i\\), \\(Z_i\\) respect \\(W_i\\).approach better?core identifying variation still random assignment \\(K\\) judges.Collapsing variation predicted value \\(Z_i\\) change source randomness.However, using predicted value may obsure experimental structure.point estimates may equivalent, inference can differ:use \\(\\hat{\\mu}_D(q)\\) first-stage effect, ignore sampling uncertainty estimating means.variance \\(\\hat{D}_i^\\perp\\) account variability projection, leading potentially understated standard errors.issue closely related many instrument problem, arises especially \\(K\\) large judge assignment sparse.many empirical cases:just-identified leniency instrument -identified judge-dummy approach yield similar standard errors.occurs \\(\\hat{\\mu}_D(q)\\) estimated high precision (.e., many observations per judge).important advantage leniency instruments naturally handle “-observation” bias.naive case, \\(\\hat{\\mu}_D(q)\\) includes individual \\(\\)’s treatment \\(D_i\\):\nintroduces mechanical endogeneity, instrument correlated error \\(Y_i\\).\nintroduces mechanical endogeneity, instrument correlated error \\(Y_i\\).leave-one-leniency score corrects :\\[\n\\tilde{\\mu}_D^{(-)}(q) = \\frac{1}{n_q - 1} \\sum_{\\substack{j \\ne \\\\ Q_j = q}} D_j\n\\]Researchers construct \\(Z_i\\) using \\(\\tilde{\\mu}_D^{(-)}(Q_i)\\), excludes \\(\\)’s data instrument.essentially finite-sample approximation jackknife IV (JIVE).bias corrected LOO exactly one arises many-instrument IV small samples.correction especially important number judges per location moderate small.Given leniency approach can understood form JIVE (J. D. Angrist, Imbens, Krueger 1999), go ? U-JIVE (Unbiased JIVE) modern refinement developed directly address issues finite samples (KolesÃ et al. 2013).many instruments many fixed effects (e.g., location time dummies), can face inference problems.U-JIVE provides consistent unbiased estimator even challenging setups.","code":""},{"path":"sec-instrumental-variables.html","id":"testing-the-exclusion-restriction","chapter":"34 Instrumental Variables","heading":"34.8.2.8 Testing the Exclusion Restriction","text":"exclusion restriction requires judge assignment affects outcome treatment—.e., \\(Q_i\\) affects \\(Y_i\\) \\(D_i\\).challenging test directly, can probe plausibility using tools familiar RCTs observational causal inference: covariate balance checks.Step 1: Predict Propensity ScoresCompute first-stage predicted treatment: \\(\\hat{\\mu}_D(Q_i)\\)judge \\(Q_i\\)’s estimated leniency (possibly residualized)Use “propensity score” treatment \\(D_i\\)Step 2: Test Covariate Balance Across LeniencyRegress observable covariates \\(X_i\\) \\(\\hat{\\mu}_D(Q_i)\\)assignment judges random (conditional covariates), see systematic relationship covariates predicted treatmentThis analogous testing pre-treatment balance randomized experiments.See Table 1 Dobbie, Goldsmith-Pinkham, Yang (2017), displays descriptive statistics tests covariate balance across predicted leniency scores. now standard empirical diagnostic papers using judge IV designs.covariates vary significantly \\(\\hat{\\mu}_D(Q_i)\\), exclusion assumption questionable—may indicate judge assignment confounded case characteristics.","code":""},{"path":"sec-instrumental-variables.html","id":"testing-the-monotonicity-assumption","chapter":"34 Instrumental Variables","heading":"34.8.2.9 Testing the Monotonicity Assumption","text":"monotonicity assumption requires one judge lenient another, lenient everyone—.e., individual treated strict judge untreated lenient one.subtle particularly hard test multi-judge (non-binary instrument) designs.Monotonicity violations imply violations LATE interpretation—IV may longer estimate meaningful average causal effect.Recent work attempts develop testable implications monotonicity (Kitagawa 2015; Frandsen, Lefgren, Leslie 2023).Let:\\(D\\) binary treatment\\(D\\) binary treatment\\(Z\\) binary instrument (e.g., judge assignment)\\(Z\\) binary instrument (e.g., judge assignment)\\(Y\\) outcome interest\\(Y\\) outcome interestDefine joint distributions:\\(P(y, d) = \\mathbb{P}(Y = y, D = d \\mid Z = 1)\\)\\(Q(y, d) = \\mathbb{P}(Y = y, D = d \\mid Z = 0)\\)Kitagawa (2015) shows binary IV case IV valid monotonic, :\\[\nP(B, 1) - Q(B, 1) \\ge 0 \\\\\nP(B, 0) - Q(B, 0) \\ge 0\n\\]\\(B\\) set outcomes (e.g., \\(Y \\\\{1\\}\\)). inequalities follow complier potential outcome structure monotonicity.testable implications. Violations suggest either invalid instruments monotonicity failures.judge IV designs, instrument binary. Frandsen, Lefgren, Leslie (2023) extended Kitagawa (2015) idea case.Key idea:Map multiple judges scalar leniency index, \\(Z_i = \\hat{\\mu}_D(Q_i)\\)Map multiple judges scalar leniency index, \\(Z_i = \\hat{\\mu}_D(Q_i)\\)Discretize \\(Z_i\\) needed, apply Kitagawa-style testsDiscretize \\(Z_i\\) needed, apply Kitagawa-style testsChallenges:mapping introduces estimation error noiseThe mapping introduces estimation error noiseTests rely finite-sample approximations may underpoweredTests rely finite-sample approximations may underpoweredStill, tests provide useful diagnostic tools evaluate monotonicity plausibility.practical question: can exclusion monotonicity assumptions fail?Estimating bounds treatment effect partial violationsExploring subgroup monotonicity (e.g., within strata judges plausibly rankable)Using alternative identification strategies (e.g., weaker forms LATE)words, lost—interpretation becomes cautious subtle.Even assumptions reasonable, inference remains challenging judge-IV setups.Many instruments \\(\\\\) weak instrument concernsMany controls (fixed effects) \\(\\\\) finite-sample bias overfittingHeteroskedasticity \\(\\\\) robust inference tricky","code":""},{"path":"sec-instrumental-variables.html","id":"sec-proxy-variables","chapter":"34 Instrumental Variables","heading":"34.8.3 Proxy Variables","text":"applied business economic analysis, often confront frustrating reality: variables truly care —like brand loyalty, employee ability, investor sentiment—directly observable. Instead, rely proxy variables, observable measures stand latent omitted variables. Though useful, proxy variables must used care, introduce risks, notably measurement error incomplete control endogeneity.proxy variable observed variable used place variable either unobservable omitted model. typically used assumption correlated latent variable explains variation.Let:\\(X^*\\) latent (unobserved) variable,\\(X^*\\) latent (unobserved) variable,\\(X\\) observed proxy,\\(X\\) observed proxy,\\(Y\\) outcome.\\(Y\\) outcome.may desire estimate: \\[\nY = \\beta_0 + \\beta_1 X^* + \\varepsilon,\n\\] since \\(X^*\\) unavailable, instead estimate:\\[\nY = \\beta_0 + \\beta_1 X + u.\n\\]effectiveness approach hinges whether \\(X\\) can validly stand \\(X^*\\).","code":""},{"path":"sec-instrumental-variables.html","id":"proxy-use-and-omitted-variable-bias","chapter":"34 Instrumental Variables","heading":"34.8.3.1 Proxy Use and Omitted Variable Bias","text":"Proxy variables sometimes used substitutes omitted variables cause endogeneity. Including proxy can reduce endogeneity, generally eliminate bias, unless strict conditions met.Key Insight: Including proxy allow us estimate effect omitted variable; rather, helps mitigate bias introduced omission.precise, let’s consider classic omitted variable setup:Suppose true model : \\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon,\n\\] \\(Z\\) omitted estimation. \\(Z\\) correlated \\(X\\), OLS estimate \\(\\beta_1\\) biased.Now, suppose proxy \\(Z_p\\) \\(Z\\). Including \\(Z_p\\) regression: \\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 Z_p + u\n\\] can help reduce bias \\(Z_p\\) meets following criteria.Let \\(Z\\) unobserved variable \\(Z_p\\) proxy. , \\(Z_p\\) valid proxy :Correlation: \\(Z_p\\) correlated \\(Z\\) (.e., \\(\\text{Cov}(Z_p, Z) \\ne 0\\)).Residual Independence: residual variation \\(Z\\) unexplained \\(Z_p\\) uncorrelated regressors (including \\(Z_p\\) \\(X\\)): \\[\nZ = \\gamma_0 + \\gamma_1 Z_p + \\nu, \\quad \\text{} \\text{Cov}(\\nu, X) = \\text{Cov}(\\nu, Z_p) = 0.\n\\]direct effect: \\(Z_p\\) affects \\(Y\\) \\(Z\\) (least directly).Violation conditions can lead biased inconsistent estimates.","code":""},{"path":"sec-instrumental-variables.html","id":"example-iq-as-a-proxy-for-ability-in-wage-regressions","chapter":"34 Instrumental Variables","heading":"34.8.3.2 Example: IQ as a Proxy for Ability in Wage Regressions","text":"labor economics, researchers often study effect education wages. ability—unobservable factor—also affects education wages, leading omitted variable bias.Let:\\(Y\\) = wage,\\(Y\\) = wage,\\(X\\) = education,\\(X\\) = education,\\(Z^*\\) = ability (unobserved),\\(Z^*\\) = ability (unobserved),\\(Z\\) = IQ test score (proxy ability).\\(Z\\) = IQ test score (proxy ability).Suppose true model : \\[\n\\text{wage} = \\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{ability} + \\varepsilon.\n\\]Since ability unobserved, estimate: \\[\n\\text{wage} = \\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{IQ} + u,\n\\] assumption: \\[\n\\text{ability} = \\gamma_0 + \\gamma_1 \\text{IQ} + \\nu,\n\\] \\(\\text{Cov}(\\nu, \\text{education}) = \\text{Cov}(\\nu, \\text{IQ}) = 0\\).inclusion IQ helps reduce endogeneity identify pure effect ability unless variation ability captured IQ.","code":""},{"path":"sec-instrumental-variables.html","id":"pros-and-cons-of-proxy-variables","chapter":"34 Instrumental Variables","heading":"34.8.3.3 Pros and Cons of Proxy Variables","text":"AdvantagesMake latent variables measurable: Allows analysis constructs directly observed.Practicality: Makes use available data address endogeneity.Improved specification: Can reduce omitted variable bias proxies well chosen.DisadvantagesMeasurement error: Proxies usually include noise, causing attenuation bias (.e., coefficients biased toward zero).\n\\(X = X^* + \\nu\\), \\(\\nu\\) classical measurement error (zero mean, uncorrelated \\(X^*\\) \\(\\varepsilon\\)), : \\[\n\\text{plim}(\\hat{\\beta}_1) = \\lambda \\beta_1, \\quad \\text{} \\lambda = \\frac{\\sigma^2_{X^*}}{\\sigma^2_{X^*} + \\sigma^2_\\nu} < 1.\n\\]Measurement error: Proxies usually include noise, causing attenuation bias (.e., coefficients biased toward zero).\\(X = X^* + \\nu\\), \\(\\nu\\) classical measurement error (zero mean, uncorrelated \\(X^*\\) \\(\\varepsilon\\)), : \\[\n\\text{plim}(\\hat{\\beta}_1) = \\lambda \\beta_1, \\quad \\text{} \\lambda = \\frac{\\sigma^2_{X^*}}{\\sigma^2_{X^*} + \\sigma^2_\\nu} < 1.\n\\]Interpretation issues: Coefficients proxies conflate causal effect proxy quality.Interpretation issues: Coefficients proxies conflate causal effect proxy quality.Insufficient control: Proxies partially reduce omitted variable bias unless meet strict independence conditions.Insufficient control: Proxies partially reduce omitted variable bias unless meet strict independence conditions.","code":""},{"path":"sec-instrumental-variables.html","id":"empirical-illustration-simulating-attenuation-bias","chapter":"34 Instrumental Variables","heading":"34.8.3.4 Empirical Illustration: Simulating Attenuation Bias","text":"Observe including proxy reduces bias coefficient education, even doesn’t eliminate entirely.","code":"\nset.seed(2025)\nn <- 1000\nability <- rnorm(n)                   # latent variable\nIQ <- ability + rnorm(n, sd = 0.5)    # proxy variable\neducation <- 12 + 0.5 * ability + rnorm(n)  # correlated regressor\nwage <- 20 + 1.5 * education + 2 * ability + rnorm(n)  # true model\n\n# Model using education only (omitted variable bias)\nmod1 <- lm(wage ~ education)\n\n# Model using education and proxy\nmod2 <- lm(wage ~ education + IQ)\n\nsummary(mod1)\n#> \n#> Call:\n#> lm(formula = wage ~ education)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -7.4949 -1.3590 -0.0082  1.3766  6.6601 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 10.51325    0.71353   14.73   <2e-16 ***\n#> education    2.28903    0.05918   38.68   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.061 on 998 degrees of freedom\n#> Multiple R-squared:  0.5999, Adjusted R-squared:  0.5995 \n#> F-statistic:  1496 on 1 and 998 DF,  p-value: < 2.2e-16\nsummary(mod2)\n#> \n#> Call:\n#> lm(formula = wage ~ education + IQ)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.3224 -0.9052  0.0523  0.9370  4.5822 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 17.96426    0.49599   36.22   <2e-16 ***\n#> education    1.67098    0.04114   40.62   <2e-16 ***\n#> IQ           1.55953    0.04096   38.07   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.317 on 997 degrees of freedom\n#> Multiple R-squared:  0.8369, Adjusted R-squared:  0.8366 \n#> F-statistic:  2558 on 2 and 997 DF,  p-value: < 2.2e-16"},{"path":"sec-instrumental-variables.html","id":"example-marketing-brand-loyalty","chapter":"34 Instrumental Variables","heading":"34.8.3.5 Example: Marketing — Brand Loyalty","text":"Suppose ’re modeling effect brand loyalty (\\(X^*\\)) repeat purchase (\\(Y\\)). Since loyalty latent, might use:Number prior purchases,Duration current brand use,Membership loyalty programs.proxies likely correlated true loyalty, none perfect substitute.Observe estimated coefficient proxy less true coefficient (2), due measurement error.","code":"\n# Simulating attenuation bias with a proxy\nset.seed(42)\nn <- 1000\nX_star <- rnorm(n)  # true unobserved brand loyalty\nproxy <- X_star + rnorm(n, sd = 0.6)  # proxy with measurement error\nerror <- rnorm(n)\nY <- 3 + 2 * X_star + error  # true model\n\n# Model using the proxy variable\nmodel_proxy <- lm(Y ~ proxy)\nsummary(model_proxy)\n#> \n#> Call:\n#> lm(formula = Y ~ proxy)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3060 -1.0130 -0.0018  0.9131  4.5493 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.98737    0.04584   65.17   <2e-16 ***\n#> proxy        1.45513    0.03921   37.11   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.449 on 998 degrees of freedom\n#> Multiple R-squared:  0.5798, Adjusted R-squared:  0.5794 \n#> F-statistic:  1377 on 1 and 998 DF,  p-value: < 2.2e-16"},{"path":"sec-instrumental-variables.html","id":"example-finance-investor-sentiment","chapter":"34 Instrumental Variables","heading":"34.8.3.6 Example: Finance — Investor Sentiment","text":"Investor sentiment affects market movements directly measured. Proxies include:Put-call ratiosPut-call ratiosBullish/bearish sentiment surveys,Bullish/bearish sentiment surveys,Volume IPO activity,Volume IPO activity,Retail investor trading flows.Retail investor trading flows.capture different dimensions sentiment, effectiveness varies context.","code":""},{"path":"sec-instrumental-variables.html","id":"strategies-to-improve-proxy-use","chapter":"34 Instrumental Variables","heading":"34.8.3.7 Strategies to Improve Proxy Use","text":"Multiple proxies: Use several proxies combine via factor analysis PCAMultiple proxies: Use several proxies combine via factor analysis PCAInstrumental variables: valid instrument exists proxy, use two-stage least squares correct measurement error.Instrumental variables: valid instrument exists proxy, use two-stage least squares correct measurement error.Latent variable models: Structural Equation Modeling (SEM) allows estimation models latent variables explicitly.Latent variable models: Structural Equation Modeling (SEM) allows estimation models latent variables explicitly.Proxy variables valuable tools empirical research used caution. offer bridge theory data important variables unobservable. However, bridge built assumptions—especially regarding correlation, measurement error, residual independence—must carefully justified.Key Takeaway: proxy can reduce bias omitted variables introduces risks—especially measurement error interpretive ambiguity. best practice use proxies transparently, test assumptions possible, consider alternative solutions instruments structural models.","code":""},{"path":"sec-matching-methods.html","id":"sec-matching-methods","chapter":"35 Matching Methods","heading":"35 Matching Methods","text":"Matching strategy aims eliminate—least minimize—potential sources bias constructing treatment comparison groups similar observed characteristics. , observed differences outcomes groups can attributed confidently treatment rather factors. observational research, matching frequently combined Difference--Differences () techniques address issues selection bias, particularly multiple pre-treatment outcomes available.Matching defined “method aims equate (”balance”) distribution covariates treated control groups.” (Stuart 2010, 1)Matching particularly useful :Outcomes yet observed, follow-studies, want construct balanced treatment/control groups.Outcomes available, wish reduce model dependence improve robustness.Conceptually, matching can also viewed lens missing data, since never observe potential outcomes \\((Y_i^T, Y_i^C)\\) unit. Hence, topic closely relates Imputation (Missing Data).","code":""},{"path":"sec-matching-methods.html","id":"introduction-and-motivation","chapter":"35 Matching Methods","heading":"35.1 Introduction and Motivation","text":"","code":""},{"path":"sec-matching-methods.html","id":"why-match","chapter":"35 Matching Methods","heading":"35.1.1 Why Match?","text":"many observational studies, researchers luxury randomization. Subjects (people, firms, schools, etc.) typically select selected treatment based certain observed /unobserved characteristics. can introduce systematic differences (selection bias) confound causal inference. Matching attempts approximate randomized experiment “balancing” observed characteristics treated non-treated (control) units.Goal: Reduce model dependence clarify causal effects ensuring treated control subjects sufficiently comparable covariates.Challenge: Even matching achieves balance observed covariates, unobserved confounders remain threat identification (.e., Matching selection observables identification strategy). Matching magically fix bias unobserved variables.understand causal inference difficult observational studies, consider:\\[\n\\begin{aligned} E(Y_i^T | T) - E(Y_i^C | C) &= E(Y_i^T - Y_i^C | T) + \\underbrace{[E(Y_i^C | T) - E(Y_i^C | C)]}_{\\text{Selection Bias}} \\\\ \\end{aligned} \\]term \\(E(Y_i^T - Y_i^C | T)\\) causal effect (specifically ATT).term \\(E(Y_i^C | T) - E(Y_i^C | C)\\) reflects selection bias due systematic differences untreated potential outcome across treated control groups.Random assignment ensures:\\[ E(Y_i^C | T) = E(Y_i^C | C) \\]eliminates selection bias. observational data, however, equality rarely holds.Matching aims mimic randomization conditioning covariates \\(X\\):\\[ E(Y_i^C | X, T) = E(Y_i^C | X, C) \\]example, propensity score matching achieves balance conditioning propensity score \\(P(X)\\):\\[ E(Y_i^C | P(X), T) = E(Y_i^C | P(X), C) \\](See Propensity Scores discussion.)Average Treatment Effect (ATE) matching typically estimated :\\[ \\frac{1}{N_T} \\sum_{=1}^{N_T} \\left(Y_i^T - \\frac{1}{N_{C_i}} \\sum_{j \\\\mathcal{C}_i} Y_j^C\\right) \\]\\(\\mathcal{C}_i\\) denotes matched controls treated unit \\(\\).Standard Errors MatchingMatching closed-form standard error ATE ATT.Therefore, rely bootstrapping estimate uncertainty.Note: Matching tends yield larger standard errors OLS reduces effective sample size discarding unmatched observations.","code":""},{"path":"sec-matching-methods.html","id":"matching-as-pruning","chapter":"35 Matching Methods","heading":"35.1.2 Matching as “Pruning”","text":"Matching can thought “pruning” (preprocessing step) (G. King, Lucas, Nielsen 2017). goal prune unmatched poorly matched units conducting analysis, reducing model dependence.Without Matching:Imbalanced data → Model dependence → Researcher discretion → Biased estimatesWith Matching:Balanced data → Reduces discretion → credible causal inferenceDegree Balance Across DesignsFully blocked exactly matched designs outperform randomized ones :ImbalanceModel dependenceEfficiency powerBiasRobustnessResearch costs","code":""},{"path":"sec-matching-methods.html","id":"matching-with-did","chapter":"35 Matching Methods","heading":"35.1.3 Matching with DiD","text":"Matching can fruitfully combined multiple pre-treatment periods available. designs can help correct selection bias certain assumptions:selection bias symmetric around treatment date, standard (implemented symmetrically around treatment date) remains consistent (Chabé-Ferret 2015 ).selection bias asymmetric, simulations Chabé-Ferret (2015) show symmetric still outperforms matching alone, although pre-treatment observations can improve matching performance.short, matching universal solution often provides helpful preprocessing step conducting causal estimation methods (J. . Smith Todd 2005).","code":""},{"path":"sec-matching-methods.html","id":"key-assumptions-4","chapter":"35 Matching Methods","heading":"35.2 Key Assumptions","text":"Matching relies standard set assumptions underpinning selection observables—also known back-door criterion (see Assumptions Identifying Treatment Effects). assumptions hold, matching can yield valid estimates causal effects constructing treated control groups comparable observed covariates.Strong Conditional Ignorability Assumption (Unconfoundedness)Also known hidden bias ignorability assumption:\\[\n(Y(0),\\, Y(1)) \\,\\perp\\, T \\,\\big|\\, X\n\\]implies , conditional covariates \\(X\\), treatment assignment independent potential outcomes. words, unobserved confounders adjust \\(X\\).assumption testable, plausible relevant confounders observed included \\(X\\).often satisfied approximately unobserved covariates highly correlated observed ones.unobserved variables unrelated \\(X\\), can:\nConduct sensitivity analysis test robustness estimates.\nApply design sensitivity techniques: unobserved confounding suspected, methods (Heller, Rosenbaum, Small 2009)’s design sensitivity approaches bounding approaches (e.g., rbounds R package) can used test robust findings hidden bias.\nConduct sensitivity analysis test robustness estimates.Apply design sensitivity techniques: unobserved confounding suspected, methods (Heller, Rosenbaum, Small 2009)’s design sensitivity approaches bounding approaches (e.g., rbounds R package) can used test robust findings hidden bias.cornerstone assumption matching: without , causal inference observational data generally invalid.Overlap (Positivity) Assumption (Common Support)\\[\n0 < P(T=1 \\mid X) < 1 \\quad \\forall X\n\\]condition ensures , every value covariates \\(X\\), positive probability receiving treatment control.assumption fails, regions covariate space either treatment control units absent, making comparison impossible.Matching enforces assumption discarding units outside region common support.pruning step strength limitation matching—improves internal validity cost generalizability.Stable Unit Treatment Value Assumption (SUTVA)SUTVA requires :potential outcomes individual unit depend treatment assignment units., interference spillover effects units.Mathematically, \\(Y_i(T_i)\\) depends \\(T_i\\), \\(T_j\\) \\(j \\neq \\).Violations can occur settings like:\nEducation (peer effects)\nEpidemiology (disease transmission)\nMarketing (network influence)\nEducation (peer effects)Epidemiology (disease transmission)Marketing (network influence)cases known spillover, efforts made reduce interactions explicitly model interference.Summary Assumptions MatchingThese three assumptions form foundation valid causal inference using matching methods.","code":""},{"path":"sec-matching-methods.html","id":"framework-for-generalization","chapter":"35 Matching Methods","heading":"35.3 Framework for Generalization","text":"Let:\\(P_t\\), \\(P_c\\): treated control populations\\(N_t\\), \\(N_c\\): random samples drawn \\(P_t\\), \\(P_c\\)\\(\\mu_i\\), \\(\\Sigma_i\\): means covariance matrices \\(p\\) covariates group \\(\\\\{t, c\\}\\)\\(X_j\\): vector covariates individual \\(j\\)\\(T_j \\\\{0, 1\\}\\): treatment indicator (1 = treated, 0 = control)\\(Y_j\\): observed outcomeAssume \\(N_t < N_c\\) (.e., controls treated)conditional treatment effect :\\[\n\\tau(x) = R_1(x) - R_0(x), \\quad \\text{} R_1(x) = E[Y(1) \\mid X = x], \\quad R_0(x) = E[Y(0) \\mid X = x]\n\\]assume constant treatment effects (parallel trends), \\(\\tau(x) = \\tau\\) \\(x\\). assumption relaxed, can still estimate average effect distribution \\(X\\).Common EstimandsAverage Treatment Effect (ATE): Average causal effect across units.Average Treatment Effect Treated (ATT): Causal effect treated units .","code":""},{"path":"sec-matching-methods.html","id":"steps-for-matching","chapter":"35 Matching Methods","heading":"35.4 Steps for Matching","text":"matching methods rely :Propensity score: summarizes \\(P(T=1|X)\\)Distance metric: measures similarityCovariates: assumed satisfy ignorability","code":""},{"path":"sec-matching-methods.html","id":"step-1-define-closeness-distance-metrics","chapter":"35 Matching Methods","heading":"35.4.1 Step 1: Define “Closeness” (Distance Metrics)","text":"Matching requires distance metric define similarity treated control units.Variable Selection GuidelinesInclude many pre-treatment covariates possible support conditional ignorability.Avoid post-treatment variables, introduce bias.cautious variables (e.g., heavy drug users) highly correlated outcome (e.g., heavy drinkers) treatment (e.g., mediators).variables uncorrelated treatment outcome, cost inclusion small.Distance MeasuresWhere \\(e_k\\) estimated propensity score \\(P(T=1 \\mid X_k)\\) unit \\(k\\).Advanced: Prognostic scores (B. B. Hansen 2008) require modeling \\(E[Y(0)|X]\\), depend outcome model.Tip: high dimensions, exact Mahalanobis matching perform poorly. Combining Mahalanobis propensity score calipers can improve robustness (Rubin Thomas 2000).Advanced methods longitudinal setting:Marginal Structural Models: time-varying treatments (Robins, Hernan, Brumback 2000)Balanced Risk Set Matching: survival analysis (Y. P. Li, Propert, Rosenbaum 2001)","code":""},{"path":"sec-matching-methods.html","id":"step-2-matching-algorithms","chapter":"35 Matching Methods","heading":"35.4.2 Step 2: Matching Algorithms","text":"Nearest Neighbor MatchingGreedy matching: Fast, suboptimal competition controls.Optimal matching: Minimizes global distance across pairs.Ratio matching (k:1): Useful controls outnumber treated; choose \\(k\\) using trade-bias variance (Rubin Thomas 1996).vs. without replacement:\nreplacement: Improves matching quality, requires frequency weights analysis.\nWithout replacement: Simpler, less flexible.\nreplacement: Improves matching quality, requires frequency weights analysis.Without replacement: Simpler, less flexible.Subclassification, Full Matching, WeightingThese methods generalize nearest-neighbor approaches assigning fractional weights.Subclassification: Partition strata based propensity score (e.g., quintiles).Full Matching: treated unit matched weighted group controls (vice versa) minimize average within-set distance.Weighting: Weighting techniques use propensity scores estimate ATE. However, weights extreme, resulting variance may inflated—due underlying probabilities, due estimation procedure . address issue, researchers can employ (1) weight trimming (2) doubly robust methods using propensity scores weighting matching.\nInverse Probability Treatment Weighting (IPTW): \\[\nw_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\n\\]\nOdds weighting: \\[\nw_i = T_i + (1 - T_i)\\frac{\\hat{e}_i}{1 - \\hat{e}_i}\n\\]\nKernel weighting: Smooth average control group (popular economics).\nTrimming Doubly-Robust Methods: Reduce variance due extreme weights.\nInverse Probability Treatment Weighting (IPTW): \\[\nw_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\n\\]Odds weighting: \\[\nw_i = T_i + (1 - T_i)\\frac{\\hat{e}_i}{1 - \\hat{e}_i}\n\\]Kernel weighting: Smooth average control group (popular economics).Trimming Doubly-Robust Methods: Reduce variance due extreme weights.Assessing Common SupportUse propensity score histograms visualize overlap.Units outside convex hull \\(X\\) (.e., unmatched regions) can discarded.Lack overlap indicates comparisons extrapolations, empirical matches.","code":""},{"path":"sec-matching-methods.html","id":"step-3-diagnosing-match-quality","chapter":"35 Matching Methods","heading":"35.4.3 Step 3: Diagnosing Match Quality","text":"Balance DiagnosticsMatching aims balance covariate distributions treated control units. well-matched sample satisfies:\\[\n\\tilde{p}(X \\mid T=1) \\approx \\tilde{p}(X \\mid T=0)\n\\]\\(\\tilde{p}\\) empirical distribution.Numerical ChecksStandardized differences means (common): \\(< 0.1\\)Standardized difference propensity scores: \\(< 0.25\\) (Rubin 2001)Variance ratio propensity scores: 0.5 2.0 (Rubin 2001)Variance residuals regression propensity score (treated vs. control) covariateAvoid using p-values diagnostics—conflate balance statistical power sensitive sample size.Graphical DiagnosticsEmpirical Distribution PlotsQuantile-Quantile (QQ) PlotsLove Plots: Summarize standardized differences /matching","code":""},{"path":"sec-matching-methods.html","id":"step-4-estimating-treatment-effects","chapter":"35 Matching Methods","heading":"35.4.4 Step 4: Estimating Treatment Effects","text":"MatchingWith k:1 matching replacement, use weights adjust reuse controls.Use regression adjustment matched samples improve precision adjust residual imbalance.Subclassification Full MatchingATT: Weight subclass-specific estimates number treated units.ATE: Weight total units per subclass.Variance EstimationMust reflect uncertainty :matching procedure (sampling distance calculation) (Step 3)outcome model (regression, difference--means, etc.) (Step 4)Often estimated via bootstrapping.","code":""},{"path":"sec-matching-methods.html","id":"special-considerations","chapter":"35 Matching Methods","heading":"35.5 Special Considerations","text":"Handling Missing DataUse generalized boosted models multiple imputation (Qu Lipkovich 2009).Violation IgnorabilityStrategies unobservables bias treatment:Use pre-treatment measures outcomeCompare multiple control groupsConduct sensitivity analysis:\nQuantify correlation unobserved confounders treatment outcome nullify observed effect\nQuantify correlation unobserved confounders treatment outcome nullify observed effect","code":""},{"path":"sec-matching-methods.html","id":"choosing-a-matching-strategy","chapter":"35 Matching Methods","heading":"35.6 Choosing a Matching Strategy","text":"","code":""},{"path":"sec-matching-methods.html","id":"based-on-estimand","chapter":"35 Matching Methods","heading":"35.6.1 Based on Estimand","text":"ATE: Use IPTW full matchingATT:\nmany controls (\\(N_c > 3N_t\\)): k:1 nearest neighbor without replacement\ncontrols: subclassification, full matching, odds weighting\nmany controls (\\(N_c > 3N_t\\)): k:1 nearest neighbor without replacementIf controls: subclassification, full matching, odds weighting","code":""},{"path":"sec-matching-methods.html","id":"based-on-diagnostics","chapter":"35 Matching Methods","heading":"35.6.2 Based on Diagnostics","text":"balanced: proceed regression matched samplesIf imbalance covariates: Mahalanobis matching thoseIf imbalance many covariates: Try k:1 matching replacement","code":""},{"path":"sec-matching-methods.html","id":"selection-criteria","chapter":"35 Matching Methods","heading":"35.6.3 Selection Criteria","text":"Minimize standardized differences across many covariatesEspecially prioritize prognostic covariatesMinimize number covariates large (\\(>0.25\\)) imbalance (Diamond Sekhon 2013)Matching one-size-fits-. Choose methods based target estimand, data structure, diagnostic results.","code":""},{"path":"sec-matching-methods.html","id":"matching-vs.-regression","chapter":"35 Matching Methods","heading":"35.7 Matching vs. Regression","text":"Matching regression two core strategies used observational studies adjust differences covariates \\(X\\) estimate causal effects. aim remove bias due confounding, approach problem differently, particularly weight observations, handle functional form assumptions, address covariate balance.Neither method can resolve issue unobserved confounding, can powerful tool used care supported appropriate diagnostics.Matching emphasizes covariate balance pruning dataset retain comparable units. nonparametric, focusing ATT.Regression (typically OLS) emphasizes functional form allows model-based adjustment, enabling estimation ATE continuous interactive effects treatment.matching regression assign implicit explicit weights observations estimation:Matching: Weights observations heavily strata treated units, aligning ATT estimand.OLS Regression: Places weight strata variance treatment assignment highest—.e., groups approximately balanced treated control (near 50/50).results differing estimands sensitivities:Important Caveat: OLS estimate biased due unobserved confounding, matching estimate likely biased . depend selection observables assumption.explore difference estimands matching regression, especially estimating ATT.","code":""},{"path":"sec-matching-methods.html","id":"matching-estimand","chapter":"35 Matching Methods","heading":"35.7.1 Matching Estimand","text":"Suppose want treatment effect treated:\\[\n\\delta_{\\text{TOT}} = E[Y_{1i} - Y_{0i} \\mid D_i = 1]\n\\]Using Law Iterated Expectation:\\[\n\\delta_{\\text{TOT}} = E\\left[ E[Y_{1i} \\mid X_i, D_i = 1] - E[Y_{0i} \\mid X_i, D_i = 1] \\mid D_i = 1 \\right]\n\\]Assuming conditional independence:\\[\nE[Y_{0i} \\mid X_i, D_i = 0] = E[Y_{0i} \\mid X_i, D_i = 1]\n\\],\\[\n\\begin{aligned}\n\\delta_{TOT} &= E [ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1 ] \\\\\n&= E\\left[ E[Y_i \\mid X_i, D_i = 1] - E[Y_i \\mid X_i, D_i = 0] \\mid D_i = 1 \\right] \\\\\n&= E[\\delta_X |D_i = 1]\n\\end{aligned}\n\\]\\(\\delta_X\\) \\(X\\)-specific difference means covariate value \\(X_i\\)\\(X_i\\) discrete, matching estimand becomes:\\[\n\\delta_M = \\sum_x \\delta_x P(X_i = x \\mid D_i = 1)\n\\]\\(P(X_i = x |D_i = 1)\\) probability mass function \\(X_i\\) given \\(D_i = 1\\)Bayes’ rule:\\[\nP(X_i = x \\mid D_i = 1) = \\frac{P(D_i = 1 \\mid X_i = x) P(X_i = x)}{P(D_i = 1)}\n\\],\\[\n\\begin{aligned}\n\\delta_M &= \\frac{\\sum_x \\delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}\n\\end{aligned}\n\\]","code":""},{"path":"sec-matching-methods.html","id":"regression-estimand","chapter":"35 Matching Methods","heading":"35.7.2 Regression Estimand","text":"regression:\\[\nY_i = \\sum_x d_{ix} \\beta_x + \\delta_R D_i + \\varepsilon_i\n\\]\\(d_{ix}\\) = indicator \\(X_i = x\\)\\(\\beta_x\\) = baseline outcome \\(X = x\\)\\(\\delta_R\\) = regression estimandThen,\\[\n\\begin{aligned}\n\\delta_R &= \\frac{\\sum_x \\delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}\n\\end{aligned}\n\\]","code":""},{"path":"sec-matching-methods.html","id":"interpretation-weighting-differences","chapter":"35 Matching Methods","heading":"35.7.3 Interpretation: Weighting Differences","text":"distinction matching regression comes covariate-specific treatment effects \\(\\delta_x\\) weighted:Summary Table: Matching vs. RegressionQualitative Comparisons","code":""},{"path":"sec-matching-methods.html","id":"software-and-practical-implementation","chapter":"35 Matching Methods","heading":"35.8 Software and Practical Implementation","text":"Many R packages provide functionality implementing various matching methods discussed . overview popular options:MatchIt:\nImplements wide range matching methods (nearest neighbor, optimal, full, subclassification, exact, etc.). focuses “preprocessing” data final outcome analysis.MatchIt:\nImplements wide range matching methods (nearest neighbor, optimal, full, subclassification, exact, etc.). focuses “preprocessing” data final outcome analysis.Matching:\nProvides multivariate propensity score matching, including options exact nearest neighbor matching. package also offers functions evaluate balance conduct sensitivity analyses.Matching:\nProvides multivariate propensity score matching, including options exact nearest neighbor matching. package also offers functions evaluate balance conduct sensitivity analyses.cem (Coarsened Exact Matching):\nUses coarsening approach create strata within exact matching can performed. can reduce imbalance discarding units overlap coarsened covariate space.cem (Coarsened Exact Matching):\nUses coarsening approach create strata within exact matching can performed. can reduce imbalance discarding units overlap coarsened covariate space.optmatch:\nEnables optimal matching variable matching ratios full matching, allowing flexible group constructions minimize overall distance.optmatch:\nEnables optimal matching variable matching ratios full matching, allowing flexible group constructions minimize overall distance.MatchingFrontier (G. King, Lucas, Nielsen 2017):\nFinds “frontier” matching solutions balancing sample size (constraints) covariate balance. Allows analysts see trade-offs real time.MatchingFrontier (G. King, Lucas, Nielsen 2017):\nFinds “frontier” matching solutions balancing sample size (constraints) covariate balance. Allows analysts see trade-offs real time.CBPS (Covariate Balancing Propensity Score):\nEstimates propensity scores covariate balance directly optimized. can help avoid iterative re-specification propensity score model.CBPS (Covariate Balancing Propensity Score):\nEstimates propensity scores covariate balance directly optimized. can help avoid iterative re-specification propensity score model.PanelMatch (Rauh, Kim, Imai 2025):\nTailored panel (longitudinal) data settings, providing matching methods exploit repeated observations time (e.g., -type analyses time-series cross-sectional environment).PanelMatch (Rauh, Kim, Imai 2025):\nTailored panel (longitudinal) data settings, providing matching methods exploit repeated observations time (e.g., -type analyses time-series cross-sectional environment).PSAgraphics:\nSpecializes visual diagnostics propensity score analyses, offering graphical tools inspect balance common support.PSAgraphics:\nSpecializes visual diagnostics propensity score analyses, offering graphical tools inspect balance common support.rbounds:\nConducts Rosenbaum bounds sensitivity analysis matched data. Researchers can examine hypothetical unmeasured confounder undermine estimated treatment effects.rbounds:\nConducts Rosenbaum bounds sensitivity analysis matched data. Researchers can examine hypothetical unmeasured confounder undermine estimated treatment effects.twang:\nImplements generalized boosted models (GBM) estimate propensity scores. Often used weighting approaches inverse probability weighting (IPW).twang:\nImplements generalized boosted models (GBM) estimate propensity scores. Often used weighting approaches inverse probability weighting (IPW).practice, choice software methods hinges study design, nature data, researcher’s theoretical expectations regarding treatment assignment.","code":""},{"path":"sec-matching-methods.html","id":"sec-selection-on-observables","chapter":"35 Matching Methods","heading":"35.9 Selection on Observables","text":"observational studies, treatment assignment typically randomized. poses challenge estimating causal effects, treated control groups may differ systematically. central assumption allows us estimate causal effects data selection observables, also known unconfoundedness conditional independence.Suppose observe binary treatment indicator \\(T_i \\\\{0, 1\\}\\) outcome \\(Y_i\\). unit \\(\\) two potential outcomes:\\(Y_i(1)\\): outcome treated\\(Y_i(0)\\): outcome untreatedHowever, one outcomes observed unit. average treatment effect treated (ATT) :\\[\n\\text{ATT} = \\mathbb{E}[Y(1) - Y(0) \\mid T = 1]\n\\]identify data, invoke conditional independence assumption:\\[\n(Y(0), Y(1)) \\perp T \\mid X\n\\]assumption means controlling covariates \\(X\\), treatment good randomly assigned. secondary assumption overlap:\\[\n0 < \\mathbb{P}(T = 1 \\mid X = x) < 1 \\quad \\text{} x\n\\]ensures every covariate profile, positive probability treated untreated.Matching attempts approximate conditions randomized experiment creating comparison group similar treated group terms observed covariates. Instead relying solely model-based adjustment (e.g., regression), matching balances covariate distribution across treatment groups estimation.","code":""},{"path":"sec-matching-methods.html","id":"matching-with-matchit","chapter":"35 Matching Methods","heading":"35.9.1 Matching with MatchIt","text":"demonstrate matching procedure using lalonde dataset, classic example causal inference literature, investigates effect job training subsequent earnings.focus estimating effect treatment (treat) earnings 1978 (re78), conditional covariates.Step 1: Planning AnalysisBefore conducting matching, several strategic decisions must made:Estimand: want ATT (effect treated), ATE (effect population), ATC (effect controls)? Matching typically targets ATT.Estimand: want ATT (effect treated), ATE (effect population), ATC (effect controls)? Matching typically targets ATT.Covariate Selection: include pre-treatment variables potential confounders—.e., affect treatment assignment outcome (Austin 2011; T. J. VanderWeele 2019).Covariate Selection: include pre-treatment variables potential confounders—.e., affect treatment assignment outcome (Austin 2011; T. J. VanderWeele 2019).Distance Measure: Choose quantify similarity units (e.g., propensity score, Mahalanobis distance).Distance Measure: Choose quantify similarity units (e.g., propensity score, Mahalanobis distance).Matching Method: Determine method (e.g., nearest neighbor, full matching, genetic matching).Matching Method: Determine method (e.g., nearest neighbor, full matching, genetic matching).demonstration, focus ATT using propensity score matching.Step 2: Assessing Initial ImbalanceWe first assess imbalance treatment control groups matching.summary provides:Standardized mean differencesStandardized mean differencesVariance ratiosVariance ratiosPropensity score distributionsPropensity score distributionsThese diagnostics help us understand extent covariate imbalance.Step 3: Implementing MatchingNearest Neighbor Matching (1:1 without Replacement)Matching based estimated propensity scores. treated unit matched closest control unit.Assess Balance MatchingInterpretation: Good matches show overlapping distributions covariates across groups, standardized differences 0.1 absolute value.Full MatchingAllows many--one one--many matches, minimizing overall distance.Exact MatchingOnly matches units exactly covariate values (usually categorical):Optimal MatchingMinimizes total distance matched units across sample.Genetic MatchingSearches weights assigned covariates optimize balance.Step 4: Estimating Treatment EffectsOnce matching complete, use matched data estimate treatment effects.coefficient treat estimate ATT. weights subclass clustering account matched design.","code":"\nlibrary(MatchIt)\ndata(\"lalonde\")\n# Estimate propensity scores with logistic regression\nm.out0 <- matchit(\n  treat ~ age + educ + race + married + nodegree + re74 + re75,\n  data = MatchIt::lalonde,\n  method = NULL,     # no matching, only estimates propensity scores\n  distance = \"glm\"\n)\n\n# Summary of balance statistics before matching\nsummary(m.out0)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ + race + married + nodegree + \n#>     re74 + re75, data = MatchIt::lalonde, method = NULL, distance = \"glm\")\n#> \n#> Summary of Balance for All Data:\n#>            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance          0.5774        0.1822          1.7941     0.9211    0.3774\n#> age              25.8162       28.0303         -0.3094     0.4400    0.0813\n#> educ             10.3459       10.2354          0.0550     0.4959    0.0347\n#> raceblack         0.8432        0.2028          1.7615          .    0.6404\n#> racehispan        0.0595        0.1422         -0.3498          .    0.0827\n#> racewhite         0.0973        0.6550         -1.8819          .    0.5577\n#> married           0.1892        0.5128         -0.8263          .    0.3236\n#> nodegree          0.7081        0.5967          0.2450          .    0.1114\n#> re74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\n#> re75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n#>            eCDF Max\n#> distance     0.6444\n#> age          0.1577\n#> educ         0.1114\n#> raceblack    0.6404\n#> racehispan   0.0827\n#> racewhite    0.5577\n#> married      0.3236\n#> nodegree     0.1114\n#> re74         0.4470\n#> re75         0.2876\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           429     185\n#> Matched       429     185\n#> Unmatched       0       0\n#> Discarded       0       0\nm.out1 <- matchit(\n  treat ~ age + educ + race + married + nodegree + re74 + re75,\n  data = MatchIt::lalonde,\n  method = \"nearest\",\n  distance = \"glm\"\n)\nsummary(m.out1, un = FALSE)  # only show post-matching stats\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ + race + married + nodegree + \n#>     re74 + re75, data = MatchIt::lalonde, method = \"nearest\", \n#>     distance = \"glm\")\n#> \n#> Summary of Balance for Matched Data:\n#>            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance          0.5774        0.3629          0.9739     0.7566    0.1321\n#> age              25.8162       25.3027          0.0718     0.4568    0.0847\n#> educ             10.3459       10.6054         -0.1290     0.5721    0.0239\n#> raceblack         0.8432        0.4703          1.0259          .    0.3730\n#> racehispan        0.0595        0.2162         -0.6629          .    0.1568\n#> racewhite         0.0973        0.3135         -0.7296          .    0.2162\n#> married           0.1892        0.2108         -0.0552          .    0.0216\n#> nodegree          0.7081        0.6378          0.1546          .    0.0703\n#> re74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\n#> re75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n#>            eCDF Max Std. Pair Dist.\n#> distance     0.4216          0.9740\n#> age          0.2541          1.3938\n#> educ         0.0757          1.2474\n#> raceblack    0.3730          1.0259\n#> racehispan   0.1568          1.0743\n#> racewhite    0.2162          0.8390\n#> married      0.0216          0.8281\n#> nodegree     0.0703          1.0106\n#> re74         0.2757          0.7965\n#> re75         0.2054          0.7381\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           429     185\n#> Matched       185     185\n#> Unmatched     244       0\n#> Discarded       0       0\n\n# Visual diagnostic: jitter plot of propensity scores\nplot(m.out1, type = \"jitter\", interactive = FALSE)\n\n# QQ plot for individual covariates\nplot(m.out1, type = \"qq\", which.xs = c(\"age\", \"re74\"))\nm.out2 <- matchit(\n  treat ~ age + educ + race + married + nodegree + re74 + re75,\n  data = MatchIt::lalonde,\n  method = \"full\",\n  distance = \"glm\",\n  link = \"probit\"\n)\n\nsummary(m.out2, un = FALSE)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ + race + married + nodegree + \n#>     re74 + re75, data = MatchIt::lalonde, method = \"full\", distance = \"glm\", \n#>     link = \"probit\")\n#> \n#> Summary of Balance for Matched Data:\n#>            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance          0.5773        0.5764          0.0045     0.9949    0.0043\n#> age              25.8162       25.5347          0.0393     0.4790    0.0787\n#> educ             10.3459       10.5381         -0.0956     0.6192    0.0253\n#> raceblack         0.8432        0.8389          0.0119          .    0.0043\n#> racehispan        0.0595        0.0492          0.0435          .    0.0103\n#> racewhite         0.0973        0.1119         -0.0493          .    0.0146\n#> married           0.1892        0.1633          0.0660          .    0.0259\n#> nodegree          0.7081        0.6577          0.1110          .    0.0504\n#> re74           2095.5737     2100.2150         -0.0009     1.3467    0.0314\n#> re75           1532.0553     1561.4420         -0.0091     1.5906    0.0536\n#>            eCDF Max Std. Pair Dist.\n#> distance     0.0486          0.0198\n#> age          0.2742          1.2843\n#> educ         0.0730          1.2179\n#> raceblack    0.0043          0.0162\n#> racehispan   0.0103          0.4412\n#> racewhite    0.0146          0.3454\n#> married      0.0259          0.4473\n#> nodegree     0.0504          0.9872\n#> re74         0.1881          0.8387\n#> re75         0.1984          0.8240\n#> \n#> Sample Sizes:\n#>               Control Treated\n#> All            429.       185\n#> Matched (ESS)   50.76     185\n#> Matched        429.       185\n#> Unmatched        0.         0\n#> Discarded        0.         0\nm.out3 <- matchit(\n  treat ~ race + nodegree,\n  data = MatchIt::lalonde,\n  method = \"exact\"\n)\nm.out4 <- matchit(\n  treat ~ age + educ + re74 + re75,\n  data = MatchIt::lalonde,\n  method = \"optimal\",\n  ratio = 2\n)\nm.out5 <- matchit(\n  treat ~ age + educ + re74 + re75,\n  data = MatchIt::lalonde,\n  method = \"genetic\"\n)\nlibrary(lmtest)\nlibrary(sandwich)\n\n# Extract matched data\nmatched_data <- match.data(m.out1)\n\n# Estimate ATT with robust standard errors\nmodel <- lm(re78 ~ treat + age + educ + race + re74 + re75,\n            data = matched_data,\n            weights = weights)\n\ncoeftest(model, vcov. = vcovCL, cluster = ~subclass)\n#> \n#> t test of coefficients:\n#> \n#>                Estimate  Std. Error t value Pr(>|t|)   \n#> (Intercept) -437.664937 1912.759171 -0.2288 0.819143   \n#> treat       1398.134870  723.745751  1.9318 0.054164 . \n#> age           -0.343085   39.256789 -0.0087 0.993032   \n#> educ         470.767350  147.892765  3.1832 0.001583 **\n#> racehispan  1518.303924 1035.083141  1.4668 0.143287   \n#> racewhite    557.295853  897.121013  0.6212 0.534856   \n#> re74           0.017244    0.166298  0.1037 0.917470   \n#> re75           0.226076    0.165722  1.3642 0.173357   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"sec-matching-methods.html","id":"reporting-standards","chapter":"35 Matching Methods","heading":"35.9.2 Reporting Standards","text":"ensure transparency reproducibility, always report following:Matching method (e.g., nearest neighbor, genetic)Matching method (e.g., nearest neighbor, genetic)Distance metric (e.g., propensity score via logistic regression)Distance metric (e.g., propensity score via logistic regression)Covariates matched justification inclusionCovariates matched justification inclusionBalance statistics (e.g., standardized mean differences /)Balance statistics (e.g., standardized mean differences /)Sample sizes: total, matched, unmatched, discardedSample sizes: total, matched, unmatched, discardedEstimation model: whether treatment effects estimated using regression adjustment, without weightsEstimation model: whether treatment effects estimated using regression adjustment, without weightsAssumptions: especially unconfoundedness overlapAssumptions: especially unconfoundedness overlap","code":""},{"path":"sec-matching-methods.html","id":"optimization-based-matching-via-designmatch","chapter":"35 Matching Methods","heading":"35.9.3 Optimization-Based Matching via designmatch","text":"advanced applications, designmatch package provides matching methods based combinatorial optimization.Notable methods:distmatch(): Distance-based matching custom constraintsdistmatch(): Distance-based matching custom constraintsbmatch(): Bipartite matching using linear programmingbmatch(): Bipartite matching using linear programmingcardmatch(): Cardinality matching maximum matched sample size balance constraintscardmatch(): Cardinality matching maximum matched sample size balance constraintsprofmatch(): Profile matching stratified treatment allocationprofmatch(): Profile matching stratified treatment allocationnmatch(): Non-bipartite matching (e.g., interference settings)nmatch(): Non-bipartite matching (e.g., interference settings)","code":"\nlibrary(designmatch)"},{"path":"sec-matching-methods.html","id":"matchingfrontier","chapter":"35 Matching Methods","heading":"35.9.4 MatchingFrontier","text":"mentioned MatchIt, make trade-(also known bias-variance trade-) balance sample size. automated procedure optimize trade-implemented MatchingFrontier (G. King, Lucas, Nielsen 2017), solves joint optimization problem.Following MatchingFrontier guide","code":"\n# library(devtools)\n# install_github('ChristopherLucas/MatchingFrontier')\nlibrary(MatchingFrontier)\ndata(\"lalonde\", package = \"MatchIt\")\n# choose var to match on\nmatch.on <-\n    colnames(lalonde)[!(colnames(lalonde) %in% c('re78', 'treat'))]\nmatch.on\n\n# Mahanlanobis frontier (default)\nmahal.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = \"treat\",\n        match.on = match.on\n    )\nmahal.frontier\n\n# L1 frontier\nL1.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = 'treat',\n        match.on = match.on,\n        QOI = 'SATT',\n        metric = 'L1',\n        ratio = 'fixed'\n    )\nL1.frontier\n\n# estimate effects along the frontier\n\n# Set base form\nmy.form <-\n    as.formula(re78 ~ treat + age + black + education \n               + hispanic + married + nodegree + re74 + re75)\n\n# Estimate effects for the mahalanobis frontier\nmahal.estimates <-\n    estimateEffects(\n        mahal.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Estimate effects for the L1 frontier\nL1.estimates <-\n    estimateEffects(\n        L1.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Plot covariates means \n# plotPrunedMeans()\n\n\n# Plot estimates (deprecated)\n# plotEstimates(\n#     L1.estimates,\n#     ylim = c(-10000, 3000),\n#     cex.lab = 1.4,\n#     cex.axis = 1.4,\n#     panel.first = grid(NULL, NULL, lwd = 2,)\n# )\n\n# Plot estimates\nplotMeans(L1.frontier)\n\n\n# parallel plot\nparallelPlot(\n    L1.frontier,\n    N = 400,\n    variables = c('age', 're74', 're75', 'black'),\n    treated.col = 'blue',\n    control.col = 'gray'\n)\n\n# export matched dataset\n# take 400 units\nmatched.data <- generateDataset(L1.frontier, N = 400) "},{"path":"sec-matching-methods.html","id":"sec-propensity-scores","chapter":"35 Matching Methods","heading":"35.9.5 Propensity Scores","text":"Propensity score methods widely used estimating causal effects observational studies, random assignment treatment control groups feasible. core idea mimic randomized experiment adjusting confounding variables predict treatment assignment. Formally, propensity score defined probability assignment treatment conditional observed covariates (Rosenbaum Rubin 1983, 1985):\\[\ne_i(X_i) = P(T_i = 1 \\mid X_i)\n\\]:\\(T_i \\\\{0, 1\\}\\) binary treatment indicator unit \\(\\),\\(X_i\\) vector observed pre-treatment covariates unit \\(\\).key insight Rosenbaum Rubin (1983) conditioning propensity score sufficient remove bias due confounding observed covariates, certain assumptions.","code":""},{"path":"sec-matching-methods.html","id":"assumptions-for-identification-1","chapter":"35 Matching Methods","heading":"35.9.5.1 Assumptions for Identification","text":"identify causal effects using propensity scores, following assumptions must hold:Unconfoundedness / Conditional Independence Assumption (CIA):\\[\n(Y_i(0), Y_i(1)) \\perp T_i \\mid X_i\n\\]Positivity (Overlap):\\[\n0 < P(T_i = 1 \\mid X_i) < 1 \\quad \\text{} \n\\]assumptions ensure unit, can observe comparable treated untreated units sample. Violations positivity, especially high-dimensional covariate spaces, critical weakness propensity score matching.","code":""},{"path":"sec-matching-methods.html","id":"why-psm-is-not-recommended-anymore","chapter":"35 Matching Methods","heading":"35.9.5.2 Why PSM Is Not Recommended Anymore","text":"Despite intuitive appeal, recent literature strongly cautioned using propensity score matching causal inference (G. King Nielsen 2019). main criticisms follows:Imbalance: PSM often fails achieve covariate balance better simpler techniques like covariate adjustment via regression exact matching. Matching propensity score scalar reduction multivariate distribution, reduction can distort multivariate relationships.Inefficiency: Discarding unmatched units reduces statistical efficiency, especially better estimators (e.g., inverse probability weighting doubly robust estimators) can use data.Model dependence: Small changes specification propensity score model can lead large changes matches estimated treatment effects.Bias: Poor matches irrelevant covariates can introduce additional bias rather reduce .Abadie Imbens (2016) show asymptotic distribution treatment effect estimators sensitive estimation propensity score :estimated propensity score can improve efficiency using true propensity score estimating ATE. Formally, adjustment asymptotic variance non-positive.However, ATT, sign adjustment data-dependent. Estimation error propensity score can lead misestimated confidence intervals: may wide narrow.result suggests even large samples, failure account estimation uncertainty propensity score can produce misleading inference.fundamental flaw PSM asymmetry match quality:\\(X_c = X_t\\), must \\(e(X_c) = e(X_t)\\).However, converse hold:\\(e(X_c) = e(X_t) \\nRightarrow X_c = X_t\\)Therefore, two units identical propensity scores may still differ substantially covariate space. undermines matching goal achieving similarity across covariates.","code":""},{"path":"sec-matching-methods.html","id":"estimating-the-propensity-score","chapter":"35 Matching Methods","heading":"35.9.5.3 Estimating the Propensity Score","text":"Estimation propensity score typically carried using:Parametric models:\nLogistic regression:\\[\n\\hat{e}_i = \\frac{1}{1 + \\exp(-X_i^\\top \\hat{\\beta})}\n\\]\nLogistic regression:\\[\n\\hat{e}_i = \\frac{1}{1 + \\exp(-X_i^\\top \\hat{\\beta})}\n\\]Nonparametric / machine learning methods:\nGeneralized Boosted Models (GBM)\nBoosted Classification Regression Trees (CART)\nRandom forests Bayesian Additive Regression Trees (BART)\nGeneralized Boosted Models (GBM)Boosted Classification Regression Trees (CART)Random forests Bayesian Additive Regression Trees (BART)machine learning approaches often yield better balance due flexible functional forms, also complicate interpretation inference.","code":""},{"path":"sec-matching-methods.html","id":"matching-algorithms","chapter":"35 Matching Methods","heading":"35.9.5.4 Matching Algorithms","text":"Reduce high-dimensional vector \\(X_i\\) scalar \\(\\hat{e}_i\\).Calculate distances treated control units based \\(\\hat{e}_i\\): \\[\nd(, j) = |\\hat{e}_i - \\hat{e}_j|\n\\]Match treated unit \\(\\) control unit \\(j\\) closest propensity score.Prune:\nControl units used match discarded.\nMatches exceeding caliper (maximum distance threshold) also discarded.\nControl units used match discarded.Matches exceeding caliper (maximum distance threshold) also discarded.replacement typically assumed — control unit matched .Let \\(c > 0\\) caliper. treated unit \\(\\) matched control unit \\(j\\) :\\[\n|\\hat{e}_i - \\hat{e}_j| < c\n\\]Caliper matching helps reduce poor matches, may result random pruning, reducing balance efficiency.","code":""},{"path":"sec-matching-methods.html","id":"practical-recommendations","chapter":"35 Matching Methods","heading":"35.9.5.5 Practical Recommendations","text":"include irrelevant covariates: Including variables unrelated outcome can increase variability estimated propensity score reduce matching quality.Avoid instrumental variables (Bhattacharya Vogt 2007): Including IVs propensity score model can inflate bias introducing variation treatment assignment unrelated potential outcomes.Focus covariates confounders, .e., affect treatment outcome.remains pruning consequential initial covariate set. Matching can discard large portion sample, distorts representativeness increases variance.","code":""},{"path":"sec-matching-methods.html","id":"diagnostics-and-evaluation","chapter":"35 Matching Methods","heading":"35.9.5.6 Diagnostics and Evaluation","text":"matching, primary diagnostic tool covariate balance. Key diagnostics include:Standardized mean differences (SMD) treatment groups matchingVariance ratiosVisual inspection via Love plots density plotsStatistical model fit criteria (e.g., AIC, BIC, c-statistics) valid evaluating propensity score models, since goal prediction, achieving balance.need worry collinearity covariates estimating propensity scores — unlike outcome regression models, multicollinearity can inflate standard errors.","code":""},{"path":"sec-matching-methods.html","id":"applications-in-business-and-finance","chapter":"35 Matching Methods","heading":"35.9.5.7 Applications in Business and Finance","text":"Propensity score methods used empirical finance marketing, though increasingly replaced robust approaches. One illustrative application found Hirtle, Kovner, Plosser (2020), investigates causal effect regulatory bank supervision firm-level outcomes:Treatment: Degree supervisory attention.Outcomes: Loan risk, profitability, volatility, firm growth.Method: Propensity score matching construct treated control groups banks comparable observed characteristics.matched sample analysis reveals intensified supervision leads :Lower risk (conservative loan portfolios)Reduced volatilityNo significant loss profitability growth","code":""},{"path":"sec-matching-methods.html","id":"conclusion-1","chapter":"35 Matching Methods","heading":"35.9.5.8 Conclusion","text":"theoretical motivation behind propensity scores remains sound, application via naive matching methods longer considered best practice. statistical community increasingly favors alternatives :Covariate adjustment via regressionInverse probability weighting (IPW)Doubly robust estimatorsTargeted Maximum Likelihood Estimation (TMLE)approaches better utilize full dataset, yield efficient estimators, offer transparent diagnostics. Matching estimated propensity scores may still useful illustration sensitivity analysis, primary method causal inference applied research.","code":""},{"path":"sec-matching-methods.html","id":"look-ahead-propensity-score-matching","chapter":"35 Matching Methods","heading":"35.9.5.9 Look-Ahead Propensity Score Matching","text":"observational data, estimating causal effects complicated self-selection bias: individuals receive treatment may differ systematically . Standard PSM methods attempt control biases matching treated untreated units observable characteristics.However, unobservable latent traits influence treatment assignment outcomes, traditional PSM may produce biased estimates. Bapna, Ramaprasad, Umyarov (2018) introduce Look-Ahead Propensity Score Matching (LA-PSM), novel technique leverages future behavior correct time-invariant unobserved confounding, particularly useful rare-event economic decisions.study Bapna, Ramaprasad, Umyarov (2018):Treatment: user pays premium subscription.Outcome: social engagement (songs listened, playlists created, friends made).Problem: users choosing subscribe systematically different (e.g., engaged).LA-PSM Implementation:Treated group: users subscribing Sept 2011 - June 2012.Treated group: users subscribing Sept 2011 - June 2012.Control group: users subscribe later (June 2012 - Jan 2015).Control group: users subscribe later (June 2012 - Jan 2015).Matching : observed demographics, past behavior.Matching : observed demographics, past behavior.Analysis: Difference--Differences estimation.Analysis: Difference--Differences estimation.Results: Premium adoption increases:Songs listened: +287.2%Playlists created: +1.92%Forum posts: +2.01%Friends added: +15.77%standard PSM:Match individuals received treatment individuals , based observed covariates.Assume selection treatment strongly ignorable, conditional covariates.Problem:Unobserved factors (e.g., ambition, risk tolerance) may bias treatment outcome.Unobserved factors (e.g., ambition, risk tolerance) may bias treatment outcome.Standard PSM correct .Standard PSM correct .Solution: Look-Ahead PSMMatch treated units future-treated units: haven’t yet received treatment future.Match treated units future-treated units: haven’t yet received treatment future.Future-treated units share unobservable traits current-treated units.Future-treated units share unobservable traits current-treated units.Thus, LA-PSM controls time-invariant unobserved confounders.Let:\\(\\\\{1, \\ldots, N\\}\\) index individuals.\\(\\\\{1, \\ldots, N\\}\\) index individuals.\\(D_i(t)\\) = 1 individual \\(\\) treated time \\(t\\), 0 otherwise.\\(D_i(t)\\) = 1 individual \\(\\) treated time \\(t\\), 0 otherwise.\\(X_i\\) = observed covariates.\\(X_i\\) = observed covariates.\\(U_i\\) = unobserved time-invariant confounders.\\(U_i\\) = unobserved time-invariant confounders.\\(Y_i(t)\\) = outcome time \\(t\\).\\(Y_i(t)\\) = outcome time \\(t\\).standard PSM:Match \\(\\mathbb{P}(D_i(t) = 1 \\mid X_i)\\).LA-PSM:Match current treated individuals future treated individuals based : \\[ \\mathbb{P}(D_i(t) = 1 \\mid X_i) \\]Match current treated individuals future treated individuals based : \\[ \\mathbb{P}(D_i(t) = 1 \\mid X_i) \\]require: \\[ \\exists \\, s > t \\quad \\text{} \\quad D_i(s) = 1 \\]require: \\[ \\exists \\, s > t \\quad \\text{} \\quad D_i(s) = 1 \\]Formally, define:Treatment group: \\[ T = \\{ \\mid D_i(t) = 1 \\} \\]Treatment group: \\[ T = \\{ \\mid D_i(t) = 1 \\} \\]Control group: \\[ C = \\{ j \\mid D_j(t) = 0 \\quad \\text{} \\quad \\exists \\, s>t : D_j(s) = 1 \\} \\]Control group: \\[ C = \\{ j \\mid D_j(t) = 0 \\quad \\text{} \\quad \\exists \\, s>t : D_j(s) = 1 \\} \\]Thus, treatment control groups “eventual adopters,” just different times.Proposition:\n\\(U_i\\) time-invariant affects treatment assignment outcomes, LA-PSM produces unbiased causal estimates weaker assumptions standard PSM.regular PSM, matching \\(X_i\\) adjust \\(U_i\\).LA-PSM, restricting controls future adopters, select units share latent traits \\(U_i\\) driving adoption.Thus, \\(U_i\\) balanced across treatment control, eliminating bias \\(U_i\\).Remaining bias due differences \\(X_i\\), adjusted via matching.Practical Implementation StepsIn Static Look-Ahead PSM, :Fix single matching window given time \\(t\\).Define:\nTreatment Group: Individuals receive treatment time \\(t\\).\nControl Group: Individuals yet treated \\(t\\) receive treatment later time \\(t'>t\\).\nTreatment Group: Individuals receive treatment time \\(t\\).Control Group: Individuals yet treated \\(t\\) receive treatment later time \\(t'>t\\).Estimate propensity scores based observed covariates \\(X\\).Match treated (future-treated) control units based propensity scores.ensures treated control individuals similar observed \\(X\\) similar unobserved \\(U\\), assuming \\(U\\) time-invariant.implement Static LA-PSM:Identify individuals treated now (treatment group) future adopters (control group).Estimate propensity scores \\(\\mathbb{P}(D=1 \\mid X)\\).Match treated control units using nearest-neighbor matching.Estimate treatment effects via regression matched sample.Important:Drop individuals never treated analysis.Drop individuals never treated analysis.compare current-treated future-treated!compare current-treated future-treated!Dynamic Look-Ahead PSM, :Allow time move forward (\\(t=1,2,3,\\dots\\)).time \\(t\\):\nDefine treated individuals (receive treatment \\(t\\)).\nDefine control individuals (untreated \\(t\\) adopt later).\nDefine treated individuals (receive treatment \\(t\\)).Define control individuals (untreated \\(t\\) adopt later).Repeat matching separately \\(t\\).Aggregate results across different \\(t\\) periods estimate overall effect.Dynamic LA-PSM flexible:updates control groups time.updates control groups time.Better handles situations treatment adoption spreads gradually time.Better handles situations treatment adoption spreads gradually time.implement Dynamic LA-PSM:time period \\(t\\):\nDefine treated-now = individuals treated \\(t\\).\nDefine control = individuals untreated \\(t\\) adopt later.\nDefine treated-now = individuals treated \\(t\\).Define control = individuals untreated \\(t\\) adopt later.Estimate propensity scores \\(\\mathbb{P}(D=1 \\mid X)\\) within time window.Match treated controls \\(t\\) separately.Pool matched samples \\(t\\) together.Estimate treatment effects using pooled sample.Important:Keep track time — matching done separately period.Keep track time — matching done separately period.Aggregate treatment effect estimates carefully (either averaging pooling matched data).Aggregate treatment effect estimates carefully (either averaging pooling matched data).Challenge:\nSimulate dataset hidden confounding implement Static Dynamic Look-Ahead PSM.\nCompare bias Standard PSM Randomized Assignment.","code":""},{"path":"sec-matching-methods.html","id":"sec-mahalanobis","chapter":"35 Matching Methods","heading":"35.9.6 Mahalanobis Distance Matching","text":"Mahalanobis Distance Matching method matching units observational studies based multivariate similarity covariates. Unlike propensity score matching, reduces covariate space single scalar, Mahalanobis distance operates full multivariate space. result, Mahalanobis matching can interpreted approximating fully blocked design, treated unit paired control unit nearly identical covariate values.ideal case: \\(X_t = X_c\\), implying perfect match.practice, Mahalanobis distance allows “near-exact” matches high-dimensional space.preserves multivariate structure covariates, Mahalanobis matching faithfully emulates randomization within covariate strata, making robust specification error propensity score matching.method particularly appealing number covariates relatively small covariates continuous well-measured.Given set \\(p\\) covariates \\(X_i \\\\mathbb{R}^p\\) unit \\(\\), Mahalanobis distance treated unit \\(t\\) control unit \\(c\\) defined :\\[\nD_M(X_t, X_c) = \\sqrt{(X_t - X_c)^\\top S^{-1}(X_t - X_c)}\n\\]:\\(X_t\\) \\(X_c\\) \\(p\\)-dimensional vectors covariates treated control units, respectively,\\(S\\) sample covariance matrix covariates across units (treated control),\\(S^{-1}\\) serves standardize decorrelate covariate space, accounting scale correlation among covariates.Use Euclidean Distance?Euclidean distance:\\[\nD_E(X_t, X_c) = \\sqrt{(X_t - X_c)^\\top (X_t - X_c)}\n\\]adjust different variances among covariates correlation . Mahalanobis distance corrects incorporating inverse covariance matrix \\(S^{-1}\\), effectively transforming data space covariates standardized orthogonal.makes Mahalanobis distance scale-invariant, .e., invariant affine transformations data, essential matching variables different units (e.g., income dollars age years).","code":""},{"path":"sec-matching-methods.html","id":"mahalanobis-matching-algorithm","chapter":"35 Matching Methods","heading":"35.9.6.1 Mahalanobis Matching Algorithm","text":"Let \\(\\mathcal{T}\\) set treated units \\(\\mathcal{C}\\) set control units. procedure Mahalanobis matching can described follows:Compute covariance matrix \\(S\\) covariates \\(X\\) across units.treated unit \\(\\\\mathcal{T}\\), compute \\(D_M(X_i, X_j)\\) \\(j \\\\mathcal{C}\\).Match treated unit \\(\\) control unit \\(j\\) smallest Mahalanobis distance.Prune:\nControl units used match discarded.\nMatches \\(D_M > \\delta\\) (caliper threshold) also discarded.\nControl units used match discarded.Matches \\(D_M > \\delta\\) (caliper threshold) also discarded.caliper \\(\\delta\\) used enforce maximum allowable distance. , match made \\(\\) \\(j\\) :\\[\nD_M(X_i, X_j) < \\delta\n\\]prevents poor-quality matches helps ensure treated control units meaningfully similar. match falls within caliper given treated unit, unit left unmatched, potentially discarded analysis.","code":""},{"path":"sec-matching-methods.html","id":"properties","chapter":"35 Matching Methods","heading":"35.9.6.2 Properties","text":"Scale-invariant: Standardizes variables using \\(S^{-1}\\), ensuring variables large scales dominate distance metric.Correlation-adjusted: Accounts linear relationships among covariates, critical multivariate contexts.Non-parametric: model treatment assignment estimated; purely based observed covariates.","code":""},{"path":"sec-matching-methods.html","id":"limitations","chapter":"35 Matching Methods","heading":"35.9.6.3 Limitations","text":"Sensitive multicollinearity: covariates highly collinear, covariance matrix \\(S\\) may nearly singular, making \\(S^{-1}\\) unstable non-invertible. Regularization techniques may required.suitable high-dimensional covariate spaces: \\(p\\) increases, exact even near-exact matches become harder find. Dimensionality reduction techniques (e.g., PCA) may help.Inefficient categorical variables: Since Mahalanobis distance based continuous covariates assumes multivariate normality (implicitly), ’s less effective covariates categorical binary.","code":""},{"path":"sec-matching-methods.html","id":"hybrid-approaches","chapter":"35 Matching Methods","heading":"35.9.6.4 Hybrid Approaches","text":"Mahalanobis matching can combined propensity scores improved performance:Within propensity score calipers: Match using Mahalanobis distance within groups units fall within specified caliper propensity score space.Stratified matching: Divide sample strata based propensity scores apply Mahalanobis matching within stratum.hybrid methods aim preserve robustness Mahalanobis matching mitigating weaknesses high dimensions.","code":""},{"path":"sec-matching-methods.html","id":"practical-considerations-7","chapter":"35 Matching Methods","heading":"35.9.6.5 Practical Considerations","text":"Estimate \\(S\\) pooled sample (treated + control) ensure consistency.Standardize covariates applying Mahalanobis distance ensure comparability numerical stability.Use diagnostic plots (e.g., QQ plots multivariate distance histograms) assess match quality.","code":""},{"path":"sec-matching-methods.html","id":"sec-cem","chapter":"35 Matching Methods","heading":"35.9.7 Coarsened Exact Matching (CEM)","text":"Coarsened Exact Matching (CEM) monotonic imbalance bounding matching method designed improve covariate balance treated control units observational studies. CEM operates coarsening continuous high-cardinality covariates discrete bins applying exact matching coarsened space. result non-parametric method prioritizes covariate balance robustness modeling assumptions.Introduced formalized Iacus, King, Porro (2012), CEM well-suited causal inference covariates noisy traditional modeling-based approaches (e.g., propensity scores) unstable hard interpret.central idea discretize covariates meaningful bins (either automatically manually), sort observations strata subclasses based unique combinations binned covariate values. Exact matching applied within stratum.Let \\(X_i \\\\mathbb{R}^p\\) \\(p\\)-dimensional covariate vector unit \\(\\). Define \\(C(X_i)\\) coarsened version \\(X_i\\), :Continuous variables binned intervals (e.g., age 20–29, 30–39, etc.)Categorical variables may grouped (e.g., Likert scales aggregated fewer categories), matching proceeds follows:Temporarily coarsen covariate space \\(X \\C(X)\\).Sort observations strata defined unique values \\(C(X)\\).Prune stratum contains treated control units.Retain original (uncoarsened) units matched strata analysis.process produces matched sample stratum includes least one treated one control unit, improving internal validity design.matching methods, CEM requires ignorability assumption (also known unconfoundedness selection observables):\\[ (Y_i(0), Y_i(1)) \\perp T_i \\mid X_i \\]assumption sufficient overlap coarsened strata, CEM yields unbiased estimates causal effects within matched sample.","code":""},{"path":"sec-matching-methods.html","id":"mathematical-properties","chapter":"35 Matching Methods","heading":"35.9.7.1 Mathematical Properties","text":"Monotonic Imbalance BoundingCEM Monotonic Imbalance Bounding method, meaning user can pre-specify maximum level imbalance allowed covariate. imbalance measure guaranteed weakly decreasing coarsening becomes finer. provides:Transparency: imbalance tradeoff determined ex ante user.Control: Users can make direct decision much imbalance tolerable.Formally, let \\(\\mathcal{L}(C(X))\\) denote measure imbalance coarsened covariates. , refinements \\(C(X)\\):\\[\n\\text{} C_1(X) \\preceq C_2(X), \\text{ } \\mathcal{L}(C_1(X)) \\leq \\mathcal{L}(C_2(X))\n\\], finer coarsenings (bins) increase imbalance.Congruence PrincipleCEM respects congruence principle, asserts analysis precise data allow. coarsening, CEM protects overfitting artificial precision, especially measurement error present.RobustnessRobust measurement error: Discretization makes matching less sensitive noise covariates.Works missing data: cem R package supports partial handling missingness.Multiple imputation: CEM can applied within imputation routines preserve matching structure across imputed datasets.Supports multi-valued treatments: Matching can extended treatments beyond binary \\(T \\\\{0, 1\\}\\).Load Prepare DataDefine Pre-Treatment CovariatesAutomatically Coarsen Matchmat$w contains weights matched units.mat$w contains weights matched units.Summary matched strata units retained.Summary matched strata units retained.Manual Coarsening (User-Controlled)Users may coarsen variables explicitly based theoretical knowledge data distribution.Example: Grouped Categorical Binned Continuous CovariatesThis allows domain-specific discretion enhances interpretability.","code":"\nlibrary(cem)\ndata(LeLonde)\n\n# Remove missing values\nLe <- na.omit(LeLonde)\n\n# Treatment and control indices\ntr <- which(Le$treated == 1)\nct <- which(Le$treated == 0)\nntr <- length(tr)\nnct <- length(ct)\n\n# Unadjusted bias (naïve difference in means)\nmean(Le$re78[tr]) - mean(Le$re78[ct])\n#> [1] 759.0479\nvars <- c(\n    \"age\", \"education\", \"black\", \"married\", \"nodegree\",\n    \"re74\", \"re75\", \"hispanic\", \"u74\", \"u75\", \"q1\"\n)\n\n# Pre-treatment imbalance\nimbalance(group = Le$treated, data = Le[vars]) # L1 imbalance = 0.902\n#> \n#> Multivariate Imbalance Measure: L1=0.902\n#> Percentage of local common support: LCS=5.8%\n#> \n#> Univariate Imbalance Measures:\n#> \n#>               statistic   type           L1 min 25%      50%       75%\n#> age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n#> education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n#> black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n#> married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n#> nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n#> re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n#> re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n#> hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n#> u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n#> u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n#> q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n#>                  max\n#> age          -6.0000\n#> education     1.0000\n#> black         0.0000\n#> married       0.0000\n#> nodegree      0.0000\n#> re74      -2139.0195\n#> re75        490.3945\n#> hispanic      0.0000\n#> u74           0.0000\n#> u75           0.0000\n#> q1                NA\nmat <- cem(\n    treatment = \"treated\",\n    data = Le,\n    drop = \"re78\",     # outcome variable\n    keep.all = TRUE    # retain unmatched units in output\n)\n#> \n#> Using 'treated'='1' as baseline group\nmat\n#>            G0  G1\n#> All       392 258\n#> Matched    95  84\n#> Unmatched 297 174\n# Inspect levels for grouping\nlevels(Le$q1)\n#> [1] \"agree\"             \"disagree\"          \"neutral\"          \n#> [4] \"no opinion\"        \"strongly agree\"    \"strongly disagree\"\n\n# Group Likert responses\nq1.grp <- list(\n    c(\"strongly agree\", \"agree\"),\n    c(\"neutral\", \"no opinion\"),\n    c(\"strongly disagree\", \"disagree\")\n)\n\n# Custom cutpoints for education\ntable(Le$education)\n#> \n#>   3   4   5   6   7   8   9  10  11  12  13  14  15 \n#>   1   5   4   6  12  55 106 146 173 113  19   9   1\neducut <- c(0, 6.5, 8.5, 12.5, 17)\n\n# Run CEM with manual coarsening\nmat1 <- cem(\n    treatment = \"treated\",\n    data = Le,\n    drop = \"re78\",\n    cutpoints = list(education = educut),\n    grouping = list(q1 = q1.grp)\n)\n#> \n#> Using 'treated'='1' as baseline group\nmat1\n#>            G0  G1\n#> All       392 258\n#> Matched   158 115\n#> Unmatched 234 143"},{"path":"sec-matching-methods.html","id":"progressive-coarsening","chapter":"35 Matching Methods","heading":"35.9.7.2 Progressive Coarsening","text":"CEM supports progressive coarsening, matching attempted fine coarsening first. insufficient matches found, coarsening gradually relaxed suitable number matched units retained.strategy balances:Precision (finer bins)Precision (finer bins)Sample size retention (coarser bins)Sample size retention (coarser bins)","code":""},{"path":"sec-matching-methods.html","id":"summary-2","chapter":"35 Matching Methods","heading":"35.9.7.3 Summary","text":"StrengthsTransparent tunable matching procedureTransparent tunable matching procedureNon-parametric: require estimation treatment assignment modelNon-parametric: require estimation treatment assignment modelRobust measurement error model misspecificationRobust measurement error model misspecificationSupports multi-valued treatments partial missingnessSupports multi-valued treatments partial missingnessTheoretically grounded via monotonic imbalance boundingTheoretically grounded via monotonic imbalance boundingLimitationsLoss information due coarseningLoss information due coarseningArbitrary binning may influence results theoretically motivatedArbitrary binning may influence results theoretically motivatedNot designed high-dimensional settings without careful variable selectionNot designed high-dimensional settings without careful variable selectionCannot account unobserved confoundingCannot account unobserved confoundingCoarsened Exact Matching offers powerful, transparent, theoretically principled method preprocessing observational data estimating causal effects. overcomes several limitations traditional matching approaches, particularly propensity score matching, giving researchers direct control quality matches anchoring inference empirical balance rather model-dependent estimates.","code":""},{"path":"sec-matching-methods.html","id":"sec-genetic-matching","chapter":"35 Matching Methods","heading":"35.9.8 Genetic Matching","text":"Genetic Matching (GM) generalization propensity score Mahalanobis distance matching, leveraging search algorithm inspired evolutionary biology optimize covariate balance. Unlike classical matching techniques, rely pre-specified distance metrics, Genetic Matching learns optimal distance metric iterative process aimed minimizing imbalance treated control groups.Genetic Matching introduced Diamond Sekhon (2013) designed improve balance observed covariates selecting optimal weights covariate. core idea define generalized Mahalanobis distance metric, weights used distance calculation chosen genetic search algorithm. process adaptively explores space possible weighting matrices evolves toward set weights result optimal covariate balance across treatment groups.method combines two components:Propensity Score Matching: Balances estimated probability treatment assignment given covariates.Mahalanobis Distance Matching: Accounts correlations among covariates ensures geometric proximity multivariate space.Genetic Matching approach treats selection covariate weights optimization problem, solved using techniques inspired natural selection—mutation, crossover, survival fittest.Compared traditional matching methods :Nearest Neighbor Matching: May result poor balance high-dimensional imbalanced datasets.Full Matching: efficient always feasible severe imbalance.Genetic Matching particularly powerful situations traditional distance metrics insufficient achieve balance. adaptively searches weight space ensure better covariate overlap, crucial unbiased causal inference.empirical settings, often results superior balance observed covariates credible estimates treatment effects.Let:\\(T_i \\\\{0, 1\\}\\) binary treatment indicator unit \\(\\).\\(T_i \\\\{0, 1\\}\\) binary treatment indicator unit \\(\\).\\(\\mathbf{X}_i \\\\mathbb{R}^p\\) covariate vector unit \\(\\), \\(p\\) number observed covariates.\\(\\mathbf{X}_i \\\\mathbb{R}^p\\) covariate vector unit \\(\\), \\(p\\) number observed covariates.\\(\\mathbf{W}\\) \\(p \\times p\\) positive definite weight matrix optimized algorithm.\\(\\mathbf{W}\\) \\(p \\times p\\) positive definite weight matrix optimized algorithm.generalized Mahalanobis distance unit \\(\\) \\(j\\) defined :\\[\nD_{ij} = \\sqrt{(\\mathbf{X}_i - \\mathbf{X}_j)^\\top \\mathbf{W} (\\mathbf{X}_i - \\mathbf{X}_j)}\n\\], \\(\\mathbf{W}\\) necessarily inverse covariance matrix (standard Mahalanobis distance), data-driven weighting matrix found Genetic Matching optimize covariate balance.genetic algorithm optimizes weights \\(\\mathbf{W}\\) minimize global imbalance metric \\(\\mathcal{B}\\) across covariates. can involve different balance criteria:Paired \\(t\\)-tests continuous dichotomous variablesKolmogorov–Smirnov (K–S) test statistics distributional similarityStandardized mean differencesLet \\(b_k\\) denote balance metric \\(k\\)th covariate. total imbalance :\\[\n\\mathcal{B} = \\sum_{k=1}^{p} w_k b_k^2\n\\]optimization objective becomes finding \\(\\mathbf{W}\\) minimizes \\(\\mathcal{B}\\).Genetic Matching implemented Matching package R via GenMatch() function. function:Accepts treatment indicator (Tr) matrix covariates (X).Optionally takes Balance Matrix (BalanceMatrix) specifies variables balanced.Uses genetic search find weight matrix best achieves balance specified variables.Balance assessed via:Paired \\(t\\)-tests dichotomous continuous variablesKolmogorov-Smirnov (K–S) tests distributional differencesMatching can performed:without replacement (replacement often improves match quality cost increased variance).different estimands: Average Treatment Effect, Average Treatment Effect Treated, Average Treatment Effect Control.Extensions ConsiderationsThe method can extended multinomial treatments (via generalized entropy balancing).method can extended multinomial treatments (via generalized entropy balancing).longitudinal panel data, entropy balancing can adapted match across time points.longitudinal panel data, entropy balancing can adapted match across time points.One avoid balancing post-treatment variables, bias estimates.One avoid balancing post-treatment variables, bias estimates.Entropy balancing fits naturally modern workflows causal inference, especially valuable researchers want prioritize design modeling.","code":"\nlibrary(Matching)\ndata(lalonde, package = \"MatchIt\")\nattach(lalonde)\n\n# Define the covariates to match on\nX <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n\n# Define the covariates to balance on\nBalanceMat <- cbind(\n  age, educ, black, hisp, married, nodegr,\n  u74, u75, re75, re74,\n  I(re74 * re75) # Include interaction term\n)\n\n# Genetic Matching to optimize covariate balance\n# Note: pop.size = 16 is too small for real applications\ngenout <- GenMatch(\n  Tr = treat,\n  X = X,\n  BalanceMatrix = BalanceMat,\n  estimand = \"ATE\",\n  M = 1,\n  pop.size = 16,\n  max.generations = 10,\n  wait.generations = 1\n)\n\n# Define the outcome variable\nY <- re78 / 1000\n\n# Perform matching with the optimized weights\nmout <- Match(\n  Y = Y,\n  Tr = treat,\n  X = X,\n  estimand = \"ATE\",\n  Weight.matrix = genout\n)\n\n# Summarize the treatment effect estimates\nsummary(mout)\n\n# Assess post-matching balance\nmb <- MatchBalance(\n  treat ~ age + educ + black + hisp + married + nodegr +\n    u74 + u75 + re75 + re74 + I(re74 * re75),\n  match.out = mout,\n  nboots = 500\n)"},{"path":"sec-matching-methods.html","id":"entropy-balancing","chapter":"35 Matching Methods","heading":"35.9.9 Entropy Balancing","text":"Entropy balancing data preprocessing technique causal inference observational studies. Introduced Hainmueller (2012), particularly useful settings binary treatments covariate imbalance treated control groups threatens validity causal estimates.method applies maximum entropy reweighting scheme assign unit weights control group observations reweighted covariate distribution control group matches sample moments (e.g., means, variances) treated group.Entropy balancing directly targets covariate balance avoids trial--error iterative propensity score matching methods. also reduces reliance outcome models ensuring pre-treatment covariates aligned groups.Goal: Create set weights control group covariate distribution matches treated group.Approach: Solve constrained optimization problem minimizes information loss (measured via Shannon entropy) subject balance constraints covariates.Entropy balancing:Achieves exact balance specified covariate moments (e.g., means, variances).Achieves exact balance specified covariate moments (e.g., means, variances).Produces unique, data-driven weights without need iterative matching.Produces unique, data-driven weights without need iterative matching.compatible outcome model second stage (e.g., weighted regression, IPW, , etc.).compatible outcome model second stage (e.g., weighted regression, IPW, , etc.).Advantages Entropy BalancingExact balance: Guarantees moment balance without iterative diagnostics.Flexibility: Can balance higher moments, interactions, non-linear transformations.Model independence: Reduces reliance correct specification outcome model.Stability: optimization procedure smooth produces stable, interpretable weights.Entropy balancing especially useful high-dimensional settings working small treated groups, matching can perform poorly fail entirely.Suppose \\(n_1\\) treated units \\(n_0\\) control units, covariates \\(\\mathbf{X}_i \\\\mathbb{R}^p\\) unit \\(\\). Let \\(T_i \\\\{0,1\\}\\) treatment indicator.Let treatment group’s sample moment vector :\\[\n\\bar{\\mathbf{X}}_T = \\frac{1}{n_1} \\sum_{: T_i=1} \\mathbf{X}_i\n\\]seek set weights \\(\\{w_i\\}_{: T_i=0}\\) control units :Covariate balancing constraints (e.g., mean balance):\\[\n\\sum_{: T_i=0} w_i \\mathbf{X}_i = \\bar{\\mathbf{X}}_T\n\\]Minimum entropy divergence uniform weights:minimize Kullback-Leibler divergence new weights \\(w_i\\) uniform base weights \\(w_i^{(0)} = \\frac{1}{n_0}\\):\\[\n\\min_{\\{w_i\\}} \\sum_{: T_i=0} w_i \\log \\left( \\frac{w_i}{w_i^{(0)}} \\right)\n\\]subject :\\(\\sum w_i = 1\\) (weights must sum 1)\\(\\sum w_i \\mathbf{X}_i = \\bar{\\mathbf{X}}_T\\)(Optionally) higher-order moments, e.g., variances, skewness, etc.convex optimization problem, ensuring unique global solution.","code":"\n# Load package\nlibrary(ebal)\n\n# Simulate data\nset.seed(123)\nn_treat <- 50\nn_control <- 100\n\n# Covariates\nX_treat <- matrix(rnorm(n_treat * 3, mean = 1), ncol = 3)\nX_control <- matrix(rnorm(n_control * 3, mean = 0), ncol = 3)\nX <- rbind(X_control, X_treat)  # Order: control first, then treated\n\n# Treatment vector: 0 = control, 1 = treated\ntreatment <- c(rep(0, n_control), rep(1, n_treat))\n\n# Apply entropy balancing\neb_out <- ebalance(Treatment = treatment, X = X)\n\n# Simulate outcome variable\nY_control <- rnorm(n_control, mean = 1)\nY_treat <- rnorm(n_treat, mean = 3)\nY <- c(Y_control, Y_treat)\n\n# Construct weights: treated get weight 1, control get weights from ebalance\nweights <- c(eb_out$w, rep(1, n_treat))\n\n# Estimate ATE using weighted linear regression\ndf <- data.frame(Y = Y, treat = treatment, weights = weights)\nfit <- lm(Y ~ treat, data = df, weights = weights)\nsummary(fit)"},{"path":"sec-matching-methods.html","id":"matching-for-high-dimensional-data","chapter":"35 Matching Methods","heading":"35.9.10 Matching for High-Dimensional Data","text":"dimensionality covariates increases, traditional matching methods (nearest neighbor matching using propensity scores Mahalanobis distance) become increasingly unreliable. issue, often referred curse dimensionality, undermines ability find good matches distances points become less informative high-dimensional space.high-dimensional settings, number covariates large (potentially exceeding number observations), careful preprocessing necessary reduce dimensionality matching can effectively applied. following approaches commonly used mitigate challenges reducing feature space preserving meaningful structure relevant treatment assignment outcomes.","code":""},{"path":"sec-matching-methods.html","id":"dimensionality-reduction-techniques","chapter":"35 Matching Methods","heading":"35.9.10.1 Dimensionality Reduction Techniques","text":"variety dimensionality reduction techniques can employed prior matching. method distinct assumptions use cases:Lasso Regression (Least Absolute Shrinkage Selection Operator)\nLasso imposes \\(L_1\\) penalty regression coefficients, effectively shrinking coefficients zero. property particularly useful high-dimensional settings, performs regularization variable selection.\nLasso can used identify smaller set covariates predictive treatment assignment outcome, can used matching procedures (Gordon et al. 2019).Lasso Regression (Least Absolute Shrinkage Selection Operator)\nLasso imposes \\(L_1\\) penalty regression coefficients, effectively shrinking coefficients zero. property particularly useful high-dimensional settings, performs regularization variable selection.\nLasso can used identify smaller set covariates predictive treatment assignment outcome, can used matching procedures (Gordon et al. 2019).Penalized Logistic Regression\ntreatment binary, penalized logistic regression (e.g., using \\(L_1\\) \\(L_2\\) penalty) can employed estimate propensity scores avoiding overfitting high-dimensional spaces. penalized models provide stable estimates propensity score, essential reliable matching (Eckles Bakshy 2021).Penalized Logistic Regression\ntreatment binary, penalized logistic regression (e.g., using \\(L_1\\) \\(L_2\\) penalty) can employed estimate propensity scores avoiding overfitting high-dimensional spaces. penalized models provide stable estimates propensity score, essential reliable matching (Eckles Bakshy 2021).Principal Component Analysis (PCA)\nPCA unsupervised linear transformation projects original features lower-dimensional space retaining directions maximum variance. PCA consider treatment assignment directly, effective denoising compressing data, particularly covariates highly correlated.\nprincipal components can used inputs standard matching methods.Principal Component Analysis (PCA)\nPCA unsupervised linear transformation projects original features lower-dimensional space retaining directions maximum variance. PCA consider treatment assignment directly, effective denoising compressing data, particularly covariates highly correlated.\nprincipal components can used inputs standard matching methods.Locality Preserving Projections (LPP)\nLPP linear dimensionality reduction technique , unlike PCA, preserves local neighborhood structures. constructs similarity graph projects data lower-dimensional space nearby points remain close. locality-preserving property beneficial matching, helps maintain integrity local relationships within data (S. Li et al. 2016).Locality Preserving Projections (LPP)\nLPP linear dimensionality reduction technique , unlike PCA, preserves local neighborhood structures. constructs similarity graph projects data lower-dimensional space nearby points remain close. locality-preserving property beneficial matching, helps maintain integrity local relationships within data (S. Li et al. 2016).Random Projection\nRandom projection reduces dimensionality projecting data onto lower-dimensional subspace using random matrix. computationally efficient theoretical guarantees (e.g., Johnson–Lindenstrauss lemma) distances points approximately preserved. makes viable option extremely high-dimensional data exact structure preservation less critical.Random Projection\nRandom projection reduces dimensionality projecting data onto lower-dimensional subspace using random matrix. computationally efficient theoretical guarantees (e.g., Johnson–Lindenstrauss lemma) distances points approximately preserved. makes viable option extremely high-dimensional data exact structure preservation less critical.Autoencoders\nAutoencoders neural network architectures designed learn efficient, nonlinear representations (encodings) data. autoencoder consists encoder compresses input decoder attempts reconstruct original input compressed representation.\nAutoencoders particularly effective capturing complex, nonlinear relationships among features, may missed linear methods like PCA. latent representation obtained encoder can used matching (Ramachandra 2018).Autoencoders\nAutoencoders neural network architectures designed learn efficient, nonlinear representations (encodings) data. autoencoder consists encoder compresses input decoder attempts reconstruct original input compressed representation.\nAutoencoders particularly effective capturing complex, nonlinear relationships among features, may missed linear methods like PCA. latent representation obtained encoder can used matching (Ramachandra 2018).","code":""},{"path":"sec-matching-methods.html","id":"joint-dimensionality-reduction-and-distribution-balancing","chapter":"35 Matching Methods","heading":"35.9.10.2 Joint Dimensionality Reduction and Distribution Balancing","text":"Rather performing dimensionality reduction matching separate steps, emerging strategy jointly learn representations simultaneously:Reduce dimensionality, andBalance covariate distributions treated control groups.exemplified representation learning causal inference approaches. One method, proposed Yao et al. (2018), integrates neural networks matching objectives learn latent representations covariates low-dimensional balanced. representations optimized distributions treated control units latent space similar (e.g., using maximum mean discrepancy balancing metrics), thereby improving quality matches robustness treatment effect estimates.","code":""},{"path":"sec-matching-methods.html","id":"summary-of-approaches","chapter":"35 Matching Methods","heading":"35.9.10.3 Summary of Approaches","text":"","code":""},{"path":"sec-matching-methods.html","id":"practical-considerations-8","chapter":"35 Matching Methods","heading":"35.9.10.4 Practical Considerations","text":"Model selection validation: applying dimensionality reduction prior matching, one must ensure reduced representation still contains sufficient information confounding adjustment. Cross-validation balance metrics (e.g., standardized mean differences) used assess adequacy transformation.Model selection validation: applying dimensionality reduction prior matching, one must ensure reduced representation still contains sufficient information confounding adjustment. Cross-validation balance metrics (e.g., standardized mean differences) used assess adequacy transformation.Interpretability: methods like PCA autoencoders can effective, may obscure interpretability matches since transformed features may correspond original covariates. Sparse methods like Lasso retain interpretability selecting original covariates.Interpretability: methods like PCA autoencoders can effective, may obscure interpretability matches since transformed features may correspond original covariates. Sparse methods like Lasso retain interpretability selecting original covariates.Computational efficiency: Techniques random projection penalized regression computationally efficient scalable large datasets, autoencoders joint learning approaches may require extensive training hyperparameter tuning.Computational efficiency: Techniques random projection penalized regression computationally efficient scalable large datasets, autoencoders joint learning approaches may require extensive training hyperparameter tuning.","code":""},{"path":"sec-matching-methods.html","id":"matching-for-multiple-treatments","chapter":"35 Matching Methods","heading":"35.9.11 Matching for Multiple Treatments","text":"many applied settings, researchers face challenge estimating causal effects two treatment levels. example, marketing campaign may three variants (e.g., control, light exposure, heavy exposure), policy evaluation may involve multiple interventions. Standard binary treatment matching methods fall short cases, necessitating methodological extensions.Suppose dataset includes \\(T\\) distinct treatment groups: \\(\\mathcal{T} = \\{0, 1, ..., T-1\\}\\). , \\(T = 0\\) typically denotes control group, \\(T \\\\{1, ..., T-1\\}\\) active treatments. goal estimate Average Treatment Effect Pairwise Treatment Effects, \\(\\text{ATE}_{j,k} = \\mathbb{E}[Y(j) - Y(k)]\\) \\(j, k \\\\mathcal{T}\\).Key challenges setting include:Ensuring common support across multiple groupsAdjusting confounders balanced way across treatment pairsManaging covariate imbalance dimensionality number treatments increases","code":""},{"path":"sec-matching-methods.html","id":"matching-approaches-for-multiple-treatments","chapter":"35 Matching Methods","heading":"35.9.11.1 Matching Approaches for Multiple Treatments","text":"Several strategies exist matching presence multiple treatments:Generalized Propensity Scores (GPS)generalized propensity score defined conditional probability receiving treatment level given covariates:\\[\ne_t(X) = \\mathbb{P}(T = t \\mid X), \\quad \\text{} t = 0, 1, ..., T-1\n\\]Estimation typically done using multinomial logistic regression. GPS scores estimated, matching can proceed via:One-vs-: treatment level, match treated units others combined.Pairwise matching: Conduct separate pairwise comparisons treatment levels.Full matching: Attempt construct global matched sample covering treatments, using GPS vector.(McCaffrey et al. 2013) provides comprehensive overview GPS-based methods, including implementation twang package.Covariate Balancing Propensity Scores (CBPS) Multiple Treatments(Lopez Gutman 2017) extend CBPS multinomial case. Rather merely estimating GPS, CBPS directly optimizes covariate balance across treatment groups estimating GPS parameters. dual-objective estimation leads robust causal estimates finite samples.Kernel Distance-Based Matching Multinomial ScoresInstead reducing GPS scalar scores, matching can performed using full vector propensity scores, using distance metrics Euclidean Mahalanobis distance GPS space. approach aligns Q.-Y. Zhao et al. (2021), also consider continuous treatments using Generalized Propensity Score Density Estimation.","code":""},{"path":"sec-matching-methods.html","id":"matching-with-multiple-treatments-using-matchit-and-alternatives","chapter":"35 Matching Methods","heading":"35.9.11.2 Matching with Multiple Treatments Using MatchIt and Alternatives","text":"MatchIt package R originally developed binary treatment settings mind, can adapted handle multiple treatment groups pairwise matching careful design. However, approach requires manual data preparation clear understanding causal estimands interest—ATT, ATC, ATE.Pairwise Matching Shared Control GroupTo estimate effect multiple treatments compared shared control group, one straightforward method perform separate pairwise matchings control group treatment group:approach estimates ATT treatment group relative control. , treated group (e.g., , B, C), ask: outcome received treatment instead received control?Estimating ATC Using MatchItIn cases, may wish estimate ATC—.e., happened control group received treatments. , match control units treated group, reversing focal population. Practically, means keeping control group intact separately matching treatment group .Regression Effect EstimationAfter constructing matched dataset, one can use regression matched sample estimate treatment effects:Caveats LimitationsMatching one treatment group time preserve global covariate balance across treatment groups, pairwise balance control.Matching one treatment group time preserve global covariate balance across treatment groups, pairwise balance control.Overlap (common support) assumptions verified individually pairwise comparison.Overlap (common support) assumptions verified individually pairwise comparison.Matched samples may vary comparison, complicating aggregate inference joint hypothesis testing.Matched samples may vary comparison, complicating aggregate inference joint hypothesis testing.","code":"\n# Load required libraries\nlibrary(MatchIt)\nlibrary(cobalt)  # For balance checking\n\n# Example dataset: Simulated\nset.seed(123)\nn <- 400\ndf <- data.frame(\n  treat = factor(sample(c(\"control\", \"A\", \"B\", \"C\"), n, replace = TRUE)),\n  cov1 = rnorm(n),\n  cov2 = runif(n),\n  cov3 = rbinom(n, 1, 0.5),\n  outcome = rnorm(n)\n)\n\n# Define treatment levels\ntreatment_levels <- c(\"A\", \"B\", \"C\")\ncontrol_level <- \"control\"\n\n# Initialize weight column\ndf$match.weights <- 0\n\n# Perform pairwise matching of each treatment group vs control\nfor (treat in treatment_levels) {\n  # Subset to control and current treatment\n  subset_df <- df[df$treat %in% c(treat, control_level), ]\n  \n  # Create a binary treatment variable: 1 for current treatment, 0 for control\n  subset_df$treat_binary <- as.numeric(subset_df$treat == treat)\n\n  # Run matching\n  m.out <- matchit(treat_binary ~ cov1 + cov2 + cov3,\n                   data = subset_df, method = \"nearest\")\n\n  # Assign weights back to the original dataset\n  matched_units <- names(m.out$weights[m.out$weights > 0])\n  df[matched_units, \"match.weights\"] <- m.out$weights[matched_units]\n}\n\n# Check covariate balance\nbal.tab(treat ~ cov1 + cov2 + cov3, data = df,\n        weights = df$match.weights, method = \"matching\")\n#> Balance summary across all treatment pairs\n#>         Type Max.Diff.Adj\n#> cov1 Contin.       0.2200\n#> cov2 Contin.       0.1416\n#> cov3  Binary       0.0412\n#> \n#> Sample sizes\n#>           A  B  C control\n#> All     104 97 85     114\n#> Matched 104 97 85     114\n\n# Estimate treatment effects using weighted regression\nmodel <- glm(outcome ~ relevel(treat, ref = \"control\"),\n             data = df[df$match.weights > 0, ],\n             weights = match.weights)\n\nsummary(model)\n#> \n#> Call:\n#> glm(formula = outcome ~ relevel(treat, ref = \"control\"), data = df[df$match.weights > \n#>     0, ], weights = match.weights)\n#> \n#> Coefficients:\n#>                                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)                      -0.03758    0.09349  -0.402    0.688\n#> relevel(treat, ref = \"control\")A  0.08584    0.13536   0.634    0.526\n#> relevel(treat, ref = \"control\")B  0.06877    0.13789   0.499    0.618\n#> relevel(treat, ref = \"control\")C  0.03806    0.14305   0.266    0.790\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.9964463)\n#> \n#>     Null deviance: 395.05  on 399  degrees of freedom\n#> Residual deviance: 394.59  on 396  degrees of freedom\n#> AIC: 1139.7\n#> \n#> Number of Fisher Scoring iterations: 2\n# Load required libraries\nlibrary(MatchIt)\nlibrary(cobalt)\n\n# Simulate example data\nset.seed(456)\nn <- 400\ndf <- data.frame(\n  treat = factor(sample(c(\"control\", \"A\", \"B\", \"C\"), n, replace = TRUE)),\n  cov1 = rnorm(n),\n  cov2 = runif(n),\n  cov3 = rbinom(n, 1, 0.5),\n  outcome = rnorm(n)\n)\n\n# Define treatment levels\ntreatment_levels <- c(\"A\", \"B\", \"C\")\ncontrol_level <- \"control\"\n\n# Initialize weight column\ndf$match.weights <- 0\n\n# Estimate ATC by matching treated units to the control group\nfor (treat in treatment_levels) {\n  # Subset to current treatment and control\n  subset_df <- df[df$treat %in% c(treat, control_level), ]\n  \n  # Binary treatment variable: 0 for treatment group, 1 for control\n  # This reverses the focus, targeting the control as the treated group\n  subset_df$treat_binary <- as.numeric(subset_df$treat == control_level)\n\n  # Perform matching\n  m.out <- matchit(treat_binary ~ cov1 + cov2 + cov3,\n                   data = subset_df, method = \"nearest\")\n  \n  # Extract matched unit IDs\n  matched_ids <- names(m.out$weights[m.out$weights > 0])\n  \n  # Assign weights back to original dataset\n  df[matched_ids, \"match.weights\"] <- m.out$weights[matched_ids]\n}\n\n# Check balance (optional)\nbal.tab(treat ~ cov1 + cov2 + cov3, data = df,\n        weights = df$match.weights, method = \"matching\")\n#> Balance summary across all treatment pairs\n#>         Type Max.Diff.Adj\n#> cov1 Contin.       0.1229\n#> cov2 Contin.       0.2695\n#> cov3  Binary       0.1985\n#> \n#> Sample sizes\n#>            A  B  C control\n#> All       99 90 98     113\n#> Matched   99 90 98     111\n#> Unmatched  0  0  0       2\n\n# Estimate ATC via weighted regression\nmodel <- glm(outcome ~ relevel(treat, ref = \"control\"),\n             data = df[df$match.weights > 0, ],\n             weights = match.weights)\n\nsummary(model)\n#> \n#> Call:\n#> glm(formula = outcome ~ relevel(treat, ref = \"control\"), data = df[df$match.weights > \n#>     0, ], weights = match.weights)\n#> \n#> Coefficients:\n#>                                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)                       0.06826    0.09176   0.744    0.457\n#> relevel(treat, ref = \"control\")A -0.05103    0.13364  -0.382    0.703\n#> relevel(treat, ref = \"control\")B  0.08667    0.13713   0.632    0.528\n#> relevel(treat, ref = \"control\")C  0.09488    0.13400   0.708    0.479\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.9346181)\n#> \n#>     Null deviance: 369.69  on 397  degrees of freedom\n#> Residual deviance: 368.24  on 394  degrees of freedom\n#> AIC: 1108.5\n#> \n#> Number of Fisher Scoring iterations: 2\n# Subset the data first\nmatched_data <- df[df$match.weights > 0, ]\n\n# Then fit the weighted regression using the weights from the subset\nmodel <- glm(outcome ~ relevel(treat, ref = \"control\"),\n             data = matched_data,\n             weights = matched_data$match.weights)\n\nsummary(model)\n#> \n#> Call:\n#> glm(formula = outcome ~ relevel(treat, ref = \"control\"), data = matched_data, \n#>     weights = matched_data$match.weights)\n#> \n#> Coefficients:\n#>                                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)                       0.06826    0.09176   0.744    0.457\n#> relevel(treat, ref = \"control\")A -0.05103    0.13364  -0.382    0.703\n#> relevel(treat, ref = \"control\")B  0.08667    0.13713   0.632    0.528\n#> relevel(treat, ref = \"control\")C  0.09488    0.13400   0.708    0.479\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.9346181)\n#> \n#>     Null deviance: 369.69  on 397  degrees of freedom\n#> Residual deviance: 368.24  on 394  degrees of freedom\n#> AIC: 1108.5\n#> \n#> Number of Fisher Scoring iterations: 2"},{"path":"sec-matching-methods.html","id":"alternative-weighting-for-multiple-treatments-with-weightit","chapter":"35 Matching Methods","heading":"35.9.11.3 Alternative: Weighting for Multiple Treatments with WeightIt","text":"Matching can cumbersome multiple-treatment settings. Weighting approaches offer seamless framework, especially estimating ATE ATC across treatment levels simultaneously. WeightIt package extends propensity score weighting multi-treatment scenarios strong support diagnostics flexible estimation.Additional Notes:estimate ATT specific treatment (e.g., treatment ), set focal = \"\" estimand = \"ATT\" weightit().estimate ATT specific treatment (e.g., treatment ), set focal = \"\" estimand = \"ATT\" weightit().Setting method = \"gbm\" WeightIt use boosted models, equivalent twang package.Setting method = \"gbm\" WeightIt use boosted models, equivalent twang package.Weighting approaches avoid sample size loss discarding unmatched units provide smoother covariate balance optimization.Weighting approaches avoid sample size loss discarding unmatched units provide smoother covariate balance optimization.","code":"\n# Load required libraries\nlibrary(WeightIt)\nlibrary(cobalt)\n\n# Simulate example data\nset.seed(789)\nn <- 400\ndf <- data.frame(\n  treat = factor(sample(c(\"control\", \"A\", \"B\", \"C\"), n, replace = TRUE)),\n  cov1 = rnorm(n),\n  cov2 = runif(n),\n  cov3 = rbinom(n, 1, 0.5),\n  outcome = rnorm(n)\n)\n\n# Estimate weights for ATE across all treatment levels using multinomial logistic regression\nw.out <- weightit(\n  treat ~ cov1 + cov2 + cov3,\n  data = df,\n  estimand = \"ATE\",\n  method = \"glm\"  # multinomial model when treat is a factor with >2 levels\n)\n\n# Check covariate balance\nbal.tab(w.out)\n#> Balance summary across all treatment pairs\n#>         Type Max.Diff.Adj\n#> cov1 Contin.       0.0309\n#> cov2 Contin.       0.0341\n#> cov3  Binary       0.0109\n#> \n#> Effective sample sizes\n#>                 A     B      C control\n#> Unadjusted 112.   99.   107.     82.  \n#> Adjusted   109.59 92.71 100.11   80.73\n\nbal.tab(w.out, which.treat = c(\"A\", \"B\", \"C\"))\n#> Balance by treatment pair\n#> \n#>  - - - A (0) vs. B (1) - - - \n#> Balance Measures\n#>         Type Diff.Adj\n#> cov1 Contin.  -0.0211\n#> cov2 Contin.  -0.0018\n#> cov3  Binary   0.0013\n#> \n#> Effective sample sizes\n#>                 A     B\n#> Unadjusted 112.   99.  \n#> Adjusted   109.59 92.71\n#> \n#>  - - - A (0) vs. C (1) - - - \n#> Balance Measures\n#>         Type Diff.Adj\n#> cov1 Contin.  -0.0034\n#> cov2 Contin.   0.0322\n#> cov3  Binary   0.0109\n#> \n#> Effective sample sizes\n#>                 A      C\n#> Unadjusted 112.   107.  \n#> Adjusted   109.59 100.11\n#> \n#>  - - - B (0) vs. C (1) - - - \n#> Balance Measures\n#>         Type Diff.Adj\n#> cov1 Contin.   0.0176\n#> cov2 Contin.   0.0341\n#> cov3  Binary   0.0097\n#> \n#> Effective sample sizes\n#>                B      C\n#> Unadjusted 99.   107.  \n#> Adjusted   92.71 100.11\n#>  - - - - - - - - - - - - - - - -\n\n\n# Estimate treatment effects with robust SEs\nlibrary(jtools)\nmodel <- glm(outcome ~ relevel(treat, ref = \"control\"),\n             data = df,\n             weights = w.out$weights)\nsumm(model, robust = \"HC1\")"},{"path":"sec-matching-methods.html","id":"summary-matching-vs.-weighting-in-multi-treatment-settings","chapter":"35 Matching Methods","heading":"35.9.11.4 Summary: Matching vs. Weighting in Multi-Treatment Settings","text":"general, weighting scalable coherent estimating causal effects multi-treatment contexts, matching can useful interpretability transparency individual pairings important.","code":""},{"path":"sec-matching-methods.html","id":"matching-for-multi-level-treatments","chapter":"35 Matching Methods","heading":"35.9.12 Matching for Multi-Level Treatments","text":"treatments just categorical, ordinal (multi-level), “low”, “medium”, “high” dosage levels. differ multinomial treatments levels natural order. Incorporating structure can improve estimation efficiency interpretability.","code":""},{"path":"sec-matching-methods.html","id":"propensity-score-estimation","chapter":"35 Matching Methods","heading":"35.9.12.1 Propensity Score Estimation","text":"context, researchers often use ordinal logistic regression estimate probability treatment level:\\[\n\\text{logit}\\left(\\mathbb{P}(T \\leq t \\mid X)\\right) = \\alpha_t - X^\\top \\beta\n\\]model imposes proportional odds assumption, log-odds linear covariates share coefficients across cutoffs.","code":""},{"path":"sec-matching-methods.html","id":"matching-strategies","chapter":"35 Matching Methods","heading":"35.9.12.2 Matching Strategies","text":"Yang et al. (2016) provide framework matching ordinal treatments, arguing matching generalized propensity score derived ordinal model helps preserve order improves balance. Key considerations include:Matching estimated ordinal probabilities cumulative logitsUsing Mahalanobis distance latent score spaceImplementing stratification subclassification based predicted scoresA custom package, shuyang1987/multilevelMatching, provides tools kind analysis. package includes:Functions estimate ordinal GPSMatching subclassification routinesDiagnostics evaluate covariate balance","code":""},{"path":"sec-matching-methods.html","id":"matching-for-repeated-treatments-time-varying-treatments","chapter":"35 Matching Methods","heading":"35.9.13 Matching for Repeated Treatments (Time-Varying Treatments)","text":"longitudinal studies, treatments often administered multiple time points. Examples include:patient receiving drug multiple doses weeksA user receiving marketing emails several daysThis setting raises unique challenges, :Time-varying confounding: Covariates affected prior treatment may influence future treatment outcomesCumulative dose effects: Treatment assignment longer one-time event","code":""},{"path":"sec-matching-methods.html","id":"marginal-structural-models","chapter":"35 Matching Methods","heading":"35.9.13.1 Marginal Structural Models","text":"popular framework analyzing repeated treatments Marginal Structural Model (MSMs), estimates causal effects weighting observation using Inverse Probability Treatment Weights (IPTW).Let \\(A_t\\) treatment time \\(t\\), \\(X_t\\) time-varying covariates. IPTW trajectory :\\[\nw_i = \\prod_{t=1}^{T} \\frac{1}{\\mathbb{P}(A_{} \\mid \\bar{}_{,t-1}, \\bar{X}_{,t})}\n\\]Weights estimated using logistic regression models time point. outcome model regresses \\(Y\\) \\(\\bar{}_T\\) using weights.twang package provides tools :Estimating time-varying propensity scoresComputing IPTWsFitting marginal structural modelsChecking covariate balance timePractical NotesStabilized weights help reduce varianceTrimming truncating extreme weights often necessary maintain robust inferenceDynamic treatment regimes may require generalizations structural nested mean models.","code":""},{"path":"sec-matching-methods.html","id":"sec-selection-on-unobservables","chapter":"35 Matching Methods","heading":"35.10 Selection on Unobservables","text":"randomized experiments help eliminate bias unobserved factors, observational data often leave us vulnerable selection unobservables—scenario omitted variables jointly affect treatment assignment outcomes. Unlike selection observables, unobserved confounders directly controlled regression matching techniques.One popular strategy mitigate bias observable variables matching methods. methods widely used estimate causal effects assumption selection observables offer two primary advantages:reduce reliance functional form assumptions (unlike parametric regression).rely assumption covariates influencing treatment assignment observed.However, key assumption often unrealistic. relevant covariates unmeasured, bias unobservables remains threat. address concern, researchers turn Rosenbaum Bounds, sensitivity analysis framework quantifies strong hidden bias must explain away observed treatment effect.Several econometric methods developed test robustness causal estimates hidden bias. key strategies researchers commonly employ:Rosenbaum Bounds\napproach assesses sensitivity treatment effects potential hidden bias bounding treatment effect varying assumptions strength unobserved confounding. particularly useful treatment assignment believed conditionally ignorable, assumption may imperfect.Rosenbaum Bounds\napproach assesses sensitivity treatment effects potential hidden bias bounding treatment effect varying assumptions strength unobserved confounding. particularly useful treatment assignment believed conditionally ignorable, assumption may imperfect.Endogenous Sample Selection (Heckman-style correction)\nsample selection non-random correlated unobserved variables influencing outcome, Heckman selection model provides correction. introduces inverse Mills ratio \\(\\lambda\\) control function. statistically significant \\(\\lambda\\) indicates unobserved factors influencing selection also affect outcome, pointing endogenous selection.\nInterpretation tip: significant \\(\\lambda\\) suggests selection bias due unobservables, Heckman model adjusts outcome equation accordingly.\nmethod frequently applied labor economics marketing analytics (e.g., correcting self-selection surveys consumer choice modeling).\nEndogenous Sample Selection (Heckman-style correction)\nsample selection non-random correlated unobserved variables influencing outcome, Heckman selection model provides correction. introduces inverse Mills ratio \\(\\lambda\\) control function. statistically significant \\(\\lambda\\) indicates unobserved factors influencing selection also affect outcome, pointing endogenous selection.Interpretation tip: significant \\(\\lambda\\) suggests selection bias due unobservables, Heckman model adjusts outcome equation accordingly.method frequently applied labor economics marketing analytics (e.g., correcting self-selection surveys consumer choice modeling).Relative Correlation Restrictions\nmethod imposes assumptions correlation observed unobserved variables. instance, assuming selection unobservables stronger selection observables (Altonji, Elder, Taber (2005)), one can identify bounds causal effects. approaches often rely auxiliary assumptions symmetry restrictions enable partial identification.Relative Correlation Restrictions\nmethod imposes assumptions correlation observed unobserved variables. instance, assuming selection unobservables stronger selection observables (Altonji, Elder, Taber (2005)), one can identify bounds causal effects. approaches often rely auxiliary assumptions symmetry restrictions enable partial identification.Coefficient-Stability Bounds\ntechnique evaluates treatment effect estimates change inclusion additional control variables. coefficient treatment variable remains stable covariates added, suggests robustness omitted variable bias.\nidea underlies methods like Oster’s \\(\\delta\\) method (Oster 2019), quantifies much selection unobservables required nullify observed effect, assuming proportional selection observables unobservables.\nCoefficient-Stability Bounds\ntechnique evaluates treatment effect estimates change inclusion additional control variables. coefficient treatment variable remains stable covariates added, suggests robustness omitted variable bias.idea underlies methods like Oster’s \\(\\delta\\) method (Oster 2019), quantifies much selection unobservables required nullify observed effect, assuming proportional selection observables unobservables.applied business research—customer retention, credit scoring, pricing analytics—selection unobservables often unavoidable. methods eliminate problem provide frameworks quantify interpret risk hidden bias. choice among tools depends specific context, data availability, plausibility underlying assumptions.","code":""},{"path":"sec-matching-methods.html","id":"sec-rosenbaum-bounds","chapter":"35 Matching Methods","heading":"35.10.1 Rosenbaum Bounds","text":"Rosenbaum Bounds assess robustness treatment effect estimates hidden bias introducing sensitivity parameter \\(\\Gamma\\) (gamma), captures potential effect unobserved variable treatment assignment.\\(\\Gamma = 1\\): Indicates perfect randomization—units matched pair equally likely receive treatment.\\(\\Gamma = 2\\): Suggests one unit matched pair twice likely receive treatment due unmeasured confounder.Since \\(\\Gamma\\) unknown, sensitivity analysis proceeds varying \\(\\Gamma\\) examining statistical significance magnitude treatment effect respond. allows us evaluate strong unobserved confounder need invalidate findings.unobserved variable must affect selection treatment outcomes, Rosenbaum Bounds often described worst-case analysis (DiPrete Gangl 2004).Imagine match two individuals based age education. One receives treatment, . ’s unmeasured trait (say, motivation) makes one twice likely receive treatment, equivalent \\(\\Gamma = 2\\).ask: level \\(\\Gamma\\) result lose significance? level high (e.g., \\(\\Gamma = 3\\) ), can confident findings.layman’s terms, consider two matched individuals differ unobserved trait (e.g., motivation). one twice likely receive treatment unobservable, corresponds \\(\\Gamma = 2\\). ask: treatment effect remain significant level hidden bias?estimated effect becomes insignificant high values \\(\\Gamma\\) (e.g., \\(\\Gamma > 2\\)), effect considered robust. Conversely, statistical significance vanishes \\(\\Gamma \\approx 1.1\\), effect highly sensitive hidden bias.","code":""},{"path":"sec-matching-methods.html","id":"technical-summary","chapter":"35 Matching Methods","heading":"35.10.1.1 Technical Summary","text":"Rosenbaum Bounds assess sensitivity without requiring knowledge direction unobserved bias.apply matched data, typically using Wilcoxon signed-rank tests non-parametric statistics.presence random treatment assignment, non-parametric tests valid directly.observational data, valid assumption selection observables. assumption questionable, Rosenbaum Bounds offer way test believability (Rosenbaum 2002).typical Rosenbaum Bounds analysis presents:\\(p\\)-value statistical significance increasing levels \\(\\Gamma\\).Hodges-Lehmann point estimate (robust median-based treatment effect estimator).critical \\(\\Gamma\\) value treatment effect becomes insignificant.analyses provide exact bounds treatment effect estimates. , approaches Relative Correlation Restrictions required.","code":""},{"path":"sec-matching-methods.html","id":"technical-framework","chapter":"35 Matching Methods","heading":"35.10.1.2 Technical Framework","text":"Let \\(\\pi_i\\) denote probability unit \\(\\) receives treatment. odds treatment :\\[\n\\text{Odds}_i = \\frac{\\pi_i}{1 - \\pi_i}\n\\]Ideally, matching, ’s hidden bias, \\(\\pi_i = \\pi_j\\) unit \\(\\) \\(j\\).Rosenbaum bounds constrain odds ratio two matched units \\(\\) \\(j\\):\\[\n\\frac{1}{\\Gamma} \\le \\frac{\\text{Odds}_i}{\\text{Odds}_j} \\le \\Gamma\n\\]relationship links hidden bias directly selection probabilities.Suppose treatment assignment follows:\\[\n\\log\\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\kappa x_i + \\gamma u_i\n\\]\\(u_i\\) unobserved covariate, \\(x_i\\) observed covariate. greater \\(\\gamma\\), stronger influence unobservable.","code":""},{"path":"sec-matching-methods.html","id":"sensitivity-analysis-procedure","chapter":"35 Matching Methods","heading":"35.10.1.3 Sensitivity Analysis Procedure","text":"Choose plausible range \\(\\Gamma\\) (typically 1 2.5).Assess p-value magnitude treatment effect (Hodges Jr Lehmann 2011) (details, see Hollander, Wolfe, Chicken (2013)) changes varying \\(\\Gamma\\) values.Employ specific randomization tests based type outcome establish bounds inferences.\nReport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017).\nReport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017).treatment clustered (e.g., region school), standard Rosenbaum bounds must adjusted. See (B. B. Hansen, Rosenbaum, Small 2014) methods extend sensitivity analysis clustered settings—analogous using clustered standard errors regression.","code":""},{"path":"sec-matching-methods.html","id":"empirical-marketing-examples","chapter":"35 Matching Methods","heading":"35.10.1.4 Empirical Marketing Examples","text":"table shows \\(\\Gamma\\) thresholds needed nullify treatment effects real-world marketing studies:Packagesrbounds (L. Keele 2010)rbounds (L. Keele 2010)sensitivitymv (Rosenbaum 2015)sensitivitymv (Rosenbaum 2015)Since typically assess estimate sensitivity unboservables matching, first matching.matching one control per treated unit:Alternative Sensitivity Analysis Toolssensitivitymw faster sensitivitymw. sensitivitymw can match matched sets can differing numbers controls (Rosenbaum 2015).","code":"\nlibrary(MatchIt)\nlibrary(Matching)\ndata(\"lalonde\", package = \"MatchIt\")\n\n# Matching\nmatched <- matchit(treat ~ age + educ, data = MatchIt::lalonde, method = \"nearest\")\nsummary(matched)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ, data = MatchIt::lalonde, \n#>     method = \"nearest\")\n#> \n#> Summary of Balance for All Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3080        0.2984          0.2734     0.4606    0.0881\n#> age            25.8162       28.0303         -0.3094     0.4400    0.0813\n#> educ           10.3459       10.2354          0.0550     0.4959    0.0347\n#>          eCDF Max\n#> distance   0.1663\n#> age        0.1577\n#> educ       0.1114\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3080        0.3077          0.0094     0.9963    0.0033\n#> age            25.8162       25.8649         -0.0068     1.0300    0.0050\n#> educ           10.3459       10.2865          0.0296     0.5886    0.0253\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0432          0.0146\n#> age        0.0162          0.0597\n#> educ       0.1189          0.8146\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           429     185\n#> Matched       185     185\n#> Unmatched     244       0\n#> Discarded       0       0\nmatched_data <- match.data(matched)\n\n# Split into groups\ntreatment_group <- subset(matched_data, treat == 1)\ncontrol_group   <- subset(matched_data, treat == 0)\n\n# Load rbounds package\nlibrary(rbounds)\n\n# P-value sensitivity\npsens_res <- psens(treatment_group$re78,\n                   control_group$re78,\n                   Gamma = 2,\n                   GammaInc = 0.1)\npsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n#>  \n#> Unconfounded estimate ....  0.9651 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0      0.9651      0.9651\n#>    1.1      0.8969      0.9910\n#>    1.2      0.7778      0.9980\n#>    1.3      0.6206      0.9996\n#>    1.4      0.4536      0.9999\n#>    1.5      0.3047      1.0000\n#>    1.6      0.1893      1.0000\n#>    1.7      0.1097      1.0000\n#>    1.8      0.0597      1.0000\n#>    1.9      0.0308      1.0000\n#>    2.0      0.0151      1.0000\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \n\n# Hodges-Lehmann estimate sensitivity\nhlsens_res <- hlsens(treatment_group$re78,\n                     control_group$re78,\n                     Gamma = 2,\n                     GammaInc = 0.1)\nhlsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n#>  \n#> Unconfounded estimate ....  1376.248 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0     1376.20      1376.2\n#>    1.1      852.45      1682.0\n#>    1.2      456.55      2023.1\n#>    1.3      137.25      2405.9\n#>    1.4     -133.45      2765.2\n#>    1.5     -461.65      3063.6\n#>    1.6     -751.85      3328.5\n#>    1.7     -946.45      3591.6\n#>    1.8    -1185.50      3864.8\n#>    1.9    -1404.70      4133.1\n#>    2.0    -1624.60      4358.4\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \nlibrary(Matching)\nlibrary(MatchIt)\n\nn_ratio <- 2\nmatched <-\n    matchit(treat ~ age + educ,\n            data = MatchIt::lalonde,\n            method = \"nearest\",\n            ratio = n_ratio)\nmatched_data <- match.data(matched)\n\nmcontrol_res <- rbounds::mcontrol(\n    y          = matched_data$re78,\n    grp.id     = matched_data$subclass,\n    treat.id   = matched_data$treat,\n    group.size = n_ratio + 1,\n    Gamma      = 2.5,\n    GammaInc   = 0.1\n)\nmcontrol_res\n#> \n#>  Rosenbaum Sensitivity Test for Wilcoxon Strat. Rank P-Value \n#>  \n#> Unconfounded estimate ....  0.9977 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0      0.9977      0.9977\n#>    1.1      0.9892      0.9996\n#>    1.2      0.9651      0.9999\n#>    1.3      0.9144      1.0000\n#>    1.4      0.8301      1.0000\n#>    1.5      0.7149      1.0000\n#>    1.6      0.5809      1.0000\n#>    1.7      0.4445      1.0000\n#>    1.8      0.3209      1.0000\n#>    1.9      0.2195      1.0000\n#>    2.0      0.1428      1.0000\n#>    2.1      0.0888      1.0000\n#>    2.2      0.0530      1.0000\n#>    2.3      0.0305      1.0000\n#>    2.4      0.0170      1.0000\n#>    2.5      0.0092      1.0000\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \nlibrary(sensitivitymv)\ndata(lead150)\nsenmv(lead150, gamma = 2, trim = 2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524\n\nlibrary(sensitivitymw)\nsenmw(lead150, gamma = 2, trim = 2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524"},{"path":"sec-matching-methods.html","id":"sec-relative-correlation-restrictions","chapter":"35 Matching Methods","heading":"35.10.2 Relative Correlation Restrictions","text":"many observational studies, researchers concerned impact unobserved variables included regression model. Even including comprehensive set controls, always possibility omitted variables still bias estimated treatment effect. Relative Correlation Restrictions (RCR) offer formal way assess potential magnitude bias comparing observed selection.Let’s begin linear outcome model:\\[\nY_i = X_i \\beta + C_i \\gamma + \\epsilon_i\n\\]:\\(Y_i\\) outcome variable (e.g., revenue, engagement, churn),\\(X_i\\) treatment variable interest (e.g., paywall suspension, ad exposure),\\(C_i\\) set observed control covariates,\\(\\epsilon_i\\) error term, may include unobserved factors.standard OLS regression, assume:\\[\n\\text{Cov}(X_i, \\epsilon_i) = 0\n\\], treatment \\(X_i\\) uncorrelated omitted variables contained \\(\\epsilon_i\\). assumption crucial unbiased estimation \\(\\beta\\), treatment effect.However, observational settings, assumption may hold. may unobserved confounders influence \\(X_i\\) \\(Y_i\\), biasing OLS estimate \\(\\beta\\). Relative Correlation Restrictions framework provides way bound treatment effect assumptions relative strength selection unobservables compared selection observables.method originally proposed Altonji, Elder, Taber (2005) later extended Krauth (2016). core idea assume proportional relationship correlation treatment error term correlation observed controls:\\[\n\\text{Corr}(X_i, \\epsilon_i) = \\lambda \\cdot \\text{Corr}(X_i, C_i \\gamma)\n\\], \\(\\lambda\\) represents relative strength selection unobservables compared observables.\\(\\lambda = 0\\), return standard OLS assumption: unobserved selection.\\(\\lambda = 1\\), assume selection unobservables strong selection observables.\\(\\lambda > 1\\), unobserved selection assumed stronger observable selection.practice, evaluate treatment effects range plausible values \\(\\lambda\\).approach allows us bound treatment effect based assumptions degree omitted variable bias.choice \\(\\lambda\\) crucial inherently subjective, guidelines empirical precedents help:Small values \\(\\lambda\\) (e.g., 0–1) represent modest levels bias suggest selection unobservables dominant.Large values (e.g., \\(\\lambda > 2\\) higher) imply omitted variables must far predictive treatment observed controls—strong often implausible assumption well-specified models.makes RCR particularly useful stress testing causal estimates: “bad unobserved selection overturn result?”method applied various marketing digital strategy settings assess robustness estimated effects. notable examples:high \\(\\lambda\\) values imply unobserved selection need 3 7 times stronger observable selection eliminate estimated treatment effect. practical terms, offers strong evidence effects robust omitted variable bias.can estimate Relative Correlation Restrictions using rcrbounds package R, developed Krauth (2016). package estimates bounds treatment effect across range \\(\\lambda\\) values provides inference whether effect remain statistically significant.plot shows estimated treatment effect varies allow stronger selection unobservables (.e., increasing \\(\\lambda\\)). effect remains consistently different zero even high \\(\\lambda\\), provides graphical evidence robustness.","code":"\n# Install if necessary:\n# remotes::install_github(\"bvkrauth/rcr/r/rcrbounds\")\nlibrary(rcrbounds)\n\n# Example using ChickWeight dataset\ndata(\"ChickWeight\")\n\n# Estimate treatment effect of Time on weight, controlling for Diet\nrcr_res <- rcrbounds::rcr(\n    formula = weight ~ Time | Diet, \n    data = ChickWeight, \n    # rc_range = c(0, 10) # Test lambda from 0 to 10\n    rc_range = c(0, 1)\n)\n\n# Print summary\nsummary(rcr_res)\n#> \n#> Call:\n#> rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, \n#>     rc_range = c(0, 1))\n#> \n#> Coefficients:\n#>            Estimate  Std. Error    t value      Pr(>|t|)\n#> rcInf     34.676505  50.1295005  0.6917385  4.891016e-01\n#> effectInf 71.989336 112.5711682  0.6395007  5.224973e-01\n#> rc0       34.741955  58.7169195  0.5916856  5.540611e-01\n#> effectL    8.624677   0.3337819 25.8392614 3.212707e-147\n#> effectH    8.750492   0.2607671 33.5567355 7.180405e-247\n#> ---\n#> conservative confidence interval:\n#>          2.5  %  97.5  %\n#> effect 7.970477 9.261586\n\n# Test whether the treatment effect is significantly different from 0\nrcrbounds::effect_test(rcr_res, h0 = 0)\n#> [1] 5.960464e-08\n\n# Plot the bounds of the treatment effect\nplot(rcr_res)"},{"path":"sec-matching-methods.html","id":"sec-coefficient-stability-bounds","chapter":"35 Matching Methods","heading":"35.10.3 Coefficient-Stability Bounds","text":"Robust regression analysis requires methods account potential omitted variable bias. contribution literature coefficient-stability framework developed Oster (2019), builds proportional selection model Altonji, Elder, Taber (2005). Oster (2019) derives conditions can bound bias due unobservables using movements coefficient interest changes model fit, measured \\(R^2\\).recently, Masten Poirier (2022) revealed Oster’s commonly used “delta” metric insufficient guarantee robustness sign treatment effect. propose distinguishing two types breakdown points: one effect becomes zero (explain-away), another changes sign.Suppose data‑generating process outcome \\(Y\\) \\[\nY\n=\\beta X+\\underbrace{W_1^{\\!\\top}\\gamma_1}_{\\text{observed}}\n+\\underbrace{W_2^{\\!\\top}\\gamma_2}_{\\text{unobserved}}\n+\\varepsilon ,\n\\quad\n\\operatorname{E}[\\varepsilon|X,W_1,W_2]=0,\n\\tag{1}\n\\]econometrician can regress \\(Y\\) \\(X\\) observed covariates \\(W_1\\). omitted‑variable‑bias (OVB) formula,\\[\n\\widehat\\beta_{\\text{med}}-\\beta\n= \\frac{\\operatorname{Cov}(X,W_2^{\\!\\top}\\gamma_2)}{\\operatorname{Var}(X)} .\n\\tag{2}\n\\]\\(W_2\\) hidden, nothing inside sample alone fixes (2). literature therefore introduces sensitivity parameter gauges strongly \\(X\\) correlated missing variables.Altonji, Elder, Taber (2005) postulated selection unobservables proportional selection observables. Oster (2019) formalized idea:\\[\n\\frac{\\operatorname{Cov}(X,W_2^{\\!\\top}\\gamma_2)}\n     {\\operatorname{Var}(W_2^{\\!\\top}\\gamma_2)}\n=\\delta\\;\n\\frac{\\operatorname{Cov}(X,W_1^{\\!\\top}\\gamma_1)}\n     {\\operatorname{Var}(W_1^{\\!\\top}\\gamma_1)} .\n\\tag{3}\n\\]\\(\\delta\\) selection ratio.\\(\\delta = 1\\) says “unobservables correlated \\(X\\) observables,” yielding canonical “Altonji‑Elder‑Taber benchmark.”Oster (2019) also assumes coefficient alignment (\\(\\gamma_1=C\\pi_1\\)) can link bias movement \\(\\widehat\\beta\\) \\(R^2\\) controls added. (1)–(3) long‑run coefficient (\\(W_2\\) observed) satisfies\\[\n\\boxed{\\;\n\\beta_{\\text{long}}\n=\\beta_{\\text{med}}\n+\\bigl(\\beta_{\\text{med}}-\\beta_{\\text{short}}\\bigr)\n     \\frac{R^2_{\\text{long}}-R^2_{\\text{med}}}\n          {R^2_{\\text{med}}-R^2_{\\text{short}}}\n\\;}\n\\tag{4}\n\\] “short” means controls, “med” means \\(W_1\\), “long” unfeasible full model. Equation (4) coefficient‑stability adjustment. two sensitivity objects areBias‑adjusted coefficient\n\\[\n\\beta^*(\\delta,R^2_{\\max})\\equiv \\beta_{\\text{long}}\n\\quad\\text{substituting }R^2_{\\text{long}}=R^2_{\\max},\\;\n\\delta\\text{ (4);} \\tag{5}\n\\]Bias‑adjusted coefficient\\[\n\\beta^*(\\delta,R^2_{\\max})\\equiv \\beta_{\\text{long}}\n\\quad\\text{substituting }R^2_{\\text{long}}=R^2_{\\max},\\;\n\\delta\\text{ (4);} \\tag{5}\n\\]Breakdown point\n\\[\n\\delta^{\\text{EA}}\n\\;=\\;\n\\inf\\bigl\\{\\delta:\\beta^*(\\delta,R^2_{\\max})=0\\bigr\\}.\n\\tag{6}\n\\]Breakdown point\\[\n\\delta^{\\text{EA}}\n\\;=\\;\n\\inf\\bigl\\{\\delta:\\beta^*(\\delta,R^2_{\\max})=0\\bigr\\}.\n\\tag{6}\n\\]\\(\\delta^{\\text{EA}}>1\\), unobservables must predictive \\(X\\) observables wipe estimate.Masten Poirier (2022) prove stronger result: Assumptions (1)–(3) sign‑change breakdown\\[\n\\delta^{\\text{SC}}\n=\\inf\\bigl\\{\\delta:\\operatorname{sign}\\beta^*(\\delta,R^2_{\\max})\n                \\neq \\operatorname{sign}\\beta_{\\text{med}}\\bigr\\}\n\\tag{7}\n\\]always ≤ 1 \\(R^2_{\\max}>R^2_{\\text{med}}\\) \\(\\beta_{\\text{short}}\\neq\\beta_{\\text{med}}\\). Hence traditional “\\(\\delta = 1\\) rule” can never guarantee direction effect robust; merely speaks magnitude. Putting \\(\\delta^{EA}\\) \\(\\delta^{SC}\\) table therefore critical.Oster (2019) recommends \\(R^2_{\\max}=1.3\\,R^2_{\\text{med}}\\) based external evidence randomized experiments, researchers vary choice—especially \\(R^2_{\\text{med}}\\) modest. Cinelli Hazlett (2020) re‑express problem partial‑\\(R^2\\) space propose robustness value, implemented sensemakr package. G. W. Imbens (2003) supply complementary contour‑plot tools.Explain‑away curve: plots \\(\\delta^*\\) range \\(R^2_{\\max}\\).Bootstrap histogram: sampling distribution \\(\\delta^*\\), 90/95/99 % bands.\\(\\beta^*\\) vs. \\(R^2_{\\max}\\): bias‑adjusted coefficient moves tighten loosen assumed upper bound \\(R^2\\).","code":"\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(robomit)\n\n\ndata(\"lalonde\", package = \"MatchIt\")\nlalonde <- lalonde %>% mutate(log_re78 = log(re78 + 1))\n\n# create race dummies if needed\nif (!(\"black\" %in% names(lalonde))) {\n  lalonde <- lalonde %>% mutate(\n    black  = as.integer(race == \"black\"),\n    hispan = as.integer(race == \"hispanic\")\n  )\n}\n\n# nodegree naming patch\nif (!(\"nodegr\" %in% names(lalonde))) {\n  if (\"nodegree\" %in% names(lalonde)) {\n    lalonde <- lalonde %>% rename(nodegr = nodegree)\n  }\n}\n\n# assure 0/1 indicators\nlalonde <-\n    lalonde %>% mutate(across(c(treat, black, hispan, married, nodegr),\n                              as.numeric))\n\n## analysis sample & R^2\nvars <- c(\n    \"log_re78\",\n    \"treat\",\n    \"age\",\n    \"educ\",\n    \"black\",\n    \"hispan\",\n    \"married\",\n    \"nodegr\",\n    \"re74\",\n    \"re75\"\n)\nmodel_df <- lalonde %>% dplyr::select(all_of(vars)) %>% na.omit()\n\nR2_med <- summary(\n    lm(\n        log_re78 ~ treat + age + educ + black + hispan +\n            married + nodegr + re74 + re75,\n        data = model_df\n    )\n)$r.squared\nR2_max <- 1.3 * R2_med # Oster default upper bound\n\n## β* (δ = 1) & δ* (β = 0) \nbeta_tbl  <- o_beta(\n    y = \"log_re78\",\n    x = \"treat\",\n    con = \"age + educ + black + hispan + married + nodegr + re74 + re75\",\n    delta = 1,\n    R2max = R2_max,\n    type = \"lm\",\n    data = model_df\n)\n\ndelta_tbl <- o_delta(\n    y = \"log_re78\",\n    x = \"treat\",\n    con = \"age + educ + black + hispan + married + nodegr + re74 + re75\",\n    beta = 0,\n    R2max = R2_max,\n    type = \"lm\",\n    data = model_df\n)\n\nbeta_val  <- beta_tbl  %>% filter(Name == \"beta*\")  %>% pull(Value)\ndelta_val <- delta_tbl %>% filter(Name == \"delta*\") %>% pull(Value)\n\nlist(\n  bias_adjusted_beta = beta_val,\n  explain_away_delta = delta_val\n)\n#> $bias_adjusted_beta\n#> [1] 1.378256\n#> \n#> $explain_away_delta\n#> [1] -1.845051\n# 1.  δ* as Rmax varies (explain‑away curve)\no_delta_rsq_viz(\n  y   = \"log_re78\", x = \"treat\",\n  con = \"age + educ + black + hispan + married + nodegr + re74 + re75\",\n  beta = 0,                     # explain‑away target\n  type = \"lm\",\n  data = model_df\n)\n\n# 2.  bootstrap sampling distribution of δ*\no_delta_boot_viz(\n  y   = \"log_re78\", x = \"treat\",\n  con = \"age + educ + black + hispan + married + nodegr + re74 + re75\",\n  beta   = 0,\n  R2max  = R2_max,\n  sim    = 100,                 # number of bootstrap draws\n  obs    = nrow(model_df),      # draw full‑sample size each time\n  rep    = TRUE,                # with replacement\n  CI     = c(90, 95, 99),       # show three confidence bands\n  type   = \"lm\",\n  norm   = TRUE,                # overlay normal curve\n  bin    = 120,                 # histogram bins\n  data   = model_df\n)\n\n# 3.  bias‑adjusted β* over a grid of Rmax value\no_beta_rsq_viz(\n  y   = \"log_re78\", x = \"treat\",\n  con = \"age + educ + black + hispan + married + nodegr + re74 + re75\",\n  delta = 1,                    # proportional selection benchmark\n  type  = \"lm\",\n  data  = model_df\n)"},{"path":"sec-endogeneity.html","id":"sec-endogeneity","chapter":"36 Endogeneity","heading":"36 Endogeneity","text":"applied research, ’s often tempting treat regression coefficients represent causal relationships. positive coefficient advertising spend, example, might interpreted evidence increasing ad budgets increase sales. interpretations rely critical assumption: independent variables include model exogenous.chapter explores central threat assumption: endogeneity.Endogeneity refers situation explanatory variable correlated error term regression model. happens, coefficient estimates biased inconsistent, causal claims invalid.understand endogeneity comes , let’s begin familiar linear regression model:\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]:\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector observed outcomes,\\(\\mathbf{X}\\) \\(n \\times k\\) matrix explanatory variables (including column ones intercept, present),\\(\\beta\\) \\(k \\times 1\\) vector unknown parameters,\\(\\epsilon\\) \\(n \\times 1\\) vector unobserved error terms.Ordinary Least Squares estimator :\\[\n\\begin{aligned}\n\\hat{\\beta}_{OLS} &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{Y}) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'(\\mathbf{X\\beta + \\epsilon})) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{X})\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\epsilon) \\\\\n&= \\beta + (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\epsilon)\n\\end{aligned}\n\\]derivation makes clear: OLS unbiased second term vanishes expectation. :\\[\nE[(\\mathbf{X}'\\mathbf{\\epsilon})] = 0 \\quad \\text{equivalently,} \\quad Cov(\\mathbf{X}, \\epsilon) = 0\n\\]produce valid estimates, OLS requires two conditions:Zero Conditional Mean:\\[\nE[\\epsilon \\mid \\mathbf{X}] = 0\n\\] implies condition regressors, systematic error left.Zero Conditional Mean:\\[\nE[\\epsilon \\mid \\mathbf{X}] = 0\n\\] implies condition regressors, systematic error left.Correlation Regressors Errors:\\[\nCov(\\mathbf{X}, \\epsilon) = 0\n\\] stronger requirement. fails, endogeneity problem.Correlation Regressors Errors:\\[\nCov(\\mathbf{X}, \\epsilon) = 0\n\\] stronger requirement. fails, endogeneity problem.first condition often satisfied including intercept accounting distributional properties errors. second condition—lack correlation \\(\\mathbf{X}\\) \\(\\epsilon\\)—much harder satisfy, especially observational data.Endogeneity violates one core assumptions regression, serious consequences:Coefficient bias: Estimates systematically differ true parameter values.Inconsistency: bias vanish sample size increases.Incorrect inference: Hypothesis tests confidence intervals become unreliable.Misleading decisions: business policy settings, can lead costly errors.several common sources endogeneity (Hill et al. 2021). However, problems fall two broad categories:Endogenous TreatmentOmitted Variable Bias (OVB)occurs relevant variable :Left model (.e., omitted),Correlated explanatory variable(s) outcome.OVB problem?omitted variable correlated included regressor.also affects dependent variable.either condition fails, ’s bias.Example (Economics): want estimate effect school earnings, typical unobservables (e.g., motivation, ability/talent, self-selection) pose threat identification strategy.Example (Marketing): Suppose regress sales advertising spend, omit product quality. higher-quality products get advertising also generate sales, ad spend coefficient picks effect quality—resulting upward bias.Example (Finance): Regressing firm performance executive compensation might omit executive ability. able executives command higher compensation deliver better results, OVB leads biased inferences.Simultaneity (Feedback Effects)Simultaneity arises dependent variable explanatory variable determined jointly, equilibrium.Example (Economics): Price quantity demanded determined together supply--demand models. regression quantity price without modeling supply yield biased estimate demand sensitivity.Reverse CausalityA special case simultaneity causation runs opposite model assumes.Example (Health Policy): naive model might regress health outcomes insurance coverage. ’s plausible people poor health likely purchase insurance, causing reverse causality.longer time intervals (e.g., yearly business data), reverse causality can look just like simultaneity terms effect regression estimates.Measurement ErrorsEven relevant variable included, imprecise measurement introduces bias.Classical Measurement Error (\\(X\\)):Leads attenuation bias—estimated coefficients biased toward zero.Occurs frequently survey data, behavioral measures, administrative records.Example (Digital Marketing): Click-rates exposure ads may tracked browser cookies device IDs, identifiers imperfect. resulting measurement error biases estimated effect advertising downward.Endogenous Sample SelectionSample selection becomes source endogeneity inclusion sample related outcome variable.Example (Labor Economics): Estimating effect education wages using employed individuals excludes currently working. employment correlated unobserved traits (e.g., motivation), wage equation biased.Summary Table: Types EndogeneityHealth \\(\\\\) InsuranceRevenue \\(\\\\) Ad SpendEndogeneity always fatal—can identify adjust , can still make credible inferences.Control VariablesIf suspect omitted variable data , can include control. called “selection observables” approach.However, strategy often insufficient :Many important factors unobserved (e.g., motivation, ability, expectations).Measured variables may contain measurement error, creating new biases.Toolbox EndogeneityTo address complex cases, including involving unobservables, introduce advanced methods (see Causal Inference Toolbox)","code":""},{"path":"sec-endogeneity.html","id":"sec-endogenous-treatment","chapter":"36 Endogeneity","heading":"36.1 Endogenous Treatment","text":"Endogenous treatment occurs variable interest (“treatment”) randomly assigned correlated unobserved determinants outcome. discussed earlier, can arise omitted variables, simultaneity, reverse causality. even true variable theoretically exogenous, measurement error can make endogenous practice.section focuses measurement errors, especially explanatory variables, introduce bias—typically attenuation bias—central concern applied research.","code":""},{"path":"sec-endogeneity.html","id":"sec-measurement-error","chapter":"36 Endogeneity","heading":"36.1.1 Measurement Errors","text":"Measurement error refers difference true value variable observed (measured) value.Sources measurement error:\nCoding errors: Manual software-induced data entry mistakes.\nReporting errors: Self-report bias, recall issues, strategic misreporting.\nCoding errors: Manual software-induced data entry mistakes.Reporting errors: Self-report bias, recall issues, strategic misreporting.Two Broad Types Measurement ErrorRandom (Stochastic) Error — Classical Measurement Error\nNoise unpredictable averages expectation.\nError uncorrelated true variable regression error.\nCommon survey data, tracking errors.\nNoise unpredictable averages expectation.Error uncorrelated true variable regression error.Common survey data, tracking errors.Systematic (Non-classical) Error — Non-Random Bias\nMeasurement error exhibits consistent patterns across observations.\nOften arises :\nInstrument error: e.g., faulty sensors, uncalibrated scales.\nMethod error: poor sampling, survey design flaws.\nHuman error: judgment errors, social desirability bias.\n\nMeasurement error exhibits consistent patterns across observations.Often arises :\nInstrument error: e.g., faulty sensors, uncalibrated scales.\nMethod error: poor sampling, survey design flaws.\nHuman error: judgment errors, social desirability bias.\nInstrument error: e.g., faulty sensors, uncalibrated scales.Method error: poor sampling, survey design flaws.Human error: judgment errors, social desirability bias.Key insight:Random error adds noise, pushing estimates toward zero.Systematic error introduces bias, pushing estimates either upward downward.","code":""},{"path":"sec-endogeneity.html","id":"sec-classical-measurement-error","chapter":"36 Endogeneity","heading":"36.1.1.1 Classical Measurement Error","text":"","code":""},{"path":"sec-endogeneity.html","id":"sec-right-hand-side-variable","chapter":"36 Endogeneity","heading":"36.1.1.1.1 Right-Hand Side Variable","text":"Let’s examine common analytically tractable case: classical measurement error explanatory variable.Suppose true model :\\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\]observe \\(X_i\\) directly. Instead, observe:\\[\n\\tilde{X}_i = X_i + e_i\n\\]\\(e_i\\) measurement error, assumed classical:\\(E[e_i] = 0\\)\\(Cov(X_i, e_i) = 0\\)\\(Cov(e_i, u_i) = 0\\)Now, substitute \\(\\tilde{X}_i\\) regression:\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 ( \\tilde{X}_i - e_i ) + u_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + (u_i - \\beta_1 e_i) \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + v_i\n\\end{aligned}\n\\]\\(v_i = u_i - \\beta_1 e_i\\) composite error term.Since \\(\\tilde{X}_i\\) contains \\(e_i\\), \\(v_i\\) contains \\(e_i\\), now :\\[\nCov(\\tilde{X}_i, v_i) \\neq 0\n\\]correlation violates exogeneity assumption introduces endogeneity.can derive asymptotic bias:\\[\n\\begin{aligned}\nE[\\tilde{X}_i v_i] &= E[(X_i + e_i)(u_i - \\beta_1 e_i)] \\\\\n&= -\\beta_1 Var(e_i) \\\\\n&\\neq 0\n\\end{aligned}\n\\]implies:\\(\\beta_1 > 0\\), \\(\\hat{\\beta}_1\\) biased downward.\\(\\beta_1 < 0\\), \\(\\hat{\\beta}_1\\) biased upward.called attenuation bias: estimated effect biased toward zero.variance error \\(Var(e_i)\\) increases \\(\\frac{Var(e_i)}{Var(\\tilde{X}_i)} \\1\\), bias becomes severe.Attenuation FactorThe OLS estimator based noisy regressor \\[\n\\hat{\\beta}_{OLS} = \\frac{ \\text{cov}(\\tilde{X}, Y)}{\\text{var}(\\tilde{X})} = \\frac{\\text{cov}(X + e, \\beta X + u)}{\\text{var}(X + e)}.\n\\]Using assumptions classical measurement error, follows :\\[\nplim\\ \\hat{\\beta}_{OLS} = \\beta \\cdot \\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_e^2} = \\beta \\cdot \\lambda,\n\\]:\\(\\sigma_X^2\\) variance true regressor \\(X\\),\\(\\sigma_e^2\\) variance measurement error \\(e\\), \\(\\lambda = \\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_e^2}\\) called reliability ratio, signal--total variance ratio, attenuation factor.Since \\(\\lambda \\(0, 1]\\), bias always attenuates estimate toward zero. degree attenuation bias :\\[\n\\hat{\\beta}_{OLS} - \\beta = - (1 - \\lambda)\\beta,\n\\]implies:\\(\\lambda = 1\\), \\(\\hat{\\beta}_{OLS} = \\beta\\) — bias (measurement error).\\(\\lambda < 1\\), \\(\\hat{\\beta}_{OLS} < \\beta\\) — attenuation toward zero.Important Notes Measurement ErrorData transformations can magnify measurement error.\nSuppose true model nonlinear:\n\\[\ny = \\beta x + \\gamma x^2 + \\epsilon,\n\\]\n\\(x\\) measured classical error. , attenuation factor \\(\\hat{\\gamma}\\) approximately square attenuation factor \\(\\hat{\\beta}\\):\n\\[\n\\lambda_{\\hat{\\gamma}} \\approx \\lambda_{\\hat{\\beta}}^2.\n\\]\nshows nonlinear transformations (e.g., squares, logs) can exacerbate measurement error problems.Data transformations can magnify measurement error.Suppose true model nonlinear:\\[\ny = \\beta x + \\gamma x^2 + \\epsilon,\n\\]\\(x\\) measured classical error. , attenuation factor \\(\\hat{\\gamma}\\) approximately square attenuation factor \\(\\hat{\\beta}\\):\\[\n\\lambda_{\\hat{\\gamma}} \\approx \\lambda_{\\hat{\\beta}}^2.\n\\]shows nonlinear transformations (e.g., squares, logs) can exacerbate measurement error problems.Including covariates can increase attenuation bias.\nAdding covariates correlated mismeasured variable can worsen bias coefficient interest, especially measurement error accounted covariates.Including covariates can increase attenuation bias.Adding covariates correlated mismeasured variable can worsen bias coefficient interest, especially measurement error accounted covariates.Remedies Measurement ErrorTo address attenuation bias caused classical measurement error, consider following strategies:Use validation data survey information estimate \\(\\sigma_X^2\\), \\(\\sigma_e^2\\), \\(\\lambda\\) apply correction methods (e.g., SIMEX, regression calibration).Instrumental Variables Approach\nUse instrument \\(Z\\) :\ncorrelated true variable \\(X\\),\nuncorrelated regression error \\(\\epsilon\\), \nuncorrelated measurement error \\(e\\).\ncorrelated true variable \\(X\\),uncorrelated regression error \\(\\epsilon\\), andIs uncorrelated measurement error \\(e\\).Abandon project\ngood instruments validation data exist, attenuation bias severe, may prudent reconsider analysis research question. (Said love academic humility.)","code":""},{"path":"sec-endogeneity.html","id":"sec-left-hand-side-variable","chapter":"36 Endogeneity","heading":"36.1.1.1.2 Left-Hand Side Variable","text":"Measurement error dependent variable (.e., response outcome) fundamentally different measurement error explanatory variables. consequences often less problematic consistent estimation regression coefficients (e.g., zero conditional mean assumption violated), necessarily statistical inference (e.g., higher standard errors) model fit.Suppose interested standard linear regression model:\\[\nY_i = \\beta X_i + u_i,\n\\]observe \\(Y_i\\) directly. Instead, observe:\\[\n\\tilde{Y}_i = Y_i + v_i,\n\\]:\\(v_i\\) measurement error dependent variable,\\(E[v_i] = 0\\) (mean-zero),\\(v_i\\) uncorrelated \\(X_i\\) \\(u_i\\),\\(v_i\\) homoskedastic independent across observations.extra careful !classical‐error assumptions:Mean zero: \\(\\mathbb{E}[v\\mid X]=0\\).Exogeneity: \\(v\\) uncorrelated regressor structural disturbance \\(\\epsilon\\) (.e., \\(\\operatorname{Cov}(X,v)=\\operatorname{Cov}(\\epsilon,v)=0\\)).Homoskedasticity / finite moments law‑‑large‑numbers apply.regression actually estimate :\\[\n\\tilde{Y}_i = \\beta X_i + u_i + v_i.\n\\]can define composite error term:\\[\n\\tilde{u}_i = u_i + v_i,\n\\]model becomes:\\[\n\\tilde{Y}_i = \\beta X_i + \\tilde{u}_i.\n\\]classical-error assumptions, extra noise simply enlarges composite error term \\(\\tilde{u}_i\\), leaving\\[\n\\hat\\beta^{\\text{OLS}}    =\\beta + ( X' X)^{-1} X'(u+v)    \\;\\xrightarrow{p} \\beta ,\n\\]estimator remains consistent variance rises.Key InsightsUnbiasedness Consistency \\(\\hat{\\beta}\\):\nlong \\(E[\\tilde{u}_i \\mid X_i] = 0\\), holds classical assumptions (.e., \\(E[u_i \\mid X_i] = 0\\) \\(E[v_i \\mid X_i] = 0\\)), OLS estimator \\(\\beta\\) remains unbiased consistent.\nmeasurement error left-hand side induce endogeneity. zero conditional mean assumption preserved.Unbiasedness Consistency \\(\\hat{\\beta}\\):long \\(E[\\tilde{u}_i \\mid X_i] = 0\\), holds classical assumptions (.e., \\(E[u_i \\mid X_i] = 0\\) \\(E[v_i \\mid X_i] = 0\\)), OLS estimator \\(\\beta\\) remains unbiased consistent.measurement error left-hand side induce endogeneity. zero conditional mean assumption preserved.Interpretation (Econometricians Don’t Panic):\nEconometricians causal researchers often focus consistent estimation causal effects strict exogeneity. Since \\(v_i\\) just adds noise outcome doesn’t systematically relate \\(X_i\\), slope estimate \\(\\hat{\\beta}\\) remains valid estimate causal effect \\(\\beta\\).Interpretation (Econometricians Don’t Panic):Econometricians causal researchers often focus consistent estimation causal effects strict exogeneity. Since \\(v_i\\) just adds noise outcome doesn’t systematically relate \\(X_i\\), slope estimate \\(\\hat{\\beta}\\) remains valid estimate causal effect \\(\\beta\\).Statistical Implications (Statisticians Might Worry):\nAlthough \\(\\hat{\\beta}\\) consistent, variance error term increases due added noise \\(v_i\\). Specifically:\n\\[\n\\text{Var}(\\tilde{u}_i) = \\text{Var}(u_i) + \\text{Var}(v_i) = \\sigma_u^2 + \\sigma_v^2.\n\\]\nleads :\nHigher residual variance \\(\\Rightarrow\\) lower \\(R^2\\)\nHigher standard errors coefficient estimates\nWider confidence intervals, reducing precision inference\nThus, even though point estimate valid, inference becomes weaker: hypothesis tests less powerful, conclusions less precise.Statistical Implications (Statisticians Might Worry):Although \\(\\hat{\\beta}\\) consistent, variance error term increases due added noise \\(v_i\\). Specifically:\\[\n\\text{Var}(\\tilde{u}_i) = \\text{Var}(u_i) + \\text{Var}(v_i) = \\sigma_u^2 + \\sigma_v^2.\n\\]leads :Higher residual variance \\(\\Rightarrow\\) lower \\(R^2\\)Higher standard errors coefficient estimatesWider confidence intervals, reducing precision inferenceThus, even though point estimate valid, inference becomes weaker: hypothesis tests less powerful, conclusions less precise.Practical IllustrationSuppose \\(X\\) marketing investment \\(Y\\) sales revenue.sales measured noise (e.g., misrecorded sales data, rounding, reporting delays), coefficient marketing still consistently estimated.However, uncertainty around estimate grows: wider confidence intervals might make harder detect statistically significant effects, especially small samples.Summary Table: Measurement Error Consequences","code":""},{"path":"sec-endogeneity.html","id":"sec-non-classical-measurement-error","chapter":"36 Endogeneity","heading":"36.1.1.2 Non-Classical Measurement Error","text":"classical measurement error model, assume measurement error \\(\\epsilon\\) independent true variable \\(X\\) regression disturbance \\(u\\). However, many realistic data scenarios, assumption hold. Non-classical measurement error refers cases :\\(\\epsilon\\) correlated \\(X\\),possibly even correlated \\(u\\).Violating classical assumptions introduces additional potentially complex biases OLS estimation.Recall classical measurement error model, observe:\\[\n\\tilde{X} = X + \\epsilon,\n\\]:\\(\\epsilon\\) independent \\(X\\) \\(u\\),\\(E[\\epsilon] = 0\\).true model :\\[\nY = \\beta X + u.\n\\], OLS based mismeasured regressor gives:\\[\n\\hat{\\beta}_{OLS} = \\frac{\\text{cov}(\\tilde{X}, Y)}{\\text{var}(\\tilde{X})} = \\frac{\\text{cov}(X + \\epsilon, \\beta X + u)}{\\text{var}(X + \\epsilon)}.\n\\]classical assumptions, simplifies :\\[\nplim\\ \\hat{\\beta}_{OLS} = \\beta \\cdot \\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_\\epsilon^2} = \\beta \\cdot \\lambda,\n\\]\\(\\lambda\\) reliability ratio, attenuates \\(\\hat{\\beta}\\) toward zero.Let us now relax independence assumption allow correlation \\(X\\) \\(\\epsilon\\). particular, suppose:\\(\\text{cov}(X, \\epsilon) = \\sigma_{X\\epsilon} \\ne 0\\).probability limit OLS estimator becomes:\\[\n\\begin{aligned}\nplim\\ \\hat{\\beta}\n&= \\frac{\\text{cov}(X + \\epsilon, \\beta X + u)}{\\text{var}(X + \\epsilon)} \\\\\n&= \\frac{\\beta (\\sigma_X^2 + \\sigma_{X\\epsilon})}{\\sigma_X^2 + \\sigma_\\epsilon^2 + 2 \\sigma_{X\\epsilon}}.\n\\end{aligned}\n\\]can rewrite :\\[\n\\begin{aligned}\nplim\\ \\hat{\\beta}\n&= \\beta \\left(1 - \\frac{\\sigma_\\epsilon^2 + \\sigma_{X\\epsilon}}{\\sigma_X^2 + \\sigma_\\epsilon^2 + 2 \\sigma_{X\\epsilon}} \\right) \\\\\n&= \\beta (1 - b_{\\epsilon \\tilde{X}}),\n\\end{aligned}\n\\]\\(b_{\\epsilon \\tilde{X}}\\) regression coefficient \\(\\epsilon\\) \\(\\tilde{X}\\), precisely:\\[\nb_{\\epsilon \\tilde{X}} = \\frac{\\text{cov}(\\epsilon, \\tilde{X})}{\\text{var}(\\tilde{X})}.\n\\]makes clear bias \\(\\hat{\\beta}\\) depends strongly measurement error correlated observed regressor \\(\\tilde{X}\\). general formulation nests classical case special case:classical error: \\(\\sigma_{X\\epsilon} = 0 \\Rightarrow b_{\\epsilon \\tilde{X}} = \\frac{\\sigma^2_\\epsilon}{\\sigma^2_X + \\sigma^2_\\epsilon} = 1 - \\lambda\\).Implications Non-Classical Measurement ErrorWhen \\(\\sigma_{X\\epsilon} > 0\\), attenuation bias can increase decrease depending balance variances.particular:\nhalf variance \\(\\tilde{X}\\) due measurement error, increasing \\(\\sigma_{X\\epsilon}\\) increases attenuation.\nless half due measurement error, can actually reduce attenuation.\nhalf variance \\(\\tilde{X}\\) due measurement error, increasing \\(\\sigma_{X\\epsilon}\\) increases attenuation.less half due measurement error, can actually reduce attenuation.phenomenon sometimes called mean-reverting measurement error: measurement error pulls observed values toward mean, distorting estimates Bound, Brown, Mathiowetz (2001).","code":""},{"path":"sec-endogeneity.html","id":"a-general-framework-for-non-classical-measurement-error","chapter":"36 Endogeneity","heading":"36.1.1.2.1 A General Framework for Non-Classical Measurement Error","text":"Bound, Brown, Mathiowetz (2001) offer unified matrix framework accommodates measurement error independent dependent variables.Let true model :\\[\n\\mathbf{Y = X \\beta + \\epsilon},\n\\]observe \\(\\tilde{X} = X + U\\) \\(\\tilde{Y} = Y + v\\), :\\(U\\) matrix measurement error \\(X\\),\\(v\\) vector measurement error \\(Y\\)., observed model becomes:\\[\n\\hat{\\beta} = (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' \\tilde{Y}.\n\\]Substituting observed quantities:\\[\n\\begin{aligned}\n\\tilde{Y} &= Y + v = X \\beta + \\epsilon + v, \\\\\n&= \\tilde{X} \\beta - U \\beta + v + \\epsilon.\n\\end{aligned}\n\\]Hence,\\[\n\\hat{\\beta} = (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (\\tilde{X} \\beta - U \\beta + v + \\epsilon),\n\\]simplifies :\\[\n\\hat{\\beta} = \\beta + (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (-U \\beta + v + \\epsilon).\n\\]Taking probability limit:\\[\nplim\\ \\hat{\\beta} = \\beta + plim\\ [(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (-U \\beta + v)],\n\\]Now define:\\[\nW = [U \\quad v],\n\\]can express bias compactly :\\[\nplim\\ \\hat{\\beta} = \\beta + plim\\ [(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' W\n\\begin{bmatrix}\n- \\beta \\\\\n1\n\\end{bmatrix}\n].\n\\]formulation highlights powerful insight:Bias \\(\\hat{\\beta}\\) arises linear projection measurement errors onto observed \\(\\tilde{X}\\).expression assert \\(v\\) necessarily biases \\(\\hat\\beta\\); simply makes explicit bias arises whenever linear projection \\((U\\beta-v)\\) onto \\(\\tilde X\\) non‑zero. Three cases illustrate point:Classical Y‑error \\(U\\equiv0,\\; \\operatorname{Cov}(\\tilde X,v)=0\\)Correlated Y‑error\\(U\\equiv0,\\; \\operatorname{Cov}(\\tilde X,v)\\neq0\\)X‑ Y‑error, independent\\(\\operatorname{Cov}(X,U)\\neq0,\\; \\operatorname{Cov}(\\tilde X,v)=0\\)Hence, usual “harmless \\(Y\\)-noise” result special case first row.Practical implicationsCheck assumptions explicitly. dataset generated self‑reports, simultaneous proxies, modelled outcomes, rarely safe assume \\(\\operatorname{Cov}(X,v)=0\\).Check assumptions explicitly. dataset generated self‑reports, simultaneous proxies, modelled outcomes, rarely safe assume \\(\\operatorname{Cov}(X,v)=0\\).Correlated errors \\(Y\\) can creep :\nCommon data‑generating mechanisms (e.g., survey module records earnings (\\(Y\\)) hours worked (\\(X\\))).\nPrediction‑generated variables \\(v\\) inherits correlation features used build \\(\\tilde Y\\).\nCorrelated errors \\(Y\\) can creep :Common data‑generating mechanisms (e.g., survey module records earnings (\\(Y\\)) hours worked (\\(X\\))).Prediction‑generated variables \\(v\\) inherits correlation features used build \\(\\tilde Y\\).Joint mis‑measurement (\\(U\\) \\(v\\) correlated) common administrative sensor data; , even “classical” \\(v\\) respect \\(X\\) can correlate \\(\\tilde X=X+U\\).Joint mis‑measurement (\\(U\\) \\(v\\) correlated) common administrative sensor data; , even “classical” \\(v\\) respect \\(X\\) can correlate \\(\\tilde X=X+U\\).Measurement error \\(Y\\) benign strong exogeneity independence conditions. Bound–Brown–Mathiowetz matrix form (Bound, Brown, Mathiowetz 2001) simply shows conditions fail—\\(X\\) mis‑measured—projection logic produces attenuation bias \\(X\\) can also transmit bias \\(v\\) \\(\\hat\\beta\\).rule thumb learned true narrow, classical setting, Bound, Brown, Mathiowetz (2001) remind us empirical work often strays outside safe harbor.Consequences CorrectionNon-classical error can lead - underestimation, unlike always-attenuating classical case.direction magnitude bias depend correlation structure \\(X\\), \\(\\epsilon\\), \\(v\\).poses serious problems many survey administrative data settings systematic misreporting occurs.Practical SolutionsInstrumental Variables\nUse instrument \\(Z\\) correlated true variable \\(X\\), uncorrelated measurement error regression disturbance. IV can help eliminate classical non-classical error-induced biases.Instrumental Variables\nUse instrument \\(Z\\) correlated true variable \\(X\\), uncorrelated measurement error regression disturbance. IV can help eliminate classical non-classical error-induced biases.Validation Studies\nUse subset data accurate measures estimate structure measurement error correct estimates via techniques regression calibration, multiple imputation, SIMEX.Validation Studies\nUse subset data accurate measures estimate structure measurement error correct estimates via techniques regression calibration, multiple imputation, SIMEX.Modeling Error Process\nExplicitly model measurement error process, especially longitudinal panel data (e.g., via state-space models Bayesian approaches).Modeling Error Process\nExplicitly model measurement error process, especially longitudinal panel data (e.g., via state-space models Bayesian approaches).Binary/Dummy Variable Case\nNon-classical error binary regressors (e.g., misclassification) also leads bias, IV methods still apply. example, education level misreported survey data, valid instrument (e.g., policy-based variation) can correct misclassification bias.Binary/Dummy Variable Case\nNon-classical error binary regressors (e.g., misclassification) also leads bias, IV methods still apply. example, education level misreported survey data, valid instrument (e.g., policy-based variation) can correct misclassification bias.SummaryIn short, non-classical measurement error breaks comforting regularity attenuation bias. can produce arbitrary biases depending nature structure error. Instrumental variables validation studies often reliable tools addressing complex problem.","code":""},{"path":"sec-endogeneity.html","id":"solution-to-measurement-errors-in-correlation-estimation","chapter":"36 Endogeneity","heading":"36.1.1.3 Solution to Measurement Errors in Correlation Estimation","text":"","code":""},{"path":"sec-endogeneity.html","id":"bayesian-correction-for-correlation-coefficient","chapter":"36 Endogeneity","heading":"36.1.1.3.1 Bayesian Correction for Correlation Coefficient","text":"begin expressing Bayesian posterior correlation coefficient \\(\\rho\\):\\[\n\\begin{aligned}\nP(\\rho \\mid \\text{data}) &= \\frac{P(\\text{data} \\mid \\rho) P(\\rho)}{P(\\text{data})} \\\\\n\\text{Posterior Probability} &\\propto \\text{Likelihood} \\times \\text{Prior Probability}\n\\end{aligned}\n\\]:\\(\\rho\\) true population correlation coefficient\\(P(\\text{data} \\mid \\rho)\\) likelihood function\\(P(\\rho)\\) prior density \\(\\rho\\)\\(P(\\text{data})\\) marginal likelihood (normalizing constant)sample correlation coefficient \\(r\\):\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx} S_{yy}}}\n\\]According Schisterman et al. (2003), pp. 3, posterior density \\(\\rho\\) can approximated :\\[\nP(\\rho \\mid x, y) \\propto P(\\rho) \\cdot \\frac{(1 - \\rho^2)^{(n - 1)/2}}{(1 - \\rho r)^{n - 3/2}}\n\\]approximation leads posterior can modeled via Fisher transformation:Let \\(\\rho = \\tanh(\\xi)\\), \\(\\xi \\sim N(z, 1/n)\\)\\(r = \\tanh(z)\\) Fisher-transformed correlationUsing conjugate normal approximations, derive posterior transformed correlation \\(\\xi\\) :Posterior Variance:\\[\n\\sigma^2_{\\text{posterior}} = \\frac{1}{n_{\\text{prior}} + n_{\\text{likelihood}}}\n\\]Posterior Mean:\\[\n\\mu_{\\text{posterior}} = \\sigma^2_{\\text{posterior}} \\left(n_{\\text{prior}} \\cdot \\tanh^{-1}(r_{\\text{prior}}) + n_{\\text{likelihood}} \\cdot \\tanh^{-1}(r_{\\text{likelihood}})\\right)\n\\]simplify mathematics, may assume prior form:\\[\nP(\\rho) \\propto (1 - \\rho^2)^c\n\\]\\(c\\) controls strength prior. prior information available, can set \\(c = 0\\) \\(P(\\rho) \\propto 1\\).Example: Combining Estimates Two StudiesLet:Current study: \\(r_{\\text{likelihood}} = 0.5\\), \\(n_{\\text{likelihood}} = 200\\)Prior study: \\(r_{\\text{prior}} = 0.2765\\), \\(n_{\\text{prior}} = 50205\\)Step 1: Posterior Variance\\[\n\\sigma^2_{\\text{posterior}} = \\frac{1}{50205 + 200} = 0.0000198393\n\\]Step 2: Posterior MeanApply Fisher transformation:\\(\\tanh^{-1}(0.2765) \\approx 0.2841\\)\\(\\tanh^{-1}(0.5) = 0.5493\\):\\[\n\\begin{aligned}\n\\mu_{\\text{posterior}} &= 0.0000198393 \\times (50205 \\times 0.2841 + 200 \\times 0.5493) \\\\\n&= 0.0000198393 \\times (14260.7 + 109.86) \\\\\n&= 0.0000198393 \\times 14370.56 = 0.2850\n\\end{aligned}\n\\]Thus, posterior distribution \\(\\xi = \\tanh^{-1}(\\rho)\\) :\\[\n\\xi \\sim N(0.2850, 0.0000198393)\n\\]Transforming back:Posterior mean correlation: \\(\\rho = \\tanh(0.2850) = 0.2776\\)95% CI \\(\\xi\\): \\(0.2850 \\pm 1.96 \\cdot \\sqrt{0.0000198393} = (0.2762, 0.2937)\\)Transforming endpoints: \\(\\tanh(0.2762) = 0.2694\\), \\(\\tanh(0.2937) = 0.2855\\)Bayesian posterior distribution correlation coefficient :Mean: \\(\\hat{\\rho}_{\\text{posterior}} = 0.2776\\)95% CI: \\((0.2694,\\ 0.2855)\\)Bayesian adjustment especially useful :high sampling variation due small sample sizesMeasurement error attenuates observed correlationCombining evidence multiple studies (meta-analytic context)leveraging prior information applying Fisher transformation, researchers can obtain stable accurate estimate true underlying correlation.","code":"\n# Define inputs\nn_new  <- 200\nr_new  <- 0.5\nalpha  <- 0.05\n\n# Bayesian update function for correlation coefficient\nupdate_correlation <- function(n_new, r_new, alpha) {\n  \n  # Prior (meta-analysis study)\n  n_meta <- 50205\n  r_meta <- 0.2765\n  \n  # Step 1: Posterior variance (in Fisher-z space)\n  var_xi <- 1 / (n_new + n_meta)\n  \n  # Step 2: Posterior mean (in Fisher-z space)\n  mu_xi <- var_xi * (n_meta * atanh(r_meta) + n_new * atanh(r_new))\n  \n  # Step 3: Confidence interval in Fisher-z space\n  z_crit    <- qnorm(1 - alpha / 2)\n  upper_xi  <- mu_xi + z_crit * sqrt(var_xi)\n  lower_xi  <- mu_xi - z_crit * sqrt(var_xi)\n  \n  # Step 4: Transform back to correlation scale\n  mean_rho  <- tanh(mu_xi)\n  upper_rho <- tanh(upper_xi)\n  lower_rho <- tanh(lower_xi)\n  \n  # Return all values as a list\n  list(\n    mu_xi     = mu_xi,\n    var_xi    = var_xi,\n    upper_xi  = upper_xi,\n    lower_xi  = lower_xi,\n    mean_rho  = mean_rho,\n    upper_rho = upper_rho,\n    lower_rho = lower_rho\n  )\n}\n\n\n# Run update\nupdated <-\n    update_correlation(n_new = n_new,\n                       r_new = r_new,\n                       alpha = alpha)\n\n# Display updated posterior mean and confidence interval\ncat(\"Posterior mean of rho:\", round(updated$mean_rho, 4), \"\\n\")\n#> Posterior mean of rho: 0.2775\ncat(\n    \"95% CI for rho: (\",\n    round(updated$lower_rho, 4),\n    \",\",\n    round(updated$upper_rho, 4),\n    \")\\n\"\n)\n#> 95% CI for rho: ( 0.2694 , 0.2855 )\n\n# For comparison: Classical (frequentist) confidence interval around r_new\nse_r  <- sqrt(1 / n_new)\nz_r   <- qnorm(1 - alpha / 2) * se_r\nci_lo <- r_new - z_r\nci_hi <- r_new + z_r\n\ncat(\"Frequentist 95% CI for r:\",\n    round(ci_lo, 4),\n    \"to\",\n    round(ci_hi, 4),\n    \"\\n\")\n#> Frequentist 95% CI for r: 0.3614 to 0.6386"},{"path":"sec-endogeneity.html","id":"sec-simultaneity","chapter":"36 Endogeneity","heading":"36.1.2 Simultaneity","text":"Simultaneity arises least one explanatory variables regression model jointly determined dependent variable, violating critical assumption causal inference: temporal precedence.Simultaneity MattersIn classical regression, assume regressors determined exogenously—influenced dependent variable.Simultaneity introduces endogeneity, regressors correlated error term, rendering OLS estimators biased inconsistent.major implications fields like economics, marketing, finance, social sciences, feedback mechanisms equilibrium processes common.Real-World ExamplesDemand supply: Price quantity determined together market equilibrium.Sales advertising: Advertising influences sales, firms also adjust advertising based current anticipated sales.Productivity investment: Higher productivity may attract investment, investment can improve productivity.","code":""},{"path":"sec-endogeneity.html","id":"simultaneous-equation-system","chapter":"36 Endogeneity","heading":"36.1.2.1 Simultaneous Equation System","text":"begin basic two-equation structural model:\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + u_i \\quad \\text{(Structural equation } Y) \\\\\nX_i &= \\alpha_0 + \\alpha_1 Y_i + v_i \\quad \\text{(Structural equation } X)\n\\end{aligned}\n\\]:\\(Y_i\\) \\(X_i\\) endogenous variables — determined within system.\\(u_i\\) \\(v_i\\) structural error terms, assumed uncorrelated exogenous variables ().equations form simultaneous system endogenous variable appears right-hand side ’s equation.uncover statistical properties equations, solve \\(Y_i\\) \\(X_i\\) functions error terms :\\[\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 v_i + u_i}{1 - \\alpha_1 \\beta_1} \\\\\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}\n\\end{aligned}\n\\]reduced-form equations, expressing endogenous variables functions exogenous factors disturbances.","code":""},{"path":"sec-endogeneity.html","id":"simultaneity-bias-in-ols","chapter":"36 Endogeneity","heading":"36.1.2.2 Simultaneity Bias in OLS","text":"naïvely estimate first equation using OLS, assuming \\(X_i\\) exogenous, get:\\[\n\\text{Bias: } \\quad Cov(X_i, u_i) = Cov\\left(\\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}, u_i\\right) = \\frac{\\alpha_1}{1 - \\alpha_1 \\beta_1} \\cdot Var(u_i)\n\\]violates Gauss-Markov Theorem regressors uncorrelated error term. OLS estimator \\(\\beta_1\\) biased inconsistent.allow identification estimation, introduce exogenous variables:\\[\n\\begin{cases}\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + u_i \\\\\nX_i = \\alpha_0 + \\alpha_1 Y_i + \\alpha_2 Z_i + v_i\n\\end{cases}\n\\]:\\(X_i\\), \\(Y_i\\) — endogenous variables\\(T_i\\), \\(Z_i\\) — exogenous variables, influenced variable systemSolving system algebraically yields reduced form model:\\[\n\\begin{cases}\\begin{aligned}Y_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 \\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{u}_i \\\\&= B_0 + B_1 Z_i + B_2 T_i + \\tilde{u}_i\\end{aligned}\\\\\\begin{aligned}X_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\alpha_1\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{v}_i \\\\&= A_0 + A_1 Z_i + A_2 T_i + \\tilde{v}_i\\end{aligned}\\end{cases}\n\\]reduced form expresses endogenous variables functions exogenous instruments, can estimate using OLS.Using reduced-form estimates \\((A_1, A_2, B_1, B_2)\\), can identify (recover) structural coefficients:\\[\n\\begin{aligned}\n\\beta_1 &= \\frac{B_1}{A_1} \\\\\n\\beta_2 &= B_2 \\left(1 - \\frac{B_1 A_2}{A_1 B_2}\\right) \\\\\n\\alpha_1 &= \\frac{A_2}{B_2} \\\\\n\\alpha_2 &= A_1 \\left(1 - \\frac{B_1 A_2}{A_1 B_2} \\right)\n\\end{aligned}\n\\]","code":""},{"path":"sec-endogeneity.html","id":"identification-conditions","chapter":"36 Endogeneity","heading":"36.1.2.3 Identification Conditions","text":"Estimation structural parameters possible model identified.Order Condition (Necessary Sufficient)structural equation identified :\\[\nK - k \\ge m - 1\n\\]:\\(M\\) = total number endogenous variables system\\(M\\) = total number endogenous variables system\\(m\\) = number endogenous variables given equation\\(m\\) = number endogenous variables given equation\\(K\\) = number total exogenous variables system\\(K\\) = number total exogenous variables system\\(k\\) = number exogenous variables appearing given equation\\(k\\) = number exogenous variables appearing given equationJust-identified: \\(K - k = m - 1\\) (exact identification)Just-identified: \\(K - k = m - 1\\) (exact identification)-identified: \\(K - k > m - 1\\) (instruments necessary)-identified: \\(K - k > m - 1\\) (instruments necessary)-identified: \\(K - k < m - 1\\) (estimated)-identified: \\(K - k < m - 1\\) (estimated)Note: order condition necessary sufficient. rank condition must also satisfied full identification, cover Instrumental Variables.simultaneous equations framework provides foundation instrumental variable estimation, :Exogenous variables appearing structural equation serve instruments.instruments allow consistent estimation endogenous regressors’ effects.reduced-form equations often used generate fitted values endogenous regressors, used Two-Stage Least Squares estimation process","code":""},{"path":"sec-endogeneity.html","id":"sec-reverse-causality","chapter":"36 Endogeneity","heading":"36.1.3 Reverse Causality","text":"Reverse causality refers situation direction causation opposite presumed. Specifically, may model relationship variable \\(X\\) assumed cause \\(Y\\), reality, \\(Y\\) causes \\(X\\), influence feedback loop.violates fundamental assumption causal inference: temporal precedence — cause must come effect. presence reverse causality, relationship \\(X\\) \\(Y\\) becomes ambiguous, statistical estimators OLS become biased inconsistent.standard linear regression model:\\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\]interpret \\(\\beta_1\\) causal effect \\(X\\) \\(Y\\). However, interpretation implicitly assumes :\\(X_i\\) exogenous (uncorrelated \\(u_i\\))Changes \\(X_i\\) occur prior independently changes \\(Y_i\\)\\(Y_i\\) also affects \\(X_i\\), \\(X_i\\) exogenous — endogenous, correlated \\(u_i\\) via reverse causal path.Reverse causality especially problematic observational data interventions randomly assigned. key examples include:Health income: Higher income may improve health outcomes, healthier individuals may also earn (e.g., due better productivity fewer sick days).Education wages: Education raises wages, higher-income individuals might afford better education — individuals higher innate ability (reflected \\(u\\)) pursue education also earn .Crime policing: Increased police presence often assumed reduce crime, high-crime areas also likely receive police resources.Advertising sales: Firms advertise boost sales, high sales may also lead higher advertising budgets — especially revenue reinvested marketing.model reverse causality explicitly, consider:","code":""},{"path":"sec-endogeneity.html","id":"system-of-equations","chapter":"36 Endogeneity","heading":"36.1.3.1 System of Equations","text":"\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + u_i \\quad &\\text{(Y depends X)} \\\\\nX_i &= \\gamma_0 + \\gamma_1 Y_i + v_i \\quad &\\text{(X depends Y)}\n\\end{aligned}\n\\]feedback loop represents simultaneous system, causality direction unclear. two equations indicate variables endogenous.Even estimate first equation using OLS, bias becomes apparent:\\[\nCov(X_i, u_i) \\ne 0 \\quad \\Rightarrow \\quad \\hat{\\beta}_1 \\text{ biased}\n\\]? \\(X_i\\) determined \\(Y_i\\), depends \\(u_i\\). Thus, \\(X_i\\) indirectly depends \\(u_i\\).causal diagram notation (Directed Acyclic Graphs, DAGs), reverse causality violates acyclicity assumption. ’s example:Intended model: \\(X \\rightarrow Y\\)Reality: \\(X \\leftrightarrow Y\\) (feedback loop)non-directional causality prevents us interpreting coefficients causally unless additional identification strategies applied.OLS assumes:\\[\nE[u_i \\mid X_i] = 0\n\\]reverse causality, condition fails. resulting estimator \\(\\hat{\\beta}_1\\) captures effect \\(X\\) \\(Y\\) feedback \\(Y\\) \\(X\\), leading :Omitted variable bias: \\(X_i\\) captures unobserved information \\(Y_i\\)Simultaneity bias: caused endogenous nature \\(X_i\\)","code":""},{"path":"sec-endogeneity.html","id":"distinction-from-simultaneity","chapter":"36 Endogeneity","heading":"36.1.3.2 Distinction from Simultaneity","text":"Reverse causality special case endogeneity, often manifesting simultaneity. However, key distinction :Simultaneity: Variables determined together (e.g., equilibrium models), modeled explicitly system.Reverse causality: one equation estimated, true causal direction unknown opposite modeled.Reverse causality may may involve full simultaneous system — ’s often unrecognized assumed away, making especially dangerous empirical research.mechanical tests definitively detect reverse causality, researchers can:Use temporal data (lags): Estimate \\(Y_{} = \\beta_0 + \\beta_1 X_{,t-1} + u_{}\\) examine temporal precedence variables.Apply Granger causality tests time series (strictly causal, helpful diagnostically).Use theoretical reasoning justify directionality.Check robustness across different time frames instrumental variables.","code":""},{"path":"sec-endogeneity.html","id":"solutions-to-reverse-causality","chapter":"36 Endogeneity","heading":"36.1.3.3 Solutions to Reverse Causality","text":"following methods can mitigate reverse causality:Instrumental VariablesFind variable \\(Z\\) affects \\(X\\) affected \\(Y\\), correlated \\(u_i\\).First stage: \\(X_i = \\pi_0 + \\pi_1 Z_i + e_i\\)Second stage: \\(\\hat{X}_i\\) first stage used regression \\(Y\\).Randomized Controlled Trials (RCTs)experiments, treatment (e.g., \\(X\\)) assigned randomly therefore exogenous design.Natural Experiments / Quasi-Experimental DesignsUse external shocks policy changes affect \\(X\\) \\(Y\\) directly (e.g., difference--differences, regression discontinuity).Panel Data MethodsUse fixed-effects difference estimators eliminate time-invariant confounders.Lag independent variables examine delayed effects improve causal direction inference.Structural Equation ModelingEstimate full system equations explicitly model feedback.","code":""},{"path":"sec-endogeneity.html","id":"sec-omitted-variable-bias","chapter":"36 Endogeneity","heading":"36.1.4 Omitted Variable Bias","text":"Omitted Variable Bias (OVB) arises relevant explanatory variable influences dependent variable left regression model, omitted variable correlated one included regressors. violates exogeneity assumption OLS leads biased inconsistent estimators.Suppose interested estimating effect independent variable \\(X\\) outcome \\(Y\\), true data-generating process :\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + u_i\n\\]However, omit \\(Z_i\\) estimate model:\\[\nY_i = \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i\n\\]estimate \\(\\hat{\\gamma}_1\\) may biased \\(X_i\\) may correlated \\(Z_i\\), \\(Z_i\\) influences \\(Y_i\\).Let us derive bias formally.True model:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + u_i \\quad \\text{(1)}\n\\]Estimated model (\\(Z_i\\) omitted):\\[\nY_i = \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i \\quad \\text{(2)}\n\\]Now, substitute true model omitted model:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + u_i = \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i\n\\]Comparing models, omitted variable becomes part new error term:\\[\n\\varepsilon_i = \\beta_2 Z_i + u_i\n\\]Now, consider OLS assumption:\\[\nE[\\varepsilon_i \\mid X_i] = 0 \\quad \\text{(OLS requirement)}\n\\]since \\(\\varepsilon_i = \\beta_2 Z_i + u_i\\) \\(Z_i\\) correlated \\(X_i\\), :\\[\nCov(X_i, \\varepsilon_i) = \\beta_2 Cov(X_i, Z_i) \\ne 0\n\\]Therefore, OLS assumption fails, \\(\\hat{\\gamma}_1\\) biased.Let us calculate expected value OLS estimator \\(\\hat{\\gamma}_1\\).regression theory, omitting \\(Z_i\\), expected value \\(\\hat{\\gamma}_1\\) :\\[\nE[\\hat{\\gamma}_1] = \\beta_1 + \\beta_2 \\cdot \\frac{Cov(X_i, Z_i)}{Var(X_i)}\n\\]Omitted Variable Bias formula.Interpretation:\nbias \\(\\hat{\\gamma}_1\\) depends : - \\(\\beta_2\\): true effect omitted variable \\(Y\\) - \\(Cov(X_i, Z_i)\\): correlation \\(X\\) omitted variable \\(Z\\)","code":""},{"path":"sec-endogeneity.html","id":"direction-of-the-bias","chapter":"36 Endogeneity","heading":"36.1.4.1 Direction of the Bias","text":"\\(\\beta_2 > 0\\) \\(Cov(X_i, Z_i) > 0\\): \\(\\hat{\\gamma}_1\\) upward biasedIf \\(\\beta_2 < 0\\) \\(Cov(X_i, Z_i) > 0\\): \\(\\hat{\\gamma}_1\\) downward biasedIf \\(Cov(X_i, Z_i) = 0\\): bias, even \\(Z_i\\) omittedNote: Uncorrelated omitted variables bias OLS estimator, although may reduce precision.","code":""},{"path":"sec-endogeneity.html","id":"practical-example-education-and-earnings","chapter":"36 Endogeneity","heading":"36.1.4.2 Practical Example: Education and Earnings","text":"Suppose model:\\[\n\\text{Earnings}_i = \\gamma_0 + \\gamma_1 \\cdot \\text{Education}_i + \\varepsilon_i\n\\]true model includes ability (\\(Z_i\\)):\\[\n\\text{Earnings}_i = \\beta_0 + \\beta_1 \\cdot \\text{Education}_i + \\beta_2 \\cdot \\text{Ability}_i + u_i\n\\]Omitting “ability” — determinant education earnings — leads bias estimated effect education:able individuals pursue education ability raises earnings (\\(\\beta_2 > 0\\)), \\(\\hat{\\gamma}_1\\) overstates true return education.","code":""},{"path":"sec-endogeneity.html","id":"generalization-to-multiple-regression","chapter":"36 Endogeneity","heading":"36.1.4.3 Generalization to Multiple Regression","text":"models multiple regressors, omitting relevant variable correlated least one included regressor bias coefficients affected correlation structure.example:\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\n\\]\\(X_2\\) omitted, \\(Cov(X_1, X_2) \\ne 0\\), :\\[\nE[\\hat{\\gamma}_1] = \\beta_1 + \\beta_2 \\cdot \\frac{Cov(X_1, X_2)}{Var(X_1)}\n\\]","code":""},{"path":"sec-endogeneity.html","id":"remedies-for-ovb","chapter":"36 Endogeneity","heading":"36.1.4.4 Remedies for OVB","text":"Include omitted variableIf \\(Z\\) observed, include regression model.Use Instrumental VariablesIf \\(Z\\) unobserved \\(X\\) endogenous, find instrument \\(W\\):\nRelevance: \\(Cov(W, X) \\ne 0\\)\nExogeneity: \\(Cov(W, u) = 0\\)\n\\(Z\\) unobserved \\(X\\) endogenous, find instrument \\(W\\):Relevance: \\(Cov(W, X) \\ne 0\\)Exogeneity: \\(Cov(W, u) = 0\\)Use Panel Data MethodsFixed Effects: eliminate time-invariant omitted variables.Difference--Differences: exploit temporal variation isolate effects.Experimental DesignsRandomization ensures omitted variables orthogonal treatment, avoiding bias.","code":""},{"path":"sec-endogeneity.html","id":"sec-endogenous-sample-selection","chapter":"36 Endogeneity","heading":"36.2 Endogenous Sample Selection","text":"Endogenous sample selection arises observational non-experimental research whenever inclusion observations (assignment treatment) random, unobservable factors influencing selection also affect outcome interest. scenario leads biased inconsistent estimates causal parameters (e.g., Average Treatment Effect) properly addressed.problem first formalized econometric literature J. Heckman (1974), J. J. Heckman (1976b), J. J. Heckman (1979), whose work addressed issue context labor force participation among women. Later, Amemiya (1984) generalize method. Now, since applied widely across social sciences, epidemiology, marketing, finance.Endogenous sample selection often conflated general selection bias, important understand sample selection refers specifically inclusion observations estimation sample, just assignment treatment (.e., selection bias).problem comes many names self-selection problem, incidental truncation, omitted variable (.e., omitted variable people selected sample). disciplines consider nonresponse/selection bias sample selection:unobservable factors affect sample independent unobservable factors affect outcome, sample selection endogenous. Hence, sample selection ignorable estimator ignores sample selection still consistent.unobservable factors affect included sample correlated unobservable factors affect outcome, sample selection endogenous ignorable, estimators ignore endogenous sample selection consistent (don’t know part observable outcome related causal relationship part due different people selected treatment control groups).Many evaluation studies use observational data, data:Participants randomly assigned.Treatment exposure determined individual institutional choices.Counterfactual outcomes observed.treatment indicator often endogenous.notable terminologies include:Truncation: Occurs data collected restricted subpopulation based value variable.\nLeft truncation: Values threshold excluded (e.g., high-income individuals surveyed).\nRight truncation: Values threshold excluded.\nLeft truncation: Values threshold excluded (e.g., high-income individuals surveyed).Right truncation: Values threshold excluded.Censoring: Occurs variable observed coarsened beyond threshold.\nE.g., incomes certain level coded zero; arrest counts threshold top-coded.\nE.g., incomes certain level coded zero; arrest counts threshold top-coded.Incidental Truncation: Refers selection based latent variable (e.g., employment decisions), directly observed. makes Heckman’s model distinct.\nAlso called non-random sample selection.\nerror outcome equation correlated selection indicator.\nAlso called non-random sample selection.error outcome equation correlated selection indicator.Researchers often categorize self-selection :Negative (Mitigation-Based) Selection: Individuals select treatment sample address existing problem, start worse potential outcomes.\nBias direction: Underestimates true treatment effects (makes treatment look less effective ).\nIndividuals select treatment combat problem already face.\nExamples:\nPeople high risk severe illness (e.g., elderly immunocompromised individuals) likely get vaccinated. compare vaccinated vs. unvaccinated individuals without adjusting risk factors, might mistakenly conclude vaccines ineffective simply vaccinated individuals worse initial health conditions.\nEvaluating effect job training programs—unemployed individuals greatest difficulty finding jobs likely enroll, leading underestimated program benefits.\n\nBias direction: Underestimates true treatment effects (makes treatment look less effective ).Individuals select treatment combat problem already face.Examples:\nPeople high risk severe illness (e.g., elderly immunocompromised individuals) likely get vaccinated. compare vaccinated vs. unvaccinated individuals without adjusting risk factors, might mistakenly conclude vaccines ineffective simply vaccinated individuals worse initial health conditions.\nEvaluating effect job training programs—unemployed individuals greatest difficulty finding jobs likely enroll, leading underestimated program benefits.\nPeople high risk severe illness (e.g., elderly immunocompromised individuals) likely get vaccinated. compare vaccinated vs. unvaccinated individuals without adjusting risk factors, might mistakenly conclude vaccines ineffective simply vaccinated individuals worse initial health conditions.Evaluating effect job training programs—unemployed individuals greatest difficulty finding jobs likely enroll, leading underestimated program benefits.Positive (Preference-Based) Selection: Individuals select treatment sample advantageous traits, preferences, resources. Hence, take treatment systematically better compared .\nBias direction: Overestimates true treatment effects (makes treatment look effective ).\nIndividuals select treatment inherently prefer , rather underlying problem.\nExamples:\nPeople health-conscious physically active likely join fitness program. compare fitness program participants non-participants, might falsely attribute better health outcomes program, reality, pre-existing lifestyle contributed improved health.\nEvaluating effect private school education—students attend private schools often come wealthier families greater academic support, making difficult isolate true impact school .\n\nBias direction: Overestimates true treatment effects (makes treatment look effective ).Individuals select treatment inherently prefer , rather underlying problem.Examples:\nPeople health-conscious physically active likely join fitness program. compare fitness program participants non-participants, might falsely attribute better health outcomes program, reality, pre-existing lifestyle contributed improved health.\nEvaluating effect private school education—students attend private schools often come wealthier families greater academic support, making difficult isolate true impact school .\nPeople health-conscious physically active likely join fitness program. compare fitness program participants non-participants, might falsely attribute better health outcomes program, reality, pre-existing lifestyle contributed improved health.Evaluating effect private school education—students attend private schools often come wealthier families greater academic support, making difficult isolate true impact school .forms selection reflect correlation unobservables (driving selection) potential outcomes—hallmark endogenous selection bias.seminal applied works area include:Labor Force Participation (J. Heckman 1974)Wages observed women choose work.Unobservable preferences (reservation wages) drive participation.Ignoring leads biased estimates returns education.Union Membership (Lewis 1986)Wages differ union non-union workers.union membership exogenous—workers choose join based anticipated benefits.Naïve OLS yields biased estimates union premium.College Attendance (Card 1999, 2001)Comparing income college graduates vs. non-graduates.Attending college choice based expected gains, ability, family background.treatment effect model (described next) appropriate .","code":""},{"path":"sec-endogeneity.html","id":"unifying-model-frameworks","chapter":"36 Endogeneity","heading":"36.2.1 Unifying Model Frameworks","text":"Though often conflated, several overlapping models address endogenous selection:Sample Selection Model (J. J. Heckman 1979): Outcome unobserved agent selected sample.Treatment Effect Model: Outcome observed groups (treated vs. untreated), treatment assignment endogenous.Heckman-Type / Control Function Approaches: Decompose endogenous regressor incorporate correction term (Inverse Mills Ratio residual) control endogeneity.revolve around challenge: unobserved factors affect included (treated) outcomes.formalize problem, consider outcome selection equations. Let:\\(y_i\\): observed outcome (e.g., wage)\\(x_i\\): covariates affecting outcome\\(z_i\\): covariates affecting selection\\(w_i\\): binary indicator selection sample (e.g., employment)","code":""},{"path":"sec-endogeneity.html","id":"sec-sample-selection-model","chapter":"36 Endogeneity","heading":"36.2.1.1 Sample Selection Model","text":"begin outcome equation, describes variable interest \\(y_i\\). However, observe \\(y_i\\) certain selection mechanism indicates part sample. mechanism captured binary indicator \\(w_i = 1\\). Formally, observed outcome equation :\\[\n\\begin{aligned}\ny_i &= x_i' \\beta + \\varepsilon_i, \\quad &\\text{(Observed } w_i = 1\\text{)}, \\\\\n\\varepsilon_i &\\sim N(0, \\sigma_\\varepsilon^2).\n\\end{aligned}\n\\], \\(x_i\\) vector explanatory variables (covariates) explain \\(y_i\\). noise term \\(\\varepsilon_i\\) assumed normally distributed mean zero variance \\(\\sigma_\\varepsilon^2\\). However, see \\(y_i\\) cases \\(w_i = 1\\), must account selection occurs.Next, specify selection equation via latent index model. Let \\(w_i^*\\) unobserved latent variable:\\[\n\\begin{aligned}\nw_i^* &= z_i' \\gamma + u_i, \\\\\nw_i &= \\begin{cases}\n1 & \\text{} w_i^* > 0, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\end{aligned}\n\\], \\(z_i\\) vector variables influence whether \\(y_i\\) observed. practice, \\(z_i\\) may overlap \\(x_i\\), can also include variables outcome equation. identification, normalize \\(\\mathrm{Var}(u_i) = 1\\). analogous probit model’s standard normalization.\\(w_i = 1\\) exactly \\(w_i^* > 0\\), event occurs \\(u_i \\ge -\\,z_i' \\gamma\\). Therefore,\\[\n\\begin{aligned}\nP(w_i = 1)\n&= P\\bigl(u_i \\ge -z_i' \\gamma\\bigr),\\\\\n&= 1 - \\Phi\\bigl(-z_i'\\gamma\\bigr), \\\\\n&= \\Phi\\bigl(z_i'\\gamma\\bigr),\n\\end{aligned}\n\\]use symmetry standard normal distribution.assume \\((\\varepsilon_i, u_i)\\) jointly normally distributed correlation \\(\\rho\\). words,\\[\n\\begin{pmatrix}\n\\varepsilon_i \\\\\nu_i\n\\end{pmatrix}\n\\;\\sim\\; \\mathcal{N} \\!\\Biggl(\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix} \\sigma^2_\\varepsilon & \\rho \\,\\sigma_\\varepsilon \\\\\n\\rho \\,\\sigma_\\varepsilon & 1 \\end{pmatrix}\n\\Biggr).\n\\]\\(\\rho = 0\\), selection exogenous: whether \\(y_i\\) observed unrelated unobserved determinants \\(y_i\\).\\(\\rho \\neq 0\\), sample selection endogenous. Failing model selection mechanism leads biased estimates \\(\\beta\\).Interpreting \\(\\rho\\)\\(\\rho > 0\\): Individuals higher unobserved components \\(\\varepsilon_i\\) (thus typically larger \\(y_i\\)) likely appear sample. (Positive selection)\\(\\rho < 0\\): Individuals higher unobserved components \\(\\varepsilon_i\\) less likely appear. (Negative selection)\\(\\rho = 0\\): endogenous selection. Observed outcomes effectively random respect unobserved part \\(y_i\\).empirical practice, \\(\\rho\\) signals direction bias one might expect selection process ignored.Often, helpful visualize part distribution \\(u_i\\) (error selection equation) truncated based threshold \\(w_i^*>0\\). notional R code snippet draws normal density shades region \\(u_i > -z_i'\\gamma\\).figure, gray‐shaded area represents \\(u_i > -z_i'\\gamma\\). Observations range included sample. \\(\\rho\\neq 0\\), unobserved factors drive \\(u_i\\) also affect \\(\\varepsilon_i\\), causing non‐representative sample \\(\\varepsilon_i\\).core insight Heckman model conditional expectation \\(y_i\\) given \\(w_i=1\\):\\[\nE\\bigl(y_i \\mid w_i = 1\\bigr)\n\\;=\\;\nE\\bigl(y_i \\mid w_i^*>0\\bigr)\n\\;=\\;\nE\\bigl(x_i'\\beta + \\varepsilon_i \\mid u_i > -z_i'\\gamma\\bigr).\n\\]Since \\(x_i'\\beta\\) nonrandom (conditional \\(x_i\\)), get\\[\nE\\bigl(y_i \\mid w_i=1\\bigr)\n= x_i'\\beta + E\\bigl(\\varepsilon_i \\mid u_i > -z_i'\\gamma\\bigr).\n\\]bivariate normal properties:\\[\n\\varepsilon_i \\,\\bigl\\lvert\\, u_i=\n\\;\\sim\\;\nN\\!\\Bigl(\\rho\\,\\sigma_{\\varepsilon}\\cdot ,\\; (1-\\rho^2)\\,\\sigma_{\\varepsilon}^2\\Bigr).\n\\]Thus,\\[\nE\\bigl(\\varepsilon_i \\mid u_i > -z_i'\\gamma\\bigr)\n=\\;\n\\rho\\,\\sigma_{\\varepsilon}\\,\nE\\bigl(u_i \\mid u_i > -z_i'\\gamma\\bigr).\n\\]\\(U\\sim N(0,1)\\), \\[\nE(U \\mid U>)\n= \\frac{\\phi()}{1-\\Phi()}\n= \\frac{\\phi()}{\\Phi(-)},\n\\] \\(\\phi\\) standard normal pdf, \\(\\Phi\\) cdf. symmetry, \\(\\phi(-)=\\phi()\\) \\(\\Phi(-)=1-\\Phi()\\). Letting \\(= -\\,z_i'\\gamma\\) yields\\[\nE\\bigl(U \\mid U > -z_i'\\gamma\\bigr)\n= \\frac{\\phi(-z_i'\\gamma)}{1-\\Phi(-z_i'\\gamma)}\n= \\frac{\\phi(z_i'\\gamma)}{\\Phi(z_i'\\gamma)}.\n\\]Define Inverse Mills Ratio (IMR) \\[\n\\lambda(x)\n= \\frac{\\phi(x)}{\\Phi(x)}.\n\\]Hence,\\[\nE\\bigl(\\varepsilon_i \\mid u_i > -z_i'\\gamma\\bigr)\n= \\rho\\,\\sigma_{\\varepsilon}\\,\\lambda\\bigl(z_i'\\gamma\\bigr),\n\\] therefore\\[\n\\boxed{\nE\\bigl(y_i \\mid w_i=1\\bigr)\n= x_i'\\beta\n\\;+\\;\n\\rho\\,\\sigma_{\\varepsilon}\\,\n\\frac{\\phi\\bigl(z_i'\\gamma\\bigr)}{\\Phi\\bigl(z_i'\\gamma\\bigr)}.\n}\n\\]extra term ‐called Heckman correction.IMR appears two‐step procedure regressor bias correction. useful derivatives:\\[\n\\frac{d}{dx}\\Bigl[\\text{IMR}(x)\\Bigr]\n= \\frac{d}{dx}\\Bigl[\\frac{\\phi(x)}{\\Phi(x)}\\Bigr]\n= -x\\,\\text{IMR}(x)\\;-\\;\\bigl[\\text{IMR}(x)\\bigr]^2.\n\\]arises quotient rule fact \\(\\phi'(x)=-x\\phi(x)\\), \\(\\Phi'(x)=\\phi(x)\\). derivative property also helps interpreting marginal effects selection models.","code":"\nx = seq(-3, 3, length = 200)\ny = dnorm(x, mean = 0, sd = 1)\nplot(x,\n     y,\n     type = \"l\",\n     main =  bquote(\"Probabibility distribution of\" ~ u[i]))\n\nx_shaded = seq(0.3, 3, length = 100)\ny_shaded = dnorm(x_shaded, 0, 1)\npolygon(c(0.3, x_shaded, 3), c(0, y_shaded, 0), col = \"gray\")\n\ntext(1, 0.1, expression(1 - Phi(-z[i] * gamma)))\narrows(-0.5, 0.1, 0.3, 0, length = 0.15)\ntext(-0.5, 0.12, expression(-z[i] * gamma))\nlegend(\"topright\",\n       \"Gray = Prob of Observed\",\n       pch = 1,\n       inset = 0.02)"},{"path":"sec-endogeneity.html","id":"sec-treatment-effect-switching-model","chapter":"36 Endogeneity","heading":"36.2.1.2 Treatment Effect (Switching) Model","text":"sample selection model used outcome observed one group (e.g., \\(D = 1\\)), treatment effect model used outcomes observed groups, treatment assignment endogenous.Treatment Effect Model Equations:Outcome: \\[ y_i = x_i' \\beta + D_i \\delta + \\varepsilon_i \\]Selection: \\[ D_i^* = z_i' \\gamma + u_i \\\\ D_i = 1 \\text{ } D_i^* > 0 \\]:\\(D_i\\) treatment indicator.\\(D_i\\) treatment indicator.\\((\\varepsilon_i, u_i)\\) bivariate normal correlation \\(\\rho\\).\\((\\varepsilon_i, u_i)\\) bivariate normal correlation \\(\\rho\\).treatment effect model sometimes called switching regression.","code":""},{"path":"sec-endogeneity.html","id":"sec-heckman-type-control-function","chapter":"36 Endogeneity","heading":"36.2.1.3 Heckman-Type vs. Control Function","text":"Heckman Sample Selection: Insert Inverse Mills Ratio (IMR) adjust outcome equation non-random truncation.Control Function: Residual-based predicted-endogenous-variable approach mirrors IV logic, typically still requires instrument parametric assumption.Differences Heckman Sample Selection vs. Heckman-type correction","code":""},{"path":"sec-endogeneity.html","id":"estimation-methods-2","chapter":"36 Endogeneity","heading":"36.2.2 Estimation Methods","text":"","code":""},{"path":"sec-endogeneity.html","id":"heckmans-two-step-estimator-heckit","chapter":"36 Endogeneity","heading":"36.2.2.1 Heckman’s Two-Step Estimator (Heckit)","text":"Step 1: Estimate Selection Equation ProbitWe estimate probability included sample: \\[ P(w_i = 1 \\mid z_i) = \\Phi(z_i' \\gamma) \\]estimated model, compute Inverse Mills Ratio (IMR): \\[ \\lambda_i = \\frac{\\phi(z_i' \\hat{\\gamma})}{\\Phi(z_i' \\hat{\\gamma})} \\]term captures expected value error outcome equation, conditional selection.Step 2: Include IMR Outcome EquationWe estimate regression: \\[ y_i = x_i' \\beta + \\delta \\lambda_i + \\nu_i \\]\\(\\delta\\) significantly different 0, selection bias present.\\(\\lambda_i\\) corrects non-random selection.OLS augmented model yields consistent estimates \\(\\beta\\) joint normality assumption.Pros: Conceptually simple; widely used.Cons: Relies heavily bivariate normal assumption \\((\\varepsilon_i, u_i)\\). good exclusion variable available, identification rests functional form.Specifically, model can identified without exclusion restriction, cases, identification driven purely non-linearity probit function normality assumption (IMR). fragile.strong exclusion restriction covariate correction equation, variation variable can help identify control selection.weak exclusion restriction, variable exists steps, ’s assumed error structure identifies control selection (J. Heckman Navarro-Lozano 2004).management, Wolfolds Siegel (2019) found papers valid exclusion conditions, without , simulations show results using Heckman method less reliable obtained OLS.robust identification, prefer exclusion restriction:variable affects selection (\\(z_i\\)) outcome.Example: Distance training center might affect probability enrollment, post-training income.Without variable, model relies solely functional form.Heckman two-step estimation procedure less efficient FIML. One key limitation two-step estimator fully exploit joint distribution error terms across equations, leading loss efficiency. Moreover, two-step approach may introduce measurement error second stage. arises inverse Mills ratio used second stage estimated regressor, can lead biased standard errors inference.","code":"\n###########################\n#   SIM 1: Heckman 2-step #\n###########################\nsuppressPackageStartupMessages(library(MASS))\n\nset.seed(123)\nn <- 1000\nrho <- 0.5\nbeta_true <- 2\n\ngamma_true <- 1.0\nSigma <- matrix(c(1, rho, rho, 1), 2)\nerrors <- mvrnorm(n, c(0,0), Sigma)\n\nepsilon <- errors[,1]\nu       <- errors[,2]\n\nx <- rnorm(n)\nw <- rnorm(n)\n\n# Selection\nz_star <- w*gamma_true + u\nz <- ifelse(z_star>0,1,0)\n\n# Outcome\n\ny_star <- x * beta_true + epsilon\n# Observed only if z=1\ny_obs <- ifelse(z == 1, y_star, NA)\n\n# Step 1: Probit\nsel_mod <- glm(z ~ w, family = binomial(link = \"probit\"))\nz_hat <- predict(sel_mod, type = \"link\")\nlambda_vals <- dnorm(z_hat) / pnorm(z_hat)\n\n# Step 2: OLS on observed + IMR\ndata_heck <- data.frame(y = y_obs,\n                        x = x,\n                        imr = lambda_vals,\n                        z = z)\nobserved_data <- subset(data_heck, z == 1)\nheck_lm <- lm(y ~ x + imr, data = observed_data)\nsummary(heck_lm)\n#> \n#> Call:\n#> lm(formula = y ~ x + imr, data = observed_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.76657 -0.60099 -0.02776  0.56317  2.74797 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.01715    0.07068   0.243    0.808    \n#> x            1.95925    0.03934  49.800  < 2e-16 ***\n#> imr          0.41900    0.10063   4.164 3.69e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.8942 on 501 degrees of freedom\n#> Multiple R-squared:  0.8332, Adjusted R-squared:  0.8325 \n#> F-statistic:  1251 on 2 and 501 DF,  p-value: < 2.2e-16\n\ncat(\"True beta=\", beta_true, \"\\n\")\n#> True beta= 2\ncat(\"Heckman 2-step estimated beta=\", coef(heck_lm)[\"x\"], \"\\n\")\n#> Heckman 2-step estimated beta= 1.959249"},{"path":"sec-endogeneity.html","id":"full-information-maximum-likelihood","chapter":"36 Endogeneity","heading":"36.2.2.2 Full Information Maximum Likelihood","text":"Jointly estimates selection outcome equations via ML, assuming:\\[\n\\biggl(\\varepsilon_i, u_i\\biggr) \\sim \\mathcal{N}\\biggl(\\begin{pmatrix}0\\\\0\\end{pmatrix},\\begin{pmatrix}\\sigma_{\\varepsilon}^2 & \\rho\\,\\sigma_{\\varepsilon}\\\\\\rho\\,\\sigma_{\\varepsilon} & 1\\end{pmatrix}\\biggr).\n\\]Pros: efficient distributional assumption correct. Allows direct test \\(\\rho=0\\) (LR test).Cons: sensitive specification errors (.e., requires stronger distributional assumptions); potentially complex implement.can use sampleSelection package R perform full maximum likelihood estimation data:can compare coefficient estimates x heck2 vs. heckFIML. large samples, converge true \\(\\beta\\). FIML typically efficient, normality assumption violated, can biased.","code":"\n#############################\n# SIM 2: 2-step vs. FIML    #\n#############################\n\nsuppressPackageStartupMessages(library(sampleSelection))\n\n# Using same data (z, x, y_obs) from above\n\n# 1) Heckman 2-step (built-in)\nheck2 <-\n    selection(z ~ w, y_obs ~ x, method = \"2step\", data = data_heck)\nsummary(heck2)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> 2-step Heckman / heckit estimation\n#> 1000 observations (496 censored and 504 observed)\n#> 7 free parameters (df = 994)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02053    0.04494   0.457    0.648    \n#> w            0.94063    0.05911  15.913   <2e-16 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.01715    0.07289   0.235    0.814    \n#> x            1.95925    0.03924  49.932   <2e-16 ***\n#> Multiple R-Squared:0.8332,   Adjusted R-Squared:0.8325\n#>    Error terms:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> invMillsRatio   0.4190     0.1018   4.116 4.18e-05 ***\n#> sigma           0.9388         NA      NA       NA    \n#> rho             0.4463         NA      NA       NA    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n\n# 2) FIML\nheckFIML <-\n    selection(z ~ w, y_obs ~ x, method = \"ml\", data = data_heck)\nsummary(heckFIML)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 2 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -1174.233 \n#> 1000 observations (496 censored and 504 observed)\n#> 6 free parameters (df = 994)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02169    0.04488   0.483    0.629    \n#> w            0.94203    0.05908  15.945   <2e-16 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.008601   0.071315   0.121    0.904    \n#> x           1.959195   0.039124  50.077   <2e-16 ***\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.94118    0.03503  26.867  < 2e-16 ***\n#> rho    0.46051    0.09411   4.893 1.16e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"sec-endogeneity.html","id":"cf-and-iv-approaches","chapter":"36 Endogeneity","heading":"36.2.2.3 CF and IV Approaches","text":"Control FunctionResidual-based approach: Regress selection (treatment) variable excluded instruments included controls. Obtain predicted residual. Include residual main outcome regression.correlated residual significant, indicates endogeneity; adjusting can correct bias.Often used context treatment effect models simultaneously IV logic.Instrumental VariablesIn pure treatment effect context, IV must affect treatment assignment outcome directly.sample selection, exclusion restriction (“instrument”) must shift selection outcomes.Example: Distance training center influences participation job program post-training earnings.","code":""},{"path":"sec-endogeneity.html","id":"theoretical-connections","chapter":"36 Endogeneity","heading":"36.2.3 Theoretical Connections","text":"","code":""},{"path":"sec-endogeneity.html","id":"conditional-expectation-from-truncated-distributions","chapter":"36 Endogeneity","heading":"36.2.3.1 Conditional Expectation from Truncated Distributions","text":"sample selection scenario:\\[\n\\begin{aligned}\n\\mathbb{E}[y_i\\mid w_i=1] &= x_i\\beta + \\mathbb{E}[\\varepsilon_i\\mid w_i^*>0],\\\\\n&= x_i\\beta + \\rho\\,\\sigma_{\\varepsilon}\\,\\frac{\\phi(z_i'\\gamma)}{\\Phi(z_i'\\gamma)},\n\\end{aligned}\n\\]\\(\\rho\\,\\sigma_{\\varepsilon}\\) covariance term \\(\\phi\\) \\(\\Phi\\) standard normal PDF CDF, respectively. formula underpins inverse Mills ratio correction.\\(\\rho>0\\), unobservables increase likelihood selection also increase outcomes, implying positive selection.\\(\\rho<0\\), selection negatively correlated outcomes.\\(\\hat{\\rho}\\): Estimated correlation error terms. significantly different 0, endogenous selection present.Wald Likelihood Ratio Test: Used test \\(H_0: \\rho = 0\\).Lambda (\\(\\hat{\\lambda}\\)): Product \\(\\hat{\\rho} \\hat{\\sigma}_\\varepsilon\\)—measures strength selection bias.Inverse Mills Ratio: Can saved inspected understand sample inclusion probabilities.","code":""},{"path":"sec-endogeneity.html","id":"relationship-among-models","chapter":"36 Endogeneity","heading":"36.2.3.2 Relationship Among Models","text":"models Unifying Model Frameworks can seen special generalized cases:one data selected group, ’s sample selection setup.one data selected group, ’s sample selection setup.data groups exist, assignment endogenous, ’s treatment effect problem.data groups exist, assignment endogenous, ’s treatment effect problem.’s valid instrument, one can control function IV approach.’s valid instrument, one can control function IV approach.normality assumption holds selection truly parametric, Heckman FIML correct bias.normality assumption holds selection truly parametric, Heckman FIML correct bias.Summary Table Methods","code":""},{"path":"sec-endogeneity.html","id":"concerns","chapter":"36 Endogeneity","heading":"36.2.3.3 Concerns","text":"Small Samples: Two-step procedures can unstable smaller datasets.Exclusion Restrictions: Without credible variable predicts selection outcomes, identification depends purely functional form (bivariate normal + nonlinearity probit).Distributional Assumption: normality seriously violated, neither 2-step FIML may reliably remove bias.Measurement Error IMR: second-stage includes estimated regressor \\(\\hat{\\lambda}_i\\), can add noise.Connection IV: strong instrument exists, one proceed control function standard IV treatment effect setup. sample selection (lack data unselected), Heckman approach common.Presence Correlation Error Terms: Heckman treatment effect model outperforms OLS \\(\\rho \\neq 0\\) corrects selection bias due unobserved factors. However, \\(\\rho = 0\\), correction unnecessary can introduce inefficiency, making simpler methods accurate.","code":""},{"path":"sec-endogeneity.html","id":"tobit-2-heckmans-sample-selection-model","chapter":"36 Endogeneity","heading":"36.2.4 Tobit-2: Heckman’s Sample Selection Model","text":"Tobit-2 model, also known Heckman’s standard sample selection model, designed correct sample selection bias. arises outcome variable observed non-random subset population, selection process correlated outcome interest.key assumption model joint normality error terms selection outcome equations.","code":""},{"path":"sec-endogeneity.html","id":"panel-study-of-income-dynamics","chapter":"36 Endogeneity","heading":"36.2.4.1 Panel Study of Income Dynamics","text":"demonstrate model using classic dataset Mroz (1984), provides data 1975 Panel Study Income Dynamics married women’s labor-force participation wages.aim estimate log hourly wages married women, using:educ: Years educationexper: Years work experienceexper^2: Experience squared (capture non-linear effects)city: dummy residence big cityHowever, wages observed participated labor force, meaning OLS regression using subsample suffer selection bias.also data non-participants, can use Heckman’s two-step method correct bias.Load Prepare DataModel OverviewThe two-step Heckman selection model proceeds follows:Selection Equation (Probit):\nModels probability labor force participation (lfp = 1) function variables affect decision work.Selection Equation (Probit):\nModels probability labor force participation (lfp = 1) function variables affect decision work.Outcome Equation (Wage):\nModels log wages conditional working. correction term, IMR, included account non-random selection work.Outcome Equation (Wage):\nModels log wages conditional working. correction term, IMR, included account non-random selection work.Step 1: Naive OLS Observed WagesThis OLS biased includes women chose work.Step 2: Heckman Two-Step EstimationThe variable kids used selection equation. follows good practice: least one variable appear selection equation (serving instrument) help identify model.MLE approach jointly estimates equations, yielding consistent asymptotically efficient estimates.Manual Implementation: Constructing IMREquivalent Method Using glm Manual IMR CalculationComparing ModelsIMR: coefficient IMR statistically significant, suggests selection bias present OLS estimates biased.IMR: coefficient IMR statistically significant, suggests selection bias present OLS estimates biased.\\(\\rho\\): Represents estimated correlation error terms selection outcome equations. significant \\(\\rho\\) implies non-random selection, justifying Heckman correction.\\(\\rho\\): Represents estimated correlation error terms selection outcome equations. significant \\(\\rho\\) implies non-random selection, justifying Heckman correction.case, IMR coefficient statistically different zero, selection bias may serious concern.case, IMR coefficient statistically different zero, selection bias may serious concern.","code":"\nlibrary(sampleSelection)\nlibrary(dplyr)\nlibrary(nnet)\nlibrary(ggplot2)\nlibrary(reshape2)\n\ndata(\"Mroz87\")  # PSID data on married women in 1975\nMroz87 = Mroz87 %>%\n  mutate(kids = kids5 + kids618)  # total number of children\nhead(Mroz87)\n#>   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage\n#> 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288\n#> 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416\n#> 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807\n#> 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417\n#> 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000\n#> 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106\n#>   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll\n#> 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE\n#> 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE\n#> 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE\n#> 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE\n#> 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE\n#> 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE\n#>   kids\n#> 1    1\n#> 2    2\n#> 3    4\n#> 4    3\n#> 5    3\n#> 6    0\nols1 = lm(log(wage) ~ educ + exper + I(exper ^ 2) + city,\n          data = subset(Mroz87, lfp == 1))\nsummary(ols1)\n#> \n#> Call:\n#> lm(formula = log(wage) ~ educ + exper + I(exper^2) + city, data = subset(Mroz87, \n#>     lfp == 1))\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.10084 -0.32453  0.05292  0.36261  2.34806 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.5308476  0.1990253  -2.667  0.00794 ** \n#> educ         0.1057097  0.0143280   7.378 8.58e-13 ***\n#> exper        0.0410584  0.0131963   3.111  0.00199 ** \n#> I(exper^2)  -0.0007973  0.0003938  -2.025  0.04352 *  \n#> city         0.0542225  0.0680903   0.796  0.42629    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6667 on 423 degrees of freedom\n#> Multiple R-squared:  0.1581, Adjusted R-squared:  0.1501 \n#> F-statistic: 19.86 on 4 and 423 DF,  p-value: 5.389e-15\n# Heckman 2-step estimation\nheck1 = heckit(\n    selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ,\n    outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city,\n    data = Mroz87\n)\n\n# Stage 1: Selection equation (probit)\nsummary(heck1$probit)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                  Estimate  Std. error t value   Pr(> t)    \n#> XS(Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> XSage          0.18608901  0.06517476  2.8552  0.004301 ** \n#> XSI(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> XSkids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> XShuswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> XSeduc         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\n\n# Stage 2: Wage equation with selection correction\nsummary(heck1$lm)\n#> \n#> Call:\n#> lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> XO(Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> XOeduc         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> XOexper        0.0419205  0.0136176   3.078  0.00222 ** \n#> XOI(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> XOcity         0.0510492  0.0692414   0.737  0.46137    \n#> imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.7734, Adjusted R-squared:  0.7702 \n#> F-statistic:   240 on 6 and 422 DF,  p-value: < 2.2e-16\n# ML estimation of Heckman selection model\nml1 = selection(\n  selection = lfp ~ age + I(age^2) + kids + huswage + educ,\n  outcome = log(wage) ~ educ + exper + I(exper^2) + city,\n  data = Mroz87\n)\nsummary(ml1)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 3 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -914.0777 \n#> 753 observations (325 censored and 428 observed)\n#> 13 free parameters (df = 740)\n#> Probit selection equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.1484037  1.4109302  -2.940 0.003382 ** \n#> age          0.1842132  0.0658041   2.799 0.005253 ** \n#> I(age^2)    -0.0023925  0.0007664  -3.122 0.001868 ** \n#> kids        -0.1488158  0.0384888  -3.866 0.000120 ***\n#> huswage     -0.0434253  0.0123229  -3.524 0.000451 ***\n#> educ         0.1255639  0.0229229   5.478 5.91e-08 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.5814781  0.3052031  -1.905  0.05714 .  \n#> educ         0.1078481  0.0172998   6.234 7.63e-10 ***\n#> exper        0.0415752  0.0133269   3.120  0.00188 ** \n#> I(exper^2)  -0.0008125  0.0003974  -2.044  0.04129 *  \n#> city         0.0522990  0.0682652   0.766  0.44385    \n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.66326    0.02309  28.729   <2e-16 ***\n#> rho    0.05048    0.23169   0.218    0.828    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n# Step 1: Probit model\nmyprob <- probit(lfp ~ age + I(age^2) + kids + huswage + educ,\n                 data = Mroz87)\nsummary(myprob)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                Estimate  Std. error t value   Pr(> t)    \n#> (Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> age          0.18608901  0.06517476  2.8552  0.004301 ** \n#> I(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> kids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> huswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> educ         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\n\n# Step 2: Compute IMR\nimr <- invMillsRatio(myprob)\nMroz87$IMR1 <- imr$IMR1\n\n# Step 3: Wage regression including IMR\nmanually_est <- lm(log(wage) ~ educ + exper + I(exper^2) + city + IMR1,\n                   data = Mroz87,\n                   subset = (lfp == 1))\nsummary(manually_est)\n#> \n#> Call:\n#> lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, \n#>     data = Mroz87, subset = (lfp == 1))\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> IMR1         0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.1582, Adjusted R-squared:  0.1482 \n#> F-statistic: 15.86 on 5 and 422 DF,  p-value: 2.505e-14\n# Probit via glm\nprobit_selection <- glm(\n  lfp ~ age + I(age^2) + kids + huswage + educ,\n  data = Mroz87,\n  family = binomial(link = 'probit')\n)\n\n# Compute predicted latent index and IMR\nprobit_lp <- -predict(probit_selection)\ninv_mills <- dnorm(probit_lp) / (1 - pnorm(probit_lp))\nMroz87$inv_mills <- inv_mills\n\n# Second stage: Wage regression with correction\nprobit_outcome <- glm(\n  log(wage) ~ educ + exper + I(exper^2) + city + inv_mills,\n  data = Mroz87,\n  subset = (lfp == 1)\n)\nsummary(probit_outcome)\n#> \n#> Call:\n#> glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + \n#>     inv_mills, data = Mroz87, subset = (lfp == 1))\n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143383  0.3768798  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> inv_mills    0.0551179  0.2111918   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.4454809)\n#> \n#>     Null deviance: 223.33  on 427  degrees of freedom\n#> Residual deviance: 187.99  on 422  degrees of freedom\n#> AIC: 876.49\n#> \n#> Number of Fisher Scoring iterations: 2\nlibrary(stargazer)\nlibrary(plm)\nlibrary(sandwich)\n\n# Custom robust SE function\ncse = function(reg) {\n  sqrt(diag(vcovHC(reg, type = \"HC1\")))\n}\n\n# Comparison table\nstargazer(\n  ols1, heck1, ml1, manually_est,\n  se = list(cse(ols1), NULL, NULL, NULL),\n  title = \"Married Women's Wage Regressions: OLS vs Heckman Models\",\n  type = \"text\",\n  df = FALSE,\n  digits = 4,\n  selection.equation = TRUE\n)\n#> \n#> Married Women's Wage Regressions: OLS vs Heckman Models\n#> =========================================================================\n#>                                      Dependent variable:                 \n#>                     -----------------------------------------------------\n#>                     log(wage)                lfp               log(wage) \n#>                        OLS         Heckman        selection       OLS    \n#>                                   selection                              \n#>                        (1)           (2)             (3)          (4)    \n#> -------------------------------------------------------------------------\n#> age                               0.1861***       0.1842***              \n#>                                   (0.0652)        (0.0658)               \n#>                                                                          \n#> I(age2)                          -0.0024***      -0.0024***              \n#>                                   (0.0008)        (0.0008)               \n#>                                                                          \n#> kids                             -0.1496***      -0.1488***              \n#>                                   (0.0383)        (0.0385)               \n#>                                                                          \n#> huswage                          -0.0430***      -0.0434***              \n#>                                   (0.0122)        (0.0123)               \n#>                                                                          \n#> educ                0.1057***     0.1250***       0.1256***    0.1092*** \n#>                      (0.0130)     (0.0228)        (0.0229)      (0.0197) \n#>                                                                          \n#> exper               0.0411***                                  0.0419*** \n#>                      (0.0154)                                   (0.0136) \n#>                                                                          \n#> I(exper2)            -0.0008*                                  -0.0008** \n#>                      (0.0004)                                   (0.0004) \n#>                                                                          \n#> city                  0.0542                                     0.0510  \n#>                      (0.0653)                                   (0.0692) \n#>                                                                          \n#> IMR1                                                             0.0551  \n#>                                                                 (0.2112) \n#>                                                                          \n#> Constant            -0.5308***   -4.1815***      -4.1484***     -0.6143  \n#>                      (0.2032)     (1.4024)        (1.4109)      (0.3769) \n#>                                                                          \n#> -------------------------------------------------------------------------\n#> Observations           428           753             753          428    \n#> R2                    0.1581       0.1582                        0.1582  \n#> Adjusted R2           0.1501       0.1482                        0.1482  \n#> Log Likelihood                                    -914.0777              \n#> rho                                0.0830      0.0505 (0.2317)           \n#> Inverse Mills Ratio            0.0551 (0.2099)                           \n#> Residual Std. Error   0.6667                                     0.6674  \n#> F Statistic         19.8561***                                 15.8635***\n#> =========================================================================\n#> Note:                                         *p<0.1; **p<0.05; ***p<0.01"},{"path":"sec-endogeneity.html","id":"the-role-of-exclusion-restrictions-in-heckmans-model","chapter":"36 Endogeneity","heading":"36.2.4.2 The Role of Exclusion Restrictions in Heckman’s Model","text":"example, adapted sampleSelection, demonstrates identification sample selection model using simulated data.compare two cases:One exclusion restriction present (.e., selection outcome equations use different regressors).One exclusion restriction present (.e., selection outcome equations use different regressors).One regressor used equations.One regressor used equations.Case 1: Exclusion RestrictionKey Observations:variables xs xo independent, fulfilling exclusion restriction.variables xs xo independent, fulfilling exclusion restriction.outcome equation identified non-linearity model also presence regressor selection equation absent outcome equation.outcome equation identified non-linearity model also presence regressor selection equation absent outcome equation.mirrors realistic scenarios applied business settings, :Participation labor force driven family geographic factors (xs), wages depend skills education (xo).Participation labor force driven family geographic factors (xs), wages depend skills education (xo).Loan application driven personal risk preferences, interest rates depend credit score income.Loan application driven personal risk preferences, interest rates depend credit score income.Case 2: Without Exclusion RestrictionWhat changes?estimates remain approximately unbiased model still identified functional form (non-linear selection mechanism).estimates remain approximately unbiased model still identified functional form (non-linear selection mechanism).However, standard errors substantially larger, leading less precise inference.However, standard errors substantially larger, leading less precise inference.Exclusion Restrictions MatterThe exclusion restriction improves identification introducing additional variation affects selection outcome. common recommendation empirical work:first case, xs helps explain selected, xo helps explain outcome, enabling precise estimates.first case, xs helps explain selected, xo helps explain outcome, enabling precise estimates.second case, variation comes xs, meaning model relies solely distributional assumptions (e.g., joint normality errors) identification.second case, variation comes xs, meaning model relies solely distributional assumptions (e.g., joint normality errors) identification.Best Practice (applied researchers):Always try include least one variable selection equation appear outcome equation. helps identify model improves estimation precision.","code":"\nset.seed(0)\nlibrary(sampleSelection)\nlibrary(mvtnorm)\n\n\n# Simulate bivariate normal error terms with correlation -0.7\neps <-\n    rmvnorm(500, mean = c(0, 0), sigma = matrix(c(1,-0.7,-0.7, 1), 2, 2))\n\n# Independent explanatory variable for selection equation\nxs <- runif(500)\n\n# Selection: Probit model (latent utility model)\nys_latent <- xs + eps[, 1]\nys <- ys_latent > 0  # observed participation indicator (TRUE/FALSE)\n\n# Independent explanatory variable for outcome equation\nxo <- runif(500)\n\n# Latent outcome variable\nyo_latent <- xo + eps[, 2]\n\n# Observed outcome: only when selected (ys == TRUE)\nyo <- yo_latent * ys\n\n# Estimate Heckman's selection model\nmodel_with_exclusion <-\n    selection(selection = ys ~ xs, outcome = yo ~ xo)\nsummary(model_with_exclusion)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 5 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -712.3163 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  \n#> xs            1.3377     0.2014   6.642 8.18e-11 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.0002265  0.1294178  -0.002    0.999    \n#> xo           0.7299070  0.1635925   4.462 1.01e-05 ***\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma   0.9190     0.0574  16.009  < 2e-16 ***\n#> rho    -0.5392     0.1521  -3.544 0.000431 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n# Now use the same regressor (xs) in both equations\nyo_latent2 <- xs + eps[, 2]\nyo2 <- yo_latent2 * ys\n\n# Re-estimate model without exclusion restriction\nmodel_no_exclusion <-\n    selection(selection = ys ~ xs, outcome = yo2 ~ xs)\nsummary(model_no_exclusion)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 14 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -712.8298 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  \n#> xs            1.2907     0.2085   6.191 1.25e-09 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  -0.5499     0.5644  -0.974  0.33038   \n#> xs            1.3987     0.4482   3.120  0.00191 **\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.85091    0.05352  15.899   <2e-16 ***\n#> rho   -0.13226    0.72684  -0.182    0.856    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"sec-endogeneity.html","id":"tobit-5-switching-regression-model","chapter":"36 Endogeneity","heading":"36.2.5 Tobit-5: Switching Regression Model","text":"Tobit-5 model, also known Switching Regression Model, generalizes Heckman’s sample selection model allow two separate outcome equations:One model participantsA different model non-participantsAssumptions:selection process determines outcome equation observed.least one variable selection equation appear outcome equations (.e., exclusion restriction), improves identification.errors three equations (selection, outcome1, outcome2) assumed jointly normally distributed.model especially relevant selection endogenous groups (selected unselected) distinct data-generating processes.","code":""},{"path":"sec-endogeneity.html","id":"simulated-example-with-exclusion-restriction","chapter":"36 Endogeneity","heading":"36.2.5.1 Simulated Example: With Exclusion Restriction","text":"estimated coefficients close true values (intercept = 0, slope = 1), model converges well due correct specification valid exclusion restriction.","code":"\nset.seed(0)\nlibrary(sampleSelection)\nlibrary(mvtnorm)\n\n# Define a 3x3 covariance matrix with positive correlations\nvc <- diag(3)\nvc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)\nvc[upper.tri(vc)] <- t(vc)[upper.tri(vc)]\n\n# Generate multivariate normal error terms\neps <- rmvnorm(500, mean = c(0, 0, 0), sigma = vc)\n\n# Selection equation regressor (xs), uniformly distributed\nxs <- runif(500)\n\n# Binary selection indicator: ys = 1 if selected\nys <- xs + eps[, 1] > 0\n\n# Separate regressors for two regimes (fulfilling exclusion restriction)\nxo1 <- runif(500)\nyo1 <- xo1 + eps[, 2]  # Outcome for selected group\n\nxo2 <- runif(500)\nyo2 <- xo2 + eps[, 3]  # Outcome for non-selected group\n\n# Fit switching regression model\nmodel_switch <- selection(ys ~ xs, list(yo1 ~ xo1, yo2 ~ xo2))\nsummary(model_switch)\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 11 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -895.8201 \n#> 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)\n#> 10 free parameters (df = 490)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1550     0.1051  -1.474    0.141    \n#> xs            1.1408     0.1785   6.390 3.86e-10 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02708    0.16395   0.165    0.869    \n#> xo1          0.83959    0.14968   5.609  3.4e-08 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.1583     0.1885   0.840    0.401    \n#> xo2           0.8375     0.1707   4.908 1.26e-06 ***\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.93191    0.09211  10.118   <2e-16 ***\n#> sigma2  0.90697    0.04434  20.455   <2e-16 ***\n#> rho1    0.88988    0.05353  16.623   <2e-16 ***\n#> rho2    0.17695    0.33139   0.534    0.594    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"sec-endogeneity.html","id":"example-functional-form-misspecification","chapter":"36 Endogeneity","heading":"36.2.5.2 Example: Functional Form Misspecification","text":"demonstrate non-normal errors skewed distributions affect model, consider following:Even though exclusion restriction preserved, non-normal errors introduce bias intercept estimates, convergence less reliable. illustrates functional form misspecification (.e., deviations assumed distributional forms) impacts performance.","code":"\nset.seed(0)\neps <- rmvnorm(1000, mean = rep(0, 3), sigma = vc)\n\n# Induce skewness: squared errors minus 1 to ensure mean zero\neps <- eps^2 - 1\n\n# Generate xs on a skewed interval [-1, 0]\nxs <- runif(1000, -1, 0)\nys <- xs + eps[, 1] > 0\n\nxo1 <- runif(1000)\nyo1 <- xo1 + eps[, 2]\n\nxo2 <- runif(1000)\nyo2 <- xo2 + eps[, 3]\n\n# Attempt model estimation\nsummary(selection(ys ~ xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim = 20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 12 iterations\n#> Return code 3: Last step could not find a value above the current.\n#> Boundary of parameter space?  \n#> Consider switching to a more robust optimisation method temporarily.\n#> Log-Likelihood: -1695.102 \n#> 1000 observations: 782 selection 1 (FALSE) and 218 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.660315   0.082477  -8.006  3.3e-15 ***\n#> xs           0.007167   0.088630   0.081    0.936    \n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.31351    0.04868   -6.44 1.86e-10 ***\n#> xo1          1.03862    0.08049   12.90  < 2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -2.6835     0.2043 -13.132  < 2e-16 ***\n#> xo2           1.0230     0.1309   7.814 1.41e-14 ***\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.70172    0.02000   35.09   <2e-16 ***\n#> sigma2  2.49651        NaN     NaN      NaN    \n#> rho1    0.51564    0.04216   12.23   <2e-16 ***\n#> rho2    1.00000        NaN     NaN      NaN    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"sec-endogeneity.html","id":"example-no-exclusion-restriction","chapter":"36 Endogeneity","heading":"36.2.5.3 Example: No Exclusion Restriction","text":"remove exclusion restriction using regressor (xs) outcome equations.model may fail converge produce biased estimates, especially intercepts. Even converge, reliability inference questionable due weak identification using regressor across equations.Notes Estimation ConvergenceThe log-likelihood function switching regression models globally concave, estimation process may:Fail convergeFail convergeConverge local maximumConverge local maximumPractical Tips:Try different starting values random seedsTry different starting values random seedsUse alternative maximization algorithms (e.g., optim control)Use alternative maximization algorithms (e.g., optim control)Consider rescaling variables centering predictorsConsider rescaling variables centering predictorsRefer Non-Linear Regression advanced diagnosticsRefer Non-Linear Regression advanced diagnosticsModel Comparison Summary","code":"\nset.seed(0)\nxs <- runif(1000, -1, 1)\nys <- xs + eps[, 1] > 0\n\nyo1 <- xs + eps[, 2]\nyo2 <- xs + eps[, 3]\n\n# Fit switching regression without exclusion restriction\nsummary(tmp <-\n            selection(ys ~ xs, list(yo1 ~ xs, yo2 ~ xs), iterlim = 20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 16 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -1879.552 \n#> 1000 observations: 615 selection 1 (FALSE) and 385 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.33425    0.04280   -7.81 1.46e-14 ***\n#> xs           0.94762    0.07763   12.21  < 2e-16 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.49592    0.06800  -7.293 6.19e-13 ***\n#> xs           0.84530    0.06789  12.450  < 2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   0.3861     0.4967   0.777   0.4371  \n#> xs            0.6254     0.3322   1.882   0.0601 .\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.61693    0.02054  30.029   <2e-16 ***\n#> sigma2  1.59059    0.05745  27.687   <2e-16 ***\n#> rho1    0.19981    0.15863   1.260    0.208    \n#> rho2   -0.01259    0.29339  -0.043    0.966    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"sec-endogeneity.html","id":"sec-pattern-mixture-models","chapter":"36 Endogeneity","heading":"36.2.6 Pattern-Mixture Models","text":"context endogenous sample selection, one central challenges modeling joint distribution outcome selection mechanism data Missing Random (MNAR). framework, probability outcome observed may depend unobserved values outcome, making missingness mechanism nonignorable.Previously, discussed selection model approach (e.g., Heckman’s Tobit-2 model), factorizes joint distribution :\\[\n\\mathbb{P}(Y, R) = \\mathbb{P}(Y) \\cdot \\mathbb{P}(R \\mid Y),\n\\]:\\(Y\\) outcome interest,\\(Y\\) outcome interest,\\(R\\) response indicator (\\(R = 1\\) \\(Y\\) observed, \\(R = 0\\) otherwise).\\(R\\) response indicator (\\(R = 1\\) \\(Y\\) observed, \\(R = 0\\) otherwise).approach models selection process explicitly via \\(\\mathbb{P}(R \\mid Y)\\), often using parametric model probit.However, pattern-mixture model (PMM) offers alternative equally valid factorization:\\[\n\\mathbb{P}(Y, R) = \\mathbb{P}(Y \\mid R) \\cdot \\mathbb{P}(R),\n\\]decomposes joint distribution conditioning response pattern. approach particularly advantageous selection mechanism complex, interest lies modeling outcome distribution varies across response strata.","code":""},{"path":"sec-endogeneity.html","id":"definition-of-the-pattern-mixture-model","chapter":"36 Endogeneity","heading":"36.2.6.1 Definition of the Pattern-Mixture Model","text":"Let \\((Y_i, R_i)\\) \\(= 1, \\dots, n\\) denote observed data, :\\(Y_i \\\\mathbb{R}^p\\) multivariate outcome interest,\\(R_i \\\\{0,1\\}^p\\) missingness pattern indicator vector, \\(R_{ij} = 1\\) indicates \\(Y_{ij}\\) observed.Define \\(\\mathcal{R}\\) finite set possible response patterns. pattern \\(r \\\\mathcal{R}\\), partition outcome vector \\(Y\\) :\\(Y_{(r)}\\): observed components \\(Y\\) pattern \\(r\\),\\(Y_{(\\bar{r})}\\): missing components \\(Y\\) pattern \\(r\\).joint distribution can factorized according pattern-mixture model :\\[\nf(Y, R) = f(Y \\mid R = r) \\cdot \\mathbb{P}(R = r), \\quad r \\\\mathcal{R}.\n\\]marginal distribution \\(Y\\) obtained summing response patterns:\\[\nf(Y) = \\sum_{r \\\\mathcal{R}} f(Y \\mid R = r) \\cdot \\mathbb{P}(R = r).\n\\]simplest case binary response patterns (complete responders \\(R=1\\) complete nonresponders \\(R=0\\)), reduces :\\[\n\\mathbb{P}(Y) = \\mathbb{P}(Y \\mid R = 1) \\cdot \\mathbb{P}(R = 1) + \\mathbb{P}(Y \\mid R = 0) \\cdot \\mathbb{P}(R = 0).\n\\]Thus, overall distribution \\(Y\\) explicitly treated mixture distributions responders nonresponders. \\(\\mathbb{P}(Y \\mid R = 1)\\) can directly estimated observed data, \\(\\mathbb{P}(Y \\mid R = 0)\\) identified data alone, necessitating additional assumptions sensitivity parameters estimation.Common approaches addressing non-identifiability involve defining plausible deviations \\(\\mathbb{P}(Y \\mid R = 1)\\) using specific sensitivity parameters :Shift Bias: Adjusting imputed values constant \\(\\delta\\) reflect systematic differences nonresponders.Scale Bias: Scaling imputed values account differing variability.Shape Bias: Reweighting imputations, example, using methods like Selection-Indicator Reweighting (SIR), capture distributional shape differences.discussion, primary focus placed shift parameter \\(\\delta\\), hypothesizing nonresponse shifts mean \\(Y\\) \\(\\delta\\) units.practice, since observed components \\(Y_{(r)}\\) available individuals missing data, modeling full conditional distribution \\(f(Y \\mid R = r)\\) requires extrapolating observed unobserved components.","code":""},{"path":"sec-endogeneity.html","id":"modeling-strategy-and-identifiability","chapter":"36 Endogeneity","heading":"36.2.6.2 Modeling Strategy and Identifiability","text":"Suppose full-data conditional distribution parameterized pattern-specific parameters \\(\\theta_r\\) response pattern \\(r \\\\mathcal{R}\\):\\[\nf(Y \\mid R = r; \\theta_r), \\quad \\text{} \\theta = \\{\\theta_r : r \\\\mathcal{R}\\}.\n\\], marginal distribution \\(Y\\) can expressed :\\[\nf(Y; \\theta, \\psi) = \\sum_{r \\\\mathcal{R}} f(Y \\mid R = r; \\theta_r) \\cdot \\mathbb{P}(R = r; \\psi),\n\\]\\(\\psi\\) parameterizes distribution response patterns.Important points identifiability include:model inherently non-identifiable without explicit assumptions regarding distribution missing components \\(Y_{(\\bar{r})}\\).Estimating \\(f(Y_{(\\bar{r})} \\mid Y_{(r)}, R = r)\\) requires external information, expert knowledge, assumptions encapsulated sensitivity parameters.conditional density response pattern can decomposed :\\[\nf(Y \\mid R = r) = f(Y_{(r)} \\mid R = r) \\cdot f(Y_{(\\bar{r})} \\mid Y_{(r)}, R = r).\n\\]first term \\(f(Y_{(r)} \\mid R = r)\\) estimable directly observed data.second term \\(f(Y_{(\\bar{r})} \\mid Y_{(r)}, R = r)\\) identified observed data alone must rely assumptions additional sensitivity analysis.","code":""},{"path":"sec-endogeneity.html","id":"location-shift-models","chapter":"36 Endogeneity","heading":"36.2.6.3 Location Shift Models","text":"widely used strategy PMMs introduce location shift unobserved outcomes. Specifically, assume :\\[\nY_{(\\bar{r})} \\mid Y_{(r)}, R = r \\sim \\mathcal{L}(Y_{(\\bar{r})} \\mid Y_{(r)}, R = r = r^*) + \\delta_r,\n\\]:\\(r^*\\) denotes fully observed (reference) pattern (e.g., \\(R = 1^p\\)),\\(r^*\\) denotes fully observed (reference) pattern (e.g., \\(R = 1^p\\)),\\(\\delta_r\\) vector sensitivity parameters quantify mean shift missing outcomes pattern \\(r\\).\\(\\delta_r\\) vector sensitivity parameters quantify mean shift missing outcomes pattern \\(r\\).formally, \\(r \\\\mathcal{R}\\) unobserved component \\(j \\\\bar{r}\\), specify:\\[\n\\mathbb{E}[Y_j \\mid Y_{(r)}, R = r] = \\mathbb{E}[Y_j \\mid Y_{(r)}, R = r^*] + \\delta_{rj}.\n\\]framework:Setting \\(\\delta_r = 0\\) corresponds MAR assumption (.e., ignorability).Nonzero \\(\\delta_r\\) introduces controlled deviations MAR enables sensitivity analysis.","code":""},{"path":"sec-endogeneity.html","id":"sensitivity-analysis-in-pattern-mixture-models","chapter":"36 Endogeneity","heading":"36.2.6.4 Sensitivity Analysis in Pattern-Mixture Models","text":"evaluate robustness inferences missing data mechanism, perform sensitivity analysis varying \\(\\delta_r\\) plausible ranges.Bias EstimationLet \\(\\mu = \\mathbb{E}[Y]\\) target estimand. pattern-mixture decomposition:\\[\n\\mu = \\sum_{r \\\\mathcal{R}} \\mathbb{E}[Y \\mid R = r] \\cdot \\mathbb{P}(R = r),\n\\]shift model:\\[\n\\mathbb{E}[Y \\mid R = r] = \\tilde{\\mu}_r + \\delta_r^{*},\n\\]:\\(\\tilde{\\mu}_r\\) mean computed using observed data (e.g., via imputation MAR),\\(\\tilde{\\mu}_r\\) mean computed using observed data (e.g., via imputation MAR),\\(\\delta_r^{*}\\) vector entries equal mean shifts \\(\\delta_{rj}\\) missing components 0 observed components.\\(\\delta_r^{*}\\) vector entries equal mean shifts \\(\\delta_{rj}\\) missing components 0 observed components.Therefore, overall bias induced nonzero shifts :\\[\n\\Delta = \\sum_{r \\\\mathcal{R}} \\delta_r^{*} \\cdot \\mathbb{P}(R = r).\n\\]highlights overall expectation depends linearly sensitivity parameters associated pattern probabilities.Practical RecommendationsUse subject-matter knowledge specify plausible ranges \\(\\delta_r\\).Consider standard increments (e.g., \\(\\pm 0.2\\) SD units) domain-specific metrics (e.g., \\(\\pm 5\\) P/E valuation).dataset large number missingness patterns small (e.g., monotone dropout), detailed pattern-specific modeling feasible.","code":""},{"path":"sec-endogeneity.html","id":"generalization-to-multivariate-and-longitudinal-data","chapter":"36 Endogeneity","heading":"36.2.6.5 Generalization to Multivariate and Longitudinal Data","text":"Suppose \\(Y = (Y_1, \\dots, Y_T)\\) longitudinal outcome. Let \\(D \\\\{1, \\dots, T+1\\}\\) denote dropout time, \\(D = t\\) implies observation time \\(t-1\\).pattern-mixture factorization becomes:\\[\nf(Y, D) = f(Y \\mid D = t) \\cdot \\mathbb{P}(D = t), \\quad t = 2, \\dots, T+1.\n\\]Assume parametric model:\\[\nY_j \\mid D = t \\sim \\mathcal{N}(\\beta_{0t} + \\beta_{1t} t_j, \\sigma_t^2), \\quad \\text{} j = 1, \\dots, T.\n\\]overall mean trajectory becomes:\\[\n\\mathbb{E}[Y_j] = \\sum_{t=2}^{T+1} (\\beta_{0t} + \\beta_{1t} t_j) \\cdot \\mathbb{P}(D = t).\n\\]model allows dropout-pattern-specific trajectories can flexibly account deviations distributional shape due dropout.","code":""},{"path":"sec-endogeneity.html","id":"comparison-with-selection-models","chapter":"36 Endogeneity","heading":"36.2.6.6 Comparison with Selection Models","text":"Simulate Longitudinal Dropout DataWe simulate 3-time-point longitudinal outcome, dropout depends unobserved future outcomes, .e., MNAR.Fit Pattern-Mixture Model Delta AdjustmentWe model \\(Y_3\\) function \\(Y_1\\) \\(Y_2\\), using observed cases (complete cases), create multiple imputed datasets various shift parameters \\(\\delta\\) represent different MNAR scenarios.Estimate Full-Data Mean Trajectory DeltaWe now estimate mean \\(Y_3\\) full outcome trajectory across delta values.Visualization: Sensitivity AnalysisWe visualize mean trajectory changes across different sensitivity parameters \\(\\delta\\).Sensitivity Table ReportingThis table quantifies change \\(\\mu_{Y3}\\) across sensitivity scenarios.Table 36.1: Sensitivity Estimated Mean Y3 Delta AdjustmentsInterpretation DiscussionThe estimated mean \\(Y_3\\) decreases linearly increasingly negative \\(\\delta\\), expected.estimated mean \\(Y_3\\) decreases linearly increasingly negative \\(\\delta\\), expected.illustrates non-identifiability distribution missing data without unverifiable assumptions.illustrates non-identifiability distribution missing data without unverifiable assumptions.results provide tipping point analysis: \\(\\delta\\) inference mean (treatment effect, causal study) substantially change?results provide tipping point analysis: \\(\\delta\\) inference mean (treatment effect, causal study) substantially change?","code":"\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(mvtnorm)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(patchwork)\nset.seed(123)\n\n# Parameters\nn <- 100\ntime <- c(0, 1, 2)\nbeta <- c(100, 5)  # intercept and slope\nsigma <- 5\ndropout_bias <- -10  # MNAR: lower future Y -> more dropout\n\n# Simulate full data (Y1, Y2, Y3)\nY <- matrix(NA, nrow = n, ncol = 3)\nfor (i in 1:3) {\n    Y[, i] <- beta[1] + beta[2] * time[i] + rnorm(n, 0, sigma)\n}\n\n# Dropout mechanism: higher chance to drop if Y3 is low\nprob_dropout <- plogis(-0.5 + 0.2 * (dropout_bias - Y[, 3]))\ndrop <- rbinom(n, 1, prob_dropout)\n\n# Define dropout time: if drop == 1, then Y3 is missing\nR <- matrix(1, nrow = n, ncol = 3)\nR[drop == 1, 3] <- 0\n\n# Observed data\nY_obs <- Y\nY_obs[R == 0] <- NA\n\n# Combine into a data frame\ndf <-\n    data.frame(\n        id = 1:n,\n        Y1 = Y_obs[, 1],\n        Y2 = Y_obs[, 2],\n        Y3 = Y_obs[, 3],\n        R3 = R[, 3]\n    )\n# Base linear model for Y3 ~ Y1 + Y2\nmodel_cc <- lm(Y3 ~ Y1 + Y2, data = df, subset = R3 == 1)\n\n# Predict for missing Y3s\npreds <- predict(model_cc, newdata = df)\n\n# Sensitivity values for delta\ndelta_seq <- c(0, -2.5, -5, -7.5, -10)  # in units of Y3\n\n# Perform delta adjustment\nimputed_dfs <- lapply(delta_seq, function(delta) {\n  df_new <- df\n  df_new$Y3_imp <- ifelse(is.na(df$Y3),\n                          preds + delta,\n                          df$Y3)\n  df_new$delta <- delta\n  return(df_new)\n})\n\n# Stack all results\ndf_imp_all <- bind_rows(imputed_dfs)\nestimates <- df_imp_all %>%\n    group_by(delta) %>%\n    summarise(\n        mu_Y1 = mean(Y1, na.rm = TRUE),\n        mu_Y2 = mean(Y2, na.rm = TRUE),\n        mu_Y3 = mean(Y3_imp),\n        .groups = \"drop\"\n    ) %>%\n    pivot_longer(cols = starts_with(\"mu\"),\n                 names_to = \"Time\",\n                 values_to = \"Mean\") %>%\n    mutate(Time = factor(\n        Time,\n        levels = c(\"mu_Y1\", \"mu_Y2\", \"mu_Y3\"),\n        labels = c(\"Time 1\", \"Time 2\", \"Time 3\")\n    ))\nggplot(estimates, aes(\n    x = Time,\n    y = Mean,\n    color = as.factor(delta),\n    group = delta\n)) +\n    geom_line(linewidth = 1) +\n    geom_point(size = 2) +\n    scale_color_viridis_d(name = expression(delta)) +\n    labs(\n        title = \"Pattern-Mixture Model Sensitivity Analysis\",\n        y = \"Mean of Y_t\",\n        x = \"Time\"\n    ) +\n    causalverse::ama_theme()\ndf_summary <- df_imp_all %>%\n    group_by(delta) %>%\n    summarise(\n        Mean_Y3 = mean(Y3_imp),\n        SD_Y3 = sd(Y3_imp),\n        N_Missing = sum(is.na(Y3)),\n        N_Observed = sum(!is.na(Y3))\n    )\n\nknitr::kable(df_summary, digits = 2,\n             caption = \"Sensitivity of Estimated Mean of Y3 to Delta Adjustments\")"},{"path":"other-biases.html","id":"other-biases","chapter":"37 Other Biases","heading":"37 Other Biases","text":"econometrics, main objective often uncover causal relationships. However, coefficient estimates can affected various biases. ’s list common biases can affect coefficient estimates:’ve covered far (see Linear Regression Endogeneity):Endogeneity Bias: Occurs error term correlated independent variable. can due :\nSimultaneity (Reverse Causality): dependent variable simultaneously affects independent variable. Happens dependent variable causes changes independent variable, leading two-way causation.\nOmitted variables. Arises variable affects dependent variable correlated independent variable left regression.\nMeasurement error independent variable. Bias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.\nEndogeneity Bias: Occurs error term correlated independent variable. can due :Simultaneity (Reverse Causality): dependent variable simultaneously affects independent variable. Happens dependent variable causes changes independent variable, leading two-way causation.Simultaneity (Reverse Causality): dependent variable simultaneously affects independent variable. Happens dependent variable causes changes independent variable, leading two-way causation.Omitted variables. Arises variable affects dependent variable correlated independent variable left regression.Omitted variables. Arises variable affects dependent variable correlated independent variable left regression.Measurement error independent variable. Bias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.Measurement error independent variable. Bias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.Sample Selection Bias: Arises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.Sample Selection Bias: Arises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.Multicollinearity: bias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.Multicollinearity: bias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.Specification Errors: Arise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.Specification Errors: Arise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.Autocorrelation (Serial Correlation): Occurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.Autocorrelation (Serial Correlation): Occurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.Heteroskedasticity: Occurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.Heteroskedasticity: Occurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.Aggregation Bias: Introduced data aggregated, analysis conducted aggregate level rather individual level.Aggregation Bias: Introduced data aggregated, analysis conducted aggregate level rather individual level.Survivorship Bias (much related Sample Selection): Arises sample includes “survivors” “passed” certain threshold. Common finance funds firms “survive” analyzed.Survivorship Bias (much related Sample Selection): Arises sample includes “survivors” “passed” certain threshold. Common finance funds firms “survive” analyzed.Publication Bias: bias econometric estimation per se, relevant context empirical studies. refers tendency journals publish significant positive results, leading overrepresentation results literature.Publication Bias: bias econometric estimation per se, relevant context empirical studies. refers tendency journals publish significant positive results, leading overrepresentation results literature.","code":""},{"path":"other-biases.html","id":"sec-aggregation-bias","chapter":"37 Other Biases","heading":"37.1 Aggregation Bias","text":"Aggregation bias, also known ecological fallacy, refers error introduced data aggregated analysis conducted aggregate level, rather individual level. can especially problematic econometrics, analysts often concerned understanding individual behavior.relationship variables different aggregate level individual level, aggregation bias can result. bias arises inferences individual behaviors made based aggregate data.Example: Suppose data individuals’ incomes personal consumption. individual level, ’s possible income rises, consumption also rises. However, aggregate data , say, neighborhood level, neighborhoods diverse income levels might similar average consumption due unobserved factors.Step 1: Create individual level dataThis show significant positive relationship income consumption.Step 2: Aggregate data ‘neighborhood’ levelIf aggregation bias present, coefficient income aggregate regression might different coefficient individual regression, even individual relationship significant strong.plots, can see relationship individual level, neighborhood colored differently first plot. second plot shows aggregate data, point now represents whole neighborhood.Direction Bias: direction aggregation bias isn’t predetermined. depends underlying relationship data distribution. cases, aggregation might attenuate (reduce) relationship, cases, might exaggerate .Relation Biases: Aggregation bias closely related several biases econometrics:Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.[Measurement Error]: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.[Measurement Error]: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.","code":"\nset.seed(123)\n\n# Generate data for 1000 individuals\nn <- 1000\n\nincome <- rnorm(n, mean = 50, sd = 10)\nconsumption <- 0.5 * income + rnorm(n, mean = 0, sd = 5)\n\n# Individual level regression\nindividual_lm <- lm(consumption ~ income)\nsummary(individual_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -15.1394  -3.4572   0.0213   3.5436  16.4557 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1.99596    0.82085  -2.432   0.0152 *  \n#> income       0.54402    0.01605  33.888   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.032 on 998 degrees of freedom\n#> Multiple R-squared:  0.535,  Adjusted R-squared:  0.5346 \n#> F-statistic:  1148 on 1 and 998 DF,  p-value: < 2.2e-16\n# Assume 100 neighborhoods with 10 individuals each\nn_neighborhoods <- 100\n\ndf <- data.frame(income, consumption)\ndf$neighborhood <- rep(1:n_neighborhoods, each = n / n_neighborhoods)\n\naggregate_data <- aggregate(. ~ neighborhood, data = df, FUN = mean)\n\n# Aggregate level regression\naggregate_lm <- lm(consumption ~ income, data = aggregate_data)\nsummary(aggregate_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income, data = aggregate_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.4517 -0.9322 -0.0826  1.0556  3.5728 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.94338    2.60699  -1.896   0.0609 .  \n#> income       0.60278    0.05188  11.618   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.54 on 98 degrees of freedom\n#> Multiple R-squared:  0.5794, Adjusted R-squared:  0.5751 \n#> F-statistic:   135 on 1 and 98 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Individual scatterplot\np1 <- ggplot(df, aes(x = income, y = consumption)) +\n    geom_point(aes(color = neighborhood), alpha = 0.6) +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Individual Level Data\") +\n    causalverse::ama_theme()\n\n# Aggregate scatterplot\np2 <- ggplot(aggregate_data, aes(x = income, y = consumption)) +\n    geom_point(color = \"red\") +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Aggregate Level Data\") +\n    causalverse::ama_theme()\n\n# print(p1)\n# print(p2)\n\ngridExtra::grid.arrange(grobs = list(p1, p2), ncol = 2)"},{"path":"other-biases.html","id":"simpsons-paradox-1","chapter":"37 Other Biases","heading":"37.1.1 Simpson’s Paradox","text":"Simpson’s Paradox, also known Yule-Simpson effect, phenomenon probability statistics trend appears different groups data disappears reverses groups combined. ’s striking example aggregated data can sometimes provide misleading representation actual situation.Illustration Simpson’s Paradox:Consider hypothetical scenario involving two hospitals: Hospital Hospital B. want analyze success rates treatments hospitals. break data severity cases (.e., minor cases vs. major cases):Hospital :\nMinor cases: 95% success rate\nMajor cases: 80% success rate\nHospital :Minor cases: 95% success rateMinor cases: 95% success rateMajor cases: 80% success rateMajor cases: 80% success rateHospital B:\nMinor cases: 90% success rate\nMajor cases: 85% success rate\nHospital B:Minor cases: 90% success rateMinor cases: 90% success rateMajor cases: 85% success rateMajor cases: 85% success rateFrom breakdown, Hospital appears better treating minor major cases since higher success rate categories.However, let’s consider overall success rates without considering case severity:Hospital : 83% overall success rateHospital : 83% overall success rateHospital B: 86% overall success rateHospital B: 86% overall success rateSuddenly, Hospital B seems better overall. surprising reversal happens two hospitals might handle different proportions minor major cases. example, Hospital treats many major cases (lower success rates) Hospital B, can drag overall success rate.Causes:Simpson’s Paradox can arise due :lurking confounding variable wasn’t initially considered (example, severity medical cases).lurking confounding variable wasn’t initially considered (example, severity medical cases).Different group sizes, one group might much larger , influencing aggregate results.Different group sizes, one group might much larger , influencing aggregate results.Implications:Simpson’s Paradox highlights dangers interpreting aggregated data without considering potential underlying sub-group structures. underscores importance disaggregating data aware context ’s analyzed.Relation Aggregation BiasIn extreme case, aggregation bias can reverse coefficient sign relationship interest (.e., Simpson’s Paradox).Example: Suppose studying effect new study technique student grades. two groups students: used new technique (treatment = 1) (treatment = 0). want see using new study technique related higher grades.Let’s assume grades influenced starting ability students. Perhaps sample, many high-ability students didn’t use new technique (felt didn’t need ), many low-ability students .’s setup:High-ability students tend high grades regardless technique.High-ability students tend high grades regardless technique.new technique positive effect grades, masked fact many low-ability students use .new technique positive effect grades, masked fact many low-ability students use .simulation:overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.classic example Simpson’s Paradox: within ability group, technique appears beneficial, data aggregated, effect seems negative distribution technique across ability groups.","code":"\nset.seed(123)\n\n# Generate data for 1000 students\nn <- 1000\n\n# 500 students are of high ability, 500 of low ability\nability <- c(rep(\"high\", 500), rep(\"low\", 500))\n\n# High ability students are less likely to use the technique\ntreatment <-\n  ifelse(ability == \"high\", rbinom(500, 1, 0.2), rbinom(500, 1, 0.8))\n\n# Grades are influenced by ability and treatment (new technique),\n# but the treatment has opposite effects based on ability.\ngrades <-\n  ifelse(\n    ability == \"high\",\n    rnorm(500, mean = 85, sd = 5) + treatment * -3,\n    rnorm(500, mean = 60, sd = 5) + treatment * 5\n  )\n\ndf <- data.frame(ability, treatment, grades)\n\n# Regression without considering ability\noverall_lm <- lm(grades ~ factor(treatment), data = df)\nsummary(overall_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -33.490  -4.729   0.986   6.368  25.607 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         80.0133     0.4373   183.0   <2e-16 ***\n#> factor(treatment)1 -11.7461     0.6248   -18.8   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.877 on 998 degrees of freedom\n#> Multiple R-squared:  0.2615, Adjusted R-squared:  0.2608 \n#> F-statistic: 353.5 on 1 and 998 DF,  p-value: < 2.2e-16\n\n# Regression within ability groups\nhigh_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"high\",])\nlow_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"low\",])\nsummary(high_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"high\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -14.2156  -3.4813   0.1186   3.4952  13.2919 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         85.1667     0.2504 340.088  < 2e-16 ***\n#> factor(treatment)1  -3.9489     0.5776  -6.837 2.37e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.046 on 498 degrees of freedom\n#> Multiple R-squared:  0.08581,    Adjusted R-squared:  0.08398 \n#> F-statistic: 46.75 on 1 and 498 DF,  p-value: 2.373e-11\nsummary(low_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"low\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.3717  -3.5413   0.1097   3.3531  17.0568 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         59.8950     0.4871 122.956   <2e-16 ***\n#> factor(treatment)1   5.2979     0.5474   9.679   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.968 on 498 degrees of freedom\n#> Multiple R-squared:  0.1583, Adjusted R-squared:  0.1566 \n#> F-statistic: 93.68 on 1 and 498 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Scatterplot for overall data\np1 <-\n  ggplot(df, aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Overall Effect of Study Technique on Grades\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for high-ability students\np2 <-\n  ggplot(df[df$ability == \"high\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (High Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for low-ability students\np3 <-\n  ggplot(df[df$ability == \"low\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (Low Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# print(p1)\n# print(p2)\n# print(p3)\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), ncol = 1)"},{"path":"other-biases.html","id":"sec-contamination-bias","chapter":"37 Other Biases","heading":"37.2 Contamination Bias","text":"Goldsmith-Pinkham, Hull, Kolesár (2022) show regressions multiple treatments flexible controls often fail estimate convex averages heterogeneous treatment effects, resulting contamination non-convex averages treatments’ effects.3 estimation methods avoid bias find significant contamination bias observational studies, experimental studies showing less due lower variability propensity scores.","code":""},{"path":"other-biases.html","id":"sec-survivorship-bias","chapter":"37 Other Biases","heading":"37.3 Survivorship Bias","text":"Survivorship bias refers logical error concentrating entities made past selection process overlooking didn’t, typically lack visibility. can skew results lead overly optimistic conclusions.Example: analyze success companies based ones still business today, ’d miss insights failed. give distorted view makes successful company, wouldn’t account attributes didn’t succeed.Relation Biases:Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Using histogram visualize distribution earnings, highlighting “survivors”.plot, “True Avg” might lower “Survivor Avg”, indicating looking survivors, overestimate average earnings.Remedies:Awareness: Recognizing potential survivorship bias first step.Awareness: Recognizing potential survivorship bias first step.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.Sensitivity Analysis: Test sensitive results assumptions non-survivors.Sensitivity Analysis: Test sensitive results assumptions non-survivors.","code":"\nset.seed(42)\n\n# Generating data for 100 companies\nn <- 100\n\n# Randomly generate earnings; assume true average earnings is 50\nearnings <- rnorm(n, mean = 50, sd = 10)\n\n# Threshold for bankruptcy\nthreshold <- 40\n\n# Only companies with earnings above the threshold \"survive\"\nsurvivor_earnings <- earnings[earnings > threshold]\n\n# Average earnings for all companies vs. survivors\ntrue_avg <- mean(earnings)\nsurvivor_avg <- mean(survivor_earnings)\n\ntrue_avg\n#> [1] 50.32515\nsurvivor_avg\n#> [1] 53.3898\nlibrary(ggplot2)\n\ndf <- data.frame(earnings)\n\np <- ggplot(df, aes(x = earnings)) +\n  geom_histogram(\n    binwidth = 2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(aes(xintercept = true_avg, color = \"True Avg\"),\n             linetype = \"dashed\",\n             size = 1) +\n  geom_vline(\n    aes(xintercept = survivor_avg, color = \"Survivor Avg\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = c(\"True Avg\" = \"blue\", \"Survivor Avg\" = \"red\"),\n                     name = \"Average Type\") +\n  labs(title = \"Distribution of Company Earnings\",\n       x = \"Earnings\",\n       y = \"Number of Companies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"other-biases.html","id":"sec-publication-bias","chapter":"37 Other Biases","heading":"37.4 Publication Bias","text":"Publication bias occurs results studies influence likelihood published. Typically, studies significant, positive, sensational results likely published non-significant negative results. can skew perceived effectiveness results researchers conduct meta-analyses literature reviews, leading draw inaccurate conclusions.Example: Imagine pharmaceutical research. 10 studies done new drug, 2 show positive effect 8 show effect, 2 positive studies get published, later review literature might erroneously conclude drug effective.Relation Biases:Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Let’s simulate experiment new treatment. ’ll assume treatment effect, due random variation, studies show significant positive negative effects.Using histogram visualize distribution study results, highlighting “published” studies.plot might show “True Avg Effect” around zero, “Published Avg Effect” likely higher lower, depending studies happen significant results simulation.Remedies:Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).","code":"\nset.seed(42)\n\n# Number of studies\nn <- 100\n\n# Assuming no real effect (effect size = 0)\ntrue_effect <- 0\n\n# Random variation in results\nresults <- rnorm(n, mean = true_effect, sd = 1)\n\n# Only \"significant\" results get published \n# (arbitrarily defining significant as abs(effect) > 1.5)\npublished_results <- results[abs(results) > 1.5]\n\n# Average effect for all studies vs. published studies\ntrue_avg_effect <- mean(results)\npublished_avg_effect <- mean(published_results)\n\ntrue_avg_effect\n#> [1] 0.03251482\npublished_avg_effect\n#> [1] -0.3819601\nlibrary(ggplot2)\n\ndf <- data.frame(results)\n\np <- ggplot(df, aes(x = results)) +\n  geom_histogram(\n    binwidth = 0.2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(\n    aes(xintercept = true_avg_effect,\n        color = \"True Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  geom_vline(\n    aes(xintercept = published_avg_effect,\n        color = \"Published Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\n      \"True Avg Effect\" = \"blue\",\n      \"Published Avg Effect\" = \"red\"\n    ),\n    name = \"Effect Type\"\n  ) +\n  labs(title = \"Distribution of Study Results\",\n       x = \"Effect Size\",\n       y = \"Number of Studies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"sec-directed-acyclic-graphs.html","id":"sec-directed-acyclic-graphs","chapter":"38 Directed Acyclic Graphs","heading":"38 Directed Acyclic Graphs","text":"Directed Acyclic Graphs (DAGs) provide formal visual framework representing assumptions causal structures. modern data analysis, essential tools understanding, identifying, validating causal effects.DAG graph composed nodes (representing variables) directed edges (arrows) showing direction causality. “Acyclic” means graph contains feedback loops; return node following direction arrows.Understanding DAGs helps analysts reason :variables control regression modelHow avoid collider bias confoundingWhat types data needed estimate causal effectsWhere causal identification fails due unobserved variablesThese R packages facilitate DAG creation, visualization, causal analysis:dagitty: Powerful syntax defining DAGs, checking d-separation, performing adjustment set analysis.ggdag: ggplot2-based visualization tool DAGs, compatible dagitty, providing publication-ready DAGs.dagR: Focuses applied epidemiological use DAGs, particularly teaching.r-causal: Developed Center Causal Discovery. Offers methods causal discovery data (also Python).Web ToolsPublication-ready DAG editor: shinyDAG – R-based Shiny app creating beautiful DAGs interactively.Standalone DAG tool: DAG program Sven Knüppel – Excellent beginners needing intuitive graphical interface.","code":""},{"path":"sec-directed-acyclic-graphs.html","id":"basic-notation-and-graph-structures","chapter":"38 Directed Acyclic Graphs","heading":"38.1 Basic Notation and Graph Structures","text":"Directed Acyclic Graphs composed basic building blocks define relationships variables.Mediators (Chains)\\[\nX \\Z \\Y\n\\]Variable \\(Z\\) mediates effect \\(X\\) \\(Y\\).Controlling \\(Z\\) blocks indirect effect \\(X\\) \\(Y\\).Use case marketing: Email promotion (\\(X\\)) → customer interest (\\(Z\\)) → purchase (\\(Y\\)). Controlling interest removes indirect path, isolating direct impact.Common Causes (Forks)\\[\nX \\leftarrow Z \\Y\n\\]\\(Z\\) confounder, creating spurious association \\(X\\) \\(Y\\).estimate causal effect \\(X\\) \\(Y\\), \\(Z\\) must controlled.Use case finance: economic indicator (\\(Z\\)) affects stock investment decisions (\\(X\\)) market returns (\\(Y\\)).Key concept: \\(Z\\) controlled, \\(X\\) \\(Y\\) may appear correlated due shared cause rather causal link.Common Effects (Colliders)\\[\nX \\Z \\leftarrow Y\n\\]\\(Z\\) collider, controlling induces spurious association \\(X\\) \\(Y\\).control \\(Z\\) descendants.Use case HR analytics: Two independent hiring factors (\\(X\\) = education, \\(Y\\) = experience) influence decision variable \\(Z\\) (hiring outcome). Conditioning hired can create artificial correlation education experience.ConceptsDescendants: variable downstream node; controlling descendant can similar effects controlling ancestor.d-Separation: graphical criterion determine conditional independence. paths \\(X\\) \\(Y\\) blocked controlling set variables \\(Z\\), \\(X\\) d-separated \\(Y\\) given \\(Z\\).","code":""},{"path":"sec-directed-acyclic-graphs.html","id":"rule-of-thumb-for-causal-inference","chapter":"38 Directed Acyclic Graphs","heading":"38.2 Rule of Thumb for Causal Inference","text":"validly estimate causal effect \\(X \\Y\\):Close backdoor paths \\(X\\) \\(Y\\) (paths start arrow \\(X\\)). removes confounding.block part direct indirect causal path \\(X\\) \\(Y\\) (especially via mediators).known backdoor criterion—formal condition Pearl’s Causal Inference framework.","code":""},{"path":"sec-directed-acyclic-graphs.html","id":"example-dag","chapter":"38 Directed Acyclic Graphs","heading":"38.3 Example DAG","text":"DAG :mediator path: \\(X \\Z \\Y\\)backdoor path unobserved confounder: \\(X \\leftarrow U \\Y\\)Use adjustmentSets(dag, exposure = \"X\", outcome = \"Y\") identify valid adjustment set.","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\ndag <- dagitty(\"dag {\n  X -> Z -> Y\n  Z <- U -> Y\n}\")\n\ncoordinates(dag) <- list(\n  x = c(X = 1, Z = 2, Y = 3, U = 1.5),\n  y = c(X = 1, Z = 1, Y = 1, U = 2)\n)\n\nggdag(dag) +\n  theme_dag()"},{"path":"sec-directed-acyclic-graphs.html","id":"causal-discovery","chapter":"38 Directed Acyclic Graphs","heading":"38.4 Causal Discovery","text":"Causal discovery involves algorithmically identifying causal relationships data set assumptions (like faithfulness causal sufficiency). Key algorithms include:PC algorithm: Constraint-based, uses conditional independence testingGES (Greedy Equivalence Search): Score-based methodFCI (Fast Causal Inference): Extends PC handle latent confoundersSee (Eberhardt, Kaynar, Siddiq 2024) comprehensive discussion assumptions limitations discovery algorithms practice.","code":""},{"path":"sec-directed-acyclic-graphs.html","id":"section","chapter":"38 Directed Acyclic Graphs","heading":"38.5 ","text":"","code":""},{"path":"sec-controls.html","id":"sec-controls","chapter":"39 Controls","heading":"39 Controls","text":"Traditionally, adding control variables regression models often considered harmless even beneficial. idea simple: controlling potential confounders (variables influence treatment outcome) help isolate causal effect variable interest. However, intuition can misleading, especially comes overcontrolling—issue often overlooked academic research review process.many fields, peer review process frequently encourages addition control variables. Reviewers often ask authors add variables “control” potential confounding. seems reasonable surface, ’s important note practice adding controls without solid theoretical justification can lead erroneous conclusions.Question: reviewers often quick suggest adding controls?\nAnswer: often assumed control variables lead accurate models, assumption hold many situations. true causal relationship treatment (e.g., marketing campaign) outcome (e.g., sales) may become obscured irrelevant inappropriate variables controlled .\nQuestion: reviewers often quick suggest adding controls?Answer: often assumed control variables lead accurate models, assumption hold many situations. true causal relationship treatment (e.g., marketing campaign) outcome (e.g., sales) may become obscured irrelevant inappropriate variables controlled .Counterpoint: Rarely reviewer suggest removing variables distort analysis, even though may appropriate cases. subtle issue often connected concept Coefficient Stability.Counterpoint: Rarely reviewer suggest removing variables distort analysis, even though may appropriate cases. subtle issue often connected concept Coefficient Stability.Overcontrolling occurs include control variables truly affect causal relationship exposure (e.g., treatment) outcome. practice can introduce bias estimated causal effect. two main issues overcontrolling:Overcontrol Bias: control variable collider (described Directed Acyclic Graphs), conditioning can induce spurious correlation exposure outcome.Loss Variability: Including many control variables can lead multicollinearity, reduces variability available estimate relationship treatment outcome.","code":"\nlibrary(dagitty)\nlibrary(ggdag)"},{"path":"sec-controls.html","id":"bad-controls","chapter":"39 Controls","heading":"39.1 Bad Controls","text":"","code":""},{"path":"sec-controls.html","id":"m-bias","chapter":"39 Controls","heading":"39.1.1 M-bias","text":"common intuition causal inference control variable precedes treatment. logic underpins much guidance traditional econometric texts (G. W. Imbens Rubin 2015; J. D. Angrist Pischke 2009), pre-treatment variables like \\(Z\\) often recommended controls correlate treatment \\(X\\) outcome \\(Y\\).perspective especially prevalent Matching Methods, observed pre-treatment covariates typically included matching process. However, controlling every pre-treatment variable can lead bad control bias.One example M-bias, arises conditioning collider — variable influenced two unobserved causes. DAG illustrates case \\(Z\\) appears good control actually opens biasing path:structure, \\(Z\\) collider path \\(X \\leftarrow U_1 \\rightarrow Z \\leftarrow U_2 \\rightarrow Y\\). Controlling \\(Z\\) opens path, introducing spurious association \\(X\\) \\(Y\\) even none existed originally.Even though \\(Z\\) statistically correlated \\(X\\) \\(Y\\), confounder, lie back-door path needs blocked. Instead, adjusting \\(Z\\) biases estimate causal effect \\(X \\Y\\).Let’s illustrate simulation:Notice adjusting \\(Z\\) changes estimate effect \\(X\\) \\(Y\\), even though \\(Z\\) true confounder. textbook example M-bias practice.","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG specification\nmodel <- dagitty(\"dag{\n  x -> y\n  u1 -> x\n  u1 -> z\n  u2 -> z\n  u2 -> y\n}\")\n\n# Set latent variables\nlatents(model) <- c(\"u1\", \"u2\")\n\n# Coordinates for plotting\ncoordinates(model) <- list(\n  x = c(x = 1, u1 = 1, z = 2, u2 = 3, y = 3),\n  y = c(x = 1, u1 = 2, z = 1.5, u2 = 2, y = 1)\n)\n\n# Plot the DAG\nggdag(model) + theme_dag()\nset.seed(123)\n\nn <- 1e4\nu1 <- rnorm(n)\nu2 <- rnorm(n)\nz <- u1 + u2 + rnorm(n)\nx <- u1 + rnorm(n)\ncausal_coef <- 2\ny <- causal_coef * x - 4 * u2 + rnorm(n)\n\n# Compare unadjusted and adjusted models\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Adjusted\")\n)"},{"path":"sec-controls.html","id":"worse-m-bias-with-direct-effect-from-z-to-y","chapter":"39 Controls","heading":"39.1.1.1 Worse: M-bias with Direct Effect from Z to Y","text":"difficult case arises \\(Z\\) also direct effect \\(Y\\). Consider DAG :situation presents dilemma:controlling \\(Z\\) leaves back-door path \\(X \\leftarrow U_1 \\Z \\Y\\) open, introducing confounding bias.controlling \\(Z\\) leaves back-door path \\(X \\leftarrow U_1 \\Z \\Y\\) open, introducing confounding bias.Controlling \\(Z\\) opens collider path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), also biases estimate.Controlling \\(Z\\) opens collider path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), also biases estimate.short, adjustment strategy can fully remove bias estimate \\(X \\Y\\) using observed data alone.Can Done?facing situations, often turn sensitivity analysis assess robust causal conclusions unmeasured confounding. Specifically, recent advances (Cinelli et al. 2019; Cinelli Hazlett 2020) allow us quantify:Plausible bounds strength direct effect \\(Z \\Y\\)Plausible bounds strength direct effect \\(Z \\Y\\)Sensitivity parameters reflecting possible influence latent variables \\(U_1\\) \\(U_2\\)Sensitivity parameters reflecting possible influence latent variables \\(U_1\\) \\(U_2\\)tools help us understand large unmeasured biases order overturn conclusions — pragmatic approach perfect control impossible.","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG specification\nmodel <- dagitty(\"dag{\n  x -> y\n  u1 -> x\n  u1 -> z\n  u2 -> z\n  u2 -> y\n  z -> y\n}\")\n\n# Set latent variables\nlatents(model) <- c(\"u1\", \"u2\")\n\n# Coordinates for plotting\ncoordinates(model) <- list(\n  x = c(x = 1, u1 = 1, z = 2, u2 = 3, y = 3),\n  y = c(x = 1, u1 = 2, z = 1.5, u2 = 2, y = 1)\n)\n\n# Plot the DAG\nggdag(model) + theme_dag()"},{"path":"sec-controls.html","id":"bias-amplification-1","chapter":"39 Controls","heading":"39.1.2 Bias Amplification","text":"Bias amplification occurs controlling variable confounder — fact, controlling increases bias due unobserved confounder.DAG , \\(U\\) unobserved common cause \\(X\\) \\(Y\\). \\(Z\\) influences \\(X\\) causal relationship \\(Y\\). Including \\(Z\\) model block back-door path instead increases bias \\(U\\) amplifying association \\(X\\).Even though \\(Z\\) strong predictor \\(X\\), confounder, common cause \\(X\\) \\(Y\\). Controlling \\(Z\\) increases portion \\(X\\)’s variation explained \\(U\\), thus amplifying bias estimating effect \\(X\\) \\(Y\\).Simulation:Observe adjusted model biased unadjusted one. illustrates controlling variable like \\(Z\\) can amplify omitted variable bias.","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG specification\nmodel <- dagitty(\"dag{\n  x -> y\n  u -> x\n  u -> y\n  z -> x\n}\")\n\n# Set latent variable\nlatents(model) <- c(\"u\")\n\n# Coordinates for plotting\ncoordinates(model) <- list(\n  x = c(z = 1, x = 2, u = 3, y = 4),\n  y = c(z = 1, x = 1, u = 2, y = 1)\n)\n\n# Plot the DAG\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nz <- rnorm(n)\nu <- rnorm(n)\nx <- 2*z + u + rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Adjusted\")\n)"},{"path":"sec-controls.html","id":"sec-overcontrol-bias","chapter":"39 Controls","heading":"39.1.3 Overcontrol Bias","text":"Overcontrol bias arises adjust variables lie causal path treatment outcome, serve proxies outcome.","code":""},{"path":"sec-controls.html","id":"mediator-control","chapter":"39 Controls","heading":"39.1.3.1 Mediator Control","text":"Controlling mediator — variable lies causal path treatment outcome — removes part effect trying estimate.want estimate total effect \\(X\\) \\(Y\\), controlling \\(Z\\) (mediator) leads overcontrol bias., \\(Z\\) appear significant, including blocks causal path \\(X\\) \\(Y\\). misleading goal estimate total effect \\(X\\).","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG: X → Z → Y\nmodel <- dagitty(\"dag{\n  x -> z\n  z -> y\n}\")\n\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 3),\n  y = c(x = 1, z = 1, y = 1)\n)\n\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nx <- rnorm(n)\nz <- x + rnorm(n)\ny <- z + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Total Effect\", \"Controlled for Mediator\")\n)"},{"path":"sec-controls.html","id":"proxy-for-mediator","chapter":"39 Controls","heading":"39.1.3.2 Proxy for Mediator","text":"complex scenarios, controlling variables proxy mediators can introduce similar distortions.Even though \\(Z\\) path \\(X\\) \\(Y\\), controlling removes part causal variation coming \\(M\\).","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG: X → M → Z, M → Y\nmodel <- dagitty(\"dag{\n  x -> m\n  m -> z\n  m -> y\n}\")\n\ncoordinates(model) <- list(\n  x = c(x = 1, m = 2, z = 2, y = 3),\n  y = c(x = 2, m = 2, z = 1, y = 2)\n)\n\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nx <- rnorm(n)\nm <- x + rnorm(n)\nz <- m + rnorm(n)\ny <- m + rnorm(n)\n\n\njtools::export_summs(lm(y ~ x),\n                     lm(y ~ x + z),\n                     model.names = c(\"Total Effect\", \"Controlled for Proxy Z\"))"},{"path":"sec-controls.html","id":"overcontrol-with-unobserved-confounding","chapter":"39 Controls","heading":"39.1.3.3 Overcontrol with Unobserved Confounding","text":"\\(Z\\) influenced \\(X\\) latent confounder \\(U\\) also affects \\(Y\\), controlling \\(Z\\) biases estimate.Although total effect \\(X\\) \\(Y\\) correctly captured unadjusted model, adjusting \\(Z\\) introduces bias via collider path \\(X \\Z \\leftarrow U \\Y\\).Insight: Controlling \\(Z\\) inadvertently blocks direct effect \\(X\\) opens biasing path \\(U\\). makes adjusted model unreliable causal inference.examples highlight importance conceptual clarity causal reasoning model specification. covariates controlled — especially :Mediators (causal path)Mediators (causal path)Proxies mediators outcomesProxies mediators outcomesColliders descendants collidersColliders descendants collidersIn business contexts, often arises analysts include intermediate variables like sales leads, customer engagement scores, operational metrics without understanding whether mediate effect treatment (e.g., ad spend) confound .","code":"\n# Clean workspace\nrm(list = ls())\n\n# DAG: X → Z → Y; U → Z, U → Y\nmodel <- dagitty(\"dag{\n  x -> z\n  z -> y\n  u -> z\n  u -> y\n}\")\n\nlatents(model) <- \"u\"\n\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, u = 3, y = 4),\n  y = c(x = 1, z = 1, u = 2, y = 1)\n)\n\nggdag(model) + theme_dag()\nset.seed(1)\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u + rnorm(n)\ny <- z + u + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Controlled for Z\")\n)"},{"path":"sec-controls.html","id":"selection-bias","chapter":"39 Controls","heading":"39.1.4 Selection Bias","text":"Selection bias — also known collider stratification bias — occurs conditioning variable collider (common effect two variables). inadvertently opens non-causal paths, inducing spurious associations variables otherwise independent unconfounded.","code":""},{"path":"sec-controls.html","id":"classic-collider-bias","chapter":"39 Controls","heading":"39.1.4.1 Classic Collider Bias","text":"DAG , \\(Z\\) collider \\(X\\) latent variable \\(U\\). Controlling \\(Z\\) opens back-door path \\(X\\) \\(Y\\) \\(U\\), introducing bias.Simulation:Controlling \\(Z\\) opens non-causal path \\(X \\Z \\leftarrow U \\Y\\), resulting biased estimates effect \\(X\\) \\(Y\\).","code":"\nrm(list = ls())\n\n# DAG\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> z\n  u -> z\n  u -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, u = 2, y = 3),\n  y = c(x = 3, z = 2, u = 4, y = 3)\n)\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u + rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Adjusted for Z (collider)\")\n)"},{"path":"sec-controls.html","id":"collider-between-treatment-and-outcome","chapter":"39 Controls","heading":"39.1.4.2 Collider Between Treatment and Outcome","text":"cases, collider influenced directly treatment outcome. setting also highly relevant observational designs, particularly retrospective convenience sampling scenarios.Simulation:Even though \\(Z\\) associated \\(X\\) $Y$, controlled , opens collider path \\(X \\Z \\leftarrow Y\\), generating spurious dependence.","code":"\nrm(list = ls())\n\n# DAG: X → Z ← Y\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> z\n  y -> z\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 3),\n  y = c(x = 2, z = 1, y = 2)\n)\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- x + y + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Adjusted for Collider Z\")\n)"},{"path":"sec-controls.html","id":"case-control-bias","chapter":"39 Controls","heading":"39.1.5 Case-Control Bias","text":"Case-control studies often condition outcome (descendants), can lead collider bias properly accounted .DAG , \\(Z\\) descendant collider. Controlling can induce spurious correlations opening non-causal paths.Simulation:Note subtlety: \\(X\\) true causal effect \\(Y\\), controlling \\(Z\\) biases estimate. However, \\(X\\) causal effect \\(Y\\), \\(X\\) d-separated \\(Y\\), even adjusting \\(Z\\). special case, controlling \\(Z\\) falsely suggest effect.Key Insight: Whether adjustment induces bias depends presence absence true causal path. highlights importance DAGs clarifying assumptions guiding valid statistical inference.","code":"\nrm(list = ls())\n\n# DAG: X → Y → Z\nmodel <- dagitty(\"dag{\n  x -> y\n  y -> z\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 3),\n  y = c(x = 2, z = 1, y = 2)\n)\nggdag(model) + theme_dag()\nset.seed(123)\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- y + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Unadjusted\", \"Adjusted for Descendant Z\")\n)"},{"path":"sec-controls.html","id":"summary-3","chapter":"39 Controls","heading":"39.1.6 Summary","text":"","code":""},{"path":"sec-controls.html","id":"good-controls","chapter":"39 Controls","heading":"39.2 Good Controls","text":"","code":""},{"path":"sec-controls.html","id":"omitted-variable-bias-correction","chapter":"39 Controls","heading":"39.2.1 Omitted Variable Bias Correction","text":"variable \\(Z\\) good control blocks back-door paths treatment \\(X\\) outcome \\(Y\\). fundamental criterion back-door adjustment theorem causal inference.","code":""},{"path":"sec-controls.html","id":"simple-confounder","chapter":"39 Controls","heading":"39.2.1.1 Simple Confounder","text":"DAG, \\(Z\\) common cause \\(X\\) \\(Y\\), .e., confounder.Controlling \\(Z\\) removes bias back-door path \\(X \\leftarrow Z \\rightarrow Y\\).","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  z -> x\n  z -> y\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 3),\n  y = c(x = 1, z = 2, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\ncausal_coef <- 2\nbeta2 <- 3\nx <- z + rnorm(n)\ny <- causal_coef * x + beta2 * z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"confounding-via-a-latent-variable","chapter":"39 Controls","heading":"39.2.1.2 Confounding via a Latent Variable","text":"structure, \\(U\\) unobserved causes \\(Z\\) \\(Y\\), \\(Z\\) affects \\(X\\). Even though \\(U\\) observed, adjusting \\(Z\\) helps block back-door path \\(X\\) \\(Y\\) goes \\(U\\).Even though $Z$ appears significant, inclusion serves reduce omitted variable bias rather causal interpretation .","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> x\n  u -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, u = 3, y = 4),\n  y = c(x = 1, z = 2, u = 3, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\nx <- z + rnorm(n)\ny <- 2 * x + u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"z-is-caused-by-u-but-also-causes-y","chapter":"39 Controls","heading":"39.2.1.3 \\(Z\\) is caused by \\(U\\), but also causes \\(Y\\)","text":"DAG illustrates subtle case \\(Z\\) non-causal path \\(X\\) \\(Y\\) helps block bias shared cause \\(U\\)., interpret coefficient \\(Z\\) causally, including \\(Z\\) helps reduce omitted variable bias unobserved confounder \\(U\\).","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  u -> z\n  u -> x\n  z -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <- list(\n  x = c(x = 1, z = 3, u = 2, y = 4),\n  y = c(x = 1, z = 2, u = 3, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\nx <- u + rnorm(n)\ny <- 2 * x + z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"summary-of-omitted-variable-correction","chapter":"39 Controls","heading":"39.2.1.4 Summary of Omitted Variable Correction","text":"","code":"\n# Model 1: Z is a confounder\nmodel1 <- dagitty(\"dag{\n  x -> y\n  z -> x\n  z -> y\n}\")\ncoordinates(model1) <-\n    list(x = c(x = 1, z = 2, y = 3), y = c(x = 1, z = 2, y = 1))\n\n# Model 2: Z is on path from U to X\nmodel2 <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> x\n  u -> y\n}\")\nlatents(model2) <- \"u\"\ncoordinates(model2) <-\n    list(x = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        y = 4\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        y = 1\n    ))\n\n# Model 3: Z influenced by U, affects Y\nmodel3 <- dagitty(\"dag{\n  x -> y\n  u -> z\n  u -> x\n  z -> y\n}\")\nlatents(model3) <- \"u\"\ncoordinates(model3) <-\n    list(x = c(\n        x = 1,\n        z = 3,\n        u = 2,\n        y = 4\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        y = 1\n    ))\n\npar(mfrow = c(1, 3))\nggdag(model1) + theme_dag()\nggdag(model2) + theme_dag()\nggdag(model3) + theme_dag()"},{"path":"sec-controls.html","id":"omitted-variable-bias-in-mediation-correction","chapter":"39 Controls","heading":"39.2.2 Omitted Variable Bias in Mediation Correction","text":"variable \\(Z\\) confounder treatment \\(X\\) mediator \\(M\\), controlling \\(Z\\) helps isolate indirect direct effects accurately.","code":""},{"path":"sec-controls.html","id":"observed-confounder-of-mediator-and-treatment","chapter":"39 Controls","heading":"39.2.2.1 Observed Confounder of Mediator and Treatment","text":"","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  z -> x\n  x -> m\n  z -> m\n  m -> y\n}\")\ncoordinates(model) <-\n    list(x = c(\n        x = 1,\n        z = 2,\n        m = 3,\n        y = 4\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        m = 1,\n        y = 1\n    ))\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- z + rnorm(n)\nm <- 2 * x + z + rnorm(n)\ny <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"latent-common-cause-of-mediator-and-treatment","chapter":"39 Controls","heading":"39.2.2.2 Latent Common Cause of Mediator and Treatment","text":"","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> x\n  x -> m\n  u -> m\n  m -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <-\n    list(x = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 4,\n        y = 5\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 1,\n        y = 1\n    ))\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\nx <- z + rnorm(n)\nm <- 2 * x + u + rnorm(n)\ny <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"z-affects-mediator-u-affects-both-x-and-z","chapter":"39 Controls","heading":"39.2.2.3 Z Affects Mediator, U Affects Both X and Z","text":"","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> m\n  x -> m\n  u -> x\n  m -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <-\n    list(x = c(\n        x = 1,\n        z = 3,\n        u = 2,\n        m = 4,\n        y = 5\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 1,\n        y = 1\n    ))\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\nx <- u + rnorm(n)\nm <- 2 * x + z + rnorm(n)\ny <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"sec-controls.html","id":"summary-of-mediation-correction","chapter":"39 Controls","heading":"39.2.2.4 Summary of Mediation Correction","text":"\\(Z\\) may statistically significant, imply causal effect unless \\(Z\\) directly causal path \\(X\\) \\(Y\\). many valid control scenarios, \\(Z\\) simply serves isolate causal effect \\(X\\), interpreted cause .","code":"\n# Model 4\nmodel4 <- dagitty(\"dag{\n  x -> y\n  z -> x\n  x -> m\n  z -> m\n  m -> y\n}\")\ncoordinates(model4) <-\n    list(x = c(\n        x = 1,\n        z = 2,\n        m = 3,\n        y = 4\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        m = 1,\n        y = 1\n    ))\n\n# Model 5\nmodel5 <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> x\n  x -> m\n  u -> m\n  m -> y\n}\")\nlatents(model5) <- \"u\"\ncoordinates(model5) <-\n    list(x = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 4,\n        y = 5\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 1,\n        y = 1\n    ))\n\n# Model 6\nmodel6 <- dagitty(\"dag{\n  x -> y\n  u -> z\n  z -> m\n  x -> m\n  u -> x\n  m -> y\n}\")\nlatents(model6) <- \"u\"\ncoordinates(model6) <-\n    list(x = c(\n        x = 1,\n        z = 3,\n        u = 2,\n        m = 4,\n        y = 5\n    ),\n    y = c(\n        x = 1,\n        z = 2,\n        u = 3,\n        m = 1,\n        y = 1\n    ))\n\npar(mfrow = c(1, 3))\nggdag(model4) + theme_dag()\nggdag(model5) + theme_dag()\nggdag(model6) + theme_dag()"},{"path":"sec-controls.html","id":"neutral-controls","chapter":"39 Controls","heading":"39.3 Neutral Controls","text":"covariates used regression adjustment necessary identification. Neutral controls help causal identification may affect estimation precision. Including :introduce bias, lie back-door collider paths.May reduce standard errors, explaining additional variation outcome.","code":""},{"path":"sec-controls.html","id":"good-predictive-controls","chapter":"39 Controls","heading":"39.3.1 Good Predictive Controls","text":"variable correlated outcome \\(Y\\), cause treatment \\(X\\), controlling optional identification may increase precision.","code":""},{"path":"sec-controls.html","id":"z-predicts-y-not-x","chapter":"39 Controls","heading":"39.3.1.1 \\(Z\\) predicts \\(Y\\), not \\(X\\)","text":"coefficient \\(X\\) remains unbiased models, standard errors smaller model \\(Z\\).","code":"\n# Clean workspace\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  z -> y\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 2),\n  y = c(x = 1, z = 2, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\ny <- x + 2 * z + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Without Z\", \"With Predictive Z\")\n)"},{"path":"sec-controls.html","id":"z-predicts-a-mediator-m","chapter":"39 Controls","heading":"39.3.1.2 \\(Z\\) predicts a mediator \\(M\\)","text":"Even though \\(Z\\) causal path \\(X\\) \\(Y\\), controlling may reduce residual variance \\(Y\\), hence increasing precision.","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> m\n  z -> m\n  m -> y\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, m = 2, y = 3),\n  y = c(x = 1, z = 2, m = 1, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\nm <- 2 * z + rnorm(n)\ny <- x + 2 * m + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Without Z\", \"With Predictive Z\")\n)"},{"path":"sec-controls.html","id":"good-selection-bias","chapter":"39 Controls","heading":"39.3.2 Good Selection Bias","text":"complex selection structures, adjusting selection variables can improve identification, presence additional post-selection information.","code":""},{"path":"sec-controls.html","id":"w-is-a-collider-z-helps-condition-on-selection","chapter":"39 Controls","heading":"39.3.2.1 \\(W\\) is a collider; \\(Z\\) helps condition on selection","text":"Unadjusted model unbiased.Unadjusted model unbiased.Controlling \\(W\\) biased due collider path \\(X \\Z \\W \\leftarrow U \\Y\\).Controlling \\(W\\) biased due collider path \\(X \\Z \\W \\leftarrow U \\Y\\).Adding \\(Z\\) restores identification blocking path.Adding \\(Z\\) restores identification blocking path.","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> z\n  z -> w\n  u -> w\n  u -> y\n}\")\nlatents(model) <- \"u\"\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, w = 3, u = 3, y = 5),\n  y = c(x = 3, z = 2, w = 1, u = 4, y = 3)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + rnorm(n)\nw <- z + u + rnorm(n)\ny <- x - 2 * u + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + w),\n  lm(y ~ x + z + w),\n  model.names = c(\"Unadjusted\", \"Control W\", \"Control W + Z\")\n)"},{"path":"sec-controls.html","id":"bad-predictive-controls","chapter":"39 Controls","heading":"39.3.3 Bad Predictive Controls","text":"predictive variables useful — may reduce precision soaking degrees freedom increasing multicollinearity.","code":""},{"path":"sec-controls.html","id":"z-predicts-x-not-y","chapter":"39 Controls","heading":"39.3.3.1 \\(Z\\) predicts \\(X\\), not \\(Y\\)","text":"\\(Z\\) adds explanatory power \\(Y\\), thus increases SE estimate \\(X\\)’s effect.","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  z -> x\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 1, y = 2),\n  y = c(x = 1, z = 2, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- 2 * z + rnorm(n)\ny <- x + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Without Z\", \"With Z (predicts X)\")\n)"},{"path":"sec-controls.html","id":"z-is-a-child-of-x","chapter":"39 Controls","heading":"39.3.3.2 \\(Z\\) is a child of \\(X\\)","text":", \\(Z\\) post-treatment variable. Though bias estimate \\(X\\), adds noise increases SE.","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> z\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 1, y = 2),\n  y = c(x = 1, z = 2, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nz <- 2 * x + rnorm(n)\ny <- x + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Without Z\", \"With Child Z\")\n)"},{"path":"sec-controls.html","id":"bad-selection-bias","chapter":"39 Controls","heading":"39.3.4 Bad Selection Bias","text":"Controlling post-treatment variables can hurt precision without affecting bias.Although \\(Z\\) lies causal path \\(X\\) \\(Z\\) (\\(Z\\) \\(Y\\)), including \\(Z\\) adds redundant information may inflate standard errors. case, ’s “bad control” M-bias sense, still suboptimal.","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{\n  x -> y\n  x -> z\n}\")\ncoordinates(model) <- list(\n  x = c(x = 1, z = 2, y = 2),\n  y = c(x = 1, z = 2, y = 1)\n)\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nz <- 2 * x + rnorm(n)\ny <- x + rnorm(n)\n\njtools::export_summs(\n  lm(y ~ x),\n  lm(y ~ x + z),\n  model.names = c(\"Without Z\", \"With Post-treatment Z\")\n)"},{"path":"sec-controls.html","id":"summary-table-predictive-vs.-causal-utility-of-controls","chapter":"39 Controls","heading":"39.3.5 Summary Table: Predictive vs. Causal Utility of Controls","text":"","code":""},{"path":"sec-controls.html","id":"choosing-controls","chapter":"39 Controls","heading":"39.4 Choosing Controls","text":"Identifying variables control one important — difficult — steps causal inference. goal block back-door paths treatment \\(X\\) outcome \\(Y\\), without introducing bias colliders mediators.done correctly, adjustment removes confounding bias. done incorrectly, can introduce bias, increase variance, obscure true causal relationship.","code":""},{"path":"sec-controls.html","id":"step-1-use-a-causal-diagram-dag","chapter":"39 Controls","heading":"39.4.1 Step 1: Use a Causal Diagram (DAG)","text":"Causal diagrams provide graphical representation assumptions data-generating process. DAG, can:Identify back-door paths \\(X\\) \\(Y\\)Determine paths blocked opened conditioningUse software identify minimal sufficient adjustment setsFor example, using dagitty:return set(s) covariates must controlled estimate causal effect \\(X\\) \\(Y\\) back-door criterion.","code":"\nlibrary(dagitty)\n\ndag <- dagitty(\"dag {\n  X -> Y\n  Z -> X\n  Z -> Y\n  U -> X\n  U -> Y\n}\")\n\nadjustmentSets(dag, exposure = \"X\", outcome = \"Y\")"},{"path":"sec-controls.html","id":"step-2-use-algorithmic-tools","chapter":"39 Controls","heading":"39.4.2 Step 2: Use Algorithmic Tools","text":"Several tools can automate process selecting appropriate controls given DAG:DAGitty provides intuitive browser-based interface :Draw causal diagramsDraw causal diagramsIdentify minimal sufficient adjustment setsIdentify minimal sufficient adjustment setsSimulate interventions (-calculus)Simulate interventions (-calculus)Diagnose overcontrol collider biasDiagnose overcontrol collider biasIt supports direct integration R allows reproducible workflows.Fusion powerful tool :Computing identification formulas using -calculusComputing identification formulas using -calculusHandling complex longitudinal data selection biasHandling complex longitudinal data selection biasFormalizing queries total, direct, mediated effectsFormalizing queries total, direct, mediated effectsFusion implements algorithms go beyond standard adjustment allow nonparametric identification latent confounders present.","code":""},{"path":"sec-controls.html","id":"step-3-theoretical-principles","chapter":"39 Controls","heading":"39.4.3 Step 3: Theoretical Principles","text":"Key guidelines include:control mediators estimating total effectDo control mediators estimating total effectControl pre-treatment confounders (common causes treatment outcome)Control pre-treatment confounders (common causes treatment outcome)Avoid colliders descendantsAvoid colliders descendantsConsider use instrumental variables suitable adjustment set existsConsider use instrumental variables suitable adjustment set exists","code":""},{"path":"sec-controls.html","id":"step-4-consider-sensitivity-analysis","chapter":"39 Controls","heading":"39.4.4 Step 4: Consider Sensitivity Analysis","text":"Even well-reasoned DAGs, control set may imperfect, especially variables unobserved measured error. cases, sensitivity analysis tools help quantify robust causal conclusions .sensemakr package (Cinelli et al. 2019; Cinelli Hazlett 2020) allows :Quantifying strong unmeasured confounding change conclusionsQuantifying strong unmeasured confounding change conclusionsReporting robustness values: minimal strength confounding needed explain away effectReporting robustness values: minimal strength confounding needed explain away effectGraphical summaries confounding thresholdsGraphical summaries confounding thresholdsThis allows researchers report assumptions transparently, even presence unmeasured bias.","code":""},{"path":"sec-controls.html","id":"step-5-know-when-not-to-control","chapter":"39 Controls","heading":"39.4.5 Step 5: Know When Not to Control","text":"Remember, every variable adjusted . table summarizes control avoid:","code":""},{"path":"sec-controls.html","id":"summary-control-selection-pipeline","chapter":"39 Controls","heading":"39.4.6 Summary: Control Selection Pipeline","text":"Define causal question clearly (total effect, direct effect, etc.)Define causal question clearly (total effect, direct effect, etc.)Draw DAG reflects substantive knowledgeDraw DAG reflects substantive knowledgeUse DAGitty/Fusion identify minimal sufficient control setsUse DAGitty/Fusion identify minimal sufficient control setsDouble-check bad controls (colliders, mediators)Double-check bad controls (colliders, mediators)doubt, conduct sensitivity analysis using sensemakrIf doubt, conduct sensitivity analysis using sensemakrReport assumptions transparently — causal conclusions valid assumptions rely onReport assumptions transparently — causal conclusions valid assumptions rely onThe important confounders often unmeasured. recognizing ones measured already half battle.","code":""},{"path":"report.html","id":"report","chapter":"40 Report","heading":"40 Report","text":"StructureExploratory analysis\nplots\npreliminary results\ninteresting structure/features data\noutliers\nExploratory analysisplotspreliminary resultsinteresting structure/features dataoutliersModel\nAssumptions\nmodel/ model best one?\nConsideration: interactions, collinearity, dependence\nModelAssumptionsWhy model/ model best one?Consideration: interactions, collinearity, dependenceModel Fit\nwell fit?\nmodel assumptions met?\nResidual analysis\n\nModel FitHow well fit?well fit?model assumptions met?\nResidual analysis\nmodel assumptions met?Residual analysisInference/ Prediction\ndifferent way support inference?\nInference/ PredictionAre different way support inference?Conclusion\nRecommendation\nLimitation analysis\ncorrect future\nConclusionRecommendationRecommendationLimitation analysisLimitation analysisHow correct futureHow correct futureThis chapter based jtools package. information can found .","code":""},{"path":"report.html","id":"one-summary-table","chapter":"40 Report","heading":"40.1 One summary table","text":"Packages reporting:Summary Statistics Table:qwraps2vtablegtsummaryapaTablesstargazerRegression TablegtsummarysjPlot,sjmisc, sjlabelledstargazer: recommended (Example)modelsummaryModel Equation","code":"\nlibrary(jtools)\ndata(movies)\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\nsumm(fit)\nsumm(\n    fit,\n    scale = TRUE,\n    vifs = TRUE,\n    part.corr = TRUE,\n    confint = TRUE,\n    pvals = FALSE\n) # notice that scale here is TRUE\n\n#obtain clsuter-robust SE\ndata(\"PetersenCL\", package = \"sandwich\")\nfit2 <- lm(y ~ x, data = PetersenCL)\nsumm(fit2, robust = \"HC3\", cluster = \"firm\") \n# install.packages(\"equatiomatic\") # not available for R 4.2\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\n# show the theoretical model\nequatiomatic::extract_eq(fit)\n# display the actual coefficients\nequatiomatic::extract_eq(fit, use_coefs = TRUE)"},{"path":"report.html","id":"model-comparison","chapter":"40 Report","heading":"40.2 Model Comparison","text":"Another package modelsummaryAnother package stargazerCorrelation Table","code":"\nfit <- lm(metascore ~ log(budget), data = movies)\nfit_b <- lm(metascore ~ log(budget) + log(us_gross), data = movies)\nfit_c <- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies)\ncoef_names <- c(\"Budget\" = \"log(budget)\", \"US Gross\" = \"log(us_gross)\",\n                \"Runtime (Hours)\" = \"runtime\", \"Constant\" = \"(Intercept)\")\nexport_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nlibrary(modelsummary)\nlm_mod <- lm(mpg ~ wt + hp + cyl, mtcars)\nmsummary(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nmodelplot(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nlibrary(\"stargazer\")\nstargazer(attitude)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Wed, May 14, 2025 - 8:45:23 PM\n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lccccc} \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#> Statistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Max} \\\\ \n#> \\hline \\\\[-1.8ex] \n#> rating & 30 & 64.633 & 12.173 & 40 & 85 \\\\ \n#> complaints & 30 & 66.600 & 13.315 & 37 & 90 \\\\ \n#> privileges & 30 & 53.133 & 12.235 & 30 & 83 \\\\ \n#> learning & 30 & 56.367 & 11.737 & 34 & 75 \\\\ \n#> raises & 30 & 64.633 & 10.397 & 43 & 88 \\\\ \n#> critical & 30 & 74.767 & 9.895 & 49 & 92 \\\\ \n#> advance & 30 & 42.933 & 10.289 & 25 & 72 \\\\ \n#> \\hline \\\\[-1.8ex] \n#> \\end{tabular} \n#> \\end{table}\n## 2 OLS models\nlinear.1 <-\n    lm(rating ~ complaints + privileges + learning + raises + critical,\n       data = attitude)\nlinear.2 <-\n    lm(rating ~ complaints + privileges + learning, data = attitude)\n## create an indicator dependent variable, and run a probit model\nattitude$high.rating <- (attitude$rating > 70)\n\nprobit.model <-\n    glm(\n        high.rating ~ learning + critical + advance,\n        data = attitude,\n        family = binomial(link = \"probit\")\n    )\nstargazer(linear.1,\n          linear.2,\n          probit.model,\n          title = \"Results\",\n          align = TRUE)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Wed, May 14, 2025 - 8:45:23 PM\n#> % Requires LaTeX packages: dcolumn \n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{Results} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#>  & \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ \n#> \\cline{2-4} \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{rating} & \\multicolumn{1}{c}{high.rating} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{\\textit{OLS}} & \\multicolumn{1}{c}{\\textit{probit}} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{1}{c}{(1)} & \\multicolumn{1}{c}{(2)} & \\multicolumn{1}{c}{(3)}\\\\ \n#> \\hline \\\\[-1.8ex] \n#>  complaints & 0.692^{***} & 0.682^{***} &  \\\\ \n#>   & (0.149) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  privileges & -0.104 & -0.103 &  \\\\ \n#>   & (0.135) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  learning & 0.249 & 0.238^{*} & 0.164^{***} \\\\ \n#>   & (0.160) & (0.139) & (0.053) \\\\ \n#>   & & & \\\\ \n#>  raises & -0.033 &  &  \\\\ \n#>   & (0.202) &  &  \\\\ \n#>   & & & \\\\ \n#>  critical & 0.015 &  & -0.001 \\\\ \n#>   & (0.147) &  & (0.044) \\\\ \n#>   & & & \\\\ \n#>  advance &  &  & -0.062 \\\\ \n#>   &  &  & (0.042) \\\\ \n#>   & & & \\\\ \n#>  Constant & 11.011 & 11.258 & -7.476^{**} \\\\ \n#>   & (11.704) & (7.318) & (3.570) \\\\ \n#>   & & & \\\\ \n#> \\hline \\\\[-1.8ex] \n#> Observations & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} \\\\ \n#> R$^{2}$ & \\multicolumn{1}{c}{0.715} & \\multicolumn{1}{c}{0.715} &  \\\\ \n#> Adjusted R$^{2}$ & \\multicolumn{1}{c}{0.656} & \\multicolumn{1}{c}{0.682} &  \\\\ \n#> Log Likelihood &  &  & \\multicolumn{1}{c}{-9.087} \\\\ \n#> Akaike Inf. Crit. &  &  & \\multicolumn{1}{c}{26.175} \\\\ \n#> Residual Std. Error & \\multicolumn{1}{c}{7.139 (df = 24)} & \\multicolumn{1}{c}{6.863 (df = 26)} &  \\\\ \n#> F Statistic & \\multicolumn{1}{c}{12.063$^{***}$ (df = 5; 24)} & \\multicolumn{1}{c}{21.743$^{***}$ (df = 3; 26)} &  \\\\ \n#> \\hline \n#> \\hline \\\\[-1.8ex] \n#> \\textit{Note:}  & \\multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n#> \\end{tabular} \n#> \\end{table}\n# Latex\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\n# ASCII text output\nstargazer(\n    linear.1,\n    linear.2,\n    type = \"text\",\n    title = \"Regression Results\",\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    ci = TRUE,\n    ci.level = 0.90,\n    single.row = TRUE\n)\n#> \n#> Regression Results\n#> ========================================================================\n#>                                        Dependent variable:              \n#>                          -----------------------------------------------\n#>                                          Overall Rating                 \n#>                                    (1)                     (2)          \n#> ------------------------------------------------------------------------\n#> Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)\n#> No Special Privileges    -0.104 (-0.325, 0.118)  -0.103 (-0.316, 0.109) \n#> Opportunity to Learn      0.249 (-0.013, 0.512)   0.238* (0.009, 0.467) \n#> Performance-Based Raises -0.033 (-0.366, 0.299)                         \n#> Too Critical              0.015 (-0.227, 0.258)                         \n#> Advancement              11.011 (-8.240, 30.262) 11.258 (-0.779, 23.296)\n#> ------------------------------------------------------------------------\n#> Observations                       30                      30           \n#> R2                                0.715                   0.715         \n#> Adjusted R2                       0.656                   0.682         \n#> ========================================================================\n#> Note:                                        *p<0.1; **p<0.05; ***p<0.01\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\ncorrelation.matrix <-\n    cor(attitude[, c(\"rating\", \"complaints\", \"privileges\")])\nstargazer(correlation.matrix, title = \"Correlation Matrix\")"},{"path":"report.html","id":"changes-in-an-estimate","chapter":"40 Report","heading":"40.3 Changes in an estimate","text":"","code":"\ncoef_names <- coef_names[1:3] # Dropping intercept for plots\nplot_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nplot_summs(\n    fit_c,\n    robust = \"HC3\",\n    coefs = coef_names,\n    plot.distributions = TRUE\n)"},{"path":"report.html","id":"standard-errors-2","chapter":"40 Report","heading":"40.4 Standard Errors","text":"sandwich vignetteHeterogeneityWhite’s estimatorAll heterogeneity SE methods derivatives .small sample bias adjustmentUses degrees freedom-based correctionWhen number clusters small, HC2 HC3 better (Cameron, Gelbach, Miller 2008)Better linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrixBetter linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrix","code":"\ndata(cars)\nmodel <- lm(speed ~ dist, data = cars)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = speed ~ dist, data = cars)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -7.5293 -2.1550  0.3615  2.4377  6.4179 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  8.28391    0.87438   9.474 1.44e-12 ***\n#> dist         0.16557    0.01749   9.464 1.49e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.156 on 48 degrees of freedom\n#> Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 \n#> F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\nlmtest::coeftest(model, vcov. = sandwich::vcovHC(model, type = \"HC1\"))\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept) 8.283906   0.891860  9.2883 2.682e-12 ***\n#> dist        0.165568   0.019402  8.5335 3.482e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"report.html","id":"coefficient-uncertainty-and-distribution","chapter":"40 Report","heading":"40.5 Coefficient Uncertainty and Distribution","text":"ggdist allows us visualize uncertainty frequentist Bayesian frameworks","code":"\nlibrary(ggdist)"},{"path":"report.html","id":"descriptive-tables","chapter":"40 Report","heading":"40.6 Descriptive Tables","text":"Export APA themeExport LatexHowever, codes play well notes. Hence, create custom code follows AMA guidelines","code":"\ndata(\"mtcars\")\n\nlibrary(flextable)\ntheme_apa(flextable(mtcars[1:5,1:5]))\nprint(xtable::xtable(mtcars, type = \"latex\"),\n      file = file.path(getwd(), \"output\", \"mtcars_xtable.tex\"))\n\n# American Economic Review style\nstargazer::stargazer(\n    mtcars,\n    title = \"Testing\",\n    style = \"aer\",\n    out = file.path(getwd(), \"output\", \"mtcars_stargazer.tex\")\n)\n\n# other styles include\n# Administrative Science Quarterly\n# Quarterly Journal of Economics\nama_tbl <- function(data, caption, label, note, output_path) {\n  library(tidyverse)\n  library(xtable)\n  # Function to determine column alignment\n  get_column_alignment <- function(data) {\n    # Start with the alignment for the header row\n    alignment <- c(\"l\", \"l\")\n    \n    # Check each column\n    for (col in seq_len(ncol(data))[-1]) {\n      if (is.numeric(data[[col]])) {\n        alignment <- c(alignment, \"r\")  # Right alignment for numbers\n      } else {\n        alignment <- c(alignment, \"c\")  # Center alignment for other data\n      }\n    }\n    \n    return(alignment)\n  }\n  \n  data %>%\n    # bold + left align first column \n    rename_with(~paste(\"\\\\multicolumn{1}{l}{\\\\textbf{\", ., \"}}\"), 1) %>% \n    # bold + center align all other columns\n    `colnames<-`(ifelse(colnames(.) != colnames(.)[1],\n                        paste(\"\\\\multicolumn{1}{c}{\\\\textbf{\", colnames(.), \"}}\"),\n                        colnames(.))) %>% \n    \n    xtable(caption = caption,\n           label = label,\n           align = get_column_alignment(data),\n           auto = TRUE) %>%\n    print(\n      include.rownames = FALSE,\n      caption.placement = \"top\",\n      \n      hline.after=c(-1, 0),\n      \n       # p{0.9\\linewidth} sets the width of the column to 90% of the line width, and the @{} removes any extra padding around the cell.\n      \n      add.to.row = list(pos = list(nrow(data)), # Add at the bottom of the table\n                        command = c(paste0(\"\\\\hline \\n \\\\multicolumn{\",ncol(data), \"}{l} {\", \"\\n \\\\begin{tabular}{@{}p{0.9\\\\linewidth}@{}} \\n\",\"Note: \", note, \"\\n \\\\end{tabular}  } \\n\"))), # Add your note here\n      \n      # make sure your heading is untouched (because you manually change it above)\n      sanitize.colnames.function = identity,\n      \n      # place a the top of the page\n      table.placement = \"h\",\n      \n      file = output_path\n    )\n}\nama_tbl(\n    mtcars,\n    caption     = \"This is caption\",\n    label       = \"tab:this_is_label\",\n    note        = \"this is note\",\n    output_path = file.path(getwd(), \"output\", \"mtcars_custom_ama.tex\")\n)"},{"path":"report.html","id":"visualizations-and-plots","chapter":"40 Report","heading":"40.7 Visualizations and Plots","text":"can customize plots based preferred journals. , creating custom setting American Marketing Association.American-Marketing-Association-ready theme plotsExampleOther pre-specified themes","code":"\nlibrary(ggplot2)\n\n# check available fonts\n# windowsFonts()\n\n# for Times New Roman\n# names(windowsFonts()[windowsFonts()==\"TT Times New Roman\"])\n# Making a theme\namatheme = theme_bw(base_size = 14, base_family = \"serif\") + # This is Time New Roman\n    \n    theme(\n        # remove major gridlines\n        panel.grid.major   = element_blank(),\n\n        # remove minor gridlines\n        panel.grid.minor   = element_blank(),\n\n        # remove panel border\n        panel.border       = element_blank(),\n\n        line               = element_line(),\n\n        # change font\n        text               = element_text(),\n\n        # if you want to remove legend title\n        # legend.title     = element_blank(),\n\n        legend.title       = element_text(size = rel(0.6), face = \"bold\"),\n\n        # change font size of legend\n        legend.text        = element_text(size = rel(0.6)),\n        \n        legend.background  = element_rect(color = \"black\"),\n        \n        # legend.margin    = margin(t = 5, l = 5, r = 5, b = 5),\n        # legend.key       = element_rect(color = NA, fill = NA),\n\n        # change font size of main title\n        plot.title         = element_text(\n            size           = rel(1.2),\n            face           = \"bold\",\n            hjust          = 0.5,\n            margin         = margin(b = 15)\n        ),\n        \n        plot.margin        = unit(c(1, 1, 1, 1), \"cm\"),\n\n        # add black line along axes\n        axis.line          = element_line(colour = \"black\", linewidth = .8),\n        \n        axis.ticks         = element_line(),\n        \n\n        # axis title\n        axis.title.x       = element_text(size = rel(1.2), face = \"bold\"),\n        axis.title.y       = element_text(size = rel(1.2), face = \"bold\"),\n\n        # axis text size\n        axis.text.y        = element_text(size = rel(1)),\n        axis.text.x        = element_text(size = rel(1))\n    )\nlibrary(tidyverse)\nlibrary(ggsci)\ndata(\"mtcars\")\nyourplot <- mtcars %>%\n    select(mpg, cyl, gear) %>%\n    ggplot(., aes(x = mpg, y = cyl, fill = gear)) + \n    geom_point() +\n    labs(title=\"Some Plot\") \n\nyourplot + \n    amatheme + \n    # choose different color theme\n    scale_color_npg() \n\nyourplot + \n    amatheme + \n    scale_color_continuous()\nlibrary(ggthemes)\n\n\n# Stata theme\nyourplot +\n    theme_stata()\n\n# The economist theme\nyourplot + \n    theme_economist()\n\nyourplot + \n    theme_economist_white()\n\n# Wall street journal theme\nyourplot + \n    theme_wsj()\n\n# APA theme\nyourplot +\n    jtools::theme_apa(\n        legend.font.size = 24,\n        x.font.size = 20,\n        y.font.size = 20\n    )"},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"41 Exploratory Data Analysis","heading":"41 Exploratory Data Analysis","text":"Data ReportFeature EngineeringMissing DataError IdentificationSummary statisticsNot code-y processQuick dirty way look dataCode generation wranglingShiny-app based Tableu styleCustomized daily/automatic report","code":"\n# load to get txhousing data\nlibrary(ggplot2)\n# install.packages(\"DataExplorer\")\nlibrary(DataExplorer)\n\n# creat a html file that contain all reports\ncreate_report(txhousing)\n\nintroduce() # see basic info\n\n\ndummify() # create binary columns from discrete variables\nsplit_columns() # split data into discrete and continuous parts\n\n\n\nplot_correlation() # heatmap for discrete var\nplot_intro() \n\nplot_missing() # plot missing value\nprofile_missing() # profile missing values\n\n\nplot_prcomp() # plot PCA\n# install.packages(\"dataReporter\")\nlibrary(dataReporter)\nmakeDataReport() # detailed report like DataExplorer\nlibrary(skimr)\nskim() # give only few quick summary stat, not as detailed as the other two packages\n# install.packages(\"rpivotTable\")\nlibrary(rpivotTable)\n# give set up just like Excel table \ndata %>% \n    rpivotTable::rpivotTable()\n# install.packages(\"esquisse\")\nlibrary(esquisse)\nesquisse::esquisser()\n# install.packages(\"chronicle\")\nlibrary(chronicle)\n# install.packages(\"dlookr\")\n# install.packages(\"descriptr\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"sensitivity-analysis-robustness-check","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42 Sensitivity Analysis/ Robustness Check","text":"","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"specification-curve","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42.1 Specification curve","text":"also known Specification robustness graph coefficient stability plotResourcesIn Stata speccurveIn Stata speccurve(Simonsohn, Simmons, Nelson 2020)(Simonsohn, Simmons, Nelson 2020)","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"starbility","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42.1.1 starbility","text":"RecommendInstallationExample package’s authorPlot different combinations controlsNote:\\(p < 0.01\\): red\\(p < 0.05\\): green\\(p < 0.1\\): blue\\(p > 0.1\\): blackMore Advanced StuffIn step 2, can modify use function (e.g., glm)getting specification (e.g., different CI)get customized plotTo get different model specification (e.g., probit vs. logit)","code":"\ndevtools::install_github('https://github.com/AakaashRao/starbility')\nlibrary(starbility)\nlibrary(tidyverse)\nlibrary(starbility)\nlibrary(lfe)\ndata(\"diamonds\")\nset.seed(43)\nindices = sample(1:nrow(diamonds),\n                 replace = F,\n                 size = round(nrow(diamonds) / 20))\ndiamonds = diamonds[indices, ]\n\n# If you want to make the diamond dimensions as base control\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z' # include all variables under 1 dimension\n)\n\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding fixed effects\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding instrumental variables \ninstruments = 'x+y+z'\n\n# clustering and weights \ndiamonds$sample_weights = runif(n = nrow(diamonds))\n\n\n# robust standard errors \nstarb_felm_custom = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = lfe::felm(spec, data=data) %>% broom::tidy()\n\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n  \n  # 99% confidence interval\n  z = qnorm(0.995) \n  # one-tailed test\n  return(c(coef, p/2, coef+z*se, coef-z*se))\n}\n\nplots = stability_plot(\n    data = diamonds,\n    lhs = 'price',\n    rhs = 'carat',\n    error_geom = 'ribbon', # make the plot more aesthetics\n    # error_geom = 'none', # if you don't want ribbon (i.e., error bar)\n    model = starb_felm_custom,\n    cluster = 'cut',\n    weights = 'sample_weights',\n    # iv = instruments,\n    perm = perm_controls,\n    base = base_controls,\n    # perm_fe = perm_fe_controls,\n    \n    # if you want to include fixed effects sequentially (not all combinations) \n    # (e.g., you want to test country or state fixed effect, not both )\n    # nonperm_fe = nonperm_fe_controls, \n    # fe_always = F,  # if you want to have a model without any Fixed Effects\n    \n    # sort \"asc\", \"desc\", or by fixed effects: \"asc-by-fe\" or \"desc-by-fe\"\n    sort = \"asc-by-fe\", \n    \n    # if you have less variables and want more aesthetics \n    # control_geom = 'circle',\n    # point_size = 2,\n    # control_spacing = 0.3,\n    \n    \n    # error_alpha = 0.2, # change alpha of the error geom\n    # point_size = 1.5, # change the size of the coefficient points\n    # control_text_size = 10, # change the size of the control labels\n    # coef_ylim = c(-5000, 35000), # change the endpoints of the y-axis\n    # trip_top = 3, # change the spacing between the two panels\n    \n    rel_height = 0.6\n)\nplots\n\n# add comments\n# replacement_coef_panel = plots[[1]] +\n#   scale_y_reverse() +\n#   theme(panel.grid.minor = element_blank()) +\n#   geom_vline(xintercept = 41,\n#              linetype = 'dashed',\n#              alpha = 0.4) +\n#   annotate(\n#     geom = 'label',\n#     x = 52,\n#     y = 30000,\n#     label = 'What a great\\nspecification!',\n#     alpha = 0.75\n#   )\n# \n# combine_plots(replacement_coef_panel,\n#               plots[[2]],\n#               rel_height = 0.6)\n# Step 1: Control Grid\n\ndiamonds$high_clarity = diamonds$clarity %in% c('VS1','VVS2','VVS1','IF')\n\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z'\n)\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nperm_fe_controls = c(\n  'Cut FE' = 'cut',\n  'Color FE' = 'color'\n)\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\ngrid1 = stability_plot(data = diamonds, \n                      lhs = 'price', \n                      rhs = 'carat', \n                      perm = perm_controls,\n                      base = base_controls, \n                      perm_fe = perm_fe_controls, \n                      nonperm_fe = nonperm_fe_controls, \n                      run_to=2)\n\nknitr::kable(grid1 %>% head(10))\n\n# Step 2: Get model expression\n\ngrid2 = stability_plot(grid = grid1,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=2,\n                      run_to=3)\n\n\nknitr::kable(grid2 %>% head(10))\n\n# Step 3: Estimate models\ngrid3 = stability_plot(grid = grid2,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=3,\n                      run_to=4)\n\nknitr::kable(grid3 %>% head(10))\n\n# Step 4: Get dataframe to draw\ndfs = stability_plot(grid = grid3,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=4,\n                      run_to=5)\n\ncoef_grid = dfs[[1]]\ncontrol_grid = dfs[[2]]\n\nknitr::kable(coef_grid %>% head(10))\n\n# Step 5: plot the sensitivity graph \npanels = stability_plot(data = diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      coef_grid = coef_grid,\n                      control_grid = control_grid,\n                      run_from=5,\n                      run_to=6)\n\nstability_plot(data = diamonds,\n               lhs='price', \n               rhs='carat', \n               coef_panel = panels[[1]],\n               control_panel = panels[[2]],\n               run_from = 6,\n               run_to = 7)\ndiamonds$above_med_price = as.numeric(diamonds$price > median(diamonds$price))\n\nbase_controls = c('Diamond dimensions' = 'x + y + z')\n\nperm_controls = c('Depth' = 'depth',\n                  'Table width' = 'table',\n                  'Clarity' = 'clarity')\nlhs_var = 'above_med_price'\nrhs_var = 'carat'\n\ngrid1 = stability_plot(\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_to = 2\n)\n\n# Create control part of formula\nbase_perm = c(base_controls, perm_controls)\ngrid1$expr = apply(grid1[, 1:length(base_perm)], 1,\n                   function(x)\n                     paste(base_perm[names(base_perm)[which(x == 1)]], \n                           collapse = '+'))\n\n# Complete formula with LHS and RHS variables\ngrid1$expr = paste(lhs_var, '~', rhs_var, '+', grid1$expr, sep = '')\n\nknitr::kable(grid1 %>% head(10))\n\n# customer function for the logit model\nstarb_logit = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n    broom::tidy()\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n\n  return(c(coef, p, coef+1.96*se, coef-1.96*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from=3)\nlibrary(margins)\nstarb_logit_enhanced = function(spec, data, rhs, ...) {\n  # Unpack ...\n  l = list(...)\n  get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n  \n  spec = as.formula(spec)\n  if (get_mfx) {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      margins() %>%\n      summary\n    row = which(model$factor==rhs)\n    coef = model[row, 'AME'] %>% as.numeric()\n    se   = model[row, 'SE'] %>% as.numeric()\n    p    = model[row, 'p'] %>% as.numeric()\n  } else {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      broom::tidy()\n    row = which(model$term==rhs)\n    coef = model[row, 'estimate'] %>% as.numeric()\n    se   = model[row, 'std.error'] %>% as.numeric()\n    p    = model[row, 'p.value'] %>% as.numeric()\n  }\n\n  z = qnorm(0.995)\n  return(c(coef, p, coef+z*se, coef-z*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3)\ndfs = stability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3,\n               run_to = 5)\n\ncoef_grid_logit = dfs[[1]]\ncontrol_grid_logit = dfs[[2]]\n\nmin_space = 0.5\n\ncoef_plot = ggplot2::ggplot(coef_grid_logit, aes(\n  x = model,\n  y = coef,\n  shape = p,\n  group = p\n)) +\n  geom_linerange(aes(ymin = error_low, ymax = error_high), alpha = 0.75) +\n  geom_point(size = 5, aes(col = p, fill = p), alpha = 1) +\n  viridis::scale_color_viridis(discrete = TRUE, option = \"D\") +\n  scale_shape_manual(values = c(15, 17, 18, 19)) +\n  theme_classic() +\n  geom_hline(yintercept = 0, linetype = 'dotted') +\n  ggtitle('A custom coefficient stability plot!') +\n  labs(subtitle = \"Error bars represent 99% confidence intervals\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks.x = element_blank()\n  ) +\n  coord_cartesian(xlim = c(1 - min_space, max(coef_grid_logit$model) + min_space),\n                  ylim = c(-0.1, 1.6)) +\n  guides(fill = F, shape = F, col = F)\n\n\ncontrol_plot = ggplot(control_grid_logit) +\n  geom_point(aes(x = model, y = y, fill=value), shape=23, size=4) +\n  scale_fill_manual(values=c('#FFFFFF', '#000000')) +\n  guides(fill=F) +\n  scale_y_continuous(breaks = unique(control_grid_logit$y), \n                     labels = unique(control_grid_logit$key),\n                     limits=c(min(control_grid_logit$y)-1, max(control_grid_logit$y)+1)) +\n  scale_x_continuous(breaks=c(1:max(control_grid_logit$model))) +\n  coord_cartesian(xlim=c(1-min_space, max(control_grid_logit$model)+min_space)) +\n  theme_classic() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title = element_blank(),\n        axis.text.y = element_text(size=10),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()) \n\ncowplot::plot_grid(coef_plot, control_plot, rel_heights=c(1,0.5), \n                   align='v', ncol=1, axis='b')\nstarb_probit = function(spec, data, rhs, ...) {\n    # Unpack ...\n    l = list(...)\n    get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n    \n    spec = as.formula(spec)\n    if (get_mfx) {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            margins() %>%\n            summary\n        row = which(model$factor == rhs)\n        coef = model[row, 'AME'] %>% as.numeric()\n        se   = model[row, 'SE'] %>% as.numeric()\n        p    = model[row, 'p'] %>% as.numeric()\n    } else {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            broom::tidy()\n        row = which(model$term == rhs)\n        coef = model[row, 'estimate'] %>% as.numeric()\n        se   = model[row, 'std.error'] %>% as.numeric()\n        p    = model[row, 'p.value'] %>% as.numeric()\n    }\n    \n    z = qnorm(0.995)\n    return(c(coef, p, coef + z * se, coef - z * se))\n}\n\nprobit_dfs = stability_plot(\n    grid = grid1,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    model = starb_probit,\n    get_mfx = T,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 3,\n    run_to = 5\n)\n\n# We'll put the probit DFs on the left, \n #so we need to adjust the model numbers accordingly\n# so the probit and logit DFs don't plot on top of one another!\ncoef_grid_probit = probit_dfs[[1]] %>% \n    mutate(model = model + max(coef_grid_logit$model))\n\ncontrol_grid_probit = probit_dfs[[2]] %>% \n    mutate(model = model + max(control_grid_logit$model))\n\ncoef_grid    = bind_rows(coef_grid_logit, coef_grid_probit)\ncontrol_grid = bind_rows(control_grid_logit, control_grid_probit)\n\npanels = stability_plot(\n    coef_grid = coef_grid,\n    control_grid = control_grid,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 5,\n    run_to = 6\n)\n\ncoef_plot = panels[[1]] + geom_vline(xintercept = 8.5,\n                                     linetype = 'dashed',\n                                     alpha = 0.8) +\n    annotate(\n        geom = 'label',\n        x = 4.25,\n        y = 1.8,\n        label = 'Logit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    annotate(\n        geom = 'label',\n        x = 12.75,\n        y = 1.8,\n        label = 'Probit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    coord_cartesian(ylim = c(-0.5, 1.9))\n\ncontrol_plot = panels[[2]] + geom_vline(xintercept = 8.5,\n                                        linetype = 'dashed',\n                                        alpha = 0.8)\n\ncowplot::plot_grid(\n    coef_plot,\n    control_plot,\n    rel_heights = c(1, 0.5),\n    align = 'v',\n    ncol = 1,\n    axis = 'b'\n)"},{"path":"sensitivity-analysis-robustness-check.html","id":"rdfanalysis","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42.1.2 rdfanalysis","text":"recommendInstallationExample package’s authorShiny app readers explore","code":"\ndevtools::install_github(\"joachim-gassen/rdfanalysis\")\nlibrary(rdfanalysis)\nload(url(\"https://joachim-gassen.github.io/data/rdf_ests.RData\"))\nplot_rdf_spec_curve(ests, \"est\", \"lb\", \"ub\") \ndesign <- define_design(steps = c(\"read_data\",\n                                  \"select_idvs\",\n                                  \"treat_extreme_obs\",\n                                  \"specify_model\",\n                                  \"est_model\"),\n                        rel_dir = \"vignettes/case_study_code\")\n\nshiny_rdf_spec_curve(ests, list(\"est\", \"lb\", \"ub\"),\n                     design, \"vignettes/case_study_code\",\n                     \"https://joachim-gassen.github.io/data/wb_new.csv\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"coefficient-stability","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42.2 Coefficient stability","text":"(Oster 2019)Coefficient stability can evident omitted variable bias.Coefficient stability can evident omitted variable bias.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.Packagesmplot: graphical Model stability Variable Selectionmplot: graphical Model stability Variable Selectionrobomit: Robustness checks omitted variable bias (implementation ofrobomit: Robustness checks omitted variable bias (implementation ","code":"\nlibrary(robomit)\n\n# estimate beta \no_beta(\n  y     = \"mpg\",       # dependent variable\n  x     = \"wt\",        # independent treatment variable\n  con   = \"hp + qsec\", # related control variables\n  delta = 1,           # delta\n  R2max = 0.9,         # maximum R-square\n  type  = \"lm\",        # model type\n  data  = mtcars       # dataset\n) \n#> # A tibble: 10 × 2\n#>    Name                           Value\n#>    <chr>                          <dbl>\n#>  1 beta*                         -2.00 \n#>  2 (beta*-beta controlled)^2      5.56 \n#>  3 Alternative Solution 1        -7.01 \n#>  4 (beta[AS1]-beta controlled)^2  7.05 \n#>  5 Uncontrolled Coefficient      -5.34 \n#>  6 Controlled Coefficient        -4.36 \n#>  7 Uncontrolled R-square          0.753\n#>  8 Controlled R-square            0.835\n#>  9 Max R-square                   0.9  \n#> 10 delta                          1"},{"path":"sensitivity-analysis-robustness-check.html","id":"omitted-variable-bias-quantification","chapter":"42 Sensitivity Analysis/ Robustness Check","heading":"42.3 Omitted Variable Bias Quantification","text":"quantify bias needed change substantive conclusion causal inference study.","code":"\nlibrary(konfound)\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5\n)\n#> Robustness of Inference to Replacement (RIR):\n#> RIR = 215\n#> \n#> To invalidate the inference of an effect using the threshold of 3.925 for\n#> statistical significance (with null hypothesis = 0 and alpha = 0.05), 21.506%\n#> of the (5) estimate would have to be due to bias. This implies that to\n#> invalidate the inference one would expect to have to replace 215 (21.506%)\n#> observations with data points for which the effect is 0 (RIR = 215).\n#> \n#> See Frank et al. (2013) for a description of the method.\n#> \n#> Citation: Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. (2013).\n#> What would it take to change an inference?\n#> Using Rubin's causal model to interpret the robustness of causal inferences.\n#> Education, Evaluation and Policy Analysis, 35 437-460.\n#> \n#> Accuracy of results increases with the number of decimals reported.\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"thresh_plot\"\n)\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"corr_plot\"\n)"},{"path":"replication-and-synthetic-data.html","id":"replication-and-synthetic-data","chapter":"43 Replication and Synthetic Data","heading":"43 Replication and Synthetic Data","text":"Access comprehensive data pivotal replication, especially realm social sciences. Yet, data often inaccessible due proprietary restrictions, privacy concerns, logistical constraints, making replication challenge (G. King 1995). chapter explores nuances replication, exceptions norms, significance synthetic data solution.","code":""},{"path":"replication-and-synthetic-data.html","id":"the-replication-standard","chapter":"43 Replication and Synthetic Data","heading":"43.1 The Replication Standard","text":"Replicability research ensures:Credibility – Reinforces trust empirical studies allowing independent verification.Continuity – Enables future research build upon prior findings, promoting cumulative knowledge.Visibility – Increases readership citations, benefiting individual researchers broader academic community.research replicable, adhering replication standard essential. standard requires researchers provide necessary information—data, code, methodological details—third parties can independently reproduce study’s findings. quantitative research often allows clearer replication, qualitative studies pose challenges due depth, contextual nature, reliance subjective interpretation.","code":""},{"path":"replication-and-synthetic-data.html","id":"solutions-for-empirical-replication","chapter":"43 Replication and Synthetic Data","heading":"43.1.1 Solutions for Empirical Replication","text":"Several approaches help address replication challenges empirical research:Role Individual Authors\nResearchers must commit transparency provide well-documented data code.\nRepositories Inter-University Consortium Political Social Research (ICPSR) offer secure, long-term storage replication datasets.\nResearchers must commit transparency provide well-documented data code.Repositories Inter-University Consortium Political Social Research (ICPSR) offer secure, long-term storage replication datasets.Creation Replication Data Set\ndedicated replication dataset include original data, relevant supplementary data, exact procedures used analysis.\nMetadata documentation provided ensure clarity.\ndedicated replication dataset include original data, relevant supplementary data, exact procedures used analysis.Metadata documentation provided ensure clarity.Professional Data Archives\nOrganizations like ICPSR, Dataverse, Zenodo facilitate open access datasets maintaining proper governance sensitive information.\narchives help address data accessibility preservation issues.\nOrganizations like ICPSR, Dataverse, Zenodo facilitate open access datasets maintaining proper governance sensitive information.archives help address data accessibility preservation issues.Educational Implications\nTeaching replication strengthens students’ understanding empirical methods reproducibility.\nMany graduate programs now incorporate replication studies coursework, emphasizing importance methodological rigor.\nTeaching replication strengthens students’ understanding empirical methods reproducibility.Many graduate programs now incorporate replication studies coursework, emphasizing importance methodological rigor.","code":""},{"path":"replication-and-synthetic-data.html","id":"free-data-repositories","chapter":"43 Replication and Synthetic Data","heading":"43.1.2 Free Data Repositories","text":"Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.figshare: Allows researchers upload, share, cite datasets.figshare: Allows researchers upload, share, cite datasets.Dryad: Primarily datasets associated published articles biological medical sciences.Dryad: Primarily datasets associated published articles biological medical sciences.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.SocArXiv: open archive social sciences.SocArXiv: open archive social sciences.EarthArXiv: preprints archive earth science.EarthArXiv: preprints archive earth science.Protein Data Bank (PDB): 3D structures large biological molecules.Protein Data Bank (PDB): 3D structures large biological molecules.Gene Expression Omnibus (GEO): public functional genomics data repository.Gene Expression Omnibus (GEO): public functional genomics data repository.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.","code":""},{"path":"replication-and-synthetic-data.html","id":"exceptions-to-replication","chapter":"43 Replication and Synthetic Data","heading":"43.1.3 Exceptions to Replication","text":"replication standard fundamental scientific integrity, certain constraints may prevent full adherence. common exceptions include:Confidentiality\ndatasets contain highly sensitive information (e.g., medical records, personal financial data) disclosed, even fragmented form.\nAnonymization techniques data aggregation can sometimes mitigate concerns, privacy regulations (e.g., GDPR, HIPAA) impose strict limitations.\ndatasets contain highly sensitive information (e.g., medical records, personal financial data) disclosed, even fragmented form.Anonymization techniques data aggregation can sometimes mitigate concerns, privacy regulations (e.g., GDPR, HIPAA) impose strict limitations.Proprietary Data\nDatasets owned corporations, governments, third-party vendors often restricted access due intellectual property concerns.\nmany cases, researchers can share summary statistics, derived variables, synthetic versions data respecting proprietary restrictions.\nDatasets owned corporations, governments, third-party vendors often restricted access due intellectual property concerns.many cases, researchers can share summary statistics, derived variables, synthetic versions data respecting proprietary restrictions.Rights First Publication\nstudies involve data embargoes, researchers must delay public release initial publications completed.\nDespite embargoes, essential data methodology eventually accessible ensure transparency.\nstudies involve data embargoes, researchers must delay public release initial publications completed.Despite embargoes, essential data methodology eventually accessible ensure transparency.","code":""},{"path":"replication-and-synthetic-data.html","id":"replication-landscape","chapter":"43 Replication and Synthetic Data","heading":"43.1.4 Replication Landscape","text":"Brodeur et al. (2025) finds AI-assisted teams improve upon AI-led approaches, human-teams remain effective detecting major errors ensuring reproducibility quantitative social science research.Human teams AI-assisted teams achieved similar reproducibility success rates, significantly outperforming AI-led teams.Human teams AI-assisted teams achieved similar reproducibility success rates, significantly outperforming AI-led teams.Human-teams 57 percentage points successful AI-led teams (p < 0.001).Human-teams 57 percentage points successful AI-led teams (p < 0.001).Error detection: Human teams identified significantly major errors AI-assisted teams (0.7 errors per team, p = 0.017) AI-led teams (1.1 errors per team, p < 0.001).Error detection: Human teams identified significantly major errors AI-assisted teams (0.7 errors per team, p = 0.017) AI-led teams (1.1 errors per team, p < 0.001).AI-assisted teams detected 0.4 errors per team AI-led teams (p = 0.029) still fewer human teams.AI-assisted teams detected 0.4 errors per team AI-led teams (p = 0.029) still fewer human teams.Robustness checks: human AI-assisted teams significantly better AI-led teams proposing (25 percentage points, p = 0.017) implementing (33 percentage points, p = 0.005) comprehensive robustness checks.Robustness checks: human AI-assisted teams significantly better AI-led teams proposing (25 percentage points, p = 0.017) implementing (33 percentage points, p = 0.005) comprehensive robustness checks.Huntington-Klein et al. (2025) uses three-stage many-analysts design examine researcher decisions influence variation treatment effect estimates.146 research teams completed causal inference task three times increasingly standardized conditions:Key findings:Sample size convergence:","code":"-    **Stage 1:** Few constraints (free-form analysis).\n\n-    **Stage 2:** Prescribed research design.\n\n-    **Stage 3:** Prescribed design plus pre-cleaned data.-    **Stage 1:** High variation in reported effects (IQR = 3.1 percentage points), with outliers.\n\n-    **Stage 2:** Even greater variation (IQR = 4.0), due to imperfect protocol adherence.\n\n-    **Stage 3:** Lowest variation (IQR = 2.4), suggesting data cleaning substantially reduces result heterogeneity.-   IQR dropped from 295,187 (Stage 1) to 29,144 (Stage 2), and was effectively zero in Stage 3.\n\nThe results highlight the **critical role of data cleaning** in applied microeconomics and suggest **new directions for replication research**."},{"path":"replication-and-synthetic-data.html","id":"synthetic-data","chapter":"43 Replication and Synthetic Data","heading":"43.2 Synthetic Data","text":"Synthetic data, models real data ensuring anonymity, becoming essential tool research. generating artificial datasets retain key statistical properties original data, researchers can preserve privacy, enhance data accessibility, facilitate replication. However, synthetic data also introduces complexities used caution.","code":""},{"path":"replication-and-synthetic-data.html","id":"benefits-of-synthetic-data","chapter":"43 Replication and Synthetic Data","heading":"43.2.1 Benefits of Synthetic Data","text":"Privacy Preservation\nProtects sensitive proprietary information enabling research collaboration.\nProtects sensitive proprietary information enabling research collaboration.Data Fairness Augmentation\nHelps mitigate biases generating balanced datasets.\nCan supplement real data sample sizes limited.\nHelps mitigate biases generating balanced datasets.Can supplement real data sample sizes limited.Acceleration Research\nAllows data sharing environments access real data restricted.\nEnables large-scale simulations without legal ethical constraints.\nAllows data sharing environments access real data restricted.Enables large-scale simulations without legal ethical constraints.","code":""},{"path":"replication-and-synthetic-data.html","id":"concerns-and-limitations","chapter":"43 Replication and Synthetic Data","heading":"43.2.2 Concerns and Limitations","text":"Misconceptions Privacy\nSynthetic data guarantee absolute privacy—re-identification risks remain similar real dataset.\nSynthetic data guarantee absolute privacy—re-identification risks remain similar real dataset.Challenges Data Outliers\nRare important data points may poorly represented excluded.\nRare important data points may poorly represented excluded.Risks Solely Relying Synthetic Data\nModels trained exclusively synthetic data may lack generalizability.\nDifferences real synthetic distributions can introduce biases.\nModels trained exclusively synthetic data may lack generalizability.Differences real synthetic distributions can introduce biases.","code":""},{"path":"replication-and-synthetic-data.html","id":"further-insights-on-synthetic-data","chapter":"43 Replication and Synthetic Data","heading":"43.2.3 Further Insights on Synthetic Data","text":"Synthetic data acts bridge model-centric data-centric perspectives, making vital tool modern research. analogy can drawn viewing replica Mona Lisa—essence remains, original securely stored.deeper dive synthetic data applications, refer (Jordon et al. 2022).","code":""},{"path":"replication-and-synthetic-data.html","id":"generating-synthetic-data","chapter":"43 Replication and Synthetic Data","heading":"43.2.4 Generating Synthetic Data","text":"generating synthetic data, approach depends whether researchers full access original dataset working restricted conditions.","code":""},{"path":"replication-and-synthetic-data.html","id":"when-you-have-access-to-the-original-dataset","chapter":"43 Replication and Synthetic Data","heading":"43.2.4.1 When You Have Access to the Original Dataset","text":"researchers can directly use dataset, various techniques can employed generate synthetic data preserving statistical properties original:Statistical Approaches\nParametric models (e.g., Gaussian Mixture Models)\nFit statistical distributions real data sample synthetic observations.\n\nParametric models (e.g., Gaussian Mixture Models)\nFit statistical distributions real data sample synthetic observations.\nFit statistical distributions real data sample synthetic observations.Machine Learning-Based Methods\nVariational Autoencoders (VAEs) – Useful structured, complex data representations.\nGenerative Adversarial Networks (GANs) – Effective generating high-dimensional data (e.g., tabular, image, text data).\nCTGAN (Conditional Tabular GAN) – Specifically designed structured, tabular datasets, addressing categorical imbalanced data challenges.\nVariational Autoencoders (VAEs) – Useful structured, complex data representations.Generative Adversarial Networks (GANs) – Effective generating high-dimensional data (e.g., tabular, image, text data).CTGAN (Conditional Tabular GAN) – Specifically designed structured, tabular datasets, addressing categorical imbalanced data challenges.Differential Privacy Techniques\nNoise Addition – Introduces controlled noise maintaining overall statistical structure.\nNoise Addition – Introduces controlled noise maintaining overall statistical structure.","code":""},{"path":"replication-and-synthetic-data.html","id":"when-you-have-a-restricted-dataset","chapter":"43 Replication and Synthetic Data","heading":"43.2.4.2 When You Have a Restricted Dataset","text":"cases data exported due security, privacy, proprietary constraints, researchers must rely alternative strategies generate synthetic data:Summarization Approximation\nExtract summary statistics (e.g., means, variances, correlations) approximate dataset’s structure.\npermitted, share aggregated anonymized data instead raw observations.\nExtract summary statistics (e.g., means, variances, correlations) approximate dataset’s structure.permitted, share aggregated anonymized data instead raw observations.Server-Based Computation\nConduct -server analyses raw data remains inaccessible, synthetic outputs can generated secure system.\nConduct -server analyses raw data remains inaccessible, synthetic outputs can generated secure system.Synthetic Data Generation Preserved Properties\nUse models trained secure dataset produce synthetic data without directly copying real observations.\nEnsure key statistical relationships maintained, even individual values differ.\nUse models trained secure dataset produce synthetic data without directly copying real observations.Ensure key statistical relationships maintained, even individual values differ.","code":""},{"path":"replication-and-synthetic-data.html","id":"application-5","chapter":"43 Replication and Synthetic Data","heading":"43.3 Application","text":"","code":""},{"path":"replication-and-synthetic-data.html","id":"original-dataset","chapter":"43 Replication and Synthetic Data","heading":"43.3.1 Original Dataset","text":"Import librariesSimulate Complex, Nonlinear, Hierarchical Time SeriesSuppose :\\(G = 3\\) groups (e.g., groups 1, 2, 3)\\(G = 3\\) groups (e.g., groups 1, 2, 3)group \\(N = 50\\) units (e.g., individuals devices)group \\(N = 50\\) units (e.g., individuals devices)unit measured \\(T = 20\\) time pointsEach unit measured \\(T = 20\\) time pointsWe’ll create four continuous variables, X1 X4, influenced :group-level random effect (different intercept group)group-level random effect (different intercept group)unit-level random effect (different intercept unit)unit-level random effect (different intercept unit)Time (nonlinear relationships, e.g., sine, polynomial)Time (nonlinear relationships, e.g., sine, polynomial)Nonlinear cross-relationships among X1–X4Nonlinear cross-relationships among X1–X4This gives us total \\(3 \\times 50 \\times 20=3000\\) rows “original” dataset.Explore Original DatasetLet’s descriptive statistics look correlations among X1–X4.\nrepeated measures (time series) nested units groups, correlations “pooled” across rows. simplification, let us demonstrate copula-based synthetic approach.Convert Pseudo-Observations Copula FittingCopulas need variables \\([0,1]\\) space, use empirical CDF (“probability integral transform”) variable.Important: discrete variables like group, ID-like columns unit. synthetic generation group/unit/time, multiple strategies:Model random: e.g., re-sample group, unit, time original distribution.Model random: e.g., re-sample group, unit, time original distribution.Treat continuous copula (recommended true IDs).Treat continuous copula (recommended true IDs).hierarchical approach: fit separate copula group time slice (advanced).hierarchical approach: fit separate copula group time slice (advanced).simplicity, ’ll:Re-sample group time original distributions (like “bootstrapping”).Re-sample group time original distributions (like “bootstrapping”).Use multivariate copula X1–X4.Use multivariate copula X1–X4.Fit Copula ModelWe’ll fit Gaussian copula (try t-copula vine copulas heavier tails complex dependencies). use maximum likelihood estimation:Check estimated correlation matrix within copula. reflect dependency among X1–X4 (though time, group, unit).Generate Synthetic DataSynthetic X1–X4\nSample fitted copula get synthetic \\([0,1]\\) values.\nInvert via original empirical distributions (quantiles).\nSample fitted copula get synthetic \\([0,1]\\) values.Invert via original empirical distributions (quantiles).Synthetic Group, Unit, TimeA simple approach :Re-sample “group” probabilities original distribution.Re-sample “group” probabilities original distribution.Re-sample “unit” within group treat purely random labels (depending needs).Re-sample “unit” within group treat purely random labels (depending needs).Re-sample “time” original distribution replicate time points.Re-sample “time” original distribution replicate time points., simplistic approach: row, pick random row original data copy group, unit, time. preserves real distribution group/time pairs frequency unit. (preserve original time-series ordering autoregressive structure!)need preserve exact time-ordering real “per-unit” correlation across time, ’d need advanced approach (e.g., separate copula unit hierarchical time-series model).Validate Synthetic DataCompare Descriptive StatisticsCompare Correlation MatricesVisual Comparison DistributionsCorrelation Plot Synthetic DataIndistinguishability: synthetic summary statistics (means, variances, skewness, kurtosis) correlation structure match closely, synthetic data often “indistinguishable” original many analytical purposes.Indistinguishability: synthetic summary statistics (means, variances, skewness, kurtosis) correlation structure match closely, synthetic data often “indistinguishable” original many analytical purposes.Hierarchical / Time-Series: True hierarchical time-series replication (.e., preserving unit’s time autocorrelation group structure) may require advanced methods, :\nHierarchical copulas vine copulas time slices.\nMixed-effects / random-effects modeling (e.g., group unit) plus copula residuals.\nDeep generative approaches (e.g., TimeGAN) strong temporal dynamics, currently common Python.\nHierarchical / Time-Series: True hierarchical time-series replication (.e., preserving unit’s time autocorrelation group structure) may require advanced methods, :Hierarchical copulas vine copulas time slices.Hierarchical copulas vine copulas time slices.Mixed-effects / random-effects modeling (e.g., group unit) plus copula residuals.Mixed-effects / random-effects modeling (e.g., group unit) plus copula residuals.Deep generative approaches (e.g., TimeGAN) strong temporal dynamics, currently common Python.Deep generative approaches (e.g., TimeGAN) strong temporal dynamics, currently common Python.Categorical Variables: strictly categorical variables (e.g., group, unit ID), can:\nFit separate copulas within group.\nConvert categories numeric naive way (recommended actual IDs) use specialized discrete copulas.\nCategorical Variables: strictly categorical variables (e.g., group, unit ID), can:Fit separate copulas within group.Fit separate copulas within group.Convert categories numeric naive way (recommended actual IDs) use specialized discrete copulas.Convert categories numeric naive way (recommended actual IDs) use specialized discrete copulas.Privacy Considerations: Even data synthetic, check doesn’t inadvertently leak private information (e.g., via memorizing outliers). Techniques like differential privacy post-hoc checks might required.Privacy Considerations: Even data synthetic, check doesn’t inadvertently leak private information (e.g., via memorizing outliers). Techniques like differential privacy post-hoc checks might required.","code":"\nlibrary(copula)\nlibrary(moments)\nlibrary(PerformanceAnalytics)  # For correlation plots\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(123)  # For reproducibility\n\nG <- 3    # Number of groups\nN <- 50   # Units per group\nTt <- 20  # Time points per unit\n\n# Create a data frame structure\ndf_list <- list()\n\nfor(g in 1:G) {\n  \n  # Group-level random intercept\n  group_intercept <- rnorm(1, mean = 0, sd = 1)\n  \n  for(u in 1:N) {\n    \n    # Unit-level random intercept\n    unit_intercept <- rnorm(1, mean = 0, sd = 0.5)\n    \n    # Simulate time points\n    time_points <- 1:Tt\n    \n    # Create some base patterns\n    X1_base <- group_intercept + unit_intercept + \n               sin(0.2 * time_points) +  # Nonlinear time pattern\n               rnorm(Tt, mean = 0, sd = 0.2)\n    \n    # Introduce different relationships for X2, X3, X4\n    # Some polynomial in time, plus dependence on X1\n    X2_base <- (X1_base^2) + 0.5 * time_points + rnorm(Tt, 0, 0.3)\n    X3_base <- 1 + group_intercept - 0.3 * X1_base + log(time_points+1) + \n               rnorm(Tt, mean = 0, sd = 0.2)\n    X4_base <- exp(0.1 * X1_base) + 0.2 * (X2_base) - 0.5 * (X3_base) + \n               rnorm(Tt, mean = 0, sd = 0.5)\n    \n    df_temp <- data.frame(\n      group = g,\n      unit  = paste0(\"G\", g, \"_U\", u),\n      time  = time_points,\n      X1    = X1_base,\n      X2    = X2_base,\n      X3    = X3_base,\n      X4    = X4_base\n    )\n    \n    df_list[[length(df_list) + 1]] <- df_temp\n  }\n}\n\ndf_original <- do.call(rbind, df_list)\nrow.names(df_original) <- NULL\n\n# Inspect the first rows\nhead(df_original)\n#>   group  unit time           X1        X2        X3         X4\n#> 1     1 G1_U1    1 -0.165153398 0.2194743 0.9291383  0.3963423\n#> 2     1 G1_U1    2 -0.272044371 0.8553408 2.0535411 -0.3918278\n#> 3     1 G1_U1    3 -0.085064371 1.3197242 2.0929304 -0.3268864\n#> 4     1 G1_U1    4  0.384804697 1.6420667 1.7088991  0.6649585\n#> 5     1 G1_U1    5  0.258089835 2.8179465 2.0732799  0.7771992\n#> 6     1 G1_U1    6  0.003462448 3.0460239 2.2910647  0.4905209\n# Descriptive stats (overall)\nsummary(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")])\n#>        X1                X2                 X3                X4         \n#>  Min.   :-4.8316   Min.   :-0.02119   Min.   :-0.7811   Min.   :-1.2191  \n#>  1st Qu.:-1.8272   1st Qu.: 4.47120   1st Qu.: 1.5865   1st Qu.: 0.5339  \n#>  Median :-0.3883   Median : 7.08316   Median : 2.4366   Median : 1.1073  \n#>  Mean   :-0.6520   Mean   : 7.76411   Mean   : 2.3832   Mean   : 1.3118  \n#>  3rd Qu.: 0.4869   3rd Qu.: 9.65271   3rd Qu.: 3.3120   3rd Qu.: 1.8859  \n#>  Max.   : 2.5247   Max.   :33.30223   Max.   : 4.9642   Max.   : 6.8820\n\n# Skewness & Kurtosis\napply(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], 2, skewness)\n#>         X1         X2         X3         X4 \n#> -0.3515255  1.3614289 -0.3142704  0.9435834\napply(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], 2, kurtosis)\n#>         X1         X2         X3         X4 \n#> -0.7689287  2.6280956 -0.6086295  1.1138128\n\n# Correlation matrix\n(cor_mat <- cor(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")]))\n#>            X1         X2         X3         X4\n#> X1  1.0000000 -0.6865655  0.3948406 -0.7035793\n#> X2 -0.6865655  1.0000000  0.1301602  0.7532715\n#> X3  0.3948406  0.1301602  1.0000000 -0.3660620\n#> X4 -0.7035793  0.7532715 -0.3660620  1.0000000\n\nchart.Correlation(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], \n                  histogram = TRUE, pch = 19)\n# Extract the numeric columns we want to transform\noriginal_data <- df_original[, c(\"X1\",\"X2\",\"X3\",\"X4\")]\n\n# Convert each to uniform [0,1] by empirical CDF\nu_data <- pobs(as.matrix(original_data))\n\n# Check ranges (should be between 0 and 1)\napply(u_data, 2, range)\n#>                X1           X2           X3           X4\n#> [1,] 0.0003332223 0.0003332223 0.0003332223 0.0003332223\n#> [2,] 0.9996667777 0.9996667777 0.9996667777 0.9996667777\n# Define an unstructured Gaussian copula\ngaussCop <- normalCopula(dim = ncol(u_data), dispstr = \"un\")\n\n# Fit to the pseudo-observations\nfit_gauss <- fitCopula(gaussCop, data = u_data, method = \"ml\")\n\nsummary(fit_gauss)\n#> Call: fitCopula(gaussCop, data = u_data, method = \"ml\")\n#> Fit based on \"maximum likelihood\" and 3000 4-dimensional observations.\n#> Normal copula, dim. d = 4 \n#>       Estimate Std. Error\n#> rho.1  -0.5746      0.010\n#> rho.2   0.3024      0.014\n#> rho.3  -0.5762      0.010\n#> rho.4   0.2188      0.013\n#> rho.5   0.6477      0.008\n#> rho.6  -0.3562      0.012\n#> The maximized loglikelihood is 2885 \n#> Optimization converged\n#> Number of loglikelihood evaluations:\n#> function gradient \n#>       68       14\nn_synth <- nrow(df_original)  # same size as original\n\n# Sample from the copula\nu_synth <- rCopula(n_synth, fit_gauss@copula)\n\n# Convert from [0,1] -> real scale by matching original distribution\nsynth_X <- data.frame(\n    X1_synth = quantile(original_data$X1, probs = u_synth[, 1],\n                        type = 8),\n    X2_synth = quantile(original_data$X2, probs = u_synth[, 2],\n                        type = 8),\n    X3_synth = quantile(original_data$X3, probs = u_synth[, 3],\n                        type = 8),\n    X4_synth = quantile(original_data$X4, probs = u_synth[, 4],\n                        type = 8)\n)\n\nhead(synth_X)\n#>                X1_synth X2_synth  X3_synth   X4_synth\n#> 68.93791982%  0.2965446 3.488621 2.0876565  0.9336342\n#> 40.75838264% -0.8903883 5.341148 3.4134562  0.4857101\n#> 80.51755107%  0.6841892 2.536933 0.5329199  1.3494999\n#> 29.87601496% -1.5123574 8.127288 0.4861639  3.2120501\n#> 68.75449744%  0.2855353 3.219903 2.1439338 -0.3848253\n#> 35.73763642% -1.1651872 5.717179 2.7372298  0.6252287\nindices <-\n    sample(seq_len(nrow(df_original)),\n           size = n_synth,\n           replace = TRUE)\nsynth_meta <- df_original[indices, c(\"group\", \"unit\", \"time\")]\n\n# Combine the meta-info with the synthetic X's\ndf_synth <- cbind(synth_meta, synth_X)\n\nhead(df_synth)\n#>      group   unit time   X1_synth X2_synth  X3_synth   X4_synth\n#> 1029     2  G2_U2    9  0.2965446 3.488621 2.0876565  0.9336342\n#> 2279     3 G3_U14   19 -0.8903883 5.341148 3.4134562  0.4857101\n#> 1885     2 G2_U45    5  0.6841892 2.536933 0.5329199  1.3494999\n#> 2251     3 G3_U13   11 -1.5123574 8.127288 0.4861639  3.2120501\n#> 1160     2  G2_U8   20  0.2855353 3.219903 2.1439338 -0.3848253\n#> 2222     3 G3_U12    2 -1.1651872 5.717179 2.7372298  0.6252287\n# Original\norig_means  <- colMeans(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")])\norig_sds    <- apply(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], 2, sd)\norig_skew   <-\n    apply(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], 2, skewness)\norig_kurt   <-\n    apply(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")], 2, kurtosis)\n\n# Synthetic\nsynth_means <-\n    colMeans(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")])\nsynth_sds   <-\n    apply(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")], 2, sd)\nsynth_skew  <-\n    apply(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")], 2, skewness)\nsynth_kurt  <-\n    apply(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")], 2, kurtosis)\n\ncat(\n    \"### Means ###\\nOriginal:\",\n    round(orig_means, 3),\n    \"\\nSynthetic:\",\n    round(synth_means, 3),\n    \"\\n\\n\"\n)\n#> ### Means ###\n#> Original: -0.652 7.764 2.383 1.312 \n#> Synthetic: -0.642 7.741 2.372 1.287\n\ncat(\n    \"### SDs ###\\nOriginal:\",\n    round(orig_sds, 3),\n    \"\\nSynthetic:\",\n    round(synth_sds, 3),\n    \"\\n\\n\"\n)\n#> ### SDs ###\n#> Original: 1.448 4.937 1.16 1.102 \n#> Synthetic: 1.449 5.046 1.159 1.102\n\ncat(\n    \"### Skewness ###\\nOriginal:\",\n    round(orig_skew, 3),\n    \"\\nSynthetic:\",\n    round(synth_skew, 3),\n    \"\\n\\n\"\n)\n#> ### Skewness ###\n#> Original: -0.352 1.361 -0.314 0.944 \n#> Synthetic: -0.349 1.353 -0.317 1.017\n\ncat(\n    \"### Kurtosis ###\\nOriginal:\",\n    round(orig_kurt, 3),\n    \"\\nSynthetic:\",\n    round(synth_kurt, 3),\n    \"\\n\\n\"\n)\n#> ### Kurtosis ###\n#> Original: -0.769 2.628 -0.609 1.114 \n#> Synthetic: -0.744 2.465 -0.619 1.381\ncat(\"Original correlation:\\n\")\n#> Original correlation:\nround(cor(df_original[, c(\"X1\", \"X2\", \"X3\", \"X4\")]), 3)\n#>        X1     X2     X3     X4\n#> X1  1.000 -0.687  0.395 -0.704\n#> X2 -0.687  1.000  0.130  0.753\n#> X3  0.395  0.130  1.000 -0.366\n#> X4 -0.704  0.753 -0.366  1.000\n\ncat(\"\\nSynthetic correlation:\\n\")\n#> \n#> Synthetic correlation:\nround(cor(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")]), 3)\n#>          X1_synth X2_synth X3_synth X4_synth\n#> X1_synth    1.000   -0.562    0.288   -0.565\n#> X2_synth   -0.562    1.000    0.195    0.636\n#> X3_synth    0.288    0.195    1.000   -0.346\n#> X4_synth   -0.565    0.636   -0.346    1.000\npar(mfrow = c(2, 2))\nvars <- c(\"X1\", \"X2\", \"X3\", \"X4\")\nfor (i in seq_along(vars)) {\n    hist(\n        df_original[[vars[i]]],\n        probability = TRUE,\n        breaks = 30,\n        main = paste(\"Original\", vars[i]),\n        col = rgb(1, 0, 0, 0.5)\n    )\n    hist(\n        df_synth[[paste0(vars[i], \"_synth\")]],\n        probability = TRUE,\n        breaks = 30,\n        main = paste(\"Synthetic\", vars[i]),\n        col = rgb(0, 0, 1, 0.5),\n        add = TRUE\n    )\n    legend(\n        \"topright\",\n        legend = c(\"Original\", \"Synthetic\"),\n        fill = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5))\n    )\n}\nchart.Correlation(df_synth[, c(\"X1_synth\", \"X2_synth\", \"X3_synth\", \"X4_synth\")],\n                  histogram = TRUE, pch = 19)"},{"path":"replication-and-synthetic-data.html","id":"restricted-dataset","chapter":"43 Replication and Synthetic Data","heading":"43.3.2 Restricted Dataset","text":"Generate “Original” Complex DatasetWe’ll simulate hierarchical time-series :\\(G = 3\\) groups\\(G = 3\\) groups\\(N = 50\\) units per group\\(N = 50\\) units per group\\(T = 20\\) time points per unit\\(T = 20\\) time points per unitNonlinear relationships X1, X2, X3, X4.Nonlinear relationships X1, X2, X3, X4.point, imagine df_original lives partner’s server exported raw form.Manually Collect Summary Statistics (Inside Secure Server)Within secure environment, run commands get:Means, standard deviations variableMeans, standard deviations variableCorrelation matrixCorrelation matrixGroup distribution info (many groups, units, etc.)Group distribution info (many groups, units, etc.)relevant stats (min, max, skewness, kurtosis, etc.) might useAny relevant stats (min, max, skewness, kurtosis, etc.) might useBelow, ’ll directly code—reality, just write numbers save doc, export raw data.Example Output (numbers vary):Means, SDs variableMeans, SDs variable4×4 correlation matrix4×4 correlation matrixgroup_sizes: group 50×20 = 1000 rowsgroup_sizes: group 50×20 = 1000 rowsN_groups: 3N_groups: 3Simulating “Take ”Pretend data ’re allowed copy local machine:Reconstruct Covariance Matrix Distribution (Outside)Outside, now :mean vector (X1, X2, X3, X4)mean vector (X1, X2, X3, X4)Standard deviations eachStandard deviations eachA correlation matrix \\(R\\)correlation matrix \\(R\\)Basic knowledge: 3 groups, 50 units , 20 time points (however real data structured)Basic knowledge: 3 groups, 50 units , 20 time points (however real data structured)Build covariance \\(\\Sigma\\) correlation matrix SDs:Generate Synthetic Dataset Matching StatsWe’ll replicate hierarchical shape: 3 groups, 50 units, 20 time points. ’ll fill (X1, X2, X3, X4) sampling multivariate normal (means, Sigma).practice, might want add back random intercepts groups time trends manual stats include . However, overall means, SDs, correlation matrix, simplest approach assume single global distribution X1–X4.point, df_synth dataset shape (3 groups × 50 units × 20 time points = 3000 rows) drawn approximate distribution (matching partner’s means, SDs, correlation matrix).Alternatively, goal capture even skewness kurtosis, ’s bit complex.Evaluate & CompareIn reality, might comparison inside partner’s environment confirm synthetic data close match. demonstration, ’ll just compare directly .see synthetic dataset’s means, SDs, correlation matrix close manually collected values df_original.","code":"\n# Step 1: Generate \"df_original\" (what the partner owns internally)\nset.seed(123)  # For reproducibility\n\nG <- 3    # Number of groups\nN <- 50   # Units per group\nTt <- 20  # Time points per unit\n\ndf_list <- list()\n\nfor(g in 1:G) {\n  \n  # Group-level random intercept\n  group_intercept <- rnorm(1, mean = 0, sd = 1)\n  \n  for(u in 1:N) {\n    \n    # Unit-level random intercept\n    unit_intercept <- rnorm(1, mean = 0, sd = 0.5)\n    \n    # Simulate time points\n    time_points <- 1:Tt\n    \n    # Create some base patterns (X1)\n    X1_base <- group_intercept + unit_intercept + \n               sin(0.2 * time_points) +  # Nonlinear time pattern\n               rnorm(Tt, mean = 0, sd = 0.2)\n    \n    # X2 depends on polynomial in time, plus dependence on X1\n    X2_base <- (X1_base^2) + 0.5 * time_points + rnorm(Tt, 0, 0.3)\n    \n    # X3 depends on group intercept, negative correlation with X1, and log(time)\n    X3_base <- 1 + group_intercept - 0.3 * X1_base + log(time_points + 1) + \n               rnorm(Tt, mean = 0, sd = 0.2)\n    \n    # X4 depends on X1, X2, X3 in a more complex, nonlinear form\n    X4_base <- exp(0.1 * X1_base) + 0.2 * X2_base - 0.5 * X3_base + \n               rnorm(Tt, mean = 0, sd = 0.5)\n    \n    df_temp <- data.frame(\n      group = g,\n      unit  = paste0(\"G\", g, \"_U\", u),\n      time  = time_points,\n      X1    = X1_base,\n      X2    = X2_base,\n      X3    = X3_base,\n      X4    = X4_base\n    )\n    \n    df_list[[length(df_list) + 1]] <- df_temp\n  }\n}\n\ndf_original <- do.call(rbind, df_list)\nrow.names(df_original) <- NULL\n\n# Inspect the first rows (just for illustration)\nhead(df_original)\n#>   group  unit time           X1        X2        X3         X4\n#> 1     1 G1_U1    1 -0.165153398 0.2194743 0.9291383  0.3963423\n#> 2     1 G1_U1    2 -0.272044371 0.8553408 2.0535411 -0.3918278\n#> 3     1 G1_U1    3 -0.085064371 1.3197242 2.0929304 -0.3268864\n#> 4     1 G1_U1    4  0.384804697 1.6420667 1.7088991  0.6649585\n#> 5     1 G1_U1    5  0.258089835 2.8179465 2.0732799  0.7771992\n#> 6     1 G1_U1    6  0.003462448 3.0460239 2.2910647  0.4905209\n# Step 2: Summaries from \"df_original\" (pretend we can't take the actual df out)\nlibrary(dplyr)\n\n# For demonstration, we'll compute them here:\nstats_summary <- df_original %>%\n  summarise(\n    mean_X1 = mean(X1),\n    mean_X2 = mean(X2),\n    mean_X3 = mean(X3),\n    mean_X4 = mean(X4),\n    \n    sd_X1 = sd(X1),\n    sd_X2 = sd(X2),\n    sd_X3 = sd(X3),\n    sd_X4 = sd(X4)\n  )\n\n# Extract the correlation matrix among (X1, X2, X3, X4)\ncor_matrix <- cor(df_original[, c(\"X1\",\"X2\",\"X3\",\"X4\")])\n\n# Also note the group info\nunique_groups <- unique(df_original$group)\ngroup_sizes   <- table(df_original$group)\nN_groups      <- length(unique_groups)\nunit_example  <- length(unique(df_original$unit[df_original$group == 1]))\ntime_points   <- length(unique(df_original$time[df_original$group == 1 & df_original$unit == \"G1_U1\"]))\n\n# Print them out as if we wrote them down\nstats_summary\n#>     mean_X1  mean_X2  mean_X3  mean_X4    sd_X1    sd_X2    sd_X3    sd_X4\n#> 1 -0.651988 7.764111 2.383214 1.311789 1.448297 4.937284 1.160375 1.102264\ncor_matrix\n#>            X1         X2         X3         X4\n#> X1  1.0000000 -0.6865655  0.3948406 -0.7035793\n#> X2 -0.6865655  1.0000000  0.1301602  0.7532715\n#> X3  0.3948406  0.1301602  1.0000000 -0.3660620\n#> X4 -0.7035793  0.7532715 -0.3660620  1.0000000\ngroup_sizes\n#> \n#>    1    2    3 \n#> 1000 1000 1000\nN_groups\n#> [1] 3\nunit_example\n#> [1] 50\ntime_points\n#> [1] 20\n# (Pretend these are typed or copy-pasted from the secure environment)\n\n# Means and SDs:\nmeans <- c(stats_summary$mean_X1, stats_summary$mean_X2, \n           stats_summary$mean_X3, stats_summary$mean_X4)\nsds   <- c(stats_summary$sd_X1,   stats_summary$sd_X2,   \n           stats_summary$sd_X3,   stats_summary$sd_X4)\n\n# Correlation matrix:\nR <- cor_matrix\n\n# Hierarchical structure info:\nG_outside   <- N_groups  # 3\nN_outside   <- unit_example  # 50\nTt_outside  <- time_points   # 20\n# Step 3: Covariance matrix = diag(SDs) %*% R %*% diag(SDs)\nSigma <- diag(sds) %*% R %*% diag(sds)\nSigma\n#>            [,1]       [,2]       [,3]       [,4]\n#> [1,]  2.0975651 -4.9093928  0.6635565 -1.1231984\n#> [2,] -4.9093928 24.3767734  0.7457009  4.0994482\n#> [3,]  0.6635565  0.7457009  1.3464702 -0.4682079\n#> [4,] -1.1231984  4.0994482 -0.4682079  1.2149866\nlibrary(MASS)\n\nset.seed(999)  # Synthetic data seed (different from original)\n\ndf_synth <- data.frame()\n\nfor(g in 1:G_outside) {\n  for(u in 1:N_outside) {\n    for(t in 1:Tt_outside) {\n      # Draw one sample from the 4D normal\n      X_vector <- mvrnorm(n = 1, mu = means, Sigma = Sigma)\n      \n      df_temp <- data.frame(\n        group = g,\n        unit  = paste0(\"G\", g, \"_U\", u),\n        time  = t,\n        X1    = X_vector[1],\n        X2    = X_vector[2],\n        X3    = X_vector[3],\n        X4    = X_vector[4]\n      )\n      df_synth <- rbind(df_synth, df_temp)\n    }\n  }\n}\n\n# Check the first rows of the synthetic dataset\nhead(df_synth)\n#>   group  unit time          X1        X2       X3        X4\n#> 1     1 G1_U1    1  0.55555434  9.484499 3.729036 1.3156911\n#> 2     1 G1_U1    2 -1.33012636  9.275848 3.202148 0.1467821\n#> 3     1 G1_U1    3 -0.02332833 12.858857 3.425945 2.1621458\n#> 4     1 G1_U1    4  0.84430687  3.253564 1.536901 0.5493251\n#> 5     1 G1_U1    5  0.04440973  7.617650 1.552945 1.0185252\n#> 6     1 G1_U1    6 -2.70087059 13.626038 1.959473 2.6224373\n# Load required libraries\nlibrary(MASS)   # For multivariate normal correlation structure\nlibrary(sn)     # For skewed normal distribution\n\n# Step 1: Define Structure\nnum_groups <- 10   # Number of groups\nnum_timepoints <- 50  # Time series length per group\ntotal_samples <- num_groups * num_timepoints  # Total data points\n\n# Define statistical properties for each group\nset.seed(123)  # For reproducibility\n\ngroup_means <- rnorm(num_groups, mean=50, sd=10)  # Each group has a different mean\ngroup_variances <- runif(num_groups, 50, 150)  # Random variance per group\ngroup_skewness <- runif(num_groups, -1, 2)  # Skewness for each group\n# group_kurtosis <- runif(num_groups, 3, 6)  # Excess kurtosis for each group\n\n# Define AR(3) autocorrelation coefficients\nphi <- c(0.5, 0.3, 0.2)  # AR(3) coefficients (must sum to < 1 for stationarity)\np <- length(phi)  # Order of the AR process\n\n# Define correlation matrix for groups (cross-sectional correlation)\ngroup_corr_matrix <- matrix(0.5, nrow=num_groups, ncol=num_groups)  # Moderate correlation\ndiag(group_corr_matrix) <- 1  # Set diagonal to 1 for perfect self-correlation\n\n# Cholesky decomposition for group-level correlation\nchol_decomp <- chol(group_corr_matrix)\n\n# Step 2: Generate Hierarchical Time Series Data\ndata_list <- list()\n\nfor (g in 1:num_groups) {\n    # Generate base time series (AR(p) process)\n    ts_data <- numeric(num_timepoints)\n    \n    # Initialize first 'p' values randomly\n    ts_data[1:p] <- rnorm(p, mean=group_means[g], sd=sqrt(group_variances[g]))  \n    \n    for (t in (p+1):num_timepoints) {\n        # AR(p) process with multiple past values\n        ts_data[t] <- sum(phi * ts_data[(t-p):(t-1)]) + \n            rnorm(1, mean=0, sd=sqrt(group_variances[g] * (1 - sum(phi^2))))\n    }\n    \n    # Add skewness using skewed normal distribution\n    ts_data <- rsn(num_timepoints, xi=mean(ts_data), omega=sd(ts_data), alpha=group_skewness[g])\n    \n    # Store data in list\n    data_list[[g]] <- data.frame(\n        Group = g,\n        Time = 1:num_timepoints,\n        Value = ts_data\n    )\n}\n\n# Combine all group data into a single DataFrame\ndf <- do.call(rbind, data_list)\n\n# Step 3: Apply Cross-Group Correlation\n# Reshape the dataset for correlation application\nwide_df <- reshape(df, idvar=\"Time\", timevar=\"Group\", direction=\"wide\")\n\n# Apply correlation across groups at each time step\nfor (t in 1:num_timepoints) {\n    wide_df[t, -1] <- as.numeric(as.matrix(wide_df[t, -1]) %*% chol_decomp)\n}\n\n# Convert back to long format correctly\nlong_df <- reshape(wide_df, \n                   varying=colnames(wide_df)[-1],  # Select all group columns\n                   v.names=\"Value\", \n                   idvar=\"Time\", \n                   timevar=\"Group\", \n                   times=1:num_groups, \n                   direction=\"long\")\n\n# Ensure no unexpected columns\nlong_df <- long_df[, c(\"Time\", \"Group\", \"Value\")]\n\n\n\n# Display first few rows\nhead(long_df)\n# Step 5: Evaluate\n\n# A) Check Means & SDs\nsynth_means <- colMeans(df_synth[, c(\"X1\",\"X2\",\"X3\",\"X4\")])\nsynth_sds   <- apply(df_synth[, c(\"X1\",\"X2\",\"X3\",\"X4\")], 2, sd)\n\ncat(\"Original (Collected) Means:\\n\", round(means, 3), \"\\n\")\n#> Original (Collected) Means:\n#>  -0.652 7.764 2.383 1.312\ncat(\"Synthetic Means:\\n\", round(synth_means, 3), \"\\n\\n\")\n#> Synthetic Means:\n#>  -0.627 7.882 2.441 1.315\n\ncat(\"Original (Collected) SDs:\\n\", round(sds, 3), \"\\n\")\n#> Original (Collected) SDs:\n#>  1.448 4.937 1.16 1.102\ncat(\"Synthetic SDs:\\n\", round(synth_sds, 3), \"\\n\\n\")\n#> Synthetic SDs:\n#>  1.467 4.939 1.159 1.103\n\n# B) Check Correlation\nsynth_cor <- cor(df_synth[, c(\"X1\",\"X2\",\"X3\",\"X4\")])\ncat(\"Original (Collected) Correlation Matrix:\\n\")\n#> Original (Collected) Correlation Matrix:\nprint(round(R, 3))\n#>        X1     X2     X3     X4\n#> X1  1.000 -0.687  0.395 -0.704\n#> X2 -0.687  1.000  0.130  0.753\n#> X3  0.395  0.130  1.000 -0.366\n#> X4 -0.704  0.753 -0.366  1.000\ncat(\"\\nSynthetic Correlation Matrix:\\n\")\n#> \n#> Synthetic Correlation Matrix:\nprint(round(synth_cor, 3))\n#>        X1     X2     X3     X4\n#> X1  1.000 -0.693  0.402 -0.704\n#> X2 -0.693  1.000  0.114  0.756\n#> X3  0.402  0.114  1.000 -0.376\n#> X4 -0.704  0.756 -0.376  1.000\n# Histograms or density plots\npar(mfrow = c(2,2))\nhist(df_synth$X1, main=\"X1 Synthetic\", col=\"lightblue\", breaks=30)\nhist(df_synth$X2, main=\"X2 Synthetic\", col=\"lightblue\", breaks=30)\nhist(df_synth$X3, main=\"X3 Synthetic\", col=\"lightblue\", breaks=30)\nhist(df_synth$X4, main=\"X4 Synthetic\", col=\"lightblue\", breaks=30)\n\n# Pairwise correlation scatterplots\nlibrary(PerformanceAnalytics)\nchart.Correlation(df_synth[, c(\"X1\",\"X2\",\"X3\",\"X4\")], \n                  histogram=TRUE, pch=19)"},{"path":"replication-and-synthetic-data.html","id":"synthpop","chapter":"43 Replication and Synthetic Data","heading":"43.3.3 Synthpop","text":"easiest way create synthetic data use synthpop package.Open data can assessed utility two distinct ways:General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.General utilitySpecific utilityYou basically want lack--fit test non-significant.","code":"\nlibrary(synthpop)\nlibrary(tidyverse)\nlibrary(performance)\n\n# library(effectsize)\n# library(see)\n# library(patchwork)\n# library(knitr)\n# library(kableExtra)\n\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\n\nsynthpop::codebook.syn(iris)\n#> $tab\n#>       variable   class nmiss perctmiss ndistinct\n#> 1 Sepal.Length numeric     0         0        35\n#> 2  Sepal.Width numeric     0         0        23\n#> 3 Petal.Length numeric     0         0        43\n#> 4  Petal.Width numeric     0         0        22\n#> 5      Species  factor     0         0         3\n#>                             details\n#> 1                  Range: 4.3 - 7.9\n#> 2                    Range: 2 - 4.4\n#> 3                    Range: 1 - 6.9\n#> 4                  Range: 0.1 - 2.5\n#> 5 'setosa' 'versicolor' 'virginica'\n#> \n#> $labs\n#> NULL\n\nsyn_df <- syn(iris, seed = 3)\n#> \n#> Synthesis\n#> -----------\n#>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n\n# check for replciated uniques\nreplicated.uniques(syn_df, iris)\n#> Uniques and replicated uniques for  1  synthesised data set(s)\n#> \n#>  from keys:  Sepal.Length Sepal.Width Petal.Length Petal.Width Species \n#> \n#>                               Number from total     %\n#> Original                         148        150 98.67\n#> Synthetic                        137        150 91.33\n#> Synthetic uniques in original     17        150 11.33\n#> Replicated uniques                17        150 11.33\n#> To view possible exclusions from synthesised data set(s) check components\n#>   synU.rm (suggested for low-fidelity synthesis) or  repU.rm (for high fidelity)\n\n\n# remove replicated uniques and adds a FAKE_DATA label \n# (in case a person can see his or own data in \n# the replicated data by chance)\n\nsyn_df_sdc <- sdc(syn_df, iris, \n                  label = \"FAKE_DATA\",\n                  rm.replicated.uniques = T)\n#> no. of replicated uniques removed: 17\niris |> \n    GGally::ggpairs()\n\nsyn_df$syn |> \n    GGally::ggpairs()\nlm_ori <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = iris)\n# performance::check_model(lm_ori)\nsummary(lm_ori)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.96159 -0.23489  0.00077  0.21453  0.78557 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.24914    0.24797    9.07 7.04e-16 ***\n#> Sepal.Width   0.59552    0.06933    8.59 1.16e-14 ***\n#> Petal.Length  0.47192    0.01712   27.57  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3333 on 147 degrees of freedom\n#> Multiple R-squared:  0.8402, Adjusted R-squared:  0.838 \n#> F-statistic: 386.4 on 2 and 147 DF,  p-value: < 2.2e-16\n\nlm_syn <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df$syn)\n# performance::check_model(lm_syn)\nsummary(lm_syn)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = syn_df$syn)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.79165 -0.22790 -0.01448  0.15893  1.13360 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.96449    0.24538  12.081  < 2e-16 ***\n#> Sepal.Width   0.39214    0.06816   5.754  4.9e-08 ***\n#> Petal.Length  0.45267    0.01743  25.974  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3658 on 147 degrees of freedom\n#> Multiple R-squared:  0.8246, Adjusted R-squared:  0.8222 \n#> F-statistic: 345.6 on 2 and 147 DF,  p-value: < 2.2e-16\ncompare(syn_df, iris)\n# just like regular lm, but for synthetic data\nlm_syn <- lm.synds(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df)\ncompare(lm_syn, iris)\n#> \n#> Call used to fit models to the data:\n#> lm.synds(formula = Sepal.Length ~ Sepal.Width + Petal.Length, \n#>     data = syn_df)\n#> \n#> Differences between results based on synthetic and observed data:\n#>              Synthetic  Observed        Diff Std. coef diff CI overlap\n#> (Intercept)  2.9644900 2.2491402  0.71534988       2.884829  0.2640608\n#> Sepal.Width  0.3921429 0.5955247 -0.20338187      -2.933611  0.2516161\n#> Petal.Length 0.4526695 0.4719200 -0.01925058      -1.124602  0.7131064\n#> \n#> Measures for one synthesis and 3 coefficients\n#> Mean confidence interval overlap:  0.4095944\n#> Mean absolute std. coef diff:  2.314347\n#> \n#> Mahalanobis distance ratio for lack-of-fit (target 1.0): 3.08\n#> Lack-of-fit test: 9.23442; p-value 0.0263 for test that synthesis model is\n#> compatible with a chi-squared test with 3 degrees of freedom.\n#> \n#> Confidence interval plot:\n\n# summary(lm_syn)"},{"path":"high-performance-computing.html","id":"high-performance-computing","chapter":"44 High-Performance Computing","heading":"44 High-Performance Computing","text":"High-Performance Computing (HPC) refers use advanced computational systems techniques solve complex, resource-intensive problems impractical inefficient standard computing platforms. realm data analysis, HPC enables handling large-scale data processing, computationally intensive simulations, advanced statistical modeling, particularly fields like finance, marketing, bioinformatics, engineering.leveraging parallel processing, either multi-core processors single machine distributed nodes computing cluster, HPC environments dramatically reduce execution times expand analytical capabilities.R programming ecosystem, several packages frameworks facilitate parallel distributed computing. include:parallel: Built-R package basic parallel operations.foreach + doParallel: Enables parallel loops independent tasks.future: Provides unified API parallelization different backends.BiocParallel: Optimized parallel evaluation bioinformatics pipelines.sparklyr: Connects R Apache Spark big data processing.snow: simpler parallel computing Windows Linux clusters.However, effective HPC implementation requires just code parallelization. Testing, monitoring, estimating resource requirements, iterative scaling essential components maximize efficiency prevent costly mistakes.","code":""},{"path":"high-performance-computing.html","id":"best-practices-for-hpc-in-data-analysis","chapter":"44 High-Performance Computing","heading":"44.1 Best Practices for HPC in Data Analysis","text":"Test First Small ScaleBefore scaling analysis full production, begin small-scale tests. ensures code runs expected, also providing crucial information resource usage.Start small: Run analysis subset data (e.g., 1% 10% total).Measure resource usage:\nExecution time (wall clock time vs. CPU time).\nMemory footprint.\nCPU disk /O utilization.\nExecution time (wall clock time vs. CPU time).Memory footprint.CPU disk /O utilization.Record metrics:\nCreate logs structured reports future reference.\nCreate logs structured reports future reference.Example:\n’re modeling customer churn dataset 10 million records, start 100,000 profile behavior.Estimate Resource RequirementsUse insights small-scale tests extrapolate resources needed full analysis.Estimate CPU cores, memory, execution time requirements.Add overhead buffer:\nParallel tasks introduce communication overhead synchronization delays.\nReal-world data can higher complexity test data.\nParallel tasks introduce communication overhead synchronization delays.Real-world data can higher complexity test data.Guideline: small test consumes 1 GB RAM runs 10 minutes single core, 100x dataset may scale linearly. Communication costs, disk /O, parallel inefficiencies require different scaling factor.Parallelize StrategicallyNot parts analysis benefit equally parallelization. Identify prioritize computational bottlenecks.Analyze workflow:\nData ingestion: Can parallelize reading large files querying databases?\nTransformations: data wrangling tasks parallelizable?\nModeling/Training: Can independent model fits simulations distributed?\nData ingestion: Can parallelize reading large files querying databases?Transformations: data wrangling tasks parallelizable?Modeling/Training: Can independent model fits simulations distributed?Balance granularity:\nOverly fine-grained parallelization leads high communication overhead.\nCoarser tasks generally efficient parallel environments.\nOverly fine-grained parallelization leads high communication overhead.Coarser tasks generally efficient parallel environments.Tip: Use embarrassingly parallel strategies possible—tasks require minimal communication workers.Use Adequate Scheduling Queue SystemsIn HPC clusters, job scheduling systems manage resource allocation prioritize workloads.Common systems include:Slurm (Simple Linux Utility Resource Management)Slurm (Simple Linux Utility Resource Management)PBS (Portable Batch System)PBS (Portable Batch System)LSF (Load Sharing Facility)LSF (Load Sharing Facility)SGE (Sun Grid Engine)SGE (Sun Grid Engine)Best practices:Write job submission scripts specifying:\nWall time limits.\nMemory CPU requests.\nNode allocation (required).\nWrite job submission scripts specifying:Wall time limits.Wall time limits.Memory CPU requests.Memory CPU requests.Node allocation (required).Node allocation (required).Monitor jobs:\nExamine logs resource utilization (memory, time, CPU load).\nUse scheduler tools (e.g., sacct Slurm) assess historical performance.\nMonitor jobs:Examine logs resource utilization (memory, time, CPU load).Examine logs resource utilization (memory, time, CPU load).Use scheduler tools (e.g., sacct Slurm) assess historical performance.Use scheduler tools (e.g., sacct Slurm) assess historical performance.Example Slurm job script:Incremental ScalingResist temptation scale small tests directly massive production jobs.Iterate gradually:\nStart small jobs, progress medium-sized runs, scale full production.\nIterate gradually:Start small jobs, progress medium-sized runs, scale full production.Monitor /O overhead:\nParallel jobs often stress shared storage systems.\nOptimize data locality prefetching possible.\nMonitor /O overhead:Parallel jobs often stress shared storage systems.Parallel jobs often stress shared storage systems.Optimize data locality prefetching possible.Optimize data locality prefetching possible.Tip: Use asynchronous job submission (batchtools::submitJobs() future::future()) manage job batches efficiently.Documentation ReportingGood documentation facilitates reproducibility future optimization.Maintain structured report run, including:Input parameters: Dataset size, preprocessing steps, model parameters.Input parameters: Dataset size, preprocessing steps, model parameters.Cluster specification: Number nodes, CPUs per node, memory allocations.Cluster specification: Number nodes, CPUs per node, memory allocations.Execution logs: Total run time, CPU utilization, memory usage.Execution logs: Total run time, CPU utilization, memory usage.Software environment: R version, package versions, job scheduler version.Software environment: R version, package versions, job scheduler version.Template documentation:","code":"#!/bin/bash\n#SBATCH --job-name=churn_model\n#SBATCH --output=churn_model_%j.out\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n\nmodule load R/4.2.0\nRscript run_churn_model.R"},{"path":"high-performance-computing.html","id":"example-workflow-in-r","chapter":"44 High-Performance Computing","heading":"44.2 Example Workflow in R","text":"conceptual R workflow demonstrating :Load data define test subset.Load data define test subset.Perform computations parallel test subset using parallel foreach approach.Perform computations parallel test subset using parallel foreach approach.Measure resource usage (basic approach R; deeper HPC environment metrics, rely cluster’s job scheduler logs).Measure resource usage (basic approach R; deeper HPC environment metrics, rely cluster’s job scheduler logs).Extrapolate resource usage.Extrapolate resource usage.Submit scaled-job new allocations.Submit scaled-job new allocations.","code":""},{"path":"high-performance-computing.html","id":"recommendations","chapter":"44 High-Performance Computing","heading":"44.3 Recommendations","text":"Incremental TestingAlways begin small fraction data validate HPC pipeline catch issues early.Collect critical metrics:\nRun time (wall clock time CPU time).\nCPU utilization (requested cores working efficiently?).\nMemory usage (peak RAM consumption).\nRun time (wall clock time CPU time).CPU utilization (requested cores working efficiently?).Memory usage (peak RAM consumption).Use tools system.time(), profvis::profvis(), Rprof() R profile code locally scaling.Resource AllocationUse test metrics guide resource requests (memory, cores, wall time).Avoid:\n-requesting: jobs may fail due --memory (OOM) errors timeouts.\n-requesting: results longer queue times, resource underutilization, potential policy violations shared systems.\n-requesting: jobs may fail due --memory (OOM) errors timeouts.-requesting: results longer queue times, resource underutilization, potential policy violations shared systems.Rule thumb:\nRequest slightly memory observed usage (e.g., 10-20% buffer).\nRequest wall time based observed run time, plus additional safety margin (e.g., 15-30%).\nRequest slightly memory observed usage (e.g., 10-20% buffer).Request wall time based observed run time, plus additional safety margin (e.g., 15-30%).Parallelization Strategy\nIdentify tasks can embarrassingly parallel:\nMonte Carlo simulations.\nBootstrap resampling.\nModel fitting independent data partitions.\n\nFocus minimizing communication overhead:\nSend essential data processes.\nUse shared-memory parallelism possible avoid data duplication.\ndistributed nodes, serialize compress data transmission feasible.\n\nParallelization StrategyIdentify tasks can embarrassingly parallel:\nMonte Carlo simulations.\nBootstrap resampling.\nModel fitting independent data partitions.\nMonte Carlo simulations.Bootstrap resampling.Model fitting independent data partitions.Focus minimizing communication overhead:\nSend essential data processes.\nUse shared-memory parallelism possible avoid data duplication.\ndistributed nodes, serialize compress data transmission feasible.\nSend essential data processes.Use shared-memory parallelism possible avoid data duplication.distributed nodes, serialize compress data transmission feasible.Monitoring Logging\nUse HPC job scheduler logs track job performance:\nSlurm: sacct, scontrol show job, seff.\nPBS/SGE: check standard output/error logs resource summaries.\n\nCapture R logs console output:\nDirect output log files using sink() redirecting standard output job script.\n\nRecord:\nStart end times.\nMemory CPU metrics.\nWarnings error messages.\n\nMonitoring LoggingUse HPC job scheduler logs track job performance:\nSlurm: sacct, scontrol show job, seff.\nPBS/SGE: check standard output/error logs resource summaries.\nSlurm: sacct, scontrol show job, seff.PBS/SGE: check standard output/error logs resource summaries.Capture R logs console output:\nDirect output log files using sink() redirecting standard output job script.\nDirect output log files using sink() redirecting standard output job script.Record:\nStart end times.\nMemory CPU metrics.\nWarnings error messages.\nStart end times.Memory CPU metrics.Warnings error messages.Scaling\nlarge-scale runs (e.g., 100x initial test), jump small-scale full-scale directly.\nRun intermediate-scale tests (e.g., 10x, 50x).\nConfirm resource usage scales expected.\n\nWatch non-linear effects:\nIncreased /O overhead parallel file reads/writes.\nCommunication overhead distributed tasks.\nLoad balancing inefficiencies (stragglers can delay job completion).\n\nScalingFor large-scale runs (e.g., 100x initial test), jump small-scale full-scale directly.\nRun intermediate-scale tests (e.g., 10x, 50x).\nConfirm resource usage scales expected.\nRun intermediate-scale tests (e.g., 10x, 50x).Confirm resource usage scales expected.Watch non-linear effects:\nIncreased /O overhead parallel file reads/writes.\nCommunication overhead distributed tasks.\nLoad balancing inefficiencies (stragglers can delay job completion).\nIncreased /O overhead parallel file reads/writes.Communication overhead distributed tasks.Load balancing inefficiencies (stragglers can delay job completion).Future-Proofing\nHPC hardware software evolve rapidly:\nFaster networks (InfiniBand, RDMA).\nImproved schedulers resource managers.\nContainerization (Docker, Singularity) reproducible environments.\n\nRegularly test validate reproducibility :\nSoftware/package updates.\nHardware upgrades.\nHPC policy changes (e.g., new quotas job priorities).\n\nFuture-ProofingHPC hardware software evolve rapidly:\nFaster networks (InfiniBand, RDMA).\nImproved schedulers resource managers.\nContainerization (Docker, Singularity) reproducible environments.\nFaster networks (InfiniBand, RDMA).Improved schedulers resource managers.Containerization (Docker, Singularity) reproducible environments.Regularly test validate reproducibility :\nSoftware/package updates.\nHardware upgrades.\nHPC policy changes (e.g., new quotas job priorities).\nSoftware/package updates.Hardware upgrades.HPC policy changes (e.g., new quotas job priorities).","code":""},{"path":"high-performance-computing.html","id":"demonstration","chapter":"44 High-Performance Computing","heading":"44.4 Demonstration","text":"Runtime Plot\nSmall Dataset: methods fast mostly seeing overhead differences. differences bar heights reflect parallel framework handles overhead.\nLarge Dataset: see separation. instance, parLapply 2 cores might slower 1 core due overhead. 4 cores, might improve , depending overhead actual workload balance .\nRuntime PlotSmall Dataset: methods fast mostly seeing overhead differences. differences bar heights reflect parallel framework handles overhead.Small Dataset: methods fast mostly seeing overhead differences. differences bar heights reflect parallel framework handles overhead.Large Dataset: see separation. instance, parLapply 2 cores might slower 1 core due overhead. 4 cores, might improve , depending overhead actual workload balance .Large Dataset: see separation. instance, parLapply 2 cores might slower 1 core due overhead. 4 cores, might improve , depending overhead actual workload balance .Memory Usage Plot\nbars hover around 0.02 MB measuring size returned result, -memory size large dataset. function object.size(result) capturing total RAM usage entire parallel job.\nMemory Usage PlotAll bars hover around 0.02 MB measuring size returned result, -memory size large dataset. function object.size(result) capturing total RAM usage entire parallel job.","code":"\n###############################################################\n# HPC Workflow Demo in R: Scaling Resources and Parallelism\n###############################################################\n\n\n# -- 1. Libraries --\nlibrary(parallel)       # base parallel\nlibrary(doParallel)     # foreach backend\nlibrary(foreach)        # parallel loops\nlibrary(future)         # futures\nlibrary(future.apply)   # apply with futures\nlibrary(ggplot2)        # plotting results\n\n# Optional: install.packages(\"pryr\") for memory usage (optional)\n# library(pryr)\n\n###############################################################\n# -- 2. Generate or Load Data (Small and Large for Scaling) --\n###############################################################\n\nset.seed(42)\n\nn_small <- 1e4    # Small dataset (test)\nn_large <- 1e6    # Large dataset (scale-up demo)\n\ngenerate_data <- function(n) {\n    data.frame(\n        id = 1:n,\n        x = rnorm(n, mean = 50, sd = 10),\n        y = rnorm(n, mean = 100, sd = 25)\n    )\n}\n\ndata_small <- generate_data(n_small)\ndata_large <- generate_data(n_large)\n\n###############################################################\n# -- 3. Preprocessing Function --\n###############################################################\n\nclean_and_transform <- function(df) {\n    df$dist <- sqrt(df$x ^ 2 + df$y ^ 2)\n    return(df)\n}\n\ndata_small_clean <- clean_and_transform(data_small)\ndata_large_clean <- clean_and_transform(data_large)\n\n###############################################################\n# -- 4. Heavy Computation (Simulated Workload) --\n###############################################################\n\nheavy_computation <- function(df, reps = 500) {\n    replicate(reps, sum(df$dist))\n}\n\n###############################################################\n# -- 5. Resource Measurement Helper Function --\n###############################################################\n\nmeasure_resources <-\n    function(expr, approach_name, data_size, cores) {\n        start_time <- Sys.time()\n        \n        # baseline_mem <- pryr::mem_used()  # If pryr is available\n        result <- eval(expr)\n        end_time <- Sys.time()\n        \n        time_elapsed <-\n            as.numeric(difftime(end_time, start_time, units = \"secs\"))\n        # used_mem <- as.numeric(pryr::mem_used() - baseline_mem) / 1e6  # MB\n        \n        # If pryr isn't available, approximate:\n        used_mem <- as.numeric(object.size(result)) / 1e6  # MB\n        \n        return(\n            data.frame(\n                Approach = approach_name,\n                Data_Size = data_size,\n                Cores = cores,\n                Time_Sec = time_elapsed,\n                Memory_MB = round(used_mem, 2)\n            )\n        )\n    }\n\n###############################################################\n# -- 6. Define and Run Parallel Approaches --\n###############################################################\n\n# Collect results in a dataframe\nresults <- data.frame()\n\n### 6.1 Single Core (Baseline)\nres_single_small <- measure_resources(\n    expr = quote(heavy_computation(data_small_clean)),\n    approach_name = \"SingleCore\",\n    data_size = nrow(data_small_clean),\n    cores = 1\n)\n\nres_single_large <- measure_resources(\n    expr = quote(heavy_computation(data_large_clean)),\n    approach_name = \"SingleCore\",\n    data_size = nrow(data_large_clean),\n    cores = 1\n)\n\nresults <- rbind(results, res_single_small, res_single_large)\n\n# clusterExport(cl, varlist = c(\"heavy_computation\"))\n\n\n### 6.2 Base Parallel (Cross-Platform): parLapply (Cluster Based)\ncores_to_test <- c(2, 4)\n\nfor (cores in cores_to_test) {\n    cl <- makeCluster(cores)\n    \n    # Export both the heavy function and the datasets\n    clusterExport(cl,\n                  varlist = c(\n                      \"heavy_computation\",\n                      \"data_small_clean\",\n                      \"data_large_clean\"\n                  ))\n    \n    res_par_small <- measure_resources(\n        expr = quote({\n            parLapply(\n                cl = cl,\n                X = 1:cores,\n                fun = function(i)\n                    heavy_computation(data_small_clean)\n            )\n        }),\n        approach_name = \"parLapply\",\n        data_size = nrow(data_small_clean),\n        cores = cores\n    )\n    \n    res_par_large <- measure_resources(\n        expr = quote({\n            parLapply(\n                cl = cl,\n                X = 1:cores,\n                fun = function(i)\n                    heavy_computation(data_large_clean)\n            )\n        }),\n        approach_name = \"parLapply\",\n        data_size = nrow(data_large_clean),\n        cores = cores\n    )\n    \n    stopCluster(cl)\n    \n    results <- rbind(results, res_par_small, res_par_large)\n}\n\n\n\n### 6.3 foreach + doParallel\nfor (cores in cores_to_test) {\n    cl <- makeCluster(cores)\n    registerDoParallel(cl)\n    \n    clusterExport(cl,\n                  varlist = c(\n                      \"heavy_computation\",\n                      \"data_small_clean\",\n                      \"data_large_clean\"\n                  ))\n    registerDoParallel(cl)\n    \n    # Small dataset\n    res_foreach_small <- measure_resources(\n        expr = quote({\n            foreach(i = 1:cores, .combine = c) %dopar% {\n                heavy_computation(data_small_clean)\n            }\n        }),\n        approach_name = \"foreach_doParallel\",\n        data_size = nrow(data_small_clean),\n        cores = cores\n    )\n    \n    # Large dataset\n    res_foreach_large <- measure_resources(\n        expr = quote({\n            foreach(i = 1:cores, .combine = c) %dopar% {\n                heavy_computation(data_large_clean)\n            }\n        }),\n        approach_name = \"foreach_doParallel\",\n        data_size = nrow(data_large_clean),\n        cores = cores\n    )\n    \n    stopCluster(cl)\n    \n    results <- rbind(results, res_foreach_small, res_foreach_large)\n}\n\n### 6.4 future + future.apply\nfor (cores in cores_to_test) {\n    plan(multicore, workers = cores)  # multicore only works on Unix; use multisession on Windows\n    \n    # Small dataset\n    res_future_small <- measure_resources(\n        expr = quote({\n            future_lapply(1:cores, function(i)\n                heavy_computation(data_small_clean))\n        }),\n        approach_name = \"future_lapply\",\n        data_size = nrow(data_small_clean),\n        cores = cores\n    )\n    \n    # Large dataset\n    res_future_large <- measure_resources(\n        expr = quote({\n            future_lapply(1:cores, function(i)\n                heavy_computation(data_large_clean))\n        }),\n        approach_name = \"future_lapply\",\n        data_size = nrow(data_large_clean),\n        cores = cores\n    )\n    \n    results <- rbind(results, res_future_small, res_future_large)\n}\n\n# Reset plan to sequential\nplan(sequential)\n\n###############################################################\n# -- 7. Summarize and Plot Results --\n###############################################################\n\n# Print table\nprint(results)\n#>              Approach Data_Size Cores   Time_Sec Memory_MB\n#> 1          SingleCore     10000     1 0.01581287      0.00\n#> 2          SingleCore   1000000     1 1.43626785      0.00\n#> 3           parLapply     10000     2 0.01978612      0.01\n#> 4           parLapply   1000000     2 1.42945695      0.01\n#> 5           parLapply     10000     4 0.01906514      0.02\n#> 6           parLapply   1000000     4 1.46021295      0.02\n#> 7  foreach_doParallel     10000     2 0.06499314      0.01\n#> 8  foreach_doParallel   1000000     2 1.42544699      0.01\n#> 9  foreach_doParallel     10000     4 0.05926299      0.02\n#> 10 foreach_doParallel   1000000     4 1.43057013      0.02\n#> 11      future_lapply     10000     2 0.11854696      0.01\n#> 12      future_lapply   1000000     2 2.88425088      0.01\n#> 13      future_lapply     10000     4 0.22660494      0.02\n#> 14      future_lapply   1000000     4 5.74241900      0.02\n\n# Save to CSV\n# write.csv(results, \"HPC_parallel_results.csv\", row.names = FALSE)\n\n# Plot Time vs. Data Size / Cores\nggplot(results, aes(x = as.factor(Cores), y = Time_Sec, fill = Approach)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    facet_wrap( ~ Data_Size, scales = \"free\", labeller = label_both) +\n    labs(title = \"Runtime by Cores, Approach, and Data Size\",\n         x = \"Number of Cores\",\n         y = \"Time (Seconds)\") +\n    theme_minimal()\n\n# Plot Memory Usage\nggplot(results, aes(x = as.factor(Cores), y = Memory_MB, fill = Approach)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    facet_wrap(~ Data_Size, scales = \"free\", labeller = label_both) +\n    labs(title = \"Memory Usage by Cores, Approach, and Data Size\",\n         x = \"Number of Cores\",\n         y = \"Memory (MB)\") +\n    theme_minimal()"},{"path":"appendix.html","id":"appendix","chapter":"A Appendix","heading":"A Appendix","text":"","code":""},{"path":"appendix.html","id":"git","chapter":"A Appendix","heading":"A.1 Git","text":"appendix provides concise reference essential Git concepts commands, tailored data analysts researchers managing code collaboration. extended learning, explore following resources:Git Cheat Sheet (PDF)Git Cheat Sheets LanguagesInteractive Git TutorialVisual Git Cheat SheetHappy Git R (R Users)","code":""},{"path":"appendix.html","id":"basic-setup-1","chapter":"A Appendix","heading":"A.1.1 Basic Setup","text":"Configure Git environment using git config command:Set name email (used commits):\ngit config --global user.name \"Name\"\ngit config --global user.email \".email@example.com\"Set name email (used commits):Set preferred text editor (e.g., writing commit messages):\ngit config --global core.editor \"code --wait\"  # VS CodeSet preferred text editor (e.g., writing commit messages):","code":"git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"git config --global core.editor \"code --wait\"  # VS Code"},{"path":"appendix.html","id":"creating-a-repository","chapter":"A Appendix","heading":"A.1.2 Creating a Repository","text":"create new Git repository project directory:creates .git directory Git stores version control information.","code":"git init"},{"path":"appendix.html","id":"tracking-changes","chapter":"A Appendix","heading":"A.1.3 Tracking Changes","text":"Git tracks changes three-tier structure:Working Directory: local folder files.Staging Area: prepare changes committing.Local Repository: stores committed snapshots code.Common commands:Check status:\ngit statusCheck status:Add files staging area:\ngit add filename\ngit add .  # Add changesAdd files staging area:Commit staged changes:\ngit commit -m \"brief message describing change\"Commit staged changes:","code":"git statusgit add filename\ngit add .  # Add all changesgit commit -m \"A brief message describing the change\""},{"path":"appendix.html","id":"viewing-history-and-changes","chapter":"A Appendix","heading":"A.1.4 Viewing History and Changes","text":"Show changes yet staged:\ngit diffShow changes yet staged:Show committed changes:\ngit logShow committed changes:Restore previous versions files:\ngit checkout HEAD filename  # Restore last committed version\ngit checkout <commit-id> filename  # Restore specific commitRestore previous versions files:","code":"git diffgit loggit checkout HEAD filename  # Restore last committed version\ngit checkout <commit-id> filename  # Restore from specific commit"},{"path":"appendix.html","id":"ignoring-files","chapter":"A Appendix","heading":"A.1.5 Ignoring Files","text":"prevent certain files tracked Git, create .gitignore file. example:View contents using:\ncat .gitignoreView contents using:","code":"# .gitignore\n*.dat\nresults/cat .gitignore"},{"path":"appendix.html","id":"remote-repositories","chapter":"A Appendix","heading":"A.1.6 Remote Repositories","text":"Git supports linking local remote repositories (e.g., GitHub):Add remote:\ngit remote add origin https://github.com/yourname/repo.gitAdd remote:Push changes remote:\ngit push origin main  # 'master' depending default branchPush changes remote:Pull changes remote:\ngit pull origin mainPull changes remote:","code":"git remote add origin https://github.com/yourname/repo.gitgit push origin main  # or 'master' depending on default branchgit pull origin main"},{"path":"appendix.html","id":"collaboration","chapter":"A Appendix","heading":"A.1.7 Collaboration","text":"Clone remote repository:\ngit clone https://github.com/username/repository.git\ncreates local copy sets remote named origin.Clone remote repository:creates local copy sets remote named origin.","code":"git clone https://github.com/username/repository.git"},{"path":"appendix.html","id":"branching-and-merging","chapter":"A Appendix","heading":"A.1.8 Branching and Merging","text":"Create switch new branch:\ngit checkout -b new-branch-nameCreate switch new branch:Switch back main branch:\ngit checkout mainSwitch back main branch:Merge another branch current one:\ngit merge feature-branchMerge another branch current one:","code":"git checkout -b new-branch-namegit checkout maingit merge feature-branch"},{"path":"appendix.html","id":"handling-conflicts","chapter":"A Appendix","heading":"A.1.9 Handling Conflicts","text":"Merge conflicts occur multiple changes affect lines file. Git :Mark conflict file.Require manual resolution committing.Always review test code resolving conflicts.","code":""},{"path":"appendix.html","id":"licensing","chapter":"A Appendix","heading":"A.1.10 Licensing","text":"Understanding software licensing essential open-source collaboration:GPL (General Public License): Requires derivative software also GPL-licensed.Creative Commons: Offers flexible combinations attribution, sharing, commercial use restrictions.Choose licenses aligned intended use contributions.","code":""},{"path":"appendix.html","id":"citing-repositories","chapter":"A Appendix","heading":"A.1.11 Citing Repositories","text":"guide citation practices:Include CITATION file repository.Provide preferred citation formats (e.g., BibTeX, DOI).helps others acknowledge work academic professional settings.","code":""},{"path":"appendix.html","id":"hosting-and-legal-considerations","chapter":"A Appendix","heading":"A.1.12 Hosting and Legal Considerations","text":"Whether hosted GitHub, GitLab, institutional servers:Respect intellectual property.Avoid storing sensitive personal data version control.Follow organizational institutional data security policies.","code":""},{"path":"appendix.html","id":"short-cut","chapter":"A Appendix","heading":"A.2 Short-cut","text":"shortcuts probably remember working R. Even though might take bit time learn use second nature, save lot time.\nJust like learning another language, speak practice , comfortable speaking .Sometimes can’t stage folder ’s large. case, use Terminal pane Rstudio type git add -stage changes commit push like usual.","code":""},{"path":"appendix.html","id":"function-short-cut","chapter":"A Appendix","heading":"A.3 Function short-cut","text":"apply one function data create new variable: mutate(mod=map(data,function))\ninstead using 1:length(object): (seq_along(object))\napply multiple function: map_dbl\napply multiple function multiple variables:map2autoplot(data) plot times series datamod_tidy = linear(reg) %>% set_engine('lm') %>% fit(price ~ ., data=data) fit lm model. also fit models (stan, spark, glmnet, keras)Sometimes, data-masking able recognize whether ’re calling environment data variables. bypass , use .data$variable .env$variable. example data %>% mutate(x=.env$variable/.data$variableProblems data-masking:Unexpected masking data-var: Use .data .env disambiguate\nData-var cant get :\nTunnel data-var {{}} + Subset .data [[]]\nUnexpected masking data-var: Use .data .env disambiguateData-var cant get :Tunnel data-var {{}} + Subset .data [[]]Passing Data-variables argumentsTrouble selection:","code":"\nlibrary(\"dplyr\")\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{{var}}\":=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings\n}\n\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{var}\":=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions\n}\nlibrary(\"purrr\")\nname <- c(\"mass\",\"height\")\nstarwars %>% select(name) # Data-var. Here you are referring to variable named \"name\"\n\nstarwars %>% select(all_of((name))) # use all_of() to disambiguate when \n\naverages <- function(data,vars){ # take character vectors with all_of()\n    data %>%\n        select(all_of(vars)) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)\n\n\n# Another way\naverages <- function(data,vars){ # Tunnel selectiosn with {{}}\n    data %>%\n        select({{vars}}) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)"},{"path":"appendix.html","id":"citation","chapter":"A Appendix","heading":"A.4 Citation","text":"cite R packages used session, following code prints BibTeX-formatted citations:may wish redirect output .bib file integration LaTeX R Markdown documents using writeLines().","code":"\n# List all non-base packages loaded in the session\npackages <- ls(sessionInfo()$loadedOnly)\n\n# Print BibTeX citations for each package\nfor (pkg in packages) {\n  print(toBibtex(citation(pkg)))\n}"},{"path":"appendix.html","id":"install-all-necessary-packages-on-your-local-machine","chapter":"A Appendix","heading":"A.5 Install All Necessary Packages on Your Local Machine","text":"replicate environment used book session another machine, can follow steps.","code":""},{"path":"appendix.html","id":"step-1-export-installed-packages-from-your-current-session","chapter":"A Appendix","heading":"A.5.1 Step 1: Export Installed Packages from Your Current Session","text":"","code":"\n# Get all installed packages\ninstalled <- as.data.frame(installed.packages())\n\n# Preview the installed packages\nhead(installed)\n#>             Package                            LibPath Version Priority\n#> abind         abind C:/Program Files/R/R-4.4.3/library   1.4-8     <NA>\n#> ade4           ade4 C:/Program Files/R/R-4.4.3/library  1.7-23     <NA>\n#> ADGofTest ADGofTest C:/Program Files/R/R-4.4.3/library     0.3     <NA>\n#> admisc       admisc C:/Program Files/R/R-4.4.3/library    0.38     <NA>\n#> AER             AER C:/Program Files/R/R-4.4.3/library  1.2-14     <NA>\n#> afex           afex C:/Program Files/R/R-4.4.3/library   1.4-1     <NA>\n#>                                                                                          Depends\n#> abind                                                                               R (>= 1.5.0)\n#> ade4                                                                                R (>= 3.5.0)\n#> ADGofTest                                                                                   <NA>\n#> admisc                                                                              R (>= 3.5.0)\n#> AER       R (>= 3.0.0), car (>= 2.0-19), lmtest, sandwich (>= 2.4-0),\\nsurvival (>= 2.37-5), zoo\n#> afex                                                               R (>= 3.5.0), lme4 (>= 1.1-8)\n#>                                                                                   Imports\n#> abind                                                                      methods, utils\n#> ade4                  graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\\nRcpp\n#> ADGofTest                                                                            <NA>\n#> admisc                                                                            methods\n#> AER                                                             stats, Formula (>= 0.2-0)\n#> afex      pbkrtest (>= 0.4-1), lmerTest (>= 3.0-0), car, reshape2,\\nstats, methods, utils\n#>                     LinkingTo\n#> abind                    <NA>\n#> ade4      Rcpp, RcppArmadillo\n#> ADGofTest                <NA>\n#> admisc                   <NA>\n#> AER                      <NA>\n#> afex                     <NA>\n#>                                                                                                                                                                                                                                                                                                                                                                                                  Suggests\n#> abind                                                                                                                                                                                                                                                                                                                                                                                                <NA>\n#> ade4                                                                                                                                                                                                                      ade4TkGUI, adegraphics, adephylo, adespatial, ape, CircStats,\\ndeldir, lattice, spdep, splancs, waveslim, progress, foreach,\\nparallel, doParallel, iterators, knitr, rmarkdown\n#> ADGofTest                                                                                                                                                                                                                                                                                                                                                                                            <NA>\n#> admisc                                                                                                                                                                                                                                                                                                                                                                                       QCA (>= 3.7)\n#> AER                                                                                                                                    boot, dynlm, effects, fGarch, forecast, foreign, ineq,\\nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\\nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\\nscatterplot3d, strucchange, systemfit (>= 1.1-20), truncreg,\\ntseries, urca, vars\n#> afex      emmeans (>= 1.4), coin, xtable, parallel, plyr, optimx,\\nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\\nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\\npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\\ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\\nstatmod, performance (>= 0.7.2), see (>= 0.6.4), ez,\\nggResidpanel, grid, vdiffr\n#>           Enhances            License License_is_FOSS License_restricts_use\n#> abind         <NA> MIT + file LICENSE            <NA>                  <NA>\n#> ade4          <NA>         GPL (>= 2)            <NA>                  <NA>\n#> ADGofTest     <NA>                GPL            <NA>                  <NA>\n#> admisc        <NA>         GPL (>= 3)            <NA>                  <NA>\n#> AER           <NA>      GPL-2 | GPL-3            <NA>                  <NA>\n#> afex          <NA>         GPL (>= 2)            <NA>                  <NA>\n#>           OS_type MD5sum NeedsCompilation Built\n#> abind        <NA>   <NA>               no 4.4.1\n#> ade4         <NA>   <NA>              yes 4.4.3\n#> ADGofTest    <NA>   <NA>             <NA> 4.4.0\n#> admisc       <NA>   <NA>              yes 4.4.3\n#> AER          <NA>   <NA>               no 4.4.3\n#> afex         <NA>   <NA>               no 4.4.3\n\n# Export the list to a CSV file\nwrite.csv(installed$Package, file = file.path(getwd(), \"installed.csv\"), row.names = FALSE)"},{"path":"appendix.html","id":"step-2-install-packages-on-a-new-machine","chapter":"A Appendix","heading":"A.5.2 Step 2: Install Packages on a New Machine","text":"transferred installed.csv file new machine, run following code install missing packages.⚠️ Note: approach assumes packages available CRAN. packages GitHub Bioconductor, use devtools::install_github() BiocManager::install() appropriate.approach ensures reproducible computational environment, essential robust data analysis collaboration.","code":"\n# Read the list of required packages\nrequired <- read.csv(\"installed.csv\", stringsAsFactors = FALSE)$Package\n\n# Get the list of already installed packages on the current machine\ncurrent <- installed.packages()[, \"Package\"]\n\n# Identify packages that are not yet installed\nmissing <- setdiff(required, current)\n\n# Install the missing packages\ninstall.packages(missing)"},{"path":"bookdown-cheat-sheet.html","id":"bookdown-cheat-sheet","chapter":"B Bookdown cheat sheet","heading":"B Bookdown cheat sheet","text":"","code":"\n# to see non-scientific notation a result\nformat(12e-17, scientific = FALSE)\n#> [1] \"0.00000000000000012\""},{"path":"bookdown-cheat-sheet.html","id":"operation","chapter":"B Bookdown cheat sheet","heading":"B.1 Operation","text":"R commands taking derivatives defined function involve expression, D, eval functions. wrap function want differentiate expression(), apply D() take derivative, use eval() compute result.simple exampleEvaluateThe first argument passed eval expression want evaluatethe second list containing values quantities defined elsewhere.","code":"\n#define a function\nf=expression(sqrt(x))\n\n#take the first derivative\ndf.dx=D(f,'x')\ndf.dx\n#> 0.5 * x^-0.5\n\n#take the second derivative\nd2f.dx2=D(D(f,'x'),'x')\nd2f.dx2\n#> 0.5 * (-0.5 * x^-1.5)\n#evaluate the function at a given x\neval(f,list(x=3))\n#> [1] 1.732051\n\n#evaluate the first derivative at a given x\neval(df.dx,list(x=3))\n#> [1] 0.2886751\n\n#evaluate the second derivative at a given x\neval(d2f.dx2,list(x=3))\n#> [1] -0.04811252"},{"path":"bookdown-cheat-sheet.html","id":"math-expression-syntax","chapter":"B Bookdown cheat sheet","heading":"B.2 Math Expression/ Syntax","text":"Full listAligning equations\\[\n\\begin{aligned}\n& = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\n\\]Cross-reference equationto refer sentence (B.1) (\\@ref(eq:test))Limit P(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\\[\nP(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\n\\]Matrices\\[\n\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n\\]\\[\n\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n\\]Aligning EquationsAligning Equations Comments\\[\n\\begin{aligned}\n    3+x &=4 & &\\text{(Solve } x \\text{.)} \\\\\n    x &=4-3 && \\text{(Subtract 3 sides.)} \\\\\n    x &=1   && \\text{(Yielding solution.)}\n\\end{aligned}\n\\]","code":"\\begin{aligned}\na & = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\\begin{equation} \na = b\n(\\#eq:test)\n\\end{equation}$$\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n$$$$\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n$$\\begin{aligned}\n    3+x &=4 && \\text{(Solve for} x \\text{.)}\\\\\n    x &=4-3 && \\text{(Subtract 3 from both sides.)}\\\\\n    x &=1   && \\text{(Yielding the solution.)}\n\\end{aligned}"},{"path":"bookdown-cheat-sheet.html","id":"statistics-notation","chapter":"B Bookdown cheat sheet","heading":"B.2.1 Statistics Notation","text":"\\[\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n\\]\\[\n\\begin{cases}\n\\frac{1}{b-} & \\text{} x\\[,b]\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\]","code":"$$\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n$$\\begin{cases}\n\\frac{1}{b-a}&\\text{for $x\\in[a,b]$}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}"},{"path":"bookdown-cheat-sheet.html","id":"table","chapter":"B Bookdown cheat sheet","heading":"B.3 Table","text":"built-wrapperbright colorcures scurvytasty\\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\)","code":"+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| *Bananas*     | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - **tasty**        |\n+---------------+---------------+--------------------+(\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
