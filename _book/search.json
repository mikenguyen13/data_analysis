[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"intended audience includes little experience statistics, econometrics, data science, well individuals budding interest fields eager deepen knowledge. primary domain interest marketing, principles methods discussed book universally applicable discipline employs scientific methods data analysis.hope book provides valuable starting point aspiring statisticians, econometricians, data scientists, empowering navigate fascinating world causal inference data analysis confidence.","code":""},{"path":"index.html","id":"how-to-cite-this-book","chapter":"Preface","heading":"How to cite this book","text":"1. APA (7th edition):Nguyen, M. (2020). Guide Data Analysis. Bookdown.https://bookdown.org/mike/data_analysis/2. MLA (8th edition):Nguyen, Mike. Guide Data Analysis. Bookdown, 2020. https://bookdown.org/mike/data_analysis/3. Chicago (17th edition):Nguyen, Mike. 2020. Guide Data Analysis. Bookdown. https://bookdown.org/mike/data_analysis/4. Harvard:Nguyen, M. (2020) Guide Data Analysis. Bookdown. Available : https://bookdown.org/mike/data_analysis/","code":"@book{nguyen2020guide,\n  title={A Guide on Data Analysis},\n  author={Nguyen, Mike},\n  year={2020},\n  publisher={Bookdown},\n  url={https://bookdown.org/mike/data_analysis/}\n}"},{"path":"index.html","id":"more-books","chapter":"Preface","heading":"More books","text":"books author can found :Advanced Data Analysis: second book data analysis series, covers machine learning models (focus prediction)Marketing ResearchCommunication Theory","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Since turn century, witnessed remarkable advancements innovations, particularly statistics, information technology, computer science, rapidly emerging field data science. However, one challenge developments overuse buzzwords like big data, machine learning, deep learning. terms powerful context, can sometimes obscure foundational principles underlying application.Every substantive field often specialized metric subfield, :Econometrics economicsPsychometrics psychologyChemometrics chemistrySabermetrics sports analyticsBiostatistics public health medicineTo layperson, disciplines often grouped broader terms like:Data ScienceApplied StatisticsComputational Social ScienceAs exciting explore new tools techniques, must admit retaining concepts can challenging. , effective way internalize apply ideas document data analysis process start finish.mind, let’s dive explore fascinating world data analysis together.","code":""},{"path":"introduction.html","id":"general-recommendations","chapter":"1 Introduction","heading":"1.1 General Recommendations","text":"journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.Readers can approach book several ways:\nFocused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.\nSequential Learning: follow traditional path data analysis, start Linear Regression section.\nExperimental Approach: interested designing experiments testing hypotheses, explore Analysis Variance (ANOVA) section.\nReaders can approach book several ways:Focused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.Sequential Learning: follow traditional path data analysis, start Linear Regression section.Experimental Approach: interested designing experiments testing hypotheses, explore Analysis Variance (ANOVA) section.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.customize code examples provided book, use R’s built-help functions. instance:\nlearn specific function, type help(function_name) ?function_name R console.\nexample, find details hist function, type ?hist help(hist) console.\ncustomize code examples provided book, use R’s built-help functions. instance:learn specific function, type help(function_name) ?function_name R console.example, find details hist function, type ?hist help(hist) console.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.adopting strategies, can tailor learning experience maximize value book.Tools statisticsProbability TheoryMathematical AnalysisComputer ScienceNumerical AnalysisDatabase ManagementCode ReplicationThis book built R version 4.2.3 (2023-03-15 ucrt) following packages:","code":"#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.3 (2023-03-15 ucrt)\n#>  os       Windows 10 x64 (build 22631)\n#>  system   x86_64, mingw32\n#>  ui       RTerm\n#>  language (EN)\n#>  collate  English_United States.utf8\n#>  ctype    English_United States.utf8\n#>  tz       America/Los_Angeles\n#>  date     2024-02-08\n#>  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package     * version date (UTC) lib source\n#>  bookdown      0.35    2023-08-09 [1] CRAN (R 4.2.3)\n#>  bslib         0.6.1   2023-11-28 [1] CRAN (R 4.2.3)\n#>  cachem        1.0.8   2023-05-01 [1] CRAN (R 4.2.3)\n#>  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.3)\n#>  codetools     0.2-19  2023-02-01 [1] CRAN (R 4.2.3)\n#>  colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.2.3)\n#>  desc          1.4.3   2023-12-10 [1] CRAN (R 4.2.3)\n#>  devtools      2.4.5   2022-10-11 [1] CRAN (R 4.2.3)\n#>  digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.3)\n#>  dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.2.3)\n#>  ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.3)\n#>  evaluate      0.23    2023-11-01 [1] CRAN (R 4.2.3)\n#>  fansi         1.0.4   2023-01-22 [1] CRAN (R 4.2.3)\n#>  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.3)\n#>  forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.2.3)\n#>  fs            1.6.3   2023-07-20 [1] CRAN (R 4.2.3)\n#>  generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.3)\n#>  ggplot2     * 3.4.4   2023-10-12 [1] CRAN (R 4.2.3)\n#>  glue          1.6.2   2022-02-24 [1] CRAN (R 4.2.3)\n#>  gtable        0.3.4   2023-08-21 [1] CRAN (R 4.2.3)\n#>  highr         0.10    2022-12-22 [1] CRAN (R 4.2.3)\n#>  hms           1.1.3   2023-03-21 [1] CRAN (R 4.2.3)\n#>  htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.2.3)\n#>  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.3)\n#>  httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.2.3)\n#>  jpeg        * 0.1-10  2022-11-29 [1] CRAN (R 4.2.2)\n#>  jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.2.3)\n#>  jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.2.3)\n#>  knitr         1.45    2023-10-30 [1] CRAN (R 4.2.3)\n#>  later         1.3.1   2023-05-02 [1] CRAN (R 4.2.3)\n#>  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.2.3)\n#>  lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.2.3)\n#>  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.3)\n#>  memoise       2.0.1   2021-11-26 [1] CRAN (R 4.2.3)\n#>  mime          0.12    2021-09-28 [1] CRAN (R 4.2.0)\n#>  miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.2.3)\n#>  munsell       0.5.0   2018-06-12 [1] CRAN (R 4.2.3)\n#>  pillar        1.9.0   2023-03-22 [1] CRAN (R 4.2.3)\n#>  pkgbuild      1.4.3   2023-12-10 [1] CRAN (R 4.2.3)\n#>  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.3)\n#>  pkgload       1.3.3   2023-09-22 [1] CRAN (R 4.2.3)\n#>  profvis       0.3.8   2023-05-02 [1] CRAN (R 4.2.3)\n#>  promises      1.2.1   2023-08-10 [1] CRAN (R 4.2.3)\n#>  purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.2.3)\n#>  R6            2.5.1   2021-08-19 [1] CRAN (R 4.2.3)\n#>  Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.2.3)\n#>  readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.2.3)\n#>  remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.2.3)\n#>  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.3)\n#>  rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.2.3)\n#>  rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.3)\n#>  sass          0.4.8   2023-12-06 [1] CRAN (R 4.2.3)\n#>  scales      * 1.3.0   2023-11-28 [1] CRAN (R 4.2.3)\n#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.3)\n#>  shiny         1.7.5   2023-08-12 [1] CRAN (R 4.2.3)\n#>  stringi       1.7.12  2023-01-11 [1] CRAN (R 4.2.2)\n#>  stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.2.3)\n#>  tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.2.3)\n#>  tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.2.3)\n#>  tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.2.3)\n#>  tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.2.3)\n#>  timechange    0.2.0   2023-01-11 [1] CRAN (R 4.2.3)\n#>  tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.2.3)\n#>  urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.2.3)\n#>  usethis       2.2.2   2023-07-06 [1] CRAN (R 4.2.3)\n#>  utf8          1.2.3   2023-01-31 [1] CRAN (R 4.2.3)\n#>  vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.2.3)\n#>  withr         2.5.2   2023-10-30 [1] CRAN (R 4.2.3)\n#>  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.3)\n#>  xtable        1.8-4   2019-04-21 [1] CRAN (R 4.2.3)\n#>  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.3)\n#> \n#>  [1] C:/Program Files/R/R-4.2.3/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"},{"path":"prerequisites.html","id":"prerequisites","chapter":"2 Prerequisites","heading":"2 Prerequisites","text":"chapter serves concise review fundamental concepts Matrix Theory Probability Theory.confident understanding topics, can proceed directly Descriptive Statistics section begin exploring applied data analysis.","code":""},{"path":"prerequisites.html","id":"matrix-theory","chapter":"2 Prerequisites","heading":"2.1 Matrix Theory","text":"Matrix \\(\\) represents original matrix. ’s 2x2 matrix elements \\(a_{ij}\\), \\(\\) represents row \\(j\\) represents column.\\[\n=\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\] \\('\\) transpose \\(\\). transpose matrix flips rows columns.\\[\n' =\n\\begin{bmatrix}\na_{11} & a_{21} \\\\\na_{12} & a_{22}\n\\end{bmatrix}\n\\]Fundamental properties rules matrices, essential understanding operations linear algebra:\\[\n\\begin{aligned}\n\\mathbf{(ABC)'}   & = \\mathbf{C'B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(B+C)}   & = \\mathbf{AB + AC} \\quad &\\text{(Distributive property)} \\\\\n\\mathbf{AB}       & \\neq \\mathbf{BA} \\quad &\\text{(Multiplication commutative)} \\\\\n\\mathbf{(')'}    & = \\mathbf{} \\quad &\\text{(Double transpose original matrix)} \\\\\n\\mathbf{(+B)'}   & = \\mathbf{' + B'} \\quad &\\text{(Transpose sum sum transposes)} \\\\\n\\mathbf{(AB)'}    & = \\mathbf{B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(AB)^{-1}} & = \\mathbf{B^{-1}^{-1}} \\quad &\\text{(Inverse reverses order product)} \\\\\n\\mathbf{+B}      & = \\mathbf{B + } \\quad &\\text{(Addition commutative)} \\\\\n\\mathbf{AA^{-1}}  & = \\mathbf{} \\quad &\\text{(Matrix times inverse identity)}\n\\end{aligned}\n\\] properties critical solving systems equations, optimizing models, performing data transformations.matrix \\(\\mathbf{}\\) inverse, called invertible. \\(\\mathbf{}\\) inverse, referred singular.product two matrices \\(\\mathbf{}\\) \\(\\mathbf{B}\\) computed :\\[\n\\begin{aligned}\n\\mathbf{} &=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \\sum_{=1}^{3}a_{1i}b_{i2} & \\sum_{=1}^{3}a_{1i}b_{i3} \\\\\n\\sum_{=1}^{3}a_{2i}b_{i1} & \\sum_{=1}^{3}a_{2i}b_{i2} & \\sum_{=1}^{3}a_{2i}b_{i3}\n\\end{bmatrix}\n\\end{aligned}\n\\]Quadratic FormLet \\(\\mathbf{}\\) \\(3 \\times 1\\) vector. quadratic form involving matrix \\(\\mathbf{B}\\) given :\\[\n\\mathbf{'Ba} = \\sum_{=1}^{3}\\sum_{j=1}^{3}a_i b_{ij} a_{j}\n\\]Length VectorThe length (2-norm) vector \\(\\mathbf{}\\), denoted \\(||\\mathbf{}||\\), defined square root inner product vector :\\[\n||\\mathbf{}|| = \\sqrt{\\mathbf{'}}\n\\]","code":""},{"path":"prerequisites.html","id":"rank-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.1 Rank of a Matrix","text":"rank matrix refers :dimension space spanned columns (rows).number linearly independent columns rows.\\(n \\times k\\) matrix \\(\\mathbf{}\\) \\(k \\times k\\) matrix \\(\\mathbf{B}\\), following properties hold:\\(\\text{rank}(\\mathbf{}) \\leq \\min(n, k)\\)\\(\\text{rank}(\\mathbf{}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{AA'})\\)\\(\\text{rank}(\\mathbf{AB}) = \\min(\\text{rank}(\\mathbf{}), \\text{rank}(\\mathbf{B}))\\)\\(\\mathbf{B}\\) invertible (non-singular) \\(\\text{rank}(\\mathbf{B}) = k\\).","code":""},{"path":"prerequisites.html","id":"inverse-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.2 Inverse of a Matrix","text":"scalar algebra, \\(= 0\\), \\(1/\\) exist.matrix algebra, matrix invertible non-singular, meaning non-zero determinant inverse exists. square matrix \\(\\mathbf{}\\) invertible exists another square matrix \\(\\mathbf{B}\\) :\\[\n\\mathbf{AB} = \\mathbf{} \\quad \\text{(Identity Matrix)}.\n\\]case, \\(\\mathbf{}^{-1} = \\mathbf{B}\\).\\(2 \\times 2\\) matrix:\\[\n\\mathbf{} =\n\\begin{bmatrix}\n& b \\\\\nc & d\n\\end{bmatrix}\n\\]inverse :\\[\n\\mathbf{}^{-1} =\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & \n\\end{bmatrix}\n\\]inverse exists \\(ad - bc \\neq 0\\), \\(ad - bc\\) determinant \\(\\mathbf{}\\).partitioned block matrix:\\[\n\\begin{bmatrix}\n& B \\\\\nC & D\n\\end{bmatrix}^{-1}\n=\n\\begin{bmatrix}\n\\mathbf{(-BD^{-1}C)^{-1}} & \\mathbf{-(-BD^{-1}C)^{-1}BD^{-1}} \\\\\n\\mathbf{-D^{-1}C(-BD^{-1}C)^{-1}} & \\mathbf{D^{-1}+D^{-1}C(-BD^{-1}C)^{-1}BD^{-1}}\n\\end{bmatrix}\n\\]formula assumes \\(\\mathbf{D}\\) \\(\\mathbf{- BD^{-1}C}\\) invertible.Properties Inverse Non-Singular Matrices\\(\\mathbf{(^{-1})^{-1}} = \\mathbf{}\\)non-zero scalar \\(b\\), \\(\\mathbf{(bA)^{-1} = b^{-1}^{-1}}\\)matrix \\(\\mathbf{B}\\), \\(\\mathbf{(BA)^{-1} = B^{-1}^{-1}}\\) (\\(\\mathbf{B}\\) non-singular).\\(\\mathbf{(^{-1})' = (')^{-1}}\\) (transpose inverse equals inverse transpose).Never notate \\(\\mathbf{1/}\\); use \\(\\mathbf{^{-1}}\\) instead.Notes: - determinant matrix determines whether invertible. square matrices, determinant \\(0\\) means matrix singular inverse.\n- Always verify conditions invertibility, particularly dealing partitioned block matrices.","code":""},{"path":"prerequisites.html","id":"definiteness-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.3 Definiteness of a Matrix","text":"symmetric square \\(k \\times k\\) matrix \\(\\mathbf{}\\) classified based following conditions:Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.identity matrix always positive definite (PD).ExampleLet \\(\\mathbf{x} = (x_1, x_2)'\\), consider \\(2 \\times 2\\) identity matrix \\(\\mathbf{}\\):\\[\n\\begin{aligned}\n\\mathbf{x'Ix}\n&= (x_1, x_2)\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\n(x_1, x_2)\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\nx_1^2 + x_2^2 \\geq 0.\n\\end{aligned}\n\\]Thus, \\(\\mathbf{}\\) PD \\(\\mathbf{x'Ix} > 0\\) non-zero \\(\\mathbf{x}\\).Properties DefinitenessAny variance-covariance matrix PSD.matrix \\(\\mathbf{}\\) PSD exists matrix \\(\\mathbf{B}\\) : \\[\n\\mathbf{= B'B}.\n\\]\\(\\mathbf{}\\) PSD, \\(\\mathbf{B'AB}\\) also PSD conformable matrix \\(\\mathbf{B}\\).\\(\\mathbf{}\\) \\(\\mathbf{C}\\) non-singular, \\(\\mathbf{- C}\\) PSD \\(\\mathbf{C^{-1} - ^{-1}}\\) PSD.\\(\\mathbf{}\\) PD (ND), \\(\\mathbf{^{-1}}\\) also PD (ND).NotesAn indefinite matrix \\(\\mathbf{}\\) neither PSD NSD. concept direct counterpart scalar algebra.square matrix PSD invertible, PD.Examples DefinitenessInvertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Invertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"matrix-calculus","chapter":"2 Prerequisites","heading":"2.1.4 Matrix Calculus","text":"Consider scalar function \\(y = f(x_1, x_2, \\dots, x_k) = f(x)\\), \\(x\\) \\(1 \\times k\\) row vector.","code":""},{"path":"prerequisites.html","id":"gradient-first-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.1 Gradient (First-Order Derivative)","text":"gradient, first-order derivative \\(f(x)\\) respect vector \\(x\\), given :\\[\n\\frac{\\partial f(x)}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f(x)}{\\partial x_1} \\\\\n\\frac{\\partial f(x)}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f(x)}{\\partial x_k}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"hessian-second-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.2 Hessian (Second-Order Derivative)","text":"Hessian, second-order derivative \\(f(x)\\) respect \\(x\\), symmetric matrix defined :\\[\n\\frac{\\partial^2 f(x)}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n\\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_k} \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_k^2}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"derivative-of-a-scalar-function-with-respect-to-a-matrix","chapter":"2 Prerequisites","heading":"2.1.4.3 Derivative of a Scalar Function with Respect to a Matrix","text":"Let \\(f(\\mathbf{X})\\) scalar function, \\(\\mathbf{X}\\) \\(n \\times p\\) matrix. derivative :\\[\n\\frac{\\partial f(\\mathbf{X})}{\\partial \\mathbf{X}} =\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{11}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{1p}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{n1}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{np}}\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"common-matrix-derivatives","chapter":"2 Prerequisites","heading":"2.1.4.4 Common Matrix Derivatives","text":"\\(\\mathbf{}\\) vector \\(\\mathbf{}\\) matrix independent \\(\\mathbf{y}\\):\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\n\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\\(\\mathbf{X}\\) symmetric:\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\\(\\mathbf{X}\\) symmetric \\(\\mathbf{}\\) matrix independent \\(\\mathbf{X}\\):\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\\(\\mathbf{X}\\) symmetric, let \\(\\mathbf{J}_{ij}\\) matrix 1 \\((,j)\\)-th position 0 elsewhere:\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)","code":""},{"path":"prerequisites.html","id":"optimization-in-scalar-and-vector-spaces","chapter":"2 Prerequisites","heading":"2.1.5 Optimization in Scalar and Vector Spaces","text":"Optimization process finding minimum maximum function. conditions optimization differ depending whether function involves scalar vector. comparison scalar vector optimization:Second-Order ConditionFor convex functions, implies minimum.Key ConceptsFirst-Order Condition:\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\n\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.vector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.Second-Order Condition:\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\n\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.vector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\nPositive Definite: minimum (convex function).Negative Definite: maximum (concave function).Indefinite: saddle point (neither minimum maximum).Convex Concave Functions:\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\n\nConvexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.Concave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.Convexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.Hessian Matrix:\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\n\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\nPositive definite Hessian: eigenvalues positive.Negative definite Hessian: eigenvalues negative.Indefinite Hessian: Eigenvalues mixed signs.","code":""},{"path":"prerequisites.html","id":"cholesky-decomposition","chapter":"2 Prerequisites","heading":"2.1.6 Cholesky Decomposition","text":"statistical analysis numerical linear algebra, decomposing matrices tractable forms crucial efficient computation. One important factorization Cholesky Decomposition. applies Hermitian (complex case) symmetric (real case), positive-definite matrices.Given \\(n \\times n\\) positive-definite matrix \\(\\), Cholesky Decomposition states:\\[\n= L L^{*},\n\\]:\\(L\\) lower-triangular matrix strictly positive diagonal entries.\\(L^{*}\\) denotes conjugate transpose \\(L\\) (simply transpose \\(L^{T}\\) real matrices).Cholesky Decomposition computationally efficient numerically stable, making go-technique many applications—particularly statistics deal extensively covariance matrices, linear systems, probability distributions.diving compute Cholesky Decomposition, need clarify means matrix positive-definite. real symmetric matrix \\(\\):\\(\\) positive-definite every nonzero vector \\(x\\), \\[\nx^T \\, x > 0.\n\\]Alternatively, can characterize positive-definiteness noting eigenvalues \\(\\) strictly positive.Many important matrices statistics—particularly covariance precision matrices—symmetric positive-definite.","code":""},{"path":"prerequisites.html","id":"existence","chapter":"2 Prerequisites","heading":"2.1.6.1 Existence","text":"real \\(n \\times n\\) matrix \\(\\) symmetric positive-definite always admits Cholesky Decomposition \\(= L L^T\\). theorem guarantees covariance matrix statistics—assuming valid (.e., positive-definite)—can decompose via Cholesky.","code":""},{"path":"prerequisites.html","id":"uniqueness","chapter":"2 Prerequisites","heading":"2.1.6.2 Uniqueness","text":"additionally require diagonal entries \\(L\\) strictly positive, \\(L\\) unique. , lower-triangular matrix strictly positive diagonal entries produce factorization. uniqueness helpful ensuring consistent numerical outputs software implementations.","code":""},{"path":"prerequisites.html","id":"constructing-the-cholesky-factor-l","chapter":"2 Prerequisites","heading":"2.1.6.3 Constructing the Cholesky Factor \\(L\\)","text":"Given real, symmetric, positive-definite matrix \\(\\\\mathbb{R}^{n \\times n}\\), want find lower-triangular matrix \\(L\\) \\(= LL^T\\). One way using simple step--step procedure (often part standard linear algebra libraries):Initialize \\(L\\) \\(n \\times n\\) zero matrix.Iterate rows \\(= 1, 2, \\dots, n\\):\nrow \\(\\), compute \\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{-1} L_{ik}^2}.\n\\]\ncolumn \\(j = +1, +2, \\dots, n\\): \\[\nL_{ji} = \\frac{1}{L_{ii}}\n          \\Bigl(A_{ji} - \\sum_{k=1}^{-1} L_{jk} L_{ik}\\Bigr).\n\\]\nentries \\(L\\) remain zero computed subsequent steps.\nrow \\(\\), compute \\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{-1} L_{ik}^2}.\n\\]column \\(j = +1, +2, \\dots, n\\): \\[\nL_{ji} = \\frac{1}{L_{ii}}\n          \\Bigl(A_{ji} - \\sum_{k=1}^{-1} L_{jk} L_{ik}\\Bigr).\n\\]entries \\(L\\) remain zero computed subsequent steps.Result: \\(L\\) lower-triangular, \\(L^T\\) transpose.Cholesky Decomposition roughly half computational cost general LU Decomposition. Specifically, requires order \\(\\frac{1}{3} n^3\\) floating-point operations (flops), making significantly efficient practice decompositions positive-definite systems.","code":""},{"path":"prerequisites.html","id":"illustrative-example","chapter":"2 Prerequisites","heading":"2.1.6.4 Illustrative Example","text":"Consider small \\(3 \\times 3\\) positive-definite matrix:\\[\n=\n\\begin{pmatrix}\n4 & 2 & 4 \\\\\n2 & 5 & 6 \\\\\n4 & 6 & 20\n\\end{pmatrix}.\n\\]claim \\(\\) positive-definite (one check calculating principal minors verifying \\(x^T x > 0\\) \\(x \\neq 0\\)). find \\(L\\) step--step:Compute \\(L_{11}\\):\\[\nL_{11} = \\sqrt{A_{11}} = \\sqrt{4} = 2.\n\\]Compute \\(L_{21}\\) \\(L_{31}\\):\n\\(L_{21} = \\frac{A_{21}}{L_{11}} = \\frac{2}{2} = 1.\\)\n\\(L_{31} = \\frac{A_{31}}{L_{11}} = \\frac{4}{2} = 2.\\)\n\\(L_{21} = \\frac{A_{21}}{L_{11}} = \\frac{2}{2} = 1.\\)\\(L_{31} = \\frac{A_{31}}{L_{11}} = \\frac{4}{2} = 2.\\)Compute \\(L_{22}\\):\\[\nL_{22} = \\sqrt{A_{22} - L_{21}^2}\n        = \\sqrt{5 - 1^2}\n        = \\sqrt{4} = 2.\n\\]Compute \\(L_{32}\\):\\[\nL_{32} = \\frac{A_{32} - L_{31} L_{21}}{L_{22}}\n        = \\frac{6 - (2)(1)}{2}\n        = \\frac{4}{2} = 2.\n\\]Compute \\(L_{33}\\):\\[\nL_{33} = \\sqrt{A_{33} - (L_{31}^2 + L_{32}^2)}\n        = \\sqrt{20 - (2^2 + 2^2)}\n        = \\sqrt{20 - 8}\n        = \\sqrt{12}\n        = 2\\sqrt{3}.\n\\]Thus,\\[\nL =\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 2 & 2\\sqrt{3}\n\\end{pmatrix}.\n\\]One can verify \\(L L^T = \\).","code":""},{"path":"prerequisites.html","id":"applications-in-statistics","chapter":"2 Prerequisites","heading":"2.1.6.5 Applications in Statistics","text":"","code":""},{"path":"prerequisites.html","id":"solving-linear-systems","chapter":"2 Prerequisites","heading":"2.1.6.5.1 Solving Linear Systems","text":"common statistical problem solving \\(x = b\\) \\(x\\). instance, regression computing Bayesian posterior modes, often need solve linear equations covariance precision matrices. \\(= LL^T\\):Forward Substitution: Solve \\(L \\, y = b\\).Backward Substitution: Solve \\(L^T x = y\\).two-step process stable efficient directly inverting \\(\\) (typically discouraged due numerical issues).","code":""},{"path":"prerequisites.html","id":"generating-correlated-random-vectors","chapter":"2 Prerequisites","heading":"2.1.6.5.2 Generating Correlated Random Vectors","text":"simulation-based statistics (e.g., Monte Carlo methods), often need generate random draws multivariate normal distribution \\(\\mathcal{N}(\\mu, \\Sigma)\\), \\(\\Sigma\\) covariance matrix. steps :Generate vector \\(z \\sim \\mathcal{N}(0, )\\) independent standard normal variables.Compute \\(x = \\mu + Lz\\), \\(\\Sigma = LL^T\\).\\(x\\) desired covariance structure \\(\\Sigma\\). technique widely used Bayesian statistics (e.g., MCMC sampling) financial modeling (e.g., portfolio simulations).","code":""},{"path":"prerequisites.html","id":"gaussian-processes-and-kriging","chapter":"2 Prerequisites","heading":"2.1.6.5.3 Gaussian Processes and Kriging","text":"Gaussian Process modeling (common spatial statistics, machine learning, geostatistics), frequently work large covariance matrices describe correlations observed data points:\\[\n\\Sigma =\n\\begin{pmatrix}\nk(x_1, x_1) & k(x_1, x_2) & \\cdots & k(x_1, x_n) \\\\\nk(x_2, x_1) & k(x_2, x_2) & \\cdots & k(x_2, x_n) \\\\\n\\vdots      & \\vdots      & \\ddots & \\vdots      \\\\\nk(x_n, x_1) & k(x_n, x_2) & \\cdots & k(x_n, x_n)\n\\end{pmatrix},\n\\]\\(k(\\cdot, \\cdot)\\) covariance (kernel) function. may need invert factorize \\(\\Sigma\\) repeatedly evaluate log-likelihood:\\[\n\\log \\mathcal{L}(\\theta) \\sim\n- \\tfrac{1}{2} \\bigl( y - m(\\theta) \\bigr)^T \\Sigma^{-1} \\bigl( y - m(\\theta) \\bigr)\n- \\tfrac{1}{2} \\log \\bigl| \\Sigma \\bigr|,\n\\]\\(m(\\theta)\\) mean function \\(\\theta\\) parameters. Using Cholesky factor \\(L\\) \\(\\Sigma\\) helps:\\(\\Sigma^{-1}\\) can implied solving systems \\(L\\) instead explicitly computing inverse.\\(\\log|\\Sigma|\\) can computed \\(2 \\sum_{=1}^n \\log L_{ii}\\).Hence, Cholesky Decomposition becomes backbone Gaussian Process computations.","code":""},{"path":"prerequisites.html","id":"bayesian-inference-with-covariance-matrices","chapter":"2 Prerequisites","heading":"2.1.6.5.4 Bayesian Inference with Covariance Matrices","text":"Many Bayesian models—especially hierarchical models—assume multivariate normal prior parameters. Cholesky Decomposition used :Sample priors posterior distributions.Regularize large covariance matrices.Speed Markov Chain Monte Carlo (MCMC) computations factorizing covariance structures.","code":""},{"path":"prerequisites.html","id":"other-notes","chapter":"2 Prerequisites","heading":"2.1.6.6 Other Notes","text":"Numerical Stability ConsiderationsCholesky Decomposition considered stable general LU Decomposition applied positive-definite matrices. Since row column pivots required, rounding errors can smaller. course, practice, software implementations can vary, extremely ill-conditioned matrices can still pose numerical challenges.Don’t Usually Compute \\(\\mathbf{}^{-1}\\)common statistics (especially older texts) see formulas involving \\(\\Sigma^{-1}\\). However, computing inverse explicitly often discouraged :numerically less stable.requires computations.Many tasks appear need \\(\\Sigma^{-1}\\) can done efficiently solving systems via Cholesky factor \\(L\\).Hence, “solve, don’t invert” common mantra. see expression like \\(\\Sigma^{-1} b\\), can use Cholesky factors \\(L\\) \\(L^T\\) solve \\(\\Sigma x = b\\) forward backward substitution, bypassing direct inverse calculation.Variants ExtensionsIncomplete Cholesky: Sometimes used iterative solvers full Cholesky factorization expensive, especially large sparse systems.LDL^T Decomposition: variant avoids taking square roots; used positive semi-definite indefinite systems, caution pivoting strategies.","code":""},{"path":"prerequisites.html","id":"probability-theory","chapter":"2 Prerequisites","heading":"2.2 Probability Theory","text":"","code":""},{"path":"prerequisites.html","id":"axioms-and-theorems-of-probability","chapter":"2 Prerequisites","heading":"2.2.1 Axioms and Theorems of Probability","text":"Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)probability empty set : \\[\nP[\\emptyset] = 0\n\\]probability empty set : \\[\nP[\\emptyset] = 0\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]","code":""},{"path":"prerequisites.html","id":"conditional-probability","chapter":"2 Prerequisites","heading":"2.2.1.1 Conditional Probability","text":"conditional probability \\(\\) given \\(B\\) defined :\\[\nP[|B] = \\frac{P[\\cap B]}{P[B]}, \\quad \\text{provided } P[B] \\neq 0.\n\\]","code":""},{"path":"prerequisites.html","id":"independent-events","chapter":"2 Prerequisites","heading":"2.2.1.2 Independent Events","text":"Two events \\(\\) \\(B\\) independent :\\(P[\\cap B] = P[]P[B]\\)\\(P[|B] = P[]\\)\\(P[B|] = P[B]\\)collection events \\(A_1, A_2, \\dots, A_n\\) independent every subcollection independent.","code":""},{"path":"prerequisites.html","id":"multiplication-rule","chapter":"2 Prerequisites","heading":"2.2.1.3 Multiplication Rule","text":"probability intersection two events can calculated : \\[\nP[\\cap B] = P[|B]P[B] = P[B|]P[].\n\\]","code":""},{"path":"prerequisites.html","id":"bayes-theorem","chapter":"2 Prerequisites","heading":"2.2.1.4 Bayes’ Theorem","text":"Let \\(A_1, A_2, \\dots, A_n\\) collection mutually exclusive events whose union \\(S\\), let \\(B\\) event \\(P[B] \\neq 0\\). , event \\(A_j\\) (\\(j = 1, 2, \\dots, n\\)): \\[\nP[A_j|B] = \\frac{P[B|A_j]P[A_j]}{\\sum_{=1}^n P[B|A_i]P[A_i]}.\n\\]","code":""},{"path":"prerequisites.html","id":"jensens-inequality","chapter":"2 Prerequisites","heading":"2.2.1.5 Jensen’s Inequality","text":"\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]Jensen’s inequality provides useful way demonstrate standard error calculated using sample standard deviation (\\(s\\)) proxy population standard deviation (\\(\\sigma\\)) biased estimator.population standard deviation \\(\\sigma\\) defined : \\[\n\\sigma = \\sqrt{\\mathbb{E}[(X - \\mu)^2]},\n\\] \\(\\mu = \\mathbb{E}[X]\\) population mean.population standard deviation \\(\\sigma\\) defined : \\[\n\\sigma = \\sqrt{\\mathbb{E}[(X - \\mu)^2]},\n\\] \\(\\mu = \\mathbb{E}[X]\\) population mean.sample standard deviation \\(s\\) given : \\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n (X_i - \\bar{X})^2},\n\\] \\(\\bar{X}\\) sample mean.sample standard deviation \\(s\\) given : \\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n (X_i - \\bar{X})^2},\n\\] \\(\\bar{X}\\) sample mean.\\(s\\) used estimator \\(\\sigma\\), expectation involves square root function, concave.\\(s\\) used estimator \\(\\sigma\\), expectation involves square root function, concave.Applying Jensen’s InequalityThe standard error formula involves square root: \\[\n\\sqrt{\\mathbb{E}[s^2]}.\n\\]However, square root function concave, Jensen’s inequality implies: \\[\n\\sqrt{\\mathbb{E}[s^2]} \\leq \\mathbb{E}[\\sqrt{s^2}] = \\mathbb{E}[s].\n\\]inequality shows expected value \\(s\\) (sample standard deviation) systematically underestimates population standard deviation \\(\\sigma\\).Quantifying BiasThe bias arises : \\[\n\\mathbb{E}[s] \\neq \\sigma.\n\\]correct bias, note sample standard deviation related population standard deviation : \\[\n\\mathbb{E}[s] = \\sigma \\cdot \\sqrt{\\frac{n-1}{n}},\n\\] \\(n\\) sample size. bias decreases \\(n\\) increases, estimator becomes asymptotically unbiased.leveraging Jensen’s inequality, observe concavity square root function ensures \\(s\\) biased estimator \\(\\sigma\\), systematically underestimating population standard deviation.","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectation","chapter":"2 Prerequisites","heading":"2.2.1.6 Law of Iterated Expectation","text":"Law Iterated Expectation states random variables \\(X\\) \\(Y\\):\\[\nE(X) = E(E(X|Y)).\n\\]means expected value \\(X\\) can obtained first calculating conditional expectation \\(E(X|Y)\\) taking expectation quantity distribution \\(Y\\).","code":""},{"path":"prerequisites.html","id":"correlation-and-independence","chapter":"2 Prerequisites","heading":"2.2.1.7 Correlation and Independence","text":"strength relationship random variables can ranked strongest weakest :Independence:\n\\(f(x, y) = f_X(x)f_Y(y)\\)\n\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\n\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)\n\\(f(x, y) = f_X(x)f_Y(y)\\)\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)Mean Independence (implied independence):\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\n\\(E[Xg(Y)] = E[X]E[g(Y)]\\)\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\\(E[Xg(Y)] = E[X]E[g(Y)]\\)Uncorrelatedness (implied independence mean independence):\n\\(\\text{Cov}(X, Y) = 0\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\\(E[XY] = E[X]E[Y]\\)\n\\(\\text{Cov}(X, Y) = 0\\)\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\\(E[XY] = E[X]E[Y]\\)","code":""},{"path":"prerequisites.html","id":"central-limit-theorem","chapter":"2 Prerequisites","heading":"2.2.2 Central Limit Theorem","text":"Central Limit Theorem states sufficiently large sample size (\\(n \\geq 25\\)), sampling distribution sample mean proportion approaches normal distribution, regardless population’s original distribution.Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) distribution \\(X\\) mean \\(\\mu\\) variance \\(\\sigma^2\\). , large \\(n\\):sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]following random variables approximately standard normal:\n\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\n\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\n\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\nfollowing random variables approximately standard normal:\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)","code":""},{"path":"prerequisites.html","id":"limiting-distribution-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.2.2.1 Limiting Distribution of the Sample Mean","text":"\\(\\{X_i\\}_{=1}^{n}\\) iid random sample distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), sample mean \\(\\bar{X}\\) scaled \\(\\sqrt{n}\\) following limiting distribution:\\[\n\\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2).\n\\]Standardizing sample mean gives: \\[\n\\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\xrightarrow{d} N(0, 1).\n\\]Notes:CLT holds random samples distribution (continuous, discrete, unknown).extends multivariate case: random sample random vector converges multivariate normal distribution.","code":""},{"path":"prerequisites.html","id":"asymptotic-variance-and-limiting-variance","chapter":"2 Prerequisites","heading":"2.2.2.2 Asymptotic Variance and Limiting Variance","text":"Asymptotic Variance (Avar): \\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\sigma^2.\n\\]Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).Limiting Variance (\\(\\lim_{n \\\\infty} Var\\))\\[\n\\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2\n\\]Represents value actual variance \\(\\sqrt{n}(\\bar{x} - \\mu)\\) converges \\(n \\\\infty\\).well-behaved estimator,\\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2.\n\\]However, asymptotic variance necessarily equal limiting value variance asymptotic variance derived limiting distribution, limiting variance convergence result sequence variances.\\[\nAvar(.) \\neq lim_{n \\\\infty} Var(.)\n\\]asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\n\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\n\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\n\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.Cases two match:Sample Quantiles: Consider sample quantile order \\(p\\), \\(0 < p < 1\\). regularity conditions, asymptotic distribution sample quantile normal, variance depends \\(p\\) density distribution \\(p\\)-th quantile. However, variance sample quantile necessarily converge limit sample size grows.Bootstrap Methods: using bootstrapping techniques estimate distribution statistic, bootstrap distribution might converge different limiting distribution original statistic. cases, variance bootstrap distribution (bootstrap variance) might differ limiting variance original statistic.Statistics Randomly Varying Asymptotic Behavior: cases, asymptotic behavior statistic can vary randomly depending sample path. statistics, asymptotic variance might provide consistent estimate limiting variance.M-estimators Varying Asymptotic Behavior: M-estimators can sometimes different asymptotic behaviors depending tail behavior underlying distribution. heavy-tailed distributions, variance estimator might stabilize even sample size grows large, making asymptotic variance different variance limiting distribution.","code":""},{"path":"prerequisites.html","id":"random-variable","chapter":"2 Prerequisites","heading":"2.2.3 Random Variable","text":"Random variables can categorized either discrete continuous, distinct properties functions defining type.Expected Value Properties\\(E[c] = c\\) constant \\(c\\).\\(E[cX] = cE[X]\\) constant \\(c\\).\\(E[X + Y] = E[X] + E[Y]\\).\\(E[XY] = E[X]E[Y]\\) (\\(X\\) \\(Y\\) independent).Variance Properties\\(\\text{Var}(c) = 0\\) constant \\(c\\).\\(\\text{Var}(cX) = c^2 \\text{Var}(X)\\) constant \\(c\\).\\(\\text{Var}(X) \\geq 0\\).\\(\\text{Var}(X) = E[X^2] - (E[X])^2\\).\\(\\text{Var}(X + c) = \\text{Var}(X)\\).\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (\\(X\\) \\(Y\\) independent).standard deviation \\(\\sigma\\) given : \\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\text{Var}(X)}.\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-random-variables","chapter":"2 Prerequisites","heading":"2.2.3.1 Multivariate Random Variables","text":"Suppose \\(y_1, \\dots, y_p\\) random variables means \\(\\mu_1, \\dots, \\mu_p\\). :\\[\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_p\n\\end{bmatrix}, \\quad E[\\mathbf{y}] = \\begin{bmatrix}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix} = \\boldsymbol{\\mu}.\n\\]covariance \\(y_i\\) \\(y_j\\) \\(\\sigma_{ij} = \\text{Cov}(y_i, y_j)\\). variance-covariance (dispersion) matrix :\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij})= \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n\\end{bmatrix}.\n\\]\\(\\mathbf{\\Sigma}\\) symmetric \\((p+1)p/2\\) unique parameters.Alternatively, let \\(u_{p \\times 1}\\) \\(v_{v \\times 1}\\) random vectors means \\(\\mathbf{\\mu_u}\\) \\(\\mathbf{\\mu_v}\\). \\[ \\mathbf{\\Sigma_{uv}} = cov(\\mathbf{u,v}) = E[\\mathbf{(u-\\mu_u)(v-\\mu_v)'}] \\]\\(\\Sigma_{uv} \\neq \\Sigma_{vu}\\) (\\(\\Sigma_{uv} = \\Sigma_{vu}'\\))Properties Covariance MatricesSymmetry: \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\).Eigen-Decomposition (spectral decomposition,symmetric decomposition): \\(\\mathbf{\\Sigma = \\Phi \\Lambda \\Phi}\\), \\(\\mathbf{\\Phi}\\) matrix eigenvectors \\(\\mathbf{\\Phi \\Phi' = }\\) (orthonormal), \\(\\mathbf{\\Lambda}\\) diagonal matrix eigenvalues \\((\\lambda_1,...,\\lambda_p)\\) diagonal.Non-Negative Definiteness: \\(\\mathbf{\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\). Equivalently, eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge ... \\ge \\lambda_p \\ge 0\\)Generalized Variance: \\(|\\mathbf{\\Sigma}| = \\lambda_1 \\dots \\lambda_p \\geq 0\\).Trace: \\(\\text{tr}(\\mathbf{\\Sigma}) = \\lambda_1 + \\dots + \\lambda_p = \\sigma_{11} + \\dots+ \\sigma_{pp} = \\sum \\sigma_{ii}\\) = sum variances (total variance).Note: \\(\\mathbf{\\Sigma}\\) required positive definite. implies eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma}= \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\)","code":""},{"path":"prerequisites.html","id":"correlation-matrices","chapter":"2 Prerequisites","heading":"2.2.3.2 Correlation Matrices","text":"correlation coefficient \\(\\rho_{ij}\\) correlation matrix \\(\\mathbf{R}\\) defined :\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}, \\quad \\mathbf{R} = \\begin{bmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\dots & 1\n\\end{bmatrix}.\n\\]\\(\\rho_{ii} = 1 \\forall \\)","code":""},{"path":"prerequisites.html","id":"linear-transformations","chapter":"2 Prerequisites","heading":"2.2.3.3 Linear Transformations","text":"Let \\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants, \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants. :\\(E[\\mathbf{Ay + c}] = \\mathbf{\\mu_y + c}\\).\\(\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{\\Sigma_y '}\\).\\(\\text{Cov}(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B'}\\).","code":""},{"path":"prerequisites.html","id":"moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4 Moment Generating Function","text":"","code":""},{"path":"prerequisites.html","id":"properties-of-the-moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4.1 Properties of the Moment Generating Function","text":"\\(\\frac{d^k(m_X(t))}{dt^k} \\bigg|_{t=0} = E[X^k]\\) (\\(k\\)-th derivative \\(t=0\\) gives \\(k\\)-th moment \\(X\\)).\\(\\mu = E[X] = m_X'(0)\\) (first derivative \\(t=0\\) gives mean).\\(E[X^2] = m_X''(0)\\) (second derivative \\(t=0\\) gives second moment).","code":""},{"path":"prerequisites.html","id":"theorems-involving-mgfs","chapter":"2 Prerequisites","heading":"2.2.4.2 Theorems Involving MGFs","text":"Let \\(X_1, X_2, \\dots, X_n, Y\\) random variables MGFs \\(m_{X_1}(t), m_{X_2}(t), \\dots, m_{X_n}(t), m_Y(t)\\):\\(m_{X_1}(t) = m_{X_2}(t)\\) \\(t\\) open interval 0, \\(X_1\\) \\(X_2\\) distribution.\\(Y = \\alpha + \\beta X_1\\), : \\[\nm_Y(t) = e^{\\alpha t}m_{X_1}(\\beta t).\n\\]\\(X_1, X_2, \\dots, X_n\\) independent \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), \\(\\alpha_0, \\alpha_1, \\dots, \\alpha_n\\) constants, : \\[\nm_Y(t) = e^{\\alpha_0 t} m_{X_1}(\\alpha_1 t) m_{X_2}(\\alpha_2 t) \\dots m_{X_n}(\\alpha_n t).\n\\]Suppose \\(X_1, X_2, \\dots, X_n\\) independent normal random variables means \\(\\mu_1, \\mu_2, \\dots, \\mu_n\\) variances \\(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2\\). \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), :\n\\(Y\\) normally distributed.\nMean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).\nVariance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).\n\\(Y\\) normally distributed.Mean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).Variance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).","code":""},{"path":"prerequisites.html","id":"moments","chapter":"2 Prerequisites","heading":"2.2.5 Moments","text":"Skewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\)\nDefinition: Skewness measures asymmetry probability distribution around mean.\nInterpretation:\nPositive skewness: right tail (higher values) longer heavier left tail.\nNegative skewness: left tail (lower values) longer heavier right tail.\nZero skewness: data symmetric.\n\nSkewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\)Definition: Skewness measures asymmetry probability distribution around mean.Interpretation:\nPositive skewness: right tail (higher values) longer heavier left tail.\nNegative skewness: left tail (lower values) longer heavier right tail.\nZero skewness: data symmetric.\nPositive skewness: right tail (higher values) longer heavier left tail.Positive skewness: right tail (higher values) longer heavier left tail.Negative skewness: left tail (lower values) longer heavier right tail.Negative skewness: left tail (lower values) longer heavier right tail.Zero skewness: data symmetric.Zero skewness: data symmetric.Kurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\)\nDefinition: Kurtosis measures “tailedness” heaviness tails probability distribution.\nExcess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).\nInterpretation:\nHigh kurtosis (>3): Heavy tails, extreme outliers.\nLow kurtosis (<3): Light tails, fewer outliers.\nNormal distribution kurtosis = 3: Benchmark comparison.\n\nKurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\)Definition: Kurtosis measures “tailedness” heaviness tails probability distribution.Definition: Kurtosis measures “tailedness” heaviness tails probability distribution.Excess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).Excess kurtosis (often reported) kurtosis minus 3 (compare normal distribution’s kurtosis 3).Interpretation:\nHigh kurtosis (>3): Heavy tails, extreme outliers.\nLow kurtosis (<3): Light tails, fewer outliers.\nNormal distribution kurtosis = 3: Benchmark comparison.\nInterpretation:High kurtosis (>3): Heavy tails, extreme outliers.High kurtosis (>3): Heavy tails, extreme outliers.Low kurtosis (<3): Light tails, fewer outliers.Low kurtosis (<3): Light tails, fewer outliers.Normal distribution kurtosis = 3: Benchmark comparison.Normal distribution kurtosis = 3: Benchmark comparison.","code":""},{"path":"prerequisites.html","id":"skewness","chapter":"2 Prerequisites","heading":"2.2.6 Skewness","text":"Skewness measures asymmetry distribution:Positive skew: right side (high values) stretched .\nPositive skew occurs right tail (higher values) distribution longer heavier.\nExamples:\nIncome Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.\nHousing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.\n\nPositive skew: right side (high values) stretched .Positive skew occurs right tail (higher values) distribution longer heavier.Positive skew occurs right tail (higher values) distribution longer heavier.Examples:\nIncome Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.\nHousing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.\nExamples:Income Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.Income Distribution: many countries, people earn moderate income, small fraction ultra-high earners stretches distribution’s right tail.Housing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.Housing Prices: homes may around affordable price, extravagant mansions create long (expensive) upper tail.Income Distribution example, people earn moderate incomes, high earners stretch right tail.Housing Prices example, homes reasonably priced, mansions create long, expensive right tail.Negative Skew (Left Skew)Negative skew occurs left tail (lower values) distribution longer heavier.Negative skew occurs left tail (lower values) distribution longer heavier.Examples:\nScores Easy Test: exam easy, students score quite high, students score low, creating left tail.\nAge Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).\nExamples:Scores Easy Test: exam easy, students score quite high, students score low, creating left tail.Scores Easy Test: exam easy, students score quite high, students score low, creating left tail.Age Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).Age Retirement: people might retire around common age (say 65+), fewer retiring early (stretching left tail).Easy Test Scores example, students perform well, low scores stretch left tail.Easy Test Scores example, students perform well, low scores stretch left tail.Retirement Age example, people retire around age, small number individuals retire early, stretching left tail.Retirement Age example, people retire around age, small number individuals retire early, stretching left tail.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Simulate data for positive skew\nset.seed(123)\npositive_skew_income <-\n    rbeta(1000, 5, 2) * 100  # Income distribution example\npositive_skew_housing <-\n    rbeta(1000, 5, 2) * 1000  # Housing prices example\n\n# Combine data\ndata_positive_skew <- data.frame(\n    value = c(positive_skew_income, positive_skew_housing),\n    example = c(rep(\"Income Distribution\", 1000), rep(\"Housing Prices\", 1000))\n)\n\n# Plot positive skew\nggplot(data_positive_skew, aes(x = value, fill = example)) +\n    geom_histogram(bins = 30,\n                   alpha = 0.7,\n                   position = \"identity\") +\n    facet_wrap( ~ example, scales = \"free\") +\n    labs(title = \"Visualization of Positive Skew\",\n         x = \"Value\",\n         y = \"Frequency\") +\n    theme_minimal()\n# Simulate data for negative skew\nset.seed(123)\nnegative_skew_test <-\n    10 - rbeta(1000, 5, 2) * 10  # Easy test scores example\nnegative_skew_retirement <-\n    80 - rbeta(1000, 5, 2) * 20  # Retirement age example\n\n# Combine data\ndata_negative_skew <- data.frame(\n    value = c(negative_skew_test, negative_skew_retirement),\n    example = c(rep(\"Easy Test Scores\", 1000), rep(\"Retirement Age\", 1000))\n)\n\n# Plot negative skew\nggplot(data_negative_skew, aes(x = value, fill = example)) +\n    geom_histogram(bins = 30,\n                   alpha = 0.7,\n                   position = \"identity\") +\n    facet_wrap( ~ example, scales = \"free\") +\n    labs(title = \"Visualization of Negative Skew\",\n         x = \"Value\",\n         y = \"Frequency\") +\n    theme_minimal()"},{"path":"prerequisites.html","id":"kurtosis","chapter":"2 Prerequisites","heading":"2.2.7 Kurtosis","text":"Kurtosis measures “peakedness” heaviness tails:High kurtosis: Tall, sharp peak heavy tails.\nExample: Financial market returns crisis (extreme losses gains).\nHigh kurtosis: Tall, sharp peak heavy tails.Example: Financial market returns crisis (extreme losses gains).Low kurtosis: Flatter peak thinner tails.\nExample: Human height distribution (fewer extreme deviations mean).\nLow kurtosis: Flatter peak thinner tails.Example: Human height distribution (fewer extreme deviations mean).left panel shows low kurtosis, similar distribution human height, flatter peak thinner tails.left panel shows low kurtosis, similar distribution human height, flatter peak thinner tails.right panel shows high kurtosis, reflecting financial market returns, extreme outliers gains losses.right panel shows high kurtosis, reflecting financial market returns, extreme outliers gains losses.","code":"\n# Simulate data for kurtosis\nlow_kurtosis <- runif(1000, 0, 10)  # Low kurtosis\nhigh_kurtosis <- c(rnorm(900, 5, 1), rnorm(100, 5, 5))  # High kurtosis\n\n# Combine data\ndata_kurtosis <- data.frame(\n  value = c(low_kurtosis, high_kurtosis),\n  kurtosis_type = c(rep(\"Low Kurtosis (Height Distribution)\", 1000), \n                    rep(\"High Kurtosis (Market Returns)\", 1000))\n)\n\n# Plot kurtosis\nggplot(data_kurtosis, aes(x = value, fill = kurtosis_type)) +\n  geom_histogram(bins = 30, alpha = 0.7, position = \"identity\") +\n  facet_wrap(~kurtosis_type) +\n  labs(\n    title = \"Visualization of Kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()"},{"path":"prerequisites.html","id":"conditional-moments","chapter":"2 Prerequisites","heading":"2.2.7.1 Conditional Moments","text":"random variable \\(Y\\) given \\(X=x\\):Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-moments","chapter":"2 Prerequisites","heading":"2.2.7.2 Multivariate Moments","text":"Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}\n\\]Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}\n\\]Variance-Covariance Matrix: \\[\n\\begin{aligned}\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}\n\\end{aligned}\n\\]Variance-Covariance Matrix: \\[\n\\begin{aligned}\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"prerequisites.html","id":"properties-of-moments","chapter":"2 Prerequisites","heading":"2.2.7.3 Properties of Moments","text":"\\(E[aX + + c] = aE[X] + [Y] + c\\)\\(\\text{Var}(aX + + c) = ^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\\)\\(\\text{Cov}(aX + , cX + dY) = ac \\text{Var}(X) + bd \\text{Var}(Y) + (ad + bc) \\text{Cov}(X, Y)\\)Correlation: \\(\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)","code":""},{"path":"prerequisites.html","id":"distributions","chapter":"2 Prerequisites","heading":"2.2.8 Distributions","text":"","code":""},{"path":"prerequisites.html","id":"conditional-distributions","chapter":"2 Prerequisites","heading":"2.2.8.1 Conditional Distributions","text":"\\[\nf_{X|Y}(x|y) = \\frac{f(x, y)}{f_Y(y)}\n\\]\\(X\\) \\(Y\\) independent:\\[\nf_{X|Y}(x|y) = f_X(x).\n\\]","code":""},{"path":"prerequisites.html","id":"discrete-distributions","chapter":"2 Prerequisites","heading":"2.2.8.2 Discrete Distributions","text":"","code":""},{"path":"prerequisites.html","id":"bernoulli-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.1 Bernoulli Distribution","text":"random variable \\(X\\) follows Bernoulli distribution, denoted \\(X \\sim \\text{Bernoulli}(p)\\), represents single trial :Success probability \\(p\\)Success probability \\(p\\)Failure probability \\(q = 1-p\\).Failure probability \\(q = 1-p\\).Density Function\\[\nf(x) = p^x (1-p)^{1-x}, \\quad x \\\\{0, 1\\}\n\\]CDF: Use table manual computation.PDFMean\\[\n\\mu = E[X] = p\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = p(1-p)\n\\]","code":"\nhist(\n    mc2d::rbern(1000, prob = 0.5),\n    main = \"Histogram of Bernoulli Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"binomial-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.2 Binomial Distribution","text":"\\(X \\sim B(n, p)\\) number successes \\(n\\) independent Bernoulli trials, :\\(n\\) number trials\\(n\\) number trials\\(p\\) success probability.\\(p\\) success probability.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.Density Function\\[\nf(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n\n\\]PDFMGF\\[\nm_X(t) = (1 - p + p e^t)^n\n\\]Mean\\[\n\\mu = np\n\\]Variance\\[\n\\sigma^2 = np(1-p)\n\\]","code":"\nhist(\n    rbinom(1000, size = 100, prob = 0.5),\n    main = \"Histogram of Binomial Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"poisson-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.3 Poisson Distribution","text":"\\(X \\sim \\text{Poisson}(\\lambda)\\) models number occurrences event fixed interval, average rate \\(\\lambda\\).Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.random variable \\(X\\) number occurrences event within interval \\(s\\) units.random variable \\(X\\) number occurrences event within interval \\(s\\) units.parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).Density Function\\[\nf(x) = \\frac{e^{-k} k^x}{x!}, \\quad x = 0, 1, 2, \\dots\n\\]CDFPDFMGF\\[\nm_X(t) = e^{k (e^t - 1)}\n\\]Mean\\[\n\\mu = E(X) = k\n\\]Variance\\[\n\\sigma^2 = Var(X) = k\n\\]","code":"\nhist(rpois(1000, lambda = 5),\n     main = \"Histogram of Poisson Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"geometric-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.4 Geometric Distribution","text":"\\(X \\sim \\text{G}(p)\\) models number trials needed obtain first success, :\\(p\\): probability success\\(p\\): probability success\\(q = 1-p\\): probability failure.\\(q = 1-p\\): probability failure.experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.Density Function\\[\nf(x) = p(1-p)^{x-1}, \\quad x = 1, 2, \\dots\n\\]CDF\\[\nF(x) = 1 - (1-p)^x\n\\]PDFMGF\\[\nm_X(t) = \\frac{p e^t}{1 - (1-p)e^t}, \\quad t < -\\ln(1-p)\n\\]Mean\\[\n\\mu = \\frac{1}{p}\n\\]Variance\\[\n\\sigma^2 = \\frac{1-p}{p^2}\n\\]","code":"\nhist(rgeom(1000, prob = 0.5),\n     main = \"Histogram of Geometric Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"hypergeometric-distribution","chapter":"2 Prerequisites","heading":"2.2.8.2.5 Hypergeometric Distribution","text":"\\(X \\sim \\text{H}(N, r, n)\\) models number successes sample size \\(n\\) drawn without replacement population size \\(N\\), :\\(r\\) objects trait interest\\(r\\) objects trait interest\\(N-r\\) trait.\\(N-r\\) trait.Density Function\\[\nf(x) = \\frac{\\binom{r}{x} \\binom{N-r}{n-x}}{\\binom{N}{n}}, \\quad \\max(0, n-(N-r)) \\leq x \\leq \\min(n, r)\n\\]PDFMean\\[\n\\mu = E[X] = \\frac{n r}{N}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = n \\frac{r}{N} \\frac{N-r}{N} \\frac{N-n}{N-1}\n\\]Note: large \\(N\\) (\\(\\frac{n}{N} \\leq 0.05\\)), hypergeometric distribution can approximated binomial distribution \\(p = \\frac{r}{N}\\).","code":"\nhist(\n    rhyper(1000, m = 50, n = 20, k = 30),\n    main = \"Histogram of Hypergeometric Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"continuous-distributions","chapter":"2 Prerequisites","heading":"2.2.8.3 Continuous Distributions","text":"","code":""},{"path":"prerequisites.html","id":"uniform-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.1 Uniform Distribution","text":"Defined interval \\((, b)\\), probabilities “equally likely” subintervals equal length.Density Function: \\[\nf(x) = \\frac{1}{b-}, \\quad < x < b\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x < \\\\\n\\frac{x-}{b-} & \\le x \\le b \\\\\n1 & \\text{} x > b\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) =\n\\begin{cases}\n\\frac{e^{tb} - e^{ta}}{t(b-)} & \\text{} t \\neq 0 \\\\\n1 & \\text{} t = 0\n\\end{cases}\n\\]Mean\\[\n\\mu = E[X] = \\frac{+ b}{2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{(b-)^2}{12}\n\\]","code":"\nhist(\n    runif(1000, min = 0, max = 1),\n    main = \"Histogram of Uniform Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"gamma-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.2 Gamma Distribution","text":"gamma distribution used define exponential \\(\\chi^2\\) distributions.gamma function defined : \\[\n\\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz, \\quad \\alpha > 0\n\\]Properties Gamma Function:\\(\\Gamma(1) = 1\\)\\(\\Gamma(1) = 1\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)Density Function:\\[\nf(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n\\]CDF (\\(\\alpha = n\\), \\(x>0\\) positive integer):\\[\nF(x, n, \\beta) = 1 - \\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!}\n\\]PDF:MGF\\[\nm_X(t) = (1 - \\beta t)^{-\\alpha}, \\quad t < \\frac{1}{\\beta}\n\\]Mean\\[\n\\mu = E[X] = \\alpha \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\alpha \\beta^2\n\\]","code":"\nhist(\n    rgamma(n = 1000, shape = 5, rate = 1),\n    main = \"Histogram of Gamma Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.3 Normal Distribution","text":"normal distribution, denoted \\(N(\\mu, \\sigma^2)\\), symmetric bell-shaped parameters \\(\\mu\\) (mean) \\(\\sigma^2\\) (variance). also known Gaussian distribution.Density Function:\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\quad -\\infty < x < \\infty, \\; \\sigma > 0\n\\]CDF: Use table numerical methods.PDFMGF\\[\nm_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}\n\\]Mean\\[\n\\mu = E[X]\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X)\n\\]Standard Normal Random Variable:normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]Normal Approximation Binomial Distribution:Let \\(X\\) binomial parameters \\(n\\) \\(p\\). large \\(n\\):\\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p > 0.5\\) \\(n(1-p) > 5\\),\\(p > 0.5\\) \\(n(1-p) > 5\\),\\(X\\) approximately normally distributed mean \\(\\mu = np\\) standard deviation \\(\\sigma = \\sqrt{np(1-p)}\\).using normal approximation, add subtract 0.5 needed continuity correction.Discrete Approximate Normal (Corrected):Normal Probability RuleIf X normally distributed parameters \\(\\mu\\) \\(\\sigma\\), \\(P(-\\sigma < X - \\mu < \\sigma) \\approx .68\\)\\(P(-2\\sigma < X - \\mu < 2\\sigma) \\approx .95\\)\\(P(-3\\sigma < X - \\mu < 3\\sigma) \\approx .997\\)","code":"\nhist(\n    rnorm(1000, mean = 0, sd = 1),\n    main = \"Histogram of Normal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"logistic-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.4 Logistic Distribution","text":"logistic distribution continuous probability distribution commonly used logistic regression types statistical modeling. resembles normal distribution heavier tails, allowing extreme values. - logistic distribution symmetric around \\(\\mu\\). - heavier tails make useful modeling outcomes occasional extreme values.Density Function\\[\nf(x; \\mu, s) = \\frac{e^{-(x-\\mu)/s}}{s \\left(1 + e^{-(x-\\mu)/s}\\right)^2}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(s > 0\\) scale parameter.CDF\\[\nF(x; \\mu, s) = \\frac{1}{1 + e^{-(x-\\mu)/s}}, \\quad -\\infty < x < \\infty\n\\]PDFMGFThe MGF logistic distribution exist expected value diverges \\(t\\).Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{\\pi^2 s^2}{3}\n\\]","code":"\nhist(\n    rlogis(1000, location = 0, scale = 1),\n    main = \"Histogram of Logistic Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"laplace-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.5 Laplace Distribution","text":"Laplace distribution, also known double exponential distribution, continuous probability distribution often used economics, finance, engineering. characterized peak mean heavier tails compared normal distribution.Laplace distribution symmetric around \\(\\mu\\).heavier tails normal distribution, making suitable modeling data extreme outliers.Density Function\\[\nf(x; \\mu, b) = \\frac{1}{2b} e^{-|x-\\mu|/b}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(b > 0\\) scale parameter.CDF\\[\nF(x; \\mu, b) =\n\\begin{cases}\n    \\frac{1}{2} e^{(x-\\mu)/b} & \\text{} x < \\mu \\\\\n    1 - \\frac{1}{2} e^{-(x-\\mu)/b} & \\text{} x \\ge \\mu\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = \\frac{e^{\\mu t}}{1 - b^2 t^2}, \\quad |t| < \\frac{1}{b}\n\\]Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2b^2\n\\]","code":"\nhist(\n    VGAM::rlaplace(1000, location = 0, scale = 1),\n    main = \"Histogram of Laplace Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"log-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.6 Log-normal Distribution","text":"log-normal distribution denoted \\(\\text{Lognormal}(\\mu, \\sigma^2)\\).PDF","code":"\nhist(rlnorm(n = 1000, meanlog = 0, sdlog = 1), main=\"Histogram of Log-normal Distribution\", xlab=\"Value\", ylab=\"Frequency\")"},{"path":"prerequisites.html","id":"lognormal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.7 Lognormal Distribution","text":"lognormal distribution continuous probability distribution random variable whose logarithm normally distributed. often used model variables positively skewed, income biological measurements.lognormal distribution positively skewed.useful modeling data take negative values often used finance environmental studies.Density Function\\[\nf(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-(\\ln(x) - \\mu)^2 / (2\\sigma^2)}, \\quad x > 0\n\\]\\(\\mu\\) mean underlying normal distribution \\(\\sigma > 0\\) standard deviation.CDFThe cumulative distribution function lognormal distribution given :\\[\nF(x; \\mu, \\sigma) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{\\ln(x) - \\mu}{\\sigma \\sqrt{2}} \\right) \\right], \\quad x > 0\n\\]PDFMGFThe moment generating function (MGF) lognormal distribution exist simple closed form.Mean\\[\nE[X] = e^{\\mu + \\sigma^2 / 2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\left( e^{\\sigma^2} - 1 \\right) e^{2\\mu + \\sigma^2}\n\\]","code":"\nhist(\n    rlnorm(1000, meanlog = 0, sdlog = 1),\n    main = \"Histogram of Lognormal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"exponential-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.8 Exponential Distribution","text":"exponential distribution, denoted \\(\\text{Exp}(\\lambda)\\), special case gamma distribution \\(\\alpha = 1\\).commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.exponential distribution memoryless, meaning probability event occurring future independent past.exponential distribution memoryless, meaning probability event occurring future independent past.commonly used model waiting times, time next customer arrives time radioactive particle decays.commonly used model waiting times, time next customer arrives time radioactive particle decays.Density Function\\[\nf(x) = \\frac{1}{\\beta} e^{-x/\\beta}, \\quad x, \\beta > 0\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x \\le 0 \\\\\n1 - e^{-x/\\beta} & \\text{} x > 0\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = (1-\\beta t)^{-1}, \\quad t < 1/\\beta\n\\]Mean\\[\n\\mu = E[X] = \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\beta^2\n\\]","code":"\nhist(rexp(n = 1000, rate = 1),\n     main = \"Histogram of Exponential Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"chi-squared-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.9 Chi-Squared Distribution","text":"chi-squared distribution continuous probability distribution commonly used statistical inference, particularly hypothesis testing construction confidence intervals variance. also used goodness--fit tests.chi-squared distribution defined positive values.often used model distribution sum squares \\(k\\) independent standard normal random variables.Density Function\\[\nf(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x \\ge 0\n\\]\\(k\\) degrees freedom \\(\\Gamma\\) gamma function.CDFThe cumulative distribution function chi-squared distribution given :\\[\nF(x; k) = \\frac{\\gamma(k/2, x/2)}{\\Gamma(k/2)}, \\quad x \\ge 0\n\\]\\(\\gamma\\) lower incomplete gamma function.PDFMGF\\[\nm_X(t) = (1 - 2t)^{-k/2}, \\quad t < \\frac{1}{2}\n\\]Mean\\[\nE[X] = k\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2k\n\\]","code":"\nhist(\n    rchisq(1000, df = 5),\n    main = \"Histogram of Chi-Squared Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"students-t-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.10 Student’s T Distribution","text":"Student’s t-distribution named William Sealy Gosset, statistician Guinness Brewery early 20th century. Gosset developed t-distribution address small-sample problems quality control. Since Guinness prohibited employees publishing names, Gosset used pseudonym “Student” published work 1908 (Student 1908). name stuck ever since, honoring contribution statistics.Student’s t-distribution, denoted \\(T(v)\\), defined : \\[\nT = \\frac{Z}{\\sqrt{\\chi^2_v / v}},\n\\] \\(Z\\) standard normal random variable \\(\\chi^2_v\\) follows chi-squared distribution \\(v\\) degrees freedom.Student’s T distribution continuous probability distribution used statistical inference, particularly estimating population parameters sample size small /population variance unknown. similar normal distribution heavier tails, makes robust small sample sizes.Student’s T distribution symmetric around 0.heavier tails normal distribution, making useful dealing outliers small sample sizes.Density Function\\[\nf(x;v) = \\frac{\\Gamma((v + 1)/2)}{\\sqrt{v \\pi} \\Gamma(v/2)} \\left( 1 + \\frac{x^2}{v} \\right)^{-(v + 1)/2}\n\\]\\(v\\) degrees freedom \\(\\Gamma(x)\\) Gamma function.CDFThe cumulative distribution function Student’s T distribution complex typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) Student’s T distribution exist simple closed form.MeanFor \\(v > 1\\):\\[\nE[X] = 0\n\\]VarianceFor \\(v > 2\\):\\[\n\\sigma^2 =  \\text{Var}(X) = \\frac{v}{v - 2}\n\\]","code":"\nhist(\n    rt(1000, df = 5),\n    main = \"Histogram of Student's T Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"non-central-t-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.11 Non-central T Distribution","text":"non-central t-distribution, denoted \\(T(v, \\lambda)\\), generalization Student’s t-distribution. defined : \\[\nT = \\frac{Z + \\lambda}{\\sqrt{\\chi^2_v / v}},\n\\] \\(Z\\) standard normal random variable, \\(\\chi^2_v\\) follows chi-squared distribution \\(v\\) degrees freedom, \\(\\lambda\\) non-centrality parameter. additional parameter introduces asymmetry distribution.non-central t-distribution arises scenarios null hypothesis hold, alternative hypothesis hypothesis testing. non-centrality parameter \\(\\lambda\\) represents degree mean deviates zero.\\(\\lambda = 0\\), non-central t-distribution reduces Student’s t-distribution.distribution skewed \\(\\lambda \\neq 0\\), skewness increasing \\(\\lambda\\) grows.Density FunctionThe density function non-central t-distribution complex depends \\(v\\) \\(\\lambda\\). can expressed terms infinite sum:\\[\nf(x; v, \\lambda) = \\sum_{k=0}^\\infty \\frac{e^{-\\lambda^2/2}(\\lambda^2/2)^k}{k!} \\cdot \\frac{\\Gamma((v + k + 1)/2)}{\\sqrt{v \\pi} \\Gamma((v + k)/2)} \\left( 1 + \\frac{x^2}{v} \\right)^{-(v + k + 1)/2}.\n\\]PDFCDFThe cumulative distribution function non-central t-distribution typically computed using numerical methods due complexity.MeanFor \\(v > 1\\):\\[\nE[T] = \\lambda \\sqrt{\\frac{v}{2}} \\cdot \\frac{\\Gamma((v - 1)/2)}{\\Gamma(v/2)}.\n\\]VarianceFor \\(v > 2\\):\\[\n\\text{Var}(T) = \\frac{v}{v - 2} + \\lambda^2.\n\\]Comparison: Student’s T vs. Non-central TShape \\(v \\\\infty\\)(df \\(\\\\infty\\))Student’s t-distribution used standard hypothesis testing confidence intervals, non-central t-distribution finds applications scenarios involving non-null hypotheses, power sample size calculations.","code":"\nn <- 100  # Number of samples\ndf <- 5    # Degrees of freedom\nlambda <- 2  # Non-centrality parameter\n\n\nhist(\n  rt(n, df = df, ncp = lambda),\n  main = \"Histogram of Non-central T Distribution\",\n  xlab = \"Value\",\n  ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"f-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.12 F Distribution","text":"F-distribution, denoted \\(F(d_1, d_2)\\), strictly positive used compare variances.Definition: \\[\nF = \\frac{\\chi^2_{d_1} / d_1}{\\chi^2_{d_2} / d_2},\n\\] \\(\\chi^2_{d_1}\\) \\(\\chi^2_{d_2}\\) independent chi-squared random variables degrees freedom \\(d_1\\) \\(d_2\\), respectively.distribution asymmetric never negative.F distribution arises frequently null distribution test statistic, especially context comparing variances, analysis variance (ANOVA).Density Function\\[\nf(x; d_1, d_2) = \\frac{\\sqrt{\\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\\left( \\frac{d_1}{2}, \\frac{d_2}{2} \\right)}, \\quad x > 0\n\\]\\(d_1\\) \\(d_2\\) degrees freedom \\(B\\) beta function.CDFThe cumulative distribution function F distribution typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) F distribution exist simple closed form.MeanFor \\(d_2 > 2\\):\\[\nE[X] = \\frac{d_2}{d_2 - 2}\n\\]VarianceFor \\(d_2 > 4\\):\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}\n\\]","code":"\nhist(\n    rf(1000, df1 = 5, df2 = 2),\n    main = \"Histogram of F Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"cauchy-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.13 Cauchy Distribution","text":"Cauchy distribution continuous probability distribution often used physics heavier tails normal distribution. notable finite mean variance.Cauchy distribution finite mean variance.Central Limit Theorem Weak Law Large Numbers apply Cauchy distribution.Density Function\\[\nf(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\left[ 1 + \\left( \\frac{x - x_0}{\\gamma}\n\\right)^2 \\right]}\n\\]\\(x_0\\) location parameter \\(\\gamma > 0\\) scale parameter.CDFThe cumulative distribution function Cauchy distribution given :\\[\nF(x; x_0, \\gamma) = \\frac{1}{\\pi} \\arctan \\left( \\frac{x - x_0}{\\gamma}  \\right) + \\frac{1}{2}\n\\]PDFMGFThe MGF Cauchy distribution exist.MeanThe mean Cauchy distribution undefined.VarianceThe variance Cauchy distribution undefined.","code":"\nhist(\n    rcauchy(1000, location = 0, scale = 1),\n    main = \"Histogram of Cauchy Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"multivariate-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.8.3.14 Multivariate Normal Distribution","text":"Let \\(y\\) \\(p\\)-dimensional multivariate normal (MVN) random variable mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\). density function \\(y\\) given :\\[ f(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mu)' \\Sigma^{-1} (\\mathbf{y}-\\mu)\\right) \\]\\(|\\mathbf{\\Sigma}|\\) represents determinant variance-covariance matrix \\(\\Sigma\\), \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\).Properties:Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{y} \\sim N_r(\\mathbf{\\mu}, \\mathbf{\\Sigma '})\\). Note \\(r \\le p\\), rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma '}\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma^{-1} = G G'}\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G'\\mu}, \\mathbf{})\\) \\(\\mathbf{G'(y - \\mu)} \\sim N_p(\\mathbf{0}, \\mathbf{})\\).fixed linear combination \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c'y}\\), follows \\(\\mathbf{c'y} \\sim N_1(\\mathbf{c'\\mu}, \\mathbf{c'\\Sigma c})\\).Large Sample PropertiesSuppose \\(y_1, \\dots, y_n\\) random sample population mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\):\\[ \\mathbf{Y} \\sim MVN(\\mathbf{\\mu}, \\mathbf{\\Sigma}) \\]:\\(\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{=1}^n \\mathbf{y}_i\\) consistent estimator \\(\\mathbf{\\mu}\\).\\(\\mathbf{S} = \\frac{1}{n-1} \\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})'\\) consistent estimator \\(\\mathbf{\\Sigma}\\).Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\sim N_p(\\mathbf{0}, \\mathbf{\\Sigma})\\) \\(n\\) large relative \\(p\\) (e.g., \\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma/n})\\).Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S^{-1}} (\\bar{\\mathbf{y}} - \\mu) \\sim \\chi^2_{(p)}\\) \\(n\\) large relative \\(p\\).Density Function\\[\nf(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} | \\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\n\\right)\n\\]\\(\\boldsymbol{\\mu}\\) mean vector, \\(\\boldsymbol{\\Sigma}\\) covariance matrix, \\(\\mathbf{x} \\\\mathbb{R}^k\\) \\(k\\) number variables.CDFThe cumulative distribution function multivariate normal distribution simple closed form typically evaluated using numerical methods.PDFMGF\\[\nm_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left(\\boldsymbol{\\mu}^T \\mathbf{t} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t}\n\\right)\n\\]Mean\\[\nE[\\mathbf{X}] = \\boldsymbol{\\mu}\n\\]Variance\\[\n\\text{Var}(\\mathbf{X}) = \\boldsymbol{\\Sigma}\n\\]","code":"\nk <- 2\nn <- 1000\nmu <- c(0, 0)\nsigma <- matrix(c(1, 0.5, 0.5, 1), nrow = k)\nlibrary(MASS)\nhist(\n    mvrnorm(n, mu = mu, Sigma = sigma)[,1],\n    main = \"Histogram of MVN Distribution (1st Var)\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"general-math","chapter":"2 Prerequisites","heading":"2.3 General Math","text":"","code":""},{"path":"prerequisites.html","id":"number-sets","chapter":"2 Prerequisites","heading":"2.3.1 Number Sets","text":"","code":""},{"path":"prerequisites.html","id":"summation-notation-and-series","chapter":"2 Prerequisites","heading":"2.3.2 Summation Notation and Series","text":"","code":""},{"path":"prerequisites.html","id":"chebyshevs-inequality","chapter":"2 Prerequisites","heading":"2.3.2.1 Chebyshev’s Inequality","text":"Let \\(X\\) random variable mean \\(\\mu\\) standard deviation \\(\\sigma\\). positive number \\(k\\), Chebyshev’s Inequality states:\\[\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\n\\]provides probabilistic bound deviation \\(X\\) mean require \\(X\\) follow normal distribution.","code":""},{"path":"prerequisites.html","id":"geometric-sum","chapter":"2 Prerequisites","heading":"2.3.2.2 Geometric Sum","text":"geometric series form \\(\\sum_{k=0}^{n-1} ar^k\\), sum given :\\[\n\\sum_{k=0}^{n-1} ar^k = \\frac{1-r^n}{1-r} \\quad \\text{} r \\neq 1\n\\]","code":""},{"path":"prerequisites.html","id":"infinite-geometric-series","chapter":"2 Prerequisites","heading":"2.3.2.3 Infinite Geometric Series","text":"\\(|r| < 1\\), geometric series converges :\\[\n\\sum_{k=0}^\\infty ar^k = \\frac{}{1-r}\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-theorem","chapter":"2 Prerequisites","heading":"2.3.2.4 Binomial Theorem","text":"binomial expansion \\((x + y)^n\\) :\\[\n(x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} y^k \\quad \\text{} n \\geq 0\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-series","chapter":"2 Prerequisites","heading":"2.3.2.5 Binomial Series","text":"non-integer exponents \\(\\alpha\\):\\[\n\\sum_{k=0}^\\infty \\binom{\\alpha}{k} x^k = (1 + x)^\\alpha \\quad \\text{} |x| < 1\n\\]","code":""},{"path":"prerequisites.html","id":"telescoping-sum","chapter":"2 Prerequisites","heading":"2.3.2.6 Telescoping Sum","text":"telescoping sum simplifies intermediate terms cancel, leaving:\\[\n\\sum_{\\leq k < b} \\Delta F(k) = F(b) - F() \\quad \\text{} , b \\\\mathbb{Z}, \\leq b\n\\]","code":""},{"path":"prerequisites.html","id":"vandermonde-convolution","chapter":"2 Prerequisites","heading":"2.3.2.7 Vandermonde Convolution","text":"Vandermonde convolution identity :\\[\n\\sum_{k=0}^n \\binom{r}{k} \\binom{s}{n-k} = \\binom{r+s}{n} \\quad \\text{} n \\\\mathbb{Z}\n\\]","code":""},{"path":"prerequisites.html","id":"exponential-series","chapter":"2 Prerequisites","heading":"2.3.2.8 Exponential Series","text":"exponential function \\(e^x\\) can represented :\\[\n\\sum_{k=0}^\\infty \\frac{x^k}{k!} = e^x \\quad \\text{} x \\\\mathbb{C}\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-series","chapter":"2 Prerequisites","heading":"2.3.2.9 Taylor Series","text":"Taylor series expansion function \\(f(x)\\) \\(x=\\) :\\[\n\\sum_{k=0}^\\infty \\frac{f^{(k)}()}{k!} (x-)^k = f(x)\n\\]\\(= 0\\), becomes Maclaurin series.","code":""},{"path":"prerequisites.html","id":"maclaurin-series-for-ez","chapter":"2 Prerequisites","heading":"2.3.2.10 Maclaurin Series for \\(e^z\\)","text":"special case Taylor series, Maclaurin expansion \\(e^z\\) :\\[\ne^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots\n\\]","code":""},{"path":"prerequisites.html","id":"eulers-summation-formula","chapter":"2 Prerequisites","heading":"2.3.2.11 Euler’s Summation Formula","text":"Euler’s summation formula connects sums integrals:\\[\n\\sum_{\\leq k < b} f(k) = \\int_a^b f(x) \\, dx + \\sum_{k=1}^m \\frac{B_k}{k!} \\left[f^{(k-1)}(x)\\right]_a^b\n+ (-1)^{m+1} \\int_a^b \\frac{B_m(x-\\lfloor x \\rfloor)}{m!} f^{(m)}(x) \\, dx\n\\], \\(B_k\\) Bernoulli numbers.\\(m=1\\) (Trapezoidal Rule):\\[\n\\sum_{\\leq k < b} f(k) \\approx \\int_a^b f(x) \\, dx - \\frac{1}{2}(f(b) - f())\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-expansion","chapter":"2 Prerequisites","heading":"2.3.3 Taylor Expansion","text":"differentiable function, \\(G(x)\\), can written infinite sum derivatives. specifically, \\(G(x)\\) infinitely differentiable evaluated \\(\\), Taylor expansion :\\[\nG(x) = G() + \\frac{G'()}{1!} (x-) + \\frac{G''()}{2!}(x-)^2 + \\frac{G'''()}{3!}(x-)^3 + \\dots\n\\]expansion valid within radius convergence.","code":""},{"path":"prerequisites.html","id":"law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4 Law of Large Numbers","text":"Let \\(X_1, X_2, \\ldots\\) infinite sequence independent identically distributed (..d.) random variables finite mean \\(\\mu\\) variance \\(\\sigma^2\\). Law Large Numbers (LLN) states sample average:\\[\n\\bar{X}_n = \\frac{1}{n} \\sum_{=1}^n X_i\n\\]converges expected value \\(\\mu\\) \\(n \\rightarrow \\infty\\). can expressed :\\[\n\\bar{X}_n \\rightarrow \\mu \\quad \\text{($n \\rightarrow \\infty$)}.\n\\]","code":""},{"path":"prerequisites.html","id":"variance-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.3.4.1 Variance of the Sample Mean","text":"variance sample mean decreases sample size increases:\\[\nVar(\\bar{X}_n) = Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) = \\frac{\\sigma^2}{n}.\n\\]\\[\n\\begin{aligned}\nVar(\\bar{X}_n) &= Var(\\frac{1}{n}(X_1 + ... + X_n)) =Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2}Var(X_1 + ... + X_n) \\\\\n&=\\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}\n\\end{aligned}\n\\]Note: connection Law Large Numbers Normal Distribution lies Central Limit Theorem. CLT states , regardless original distribution dataset, distribution sample means tend follow normal distribution sample size becomes larger.difference [Weak Law] [Strong Law] regards mode convergence.","code":""},{"path":"prerequisites.html","id":"weak-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.2 Weak Law of Large Numbers","text":"Weak Law Large Numbers states sample average converges probability expected value:\\[\n\\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Formally, \\(\\epsilon > 0\\):\\[\n\\lim_{n \\\\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0.\n\\]Additionally, sample mean ..d. random sample (\\(\\{ X_i \\}_{=1}^n\\)) population finite mean variance consistent estimator population mean \\(\\mu\\):\\[\nplim(\\bar{X}_n) = plim\\left(\\frac{1}{n}\\sum_{=1}^{n} X_i\\right) = \\mu.\n\\]","code":""},{"path":"prerequisites.html","id":"strong-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.3 Strong Law of Large Numbers","text":"Strong Law Large Numbers states sample average converges almost surely expected value:\\[\n\\bar{X}_n \\xrightarrow{.s.} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Equivalently, can expressed :\\[\nP\\left(\\lim_{n \\\\infty} \\bar{X}_n = \\mu\\right) = 1.\n\\]","code":""},{"path":"prerequisites.html","id":"convergence","chapter":"2 Prerequisites","heading":"2.3.5 Convergence","text":"","code":""},{"path":"prerequisites.html","id":"convergence-in-probability","chapter":"2 Prerequisites","heading":"2.3.5.1 Convergence in Probability","text":"\\(n \\rightarrow \\infty\\), estimator (random variable) \\(\\theta_n\\) said converge probability constant \\(c\\) :\\[\n\\lim_{n \\\\infty} P(|\\theta_n - c| \\geq \\epsilon) = 0 \\quad \\text{} \\epsilon > 0.\n\\]denoted :\\[\nplim(\\theta_n) = c \\quad \\text{equivalently, } \\theta_n \\xrightarrow{p} c.\n\\]Properties Convergence Probability:Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\n\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\n\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\n\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\n\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).\n\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).properties extend random vectors matrices.properties extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"convergence-in-distribution","chapter":"2 Prerequisites","heading":"2.3.5.2 Convergence in Distribution","text":"\\(n \\rightarrow \\infty\\), distribution random variable \\(X_n\\) may converge another (“fixed”) distribution. Formally, \\(X_n\\) CDF \\(F_n(x)\\) converges distribution \\(X\\) CDF \\(F(x)\\) :\\[\n\\lim_{n \\\\infty} |F_n(x) - F(x)| = 0\n\\]points continuity \\(F(x)\\). denoted :\\[\nX_n \\xrightarrow{d} X \\quad \\text{equivalently, } F(x) \\text{ limiting distribution } X_n.\n\\]Asymptotic Properties:\\(E(X)\\): Limiting mean (asymptotic mean).\\(Var(X)\\): Limiting variance (asymptotic variance).Note: Limiting expectations variances necessarily match expectations variances \\(X_n\\):\\[\n\\begin{aligned}\nE(X) &\\neq \\lim_{n \\\\infty} E(X_n), \\\\\nAvar(X_n) &\\neq \\lim_{n \\\\infty} Var(X_n).\n\\end{aligned}\n\\]Properties Convergence Distribution:Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\n\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]\\(Y_n \\xrightarrow{d} c\\) (constant), :\n\\(X_n + Y_n \\xrightarrow{d} X + c\\),\n\\(Y_n X_n \\xrightarrow{d} c X\\),\n\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).\n\\(Y_n \\xrightarrow{d} c\\) (constant), :\\(X_n + Y_n \\xrightarrow{d} X + c\\),\\(Y_n X_n \\xrightarrow{d} c X\\),\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).properties also extend random vectors matrices.properties also extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"summary-properties-of-convergence","chapter":"2 Prerequisites","heading":"2.3.5.3 Summary: Properties of Convergence","text":"Relationship Convergence Types:Convergence Probability stronger Convergence Distribution. Therefore:Convergence Distribution guarantee Convergence Probability.","code":""},{"path":"prerequisites.html","id":"sufficient-statistics-and-likelihood","chapter":"2 Prerequisites","heading":"2.3.6 Sufficient Statistics and Likelihood","text":"","code":""},{"path":"prerequisites.html","id":"likelihood","chapter":"2 Prerequisites","heading":"2.3.6.1 Likelihood","text":"likelihood describes degree observed data supports particular value parameter \\(\\theta\\).exact value likelihood meaningful; relative comparisons matter.Likelihood informative comparing parameter values, helping identify values \\(\\theta\\) plausible given data.single observation \\(Y = y\\), likelihood function defined :\\[\nL(\\theta_0; y) = P(Y = y \\mid \\theta = \\theta_0) = f_Y(y; \\theta_0),\n\\]\\(f_Y(y; \\theta_0)\\) probability density (mass) function \\(Y\\) parameter \\(\\theta_0\\).Key Insight: likelihood tells us plausible \\(\\theta\\) , given data observed. probability, proportional probability observing data given parameter value.Example: Suppose \\(Y\\) follows binomial distribution \\(n=10\\) trials probability success \\(p\\):\\[\nP(Y = y \\mid p) = \\binom{10}{y} p^y (1-p)^{10-y}.\n\\]\\(y=7\\) observed successes, likelihood function becomes:\\[\nL(p; y=7) = \\binom{10}{7} p^7 (1-p)^3.\n\\]can use compare well different values \\(p\\) explain observed data.","code":""},{"path":"prerequisites.html","id":"likelihood-ratio","chapter":"2 Prerequisites","heading":"2.3.6.2 Likelihood Ratio","text":"likelihood ratio compares relative likelihood two parameter values \\(\\theta_0\\) \\(\\theta_1\\) given observed data:\\[\n\\text{Likelihood Ratio} = \\frac{L(\\theta_0; y)}{L(\\theta_1; y)}.\n\\]likelihood ratio greater 1 implies \\(\\theta_0\\) likely \\(\\theta_1\\), given observed data.Likelihood ratios widely used hypothesis testing model comparison evaluate evidence null hypothesis.Example: binomial example , consider \\(p_0 = 0.7\\) \\(p_1 = 0.5\\). likelihood ratio :\\[\n\\frac{L(p_0; y=7)}{L(p_1; y=7)} = \\frac{\\binom{10}{7} (0.7)^7 (0.3)^3}{\\binom{10}{7} (0.5)^7 (0.5)^3}.\n\\]simplifies :\\[\n\\frac{(0.7)^7 (0.3)^3}{(0.5)^7 (0.5)^3}.\n\\]likelihood ratio quantifies much likely \\(p_0\\) compared \\(p_1\\) given observed data.","code":""},{"path":"prerequisites.html","id":"likelihood-function","chapter":"2 Prerequisites","heading":"2.3.6.3 Likelihood Function","text":"given sample, likelihood possible values \\(\\theta\\) forms likelihood function:\\[\nL(\\theta) = L(\\theta; y) = f_Y(y; \\theta).\n\\]sample size \\(n\\), assuming independence among observations:\\[\nL(\\theta) = \\prod_{=1}^{n} f_Y(y_i; \\theta).\n\\]Taking natural logarithm likelihood gives log-likelihood function:\\[\nl(\\theta) = \\sum_{=1}^{n} \\log f_Y(y_i; \\theta).\n\\]Log-Likelihood?log-likelihood simplifies computation turning products sums.particularly useful optimization, many numerical methods (e.g., gradient-based algorithms) perform better sums products.Example: \\(Y_1, Y_2, \\dots, Y_n\\) ..d. observations normal distribution \\(N(\\mu, \\sigma^2)\\), likelihood :\\[\nL(\\mu, \\sigma^2) = \\prod_{=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right).\n\\]log-likelihood :\\[\nl(\\mu, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{=1}^n (y_i - \\mu)^2.\n\\]","code":""},{"path":"prerequisites.html","id":"sufficient-statistics","chapter":"2 Prerequisites","heading":"2.3.6.4 Sufficient Statistics","text":"sufficient statistic \\(T(y)\\) summary data retains information parameter \\(\\theta\\). allows us focus condensed statistic without losing inferential power regarding \\(\\theta\\).Formal Definition:statistic \\(T(y)\\) sufficient parameter \\(\\theta\\) conditional probability distribution data \\(y\\), given \\(T(y)\\) \\(\\theta\\), depend \\(\\theta\\). Mathematically:\\[ P(Y = y \\mid T(y), \\theta) = P(Y = y \\mid T(y)). \\]Alternatively, Factorization Theorem, \\(T(y)\\) sufficient likelihood can written :\\[ L(\\theta; y) = c(y) L^*(\\theta; T(y)), \\]:\\(c(y)\\) function data independent \\(\\theta\\).\\(L^*(\\theta; T(y))\\) function depends \\(\\theta\\) \\(T(y)\\).words, likelihood function can rewritten terms \\(T(y)\\) alone, without loss information \\(\\theta\\).Sufficient Statistics Matter:allow us simplify analysis reducing data without losing inferential power.Many inferential procedures (e.g., Maximum Likelihood Estimation, Bayesian methods) simplified working sufficient statistics.Example:Consider sample ..d. observations \\(Y_1, Y_2, \\dots, Y_n\\) normal distribution \\(N(\\mu, \\sigma^2)\\). :sample mean \\(\\bar{Y} = \\frac{1}{n} \\sum_{=1}^n Y_i\\) sufficient \\(\\mu\\).sample variance \\(S^2 = \\frac{1}{n-1} \\sum_{=1}^n (Y_i - \\bar{Y})^2\\) sufficient \\(\\sigma^2\\).Verification: joint density \\(y_1, y_2, \\dots, y_n\\) can factored :\\[\nf(y_1, \\dots, y_n; \\mu, \\sigma^2) = \\underbrace{\\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{=1}^n (y_i - \\bar{y})^2\\right)}_{L^*(\\mu, \\sigma^2; \\bar{y}, s^2)}\n\\cdot \\underbrace{\\text{[independent $\\mu$, $\\sigma^2$]}}_{c(y)}.\n\\]shows \\(\\bar{Y}\\) \\(S^2\\) sufficient.Usage Sufficient StatisticsMaximum Likelihood Estimation (MLE): MLE, sufficient statistics simplify optimization problem reducing data without losing information.\nExample: normal distribution case, \\(\\mu\\) can estimated using sufficient statistic \\(\\bar{Y}\\): \\[\n\\hat{\\mu}_{MLE} = \\bar{Y}.\n\\]Maximum Likelihood Estimation (MLE): MLE, sufficient statistics simplify optimization problem reducing data without losing information.Example: normal distribution case, \\(\\mu\\) can estimated using sufficient statistic \\(\\bar{Y}\\): \\[\n\\hat{\\mu}_{MLE} = \\bar{Y}.\n\\]Bayesian Inference: Bayesian analysis, posterior distribution depends sufficient statistic rather entire data set. normal case: \\[\nP(\\mu \\mid \\bar{Y}) \\propto P(\\mu) L(\\mu; \\bar{Y}).\n\\]Bayesian Inference: Bayesian analysis, posterior distribution depends sufficient statistic rather entire data set. normal case: \\[\nP(\\mu \\mid \\bar{Y}) \\propto P(\\mu) L(\\mu; \\bar{Y}).\n\\]Data Compression: practice, sufficient statistics reduce complexity data storage analysis condensing relevant information smaller representation.Data Compression: practice, sufficient statistics reduce complexity data storage analysis condensing relevant information smaller representation.","code":""},{"path":"prerequisites.html","id":"nuisance-parameters","chapter":"2 Prerequisites","heading":"2.3.6.5 Nuisance Parameters","text":"Parameters direct interest analysis necessary model data called nuisance parameters.Profile Likelihood: handle nuisance parameters, replace maximum likelihood estimates (MLEs) likelihood function, creating profile likelihood parameter interest.Example Profile Likelihood:regression model parameters \\(\\beta\\) (coefficients) \\(\\sigma^2\\) (error variance), \\(\\sigma^2\\) often nuisance parameter. profile likelihood \\(\\beta\\) obtained substituting MLE \\(\\sigma^2\\) likelihood:\\[\nL_p(\\beta) = L(\\beta, \\hat{\\sigma}^2),\n\\]\\(\\hat{\\sigma}^2\\) MLE \\(\\sigma^2\\) given \\(\\beta\\).simplifies problem focus parameter interest, \\(\\beta\\).","code":""},{"path":"prerequisites.html","id":"parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7 Parameter Transformations","text":"Transformations parameters often used improve interpretability statistical properties models.","code":""},{"path":"prerequisites.html","id":"log-odds-transformation","chapter":"2 Prerequisites","heading":"2.3.7.1 Log-Odds Transformation","text":"log-odds transformation commonly used logistic regression binary classification problems. transforms probabilities (bounded 0 1) real line:\\[\n\\text{Log odds} = g(\\theta) = \\ln\\left(\\frac{\\theta}{1-\\theta}\\right),\n\\]\\(\\theta\\) represents probability (e.g., success probability Bernoulli trial).","code":""},{"path":"prerequisites.html","id":"general-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7.2 General Parameter Transformations","text":"parameter \\(\\theta\\) transformation \\(g(\\cdot)\\):\\(\\theta \\(, b)\\), \\(g(\\theta)\\) may map \\(\\theta\\) different range (e.g., \\(\\mathbb{R}\\)).Useful transformations include:\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).\nExponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).\nSquare root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).Exponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).Square root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).Jacobian Adjustment Transformations: transforming parameter Bayesian inference, Jacobian transformation must included ensure proper posterior scaling.","code":""},{"path":"prerequisites.html","id":"applications-of-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.7.3 Applications of Parameter Transformations","text":"Improving Interpretability:\nProbabilities can transformed odds log-odds logistic models.\nRates can transformed logarithmically multiplicative effects.\nProbabilities can transformed odds log-odds logistic models.Rates can transformed logarithmically multiplicative effects.Statistical Modeling:\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).\nRegularization simplification complex relationships.\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).Regularization simplification complex relationships.Optimization:\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.","code":""},{"path":"prerequisites.html","id":"data-importexport","chapter":"2 Prerequisites","heading":"2.4 Data Import/Export","text":"Extended Manual RTable Rio VignetteR limitations:default, R use 1 core CPUBy default, R use 1 core CPUR puts data memory (limit around 2-4 GB), SAS uses data files demandR puts data memory (limit around 2-4 GB), SAS uses data files demandCategorization\nMedium-size file: within RAM limit, around 1-2 GB\nLarge file: 2-10 GB, might workaround solution\nlarge file > 10 GB, use distributed parallel computing\nCategorizationMedium-size file: within RAM limit, around 1-2 GBMedium-size file: within RAM limit, around 1-2 GBLarge file: 2-10 GB, might workaround solutionLarge file: 2-10 GB, might workaround solutionVery large file > 10 GB, use distributed parallel computingVery large file > 10 GB, use distributed parallel computingSolutions:buy RAMbuy RAMHPC packages\nExplicit Parallelism\nImplicit Parallelism\nLarge Memory\nMap/Reduce\nHPC packagesExplicit ParallelismExplicit ParallelismImplicit ParallelismImplicit ParallelismLarge MemoryLarge MemoryMap/ReduceMap/Reducespecify number rows columns, typically including command nrow =specify number rows columns, typically including command nrow =Use packages store data differently\nbigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class type\nmultiple class types, use ff package\nUse packages store data differentlybigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typebigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typeFor multiple class types, use ff packageFor multiple class types, use ff packageVery Large datasets use\nRHaddop package\nHadoopStreaming\nRhipe\nLarge datasets useRHaddop packageHadoopStreamingRhipe","code":""},{"path":"prerequisites.html","id":"medium-size","chapter":"2 Prerequisites","heading":"2.4.1 Medium size","text":"import multiple files directoryTo export single data fileTo export multiple data filesTo convert data file types","code":"\nlibrary(\"rio\")\nstr(import_list(dir()), which = 1)\nexport(data, \"data.csv\")\nexport(data,\"data.dta\")\nexport(data,\"data.txt\")\nexport(data,\"data_cyl.rds\")\nexport(data,\"data.rdata\")\nexport(data,\"data.R\")\nexport(data,\"data.csv.zip\")\nexport(data,\"list.json\")\nexport(list(mtcars = mtcars, iris = iris), \"data_file_type\") \n# where data_file_type should substituted with the extension listed above\n# convert Stata to SPSS\nconvert(\"data.dta\", \"data.sav\")"},{"path":"prerequisites.html","id":"large-size","chapter":"2 Prerequisites","heading":"2.4.2 Large size","text":"","code":""},{"path":"prerequisites.html","id":"cloud-computing-using-aws-for-big-data","chapter":"2 Prerequisites","heading":"2.4.2.1 Cloud Computing: Using AWS for Big Data","text":"Amazon Web Service (AWS): Compute resources can rented approximately $1/hr. Use AWS process large datasets without overwhelming local machine.","code":""},{"path":"prerequisites.html","id":"importing-large-files-as-chunks","chapter":"2 Prerequisites","heading":"2.4.2.2 Importing Large Files as Chunks","text":"","code":""},{"path":"prerequisites.html","id":"using-base-r","chapter":"2 Prerequisites","heading":"2.4.2.2.1 Using Base R","text":"","code":"\nfile_in <- file(\"in.csv\", \"r\")  # Open a connection to the file\nchunk_size <- 100000            # Define chunk size\nx <- readLines(file_in, n = chunk_size)  # Read data in chunks\nclose(file_in)                  # Close the file connection"},{"path":"prerequisites.html","id":"using-the-data.table-package","chapter":"2 Prerequisites","heading":"2.4.2.2.2 Using the data.table Package","text":"","code":"\nlibrary(data.table)\nmydata <- fread(\"in.csv\", header = TRUE)  # Fast and memory-efficient"},{"path":"prerequisites.html","id":"using-the-ff-package","chapter":"2 Prerequisites","heading":"2.4.2.2.3 Using the ff Package","text":"","code":"\nlibrary(ff)\nx <- read.csv.ffdf(\n  file = \"file.csv\",\n  nrow = 10,          # Total rows\n  header = TRUE,      # Include headers\n  VERBOSE = TRUE,     # Display progress\n  first.rows = 10000, # Initial chunk\n  next.rows = 50000,  # Subsequent chunks\n  colClasses = NA\n)"},{"path":"prerequisites.html","id":"using-the-bigmemory-package","chapter":"2 Prerequisites","heading":"2.4.2.2.4 Using the bigmemory Package","text":"","code":"\nlibrary(bigmemory)\nmy_data <- read.big.matrix('in.csv', header = TRUE)"},{"path":"prerequisites.html","id":"using-the-sqldf-package","chapter":"2 Prerequisites","heading":"2.4.2.2.5 Using the sqldf Package","text":"","code":"\nlibrary(sqldf)\nmy_data <- read.csv.sql('in.csv')\n\n# Example: Filtering during import\niris2 <- read.csv.sql(\"iris.csv\", \n    sql = \"SELECT * FROM file WHERE Species = 'setosa'\")"},{"path":"prerequisites.html","id":"using-the-rmysql-package","chapter":"2 Prerequisites","heading":"2.4.2.2.6 Using the RMySQL Package","text":"RQLite packageDownload SQLite, pick “bundle command-line tools managing SQLite database files” Window 10Unzip file, open sqlite3.exe.Type prompt\nsqlite> .cd 'C:\\Users\\data' specify path desired directory\nsqlite> .open database_name.db open database\nimport CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\n\nsqlite> .exit ’re done, exit sqlite program\nsqlite> .cd 'C:\\Users\\data' specify path desired directorysqlite> .open database_name.db open databaseTo import CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\nsqlite> .mode csv specify SQLite next file .csv filesqlite> .import file_name.csv datbase_name import csv file databasesqlite> .exit ’re done, exit sqlite program","code":"\nlibrary(RMySQL)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(\"RSQLite\")\nsetwd(\"\")\ncon <- dbConnect(RSQLite::SQLite(), \"data_base.db\")\ntbl <- tbl(con, \"data_table\")\ntbl %>% \n    filter() %>%\n    select() %>%\n    collect() # to actually pull the data into the workspace\ndbDisconnect(con)"},{"path":"prerequisites.html","id":"using-the-arrow-package","chapter":"2 Prerequisites","heading":"2.4.2.2.7 Using the arrow Package","text":"","code":"\nlibrary(arrow)\ndata <- read_csv_arrow(\"file.csv\")"},{"path":"prerequisites.html","id":"using-the-vroom-package","chapter":"2 Prerequisites","heading":"2.4.2.2.8 Using the vroom Package","text":"","code":"\nlibrary(vroom)\n\n# Import a compressed CSV file\ncompressed <- vroom_example(\"mtcars.csv.zip\")\ndata <- vroom(compressed)"},{"path":"prerequisites.html","id":"using-the-data.table-package-1","chapter":"2 Prerequisites","heading":"2.4.2.2.9 Using the data.table Package","text":"","code":"\ns = fread(\"sample.csv\")"},{"path":"prerequisites.html","id":"comparisons-regarding-storage-space","chapter":"2 Prerequisites","heading":"2.4.2.2.10 Comparisons Regarding Storage Space","text":"work large datasets, can compress csv.gz format. However, typically, R requires loading entire dataset exporting , can impractical data 10 GB. cases, processing data sequentially becomes necessary. Although read.csv slower compared readr::read_csv, can handle connections allows sequential looping, making useful large files.Currently, readr::read_csv support skip argument efficiently large data. Even specify skip, function reads preceding lines . instance, run read_csv(file, n_max = 100, skip = 0) followed read_csv(file, n_max = 200, skip = 100), first 100 rows re-read. contrast, read.csv can continue left without re-reading previous rows.encounter error :“Error (function (con, , n = 1L, size = NA_integer_, signed = TRUE): can read binary connection”,can modify connection mode \"r\" \"rb\" (read binary). Although file function designed detect appropriate format automatically, workaround can help resolve issue behave expected.","code":"\ntest = ff::read.csv.ffdf(file = \"\")\nobject.size(test) # Highest memory usage\n\ntest1 = data.table::fread(file = \"\")\nobject.size(test1) # Lowest memory usage\n\ntest2 = readr::read_csv(file = \"\")\nobject.size(test2) # Second lowest memory usage\n\ntest3 = vroom::vroom(file = \"\")\nobject.size(test3) # Similar to read_csv"},{"path":"prerequisites.html","id":"sequential-processing-for-large-data","chapter":"2 Prerequisites","heading":"2.4.2.3 Sequential Processing for Large Data","text":"","code":"\n# Open file for sequential reading\nfile_conn <- file(\"file.csv\", open = \"r\")\nwhile (TRUE) {\n  # Read a chunk of data\n  data_chunk <- read.csv(file_conn, nrows = 1000)\n  if (nrow(data_chunk) == 0) break  # Stop if no more rows\n  # Process the chunk here\n}\nclose(file_conn)  # Close connection"},{"path":"prerequisites.html","id":"data-manipulation","chapter":"2 Prerequisites","heading":"2.5 Data Manipulation","text":"verbs data manipulationselect: selecting (selecting) columns based names (eg: select columns Q1 Q25)slice: selecting (selecting) rows based position (eg: select rows 1:10)mutate: add derive new columns (variables) based existing columns (eg: create new column expresses measurement cm based existing measure inches)rename: rename variables change column names (eg: change “GraduationRate100” “grad100”)filter: selecting rows based condition (eg: rows gender = Male)arrange: ordering rows based variable(s) numeric alphabetical order (eg: sort descending order Income)sample: take random samples data (eg: sample 80% data create “training” set)summarize: condense aggregate multiple values single summary values (eg: calculate median income age group)group_by: convert tbl grouped tbl operations performed “group”; allows us summarize data apply verbs data groups (eg, gender treatment)pipe: %>%\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudio\npipe takes output function “pipes” first argument next function.\nnew pipe |> identical old one, except certain special cases.\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioThe pipe takes output function “pipes” first argument next function.pipe takes output function “pipes” first argument next function.new pipe |> identical old one, except certain special cases.new pipe |> identical old one, except certain special cases.:= (Walrus operator): similar = , cases want use glue package (.e., dynamic changes variable name left-hand side)Writing function RTunneling{{ (called curly-curly) allows tunnel data-variables arg-variables (.e., function arguments)","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# -----------------------------\n# Data Structures in R\n# -----------------------------\n\n# Create vectors\nx <- c(1, 4, 23, 4, 45)\nn <- c(1, 3, 5)\ng <- c(\"M\", \"M\", \"F\")\n\n# Create a data frame\ndf <- data.frame(n, g)\ndf  # View the data frame\n#>   n g\n#> 1 1 M\n#> 2 3 M\n#> 3 5 F\nstr(df)  # Check its structure\n#> 'data.frame':    3 obs. of  2 variables:\n#>  $ n: num  1 3 5\n#>  $ g: chr  \"M\" \"M\" \"F\"\n\n# Using tibble for cleaner outputs\ndf <- tibble(n, g)\ndf  # View the tibble\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\nstr(df)\n#> tibble [3 × 2] (S3: tbl_df/tbl/data.frame)\n#>  $ n: num [1:3] 1 3 5\n#>  $ g: chr [1:3] \"M\" \"M\" \"F\"\n\n# Create a list\nlst <- list(x, n, g, df)\nlst  # Display the list\n#> [[1]]\n#> [1]  1  4 23  4 45\n#> \n#> [[2]]\n#> [1] 1 3 5\n#> \n#> [[3]]\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> [[4]]\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Name list elements\nlst2 <- list(num = x, size = n, sex = g, data = df)\nlst2  # Named list elements are easier to reference\n#> $num\n#> [1]  1  4 23  4 45\n#> \n#> $size\n#> [1] 1 3 5\n#> \n#> $sex\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> $data\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Another list example with numeric vectors\nlst3 <- list(\n  x = c(1, 3, 5, 7),\n  y = c(2, 2, 2, 4, 5, 5, 5, 6),\n  z = c(22, 3, 3, 3, 5, 10)\n)\nlst3\n#> $x\n#> [1] 1 3 5 7\n#> \n#> $y\n#> [1] 2 2 2 4 5 5 5 6\n#> \n#> $z\n#> [1] 22  3  3  3  5 10\n\n# Find means of list elements\n# One at a time\nmean(lst3$x)\n#> [1] 4\nmean(lst3$y)\n#> [1] 3.875\nmean(lst3$z)\n#> [1] 7.666667\n\n# Using lapply to calculate means\nlapply(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Simplified output with sapply\nsapply(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# Tidyverse alternative: map() function\nmap(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Tidyverse with numeric output: map_dbl()\nmap_dbl(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# -----------------------------\n# Binding Data Frames\n# -----------------------------\n\n# Create tibbles for demonstration\ndat01 <- tibble(x = 1:5, y = 5:1)\ndat02 <- tibble(x = 10:16, y = x / 2)\ndat03 <- tibble(z = runif(5))  # 5 random numbers from (0, 1)\n\n# Row binding\nbind_rows(dat01, dat02, dat01)\n#> # A tibble: 17 × 2\n#>        x     y\n#>    <int> <dbl>\n#>  1     1   5  \n#>  2     2   4  \n#>  3     3   3  \n#>  4     4   2  \n#>  5     5   1  \n#>  6    10   5  \n#>  7    11   5.5\n#>  8    12   6  \n#>  9    13   6.5\n#> 10    14   7  \n#> 11    15   7.5\n#> 12    16   8  \n#> 13     1   5  \n#> 14     2   4  \n#> 15     3   3  \n#> 16     4   2  \n#> 17     5   1\n\n# Add a new identifier column with .id\nbind_rows(dat01, dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 1         1   5  \n#>  2 1         2   4  \n#>  3 1         3   3  \n#>  4 1         4   2  \n#>  5 1         5   1  \n#>  6 2        10   5  \n#>  7 2        11   5.5\n#>  8 2        12   6  \n#>  9 2        13   6.5\n#> 10 2        14   7  \n#> 11 2        15   7.5\n#> 12 2        16   8\n\n# Use named inputs for better identification\nbind_rows(\"dat01\" = dat01, \"dat02\" = dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 dat01     1   5  \n#>  2 dat01     2   4  \n#>  3 dat01     3   3  \n#>  4 dat01     4   2  \n#>  5 dat01     5   1  \n#>  6 dat02    10   5  \n#>  7 dat02    11   5.5\n#>  8 dat02    12   6  \n#>  9 dat02    13   6.5\n#> 10 dat02    14   7  \n#> 11 dat02    15   7.5\n#> 12 dat02    16   8\n\n# Bind a list of data frames\nlist01 <- list(\"dat01\" = dat01, \"dat02\" = dat02)\nbind_rows(list01, .id = \"source\")\n#> # A tibble: 12 × 3\n#>    source     x     y\n#>    <chr>  <int> <dbl>\n#>  1 dat01      1   5  \n#>  2 dat01      2   4  \n#>  3 dat01      3   3  \n#>  4 dat01      4   2  \n#>  5 dat01      5   1  \n#>  6 dat02     10   5  \n#>  7 dat02     11   5.5\n#>  8 dat02     12   6  \n#>  9 dat02     13   6.5\n#> 10 dat02     14   7  \n#> 11 dat02     15   7.5\n#> 12 dat02     16   8\n\n# Column binding\nbind_cols(dat01, dat03)\n#> # A tibble: 5 × 3\n#>       x     y     z\n#>   <int> <int> <dbl>\n#> 1     1     5 0.265\n#> 2     2     4 0.410\n#> 3     3     3 0.780\n#> 4     4     2 0.926\n#> 5     5     1 0.501\n\n# -----------------------------\n# String Manipulation\n# -----------------------------\n\nnames <- c(\"Ford, MS\", \"Jones, PhD\", \"Martin, Phd\", \"Huck, MA, MLS\")\n\n# Remove everything after the first comma\nstr_remove(names, pattern = \", [[:print:]]+\")\n#> [1] \"Ford\"   \"Jones\"  \"Martin\" \"Huck\"\n\n# Explanation: [[:print:]]+ matches one or more printable characters\n\n# -----------------------------\n# Reshaping Data\n# -----------------------------\n\n# Wide format data\nwide <- data.frame(\n  name = c(\"Clay\", \"Garrett\", \"Addison\"),\n  test1 = c(78, 93, 90),\n  test2 = c(87, 91, 97),\n  test3 = c(88, 99, 91)\n)\n\n# Long format data\nlong <- data.frame(\n  name = rep(c(\"Clay\", \"Garrett\", \"Addison\"), each = 3),\n  test = rep(1:3, 3),\n  score = c(78, 87, 88, 93, 91, 99, 90, 97, 91)\n)\n\n# Summary statistics\naggregate(score ~ name, data = long, mean)  # Mean score per student\n#>      name    score\n#> 1 Addison 92.66667\n#> 2    Clay 84.33333\n#> 3 Garrett 94.33333\naggregate(score ~ test, data = long, mean)  # Mean score per test\n#>   test    score\n#> 1    1 87.00000\n#> 2    2 91.66667\n#> 3    3 92.66667\n\n# Line plot of scores over tests\nggplot(long,\n       aes(\n           x = factor(test),\n           y = score,\n           color = name,\n           group = name\n       )) +\n    geom_point() +\n    geom_line() +\n    xlab(\"Test\") +\n    ggtitle(\"Test Scores by Student\")\n\n# Reshape wide to long\npivot_longer(wide, test1:test3, names_to = \"test\", values_to = \"score\")\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    test1    78\n#> 2 Clay    test2    87\n#> 3 Clay    test3    88\n#> 4 Garrett test1    93\n#> 5 Garrett test2    91\n#> 6 Garrett test3    99\n#> 7 Addison test1    90\n#> 8 Addison test2    97\n#> 9 Addison test3    91\n\n# Use names_prefix to clean column names\npivot_longer(\n    wide,\n    -name,\n    names_to = \"test\",\n    values_to = \"score\",\n    names_prefix = \"test\"\n)\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    1        78\n#> 2 Clay    2        87\n#> 3 Clay    3        88\n#> 4 Garrett 1        93\n#> 5 Garrett 2        91\n#> 6 Garrett 3        99\n#> 7 Addison 1        90\n#> 8 Addison 2        97\n#> 9 Addison 3        91\n\n# Reshape long to wide with explicit id_cols argument\npivot_wider(\n  long,\n  id_cols = name, \n  names_from = test,\n  values_from = score\n)\n#> # A tibble: 3 × 4\n#>   name      `1`   `2`   `3`\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\n\n# Add a prefix to the resulting columns\npivot_wider(\n  long,\n  id_cols = name,  \n  names_from = test,\n  values_from = score,\n  names_prefix = \"test\"\n)\n#> # A tibble: 3 × 4\n#>   name    test1 test2 test3\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\nlibrary(tidyverse)\n# -----------------------------\n# Writing Functions with {{ }}\n# -----------------------------\n\n# Define a custom function using {{ }}\nget_mean <- function(data, group_var, var_to_mean) {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(mean = mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the function\ndata(\"mtcars\")\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl  mean\n#>   <dbl> <dbl>\n#> 1     4  26.7\n#> 2     6  19.7\n#> 3     8  15.1\n\n# Dynamically name the resulting variable\nget_mean <- function(data, group_var, var_to_mean, prefix = \"mean_of\") {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(\"{prefix}_{{var_to_mean}}\" := mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the modified function\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl mean_of_mpg\n#>   <dbl>       <dbl>\n#> 1     4        26.7\n#> 2     6        19.7\n#> 3     8        15.1"},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"3 Descriptive Statistics","heading":"3 Descriptive Statistics","text":"area interest research, problem solve, relationship investigate, theoretical empirical processes help .Estimand: Defined “quantity scientific interest can calculated population change value depending data collection design used measure (.e., vary sample size, survey design, number non-respondents, follow-efforts).” (Rubin 1996)Examples estimands include:Population meansPopulation variancesCorrelationsFactor loadingsRegression coefficients","code":""},{"path":"descriptive-statistics.html","id":"numerical-measures","chapter":"3 Descriptive Statistics","heading":"3.1 Numerical Measures","text":"differences population sample:SkewnessStandardized 3rd central moment (unitless)\\(m_2 = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\overline{y})^2\\)\\(m_3 = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\overline{y})^3\\)Kurtosis(peakedness tail thickness) Standardized 4th central momentNotes:Order Statistics: \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), \\(y_{(1)} < y_{(2)} < \\ldots < y_{(n)}\\).Order Statistics: \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), \\(y_{(1)} < y_{(2)} < \\ldots < y_{(n)}\\).Coefficient Variation:\nDefined standard deviation divided mean.\nstable, unitless statistic useful comparison.\nCoefficient Variation:Defined standard deviation divided mean.stable, unitless statistic useful comparison.Symmetry:\nSymmetric distributions: Mean = Median; Skewness = 0.\nSkewed Right: Mean > Median; Skewness > 0.\nSkewed Left: Mean < Median; Skewness < 0.\nSymmetry:Symmetric distributions: Mean = Median; Skewness = 0.Skewed Right: Mean > Median; Skewness > 0.Skewed Left: Mean < Median; Skewness < 0.Central Moments:\n\\(\\mu = E(Y)\\)\n\\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\)\n\\(\\mu_3 = E[(Y-\\mu)^3]\\)\n\\(\\mu_4 = E[(Y-\\mu)^4]\\)\nCentral Moments:\\(\\mu = E(Y)\\)\\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\)\\(\\mu_3 = E[(Y-\\mu)^3]\\)\\(\\mu_4 = E[(Y-\\mu)^4]\\)Skewness (\\(\\hat{g_1}\\))Sampling Distribution:\nsamples drawn normal population:\n\\(\\hat{g_1}\\) approximately distributed \\(N(0, \\frac{6}{n})\\) \\(n > 150\\).\n\\(\\hat{g_1}\\) approximately distributed \\(N(0, \\frac{6}{n})\\) \\(n > 150\\).Inference:\nLarge Samples: Inference skewness can based standard normal distribution.\n95% confidence interval \\(g_1\\) given : \\[\n\\hat{g_1} \\pm 1.96 \\sqrt{\\frac{6}{n}}\n\\]\nSmall Samples: small samples, consult special tables :\nSnedecor Cochran (1989), Table 19()\nMonte Carlo test results\n\nLarge Samples: Inference skewness can based standard normal distribution.\n95% confidence interval \\(g_1\\) given : \\[\n\\hat{g_1} \\pm 1.96 \\sqrt{\\frac{6}{n}}\n\\]Small Samples: small samples, consult special tables :\nSnedecor Cochran (1989), Table 19()\nMonte Carlo test results\nSnedecor Cochran (1989), Table 19()Monte Carlo test resultsKurtosis (\\(\\hat{g_2}\\))Definitions Relationships:\nnormal distribution kurtosis \\(g_2^* = 3\\).\nKurtosis often redefined : \\[\ng_2 = \\frac{E[(Y - \\mu)^4]}{\\sigma^4} - 3\n\\] 4th central moment estimated : \\[\nm_4 = \\frac{\\sum_{=1}^n (y_i - \\overline{y})^4}{n}\n\\]\nnormal distribution kurtosis \\(g_2^* = 3\\).\nKurtosis often redefined : \\[\ng_2 = \\frac{E[(Y - \\mu)^4]}{\\sigma^4} - 3\n\\] 4th central moment estimated : \\[\nm_4 = \\frac{\\sum_{=1}^n (y_i - \\overline{y})^4}{n}\n\\]Sampling Distribution:\nlarge samples (\\(n > 1000\\)):\n\\(\\hat{g_2}\\) approximately distributed \\(N(0, \\frac{24}{n})\\).\n\\(\\hat{g_2}\\) approximately distributed \\(N(0, \\frac{24}{n})\\).Inference:\nLarge Samples: Inference kurtosis can use standard normal tables.\nSmall Samples: Refer specialized tables :\nSnedecor Cochran (1989), Table 19(ii)\nGeary (1936)\n\nLarge Samples: Inference kurtosis can use standard normal tables.Small Samples: Refer specialized tables :\nSnedecor Cochran (1989), Table 19(ii)\nGeary (1936)\nSnedecor Cochran (1989), Table 19(ii)Geary (1936)","code":"\n# Generate random data from a normal distribution\ndata <- rnorm(100)\n\n# Load the e1071 package for skewness and kurtosis functions\nlibrary(e1071)\n\n# Calculate skewness\nskewness_value <- skewness(data)\ncat(\"Skewness:\", skewness_value, \"\\n\")\n#> Skewness: 0.362615\n\n# Calculate kurtosis\nkurtosis_value <- kurtosis(data)\ncat(\"Kurtosis:\", kurtosis_value, \"\\n\")\n#> Kurtosis: -0.3066409"},{"path":"descriptive-statistics.html","id":"graphical-measures","chapter":"3 Descriptive Statistics","heading":"3.2 Graphical Measures","text":"following table summarizes key graphical measures along guidance use . detailed explanations, visual examples, sample code discussed table.Tips Selecting Right Plot:Focus Question: comparing groups, investigating correlations, just exploring overall shape data?Focus Question: comparing groups, investigating correlations, just exploring overall shape data?Match Plot Data Type: Continuous vs. categorical data often dictates choice chart.Match Plot Data Type: Continuous vs. categorical data often dictates choice chart.Mind Data Size: plots become cluttered lose clarity large datasets (e.g., stem--leaf), others may less informative data points.Mind Data Size: plots become cluttered lose clarity large datasets (e.g., stem--leaf), others may less informative data points.","code":""},{"path":"descriptive-statistics.html","id":"shape","chapter":"3 Descriptive Statistics","heading":"3.2.1 Shape","text":"Properly labeling graphs essential ensure viewers can easily understand data presented. several examples graphical measures used assess shape dataset.advanced plot types can provide deeper insights data:","code":"\n# Generate random data for demonstration purposes\ndata <- rnorm(100)\n\n# Histogram: A graphical representation of the distribution of a dataset.\nhist(\n    data,\n    labels = TRUE,\n    col = \"grey\",\n    breaks = 12,\n    main = \"Histogram of Random Data\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)\n\n# Interactive Histogram: Using 'highcharter' for a more interactive visualization.\n# pacman::p_load(\"highcharter\")\n# hchart(data, type = \"column\", name = \"Random Data Distribution\")\n\n# Box-and-Whisker Plot: Useful for visualizing the distribution and identifying outliers.\nboxplot(\n    count ~ spray,\n    data = InsectSprays,\n    col = \"lightgray\",\n    main = \"Boxplot of Insect Sprays\",\n    xlab = \"Spray Type\",\n    ylab = \"Count\"\n)\n\n# Notched Boxplot: The notches indicate a confidence interval around the median.\nboxplot(\n    len ~ supp * dose,\n    data = ToothGrowth,\n    notch = TRUE,\n    col = c(\"gold\", \"darkgreen\"),\n    main = \"Tooth Growth by Supplement and Dose\",\n    xlab = \"Supplement and Dose\",\n    ylab = \"Length\"\n)\n# If the notches of two boxes do not overlap, this suggests that the medians differ significantly.\n\n# Stem-and-Leaf Plot: Provides a quick way to visualize the distribution of data.\nstem(data)\n#> \n#>   The decimal point is at the |\n#> \n#>   -2 | 4321000\n#>   -1 | 87665\n#>   -1 | 44433222111000\n#>   -0 | 998888886666665555\n#>   -0 | 433322221100\n#>    0 | 0112233333344\n#>    0 | 5666677888999999\n#>    1 | 0111122344\n#>    1 | 699\n#>    2 | 34\n\n# Bagplot - A 2D Boxplot Extension: Visualizes the spread and identifies outliers in two-dimensional data.\npacman::p_load(aplpack)\nattach(mtcars)\nbagplot(wt,\n        mpg,\n        xlab = \"Car Weight\",\n        ylab = \"Miles Per Gallon\",\n        main = \"Bagplot of Car Weight vs. Miles Per Gallon\")\ndetach(mtcars)\n# boxplot.matrix(): Creates boxplots for each column in a matrix. Useful for comparing multiple variables.\ngraphics::boxplot.matrix(\n    cbind(\n        Uni05 = (1:100) / 21,\n        Norm = rnorm(100),\n        T5 = rt(100, df = 5),\n        Gam2 = rgamma(100, shape = 2)\n    ),\n    main = \"Boxplot Marix\",\n    notch = TRUE,\n    col = 1:4\n)\n\n# Violin Plot (vioplot()): Combines a boxplot with a density plot, providing more information about the distribution.\nlibrary(\"vioplot\")\nvioplot(data, col = \"lightblue\", main = \"Violin Plot Example\")"},{"path":"descriptive-statistics.html","id":"scatterplot","chapter":"3 Descriptive Statistics","heading":"3.2.2 Scatterplot","text":"Scatterplots useful visualizing relationships two continuous variables. help identify patterns, correlations, outliers.Pairwise Scatterplots: Visualizes relationships pairs variables dataset. especially useful exploring potential correlations.","code":"\npairs(mtcars,\n      main = \"Pairwise Scatterplots\",\n      pch = 19,\n      col = \"blue\")"},{"path":"descriptive-statistics.html","id":"normality-assessment","chapter":"3 Descriptive Statistics","heading":"3.3 Normality Assessment","text":"Normal (Gaussian) distribution plays critical role statistical analyses due theoretical practical applications. Many statistical methods assume normality data, making essential assess whether variable interest follows normal distribution. achieve , utilize Numerical Measures Graphical Assessment.","code":""},{"path":"descriptive-statistics.html","id":"graphical-assessment","chapter":"3 Descriptive Statistics","heading":"3.3.1 Graphical Assessment","text":"Graphical methods provide intuitive way visually inspect normality dataset. One common methods Q-Q plot (quantile-quantile plot). Q-Q plot compares quantiles sample data quantiles theoretical normal distribution. Deviations line indicate departures normality.example using qqnorm qqline functions R assess normality precip dataset, contains precipitation data (inches per year) 70 U.S. cities:InterpretationTheoretical Line: red line represents expected relationship data perfectly normally distributed.Theoretical Line: red line represents expected relationship data perfectly normally distributed.Data Points: dots represent actual empirical data.Data Points: dots represent actual empirical data.points closely align theoretical line, can conclude data likely follow normal distribution. However, noticeable deviations line, particularly systematic patterns (e.g., curves s-shaped patterns), indicate potential departures normality.TipsSmall Deviations: Minor deviations line small datasets uncommon may significantly impact analyses assume normality.Small Deviations: Minor deviations line small datasets uncommon may significantly impact analyses assume normality.Systematic Patterns: Look clear trends, clusters s-shaped curves, suggest skewness heavy tails.Systematic Patterns: Look clear trends, clusters s-shaped curves, suggest skewness heavy tails.Complementary Tests: Always pair graphical methods numerical measures (e.g., Shapiro-Wilk test) make robust conclusion.Complementary Tests: Always pair graphical methods numerical measures (e.g., Shapiro-Wilk test) make robust conclusion.interpreting Q-Q plot, helpful see ideal non-ideal scenarios. illustrative example:Normal Data: Points fall closely along line.Normal Data: Points fall closely along line.Skewed Data: Points systematically deviate line, curving upward downward.Skewed Data: Points systematically deviate line, curving upward downward.Heavy Tails: Points deviate extremes (ends) distribution.Heavy Tails: Points deviate extremes (ends) distribution.combining visual inspection numerical measures, can better understand nature data alignment assumption normality.","code":"\n# Load the required package\npacman::p_load(\"car\")\n\n# Generate a Q-Q plot\nqqnorm(precip,\n       ylab = \"Precipitation [in/yr] for 70 US cities\",\n       main = \"Q-Q Plot of Precipitation Data\")\nqqline(precip, col = \"red\")"},{"path":"descriptive-statistics.html","id":"summary-statistics","chapter":"3 Descriptive Statistics","heading":"3.3.2 Summary Statistics","text":"graphical assessments, Q-Q plots, provide visual indication normality, may always offer definitive conclusion. supplement graphical methods, statistical tests often employed. tests provide quantitative evidence support refute assumption normality. common methods can classified two categories:Methods Based Normal Probability Plot\nCorrelation Coefficient Normal Probability Plots\nShapiro-Wilk Test\nMethods Based Normal Probability PlotCorrelation Coefficient Normal Probability PlotsShapiro-Wilk TestMethods based empirical cumulative distribution function\nAnderson-Darling Test\nKolmogorov-Smirnov Test\nCramer-von Mises Test\nJarque–Bera Test\nMethods based empirical cumulative distribution functionAnderson-Darling TestKolmogorov-Smirnov TestCramer-von Mises TestJarque–Bera Test","code":""},{"path":"descriptive-statistics.html","id":"methods-based-on-normal-probability-plot","chapter":"3 Descriptive Statistics","heading":"3.3.2.1 Methods Based on Normal Probability Plot","text":"","code":""},{"path":"descriptive-statistics.html","id":"correlation-coefficient-with-normal-probability-plots","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.1 Correlation Coefficient with Normal Probability Plots","text":"described Looney Gulledge Jr (1985) Samuel S. Shapiro Francia (1972), method evaluates linearity normal probability plot calculating correlation coefficient ordered sample values \\(y_{()}\\) theoretical normal quantiles \\(m_i^*\\). perfectly linear relationship suggests data follow normal distribution.correlation coefficient, denoted \\(W^*\\), given :\\[\nW^* = \\frac{\\sum_{=1}^{n}(y_{()}-\\bar{y})(m_i^* - 0)}{\\sqrt{\\sum_{=1}^{n}(y_{()}-\\bar{y})^2 \\cdot \\sum_{=1}^{n}(m_i^* - 0)^2}}\n\\]:\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(\\bar{m^*} = 0\\) null hypothesis normality.\\(\\bar{m^*} = 0\\) null hypothesis normality.Pearson product-moment correlation formula can also used evaluate relationship:\\[\n\\hat{\\rho} = \\frac{\\sum_{=1}^{n}(y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum_{=1}^{n}(y_i - \\bar{y})^2 \\cdot \\sum_{=1}^{n}(x_i - \\bar{x})^2}}\n\\]Interpretation:\ncorrelation 1, plot exactly linear, normality assumed.\ncloser correlation 0, stronger evidence reject normality.\nInference \\(W^*\\) requires reference special tables (Looney Gulledge Jr 1985).\ncorrelation 1, plot exactly linear, normality assumed.closer correlation 0, stronger evidence reject normality.Inference \\(W^*\\) requires reference special tables (Looney Gulledge Jr 1985).","code":"\nlibrary(\"EnvStats\")\n\n# Perform Probability Plot Correlation Coefficient (PPCC) Test\ngofTest(data, test = \"ppcc\")$p.value # Probability Plot Correlation Coefficient\n#> [1] 0.3701575"},{"path":"descriptive-statistics.html","id":"shapiro-wilk-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.2 Shapiro-Wilk Test","text":"Shapiro-Wilk test (Samuel Sanford Shapiro Wilk 1965) one widely used tests assessing normality, especially sample sizes \\(n < 2000\\). test evaluates well data’s order statistics match theoretical normal distribution. test statistic, \\(W\\), computed :\\[\nW=\\frac{\\sum_{=1}^{n}a_i x_{()}}{\\sum_{=1}^{n}(x_{()}-\\bar{x})^2}\n\\]\\(n\\): sample size.\\(n\\): sample size.\\(x_{()}\\): \\(\\)-th smallest value sample (ordered data).\\(x_{()}\\): \\(\\)-th smallest value sample (ordered data).\\(\\bar{x}\\): sample mean.\\(\\bar{x}\\): sample mean.\\(a_i\\): Weights derived expected values variances order statistics normal distribution, precomputed based sample size \\(n\\).\\(a_i\\): Weights derived expected values variances order statistics normal distribution, precomputed based sample size \\(n\\).Sensitive :Symmetry\nShapiro-Wilk test assesses whether sample drawn normal distribution, assumes symmetry around mean.\ndata exhibit skewness (lack symmetry), test likely reject null hypothesis normality.\nShapiro-Wilk test assesses whether sample drawn normal distribution, assumes symmetry around mean.data exhibit skewness (lack symmetry), test likely reject null hypothesis normality.Heavy Tails\nHeavy tails refer distributions extreme values (outliers) likely compared normal distribution.\nShapiro-Wilk test also sensitive departures normality heavy tails affect spread variance, central calculation test statistic \\(W\\).\nHeavy tails refer distributions extreme values (outliers) likely compared normal distribution.Shapiro-Wilk test also sensitive departures normality heavy tails affect spread variance, central calculation test statistic \\(W\\).Hence, Shapiro-Wilk test’s sensitivity deviations makes powerful tool detecting non-normality small moderate-sized samples. However:generally sensitive symmetry (skewness) tail behavior (kurtosis).generally sensitive symmetry (skewness) tail behavior (kurtosis).large samples, even small deviations symmetry tail behavior may cause test reject null hypothesis, even data practically “normal” intended analysis.\nSmall sample sizes may lack power detect deviations normality.\nLarge sample sizes may detect minor deviations practically significant.\nlarge samples, even small deviations symmetry tail behavior may cause test reject null hypothesis, even data practically “normal” intended analysis.Small sample sizes may lack power detect deviations normality.Small sample sizes may lack power detect deviations normality.Large sample sizes may detect minor deviations practically significant.Large sample sizes may detect minor deviations practically significant.Key Steps:Sort Data: Arrange sample data ascending order, yielding \\(x_{(1)}, x_{(2)}, \\dots, x_{(n)}\\).Sort Data: Arrange sample data ascending order, yielding \\(x_{(1)}, x_{(2)}, \\dots, x_{(n)}\\).Compute Weights: weights \\(a_i\\) determined using covariance matrix normal order statistics. optimized maximize power test.Compute Weights: weights \\(a_i\\) determined using covariance matrix normal order statistics. optimized maximize power test.Calculate \\(W\\): Use formula determine \\(W\\), ranges 0 1.Calculate \\(W\\): Use formula determine \\(W\\), ranges 0 1.Decision Rule:Null Hypothesis (\\(H_0\\)): data follows normal distribution.Null Hypothesis (\\(H_0\\)): data follows normal distribution.Alternative Hypothesis (\\(H_1\\)): data follow normal distribution.Alternative Hypothesis (\\(H_1\\)): data follow normal distribution.small \\(W\\) value, along \\(p\\)-value chosen significance level (e.g., 0.05), leads rejection \\(H_0\\).\nnormality, \\(W\\) approaches 1.\nSmaller values \\(W\\) indicate deviations normality.\nsmall \\(W\\) value, along \\(p\\)-value chosen significance level (e.g., 0.05), leads rejection \\(H_0\\).normality, \\(W\\) approaches 1.normality, \\(W\\) approaches 1.Smaller values \\(W\\) indicate deviations normality.Smaller values \\(W\\) indicate deviations normality.","code":"\n# Perform Shapiro-Wilk Test (Default for gofTest)\nEnvStats::gofTest(mtcars$mpg, test = \"sw\")\n#> \n#> Results of Goodness-of-Fit Test\n#> -------------------------------\n#> \n#> Test Method:                     Shapiro-Wilk GOF\n#> \n#> Hypothesized Distribution:       Normal\n#> \n#> Estimated Parameter(s):          mean = 20.090625\n#>                                  sd   =  6.026948\n#> \n#> Estimation Method:               mvue\n#> \n#> Data:                            mtcars$mpg\n#> \n#> Sample Size:                     32\n#> \n#> Test Statistic:                  W = 0.9475647\n#> \n#> Test Statistic Parameter:        n = 32\n#> \n#> P-value:                         0.1228814\n#> \n#> Alternative Hypothesis:          True cdf does not equal the\n#>                                  Normal Distribution."},{"path":"descriptive-statistics.html","id":"methods-based-on-empirical-cumulative-distribution-function","chapter":"3 Descriptive Statistics","heading":"3.3.2.2 Methods Based on Empirical Cumulative Distribution Function","text":"Empirical Cumulative Distribution Function (ECDF) way represent distribution sample dataset cumulative terms. answers question:“fraction observations dataset less equal given value \\(x\\)?”ECDF defined :\\[\nF_n(x) = \\frac{1}{n} \\sum_{=1}^{n} \\mathbb{}(X_i \\leq x)\n\\]:\\(F_(x)\\): ECDF value \\(x\\).\\(F_(x)\\): ECDF value \\(x\\).\\(n\\): Total number data points.\\(n\\): Total number data points.\\(\\mathbb{}(X_i \\leq x)\\): Indicator function, equal 1 \\(X_i \\leq x\\), otherwise 0.\\(\\mathbb{}(X_i \\leq x)\\): Indicator function, equal 1 \\(X_i \\leq x\\), otherwise 0.method especially useful large sample sizes can applied distributions beyond normal (Gaussian) distribution.Properties ECDFStep Function: ECDF step function increases \\(1/n\\) data point.Non-decreasing: \\(x\\) increases, \\(F_n(x)\\) never decreases.Range: ECDF starts 0 ends 1:\n\\(F_n(x) = 0\\) \\(x < \\min(X)\\).\n\\(F_n(x) = 1\\) \\(x \\geq \\max(X)\\).\n\\(F_n(x) = 0\\) \\(x < \\min(X)\\).\\(F_n(x) = 1\\) \\(x \\geq \\max(X)\\).Convergence: \\(n \\\\infty\\), ECDF approaches true cumulative distribution function (CDF) population.Let’s consider sample dataset \\(\\{3, 7, 7, 10, 15\\}\\). ECDF different values \\(x\\) calculated :Applications ECDFGoodness--fit Tests: Compare ECDF theoretical CDF (e.g., using Kolmogorov-Smirnov test).Goodness--fit Tests: Compare ECDF theoretical CDF (e.g., using Kolmogorov-Smirnov test).Outlier Detection: Analyze cumulative trends spot unusual data points.Outlier Detection: Analyze cumulative trends spot unusual data points.Visual Data Exploration: Use ECDF understand spread, skewness, distribution data.Visual Data Exploration: Use ECDF understand spread, skewness, distribution data.Comparing Distributions: Compare ECDFs two datasets assess differences distributions.Comparing Distributions: Compare ECDFs two datasets assess differences distributions.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Sample dataset\ndata <- c(3, 7, 7, 10, 15)\n\n# ECDF calculation\necdf_function <- ecdf(data)\n\n# Generate a data frame for plotting\necdf_data <- data.frame(x = sort(unique(data)),\n                        ecdf = sapply(sort(unique(data)), function(x)\n                          mean(data <= x)))\n\n# Display ECDF values\nprint(ecdf_data)\n#>    x ecdf\n#> 1  3  0.2\n#> 2  7  0.6\n#> 3 10  0.8\n#> 4 15  1.0\n\n# Plot the ECDF\nggplot(ecdf_data, aes(x = x, y = ecdf)) +\n  geom_step() +\n  labs(\n    title = \"Empirical Cumulative Distribution Function\",\n    x = \"Data Values\",\n    y = \"Cumulative Proportion\"\n  ) +\n  theme_minimal()\n# Alternatively\nplot.ecdf(as.numeric(mtcars[1, ]),\n          verticals = TRUE,\n          do.points = FALSE)"},{"path":"descriptive-statistics.html","id":"anderson-darling-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.1 Anderson-Darling Test","text":"Anderson-Darling test statistic (T. W. Anderson Darling 1952) given :\\[\n^2 = \\int_{-\\infty}^{\\infty} \\frac{\\left(F_n(t) - F(t)\\right)^2}{F(t)(1 - F(t))} dF(t)\n\\]test calculates weighted average squared deviations empirical cumulative distribution function (CDF), \\(F_n(t)\\), theoretical CDF, \\(F(t)\\). weight given deviations tails distribution, makes test particularly sensitive regions.sample size \\(n\\), ordered observations \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\), Anderson-Darling test statistic can also written :\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\ln(F(y_{()})) + (2n + 1 - 2i) \\ln(1 - F(y_{()})) \\right]\n\\]normal distribution, test statistic simplified. Using transformation:\\[\np_i = \\Phi\\left(\\frac{y_{()} - \\bar{y}}{s}\\right),\n\\]:\\(p_i\\) cumulative probability standard normal distribution,\\(p_i\\) cumulative probability standard normal distribution,\\(y_{()}\\) ordered sample values,\\(y_{()}\\) ordered sample values,\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(s\\) sample standard deviation,\\(s\\) sample standard deviation,formula becomes:\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\ln(p_i) + (2n + 1 - 2i) \\ln(1 - p_i) \\right].\n\\]Key Features TestCDF-Based Weighting: Anderson-Darling test gives weight deviations tails, makes particularly sensitive detecting non-normality regions.CDF-Based Weighting: Anderson-Darling test gives weight deviations tails, makes particularly sensitive detecting non-normality regions.Sensitivity: Compared goodness--fit tests, Kolmogorov-Smirnov Test, Anderson-Darling test better identifying differences tails distribution.Sensitivity: Compared goodness--fit tests, Kolmogorov-Smirnov Test, Anderson-Darling test better identifying differences tails distribution.Integral Form: test statistic can also expressed integral theoretical CDF: \\[\n^2 = n \\int_{-\\infty}^\\infty \\frac{\\left[F_n(t) - F(t)\\right]^2}{F(t)(1 - F(t))} dF(t),\n\\] \\(F_n(t)\\) empirical CDF, \\(F(t)\\) specified theoretical CDF.Integral Form: test statistic can also expressed integral theoretical CDF: \\[\n^2 = n \\int_{-\\infty}^\\infty \\frac{\\left[F_n(t) - F(t)\\right]^2}{F(t)(1 - F(t))} dF(t),\n\\] \\(F_n(t)\\) empirical CDF, \\(F(t)\\) specified theoretical CDF.Applications:\nTesting normality distributions (e.g., exponential, Weibull).\nValidating assumptions statistical models.\nComparing data theoretical distributions.\nApplications:Testing normality distributions (e.g., exponential, Weibull).Validating assumptions statistical models.Comparing data theoretical distributions.Hypothesis TestingNull Hypothesis (\\(H_0\\)): data follows specified distribution (e.g., normal distribution).Alternative Hypothesis (\\(H_1\\)): data follow specified distribution.null hypothesis rejected \\(^2\\) large, indicating poor fit specified distribution.Critical values test statistic provided (Marsaglia Marsaglia 2004) (Stephens 1974).Applications DistributionsThe Anderson-Darling test can applied various distributions using specific transformation methods. Examples include:ExponentialExponentialLogisticLogisticGumbelGumbelExtreme-valueExtreme-valueWeibull (logarithmic transformation: \\(\\log(\\text{Weibull}) = \\text{Gumbel}\\))Weibull (logarithmic transformation: \\(\\log(\\text{Weibull}) = \\text{Gumbel}\\))GammaGammaCauchyCauchyvon Misesvon MisesLog-normal (two-parameter)Log-normal (two-parameter)details transformations critical values, consult (Stephens 1974).Alternatively, broader range distributions, use gofTest function gof package:","code":"\n# Perform Anderson-Darling Test\nlibrary(nortest)\nad_test_result <- ad.test(mtcars$mpg)\n\n# Output the test statistic and p-value\nad_test_result\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  mtcars$mpg\n#> A = 0.57968, p-value = 0.1207\n# General goodness-of-fit test with Anderson-Darling\nlibrary(EnvStats)\ngof_test_result <- EnvStats::gofTest(mtcars$mpg, test = \"ad\")\n\n# Extract the p-value\ngof_test_result$p.value\n#> [1] 0.1207371"},{"path":"descriptive-statistics.html","id":"kolmogorov-smirnov-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.2 Kolmogorov-Smirnov Test","text":"Kolmogorov-Smirnov (K-S) test nonparametric test compares empirical cumulative distribution function (ECDF) sample theoretical cumulative distribution function (CDF), compares ECDFs two samples. used assess whether sample comes specific distribution (one-sample test) compare two samples (two-sample test).test statistic \\(D_n\\) one-sample test defined :\\[\nD_n = \\sup_x \\left| F_n(x) - F(x) \\right|,\n\\]:\\(F_n(x)\\) empirical CDF sample,\\(F_n(x)\\) empirical CDF sample,\\(F(x)\\) theoretical CDF null hypothesis,\\(F(x)\\) theoretical CDF null hypothesis,\\(\\sup_x\\) denotes supremum (largest value) possible values \\(x\\).\\(\\sup_x\\) denotes supremum (largest value) possible values \\(x\\).two-sample K-S test, statistic :\\[\nD_{n,m} = \\sup_x \\left| F_{n,1}(x) - F_{m,2}(x) \\right|,\n\\]\\(F_{n,1}(x)\\) \\(F_{m,2}(x)\\) empirical CDFs two samples, sizes \\(n\\) \\(m\\), respectively.HypothesesNull hypothesis (\\(H_0\\)): sample comes specified distribution (one-sample) two samples drawn distribution (two-sample).Alternative hypothesis (\\(H_1\\)): sample come specified distribution (one-sample) two samples drawn different distributions (two-sample).PropertiesBased Largest Deviation: K-S test sensitive largest absolute difference empirical expected CDFs, making effective detecting shifts location scale.Based Largest Deviation: K-S test sensitive largest absolute difference empirical expected CDFs, making effective detecting shifts location scale.Distribution-Free: test assume specific distribution data null hypothesis. significance level determined distribution test statistic null hypothesis.Distribution-Free: test assume specific distribution data null hypothesis. significance level determined distribution test statistic null hypothesis.Limitations:\ntest sensitive near center distribution tails.\nmay perform well discrete data small sample sizes.\nLimitations:test sensitive near center distribution tails.may perform well discrete data small sample sizes.Related Tests:\nKuiper’s Test: variation K-S test sensitive deviations center tails distribution. Kuiper test statistic : \\[\nV_n = D^+ + D^-,\n\\] \\(D^+\\) \\(D^-\\) maximum positive negative deviations empirical CDF theoretical CDF.\nRelated Tests:Kuiper’s Test: variation K-S test sensitive deviations center tails distribution. Kuiper test statistic : \\[\nV_n = D^+ + D^-,\n\\] \\(D^+\\) \\(D^-\\) maximum positive negative deviations empirical CDF theoretical CDF.ApplicationsTesting normality specified distributions.Comparing two datasets determine drawn distribution.perform one-sample K-S test R, use ks.test() function. check goodness fit specific distribution, gofTest() function package like DescTools can also used.Advantages:\nSimple widely applicable.\nDistribution-free null hypothesis.\nAdvantages:Simple widely applicable.Simple widely applicable.Distribution-free null hypothesis.Distribution-free null hypothesis.Limitations:\nSensitive sample size: small deviations may lead significance large samples.\nReduced sensitivity differences tails compared Anderson-Darling test.\nLimitations:Sensitive sample size: small deviations may lead significance large samples.Sensitive sample size: small deviations may lead significance large samples.Reduced sensitivity differences tails compared Anderson-Darling test.Reduced sensitivity differences tails compared Anderson-Darling test.Kolmogorov-Smirnov test provides general-purpose method goodness--fit testing sample comparison, particular utility detecting central deviations.","code":"\n# One-sample Kolmogorov-Smirnov test for normality\ndata <- rnorm(50)  # Generate random normal data\nks.test(data, \"pnorm\", mean(data), sd(data))\n#> \n#>  Exact one-sample Kolmogorov-Smirnov test\n#> \n#> data:  data\n#> D = 0.098643, p-value = 0.6785\n#> alternative hypothesis: two-sided\n\n# Goodness-of-fit test using gofTest\nlibrary(DescTools)\ngofTest(data, test = \"ks\")$p.value  # Kolmogorov-Smirnov test p-value\n#> [1] 0.6785444"},{"path":"descriptive-statistics.html","id":"cramer-von-mises-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.3 Cramer-von Mises Test","text":"Cramer-von Mises (CVM) test nonparametric goodness--fit test evaluates agreement empirical cumulative distribution function (ECDF) sample specified theoretical cumulative distribution function (CDF). Unlike Kolmogorov-Smirnov test, focuses largest discrepancy, Cramer-von Mises test considers average squared discrepancy across entire distribution. Unlike Anderson-Darling test, weights parts distribution equally.test statistic \\(W^2\\) one-sample Cramer-von Mises test defined :\\[\nW^2 = n \\int_{-\\infty}^\\infty \\left[ F_n(t) - F(t) \\right]^2 dF(t),\n\\]:\\(F_n(t)\\) empirical CDF,\\(F_n(t)\\) empirical CDF,\\(F(t)\\) specified theoretical CDF null hypothesis,\\(F(t)\\) specified theoretical CDF null hypothesis,\\(n\\) sample size.\\(n\\) sample size.practice, \\(W^2\\) computed using ordered sample values \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\) :\\[\nW^2 = \\sum_{=1}^n \\left( F(y_{()}) - \\frac{2i - 1}{2n} \\right)^2 + \\frac{1}{12n},\n\\]:\\(F(y_{()})\\) theoretical CDF evaluated ordered sample values \\(y_{()}\\).HypothesesNull hypothesis (H0): sample data follow specified distribution.Alternative hypothesis (H1): sample data follow specified distribution.PropertiesFocus Average Discrepancy: Cramer-von Mises test measures overall goodness--fit considering squared deviations across points distribution, ensuring equal weighting discrepancies.Focus Average Discrepancy: Cramer-von Mises test measures overall goodness--fit considering squared deviations across points distribution, ensuring equal weighting discrepancies.Comparison Anderson-Darling: Unlike Anderson-Darling test, gives weight deviations tails, CVM test weights parts distribution equally.Comparison Anderson-Darling: Unlike Anderson-Darling test, gives weight deviations tails, CVM test weights parts distribution equally.Integral Representation: statistic expressed integral squared differences empirical theoretical CDFs.Integral Representation: statistic expressed integral squared differences empirical theoretical CDFs.Two-Sample Test: Cramer-von Mises framework can also extended compare two empirical CDFs. two-sample statistic based pooled empirical CDF.Two-Sample Test: Cramer-von Mises framework can also extended compare two empirical CDFs. two-sample statistic based pooled empirical CDF.ApplicationsAssessing goodness--fit theoretical distribution (e.g., normal, exponential, Weibull).Comparing two datasets determine drawn similar distributions.Validating model assumptions.perform Cramer-von Mises test R, gofTest() function DescTools package can used. example:Advantages:\nConsiders discrepancies across entire distribution.\nRobust outliers due equal weighting.\nSimple compute interpret.\nAdvantages:Considers discrepancies across entire distribution.Considers discrepancies across entire distribution.Robust outliers due equal weighting.Robust outliers due equal weighting.Simple compute interpret.Simple compute interpret.Limitations:\nLess sensitive deviations tails compared Anderson-Darling test.\nMay less powerful Kolmogorov-Smirnov test detecting central shifts.\nLimitations:Less sensitive deviations tails compared Anderson-Darling test.Less sensitive deviations tails compared Anderson-Darling test.May less powerful Kolmogorov-Smirnov test detecting central shifts.May less powerful Kolmogorov-Smirnov test detecting central shifts.","code":"\n# Generate random normal data\ndata <- rnorm(50)\n\n# Perform the Cramer-von Mises test\nlibrary(DescTools)\ngofTest(data, test = \"cvm\")$p.value  # Cramer-von Mises test p-value\n#> [1] 0.04846959"},{"path":"descriptive-statistics.html","id":"jarquebera-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.4 Jarque-Bera Test","text":"Jarque-Bera (JB) test (Bera Jarque 1981) goodness--fit test used check whether dataset follows normal distribution. based skewness kurtosis data, measure asymmetry “tailedness” distribution, respectively.Jarque-Bera test statistic defined :\\[\nJB = \\frac{n}{6}\\left(S^2 + \\frac{(K - 3)^2}{4}\\right),\n\\]:\\(n\\) sample size,\\(n\\) sample size,\\(S\\) sample skewness,\\(S\\) sample skewness,\\(K\\) sample kurtosis.\\(K\\) sample kurtosis.Skewness (\\(S\\)) calculated :\\[\nS = \\frac{\\hat{\\mu}_3}{\\hat{\\sigma}^3} = \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^3}{\\left(\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\right)^{3/2}},\n\\]:\\(\\hat{\\mu}_3\\) third central moment,\\(\\hat{\\mu}_3\\) third central moment,\\(\\hat{\\sigma}\\) standard deviation,\\(\\hat{\\sigma}\\) standard deviation,\\(\\bar{x}\\) sample mean.\\(\\bar{x}\\) sample mean.Kurtosis (\\(K\\)) calculated :\\[\nK = \\frac{\\hat{\\mu}_4}{\\hat{\\sigma}^4} = \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^4}{\\left(\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\right)^2},\n\\]:\\(\\hat{\\mu}_4\\) fourth central moment.HypothesisNull hypothesis (\\(H_0\\)): data follow normal distribution, implying:\nSkewness \\(S = 0\\),\nExcess kurtosis \\(K - 3 = 0\\).\nSkewness \\(S = 0\\),Excess kurtosis \\(K - 3 = 0\\).Alternative hypothesis (\\(H_1\\)): data follow normal distribution.Distribution JB StatisticUnder null hypothesis, Jarque-Bera statistic asymptotically follows chi-squared distribution 2 degrees freedom:\\[\nJB \\sim \\chi^2_2.\n\\]PropertiesSensitivity:\nSkewness (\\(S\\)) captures asymmetry data.\nKurtosis (\\(K\\)) measures heavy-tailed light-tailed distribution compared normal distribution.\nSkewness (\\(S\\)) captures asymmetry data.Kurtosis (\\(K\\)) measures heavy-tailed light-tailed distribution compared normal distribution.Limitations:\ntest sensitive large sample sizes; even small deviations normality may result rejection \\(H_0\\).\nAssumes data independently identically distributed.\ntest sensitive large sample sizes; even small deviations normality may result rejection \\(H_0\\).Assumes data independently identically distributed.ApplicationsTesting normality regression residuals.Validating distributional assumptions econometrics time series analysis.Jarque-Bera test can performed R using tseries package:","code":"\nlibrary(tseries)\n\n# Generate a sample dataset\ndata <- rnorm(100)  # Normally distributed data\n\n# Perform the Jarque-Bera test\njarque.bera.test(data)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  data\n#> X-squared = 0.89476, df = 2, p-value = 0.6393"},{"path":"descriptive-statistics.html","id":"bivariate-statistics","chapter":"3 Descriptive Statistics","heading":"3.4 Bivariate Statistics","text":"Bivariate statistics involve analysis relationships two variables. Understanding relationships can provide insights patterns, associations, (suggestive ) causal connections. , explore correlation different types variables:Two Continuous VariablesTwo Discrete VariablesCategorical Continuous VariablesBefore delving analysis, critical consider following:relationship linear non-linear?\nLinear relationships can modeled simpler statistical methods Pearson’s correlation, non-linear relationships may require alternative approaches, Spearman’s rank correlation regression transformations.\nLinear relationships can modeled simpler statistical methods Pearson’s correlation, non-linear relationships may require alternative approaches, Spearman’s rank correlation regression transformations.variable continuous, normal homoskedastic?\nparametric methods like Pearson’s correlation, assumptions normality homoskedasticity (equal variance) must met. assumptions fail, non-parametric methods like Spearman’s correlation robust alternatives preferred.\nparametric methods like Pearson’s correlation, assumptions normality homoskedasticity (equal variance) must met. assumptions fail, non-parametric methods like Spearman’s correlation robust alternatives preferred.big dataset?\nLarge datasets can reveal subtle patterns may lead statistically significant results practically meaningful. smaller datasets, careful selection statistical methods essential ensure reliability validity.\nLarge datasets can reveal subtle patterns may lead statistically significant results practically meaningful. smaller datasets, careful selection statistical methods essential ensure reliability validity.Chi-squared TestPhi CoefficientCramer’s VTschuprow’s TSpearman’s Rank CorrelationKendall’s TauGamma StatisticFreeman’s ThetaEpsilon-squaredGoodman Kruskal’s GammaSomers’ DKendall’s Tau-bYule’s Q YTetrachoric CorrelationPolychoric CorrelationPoint-Biserial CorrelationLogistic RegressionPearson CorrelationSpearman Correlation","code":""},{"path":"descriptive-statistics.html","id":"two-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.1 Two Continuous","text":"","code":"\nset.seed(1)\nn = 100 # (sample size)\n\ndata = data.frame(A = sample(1:20, replace = TRUE, size = n),\n                  B = sample(1:30, replace = TRUE, size = n))"},{"path":"descriptive-statistics.html","id":"pearson-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.1 Pearson Correlation","text":"Pearson correlation quantifies strength direction linear relationship two continuous variables.Formula:\\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\cdot \\sum (y_i - \\bar{y})^2}}\n\\] \\(x_i, y_i\\): Individual data points variables \\(X\\) \\(Y\\).\\(x_i, y_i\\): Individual data points variables \\(X\\) \\(Y\\).\\(\\bar{x}, \\bar{y}\\): Means \\(X\\) \\(Y\\).\\(\\bar{x}, \\bar{y}\\): Means \\(X\\) \\(Y\\).Assumptions:relationship variables linear.Variables normally distributed.Data exhibits homoscedasticity (equal variance \\(Y\\) values \\(X\\)).Use Case:Use relationship expected linear, assumptions normality homoscedasticity met.Interpretation:\\(r = +1\\): Perfect positive linear relationship.\\(r = -1\\): Perfect negative linear relationship.\\(r = 0\\): linear relationship.","code":"\n# Pearson correlation\npearson_corr <- stats::cor(data$A, data$B, method = \"pearson\")\ncat(\"Pearson Correlation (r):\", pearson_corr, \"\\n\")\n#> Pearson Correlation (r): 0.02394939"},{"path":"descriptive-statistics.html","id":"spearman-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.2 Spearman Correlation","text":"Spearman correlation measures strength monotonic relationship two variables. ranks data calculates correlation based ranks.Formula:\\[\n\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 -1)}\n\\]\\(d_i\\): Difference ranks \\(x_i\\) \\(y_i\\).\\(n\\): Number paired observations.Assumptions:Relationship must monotonic, necessarily linear.Relationship must monotonic, necessarily linear.assumptions distribution variables.assumptions distribution variables.Use Case:Use data ordinal normality linearity assumptions violated.Interpretation:\\(\\rho = +1\\): Perfect positive monotonic relationship.\\(\\rho = +1\\): Perfect positive monotonic relationship.\\(\\rho = -1\\): Perfect negative monotonic relationship.\\(\\rho = -1\\): Perfect negative monotonic relationship.\\(\\rho = 0\\): monotonic relationship.\\(\\rho = 0\\): monotonic relationship.","code":"\n# Spearman correlation\nspearman_corr <- stats::cor(data$A, data$B, method = \"spearman\")\ncat(\"Spearman Correlation (rho):\", spearman_corr, \"\\n\")\n#> Spearman Correlation (rho): 0.02304636"},{"path":"descriptive-statistics.html","id":"kendalls-tau-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.3 Kendall’s Tau Correlation","text":"Kendall’s Tau measures strength monotonic relationship comparing concordant discordant pairs.Formula:\\[\n\\tau = \\frac{(C- D)}{\\binom{n}{2}}\n\\]​\\(C\\): Number concordant pairs (ranks \\(X\\) \\(Y\\) increase decrease together).\\(C\\): Number concordant pairs (ranks \\(X\\) \\(Y\\) increase decrease together).\\(D\\): Number discordant pairs (one rank increases decreases).\\(D\\): Number discordant pairs (one rank increases decreases).\\(\\binom{n}{2}\\): Total number possible pairs.\\(\\binom{n}{2}\\): Total number possible pairs.Assumptions:specific assumptions data distribution.specific assumptions data distribution.Measures monotonic relationships.Measures monotonic relationships.Use Case:Preferred small datasets data contains outliers.Interpretation:\\(\\tau = +1\\): Perfect positive monotonic relationship.\\(\\tau = +1\\): Perfect positive monotonic relationship.\\(\\tau = -1\\): Perfect negative monotonic relationship.\\(\\tau = -1\\): Perfect negative monotonic relationship.\\(\\tau = 0\\): monotonic relationship.\\(\\tau = 0\\): monotonic relationship.","code":"\n# Kendall's Tau correlation\nkendall_corr <- stats::cor(data$A, data$B, method = \"kendall\")\ncat(\"Kendall's Tau Correlation (tau):\", kendall_corr, \"\\n\")\n#> Kendall's Tau Correlation (tau): 0.02171284"},{"path":"descriptive-statistics.html","id":"distance-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.4 Distance Correlation","text":"Distance Correlation measures linear non-linear relationships variables. require monotonicity linearity.Formula:\\[\nd Cor = \\frac{d Cov(X,Y)}{\\sqrt{d Var (X) \\cdot d Var (Y)}}\n\\]​\\(dCov\\): Distance covariance \\(X\\) \\(Y\\).\\(dCov\\): Distance covariance \\(X\\) \\(Y\\).\\(dVar\\): Distance variances \\(X\\) \\(Y\\).\\(dVar\\): Distance variances \\(X\\) \\(Y\\).Assumptions:specific assumptions relationship (linear, monotonic, otherwise).Use Case:Use complex relationships, including non-linear patterns.Interpretation:\\(dCor = 0\\): association.\\(dCor = 0\\): association.\\(dCor = 1\\): Perfect association.\\(dCor = 1\\): Perfect association.","code":"\n# Distance correlation\ndistance_corr <- energy::dcor(data$A, data$B)\ncat(\"Distance Correlation (dCor):\", distance_corr, \"\\n\")\n#> Distance Correlation (dCor): 0.1008934"},{"path":"descriptive-statistics.html","id":"summary-table-of-correlation-methods","chapter":"3 Descriptive Statistics","heading":"3.4.1.5 Summary Table of Correlation Methods","text":"","code":""},{"path":"descriptive-statistics.html","id":"categorical-and-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.2 Categorical and Continuous","text":"Analyzing relationship categorical variable (binary multi-class) continuous variable requires specialized techniques. methods assess whether categorical variable significantly influences continuous variable vice versa.focus following methods:Point-Biserial CorrelationLogistic RegressionAnalysis Variance (ANOVA)T-test","code":""},{"path":"descriptive-statistics.html","id":"point-biserial-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.2.1 Point-Biserial Correlation","text":"Point-Biserial Correlation special case Pearson correlation used assess relationship binary categorical variable (coded 0 1) continuous variable. measures strength direction linear relationship.Formula:\\[\nr_{pb} = \\frac{\\bar{Y_1} - \\bar{Y_0}}{s_Y} \\sqrt{\\frac{n_1 n_0}{n^2}}\n\\]\\(\\bar{Y_1}\\), \\(\\bar{Y_0}\\): Mean continuous variable groups coded 1 0, respectively.\\(\\bar{Y_1}\\), \\(\\bar{Y_0}\\): Mean continuous variable groups coded 1 0, respectively.\\(s_Y\\): Standard deviation continuous variable.\\(s_Y\\): Standard deviation continuous variable.\\(n_1, n_0\\): Number observations group (1 0).\\(n_1, n_0\\): Number observations group (1 0).\\(n\\): Total number observations.\\(n\\): Total number observations.Key Properties:Range: \\(-1\\) \\(1\\).\n\\(r_{pb} = +1\\): Perfect positive correlation.\n\\(r_{pb} = -1\\): Perfect negative correlation.\n\\(r_{pb} = 0\\): linear relationship.\n\\(r_{pb} = +1\\): Perfect positive correlation.\\(r_{pb} = -1\\): Perfect negative correlation.\\(r_{pb} = 0\\): linear relationship.positive \\(r_{pb}\\) indicates higher values continuous variable associated 1 group, negative \\(r_{pb}\\) indicates opposite.Assumptions:binary variable truly dichotomous (e.g., male/female, success/failure).continuous variable approximately normally distributed.Homogeneity variance across two groups (strictly required recommended).Use Case:evaluate linear relationship binary categorical variable continuous variable.","code":"\nlibrary(ltm)\n# Point-Biserial Correlation\nbiserial_corr <- ltm::biserial.cor(\n  c(12.5, 15.3, 10.7, 18.1, 11.2, 16.8, 13.4, 14.9), \n  c(0, 1, 0, 1, 0, 1, 0, 1), \n  use = \"all.obs\", \n  level = 2\n)\ncat(\"Point-Biserial Correlation:\", biserial_corr, \"\\n\")\n#> Point-Biserial Correlation: 0.8792835"},{"path":"descriptive-statistics.html","id":"logistic-regression","chapter":"3 Descriptive Statistics","heading":"3.4.2.2 Logistic Regression","text":"Logistic Regression models relationship binary categorical variable (dependent variable) one independent variables (may include continuous variables). predicts probability binary outcome (e.g., success/failure, yes/).Refer 3.4.2.2 detail.Formula:logistic regression model represented :\\[\n\\text{logit}(p) = \\ln \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 X\n\\]\\(p\\): Probability outcome 1.\\(p\\): Probability outcome 1.\\(\\beta_0\\): Intercept.\\(\\beta_0\\): Intercept.\\(\\beta_1\\): Coefficient continuous variable \\(X\\).\\(\\beta_1\\): Coefficient continuous variable \\(X\\).\\(\\text{logit}(p)\\): Log-odds probability.\\(\\text{logit}(p)\\): Log-odds probability.Key Features:Output: Odds ratio probability binary outcome.Output: Odds ratio probability binary outcome.Can include multiple predictors (continuous categorical).Can include multiple predictors (continuous categorical).Non-linear transformation ensures predictions probabilities 0 1.Non-linear transformation ensures predictions probabilities 0 1.Assumptions:dependent variable binary.dependent variable binary.Observations independent.Observations independent.linear relationship logit dependent variable independent variable.linear relationship logit dependent variable independent variable.multicollinearity predictors.multicollinearity predictors.Use Case:predict likelihood binary outcome based continuous predictor (e.g., probability success given test scores).","code":"\n# Simulated data\nset.seed(123)\nx <- rnorm(100, mean = 50, sd = 10)  # Continuous predictor\ny <- ifelse(x > 55, 1, 0)  # Binary outcome based on threshold\n\n# Logistic Regression\nlogistic_model <- glm(y ~ x, family = binomial)\nsummary(logistic_model)\n#> \n#> Call:\n#> glm(formula = y ~ x, family = binomial)\n#> \n#> Deviance Residuals: \n#>        Min          1Q      Median          3Q         Max  \n#> -2.770e-04  -2.100e-08  -2.100e-08   2.100e-08   2.548e-04  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  -3749.9   495083.0  -0.008    0.994\n#> x               67.9     8966.6   0.008    0.994\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1.2217e+02  on 99  degrees of freedom\n#> Residual deviance: 1.4317e-07  on 98  degrees of freedom\n#> AIC: 4\n#> \n#> Number of Fisher Scoring iterations: 25\n\n# Predicted probabilities\npredicted_probs <- predict(logistic_model, type = \"response\")\nprint(head(predicted_probs))\n#>         1         2         3         4         5         6 \n#> -735.6466 -511.3844  703.2134 -307.2281 -267.3187  809.3747\n# Visualize logistic regression curve\nlibrary(ggplot2)\ndata <- data.frame(x = x, y = y, predicted = predicted_probs)\n\nggplot(data, aes(x = x, y = predicted)) +\n    geom_point(aes(y = y), color = \"red\", alpha = 0.5) +\n    geom_line(color = \"blue\") +\n    labs(title = \"Logistic Regression: Continuous vs Binary\",\n         x = \"Continuous Predictor\", y = \"Predicted Probability\")"},{"path":"descriptive-statistics.html","id":"summary-table-of-methods-between-categorical-and-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.2.3 Summary Table of Methods (Between Categorical and Continuous)","text":"","code":""},{"path":"descriptive-statistics.html","id":"two-discrete","chapter":"3 Descriptive Statistics","heading":"3.4.3 Two Discrete","text":"analyzing relationship two discrete variables (categorical ordinal), various methods available quantify degree association similarity. methods can broadly classified :Distance MetricsDistance MetricsStatistical MetricsStatistical Metrics","code":""},{"path":"descriptive-statistics.html","id":"distance-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.1 Distance Metrics","text":"Distance metrics measure dissimilarity two discrete variables often used proxy correlation specific applications like clustering machine learning.","code":""},{"path":"descriptive-statistics.html","id":"euclidean-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.1 Euclidean Distance","text":"\\[\nd(x, y) = \\sqrt{\\sum_{=1}^n (x_i - y_i)^2}\n\\]Measures straight-line distance two variables Euclidean space.Measures straight-line distance two variables Euclidean space.Sensitive scaling; variables normalized meaningful comparisons.Sensitive scaling; variables normalized meaningful comparisons.","code":""},{"path":"descriptive-statistics.html","id":"manhattan-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.2 Manhattan Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n |x_i - y_i|\n\\]Measures distance summing absolute differences along dimension.Measures distance summing absolute differences along dimension.Also called L1 norm; often used grid-based problems.Also called L1 norm; often used grid-based problems.","code":""},{"path":"descriptive-statistics.html","id":"chebyshev-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.3 Chebyshev Distance","text":"\\[\nd(x, y) = \\max_{=1}^n |x_i - y_i|\n\\]Measures maximum single-step distance along dimension.Measures maximum single-step distance along dimension.Useful discrete, grid-based problems (e.g., chess moves).Useful discrete, grid-based problems (e.g., chess moves).","code":""},{"path":"descriptive-statistics.html","id":"minkowski-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.4 Minkowski Distance","text":"\\[\nd(x, y) = \\left( \\sum_{=1}^n |x_i - y_i|^p \\right)^{1/p}\n\\]Generalized distance metric. Special cases include:\n\\(p = 1\\): Manhattan Distance.\n\\(p = 2\\): Euclidean Distance.\n\\(p \\\\infty\\): Chebyshev Distance.\nGeneralized distance metric. Special cases include:\\(p = 1\\): Manhattan Distance.\\(p = 1\\): Manhattan Distance.\\(p = 2\\): Euclidean Distance.\\(p = 2\\): Euclidean Distance.\\(p \\\\infty\\): Chebyshev Distance.\\(p \\\\infty\\): Chebyshev Distance.","code":""},{"path":"descriptive-statistics.html","id":"canberra-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.5 Canberra Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n \\frac{|x_i - y_i|}{|x_i| + |y_i|}\n\\]Emphasizes proportional differences, making sensitive smaller values.","code":""},{"path":"descriptive-statistics.html","id":"hamming-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.6 Hamming Distance","text":"\\[\nd(x, y) = \\sum_{=1}^n (x_i \\neq y_i)\n\\]Counts number differing positions two sequences.Counts number differing positions two sequences.Widely used text similarity binary data.Widely used text similarity binary data.","code":""},{"path":"descriptive-statistics.html","id":"cosine-similarity-and-distance","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.7 Cosine Similarity and Distance","text":"\\[\n\\text{Cosine Similarity} = \\frac{\\sum_{=1}^n x_i y_i}{\\sqrt{\\sum_{=1}^n x_i^2} \\cdot \\sqrt{\\sum_{=1}^n y_i^2}}\n\\]\\[\n\\text{Cosine Distance} = 1 - \\text{Cosine Similarity}\n\\]Measures angle two vectors high-dimensional space.Measures angle two vectors high-dimensional space.Often used text document similarity.Often used text document similarity.","code":""},{"path":"descriptive-statistics.html","id":"sum-of-absolute-differences","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.8 Sum of Absolute Differences","text":"\\[\nd(x, y) = \\sum_{=1}^n |x_i - y_i|\n\\]Equivalent Manhattan Distance without coordinate context.","code":""},{"path":"descriptive-statistics.html","id":"sum-of-squared-differences","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.9 Sum of Squared Differences","text":"\\[\nd(x, y) = \\sum_{=1}^n (x_i - y_i)^2\n\\]Equivalent squared Euclidean Distance.","code":""},{"path":"descriptive-statistics.html","id":"mean-absolute-error","chapter":"3 Descriptive Statistics","heading":"3.4.3.1.10 Mean Absolute Error","text":"\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^n |x_i - y_i|\n\\]Measures average absolute differences.","code":"\n# Example data\nx <- c(1, 2, 3, 4, 5)\ny <- c(2, 3, 4, 5, 6)\n\n# Compute distances\neuclidean <- sqrt(sum((x - y)^2))\nmanhattan <- sum(abs(x - y))\nchebyshev <- max(abs(x - y))\nhamming <- sum(x != y)\ncosine_similarity <- sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)))\ncosine_distance <- 1 - cosine_similarity\n\n# Display results\ncat(\"Euclidean Distance:\", euclidean, \"\\n\")\n#> Euclidean Distance: 2.236068\ncat(\"Manhattan Distance:\", manhattan, \"\\n\")\n#> Manhattan Distance: 5\ncat(\"Chebyshev Distance:\", chebyshev, \"\\n\")\n#> Chebyshev Distance: 1\ncat(\"Hamming Distance:\", hamming, \"\\n\")\n#> Hamming Distance: 5\ncat(\"Cosine Distance:\", cosine_distance, \"\\n\")\n#> Cosine Distance: 0.005063324"},{"path":"descriptive-statistics.html","id":"statistical-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.2 Statistical Metrics","text":"","code":""},{"path":"descriptive-statistics.html","id":"chi-squared-test","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.1 Chi-squared Test","text":"Chi-Squared Test evaluates whether two categorical variables independent comparing observed expected frequencies contingency table.Formula:\\[\n\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\\(O_i\\): Observed frequency cell table.\\(O_i\\): Observed frequency cell table.\\(E_i\\): Expected frequency assumption independence.\\(E_i\\): Expected frequency assumption independence.Steps:Construct contingency table observed counts.Construct contingency table observed counts.Compute expected frequencies: \\(E_{ij} = \\frac{\\text{Row Total}_i \\cdot \\text{Column Total}_j}{\\text{Grand Total}}\\)Compute expected frequencies: \\(E_{ij} = \\frac{\\text{Row Total}_i \\cdot \\text{Column Total}_j}{\\text{Grand Total}}\\)Apply Chi-squared formula.Apply Chi-squared formula.Compare \\(\\chi^2\\) critical value Chi-squared distribution.Compare \\(\\chi^2\\) critical value Chi-squared distribution.Assumptions:Observations independent.Observations independent.Expected frequencies \\(\\geq 5\\) least 80% cells.Expected frequencies \\(\\geq 5\\) least 80% cells.Use Case:Tests independence two nominal variables.","code":"\n# Example data\ndt <- matrix(c(15, 25, 20, 40), nrow = 2)\nrownames(dt) <- c(\"Group A\", \"Group B\")\ncolnames(dt) <- c(\"Category 1\", \"Category 2\")\n\n# Perform Chi-Squared Test\nchi_sq_test <- chisq.test(dt)\nprint(chi_sq_test)\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  dt\n#> X-squared = 0.045788, df = 1, p-value = 0.8306"},{"path":"descriptive-statistics.html","id":"phi-coefficient","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.2 Phi Coefficient","text":"Phi Coefficient measure association two binary variables, derived Chi-squared statistic.Formula:\\[\n\\phi = \\frac{\\chi^2}{n}\n\\]\\(n\\): Total sample size.Interpretation:\\(\\phi = 0\\): association.\\(\\phi = 0\\): association.\\(\\phi = +1\\): Perfect positive association.\\(\\phi = +1\\): Perfect positive association.\\(\\phi = -1\\): Perfect negative association.\\(\\phi = -1\\): Perfect negative association.Use Case:Suitable 2x2 contingency tables.2 binary","code":"\nlibrary(psych)\n\n# Compute Phi Coefficient\nphi_coeff <- phi(dt)\ncat(\"Phi Coefficient:\", phi_coeff, \"\\n\")\n#> Phi Coefficient: 0.04"},{"path":"descriptive-statistics.html","id":"cramers-v","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.3 Cramer’s V","text":"Cramer’s V generalizes Phi coefficient handle contingency tables two rows columns.Formula:\\[\nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(r-1, c-1)}}\n\\]​​\\(r\\): Number rows.\\(r\\): Number rows.\\(c\\): Number columns.\\(c\\): Number columns.Assumptions:Variables nominal.Variables nominal.Suitable larger contingency tables.Suitable larger contingency tables.Use Case:Measures strength association nominal variables natural order.Alternatively,ncchisq noncentral Chi-squarencchisq noncentral Chi-squarenchisqadj Adjusted noncentral Chi-squarenchisqadj Adjusted noncentral Chi-squarefisher Fisher Z transformationfisher Fisher Z transformationfisheradj bias correction Fisher z transformationfisheradj bias correction Fisher z transformation","code":"\nlibrary(lsr)\n\n# Simulate data\nset.seed(1)\ndata <- data.frame(\n  A = sample(1:5, replace = TRUE, size = 100),  # Nominal variable\n  B = sample(1:6, replace = TRUE, size = 100)  # Nominal variable\n)\n\n# Compute Cramer's V\ncramers_v <- cramersV(data$A, data$B)\ncat(\"Cramer's V:\", cramers_v, \"\\n\")\n#> Cramer's V: 0.1944616\nDescTools::CramerV(data, conf.level = 0.95,method = \"ncchisqadj\")\n#>  Cramer V    lwr.ci    upr.ci \n#> 0.3472325 0.3929964 0.4033053"},{"path":"descriptive-statistics.html","id":"adjusted-cramers-v","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.4 Adjusted Cramer’s V","text":"Adjusted versions Cramer’s V correct bias, especially small samples.Adjusted formulas account non-central Chi-squared bias correction. Examples include:Non-central Chi-squared: \\(V_{adj} = \\sqrt{\\frac{\\chi^2_{nc} / n}{\\min(r-1, c-1)}}\\)​Non-central Chi-squared: \\(V_{adj} = \\sqrt{\\frac{\\chi^2_{nc} / n}{\\min(r-1, c-1)}}\\)​Bias Correction: \\(V_{adj} = V - \\text{Bias Term}\\)Bias Correction: \\(V_{adj} = V - \\text{Bias Term}\\)","code":"\nlibrary(DescTools)\n\n# Compute Adjusted Cramer's V\ncramers_v_adj <- CramerV(data, conf.level = 0.95, method = \"ncchisqadj\")\ncramers_v_adj\n#>  Cramer V    lwr.ci    upr.ci \n#> 0.3472325 0.3929964 0.4033053"},{"path":"descriptive-statistics.html","id":"tschuprows-t","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.5 Tschuprow’s T","text":"Tschuprow’s T symmetric measure association nominal variables. differs Cramer’s V considering product rows columns, making less sensitive asymmetrical tables.Formula:\\[\nT = \\sqrt{\\frac{\\chi^2/n}{\\sqrt{(r-1)(c-1)}}}\n\\]Assumptions:Applicable nominal variables.Applicable nominal variables.Suitable contingency tables unequal dimensions.Suitable contingency tables unequal dimensions.Use Case:Preferred table dimensions highly unequal.","code":"\n# Compute Tschuprow's T\ntschuprow_t <- DescTools::TschuprowT(data$A, data$B)\ncat(\"Tschuprow's T:\", tschuprow_t, \"\\n\")\n#> Tschuprow's T: 0.1839104"},{"path":"descriptive-statistics.html","id":"ordinal-association-rank-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.6 Ordinal Association (Rank correlation)","text":"least one variable ordinal, rank-based methods appropriate respect order categories. methods often used relationships monotonic (increasing decreasing consistently) necessarily linear.","code":""},{},{},{},{},{},{},{},{},{},{},{},{"path":"descriptive-statistics.html","id":"general-approach-to-bivariate-statistics","chapter":"3 Descriptive Statistics","heading":"3.4.4 General Approach to Bivariate Statistics","text":"Get correlation table continuous variables onlyAlternatively, can also theComparing correlations different types variables (e.g., continuous vs. categorical) poses unique challenges. One key issue ensuring methods appropriate nature variables analyzed. Another challenge lies detecting non-linear relationships, traditional correlation measures, Pearson’s correlation coefficient, designed assess linear associations.address challenges, potential solution utilize mutual information information theory. Mutual information quantifies much knowing one variable reduces uncertainty another, providing general measure association accommodates linear non-linear relationships.","code":"\nlibrary(tidyverse)\n\ndata(\"mtcars\")\ndf = mtcars %>%\n    dplyr::select(cyl, vs, carb)\n\n\ndf_factor = df %>%\n    dplyr::mutate(\n        cyl = factor(cyl),\n        vs = factor(vs),\n        carb = factor(carb)\n    )\n# summary(df)\nstr(df)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#>  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#>  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nstr(df_factor)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n#>  $ vs  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 1 2 1 2 2 2 ...\n#>  $ carb: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 4 1 1 2 1 4 2 2 4 ...\ncor(df)\n#>             cyl         vs       carb\n#> cyl   1.0000000 -0.8108118  0.5269883\n#> vs   -0.8108118  1.0000000 -0.5696071\n#> carb  0.5269883 -0.5696071  1.0000000\n\n# only complete obs\n# cor(df, use = \"complete.obs\")\nHmisc::rcorr(as.matrix(df), type = \"pearson\")\n#>        cyl    vs  carb\n#> cyl   1.00 -0.81  0.53\n#> vs   -0.81  1.00 -0.57\n#> carb  0.53 -0.57  1.00\n#> \n#> n= 32 \n#> \n#> \n#> P\n#>      cyl    vs     carb  \n#> cyl         0.0000 0.0019\n#> vs   0.0000        0.0007\n#> carb 0.0019 0.0007\nmodelsummary::datasummary_correlation(df)\nggcorrplot::ggcorrplot(cor(df))"},{"path":"descriptive-statistics.html","id":"approximating-mutual-information","chapter":"3 Descriptive Statistics","heading":"3.4.4.1 Approximating Mutual Information","text":"can approximate mutual information using following relationship:\\[\n\\downarrow \\text{Prediction Error} \\approx \\downarrow \\text{Uncertainty} \\approx \\uparrow \\text{Association Strength}\n\\]principle underpins X2Y metric, implemented following steps:Predict \\(y\\) without \\(x\\) (baseline model):\n\\(y\\) continuous, predict mean \\(y\\).\n\\(y\\) categorical, predict mode \\(y\\).\nPredict \\(y\\) without \\(x\\) (baseline model):\\(y\\) continuous, predict mean \\(y\\).\\(y\\) categorical, predict mode \\(y\\).Predict \\(y\\) \\(x\\) using model (e.g., linear regression, random forest, etc.).Predict \\(y\\) \\(x\\) using model (e.g., linear regression, random forest, etc.).Calculate difference prediction error steps 1 2. difference reflects reduction uncertainty \\(y\\) \\(x\\) included, serving measure association strength.Calculate difference prediction error steps 1 2. difference reflects reduction uncertainty \\(y\\) \\(x\\) included, serving measure association strength.","code":""},{"path":"descriptive-statistics.html","id":"generalizing-across-variable-types","chapter":"3 Descriptive Statistics","heading":"3.4.4.2 Generalizing Across Variable Types","text":"construct comprehensive framework handles different variable combinations, :Continuous vs. continuousCategorical vs. continuousContinuous vs. categoricalCategorical vs. categoricala flexible modeling approach required. Classification Regression Trees (CART) particularly well-suited purpose, can accommodate continuous categorical predictors outcomes. However, models, random forests generalized additive models (GAMs), may also employed.","code":""},{"path":"descriptive-statistics.html","id":"limitations-of-the-approach","chapter":"3 Descriptive Statistics","heading":"3.4.4.3 Limitations of the Approach","text":"Despite strengths, approach limitations:Asymmetry:\nmeasure symmetric, meaning \\((x, y) \\neq (y, x)\\).Asymmetry:\nmeasure symmetric, meaning \\((x, y) \\neq (y, x)\\).Comparability:\nDifferent variable pairs may yield metrics directly comparable. instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), categorical outcomes use measures like misclassification error.Comparability:\nDifferent variable pairs may yield metrics directly comparable. instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), categorical outcomes use measures like misclassification error.limitations considered interpreting results, especially multi-variable mixed-data contexts.Alternatively,general form,heat map correlation timeMore elaboration ggplot2","code":"\nlibrary(ppsr)\nlibrary(tidyverse)\n\niris <- iris %>% \n  dplyr::select(1:3)\n\n# ppsr::score_df(iris) # if you want a dataframe\nppsr::score_matrix(iris,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>              Sepal.Length Sepal.Width Petal.Length\n#> Sepal.Length   1.00000000  0.04632352    0.5491398\n#> Sepal.Width    0.06790301  1.00000000    0.2376991\n#> Petal.Length   0.61608360  0.24263851    1.0000000\n\n# if you want a similar correlation matrix\nppsr::score_matrix(df,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>             cyl        vs      carb\n#> cyl  1.00000000 0.3982789 0.2092533\n#> vs   0.02514286 1.0000000 0.2000000\n#> carb 0.30798148 0.2537309 1.0000000\ncorrplot::corrplot(cor(df))\nPerformanceAnalytics::chart.Correlation(df, histogram = T, pch = 19)\nheatmap(as.matrix(df))\nppsr::visualize_pps(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_correlations(\n    df = iris\n)\nppsr::visualize_both(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_pps(\n    df = iris,\n    color_value_high = 'red',\n    color_value_low = 'yellow',\n    color_text = 'black'\n) +\n    ggplot2::theme_classic() +\n    ggplot2::theme(plot.background = \n                       ggplot2::element_rect(fill = \"lightgrey\")) +\n    ggplot2::theme(title = ggplot2::element_text(size = 15)) +\n    ggplot2::labs(\n        title = 'Correlation aand Heatmap',\n        subtitle = 'Subtitle',\n        caption = 'Caption',\n        x = 'More info'\n    )"},{"path":"basic-statistical-inference.html","id":"basic-statistical-inference","chapter":"4 Basic Statistical Inference","heading":"4 Basic Statistical Inference","text":"Statistical inference involves drawing conclusions population parameters based sample data. two primary goals inference :Making inferences true parameter value (\\(\\beta\\)) based estimator estimate:\ninvolves interpreting sample-derived estimate understand population parameter.\nExamples include estimating population means, variances, proportions.\ninvolves interpreting sample-derived estimate understand population parameter.Examples include estimating population means, variances, proportions.Testing whether underlying assumptions hold true, including:\nAssumptions true population parameters (e.g., \\(\\mu\\), \\(\\sigma^2\\)).\nAssumptions random variables (e.g., independence, normality).\nAssumptions model specification (e.g., linearity regression).\nAssumptions true population parameters (e.g., \\(\\mu\\), \\(\\sigma^2\\)).Assumptions random variables (e.g., independence, normality).Assumptions model specification (e.g., linearity regression).Note: Statistical testing :Confirm absolute certainty hypothesis true false.Confirm absolute certainty hypothesis true false.Interpret magnitude estimated value economic, practical, business contexts without additional analysis.\nStatistical significance: Refers whether observed effect unlikely due chance.\nPractical significance: Focuses real-world importance effect.\nInterpret magnitude estimated value economic, practical, business contexts without additional analysis.Statistical significance: Refers whether observed effect unlikely due chance.Statistical significance: Refers whether observed effect unlikely due chance.Practical significance: Focuses real-world importance effect.Practical significance: Focuses real-world importance effect.Example:marketing campaign increases sales \\(0.5\\%\\), statistically significant (\\(p < 0.05\\)). However, small market, may lack practical significance.Instead, inference provides framework making probabilistic statements population parameters, given sample data.","code":""},{"path":"basic-statistical-inference.html","id":"hypothesis-testing-framework","chapter":"4 Basic Statistical Inference","heading":"4.1 Hypothesis Testing Framework","text":"Hypothesis testing one fundamental tools statistics. provides formal procedure test claims assumptions (hypotheses) population parameters using sample data. process essential various fields, including business, medicine, social sciences, helps answer questions like “new marketing strategy improve sales?” “significant difference test scores two teaching methods?”goal hypothesis testing make decisions draw conclusions population based sample data. necessary rarely access entire population. example, company wants determine whether new advertising campaign increases sales, might analyze data sample stores rather every store globally.Key Steps Hypothesis TestingFormulate Hypotheses: Define null alternative hypotheses.Choose Significance Level (\\(\\alpha\\)): Determine acceptable probability making Type error.Select Test Statistic: Identify appropriate statistical test based data hypotheses.Define Rejection Region: Specify range values null hypothesis rejected.Compute Test Statistic: Use sample data calculate test statistic.Make Decision: Compare test statistic critical value use p-value decide whether reject fail reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"null-and-alternative-hypotheses","chapter":"4 Basic Statistical Inference","heading":"4.1.1 Null and Alternative Hypotheses","text":"heart hypothesis testing lies formulation two competing hypotheses:Null Hypothesis (\\(H_0\\)):\nRepresents current state knowledge, status quo, effect.\nassumed true unless strong evidence .\nExamples:\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\n\\(H_0: \\beta = 0\\) (predictor variable effect regression model).\n\nThink \\(H_0\\) “default assumption.”\nRepresents current state knowledge, status quo, effect.assumed true unless strong evidence .Examples:\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\n\\(H_0: \\beta = 0\\) (predictor variable effect regression model).\n\\(H_0: \\mu_1 = \\mu_2\\) (difference means two groups).\\(H_0: \\beta = 0\\) (predictor variable effect regression model).Think \\(H_0\\) “default assumption.”Alternative Hypothesis (\\(H_a\\) \\(H_1\\)):\nRepresents claim contradicts null hypothesis.\ntrying prove find evidence .\nExamples:\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\n\\(H_a: \\beta \\neq 0\\) (predictor variable effect).\n\nRepresents claim contradicts null hypothesis.trying prove find evidence .Examples:\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\n\\(H_a: \\beta \\neq 0\\) (predictor variable effect).\n\\(H_a: \\mu_1 \\neq \\mu_2\\) (means two groups different).\\(H_a: \\beta \\neq 0\\) (predictor variable effect).","code":""},{"path":"basic-statistical-inference.html","id":"errors-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.2 Errors in Hypothesis Testing","text":"Hypothesis testing involves decision-making uncertainty, meaning always risk making errors. errors classified two types:Type Error (\\(\\alpha\\)):\nOccurs null hypothesis rejected, even though true.\nExample: Concluding medication effective actually effect.\nprobability making Type error denoted \\(\\alpha\\), called significance level (commonly set 0.05 5%).\nOccurs null hypothesis rejected, even though true.Example: Concluding medication effective actually effect.probability making Type error denoted \\(\\alpha\\), called significance level (commonly set 0.05 5%).Type II Error (\\(\\beta\\)):\nOccurs null hypothesis rejected, alternative hypothesis true.\nExample: Failing detect medication effective actually works.\ncomplement \\(\\beta\\) called power test (\\(1 - \\beta\\)), representing probability correctly rejecting null hypothesis.\nOccurs null hypothesis rejected, alternative hypothesis true.Example: Failing detect medication effective actually works.complement \\(\\beta\\) called power test (\\(1 - \\beta\\)), representing probability correctly rejecting null hypothesis.Analogy: Legal SystemTo make concept intuitive, consider analogy courtroom:Null Hypothesis (\\(H_0\\)): defendant innocent.Alternative Hypothesis (\\(H_a\\)): defendant guilty.Type Error: Convicting innocent person (false positive).Type II Error: Letting guilty person go free (false negative).Balancing \\(\\alpha\\) \\(\\beta\\) critical hypothesis testing, reducing one often increases . example, make harder reject \\(H_0\\) (reducing \\(\\alpha\\)), increase chance failing detect true effect (increasing \\(\\beta\\)).","code":""},{"path":"basic-statistical-inference.html","id":"the-role-of-distributions-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.3 The Role of Distributions in Hypothesis Testing","text":"Distributions play fundamental role hypothesis testing provide mathematical model understanding test statistic behaves null hypothesis (\\(H_0\\)). Without distributions, impossible determine whether observed results due random chance provide evidence reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"expected-outcomes","chapter":"4 Basic Statistical Inference","heading":"4.1.3.1 Expected Outcomes","text":"One key reasons distributions crucial describe range values test statistic likely take \\(H_0\\) true. helps us understand considered “normal” variation data due random chance. example:Imagine conducting study test whether new marketing strategy increases average monthly sales. null hypothesis, assume new strategy effect, average sales remain unchanged.collect sample calculate test statistic, compare expected distribution (e.g., normal distribution \\(z\\)-test). distribution shows range test statistic values likely occur purely due random fluctuations data, assuming \\(H_0\\) true.providing baseline “normal,” distributions allow us identify unusual results may indicate null hypothesis false.","code":""},{"path":"basic-statistical-inference.html","id":"critical-values-and-rejection-regions","chapter":"4 Basic Statistical Inference","heading":"4.1.3.2 Critical Values and Rejection Regions","text":"Distributions also help define critical values rejection regions hypothesis testing. Critical values specific points distribution mark boundaries rejection region. rejection region range values test statistic lead us reject \\(H_0\\).location critical values depends :level significance (\\(\\alpha\\)), probability rejecting \\(H_0\\) true (Type error).level significance (\\(\\alpha\\)), probability rejecting \\(H_0\\) true (Type error).shape test statistic’s distribution \\(H_0\\).shape test statistic’s distribution \\(H_0\\).example:one-tailed \\(z\\)-test \\(\\alpha = 0.05\\), critical value approximately \\(1.645\\) standard normal distribution. calculated test statistic exceeds value, reject \\(H_0\\) result unlikely \\(H_0\\).Distributions help us visually mathematically determine critical points. examining distribution, can see rejection region lies probability observing value region random chance alone.","code":""},{"path":"basic-statistical-inference.html","id":"p-values","chapter":"4 Basic Statistical Inference","heading":"4.1.3.3 P-values","text":"p-value, central concept hypothesis testing, directly derived distribution test statistic \\(H_0\\). p-value represents probability observing test statistic extreme (extreme ) one calculated, assuming \\(H_0\\) true.p-value quantifies strength evidence \\(H_0\\). represents probability observing test statistic extreme (extreme ) one calculated, assuming \\(H_0\\) true.Small p-value (< \\(\\alpha\\)): Strong evidence \\(H_0\\); reject \\(H_0\\).Large p-value (> \\(\\alpha\\)): Weak evidence \\(H_0\\); fail reject \\(H_0\\).example:Suppose calculate \\(z\\)-test statistic \\(2.1\\) one-tailed test. Using standard normal distribution, p-value area curve right \\(z = 2.1\\). area represents likelihood observing result extreme \\(z = 2.1\\) \\(H_0\\) true.Suppose calculate \\(z\\)-test statistic \\(2.1\\) one-tailed test. Using standard normal distribution, p-value area curve right \\(z = 2.1\\). area represents likelihood observing result extreme \\(z = 2.1\\) \\(H_0\\) true.case, p-value approximately \\(0.0179\\). small p-value (typically less \\(\\alpha = 0.05\\)) suggests observed result unlikely \\(H_0\\) provides evidence reject null hypothesis.case, p-value approximately \\(0.0179\\). small p-value (typically less \\(\\alpha = 0.05\\)) suggests observed result unlikely \\(H_0\\) provides evidence reject null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"why-does-all-this-matter","chapter":"4 Basic Statistical Inference","heading":"4.1.3.4 Why Does All This Matter?","text":"summarize, distributions backbone hypothesis testing allow us :Define expected \\(H_0\\) modeling behavior test statistic.Define expected \\(H_0\\) modeling behavior test statistic.Identify results unlikely occur random chance, leads rejection \\(H_0\\).Identify results unlikely occur random chance, leads rejection \\(H_0\\).Calculate p-values quantify strength evidence \\(H_0\\).Calculate p-values quantify strength evidence \\(H_0\\).Distributions provide framework understanding role chance statistical analysis. essential determining expected outcomes, setting thresholds decision-making (critical values rejection regions), calculating p-values. solid grasp distributions greatly enhance ability interpret conduct hypothesis tests, making easier draw meaningful conclusions data.","code":""},{"path":"basic-statistical-inference.html","id":"the-test-statistic","chapter":"4 Basic Statistical Inference","heading":"4.1.4 The Test Statistic","text":"test statistic crucial component hypothesis testing, quantifies far observed data deviates expect null hypothesis (\\(H_0\\)) true. Essentially, provides standardized way compare observed outcomes expectations set \\(H_0\\), enabling us assess whether observed results likely due random chance indicative significant effect.general formula test statistic :\\[\n\\text{Test Statistic} = \\frac{\\text{Observed Value} - \\text{Expected Value } H_0}{\\text{Standard Error}}\n\\]component formula important role:Numerator:\nnumerator represents difference actual data (observed value) hypothetical value (expected value) assumed \\(H_0\\).\ndifference quantifies extent deviation. larger deviation suggests stronger evidence \\(H_0\\).\nnumerator represents difference actual data (observed value) hypothetical value (expected value) assumed \\(H_0\\).difference quantifies extent deviation. larger deviation suggests stronger evidence \\(H_0\\).Denominator:\ndenominator standard error, measures variability spread data. accounts factors sample size inherent randomness data.\ndividing numerator standard error, test statistic standardized, allowing comparisons across different studies, sample sizes, distributions.\ndenominator standard error, measures variability spread data. accounts factors sample size inherent randomness data.dividing numerator standard error, test statistic standardized, allowing comparisons across different studies, sample sizes, distributions.test statistic plays central role determining whether reject \\(H_0\\). calculated, compared known distribution (e.g., standard normal distribution \\(z\\)-tests \\(t\\)-distribution \\(t\\)-tests). comparison allows us evaluate likelihood observing test statistic \\(H_0\\):test statistic close 0: indicates observed data close expected \\(H_0\\). little evidence suggest rejecting \\(H_0\\).test statistic far 0 (tails distribution): suggests observed data deviates significantly expectations \\(H_0\\). deviations may provide strong evidence \\(H_0\\).","code":""},{"path":"basic-statistical-inference.html","id":"why-standardizing-matters","chapter":"4 Basic Statistical Inference","heading":"4.1.4.1 Why Standardizing Matters","text":"Standardizing difference observed expected values ensures test statistic biased factors scale measurement size sample. instance:raw difference 5 might highly significant one context negligible another, depending variability (standard error).raw difference 5 might highly significant one context negligible another, depending variability (standard error).Standardizing ensures magnitude test statistic reflects size difference reliability sample data.Standardizing ensures magnitude test statistic reflects size difference reliability sample data.","code":""},{"path":"basic-statistical-inference.html","id":"interpreting-the-test-statistic","chapter":"4 Basic Statistical Inference","heading":"4.1.4.2 Interpreting the Test Statistic","text":"calculating test statistic, used :Compare critical value: example, \\(z\\)-test \\(\\alpha = 0.05\\), critical values \\(-1.96\\) \\(1.96\\) two-tailed test. test statistic falls beyond values, \\(H_0\\) rejected.Calculate p-value: p-value derived distribution reflects probability observing test statistic extreme one calculated \\(H_0\\) true.","code":""},{"path":"basic-statistical-inference.html","id":"critical-values-and-rejection-regions-1","chapter":"4 Basic Statistical Inference","heading":"4.1.5 Critical Values and Rejection Regions","text":"critical value point distribution separates rejection region non-rejection region:Rejection Region: test statistic falls region, reject \\(H_0\\).Non-Rejection Region: test statistic falls , fail reject \\(H_0\\).rejection region depends significance level (\\(\\alpha\\)). two-tailed test \\(\\alpha = 0.05\\), critical values correspond top 2.5% bottom 2.5% distribution.","code":""},{"path":"basic-statistical-inference.html","id":"visualizing-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.1.6 Visualizing Hypothesis Testing","text":"Let’s create visualization tie concepts together:","code":"\n# Parameters\nalpha <- 0.05  # Significance level\ndf <- 29       # Degrees of freedom (for t-distribution)\nt_critical <-\n    qt(1 - alpha / 2, df)  # Critical value for two-tailed test\n\n# Generate t-distribution values\nt_values <- seq(-4, 4, length.out = 1000)\ndensity <- dt(t_values, df)\n\n# Observed test statistic\nt_obs <- 2.5  # Example observed test statistic\n\n# Plot the t-distribution\nplot(\n    t_values,\n    density,\n    type = \"l\",\n    lwd = 2,\n    col = \"blue\",\n    main = \"Hypothesis Testing with Distribution\",\n    xlab = \"Test Statistic (t-value)\",\n    ylab = \"Density\",\n    ylim = c(0, 0.4)\n)\n\n# Shade the rejection regions\npolygon(c(t_values[t_values <= -t_critical], -t_critical),\n        c(density[t_values <= -t_critical], 0),\n        col = \"red\",\n        border = NA)\npolygon(c(t_values[t_values >= t_critical], t_critical),\n        c(density[t_values >= t_critical], 0),\n        col = \"red\",\n        border = NA)\n\n# Add observed test statistic\npoints(\n    t_obs,\n    dt(t_obs, df),\n    col = \"green\",\n    pch = 19,\n    cex = 1.5\n)\ntext(\n    t_obs,\n    dt(t_obs, df) + 0.02,\n    paste(\"Observed t:\", round(t_obs, 2)),\n    col = \"green\",\n    pos = 3\n)\n\n# Highlight the critical values\nabline(\n    v = c(-t_critical, t_critical),\n    col = \"black\",\n    lty = 2\n)\ntext(\n    -t_critical,\n    0.05,\n    paste(\"Critical Value:\", round(-t_critical, 2)),\n    pos = 4,\n    col = \"black\"\n)\ntext(\n    t_critical,\n    0.05,\n    paste(\"Critical Value:\", round(t_critical, 2)),\n    pos = 4,\n    col = \"black\"\n)\n\n# Calculate p-value\np_value <- 2 * (1 - pt(abs(t_obs), df))  # Two-tailed p-value\ntext(0,\n     0.35,\n     paste(\"P-value:\", round(p_value, 4)),\n     col = \"blue\",\n     pos = 3)\n\n# Annotate regions\ntext(-3,\n     0.15,\n     \"Rejection Region\",\n     col = \"red\",\n     pos = 3)\ntext(3, 0.15, \"Rejection Region\", col = \"red\", pos = 3)\ntext(0,\n     0.05,\n     \"Non-Rejection Region\",\n     col = \"blue\",\n     pos = 3)\n\n# Add legend\nlegend(\n    \"topright\",\n    legend = c(\"Rejection Region\", \"Critical Value\", \"Observed Test Statistic\"),\n    col = c(\"red\", \"black\", \"green\"),\n    lty = c(NA, 2, NA),\n    pch = c(15, NA, 19),\n    bty = \"n\"\n)"},{"path":"basic-statistical-inference.html","id":"key-concepts-and-definitions","chapter":"4 Basic Statistical Inference","heading":"4.2 Key Concepts and Definitions","text":"","code":""},{"path":"basic-statistical-inference.html","id":"random-sample","chapter":"4 Basic Statistical Inference","heading":"4.2.1 Random Sample","text":"random sample size \\(n\\) consists \\(n\\) independent observations, drawn underlying population distribution. Independence ensures observation influences another, identical distribution guarantees observations governed probability rules.","code":""},{"path":"basic-statistical-inference.html","id":"sample-statistics","chapter":"4 Basic Statistical Inference","heading":"4.2.2 Sample Statistics","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sample-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.2.1 Sample Mean","text":"sample mean measure central tendency:\\[\n\\bar{X} = \\frac{\\sum_{=1}^{n} X_i}{n}\n\\]Example: Suppose measure heights 5 individuals (cm): \\(170, 165, 180, 175, 172\\). sample mean :\\[\n\\bar{X} = \\frac{170 + 165 + 180 + 175 + 172}{5} = 172.4 \\, \\text{cm}.\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-median","chapter":"4 Basic Statistical Inference","heading":"4.2.2.2 Sample Median","text":"sample median middle value ordered data:\\[\n\\tilde{x} = \\begin{cases}\n\\text{Middle observation,} & \\text{} n \\text{ odd}, \\\\\n\\text{Average two middle observations,} & \\text{} n \\text{ even}.\n\\end{cases}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.2.3 Sample Variance","text":"sample variance measures data spread:\\[\nS^2 = \\frac{\\sum_{=1}^{n}(X_i - \\bar{X})^2}{n-1}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-standard-deviation","chapter":"4 Basic Statistical Inference","heading":"4.2.2.4 Sample Standard Deviation","text":"sample standard deviation square root variance:\\[\nS = \\sqrt{S^2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-proportions","chapter":"4 Basic Statistical Inference","heading":"4.2.2.5 Sample Proportions","text":"Used categorical data:\\[\n\\hat{p} = \\frac{X}{n} = \\frac{\\text{Number successes}}{\\text{Sample size}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"estimators","chapter":"4 Basic Statistical Inference","heading":"4.2.2.6 Estimators","text":"Point Estimator: statistic (\\(\\hat{\\theta}\\)) used estimate population parameter (\\(\\theta\\)).Point Estimate:numerical value assumed \\(\\hat{\\theta}\\) evaluated given sample.Unbiased Estimator: point estimator \\(\\hat{\\theta}\\) unbiased \\(E(\\hat{\\theta}) = \\theta\\).Examples unbiased estimators:\\(\\bar{X}\\) \\(\\mu\\) (population mean).\\(\\bar{X}\\) \\(\\mu\\) (population mean).\\(S^2\\) \\(\\sigma^2\\) (population variance).\\(S^2\\) \\(\\sigma^2\\) (population variance).\\(\\hat{p}\\) \\(p\\) (population proportion).\\(\\hat{p}\\) \\(p\\) (population proportion).\\(\\widehat{p_1-p_2}\\) \\(p_1- p_2\\) (population proportion difference)\\(\\widehat{p_1-p_2}\\) \\(p_1- p_2\\) (population proportion difference)\\(\\bar{X_1} - \\bar{X_2}\\) \\(\\mu_1 - \\mu_2\\) (population mean difference)\\(\\bar{X_1} - \\bar{X_2}\\) \\(\\mu_1 - \\mu_2\\) (population mean difference)Note: \\(S^2\\) unbiased \\(\\sigma^2\\), \\(S\\) biased estimator \\(\\sigma\\).","code":""},{"path":"basic-statistical-inference.html","id":"distribution-of-the-sample-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.3 Distribution of the Sample Mean","text":"sampling distribution mean \\(\\bar{X}\\) depends :Population Distribution:\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\).\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\).Central Limit Theorem:\nlarge \\(n\\), \\(\\bar{X}\\) approximately follows normal distribution, regardless population’s shape.\nlarge \\(n\\), \\(\\bar{X}\\) approximately follows normal distribution, regardless population’s shape.","code":""},{"path":"basic-statistical-inference.html","id":"standard-error-of-the-mean","chapter":"4 Basic Statistical Inference","heading":"4.2.3.1 Standard Error of the Mean","text":"standard error quantifies variability \\(\\bar{X}\\):\\[\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]Example: - Suppose \\(\\sigma = 10\\) \\(n = 25\\). : \\[\n  \\sigma_{\\bar{X}} = \\frac{10}{\\sqrt{25}} = 2.\n  \\]smaller standard error, precise estimate population mean.","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.3 One-Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-single-mean","chapter":"4 Basic Statistical Inference","heading":"4.3.1 For Single Mean","text":"Consider scenario \\[\nY_i \\sim \\text{..d. } N(\\mu, \\sigma^2),\n\\]..d. stands “independent identically distributed.” model can expressed :\\[\nY_i = \\mu + \\epsilon_i,\n\\]:\\(\\epsilon_i \\sim^{\\text{..d.}} N(0, \\sigma^2)\\),\\(E(Y_i) = \\mu\\),\\(\\text{Var}(Y_i) = \\sigma^2\\),\\(\\bar{y} \\sim N(\\mu, \\sigma^2 / n)\\).\\(\\sigma^2\\) estimated \\(s^2\\), standardized test statistic follows \\(t\\)-distribution:\\[\n\\frac{\\bar{y} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}.\n\\]\\(100(1-\\alpha)\\%\\) confidence interval \\(\\mu\\) obtained :\\[\n1 - \\alpha = P\\left(-t_{\\alpha/2;n-1} \\leq \\frac{\\bar{y} - \\mu}{s / \\sqrt{n}} \\leq t_{\\alpha/2;n-1}\\right),\n\\]equivalently,\\[\nP\\left(\\bar{y} - t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{y} + t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}}\\right).\n\\]confidence interval expressed :\\[\n\\bar{y} \\pm t_{\\alpha/2;n-1}\\frac{s}{\\sqrt{n}},\n\\]\\(s / \\sqrt{n}\\) standard error \\(\\bar{y}\\).experiment repeated many times, \\(100(1-\\alpha)\\%\\) intervals contain \\(\\mu\\).","code":""},{"path":"basic-statistical-inference.html","id":"power-in-hypothesis-testing","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1 Power in Hypothesis Testing","text":"Power (\\(\\pi(\\mu)\\)) hypothesis test represents probability correctly rejecting null hypothesis (\\(H_0\\)) false (.e., alternative hypothesis \\(H_A\\) true). Formally, expressed :\\[ \\begin{aligned} \\text{Power} &= \\pi(\\mu) = 1 - \\beta \\\\ &= P(\\text{test rejects } H_0|\\mu) \\\\ &= P(\\text{test rejects } H_0| H_A \\text{ true}), \\end{aligned} \\]\\(\\beta\\) probability Type II error (failing reject \\(H_0\\) false).calculate probability:\\(H_0\\): distribution test statistic centered around null parameter (e.g., \\(\\mu_0\\)).\\(H_0\\): distribution test statistic centered around null parameter (e.g., \\(\\mu_0\\)).\\(H_A\\): test statistic distributed differently, shifted according true value \\(H_A\\) (e.g., \\(\\mu_1\\)).\\(H_A\\): test statistic distributed differently, shifted according true value \\(H_A\\) (e.g., \\(\\mu_1\\)).Hence, evaluate power, crucial determine distribution test statistic alternative hypothesis, \\(H_A\\)., derive power one-sided two-sided z-tests.","code":""},{"path":"basic-statistical-inference.html","id":"one-sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.1 One-Sided z-Test","text":"Consider hypotheses:\\[ H_0: \\mu \\leq \\mu_0 \\quad \\text{vs.} \\quad H_A: \\mu > \\mu_0 \\]power one-sided z-test derived follows:test rejects \\(H_0\\) \\(\\bar{y} > \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}\\), \\(z_{\\alpha}\\) critical value test significance level \\(\\alpha\\).alternative hypothesis, distribution \\(\\bar{y}\\) centered \\(\\mu\\), standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).power :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P\\left(\\bar{y} > \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) \\\\\n&= P\\left(Z > z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\middle| \\mu \\right), \\quad \\text{} Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} \\\\\n&= 1 - \\Phi\\left(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}\\right) \\\\\n&= \\Phi\\left(-z_{\\alpha} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right).\n\\end{aligned}\n\\], use symmetry standard normal distribution: \\(1 - \\Phi(x) = \\Phi(-x)\\).Suppose wish show mean response \\(\\mu\\) treatment higher mean response \\(\\mu_0\\) without treatment (.e., treatment effect \\(\\delta = \\mu - \\mu_0\\) large).Since power increasing function \\(\\mu - \\mu_0\\), suffices find sample size \\(n\\) achieves desired power \\(1 - \\beta\\) \\(\\mu = \\mu_0 + \\delta\\). power \\(\\mu = \\mu_0 + \\delta\\) :\\[\n\\pi(\\mu_0 + \\delta) = \\Phi\\left(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}\\right) = 1 - \\beta\n\\]Given \\(\\Phi(z_{\\beta}) = 1 - \\beta\\), :\\[\n-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta}\n\\]Solving \\(n\\), obtain:\\[\nn = \\left(\\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta}\\right)^2\n\\]Larger sample sizes required :sample variability large (\\(\\sigma\\) large).significance level \\(\\alpha\\) small (\\(z_{\\alpha}\\) large).desired power \\(1 - \\beta\\) large (\\(z_{\\beta}\\) large).magnitude effect small (\\(\\delta\\) small).practice, \\(\\delta\\) \\(\\sigma\\) often unknown. estimate \\(\\sigma\\), can:Use prior studies pilot studies.Approximate \\(\\sigma\\) based anticipated range observations (excluding outliers). normally distributed data, dividing range 4 provides reasonable estimate \\(\\sigma\\).considerations ensure test adequately powered detect meaningful effects balancing practical constraints sample size.","code":""},{"path":"basic-statistical-inference.html","id":"two-sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.2 Two-Sided z-Test","text":"two-sided test, hypotheses :\\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs.} \\quad H_A: \\mu \\neq \\mu_0\n\\]test rejects \\(H_0\\) \\(\\bar{y}\\) lies outside interval \\(\\mu_0 \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\). power test :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P\\left(\\bar{y} < \\mu_0 - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) + P\\left(\\bar{y} > \\mu_0 + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\middle| \\mu \\right) \\\\\n&= \\Phi\\left(-z_{\\alpha/2} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) + \\Phi\\left(-z_{\\alpha/2} - \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right).\n\\end{aligned}\n\\]ensure power \\(1-\\beta\\) treatment effect \\(\\delta = |\\mu - \\mu_0|\\) least certain value, solve \\(n\\). Since power function two-sided test increasing symmetric \\(|\\mu - \\mu_0|\\), suffices find \\(n\\) power equals \\(1-\\beta\\) \\(\\mu = \\mu_0 + \\delta\\). gives:\\[\nn = \\left(\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta}\\right)^2\n\\]Alternatively, required sample size can determined using confidence interval approach. two-sided \\(\\alpha\\)-level confidence interval form:\\[\n\\bar{y} \\pm D\n\\]\\(D = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\), solving \\(n\\) gives:\\[\nn = \\left(\\frac{z_{\\alpha/2} \\sigma}{D}\\right)^2\n\\]value rounded nearest integer ensure required precision.one-sided hypothesis test, testing \\(H_0: \\mu \\geq 30\\) versus \\(H_a: \\mu < 30\\):\\(\\sigma\\) unknown, can estimate using:Prior studies pilot studies.Prior studies pilot studies.range observations (excluding outliers) divided 4, provides reasonable approximation normally distributed data.range observations (excluding outliers) divided 4, provides reasonable approximation normally distributed data.","code":"\n# Generate random data and compute a 95% confidence interval\ndata <- rnorm(100) # Generate 100 random values\nt.test(data, conf.level = 0.95) # Perform t-test with 95% confidence interval\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = -1.3809, df = 99, p-value = 0.1704\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.33722662  0.06046365\n#> sample estimates:\n#>  mean of x \n#> -0.1383815\n# Perform one-sided t-test\nt.test(data, mu = 30, alternative = \"less\")\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = -300.74, df = 99, p-value < 2.2e-16\n#> alternative hypothesis: true mean is less than 30\n#> 95 percent confidence interval:\n#>        -Inf 0.02801196\n#> sample estimates:\n#>  mean of x \n#> -0.1383815"},{"path":"basic-statistical-inference.html","id":"z-test-summary","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.3 z-Test Summary","text":"one-sided tests:\\[ \\pi(\\mu) = \\Phi\\left(-z_{\\alpha} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) \\]two-sided tests:\\[ \\pi(\\mu) = \\Phi\\left(-z_{\\alpha/2} + \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) + \\Phi\\left(-z_{\\alpha/2} - \\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}\\right) \\]Factors Affecting PowerEffect Size (\\(\\mu - \\mu_0\\)): Larger differences \\(\\mu\\) \\(\\mu_0\\) increase power.Sample Size (\\(n\\)): Larger \\(n\\) reduces standard error, increasing power.Variance (\\(\\sigma^2\\)): Smaller variance increases power.Significance Level (\\(\\alpha\\)): Increasing \\(\\alpha\\) (making test liberal) increases power \\(z_{\\alpha}\\).","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-t-test","chapter":"4 Basic Statistical Inference","heading":"4.3.1.1.4 One-Sample t-test","text":"hypothesis testing, calculating power determining required sample size t-tests complex z-tests. complexity arises involvement Student’s t-distribution generalized form, non-central t-distribution.power function one-sample t-test can expressed :\\[\n\\pi(\\mu) = P\\left(\\frac{\\bar{y} - \\mu_0}{s / \\sqrt{n}} > t_{n-1; \\alpha} \\mid \\mu \\right)\n\\]:\\(\\mu_0\\) hypothesized population mean null hypothesis,\\(\\mu_0\\) hypothesized population mean null hypothesis,\\(\\bar{y}\\) sample mean,\\(\\bar{y}\\) sample mean,\\(s\\) sample standard deviation,\\(s\\) sample standard deviation,\\(n\\) sample size,\\(n\\) sample size,\\(t_{n-1; \\alpha}\\) critical t-value Student’s t-distribution \\(n-1\\) degrees freedom significance level \\(\\alpha\\).\\(t_{n-1; \\alpha}\\) critical t-value Student’s t-distribution \\(n-1\\) degrees freedom significance level \\(\\alpha\\).\\(\\mu > \\mu_0\\) (.e., \\(\\mu - \\mu_0 = \\delta\\)), random variable\\[\nT = \\frac{\\bar{y} - \\mu_0}{s / \\sqrt{n}}\n\\]follow Student’s t-distribution. Instead, follows non-central t-distribution :non-centrality parameter \\(\\lambda = \\delta \\sqrt{n} / \\sigma\\), \\(\\sigma\\) population standard deviation,non-centrality parameter \\(\\lambda = \\delta \\sqrt{n} / \\sigma\\), \\(\\sigma\\) population standard deviation,degrees freedom \\(n-1\\).degrees freedom \\(n-1\\).Key Properties Power FunctionThe power \\(\\pi(\\mu)\\) increasing function non-centrality parameter \\(\\lambda\\).\\(\\delta = 0\\) (.e., null hypothesis true), non-central t-distribution simplifies regular Student’s t-distribution.calculate power practice, numerical procedures (see ) precomputed charts typically required.Approximate Sample Size Adjustment t-testsWhen planning study, researchers often start approximation based z-tests adjust specifics t-test. ’s process:1. Start Sample Size z-testFor two-sided test: \\[\nn_z = \\frac{\\left(z_{\\alpha/2} + z_\\beta\\right)^2 \\sigma^2}{\\delta^2}\n\\] :\\(z_{\\alpha/2}\\) critical value standard normal distribution two-tailed test,\\(z_{\\alpha/2}\\) critical value standard normal distribution two-tailed test,\\(z_\\beta\\) corresponds desired power \\(1 - \\beta\\),\\(z_\\beta\\) corresponds desired power \\(1 - \\beta\\),\\(\\delta\\) effect size \\(\\mu - \\mu_0\\),\\(\\delta\\) effect size \\(\\mu - \\mu_0\\),\\(\\sigma\\) population standard deviation.\\(\\sigma\\) population standard deviation.2. Adjust t-distributionLet \\(v = n - 1\\), \\(n\\) sample size derived z-test. two-sided t-test, approximate sample size :\\[\nn^* = \\frac{\\left(t_{v; \\alpha/2} + t_{v; \\beta}\\right)^2 \\sigma^2}{\\delta^2}\n\\]:\\(t_{v; \\alpha/2}\\) \\(t_{v; \\beta}\\) critical values Student’s t-distribution significance level \\(\\alpha\\) desired power, respectively.\\(t_{v; \\alpha/2}\\) \\(t_{v; \\beta}\\) critical values Student’s t-distribution significance level \\(\\alpha\\) desired power, respectively.Since \\(v\\) depends \\(n^*\\), process may require iterative refinement.Since \\(v\\) depends \\(n^*\\), process may require iterative refinement.Notes:Approximations: formulas provide intuitive starting point may require adjustments based exact numerical solutions.Insights: Power increasing function :\neffect size \\(\\delta\\),\nsample size \\(n\\),\ndecreasing function population variability \\(\\sigma\\).\neffect size \\(\\delta\\),sample size \\(n\\),decreasing function population variability \\(\\sigma\\).","code":"\n# Example: Power calculation for a one-sample t-test\nlibrary(pwr)\n\n# Parameters\neffect_size <- 0.5  # Cohen's d\nalpha <- 0.05       # Significance level\npower <- 0.8        # Desired power\n\n# Compute sample size\nsample_size <-\n    pwr.t.test(\n        d = effect_size,\n        sig.level = alpha,\n        power = power,\n        type = \"one.sample\"\n    )$n\n\n# Print result\ncat(\"Required sample size for one-sample t-test:\",\n    ceiling(sample_size),\n    \"\\n\")\n#> Required sample size for one-sample t-test: 34\n\n# Power calculation for a given sample size\ncalculated_power <-\n    pwr.t.test(\n        n = ceiling(sample_size),\n        d = effect_size,\n        sig.level = alpha,\n        type = \"one.sample\"\n    )$power\ncat(\"Achieved power with computed sample size:\",\n    calculated_power,\n    \"\\n\")\n#> Achieved power with computed sample size: 0.8077775"},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-independent-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.2 For Difference of Means, Independent Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-paired-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.3 For Difference of Means, Paired Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-two-proportions","chapter":"4 Basic Statistical Inference","heading":"4.3.4 For Difference of Two Proportions","text":"mean difference two sample proportions given :\\[\n\\hat{p_1} - \\hat{p_2}\n\\]variance difference proportions :\\[\n\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}\n\\]\\(100(1-\\alpha)\\%\\) confidence interval difference proportions calculated :\\[\n\\hat{p_1} - \\hat{p_2} \\pm z_{\\alpha/2} \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]\\(z_{\\alpha/2}\\): critical value standard normal distribution.\\(z_{\\alpha/2}\\): critical value standard normal distribution.\\(\\hat{p_1}\\), \\(\\hat{p_2}\\): Sample proportions.\\(\\hat{p_1}\\), \\(\\hat{p_2}\\): Sample proportions.\\(n_1\\), \\(n_2\\): Sample sizes.\\(n_1\\), \\(n_2\\): Sample sizes.Sample Size Desired Confidence Level Margin ErrorTo achieve margin error \\(d\\) given confidence level, required sample size can estimated follows:Prior Estimates \\(\\hat{p_1}\\) \\(\\hat{p_2}\\): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\left[p_1(1-p_1) + p_2(1-p_2)\\right]}{d^2}\n\\]Prior Estimates \\(\\hat{p_1}\\) \\(\\hat{p_2}\\): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\left[p_1(1-p_1) + p_2(1-p_2)\\right]}{d^2}\n\\]Without Prior Estimates (assuming maximum variability, \\(\\hat{p} = 0.5\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Without Prior Estimates (assuming maximum variability, \\(\\hat{p} = 0.5\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Hypothesis Testing Difference ProportionsThe test statistic hypothesis testing depends null hypothesis:\\((p_1 - p_2) \\neq 0\\): \\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2}) - (p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]\\((p_1 - p_2) \\neq 0\\): \\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2}) - (p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]\\((p_1 - p_2)_0 = 0\\) (testing equality proportions): \\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p}) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\]\\((p_1 - p_2)_0 = 0\\) (testing equality proportions): \\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p}) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\]\\(\\hat{p}\\) pooled sample proportion:\\[\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1\\hat{p_1} + n_2\\hat{p_2}}{n_1 + n_2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"for-single-proportion","chapter":"4 Basic Statistical Inference","heading":"4.3.5 For Single Proportion","text":"\\(100(1-\\alpha)\\%\\) confidence interval population proportion \\(p\\) :\\[\n\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]Sample Size DeterminationWith Prior Estimate (\\(\\hat{p}\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\n\\]Prior Estimate (\\(\\hat{p}\\)): \\[\nn \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\n\\]Without Prior Estimate: \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\n\\]Without Prior Estimate: \\[\nn \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\n\\]test statistic \\(H_0: p = p_0\\) :\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"for-single-variance","chapter":"4 Basic Statistical Inference","heading":"4.3.6 For Single Variance","text":"sample variance \\(s^2\\) \\(n\\) observations, \\(100(1-\\alpha)\\%\\) confidence interval population variance \\(\\sigma^2\\) :\\[\n\\begin{aligned}\n1 - \\alpha &= P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2)\\\\\n&=P\\left(\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2; n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2; n-1}}\\right)\n\\end{aligned}\n\\]Equivalently, confidence interval can written :\\[\n\\left(\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\\right)\n\\]find confidence limits \\(\\sigma\\), compute square root interval bounds:\\[\n\\text{Confidence Interval } \\sigma: \\quad \\left(\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}}\\right)\n\\]Hypothesis Testing VarianceThe test statistic testing null hypothesis population variance (\\(\\sigma^2_0\\)) :\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0}\n\\]test statistic follows chi-squared distribution \\(n-1\\) degrees freedom null hypothesis.","code":""},{"path":"basic-statistical-inference.html","id":"non-parametric-tests","chapter":"4 Basic Statistical Inference","heading":"4.3.7 Non-parametric Tests","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sign-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.1 Sign Test","text":"Sign Test used test hypotheses median population, \\(\\mu_{(0.5)}\\), without assuming specific distribution data. test ideal small sample sizes normality assumptions met.test population median, consider hypotheses:Null Hypothesis: \\(H_0: \\mu_{(0.5)} = 0\\)Alternative Hypothesis: \\(H_a: \\mu_{(0.5)} > 0\\) (one-sided test)Steps:Count Positive Negative Deviations:\nCount observations (\\(y_i\\)) greater 0: \\(s_+\\) (number positive signs).\nCount observations less 0: \\(s_-\\) (number negative signs).\n\\(s_- = n - s_+\\).\nCount Positive Negative Deviations:Count observations (\\(y_i\\)) greater 0: \\(s_+\\) (number positive signs).Count observations less 0: \\(s_-\\) (number negative signs).\\(s_- = n - s_+\\).Decision Rule:\nReject \\(H_0\\) \\(s_+\\) large (equivalently, \\(s_-\\) small).\ndetermine large \\(s_+\\) must , use distribution \\(S_+\\) \\(H_0\\), Binomial \\(p = 0.5\\).\nDecision Rule:Reject \\(H_0\\) \\(s_+\\) large (equivalently, \\(s_-\\) small).determine large \\(s_+\\) must , use distribution \\(S_+\\) \\(H_0\\), Binomial \\(p = 0.5\\).Null Distribution:\n\\(H_0\\), \\(S_+\\) follows: \\[\nS_+ \\sim Binomial(n, p = 0.5)\n\\]Null Distribution:\n\\(H_0\\), \\(S_+\\) follows: \\[\nS_+ \\sim Binomial(n, p = 0.5)\n\\]Critical Value:\nReject \\(H_0\\) : \\[\ns_+ \\ge b_{n,\\alpha}\n\\] \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical value binomial distribution.Critical Value:\nReject \\(H_0\\) : \\[\ns_+ \\ge b_{n,\\alpha}\n\\] \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical value binomial distribution.p-value Calculation:\nCompute p-value observed (one-tailed) \\(s_+\\) : \\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{=s_+}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]\nAlternatively: \\[\nP(S \\le s_-) = \\sum_{=0}^{s_-} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]p-value Calculation:\nCompute p-value observed (one-tailed) \\(s_+\\) : \\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{=s_+}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]Alternatively: \\[\nP(S \\le s_-) = \\sum_{=0}^{s_-} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n\n\\]Large Sample Normal ApproximationFor large \\(n\\), use normal approximation binomial test. Reject \\(H_0\\) : \\[\ns_+ \\ge \\frac{n}{2} + \\frac{1}{2} + z_{\\alpha} \\sqrt{\\frac{n}{4}}\n\\] \\(z_\\alpha\\) critical value one-sided test.two-sided tests, use maximum minimum \\(s_+\\) \\(s_-\\):Test statistic: \\(s_{\\text{max}} = \\max(s_+, s_-)\\) \\(s_{\\text{min}} = \\min(s_+, s_-)\\)Test statistic: \\(s_{\\text{max}} = \\max(s_+, s_-)\\) \\(s_{\\text{min}} = \\min(s_+, s_-)\\)Reject \\(H_0\\) \\(p\\)-value less \\(\\alpha\\), : \\[\np\\text{-value} = 2 \\sum_{=s_{\\text{max}}}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n = 2 \\sum_{= 0}^{s_{min}} \\binom{n}{} \\left( \\frac{1}{2} \\right)^n\n\\]Reject \\(H_0\\) \\(p\\)-value less \\(\\alpha\\), : \\[\np\\text{-value} = 2 \\sum_{=s_{\\text{max}}}^{n} \\binom{n}{} \\left(\\frac{1}{2}\\right)^n = 2 \\sum_{= 0}^{s_{min}} \\binom{n}{} \\left( \\frac{1}{2} \\right)^n\n\\]Equivalently, rejecting \\(H_0\\) \\(s_{max} \\ge b_{n,\\alpha/2}\\).large \\(n\\), normal approximation uses: \\[\nz = \\frac{s_{\\text{max}} - \\frac{n}{2} - \\frac{1}{2}}{\\sqrt{\\frac{n}{4}}}\n\\]\nReject \\(H_0\\) \\(\\alpha\\) \\(z \\ge z_{\\alpha/2}\\).Handling zeros data common issue Sign Test:Random Assignment: Assign zeros randomly either \\(s_+\\) \\(s_-\\) (2 researchers might get different results).Fractional Assignment: Count zero \\(0.5\\) toward \\(s_+\\) \\(s_-\\) (apply Binomial Distribution afterward).Ignore Zeros: Ignore zeros, note reduces sample size power.","code":"\n# Example Data\ndata <- c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33)\n\n# Count positive signs\ns_plus <- sum(data > 0)\n\n# Sample size excluding zeros\nn <- length(data)\n\n# Perform a one-sided binomial test\nbinom.test(s_plus, n, p = 0.5, alternative = \"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  s_plus and n\n#> number of successes = 8, number of trials = 10, p-value = 0.05469\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.4930987 1.0000000\n#> sample estimates:\n#> probability of success \n#>                    0.8"},{"path":"basic-statistical-inference.html","id":"wilcoxon-signed-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.2 Wilcoxon Signed Rank Test","text":"Wilcoxon Signed Rank Test improvement Sign Test considers magnitude direction deviations null hypothesis value (e.g., 0). However, test assumes data symmetrically distributed around median, unlike Sign Test.test following hypotheses:\\[\nH_0: \\mu_{(0.5)} = 0 \\\\\nH_a: \\mu_{(0.5)} > 0\n\\]example assumes ties duplicate observations data.Procedure Signed Rank TestRank Absolute Values:\nRank observations \\(y_i\\) based absolute values.\nLet \\(r_i\\) denote rank \\(y_i\\).\nSince ties, ranks \\(r_i\\) uniquely determined form permutation integers \\(1, 2, \\dots, n\\).\nRank observations \\(y_i\\) based absolute values.Let \\(r_i\\) denote rank \\(y_i\\).Since ties, ranks \\(r_i\\) uniquely determined form permutation integers \\(1, 2, \\dots, n\\).Calculate \\(w_+\\) \\(w_-\\):\n\\(w_+\\) sum ranks corresponding positive values \\(y_i\\).\n\\(w_-\\) sum ranks corresponding negative values \\(y_i\\).\ndefinition: \\[\nw_+ + w_- = \\sum_{=1}^n r_i = \\frac{n(n+1)}{2}\n\\]\n\\(w_+\\) sum ranks corresponding positive values \\(y_i\\).\\(w_-\\) sum ranks corresponding negative values \\(y_i\\).definition: \\[\nw_+ + w_- = \\sum_{=1}^n r_i = \\frac{n(n+1)}{2}\n\\]Decision Rule:\nReject \\(H_0\\) \\(w_+\\) large (equivalently, \\(w_-\\) small).\nReject \\(H_0\\) \\(w_+\\) large (equivalently, \\(w_-\\) small).Null Distribution \\(W_+\\)null hypothesis, distributions \\(W_+\\) \\(W_-\\) identical symmetric. p-value one-sided test :\\[\n\\text{p-value} = P(W \\ge w_+) = P(W \\le w_-)\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(w_+ \\ge w_{n,\\alpha}\\), \\(w_{n,\\alpha}\\) critical value table null distribution \\(W_+\\).two-sided tests, use:\\[\np\\text{-value} = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\]Normal Approximation Large SamplesFor large \\(n\\), null distribution \\(W_+\\) can approximated normal distribution:\\[\nz = \\frac{w_+ - \\frac{n(n+1)}{4} - \\frac{1}{2}}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) :\\[\nw_+ \\ge \\frac{n(n+1)}{4} + \\frac{1}{2} + z_{\\alpha} \\sqrt{\\frac{n(n+1)(2n+1)}{24}} \\approx w_{n,\\alpha}\n\\]two-sided test, decision rule uses maximum minimum \\(w_+\\) \\(w_-\\):\\(w_{max} = \\max(w_+, w_-)\\)\\(w_{max} = \\max(w_+, w_-)\\)\\(w_{min} = \\min(w_+, w_-)\\)\\(w_{min} = \\min(w_+, w_-)\\)p-value computed :\\[\np\\text{-value} = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\]Handling Tied RanksIf observations \\(|y_i|\\) tied absolute values, assign average rank (“midrank”) tied values. example:Suppose \\(y_1 = -1\\), \\(y_2 = 3\\), \\(y_3 = -3\\), \\(y_4 = 5\\).ranks \\(|y_i|\\) :\n\\(|y_1| = 1\\): \\(r_1 = 1\\)\n\\(|y_2| = |y_3| = 3\\): \\(r_2 = r_3 = \\frac{2+3}{2} = 2.5\\)\n\\(|y_4| = 5\\): \\(r_4 = 4\\)\n\\(|y_1| = 1\\): \\(r_1 = 1\\)\\(|y_2| = |y_3| = 3\\): \\(r_2 = r_3 = \\frac{2+3}{2} = 2.5\\)\\(|y_4| = 5\\): \\(r_4 = 4\\)large samples, can use normal approximation setting exact = FALSE:","code":"\n# Example Data\ndata <- c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33)\n\n# Perform Wilcoxon Signed Rank Test (exact test)\nwilcox_exact <- wilcox.test(data, exact = TRUE)\n\n# Display results\nwilcox_exact\n#> \n#>  Wilcoxon signed rank exact test\n#> \n#> data:  data\n#> V = 52, p-value = 0.009766\n#> alternative hypothesis: true location is not equal to 0\n# Perform Wilcoxon Signed Rank Test (normal approximation)\nwilcox_normal <- wilcox.test(data, exact = FALSE)\n\n# Display results\nwilcox_normal\n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  data\n#> V = 52, p-value = 0.01443\n#> alternative hypothesis: true location is not equal to 0"},{"path":"basic-statistical-inference.html","id":"wald-wolfowitz-runs-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.3 Wald-Wolfowitz Runs Test","text":"Runs Test non-parametric test used examine randomness sequence. Specifically, tests whether order observations sequence random. test useful detecting non-random patterns, trends, clustering, periodicity.hypotheses Runs Test :Null Hypothesis: \\(H_0\\): sequence random.Alternative Hypothesis: \\(H_a\\): sequence random.run sequence consecutive observations type. example: - binary sequence + + - - + - + +, 5 runs: ++, --, +, -, ++.Runs can formed based classification criteria, :Positive vs. Negative valuesPositive vs. Negative valuesAbove vs. medianAbove vs. medianSuccess vs. Failure binary outcomesSuccess vs. Failure binary outcomesTest StatisticNumber Runs (\\(R\\)):\nobserved number runs sequence.Number Runs (\\(R\\)):\nobserved number runs sequence.Expected Number Runs (\\(E[R]\\)):\nnull hypothesis randomness, expected number runs : \\[\nE[R] = \\frac{2 n_1 n_2}{n_1 + n_2} + 1\n\\] :\n\\(n_1\\): Number observations first category (e.g., positives).\n\\(n_2\\): Number observations second category (e.g., negatives).\n\\(n = n_1 + n_2\\): Total number observations.\nExpected Number Runs (\\(E[R]\\)):\nnull hypothesis randomness, expected number runs : \\[\nE[R] = \\frac{2 n_1 n_2}{n_1 + n_2} + 1\n\\] :\\(n_1\\): Number observations first category (e.g., positives).\\(n_2\\): Number observations second category (e.g., negatives).\\(n = n_1 + n_2\\): Total number observations.Variance Runs (\\(\\text{Var}[R]\\)):\nvariance number runs given : \\[\n\\text{Var}[R] = \\frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}\n\\]Variance Runs (\\(\\text{Var}[R]\\)):\nvariance number runs given : \\[\n\\text{Var}[R] = \\frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge samples (\\(n \\geq 20\\)), test statistic approximately normally distributed: \\[\nz = \\frac{R - E[R]}{\\sqrt{\\text{Var}[R]}}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge samples (\\(n \\geq 20\\)), test statistic approximately normally distributed: \\[\nz = \\frac{R - E[R]}{\\sqrt{\\text{Var}[R]}}\n\\]Decision RuleCompute \\(z\\)-value compare critical value standard normal distribution.significance level \\(\\alpha\\):\nReject \\(H_0\\) \\(|z| \\ge z_{\\alpha/2}\\) (two-sided test).\nReject \\(H_0\\) \\(z \\ge z_\\alpha\\) \\(z \\le -z_\\alpha\\) one-sided tests.\nReject \\(H_0\\) \\(|z| \\ge z_{\\alpha/2}\\) (two-sided test).Reject \\(H_0\\) \\(z \\ge z_\\alpha\\) \\(z \\le -z_\\alpha\\) one-sided tests.Steps Conducting Runs Test:Classify data two groups (e.g., /median, positive/negative).Count total number runs (\\(R\\)).Compute \\(E[R]\\) \\(\\text{Var}[R]\\) based \\(n_1\\) \\(n_2\\).Compute \\(z\\)-value observed number runs.Compare \\(z\\)-value critical value decide whether reject \\(H_0\\).numerical dataset test based values median:output runs.test function includes:Observed Runs: actual number runs sequence.Observed Runs: actual number runs sequence.Expected Runs: expected number runs \\(H_0\\).Expected Runs: expected number runs \\(H_0\\).p-value: probability observing number runs extreme observed one \\(H_0\\).p-value: probability observing number runs extreme observed one \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude sequence random.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude sequence random.Limitations Runs TestThe test assumes observations independent.test assumes observations independent.small sample sizes, test may limited power.small sample sizes, test may limited power.Ties data must resolved predefined rule (e.g., treating ties belonging one group excluding ).Ties data must resolved predefined rule (e.g., treating ties belonging one group excluding ).","code":"\n# Example dataset\ndata <- c(1.2, -0.5, 3.4, -1.1, 2.8, -0.8, 4.5, 0.7)\n\nlibrary(randtests)\n# Perform Runs Test (above/below median)\nruns.test(data)\n#> \n#>  Runs Test\n#> \n#> data:  data\n#> statistic = 2.2913, runs = 8, n1 = 4, n2 = 4, n = 8, p-value = 0.02195\n#> alternative hypothesis: nonrandomness"},{"path":"basic-statistical-inference.html","id":"quantile-or-percentile-test","chapter":"4 Basic Statistical Inference","heading":"4.3.7.4 Quantile (or Percentile) Test","text":"Quantile Test (also called Percentile Test) non-parametric test used evaluate whether proportion observations falling within specific quantile matches expected proportion null hypothesis. test useful assessing distribution data specific quantiles (e.g., medians percentiles) interest.Suppose want test whether true proportion data specified quantile \\(q\\) matches given probability \\(p\\). hypotheses :Null Hypothesis: \\(H_0\\): true proportion equal \\(p\\).Alternative Hypothesis: \\(H_a\\): true proportion equal \\(p\\) (two-sided), greater \\(p\\) (right-tailed), less \\(p\\) (left-tailed).Test StatisticThe test statistic based observed count data points specified quantile.Observed Count (\\(k\\)):\nnumber data points \\(y_i\\) \\(y_i \\leq q\\).Observed Count (\\(k\\)):\nnumber data points \\(y_i\\) \\(y_i \\leq q\\).Expected Count (\\(E[k]\\)):\nexpected number observations quantile \\(q\\) \\(H_0\\) : \\[\nE[k] = n \\cdot p\n\\]Expected Count (\\(E[k]\\)):\nexpected number observations quantile \\(q\\) \\(H_0\\) : \\[\nE[k] = n \\cdot p\n\\]Variance:\nbinomial distribution, variance : \\[\n\\text{Var}[k] = n \\cdot p \\cdot (1 - p)\n\\]Variance:\nbinomial distribution, variance : \\[\n\\text{Var}[k] = n \\cdot p \\cdot (1 - p)\n\\]Standardized Test Statistic (\\(z\\)):\nlarge \\(n\\), test statistic approximately normally distributed: \\[\nz = \\frac{k - E[k]}{\\sqrt{\\text{Var}[k]}} = \\frac{k - n \\cdot p}{\\sqrt{n \\cdot p \\cdot (1 - p)}}\n\\]Standardized Test Statistic (\\(z\\)):\nlarge \\(n\\), test statistic approximately normally distributed: \\[\nz = \\frac{k - E[k]}{\\sqrt{\\text{Var}[k]}} = \\frac{k - n \\cdot p}{\\sqrt{n \\cdot p \\cdot (1 - p)}}\n\\]Decision RuleCompute \\(z\\)-value observed count.Compare \\(z\\)-value critical value standard normal distribution:\ntwo-sided test, reject \\(H_0\\) \\(|z| \\geq z_{\\alpha/2}\\).\none-sided test, reject \\(H_0\\) \\(z \\geq z_\\alpha\\) (right-tailed) \\(z \\leq -z_\\alpha\\) (left-tailed).\ntwo-sided test, reject \\(H_0\\) \\(|z| \\geq z_{\\alpha/2}\\).one-sided test, reject \\(H_0\\) \\(z \\geq z_\\alpha\\) (right-tailed) \\(z \\leq -z_\\alpha\\) (left-tailed).Alternatively, calculate p-value reject \\(H_0\\) p-value \\(\\leq \\alpha\\).Suppose dataset want test whether proportion observations 50th percentile (median) matches expected value \\(p = 0.5\\).one-sided test (e.g., testing whether proportion greater \\(p\\)):Interpretation Resultsp-value: p-value less \\(\\alpha\\), reject \\(H_0\\) conclude proportion observations quantile deviates significantly \\(p\\).p-value: p-value less \\(\\alpha\\), reject \\(H_0\\) conclude proportion observations quantile deviates significantly \\(p\\).Quantile Test Statistic (\\(z\\)): \\(z\\)-value indicates many standard deviations observed count expected count null hypothesis. Large positive negative \\(z\\) values suggest non-random deviations.Quantile Test Statistic (\\(z\\)): \\(z\\)-value indicates many standard deviations observed count expected count null hypothesis. Large positive negative \\(z\\) values suggest non-random deviations.Assumptions TestObservations independent.Observations independent.sample size large enough normal approximation binomial distribution valid (\\(n \\cdot p \\geq 5\\) \\(n \\cdot (1 - p) \\geq 5\\)).sample size large enough normal approximation binomial distribution valid (\\(n \\cdot p \\geq 5\\) \\(n \\cdot (1 - p) \\geq 5\\)).Limitations TestFor small sample sizes, normal approximation may hold. cases, exact binomial tests appropriate.small sample sizes, normal approximation may hold. cases, exact binomial tests appropriate.test assumes quantile used (e.g., median) well-defined correctly calculated data.test assumes quantile used (e.g., median) well-defined correctly calculated data.","code":"\n# Example data\ndata <- c(12, 15, 14, 10, 13, 11, 14, 16, 15, 13)\n\n# Define the quantile to test\nquantile_value <- quantile(data, 0.5) # Median\np <- 0.5                             # Proportion under H0\n\n# Count observed values below or equal to the quantile\nk <- sum(data <= quantile_value)\n\n# Sample size\nn <- length(data)\n\n# Expected count under H0\nexpected_count <- n * p\n\n# Variance\nvariance <- n * p * (1 - p)\n\n# Test statistic (z-value)\nz <- (k - expected_count) / sqrt(variance)\n\n# Calculate p-value for two-sided test\np_value <- 2 * (1 - pnorm(abs(z)))\n\n# Output results\nlist(\n  quantile_value = quantile_value,\n  observed_count = k,\n  expected_count = expected_count,\n  z_value = z,\n  p_value = p_value\n)\n#> $quantile_value\n#>  50% \n#> 13.5 \n#> \n#> $observed_count\n#> [1] 5\n#> \n#> $expected_count\n#> [1] 5\n#> \n#> $z_value\n#> [1] 0\n#> \n#> $p_value\n#> [1] 1\n# Calculate one-sided p-value\np_value_one_sided <- 1 - pnorm(z)\n\n# Output one-sided p-value\np_value_one_sided\n#> [1] 0.5"},{"path":"basic-statistical-inference.html","id":"two-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.4 Two-Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-means","chapter":"4 Basic Statistical Inference","heading":"4.4.1 For Means","text":"Suppose two sets observations:\\(y_1, \\dots, y_{n_y}\\)\\(x_1, \\dots, x_{n_x}\\)random samples two independent populations means \\(\\mu_y\\) \\(\\mu_x\\) variances \\(\\sigma_y^2\\) \\(\\sigma_x^2\\). goal compare \\(\\mu_y\\) \\(\\mu_x\\) test whether \\(\\sigma_y^2 = \\sigma_x^2\\).","code":""},{"path":"basic-statistical-inference.html","id":"large-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.4.1.1 Large Sample Tests","text":"\\(n_y\\) \\(n_x\\) large (\\(\\geq 30\\)), Central Limit Theorem allows us make following assumptions:Expectation: \\[\nE(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x\n\\]Variance: \\[\n\\text{Var}(\\bar{y} - \\bar{x}) = \\frac{\\sigma_y^2}{n_y} + \\frac{\\sigma_x^2}{n_x}\n\\]test statistic :\\[\nZ = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\frac{\\sigma_y^2}{n_y} + \\frac{\\sigma_x^2}{n_x}}} \\sim N(0,1)\n\\]large samples, replace variances unbiased estimators \\(s_y^2\\) \\(s_x^2\\), yielding large sample distribution.Confidence IntervalAn approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) :\\[\n\\bar{y} - \\bar{x} \\pm z_{\\alpha/2} \\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}\n\\]Hypothesis TestTesting:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]test statistic:\\[\nz = \\frac{\\bar{y} - \\bar{x} - \\delta_0}{\\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}}\n\\]Reject \\(H_0\\) \\(\\alpha\\)-level :\\[\n|z| > z_{\\alpha/2}\n\\]\\(\\delta_0 = 0\\), tests whether two means equal.","code":"\n# Large sample test\ny <- c(10, 12, 14, 16, 18)\nx <- c(9, 11, 13, 15, 17)\n\n# Mean and variance\nmean_y <- mean(y)\nmean_x <- mean(x)\nvar_y <- var(y)\nvar_x <- var(x)\nn_y <- length(y)\nn_x <- length(x)\n\n# Test statistic\nz <- (mean_y - mean_x) / sqrt(var_y / n_y + var_x / n_x)\np_value <- 2 * (1 - pnorm(abs(z)))\n\nlist(z = z, p_value = p_value)\n#> $z\n#> [1] 0.5\n#> \n#> $p_value\n#> [1] 0.6170751"},{"path":"basic-statistical-inference.html","id":"small-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2 Small Sample Tests","text":"samples small, assume data come independent normal distributions:\\(y_i \\sim N(\\mu_y, \\sigma_y^2)\\)\\(y_i \\sim N(\\mu_y, \\sigma_y^2)\\)\\(x_i \\sim N(\\mu_x, \\sigma_x^2)\\)\\(x_i \\sim N(\\mu_x, \\sigma_x^2)\\)can inference based Student’s T Distribution, 2 cases:Equal VariancesEqual VariancesUnequal VariancesUnequal VariancesF-TestLevene’s TestModified Levene Test (Brown-Forsythe Test)Bartlett’s TestBoxplots overlayed meansResiduals spread plots","code":""},{"path":"basic-statistical-inference.html","id":"equal-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2.1 Equal Variances","text":"AssumptionsIndependence Identically Distributed (..d.) ObservationsAssume observations sample ..d., implies:\\[\nvar(\\bar{y}) = \\frac{\\sigma^2_y}{n_y}, \\quad var(\\bar{x}) = \\frac{\\sigma^2_x}{n_x}\n\\]Independence SamplesThe samples assumed independent, meaning observation one sample influences observations . independence allows us write:\\[\n\\begin{aligned}\nvar(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var(\\bar{x}) - 2cov(\\bar{y}, \\bar{x}) \\\\\n&= var(\\bar{y}) + var(\\bar{x}) \\\\\n&= \\frac{\\sigma^2_y}{n_y} + \\frac{\\sigma^2_x}{n_x}\n\\end{aligned}\n\\]calculation assumes \\(cov(\\bar{y}, \\bar{x}) = 0\\) due independence samples.Normality AssumptionWe assume underlying populations normally distributed. assumption justifies use Student’s T Distribution, critical hypothesis testing constructing confidence intervals.Equality VariancesIf population variances equal, .e., \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\), \\(s^2_y\\) \\(s^2_x\\) unbiased estimators \\(\\sigma^2\\). allows us pool variances.pooled variance estimator calculated :\\[\ns^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y - 1) + (n_x - 1)}\n\\]pooled variance estimate degrees freedom equal :\\[\ndf = (n_y + n_x - 2)\n\\]Test StatisticThe test statistic : \\[\nT = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}} \\sim t_{n_y + n_x - 2}\n\\]Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) : \\[\n\\bar{y} - \\bar{x} \\pm t_{n_y + n_x - 2, \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}\n\\]Hypothesis TestTesting: \\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]Reject \\(H_0\\) : \\[\n|T| > t_{n_y + n_x - 2, \\alpha/2}\n\\]","code":"\n# Small sample test with equal variance\nt_test_equal <- t.test(y, x, var.equal = TRUE)\nt_test_equal\n#> \n#>  Two Sample t-test\n#> \n#> data:  y and x\n#> t = 0.5, df = 8, p-value = 0.6305\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -3.612008  5.612008\n#> sample estimates:\n#> mean of x mean of y \n#>        14        13"},{"path":"basic-statistical-inference.html","id":"unequal-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.1.2.2 Unequal Variances","text":"AssumptionsIndependence Identically Distributed (..d.) ObservationsAssume observations sample ..d., implies:\\[ var(\\bar{y}) = \\frac{\\sigma^2_y}{n_y}, \\quad var(\\bar{x}) = \\frac{\\sigma^2_x}{n_x} \\]Independence SamplesThe samples assumed independent, meaning observation one sample influences observations . independence allows us write:\\[ \\begin{aligned} var(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var(\\bar{x}) - 2cov(\\bar{y}, \\bar{x}) \\\\ &= var(\\bar{y}) + var(\\bar{x}) \\\\ &= \\frac{\\sigma^2_y}{n_y} + \\frac{\\sigma^2_x}{n_x} \\end{aligned} \\]calculation assumes \\(cov(\\bar{y}, \\bar{x}) = 0\\) due independence samples.Normality AssumptionWe assume underlying populations normally distributed. assumption justifies use Student’s T Distribution, critical hypothesis testing constructing confidence intervals.Unequal Variances\\(\\sigma_y^2 \\neq \\sigma_x^2\\)Test StatisticThe test statistic :\\[\nT = \\frac{\\bar{y} - \\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}}\n\\]Degrees Freedom (Welch-Satterthwaite Approximation) (Satterthwaite 1946)degrees freedom approximated :\\[\nv = \\frac{\\left(\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}\\right)^2}{\\frac{\\left(\\frac{s_y^2}{n_y}\\right)^2}{n_y - 1} + \\frac{\\left(\\frac{s_x^2}{n_x}\\right)^2}{n_x - 1}}\n\\]Since \\(v\\) fractional, truncate nearest integer.Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_y - \\mu_x\\) :\\[\n\\bar{y} - \\bar{x} \\pm t_{v, \\alpha/2} \\sqrt{\\frac{s_y^2}{n_y} + \\frac{s_x^2}{n_x}}\n\\]Hypothesis TestTesting:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x \\neq \\delta_0\n\\]Reject \\(H_0\\) :\\[\n|T| > t_{v, \\alpha/2}\n\\]\\[\nt = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}}\n\\]","code":"\n# Small sample test with unequal variance\nt_test_unequal <- t.test(y, x, var.equal = FALSE)\nt_test_unequal\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  y and x\n#> t = 0.5, df = 8, p-value = 0.6305\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -3.612008  5.612008\n#> sample estimates:\n#> mean of x mean of y \n#>        14        13"},{"path":"basic-statistical-inference.html","id":"for-variances","chapter":"4 Basic Statistical Inference","heading":"4.4.2 For Variances","text":"compare variances two independent samples, can use F-test. test statistic defined :\\[\nF_{ndf,ddf} = \\frac{s_1^2}{s_2^2}\n\\]\\(s_1^2 > s_2^2\\), \\(ndf = n_1 - 1\\), \\(ddf = n_2 - 1\\) numerator denominator degrees freedom, respectively.","code":""},{"path":"basic-statistical-inference.html","id":"f-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.1 F-Test","text":"hypotheses F-test :\\[\nH_0: \\sigma_y^2 = \\sigma_x^2 \\quad \\text{(equal variances)} \\\\\nH_a: \\sigma_y^2 \\neq \\sigma_x^2 \\quad \\text{(unequal variances)}\n\\]test statistic :\\[\nF = \\frac{s_y^2}{s_x^2}\n\\]\\(s_y^2\\) \\(s_x^2\\) sample variances two groups.Decision RuleReject \\(H_0\\) :\\(F > F_{n_y-1, n_x-1, \\alpha/2}\\) (upper critical value), \\(F > F_{n_y-1, n_x-1, \\alpha/2}\\) (upper critical value), \\(F < F_{n_y-1, n_x-1, 1-\\alpha/2}\\) (lower critical value).\\(F < F_{n_y-1, n_x-1, 1-\\alpha/2}\\) (lower critical value).:\\(F_{n_y-1, n_x-1, \\alpha/2}\\) \\(F_{n_y-1, n_x-1, 1-\\alpha/2}\\) critical points F-distribution, \\(n_y - 1\\) \\(n_x - 1\\) degrees freedom.AssumptionsThe F-test requires data groups follow normal distribution.F-test sensitive deviations normality (e.g., heavy-tailed distributions). normality assumption violated, may lead inflated Type error rate (false positives).Limitations AlternativesSensitivity Non-Normality:\ndata long-tailed distributions (positive kurtosis), F-test may produce misleading results.\nassess normality, see Normality Assessment.\ndata long-tailed distributions (positive kurtosis), F-test may produce misleading results.assess normality, see Normality Assessment.Nonparametric Alternatives:\nnormality assumption met, use robust tests Modified Levene Test (Brown-Forsythe Test), compares group variances based medians instead means.\nnormality assumption met, use robust tests Modified Levene Test (Brown-Forsythe Test), compares group variances based medians instead means.","code":"\n# Load iris dataset\ndata(iris)\n\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform F-test\nf_test <- var.test(irisVe, irisVi)\n\n# Display results\nf_test\n#> \n#>  F test to compare two variances\n#> \n#> data:  irisVe and irisVi\n#> F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\n#> alternative hypothesis: true ratio of variances is not equal to 1\n#> 95 percent confidence interval:\n#>  0.2941935 0.9135614\n#> sample estimates:\n#> ratio of variances \n#>          0.5184243"},{"path":"basic-statistical-inference.html","id":"levenes-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.2 Levene’s Test","text":"Levene’s Test robust method testing equality variances across multiple groups. Unlike F-test, less sensitive departures normality particularly useful handling non-normal distributions datasets outliers. test works analyzing deviations individual observations group mean median.Test ProcedureCompute absolute deviations observation group mean median:\ngroup \\(y\\): \\[\nd_{y,} = |y_i - \\text{Central Value}_y|\n\\]\ngroup \\(x\\): \\[\nd_{x,j} = |x_j - \\text{Central Value}_x|\n\\]\n“central value” can either mean (classic Levene’s test) median (Modified Levene Test (Brown-Forsythe Test) variation, robust non-normal data).\ngroup \\(y\\): \\[\nd_{y,} = |y_i - \\text{Central Value}_y|\n\\]group \\(x\\): \\[\nd_{x,j} = |x_j - \\text{Central Value}_x|\n\\]“central value” can either mean (classic Levene’s test) median (Modified Levene Test (Brown-Forsythe Test) variation, robust non-normal data).Perform one-way ANOVA absolute deviations test differences group variances.HypothesesNull Hypothesis (\\(H_0\\)): groups equal variances.Alternative Hypothesis (\\(H_a\\)): least one group variance different others.Test StatisticThe Levene test statistic calculated ANOVA absolute deviations. Let:\\(k\\): Number groups,\\(k\\): Number groups,\\(n_i\\): Number observations group \\(\\),\\(n_i\\): Number observations group \\(\\),\\(n\\): Total number observations.\\(n\\): Total number observations.test statistic :\\[\nW = \\frac{(n - k) \\sum_{=1}^k n_i (\\bar{d}_i - \\bar{d})^2}{(k - 1) \\sum_{=1}^k \\sum_{j=1}^{n_i} (d_{,j} - \\bar{d}_i)^2}\n\\]:\\(d_{,j}\\): Absolute deviations within group \\(\\),\\(d_{,j}\\): Absolute deviations within group \\(\\),\\(\\bar{d}_i\\): Mean absolute deviations group \\(\\),\\(\\bar{d}_i\\): Mean absolute deviations group \\(\\),\\(\\bar{d}\\): Overall mean absolute deviations.\\(\\bar{d}\\): Overall mean absolute deviations.null hypothesis, \\(W \\sim F_{k-1, n - k}\\).Decision RuleCompute test statistic \\(W\\).Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nW > F_{k-1, n-k, \\alpha}\n\\]output includes:Df: Degrees freedom numerator denominator.Df: Degrees freedom numerator denominator.F-value: computed value test statistic \\(W\\).F-value: computed value test statistic \\(W\\).p-value: probability observing value null hypothesis.p-value: probability observing value null hypothesis.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude group variances significantly different.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude group variances significantly different.Otherwise, fail reject \\(H_0\\) conclude evidence difference variances.Otherwise, fail reject \\(H_0\\) conclude evidence difference variances.Advantages Levene’s TestRobustness:\nHandles non-normal data outliers better F-test.\nRobustness:Handles non-normal data outliers better F-test.Flexibility:\nchoosing center value (mean median), can adapt different data characteristics:\nUse mean symmetric distributions.\nUse median non-normal skewed data.\n\nFlexibility:choosing center value (mean median), can adapt different data characteristics:\nUse mean symmetric distributions.\nUse median non-normal skewed data.\nchoosing center value (mean median), can adapt different data characteristics:Use mean symmetric distributions.Use mean symmetric distributions.Use median non-normal skewed data.Use median non-normal skewed data.Versatility:\nApplicable comparing variances across two groups, unlike Modified Levene Test (Brown-Forsythe Test), limited two groups.\nVersatility:Applicable comparing variances across two groups, unlike Modified Levene Test (Brown-Forsythe Test), limited two groups.","code":"\n# Load required package\nlibrary(car)\n\n# Perform Levene's Test (absolute deviations from the mean)\nlevene_test_mean <- leveneTest(Petal.Width ~ Species, data = iris)\n\n# Perform Levene's Test (absolute deviations from the median)\nlevene_test_median <-\n    leveneTest(Petal.Width ~ Species, data = iris, center = median)\n\n# Display results\nlevene_test_mean\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value    Pr(>F)    \n#> group   2  19.892 2.261e-08 ***\n#>       147                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlevene_test_median\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value    Pr(>F)    \n#> group   2  19.892 2.261e-08 ***\n#>       147                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"basic-statistical-inference.html","id":"modified-levene-test-brown-forsythe-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.3 Modified Levene Test (Brown-Forsythe Test)","text":"Modified Levene Test robust alternative F-test comparing variances two groups. Instead using squared deviations (F-test), test considers absolute deviations median, making less sensitive non-normal data long-tailed distributions. , however, still appropriate normally distributed data.sample, compute absolute deviations median:\\[\nd_{y,} = |y_i - y_{.5}| \\quad \\text{} \\quad d_{x,} = |x_i - x_{.5}|\n\\]Let:\\(\\bar{d}_y\\) \\(\\bar{d}_x\\) means absolute deviations groups \\(y\\) \\(x\\), respectively.test statistic :\\[\nt_L^* = \\frac{\\bar{d}_y - \\bar{d}_x}{s \\sqrt{\\frac{1}{n_y} + \\frac{1}{n_x}}}\n\\]pooled variance \\(s^2\\) :\\[\ns^2 = \\frac{\\sum_{=1}^{n_y} (d_{y,} - \\bar{d}_y)^2 + \\sum_{j=1}^{n_x} (d_{x,j} - \\bar{d}_x)^2}{n_y + n_x - 2}\n\\]AssumptionsConstant Variance Error Terms:\ntest assumes equal error variances group null hypothesis.Constant Variance Error Terms:\ntest assumes equal error variances group null hypothesis.Moderate Sample Size:\napproximation \\(t_L^* \\sim t_{n_y + n_x - 2}\\) holds well moderate large sample sizes.Moderate Sample Size:\napproximation \\(t_L^* \\sim t_{n_y + n_x - 2}\\) holds well moderate large sample sizes.Decision RuleCompute \\(t_L^*\\) using formula .Reject null hypothesis equal variances : \\[\n|t_L^*| > t_{n_y + n_x - 2; \\alpha/2}\n\\]equivalent applying two-sample t-test absolute deviations.small sample sizes, use unequal variance t-test directly original data robust alternative:","code":"\n# Absolute deviations from the median\ndVe <- abs(irisVe - median(irisVe))\ndVi <- abs(irisVi - median(irisVi))\n\n# Perform t-test on absolute deviations\nlevene_test <- t.test(dVe, dVi, var.equal = TRUE)\n\n# Display results\nlevene_test\n#> \n#>  Two Sample t-test\n#> \n#> data:  dVe and dVi\n#> t = -2.5584, df = 98, p-value = 0.01205\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.12784786 -0.01615214\n#> sample estimates:\n#> mean of x mean of y \n#>     0.154     0.226\n# Small sample t-test with unequal variances\nsmall_sample_test <- t.test(irisVe, irisVi, var.equal = FALSE)\n\n# Display results\nsmall_sample_test\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  irisVe and irisVi\n#> t = -14.625, df = 89.043, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.7951002 -0.6048998\n#> sample estimates:\n#> mean of x mean of y \n#>     1.326     2.026"},{"path":"basic-statistical-inference.html","id":"bartletts-test","chapter":"4 Basic Statistical Inference","heading":"4.4.2.4 Bartlett’s Test","text":"Bartlett’s Test statistical procedure testing equality variances across multiple groups. assumes data group normally distributed sensitive deviations normality. assumption normality holds, Bartlett’s Test powerful Levene’s Test.Hypotheses Bartlett’s TestNull Hypothesis (\\(H_0\\)): groups equal variances.Alternative Hypothesis (\\(H_a\\)): least one group variance different others.test statistic Bartlett’s Test :\\[\nB = \\frac{(n - k) \\log(S_p^2) - \\sum_{=1}^k (n_i - 1) \\log(S_i^2)}{1 + \\frac{1}{3(k - 1)} \\left( \\sum_{=1}^k \\frac{1}{n_i - 1} - \\frac{1}{n - k} \\right)}\n\\]:\\(k\\): Number groups,\\(k\\): Number groups,\\(n_i\\): Number observations group \\(\\),\\(n_i\\): Number observations group \\(\\),\\(n = \\sum_{=1}^k n_i\\): Total number observations,\\(n = \\sum_{=1}^k n_i\\): Total number observations,\\(S_i^2\\): Sample variance group \\(\\),\\(S_i^2\\): Sample variance group \\(\\),\\(S_p^2\\): Pooled variance, given : \\[\n  S_p^2 = \\frac{\\sum_{=1}^k (n_i - 1) S_i^2}{n - k}\n  \\]\\(S_p^2\\): Pooled variance, given : \\[\n  S_p^2 = \\frac{\\sum_{=1}^k (n_i - 1) S_i^2}{n - k}\n  \\]null hypothesis, test statistic \\(B \\sim \\chi^2_{k - 1}\\).AssumptionsNormality: data group must follow normal distribution.Independence: Observations within groups must independent.Equal Sample Sizes (Optional): Bartlett’s Test robust sample sizes approximately equal.Decision RuleCompute test statistic \\(B\\).Compare \\(B\\) critical value Chi-Square distribution \\(\\alpha\\) \\(k - 1\\) degrees freedom.Reject \\(H_0\\) : \\[\nB > \\chi^2_{k-1, \\alpha}\n\\]Alternatively, use p-value:Reject \\(H_0\\) p-value \\(\\leq \\alpha\\).output includes:Bartlett’s K-squared: value test statistic \\(B\\).Bartlett’s K-squared: value test statistic \\(B\\).df: Degrees freedom (\\(k - 1\\)), \\(k\\) number groups.df: Degrees freedom (\\(k - 1\\)), \\(k\\) number groups.p-value: probability observing value \\(B\\) \\(H_0\\).p-value: probability observing value \\(B\\) \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude variances significantly different across groups.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude variances significantly different across groups.p-value greater \\(\\alpha\\), fail reject \\(H_0\\) conclude significant evidence variance differences.p-value greater \\(\\alpha\\), fail reject \\(H_0\\) conclude significant evidence variance differences.Limitations Bartlett’s TestSensitivity Non-Normality:\nBartlett’s Test highly sensitive departures normality. Even slight deviations can lead misleading results.Sensitivity Non-Normality:\nBartlett’s Test highly sensitive departures normality. Even slight deviations can lead misleading results.Robust Outliers:\nOutliers can disproportionately affect test result.Robust Outliers:\nOutliers can disproportionately affect test result.Alternatives:\nnormality assumption violated, use robust alternatives like:\nLevene’s Test (absolute deviations)\nModified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)\nAlternatives:\nnormality assumption violated, use robust alternatives like:Levene’s Test (absolute deviations)Levene’s Test (absolute deviations)Modified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)Modified Levene Test (Brown-Forsythe Test) (median-based absolute deviations)Advantages Bartlett’s TestHigh Power: Bartlett’s Test powerful robust alternatives normality assumption holds.High Power: Bartlett’s Test powerful robust alternatives normality assumption holds.Simple Implementation: test easy perform interpret.Simple Implementation: test easy perform interpret.","code":"\n# Perform Bartlett's Test\nbartlett_test <- bartlett.test(Petal.Width ~ Species, data = iris)\n\n# Display results\nbartlett_test\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  Petal.Width by Species\n#> Bartlett's K-squared = 39.213, df = 2, p-value = 3.055e-09"},{"path":"basic-statistical-inference.html","id":"power","chapter":"4 Basic Statistical Inference","heading":"4.4.3 Power","text":"evaluate power test, consider situation variances equal across groups:\\[\n\\sigma_y^2 = \\sigma_x^2 = \\sigma^2\n\\]assumption equal variances, take equal sample sizes groups, .e., \\(n_y = n_x = n\\).Hypotheses One-Sided TestingWe testing:\\[\nH_0: \\mu_y - \\mu_x \\leq 0 \\quad \\text{vs.} \\quad H_a: \\mu_y - \\mu_x > 0\n\\]Test StatisticThe \\(\\alpha\\)-level z-test rejects \\(H_0\\) test statistic:\\[\nz = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{\\frac{2}{n}}} > z_\\alpha\n\\]:\\(\\bar{y}\\) \\(\\bar{x}\\) sample means,\\(\\bar{y}\\) \\(\\bar{x}\\) sample means,\\(\\sigma\\) common standard deviation,\\(\\sigma\\) common standard deviation,\\(z_\\alpha\\) critical value standard normal distribution.\\(z_\\alpha\\) critical value standard normal distribution.Power FunctionThe power test, denoted \\(\\pi(\\mu_y - \\mu_x)\\), probability correctly rejecting \\(H_0\\) \\(\\mu_y - \\mu_x\\) specified value. alternative hypothesis, power function :\\[\n\\pi(\\mu_y - \\mu_x) = \\Phi\\left(-z_\\alpha + \\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\right)\n\\]:\\(\\Phi\\) cumulative distribution function (CDF) standard normal distribution,\\(\\Phi\\) cumulative distribution function (CDF) standard normal distribution,\\(\\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\) represents standardized effect size.\\(\\frac{\\mu_y - \\mu_x}{\\sigma} \\sqrt{\\frac{n}{2}}\\) represents standardized effect size.Determining Required Sample SizeTo achieve desired power \\(1 - \\beta\\) true difference \\(\\delta\\) (smallest difference interest), solve required sample size \\(n\\). power equation :\\[\n\\Phi\\left(-z_\\alpha + \\frac{\\delta}{\\sigma} \\sqrt{\\frac{n}{2}}\\right) = 1 - \\beta\n\\]Rearranging \\(n\\), required sample size :\\[\nn = \\frac{2 \\sigma^2}{\\delta^2} \\left(z_\\alpha + z_\\beta\\right)^2\n\\]:\\(\\sigma\\): common standard deviation,\\(\\sigma\\): common standard deviation,\\(z_{\\alpha}\\): critical value Type error rate \\(\\alpha\\) (one-sided test),\\(z_{\\alpha}\\): critical value Type error rate \\(\\alpha\\) (one-sided test),\\(z_{\\beta}\\): critical value Type II error rate \\(\\beta\\) (related power \\(1 - \\beta\\)),\\(z_{\\beta}\\): critical value Type II error rate \\(\\beta\\) (related power \\(1 - \\beta\\)),\\(\\delta\\): minimum detectable difference means.\\(\\delta\\): minimum detectable difference means.Sample Size Two-Sided TestsFor two-sided test, replace \\(z_{\\alpha}\\) \\(z_{\\alpha/2}\\) account two-tailed critical region:\\[\nn = 2 \\left( \\frac{\\sigma (z_{\\alpha/2} + z_{\\beta})}{\\delta} \\right)^2\n\\]ensures test required power \\(1 - \\beta\\) detect difference size \\(\\delta\\) means significance level \\(\\alpha\\).Adjustment Exact t-TestWhen conducting exact two-sample t-test small sample sizes, sample size calculation involves non-central t-distribution. approximate correction can applied using critical values t-distribution instead z-distribution.adjusted sample size :\\[\nn^* = 2 \\left( \\frac{\\sigma (t_{2n-2; \\alpha/2} + t_{2n-2; \\beta})}{\\delta} \\right)^2\n\\]:\\(t_{2n-2; \\alpha/2}\\): critical value t-distribution \\(2n - 2\\) degrees freedom significance level \\(\\alpha/2\\),\\(t_{2n-2; \\alpha/2}\\): critical value t-distribution \\(2n - 2\\) degrees freedom significance level \\(\\alpha/2\\),\\(t_{2n-2; \\beta}\\): critical value t-distribution \\(2n - 2\\) degrees freedom power \\(1 - \\beta\\).\\(t_{2n-2; \\beta}\\): critical value t-distribution \\(2n - 2\\) degrees freedom power \\(1 - \\beta\\).correction adjusts increased variability t-distribution, especially important small sample sizes.Key InsightsZ-Test vs. T-Test:\nlarge samples, normal approximation (z-test) works well. small samples, t-test correction using t-distribution essential.Z-Test vs. T-Test:\nlarge samples, normal approximation (z-test) works well. small samples, t-test correction using t-distribution essential.Effect Power Significance Level:\nIncreasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.\nsmaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.\nEffect Power Significance Level:Increasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.Increasing power (\\(1 - \\beta\\)) decreasing \\(\\alpha\\) requires larger sample sizes.smaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.smaller minimum detectable difference (\\(\\delta\\)) also requires larger sample size.Two-Sided Tests:\nTwo-sided tests require larger sample sizes compared one-sided tests due split critical region.Two-Sided Tests:\nTwo-sided tests require larger sample sizes compared one-sided tests due split critical region.Formula Summary","code":"\n# Parameters\nalpha <- 0.05   # Significance level\nbeta <- 0.2     # Type II error rate (1 - Power = 0.2)\nsigma <- 1      # Common standard deviation\ndelta <- 0.5    # Minimum detectable difference\n\n# Critical values\nz_alpha <- qnorm(1 - alpha)\nz_beta <- qnorm(1 - beta)\n\n# Sample size calculation\nn <- (2 * sigma ^ 2 * (z_alpha + z_beta) ^ 2) / delta ^ 2\n\n# Output the required sample size (per group)\nceiling(n)\n#> [1] 50\n# Parameters\nalpha <- 0.05    # Significance level\npower <- 0.8     # Desired power\nsigma <- 1       # Common standard deviation\ndelta <- 0.5     # Minimum detectable difference\n\n# Calculate sample size for two-sided test\nsample_size <-\n    power.t.test(\n        delta = delta,\n        sd = sigma,\n        sig.level = alpha,\n        power = power,\n        type = \"two.sample\",\n        alternative = \"two.sided\"\n    )\n\n# Display results\nsample_size\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 63.76576\n#>           delta = 0.5\n#>              sd = 1\n#>       sig.level = 0.05\n#>           power = 0.8\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group"},{"path":"basic-statistical-inference.html","id":"matched-pair-designs","chapter":"4 Basic Statistical Inference","heading":"4.4.4 Matched Pair Designs","text":"matched pair designs, two treatments compared measuring responses subjects treatments. ensures effects subject--subject variability minimized, subject serves control.two treatments, data structured follows::\\(y_i\\) represents observation Treatment ,\\(y_i\\) represents observation Treatment ,\\(x_i\\) represents observation Treatment B,\\(x_i\\) represents observation Treatment B,\\(d_i = y_i - x_i\\) difference subject \\(\\).\\(d_i = y_i - x_i\\) difference subject \\(\\).AssumptionsObservations \\(y_i\\) \\(x_i\\) measured subjects, inducing correlation.differences \\(d_i\\) independent identically distributed (iid), follow normal distribution: \\[\nd_i \\sim N(\\mu_D, \\sigma_D^2)\n\\]Mean Variance DifferenceThe mean difference \\(\\mu_D\\) variance \\(\\sigma_D^2\\) given :\\[\n\\mu_D = E(y_i - x_i) = \\mu_y - \\mu_x\n\\]\\[\n\\sigma_D^2 = \\text{Var}(y_i - x_i) = \\text{Var}(y_i) + \\text{Var}(x_i) - 2 \\cdot \\text{Cov}(y_i, x_i)\n\\]covariance \\(y_i\\) \\(x_i\\) positive (typical case), variance differences \\(\\sigma_D^2\\) reduced compared independent sample case.key benefit Matched Pair Designs: reduced variability increases precision estimates.Sample StatisticsFor differences \\(d_i = y_i - x_i\\):sample mean differences: \\[\n\\bar{d} = \\frac{1}{n} \\sum_{=1}^n d_i = \\bar{y} - \\bar{x}\n\\]sample mean differences: \\[\n\\bar{d} = \\frac{1}{n} \\sum_{=1}^n d_i = \\bar{y} - \\bar{x}\n\\]sample variance differences: \\[\ns_d^2 = \\frac{1}{n-1} \\sum_{=1}^n (d_i - \\bar{d})^2\n\\]sample variance differences: \\[\ns_d^2 = \\frac{1}{n-1} \\sum_{=1}^n (d_i - \\bar{d})^2\n\\]data converted differences \\(d_i\\), problem reduces one-sample inference. can use tests confidence intervals (CIs) mean single sample.Hypothesis TestWe test following hypotheses:\\[\nH_0: \\mu_D = 0 \\quad \\text{vs.} \\quad H_a: \\mu_D \\neq 0\n\\]test statistic :\\[\nt = \\frac{\\bar{d}}{s_d / \\sqrt{n}} \\sim t_{n-1}\n\\]\\(n\\) number subjects.Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\n|t| > t_{n-1, \\alpha/2}\n\\]Confidence IntervalA \\(100(1 - \\alpha)\\%\\) confidence interval \\(\\mu_D\\) :\\[\n\\bar{d} \\pm t_{n-1, \\alpha/2} \\cdot \\frac{s_d}{\\sqrt{n}}\n\\]output includes:t-statistic: calculated test statistic matched pairs.t-statistic: calculated test statistic matched pairs.p-value: probability observing difference null hypothesis.p-value: probability observing difference null hypothesis.Confidence Interval: range plausible values mean difference \\(\\mu_D\\).Confidence Interval: range plausible values mean difference \\(\\mu_D\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant difference two treatments.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant difference two treatments.confidence interval include 0, supports conclusion significant difference.confidence interval include 0, supports conclusion significant difference.Key InsightsReduced Variability: Positive correlation paired observations reduces variance differences, increasing test power.Reduced Variability: Positive correlation paired observations reduces variance differences, increasing test power.Use Differences: paired design converts data single-sample problem inference.Use Differences: paired design converts data single-sample problem inference.Robustness: paired t-test assumes normality differences \\(d_i\\). larger \\(n\\), Central Limit Theorem ensures robustness non-normality.Robustness: paired t-test assumes normality differences \\(d_i\\). larger \\(n\\), Central Limit Theorem ensures robustness non-normality.Matched pair designs powerful way control subject-specific variability, leading precise comparisons treatments.","code":"\n# Sample data\ntreatment_a <- c(85, 90, 78, 92, 88)\ntreatment_b <- c(80, 86, 75, 89, 85)\n\n# Compute differences\ndifferences <- treatment_a - treatment_b\n\n# Perform one-sample t-test on the differences\nt_test <- t.test(differences, mu = 0, alternative = \"two.sided\")\n\n# Display results\nt_test\n#> \n#>  One Sample t-test\n#> \n#> data:  differences\n#> t = 9, df = 4, p-value = 0.0008438\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  2.489422 4.710578\n#> sample estimates:\n#> mean of x \n#>       3.6"},{"path":"basic-statistical-inference.html","id":"nonparametric-tests-for-two-samples","chapter":"4 Basic Statistical Inference","heading":"4.4.5 Nonparametric Tests for Two Samples","text":"Matched Pair Designs independent samples normality assumed, use nonparametric tests. tests assume specific distribution data robust alternatives parametric methods.Stochastic Order Location ShiftSuppose \\(Y\\) \\(X\\) random variables cumulative distribution functions (CDFs) \\(F_Y\\) \\(F_X\\). \\(Y\\) stochastically larger \\(X\\) , real numbers \\(u\\):\\[\nP(Y > u) \\geq P(X > u) \\quad \\text{(equivalently, } F_Y(u) \\leq F_X(u)).\n\\]two distributions differ location parameters, say \\(\\theta_y\\) \\(\\theta_x\\), can frame relationship :\\[\nY > X \\quad \\text{} \\quad \\theta_y > \\theta_x.\n\\]test following hypotheses:Two-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y \\neq F_X\n\\]Upper One-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y < F_X\n\\]Lower One-Sided Hypothesis: \\[\nH_0: F_Y = F_X \\quad \\text{vs.} \\quad H_a: F_Y > F_X\n\\]generally avoid completely non-directional alternative \\(H_a: F_Y \\neq F_X\\) allows arbitrary differences distributions, without requiring one distribution stochastically larger .Nonparametric TestsWhen focus whether two distributions differ location parameters, two equivalent nonparametric tests commonly used:Wilcoxon Signed Rank TestMann-Whitney U TestBoth tests mathematically equivalent test whether one sample systematically larger .","code":""},{"path":"basic-statistical-inference.html","id":"wilcoxon-rank-sum-test","chapter":"4 Basic Statistical Inference","heading":"4.4.5.1 Wilcoxon Rank-Sum Test","text":"Wilcoxon Rank Test nonparametric test used compare two independent samples assess whether distributions differ location. based ranks combined observations rather actual values.ProcedureCombine Rank Observations:\nCombine \\(n = n_y + n_x\\) observations (groups) single dataset rank ascending order. ties exist, assign average rank tied values.Combine Rank Observations:\nCombine \\(n = n_y + n_x\\) observations (groups) single dataset rank ascending order. ties exist, assign average rank tied values.Calculate Rank Sums:\nCompute sum ranks group:\n\\(w_y\\): Sum ranks group \\(y\\) (sample 1),\n\\(w_x\\): Sum ranks group \\(x\\) (sample 2).\ndefinition: \\[\nw_y + w_x = \\frac{n(n+1)}{2}\n\\]\nCalculate Rank Sums:\nCompute sum ranks group:\\(w_y\\): Sum ranks group \\(y\\) (sample 1),\\(w_x\\): Sum ranks group \\(x\\) (sample 2).\ndefinition: \\[\nw_y + w_x = \\frac{n(n+1)}{2}\n\\]Test Statistic:\ntest focuses rank sum \\(w_y\\). Reject \\(H_0\\) \\(w_y\\) large (indicating \\(y\\) systematically larger values) equivalently, \\(w_x\\) small.Test Statistic:\ntest focuses rank sum \\(w_y\\). Reject \\(H_0\\) \\(w_y\\) large (indicating \\(y\\) systematically larger values) equivalently, \\(w_x\\) small.Null Distribution:\n\\(H_0\\) (difference groups), possible arrangements ranks among \\(y\\) \\(x\\) equally likely. total number possible rank arrangements :\n\\[\n\\frac{(n_y + n_x)!}{n_y! \\, n_x!}\n\\]Null Distribution:\n\\(H_0\\) (difference groups), possible arrangements ranks among \\(y\\) \\(x\\) equally likely. total number possible rank arrangements :\\[\n\\frac{(n_y + n_x)!}{n_y! \\, n_x!}\n\\]Computational Considerations:\nsmall samples, exact null distribution rank sums can calculated.\nlarge samples, approximate normal distribution can used.\nComputational Considerations:small samples, exact null distribution rank sums can calculated.large samples, approximate normal distribution can used.HypothesesNull Hypothesis (\\(H_0\\)): two samples come identical distributions.Null Hypothesis (\\(H_0\\)): two samples come identical distributions.Alternative Hypothesis (\\(H_a\\)): two samples come different distributions, one distribution systematically larger.Alternative Hypothesis (\\(H_a\\)): two samples come different distributions, one distribution systematically larger.Two-Sided Test: \\[\nH_a: F_Y \\neq F_X\n\\]Two-Sided Test: \\[\nH_a: F_Y \\neq F_X\n\\]One-Sided Test: \\[\nH_a: F_Y > F_X \\quad \\text{} \\quad H_a: F_Y < F_X\n\\]One-Sided Test: \\[\nH_a: F_Y > F_X \\quad \\text{} \\quad H_a: F_Y < F_X\n\\]output wilcox.test includes:W: test statistic, smaller two rank sums.W: test statistic, smaller two rank sums.p-value: probability observing difference rank sums \\(H_0\\).p-value: probability observing difference rank sums \\(H_0\\).Alternative Hypothesis: Specifies whether test one-sided two-sided.Alternative Hypothesis: Specifies whether test one-sided two-sided.Confidence Interval (applicable): Provides range difference medians.Confidence Interval (applicable): Provides range difference medians.Decision RuleReject \\(H_0\\) significance level \\(\\alpha\\) p-value \\(\\leq \\alpha\\).Reject \\(H_0\\) significance level \\(\\alpha\\) p-value \\(\\leq \\alpha\\).large samples, compare test statistic critical value normal approximation.large samples, compare test statistic critical value normal approximation.Key FeaturesRobustness:\ntest require assumptions normality robust outliers.Robustness:\ntest require assumptions normality robust outliers.Distribution-Free:\nevaluates whether two samples differ location without assuming specific distribution.Distribution-Free:\nevaluates whether two samples differ location without assuming specific distribution.Rank-Based:\nuses ranks observations, makes scale-invariant (resistant data transformation).Rank-Based:\nuses ranks observations, makes scale-invariant (resistant data transformation).Computational ConsiderationsFor small sample sizes, exact distribution rank sums used.small sample sizes, exact distribution rank sums used.large sample sizes, normal approximation continuity correction applied computational efficiency.large sample sizes, normal approximation continuity correction applied computational efficiency.","code":"\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform Wilcoxon Rank Test (approximate version, large sample)\nwilcox_result <- wilcox.test(\n    irisVe,\n    irisVi,\n    alternative = \"two.sided\", # Two-sided test\n    conf.level = 0.95,         # Confidence level\n    exact = FALSE,             # Approximate test for large samples\n    correct = TRUE             # Apply continuity correction\n)\n\n# Display results\nwilcox_result\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  irisVe and irisVi\n#> W = 49, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"mann-whitney-u-test-1","chapter":"4 Basic Statistical Inference","heading":"4.4.5.2 Mann-Whitney U Test","text":"Mann-Whitney U Test nonparametric test used compare two independent samples. evaluates whether one sample tends produce larger observations , based pairwise comparisons. test assume normality robust outliers.ProcedurePairwise Comparisons:\nCompare observation \\(y_i\\) sample \\(Y\\) observation \\(x_j\\) sample \\(X\\).\nLet \\(u_y\\) number pairs \\(y_i > x_j\\).\nLet \\(u_x\\) number pairs \\(y_i < x_j\\).\ndefinition: \\[\nu_y + u_x = n_y n_x\n\\] \\(n_y\\) sample size group \\(Y\\), \\(n_x\\) sample size group \\(X\\).Pairwise Comparisons:\nCompare observation \\(y_i\\) sample \\(Y\\) observation \\(x_j\\) sample \\(X\\).Let \\(u_y\\) number pairs \\(y_i > x_j\\).Let \\(u_x\\) number pairs \\(y_i < x_j\\).definition: \\[\nu_y + u_x = n_y n_x\n\\] \\(n_y\\) sample size group \\(Y\\), \\(n_x\\) sample size group \\(X\\).Test Statistic:\nReject \\(H_0\\) \\(u_y\\) large (equivalently, \\(u_x\\) small).\nMann-Whitney U Test Wilcoxon Rank-Sum Test related rank sums:\n\\[\nu_y = w_y - \\frac{n_y (n_y + 1)}{2}, \\quad u_x = w_x - \\frac{n_x (n_x + 1)}{2}\n\\]\n, \\(w_y\\) \\(w_x\\) rank sums groups \\(Y\\) \\(X\\), respectively.Test Statistic:\nReject \\(H_0\\) \\(u_y\\) large (equivalently, \\(u_x\\) small).Mann-Whitney U Test Wilcoxon Rank-Sum Test related rank sums:\\[\nu_y = w_y - \\frac{n_y (n_y + 1)}{2}, \\quad u_x = w_x - \\frac{n_x (n_x + 1)}{2}\n\\], \\(w_y\\) \\(w_x\\) rank sums groups \\(Y\\) \\(X\\), respectively.HypothesesNull Hypothesis (\\(H_0\\)): two samples come identical distributions.Alternative Hypothesis (\\(H_a\\)):\nUpper One-Sided: \\(F_Y < F_X\\) (Sample \\(Y\\) stochastically larger).\nLower One-Sided: \\(F_Y > F_X\\) (Sample \\(X\\) stochastically larger).\nTwo-Sided: \\(F_Y \\neq F_X\\) (Distributions differ location).\nUpper One-Sided: \\(F_Y < F_X\\) (Sample \\(Y\\) stochastically larger).Lower One-Sided: \\(F_Y > F_X\\) (Sample \\(X\\) stochastically larger).Two-Sided: \\(F_Y \\neq F_X\\) (Distributions differ location).Test Statistic Large SamplesFor large sample sizes \\(n_y\\) \\(n_x\\), null distribution \\(U\\) can approximated normal distribution :Mean: \\[\nE(U) = \\frac{n_y n_x}{2}\n\\]Mean: \\[\nE(U) = \\frac{n_y n_x}{2}\n\\]Variance: \\[\n\\text{Var}(U) = \\frac{n_y n_x (n_y + n_x + 1)}{12}\n\\]Variance: \\[\n\\text{Var}(U) = \\frac{n_y n_x (n_y + n_x + 1)}{12}\n\\]standardized test statistic \\(z\\) :\\[\nz = \\frac{u_y - \\frac{n_y n_x}{2} - \\frac{1}{2}}{\\sqrt{\\frac{n_y n_x (n_y + n_x + 1)}{12}}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) :\\[\nz \\ge z_{\\alpha} \\quad \\text{(one-sided)} \\quad \\text{} \\quad |z| \\ge z_{\\alpha/2} \\quad \\text{(two-sided)}.\n\\]two-sided test, use:\\(u_{\\text{max}} = \\max(u_y, u_x)\\), \\(u_{\\text{max}} = \\max(u_y, u_x)\\), \\(u_{\\text{min}} = \\min(u_y, u_x)\\).\\(u_{\\text{min}} = \\min(u_y, u_x)\\).p-value given :\\[\np\\text{-value} = 2P(U \\ge u_{\\text{max}}) = 2P(U \\le u_{\\text{min}}).\n\\]\\(y_i = x_j\\) (ties), assign value \\(1/2\\) \\(u_y\\) \\(u_x\\) pair. exact sampling distribution differs slightly ties exist, large sample normal approximation remains reasonable.Decision RuleReject \\(H_0\\) p-value less \\(\\alpha\\).Reject \\(H_0\\) p-value less \\(\\alpha\\).large samples, check whether $z \\ge z_{\\alpha}$ (one-sided) $|z| \\ge z_{\\alpha/2}$ (two-sided).large samples, check whether $z \\ge z_{\\alpha}$ (one-sided) $|z| \\ge z_{\\alpha/2}$ (two-sided).Key InsightsRobustness: Mann-Whitney U Test assume normality robust outliers.Robustness: Mann-Whitney U Test assume normality robust outliers.Relationship Wilcoxon Test: test equivalent Wilcoxon Rank-Sum Test formulated differently (based pairwise comparisons).Relationship Wilcoxon Test: test equivalent Wilcoxon Rank-Sum Test formulated differently (based pairwise comparisons).Large Sample Approximation: large \\(n_y\\) \\(n_x\\), test statistic \\(U\\) follows approximate normal distribution, simplifying computation.Large Sample Approximation: large \\(n_y\\) \\(n_x\\), test statistic \\(U\\) follows approximate normal distribution, simplifying computation.Handling Ties: Ties accounted assigning fractional contributions \\(u_y\\) \\(u_x\\).Handling Ties: Ties accounted assigning fractional contributions \\(u_y\\) \\(u_x\\).","code":"\n# Subset data for two species\nirisVe <- iris$Petal.Width[iris$Species == \"versicolor\"]\nirisVi <- iris$Petal.Width[iris$Species == \"virginica\"]\n\n# Perform Mann-Whitney U Test\nmann_whitney <- wilcox.test(\n    irisVe, irisVi, \n    alternative = \"two.sided\", \n    conf.level = 0.95,\n    exact = FALSE,   # Approximate test for large samples\n    correct = TRUE   # Apply continuity correction\n)\n\n# Display results\nmann_whitney\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  irisVe and irisVi\n#> W = 49, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"categorical-data-analysis","chapter":"4 Basic Statistical Inference","heading":"4.5 Categorical Data Analysis","text":"Categorical Data Analysis used outcome variables categorical.Nominal Variables: Categories logical order (e.g., sex: male, female).Ordinal Variables: Categories logical order, relative distances values well defined (e.g., small, medium, large).categorical data, often analyze distribution one variable changes levels another variable. example, row percentages may differ across columns contingency table.","code":""},{"path":"basic-statistical-inference.html","id":"association-tests","chapter":"4 Basic Statistical Inference","heading":"4.5.1 Association Tests","text":"","code":""},{"path":"basic-statistical-inference.html","id":"small-samples","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1 Small Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"fishers-exact-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1.1 Fisher’s Exact Test","text":"small samples, approximate tests based asymptotic normality \\(\\hat{p}_1 - \\hat{p}_2\\) (difference proportions) hold. cases, use Fisher’s Exact Test evaluate:Null Hypothesis (\\(H_0\\)): \\(p_1 = p_2\\) (association variables),Alternative Hypothesis (\\(H_a\\)): \\(p_1 \\neq p_2\\) (association exists).Assumptions\\(X_1\\) \\(X_2\\) independent Binomial random variables:\n\\(X_1 \\sim \\text{Binomial}(n_1, p_1)\\),\n\\(X_2 \\sim \\text{Binomial}(n_2, p_2)\\).\n\\(X_1 \\sim \\text{Binomial}(n_1, p_1)\\),\\(X_2 \\sim \\text{Binomial}(n_2, p_2)\\).\\(x_1\\) \\(x_2\\) observed values (successes sample).Total sample size \\(n = n_1 + n_2\\).Total successes \\(m = x_1 + x_2\\).conditioning \\(m\\), total number successes, number successes sample 1 follows Hypergeometric distribution.Test StatisticTo test \\(H_0: p_1 = p_2\\) \\(H_a: p_1 \\neq p_2\\), use test statistic:\\[\nZ^2 = \\left( \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}} \\right)^2 \\sim \\chi^2_{1, \\alpha}\n\\]:\\(\\hat{p}_1\\) \\(\\hat{p}_2\\) observed proportions successes samples 1 2,\\(\\hat{p}_1\\) \\(\\hat{p}_2\\) observed proportions successes samples 1 2,\\(\\hat{p}\\) pooled proportion: \\[\n  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2},\n  \\]\\(\\hat{p}\\) pooled proportion: \\[\n  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2},\n  \\]\\(\\chi^2_{1, \\alpha}\\) upper \\(\\alpha\\) critical value Chi-squared distribution 1 degree freedom.\\(\\chi^2_{1, \\alpha}\\) upper \\(\\alpha\\) critical value Chi-squared distribution 1 degree freedom.Fisher’s Exact Test can extended contingency table setting test whether observed frequencies differ significantly expected frequencies null hypothesis association.output fisher.test() includes:p-value: probability observing contingency table null hypothesis.p-value: probability observing contingency table null hypothesis.Alternative Hypothesis: Indicates whether test two-sided one-sided.Alternative Hypothesis: Indicates whether test two-sided one-sided.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association two variables.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association two variables.","code":"\n# Create a 2x2 contingency table\ndata_table <- matrix(c(8, 2, 1, 5), nrow = 2, byrow = TRUE)\ncolnames(data_table) <- c(\"Success\", \"Failure\")\nrownames(data_table) <- c(\"Group 1\", \"Group 2\")\n\n# Display the table\ndata_table\n#>         Success Failure\n#> Group 1       8       2\n#> Group 2       1       5\n\n# Perform Fisher's Exact Test\nfisher_result <- fisher.test(data_table)\n\n# Display the results\nfisher_result\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  data_table\n#> p-value = 0.03497\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>     1.008849 1049.791446\n#> sample estimates:\n#> odds ratio \n#>   15.46969"},{"path":"basic-statistical-inference.html","id":"exact-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.1.2 Exact Chi-Square Test","text":"small samples normal approximation apply, can compute exact Chi-Square test using Fisher’s Exact Test Monte Carlo simulation methods.Chi-Square test statistic 2x2 table :\\(\\chi^2 = \\sum_{=1}^r \\sum_{j=1}^c \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\):\\(O_{ij}\\): Observed frequency cell \\((, j)\\),\\(O_{ij}\\): Observed frequency cell \\((, j)\\),\\(E_{ij}\\): Expected frequency null hypothesis,\\(E_{ij}\\): Expected frequency null hypothesis,\\(r\\): Number rows,\\(r\\): Number rows,\\(c\\): Number columns.\\(c\\): Number columns.","code":""},{"path":"basic-statistical-inference.html","id":"large-samples","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2 Large Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"pearson-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2.1 Pearson Chi-Square Test","text":"Pearson Chi-Square Test commonly used test whether association two categorical variables. compares observed counts contingency table expected counts null hypothesis.test statistic :\\[\n\\chi^2 = \\sum_{\\text{cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]test applied settings multiple proportions frequencies compared across independent surveys experiments.Null Hypothesis (\\(H_0\\)): observed data consistent expected values (association deviation model).Alternative Hypothesis (\\(H_a\\)): observed data differ significantly expected values.Characteristics TestValidation Models:\ncases, \\(H_0\\) represents model whose validity tested. goal necessarily reject model check whether data consistent . Deviations may due random chance.Validation Models:\ncases, \\(H_0\\) represents model whose validity tested. goal necessarily reject model check whether data consistent . Deviations may due random chance.Strength Association:\nChi-Square Test detects whether association exists measure strength association. measuring strength, metrics like Cramér’s V Phi coefficient used.Strength Association:\nChi-Square Test detects whether association exists measure strength association. measuring strength, metrics like Cramér’s V Phi coefficient used.Effect Sample Size:\nChi-Square statistic reflects sample size. sample size doubled (e.g., duplicating observations), \\(\\chi^2\\) statistic also double, even though strength association remains unchanged.\nsensitivity can sometimes lead detecting significant results practically meaningful.\nEffect Sample Size:Chi-Square statistic reflects sample size. sample size doubled (e.g., duplicating observations), \\(\\chi^2\\) statistic also double, even though strength association remains unchanged.sensitivity can sometimes lead detecting significant results practically meaningful.Expected Cell Frequencies:\ntest appropriate 20% cells contingency table expected frequencies less 5.\nsmall sample sizes, Fisher’s Exact Test exact p-values used instead.\nExpected Cell Frequencies:test appropriate 20% cells contingency table expected frequencies less 5.small sample sizes, Fisher’s Exact Test exact p-values used instead.Test Single Proportion\ntest whether observed proportion successes equals 0.5.\\[\nH_0: p_J = 0.5 \\\\\nH_a: p_J < 0.5\n\\]Test Equality Proportions Two Groups: test whether proportions successes July September equal.\\[\nH_0: p_J = p_S \\\\\nH_a: p_j \\neq p_S\n\\]Comparison Proportions Multiple GroupsWe test null hypothesis:\\[\nH_0: p_1 = p_2 = \\dots = p_k\n\\]alternative least one proportion differs.Pooled ProportionAssuming \\(H_0\\) true, estimate common value probability success :\\[\n\\hat{p} = \\frac{x_1 + x_2 + \\dots + x_k}{n_1 + n_2 + \\dots + n_k}.\n\\]expected counts \\(H_0\\) :test statistic :\\[\n\\chi^2 = \\sum_{\\text{cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\\(k - 1\\) degrees freedom.Two-Way Contingency TablesWhen categorical data cross-classified, create two-way table observed counts.Sampling DesignsDesign 1: Total Sample Size Fixed\nsingle random sample size \\(n\\) drawn population.\nUnits cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.\ncell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]\nLet \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.\nNull Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]\nAlternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]\nDesign 1: Total Sample Size FixedA single random sample size \\(n\\) drawn population.single random sample size \\(n\\) drawn population.Units cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.Units cross-classified \\(r\\) rows \\(c\\) columns. row column totals random variables.cell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]cell counts \\(n_{ij}\\) follow multinomial distribution probabilities \\(p_{ij}\\) : \\[ \\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1. \\]Let \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.Let \\(p_{ij} = P(X = , Y = j)\\) joint probability, \\(X\\) row variable \\(Y\\) column variable.Null Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]Null Hypothesis Independence: \\[ H_0: p_{ij} = p_{.} p_{.j}, \\quad \\text{} p_{.} = P(X = ) \\text{ } p_{.j} = P(Y = j). \\]Alternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]Alternative Hypothesis: \\[ H_a: p_{ij} \\neq p_{.} p_{.j}. \\]Design 2: Row Totals Fixed\nRandom samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.\nrow totals \\(n_{.}\\) fixed, column totals random.\nCounts row follow independent multinomial distributions.\nnull hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]\nAlternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]\nAlternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]\nDesign 2: Row Totals FixedRandom samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.Random samples sizes \\(n_1, n_2, \\dots, n_r\\) drawn independently \\(r\\) row populations.row totals \\(n_{.}\\) fixed, column totals random.row totals \\(n_{.}\\) fixed, column totals random.Counts row follow independent multinomial distributions.Counts row follow independent multinomial distributions.null hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]null hypothesis assumes conditional probabilities column variable \\(Y\\) across rows: \\[ H_0: p_{ij} = P(Y = j | X = ) = p_j \\quad \\text{} \\text{ } j. \\]Alternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]Alternatively: \\[ H_0: (p_{i1}, p_{i2}, \\dots, p_{ic}) = (p_1, p_2, \\dots, p_c) \\quad \\text{} . \\]Alternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]Alternative Hypothesis: \\[ H_a: (p_{i1}, p_{i2}, \\dots, p_{ic}) \\text{ } . \\]Designs?Real-World Sampling Constraints:\nSometimes, control row totals (e.g., fixed group sizes stratified sampling).\ntimes, collect data without predefined group sizes, totals emerge randomly.\nReal-World Sampling Constraints:Sometimes, control row totals (e.g., fixed group sizes stratified sampling).Sometimes, control row totals (e.g., fixed group sizes stratified sampling).times, collect data without predefined group sizes, totals emerge randomly.times, collect data without predefined group sizes, totals emerge randomly.Different Null Hypotheses:\nDesign 1 tests whether two variables independent (e.g., one variable predict ?).\nDesign 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).\nDifferent Null Hypotheses:Design 1 tests whether two variables independent (e.g., one variable predict ?).Design 1 tests whether two variables independent (e.g., one variable predict ?).Design 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).Design 2 tests whether column proportions homogeneous across groups (e.g., groups similar?).counts contingency table come single multinomial sample row column totals random.counts contingency table come single multinomial sample row column totals random.Conclusion: Reject Null​. data suggests significant dependence row column variables.Conclusion: Reject Null​. data suggests significant dependence row column variables.Row totals fixed, column counts within row follow independent multinomial distributions.Row totals fixed, column counts within row follow independent multinomial distributions.Conclusion: Fail reject null. data provide evidence suggest differences column probabilities across rows.Conclusion: Fail reject null. data provide evidence suggest differences column probabilities across rows.Results Different?Data Generation Differences:\nDesign 1, entire table treated single multinomial sample. introduces dependencies counts table.\nDesign 2, rows generated independently, column probabilities tested consistency across rows.\nData Generation Differences:Design 1, entire table treated single multinomial sample. introduces dependencies counts table.Design 1, entire table treated single multinomial sample. introduces dependencies counts table.Design 2, rows generated independently, column probabilities tested consistency across rows.Design 2, rows generated independently, column probabilities tested consistency across rows.Null Hypotheses:\nDesign 1 tests independence row column variables (restrictive).\nDesign 2 tests homogeneity column probabilities across rows (less restrictive).\nNull Hypotheses:Design 1 tests independence row column variables (restrictive).Design 1 tests independence row column variables (restrictive).Design 2 tests homogeneity column probabilities across rows (less restrictive).Design 2 tests homogeneity column probabilities across rows (less restrictive).InterpretationThe results directly comparable null hypotheses different:\nDesign 1 focuses whether rows columns independent across entire table.\nDesign 2 focuses whether column distributions consistent across rows.\nresults directly comparable null hypotheses different:Design 1 focuses whether rows columns independent across entire table.Design 1 focuses whether rows columns independent across entire table.Design 2 focuses whether column distributions consistent across rows.Design 2 focuses whether column distributions consistent across rows.Real-World Implication:\ntesting independence (e.g., whether two variables unrelated), use Design 1.\ntesting consistency across groups (e.g., whether proportions across categories), use Design 2.\nReal-World Implication:testing independence (e.g., whether two variables unrelated), use Design 1.testing independence (e.g., whether two variables unrelated), use Design 1.testing consistency across groups (e.g., whether proportions across categories), use Design 2.testing consistency across groups (e.g., whether proportions across categories), use Design 2.TakeawaysThe tests use statistical machinery (Chi-squared test), interpretations differ based experimental design null hypothesis.tests use statistical machinery (Chi-squared test), interpretations differ based experimental design null hypothesis.dataset, differences assumptions can lead different conclusions.dataset, differences assumptions can lead different conclusions.","code":"\n# Observed data\njuly.x <- 480\njuly.n <- 1000\n# Test for single proportion\nprop.test(\n  x = july.x,\n  n = july.n,\n  p = 0.5,\n  alternative = \"less\",\n  correct = FALSE\n)\n#> \n#>  1-sample proportions test without continuity correction\n#> \n#> data:  july.x out of july.n, null probability 0.5\n#> X-squared = 1.6, df = 1, p-value = 0.103\n#> alternative hypothesis: true p is less than 0.5\n#> 95 percent confidence interval:\n#>  0.0000000 0.5060055\n#> sample estimates:\n#>    p \n#> 0.48\n# Observed data for two groups\nsept.x <- 704\nsept.n <- 1600\n# Test for equality of proportions\nprop.test(\n  x = c(july.x, sept.x),\n  n = c(july.n, sept.n),\n  correct = FALSE\n)\n#> \n#>  2-sample test for equality of proportions without continuity correction\n#> \n#> data:  c(july.x, sept.x) out of c(july.n, sept.n)\n#> X-squared = 3.9701, df = 1, p-value = 0.04632\n#> alternative hypothesis: two.sided\n#> 95 percent confidence interval:\n#>  0.0006247187 0.0793752813\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.48   0.44\n# Sampling Design 1: Total Sample Size Fixed\n# Parameters for the multinomial distribution\nr <- 3  # Number of rows\nc <- 4  # Number of columns\nn <- 100  # Total sample size\np <- matrix(c(0.1, 0.2, 0.1, 0.1,\n              0.05, 0.15, 0.05, 0.1,\n              0.05, 0.05, 0.025, 0.075), nrow = r, byrow = TRUE)\n\n# Generate a single random sample\nset.seed(123)  # For reproducibility\nn_ij <- rmultinom(1, size = n, prob = as.vector(p))\n\n# Reshape into a contingency table\ncontingency_table_fixed_total <- matrix(n_ij, nrow = r, ncol = c, byrow = TRUE)\nrownames(contingency_table_fixed_total) <- paste0(\"Row\", 1:r)\ncolnames(contingency_table_fixed_total) <- paste0(\"Col\", 1:c)\n\n# Hypothesis testing (Chi-squared test of independence)\nchisq_test_fixed_total <- chisq.test(contingency_table_fixed_total)\n\n# Display results\nprint(\"Contingency Table (Total Sample Size Fixed):\")\n#> [1] \"Contingency Table (Total Sample Size Fixed):\"\nprint(contingency_table_fixed_total)\n#>      Col1 Col2 Col3 Col4\n#> Row1    8    6    4   24\n#> Row2   18    1    9    7\n#> Row3    2    7    5    9\nprint(\"Chi-squared Test Results:\")\n#> [1] \"Chi-squared Test Results:\"\nprint(chisq_test_fixed_total)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  contingency_table_fixed_total\n#> X-squared = 28.271, df = 6, p-value = 8.355e-05\n# Sampling Design 2: Row Totals Fixed\n# Parameters for the fixed row totals\nn_row <- c(30, 40, 30)  # Row totals\nc <- 4  # Number of columns\np_col <- c(0.25, 0.25, 0.25, 0.25)  # Common column probabilities under H0\n\n# Generate independent multinomial samples for each row\nset.seed(123)  # For reproducibility\nrow_samples <- lapply(n_row, function(size) t(rmultinom(1, size, prob = p_col)))\n\n# Combine into a contingency table\ncontingency_table_fixed_rows <- do.call(rbind, row_samples)\nrownames(contingency_table_fixed_rows) <- paste0(\"Row\", 1:length(n_row))\ncolnames(contingency_table_fixed_rows) <- paste0(\"Col\", 1:c)\n\n# Hypothesis testing (Chi-squared test of homogeneity)\nchisq_test_fixed_rows <- chisq.test(contingency_table_fixed_rows)\n\n# Display results\nprint(\"Contingency Table (Row Totals Fixed):\")\n#> [1] \"Contingency Table (Row Totals Fixed):\"\nprint(contingency_table_fixed_rows)\n#>      Col1 Col2 Col3 Col4\n#> Row1    6   10    7    7\n#> Row2   13   13    4   10\n#> Row3    8   10    6    6\nprint(\"Chi-squared Test Results:\")\n#> [1] \"Chi-squared Test Results:\"\nprint(chisq_test_fixed_rows)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  contingency_table_fixed_rows\n#> X-squared = 3.2069, df = 6, p-value = 0.7825"},{"path":"basic-statistical-inference.html","id":"chi-square-test-for-independence","chapter":"4 Basic Statistical Inference","heading":"4.5.1.2.2 Chi-Square Test for Independence","text":"expected frequencies \\(\\hat{e}_{ij}\\) null hypothesis :\\[\n\\hat{e}_{ij} = \\frac{n_{.} n_{.j}}{n_{..}},\n\\]\\(n_{.}\\) \\(n_{.j}\\) row column totals, respectively, \\(n_{..}\\) total sample size.test statistic :\\[\n\\chi^2 = \\sum_{=1}^r \\sum_{j=1}^c \\frac{(n_{ij} - \\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi^2_{(r-1)(c-1)}.\n\\]reject \\(H_0\\) significance level \\(\\alpha\\) :\\[\n\\chi^2 > \\chi^2_{(r-1)(c-1), \\alpha}.\n\\]Notes Pearson Chi-Square TestPurpose: Test association independence two categorical variables.Sensitivity Sample Size: \\(\\chi^2\\) statistic proportional sample size. Doubling sample size doubles \\(\\chi^2\\) even strength association remains unchanged.Assumption Expected Frequencies: test valid 20% expected cell counts less 5. cases, exact tests preferred.output includes:Chi-Square Statistic (\\(\\chi^2\\)): test statistic measuring deviation observed expected counts.Chi-Square Statistic (\\(\\chi^2\\)): test statistic measuring deviation observed expected counts.p-value: probability observing deviation \\(H_0\\).p-value: probability observing deviation \\(H_0\\).Degrees Freedom: \\((r-1)(c-1)\\) \\(r \\times c\\) table.Degrees Freedom: \\((r-1)(c-1)\\) \\(r \\times c\\) table.Expected Frequencies: table expected counts \\(H_0\\).Expected Frequencies: table expected counts \\(H_0\\).p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association row column variables.p-value less \\(\\alpha\\), reject \\(H_0\\) conclude significant association row column variables.","code":"\n# Create a contingency table\ndata_table <- matrix(c(30, 10, 20, 40), nrow = 2, byrow = TRUE)\ncolnames(data_table) <- c(\"Category 1\", \"Category 2\")\nrownames(data_table) <- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nprint(data_table)\n#>         Category 1 Category 2\n#> Group 1         30         10\n#> Group 2         20         40\n\n# Perform Chi-Square Test\nchi_result <- chisq.test(data_table)\n\n# Display results\nchi_result\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  data_table\n#> X-squared = 15.042, df = 1, p-value = 0.0001052"},{"path":"basic-statistical-inference.html","id":"key-takeaways","chapter":"4 Basic Statistical Inference","heading":"4.5.1.3 Key Takeaways","text":"Fisher’s Exact Test specialized small samples fixed margins (2x2 tables).Fisher’s Exact Test specialized small samples fixed margins (2x2 tables).Exact Chi-Square Test broader version Fisher’s larger tables avoids asymptotic approximations.Exact Chi-Square Test broader version Fisher’s larger tables avoids asymptotic approximations.Pearson Chi-Square Test general framework, applications include:\nGoodness--fit testing.\nTesting independence (Chi-Square Test Independence).\nPearson Chi-Square Test general framework, applications include:Goodness--fit testing.Goodness--fit testing.Testing independence (Chi-Square Test Independence).Testing independence (Chi-Square Test Independence).Chi-Square Test Independence specific application Pearson Chi-Square Test.Chi-Square Test Independence specific application Pearson Chi-Square Test.essence:Fisher’s Exact Test Exact Chi-Square Test precise methods small datasets.Fisher’s Exact Test Exact Chi-Square Test precise methods small datasets.Pearson Chi-Square Test Chi-Square Test Independence interchangeable terms many contexts, focusing larger datasets.Pearson Chi-Square Test Chi-Square Test Independence interchangeable terms many contexts, focusing larger datasets.","code":""},{"path":"basic-statistical-inference.html","id":"ordinal-association","chapter":"4 Basic Statistical Inference","heading":"4.5.2 Ordinal Association","text":"Ordinal association refers relationship two variables levels one variable exhibit consistent pattern increase decrease response levels variable. type association particularly relevant dealing ordinal variables, naturally ordered categories, ratings (“poor”, “fair”, “good”, “excellent”) income brackets (“low”, “medium”, “high”).example:customer satisfaction ratings increase “poor” “excellent,” likelihood recommending product may also increase (positive ordinal association).customer satisfaction ratings increase “poor” “excellent,” likelihood recommending product may also increase (positive ordinal association).Alternatively, stress levels move “low” “high,” job performance may tend decrease (negative ordinal association).Alternatively, stress levels move “low” “high,” job performance may tend decrease (negative ordinal association).Key Characteristics Ordinal AssociationLogical Ordering Levels: levels variables must follow logical sequence. instance, “small,” “medium,” “large” logically ordered, whereas categories like “blue,” “round,” “tall” lack inherent order unsuitable ordinal association.Logical Ordering Levels: levels variables must follow logical sequence. instance, “small,” “medium,” “large” logically ordered, whereas categories like “blue,” “round,” “tall” lack inherent order unsuitable ordinal association.Monotonic Trends: association typically monotonic, meaning one variable moves specific direction, variable tends move consistent direction (either increasing decreasing).Monotonic Trends: association typically monotonic, meaning one variable moves specific direction, variable tends move consistent direction (either increasing decreasing).Tests Ordinal Association: Specialized statistical tests assess ordinal association, focusing rankings one variable relate . tests require data respect ordinal structure variables.Tests Ordinal Association: Specialized statistical tests assess ordinal association, focusing rankings one variable relate . tests require data respect ordinal structure variables.Practical ConsiderationsWhen using tests, keep mind:Ordinal Data Handling: Ensure data respects ordinal structure (e.g., categories correctly ranked coded).Ordinal Data Handling: Ensure data respects ordinal structure (e.g., categories correctly ranked coded).Sample Size: Larger sample sizes provide reliable estimates stronger test power.Sample Size: Larger sample sizes provide reliable estimates stronger test power.Contextual Relevance: Interpret results within context data research question. example, significant Spearman’s correlation imply causation rather consistent trend.Contextual Relevance: Interpret results within context data research question. example, significant Spearman’s correlation imply causation rather consistent trend.","code":""},{"path":"basic-statistical-inference.html","id":"mantel-haenszel-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.1 Mantel-Haenszel Chi-square Test","text":"Mantel-Haenszel Chi-square Test statistical tool evaluating ordinal associations, particularly data consists multiple \\(2 \\times 2\\) contingency tables examine association varying conditions strata. Unlike measures association correlation coefficients, test quantify strength association rather evaluates whether association exists controlling stratification.Mantel-Haenszel Test applicable \\(2 \\times 2 \\times K\\) contingency tables, \\(K\\) represents number strata. stratum \\(2 \\times 2\\) table corresponding different conditions subgroups.stratum \\(k\\), let marginal totals table :\\(n_{.1k}\\): Total observations column 1\\(n_{.1k}\\): Total observations column 1\\(n_{.2k}\\): Total observations column 2\\(n_{.2k}\\): Total observations column 2\\(n_{1.k}\\): Total observations row 1\\(n_{1.k}\\): Total observations row 1\\(n_{2.k}\\): Total observations row 2\\(n_{2.k}\\): Total observations row 2\\(n_{..k}\\): Total observations entire table\\(n_{..k}\\): Total observations entire tableThe observed cell count row 1 column 1 denoted \\(n_{11k}\\). Given marginal totals, sampling distribution \\(n_{11k}\\) follows hypergeometric distribution.assumption conditional independence:expected value \\(n_{11k}\\) : \\[\n  m_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}}\n  \\] variance \\(n_{11k}\\) : \\[\n  var(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2 (n_{..k} - 1)}\n  \\]Mantel Haenszel proposed test statistic:\\[\nM^2 = \\frac{\\left(|\\sum_k n_{11k} - \\sum_k m_{11k}| - 0.5\\right)^2}{\\sum_k var(n_{11k})} \\sim \\chi^2_{1}\n\\]whereThe 0.5 adjustment, known continuity correction, improves approximation \\(\\chi^2\\) distribution.0.5 adjustment, known continuity correction, improves approximation \\(\\chi^2\\) distribution.test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis conditional independence.test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis conditional independence.method can extended general \\(\\times J \\times K\\) contingency tables, \\(\\) \\(J\\) represent number rows columns, respectively, \\(K\\) number strata.Null Hypothesis (\\(H_0\\)):association two variables interest across strata, controlling confounder.\nmathematical terms:\\[\nH_0: \\text{Odds Ratio ()} = 1 \\; \\text{} \\; \\text{Risk Ratio (RR)} = 1\n\\]Alternative Hypothesis (\\(H_a\\)):association two variables interest across strata, controlling confounder.\nmathematical terms:\\[\nH_a: \\text{Odds Ratio ()} \\neq 1 \\; \\text{} \\; \\text{Risk Ratio (RR)} \\neq 1\n\\]Let’s consider scenario business wants evaluate relationship customer satisfaction (Satisfied vs. Satisfied) likelihood repeat purchases (Yes vs. ) across different regions (e.g., North, South, West). goal determine whether relationship holds consistently across regions.Calculate overall odds ratio (ignoring strata):Calculate conditional odds ratios region:Mantel-Haenszel Test evaluates whether relationship customer satisfaction repeat purchases remains consistent across regions:InterpretationOverall Odds Ratio: provides estimate overall association satisfaction repeat purchases, ignoring regional differences.Conditional Odds Ratios: show whether odds repeat purchases given satisfaction similar across regions.Mantel-Haenszel Test: significant test result (e.g., \\(p < 0.05\\)) suggests relationship satisfaction repeat purchases consistent across regions. Conversely, non-significant result implies regional differences may affect association. applying Mantel-Haenszel Test, businesses can determine marketing customer retention strategy uniformly applied customized account regional variations.\nstrong evidence suggest two variables interest associated across strata (North, South, West), even accounting potential confounding effects stratification.\ncommon odds ratio approximately \\(2.22\\) indicates substantial association, meaning outcome likely exposed group compared unexposed group.\nvariability stratum-specific odds ratios suggests strength association may differ slightly region, Mantel-Haenszel test assumes association consistent (homogeneous).\nstrong evidence suggest two variables interest associated across strata (North, South, West), even accounting potential confounding effects stratification.common odds ratio approximately \\(2.22\\) indicates substantial association, meaning outcome likely exposed group compared unexposed group.variability stratum-specific odds ratios suggests strength association may differ slightly region, Mantel-Haenszel test assumes association consistent (homogeneous).","code":"\n# Create a 2 x 2 x 3 contingency table\nCustomerData = array(\n    c(40, 30, 200, 300, 35, 20, 180, 265, 50, 25, 250, 275),\n    dim = c(2, 2, 3),\n    dimnames = list(\n        Satisfaction = c(\"Satisfied\", \"Not Satisfied\"),\n        RepeatPurchase = c(\"Yes\", \"No\"),\n        Region = c(\"North\", \"South\", \"West\")\n    )\n)\n\n# View marginal table (summarized across regions)\nmargin.table(CustomerData, c(1, 2))\n#>                RepeatPurchase\n#> Satisfaction    Yes  No\n#>   Satisfied     125 630\n#>   Not Satisfied  75 840\nlibrary(samplesizeCMH)\nmarginal_table = margin.table(CustomerData, c(1, 2))\nodds.ratio(marginal_table)\n#> [1] 2.222222\napply(CustomerData, 3, odds.ratio)\n#>    North    South     West \n#> 2.000000 2.576389 2.200000\nmantelhaen.test(CustomerData, correct = TRUE)\n#> \n#>  Mantel-Haenszel chi-squared test with continuity correction\n#> \n#> data:  CustomerData\n#> Mantel-Haenszel X-squared = 26.412, df = 1, p-value = 2.758e-07\n#> alternative hypothesis: true common odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  1.637116 3.014452\n#> sample estimates:\n#> common odds ratio \n#>          2.221488"},{"path":"basic-statistical-inference.html","id":"mcnemars-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.2 McNemar’s Test","text":"McNemar’s Test special case Mantel-Haenszel Chi-square Test, designed paired nominal data. particularly useful evaluating changes categorical responses treatment intervention, comparing paired responses matched samples. Unlike Mantel-Haenszel Test, handles stratified data, McNemar’s Test tailored situations single \\(2 \\times 2\\) table derived paired observations.McNemar’s Test assesses whether proportions discordant pairs (-diagonal elements \\(2 \\times 2\\) table) significantly different. Specifically, tests null hypothesis probabilities transitioning one category another equal.Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch B}) = P(\\text{Switch B })\n\\] implies probabilities transitioning one category equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) symmetric: \\[\nH_0: n_{12} = n_{21}\n\\]Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch B}) = P(\\text{Switch B })\n\\] implies probabilities transitioning one category equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) symmetric: \\[\nH_0: n_{12} = n_{21}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch B}) \\neq P(\\text{Switch B })\n\\] suggests probabilities transitioning categories equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) asymmetric: \\[\nH_A: n_{12} \\neq n_{21}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch B}) \\neq P(\\text{Switch B })\n\\] suggests probabilities transitioning categories equal, equivalently, -diagonal cell counts (\\(n_{12}\\) \\(n_{21}\\)) asymmetric: \\[\nH_A: n_{12} \\neq n_{21}\n\\]example, consider business analyzing whether new advertising campaign influences customer preference two products (B). customer surveyed campaign, resulting following \\(2 \\times 2\\) contingency table:rows: Preference Product B campaign.columns: Preference Product B campaign.Let table structure :\\(n_{12}\\): Customers switched Product B.\\(n_{21}\\): Customers switched Product B .test focuses \\(n_{12}\\) \\(n_{21}\\), represent discordant pairs.McNemar’s Test statistic : \\[\nM^2 = \\frac{(|n_{12} - n_{21}| - 0.5)^2}{n_{12} + n_{21}}\n\\] whereThe 0.5 continuity correction applied sample sizes small.0.5 continuity correction applied sample sizes small.null hypothesis preference change, \\(M^2\\) follows \\(\\chi^2\\) distribution 1 degree freedom.null hypothesis preference change, \\(M^2\\) follows \\(\\chi^2\\) distribution 1 degree freedom.Let’s analyze voting behavior study participants surveyed campaign. table represents:Rows: Voting preference campaign (Yes, ).Rows: Voting preference campaign (Yes, ).Columns: Voting preference campaign (Yes, ).Columns: Voting preference campaign (Yes, ).test provides:Test statistic (\\(M^2\\)): Quantifies asymmetry discordant pairs.Test statistic (\\(M^2\\)): Quantifies asymmetry discordant pairs.p-value: Indicates whether significant difference discordant proportions.p-value: Indicates whether significant difference discordant proportions.InterpretationTest Statistic: large \\(M^2\\) value suggests significant asymmetry discordant pairs.Test Statistic: large \\(M^2\\) value suggests significant asymmetry discordant pairs.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).\nhigh p-value fails reject null hypothesis, suggesting significant preference change.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating proportion participants switching preferences (e.g., Yes ) significantly different switching opposite direction (e.g., Yes).high p-value fails reject null hypothesis, suggesting significant preference change.high p-value fails reject null hypothesis, suggesting significant preference change.McNemar’s Test widely used business fields:Marketing Campaigns: Evaluating whether campaign shifts consumer preferences purchase intentions.Marketing Campaigns: Evaluating whether campaign shifts consumer preferences purchase intentions.Product Testing: Determining new feature redesign changes customer ratings.Product Testing: Determining new feature redesign changes customer ratings.Healthcare Studies: Analyzing treatment effects paired medical trials.Healthcare Studies: Analyzing treatment effects paired medical trials.","code":"\n# Voting preference before and after a campaign\nvote = matrix(c(682, 22, 86, 810), nrow = 2, byrow = TRUE,\n              dimnames = list(\n                \"Before\" = c(\"Yes\", \"No\"),\n                \"After\" = c(\"Yes\", \"No\")\n              ))\n\n# Perform McNemar's Test with continuity correction\nmcnemar_result <- mcnemar.test(vote, correct = TRUE)\nmcnemar_result\n#> \n#>  McNemar's Chi-squared test with continuity correction\n#> \n#> data:  vote\n#> McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09"},{"path":"basic-statistical-inference.html","id":"mcnemar-bowker-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.3 McNemar-Bowker Test","text":"McNemar-Bowker Test extension McNemar’s Test, designed analyzing paired nominal data two categories. evaluates symmetry full contingency table comparing -diagonal elements across categories. test particularly useful understanding whether changes categories uniformly distributed whether significant asymmetries exist.Let data structured \\(r \\times r\\) square contingency table, \\(r\\) number categories, -diagonal elements represent transitions categories.hypotheses McNemar-Bowker Test :Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) = P(\\text{Switch Category } j \\text{ Category } ) \\quad \\forall \\, \\neq j\n\\] implies -diagonal elements symmetric, directional preference category transitions.Null Hypothesis (\\(H_0\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) = P(\\text{Switch Category } j \\text{ Category } ) \\quad \\forall \\, \\neq j\n\\] implies -diagonal elements symmetric, directional preference category transitions.Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) \\neq P(\\text{Switch Category } j \\text{ Category } ) \\quad \\text{least one pair } (, j)\n\\] suggests -diagonal elements symmetric, indicating directional preference transitions least one pair categories.Alternative Hypothesis (\\(H_A\\)): \\[\nP(\\text{Switch Category } \\text{ Category } j) \\neq P(\\text{Switch Category } j \\text{ Category } ) \\quad \\text{least one pair } (, j)\n\\] suggests -diagonal elements symmetric, indicating directional preference transitions least one pair categories.McNemar-Bowker Test statistic : \\[\nB^2 = \\sum_{< j} \\frac{(n_{ij} - n_{ji})^2}{n_{ij} + n_{ji}}\n\\]\\(n_{ij}\\): Observed count transitions category \\(\\) category \\(j\\).\\(n_{ij}\\): Observed count transitions category \\(\\) category \\(j\\).\\(n_{ji}\\): Observed count transitions category \\(j\\) category \\(\\).\\(n_{ji}\\): Observed count transitions category \\(j\\) category \\(\\).null hypothesis, test statistic \\(B^2\\) approximately follows \\(\\chi^2\\) distribution \\(\\frac{r(r-1)}{2}\\) degrees freedom (corresponding number unique pairs categories).example, company surveys customers satisfaction implementing new policy. Satisfaction rated scale 1 3 (1 = Low, 2 = Medium, 3 = High). paired responses summarized following \\(3 \\times 3\\) contingency table.output includes:Test Statistic (\\(B^2\\)): measure asymmetry -diagonal elements.Test Statistic (\\(B^2\\)): measure asymmetry -diagonal elements.p-value: probability observing data null hypothesis symmetry.p-value: probability observing data null hypothesis symmetry.InterpretationTest Statistic: large $B^2$ value suggests substantial asymmetry transitions categories.Test Statistic: large $B^2$ value suggests substantial asymmetry transitions categories.p-value:\np-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.\np-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.\np-value:p-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.p-value less significance level (e.g., $p < 0.05$), reject null hypothesis, indicating significant asymmetry transitions least one pair categories.p-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.p-value greater significance level, fail reject null hypothesis, suggesting category transitions symmetric.McNemar-Bowker Test broad applications business fields:Customer Feedback Analysis: Evaluating changes customer satisfaction levels interventions.Customer Feedback Analysis: Evaluating changes customer satisfaction levels interventions.Marketing Campaigns: Assessing shifts brand preferences across multiple brands response advertisement.Marketing Campaigns: Assessing shifts brand preferences across multiple brands response advertisement.Product Testing: Understanding user preferences among different product features change redesign.Product Testing: Understanding user preferences among different product features change redesign.","code":"\n# Satisfaction ratings before and after the intervention\nsatisfaction_table <- matrix(c(\n    30, 10, 5,  # Before: Low\n    8, 50, 12,  # Before: Medium\n    6, 10, 40   # Before: High\n), nrow = 3, byrow = TRUE,\ndimnames = list(\n    \"Before\" = c(\"Low\", \"Medium\", \"High\"),\n    \"After\" = c(\"Low\", \"Medium\", \"High\")\n))\n\n# Function to perform McNemar-Bowker Test\nmcnemar_bowker_test <- function(table) {\n  if (!all(dim(table)[1] == dim(table)[2])) {\n    stop(\"Input must be a square matrix.\")\n  }\n  \n  # Extract off-diagonal elements\n  n <- nrow(table)\n  stat <- 0\n  df <- 0\n  \n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      nij <- table[i, j]\n      nji <- table[j, i]\n      stat <- stat + (nij - nji)^2 / (nij + nji)\n      df <- df + 1\n    }\n  }\n  \n  p_value <- pchisq(stat, df = df, lower.tail = FALSE)\n  return(list(statistic = stat, df = df, p_value = p_value))\n}\n\n# Run the test\nresult <- mcnemar_bowker_test(satisfaction_table)\n\n# Print results\ncat(\"McNemar-Bowker Test Results:\\n\")\n#> McNemar-Bowker Test Results:\ncat(\"Test Statistic (B^2):\", result$statistic, \"\\n\")\n#> Test Statistic (B^2): 0.4949495\ncat(\"Degrees of Freedom:\", result$df, \"\\n\")\n#> Degrees of Freedom: 3\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.9199996"},{"path":"basic-statistical-inference.html","id":"stuart-maxwell-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.4 Stuart-Maxwell Test","text":"Stuart-Maxwell Test used analyzing changes paired categorical data two categories. generalization McNemar’s Test, applied square contingency tables -diagonal elements represent transitions categories. Unlike McNemar-Bowker Test, tests symmetry across pairs, Stuart-Maxwell Test focuses overall marginal homogeneity.test evaluates whether marginal distributions paired data consistent across categories. particularly useful investigating whether distribution responses shifted two conditions, intervention.Hypotheses Stuart-Maxwell TestNull Hypothesis (\\(H_0\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{marginal distributions paired data homogeneous (difference).}\n\\]Stuart-Maxwell Test statistic calculated : \\[\nM^2 = \\mathbf{b}' \\mathbf{V}^{-1} \\mathbf{b}\n\\] :\\(\\mathbf{b}\\): Vector differences marginal totals paired categories.\\(\\mathbf{b}\\): Vector differences marginal totals paired categories.\\(\\mathbf{V}\\): Covariance matrix \\(\\mathbf{b}\\) null hypothesis.\\(\\mathbf{V}\\): Covariance matrix \\(\\mathbf{b}\\) null hypothesis.test statistic \\(M^2\\) follows \\(\\chi^2\\) distribution \\((r - 1)\\) degrees freedom, \\(r\\) number categories.company surveys employees satisfaction levels (Low, Medium, High) implementing new workplace policy. results summarized following \\(3 \\times 3\\) contingency table.InterpretationTest Statistic: Measures extent marginal differences table.Test Statistic: Measures extent marginal differences table.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.\nhigh p-value suggests evidence marginal differences, meaning distribution consistent across conditions.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.low p-value (e.g., \\(p < 0.05\\)) indicates significant differences marginal distributions, suggesting change distribution responses.high p-value suggests evidence marginal differences, meaning distribution consistent across conditions.high p-value suggests evidence marginal differences, meaning distribution consistent across conditions.Practical Applications Stuart-Maxwell TestEmployee Surveys: Analyzing shifts satisfaction levels policy changes.Employee Surveys: Analyzing shifts satisfaction levels policy changes.Consumer Studies: Evaluating changes product preferences marketing campaign.Consumer Studies: Evaluating changes product preferences marketing campaign.Healthcare Research: Assessing changes patient responses treatments across categories.Healthcare Research: Assessing changes patient responses treatments across categories.","code":"\n# Employee satisfaction data before and after a policy change\nsatisfaction_table <- matrix(c(\n    40, 10, 5,  # Before: Low\n    8, 50, 12,  # Before: Medium\n    6, 10, 40   # Before: High\n), nrow = 3, byrow = TRUE,\ndimnames = list(\n    \"Before\" = c(\"Low\", \"Medium\", \"High\"),\n    \"After\" = c(\"Low\", \"Medium\", \"High\")\n))\n\n# Function to perform the Stuart-Maxwell Test\nstuart_maxwell_test <- function(table) {\n  if (!all(dim(table)[1] == dim(table)[2])) {\n    stop(\"Input must be a square matrix.\")\n  }\n  \n  # Marginal totals for each category\n  row_totals <- rowSums(table)\n  col_totals <- colSums(table)\n  \n  # Vector of differences between row and column marginal totals\n  b <- row_totals - col_totals\n  \n  # Covariance matrix under the null hypothesis\n  total <- sum(table)\n  V <- diag(row_totals + col_totals) - \n       (outer(row_totals, col_totals, \"+\") / total)\n  \n  # Calculate the test statistic\n  M2 <- t(b) %*% solve(V) %*% b\n  df <- nrow(table) - 1\n  p_value <- pchisq(M2, df = df, lower.tail = FALSE)\n  \n  return(list(statistic = M2, df = df, p_value = p_value))\n}\n\n# Run the Stuart-Maxwell Test\nresult <- stuart_maxwell_test(satisfaction_table)\n\n# Print the results\ncat(\"Stuart-Maxwell Test Results:\\n\")\n#> Stuart-Maxwell Test Results:\ncat(\"Test Statistic (M^2):\", result$statistic, \"\\n\")\n#> Test Statistic (M^2): 0.01802387\ncat(\"Degrees of Freedom:\", result$df, \"\\n\")\n#> Degrees of Freedom: 2\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.9910286"},{"path":"basic-statistical-inference.html","id":"cochran-mantel-haenszel-cmh-test","chapter":"4 Basic Statistical Inference","heading":"4.5.2.5 Cochran-Mantel-Haenszel (CMH) Test","text":"Cochran-Mantel-Haenszel (CMH) Test generalization Mantel-Haenszel Chi-square Test. evaluates association two variables controlling effect third stratifying variable. test particularly suited ordinal data, allowing researchers detect trends associations across strata.CMH Test addresses scenarios :Two variables (e.g., exposure outcome) ordinal nominal.third variable (e.g., demographic environmental factor) stratifies data \\(K\\) independent groups.test answers: consistent association two variables across strata defined third variable?CMH Test three main variations depending nature data:Correlation Test Ordinal Data: Assesses whether linear association two ordinal variables across strata.General Association Test: Tests association (necessarily ordinal) two variables stratifying third.Homogeneity Test: Checks whether strength association two variables consistent across strata.HypothesesNull Hypothesis (\\(H_0\\)): association two variables across strata, strength association consistent across strata.Null Hypothesis (\\(H_0\\)): association two variables across strata, strength association consistent across strata.Alternative Hypothesis (\\(H_A\\)): association two variables least one stratum, strength association varies across strata.Alternative Hypothesis (\\(H_A\\)): association two variables least one stratum, strength association varies across strata.CMH test statistic : \\[\nCMH = \\frac{\\left( \\sum_{k} \\left(O_k - E_k \\right)\\right)^2}{\\sum_{k} V_k}\n\\] :\\(O_k\\): Observed counts stratum \\(k\\).\\(O_k\\): Observed counts stratum \\(k\\).\\(E_k\\): Expected counts stratum \\(k\\), calculated null hypothesis.\\(E_k\\): Expected counts stratum \\(k\\), calculated null hypothesis.\\(V_k\\): Variance observed counts stratum \\(k\\).\\(V_k\\): Variance observed counts stratum \\(k\\).test statistic follows \\(\\chi^2\\) distribution 1 degree freedom null hypothesis.company evaluates whether sales performance (Low, Medium, High) associated product satisfaction (Low, Medium, High) across three experience levels (Junior, Mid-level, Senior). data organized \\(3 \\times 3 \\times 3\\) contingency table.InterpretationTest Statistic: large CMH statistic suggests significant association sales performance satisfaction accounting experience level.Test Statistic: large CMH statistic suggests significant association sales performance satisfaction accounting experience level.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.\nhigh p-value suggests evidence association relationship consistent across strata.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.low p-value (e.g., \\(p < 0.05\\)) indicates significant association two variables across strata.high p-value suggests evidence association relationship consistent across strata.high p-value suggests evidence association relationship consistent across strata.Practical Applications CMH TestBusiness Performance Analysis: Investigating relationship customer satisfaction sales performance across different demographic groups.Business Performance Analysis: Investigating relationship customer satisfaction sales performance across different demographic groups.Healthcare Studies: Assessing effect treatment (e.g., dosage) outcomes controlling patient characteristics (e.g., age groups).Healthcare Studies: Assessing effect treatment (e.g., dosage) outcomes controlling patient characteristics (e.g., age groups).Educational Research: Analyzing relationship test scores study hours, stratified teaching method.Educational Research: Analyzing relationship test scores study hours, stratified teaching method.","code":"\n# Sales performance data\nsales_data <- array(\n    c(20, 15, 10, 12, 18, 15, 8, 12, 20,   # Junior\n      25, 20, 15, 20, 25, 30, 10, 15, 20,  # Mid-level\n      30, 25, 20, 28, 32, 35, 15, 20, 30), # Senior\n    dim = c(3, 3, 3),\n    dimnames = list(\n        SalesPerformance = c(\"Low\", \"Medium\", \"High\"),\n        Satisfaction = c(\"Low\", \"Medium\", \"High\"),\n        ExperienceLevel = c(\"Junior\", \"Mid-level\", \"Senior\")\n    )\n)\n\n# Load the vcd package for the CMH test\nlibrary(vcd)\n\n# Perform CMH Test\ncmh_result <- mantelhaen.test(sales_data, correct = FALSE)\ncmh_result\n#> \n#>  Cochran-Mantel-Haenszel test\n#> \n#> data:  sales_data\n#> Cochran-Mantel-Haenszel M^2 = 22.454, df = 4, p-value = 0.0001627"},{"path":"basic-statistical-inference.html","id":"summary-table-of-tests","chapter":"4 Basic Statistical Inference","heading":"4.5.2.6 Summary Table of Tests","text":"following table provides concise guide use test:Choose Right TestPaired vs. Stratified Data:\nUse McNemar’s Test McNemar-Bowker Test paired data.\nUse Mantel-Haenszel CMH Test stratified data.\nUse McNemar’s Test McNemar-Bowker Test paired data.Use Mantel-Haenszel CMH Test stratified data.Binary vs. Multi-category Variables:\nUse McNemar’s Test binary data.\nUse McNemar-Bowker Test Stuart-Maxwell Test multi-category data.\nUse McNemar’s Test binary data.Use McNemar-Bowker Test Stuart-Maxwell Test multi-category data.Ordinal Trends:\nUse CMH Test testing ordinal associations controlling stratifying variable.\nUse CMH Test testing ordinal associations controlling stratifying variable.","code":""},{"path":"basic-statistical-inference.html","id":"ordinal-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3 Ordinal Trend","text":"analyzing ordinal data, often important determine whether consistent trend exists variables. Tests trend specifically designed detect monotonic relationships changes one variable systematically associated changes another. tests widely used scenarios involving ordered categories, customer satisfaction ratings, income brackets, educational levels.primary objectives trend tests :detect monotonic relationships: Determine higher lower categories one variable associated higher lower categories another variable.account ordinal structure: Leverage inherent order data provide sensitive interpretable results compared tests designed nominal data.Key Considerations Trend TestsData Structure:\nEnsure variables natural order treated ordinal.\nVerify trend test chosen matches data structure (e.g., binary outcome vs. multi-level ordinal variables).\nEnsure variables natural order treated ordinal.Verify trend test chosen matches data structure (e.g., binary outcome vs. multi-level ordinal variables).Assumptions:\nMany tests assume monotonic trends, meaning relationship reverse direction.\nMany tests assume monotonic trends, meaning relationship reverse direction.Interpretation:\nsignificant result indicates presence trend imply causality.\ndirection strength trend carefully interpreted context data.\nsignificant result indicates presence trend imply causality.direction strength trend carefully interpreted context data.","code":""},{"path":"basic-statistical-inference.html","id":"cochran-armitage-test","chapter":"4 Basic Statistical Inference","heading":"4.5.3.1 Cochran-Armitage Test","text":"Cochran-Armitage Test Trend statistical method designed detect linear trend proportions across ordered categories predictor variable. particularly useful \\(2 \\times J\\) contingency tables, binary outcome (e.g., success/failure) ordinal predictor variable \\(J\\) ordered levels.Cochran-Armitage Test evaluates whether proportion binary outcome changes systematically across levels ordinal predictor. test leverages ordinal nature predictor enhance sensitivity power compared general chi-square tests.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{proportion binary outcome constant across levels ordinal predictor.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{proportion binary outcome constant across levels ordinal predictor.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{linear trend proportion binary outcome across levels ordinal predictor.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{linear trend proportion binary outcome across levels ordinal predictor.}\n\\]Cochran-Armitage Test statistic calculated :\\[\nZ = \\frac{\\sum_{j=1}^{J} w_j (n_{1j} - N_j \\hat{p})}{\\sqrt{\\hat{p} (1 - \\hat{p}) \\sum_{j=1}^{J} w_j^2 N_j}}\n\\]:\\(n_{1j}\\): Count binary outcome (e.g., “success”) category \\(j\\).\\(n_{1j}\\): Count binary outcome (e.g., “success”) category \\(j\\).\\(N_j\\): Total number observations category \\(j\\).\\(N_j\\): Total number observations category \\(j\\).\\(\\hat{p}\\): Overall proportion binary outcome, calculated : \\[\n  \\hat{p} = \\frac{\\sum_{j=1}^{J} n_{1j}}{\\sum_{j=1}^{J} N_j}\n  \\]\\(\\hat{p}\\): Overall proportion binary outcome, calculated : \\[\n  \\hat{p} = \\frac{\\sum_{j=1}^{J} n_{1j}}{\\sum_{j=1}^{J} N_j}\n  \\]\\(w_j\\): Score assigned \\(j\\)th category ordinal predictor, often set \\(j\\) equally spaced levels.\\(w_j\\): Score assigned \\(j\\)th category ordinal predictor, often set \\(j\\) equally spaced levels.test statistic \\(Z\\) follows standard normal distribution null hypothesis.Key AssumptionsOrdinal Predictor: categories predictor variable must natural order.Binary Outcome: response variable must dichotomous (e.g., success/failure).Independent Observations: Observations within across categories independent.Let’s consider study examining whether success rate marketing campaign varies across three income levels (Low, Medium, High). data structured \\(2 \\times 3\\) contingency table:InterpretationTest Statistic (\\(Z\\)):\n\\(Z\\) value indicates strength direction trend.\nPositive \\(Z\\): Proportions increase higher categories.\nNegative \\(Z\\): Proportions decrease higher categories.\nTest Statistic (\\(Z\\)):\\(Z\\) value indicates strength direction trend.\\(Z\\) value indicates strength direction trend.Positive \\(Z\\): Proportions increase higher categories.Positive \\(Z\\): Proportions increase higher categories.Negative \\(Z\\): Proportions decrease higher categories.Negative \\(Z\\): Proportions decrease higher categories.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.\nhigh p-value fails reject null hypothesis, suggesting evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.high p-value fails reject null hypothesis, suggesting evidence trend.high p-value fails reject null hypothesis, suggesting evidence trend.Practical ApplicationsMarketing: Analyzing whether customer success rates vary systematically across income levels demographics.Marketing: Analyzing whether customer success rates vary systematically across income levels demographics.Healthcare: Evaluating dose-response relationship medication levels recovery rates.Healthcare: Evaluating dose-response relationship medication levels recovery rates.Education: Studying whether pass rates improve higher levels educational support.Education: Studying whether pass rates improve higher levels educational support.","code":"\n# Data: Success and Failure counts by Income Level\nincome_levels <- c(\"Low\", \"Medium\", \"High\")\nsuccess <- c(20, 35, 45)\nfailure <- c(30, 15, 5)\ntotal <- success + failure\n\n# Scores for ordinal levels (can be custom weights)\nscores <- 1:length(income_levels)\n\n# Cochran-Armitage Test\n# Function to calculate Z statistic\ncochran_armitage_test <- function(success, failure, scores) {\n  N <- success + failure\n  p_hat <- sum(success) / sum(N)\n  weights <- scores\n  \n  # Calculate numerator\n  numerator <- sum(weights * (success - N * p_hat))\n  \n  # Calculate denominator\n  denominator <- sqrt(p_hat * (1 - p_hat) * sum(weights^2 * N))\n  \n  # Z statistic\n  Z <- numerator / denominator\n  p_value <- 2 * (1 - pnorm(abs(Z)))\n  \n  return(list(Z_statistic = Z, p_value = p_value))\n}\n\n# Perform the test\nresult <- cochran_armitage_test(success, failure, scores)\n\n# Print results\ncat(\"Cochran-Armitage Test for Trend Results:\\n\")\n#> Cochran-Armitage Test for Trend Results:\ncat(\"Z Statistic:\", result$Z_statistic, \"\\n\")\n#> Z Statistic: 2.004459\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.04502088"},{"path":"basic-statistical-inference.html","id":"jonckheere-terpstra-test","chapter":"4 Basic Statistical Inference","heading":"4.5.3.2 Jonckheere-Terpstra Test","text":"Jonckheere-Terpstra Test nonparametric test designed detect ordered differences groups. particularly suited ordinal data predictor response variables exhibit monotonic trend. Unlike general nonparametric tests like Kruskal-Wallis test, assess differences groups, Jonckheere-Terpstra Test specifically evaluates whether data follows prespecified ordering.Jonckheere-Terpstra Test determines whether:monotonic trend response variable across ordered groups predictor.data aligns priori hypothesized order (e.g., group 1 < group 2 < group 3).HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{trend response variable across ordered groups.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{trend response variable across ordered groups.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{response variable exhibits monotonic trend across ordered groups.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{response variable exhibits monotonic trend across ordered groups.}\n\\]trend can increasing, decreasing, otherwise hypothesized.Jonckheere-Terpstra Test statistic based number pairwise comparisons (\\(U\\)) consistent hypothesized trend. \\(k\\) groups:Compare possible pairs observations across groups.Count number pairs values consistent hypothesized order.test statistic \\(T\\) sum pairwise comparisons: \\[\nT = \\sum_{< j} T_{ij}\n\\] \\(T_{ij}\\) number concordant pairs groups \\(\\) \\(j\\).null hypothesis, \\(T\\) follows normal distribution :Mean: \\[\n  \\mu_T = \\frac{N (N - 1)}{4}\n  \\]Mean: \\[\n  \\mu_T = \\frac{N (N - 1)}{4}\n  \\]Variance: \\[\n  \\sigma_T^2 = \\frac{N (N - 1) (2N + 1)}{24}\n  \\] \\(N\\) total number observations.Variance: \\[\n  \\sigma_T^2 = \\frac{N (N - 1) (2N + 1)}{24}\n  \\] \\(N\\) total number observations.standardized test statistic : \\[\nZ = \\frac{T - \\mu_T}{\\sigma_T}\n\\]Key AssumptionsOrdinal Interval Data: response variable must least ordinal, groups must logical order.Independent Groups: Observations within groups independent.Consistent Hypothesis: trend (e.g., increasing decreasing) must specified advance.Let’s consider study analyzing whether customer satisfaction ratings (scale 1 5) improve increasing levels service tiers (Basic, Standard, Premium). data grouped service tier, hypothesize satisfaction ratings increase higher service tiers.InterpretationTest Statistic (\\(T\\)):\nRepresents sum pairwise comparisons consistent hypothesized order.\nIncludes 0.5 tied pairs.\nTest Statistic (\\(T\\)):Represents sum pairwise comparisons consistent hypothesized order.Includes 0.5 tied pairs.\\(Z\\) Statistic:\nstandardized measure strength trend.\nCalculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).\n\\(Z\\) Statistic:standardized measure strength trend.standardized measure strength trend.Calculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).Calculated using \\(T\\), expected value \\(T\\) null hypothesis (\\(\\mu_T\\)), variance \\(T\\) (\\(\\sigma_T^2\\)).p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.\nhigh p-value fails reject null hypothesis, suggesting evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant trend response variable across ordered groups.high p-value fails reject null hypothesis, suggesting evidence trend.high p-value fails reject null hypothesis, suggesting evidence trend.Practical ApplicationsCustomer Experience Analysis: Assessing whether customer satisfaction increases higher service levels product tiers.Customer Experience Analysis: Assessing whether customer satisfaction increases higher service levels product tiers.Healthcare Studies: Testing whether recovery rates improve increasing doses treatment.Healthcare Studies: Testing whether recovery rates improve increasing doses treatment.Education Research: Analyzing whether test scores improve higher levels educational intervention.Education Research: Analyzing whether test scores improve higher levels educational intervention.","code":"\n# Example Data: Customer Satisfaction Ratings by Service Tier\nsatisfaction <- list(\n  Basic = c(2, 3, 2, 4, 3),\n  Standard = c(3, 4, 3, 5, 4),\n  Premium = c(4, 5, 4, 5, 5)\n)\n\n# Prepare data\nratings <- unlist(satisfaction)\ngroups <- factor(rep(names(satisfaction), times = sapply(satisfaction, length)))\n\n# Calculate pairwise comparisons\nmanual_jonckheere <- function(ratings, groups) {\n  n_groups <- length(unique(groups))\n  pairwise_comparisons <- 0\n  total_pairs <- 0\n  \n  # Iterate over group pairs\n  for (i in 1:(n_groups - 1)) {\n    for (j in (i + 1):n_groups) {\n      group_i <- ratings[groups == levels(groups)[i]]\n      group_j <- ratings[groups == levels(groups)[j]]\n      \n      # Count concordant pairs\n      for (x in group_i) {\n        for (y in group_j) {\n          if (x < y) pairwise_comparisons <- pairwise_comparisons + 1\n          if (x == y) pairwise_comparisons <- pairwise_comparisons + 0.5\n          total_pairs <- total_pairs + 1\n        }\n      }\n    }\n  }\n  \n  # Compute test statistic\n  T <- pairwise_comparisons\n  N <- length(ratings)\n  mu_T <- total_pairs / 2\n  sigma_T <- sqrt(total_pairs * (N + 1) / 12)\n  \n  Z <- (T - mu_T) / sigma_T\n  p_value <- 2 * (1 - pnorm(abs(Z)))\n  \n  return(list(T_statistic = T, Z_statistic = Z, p_value = p_value))\n}\n\n# Perform the test\nresult <- manual_jonckheere(ratings, groups)\n\n# Print results\ncat(\"Jonckheere-Terpstra Test Results:\\n\")\n#> Jonckheere-Terpstra Test Results:\ncat(\"T Statistic (Sum of Concordant Pairs):\", result$T_statistic, \"\\n\")\n#> T Statistic (Sum of Concordant Pairs): 49.5\ncat(\"Z Statistic:\", result$Z_statistic, \"\\n\")\n#> Z Statistic: 1.2\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.2301393"},{"path":"basic-statistical-inference.html","id":"mantel-test-for-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3.3 Mantel Test for Trend","text":"Mantel Test Trend statistical method designed detect linear association two ordinal variables. extension Mantel-Haenszel Chi-square Test particularly suited analyzing trends ordinal contingency tables, \\(\\times J\\) tables variables ordinal.Mantel Test Trend evaluates whether increasing decreasing trend exists two ordinal variables. uses ordering categories assess linear relationships, making sensitive trends compared general association tests like chi-square.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{linear association two ordinal variables.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{linear association two ordinal variables.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear association two ordinal variables.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear association two ordinal variables.}\n\\]Mantel Test based Pearson correlation row column scores ordinal contingency table. test statistic : \\[\nM = \\frac{\\sum_{} \\sum_{j} w_i w_j n_{ij}}{\\sqrt{\\sum_{} w_i^2 n_{\\cdot} \\sum_{j} w_j^2 n_{\\cdot j}}}\n\\]:\\(n_{ij}\\): Observed frequency cell \\((, j)\\).\\(n_{ij}\\): Observed frequency cell \\((, j)\\).\\(n_{\\cdot}\\): Row marginal total row \\(\\).\\(n_{\\cdot}\\): Row marginal total row \\(\\).\\(n_{\\cdot j}\\): Column marginal total column \\(j\\).\\(n_{\\cdot j}\\): Column marginal total column \\(j\\).\\(w_i\\): Score \\(\\)th row.\\(w_i\\): Score \\(\\)th row.ore \\(j\\)th column.ore \\(j\\)th column.test statistic \\(M\\) asymptotically normally distributed null hypothesis.Key AssumptionsOrdinal Variables: variables must natural order.Linear Trend: Assumes linear relationship scores assigned rows columns.Independence: Observations must independent.Let’s consider marketing study evaluating whether customer satisfaction levels (Low, Medium, High) associated increasing purchase frequency (Low, Medium, High).InterpretationTest Statistic (\\(M\\)):\nRepresents strength direction linear association.\nPositive \\(M\\): Increasing trend.\nNegative \\(M\\): Decreasing trend.\nTest Statistic (\\(M\\)):Represents strength direction linear association.Represents strength direction linear association.Positive \\(M\\): Increasing trend.Positive \\(M\\): Increasing trend.Negative \\(M\\): Decreasing trend.Negative \\(M\\): Decreasing trend.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.\nhigh p-value suggests evidence trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.low p-value (e.g., \\(p < 0.05\\)) indicates significant linear association.high p-value suggests evidence trend.high p-value suggests evidence trend.Practical ApplicationsMarketing Analysis: Investigating whether satisfaction levels associated purchase behavior loyalty.Marketing Analysis: Investigating whether satisfaction levels associated purchase behavior loyalty.Healthcare Research: Testing dose-response relationship treatment levels outcomes.Healthcare Research: Testing dose-response relationship treatment levels outcomes.Social Sciences: Analyzing trends survey responses across ordered categories.Social Sciences: Analyzing trends survey responses across ordered categories.","code":"\n# Customer satisfaction and purchase frequency data\ndata <- matrix(\n  c(10, 5, 2, 15, 20, 8, 25, 30, 12), \n  nrow = 3, \n  byrow = TRUE,\n  dimnames = list(\n    Satisfaction = c(\"Low\", \"Medium\", \"High\"),\n    Frequency = c(\"Low\", \"Medium\", \"High\")\n  )\n)\n\n# Assign scores for rows and columns\nrow_scores <- 1:nrow(data)\ncol_scores <- 1:ncol(data)\n\n# Compute Mantel statistic manually\nmantel_test_manual <- function(data, row_scores, col_scores) {\n  numerator <- sum(outer(row_scores, col_scores, \"*\") * data)\n  row_marginals <- rowSums(data)\n  col_marginals <- colSums(data)\n  row_variance <- sum(row_scores^2 * row_marginals)\n  col_variance <- sum(col_scores^2 * col_marginals)\n  \n  M <- numerator / sqrt(row_variance * col_variance)\n  z_value <- M\n  p_value <- 2 * (1 - pnorm(abs(z_value))) # Two-tailed test\n  \n  return(list(Mantel_statistic = M, p_value = p_value))\n}\n\n# Perform the Mantel Test\nresult <- mantel_test_manual(data, row_scores, col_scores)\n\n# Display results\ncat(\"Mantel Test for Trend Results:\\n\")\n#> Mantel Test for Trend Results:\ncat(\"Mantel Statistic (M):\", result$Mantel_statistic, \"\\n\")\n#> Mantel Statistic (M): 0.8984663\ncat(\"p-value:\", result$p_value, \"\\n\")\n#> p-value: 0.368937"},{"path":"basic-statistical-inference.html","id":"chi-square-test-for-linear-trend","chapter":"4 Basic Statistical Inference","heading":"4.5.3.4 Chi-square Test for Linear Trend","text":"Chi-square Test Linear Trend statistical method used detect linear relationship ordinal predictor binary outcome. extension chi-square test, designed specifically ordered categories, making sensitive linear trends proportions compared general chi-square test independence.Chi-square Test Linear Trend evaluates whether proportions binary outcome (e.g., success/failure) change systematically across ordered categories predictor variable. widely used situations analyzing dose-response relationships evaluating trends survey responses.HypothesesNull Hypothesis (\\(H_0\\)): \\[\n\\text{linear trend proportions binary outcome across ordered categories.}\n\\]Null Hypothesis (\\(H_0\\)): \\[\n\\text{linear trend proportions binary outcome across ordered categories.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear trend proportions binary outcome across ordered categories.}\n\\]Alternative Hypothesis (\\(H_A\\)): \\[\n\\text{significant linear trend proportions binary outcome across ordered categories.}\n\\]test statistic :\\[\nX^2_{\\text{trend}} = \\frac{\\left( \\sum_{j=1}^J w_j (p_j - \\bar{p}) N_j \\right)^2}{\\sum_{j=1}^J w_j^2 \\bar{p} (1 - \\bar{p}) N_j}\n\\]: - \\(J\\): Number ordered categories. - \\(w_j\\): Scores assigned \\(j\\)th category (typically \\(j = 1, 2, \\dots, J\\)). - \\(p_j\\): Proportion success \\(j\\)th category. - \\(\\bar{p}\\): Overall proportion success across categories. - \\(N_j\\): Total number observations \\(j\\)th category.test statistic follows chi-square distribution 1 degree freedom null hypothesis.Key AssumptionsBinary Outcome: response variable must binary (e.g., success/failure).Ordinal Predictor: predictor variable must natural order.Independent Observations: Data across categories must independent.Let’s consider study analyzing whether proportion customers recommend product increases customer satisfaction levels (Low, Medium, High).InterpretationChi-square Statistic (\\(X^2_{\\text{trend}}\\)):\nIndicates strength linear trend proportions.\nChi-square Statistic (\\(X^2_{\\text{trend}}\\)):Indicates strength linear trend proportions.p-value:\nlow p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.\nhigh p-value suggests evidence linear trend.\np-value:low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.low p-value (e.g., \\(p < 0.05\\)) rejects null hypothesis, indicating significant linear trend.high p-value suggests evidence linear trend.high p-value suggests evidence linear trend.Practical ApplicationsMarketing: Analyzing whether customer satisfaction levels predict product recommendations repurchase intentions.Marketing: Analyzing whether customer satisfaction levels predict product recommendations repurchase intentions.Healthcare: Evaluating dose-response relationships clinical trials.Healthcare: Evaluating dose-response relationships clinical trials.Education: Testing whether higher levels intervention improve success rates.Education: Testing whether higher levels intervention improve success rates.","code":"\n# Example Data: Customer Satisfaction and Recommendation\nsatisfaction_levels <- c(\"Low\", \"Medium\", \"High\")\nsuccess <- c(20, 35, 50)  # Number of customers who recommend the product\nfailure <- c(30, 15, 10)  # Number of customers who do not recommend the product\ntotal <- success + failure\n\n# Assign ordinal scores\nscores <- 1:length(satisfaction_levels)\n\n# Calculate overall proportion of success\np_hat <- sum(success) / sum(total)"},{"path":"basic-statistical-inference.html","id":"key-takeways","chapter":"4 Basic Statistical Inference","heading":"4.5.3.5 Key Takeways","text":"","code":""},{"path":"basic-statistical-inference.html","id":"divergence-metrics-and-tests-for-comparing-distributions","chapter":"4 Basic Statistical Inference","heading":"4.6 Divergence Metrics and Tests for Comparing Distributions","text":"Divergence metrics powerful tools used measure similarity dissimilarity probability distributions. Unlike deviation deviance statistics, divergence metrics focus broader relationships entire distributions, rather individual data points specific model fit metrics. Let’s clarify differences:Deviation Statistics: Measure difference realization variable reference value (e.g., mean). Common statistics derived deviations include:\nStandard deviation\nAverage absolute deviation\nMedian absolute deviation\nMaximum absolute deviation\nStandard deviationAverage absolute deviationMedian absolute deviationMaximum absolute deviationDeviance Statistics: Assess goodness--fit statistical models. analogous sum squared residuals ordinary least squares (OLS) generalized use cases maximum likelihood estimation (MLE). Deviance statistics frequently employed generalized linear models (GLMs).Divergence statistics differ fundamentally focusing statistical distances entire probability distributions, rather individual data points model errors.1. Divergence MetricsDefinition: Divergence metrics measure much one probability distribution differs another.Definition: Divergence metrics measure much one probability distribution differs another.Key Properties:\nAsymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).\nNon-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).\nUnitless: Divergences often expressed terms information (e.g., bits nats).\nKey Properties:Asymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).Asymmetry: Many divergence metrics, Kullback-Leibler (KL) divergence, symmetric (.e., \\(D(P \\|\\| Q) \\neq D(Q \\|\\| P)\\)).Non-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).Non-Metric: don’t necessarily satisfy properties metric (e.g., symmetry, triangle inequality).Unitless: Divergences often expressed terms information (e.g., bits nats).Unitless: Divergences often expressed terms information (e.g., bits nats).Use:\nUse divergence metrics assess degree mismatch two probability distributions, especially machine learning, statistical inference, model evaluation.\nUse:Use divergence metrics assess degree mismatch two probability distributions, especially machine learning, statistical inference, model evaluation.2. Distance MetricsDefinition: Distance metrics measure “distance” dissimilarity two objects, including probability distributions, datasets, points space.Definition: Distance metrics measure “distance” dissimilarity two objects, including probability distributions, datasets, points space.Key Properties:\nSymmetry: \\(D(P, Q) = D(Q, P)\\).\nTriangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).\nNon-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).\nKey Properties:Symmetry: \\(D(P, Q) = D(Q, P)\\).Symmetry: \\(D(P, Q) = D(Q, P)\\).Triangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).Triangle Inequality: \\(D(P, R) \\leq D(P, Q) + D(Q, R)\\).Non-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).Non-Negativity: \\(D(P, Q) \\geq 0\\), \\(D(P, Q) = 0\\) \\(P=Q\\).Use:\nUse distance metrics compare datasets, distributions, clustering outcomes symmetry geometric properties important.\nUse:Use distance metrics compare datasets, distributions, clustering outcomes symmetry geometric properties important.Applications Divergence MetricsDivergence metrics found wide utility across domains, including:Detecting Data Drift Machine Learning: Used monitor whether distribution incoming data differs significantly training data.Feature Selection: Employed identify features distinguishing power comparing distributions across different classes.Variational Autoencoders (VAEs): Divergence metrics (Kullback-Leibler divergence) central loss functions used training VAEs.Reinforcement Learning: Measure similarity policy distributions improve decision-making processes.Assessing Consistency: Compare distributions two variables representing constructs test relationship agreement.Divergence metrics also highly relevant business settings, providing insights solutions variety applications, :Customer Segmentation Targeting: Compare distributions customer demographics purchase behavior across market segments identify key differences target strategies effectively.Customer Segmentation Targeting: Compare distributions customer demographics purchase behavior across market segments identify key differences target strategies effectively.Market Basket Analysis: Measure divergence distributions product co-purchases across regions customer groups optimize product bundling cross-selling strategies.Market Basket Analysis: Measure divergence distributions product co-purchases across regions customer groups optimize product bundling cross-selling strategies.Marketing Campaign Effectiveness: Evaluate whether distribution customer responses (e.g., click-rates conversions) differs significantly marketing campaign, providing insights success.Marketing Campaign Effectiveness: Evaluate whether distribution customer responses (e.g., click-rates conversions) differs significantly marketing campaign, providing insights success.Fraud Detection: Monitor divergence transaction patterns time detect anomalies may indicate fraudulent activities.Fraud Detection: Monitor divergence transaction patterns time detect anomalies may indicate fraudulent activities.Supply Chain Optimization: Compare demand distributions across time periods regions optimize inventory allocation reduce stock-outs overstocking.Supply Chain Optimization: Compare demand distributions across time periods regions optimize inventory allocation reduce stock-outs overstocking.Pricing Strategy Evaluation: Analyze divergence pricing purchase distributions across products customer segments refine pricing models improve profitability.Pricing Strategy Evaluation: Analyze divergence pricing purchase distributions across products customer segments refine pricing models improve profitability.Churn Prediction: Compare distributions engagement metrics (e.g., frequency transactions usage time) customers likely churn stay, design retention strategies.Churn Prediction: Compare distributions engagement metrics (e.g., frequency transactions usage time) customers likely churn stay, design retention strategies.Financial Portfolio Analysis: Assess divergence expected returns actual performance distributions different asset classes adjust investment strategies.Financial Portfolio Analysis: Assess divergence expected returns actual performance distributions different asset classes adjust investment strategies.","code":""},{"path":"basic-statistical-inference.html","id":"kolmogorov-smirnov-test-1","chapter":"4 Basic Statistical Inference","heading":"4.6.1 Kolmogorov-Smirnov Test","text":"Kolmogorov-Smirnov (KS) test non-parametric test used determine whether two distributions differ significantly whether sample distribution matches reference distribution. applicable continuous distributions widely used hypothesis testing model evaluation.Mathematical DefinitionThe KS statistic defined :\\[\nD = \\max |F_P(x) - F_Q(x)|\n\\]:\\(F_P(x)\\) cumulative distribution function (CDF) first distribution (sample).\\(F_P(x)\\) cumulative distribution function (CDF) first distribution (sample).\\(F_Q(x)\\) CDF second distribution (theoretical reference distribution).\\(F_Q(x)\\) CDF second distribution (theoretical reference distribution).\\(D\\) measures maximum vertical distance two CDFs.\\(D\\) measures maximum vertical distance two CDFs.HypothesesNull Hypothesis (\\(H_0\\)): empirical distribution follows specified distribution (two samples drawn distribution).Alternative Hypothesis (\\(H_1\\)): empirical distribution follow specified distribution (two samples drawn different distributions).Properties KS StatisticRange: \\[\nD \\[0, 1]\n\\]\n\\(D = 0\\): Perfect match distributions.\n\\(D = 1\\): Maximum dissimilarity distributions.\n\\(D = 0\\): Perfect match distributions.\\(D = 1\\): Maximum dissimilarity distributions.Non-parametric Nature: KS test makes assumptions underlying distribution data.KS test useful various scenarios, including:Comparing two empirical distributions evaluate similarity.Testing goodness--fit sample theoretical distribution.Detecting data drift shifts distributions time.Validating simulation outputs comparing real-world data.Example 1: Continuous DistributionsThis compares CDFs two samples. p-value indicates whether null hypothesis (samples come distribution) can rejected.Example 2: Discrete Data Bootstrapped KS TestFor discrete data, bootstrapped version KS test often used bypass continuity requirement.method performs bootstrapped version KS test, suitable discrete data. p-value indicates whether null hypothesis (samples come distribution) can rejected.Example 3: Comparing Multiple Distributions KL Divergence (Optional Enhancement)wish extend analysis include divergence measures like KL divergence, use following:calculates KL divergence pairs distributions list, offering additional insights relationships distributions.","code":"\n# Load necessary libraries\nlibrary(stats)\n\n# Generate two sample distributions\nset.seed(1)\nsample_1 <- rnorm(100)        # Sample from a standard normal distribution\nsample_2 <- rnorm(100, mean = 1)  # Sample with mean shifted to 1\n\n# Perform Kolmogorov-Smirnov test\nks_test_result <- ks.test(sample_1, sample_2)\nprint(ks_test_result)\n#> \n#>  Asymptotic two-sample Kolmogorov-Smirnov test\n#> \n#> data:  sample_1 and sample_2\n#> D = 0.36, p-value = 4.705e-06\n#> alternative hypothesis: two-sided\nlibrary(Matching)\n\n# Define two discrete samples\ndiscrete_sample_1 <- c(0:10)\ndiscrete_sample_2 <- c(0:10)\n\n# Perform bootstrapped KS test\nks_boot_result <- ks.boot(Tr = discrete_sample_1, Co = discrete_sample_2)\nprint(ks_boot_result)\n#> $ks.boot.pvalue\n#> [1] 1\n#> \n#> $ks\n#> \n#>  Exact two-sample Kolmogorov-Smirnov test\n#> \n#> data:  Tr and Co\n#> D = 0, p-value = 1\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $nboots\n#> [1] 1000\n#> \n#> attr(,\"class\")\n#> [1] \"ks.boot\"\nlibrary(entropy)\nlibrary(tidyverse)\n\n# Define multiple samples\nlst <- list(sample_1 = c(1:20), sample_2 = c(2:30), sample_3 = c(3:30))\n\n# Compute KL divergence between all pairs of distributions\nresult <- expand.grid(1:length(lst), 1:length(lst)) %>%\n    rowwise() %>%\n    mutate(KL = KL.empirical(lst[[Var1]], lst[[Var2]]))\n\nprint(result)\n#> # A tibble: 9 × 3\n#> # Rowwise: \n#>    Var1  Var2     KL\n#>   <int> <int>  <dbl>\n#> 1     1     1 0     \n#> 2     2     1 0.150 \n#> 3     3     1 0.183 \n#> 4     1     2 0.704 \n#> 5     2     2 0     \n#> 6     3     2 0.0679\n#> 7     1     3 0.622 \n#> 8     2     3 0.0870\n#> 9     3     3 0"},{"path":"basic-statistical-inference.html","id":"anderson-darling-test-1","chapter":"4 Basic Statistical Inference","heading":"4.6.2 Anderson-Darling Test","text":"Anderson-Darling (AD) test goodness--fit test evaluates whether sample data comes specific distribution. enhancement Kolmogorov-Smirnov test, greater sensitivity deviations tails distribution.Anderson-Darling test statistic defined :\\[\n^2 = -n - \\frac{1}{n} \\sum_{=1}^n \\left[ (2i - 1) \\left( \\log F(Y_i) + \\log(1 - F(Y_{n+1-})) \\right) \\right]\n\\]:\\(n\\) sample size.\\(n\\) sample size.\\(F\\) cumulative distribution function (CDF) theoretical distribution tested.\\(F\\) cumulative distribution function (CDF) theoretical distribution tested.\\(Y_i\\) ordered sample values.\\(Y_i\\) ordered sample values.AD test modifies basic framework KS test giving weight tails distribution, making particularly sensitive tail discrepancies.HypothesesNull Hypothesis (\\(H_0\\)): sample data follows specified distribution.Alternative Hypothesis (\\(H_1\\)): sample data follow specified distribution.Key PropertiesTail Sensitivity: Unlike Kolmogorov-Smirnov test, Anderson-Darling test emphasizes discrepancies tails distribution.Distribution-Specific Critical Values: AD test provides critical values tailored specific distribution tested (e.g., normal, exponential).Anderson-Darling test commonly used :Testing goodness--fit sample theoretical distributions normal, exponential, uniform.Evaluating appropriateness parametric models hypothesis testing.Assessing distributional assumptions quality control reliability analysis.Example: Testing Normality Anderson-Darling TestIf p-value chosen significance level (e.g., 0.05), null hypothesis data normally distributed rejected.Example: Comparing Two Empirical DistributionsThe AD test can also applied compare two empirical distributions using resampling techniques.evaluates whether two empirical distributions differ significantly.","code":"\nlibrary(nortest)\n\n# Generate a sample from a normal distribution\nset.seed(1)\nsample_data <- rnorm(100, mean = 0, sd = 1)\n\n# Perform the Anderson-Darling test for normality\nad_test_result <- ad.test(sample_data)\nprint(ad_test_result)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  sample_data\n#> A = 0.16021, p-value = 0.9471\n# Define two samples\nset.seed(1)\nsample_1 <- rnorm(100, mean = 0, sd = 1)\nsample_2 <- rnorm(100, mean = 1, sd = 1)\n\n# Perform resampling-based Anderson-Darling test (custom implementation or packages like twosamples)\nlibrary(twosamples)\nad_test_result_empirical <- ad_test(sample_1, sample_2)\nprint(ad_test_result_empirical)\n#>  Test Stat    P-Value \n#> 6796.70454    0.00025"},{"path":"basic-statistical-inference.html","id":"chi-square-goodness-of-fit-test","chapter":"4 Basic Statistical Inference","heading":"4.6.3 Chi-Square Goodness-of-Fit Test","text":"Chi-Square Goodness--Fit Test non-parametric statistical test used evaluate whether sample data set comes population specific distribution. compares observed frequencies expected frequencies hypothesized distribution.Null Hypothesis (\\(H_0\\)): data follow specified distribution.Alternative Hypothesis (\\(H_a\\)): data follow specified distribution.Chi-Square test statistic computed :\\[\n\\chi^2 = \\sum_{=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\\]:\\(O_i\\): Observed frequency category \\(\\).\\(O_i\\): Observed frequency category \\(\\).\\(E_i\\): Expected frequency category \\(\\).\\(E_i\\): Expected frequency category \\(\\).\\(k\\): Number categories.\\(k\\): Number categories.test statistic follows Chi-Square distribution degrees freedom:\\[\n\\nu = k - 1 - p\n\\]\\(p\\) number parameters estimated data.Assumptions TestRandom Sampling: sample data drawn randomly population.Minimum Expected Frequency: expected frequencies \\(E_i\\) sufficiently large (typically \\(E_i \\geq 5\\)).Independence: Observations sample independent .Decision RuleCompute test statistic \\(\\chi^2\\) using observed expected frequencies.Determine critical value \\(\\chi^2_{\\alpha, \\nu}\\) chosen significance level \\(\\alpha\\) degrees freedom \\(\\nu\\).Compare \\(\\chi^2\\) \\(\\chi^2_{\\alpha, \\nu}\\):\nReject \\(H_0\\) \\(\\chi^2 > \\chi^2_{\\alpha, \\nu}\\).\nAlternatively, use p-value approach:\nReject \\(H_0\\) \\(p \\leq \\alpha\\).\nFail reject \\(H_0\\) \\(p > \\alpha\\).\n\nReject \\(H_0\\) \\(\\chi^2 > \\chi^2_{\\alpha, \\nu}\\).Alternatively, use p-value approach:\nReject \\(H_0\\) \\(p \\leq \\alpha\\).\nFail reject \\(H_0\\) \\(p > \\alpha\\).\nReject \\(H_0\\) \\(p \\leq \\alpha\\).Fail reject \\(H_0\\) \\(p > \\alpha\\).Steps Chi-Square Goodness--Fit TestDefine expected frequencies based hypothesized distribution.Compute observed frequencies data.Calculate test statistic \\(\\chi^2\\).Determine degrees freedom \\(\\nu\\).Compare \\(\\chi^2\\) critical value use p-value decision-making.Example: Testing Fair DieSuppose testing whether six-sided die fair. die rolled 60 times, observed frequencies outcomes :Observed Frequencies: \\([10, 12, 8, 11, 9, 10]\\)Expected Frequencies: fair die equal probability face, \\(E_i = 60 / 6 = 10\\) face.Example: Testing Loaded DieFor die unequal probabilities (e.g., loaded die), expected probabilities defined explicitly:Limitations Chi-Square TestMinimum Expected Frequency: \\(E_i < 5\\) category, test may lose power. Consider merging categories meet criterion.Minimum Expected Frequency: \\(E_i < 5\\) category, test may lose power. Consider merging categories meet criterion.Independence: Assumes observations independent. Violations assumption can invalidate test.Independence: Assumes observations independent. Violations assumption can invalidate test.Sample Size Sensitivity: Large sample sizes may result significant \\(\\chi\\^2\\) values even minor deviations expected distribution.Sample Size Sensitivity: Large sample sizes may result significant \\(\\chi\\^2\\) values even minor deviations expected distribution.Chi-Square Goodness--Fit Test versatile tool evaluating fit observed data hypothesized distribution, widely used fields like quality control, genetics, market research.","code":"\n# Observed frequencies\nobserved <- c(10, 12, 8, 11, 9, 10)\n\n# Expected frequencies under a fair die\nexpected <- rep(10, 6)\n\n# Perform Chi-Square Goodness-of-Fit Test\nchisq_test <- chisq.test(x = observed, p = expected / sum(expected))\n\n# Display results\nchisq_test\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 1, df = 5, p-value = 0.9626\n# Observed frequencies\nobserved <- c(10, 12, 8, 11, 9, 10)\n\n# Expected probabilities (e.g., for a loaded die)\nprobabilities <- c(0.1, 0.2, 0.3, 0.1, 0.2, 0.1)\n\n# Expected frequencies\nexpected <- probabilities * sum(observed)\n\n# Perform Chi-Square Goodness-of-Fit Test\nchisq_test_loaded <- chisq.test(x = observed, p = probabilities)\n\n# Display results\nchisq_test_loaded\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 15.806, df = 5, p-value = 0.007422"},{"path":"basic-statistical-inference.html","id":"cramér-von-mises-test","chapter":"4 Basic Statistical Inference","heading":"4.6.4 Cramér-von Mises Test","text":"Cramér-von Mises (CvM) Test goodness--fit test evaluates whether sample data set comes specified distribution. Similar Kolmogorov-Smirnov Test (KS) Anderson-Darling Test (AD), assesses discrepancy empirical theoretical cumulative distribution functions (CDFs). However, CvM test equal sensitivity across entire distribution, unlike KS test (focused maximum difference) AD test (emphasizing tails).Cramér-von Mises test statistic defined :\\[\nW^2 = n \\int_{-\\infty}^{\\infty} \\left( F_n(x) - F(x) \\right)^2 dF(x)\n\\]:\\(n\\) sample size.\\(n\\) sample size.\\(F_n(x)\\) empirical cumulative distribution function (ECDF) sample.\\(F_n(x)\\) empirical cumulative distribution function (ECDF) sample.\\(F(x)\\) CDF specified theoretical distribution.\\(F(x)\\) CDF specified theoretical distribution.practical implementation, test statistic often computed :\\[\nW^2 = \\sum_{=1}^n \\left[ F(X_i) - \\frac{2i - 1}{2n} \\right]^2 + \\frac{1}{12n}\n\\]\\(X_i\\) ordered sample values.HypothesesNull Hypothesis (\\(H_0\\)): sample data follow specified distribution.Alternative Hypothesis (\\(H_a\\)): sample data follow specified distribution.Key PropertiesEqual Sensitivity:\nCvM test gives equal weight discrepancies across parts distribution, unlike AD test, emphasizes tails.\nCvM test gives equal weight discrepancies across parts distribution, unlike AD test, emphasizes tails.Non-parametric:\ntest makes strong parametric assumptions data, aside specified distribution.\ntest makes strong parametric assumptions data, aside specified distribution.Complementary KS AD Tests:\nKS test focuses maximum distance CDFs AD test emphasizes tails, CvM test provides balanced sensitivity across entire range distribution.\nKS test focuses maximum distance CDFs AD test emphasizes tails, CvM test provides balanced sensitivity across entire range distribution.Cramér-von Mises test widely used :Goodness--Fit Testing: Assessing whether data follow specified theoretical distribution (e.g., normal, exponential).Model Validation: Evaluating fit probabilistic models statistical machine learning contexts.Complementary Testing: Used alongside KS AD tests comprehensive analysis distributional assumptions.Example 1: Testing NormalityThe test evaluates whether sample data follow normal distribution.Example 2: Goodness--Fit Custom DistributionsFor distributions normal, can use resampling techniques custom implementations. ’s pseudo-implementation custom theoretical distribution:demonstrates custom calculation CvM statistic testing goodness--fit exponential distribution.Normality Test:\ncvm.test function evaluates whether sample data follow normal distribution.\nsmall p-value indicates significant deviation normality.\nNormality Test:cvm.test function evaluates whether sample data follow normal distribution.cvm.test function evaluates whether sample data follow normal distribution.small p-value indicates significant deviation normality.small p-value indicates significant deviation normality.Custom Goodness--Fit:\nCustom implementation allows testing distributions normal.\nstatistic measures squared differences empirical theoretical CDFs.\nCustom Goodness--Fit:Custom implementation allows testing distributions normal.Custom implementation allows testing distributions normal.statistic measures squared differences empirical theoretical CDFs.statistic measures squared differences empirical theoretical CDFs.Advantages LimitationsAdvantages:\nBalanced sensitivity across entire distribution.\nComplements KS AD tests providing different perspective goodness--fit.\nAdvantages:Balanced sensitivity across entire distribution.Balanced sensitivity across entire distribution.Complements KS AD tests providing different perspective goodness--fit.Complements KS AD tests providing different perspective goodness--fit.Limitations:\nCritical values distribution-specific.\ntest may less sensitive tail deviations compared AD test.\nLimitations:Critical values distribution-specific.Critical values distribution-specific.test may less sensitive tail deviations compared AD test.test may less sensitive tail deviations compared AD test.Cramér-von Mises test robust versatile goodness--fit test, offering balanced sensitivity across entire distribution. complementarity KS AD tests makes essential tool validating distributional assumptions theoretical applied contexts.","code":"\nlibrary(nortest)\n\n# Generate a sample from a normal distribution\nset.seed(1)\nsample_data <- rnorm(100, mean = 0, sd = 1)\n\n# Perform the Cramér-von Mises test for normality\ncvm_test_result <- cvm.test(sample_data)\nprint(cvm_test_result)\n#> \n#>  Cramer-von Mises normality test\n#> \n#> data:  sample_data\n#> W = 0.026031, p-value = 0.8945\n# Custom ECDF and theoretical CDF comparison\nset.seed(1)\nsample_data <-\n    rexp(100, rate = 1)  # Sample from exponential distribution\ntheoretical_cdf <-\n    function(x) {\n        pexp(x, rate = 1)\n    }  # Exponential CDF\n\n# Compute empirical CDF\nempirical_cdf <- ecdf(sample_data)\n\n# Compute CvM statistic\ncvm_statistic <-\n    sum((empirical_cdf(sample_data) - theoretical_cdf(sample_data)) ^ 2) / length(sample_data)\nprint(paste(\"Cramér-von Mises Statistic (Custom):\", round(cvm_statistic, 4)))\n#> [1] \"Cramér-von Mises Statistic (Custom): 0.0019\""},{"path":"basic-statistical-inference.html","id":"kullback-leibler-divergence","chapter":"4 Basic Statistical Inference","heading":"4.6.5 Kullback-Leibler Divergence","text":"Kullback-Leibler (KL) divergence, also known relative entropy, measure used quantify similarity two probability distributions. plays critical role statistical inference, machine learning, information theory. However, KL divergence true metric satisfy triangle inequality.Key Properties KL DivergenceNot Metric: KL divergence fails meet triangle inequality requirement, symmetric, meaning: \\[\nD_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)\n\\]Metric: KL divergence fails meet triangle inequality requirement, symmetric, meaning: \\[\nD_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)\n\\]Generalization Multivariate Case: KL divergence can extended multivariate distributions, making flexible complex analyses.Generalization Multivariate Case: KL divergence can extended multivariate distributions, making flexible complex analyses.Quantifies Information Loss: measures “information loss” approximating true distribution \\(P\\) predicted distribution \\(Q\\). Thus, smaller values indicate closer similarity distributions.Quantifies Information Loss: measures “information loss” approximating true distribution \\(P\\) predicted distribution \\(Q\\). Thus, smaller values indicate closer similarity distributions.Mathematical DefinitionsKL divergence defined differently discrete continuous distributions.1. Discrete Case\ntwo discrete probability distributions \\(P = \\{P_i\\}\\) \\(Q = \\{Q_i\\}\\), KL divergence given : \\[\nD_{KL}(P \\| Q) = \\sum_i P_i \\log\\left(\\frac{P_i}{Q_i}\\right)\n\\]2. Continuous Case\ncontinuous probability density functions \\(P(x)\\) \\(Q(x)\\): \\[\nD_{KL}(P \\| Q) = \\int P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right) dx\n\\]Range: \\[\nD_{KL}(P \\| Q) \\[0, \\infty)\n\\]\n\\(D_{KL} = 0\\) indicates identical distributions (\\(P = Q\\)).\nLarger values indicate greater dissimilarity \\(P\\) \\(Q\\).\n\\(D_{KL} = 0\\) indicates identical distributions (\\(P = Q\\)).Larger values indicate greater dissimilarity \\(P\\) \\(Q\\).Non-Symmetric Nature: noted, \\(D_{KL}(P \\| Q)\\) \\(D_{KL}(Q \\| P)\\) equal, emphasizing directed nature.Insights:Continuous case uses normalized probability values explicitly provided.Discrete case relies empirical estimation probabilities counts.Observe KL divergence quantifies “distance” two distributions.","code":"\nlibrary(philentropy)\n\n# Example 1: Continuous case\n# Define two continuous probability distributions with distinct patterns\nX_continuous <- c(0.1, 0.2, 0.3, 0.4)  # Normalized to sum to 1\nY_continuous <- c(0.4, 0.3, 0.2, 0.1)  # Normalized to sum to 1\n\n# Calculate KL divergence (logarithm base 2)\nKL_continuous <- KL(rbind(X_continuous, Y_continuous), unit = \"log2\")\nprint(paste(\"KL divergence (continuous):\", round(KL_continuous, 2)))\n#> [1] \"KL divergence (continuous): 0.66\"\n\n# Example 2: Discrete case\n# Define two discrete probability distributions\nX_discrete <- c(5, 10, 15, 20)  # Counts for events\nY_discrete <- c(20, 15, 10, 5)  # Counts for events\n\n# Estimate probabilities empirically and compute KL divergence\nKL_discrete <- KL(rbind(X_discrete, Y_discrete), est.prob = \"empirical\")\nprint(paste(\"KL divergence (discrete):\", round(KL_discrete, 2)))\n#> [1] \"KL divergence (discrete): 0.66\""},{"path":"basic-statistical-inference.html","id":"jensen-shannon-divergence","chapter":"4 Basic Statistical Inference","heading":"4.6.6 Jensen-Shannon Divergence","text":"Jensen-Shannon (JS) divergence symmetric bounded measure similarity two probability distributions. derived Kullback-Leibler Divergence (KL) addresses asymmetry unboundedness incorporating mixed distribution.Jensen-Shannon divergence defined : \\[\nD_{JS}(P \\| Q) = \\frac{1}{2} \\left( D_{KL}(P \\| M) + D_{KL}(Q \\| M) \\right)\n\\] :\\(M = \\frac{1}{2}(P + Q)\\) mixed distribution, representing average \\(P\\) \\(Q\\).\\(M = \\frac{1}{2}(P + Q)\\) mixed distribution, representing average \\(P\\) \\(Q\\).\\(D_{KL}\\) Kullback-Leibler divergence.\\(D_{KL}\\) Kullback-Leibler divergence.Key PropertiesSymmetry: Unlike KL divergence, JS divergence symmetric: \\[\nD_{JS}(P \\| Q) = D_{JS}(Q \\| P)\n\\]Symmetry: Unlike KL divergence, JS divergence symmetric: \\[\nD_{JS}(P \\| Q) = D_{JS}(Q \\| P)\n\\]Boundedness:\nbase-2 logarithms: \\[\nD_{JS} \\[0, 1]\n\\]\nnatural logarithms (base-\\(e\\)): \\[\nD_{JS} \\[0, \\ln(2)]\n\\]\nBoundedness:base-2 logarithms: \\[\nD_{JS} \\[0, 1]\n\\]natural logarithms (base-\\(e\\)): \\[\nD_{JS} \\[0, \\ln(2)]\n\\]Interpretability: JS divergence measures average information gain moving mixed distribution \\(M\\) either \\(P\\) \\(Q\\). bounded nature makes easier compare across datasets.Interpretability: JS divergence measures average information gain moving mixed distribution \\(M\\) either \\(P\\) \\(Q\\). bounded nature makes easier compare across datasets.","code":"\n# Load the required library\nlibrary(philentropy)\n\n# Example 1: Continuous case\n# Define two continuous distributions\nX_continuous <- 1:10  # Continuous sequence\nY_continuous <- 1:20  # Continuous sequence\n\n# Compute JS divergence (logarithm base 2)\nJS_continuous <- JSD(rbind(X_continuous, Y_continuous), unit = \"log2\")\nprint(paste(\"JS divergence (continuous):\", round(JS_continuous, 2)))\n#> [1] \"JS divergence (continuous): 20.03\"\n\n# X_continuous and Y_continuous represent continuous distributions.\n# The mixed distribution (M) is computed internally as the average of the two distributions.\n\n# Example 2: Discrete case\n# Define two discrete distributions\nX_discrete <- c(5, 10, 15, 20)  # Observed counts for events\nY_discrete <- c(20, 15, 10, 5)  # Observed counts for events\n\n# Compute JS divergence with empirical probability estimation\nJS_discrete <- JSD(rbind(X_discrete, Y_discrete), est.prob = \"empirical\")\nprint(paste(\"JS divergence (discrete):\", round(JS_discrete, 2)))\n#> [1] \"JS divergence (discrete): 0.15\"\n\n# X_discrete and Y_discrete represent event counts.\n# Probabilities are estimated empirically before calculating the divergence."},{"path":"basic-statistical-inference.html","id":"hellinger-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.7 Hellinger Distance","text":"Hellinger distance bounded symmetric measure similarity two probability distributions. widely used statistics machine learning quantify “close” two distributions , values ranging 0 (identical distributions) 1 (completely disjoint distributions).Mathematical DefinitionThe Hellinger distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nH(P, Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_x \\left(\\sqrt{P(x)} - \\sqrt{Q(x)}\\right)^2}\n\\]:\\(P(x)\\) \\(Q(x)\\) probability densities probabilities point \\(x\\) distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probability densities probabilities point \\(x\\) distributions \\(P\\) \\(Q\\).term \\(\\sqrt{P(x)}\\) square root probabilities, emphasizing geometric comparisons distributions.term \\(\\sqrt{P(x)}\\) square root probabilities, emphasizing geometric comparisons distributions.Alternatively, continuous distributions, Hellinger distance can expressed :\\[\nH(P, Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\int \\left(\\sqrt{P(x)} - \\sqrt{Q(x)}\\right)^2 dx}\n\\]Key PropertiesSymmetry: \\[\nH(P, Q) = H(Q, P)\n\\] distance symmetric, unlike Kullback-Leibler divergence.Symmetry: \\[\nH(P, Q) = H(Q, P)\n\\] distance symmetric, unlike Kullback-Leibler divergence.Boundedness: \\[\nH(P, Q) \\[0, 1]\n\\]\n\\(H = 0\\): distributions identical (\\(P(x) = Q(x)\\) \\(x\\)).\n\\(H = 1\\): distributions overlap (\\(P(x) \\neq Q(x)\\)).\nBoundedness: \\[\nH(P, Q) \\[0, 1]\n\\]\\(H = 0\\): distributions identical (\\(P(x) = Q(x)\\) \\(x\\)).\\(H = 1\\): distributions overlap (\\(P(x) \\neq Q(x)\\)).Interpretability:\nHellinger distance provides scale-invariant measure, making suitable comparing distributions various contexts.\nInterpretability:Hellinger distance provides scale-invariant measure, making suitable comparing distributions various contexts.Hellinger distance widely used :Hypothesis Testing: Comparing empirical distributions theoretical models.Machine Learning: Feature selection, classification, clustering tasks.Bayesian Analysis: Quantifying differences prior posterior distributions.Economics Ecology: Measuring dissimilarity distributions like income, species abundance, geographical data.","code":"\nlibrary(philentropy)\n\n# Example 1: Compute Hellinger Distance for Discrete Distributions\n# Define two discrete distributions as probabilities\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Hellinger distance\nhellinger_discrete <- distance(rbind(P_discrete, Q_discrete), method = \"hellinger\")\nprint(paste(\"Hellinger Distance (Discrete):\", round(hellinger_discrete, 4)))\n#> [1] \"Hellinger Distance (Discrete): 0.465\"\n\n# Example 2: Compute Hellinger Distance for Empirical Distributions\n# Define two empirical distributions (counts)\nP_empirical <- c(10, 20, 30, 40)  # Counts for distribution P\nQ_empirical <- c(30, 30, 20, 20)  # Counts for distribution Q\n\n# Normalize counts to probabilities\nP_normalized <- P_empirical / sum(P_empirical)\nQ_normalized <- Q_empirical / sum(Q_empirical)\n\n# Compute Hellinger distance\nhellinger_empirical <- distance(rbind(P_normalized, Q_normalized), method = \"hellinger\")\nprint(paste(\"Hellinger Distance (Empirical):\", round(hellinger_empirical, 4)))\n#> [1] \"Hellinger Distance (Empirical): 0.465\""},{"path":"basic-statistical-inference.html","id":"bhattacharyya-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.8 Bhattacharyya Distance","text":"Bhattacharyya Distance statistical measure used quantify similarity overlap two probability distributions. commonly used pattern recognition, signal processing, statistics evaluate closely related two distributions . Bhattacharyya distance particularly effective comparing discrete continuous distributions.Bhattacharyya distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nD_B(P, Q) = -\\ln \\left( \\sum_x \\sqrt{P(x) Q(x)} \\right)\n\\]continuous distributions, Bhattacharyya distance expressed :\\[\nD_B(P, Q) = -\\ln \\left( \\int \\sqrt{P(x) Q(x)} dx \\right)\n\\]:\\(P(x)\\) \\(Q(x)\\) probability densities probabilities distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probability densities probabilities distributions \\(P\\) \\(Q\\).term \\(\\int \\sqrt{P(x) Q(x)} dx\\) known Bhattacharyya coefficient.term \\(\\int \\sqrt{P(x) Q(x)} dx\\) known Bhattacharyya coefficient.Key PropertiesSymmetry: \\[\nD_B(P, Q) = D_B(Q, P)\n\\]Symmetry: \\[\nD_B(P, Q) = D_B(Q, P)\n\\]Range: \\[\nD_B(P, Q) \\[0, \\infty)\n\\]\n\\(D_B = 0\\): distributions identical (\\(P = Q\\)).\nLarger values indicate less overlap greater dissimilarity \\(P\\) \\(Q\\).\nRange: \\[\nD_B(P, Q) \\[0, \\infty)\n\\]\\(D_B = 0\\): distributions identical (\\(P = Q\\)).Larger values indicate less overlap greater dissimilarity \\(P\\) \\(Q\\).Relation Hellinger Distance:\nBhattacharyya coefficient related Hellinger distance: \\[\nH(P, Q) = \\sqrt{1 - \\sum_x \\sqrt{P(x) Q(x)}}\n\\]\nRelation Hellinger Distance:Bhattacharyya coefficient related Hellinger distance: \\[\nH(P, Q) = \\sqrt{1 - \\sum_x \\sqrt{P(x) Q(x)}}\n\\]Bhattacharyya distance widely used :Classification: Measuring similarity feature distributions machine learning.Hypothesis Testing: Evaluating closeness observed data theoretical model.Image Processing: Comparing pixel intensity distributions color histograms.Economics Ecology: Assessing similarity income distributions species abundance.Example 1: Discrete DistributionsA smaller Bhattacharyya distance indicates greater similarity two distributions.Example 2: Continuous Distributions (Approximation)continuous distributions, Bhattacharyya distance can approximated using numerical integration discretization.Continuous distributions discretized histograms compute Bhattacharyya coefficient distance.Discrete Case:\nBhattacharyya coefficient quantifies overlap \\(P\\) \\(Q\\).\nBhattacharyya distance translates overlap logarithmic measure dissimilarity.\nBhattacharyya coefficient quantifies overlap \\(P\\) \\(Q\\).Bhattacharyya distance translates overlap logarithmic measure dissimilarity.Continuous Case:\nDistributions discretized histograms approximate Bhattacharyya coefficient distance.\nDistributions discretized histograms approximate Bhattacharyya coefficient distance.","code":"\n# Define two discrete probability distributions\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Bhattacharyya coefficient\nbhattacharyya_coefficient <- sum(sqrt(P_discrete * Q_discrete))\n\n# Compute Bhattacharyya distance\nbhattacharyya_distance <- -log(bhattacharyya_coefficient)\n\n# Display results\nprint(paste(\n    \"Bhattacharyya Coefficient:\",\n    round(bhattacharyya_coefficient, 4)\n))\n#> [1] \"Bhattacharyya Coefficient: 0.9459\"\nprint(paste(\n    \"Bhattacharyya Distance (Discrete):\",\n    round(bhattacharyya_distance, 4)\n))\n#> [1] \"Bhattacharyya Distance (Discrete): 0.0556\"\n# Generate two continuous distributions\nset.seed(1)\nP_continuous <-\n    rnorm(1000, mean = 0, sd = 1)  # Standard normal distribution\nQ_continuous <-\n    rnorm(1000, mean = 1, sd = 1)  # Normal distribution with mean 1\n\n# Create histograms to approximate probabilities\nhist_P <- hist(P_continuous, breaks = 50, plot = FALSE)\nhist_Q <- hist(Q_continuous, breaks = 50, plot = FALSE)\n\n# Normalize histograms to probabilities\nprob_P <- hist_P$counts / sum(hist_P$counts)\nprob_Q <- hist_Q$counts / sum(hist_Q$counts)\n\n# Compute Bhattacharyya coefficient\nbhattacharyya_coefficient_continuous <- sum(sqrt(prob_P * prob_Q))\n\n# Compute Bhattacharyya distance\nbhattacharyya_distance_continuous <-\n    -log(bhattacharyya_coefficient_continuous)\n\n# Display results\nprint(paste(\n    \"Bhattacharyya Coefficient (Continuous):\",\n    round(bhattacharyya_coefficient_continuous, 4)\n))\n#> [1] \"Bhattacharyya Coefficient (Continuous): 0.9823\"\nprint(paste(\n    \"Bhattacharyya Distance (Continuous Approximation):\",\n    round(bhattacharyya_distance_continuous, 4)\n))\n#> [1] \"Bhattacharyya Distance (Continuous Approximation): 0.0178\""},{"path":"basic-statistical-inference.html","id":"wasserstein-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.9 Wasserstein Distance","text":"Wasserstein distance, also known Earth Mover’s Distance (EMD), measure similarity two probability distributions. quantifies “cost” transforming one distribution another, making particularly suitable continuous data applications geometry data matters.Mathematical DefinitionThe Wasserstein distance two probability distributions \\(P\\) \\(Q\\) domain \\(\\mathcal{X}\\) defined :\\[\nW_p(P, Q) = \\left( \\int_{\\mathcal{X}} |F_P(x) - F_Q(x)|^p dx \\right)^{\\frac{1}{p}}\n\\]:\\(F_P(x)\\) \\(F_Q(x)\\) cumulative distribution functions (CDFs) \\(P\\) \\(Q\\).\\(F_P(x)\\) \\(F_Q(x)\\) cumulative distribution functions (CDFs) \\(P\\) \\(Q\\).\\(p \\geq 1\\) order Wasserstein distance (commonly \\(p = 1\\)).\\(p \\geq 1\\) order Wasserstein distance (commonly \\(p = 1\\)).\\(|\\cdot|^p\\) absolute difference raised power \\(p\\).\\(|\\cdot|^p\\) absolute difference raised power \\(p\\).case \\(p = 1\\), formula simplifies :\\[\nW_1(P, Q) = \\int_{\\mathcal{X}} |F_P(x) - F_Q(x)| dx\n\\]represents minimum “cost” transforming distribution \\(P\\) \\(Q\\), cost proportional distance “unit mass” must move.Key PropertiesInterpretability: Represents “effort” required morph one distribution another.Metric: Wasserstein distance satisfies properties metric, including symmetry, non-negativity, triangle inequality.Flexibility: Can handle empirical continuous distributions.Wasserstein distance widely used various fields, including:Machine Learning:\nTraining generative models Wasserstein GANs.\nMonitoring data drift online systems.\nTraining generative models Wasserstein GANs.Monitoring data drift online systems.Statistics:\nComparing empirical distributions derived observed data.\nRobustness testing distributional shifts.\nComparing empirical distributions derived observed data.Robustness testing distributional shifts.Economics:\nQuantifying disparities income wealth distributions.\nQuantifying disparities income wealth distributions.Image Processing:\nMeasuring structural differences image distributions.\nMeasuring structural differences image distributions.","code":"\nlibrary(transport)\nlibrary(twosamples)\n\n# Example 1: Compute Wasserstein Distance (1D case)\nset.seed(1)\ndist_1 <- rnorm(100)               # Generate a sample from a standard normal distribution\ndist_2 <- rnorm(100, mean = 1)     # Generate a sample with mean shifted to 1\n\n# Calculate the Wasserstein distance\nwass_distance <- wasserstein1d(dist_1, dist_2)\nprint(paste(\"1D Wasserstein Distance:\", round(wass_distance, 4)))\n#> [1] \"1D Wasserstein Distance: 0.8533\"\n\n# Example 2: Wasserstein Metric as a Statistic\nset.seed(1)\nwass_stat_value <- wass_stat(dist_1, dist_2)\nprint(paste(\"Wasserstein Statistic:\", round(wass_stat_value, 4)))\n#> [1] \"Wasserstein Statistic: 0.8533\"\n\n# Example 3: Wasserstein Test (Permutation-based Two-sample Test)\nset.seed(1)\nwass_test_result <- wass_test(dist_1, dist_2)\nprint(wass_test_result)\n#> Test Stat   P-Value \n#> 0.8533046 0.0002500\n\n# - Example 1 calculates the simple Wasserstein distance between two distributions.\n# - Example 2 computes the Wasserstein distance as a statistical metric.\n# - Example 3 performs a permutation-based two-sample test using the Wasserstein metric."},{"path":"basic-statistical-inference.html","id":"energy-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.10 Energy Distance","text":"Energy Distance statistical metric used quantify similarity two probability distributions. particularly effective comparing multi-dimensional distributions.Energy Distance two distributions \\(P\\) \\(Q\\) defined :\\[\nE(P, Q) = 2 \\mathbb{E}[||X - Y||] - \\mathbb{E}[||X - X'||] - \\mathbb{E}[||Y - Y'||]\n\\]:\\(X\\) \\(X'\\) independent identically distributed (..d.) random variables \\(P\\).\\(X\\) \\(X'\\) independent identically distributed (..d.) random variables \\(P\\).\\(Y\\) \\(Y'\\) ..d. random variables \\(Q\\).\\(Y\\) \\(Y'\\) ..d. random variables \\(Q\\).\\(||\\cdot||\\) denotes Euclidean distance.\\(||\\cdot||\\) denotes Euclidean distance.Alternatively, empirical distributions, Energy Distance can approximated :\\[\nE(P, Q) = \\frac{2}{mn} \\sum_{=1}^m \\sum_{j=1}^n ||X_i - Y_j|| - \\frac{1}{m^2} \\sum_{=1}^m \\sum_{j=1}^m ||X_i - X_j|| - \\frac{1}{n^2} \\sum_{=1}^n \\sum_{j=1}^n ||Y_i - Y_j||\n\\]:\\(m\\) \\(n\\) sample sizes distributions \\(P\\) \\(Q\\) respectively.\\(m\\) \\(n\\) sample sizes distributions \\(P\\) \\(Q\\) respectively.\\(X_i\\) \\(Y_j\\) samples \\(P\\) \\(Q\\).\\(X_i\\) \\(Y_j\\) samples \\(P\\) \\(Q\\).Key PropertiesMetric:\nEnergy distance satisfies properties metric: symmetry, non-negativity, triangle inequality.\nEnergy distance satisfies properties metric: symmetry, non-negativity, triangle inequality.Range: \\[\nE(P, Q) \\geq 0\n\\]\n\\(E(P, Q) = 0\\): distributions identical.\nLarger values indicate greater dissimilarity.\n\\(E(P, Q) = 0\\): distributions identical.Larger values indicate greater dissimilarity.Effectiveness Multi-dimensional Data:\nEnergy distance designed work well higher-dimensional spaces, unlike traditional metrics.\nEnergy distance designed work well higher-dimensional spaces, unlike traditional metrics.Energy Distance widely used :Hypothesis Testing: Testing whether two distributions .Energy Test equality distributions.Clustering: Measuring dissimilarity clusters multi-dimensional data.Feature Selection: Comparing distributions features across different classes evaluate discriminative power.Example 1: Comparing Two DistributionsThis calculates energy distance two multi-dimensional distributions.Example 2: Energy Test Equality DistributionsThe energy test evaluates null hypothesis two distributions identical.Energy Distance:\nProvides single metric quantify dissimilarity two distributions, considering dimensions data.\nEnergy Distance:Provides single metric quantify dissimilarity two distributions, considering dimensions data.Energy Test:\nTests equality distributions using Energy Distance.\np-value indicates whether distributions significantly different.\nEnergy Test:Tests equality distributions using Energy Distance.Tests equality distributions using Energy Distance.p-value indicates whether distributions significantly different.p-value indicates whether distributions significantly different.Advantages Energy DistanceMulti-dimensional Applicability:\nWorks seamlessly high-dimensional data, unlike divergence metrics may suffer dimensionality issues.\nMulti-dimensional Applicability:Works seamlessly high-dimensional data, unlike divergence metrics may suffer dimensionality issues.Non-parametric:\nMakes assumptions form distributions.\nNon-parametric:Makes assumptions form distributions.Robustness:\nEffective even complex data structures.\nRobustness:Effective even complex data structures.","code":"\n# Load the 'energy' package\nlibrary(energy)\n\n# Generate two sample distributions\nset.seed(1)\nX <- matrix(rnorm(1000, mean = 0, sd = 1), ncol = 2)  # Distribution P\nY <- matrix(rnorm(1000, mean = 1, sd = 1), ncol = 2)  # Distribution Q\n\n# Combine X and Y and create a group identifier\ncombined <- rbind(X, Y)\ngroups <- c(rep(1, nrow(X)), rep(2, nrow(Y)))\n\n# Compute Energy Distance\nenergy_dist <- edist(combined, sizes = table(groups))\n\n# Print the Energy Distance\nprint(paste(\"Energy Distance:\", round(energy_dist, 4)))\n#> [1] \"Energy Distance: 201.9202\"\n# Perform the Energy Test\nenergy_test <-\n    eqdist.etest(rbind(X, Y), sizes = c(nrow(X), nrow(Y)), R = 999)\nprint(energy_test)\n#> \n#>  Multivariate 2-sample E-test of equal distributions\n#> \n#> data:  sample sizes 500 500, replicates 999\n#> E-statistic = 201.92, p-value = 0.001"},{"path":"basic-statistical-inference.html","id":"total-variation-distance","chapter":"4 Basic Statistical Inference","heading":"4.6.11 Total Variation Distance","text":"Total Variation (TV) Distance measure maximum difference two probability distributions. widely used probability theory, statistics, machine learning quantify dissimilar two distributions .Total Variation Distance two probability distributions \\(P\\) \\(Q\\) defined :\\[\nD_{TV}(P, Q) = \\frac{1}{2} \\sum_x |P(x) - Q(x)|\n\\]:\\(P(x)\\) \\(Q(x)\\) probabilities assigned outcome \\(x\\) distributions \\(P\\) \\(Q\\).\\(P(x)\\) \\(Q(x)\\) probabilities assigned outcome \\(x\\) distributions \\(P\\) \\(Q\\).factor \\(\\frac{1}{2}\\) ensures distance lies within range \\([0, 1]\\).factor \\(\\frac{1}{2}\\) ensures distance lies within range \\([0, 1]\\).Alternatively, continuous distributions, TV distance can expressed :\\[\nD_{TV}(P, Q) = \\frac{1}{2} \\int |P(x) - Q(x)| dx\n\\]Key PropertiesRange: \\[\nD_{TV}(P, Q) \\[0, 1]\n\\]\n\\(D_{TV} = 0\\): distributions identical (\\(P = Q\\)).\n\\(D_{TV} = 1\\): distributions completely disjoint (overlap).\nRange: \\[\nD_{TV}(P, Q) \\[0, 1]\n\\]\\(D_{TV} = 0\\): distributions identical (\\(P = Q\\)).\\(D_{TV} = 1\\): distributions completely disjoint (overlap).Symmetry: \\[\nD_{TV}(P, Q) = D_{TV}(Q, P)\n\\]Symmetry: \\[\nD_{TV}(P, Q) = D_{TV}(Q, P)\n\\]Interpretability:\n\\(D_{TV}(P, Q)\\) represents maximum probability mass needs shifted transform \\(P\\) \\(Q\\).\nInterpretability:\\(D_{TV}(P, Q)\\) represents maximum probability mass needs shifted transform \\(P\\) \\(Q\\).Total Variation Distance used :Hypothesis Testing: Quantifying difference observed expected distributions.Machine Learning: Evaluating similarity predicted true distributions.Information Theory: Comparing distributions contexts like communication cryptography.Example 1: Discrete DistributionsThis calculates maximum difference two distributions, scaled lie 0 1.Example 2: Continuous Distributions (Approximation)continuous distributions, TV distance can approximated using discretization numerical integration. ’s example using random samples:continuous distributions discretized histograms, TV distance computed based resulting probabilities.Discrete Case:\nTV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.\nexample, highlights much \\(P\\) \\(Q\\) diverge.\nDiscrete Case:TV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.TV distance quantifies maximum difference \\(P\\) \\(Q\\) terms probability mass.example, highlights much \\(P\\) \\(Q\\) diverge.example, highlights much \\(P\\) \\(Q\\) diverge.Continuous Case:\ncontinuous distributions, TV distance approximated using discretized probabilities histograms.\napproach provides intuitive measure similarity large samples.\nContinuous Case:continuous distributions, TV distance approximated using discretized probabilities histograms.continuous distributions, TV distance approximated using discretized probabilities histograms.approach provides intuitive measure similarity large samples.approach provides intuitive measure similarity large samples.Total Variation Distance provides intuitive interpretable measure maximum difference two distributions. symmetry bounded nature make versatile tool comparing discrete continuous distributions.","code":"\n# Define two discrete probability distributions\nP_discrete <- c(0.1, 0.2, 0.3, 0.4)  # Normalized probabilities\nQ_discrete <- c(0.3, 0.3, 0.2, 0.2)  # Normalized probabilities\n\n# Compute Total Variation Distance\ntv_distance <- sum(abs(P_discrete - Q_discrete)) / 2\nprint(paste(\"Total Variation Distance (Discrete):\", round(tv_distance, 4)))\n#> [1] \"Total Variation Distance (Discrete): 0.3\"\n# Generate two continuous distributions\nset.seed(1)\nP_continuous <-\n    rnorm(1000, mean = 0, sd = 1)  # Standard normal distribution\nQ_continuous <-\n    rnorm(1000, mean = 1, sd = 1)  # Normal distribution with mean 1\n\n# Create histograms to approximate probabilities\nhist_P <- hist(P_continuous, breaks = 50, plot = FALSE)\nhist_Q <- hist(Q_continuous, breaks = 50, plot = FALSE)\n\n# Normalize histograms to probabilities\nprob_P <- hist_P$counts / sum(hist_P$counts)\nprob_Q <- hist_Q$counts / sum(hist_Q$counts)\n\n# Compute Total Variation Distance\ntv_distance_continuous <- sum(abs(prob_P - prob_Q)) / 2\nprint(paste(\n    \"Total Variation Distance (Continuous Approximation):\",\n    round(tv_distance_continuous, 4)\n))\n#> [1] \"Total Variation Distance (Continuous Approximation): 0.125\""},{"path":"basic-statistical-inference.html","id":"summary","chapter":"4 Basic Statistical Inference","heading":"4.6.12 Summary","text":"1. Tests Comparing Distributions2. Divergence Metrics3. Distance Metrics","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"5 Linear Regression","heading":"5 Linear Regression","text":"Linear regression one fundamental tools statistics econometrics, widely used modeling relationships variables. forms cornerstone predictive analysis, enabling us understand quantify changes one explanatory variables associated dependent variable. simplicity versatility make essential tool fields ranging economics marketing healthcare environmental studies.core, linear regression addresses questions associations rather causation. example:advertising expenditures associated sales performance?relationship company’s revenue stock price?level education correlate income?questions patterns data—necessarily causal effects. regression can provide insights potential causal relationships, establishing causality requires just regression analysis. requires careful consideration study design, assumptions, potential confounding factors., called “linear”? term refers structure model, dependent variable (outcome) modeled linear combination one independent variables (predictors). example, simple linear regression, relationship represented :\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\\(Y\\) dependent variable, \\(X\\) independent variable, \\(\\beta_0\\) \\(\\beta_1\\) parameters estimated, \\(\\epsilon\\) error term capturing randomness unobserved factors.Linear regression serves foundation much applied data analysis wide-ranging applications:Understanding Patterns Data: Regression provides framework summarize explore relationships variables. allows us identify patterns trends associations, can guide analysis decision-making.Understanding Patterns Data: Regression provides framework summarize explore relationships variables. allows us identify patterns trends associations, can guide analysis decision-making.Prediction: Beyond exploring relationships, regression widely used making predictions. instance, given historical data, can use regression model predict future outcomes like sales, prices, demand.Prediction: Beyond exploring relationships, regression widely used making predictions. instance, given historical data, can use regression model predict future outcomes like sales, prices, demand.Building Blocks Advanced Techniques: Linear regression foundational many advanced statistical machine learning models, logistic regression, ridge regression, neural networks. Mastering linear regression equips skills tackle complex methods.Building Blocks Advanced Techniques: Linear regression foundational many advanced statistical machine learning models, logistic regression, ridge regression, neural networks. Mastering linear regression equips skills tackle complex methods.Regression Causality: Crucial DistinctionIt’s essential remember regression alone establish causation. instance, regression model might show strong association advertising sales, prove advertising directly causes sales increase. factors—seasonality, market trends, unobserved variables—also influence results.Establishing causality requires additional steps, controlled experiments, instrumental variable techniques, careful observational study designs. work details linear regression, ’ll revisit distinction highlight scenarios causality might might inferred.Estimator?heart regression lies process estimation—act using data determine unknown characteristics population model.estimator mathematical rule formula used calculate estimate unknown quantity based observed data. example, calculate average height sample estimate average height population, sample mean estimator.context regression, quantities typically estimate :Parameters: Fixed, unknown values describe relationship variables (e.g., coefficients regression equation).\nEstimating parameters → Parametric models (finite parameters, e.g., coefficients regression).\nEstimating parameters → Parametric models (finite parameters, e.g., coefficients regression).Functions: Unknown relationships patterns data, often modeled without assuming fixed functional form.\nEstimating functions → Non-parametric models (focus shapes trends, fixed number parameters).\nEstimating functions → Non-parametric models (focus shapes trends, fixed number parameters).Types EstimatorsTo better understand estimation process, let’s introduce two broad categories estimators ’ll work :Parametric Estimators\nParametric estimation focuses finite set parameters define model. example, simple linear regression:\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ntask estimate parameters \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Parametric estimators rely specific assumptions form model (e.g., linearity) distribution error term (e.g., normality).Parametric Estimators\nParametric estimation focuses finite set parameters define model. example, simple linear regression:\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ntask estimate parameters \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Parametric estimators rely specific assumptions form model (e.g., linearity) distribution error term (e.g., normality).Non-Parametric Estimators\nNon-parametric estimation avoids assuming specific functional form relationship variables. Instead, focuses estimating patterns trends directly data. example, using scatterplot smoothing technique visualize sales vary advertising spend without imposing linear quadratic relationship.Non-Parametric Estimators\nNon-parametric estimation avoids assuming specific functional form relationship variables. Instead, focuses estimating patterns trends directly data. example, using scatterplot smoothing technique visualize sales vary advertising spend without imposing linear quadratic relationship.two categories reflect fundamental trade-statistical analysis: parametric models often simpler interpretable require strong assumptions, non-parametric models flexible may require data computational resources.Desirable Properties EstimatorsRegardless whether estimating parameters functions, want estimators possess certain desirable properties. Think “golden standards” help us judge whether estimator reliable:Unbiasedness\nestimator unbiased hits true value parameter, average, repeated samples. Mathematically:\\[E[\\hat{\\beta}] = \\beta.\\]\nmeans , across multiple samples, estimator systematically overestimate underestimate true parameter.Unbiasedness\nestimator unbiased hits true value parameter, average, repeated samples. Mathematically:\\[E[\\hat{\\beta}] = \\beta.\\]\nmeans , across multiple samples, estimator systematically overestimate underestimate true parameter.Consistency\nConsistency ensures sample size increases, estimator converges true value parameter. Formally:\\[plim\\ \\hat{\\beta_n} = \\beta.\\]\nproperty relies Law Large Numbers, guarantees larger samples reduce random fluctuations, leading precise estimates.Consistency\nConsistency ensures sample size increases, estimator converges true value parameter. Formally:\\[plim\\ \\hat{\\beta_n} = \\beta.\\]\nproperty relies Law Large Numbers, guarantees larger samples reduce random fluctuations, leading precise estimates.Efficiency\nAmong unbiased estimators, efficient estimator smallest variance.\nOrdinary Least Squares method efficient Best Linear Unbiased Estimator (BLUE) Gauss-Markov Theorem.\nestimators meet specific distributional assumptions (e.g., normality), Maximum Likelihood Estimators (MLE) asymptotically efficient, meaning achieve lowest possible variance sample size grows.\nEfficiency\nAmong unbiased estimators, efficient estimator smallest variance.Ordinary Least Squares method efficient Best Linear Unbiased Estimator (BLUE) Gauss-Markov Theorem.estimators meet specific distributional assumptions (e.g., normality), Maximum Likelihood Estimators (MLE) asymptotically efficient, meaning achieve lowest possible variance sample size grows.Properties MatterUnderstanding properties crucial ensure methods use estimation reliable, precise, robust. Whether estimating coefficients regression model uncovering complex pattern data, properties provide foundation statistical inference decision-making.Now ’ve established estimators , types ’ll encounter, desirable properties, can move understanding concepts apply specifically Ordinary Least Squares method—backbone linear regression.Reference TableErrors independent, identically distributed (..d.) mean 0 constant variance.Linear relationship predictors response.Simple, well-understood method.Minimizes residual sum squares (easy interpret coefficients).Sensitive outliers violations normality.Can perform poorly predictors highly correlated (multicollinearity).Handles correlated non-constant-variance errors.flexible OLS noise structure known.Requires specifying (estimating) error covariance structure.Misspecification can lead biased estimates.Provides general framework estimating parameters well-defined probability models.Can extend complex likelihoods.Highly sensitive model misspecification.May require computation OLS GLS.Controls overfitting via regularization.Handles high-dimensional data many predictors.Can perform feature selection (e.g., Lasso).Requires choosing tuning parameter(s) (e.g., λ).Interpretation coefficients becomes less straightforward.Resistant large deviations outliers data.Often maintains good performance mild model misspecifications.Less efficient errors truly normal.Choice robust method tuning can subjective.Simultaneously reduces dimensionality fits regression.Works well collinear, high-dimensional data.Can harder interpret OLS (latent components instead original predictors).Requires choosing number components.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1 Ordinary Least Squares","text":"Ordinary Least Squares (OLS) backbone statistical modeling, method foundational often serves starting point understanding data relationships. Whether predicting sales, estimating economic trends, uncovering patterns scientific research, OLS remains critical tool. appeal lies simplicity: OLS models relationship dependent variable one predictors minimizing squared differences observed predicted values.OLS Works: Linear Nonlinear RelationshipsOLS rests Conditional Expectation Function (CEF), \\(E[Y | X]\\), describes expected value \\(Y\\) given \\(X\\). Regression shines two key scenarios:Perfect Fit (Linear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = + \\sum_{k=1}^K b_k X_{ki}\\), regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) exactly equals CEF. words, regression gives true average relationship \\(Y\\) \\(X\\).\ntrue relationship linear, regression delivers exact CEF. instance, imagine ’re estimating relationship advertising spend sales revenue. true impact linear, OLS perfectly capture .Perfect Fit (Linear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = + \\sum_{k=1}^K b_k X_{ki}\\), regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) exactly equals CEF. words, regression gives true average relationship \\(Y\\) \\(X\\).\ntrue relationship linear, regression delivers exact CEF. instance, imagine ’re estimating relationship advertising spend sales revenue. true impact linear, OLS perfectly capture .Approximation (Nonlinear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}]\\) nonlinear, OLS provides best linear approximation relationship. Specifically, minimizes expected squared deviation linear regression line nonlinear CEF.\nexample, effect advertising diminishes higher spending levels? OLS still works, providing best linear approximation nonlinear relationship minimizing squared deviations predictions true (unknown) CEF.Approximation (Nonlinear CEF):\n\\(E[Y_i | X_{1i}, \\dots, X_{Ki}]\\) nonlinear, OLS provides best linear approximation relationship. Specifically, minimizes expected squared deviation linear regression line nonlinear CEF.\nexample, effect advertising diminishes higher spending levels? OLS still works, providing best linear approximation nonlinear relationship minimizing squared deviations predictions true (unknown) CEF.words, regression just tool “linear” relationships—’s workhorse adapts remarkably well messy, real-world data.","code":""},{"path":"linear-regression.html","id":"simple-regression-basic-model","chapter":"5 Linear Regression","heading":"5.1.1 Simple Regression (Basic) Model","text":"simplest form regression straight line:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\\(Y_i\\): dependent variable outcome ’re trying predict (e.g., sales, temperature).\\(X_i\\): independent variable predictor (e.g., advertising spend, time).\\(\\beta_0\\): intercept—line crosses \\(Y\\)-axis \\(X = 0\\).\\(\\beta_1\\): slope, representing change \\(Y\\) one-unit increase \\(X\\).\\(\\epsilon_i\\): error term, accounting random factors \\(X\\) explain.Assumptions Error Term (\\(\\epsilon_i\\)):\\[\n\\begin{aligned}\nE(\\epsilon_i) &= 0 \\\\\n\\text{Var}(\\epsilon_i) &= \\sigma^2 \\\\\n\\text{Cov}(\\epsilon_i, \\epsilon_j) &= 0 \\quad \\text{} \\neq j\n\\end{aligned}\n\\]Since \\(\\epsilon_i\\) random, \\(Y_i\\) also random:\\[\n\\begin{aligned}\nE(Y_i) &= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1 X_i\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\text{Var}(Y_i) &= \\text{Var}(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= \\text{Var}(\\epsilon_i) \\\\\n&= \\sigma^2\n\\end{aligned}\n\\]Since \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\), outcomes across observations independent. Hence, \\(Y_i\\) \\(Y_j\\) uncorrelated well, conditioned \\(X\\)’s.","code":""},{"path":"linear-regression.html","id":"estimation-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.1 Estimation in Ordinary Least Squares","text":"goal OLS estimate regression parameters (\\(\\beta_0\\), \\(\\beta_1\\)) best describe relationship dependent variable \\(Y\\) independent variable \\(X\\). achieve , minimize sum squared deviations observed values \\(Y_i\\) expected values predicted model.deviation observed value \\(Y_i\\) expected value, based regression model, :\\[\nY_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i).\n\\]deviation represents error prediction \\(\\)-th observation.ensure errors don’t cancel prioritize larger deviations, consider squared deviations. sum squared deviations, denoted \\(Q\\), defined :\\[\nQ = \\sum_{=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2.\n\\]goal OLS find values \\(\\beta_0\\) \\(\\beta_1\\) minimize \\(Q\\). values called OLS estimators.minimize \\(Q\\), take partial derivatives respect \\(\\beta_0\\) \\(\\beta_1\\), set zero, solve resulting system equations. simplifying, estimators slope (\\(b_1\\)) intercept (\\(b_0\\)) obtained follows:Slope (\\(b_1\\)):\\[\nb_1 = \\frac{\\sum_{=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\], \\(\\bar{X}\\) \\(\\bar{Y}\\) represent means \\(X\\) \\(Y\\), respectively. formula reveals slope proportional covariance \\(X\\) \\(Y\\), scaled variance \\(X\\).Intercept (\\(b_0\\)):\\[\nb_0 = \\frac{1}{n} \\left( \\sum_{=1}^{n} Y_i - b_1 \\sum_{=1}^{n} X_i \\right) = \\bar{Y} - b_1 \\bar{X}.\n\\]intercept determined aligning regression line center data.Intuition Behind Estimators\\(b_1\\) (Slope): measures average change \\(Y\\) one-unit increase \\(X\\). formula uses deviations mean ensure relationship captures joint variability \\(X\\) \\(Y\\).\\(b_1\\) (Slope): measures average change \\(Y\\) one-unit increase \\(X\\). formula uses deviations mean ensure relationship captures joint variability \\(X\\) \\(Y\\).\\(b_0\\) (Intercept): ensures regression line passes mean data points \\((\\bar{X}, \\bar{Y})\\), anchoring model center observed data.\\(b_0\\) (Intercept): ensures regression line passes mean data points \\((\\bar{X}, \\bar{Y})\\), anchoring model center observed data.Equivalently, can also write parameters terms covariances.covariance two variables defined :\\[ \\text{Cov}(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \\]Properties Covariance:\\(\\text{Cov}(X_i, X_i) = \\sigma^2_X\\)\\(E(X_i) = 0\\) \\(E(Y_i) = 0\\), \\(\\text{Cov}(X_i, Y_i) = E[X_i Y_i]\\)\\(W_i = + b X_i\\) \\(Z_i = c + d Y_i\\),\\(\\text{Cov}(W_i, Z_i) = bd \\cdot \\text{Cov}(X_i, Y_i)\\)bivariate regression, slope \\(\\beta\\) bivariate regression given :\\[ \\beta = \\frac{\\text{Cov}(Y_i, X_i)}{\\text{Var}(X_i)} \\]multivariate case, slope \\(X_k\\) :\\[ \\beta_k = \\frac{\\text{Cov}(Y_i, \\tilde{X}_{ki})}{\\text{Var}(\\tilde{X}_{ki})} \\]\\(\\tilde{X}_{ki}\\) represents residual regression \\(X_{ki}\\) \\(K-1\\) covariates model.intercept :\\[ \\beta_0 = E[Y_i] - \\beta_1 E(X_i) \\]Note:OLS require assumption specific distribution variables. robustness based minimization squared errors (.e., distributional assumptions).","code":""},{"path":"linear-regression.html","id":"properties-of-least-squares-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2 Properties of Least Squares Estimators","text":"properties Ordinary Least Squares (OLS) estimators (\\(b_0\\) \\(b_1\\)) derived based statistical behavior. properties provide insights accuracy, variability, reliability estimates.","code":""},{"path":"linear-regression.html","id":"expectation-of-the-ols-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2.1 Expectation of the OLS Estimators","text":"OLS estimators \\(b_0\\) (intercept) \\(b_1\\) (slope) unbiased. means expected values equal true population parameters:\\[\n\\begin{aligned}\nE(b_1) &= \\beta_1, \\\\\nE(b_0) &= E(\\bar{Y}) - \\bar{X}\\beta_1.\n\\end{aligned}\n\\]Since expected value sample mean \\(Y\\), \\(E(\\bar{Y})\\), :\\[\nE(\\bar{Y}) = \\beta_0 + \\beta_1 \\bar{X},\n\\]expected value \\(b_0\\) simplifies :\\[\nE(b_0) = \\beta_0.\n\\]Thus, \\(b_0\\) \\(b_1\\) unbiased estimators respective population parameters \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"variance-of-the-ols-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2.2 Variance of the OLS Estimators","text":"variability OLS estimators depends spread predictor variable \\(X\\) error variance \\(\\sigma^2\\). variances given :Variance \\(b_1\\) (Slope):\\[\n\\text{Var}(b_1) = \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]Variance \\(b_0\\) (Intercept):\\[\n\\text{Var}(b_0) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]formulas highlight :\\(\\text{Var}(b_1) \\0\\) number observations increases, provided \\(X_i\\) values distributed around mean \\(\\bar{X}\\).\\(\\text{Var}(b_0) \\0\\) \\(n\\) increases, assuming \\(X_i\\) values appropriately selected (.e., clustered near mean).","code":""},{"path":"linear-regression.html","id":"mean-square-error-mse","chapter":"5 Linear Regression","heading":"5.1.1.3 Mean Square Error (MSE)","text":"Mean Square Error (MSE) quantifies average squared residual (error) model:\\[\nMSE = \\frac{SSE}{n-2} = \\frac{\\sum_{=1}^{n} e_i^2}{n-2} = \\frac{\\sum_{=1}^{n} (Y_i - \\hat{Y}_i)^2}{n-2},\n\\]\\(SSE\\) Sum Squared Errors \\(n-2\\) represents degrees freedom simple linear regression model (two parameters estimated: \\(\\beta_0\\) \\(\\beta_1\\)).expected value MSE equals error variance (.e., unbiased Estimator MSE:):\\[\nE(MSE) = \\sigma^2.\n\\]","code":""},{"path":"linear-regression.html","id":"estimating-variance-of-the-ols-coefficients","chapter":"5 Linear Regression","heading":"5.1.1.4 Estimating Variance of the OLS Coefficients","text":"sample-based estimates variances \\(b_0\\) \\(b_1\\) expressed follows:Estimated Variance \\(b_1\\) (Slope):\\[\ns^2(b_1) = \\widehat{\\text{Var}}(b_1) = \\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]Estimated Variance \\(b_0\\) (Intercept):\\[\ns^2(b_0) = \\widehat{\\text{Var}}(b_0) = MSE \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]estimates rely MSE approximate \\(\\sigma^2\\).variance estimates unbiased:\\[\n\\begin{aligned}\nE(s^2(b_1)) &= \\text{Var}(b_1), \\\\\nE(s^2(b_0)) &= \\text{Var}(b_0).\n\\end{aligned}\n\\]Implications PropertiesUnbiasedness: unbiased nature \\(b_0\\) \\(b_1\\) ensures , average, regression model accurately reflects true relationship population.Decreasing Variance: sample size \\(n\\) increases spread \\(X_i\\) values grows, variances \\(b_0\\) \\(b_1\\) decrease, leading precise estimates.Error Estimation MSE: MSE provides reliable estimate error variance \\(\\sigma^2\\), feeds directly assessing reliability \\(b_0\\) \\(b_1\\).","code":""},{"path":"linear-regression.html","id":"residuals-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.5 Residuals in Ordinary Least Squares","text":"Residuals differences observed values (\\(Y_i\\)) predicted counterparts (\\(\\hat{Y}_i\\)). play central role assessing model fit ensuring assumptions OLS met.residual \\(\\)-th observation defined :\\[\ne_i = Y_i - \\hat{Y}_i = Y_i - (b_0 + b_1 X_i),\n\\]:\\(e_i\\): Residual \\(\\)-th observation.\\(\\hat{Y}_i\\): Predicted value based regression model.\\(Y_i\\): Actual observed value.Residuals estimate unobservable error terms \\(\\epsilon_i\\):\\(e_i\\) estimate \\(\\epsilon_i = Y_i - E(Y_i)\\).\\(\\epsilon_i\\) always unknown know true values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"key-properties-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.1 Key Properties of Residuals","text":"Residuals exhibit several mathematical properties align OLS estimation process:Sum Residuals:\nresiduals sum zero:\n\\[\n\\sum_{=1}^{n} e_i = 0.\n\\]\nensures regression line passes centroid data, \\((\\bar{X}, \\bar{Y})\\).Sum Residuals:\nresiduals sum zero:\\[\n\\sum_{=1}^{n} e_i = 0.\n\\]ensures regression line passes centroid data, \\((\\bar{X}, \\bar{Y})\\).Orthogonality Residuals Predictors:\nresiduals orthogonal (uncorrelated) predictor variable \\(X\\):\n\\[\n\\sum_{=1}^{n} X_i e_i = 0.\n\\]\nreflects fact OLS minimizes squared deviations residuals along \\(Y\\)-axis, \\(X\\)-axis.Orthogonality Residuals Predictors:\nresiduals orthogonal (uncorrelated) predictor variable \\(X\\):\\[\n\\sum_{=1}^{n} X_i e_i = 0.\n\\]reflects fact OLS minimizes squared deviations residuals along \\(Y\\)-axis, \\(X\\)-axis.","code":""},{"path":"linear-regression.html","id":"expected-values-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.2 Expected Values of Residuals","text":"expected values residuals reinforce unbiased nature OLS:Mean Residuals:\nresiduals expected value zero:\n\\[\nE[e_i] = 0.\n\\]Mean Residuals:\nresiduals expected value zero:\\[\nE[e_i] = 0.\n\\]Orthogonality Predictors Fitted Values:\nResiduals uncorrelated predictor variables fitted values:\n\\[\n\\begin{aligned}\nE[X_i e_i] &= 0, \\\\\nE[\\hat{Y}_i e_i] &= 0.\n\\end{aligned}\n\\]Orthogonality Predictors Fitted Values:\nResiduals uncorrelated predictor variables fitted values:\\[\n\\begin{aligned}\nE[X_i e_i] &= 0, \\\\\nE[\\hat{Y}_i e_i] &= 0.\n\\end{aligned}\n\\]properties highlight residuals contain systematic information predictors fitted values, reinforcing idea model captured underlying relationship effectively.","code":""},{"path":"linear-regression.html","id":"practical-importance-of-residuals","chapter":"5 Linear Regression","heading":"5.1.1.5.3 Practical Importance of Residuals","text":"Model Diagnostics:\nResiduals analyzed check assumptions OLS, including linearity, homoscedasticity (constant variance), independence errors. Patterns residual plots can signal issues nonlinearity heteroscedasticity.Model Diagnostics:\nResiduals analyzed check assumptions OLS, including linearity, homoscedasticity (constant variance), independence errors. Patterns residual plots can signal issues nonlinearity heteroscedasticity.Goodness--Fit:\nsum squared residuals, \\(\\sum e_i^2\\), measures total unexplained variation \\(Y\\). smaller sum indicates better fit.Goodness--Fit:\nsum squared residuals, \\(\\sum e_i^2\\), measures total unexplained variation \\(Y\\). smaller sum indicates better fit.Influence Analysis:\nLarge residuals may indicate outliers influential points disproportionately affect regression line.Influence Analysis:\nLarge residuals may indicate outliers influential points disproportionately affect regression line.","code":""},{"path":"linear-regression.html","id":"inference-in-ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1.1.6 Inference in Ordinary Least Squares","text":"Inference allows us make probabilistic statements regression parameters (\\(\\beta_0\\), \\(\\beta_1\\)) predictions (\\(Y_h\\)). perform valid inference, certain assumptions distribution errors necessary.Normality AssumptionOLS estimation require assumption normality.However, conduct hypothesis tests construct confidence intervals \\(\\beta_0\\), \\(\\beta_1\\), predictions, distributional assumptions necessary.Inference \\(\\beta_0\\) \\(\\beta_1\\) robust moderate departures normality, especially large samples due Central Limit Theorem.Inference predicted values, \\(Y_{pred}\\), sensitive normality violations.assume normal error model, response variable \\(Y_i\\) modeled :\\[\nY_i \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma^2),\n\\]:\\(\\beta_0 + \\beta_1 X_i\\): Mean response\\(\\sigma^2\\): Variance errorsUnder model, sampling distributions OLS estimators, \\(b_0\\) \\(b_1\\), can derived.","code":""},{"path":"linear-regression.html","id":"inference-for-beta_1-slope","chapter":"5 Linear Regression","heading":"5.1.1.6.1 Inference for \\(\\beta_1\\) (Slope)","text":"normal error model:Sampling Distribution \\(b_1\\):\n\\[\nb_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right).\n\\]\nindicates \\(b_1\\) unbiased estimator \\(\\beta_1\\) variance proportional \\(\\sigma^2\\).Sampling Distribution \\(b_1\\):\\[\nb_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right).\n\\]indicates \\(b_1\\) unbiased estimator \\(\\beta_1\\) variance proportional \\(\\sigma^2\\).Test Statistic:\n\\[\nt = \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2},\n\\]\n\\(s(b_1)\\) standard error \\(b_1\\): \\[\ns(b_1) = \\sqrt{\\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}}.\n\\]Test Statistic:\\[\nt = \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2},\n\\]\\(s(b_1)\\) standard error \\(b_1\\): \\[\ns(b_1) = \\sqrt{\\frac{MSE}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}}.\n\\]Confidence Interval:\n\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_1\\) :\n\\[\nb_1 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_1).\n\\]Confidence Interval:\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_1\\) :\\[\nb_1 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_1).\n\\]","code":""},{"path":"linear-regression.html","id":"inference-for-beta_0-intercept","chapter":"5 Linear Regression","heading":"5.1.1.6.2 Inference for \\(\\beta_0\\) (Intercept)","text":"Sampling Distribution \\(b_0\\):\nnormal error model, sampling distribution \\(b_0\\) :\n\\[\nb_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)\\right).\n\\]Sampling Distribution \\(b_0\\):normal error model, sampling distribution \\(b_0\\) :\\[\nb_0 \\sim N\\left(\\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)\\right).\n\\]Test Statistic:\n\\[\nt = \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2},\n\\]\n\\(s(b_0)\\) standard error \\(b_0\\): \\[\ns(b_0) = \\sqrt{MSE \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)}.\n\\]Test Statistic:\\[\nt = \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2},\n\\]\\(s(b_0)\\) standard error \\(b_0\\): \\[\ns(b_0) = \\sqrt{MSE \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}\\right)}.\n\\]Confidence Interval:\n\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_0\\) :\n\\[\nb_0 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_0).\n\\]Confidence Interval:\\((1-\\alpha) 100\\%\\) confidence interval \\(\\beta_0\\) :\\[\nb_0 \\pm t_{1-\\alpha/2; n-2} \\cdot s(b_0).\n\\]","code":""},{"path":"linear-regression.html","id":"mean-response","chapter":"5 Linear Regression","heading":"5.1.1.6.3 Mean Response","text":"regression, often estimate mean response dependent variable \\(Y\\) given level predictor variable \\(X\\), denoted \\(X_h\\). estimation provides predicted average outcome specific value \\(X\\) based fitted regression model.Let \\(X_h\\) represent level \\(X\\) want estimate mean response.mean response \\(X = X_h\\) denoted \\(E(Y_h)\\).point estimator \\(E(Y_h)\\) \\(\\hat{Y}_h\\), predicted value regression model:\\[\n\\hat{Y}_h = b_0 + b_1 X_h.\n\\]estimator \\(\\hat{Y}_h\\) unbiased expected value equals true mean response \\(E(Y_h)\\):\\[\n\\begin{aligned}\nE(\\hat{Y}_h) &= E(b_0 + b_1 X_h) \\\\\n&= \\beta_0 + \\beta_1 X_h \\\\\n&= E(Y_h).\n\\end{aligned}\n\\]Thus, \\(\\hat{Y}_h\\) provides reliable estimate mean response \\(X_h\\).variance \\(\\hat{Y}_h\\) reflects uncertainty estimate mean response:\\[\n\\begin{aligned}\n\\text{Var}(\\hat{Y}_h) &= \\text{Var}(b_0 + b_1 X_h) \\quad\\text{(definition }\\hat{Y}_h\\text{)}\\\\[6pt]&= \\text{Var}\\bigl((\\bar{Y} - b_1 \\bar{X}) + b_1 X_h\\bigr)\\quad\\text{(since } b_0 = \\bar{Y} - b_1 \\bar{X}\\text{)}\\\\[6pt]&= \\text{Var}\\bigl(\\bar{Y} + b_1(X_h - \\bar{X})\\bigr)\\quad\\text{(factor } b_1\\text{)}\\\\[6pt]&= \\text{Var}\\bigl(\\bar{Y} + b_1 (X_h - \\bar{X}) \\bigr) \\\\\n&= \\text{Var}(\\bar{Y}) + (X_h - \\bar{X})^2 \\text{Var}(b_1) + 2(X_h - \\bar{X}) \\text{Cov}(\\bar{Y}, b_1).\n\\end{aligned}\n\\]Since \\(\\text{Cov}(\\bar{Y}, b_1) = 0\\) (due independence errors, \\(\\epsilon_i\\)), variance simplifies :\\[\n\\text{Var}(\\hat{Y}_h) = \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2}.\n\\]can also expressed :\\[\n\\text{Var}(\\hat{Y}_h) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]estimate variance \\(\\hat{Y}_h\\), replace \\(\\sigma^2\\) \\(MSE\\), mean squared error regression:\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]normal error model, sampling distribution \\(\\hat{Y}_h\\) :\\[\n\\begin{aligned}\n\\hat{Y}_h &\\sim N\\left(E(Y_h), \\text{Var}(\\hat{Y}_h)\\right), \\\\\n\\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} &\\sim t_{n-2}.\n\\end{aligned}\n\\]result follows \\(\\hat{Y}_h\\) linear combination normally distributed random variables, variance estimated using \\(s^2(\\hat{Y}_h)\\).\\(100(1-\\alpha)\\%\\) confidence interval mean response \\(E(Y_h)\\) given :\\[\n\\hat{Y}_h \\pm t_{1-\\alpha/2; n-2} \\cdot s(\\hat{Y}_h),\n\\]:\\(\\hat{Y}_h\\): Point estimate mean response,\\(s(\\hat{Y}_h)\\): Estimated standard error mean response,\\(t_{1-\\alpha/2; n-2}\\): Critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.","code":""},{"path":"linear-regression.html","id":"prediction-of-a-new-observation","chapter":"5 Linear Regression","heading":"5.1.1.6.4 Prediction of a New Observation","text":"analyzing regression results, important distinguish :Estimating mean response particular value \\(X\\).Predicting individual outcome particular value \\(X\\).Mean Response vs. Individual OutcomeSame Point Estimate\nformula estimated mean response predicted individual outcome \\(X = X_h\\) identical:\\[\n\\hat{Y}_{pred} = \\hat{Y}_h = b_0 + b_1 X_h.\n\\]Point Estimate\nformula estimated mean response predicted individual outcome \\(X = X_h\\) identical:\\[\n\\hat{Y}_{pred} = \\hat{Y}_h = b_0 + b_1 X_h.\n\\]Different Variance\nAlthough point estimates , level uncertainty differs. predicting individual outcome, must consider uncertainty estimating mean response (\\(\\hat{Y}_h\\)) also additional random variation within distribution \\(Y\\).Different Variance\nAlthough point estimates , level uncertainty differs. predicting individual outcome, must consider uncertainty estimating mean response (\\(\\hat{Y}_h\\)) also additional random variation within distribution \\(Y\\).Therefore, prediction intervals (individual outcomes) account uncertainty consequently wider confidence intervals (mean response).predict individual outcome given \\(X_h\\), combine mean response random error:\\[\nY_{pred} = \\beta_0 + \\beta_1 X_h + \\epsilon.\n\\]Using least squares predictor:\\[\n\\hat{Y}_{pred} = b_0 + b_1 X_h,\n\\]since \\(E(\\epsilon) = 0\\).variance predicted value new observation, \\(Y_{pred}\\), includes :Variance estimated mean response: \\[\n\\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right),\n\\]Variance error term, \\(\\epsilon\\), \\(\\sigma^2\\).Thus, total variance :\\[\n\\begin{aligned}\n\\text{Var}(Y_{pred}) &= \\text{Var}(b_0 + b_1 X_h + \\epsilon) \\\\\n&= \\text{Var}(b_0 + b_1 X_h) + \\text{Var}(\\epsilon) \\\\\n&= \\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right) + \\sigma^2 \\\\\n&= \\sigma^2 \\left( 1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\end{aligned}\n\\]estimate variance prediction using \\(MSE\\), mean squared error:\\[\ns^2(pred) = MSE \\left( 1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]normal error model, standardized predicted value follows \\(t\\)-distribution \\(n-2\\) degrees freedom:\\[\n\\frac{Y_{pred} - \\hat{Y}_h}{s(pred)} \\sim t_{n-2}.\n\\]\\(100(1-\\alpha)\\%\\) prediction interval \\(Y_{pred}\\) :\\[\n\\hat{Y}_{pred} \\pm t_{1-\\alpha/2; n-2} \\cdot s(pred).\n\\]","code":""},{"path":"linear-regression.html","id":"confidence-band","chapter":"5 Linear Regression","heading":"5.1.1.6.5 Confidence Band","text":"regression analysis, often want evaluate uncertainty around entire regression line, just single value predictor variable \\(X\\). achieved using confidence band, provides confidence interval mean response, \\(E(Y) = \\beta_0 + \\beta_1 X\\), entire range \\(X\\) values.Working-Hotelling confidence band method construct simultaneous confidence intervals regression line. given \\(X_h\\), confidence band expressed :\\[\n\\hat{Y}_h \\pm W s(\\hat{Y}_h),\n\\]:\\(W^2 = 2F_{1-\\alpha; 2, n-2}\\),\n\\(F_{1-\\alpha; 2, n-2}\\) critical value \\(F\\)-distribution 2 \\(n-2\\) degrees freedom.\n\\(W^2 = 2F_{1-\\alpha; 2, n-2}\\),\\(F_{1-\\alpha; 2, n-2}\\) critical value \\(F\\)-distribution 2 \\(n-2\\) degrees freedom.\\(s(\\hat{Y}_h)\\) standard error estimated mean response \\(X_h\\):\n\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]\\(s(\\hat{Y}_h)\\) standard error estimated mean response \\(X_h\\):\\[\ns^2(\\hat{Y}_h) = MSE \\left( \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2} \\right).\n\\]Key Properties Confidence BandWidth Interval:\nwidth confidence band changes \\(X_h\\) \\(s(\\hat{Y}_h)\\) depends far \\(X_h\\) mean \\(X\\) (\\(\\bar{X}\\)).\ninterval narrowest \\(X = \\bar{X}\\), variance estimated mean response minimized.\nwidth confidence band changes \\(X_h\\) \\(s(\\hat{Y}_h)\\) depends far \\(X_h\\) mean \\(X\\) (\\(\\bar{X}\\)).interval narrowest \\(X = \\bar{X}\\), variance estimated mean response minimized.Shape Band:\nboundaries confidence band form hyperbolic shape around regression line.\nreflects increasing uncertainty mean response \\(X_h\\) moves farther \\(\\bar{X}\\).\nboundaries confidence band form hyperbolic shape around regression line.reflects increasing uncertainty mean response \\(X_h\\) moves farther \\(\\bar{X}\\).Simultaneous Coverage:\nWorking-Hotelling band ensures true regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) lies within band across values \\(X\\) specified confidence level (e.g., \\(95\\%\\)).\nWorking-Hotelling band ensures true regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) lies within band across values \\(X\\) specified confidence level (e.g., \\(95\\%\\)).","code":""},{"path":"linear-regression.html","id":"analysis-of-variance-anova-in-regression","chapter":"5 Linear Regression","heading":"5.1.1.7 Analysis of Variance (ANOVA) in Regression","text":"ANOVA regression decomposes total variability response variable (\\(Y\\)) components attributed regression model residual error. context regression, ANOVA provides mechanism assess fit model test hypotheses relationship \\(X\\) \\(Y\\).corrected Total Sum Squares (SSTO) quantifies total variation \\(Y\\):\\[\nSSTO = \\sum_{=1}^n (Y_i - \\bar{Y})^2,\n\\]\\(\\bar{Y}\\) mean response variable. term “corrected” refers fact sum squares calculated relative mean (.e., uncorrected total sum squares given \\(\\sum Y_i^2\\))Using fitted regression model \\(\\hat{Y}_i = b_0 + b_1 X_i\\), estimate conditional mean \\(Y\\) \\(X_i\\). total sum squares can decomposed :\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\\n&= \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2 + 2 \\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) \\\\\n&= \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\n\\end{aligned}\n\\]cross-product term zero, shown .decomposition simplifies :\\[\nSSTO = SSE + SSR,\n\\]:\\(SSE = \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2\\): Error Sum Squares (variation unexplained model).\\(SSE = \\sum_{=1}^n (Y_i - \\hat{Y}_i)^2\\): Error Sum Squares (variation unexplained model).\\(SSR = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Regression Sum Squares (variation explained model), measure conditional mean varies central value.\\(SSR = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Regression Sum Squares (variation explained model), measure conditional mean varies central value.Degrees freedom partitioned :\\[\n\\begin{aligned}\nSSTO &= SSR + SSE \\\\\n(n-1) &= (1) + (n-2) \\\\\n\\end{aligned}\n\\]confirm cross-product term zero:\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y})\n&= \\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\quad \\text{(Expand } Y_i - \\hat{Y}_i \\text{ } \\hat{Y}_i - \\bar{Y}\\text{)} \\\\\n&=\\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))( b_1 (X_i - \\bar{X}))  \\\\\n&= b_1 \\sum_{=1}^n (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 \\quad \\text{(Distribute terms product)} \\\\\n&= b_1 \\frac{\\sum_{=1}^n (Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{=1}^n (X_i - \\bar{X})^2} \\sum_{=1}^n (X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 \\quad \\text{(Substitute } b_1 \\text{ definition)} \\\\\n&= b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2  \\\\\n&= 0\n\\end{aligned}\n\\]ANOVA table summarizes partitioning variability:expected values mean squares :\\[\n\\begin{aligned}\nE(MSE) &= \\sigma^2, \\\\\nE(MSR) &= \\sigma^2 + \\beta_1^2 \\sum_{=1}^n (X_i - \\bar{X})^2.\n\\end{aligned}\n\\]\\(\\beta_1 = 0\\):\nregression model explain variation \\(Y\\) beyond mean, \\(E(MSR) = E(MSE) = \\sigma^2\\).\ncondition corresponds null hypothesis, \\(H_0: \\beta_1 = 0\\).\nregression model explain variation \\(Y\\) beyond mean, \\(E(MSR) = E(MSE) = \\sigma^2\\).condition corresponds null hypothesis, \\(H_0: \\beta_1 = 0\\).\\(\\beta_1 \\neq 0\\):\nregression model explains variation \\(Y\\), \\(E(MSR) > E(MSE)\\).\nadditional term \\(\\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2\\) represents variance explained predictor \\(X\\).\nregression model explains variation \\(Y\\), \\(E(MSR) > E(MSE)\\).additional term \\(\\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2\\) represents variance explained predictor \\(X\\).difference \\(E(MSR)\\) \\(E(MSE)\\) allows us infer whether \\(\\beta_1 \\neq 0\\) comparing ratio.Assuming errors \\(\\epsilon_i\\) independent identically distributed \\(N(0, \\sigma^2)\\), null hypothesis \\(H_0: \\beta_1 = 0\\), :scaled \\(MSE\\) follows chi-square distribution \\(n-2\\) degrees freedom:\n\\[\n\\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2.\n\\]scaled \\(MSE\\) follows chi-square distribution \\(n-2\\) degrees freedom:\\[\n\\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2.\n\\]scaled \\(MSR\\) follows chi-square distribution \\(1\\) degree freedom:\n\\[\n\\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2.\n\\]scaled \\(MSR\\) follows chi-square distribution \\(1\\) degree freedom:\\[\n\\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2.\n\\]two chi-square random variables independent.two chi-square random variables independent.ratio two independent chi-square random variables, scaled respective degrees freedom, follows \\(F\\)-distribution. Therefore, \\(H_0\\):\\[\nF = \\frac{MSR}{MSE} \\sim F_{1, n-2}.\n\\]\\(F\\)-statistic tests whether regression model provides significant improvement null model (constant \\(E(Y)\\)).hypotheses \\(F\\)-test :Null Hypothesis (\\(H_0\\)): \\(\\beta_1 = 0\\) (relationship \\(X\\) \\(Y\\)).Alternative Hypothesis (\\(H_a\\)): \\(\\beta_1 \\neq 0\\) (significant relationship exists \\(X\\) \\(Y\\)).rejection rule \\(H_0\\) significance level \\(\\alpha\\) :\\[\nF > F_{1-\\alpha;1,n-2},\n\\]\\(F_{1-\\alpha;1,n-2}\\) critical value \\(F\\)-distribution \\(1\\) \\(n-2\\) degrees freedom.\\(F \\leq F_{1-\\alpha;1,n-2}\\):\nFail reject \\(H_0\\). insufficient evidence conclude \\(X\\) significantly explains variation \\(Y\\).\nFail reject \\(H_0\\). insufficient evidence conclude \\(X\\) significantly explains variation \\(Y\\).\\(F > F_{1-\\alpha;1,n-2}\\):\nReject \\(H_0\\). significant evidence \\(X\\) explains variation \\(Y\\).\nReject \\(H_0\\). significant evidence \\(X\\) explains variation \\(Y\\).","code":""},{"path":"linear-regression.html","id":"coefficient-of-determination-r2","chapter":"5 Linear Regression","heading":"5.1.1.8 Coefficient of Determination (\\(R^2\\))","text":"Coefficient Determination (\\(R^2\\)) measures well linear regression model accounts variability response variable \\(Y\\). defined :\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO},\n\\]:\\(SSR\\): Regression Sum Squares (variation explained model).\\(SSTO\\): Total Sum Squares (total variation \\(Y\\) mean).\\(SSE\\): Error Sum Squares (variation unexplained model).Properties \\(R^2\\)Range: \\[\n0 \\leq R^2 \\leq 1.\n\\]\n\\(R^2 = 0\\): model explains none variability \\(Y\\) (e.g., \\(\\beta_1 = 0\\)).\n\\(R^2 = 1\\): model explains variability \\(Y\\) (perfect fit).\nRange: \\[\n0 \\leq R^2 \\leq 1.\n\\]\\(R^2 = 0\\): model explains none variability \\(Y\\) (e.g., \\(\\beta_1 = 0\\)).\\(R^2 = 1\\): model explains variability \\(Y\\) (perfect fit).Proportionate Reduction Variance: \\(R^2\\) represents proportionate reduction total variation \\(Y\\) fitting model. quantifies much better model predicts \\(Y\\) compared simply using \\(\\bar{Y}\\).Proportionate Reduction Variance: \\(R^2\\) represents proportionate reduction total variation \\(Y\\) fitting model. quantifies much better model predicts \\(Y\\) compared simply using \\(\\bar{Y}\\).Potential Misinterpretation: really correct say \\(R^2\\) “variation \\(Y\\) explained \\(X\\).” term “variation explained” assumes causative deterministic explanation, always correct. example:\n\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.\ncases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).\nPotential Misinterpretation: really correct say \\(R^2\\) “variation \\(Y\\) explained \\(X\\).” term “variation explained” assumes causative deterministic explanation, always correct. example:\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.\\(R^2\\) shows much variance \\(Y\\) accounted regression model, imply causation.cases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).cases confounding variables spurious correlations, \\(R^2\\) can still high, even ’s direct causal link \\(X\\) \\(Y\\).simple linear regression, \\(R^2\\) square Pearson correlation coefficient, \\(r\\):\\[\nR^2 = (r)^2,\n\\]:\\(r = \\text{corr}(X, Y)\\) sample correlation coefficient.relationship \\(b_1\\) (slope regression line) \\(r\\) given :\\[\nb_1 = \\left(\\frac{\\sum_{=1}^n (Y_i - \\bar{Y})^2}{\\sum_{=1}^n (X_i - \\bar{X})^2}\\right)^{1/2}.\n\\]Additionally, \\(r\\) can expressed :\\[\nr = \\frac{s_y}{s_x} \\cdot r,\n\\]\\(s_y\\) \\(s_x\\) sample standard deviations \\(Y\\) \\(X\\), respectively.","code":""},{"path":"linear-regression.html","id":"lack-of-fit-in-regression","chapter":"5 Linear Regression","heading":"5.1.1.9 Lack of Fit in Regression","text":"lack fit test evaluates whether chosen regression model adequately captures relationship predictor variable \\(X\\) response variable \\(Y\\). repeated observations specific values \\(X\\), can partition Error Sum Squares (\\(SSE\\)) two components:Pure ErrorLack Fit.Given observations:\\(Y_{ij}\\): \\(j\\)-th replicate \\(\\)-th distinct value \\(X\\),\n\\(Y_{11}, Y_{21}, \\dots, Y_{n_1, 1}\\): \\(n_1\\) repeated observations \\(X_1\\)\n\\(Y_{1c}, Y_{2c}, \\dots, Y_{n_c,c}\\): \\(n_c\\) repeated observations \\(X_c\\)\n\\(Y_{11}, Y_{21}, \\dots, Y_{n_1, 1}\\): \\(n_1\\) repeated observations \\(X_1\\)\\(Y_{1c}, Y_{2c}, \\dots, Y_{n_c,c}\\): \\(n_c\\) repeated observations \\(X_c\\)\\(\\bar{Y}_j\\): mean response replicates \\(X_j\\),\\(\\hat{Y}_{ij}\\): predicted value regression model \\(X_j\\),Error Sum Squares (\\(SSE\\)) can decomposed :\\[\n\\begin{aligned}\n\\sum_{} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j - \\hat{Y}_{ij})^2 \\\\\n&= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{j} n_j (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\\n&= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{j} n_j (\\bar{Y}_j - \\hat{Y}_{ij})^2\n\\end{aligned}\n\\]cross product term zero deviations within replicates deviations replicates orthogonal.simplifies :\\[\nSSE = SSPE + SSLF,\n\\]:\\(SSPE\\) (Pure Error Sum Squares): Variation within replicates \\(X_j\\), reflecting natural variability response.\nDegrees freedom: \\(df_{pe} = n - c\\), \\(n\\) total number observations, \\(c\\) number distinct \\(X\\) values.\nDegrees freedom: \\(df_{pe} = n - c\\), \\(n\\) total number observations, \\(c\\) number distinct \\(X\\) values.\\(SSLF\\) (Lack Fit Sum Squares): Variation replicate means \\(\\bar{Y}_j\\) model-predicted values \\(\\hat{Y}_{ij}\\). SSLF large, suggests model may adequately describe relationship \\(X\\) \\(Y\\).\nDegrees freedom: \\(df_{lf} = c - 2\\), 2 accounts parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)).\nDegrees freedom: \\(df_{lf} = c - 2\\), 2 accounts parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)).Mean Square Pure Error (MSPE):\\[\nMSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c}.\n\\]Mean Square Pure Error (MSPE):\\[\nMSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c}.\n\\]Mean Square Lack Fit (MSLF):\\[\nMSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}.\n\\]Mean Square Lack Fit (MSLF):\\[\nMSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}.\n\\]","code":""},{"path":"linear-regression.html","id":"the-f-test-for-lack-of-fit","chapter":"5 Linear Regression","heading":"5.1.1.9.1 The F-Test for Lack of Fit","text":"F-test lack fit evaluates whether chosen regression model adequately captures relationship predictor variable \\(X\\) response variable \\(Y\\). Specifically, tests whether systematic deviations model exist accounted random error.Null Hypothesis (\\(H_0\\)):\nregression model adequate: \\[\nH_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Null Hypothesis (\\(H_0\\)):\nregression model adequate: \\[\nH_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Alternative Hypothesis (\\(H_a\\)):\nregression model adequate includes additional function \\(f(X_i, Z_1, \\dots)\\) account lack fit: \\[\nH_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1, \\dots) + \\epsilon_{ij}^*, \\quad \\epsilon_{ij}^* \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Alternative Hypothesis (\\(H_a\\)):\nregression model adequate includes additional function \\(f(X_i, Z_1, \\dots)\\) account lack fit: \\[\nH_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1, \\dots) + \\epsilon_{ij}^*, \\quad \\epsilon_{ij}^* \\sim \\text{..d. } N(0, \\sigma^2).\n\\]Expected Mean SquaresThe expected Mean Square Pure Error (MSPE) \\(H_0\\) \\(H_a\\):\n\\[\nE(MSPE) = \\sigma^2.\n\\]expected Mean Square Pure Error (MSPE) \\(H_0\\) \\(H_a\\):\\[\nE(MSPE) = \\sigma^2.\n\\]expected Mean Square Lack Fit (MSLF) depends whether \\(H_0\\) true:\n\\(H_0\\) (model adequate): \\[\nE(MSLF) = \\sigma^2.\n\\]\n\\(H_a\\) (model adequate): \\[\nE(MSLF) = \\sigma^2 + \\frac{\\sum n_j f(X_i, Z_1, \\dots)^2}{n-2}.\n\\]\nexpected Mean Square Lack Fit (MSLF) depends whether \\(H_0\\) true:\\(H_0\\) (model adequate): \\[\nE(MSLF) = \\sigma^2.\n\\]\\(H_a\\) (model adequate): \\[\nE(MSLF) = \\sigma^2 + \\frac{\\sum n_j f(X_i, Z_1, \\dots)^2}{n-2}.\n\\]test statistic lack--fit test :\\[\nF = \\frac{MSLF}{MSPE},\n\\]:\\(MSLF = \\frac{SSLF}{c-2}\\),\n\\(SSLF\\) Lack Fit Sum Squares.\\(MSPE = \\frac{SSPE}{n-c}\\),\n\\(SSPE\\) Pure Error Sum Squares.\\(H_0\\), \\(F\\)-statistic follows \\(F\\)-distribution:\\[\nF \\sim F_{c-2, n-c}.\n\\]Decision RuleReject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nF > F_{1-\\alpha; c-2, n-c}.\n\\]Reject \\(H_0\\) significance level \\(\\alpha\\) : \\[\nF > F_{1-\\alpha; c-2, n-c}.\n\\]Failing reject \\(H_0\\):\nIndicates evidence lack fit.\nimply model “true,” suggests model provides reasonable approximation true relationship.\nFailing reject \\(H_0\\):Indicates evidence lack fit.imply model “true,” suggests model provides reasonable approximation true relationship.summarize, repeat observations exist levels \\(X\\), Error Sum Squares (SSE) can partitioned Lack Fit (SSLF) Pure Error (SSPE). leads extended ANOVA table:Repeat observations important implications coefficient determination (\\(R^2\\)):\\(R^2\\) Can’t Attain 1 Repeat Observations:\nrepeat observations, \\(SSE\\) (Error Sum Squares) reduced 0 \\(SSPE > 0\\) (variability within replicates).\nrepeat observations, \\(SSE\\) (Error Sum Squares) reduced 0 \\(SSPE > 0\\) (variability within replicates).Maximum \\(R^2\\):\nmaximum attainable \\(R^2\\) presence repeat observations :\n\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]\nmaximum attainable \\(R^2\\) presence repeat observations :\n\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]maximum attainable \\(R^2\\) presence repeat observations :\\[\nR^2_{\\text{max}} = \\frac{SSTO - SSPE}{SSTO}.\n\\]Importance Repeat Observations:\nlevels \\(X\\) need repeat observations, presence enables separation pure error lack fit, making \\(F\\)-test lack fit possible.\nlevels \\(X\\) need repeat observations, presence enables separation pure error lack fit, making \\(F\\)-test lack fit possible.Estimation \\(\\sigma^2\\) Repeat ObservationsUse MSE:\n\\(H_0\\) appropriate (model fits well), \\(MSE\\) typically used estimate \\(\\sigma^2\\) instead \\(MSPE\\) degrees freedom provides reliable estimate.\n\\(H_0\\) appropriate (model fits well), \\(MSE\\) typically used estimate \\(\\sigma^2\\) instead \\(MSPE\\) degrees freedom provides reliable estimate.Pooling Estimates:\npractice, \\(MSE\\) \\(MSPE\\) may pooled \\(H_0\\) holds, resulting precise estimate \\(\\sigma^2\\).\npractice, \\(MSE\\) \\(MSPE\\) may pooled \\(H_0\\) holds, resulting precise estimate \\(\\sigma^2\\).","code":""},{"path":"linear-regression.html","id":"joint-inference-for-regression-parameters","chapter":"5 Linear Regression","heading":"5.1.1.10 Joint Inference for Regression Parameters","text":"Joint inference considers simultaneous coverage confidence intervals multiple regression parameters, \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope). Ensuring adequate confidence parameters together requires adjustments maintain desired family-wise confidence level.Let:\\(\\bar{}_1\\): event confidence interval \\(\\beta_0\\) covers true value.\\(\\bar{}_2\\): event confidence interval \\(\\beta_1\\) covers true value.individual confidence levels :\\[\n\\begin{aligned}\nP(\\bar{}_1) &= 1 - \\alpha, \\\\\nP(\\bar{}_2) &= 1 - \\alpha.\n\\end{aligned}\n\\]joint confidence coefficient, \\(P(\\bar{}_1 \\cap \\bar{}_2)\\), :\\[\n\\begin{aligned}\nP(\\bar{}_1 \\cap \\bar{}_2) &= 1 - P(\\bar{}_1 \\cup \\bar{}_2), \\\\\n&= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2), \\\\\n&\\geq 1 - P(A_1) - P(A_2), \\\\\n&= 1 - 2\\alpha.\n\\end{aligned}\n\\]means \\(\\alpha\\) significance level parameter, joint confidence coefficient least \\(1 - 2\\alpha\\). inequality known Bonferroni Inequality.Bonferroni Confidence IntervalsTo ensure desired joint confidence level \\((1-\\alpha)\\) \\(\\beta_0\\) \\(\\beta_1\\), Bonferroni method adjusts confidence level parameter dividing \\(\\alpha\\) number parameters. two parameters:confidence level parameter \\((1-\\alpha/2)\\).confidence level parameter \\((1-\\alpha/2)\\).resulting Bonferroni-adjusted confidence intervals :\n\\[\n\\begin{aligned}\nb_0 &\\pm B \\cdot s(b_0), \\\\\nb_1 &\\pm B \\cdot s(b_1),\n\\end{aligned}\n\\]\n\\(B = t_{1-\\alpha/4; n-2}\\) critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.resulting Bonferroni-adjusted confidence intervals :\\[\n\\begin{aligned}\nb_0 &\\pm B \\cdot s(b_0), \\\\\nb_1 &\\pm B \\cdot s(b_1),\n\\end{aligned}\n\\]\\(B = t_{1-\\alpha/4; n-2}\\) critical value \\(t\\)-distribution \\(n-2\\) degrees freedom.Interpretation Bonferroni Confidence IntervalsCoverage Probability:\nrepeated samples taken, \\((1-\\alpha)100\\%\\) joint intervals contain true values \\((\\beta_0, \\beta_1)\\).\nimplies \\(\\alpha \\times 100\\%\\) samples miss least one true parameter values.\nrepeated samples taken, \\((1-\\alpha)100\\%\\) joint intervals contain true values \\((\\beta_0, \\beta_1)\\).implies \\(\\alpha \\times 100\\%\\) samples miss least one true parameter values.Conservatism:\nBonferroni method ensures family-wise confidence level conservative. actual joint confidence level often higher \\((1-\\alpha)100\\%\\).\nconservatism reduces statistical power.\nBonferroni method ensures family-wise confidence level conservative. actual joint confidence level often higher \\((1-\\alpha)100\\%\\).conservatism reduces statistical power.red point represents estimated coefficients (b0_hat, b1_hat).blue lines represent Bonferroni-adjusted confidence intervals beta_0 beta_1.grey points represent joint confidence region based covariance matrix coefficients.Bonferroni intervals ensure family-wise confidence level conservative.Simulation results demonstrate often true values captured intervals repeated samples drawn.Notes:Conservatism Bonferroni Intervals\nBonferroni interval conservative:\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).\nconservatism results wider intervals, reducing statistical power test.\n\nAdjustments Conservatism:\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.\nhigher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.\n\nBonferroni interval conservative:\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).\nconservatism results wider intervals, reducing statistical power test.\njoint confidence level lower bound, ensuring family-wise coverage least \\((1-\\alpha)100\\%\\).conservatism results wider intervals, reducing statistical power test.Adjustments Conservatism:\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.\nhigher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.\nPractitioners often choose larger \\(\\alpha\\) (e.g., \\(\\alpha = 0.1\\)) reduce width intervals Bonferroni joint tests.higher \\(\\alpha\\) allows better balance confidence precision, especially exploratory analyses.Extending Bonferroni Multiple Parameters: Bonferroni method limited two parameters. testing \\(g\\) parameters, \\(\\beta_0, \\beta_1, \\dots, \\beta_{g-1}\\):\nAdjusted Confidence Level Parameter:\nconfidence level individual parameter \\((1-\\alpha/g)\\).\n\nCritical \\(t\\)-Value:\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.\n\nExample:\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]\ncorresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.\n\nAdjusted Confidence Level Parameter:\nconfidence level individual parameter \\((1-\\alpha/g)\\).\nconfidence level individual parameter \\((1-\\alpha/g)\\).Critical \\(t\\)-Value:\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.\ntwo-sided intervals, critical value parameter : \\[\nt_{1-\\frac{\\alpha}{2g}; n-p},\n\\] \\(p\\) total number parameters regression model.Example:\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]\ncorresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.\n\\(\\alpha = 0.05\\) \\(g = 10\\), individual confidence interval constructed : \\[\n(1 - \\frac{0.05}{10}) = 99.5\\% \\text{ confidence level}.\n\\]corresponds using \\(t_{1-\\frac{0.005}{2}; n-p}\\) formula confidence intervals.Limitations Large \\(g\\)\nWide Intervals:\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.\nissue stems conservatism Bonferroni method, prioritizes family-wise error control.\n\nSuitability Small \\(g\\):\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).\nlarger \\(g\\), alternative methods (discussed ) efficient.\n\nWide Intervals:\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.\nissue stems conservatism Bonferroni method, prioritizes family-wise error control.\n\\(g\\) increases, intervals become excessively wide, often leading reduced usefulness practical applications.issue stems conservatism Bonferroni method, prioritizes family-wise error control.Suitability Small \\(g\\):\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).\nlarger \\(g\\), alternative methods (discussed ) efficient.\nBonferroni procedure works well \\(g\\) relatively small (e.g., \\(g \\leq 5\\)).larger \\(g\\), alternative methods (discussed ) efficient.Correlation Parameters: Correlation \\(b_0\\) \\(b_1\\):\nestimated regression coefficients \\(b_0\\) \\(b_1\\) often correlated:\nNegative correlation \\(\\bar{X} > 0\\).\nPositive correlation \\(\\bar{X} < 0\\).\n\ncorrelation can complicate joint inference affect validity Bonferroni-adjusted intervals.\nestimated regression coefficients \\(b_0\\) \\(b_1\\) often correlated:\nNegative correlation \\(\\bar{X} > 0\\).\nPositive correlation \\(\\bar{X} < 0\\).\nNegative correlation \\(\\bar{X} > 0\\).Positive correlation \\(\\bar{X} < 0\\).correlation can complicate joint inference affect validity Bonferroni-adjusted intervals.Alternatives BonferroniSeveral alternative procedures provide precise joint inference, especially larger \\(g\\):Scheffé’s Method:\nConstructs simultaneous confidence regions possible linear combinations parameters.\nSuitable exploratory analyses may result even wider intervals Bonferroni.\nConstructs simultaneous confidence regions possible linear combinations parameters.Suitable exploratory analyses may result even wider intervals Bonferroni.Tukey’s Honest Significant Difference:\nDesigned pairwise comparisons ANOVA can adapted regression parameters.\nDesigned pairwise comparisons ANOVA can adapted regression parameters.Holm’s Step-Procedure:\nsequential testing procedure less conservative Bonferroni still controlling family-wise error rate.\nsequential testing procedure less conservative Bonferroni still controlling family-wise error rate.Likelihood Ratio Tests:\nConstruct joint confidence regions based likelihood function, offering precision large \\(g\\).\nConstruct joint confidence regions based likelihood function, offering precision large \\(g\\).","code":"\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(MASS)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic data\nn <- 100  # Number of observations\nx <- rnorm(n, mean = 0, sd = 1)  # Predictor\nbeta_0 <- 2  # True intercept\nbeta_1 <- 3  # True slope\nsigma <- 1  # Standard deviation of error\ny <-\n    beta_0 + beta_1 * x + rnorm(n, mean = 0, sd = sigma)  # Response\n\n# Fit linear model\nmodel <- lm(y ~ x)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.9073 -0.6835 -0.0875  0.5806  3.2904 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.89720    0.09755   19.45   <2e-16 ***\n#> x            2.94753    0.10688   27.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9707 on 98 degrees of freedom\n#> Multiple R-squared:  0.8859, Adjusted R-squared:  0.8847 \n#> F-statistic: 760.6 on 1 and 98 DF,  p-value: < 2.2e-16\n\n# Extract coefficients and standard errors\nb0_hat <- coef(model)[1]\nb1_hat <- coef(model)[2]\ns_b0 <-\n    summary(model)$coefficients[1, 2]  # Standard error of intercept\ns_b1 <- summary(model)$coefficients[2, 2]  # Standard error of slope\n\n# Desired confidence level\nalpha <- 0.05  # Overall significance level\n\n# Bonferroni correction\nadjusted_alpha <- alpha / 2  # Adjusted alpha for each parameter\n\n# Critical t-value for Bonferroni adjustment\nt_crit <-\n    qt(1 - adjusted_alpha, df = n - 2)  # n-2 degrees of freedom\n\n# Bonferroni confidence intervals\nci_b0 <- c(b0_hat - t_crit * s_b0, b0_hat + t_crit * s_b0)\nci_b1 <- c(b1_hat - t_crit * s_b1, b1_hat + t_crit * s_b1)\n\n# Print results\ncat(\"Bonferroni Confidence Intervals:\\n\")\n#> Bonferroni Confidence Intervals:\ncat(\"Intercept (beta_0): [\",\n    round(ci_b0[1], 2),\n    \",\",\n    round(ci_b0[2], 2),\n    \"]\\n\")\n#> Intercept (beta_0): [ 1.7 , 2.09 ]\ncat(\"Slope (beta_1): [\",\n    round(ci_b1[1], 2),\n    \",\",\n    round(ci_b1[2], 2),\n    \"]\\n\")\n#> Slope (beta_1): [ 2.74 , 3.16 ]\n\n# Calculate the covariance matrix of coefficients\ncov_matrix <- vcov(model)\n\n# Generate points for confidence ellipse\nellipse_points <-\n    MASS::mvrnorm(n = 1000,\n                  mu = coef(model),\n                  Sigma = cov_matrix)\n\n# Convert to data frame for plotting\nellipse_df <- as.data.frame(ellipse_points)\ncolnames(ellipse_df) <- c(\"beta_0\", \"beta_1\")\n\n# Plot confidence intervals and ellipse\np <- ggplot() +\n    # Confidence ellipse\n    geom_point(\n        data = ellipse_df,\n        aes(x = beta_0, y = beta_1),\n        alpha = 0.1,\n        color = \"grey\"\n    ) +\n    # Point estimate\n    geom_point(aes(x = b0_hat, y = b1_hat),\n               color = \"red\",\n               size = 3) +\n    # Bonferroni confidence intervals\n    geom_errorbar(aes(x = b0_hat, ymin = ci_b1[1], ymax = ci_b1[2]),\n                  width = 0.1,\n                  color = \"blue\") +\n    geom_errorbarh(aes(y = b1_hat, xmin = ci_b0[1], xmax = ci_b0[2]),\n                   height = 0.1,\n                   color = \"blue\") +\n    labs(title = \"Bonferroni Confidence Intervals and Joint Confidence Region\",\n         x = \"Intercept (beta_0)\",\n         y = \"Slope (beta_1)\") +\n    theme_minimal()\n\nprint(p)"},{"path":"linear-regression.html","id":"assumptions-of-linear-regression","chapter":"5 Linear Regression","heading":"5.1.1.11 Assumptions of Linear Regression","text":"ensure valid inference reliable predictions linear regression, following assumptions must hold. ’ll cover depth next section.","code":""},{"path":"linear-regression.html","id":"diagnostics-for-model-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.12 Diagnostics for Model Assumptions","text":"Constant VarianceTo check homoscedasticity:\nPlot residuals vs. fitted values residuals vs. predictors.\nLook patterns funnel-shaped spread indicating heteroscedasticity.\nPlot residuals vs. fitted values residuals vs. predictors.Look patterns funnel-shaped spread indicating heteroscedasticity.OutliersDetect outliers using:\nResiduals vs. predictors plot.\nBox plots.\nStem--leaf plots.\nScatter plots.\nResiduals vs. predictors plot.Box plots.Stem--leaf plots.Scatter plots.Standardized Residuals:Residuals can standardized unit variance, known studentized residuals: \\[\n  r_i = \\frac{e_i}{s(e_i)}.\n  \\]Semi-Studentized Residuals:simplified standardization using mean squared error (MSE): \\[\n  e_i^* = \\frac{e_i}{\\sqrt{MSE}}.\n  \\]Non-Independent Error TermsTo detect non-independence:\nPlot residuals vs. time time-series data.\nResiduals \\(e_i\\) independent depend \\(\\hat{Y}_i\\), derived regression function.\nPlot residuals vs. time time-series data.Residuals \\(e_i\\) independent depend \\(\\hat{Y}_i\\), derived regression function.Detect dependency plotting residual \\(\\)-th response vs. \\((-1)\\)-th.Non-Normality Error TermsTo assess normality:\nPlot distribution residuals.\nCreate box plots, stem--leaf plots, normal probability plots.\nPlot distribution residuals.Create box plots, stem--leaf plots, normal probability plots.Issues incorrect regression function non-constant error variance can distort residual distribution.Normality tests require relatively large sample sizes detect deviations.Normality ResidualsUse tests based empirical cumulative distribution function (ECDF) (check Normality Assessment)Constancy Error VarianceStatistical tests homoscedasticity:\nBrown-Forsythe Test (Modified Levene Test):\nRobust non-normality, examines variance residuals across levels predictors.\n\nBreusch-Pagan Test (Cook-Weisberg Test):\nTests heteroscedasticity regressing squared residuals predictors.\n\nBrown-Forsythe Test (Modified Levene Test):\nRobust non-normality, examines variance residuals across levels predictors.\nRobust non-normality, examines variance residuals across levels predictors.Breusch-Pagan Test (Cook-Weisberg Test):\nTests heteroscedasticity regressing squared residuals predictors.\nTests heteroscedasticity regressing squared residuals predictors.","code":""},{"path":"linear-regression.html","id":"remedial-measures-for-violations-of-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.13 Remedial Measures for Violations of Assumptions","text":"assumptions simple linear regression violated, appropriate remedial measures can applied address issues. list measures specific deviations assumptions.","code":""},{"path":"linear-regression.html","id":"general-remedies","chapter":"5 Linear Regression","heading":"5.1.1.13.1 General Remedies","text":"Use complicated models (e.g., non-linear models, generalized linear models).Apply transformations (see Variable Transformation) \\(X\\) /\\(Y\\) stabilize variance, linearize relationships, normalize residuals. Note transformations may always yield “optimal” results.","code":""},{"path":"linear-regression.html","id":"specific-remedies-for-assumption-violations","chapter":"5 Linear Regression","heading":"5.1.1.13.2 Specific Remedies for Assumption Violations","text":"","code":""},{"path":"linear-regression.html","id":"remedies-in-detail","chapter":"5 Linear Regression","heading":"5.1.1.13.3 Remedies in Detail","text":"Non-Linearity:\nTransformations: Apply transformations response variable \\(Y\\) predictor variable \\(X\\). Common transformations include:\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).\nPolynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.\n\nAlternative Models:\nPolynomial regression splines flexibility modeling non-linear relationships.\n\nTransformations: Apply transformations response variable \\(Y\\) predictor variable \\(X\\). Common transformations include:\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).\nPolynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.\nLogarithmic transformation: \\(Y' = \\log(Y)\\) \\(X' = \\log(X)\\).Polynomial terms: Include \\(X^2\\), \\(X^3\\), etc., capture curvature.Alternative Models:\nPolynomial regression splines flexibility modeling non-linear relationships.\nPolynomial regression splines flexibility modeling non-linear relationships.Non-Constant Error Variance:\nWeighted Least Squares (WLS):\nAssigns weights observations inversely proportional variance.\n\nTransformations:\nUse log square root transformation stabilize variance.\n\nWeighted Least Squares (WLS):\nAssigns weights observations inversely proportional variance.\nAssigns weights observations inversely proportional variance.Transformations:\nUse log square root transformation stabilize variance.\nUse log square root transformation stabilize variance.Correlated Errors:\ntime-series data:\nUse serially correlated error models AR(1) ARIMA.\nmodels explicitly account dependency residuals time.\n\ntime-series data:\nUse serially correlated error models AR(1) ARIMA.\nmodels explicitly account dependency residuals time.\nUse serially correlated error models AR(1) ARIMA.models explicitly account dependency residuals time.Non-Normality:\nTransformations:\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.\n\nNon-parametric regression:\nMethods like LOESS Theil-Sen regression require normality assumption.\n\nTransformations:\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.\nApply transformation \\(Y\\) (e.g., log square root) make residuals approximately normal.Non-parametric regression:\nMethods like LOESS Theil-Sen regression require normality assumption.\nMethods like LOESS Theil-Sen regression require normality assumption.Omitted Variables:\nIntroduce additional predictors:\nUse multiple regression include relevant independent variables.\n\nCheck multicollinearity adding new variables.\nIntroduce additional predictors:\nUse multiple regression include relevant independent variables.\nUse multiple regression include relevant independent variables.Check multicollinearity adding new variables.Outliers:\nRobust Regression:\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.\n\nDiagnostics:\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.\n\nRobust Regression:\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.\nUse methods Huber regression M-estimation reduce impact outliers model coefficients.Diagnostics:\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.\nIdentify outliers using Cook’s Distance, leverage statistics, studentized residuals.","code":""},{"path":"linear-regression.html","id":"transformations-in-regression-analysis","chapter":"5 Linear Regression","heading":"5.1.1.14 Transformations in Regression Analysis","text":"Transformations involve modifying one variables address issues non-linearity, non-constant variance, non-normality. However, ’s important note properties least-squares estimates apply transformed model, original variables.transforming dependent variable \\(Y\\), fit model :\\[\ng(Y_i) = b_0 + b_1 X_i,\n\\]\\(g(Y_i)\\) transformed response. interpret regression results terms original \\(Y\\), need transform back:\\[\n\\hat{Y}_i = g^{-1}(b_0 + b_1 X_i).\n\\]Direct back-transformation predictions can introduce bias. example, log-transformed model:\\[\n\\log(Y_i) = b_0 + b_1 X_i,\n\\]unbiased back-transformed prediction \\(Y_i\\) :\\[\n\\hat{Y}_i = \\exp(b_0 + b_1 X_i + \\frac{\\sigma^2}{2}),\n\\]\\(\\frac{\\sigma^2}{2}\\) accounts bias correction due log transformation.","code":""},{"path":"linear-regression.html","id":"box-cox-family-of-transformations","chapter":"5 Linear Regression","heading":"5.1.1.14.1 Box-Cox Family of Transformations","text":"Box-Cox transformation versatile family transformations defined :\\[\nY' =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\text{} \\lambda \\neq 0, \\\\\n\\ln(Y), & \\text{} \\lambda = 0.\n\\end{cases}\n\\]transformation introduces parameter \\(\\lambda\\) estimated data. Common transformations include:Choosing Transformation Parameter \\(\\lambda\\)value \\(\\lambda\\) can selected using one following methods:Trial Error:\nApply different transformations compare residual plots model fit statistics (e.g., \\(R^2\\) AIC).\nApply different transformations compare residual plots model fit statistics (e.g., \\(R^2\\) AIC).Maximum Likelihood Estimation:\nChoose \\(\\lambda\\) maximize likelihood function assumption normally distributed errors.\nChoose \\(\\lambda\\) maximize likelihood function assumption normally distributed errors.Numerical Search:\nUse computational optimization techniques minimize residual sum squares (RSS) another goodness--fit criterion.\nUse computational optimization techniques minimize residual sum squares (RSS) another goodness--fit criterion.NotesBenefits Transformations:\nStabilize Variance: Helps address heteroscedasticity.\nLinearize Relationships: Useful non-linear data.\nNormalize Residuals: Addresses non-normality issues.\nBenefits Transformations:Stabilize Variance: Helps address heteroscedasticity.Stabilize Variance: Helps address heteroscedasticity.Linearize Relationships: Useful non-linear data.Linearize Relationships: Useful non-linear data.Normalize Residuals: Addresses non-normality issues.Normalize Residuals: Addresses non-normality issues.Caveats:\nInterpretability: Transformed variables may complicate interpretation.\n-Transformation: Excessive transformations can distort relationship variables.\nCaveats:Interpretability: Transformed variables may complicate interpretation.Interpretability: Transformed variables may complicate interpretation.-Transformation: Excessive transformations can distort relationship variables.-Transformation: Excessive transformations can distort relationship variables.Applicability:\nTransformations effective issues like non-linearity non-constant variance. less effective correcting independence violations omitted variables.\nApplicability:Transformations effective issues like non-linearity non-constant variance. less effective correcting independence violations omitted variables.","code":"\n# Install and load the necessary library\nif (!require(\"MASS\")) install.packages(\"MASS\")\nlibrary(MASS)\n\n# Fit a linear model\nset.seed(123)\nn <- 50\nx <- rnorm(n, mean = 5, sd = 2)\ny <- 3 + 2 * x + rnorm(n, mean = 0, sd = 2)\nmodel <- lm(y ~ x)\n\n# Apply Box-Cox Transformation\nboxcox_result <- boxcox(model, lambda = seq(-2, 2, 0.1), plotit = TRUE)\n\n# Find the optimal lambda\noptimal_lambda <- boxcox_result$x[which.max(boxcox_result$y)]\ncat(\"Optimal lambda for Box-Cox transformation:\", optimal_lambda, \"\\n\")\n#> Optimal lambda for Box-Cox transformation: 0.8686869"},{"path":"linear-regression.html","id":"variance-stabilizing-transformations","chapter":"5 Linear Regression","heading":"5.1.1.14.2 Variance Stabilizing Transformations","text":"Variance stabilizing transformations used standard deviation response variable depends mean. delta method, applies Taylor series expansion, provides systematic approach find transformations.Given standard deviation \\(Y\\) function mean:\\[\n\\sigma = \\sqrt{\\text{var}(Y)} = f(\\mu),\n\\]\\(\\mu = E(Y)\\) \\(f(\\mu)\\) smooth function mean, aim find transformation \\(h(Y)\\) variance transformed variable \\(h(Y)\\) constant values \\(\\mu\\).Expanding \\(h(Y)\\) Taylor Expansion series around \\(\\mu\\):\\[\nh(Y) = h(\\mu) + h'(\\mu)(Y - \\mu) + \\text{higher-order terms}.\n\\]Ignoring higher-order terms, variance \\(h(Y)\\) can approximated :\\[\n\\text{var}(h(Y)) = \\text{var}(h(\\mu) + h'(\\mu)(Y - \\mu)).\n\\]Since \\(h(\\mu)\\) constant:\\[\n\\text{var}(h(Y)) = \\left(h'(\\mu)\\right)^2 \\text{var}(Y).\n\\]Substituting \\(\\text{var}(Y) = \\left(f(\\mu)\\right)^2\\), get:\\[\n\\text{var}(h(Y)) = \\left(h'(\\mu)\\right)^2 \\left(f(\\mu)\\right)^2.\n\\]stabilize variance (make constant \\(\\mu\\)), require:\\[\n\\left(h'(\\mu)\\right)^2 \\left(f(\\mu)\\right)^2 = \\text{constant}.\n\\]Thus, derivative \\(h(\\mu)\\) must proportional inverse \\(f(\\mu)\\):\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)}.\n\\]Integrating sides gives:\\[\nh(\\mu) = \\int \\frac{1}{f(\\mu)} \\, d\\mu.\n\\]specific form \\(h(\\mu)\\) depends function \\(f(\\mu)\\), describes relationship standard deviation mean.Examples Variance Stabilizing TransformationsVariance stabilizing transformations particularly useful :Poisson-distributed data: Use \\(h(Y) = 2\\sqrt{Y}\\) stabilize variance.Exponential multiplicative models: Use \\(h(Y) = \\ln(Y)\\) stabilization.Power law relationships: Use transformations like \\(h(Y) = Y^{-1}\\) forms derived \\(f(\\mu)\\).Example: Variance Stabilizing Transformation Poisson DistributionFor Poisson distribution, variance \\(Y\\) equal mean:\\[\n\\sigma^2 = \\text{var}(Y) = E(Y) = \\mu.\n\\]Thus, standard deviation :\\[\n\\sigma = f(\\mu) = \\sqrt{\\mu}.\n\\]Using relationship variance stabilizing transformations:\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)} = \\mu^{-0.5}.\n\\]Integrating \\(h'(\\mu)\\) gives variance stabilizing transformation:\\[\nh(\\mu) = \\int \\mu^{-0.5} \\, d\\mu = 2\\sqrt{\\mu}.\n\\]Hence, variance stabilizing transformation :\\[\nh(Y) = \\sqrt{Y}.\n\\]transformation widely used Poisson regression stabilize variance response variable.","code":"\n# Simulate Poisson data\nset.seed(123)\nn <- 500\nx <- rnorm(n, mean = 5, sd = 2)\ny <- rpois(n, lambda = exp(1 + 0.3 * x))  # Poisson-distributed Y\n\n# Fit linear model without transformation\nmodel_raw <- lm(y ~ x)\n\n# Apply square root transformation\ny_trans <- sqrt(y)\nmodel_trans <- lm(y_trans ~ x)\n\n# Compare residual plots\npar(mfrow = c(2, 1))\n\n# Residual plot for raw data\nplot(\n    fitted(model_raw),\n    resid(model_raw),\n    main = \"Residuals: Raw Data\",\n    xlab = \"Fitted Values\",\n    ylab = \"Residuals\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n# Residual plot for transformed data\nplot(\n    fitted(model_trans),\n    resid(model_trans),\n    main = \"Residuals: Transformed Data (sqrt(Y))\",\n    xlab = \"Fitted Values\",\n    ylab = \"Residuals\"\n)\nabline(h = 0, col = \"blue\", lty = 2)"},{"path":"linear-regression.html","id":"general-strategy-when-fmu-is-unknown","chapter":"5 Linear Regression","heading":"5.1.1.14.3 General Strategy When \\(f(\\mu)\\) Is Unknown","text":"relationship \\(\\text{var}(Y)\\) \\(\\mu\\) (.e., \\(f(\\mu)\\)) unknown, following steps can help:Trial Error:\nApply common transformations (e.g., \\(\\log(Y)\\), \\(\\sqrt{Y}\\)) examine residual plots.\nSelect transformation results stabilized variance (residuals show pattern plots).\nApply common transformations (e.g., \\(\\log(Y)\\), \\(\\sqrt{Y}\\)) examine residual plots.Select transformation results stabilized variance (residuals show pattern plots).Leverage Prior Research:\nConsult researchers literature similar experiments determine transformations typically used.\nConsult researchers literature similar experiments determine transformations typically used.Analyze Observations Predictor Value:\nmultiple observations \\(Y_{ij}\\) available \\(X\\) value:\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.\nCheck \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\n\nPerform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).\n\nmultiple observations \\(Y_{ij}\\) available \\(X\\) value:\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.\nCheck \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\n\nPerform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).\nCompute mean \\(\\bar{Y}_i\\) standard deviation \\(s_i\\) group.Check \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\).\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]\nTaking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]\nexample, assume: \\[\ns_i = \\bar{Y}_i^{\\lambda}.\n\\]Taking natural logarithm: \\[\n\\ln(s_i) = \\ln() + \\lambda \\ln(\\bar{Y}_i).\n\\]Perform regression \\(\\ln(s_i)\\) \\(\\ln(\\bar{Y}_i)\\) estimate \\(\\lambda\\) suggest form \\(f(\\mu)\\).Group Observations:\nindividual observations sparse, try grouping similar observations \\(X\\) values compute \\(\\bar{Y}_i\\) \\(s_i\\) group.\nindividual observations sparse, try grouping similar observations \\(X\\) values compute \\(\\bar{Y}_i\\) \\(s_i\\) group.","code":""},{"path":"linear-regression.html","id":"common-transformations-and-their-applications","chapter":"5 Linear Regression","heading":"5.1.1.14.4 Common Transformations and Their Applications","text":"table summarizes common transformations used stabilize variance various conditions, along appropriate contexts comments:Choosing Transformation:\nStart identifying relationship variance residuals (\\(var(\\epsilon_i)\\)) mean response variable (\\(E(Y_i)\\)).\nSelect transformation matches identified variance structure.\nStart identifying relationship variance residuals (\\(var(\\epsilon_i)\\)) mean response variable (\\(E(Y_i)\\)).Select transformation matches identified variance structure.Transformations Zero Values:\ndata zeros, transformations like \\(\\sqrt{Y+1}\\) \\(\\log(Y+1)\\) can used avoid undefined values. seriously jeopardize model assumption (J. Chen Roth 2024).\ndata zeros, transformations like \\(\\sqrt{Y+1}\\) \\(\\log(Y+1)\\) can used avoid undefined values. seriously jeopardize model assumption (J. Chen Roth 2024).Use Regression Models:\nApply transformations dependent variable \\(Y\\) regression model.\nAlways check residual plots confirm transformation stabilizes variance resolves non-linearity.\nApply transformations dependent variable \\(Y\\) regression model.Always check residual plots confirm transformation stabilizes variance resolves non-linearity.Interpretation Transformation:\ntransforming \\(Y\\), interpret results terms transformed variable.\npractical interpretation, back-transform predictions account associated bias.\ntransforming \\(Y\\), interpret results terms transformed variable.practical interpretation, back-transform predictions account associated bias.","code":""},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"5 Linear Regression","heading":"5.1.2 Multiple Linear Regression","text":"geometry least squares regression involves projecting response vector \\(\\mathbf{y}\\) onto space spanned columns design matrix \\(\\mathbf{X}\\). fitted values \\(\\mathbf{\\hat{y}}\\) can expressed :\\[\n\\begin{aligned}\n\\mathbf{\\hat{y}} &= \\mathbf{Xb} \\\\\n&= \\mathbf{X(X'X)^{-1}X'y} \\\\\n&= \\mathbf{Hy},\n\\end{aligned}\n\\]:\\(\\mathbf{H} = \\mathbf{X(X'X)^{-1}X'}\\) projection operator (sometimes denoted \\(\\mathbf{P}\\)).\\(\\mathbf{\\hat{y}}\\) projection \\(\\mathbf{y}\\) onto linear space spanned columns \\(\\mathbf{X}\\) (model space).dimension model space equal rank \\(\\mathbf{X}\\) (.e., number linearly independent columns \\(\\mathbf{X}\\)).Properties Projection Matrix \\(\\mathbf{H}\\)Symmetry:\nprojection matrix \\(\\mathbf{H}\\) symmetric: \\[\n\\mathbf{H} = \\mathbf{H}'.\n\\]\nprojection matrix \\(\\mathbf{H}\\) symmetric: \\[\n\\mathbf{H} = \\mathbf{H}'.\n\\]Idempotence:\nApplying \\(\\mathbf{H}\\) twice gives result: \\[\n\\mathbf{HH} = \\mathbf{H}.\n\\] Proof: \\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'} \\\\\n&= \\mathbf{H}.\n\\end{aligned}\n\\]\nApplying \\(\\mathbf{H}\\) twice gives result: \\[\n\\mathbf{HH} = \\mathbf{H}.\n\\] Proof: \\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'} \\\\\n&= \\mathbf{H}.\n\\end{aligned}\n\\]Dimensionality:\n\\(\\mathbf{H}\\) \\(n \\times n\\) matrix (\\(n\\) number observations).\nrank \\(\\mathbf{H}\\) equal rank \\(\\mathbf{X}\\), typically number predictors (including intercept).\n\\(\\mathbf{H}\\) \\(n \\times n\\) matrix (\\(n\\) number observations).rank \\(\\mathbf{H}\\) equal rank \\(\\mathbf{X}\\), typically number predictors (including intercept).Orthogonal Complement:\nmatrix \\(\\mathbf{(- H)}\\), : \\[\n\\mathbf{- H} = \\mathbf{- X(X'X)^{-1}X'},\n\\] also projection operator.\nprojects onto orthogonal complement space spanned columns \\(\\mathbf{X}\\) (.e., space orthogonal model space).\nmatrix \\(\\mathbf{(- H)}\\), : \\[\n\\mathbf{- H} = \\mathbf{- X(X'X)^{-1}X'},\n\\] also projection operator.projects onto orthogonal complement space spanned columns \\(\\mathbf{X}\\) (.e., space orthogonal model space).Orthogonality Projections:\n\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) orthogonal: \\[\n\\mathbf{H(- H)} = \\mathbf{0}.\n\\]\nSimilarly: \\[\n\\mathbf{(- H)H} = \\mathbf{0}.\n\\]\n\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) orthogonal: \\[\n\\mathbf{H(- H)} = \\mathbf{0}.\n\\]Similarly: \\[\n\\mathbf{(- H)H} = \\mathbf{0}.\n\\]Intuition \\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\)\\(\\mathbf{H}\\): Projects \\(\\mathbf{y}\\) onto model space, giving fitted values \\(\\mathbf{\\hat{y}}\\).\\(\\mathbf{- H}\\): Projects \\(\\mathbf{y}\\) onto residual space, giving residuals \\(\\mathbf{e}\\): \\[\n\\mathbf{e} = \\mathbf{(- H)y}.\n\\]\\(\\mathbf{H}\\) \\(\\mathbf{(- H)}\\) divide response vector \\(\\mathbf{y}\\) two components: \\[\n\\mathbf{y} = \\mathbf{\\hat{y}} + \\mathbf{e}.\n\\]\n\\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\) (fitted values).\n\\(\\mathbf{e} = \\mathbf{(- H)y}\\) (residuals).\n\\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\) (fitted values).\\(\\mathbf{e} = \\mathbf{(- H)y}\\) (residuals).properties \\(\\mathbf{H}\\) (symmetry, idempotence, dimensionality) reflect role linear transformation projects vectors onto model space.geometric perspective provides insight mechanics least squares regression, particularly response variable \\(\\mathbf{y}\\) decomposed fitted values residuals.Similar simple regression, total sum squares multiple regression analysis can partitioned components corresponding regression (model fit) residuals (errors).uncorrected total sum squares :\\[\n\\mathbf{y'y} = \\mathbf{\\hat{y}'\\hat{y} + e'e},\n\\]:\\(\\mathbf{\\hat{y} = Hy}\\) (fitted values, projected onto model space).\\(\\mathbf{\\hat{y} = Hy}\\) (fitted values, projected onto model space).\\(\\mathbf{e = (- H)y}\\) (residuals, projected onto orthogonal complement model space).\\(\\mathbf{e = (- H)y}\\) (residuals, projected onto orthogonal complement model space).Expanding using projection matrices:\\[\n\\begin{aligned}\n\\mathbf{y'y} &= \\mathbf{(Hy)'(Hy) + ((-H)y)'((-H)y)} \\\\\n&= \\mathbf{y'H'Hy + y'(-H)'(-H)y} \\\\\n&= \\mathbf{y'Hy + y'(-H)y}.\n\\end{aligned}\n\\]equation shows partition \\(\\mathbf{y'y}\\) components explained model (\\(\\mathbf{\\hat{y}}\\)) unexplained variation (residuals).corrected total sum squares, adjust mean (using projection matrix \\(\\mathbf{H_1}\\)):\\[\n\\mathbf{y'(-H_1)y = y'(H-H_1)y + y'(-H)y}.\n\\]:\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\), \\(\\mathbf{J}\\) \\(n \\times n\\) matrix ones.\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\), \\(\\mathbf{J}\\) \\(n \\times n\\) matrix ones.\\(\\mathbf{H - H_1}\\) projects onto subspace explained predictors centering.\\(\\mathbf{H - H_1}\\) projects onto subspace explained predictors centering.\\(\\mathbf{H}\\): Projects onto model space.\\(\\mathbf{-H}\\): Projects onto residuals.\\(\\mathbf{H_1} = \\frac{1}{n} \\mathbf{J}\\): Adjusts mean.\\(\\mathbf{H-H_1}\\): Projects onto predictors centering.\\(\\mathbf{-H}\\): Projects onto residuals.Correction MattersIn ANOVA regression, removing contribution mean helps isolate variability explained predictors overall level response variable.ANOVA regression, removing contribution mean helps isolate variability explained predictors overall level response variable.Corrected sums squares common comparing models computing \\(R^2\\), requires centering ensure consistency proportionate variance explained.Corrected sums squares common comparing models computing \\(R^2\\), requires centering ensure consistency proportionate variance explained.corrected total sum squares can decomposed sum squares regression (SSR) sum squares error (SSE)::\\(p\\): Number parameters (including intercept).\\(p\\): Number parameters (including intercept).\\(n\\): Number observations.\\(n\\): Number observations.Alternatively, regression model can expressed :\\[\n\\mathbf{Y = X\\hat{\\beta} + (Y - X\\hat{\\beta})},\n\\]:\\(\\mathbf{\\hat{Y} = X\\hat{\\beta}}\\): Vector fitted values (subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{\\hat{Y} = X\\hat{\\beta}}\\): Vector fitted values (subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{e = Y - X\\hat{\\beta}}\\): Vector residuals (orthogonal complement subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{e = Y - X\\hat{\\beta}}\\): Vector residuals (orthogonal complement subspace spanned \\(\\mathbf{X}\\)).\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector \\(n\\)-dimensional space \\(\\mathbb{R}^n\\).\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector \\(n\\)-dimensional space \\(\\mathbb{R}^n\\).\\(\\mathbf{X}\\) \\(n \\times p\\) full-rank matrix, columns generating \\(p\\)-dimensional subspace \\(\\mathbb{R}^n\\). Hence, estimator \\(\\mathbf{X\\hat{\\beta}}\\) also subspace.\\(\\mathbf{X}\\) \\(n \\times p\\) full-rank matrix, columns generating \\(p\\)-dimensional subspace \\(\\mathbb{R}^n\\). Hence, estimator \\(\\mathbf{X\\hat{\\beta}}\\) also subspace.linear regression, Ordinary Least Squares estimator \\(\\hat{\\beta}\\) minimizes squared Euclidean distance \\(\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\\) observed response vector \\(\\mathbf{Y}\\) fitted values \\(\\mathbf{X}\\beta\\). minimization corresponds orthogonal projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).solve optimization problem:\\[\n\\min_{\\beta} \\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2.\n\\]objective function can expanded :\\[\n\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\n= (\\mathbf{Y} - \\mathbf{X}\\beta)^\\top (\\mathbf{Y} - \\mathbf{X}\\beta).\n\\]Perform multiplication:\\[\n\\begin{aligned}\n(\\mathbf{Y} - \\mathbf{X}\\beta)^\\top (\\mathbf{Y} - \\mathbf{X}\\beta)\n&= \\mathbf{Y}^\\top \\mathbf{Y}\n- \\mathbf{Y}^\\top \\mathbf{X}\\beta\n- \\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n+ \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta.\n\\end{aligned}\n\\]Since \\(\\mathbf{Y}^\\top \\mathbf{X}\\beta\\) scalar, equals \\(\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\\). Therefore, expanded expression becomes:\\[\n\\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2\n= \\mathbf{Y}^\\top \\mathbf{Y}\n- 2\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n+ \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta.\n\\]find \\(\\beta\\) minimizes expression, take derivative respect \\(\\beta\\) set 0:\\[\n\\frac{\\partial}{\\partial \\beta}\n\\Bigl[\n  \\mathbf{Y}^\\top \\mathbf{Y}\n  - 2\\beta^\\top \\mathbf{X}^\\top \\mathbf{Y}\n  + \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) \\beta\n\\Bigr]\n= 0.\n\\]Computing gradient:\\[\n\\frac{\\partial}{\\partial \\beta} = -2\\mathbf{X}^\\top \\mathbf{Y} + 2(\\mathbf{X}^\\top \\mathbf{X})\\beta.\n\\]Setting zero:\\[\n-2\\mathbf{X}^\\top \\mathbf{Y} + 2\\mathbf{X}^\\top \\mathbf{X}\\beta = 0.\n\\]Simplify:\\[\n\\mathbf{X}^\\top \\mathbf{X}\\beta = \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\\(\\mathbf{X}^\\top \\mathbf{X}\\) invertible, solution :\\[\n\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}.\n\\]Orthogonal Projection InterpretationThe fitted values :\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\beta}.\n\\]normal equations, \\(\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}) = 0\\), implies residual vector \\(\\mathbf{Y} - \\hat{\\mathbf{Y}}\\) orthogonal every column \\(\\mathbf{X}\\). Therefore:\\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\beta}\\) orthogonal projection \\(\\mathbf{Y}\\) onto \\(\\mathrm{Col}(\\mathbf{X})\\).\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) lies orthogonal complement \\(\\mathrm{Col}(\\mathbf{X})\\).Pythagoras DecompositionThe geometric interpretation gives us decomposition:\\[\n\\mathbf{Y} = \\mathbf{X}\\hat{\\beta} + (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}),\n\\]:\\(\\mathbf{X}\\hat{\\beta}\\) projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).\\(\\mathbf{X}\\hat{\\beta}\\) projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\).\\((\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\) residual vector, orthogonal \\(\\mathbf{X}\\hat{\\beta}\\).\\((\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\) residual vector, orthogonal \\(\\mathbf{X}\\hat{\\beta}\\).Since two components orthogonal, squared norms satisfy:\\[\n\\begin{aligned}\\|\\mathbf{Y}\\|^2 &= \\mathbf{Y}^\\top \\mathbf{Y}&& \\text{(definition norm squared)} \\\\[6pt]&= (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta} + \\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta} + \\mathbf{X}\\hat{\\beta})&& \\text{(add subtract term } \\mathbf{X}\\hat{\\beta}\\text{)} \\\\[6pt]&= (\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; 2\\,(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; (\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{X}\\hat{\\beta})&& \\text{(expand }(+b)^\\top(+b)\\text{)} \\\\[6pt]&= \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2\\;+\\; 2\\,(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta})\\;+\\; \\|\\mathbf{X}\\hat{\\beta}\\|^2&& \\text{(rewrite quadratic form norm)} \\\\[6pt]&= \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\|\\mathbf{X}\\hat{\\beta}\\|^2&& \\text{(use }(\\mathbf{X}\\hat{\\beta})^\\top(\\mathbf{Y}-\\mathbf{X}\\hat{\\beta}) = 0\\text{, .e. orthogonality)} \\\\[6pt]& \\quad = \\|\\mathbf{X}\\hat{\\beta}\\|^2 \\;+\\; \\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|^2.\n\\end{aligned}\n\\]norm vector \\(\\mathbf{}\\) \\(\\mathbb{R}^p\\) defined :\\[\n\\|\\mathbf{}\\| = \\sqrt{\\mathbf{}^\\top \\mathbf{}} = \\sqrt{\\sum_{=1}^p a_i^2}.\n\\]saying \\(\\mathbf{Y}\\) decomposed two orthogonal components:\\(\\mathbf{X}\\hat{\\beta}\\) (projection onto \\(\\mathrm{Col}(\\mathbf{X})\\)\n\\(\\|\\mathbf{X}\\hat{\\beta}\\|\\) measures part \\(\\mathbf{Y}\\) explained model.\n\\(\\mathbf{X}\\hat{\\beta}\\) (projection onto \\(\\mathrm{Col}(\\mathbf{X})\\)\\(\\|\\mathbf{X}\\hat{\\beta}\\|\\) measures part \\(\\mathbf{Y}\\) explained model.\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) (residual lying orthogonal complement).\n\\(\\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|\\) measures residual error.\n\\(\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\) (residual lying orthogonal complement).\\(\\|\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}\\|\\) measures residual error.geometric interpretation (projection plus orthogonal remainder) exactly call \\(\\mathbf{X}\\hat{\\beta}\\) orthogonal projection \\(\\mathbf{Y}\\) onto column space \\(\\mathbf{X}\\). decomposition also underlies analysis variance (ANOVA) regression.coefficient multiple determination, denoted \\(R^2\\), measures proportion total variation response variable (\\(\\mathbf{Y}\\)) explained regression model. defined :\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO},\n\\]:\\(SSR\\): Regression sum squares (variation explained model).\\(SSR\\): Regression sum squares (variation explained model).\\(SSE\\): Error sum squares (unexplained variation).\\(SSE\\): Error sum squares (unexplained variation).\\(SSTO\\): Total sum squares (total variation \\(\\mathbf{Y}\\)).\\(SSTO\\): Total sum squares (total variation \\(\\mathbf{Y}\\)).adjusted \\(R^2\\) adjusts \\(R^2\\) number predictors model, penalizing adding predictors improve model’s fit substantially. defined :\\[\nR^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)SSE}{(n-p)SSTO},\n\\]:\\(n\\): Number observations.\\(n\\): Number observations.\\(p\\): Number parameters (including intercept).\\(p\\): Number parameters (including intercept).Key Differences \\(R^2\\) \\(R^2_a\\)multiple regression, \\(R^2_a\\) provides reliable measure model fit, especially comparing models different numbers predictors.regression model coefficients \\(\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_{p-1})^\\top\\), sums squares used evaluate contribution predictors explaining variation response variable.Model Sums Squares:\n\\(SSM\\): Total model sum squares, capturing variation explained predictors: \\[\nSSM = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1}).\n\\]\n\\(SSM\\): Total model sum squares, capturing variation explained predictors: \\[\nSSM = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1}).\n\\]Marginal Contribution:\n\\(SSM_m\\): Conditional model sum squares, capturing variation explained predictors accounting others: \\[\nSSM_m = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1} | \\beta_0).\n\\]\n\\(SSM_m\\): Conditional model sum squares, capturing variation explained predictors accounting others: \\[\nSSM_m = SS(\\beta_0, \\beta_1, \\dots, \\beta_{p-1} | \\beta_0).\n\\]Decompositions \\(SSM_m\\)Sequential Sums Squares (Type SS)Definition:\nSequential SS depends order predictors added model.\nrepresents additional contribution predictor given predictors precede sequence.\nDefinition:Sequential SS depends order predictors added model.represents additional contribution predictor given predictors precede sequence.Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\dots, \\beta_{p-2}).\n\\]Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\dots, \\beta_{p-2}).\n\\]Key Points:\nSequential SS unique; depends order predictors.\nDefault many statistical software functions (e.g., anova() R).\nKey Points:Sequential SS unique; depends order predictors.Default many statistical software functions (e.g., anova() R).Marginal Sums Squares (Type II SS)Definition:\nMarginal SS evaluates contribution predictor accounting predictors except collinear.\nignores hierarchical relationships interactions, focusing independent contributions.\nMarginal SS evaluates contribution predictor accounting predictors except collinear.ignores hierarchical relationships interactions, focusing independent contributions.Formula: \\(SSM_m = SS(\\beta_j | \\beta_1, \\dots, \\beta_{j-1}, \\beta_{j + 1}, \\dots, \\beta_{p-1})\\) Type II SS evaluates contribution \\(\\beta_j\\) excluding terms collinear \\(\\beta_j\\).Key Points:\nType II SS independent predictor order.\nSuitable models without interaction terms predictors balanced.\nType II SS independent predictor order.Suitable models without interaction terms predictors balanced.Partial Sums Squares (Type III SS)Definition:\nPartial SS evaluates contribution predictor accounting predictors model.\nquantifies unique contribution predictor, controlling presence others.\nDefinition:Partial SS evaluates contribution predictor accounting predictors model.quantifies unique contribution predictor, controlling presence others.Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0, \\beta_2, \\dots, \\beta_{p-1}) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\beta_1, \\dots, \\beta_{p-2}).\n\\]Formula: \\[\nSSM_m = SS(\\beta_1 | \\beta_0, \\beta_2, \\dots, \\beta_{p-1}) + \\dots + SS(\\beta_{p-1} | \\beta_0, \\beta_1, \\dots, \\beta_{p-2}).\n\\]Key Points:\nPartial SS unique given model.\ncommonly used practice assessing individual predictor importance.\nKey Points:Partial SS unique given model.commonly used practice assessing individual predictor importance.Comparison Sequential, Marginal, Partial SSPractical NotesUse Type III SS (Partial SS) :\nfocus individual predictor contributions accounting others.\nConducting hypothesis tests predictors complex models interactions hierarchical structures.\nfocus individual predictor contributions accounting others.Conducting hypothesis tests predictors complex models interactions hierarchical structures.Use Type II SS (Marginal SS) :\nWorking balanced datasets models without interaction terms.\nIgnoring interactions focusing independent effects.\nWorking balanced datasets models without interaction terms.Ignoring interactions focusing independent effects.Use Type SS (Sequential SS) :\nInterested understanding incremental contribution predictors based specific order entry (e.g., stepwise regression).\nInterested understanding incremental contribution predictors based specific order entry (e.g., stepwise regression).","code":""},{"path":"linear-regression.html","id":"ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.1 OLS Assumptions","text":"A1 LinearityA2 Full RankA3 Exogeneity Independent VariablesA4 HomoskedasticityA5 Data Generation (Random Sampling)A6 Normal Distribution","code":""},{"path":"linear-regression.html","id":"a1-linearity","chapter":"5 Linear Regression","heading":"5.1.2.1.1 A1 Linearity","text":"linear regression model expressed :\\[\nA1: y = \\mathbf{x}\\beta + \\epsilon\n\\]assumption restrictive since \\(x\\) can include nonlinear transformations (e.g., interactions, natural logarithms, quadratic terms).However, combined A3 (Exogeneity Independent Variables), linearity can become restrictive.","code":""},{},{},{},{"path":"linear-regression.html","id":"a2-full-rank","chapter":"5 Linear Regression","heading":"5.1.2.1.2 A2 Full Rank","text":"full rank assumption ensures uniqueness existence parameter estimates population regression equation. expressed :\\[\nA2: \\text{rank}(E(\\mathbf{x'x})) = k\n\\]assumption also known identification condition.Key PointsNo Perfect Multicollinearity:\ncolumns \\(\\mathbf{x}\\) (matrix predictors) must linearly independent.\ncolumn \\(\\mathbf{x}\\) can written linear combination columns.\ncolumns \\(\\mathbf{x}\\) (matrix predictors) must linearly independent.column \\(\\mathbf{x}\\) can written linear combination columns.Implications:\nEnsures parameter regression equation identifiable unique.\nPrevents computational issues, inability invert \\(\\mathbf{x'x}\\), required estimating \\(\\hat{\\beta}\\).\nEnsures parameter regression equation identifiable unique.Prevents computational issues, inability invert \\(\\mathbf{x'x}\\), required estimating \\(\\hat{\\beta}\\).Example ViolationIf two predictors, \\(x_1\\) \\(x_2\\), perfectly correlated (e.g., \\(x_2 = 2x_1\\)), rank \\(\\mathbf{x}\\) reduced, \\(\\mathbf{x'x}\\) becomes singular. cases:regression coefficients uniquely estimated.regression coefficients uniquely estimated.model fails satisfy full rank assumption.model fails satisfy full rank assumption.","code":""},{"path":"linear-regression.html","id":"a3-exogeneity-of-independent-variables","chapter":"5 Linear Regression","heading":"5.1.2.1.3 A3 Exogeneity of Independent Variables","text":"exogeneity assumption ensures independent variables (\\(\\mathbf{x}\\)) systematically related error term (\\(\\epsilon\\)). expressed :\\[\nA3: E[\\epsilon | x_1, x_2, \\dots, x_k] = E[\\epsilon | \\mathbf{x}] = 0\n\\]assumption often referred strict exogeneity mean independence (see Correlation Independence.Key PointsStrict Exogeneity:\nIndependent variables carry information error term \\(\\epsilon\\).\n[Law Iterated Expectations], \\(E(\\epsilon) = 0\\), can satisfied always including intercept regression model.\nIndependent variables carry information error term \\(\\epsilon\\).[Law Iterated Expectations], \\(E(\\epsilon) = 0\\), can satisfied always including intercept regression model.Implication:\nA3 implies: \\[\nE(y | \\mathbf{x}) = \\mathbf{x}\\beta,\n\\] meaning conditional mean function linear function \\(\\mathbf{x}\\). aligns A1 Linearity.\nA3 implies: \\[\nE(y | \\mathbf{x}) = \\mathbf{x}\\beta,\n\\] meaning conditional mean function linear function \\(\\mathbf{x}\\). aligns A1 Linearity.Relationship Independence:\nAlso referred mean independence, weaker condition full independence (see Correlation Independence).\nAlso referred mean independence, weaker condition full independence (see Correlation Independence).","code":""},{},{"path":"linear-regression.html","id":"a4-homoskedasticity","chapter":"5 Linear Regression","heading":"5.1.2.1.4 A4 Homoskedasticity","text":"homoskedasticity assumption ensures variance error term (\\(\\epsilon\\)) constant across levels independent variables (\\(\\mathbf{x}\\)). expressed :\\[\nA4: \\text{Var}(\\epsilon | \\mathbf{x}) = \\text{Var}(\\epsilon) = \\sigma^2\n\\]Key PointsDefinition:\nvariance disturbance term \\(\\epsilon\\) observations, regardless values predictors \\(\\mathbf{x}\\).\nvariance disturbance term \\(\\epsilon\\) observations, regardless values predictors \\(\\mathbf{x}\\).Practical Implication:\nHomoskedasticity ensures errors systematically vary predictors.\ncritical valid inference, standard errors coefficients rely assumption.\nHomoskedasticity ensures errors systematically vary predictors.critical valid inference, standard errors coefficients rely assumption.Violation (Heteroskedasticity):\nvariance \\(\\epsilon\\) depends \\(\\mathbf{x}\\), assumption violated.\nCommon signs include funnel-shaped patterns residual plots varying error sizes.\nvariance \\(\\epsilon\\) depends \\(\\mathbf{x}\\), assumption violated.Common signs include funnel-shaped patterns residual plots varying error sizes.","code":""},{"path":"linear-regression.html","id":"a5-data-generation-random-sampling","chapter":"5 Linear Regression","heading":"5.1.2.1.5 A5 Data Generation (Random Sampling)","text":"random sampling assumption ensures observations \\((y_i, x_{i1}, \\dots, x_{ik-1})\\) drawn independently identically distributed (iid) joint distribution \\((y, \\mathbf{x})\\). expressed :\\[\nA5: \\{y_i, x_{i1}, \\dots, x_{ik-1} : = 1, \\dots, n\\}\n\\]Key PointsRandom Sampling:\ndataset assumed random sample population.\nobservation independent others follows probability distribution.\ndataset assumed random sample population.observation independent others follows probability distribution.Implications:\nA3 (Exogeneity Independent Variables) A4 (Homoskedasticity), random sampling implies:\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).\nNon-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.\nVariance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]\n\nA3 (Exogeneity Independent Variables) A4 (Homoskedasticity), random sampling implies:\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).\nNon-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.\nVariance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]\nStrict Exogeneity: \\[\nE(\\epsilon_i | x_1, \\dots, x_n) = 0\n\\] Independent variables contain information predicting \\(\\epsilon\\).Non-Autocorrelation: \\[\nE(\\epsilon_i \\epsilon_j | x_1, \\dots, x_n) = 0 \\quad \\text{} \\neq j\n\\] error terms uncorrelated across observations, conditional independent variables.Variance Errors: \\[\n\\text{Var}(\\epsilon | \\mathbf{X}) = \\text{Var}(\\epsilon) = \\sigma^2 \\mathbf{}_n\n\\]A5 May Hold:\ntime series data, observations often autocorrelated.\nspatial data, neighboring observations may independent.\ntime series data, observations often autocorrelated.spatial data, neighboring observations may independent.Practical ConsiderationsTime Series Data:\nUse methods autoregressive models generalized least squares (GLS) address dependency observations.\nUse methods autoregressive models generalized least squares (GLS) address dependency observations.Spatial Data:\nSpatial econometric models may required handle correlation across geographic locations.\nSpatial econometric models may required handle correlation across geographic locations.Checking Random Sampling:\ntrue randomness always verified, exploratory analysis residuals (e.g., patterns autocorrelation) can help detect violations.\ntrue randomness always verified, exploratory analysis residuals (e.g., patterns autocorrelation) can help detect violations.","code":""},{},{},{"path":"linear-regression.html","id":"a6-normal-distribution","chapter":"5 Linear Regression","heading":"5.1.2.1.6 A6 Normal Distribution","text":"A6: \\(\\epsilon|\\mathbf{x} \\sim N(0, \\sigma^2 I_n)\\)assumption implies error term \\(\\epsilon\\) normally distributed mean zero variance \\(\\sigma^2 I_n\\). assumption fundamental statistical inference linear regression models.Using assumptions A1 Linearity, A2 Full Rank, A3 Exogeneity Independent Variables, derive identification (orthogonality condition) population parameter \\(\\beta\\):\\[\n\\begin{aligned}\ny &= x\\beta + \\epsilon && \\text{(A1: Model Specification)} \\\\\nx'y &= x'x\\beta + x'\\epsilon && \\text{(Multiply sides $x'$)} \\\\\nE(x'y) &= E(x'x)\\beta + E(x'\\epsilon) && \\text{(Taking expectation)} \\\\\nE(x'y) &= E(x'x)\\beta && \\text{(A3: Exogeneity, $E(x'\\epsilon) = 0$)} \\\\\n[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\\beta && \\text{(Invertibility $E(x'x)$, A2)} \\\\\n[E(x'x)]^{-1}E(x'y) &= \\beta && \\text{(Simplified solution $\\beta$)}\n\\end{aligned}\n\\]Thus, \\(\\beta\\) identified vector parameters minimizes expected squared error.find \\(\\beta\\), minimize expected value squared error:\\[\n\\underset{\\gamma}{\\operatorname{argmin}} \\ E\\big((y - x\\gamma)^2\\big)\n\\]first-order condition derived taking derivative objective function respect \\(\\gamma\\) setting zero:\\[\n\\begin{aligned}\n\\frac{\\partial E\\big((y - x\\gamma)^2\\big)}{\\partial \\gamma} &= 0 \\\\\n-2E(x'(y - x\\gamma)) &= 0 \\\\\nE(x'y) - E(x'x\\gamma) &= 0 \\\\\nE(x'y) &= E(x'x)\\gamma \\\\\n(E(x'x))^{-1}E(x'y) &= \\gamma\n\\end{aligned}\n\\]confirms \\(\\gamma = \\beta\\).second-order condition ensures solution minimizes objective function. Taking second derivative:\\[\n\\frac{\\partial^2 E\\big((y - x\\gamma)^2\\big)}{\\partial \\gamma'^2} = 0 = 2E(x'x)\n\\]assumption A3 Exogeneity Independent Variables holds, \\(E(x'x)\\) positive semi-definite (PSD). Thus, \\(2E(x'x)\\) also PSD, ensuring minimum.","code":""},{"path":"linear-regression.html","id":"hierarchy-of-ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.1.7 Hierarchy of OLS Assumptions","text":"table summarizes hierarchical nature assumptions required derive different properties OLS estimator.Usage AssumptionsA2 Full RankVariation \\(\\mathbf{X}\\)A5 Data Generation (Random Sampling)Random SamplingA1 LinearityLinearity ParametersA3 Exogeneity Independent VariablesZero Conditional MeanA4 Homoskedasticity\\(\\mathbf{H}\\) homoskedasticityA6 Normal DistributionNormality ErrorsIdentification Data Description: Ensures model identifiable coefficients can estimated.Unbiasedness Consistency: Guarantees OLS estimates unbiased converge true parameter values sample size increases.Gauss-Markov (BLUE) Asymptotic Inference: Requires additional assumptions (e.g., homoskedasticity) ensure minimum variance estimators valid inference using large-sample tests (z chi-squared).Classical LM (BUE) Small-sample Inference: Builds previous assumptions adds normality errors valid t F tests finite samples.","code":""},{"path":"linear-regression.html","id":"theorems","chapter":"5 Linear Regression","heading":"5.1.2.2 Theorems","text":"","code":""},{"path":"linear-regression.html","id":"frischwaughlovell-theorem","chapter":"5 Linear Regression","heading":"5.1.2.2.1 Frisch–Waugh–Lovell Theorem","text":"Frisch–Waugh–Lovell (FWL) Theorem fundamental result linear regression allows deeper understanding coefficients computed multiple regression setting (Lovell 2008). Informally, states:estimating effect subset variables (\\(X_1\\)) \\(y\\) presence variables (\\(X_2\\)), can “partial ” influence \\(X_2\\) \\(y\\) \\(X_1\\). , regressing residuals \\(y\\) residuals \\(X_1\\) produces coefficients \\(X_1\\) identical obtained full multiple regression.Consider multiple linear regression model:\\[ \\mathbf{y = X\\beta + \\epsilon = X_1\\beta_1 + X_2\\beta_2 + \\epsilon} \\]:\\(y\\) \\(n \\times 1\\) vector dependent variable.\\(X_1\\) \\(n \\times k_1\\) matrix regressors interest.\\(X_2\\) \\(n \\times k_2\\) matrix additional regressors.\\(\\beta_1\\) \\(\\beta_2\\) coefficient vectors sizes \\(k_1 \\times 1\\) \\(k_2 \\times 1\\), respectively.\\(\\epsilon\\) \\(n \\times 1\\) error term vector.can equivalently represented partitioned matrix form :\\[ \\left( \\begin{array}{cc} X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{array} \\right) \\left( \\begin{array}{c} \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\end{array} \\right) = \\left( \\begin{array}{c} X_1'y \\\\ X_2'y \\end{array} \\right) \\]ordinary least squares (OLS) estimator vector \\(\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\\) :\\[\n\\begin{pmatrix}\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nX_1'X_1 & X_1'X_2 \\\\\nX_2'X_1 & X_2'X_2\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nX_1'y \\\\\nX_2'y\n\\end{pmatrix}.\n\\]want coefficients \\(X_1\\), known result partitioned-inversion gives:\\[\n\\hat{\\beta}_1\n=\n\\bigl(X_1' M_2\\, X_1\\bigr)^{-1}\n\\,X_1' M_2\\, y,\n\\]\\[\nM_2\n=\n\n-\nX_2 \\bigl(X_2'X_2\\bigr)^{-1} X_2'.\n\\]matrix \\(M_2\\) often called residual-maker annihilator matrix \\(X_2\\). \\(n \\times n\\) symmetric, idempotent projection matrix projects vector \\(\\mathbb{R}^n\\) onto orthogonal complement column space \\(X_2\\). \\(M_2\\) satisfies \\(M_2^2 = M_2\\), \\(M_2 = M_2'\\).Intuitively, \\(M_2\\) captures part \\(y\\) (vector) orthogonal columns \\(X_2\\). “partialling ” \\(X_2\\) \\(y\\) \\(X_1\\) lets us isolate \\(\\hat{\\beta}_1\\).Equivalently, can also represent \\(\\hat{\\beta_1}\\) :\\[ \\mathbf{\\hat{\\beta_1} = (X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\\hat{\\beta_2}} \\]equation, can see thatBetas Multiple vs. Simple Regressions:\ncoefficients (\\(\\beta\\)) multiple regression generally coefficients separate individual simple regressions.\ncoefficients (\\(\\beta\\)) multiple regression generally coefficients separate individual simple regressions.Impact Additional Variables (\\(X_2\\)):\ninclusion different sets explanatory variables (\\(X_2\\)) affects coefficient estimates, even \\(X_1\\).\ninclusion different sets explanatory variables (\\(X_2\\)) affects coefficient estimates, even \\(X_1\\).Special Cases:\n\\(X_1'X_2 = 0\\) (orthogonality \\(X_1\\) \\(X_2\\)) \\(\\hat{\\beta_2} = 0\\), points (1 2) hold. cases, interaction coefficients \\(X_1\\) \\(X_2\\), making coefficients \\(X_1\\) unaffected \\(X_2\\).\n\\(X_1'X_2 = 0\\) (orthogonality \\(X_1\\) \\(X_2\\)) \\(\\hat{\\beta_2} = 0\\), points (1 2) hold. cases, interaction coefficients \\(X_1\\) \\(X_2\\), making coefficients \\(X_1\\) unaffected \\(X_2\\).Steps FWL:Partial \\(X_2\\) \\(y\\): Regress \\(y\\) \\(X_2\\) obtain residuals:\n\\[\n\\tilde{y} = M_2y.\n\\]Partial \\(X_2\\) \\(y\\): Regress \\(y\\) \\(X_2\\) obtain residuals:\\[\n\\tilde{y} = M_2y.\n\\]Partial \\(X_2\\) \\(X_1\\): column \\(X_1\\), regress \\(X_2\\) obtain residuals:\n\\[\n\\tilde{X}_1 = M_2X_1.\n\\]Partial \\(X_2\\) \\(X_1\\): column \\(X_1\\), regress \\(X_2\\) obtain residuals:\\[\n\\tilde{X}_1 = M_2X_1.\n\\]Regression Residuals: Regress \\(\\tilde{y}\\) \\(\\tilde{X}_1\\):\n\\[\n\\tilde{y} = \\tilde{X}_1\\beta_1 + \\text{error}.\n\\]Regression Residuals: Regress \\(\\tilde{y}\\) \\(\\tilde{X}_1\\):\\[\n\\tilde{y} = \\tilde{X}_1\\beta_1 + \\text{error}.\n\\]coefficients \\(\\beta_1\\) obtained identical full model regression:\\[\ny = X_1\\beta_1 + X_2\\beta_2 + \\epsilon.\n\\]MattersInterpretation Partial Effects: FWL Theorem provides way interpret \\(\\beta_1\\) effect \\(X_1\\) \\(y\\) removing linear dependence \\(X_2\\).Interpretation Partial Effects: FWL Theorem provides way interpret \\(\\beta_1\\) effect \\(X_1\\) \\(y\\) removing linear dependence \\(X_2\\).Computational Simplicity: allows decomposition large regression problem smaller, computationally simpler pieces.Computational Simplicity: allows decomposition large regression problem smaller, computationally simpler pieces.","code":"\n# Simulate data\nset.seed(123)\nn <- 100\nX1 <- matrix(rnorm(n * 2), n, 2)  # Two regressors of interest\nX2 <- matrix(rnorm(n * 2), n, 2)  # Two additional regressors\nbeta1 <- c(2,-1)  # Coefficients for X1\nbeta2 <- c(1, 0.5)  # Coefficients for X2undefined\nu <- rnorm(n)  # Error term\ny <- X1 %*% beta1 + X2 %*% beta2 + u  # Generate dependent variable\n\n# Full regression\nfull_model <- lm(y ~ X1 + X2)\nsummary(full_model)\n#> \n#> Call:\n#> lm(formula = y ~ X1 + X2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.47336 -0.58010  0.07461  0.68778  2.46552 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.11614    0.10000   1.161    0.248    \n#> X11          1.77575    0.10899  16.293  < 2e-16 ***\n#> X12         -1.14151    0.10204 -11.187  < 2e-16 ***\n#> X21          0.94954    0.10468   9.071 1.60e-14 ***\n#> X22          0.47667    0.09506   5.014 2.47e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9794 on 95 degrees of freedom\n#> Multiple R-squared:  0.8297, Adjusted R-squared:  0.8225 \n#> F-statistic: 115.7 on 4 and 95 DF,  p-value: < 2.2e-16\n\n# Step 1: Partial out X2 from y\ny_residual <- residuals(lm(y ~ X2))\n\n# Step 2: Partial out X2 from X1\nX1_residual <- residuals(lm(X1 ~ X2))\n\n# Step 3: Regress residuals\nfwl_model <- lm(y_residual ~ X1_residual - 1)\nsummary(fwl_model)\n#> \n#> Call:\n#> lm(formula = y_residual ~ X1_residual - 1)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.47336 -0.58010  0.07461  0.68778  2.46552 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> X1_residual1   1.7758     0.1073   16.55   <2e-16 ***\n#> X1_residual2  -1.1415     0.1005  -11.36   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9643 on 98 degrees of freedom\n#> Multiple R-squared:  0.8109, Adjusted R-squared:  0.807 \n#> F-statistic: 210.1 on 2 and 98 DF,  p-value: < 2.2e-16\n\n# Comparison of coefficients\ncat(\"Full model coefficients (X1):\", coef(full_model)[2:3], \"\\n\")\n#> Full model coefficients (X1): 1.775754 -1.141514\ncat(\"FWL model coefficients:\", coef(fwl_model), \"\\n\")\n#> FWL model coefficients: 1.775754 -1.141514"},{"path":"linear-regression.html","id":"gauss-markov-theorem","chapter":"5 Linear Regression","heading":"5.1.2.2.2 Gauss-Markov Theorem","text":"linear regression model:\\[\n\\mathbf{y = X\\beta + \\epsilon},\n\\]assumptions:A1: Linearity model.A2: Full rank \\(\\mathbf{X}\\).A3: Exogeneity \\(\\mathbf{X}\\).A4: Homoskedasticity \\(\\epsilon\\).Ordinary Least Squares (OLS) estimator:\\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y},\n\\]Best Linear Unbiased Estimator (BLUE). means \\(\\hat{\\beta}\\) minimum variance among linear unbiased estimators \\(\\beta\\).1. UnbiasednessSuppose consider linear estimator \\(\\beta\\) form:\\[\n\\tilde{\\beta} = \\mathbf{C\\,y},\n\\]\\(\\mathbf{y}\\) \\(n \\times 1\\) vector observations,\\(\\mathbf{C}\\) \\(k \\times n\\) matrix (\\(k\\) dimension \\(\\beta\\)) depends design matrix \\(\\mathbf{X}\\).regression model \\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\epsilon},\n\\quad\nE[\\boldsymbol{\\epsilon} \\mid \\mathbf{X}] = \\mathbf{0},\n\\quad\n\\mathrm{Var}(\\boldsymbol{\\epsilon} \\mid \\mathbf{X}) = \\sigma^2 \\mathbf{}.\n\\]say \\(\\tilde{\\beta}\\) unbiased conditional expectation (given \\(\\mathbf{X}\\)) equals true parameter \\(\\beta\\):\\[\nE(\\tilde{\\beta} \\mid \\mathbf{X})\n=\nE(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\n\\beta.\n\\]Substitute \\(\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\epsilon}\\):\\[\nE(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\nE\\bigl(\\mathbf{C}(\\mathbf{X}\\beta + \\boldsymbol{\\epsilon}) \\mid \\mathbf{X}\\bigr)\n=\n\\mathbf{C\\,X}\\,\\beta\n+\n\\mathbf{C}\\,E(\\boldsymbol{\\epsilon} \\mid \\mathbf{X})\n=\n\\mathbf{C\\,X}\\,\\beta.\n\\]hold \\(\\beta\\), require\\[\n\\mathbf{C\\,X} = \\mathbf{}.\n\\]words, \\(\\mathbf{C}\\) must “right-inverse” \\(\\mathbf{X}\\).hand, OLS estimator \\(\\hat{\\beta}\\) given \\[\n\\hat{\\beta}\n=\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{y}.\n\\]can verify:Let \\(\\mathbf{C}_{\\text{OLS}} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\).\\[\n\\mathbf{C}_{\\text{OLS}}\\,\\mathbf{X}\n=\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X}\n=\n\\mathbf{}.\n\\]argument , makes \\(\\hat{\\beta}\\) unbiased.Hence, linear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased must satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\).2. Minimum Variance (Gauss–Markov Part)Among estimators form \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased (\\(\\mathbf{C\\,X} = \\mathbf{}\\)), OLS achieves smallest covariance matrix.Variance General Unbiased EstimatorIf \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) \\(\\mathbf{C\\,X} = \\mathbf{}\\), :\\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n=\n\\mathrm{Var}(\\mathbf{C\\,y} \\mid \\mathbf{X})\n=\n\\mathbf{C}\\,\\mathrm{Var}(\\mathbf{y} \\mid \\mathbf{X})\\,\\mathbf{C}'\n=\n\\mathbf{C}\\bigl(\\sigma^2 \\mathbf{}\\bigr)\\mathbf{C}'\n=\n\\sigma^2\\,\\mathbf{C}\\,\\mathbf{C}'.\n\\]Variance OLS EstimatorFor OLS, \\(\\hat{\\beta} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{y}\\). Thus,\\[\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2\\,(\\mathbf{X}'\\mathbf{X})^{-1}.\n\\]Comparing \\(\\mathrm{Var}(\\tilde{\\beta})\\) \\(\\mathrm{Var}(\\hat{\\beta})\\)want show:\\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n\\;\\;\\text{positive semi-definite.}\n\\]Since \\(\\tilde{\\beta}\\) \\(\\hat{\\beta}\\) unbiased, know: \\[\n\\mathbf{C\\,X} = \\mathbf{},\n\\quad\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X} = \\mathbf{}.\n\\]Since \\(\\tilde{\\beta}\\) \\(\\hat{\\beta}\\) unbiased, know: \\[\n\\mathbf{C\\,X} = \\mathbf{},\n\\quad\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\,\\mathbf{X} = \\mathbf{}.\n\\]One can show algebraically (proof provided ) \\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2 \\bigl[\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\bigr].\n\\] condition \\(\\mathbf{C\\,X} = \\mathbf{}\\), difference \\(\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\) positive semi-definite.One can show algebraically (proof provided ) \\[\n\\mathrm{Var}(\\tilde{\\beta} \\mid \\mathbf{X})\n-\n\\mathrm{Var}(\\hat{\\beta} \\mid \\mathbf{X})\n=\n\\sigma^2 \\bigl[\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\bigr].\n\\] condition \\(\\mathbf{C\\,X} = \\mathbf{}\\), difference \\(\\mathbf{C}\\mathbf{C}' - (\\mathbf{X}'\\mathbf{X})^{-1}\\) positive semi-definite.Positive semi-definite difference meansPositive semi-definite difference means\\[\n\\mathbf{v}' \\Bigl(\\mathbf{C}\\mathbf{C}'\n-\n(\\mathbf{X}'\\mathbf{X})^{-1}\\Bigr)\\mathbf{v}\n\\ge\n0\n\\quad\n\\text{vectors } \\mathbf{v}.\n\\]Hence, \\(\\hat{\\beta}\\) smallest variance (sense covariance matrices) among linear unbiased estimators \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\).Summary Key PointsUnbiasedness:\nlinear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased \\(E(\\tilde{\\beta}\\mid \\mathbf{X}) = \\beta\\).\nforces \\(\\mathbf{C\\,X} = \\mathbf{}\\).Unbiasedness:\nlinear estimator \\(\\tilde{\\beta} = \\mathbf{C\\,y}\\) unbiased \\(E(\\tilde{\\beta}\\mid \\mathbf{X}) = \\beta\\).\nforces \\(\\mathbf{C\\,X} = \\mathbf{}\\).OLS Unbiased:\nOLS estimator \\(\\hat{\\beta} = (X'X)^{-1} X' \\, y\\) satisfies \\((X'X)^{-1} X' \\, X = \\), hence unbiased.OLS Unbiased:\nOLS estimator \\(\\hat{\\beta} = (X'X)^{-1} X' \\, y\\) satisfies \\((X'X)^{-1} X' \\, X = \\), hence unbiased.OLS Minimum Variance:\nAmong \\(\\mathbf{C}\\) satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\), matrix \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) gives smallest possible \\(\\mathrm{Var}(\\tilde{\\beta})\\).\nmatrix form, \\(\\mathrm{Var}(\\tilde{\\beta}) - \\mathrm{Var}(\\hat{\\beta})\\) positive semi-definite, showing OLS optimal (Best Linear Unbiased Estimator, BLUE).OLS Minimum Variance:\nAmong \\(\\mathbf{C}\\) satisfy \\(\\mathbf{C\\,X} = \\mathbf{}\\), matrix \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) gives smallest possible \\(\\mathrm{Var}(\\tilde{\\beta})\\).\nmatrix form, \\(\\mathrm{Var}(\\tilde{\\beta}) - \\mathrm{Var}(\\hat{\\beta})\\) positive semi-definite, showing OLS optimal (Best Linear Unbiased Estimator, BLUE).","code":""},{"path":"linear-regression.html","id":"finite-sample-properties","chapter":"5 Linear Regression","heading":"5.1.2.3 Finite Sample Properties","text":"finite sample properties estimator considered sample size \\(n\\) fixed (asymptotically large). Key properties include bias, distribution, standard deviation estimator.Bias measures close estimator , average, true parameter value \\(\\beta\\). defined :\\[\n\\text{Bias} = E(\\hat{\\beta}) - \\beta\n\\]:\\(\\beta\\): True parameter value.\\(\\hat{\\beta}\\): Estimator \\(\\beta\\).Unbiased Estimator: estimator unbiased :\\[\n\\text{Bias} = E(\\hat{\\beta}) - \\beta = 0 \\quad \\text{equivalently} \\quad E(\\hat{\\beta}) = \\beta\n\\]means estimator produce estimates , average, equal value trying estimate.estimator function random variables (data). distribution describes estimates vary across repeated samples. Key aspects include:Center: Mean distribution, relates bias.Spread: Variability estimator, captured standard deviation variance.standard deviation estimator measures spread sampling distribution. indicates variability estimator across different samples.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares-properties","chapter":"5 Linear Regression","heading":"5.1.2.3.1 Ordinary Least Squares Properties","text":"standard assumptions OLS:A1: relationship \\(Y\\) \\(X\\) linear.A2: matrix \\(\\mathbf{X'X}\\) invertible.A3: \\(E(\\epsilon|X) = 0\\) (errors uncorrelated predictors).OLS unbiased assumptions. proof follows:\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= E(\\mathbf{(X'X)^{-1}X'y}) && \\text{A2}\\\\\n               &= E(\\mathbf{(X'X)^{-1}X'(X\\beta + \\epsilon)}) && \\text{A1}\\\\\n               &= E(\\mathbf{(X'X)^{-1}X'X\\beta + (X'X)^{-1}X'\\epsilon}) \\\\\n               &= E(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n               &= \\beta + E(E(\\mathbf{(X'X)^{-1}X'\\epsilon}|X)) && \\text{LIE (Law Iterated Expectation)} \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'}E(\\epsilon|X)) \\\\\n               &= \\beta + E(\\mathbf{(X'X)^{-1}X'} \\cdot 0) && \\text{A3}\\\\\n               &= \\beta\n\\end{aligned}\n\\]Key Points:Linearity Expectation: Used separate terms involving \\(\\beta\\) \\(\\epsilon\\).Law Iterated Expectation (LIE): Simplifies nested expectations.Exogeneity Errors (A3): Ensures \\(E(\\epsilon|X) = 0\\), eliminating bias.Implications UnbiasednessOLS estimators centered around true value \\(\\beta\\) across repeated samples.small samples, OLS estimators may exhibit variability, expected value remains \\(\\beta\\).assumption exogeneity (A3) violated, OLS estimator becomes biased. Specifically, omitted variables endogeneity can introduce systematic errors estimation.Frisch-Waugh-Lovell Theorem:omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) (non-zero effect) omitted variable correlated included predictors (\\(\\mathbf{X_1'X_2} \\neq 0\\)), OLS estimator biased.bias arises omitted variable contributes variation dependent variable, effect incorrectly attributed predictors.","code":""},{"path":"linear-regression.html","id":"conditional-variance-of-ols-estimator","chapter":"5 Linear Regression","heading":"5.1.2.3.2 Conditional Variance of OLS Estimator","text":"assumptions A1, A2, A3, A4, conditional variance OLS estimator :\\[\n\\begin{aligned}\nVar(\\hat{\\beta}|\\mathbf{X}) &= Var(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon|X}) && \\text{A1-A2} \\\\\n    &= Var((\\mathbf{X'X)^{-1}X'\\epsilon|X}) \\\\\n    &= \\mathbf{(X'X)^{-1}X'} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X'X)^{-1}} \\\\\n    &= \\mathbf{(X'X)^{-1}X'} \\sigma^2 \\mathbf{X(X'X)^{-1}} && \\text{A4} \\\\\n    &= \\sigma^2 \\mathbf{(X'X)^{-1}}\n\\end{aligned}\n\\]result shows variance \\(\\hat{\\beta}\\) depends :\\(\\sigma^2\\): variance errors.\\(\\mathbf{X'X}\\): information content design matrix \\(\\mathbf{X}\\).","code":""},{"path":"linear-regression.html","id":"sources-of-variation-in-ols-estimator","chapter":"5 Linear Regression","heading":"5.1.2.3.3 Sources of Variation in OLS Estimator","text":"Unexplained Variation Dependent Variable: \\(\\sigma^2 = Var(\\epsilon_i|\\mathbf{X})\\)Large \\(\\sigma^2\\) indicates amount unexplained variation (noise) high relative explained variation (\\(\\mathbf{x_i \\beta}\\)).increases variance OLS estimator.Small Variation Predictor VariablesIf variance predictors (\\(Var(x_{i1}), Var(x_{i2}), \\dots\\)) small, design matrix \\(\\mathbf{X}\\) lacks information, leading :\nHigh variability \\(\\hat{\\beta}\\).\nPotential issues estimating coefficients accurately.\nHigh variability \\(\\hat{\\beta}\\).Potential issues estimating coefficients accurately.Small sample size exacerbates issue, fewer observations reduce robustness parameter estimates.Correlation Explanatory Variables (Collinearity)Strong correlation among explanatory variables creates problems:\n\\(x_{i1}\\) highly correlated linear combination \\(1, x_{i2}, x_{i3}, \\dots\\) contributes inflated standard errors \\(\\hat{\\beta}_1\\).\nIncluding many irrelevant variables exacerbates issue.\n\\(x_{i1}\\) highly correlated linear combination \\(1, x_{i2}, x_{i3}, \\dots\\) contributes inflated standard errors \\(\\hat{\\beta}_1\\).Including many irrelevant variables exacerbates issue.Perfect Collinearity:\n\\(x_1\\) perfectly determined linear combination predictors, matrix \\(\\mathbf{X'X}\\) becomes singular.\nviolates A2, making OLS impossible compute.\n\\(x_1\\) perfectly determined linear combination predictors, matrix \\(\\mathbf{X'X}\\) becomes singular.violates A2, making OLS impossible compute.Multicollinearity:\n\\(x_1\\) highly correlated (perfectly) linear combination variables, variance \\(\\hat{\\beta}_1\\) increases.\nMulticollinearity violate OLS assumptions weakens inference inflating standard errors.\n\\(x_1\\) highly correlated (perfectly) linear combination variables, variance \\(\\hat{\\beta}_1\\) increases.Multicollinearity violate OLS assumptions weakens inference inflating standard errors.","code":""},{"path":"linear-regression.html","id":"standard-errors","chapter":"5 Linear Regression","heading":"5.1.2.3.4 Standard Errors","text":"Standard errors measure variability estimator, specifically standard deviation \\(\\hat{\\beta}\\). crucial inference, quantify uncertainty associated parameter estimates.variance OLS estimator \\(\\hat{\\beta}\\) :\\[\nVar(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}}\n\\]:\\(\\sigma^2\\): Variance error terms.\\(\\mathbf{(X'X)^{-1}}\\): Inverse design matrix product, capturing geometry predictors.Estimation \\(\\sigma^2\\)assumptions A1 A5, can estimate \\(\\sigma^2\\) :\\[\ns^2 = \\frac{1}{n-k} \\sum_{=1}^{n} e_i^2 = \\frac{1}{n-k} SSR\n\\]:\\(n\\): Number observations.\\(k\\): Number predictors, including intercept.\\(e_i\\): Residuals regression model (\\(e_i = y_i - \\hat{y}_i\\)).\\(SSR\\): Sum squared residuals (\\(\\sum e_i^2\\)).degrees freedom adjustment (\\(n-k\\)) accounts fact residuals \\(e_i\\) true errors \\(\\epsilon_i\\). Since regression model uses \\(k\\) parameters, lose \\(k\\) degrees freedom estimating variance.standard error \\(\\sigma\\) :\\[\ns = \\sqrt{s^2}\n\\]However, \\(s\\) biased estimator \\(\\sigma\\) due Jensen’s Inequality.standard error regression coefficient \\(\\hat{\\beta}_{j-1}\\) :\\[\nSE(\\hat{\\beta}_{j-1}) = s \\sqrt{[(\\mathbf{X'X})^{-1}]_{jj}}\n\\]Alternatively, can expressed terms \\(SST_{j-1}\\) \\(R_{j-1}^2\\):\\[\nSE(\\hat{\\beta}_{j-1}) = \\frac{s}{\\sqrt{SST_{j-1}(1 - R_{j-1}^2)}}\n\\]:\\(SST_{j-1}\\): Total sum squares \\(x_{j-1}\\) regression \\(x_{j-1}\\) predictors.\\(R_{j-1}^2\\): Coefficient determination regression.formulation highlights role multicollinearity, \\(R_{j-1}^2\\) reflects correlation \\(x_{j-1}\\) predictors.","code":""},{"path":"linear-regression.html","id":"summary-of-finite-sample-properties-of-ols-under-different-assumptions","chapter":"5 Linear Regression","heading":"5.1.2.3.5 Summary of Finite Sample Properties of OLS Under Different Assumptions","text":"A1-A3: OLS unbiased. \\[ E(\\hat{\\beta}) = \\beta \\]A1-A3: OLS unbiased. \\[ E(\\hat{\\beta}) = \\beta \\]A1-A4: variance OLS estimator : \\[ Var(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}} \\]A1-A4: variance OLS estimator : \\[ Var(\\hat{\\beta}|\\mathbf{X}) = \\sigma^2 \\mathbf{(X'X)^{-1}} \\]A1-A4, A6: OLS estimator normally distributed: \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 \\mathbf{(X'X)^{-1}}) \\]A1-A4, A6: OLS estimator normally distributed: \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 \\mathbf{(X'X)^{-1}}) \\]A1-A4, Gauss-Markov Theorem: OLS BLUE (Best Linear Unbiased Estimator).A1-A4, Gauss-Markov Theorem: OLS BLUE (Best Linear Unbiased Estimator).A1-A5: standard errors \\(\\hat{\\beta}\\) unbiased estimators standard deviation \\(\\hat{\\beta}\\).A1-A5: standard errors \\(\\hat{\\beta}\\) unbiased estimators standard deviation \\(\\hat{\\beta}\\).","code":""},{"path":"linear-regression.html","id":"large-sample-properties","chapter":"5 Linear Regression","heading":"5.1.2.4 Large Sample Properties","text":"Large sample properties provide framework evaluate quality estimators finite sample properties either uninformative computationally infeasible. perspective becomes crucial modern data analysis, especially methods like GLS MLE, assumptions finite sample analysis may hold.Use Finite vs. Large Sample AnalysisFinite Sample Analysis:\nSmall sample sizes (e.g., \\(n < 30\\)).\nCritical studies exact results needed.\nUseful experimental designs case studies.\nFinite Sample Analysis:Small sample sizes (e.g., \\(n < 30\\)).Small sample sizes (e.g., \\(n < 30\\)).Critical studies exact results needed.Critical studies exact results needed.Useful experimental designs case studies.Useful experimental designs case studies.Large Sample Analysis:\nLarge datasets (e.g., \\(n > 100\\)).\nNecessary asymptotic approximations improve computational simplicity.\nLarge Sample Analysis:Large datasets (e.g., \\(n > 100\\)).Large datasets (e.g., \\(n > 100\\)).Necessary asymptotic approximations improve computational simplicity.Necessary asymptotic approximations improve computational simplicity.Key Concepts:Consistency:\nConsistency ensures estimator converges probability true parameter value sample size increases.\nMathematically, estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) : \\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]\nConsistency imply unbiasedness, unbiasedness guarantee consistency.\nConsistency ensures estimator converges probability true parameter value sample size increases.Mathematically, estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) : \\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]Consistency imply unbiasedness, unbiasedness guarantee consistency.Asymptotic Distribution:\nlimiting distribution describes shape scaled estimator \\(n \\\\infty\\).\nAsymptotic distributions often follow normality due Central Limit Theorem, underpins much inferential statistics.\nlimiting distribution describes shape scaled estimator \\(n \\\\infty\\).Asymptotic distributions often follow normality due Central Limit Theorem, underpins much inferential statistics.Asymptotic Variance:\nRepresents spread estimator respect limiting distribution.\nSmaller asymptotic variance implies greater precision large samples.\nRepresents spread estimator respect limiting distribution.Smaller asymptotic variance implies greater precision large samples.MotivationFinite Sample Properties, unbiasedness, rely strong assumptions like:A1 LinearityA3 Exogeneity Independent VariablesA4 HomoskedasticityA6 Normal DistributionWhen assumptions violated impractical verify, finite sample properties lose relevance. cases, Large Sample Propertiesserve essential alternative evaluating estimators.example, let conditional expectation function (CEF) : \\[\n\\mu(\\mathbf{X}) = E(y | \\mathbf{X}),\n\\] represents minimum mean squared predictor possible functions \\(f(\\mathbf{X})\\): \\[\n\\min_f E((y - f(\\mathbf{X}))^2).\n\\]assumptions A1 A3, CEF simplifies : \\[\n\\mu(\\mathbf{X}) = \\mathbf{X}\\beta.\n\\]linear projection given : \\[\nL(y | 1, \\mathbf{X}) = \\gamma_0 + \\mathbf{X}\\text{Var}(\\mathbf{X})^{-1}\\text{Cov}(\\mathbf{X}, y),\n\\] : \\[\n\\gamma = \\mathbf{X}\\text{Var}(\\mathbf{X})^{-1}\\text{Cov}(\\mathbf{X}, y).\n\\]linear projection minimizes mean squared error: \\[\n(\\gamma_0, \\gamma) = \\arg\\min_{(, b)} E\\left[\\left(E(y|\\mathbf{X}) - \\left(+ \\mathbf{X}b\\right)\\right)^2\\right].\n\\]Implications OLSConsistency: OLS always consistent linear projection, ensuring convergence true parameter value \\(n \\\\infty\\).Causal Interpretation: linear projection inherent causal interpretation—approximates conditional mean function.Assumption Independence: Unlike CEF, linear projection depend assumptions A1 A3.Evaluating Estimators via Large Sample PropertiesConsistency:\nMeasures estimator’s centrality true value.\nconsistent estimator ensures larger samples, estimates become arbitrarily close population parameter.\nMeasures estimator’s centrality true value.consistent estimator ensures larger samples, estimates become arbitrarily close population parameter.Limiting Distribution:\nHelps infer sampling behavior estimator \\(n\\) grows.\nOften approximated normal distribution practical use hypothesis testing confidence interval construction.\nHelps infer sampling behavior estimator \\(n\\) grows.Often approximated normal distribution practical use hypothesis testing confidence interval construction.Asymptotic Variance:\nQuantifies dispersion estimator around limiting distribution.\nSmaller variance desirable greater reliability.\nQuantifies dispersion estimator around limiting distribution.Smaller variance desirable greater reliability.estimator \\(\\hat{\\theta}\\) consistent parameter \\(\\theta\\) , sample size \\(n\\) increases, \\(\\hat{\\theta}\\) converges probability \\(\\theta\\):\\[\n\\hat{\\theta}_n \\^p \\theta \\quad \\text{} n \\\\infty.\n\\]Convergence Probability:\nprobability \\(\\hat{\\theta}\\) deviates \\(\\theta\\) small margin (matter small) approaches zero \\(n\\) increases.\nFormally: \\[\n\\forall \\epsilon > 0, \\quad P(|\\hat{\\theta}_n - \\theta| > \\epsilon) \\0 \\quad \\text{} n \\\\infty.\n\\]Convergence Probability:probability \\(\\hat{\\theta}\\) deviates \\(\\theta\\) small margin (matter small) approaches zero \\(n\\) increases.Formally: \\[\n\\forall \\epsilon > 0, \\quad P(|\\hat{\\theta}_n - \\theta| > \\epsilon) \\0 \\quad \\text{} n \\\\infty.\n\\]Interpretation: Consistency ensures estimator becomes arbitrarily close true population parameter \\(\\theta\\) sample size grows.Interpretation: Consistency ensures estimator becomes arbitrarily close true population parameter \\(\\theta\\) sample size grows.Asymptotic Behavior: Large sample properties rely consistency provide valid approximations estimator’s behavior finite samples.Asymptotic Behavior: Large sample properties rely consistency provide valid approximations estimator’s behavior finite samples.Relationship Consistency UnbiasednessUnbiasedness:\nestimator \\(\\hat{\\theta}\\) unbiased expected value equals true parameter: \\[\nE(\\hat{\\theta}) = \\theta.\n\\]\nUnbiasedness finite-sample property depend sample size.\nestimator \\(\\hat{\\theta}\\) unbiased expected value equals true parameter: \\[\nE(\\hat{\\theta}) = \\theta.\n\\]Unbiasedness finite-sample property depend sample size.Consistency:\nConsistency large-sample property requires \\(\\hat{\\theta}\\) converge \\(\\theta\\) \\(n \\\\infty\\).\nConsistency large-sample property requires \\(\\hat{\\theta}\\) converge \\(\\theta\\) \\(n \\\\infty\\).Important DistinctionsUnbiasedness Imply Consistency:\nExample: Consider unbiased estimator extremely high variance diminish \\(n\\) increases. estimator converge \\(\\theta\\) probability.\nExample: Consider unbiased estimator extremely high variance diminish \\(n\\) increases. estimator converge \\(\\theta\\) probability.Consistency Imply Unbiasedness:\nExample: \\(\\hat{\\theta}_n = \\frac{n-1}{n}\\theta\\) biased finite \\(n\\), \\(n \\\\infty\\), \\(\\hat{\\theta}_n \\^p \\theta\\), making consistent.\nExample: \\(\\hat{\\theta}_n = \\frac{n-1}{n}\\theta\\) biased finite \\(n\\), \\(n \\\\infty\\), \\(\\hat{\\theta}_n \\^p \\theta\\), making consistent.OLS formula: \\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y},\n\\] can expand : \\[\n\\hat{\\beta} = \\mathbf{(\\sum_{=1}^{n}x_i'x_i)^{-1} \\sum_{=1}^{n}x_i'y_i},\n\\] equivalently: \\[\n\\hat{\\beta} = (n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}.\n\\]Taking probability limit assumptions A2 A5, apply Weak Law Large Numbers (random sample, averages converge expectations \\(n \\\\infty\\)): \\[\nplim(\\hat{\\beta}) = plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}),\n\\] simplifies : \\[\n\\begin{aligned}\nplim(\\hat{\\beta})\n  &= (E(\\mathbf{x_i'x_i}))^{-1}E(\\mathbf{x_i'y_i}) & \\text{A1}\\\\\n  &= (E(\\mathbf{x_i'x_i}))^{-1} \\bigl(E(\\mathbf{x_i'x_i} \\,\\beta) + E(\\mathbf{x_i\\,\\epsilon_i})\\bigr) & (A3a) \\\\\n  &= (E(\\mathbf{x_i' x_i}))^{-1}E(\\mathbf{x_i' x_i})\\, \\beta & (A2)\\\\\n  &= \\beta\n\\end{aligned}\n\\]Proof:model \\(y_i = x_i \\beta + \\epsilon_i\\), \\(\\beta\\) true parameter vector, \\(\\epsilon_i\\) random error:Expanding \\(E(x_i y_i)\\):\\[\nE(x_i'y_i) = E(x_i'(x_i\\beta + \\epsilon_i))\n\\]linearity expectation:\\[\nE(x_i'y_i) = E(x_i'x_i \\beta) + E(x_i \\epsilon_i)\n\\]second term \\(E(x_i \\epsilon_i) = 0\\) assumption A3a.Thus, \\[\nE(x_i'y_i) = E(x_i'x_i)\\beta.\n\\]Consistency OLSHence, short, assumptions:A1 LinearityA2 Full RankA3a: Weaker Exogeneity AssumptionA5 Data Generation (Random Sampling)term \\(E(\\mathbf{x_i'\\epsilon_i}) = 0\\), OLS estimator consistent: \\[\nplim(\\hat{\\beta}) = \\beta.\n\\]However, OLS consistency guarantee unbiasedness small samples.","code":""},{"path":"linear-regression.html","id":"asymptotic-distribution-of-ols","chapter":"5 Linear Regression","heading":"5.1.2.4.1 Asymptotic Distribution of OLS","text":"assumptions :A1 LinearityA2 Full RankA3a: Weaker Exogeneity AssumptionA5 Data Generation (Random Sampling)\\(\\mathbf{x_i'x_i}\\) finite first second moments (required Central Limit Theorem), :Convergence \\(n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i}\\): \\[\nn^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i} \\^p E(\\mathbf{x_i'x_i}).\n\\]Convergence \\(n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i}\\): \\[\nn^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i} \\^p E(\\mathbf{x_i'x_i}).\n\\]Asymptotic normality \\(\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i})\\): \\[\n\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}) \\^d N(0, \\mathbf{B}),\n\\] \\(\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i})\\).Asymptotic normality \\(\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i})\\): \\[\n\\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}) \\^d N(0, \\mathbf{B}),\n\\] \\(\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i})\\).results, scaled difference \\(\\hat{\\beta}\\) \\(\\beta\\) follows: \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) = (n^{-1}\\sum_{=1}^n \\mathbf{x_i'x_i})^{-1} \\sqrt{n}(n^{-1}\\sum_{=1}^n \\mathbf{x_i'\\epsilon_i}).\n\\]Central Limit Theorem: \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\^d N(0, \\Sigma),\n\\] : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1} \\mathbf{B} (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]sandwich form \\(\\Sigma\\) standard.Implications Homoskedasticity (A4) vs. HeteroskedasticityNo Homoskedasticity (A4 Homoskedasticity) needed:\nCLT large-sample distribution \\(\\hat{\\beta}\\) require homoskedasticity. place homoskedasticity simplify things \\[\n\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i}) = \\sigma^2 E(\\mathbf{x_i'x_i}),\n\\]\n\\(Var(\\epsilon_i | \\mathbf{x}_i) \\sigma^2\\)\n\\[\n\\Sigma = \\sigma^2 (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Homoskedasticity (A4 Homoskedasticity) needed:CLT large-sample distribution \\(\\hat{\\beta}\\) require homoskedasticity. place homoskedasticity simplify things \\[\n\\mathbf{B} = Var(\\mathbf{x_i'\\epsilon_i}) = \\sigma^2 E(\\mathbf{x_i'x_i}),\n\\]\\(Var(\\epsilon_i | \\mathbf{x}_i) \\sigma^2\\)\\[\n\\Sigma = \\sigma^2 (E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Adjusting Heteroskedasticity:\npractice, \\(\\sigma_i^2\\) can vary \\(\\mathbf{x}_i\\)​, leading heteroskedasticity.\nstandard OLS formula \\(Var(\\hat{\\beta})\\) inconsistent heteroskedasticity, one uses robust (White) standard errors.\nHeteroskedasticity can arise (limited ):\nLimited dependent variables.\nDependent variables large skewed ranges.\n\nAdjusting Heteroskedasticity:practice, \\(\\sigma_i^2\\) can vary \\(\\mathbf{x}_i\\)​, leading heteroskedasticity.standard OLS formula \\(Var(\\hat{\\beta})\\) inconsistent heteroskedasticity, one uses robust (White) standard errors.Heteroskedasticity can arise (limited ):\nLimited dependent variables.\nDependent variables large skewed ranges.\nLimited dependent variables.Dependent variables large skewed ranges.","code":""},{"path":"linear-regression.html","id":"derivation-of-asymptotic-variance","chapter":"5 Linear Regression","heading":"5.1.2.4.2 Derivation of Asymptotic Variance","text":"asymptotic variance OLS estimator derived follows:\\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Substituting \\(\\mathbf{B} = Var(\\mathbf{x_i'}\\epsilon_i)\\): \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Substituting \\(\\mathbf{B} = Var(\\mathbf{x_i'}\\epsilon_i)\\): \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Using definition variance: \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i - 0)(\\mathbf{x_i'}\\epsilon_i - 0)'](E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Using definition variance: \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i - 0)(\\mathbf{x_i'}\\epsilon_i - 0)'](E(\\mathbf{x_i'x_i}))^{-1}.\n\\][Law Iterated Expectations] A3a: Weaker Exogeneity Assumption, : \\[\nE[(\\mathbf{x_i'}\\epsilon_i)(\\mathbf{x_i'}\\epsilon_i)'] = E[E(\\epsilon_i^2|\\mathbf{x_i})\\mathbf{x_i'x_i}],\n\\][Law Iterated Expectations] A3a: Weaker Exogeneity Assumption, : \\[\nE[(\\mathbf{x_i'}\\epsilon_i)(\\mathbf{x_i'}\\epsilon_i)'] = E[E(\\epsilon_i^2|\\mathbf{x_i})\\mathbf{x_i'x_i}],\n\\]Assuming homoskedasticity (A4 Homoskedasticity), \\(E(\\epsilon_i^2|\\mathbf{x_i}) = \\sigma^2\\), : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Assuming homoskedasticity (A4 Homoskedasticity), \\(E(\\epsilon_i^2|\\mathbf{x_i}) = \\sigma^2\\), : \\[\n\\Sigma = (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Simplifying: \\[\n\\Sigma = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Simplifying: \\[\n\\Sigma = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]Hence, assumptions:A1 LinearityA1 LinearityA2 Full RankA2 Full RankA3a: Weaker Exogeneity AssumptionA3a: Weaker Exogeneity AssumptionA4 HomoskedasticityA4 HomoskedasticityA5 Data Generation (Random Sampling)A5 Data Generation (Random Sampling)\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\^d N(0, \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}).\n\\]asymptotic variance provides approximation scaled estimator’s variance large \\(n\\). leads :\\[\nAvar(\\sqrt{n}(\\hat{\\beta} - \\beta)) = \\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}.\n\\]finite sample variance estimator can approximated using asymptotic variance large sample sizes:\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta)) &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta})\n\\end{aligned}\n\\]However, critical note asymptotic variance (\\(Avar(.)\\)) behave manner finite sample variance (\\(Var(.)\\)). distinction evident following expressions:\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\\n&\\neq Avar(\\hat{\\beta})\n\\end{aligned}\n\\]Thus, \\(Avar(.)\\) provides useful approximation large samples, conceptual properties differ finite sample variance.Finite Sample Properties, standard errors calculated estimates conditional standard deviation:\\[\nSE_{fs}(\\hat{\\beta}_{j-1}) = \\sqrt{\\hat{Var}(\\hat{\\beta}_{j-1}|\\mathbf{X})} = \\sqrt{s^2 \\cdot [\\mathbf{(X'X)}^{-1}]_{jj}},\n\\]:\\(s^2\\) estimator error variance,\\(s^2\\) estimator error variance,\\([\\mathbf{(X'X)}^{-1}]_{jj}\\) represents \\(j\\)th diagonal element inverse design matrix.\\([\\mathbf{(X'X)}^{-1}]_{jj}\\) represents \\(j\\)th diagonal element inverse design matrix.contrast, Large Sample Properties, standard errors calculated estimates square root asymptotic variance:\\[\nSE_{ls}(\\hat{\\beta}_{j-1}) = \\sqrt{\\hat{Avar}(\\sqrt{n} \\hat{\\beta}_{j-1})/n} = \\sqrt{s^2 \\cdot [\\mathbf{(X'X)}^{-1}]_{jj}}.\n\\]Interestingly, standard error estimator identical finite large samples:expressions \\(SE_{fs}\\) \\(SE_{ls}\\) mathematically .However, conceptually estimating two different quantities:\nFinite Sample Standard Error: estimate conditional standard deviation \\(\\hat{\\beta}_{j-1}\\) given \\(\\mathbf{X}\\).\nLarge Sample Standard Error: estimate square root asymptotic variance \\(\\hat{\\beta}_{j-1}\\).\nFinite Sample Standard Error: estimate conditional standard deviation \\(\\hat{\\beta}_{j-1}\\) given \\(\\mathbf{X}\\).Large Sample Standard Error: estimate square root asymptotic variance \\(\\hat{\\beta}_{j-1}\\).assumptions required estimators valid differ stringency:Finite Sample Variance (Conditional Variance):\nRequires stronger assumptions (A1-A5).\nRequires stronger assumptions (A1-A5).Asymptotic Variance:\nValid weaker assumptions (A1, A2, A3a, A4, A5).\nValid weaker assumptions (A1, A2, A3a, A4, A5).distinction highlights utility asymptotic properties providing robust approximations finite sample assumptions may hold.","code":""},{"path":"linear-regression.html","id":"diagnostics","chapter":"5 Linear Regression","heading":"5.1.2.5 Diagnostics","text":"","code":""},{"path":"linear-regression.html","id":"normality-of-errors","chapter":"5 Linear Regression","heading":"5.1.2.5.1 Normality of Errors","text":"Ensuring normality errors critical assumption many regression models. Deviations assumption can impact inference model interpretation. Diagnostics assessing normality include:Methods Based Empirical Cumulative Distribution Function: ECDF residuals can compared cumulative distribution function (CDF) standard normal distribution.Plots invaluable visual inspection normality. One common approach Q-Q plot, compares quantiles residuals standard normal distribution:","code":"\n# Example Q-Q plot\nset.seed(123) # For reproducibility\ny <- 1:100\nx <- rnorm(100) # Generating random normal data\nqqplot(x,\n       y,\n       main = \"Q-Q Plot\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")"},{"path":"linear-regression.html","id":"influential-observations-and-outliers","chapter":"5 Linear Regression","heading":"5.1.2.5.2 Influential Observations and Outliers","text":"Identifying influential observations outliers essential robust regression modeling. hat matrix (\\(\\mathbf{H}\\)) plays key role diagnosing influence.","code":""},{},{},{"path":"linear-regression.html","id":"identifying-influential-cases","chapter":"5 Linear Regression","heading":"5.1.2.5.3 Identifying Influential Cases","text":"influential, refer observations whose exclusion causes major changes fitted regression model. Note outliers influential.Types Influence MeasuresInfluence Single Fitted Values: DFFITSInfluence Fitted Values: Cook’s DInfluence Regression Coefficients: DFBETASMeasures like Cook’s D, DFFITS, DFBETAS combine leverage (hat matrix) residual size (studentized residuals) assess influence observation model whole. Hence, effectively combine impact \\(X\\)-space \\(Y\\)-space.","code":""},{},{},{},{"path":"linear-regression.html","id":"collinearity","chapter":"5 Linear Regression","heading":"5.1.2.5.4 Collinearity","text":"Collinearity (multicollinearity) refers correlation among explanatory variables regression model. can lead various issues, including:Large changes estimated regression coefficients predictor variable added removed, observations altered.Non-significant results individual tests regression coefficients important predictor variables.Regression coefficients signs opposite theoretical expectations prior experience.Large coefficients simple correlation pairs predictor variables correlation matrix.Wide confidence intervals regression coefficients representing important predictor variables.\\(X\\) variables highly correlated, inverse \\((X'X)^{-1}\\) either exist computationally unstable. can result :Non-interpretability parameters: \\[\\mathbf{b = (X'X)^{-1}X'y}\\]Infinite sampling variability: \\[\\mathbf{s^2(b) = MSE (X'X)^{-1}}\\]predictor variables (\\(X\\)) “perfectly” correlated, system becomes undetermined, leading infinite number models fit data. Specifically:\\(X'X\\) singular, \\((X'X)^{-1}\\) exist.results poor parameter estimation invalid statistical inference.","code":""},{},{},{"path":"linear-regression.html","id":"constancy-of-error-variance","chapter":"5 Linear Regression","heading":"5.1.2.5.5 Constancy of Error Variance","text":"Testing constancy error variance (homoscedasticity) ensures assumptions linear regression met.two commonly used tests assess error variance.","code":""},{},{},{"path":"linear-regression.html","id":"independence","chapter":"5 Linear Regression","heading":"5.1.2.5.6 Independence","text":"Testing independence ensures residuals regression model uncorrelated. Violation assumption may lead biased inefficient parameter estimates. , discuss three primary methods diagnosing dependence: plots, Durbin-Watson test, specific methods time-series spatial data.","code":""},{},{},{},{},{"path":"linear-regression.html","id":"generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2 Generalized Least Squares","text":"","code":""},{"path":"linear-regression.html","id":"infeasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2.1 Infeasible Generalized Least Squares","text":"Motivation Efficient EstimatorThe Gauss-Markov Theorem guarantees OLS Best Linear Unbiased Estimator (BLUE) assumptions A1-A4:\nA4: \\(Var(\\epsilon | \\mathbf{X}) = \\sigma^2 \\mathbf{}_n\\) (homoscedasticity autocorrelation).\nA4: \\(Var(\\epsilon | \\mathbf{X}) = \\sigma^2 \\mathbf{}_n\\) (homoscedasticity autocorrelation).A4 hold:\nHeteroskedasticity: \\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma^2\\).\nSerial Correlation: \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\\) (\\(\\neq j\\)).\nHeteroskedasticity: \\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma^2\\).Serial Correlation: \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\\) (\\(\\neq j\\)).Without A4, OLS unbiased longer efficient. motivates need alternative approach identify efficient estimator.unweighted (standard) regression model given :\\[\n\\mathbf{y} = \\mathbf{X \\beta} + \\boldsymbol{\\epsilon}\n\\]Assuming A1-A3 hold (linearity, full rank, exogeneity), A4 , variance error term longer proportional identity\\[\nVar(\\boldsymbol{\\epsilon} | \\mathbf{X}) = \\boldsymbol{\\Omega} \\neq \\sigma^2 \\mathbf{}_n.\n\\]address violation A4 (\\(\\boldsymbol{\\Omega} \\neq \\sigma^2 \\mathbf{}_n\\)), one can transform model premultiplying sides full-rank matrix \\(\\mathbf{w}\\) weighted (transformed) regression model:\\[\n\\mathbf{w y} = \\mathbf{w X \\beta} + \\mathbf{w \\epsilon},\n\\]\\(\\mathbf{w}\\) full-rank matrix chosen :\\[\n\\mathbf{w'w} = \\boldsymbol{\\Omega}^{-1}.\n\\]\\(\\mathbf{w}\\) Cholesky Decomposition \\(\\boldsymbol{\\Omega}^{-1}\\).Cholesky decomposition ensures \\(\\mathbf{w}\\) satisfies \\(\\mathbf{w'w = \\Omega^{-1}}\\), \\(\\mathbf{w}\\) “square root” \\(\\boldsymbol{\\Omega}^{-1}\\) matrix terms.transforming original model, variance transformed errors becomes:\\[\n\\begin{aligned}\n\\boldsymbol{\\Omega} &= Var(\\boldsymbol{\\epsilon} | \\mathbf{X}), \\\\\n\\boldsymbol{\\Omega}^{-1} &= Var(\\boldsymbol{\\epsilon} | \\mathbf{X})^{-1}.\n\\end{aligned}\n\\]transformed equation allows us compute efficient estimator.Using transformed model, Infeasible Generalized Least Squares (IGLS) estimator :\\[\n\\begin{aligned}\n\\mathbf{\\hat{\\beta}_{IGLS}} &= \\mathbf{(X'w'wX)^{-1}X'w'wy} \\\\\n&= \\mathbf{(X' \\boldsymbol{\\Omega}^{-1} X)^{-1} X' \\boldsymbol{\\Omega}^{-1} y} \\\\\n&= \\mathbf{\\beta + (X' \\boldsymbol{\\Omega}^{-1} X)^{-1} X' \\boldsymbol{\\Omega}^{-1} \\boldsymbol{\\epsilon}}.\n\\end{aligned}\n\\]UnbiasednessSince assumptions A1-A3 hold unweighted model:\\[\n\\begin{aligned}\n\\mathbf{E(\\hat{\\beta}_{IGLS}|\\mathbf{X})}\n&= \\mathbf{E(\\beta + (X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)|\\mathbf{X})} \\\\\n&= \\mathbf{\\beta + E(X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}E(\\epsilon|\\mathbf{X})}  && \\text{since A3: } E(\\epsilon|\\mathbf{X})=0, \\\\\n&= \\mathbf{\\beta}.\n\\end{aligned}\n\\]Thus, IGLS estimator unbiased.VarianceThe variance transformed errors given :\\[\n\\begin{aligned}\n\\mathbf{Var(w\\epsilon|\\mathbf{X})}\n&= \\mathbf{wVar(\\epsilon|\\mathbf{X})w'} \\\\\n&= \\mathbf{w\\Omega w'} \\\\\n&= \\mathbf{w(w'w)^{-1}w'} && \\text{since } \\mathbf{w} \\text{ full-rank,} \\\\\n&= \\mathbf{ww^{-1}(w')^{-1}w'} \\\\\n&= \\mathbf{I_n}.\n\\end{aligned}\n\\]Hence, A4 holds transformed (weighted) equation, satisfying Gauss-Markov conditions.variance IGLS estimator :\\[\n\\begin{aligned}\n\\mathbf{Var(\\hat{\\beta}_{IGLS}|\\mathbf{X})}\n&= \\mathbf{Var(\\beta + (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{Var((X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|\\mathbf{X})} \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1} Var(\\epsilon|\\mathbf{X}) \\Omega^{-1}X(X'\\Omega^{-1}X)^{-1}} && \\text{A4 holds}, \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X'\\Omega^{-1}X)^{-1}}, \\\\\n&= \\mathbf{(X'\\Omega^{-1}X)^{-1}}.\n\\end{aligned}\n\\]EfficiencyThe difference variances OLS IGLS :\\[\n\\mathbf{Var(\\hat{\\beta}_{OLS}|\\mathbf{X}) - Var(\\hat{\\beta}_{IGLS}|\\mathbf{X})} = \\mathbf{\\Omega '},\n\\]:\\[\n\\mathbf{= (X'X)^{-1}X' - (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}}.\n\\]Since \\(\\mathbf{\\Omega}\\) positive semi-definite, \\(\\mathbf{\\Omega '}\\) also positive semi-definite. Thus, IGLS estimator efficient OLS heteroskedasticity autocorrelation.short, properties \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\):Unbiasedness: \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\) remains unbiased long A1-A3 hold.Efficiency: \\(\\mathbf{\\hat{\\beta}_{IGLS}}\\) efficient OLS heteroskedasticity serial correlation since accounts structure \\(\\boldsymbol{\\Omega}\\).IGLS “Infeasible”?name infeasible arises generally impossible compute estimator directly due structure \\(\\mathbf{w}\\) (equivalently \\(\\boldsymbol{\\Omega}^{-1}\\)). matrix \\(\\mathbf{w}\\) defined :\\[\n\\mathbf{w} =\n\\begin{pmatrix}\nw_{11} & 0 & 0 & \\cdots & 0 \\\\\nw_{21} & w_{22} & 0 & \\cdots & 0 \\\\\nw_{31} & w_{32} & w_{33} & \\cdots & \\cdots \\\\\nw_{n1} & w_{n2} & w_{n3} & \\cdots & w_{nn} \\\\\n\\end{pmatrix},\n\\]\\(n(n+1)/2\\) unique elements \\(n\\) observations. results parameters data points, making direct estimation infeasible.make estimation feasible, assumptions structure \\(\\mathbf{\\Omega}\\) required. Common approaches include:Heteroskedasticity Errors: Assume multiplicative exponential model variance, \\(Var(\\epsilon_i|\\mathbf{X}) = \\sigma_i^2\\).\nAssume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]\nEstimate \\(\\sigma_i^2\\) using methods :\nModeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).\n\nHeteroskedasticity Errors: Assume multiplicative exponential model variance, \\(Var(\\epsilon_i|\\mathbf{X}) = \\sigma_i^2\\).Assume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]Assume correlation errors, allow heterogeneous variances: \\[ \\mathbf{\\Omega} = \\begin{pmatrix} \\sigma_1^2 & 0          & \\cdots & 0 \\\\ 0          & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots     & \\vdots     & \\ddots & \\vdots \\\\ 0          & 0          & \\cdots & \\sigma_n^2 \\end{pmatrix}. \\]Estimate \\(\\sigma_i^2\\) using methods :\nModeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).\nEstimate \\(\\sigma_i^2\\) using methods :Modeling \\(\\sigma_i^2\\) function predictors (e.g., \\(\\sigma_i^2 = \\exp(\\mathbf{x}_i \\gamma)\\)).Serial Correlation: Assume serial correlation follows autoregressive process AR(1) Model, e.g., \\(\\epsilon_t = \\rho \\epsilon_{t-1} + u_t\\) \\(Cov(\\epsilon_t, \\epsilon_{t -h}) = \\rho^h \\sigma^2\\), variance-covariance matrix -diagonal elements decaying geometrically: \\[ \\mathbf{\\Omega} = \\frac{\\sigma^2}{1-\\rho^2} \\begin{pmatrix} 1      & \\rho    & \\rho^2 & \\cdots & \\rho^{n-1} \\\\ \\rho   & 1       & \\rho   & \\cdots & \\rho^{n-2} \\\\ \\rho^2 & \\rho    & 1      & \\cdots & \\rho^{n-3} \\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots \\\\ \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\cdots & 1 \\end{pmatrix}. \\]Serial Correlation: Assume serial correlation follows autoregressive process AR(1) Model, e.g., \\(\\epsilon_t = \\rho \\epsilon_{t-1} + u_t\\) \\(Cov(\\epsilon_t, \\epsilon_{t -h}) = \\rho^h \\sigma^2\\), variance-covariance matrix -diagonal elements decaying geometrically: \\[ \\mathbf{\\Omega} = \\frac{\\sigma^2}{1-\\rho^2} \\begin{pmatrix} 1      & \\rho    & \\rho^2 & \\cdots & \\rho^{n-1} \\\\ \\rho   & 1       & \\rho   & \\cdots & \\rho^{n-2} \\\\ \\rho^2 & \\rho    & 1      & \\cdots & \\rho^{n-3} \\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots \\\\ \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\cdots & 1 \\end{pmatrix}. \\]Cluster Errors: Assume block-diagonal structure \\(\\mathbf{\\Omega}\\) account grouped panel data.Cluster Errors: Assume block-diagonal structure \\(\\mathbf{\\Omega}\\) account grouped panel data.assumption simplifies estimation \\(\\mathbf{\\Omega}\\) thus \\(\\mathbf{w}\\), enabling Feasible Generalized Least Squares fewer unknown parameters estimate.","code":""},{"path":"linear-regression.html","id":"feasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2.2 Feasible Generalized Least Squares","text":"","code":""},{"path":"linear-regression.html","id":"heteroskedasticity-errors","chapter":"5 Linear Regression","heading":"5.2.2.1 Heteroskedasticity Errors","text":"Heteroskedasticity occurs variance error term constant across observations. Specifically:\\[\nVar(\\epsilon_i | x_i) = E(\\epsilon_i^2 | x_i) \\neq \\sigma^2,\n\\]instead depends function \\(x_i\\):\\[\nVar(\\epsilon_i | x_i) = h(x_i) = \\sigma_i^2\n\\]violates assumption homoscedasticity (constant variance), impacting efficiency OLS estimates.model:\\[\ny_i = x_i\\beta + \\epsilon_i,\n\\]apply transformation standardize variance:\\[\n\\frac{y_i}{\\sigma_i} = \\frac{x_i}{\\sigma_i} \\beta + \\frac{\\epsilon_i}{\\sigma_i}.\n\\]scaling observation \\(1/\\sigma_i\\), variance transformed error term becomes:\\[\n\\begin{aligned}\nVar\\left(\\frac{\\epsilon_i}{\\sigma_i} \\bigg| X \\right) &= \\frac{1}{\\sigma_i^2} Var(\\epsilon_i | X) \\\\\n&= \\frac{1}{\\sigma_i^2} \\sigma_i^2 \\\\\n&= 1.\n\\end{aligned}\n\\]Thus, heteroskedasticity corrected transformed model.matrix notation, transformed model :\\[\n\\mathbf{w y} = \\mathbf{w X \\beta + w \\epsilon},\n\\]\\(\\mathbf{w}\\) weight matrix used standardize variance. weight matrix \\(\\mathbf{w}\\) defined :\\[\n\\mathbf{w} =\n\\begin{pmatrix}\n1/\\sigma_1 & 0          & 0          & \\cdots & 0 \\\\\n0          & 1/\\sigma_2 & 0          & \\cdots & 0 \\\\\n0          & 0          & 1/\\sigma_3 & \\cdots & 0 \\\\\n\\vdots     & \\vdots     & \\vdots     & \\ddots & \\vdots \\\\\n0          & 0          & 0          & \\cdots & 1/\\sigma_n\n\\end{pmatrix}.\n\\]presence heteroskedasticity, variance error term, \\(Var(\\epsilon_i|\\mathbf{x}_i)\\), constant across observations. leads inefficient OLS estimates.Infeasible Weighted Least Squares (IWLS) assumes variances \\(\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i)\\) known. allows us adjust regression equation correct heteroskedasticity.model transformed follows:\\[\ny_i = \\mathbf{x}_i\\beta + \\epsilon_i \\quad \\text{(original equation)},\n\\]\\(\\epsilon_i\\) variance \\(\\sigma_i^2\\). make errors homoskedastic, divide \\(\\sigma_i\\):\\[\n\\frac{y_i}{\\sigma_i} = \\frac{\\mathbf{x}_i}{\\sigma_i}\\beta + \\frac{\\epsilon_i}{\\sigma_i}.\n\\]Now, transformed error term \\(\\epsilon_i / \\sigma_i\\) constant variance 1:\\[\nVar\\left(\\frac{\\epsilon_i}{\\sigma_i} | \\mathbf{x}_i \\right) = 1.\n\\]IWLS estimator minimizes weighted sum squared residuals transformed model:\\[\n\\text{Minimize: } \\sum_{=1}^n \\left( \\frac{y_i - \\mathbf{x}_i\\beta}{\\sigma_i} \\right)^2.\n\\]matrix form, IWLS estimator :\\[\n\\hat{\\beta}_{IWLS} = (\\mathbf{X}'\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{W}\\mathbf{y},\n\\]\\(\\mathbf{W}\\) diagonal matrix weights:\\[\n\\mathbf{W} =\n\\begin{pmatrix}\n1/\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\sigma_n^2\n\\end{pmatrix}.\n\\]Properties IWLSValid Standard Errors:\n\\(Var(\\epsilon_i | \\mathbf{X}) = \\sigma_i^2\\), usual standard errors IWLS valid.\n\\(Var(\\epsilon_i | \\mathbf{X}) = \\sigma_i^2\\), usual standard errors IWLS valid.Robustness:\nvariance assumption incorrect (\\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma_i^2\\)), heteroskedasticity-robust standard errors must used instead.\nvariance assumption incorrect (\\(Var(\\epsilon_i | \\mathbf{X}) \\neq \\sigma_i^2\\)), heteroskedasticity-robust standard errors must used instead.primary issue IWLS \\(\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i)\\) generally unknown. Specifically, know:\\[\n\\sigma_i^2 = Var(\\epsilon_i|\\mathbf{x}_i) = E(\\epsilon_i^2|\\mathbf{x}_i).\n\\]challenges :Single Observation:\nobservation \\(\\), one \\(\\epsilon_i\\), insufficient estimate variance \\(\\sigma_i^2\\) directly.\nobservation \\(\\), one \\(\\epsilon_i\\), insufficient estimate variance \\(\\sigma_i^2\\) directly.Dependence Assumptions:\nestimate \\(\\sigma_i^2\\), must impose assumptions relationship \\(\\mathbf{x}_i\\).\nestimate \\(\\sigma_i^2\\), must impose assumptions relationship \\(\\mathbf{x}_i\\).make IWLS feasible, model \\(\\sigma_i^2\\) function predictors \\(\\mathbf{x}_i\\). common approach :\\[\n\\epsilon_i^2 = v_i \\exp(\\mathbf{x}_i\\gamma),\n\\]:\\(v_i\\) independent error term strictly positive values, representing random noise.\\(v_i\\) independent error term strictly positive values, representing random noise.\\(\\exp(\\mathbf{x}_i\\gamma)\\) deterministic function predictors \\(\\mathbf{x}_i\\).\\(\\exp(\\mathbf{x}_i\\gamma)\\) deterministic function predictors \\(\\mathbf{x}_i\\).Taking natural logarithm sides linearizes model:\\[\n\\ln(\\epsilon_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i),\n\\]\\(\\ln(v_i)\\) independent \\(\\mathbf{x}_i\\). transformation enables us estimate \\(\\gamma\\) using standard OLS techniques.Estimation Procedure Feasible GLS (FGLS)Since observe true errors \\(\\epsilon_i\\), approximate using OLS residuals \\(e_i\\). ’s step--step process:Compute OLS Residuals: First, fit original model using OLS calculate residuals:\n\\[\ne_i = y_i - \\mathbf{x}_i\\hat{\\beta}_{OLS}.\n\\]Compute OLS Residuals: First, fit original model using OLS calculate residuals:\\[\ne_i = y_i - \\mathbf{x}_i\\hat{\\beta}_{OLS}.\n\\]Approximate \\(\\epsilon_i^2\\) \\(e_i^2\\): Use squared residuals proxy squared errors:\n\\[\ne_i^2 \\approx \\epsilon_i^2.\n\\]Approximate \\(\\epsilon_i^2\\) \\(e_i^2\\): Use squared residuals proxy squared errors:\\[\ne_i^2 \\approx \\epsilon_i^2.\n\\]Log-Linear Model: Fit log-transformed model estimate \\(\\gamma\\):\n\\[\n\\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i).\n\\]\nEstimate \\(\\gamma\\) using OLS, \\(\\ln(v_i)\\) treated error term.Log-Linear Model: Fit log-transformed model estimate \\(\\gamma\\):\\[\n\\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i).\n\\]Estimate \\(\\gamma\\) using OLS, \\(\\ln(v_i)\\) treated error term.Estimate Variances: Use fitted values \\(\\hat{\\gamma}\\) estimate \\(\\sigma_i^2\\) observation:\n\\[\n\\hat{\\sigma}_i^2 = \\exp(\\mathbf{x}_i\\hat{\\gamma}).\n\\]Estimate Variances: Use fitted values \\(\\hat{\\gamma}\\) estimate \\(\\sigma_i^2\\) observation:\\[\n\\hat{\\sigma}_i^2 = \\exp(\\mathbf{x}_i\\hat{\\gamma}).\n\\]Perform Weighted Least Squares: Use estimated variances \\(\\hat{\\sigma}_i^2\\) construct weight matrix \\(\\mathbf{\\hat{W}}\\):\n\\[\n\\mathbf{\\hat{W}} =\n\\begin{pmatrix}\n1/\\hat{\\sigma}_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\hat{\\sigma}_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\hat{\\sigma}_n^2\n\\end{pmatrix}.\n\\]\n, compute Feasible GLS (FGLS) estimator:\n\\[\n\\hat{\\beta}_{FGLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}.\n\\]Perform Weighted Least Squares: Use estimated variances \\(\\hat{\\sigma}_i^2\\) construct weight matrix \\(\\mathbf{\\hat{W}}\\):\\[\n\\mathbf{\\hat{W}} =\n\\begin{pmatrix}\n1/\\hat{\\sigma}_1^2 & 0 & \\cdots & 0 \\\\\n0 & 1/\\hat{\\sigma}_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1/\\hat{\\sigma}_n^2\n\\end{pmatrix}.\n\\], compute Feasible GLS (FGLS) estimator:\\[\n\\hat{\\beta}_{FGLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}.\n\\]","code":""},{"path":"linear-regression.html","id":"serial-correlation","chapter":"5 Linear Regression","heading":"5.2.2.2 Serial Correlation","text":"Serial correlation (also called autocorrelation) occurs error terms regression model correlated across observations. Formally:\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0 \\quad \\text{} \\neq j.\n\\]violates Gauss-Markov assumption \\(Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) = 0\\), leading inefficiencies OLS estimates.","code":""},{"path":"linear-regression.html","id":"covariance-stationarity","chapter":"5 Linear Regression","heading":"5.2.2.2.1 Covariance Stationarity","text":"errors covariance stationary, covariance errors depends relative time positional difference (\\(h\\)), absolute position:\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{+h} | \\mathbf{x}_i, \\mathbf{x}_{+h}) = \\gamma_h,\n\\]\\(\\gamma_h\\) represents covariance lag \\(h\\).covariance stationarity, variance-covariance matrix error term \\(\\boldsymbol{\\epsilon}\\) takes following form:\\[\nVar(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\boldsymbol{\\Omega} =\n\\begin{pmatrix}\n\\sigma^2 & \\gamma_1 & \\gamma_2 & \\cdots & \\gamma_{n-1} \\\\\n\\gamma_1 & \\sigma^2 & \\gamma_1 & \\cdots & \\gamma_{n-2} \\\\\n\\gamma_2 & \\gamma_1 & \\sigma^2 & \\cdots & \\vdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\gamma_1 \\\\\n\\gamma_{n-1} & \\gamma_{n-2} & \\cdots & \\gamma_1 & \\sigma^2\n\\end{pmatrix}.\n\\]Key Points:diagonal elements represent variance error term: \\(\\sigma^2\\).-diagonal elements \\(\\gamma_h\\) represent covariances different lags \\(h\\).Serial Correlation Problem?matrix \\(\\boldsymbol{\\Omega}\\) introduces \\(n\\) parameters estimate (e.g., \\(\\sigma^2, \\gamma_1, \\gamma_2, \\ldots, \\gamma_{n-1}\\)). Estimating large number parameters becomes impractical, especially large datasets. address , impose additional structure reduce number parameters.","code":""},{"path":"linear-regression.html","id":"ar1","chapter":"5 Linear Regression","heading":"5.2.2.2.2 AR(1) Model","text":"AR(1) process, errors follow first-order autoregressive process:\\[\n\\begin{aligned}\ny_t &= \\beta_0 + x_t\\beta_1 + \\epsilon_t, \\\\\n\\epsilon_t &= \\rho \\epsilon_{t-1} + u_t,\n\\end{aligned}\n\\]:\\(\\rho\\) first-order autocorrelation coefficient, capturing relationship consecutive errors.\\(\\rho\\) first-order autocorrelation coefficient, capturing relationship consecutive errors.\\(u_t\\) white noise, satisfying \\(Var(u_t) = \\sigma_u^2\\) \\(Cov(u_t, u_{t-h}) = 0\\) \\(h \\neq 0\\).\\(u_t\\) white noise, satisfying \\(Var(u_t) = \\sigma_u^2\\) \\(Cov(u_t, u_{t-h}) = 0\\) \\(h \\neq 0\\).AR(1) assumption, variance-covariance matrix error term \\(\\boldsymbol{\\epsilon}\\) becomes:\\[\nVar(\\boldsymbol{\\epsilon} | \\mathbf{X}) = \\frac{\\sigma_u^2}{1-\\rho^2}\n\\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\cdots & \\rho^{n-1} \\\\\n\\rho & 1 & \\rho & \\cdots & \\rho^{n-2} \\\\\n\\rho^2 & \\rho & 1 & \\cdots & \\rho^{n-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\cdots & \\rho & 1\n\\end{pmatrix}.\n\\]Key Features:diagonal elements represent variance: \\(Var(\\epsilon_t | \\mathbf{X}) = \\sigma_u^2 / (1-\\rho^2)\\).-diagonal elements decay exponentially lag \\(h\\): \\(Cov(\\epsilon_t, \\epsilon_{t-h} | \\mathbf{X}) = \\rho^h \\cdot Var(\\epsilon_t | \\mathbf{X})\\).AR(1), one parameter \\(\\rho\\) needs estimated (addition \\(\\sigma_u^2\\)), greatly simplifying structure \\(\\boldsymbol{\\Omega}\\).OLS Properties AR(1)Consistency: assumptions A1, A2, A3a, A5a hold, OLS remains consistent.Asymptotic Normality: OLS estimates asymptotically normal.Inference Serial Correlation:\nStandard OLS errors invalid.\nUse Newey-West standard errors obtain robust inference.\nStandard OLS errors invalid.Use Newey-West standard errors obtain robust inference.","code":""},{"path":"linear-regression.html","id":"infeasible-cochrane-orcutt","chapter":"5 Linear Regression","heading":"5.2.2.2.3 Infeasible Cochrane-Orcutt","text":"Infeasible Cochrane-Orcutt procedure addresses serial correlation error terms assuming AR(1) process errors:\\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t,\n\\]\\(u_t\\) white noise \\(\\rho\\) autocorrelation coefficient.transforming original regression equation:\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t,\n\\]subtract \\(\\rho\\) times lagged equation:\\[\n\\rho y_{t-1} = \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}),\n\\]obtain weighted first-difference equation:\\[\ny_t - \\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t.\n\\]Key Points:Dependent Variable: \\(y_t - \\rho y_{t-1}\\).Independent Variable: \\(x_t - \\rho x_{t-1}\\).Error Term: \\(u_t\\), satisfies Gauss-Markov assumptions (A3, A4, A5).ICO estimator minimizes sum squared residuals transformed equation.Standard Errors:\nerrors truly follow AR(1) process, standard errors transformed equation valid.\ncomplex error structures, Newey-West HAC standard errors required.\nerrors truly follow AR(1) process, standard errors transformed equation valid.complex error structures, Newey-West HAC standard errors required.Loss Observations:\ntransformation involves first differences, means first observation (\\(y_1\\)) used. reduces effective sample size one.\ntransformation involves first differences, means first observation (\\(y_1\\)) used. reduces effective sample size one.Problem: \\(\\rho\\) UnknownThe ICO procedure infeasible requires knowledge \\(\\rho\\), autocorrelation coefficient. practice, estimate \\(\\rho\\) data.estimate \\(\\rho\\), use OLS residuals (\\(e_t\\)) proxy errors (\\(\\epsilon_t\\)). estimate \\(\\hat{\\rho}\\) given :\\[\n\\hat{\\rho} = \\frac{\\sum_{t=2}^{T} e_t e_{t-1}}{\\sum_{t=2}^{T} e_t^2}.\n\\]Estimation via OLS:Regress OLS residuals \\(e_t\\) lagged values \\(e_{t-1}\\), without intercept: \\[\ne_t = \\rho e_{t-1} + u_t.\n\\]slope regression estimate \\(\\hat{\\rho}\\).estimation efficient AR(1) assumption provides practical approximation \\(\\rho\\).","code":""},{"path":"linear-regression.html","id":"feasiable-prais-winsten","chapter":"5 Linear Regression","heading":"5.2.2.2.4 Feasible Prais-Winsten","text":"Feasible Prais-Winsten (FPW) method addresses AR(1) serial correlation regression models transforming data eliminate serial dependence errors. Unlike Infeasible Cochrane-Orcutt procedure, discards first observation, Prais-Winsten method retains using weighted transformation.FPW transformation uses following weighting matrix \\(\\mathbf{w}\\):\\[\n\\mathbf{w} =\n\\begin{pmatrix}\n\\sqrt{1 - \\hat{\\rho}^2} & 0 & 0 & \\cdots & 0 \\\\\n-\\hat{\\rho} & 1 & 0 & \\cdots & 0 \\\\\n0 & -\\hat{\\rho} & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & -\\hat{\\rho} & 1\n\\end{pmatrix}.\n\\]whereThe first row accounts transformation first observation, using \\(\\sqrt{1 - \\hat{\\rho}^2}\\).Subsequent rows represent AR(1) transformation remaining observations.Step--Step ProcedureStep 1: Initial OLS EstimationEstimate regression model using OLS:\\[\ny_t = \\mathbf{x}_t \\beta + \\epsilon_t,\n\\]compute residuals:\\[\ne_t = y_t - \\mathbf{x}_t \\hat{\\beta}.\n\\]Step 2: Estimate AR(1) Correlation CoefficientEstimate AR(1) correlation coefficient \\(\\rho\\) regressing \\(e_t\\) \\(e_{t-1}\\) without intercept:\\[\ne_t = \\rho e_{t-1} + u_t.\n\\]slope regression estimated \\(\\hat{\\rho}\\).Step 3: Transform DataApply transformation using weighting matrix \\(\\mathbf{w}\\) transform dependent variable \\(\\mathbf{y}\\) independent variables \\(\\mathbf{X}\\):\\[\n\\mathbf{wy} = \\mathbf{wX} \\beta + \\mathbf{w\\epsilon}.\n\\]Specifically: 1. \\(t=1\\), transformed dependent independent variables : \\[\n   \\tilde{y}_1 = \\sqrt{1 - \\hat{\\rho}^2} \\cdot y_1, \\quad \\tilde{\\mathbf{x}}_1 = \\sqrt{1 - \\hat{\\rho}^2} \\cdot \\mathbf{x}_1.\n   \\] 2. \\(t=2, \\dots, T\\), transformed variables : \\[\n   \\tilde{y}_t = y_t - \\hat{\\rho} \\cdot y_{t-1}, \\quad \\tilde{\\mathbf{x}}_t = \\mathbf{x}_t - \\hat{\\rho} \\cdot \\mathbf{x}_{t-1}.\n   \\]Step 4: Feasible Prais-Winsten EstimationRun OLS transformed equation:\\[\n\\mathbf{wy} = \\mathbf{wX} \\beta + \\mathbf{w\\epsilon}.\n\\]resulting estimator Feasible Prais-Winsten (FPW) estimator:\\[\n\\hat{\\beta}_{FPW} = (\\mathbf{X}'\\mathbf{w}'\\mathbf{w}\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{w}'\\mathbf{w}\\mathbf{y}.\n\\]Properties Feasible Prais-Winsten EstimatorInfeasible Prais-Winsten Estimator:\ninfeasible Prais-Winsten (PW) estimator assumes AR(1) parameter \\(\\rho\\) known.\nassumptions A1, A2, A3 unweighted equation, infeasible PW estimator unbiased efficient.\ninfeasible Prais-Winsten (PW) estimator assumes AR(1) parameter \\(\\rho\\) known.assumptions A1, A2, A3 unweighted equation, infeasible PW estimator unbiased efficient.Feasible Prais-Winsten (FPW) Estimator: FPW estimator replaces unknown \\(\\rho\\) estimate \\(\\hat{\\rho}\\) derived OLS residuals, introducing bias small samples.\nBias:\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.\n\nConsistency:\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\n\nNote: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.\n\nEfficiency\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]\nStandard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\n\n\nBias:\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.\nFPW estimator biased due estimation \\(\\hat{\\rho}\\), introduces additional layer approximation.Consistency:\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\n\nNote: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.\nFPW estimator consistent following assumptions:\nA1: model linear parameters.\nA2: independent variables linearly independent.\nA5: data generated random sampling.\nAdditionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).\nA1: model linear parameters.A2: independent variables linearly independent.A5: data generated random sampling.Additionally: \\[\nE\\big((\\mathbf{x_t - \\rho x_{t-1}})'\\big(\\epsilon_t - \\rho \\epsilon_{t-1}\\big)\\big) = 0.\n\\] condition ensures transformed error term \\(\\epsilon_t - \\rho \\epsilon_{t-1}\\) uncorrelated transformed regressors \\(\\mathbf{x_t - \\rho x_{t-1}}\\).Note: A3a (zero conditional mean error term, \\(E(\\epsilon_t|\\mathbf{x}_t) = 0\\)) sufficient condition. Full exogeneity independent variables (A3) required.Efficiency\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]\nStandard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\n\nAsymptotic Efficiency: FPW estimator asymptotically efficient OLS errors truly generated AR(1) process: \\[\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t, \\quad Var(u_t) = \\sigma^2.\n\\]Standard Errors:\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.\nRobust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.\nUsual Standard Errors: errors correctly specified AR(1) process, usual standard errors FPW valid.Robust Standard Errors: concern complex dependence structure (e.g., higher-order autocorrelation heteroskedasticity), use Newey-West Standard Errors inference. robust serial correlation heteroskedasticity.","code":""},{"path":"linear-regression.html","id":"cluster-errors","chapter":"5 Linear Regression","heading":"5.2.2.3 Cluster Errors","text":"Consider regression model clustered errors:\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi},\n\\]:\\(g\\) indexes group (e.g., households, firms, schools).\\(g\\) indexes group (e.g., households, firms, schools).\\(\\) indexes individual within group.\\(\\) indexes individual within group.covariance structure errors \\(\\epsilon_{gi}\\) defined :\\[\nCov(\\epsilon_{gi}, \\epsilon_{hj})\n\\begin{cases}\n= 0 & \\text{} g \\neq h \\text{ (independent across groups)}, \\\\\n\\neq 0 & \\text{pair } (,j) \\text{ within group } g.\n\\end{cases}\n\\]Within group, individuals’ errors may correlated (.e., intra-group correlation), errors independent across groups. violates A4 (constant variance correlation errors).Suppose three groups varying sizes. variance-covariance matrix \\(\\boldsymbol{\\Omega}\\) errors \\(\\boldsymbol{\\epsilon}\\) :\\[\nVar(\\boldsymbol{\\epsilon}| \\mathbf{X}) = \\boldsymbol{\\Omega} =\n\\begin{pmatrix}\n\\sigma^2 & \\delta_{12}^1 & \\delta_{13}^1 & 0 & 0 & 0 \\\\\n\\delta_{12}^1 & \\sigma^2 & \\delta_{23}^1 & 0 & 0 & 0 \\\\\n\\delta_{13}^1 & \\delta_{23}^1 & \\sigma^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2 & \\delta_{12}^2 & 0 \\\\\n0 & 0 & 0 & \\delta_{12}^2 & \\sigma^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{pmatrix}.\n\\]\\(\\delta_{ij}^g = Cov(\\epsilon_{gi}, \\epsilon_{gj})\\) covariance errors individuals \\(\\) \\(j\\) group \\(g\\).\\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) \\(g \\neq h\\) (independent groups).Infeasible Generalized Least Squares (Cluster)Assume Known Variance-Covariance Matrix: \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, construct \\(\\boldsymbol{\\Omega}\\) compute inverse \\(\\boldsymbol{\\Omega}^{-1}\\).Assume Known Variance-Covariance Matrix: \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, construct \\(\\boldsymbol{\\Omega}\\) compute inverse \\(\\boldsymbol{\\Omega}^{-1}\\).Infeasible GLS Estimator: infeasible generalized least squares (IGLS) estimator :\n\\[\n\\hat{\\beta}_{IGLS} = (\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{y}.\n\\]Infeasible GLS Estimator: infeasible generalized least squares (IGLS) estimator :\\[\n\\hat{\\beta}_{IGLS} = (\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{y}.\n\\]Problem:know \\(\\sigma^2\\) \\(\\delta_{ij}^g\\), making approach infeasible.Even \\(\\boldsymbol{\\Omega}\\) estimated, incorrect assumptions structure may lead invalid inference.make estimation feasible, assume group-level random effects specification error:\\[\n\\begin{aligned}\ny_{gi} &= \\mathbf{x}_{gi}\\beta + c_g + u_{gi}, \\\\\nVar(c_g|\\mathbf{x}_i) &= \\sigma_c^2, \\\\\nVar(u_{gi}|\\mathbf{x}_i) &= \\sigma_u^2,\n\\end{aligned}\n\\]:\\(c_g\\) represents group-level random effect (common shocks within group, independent across groups).\\(c_g\\) represents group-level random effect (common shocks within group, independent across groups).\\(u_{gi}\\) represents individual-level error (idiosyncratic shocks within group, independent across individuals groups).\\(u_{gi}\\) represents individual-level error (idiosyncratic shocks within group, independent across individuals groups).\\(\\epsilon_{gi} = c_g + u_{gi}\\)\\(\\epsilon_{gi} = c_g + u_{gi}\\)Independence Assumptions:\\(c_g\\) \\(u_{gi}\\) independent .mean-independent \\(\\mathbf{x}_i\\).specification, variance-covariance matrix \\(\\boldsymbol{\\Omega}\\) becomes block diagonal, block corresponds group:\\[\nVar(\\boldsymbol{\\epsilon}| \\mathbf{X}) = \\boldsymbol{\\Omega} =\n\\begin{pmatrix}\n\\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & \\sigma_c^2 & 0 & 0 & 0 \\\\\n\\sigma_c^2 & \\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & 0 & 0 & 0 \\\\\n\\sigma_c^2 & \\sigma_c^2  & \\sigma_c^2 + \\sigma_u^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_c^2 + \\sigma_u^2 & \\sigma_c^2 & 0 \\\\\n0 & 0 & 0 & \\sigma_c^2 & \\sigma_c^2 + \\sigma_u^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_c^2 + \\sigma_u^2\n\\end{pmatrix}.\n\\]variance components \\(\\sigma_c^2\\) \\(\\sigma_u^2\\) unknown, can use Feasible Group-Level Random Effects (RE) estimator simultaneously estimate variances regression coefficients \\(\\beta\\). practical approach allows us account intra-group correlation errors still obtain consistent efficient estimates parameters.Step--Step ProcedureStep 1: Initial OLS EstimationEstimate regression model using OLS:\\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}, \\]compute residuals:\\[ e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}. \\]Step 2: Estimate Variance ComponentsUse standard OLS variance estimator \\(s^2\\) estimate total variance:\\[ s^2 = \\frac{1}{n - k} \\sum_{=1}^{n} e_i^2, \\]\\(n\\) total number observations \\(k\\) number regressors (including intercept).Estimate -group variance \\(\\hat{\\sigma}_c^2\\) using:\\[ \\hat{\\sigma}_c^2 = \\frac{1}{G} \\sum_{g=1}^{G} \\left( \\frac{1}{\\sum_{=1}^{n_g - 1} } \\sum_{\\neq j} \\sum_{j=1}^{n_g} e_{gi} e_{gj} \\right), \\]:\\(G\\) total number groups,\\(G\\) total number groups,\\(n_g\\) size group \\(g\\),\\(n_g\\) size group \\(g\\),term \\(\\sum_{\\neq j} e_{gi} e_{gj}\\) accounts within-group covariance.term \\(\\sum_{\\neq j} e_{gi} e_{gj}\\) accounts within-group covariance.Estimate within-group variance :\\[ \\hat{\\sigma}_u^2 = s^2 - \\hat{\\sigma}_c^2. \\]Step 3: Construct Variance-Covariance MatrixUse estimated variances \\(\\hat{\\sigma}_c^2\\) \\(\\hat{\\sigma}_u^2\\) construct variance-covariance matrix \\(\\hat{\\Omega}\\) error term:\\[ \\hat{\\Omega}_{gi,gj} = \\begin{cases} \\hat{\\sigma}_c^2 + \\hat{\\sigma}_u^2 & \\text{} = j \\text{ (diagonal elements)}, \\\\ \\hat{\\sigma}_c^2 & \\text{} \\neq j \\text{ (-diagonal elements within group)}, \\\\ 0 & \\text{} g \\neq h \\text{ (across groups)}. \\end{cases} \\]Step 4: Feasible GLS EstimationWith \\(\\hat{\\Omega}\\) hand, perform Feasible Generalized Least Squares (FGLS) estimate \\(\\beta\\):\\[ \\hat{\\beta}_{RE} = (\\mathbf{X}'\\hat{\\Omega}^{-1}\\mathbf{X})^{-1} \\mathbf{X}'\\hat{\\Omega}^{-1}\\mathbf{y}. \\]assumptions \\(\\boldsymbol{\\Omega}\\) incorrect infeasible, use cluster-robust standard errors account intra-group correlation without explicitly modeling variance-covariance structure. standard errors remain valid arbitrary within-cluster dependence, provided clusters independent.Properties Feasible Group-Level Random Effects EstimatorInfeasible Group RE EstimatorThe infeasible RE estimator (assuming known variances) unbiased assumptions A1, A2, A3 unweighted equation.A3 requires: \\[ E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_g|\\mathbf{x}_i) + E(u_{gi}|\\mathbf{x}_i) = 0. \\] assumes:\n\\(E(c_g|\\mathbf{x}_i) = 0\\): random effects assumption (group-level effects uncorrelated regressors).\n\\(E(u_{gi}|\\mathbf{x}_i) = 0\\): endogeneity individual level.\n\\(E(c_g|\\mathbf{x}_i) = 0\\): random effects assumption (group-level effects uncorrelated regressors).\\(E(u_{gi}|\\mathbf{x}_i) = 0\\): endogeneity individual level.Feasible Group RE EstimatorThe feasible RE estimator biased variances \\(\\sigma_c^2\\) \\(\\sigma_u^2\\) estimated, introducing approximation errors.However, estimator consistent A1, A2, A3a (\\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_g) + E(\\mathbf{x}_i'u_{gi}) = 0\\)), A5a.Efficiency\nAsymptotic Efficiency:\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.\n\nStandard Errors:\nrandom effects specification correct, usual standard errors consistent.\nconcern complex dependence structures heteroskedasticity, use cluster robust standard errors.\n\nAsymptotic Efficiency:\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.\nfeasible RE estimator asymptotically efficient OLS errors follow random effects specification.Standard Errors:\nrandom effects specification correct, usual standard errors consistent.\nconcern complex dependence structures heteroskedasticity, use cluster robust standard errors.\nrandom effects specification correct, usual standard errors consistent.concern complex dependence structures heteroskedasticity, use cluster robust standard errors.","code":""},{"path":"linear-regression.html","id":"weighted-least-squares","chapter":"5 Linear Regression","heading":"5.2.3 Weighted Least Squares","text":"presence heteroskedasticity, errors \\(\\epsilon_i\\) non-constant variance \\(Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2\\). violates Gauss-Markov assumption homoskedasticity, leading inefficient OLS estimates.Weighted Least Squares (WLS) addresses applying weights inversely proportional variance errors, ensuring observations larger variances less influence estimation.Weighted Least Squares essentially Generalized Least Squares special case \\(\\mathbf{\\Omega}\\) diagonal matrix variances \\(\\sigma_i^2\\) diagonal (.e., errors uncorrelated non-constant variance).\n, assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\bigl(\\sigma_1^2, \\ldots, \\sigma_n^2\\bigr)\\)\n\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\bigl(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\bigr)\\)\nWeighted Least Squares essentially Generalized Least Squares special case \\(\\mathbf{\\Omega}\\) diagonal matrix variances \\(\\sigma_i^2\\) diagonal (.e., errors uncorrelated non-constant variance)., assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\bigl(\\sigma_1^2, \\ldots, \\sigma_n^2\\bigr)\\), assume errors uncorrelated heteroskedastic: \\(\\mathbf{\\Omega} = \\text{diag}\\bigl(\\sigma_1^2, \\ldots, \\sigma_n^2\\bigr)\\)\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\bigl(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\bigr)\\)\\(\\mathbf{\\Omega}^{-1} = \\text{diag}\\bigl(1/\\sigma_1^2, \\ldots, 1/\\sigma_n^2\\bigr)\\)Steps Feasible Weighted Least Squares (FWLS)1. Initial OLS EstimationFirst, estimate model using OLS:\\[ y_i = \\mathbf{x}_i\\beta + \\epsilon_i, \\]compute residuals:\\[ e_i = y_i - \\mathbf{x}_i \\hat{\\beta}. \\]2. Model Error VarianceTransform residuals model variance function predictors:\\[ \\ln(e_i^2) = \\mathbf{x}_i\\gamma + \\ln(v_i), \\]:\\(e_i^2\\) approximates \\(\\epsilon_i^2\\),\\(e_i^2\\) approximates \\(\\epsilon_i^2\\),\\(\\ln(v_i)\\) error term auxiliary regression, assumed independent \\(\\mathbf{x}_i\\).\\(\\ln(v_i)\\) error term auxiliary regression, assumed independent \\(\\mathbf{x}_i\\).Estimate equation using OLS obtain predicted values:\\[ \\hat{g}_i = \\mathbf{x}_i \\hat{\\gamma}. \\]3. Estimate WeightsUse predicted values auxiliary regression compute weights:\\[ \\hat{\\sigma}_i = \\sqrt{\\exp(\\hat{g}_i)}. \\]weights approximate standard deviation errors.4. Weighted RegressionTransform original equation dividing \\(\\hat{\\sigma}_i\\):\\[ \\frac{y_i}{\\hat{\\sigma}_i} = \\frac{\\mathbf{x}_i}{\\hat{\\sigma}_i}\\beta + \\frac{\\epsilon_i}{\\hat{\\sigma}_i}. \\]Estimate transformed equation using OLS. resulting estimator Feasible Weighted Least Squares (FWLS) estimator:\\[ \\hat{\\beta}_{FWLS} = (\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{\\hat{W}}\\mathbf{y}, \\]\\(\\mathbf{\\hat{W}}\\) diagonal weight matrix elements \\(1/\\hat{\\sigma}_i^2\\).Properties FWLS EstimatorUnbiasedness:\ninfeasible WLS estimator (\\(\\sigma_i\\) known) unbiased assumptions A1-A3 unweighted model.\nFWLS estimator unbiased due approximation \\(\\sigma_i\\) using \\(\\hat{\\sigma}_i\\).\ninfeasible WLS estimator (\\(\\sigma_i\\) known) unbiased assumptions A1-A3 unweighted model.FWLS estimator unbiased due approximation \\(\\sigma_i\\) using \\(\\hat{\\sigma}_i\\).Consistency:\nFWLS estimator consistent following assumptions:\nA1 (unweighted equation): model linear parameters.\nA2 (unweighted equation): independent variables linearly independent.\nA5: data randomly sampled.\n\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .\n\nFWLS estimator consistent following assumptions:\nA1 (unweighted equation): model linear parameters.\nA2 (unweighted equation): independent variables linearly independent.\nA5: data randomly sampled.\n\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .\nA1 (unweighted equation): model linear parameters.A2 (unweighted equation): independent variables linearly independent.A5: data randomly sampled.\\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma_i^2) = 0\\). A3a: Weaker Exogeneity Assumption sufficient, A3 .Efficiency:\nFWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity: \\[ Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2 = \\exp(\\mathbf{x}_i\\gamma). \\]\nFWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity: \\[ Var(\\epsilon_i|\\mathbf{x}_i) = \\sigma_i^2 = \\exp(\\mathbf{x}_i\\gamma). \\]FWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity.Usual Standard Errors:\nerrors truly multiplicative exponential heteroskedastic, usual standard errors FWLS valid.\nerrors truly multiplicative exponential heteroskedastic, usual standard errors FWLS valid.Heteroskedastic Robust Standard Errors:\npotential mis-specification multiplicative exponential model \\(\\sigma_i^2\\), heteroskedastic-robust standard errors reported ensure valid inference.\npotential mis-specification multiplicative exponential model \\(\\sigma_i^2\\), heteroskedastic-robust standard errors reported ensure valid inference.","code":""},{"path":"linear-regression.html","id":"maximum-likelihood-estimator","chapter":"5 Linear Regression","heading":"5.3 Maximum Likelihood","text":"Maximum Likelihood Estimation (MLE) statistical method used estimate parameters model maximizing likelihood observing given data. premise find parameter values maximize probability (likelihood) observed data.likelihood function, denoted \\(L(\\theta)\\), expressed :\\[\nL(\\theta) = \\prod_{=1}^{n} f(y_i|\\theta)\n\\]:\\(f(y|\\theta)\\) probability density mass function observing single value \\(Y\\) given parameter \\(\\theta\\).product runs \\(n\\) observations.different types data, \\(f(y|\\theta)\\) can take different forms. example, \\(y\\) dichotomous (e.g., success/failure), likelihood function becomes:\\[\nL(\\theta) = \\prod_{=1}^{n} \\theta^{y_i} (1-\\theta)^{1-y_i}\n\\], \\(\\hat{\\theta}\\) Maximum Likelihood Estimator (MLE) :\\[\nL(\\hat{\\theta}) > L(\\theta_0), \\quad \\forall \\theta_0 \\text{ parameter space.}\n\\]See Distributions review variable distributions.","code":""},{"path":"linear-regression.html","id":"motivation-for-mle","chapter":"5 Linear Regression","heading":"5.3.1 Motivation for MLE","text":"Suppose know conditional distribution \\(Y\\) given \\(X\\), denoted :\\[\nf_{Y|X}(y, x; \\theta)\n\\]\\(\\theta\\) unknown parameter distribution. Sometimes, concerned unconditional distribution \\(f_Y(y; \\theta)\\).sample independent identically distributed (..d.) data, joint probability sample :\\[\nf_{Y_1, \\ldots, Y_n|X_1, \\ldots, X_n}(y_1, \\ldots, y_n, x_1, \\ldots, x_n; \\theta) = \\prod_{=1}^{n} f_{Y|X}(y_i, x_i; \\theta)\n\\]joint distribution, evaluated observed data, defines likelihood function. goal MLE find parameter \\(\\theta\\) maximizes likelihood.estimate \\(\\theta\\), maximize likelihood function:\\[\n\\max_{\\theta} \\prod_{=1}^{n} f_{Y|X}(y_i, x_i; \\theta)\n\\]practice, easier work natural logarithm likelihood (log-likelihood), transforms product sum:\\[\n\\max_{\\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\theta))\n\\]Solving Maximum Likelihood EstimatorFirst-Order Condition: Solve first derivative log-likelihood function respect \\(\\theta\\):\n\\[\n\\frac{\\partial}{\\partial \\theta} \\ell(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\ln L(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) = 0\n\\]\nyields critical points likelihood maximized. derivative, sometimes written \\(U(\\theta)\\), called score. Intuitively, log-likelihood’s “peak” indicates parameter value(s) make observed data “likely.”First-Order Condition: Solve first derivative log-likelihood function respect \\(\\theta\\):\\[\n\\frac{\\partial}{\\partial \\theta} \\ell(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\ln L(\\theta) \\;=\\; \\frac{\\partial}{\\partial \\theta} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) = 0\n\\]yields critical points likelihood maximized. derivative, sometimes written \\(U(\\theta)\\), called score. Intuitively, log-likelihood’s “peak” indicates parameter value(s) make observed data “likely.”Second-Order Condition: Verify second derivative log-likelihood function negative critical point:\n\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) < 0\n\\]\nensures solution corresponds maximum.Second-Order Condition: Verify second derivative log-likelihood function negative critical point:\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\hat{\\theta}_{MLE})) < 0\n\\]ensures solution corresponds maximum.Examples Likelihood FunctionsUnconditional Poisson DistributionThe Poisson distribution models count data, number website visits day product orders per hour. likelihood function :\\[\nL(\\theta) = \\prod_{=1}^{n} \\frac{\\theta^{y_i} e^{-\\theta}}{y_i!}\n\\]Exponential DistributionThe exponential distribution often used model time events, time machine fails. probability density function (PDF) :\\[\nf_{Y|X}(y, x; \\theta) = \\frac{\\exp(-y / (x \\theta))}{x \\theta}\n\\]joint likelihood \\(n\\) observations :\\[\nL(\\theta) = \\prod_{=1}^{n} \\frac{\\exp(-y_i / (x_i \\theta))}{x_i \\theta}\n\\]taking logarithm, obtain log-likelihood ease maximization.","code":""},{"path":"linear-regression.html","id":"key-quantities-for-inference","chapter":"5 Linear Regression","heading":"5.3.2 Key Quantities for Inference","text":"Score Function\nscore given \\[\nU(\\theta) \\;=\\; \\frac{d}{d\\theta} \\ell(\\theta).\n\\]\nSetting \\(U(\\hat{\\theta}_{\\mathrm{MLE}}) = 0\\) yields critical points log-likelihood, can find \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).Score Function\nscore given \\[\nU(\\theta) \\;=\\; \\frac{d}{d\\theta} \\ell(\\theta).\n\\]\nSetting \\(U(\\hat{\\theta}_{\\mathrm{MLE}}) = 0\\) yields critical points log-likelihood, can find \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).Observed Information\nsecond derivative log-likelihood, taken MLE, called observed information:\n\\[\nI_O(\\theta) \\;=\\; - \\frac{d^2}{d\\theta^2} \\ell(\\theta).\n\\]\n(negative sign often included \\(I_O(\\theta)\\) positive \\(\\ell(\\theta)\\) concave near maximum. texts, see defined without negative sign, idea : measures “pointedness” curvature \\(\\ell(\\theta)\\) maximum.)Observed Information\nsecond derivative log-likelihood, taken MLE, called observed information:\\[\nI_O(\\theta) \\;=\\; - \\frac{d^2}{d\\theta^2} \\ell(\\theta).\n\\](negative sign often included \\(I_O(\\theta)\\) positive \\(\\ell(\\theta)\\) concave near maximum. texts, see defined without negative sign, idea : measures “pointedness” curvature \\(\\ell(\\theta)\\) maximum.)Fisher Information\nFisher Information (expected information) expectation observed information distribution data:\n\\[\n(\\theta) \\;=\\; \\mathbb{E}\\bigl[I_O(\\theta)\\bigr].\n\\]\nquantifies much information data carry parameter \\(\\theta\\). larger Fisher information suggests can estimate \\(\\theta\\) precisely.Fisher Information\nFisher Information (expected information) expectation observed information distribution data:\\[\n(\\theta) \\;=\\; \\mathbb{E}\\bigl[I_O(\\theta)\\bigr].\n\\]quantifies much information data carry parameter \\(\\theta\\). larger Fisher information suggests can estimate \\(\\theta\\) precisely.Approximate Variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\)\nOne key results standard asymptotic theory , large \\(n\\), variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\) can approximated inverse Fisher information:\n\\[\n\\mathrm{Var}\\bigl(\\hat{\\theta}_{\\mathrm{MLE}}\\bigr) \\;\\approx\\; (\\theta)^{-1}.\n\\]\nalso lays groundwork constructing confidence intervals \\(\\theta\\) large samples.Approximate Variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\)\nOne key results standard asymptotic theory , large \\(n\\), variance \\(\\hat{\\theta}_{\\mathrm{MLE}}\\) can approximated inverse Fisher information:\\[\n\\mathrm{Var}\\bigl(\\hat{\\theta}_{\\mathrm{MLE}}\\bigr) \\;\\approx\\; (\\theta)^{-1}.\n\\]also lays groundwork constructing confidence intervals \\(\\theta\\) large samples.","code":""},{"path":"linear-regression.html","id":"assumptions-of-mle","chapter":"5 Linear Regression","heading":"5.3.3 Assumptions of MLE","text":"MLE desirable properties—consistency, asymptotic normality, efficiency—come “free.” Instead, rely certain assumptions. breakdown main regularity conditions. conditions typically mild many practical settings (example, exponential families, normal distribution), need checked complex models.High-Level Regulatory AssumptionsIndependence Identical Distribution (iid)\nsample \\(\\{(x_i, y_i)\\}\\) usually assumed composed independent identically distributed observations. independence assumption simplifies likelihood product individual densities: \\[\nL(\\theta) = \\prod_{=1}^n f_{Y\\mid X}(y_i, x_i; \\theta).\n\\] practice, dependent data (e.g., time series, spatial data), modifications required likelihood function.Independence Identical Distribution (iid)\nsample \\(\\{(x_i, y_i)\\}\\) usually assumed composed independent identically distributed observations. independence assumption simplifies likelihood product individual densities: \\[\nL(\\theta) = \\prod_{=1}^n f_{Y\\mid X}(y_i, x_i; \\theta).\n\\] practice, dependent data (e.g., time series, spatial data), modifications required likelihood function.Density Function\nobservations must come conditional probability density function \\(f_{Y\\mid X}(\\cdot,\\cdot;\\theta)\\). model changes across observations, simply multiply together one unified likelihood.Density Function\nobservations must come conditional probability density function \\(f_{Y\\mid X}(\\cdot,\\cdot;\\theta)\\). model changes across observations, simply multiply together one unified likelihood.Multivariate Normality (certain models)\nmany practical cases—especially continuous outcomes—might assume (multivariate) normal distributions finite second fourth moments (Little 1988). assumptions, MLE mean vector covariance matrix consistent (conditions) asymptotically normal. assumption quite common regression, ANOVA, classical statistical frameworks.Multivariate Normality (certain models)\nmany practical cases—especially continuous outcomes—might assume (multivariate) normal distributions finite second fourth moments (Little 1988). assumptions, MLE mean vector covariance matrix consistent (conditions) asymptotically normal. assumption quite common regression, ANOVA, classical statistical frameworks.","code":""},{"path":"linear-regression.html","id":"large-sample-properties-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1 Large Sample Properties of MLE","text":"","code":""},{"path":"linear-regression.html","id":"consistency-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1.1 Consistency of MLE","text":"Definition: estimator \\(\\hat{\\theta}_n\\) consistent converges probability true parameter value \\(\\theta_0\\) sample size \\(n \\\\infty\\):\\[\n\\hat{\\theta}_n \\;\\^p\\; \\theta_0.\n\\]MLE, set regularity conditions \\(R1\\)–\\(R4\\) commonly used ensure consistency:R1\n\\(\\theta \\neq \\theta_0\\), \\[\nf_{Y\\mid X}(y_i, x_i; \\theta) \\;\\neq\\; f_{Y\\mid X}(y_i, x_i; \\theta_0).\n\\]\nsimpler terms, model identifiable: two distinct parameter values generate exact distribution data.R1\n\\(\\theta \\neq \\theta_0\\), \\[\nf_{Y\\mid X}(y_i, x_i; \\theta) \\;\\neq\\; f_{Y\\mid X}(y_i, x_i; \\theta_0).\n\\]simpler terms, model identifiable: two distinct parameter values generate exact distribution data.R2\nparameter space \\(\\Theta\\) compact (closed bounded), contains true parameter \\(\\theta_0\\). ensures \\(\\theta\\) lies “nice” region (parameter going infinity, etc.), making easier prove maximum space indeed exists.R2\nparameter space \\(\\Theta\\) compact (closed bounded), contains true parameter \\(\\theta_0\\). ensures \\(\\theta\\) lies “nice” region (parameter going infinity, etc.), making easier prove maximum space indeed exists.R3\nlog-likelihood function \\(\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\) continuous \\(\\theta\\) probability \\(1\\). Continuity important can apply theorems (like Continuous Mapping Theorem Extreme Value Theorem) find maxima.R3\nlog-likelihood function \\(\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\) continuous \\(\\theta\\) probability \\(1\\). Continuity important can apply theorems (like Continuous Mapping Theorem Extreme Value Theorem) find maxima.R4\nexpected supremum absolute value log-likelihood finite:\n\\[\n\\mathbb{E}\\!\\Bigl(\\sup_{\\theta \\\\Theta} \\bigl|\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\bigr|\\Bigr) \\;<\\;\\infty.\n\\]\ntechnical condition helps ensure can “exchange” expectations suprema, step needed many consistency proofs.R4\nexpected supremum absolute value log-likelihood finite:\\[\n\\mathbb{E}\\!\\Bigl(\\sup_{\\theta \\\\Theta} \\bigl|\\ln(f_{Y\\mid X}(y_i, x_i; \\theta))\\bigr|\\Bigr) \\;<\\;\\infty.\n\\]technical condition helps ensure can “exchange” expectations suprema, step needed many consistency proofs.conditions satisfied, can show via standard arguments (e.g., Law Large Numbers, uniform convergence log-likelihood) :\\[\n\\hat{\\theta}_{\\mathrm{MLE}} \\;\\^p\\; \\theta_0 \\quad (\\text{consistency}).\n\\]","code":""},{"path":"linear-regression.html","id":"asymptotic-normality-of-mle","chapter":"5 Linear Regression","heading":"5.3.3.1.2 Asymptotic Normality of MLE","text":"Definition: estimator \\(\\hat{\\theta}_n\\) asymptotically normal \\[\n\\sqrt{n}\\,(\\hat{\\theta}_n - \\theta_0) \\;\\^d\\; \\mathcal{N}\\bigl(0,\\Sigma\\bigr),\n\\]\\(\\^d\\) denotes convergence distribution \\(\\Sigma\\) covariance matrix. MLE, \\(\\Sigma\\) typically \\((\\theta_0)^{-1}\\), \\((\\theta_0)\\) Fisher information evaluated true parameter.Beyond \\(R1\\)–\\(R4\\), need following additional assumptions:R5\ntrue parameter \\(\\theta_0\\) interior parameter space \\(\\Theta\\). \\(\\theta_0\\) sits boundary, different arguments required handle edge effects.R5\ntrue parameter \\(\\theta_0\\) interior parameter space \\(\\Theta\\). \\(\\theta_0\\) sits boundary, different arguments required handle edge effects.R6\npdf \\(f_{Y\\mid X}(y_i, x_i; \\theta)\\) twice continuously differentiable (\\(\\theta\\)) strictly positive neighborhood \\(N\\) \\(\\theta_0\\). allows us use second-order Taylor expansions around \\(\\theta_0\\) get approximate distribution \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).R6\npdf \\(f_{Y\\mid X}(y_i, x_i; \\theta)\\) twice continuously differentiable (\\(\\theta\\)) strictly positive neighborhood \\(N\\) \\(\\theta_0\\). allows us use second-order Taylor expansions around \\(\\theta_0\\) get approximate distribution \\(\\hat{\\theta}_{\\mathrm{MLE}}\\).R7\nfollowing integrals finite neighborhood \\(N\\) \\(\\theta_0\\):\n\\(\\displaystyle \\int \\sup_{\\theta \\N} \\Bigl\\|\\partial f_{Y\\mid X}(y_i, x_i; \\theta)/\\partial \\theta \\Bigr\\|\\; d(y,x) < \\infty\\).\n\\(\\displaystyle \\int \\sup_{\\theta \\N} \\Bigl\\|\\partial^2 f_{Y\\mid X}(y_i, x_i; \\theta)/\\partial \\theta \\partial \\theta' \\Bigr\\|\\; d(y,x) < \\infty\\).\n\\(\\displaystyle \\mathbb{E}\\Bigl(\\sup_{\\theta \\N} \\Bigl\\|\\partial^2 \\ln(f_{Y\\mid X}(y_i, x_i; \\theta))/\\partial \\theta \\partial \\theta' \\Bigr\\|\\Bigr) < \\infty\\).\nconditions ensure differentiating inside integrals justified (via dominated convergence theorem) can expand log-likelihood Taylor series safely.R7\nfollowing integrals finite neighborhood \\(N\\) \\(\\theta_0\\):\\(\\displaystyle \\int \\sup_{\\theta \\N} \\Bigl\\|\\partial f_{Y\\mid X}(y_i, x_i; \\theta)/\\partial \\theta \\Bigr\\|\\; d(y,x) < \\infty\\).\\(\\displaystyle \\int \\sup_{\\theta \\N} \\Bigl\\|\\partial^2 f_{Y\\mid X}(y_i, x_i; \\theta)/\\partial \\theta \\partial \\theta' \\Bigr\\|\\; d(y,x) < \\infty\\).\\(\\displaystyle \\mathbb{E}\\Bigl(\\sup_{\\theta \\N} \\Bigl\\|\\partial^2 \\ln(f_{Y\\mid X}(y_i, x_i; \\theta))/\\partial \\theta \\partial \\theta' \\Bigr\\|\\Bigr) < \\infty\\).conditions ensure differentiating inside integrals justified (via dominated convergence theorem) can expand log-likelihood Taylor series safely.R8\ninformation matrix \\((\\theta_0)\\) exists nonsingular:\n\\[\n(\\theta_0) \\;=\\; \\mathrm{Var}\\Bigl(\\frac{\\partial}{\\partial \\theta} \\ln\\bigl(f_{Y\\mid X}(y_i, x_i; \\theta_0)\\bigr)\\Bigr) \\;\\neq\\; 0.\n\\]\nNonsingularity implies enough information data estimate \\(\\theta\\) uniquely.R8\ninformation matrix \\((\\theta_0)\\) exists nonsingular:\\[\n(\\theta_0) \\;=\\; \\mathrm{Var}\\Bigl(\\frac{\\partial}{\\partial \\theta} \\ln\\bigl(f_{Y\\mid X}(y_i, x_i; \\theta_0)\\bigr)\\Bigr) \\;\\neq\\; 0.\n\\]Nonsingularity implies enough information data estimate \\(\\theta\\) uniquely.\\(R1\\)–\\(R8\\), can show \\[\n\\sqrt{n}\\,(\\hat{\\theta}_{\\mathrm{MLE}} - \\theta_0) \\;\\^d\\; \\mathcal{N}\\Bigl(0,\\,(\\theta_0)^{-1}\\Bigr).\n\\]result central frequentist inference, allowing construct approximate confidence intervals hypothesis tests using normal approximation large \\(n\\).","code":""},{"path":"linear-regression.html","id":"properties-of-mle","chapter":"5 Linear Regression","heading":"5.3.4 Properties of MLE","text":"established earlier sections Maximum Likelihood Estimators (MLEs) consistent (Consistency MLE) asymptotically normal (Asymptotic Normality MLE) standard regularity conditions, now highlight additional properties make MLE powerful estimation technique.Asymptotic EfficiencyDefinition: estimator asymptotically efficient attains smallest possible asymptotic variance among consistent estimators (.e., achieves Cramér-Rao Lower Bound).Interpretation: large samples, MLE typically smaller standard errors consistent estimators fully use assumed distributional form.Implication: true model correctly specified, MLE efficient among broad class estimators, leading precise inference \\(\\theta\\).\nCramér-Rao Lower Bound (CRLB): theoretical lower limit variance unbiased (asymptotically unbiased) estimator C. R. Rao (1992).\nMLE Meets CRLB: correct specification standard regularity conditions, asymptotic variance MLE matches CRLB, making asymptotically efficient.\nInterpretation: Achieving CRLB means unbiased estimator can consistently outperform MLE terms variance large \\(n\\).\nCramér-Rao Lower Bound (CRLB): theoretical lower limit variance unbiased (asymptotically unbiased) estimator C. R. Rao (1992).MLE Meets CRLB: correct specification standard regularity conditions, asymptotic variance MLE matches CRLB, making asymptotically efficient.Interpretation: Achieving CRLB means unbiased estimator can consistently outperform MLE terms variance large \\(n\\).InvarianceCore Idea: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), smooth transformation \\(g(\\theta)\\), MLE \\(g(\\theta)\\) simply \\(g(\\hat{\\theta})\\).Example: \\(\\theta\\) mean parameter want MLE variance \\(\\theta^2\\), can just square MLE \\(\\theta\\).Key Point: invariance property saves considerable effort—need re-derive new likelihood transformed parameter.Explicit vs. Implicit MLEExplicit MLE:\nOccurs score equation can solved closed form. classic example MLE mean variance normal distribution.Implicit MLE:\nHappens closed-form solution exists. Iterative numerical methods, Newton-Raphson, Expectation-Maximization (EM), optimization algorithms, used find \\(\\hat{\\theta}\\).Distributional Mis-SpecificationDefinition: assume distribution \\(f_{Y|X}(\\cdot;\\theta)\\) reflect true data-generating process, MLE may become inconsistent biased finite samples.Quasi-MLE:\nstrategy handle certain forms mis-specification.\nchosen distribution belongs flexible class meets certain conditions (e.g., generalized linear models robust link), resulting parameter estimates can remain consistent parameters interest.\nstrategy handle certain forms mis-specification.chosen distribution belongs flexible class meets certain conditions (e.g., generalized linear models robust link), resulting parameter estimates can remain consistent parameters interest.Nonparametric & Semiparametric Approaches:\nRequire minimal distributional assumptions.\nrobust mis-specification can harder implement may exhibit higher variance require larger sample sizes achieve comparable precision.\nRequire minimal distributional assumptions.robust mis-specification can harder implement may exhibit higher variance require larger sample sizes achieve comparable precision.","code":""},{"path":"linear-regression.html","id":"practical-considerations","chapter":"5 Linear Regression","heading":"5.3.5 Practical Considerations","text":"Use Cases\nMLE extremely popular :\nBinary Outcomes (logistic regression)\nCount Data (Poisson regression)\nStrictly Positive Outcomes (Gamma regression)\nHeteroskedastic Settings (models variance related mean, e.g., GLMs)\n\nMLE extremely popular :\nBinary Outcomes (logistic regression)\nCount Data (Poisson regression)\nStrictly Positive Outcomes (Gamma regression)\nHeteroskedastic Settings (models variance related mean, e.g., GLMs)\nBinary Outcomes (logistic regression)Count Data (Poisson regression)Strictly Positive Outcomes (Gamma regression)Heteroskedastic Settings (models variance related mean, e.g., GLMs)Distributional Assumptions\nefficiency gains MLE stem using specific probability model.\nassumed model closely reflects data-generating process, MLE gives accurate parameter estimates reliable standard errors.\nMLE assumes knowledge conditional distribution outcome variable. assumption parallels normality assumption linear regression models (e.g., A6 Normal Distribution).\nseverely mis-specified, consider robust semi-/nonparametric methods.\nefficiency gains MLE stem using specific probability model.assumed model closely reflects data-generating process, MLE gives accurate parameter estimates reliable standard errors.MLE assumes knowledge conditional distribution outcome variable. assumption parallels normality assumption linear regression models (e.g., A6 Normal Distribution).severely mis-specified, consider robust semi-/nonparametric methods.Comparison OLS: See Comparison MLE OLS details.\nOrdinary Least Squares (OLS) special case MLE errors normally distributed homoscedastic.\ngeneral settings (e.g., non-Gaussian heteroskedastic data), MLE can outperform OLS terms smaller standard errors better inference.\nOrdinary Least Squares (OLS) special case MLE errors normally distributed homoscedastic.general settings (e.g., non-Gaussian heteroskedastic data), MLE can outperform OLS terms smaller standard errors better inference.Numerical Stability & Computation\ncomplex likelihoods, iterative methods can fail converge converge local maxima.\nProper initialization diagnostics (e.g., checking multiple start points) crucial.\ncomplex likelihoods, iterative methods can fail converge converge local maxima.Proper initialization diagnostics (e.g., checking multiple start points) crucial.","code":""},{"path":"linear-regression.html","id":"comparison-of-mle-and-ols","chapter":"5 Linear Regression","heading":"5.3.6 Comparison of MLE and OLS","text":"Maximum Likelihood Estimation (MLE) powerful estimation method, solve challenges associated Ordinary Least Squares (OLS). detailed comparison highlighting similarities, differences, limitations.Key Points ComparisonInference Methods:\nMLE:\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).\nmethods replace use F-statistics commonly associated OLS.\n\nOLS:\nRelies F-statistic hypothesis testing joint inference.\n\nMLE:\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).\nmethods replace use F-statistics commonly associated OLS.\nJoint inference typically conducted using log-likelihood calculations, likelihood ratio tests information criteria (e.g., AIC, BIC).methods replace use F-statistics commonly associated OLS.OLS:\nRelies F-statistic hypothesis testing joint inference.\nRelies F-statistic hypothesis testing joint inference.Sensitivity Functional Form:\nMLE OLS sensitive functional form model. Incorrect specification (e.g., linear vs. nonlinear relationships) can lead biased inefficient estimates cases.\nMLE OLS sensitive functional form model. Incorrect specification (e.g., linear vs. nonlinear relationships) can lead biased inefficient estimates cases.Perfect Collinearity Multicollinearity:\nmethods affected collinearity:\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.\nMulticollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.\n\nNeither MLE OLS directly resolves issues without additional measures, regularization variable selection.\nmethods affected collinearity:\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.\nMulticollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.\nPerfect collinearity (e.g., two identical predictors) makes parameter estimation impossible.Multicollinearity (highly correlated predictors) inflates standard errors, reducing precision estimates.Neither MLE OLS directly resolves issues without additional measures, regularization variable selection.Endogeneity:\nProblems like omitted variable bias simultaneous equations affect MLE OLS:\nrelevant predictors omitted, estimates methods likely biased inconsistent.\nSimilarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.\n\nMLE, efficient correct model specification, inherently address endogeneity.\nProblems like omitted variable bias simultaneous equations affect MLE OLS:\nrelevant predictors omitted, estimates methods likely biased inconsistent.\nSimilarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.\nrelevant predictors omitted, estimates methods likely biased inconsistent.Similarly, systems simultaneous equations, methods yield biased results unless endogeneity addressed instrumental variables approaches.MLE, efficient correct model specification, inherently address endogeneity.Situations MLE OLS DifferPractical ConsiderationsWhen Use MLE:\nSituations dependent variable :\nBinary (e.g., logistic regression)\nCount data (e.g., Poisson regression)\nSkewed bounded (e.g., survival models)\n\nmodel naturally arises probabilistic framework.\nSituations dependent variable :\nBinary (e.g., logistic regression)\nCount data (e.g., Poisson regression)\nSkewed bounded (e.g., survival models)\nBinary (e.g., logistic regression)Count data (e.g., Poisson regression)Skewed bounded (e.g., survival models)model naturally arises probabilistic framework.Use OLS:\nSuitable continuous dependent variables approximately linear relationships predictors outcomes.\nSimpler implement interpret assumptions linear regression reasonably met.\nSuitable continuous dependent variables approximately linear relationships predictors outcomes.Simpler implement interpret assumptions linear regression reasonably met.","code":""},{"path":"linear-regression.html","id":"applications-of-mle","chapter":"5 Linear Regression","heading":"5.3.7 Applications of MLE","text":"MLE widely used across various applications estimate parameters models tailored specific data structures. key applications MLE, categorized problem type estimation method.Hours workedDonations charityHousehold consumption goodDependent variable often censored zero (another threshold).Large fraction observations corner (e.g., 0 hours, 0 donations).Tobit regression(latent variable approach censoring)Number arrestsNumber cigarettes smokedDoctor visits per yearDependent variable consists non-negative integer counts.Possible overdispersion (variance > mean).Poisson regression,Negative Binomial regressionPoisson assumes mean = variance, often Negative Binomial preferred real data.Zero-inflated models (ZIP/ZINB) may used data excess zeros.Demand different car brandsVotes primary electionChoice travel modeDependent variable categorical choice among 3+ alternatives.category distinct, inherent ordering (e.g., brand , B, C).Multinomial logit,Multinomial probitExtension binary choice (logit/probit) multiple categories.Independence Irrelevant Alternatives (IIA) can concern multinomial logit.Self-reported happiness (low/medium/high)Income level bracketsLikert-scale surveysDependent variable ordered (e.g., low < medium < high).Distances categories necessarily equal.Ordered logit,Ordered probitProbit/logit framework adapted preserve ordinal information.Interprets latent continuous variable mapped discrete ordered categories.","code":""},{"path":"linear-regression.html","id":"binary-response-models","chapter":"5 Linear Regression","heading":"5.3.7.1 Binary Response Models","text":"binary response variable (\\(y_i\\)) follows Bernoulli distribution:\\[\nf_Y(y_i; p) = p^{y_i}(1-p)^{(1-y_i)}\n\\]\\(p\\) probability success. conditional models, likelihood becomes:\\[\nf_{Y|X}(y_i, x_i; p(.)) = p(x_i)^{y_i}(1 - p(x_i))^{(1-y_i)}\n\\]model \\(p(x_i)\\), use function \\(x_i\\) unknown parameters \\(\\theta\\). common approach involves latent variable model:\\[\n\\begin{aligned}\ny_i &= 1\\{y_i^* > 0 \\}, \\\\\ny_i^* &= x_i \\beta - \\epsilon_i,\n\\end{aligned}\n\\]:\\(y_i^*\\) unobserved (latent) variable.\\(\\epsilon_i\\) random variable mean 0, representing unobserved noise.Rewriting terms observed data:\\[\ny_i = 1\\{x_i \\beta > \\epsilon_i\\}.\n\\]probability function becomes:\\[\n\\begin{aligned}\np(x_i) &= P(y_i = 1 | x_i) \\\\\n&= P(x_i \\beta > \\epsilon_i | x_i) \\\\\n&= F_{\\epsilon|X}(x_i \\beta | x_i),\n\\end{aligned}\n\\]\\(F_{\\epsilon|X}(.)\\) cumulative distribution function (CDF) \\(\\epsilon_i\\). Assuming independence \\(\\epsilon_i\\) \\(x_i\\), probability function simplifies :\\[\np(x_i) = F_\\epsilon(x_i \\beta).\n\\]conditional expectation function equivalent:\\[\nE(y_i | x_i) = P(y_i = 1 | x_i) = F_\\epsilon(x_i \\beta).\n\\]Common Distributional AssumptionsProbit Model:\nAssumes \\(\\epsilon_i\\) follows standard normal distribution.\n\\(F_\\epsilon(.) = \\Phi(.)\\), \\(\\Phi(.)\\) standard normal CDF.\nAssumes \\(\\epsilon_i\\) follows standard normal distribution.\\(F_\\epsilon(.) = \\Phi(.)\\), \\(\\Phi(.)\\) standard normal CDF.Logit Model:\nAssumes \\(\\epsilon_i\\) follows standard logistic distribution.\n\\(F_\\epsilon(.) = \\Lambda(.)\\), \\(\\Lambda(.)\\) logistic CDF.\nAssumes \\(\\epsilon_i\\) follows standard logistic distribution.\\(F_\\epsilon(.) = \\Lambda(.)\\), \\(\\Lambda(.)\\) logistic CDF.Steps Derive MLE Binary ModelsSpecify Log-Likelihood:\nchosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\n\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]\nchosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\n\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]chosen distribution (e.g., normal Probit logistic Logit), log-likelihood :\\[\n\\ln(f_{Y|X}(y_i, x_i; \\beta)) = y_i \\ln(F_\\epsilon(x_i \\beta)) + (1 - y_i) \\ln(1 - F_\\epsilon(x_i \\beta)).\n\\]Maximize Log-Likelihood:\nFind parameter estimates maximize log-likelihood:\n\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]\nFind parameter estimates maximize log-likelihood:\n\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]Find parameter estimates maximize log-likelihood:\\[\n\\hat{\\beta}_{MLE} = \\underset{\\beta}{\\text{argmax}} \\sum_{=1}^{n} \\ln(f_{Y|X}(y_i, x_i; \\beta)).\n\\]Properties Probit Logit EstimatorsConsistency Asymptotic Normality:\nProbit Logit estimators consistent asymptotically normal :\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.\nA5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).\n\nProbit Logit estimators consistent asymptotically normal :\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.\nA5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).\nA2 Full Rank: \\(E(x_i' x_i)\\) exists non-singular.A5 Data Generation (Random Sampling): \\(\\{y_i, x_i\\}\\) iid (stationary weakly dependent).Distributional assumptions \\(\\epsilon_i\\) hold (e.g., normal logistic, independent \\(x_i\\)).Asymptotic Efficiency:\nassumptions, Probit Logit estimators asymptotically efficient variance:\n\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\n\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).\nassumptions, Probit Logit estimators asymptotically efficient variance:\n\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\n\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).assumptions, Probit Logit estimators asymptotically efficient variance:\\[\n(\\beta_0)^{-1} = \\left[E\\left(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i \\beta_0)(1 - F_\\epsilon(x_i \\beta_0))} x_i' x_i \\right)\\right]^{-1},\n\\]\\(f_\\epsilon(x_i \\beta_0)\\) PDF (derivative CDF).Interpretation Binary Response ModelsBinary response models, Probit Logit, estimate probability event occurring (\\(y_i = 1\\)) given predictor variables \\(x_i\\). However, interpreting estimated coefficients (\\(\\beta\\)) models differs significantly linear models. , explore interpret coefficients concept partial effects.Interpreting \\(\\beta\\) Binary Response ModelsIn binary response models, coefficient \\(\\beta_j\\) represents average change latent variable \\(y_i^*\\) (unobserved variable) one-unit change \\(x_{ij}\\). provides insight direction relationship:Magnitudes \\(\\beta_j\\) direct, meaningful interpretation terms \\(y_i\\).Direction \\(\\beta_j\\) meaningful:\n\\(\\beta_j > 0\\): positive association \\(x_{ij}\\) probability \\(y_i = 1\\).\n\\(\\beta_j < 0\\): negative association \\(x_{ij}\\) probability \\(y_i = 1\\).\n\\(\\beta_j > 0\\): positive association \\(x_{ij}\\) probability \\(y_i = 1\\).\\(\\beta_j < 0\\): negative association \\(x_{ij}\\) probability \\(y_i = 1\\).Partial Effects Nonlinear Binary ModelsTo interpret effect change predictor \\(x_{ij}\\) probability event occurring (\\(P(y_i = 1|x_i)\\)), use partial effect:\\[\nE(y_i | x_i) = F_\\epsilon(x_i \\beta),\n\\]\\(F_\\epsilon(.)\\) cumulative distribution function (CDF) error term \\(\\epsilon_i\\) (e.g., standard normal Probit, logistic Logit). partial effect derivative expected probability respect \\(x_{ij}\\):\\[\nPE(x_{ij}) = \\frac{\\partial E(y_i | x_i)}{\\partial x_{ij}} = f_\\epsilon(x_i \\beta) \\beta_j,\n\\]:\\(f_\\epsilon(.)\\) probability density function (PDF) error term \\(\\epsilon_i\\).\\(f_\\epsilon(.)\\) probability density function (PDF) error term \\(\\epsilon_i\\).\\(\\beta_j\\) coefficient associated \\(x_{ij}\\).\\(\\beta_j\\) coefficient associated \\(x_{ij}\\).Key Characteristics Partial EffectsScaling Factor:\npartial effect depends scaling factor, \\(f_\\epsilon(x_i \\beta)\\), derived density function \\(f_\\epsilon(.)\\).\nscaling factor varies depending values \\(x_i\\), making partial effect nonlinear context-dependent.\npartial effect depends scaling factor, \\(f_\\epsilon(x_i \\beta)\\), derived density function \\(f_\\epsilon(.)\\).scaling factor varies depending values \\(x_i\\), making partial effect nonlinear context-dependent.Non-Constant Partial Effects:\nUnlike linear models coefficients directly represent constant marginal effects, partial effect binary models changes based \\(x_i\\).\nexample, Logit model, partial effect largest \\(P(y_i = 1 | x_i)\\) around 0.5 (midpoint S-shaped logistic curve) smaller extremes (close 0 1).\nUnlike linear models coefficients directly represent constant marginal effects, partial effect binary models changes based \\(x_i\\).example, Logit model, partial effect largest \\(P(y_i = 1 | x_i)\\) around 0.5 (midpoint S-shaped logistic curve) smaller extremes (close 0 1).Single Values Partial EffectsIn practice, researchers often summarize partial effects using either:Partial Effect Average (PEA):\npartial effect calculated “average individual,” \\(x_i = \\bar{x}\\) (sample mean predictors): \\[\nPEA = f_\\epsilon(\\bar{x}\\hat{\\beta}) \\hat{\\beta}_j.\n\\]\nprovides single, interpretable value assumes average effect applies individuals.\npartial effect calculated “average individual,” \\(x_i = \\bar{x}\\) (sample mean predictors): \\[\nPEA = f_\\epsilon(\\bar{x}\\hat{\\beta}) \\hat{\\beta}_j.\n\\]provides single, interpretable value assumes average effect applies individuals.Average Partial Effect (APE):\naverage individual-level partial effects across sample: \\[\nAPE = \\frac{1}{n} \\sum_{=1}^{n} f_\\epsilon(x_i \\hat{\\beta}) \\hat{\\beta}_j.\n\\]\naccounts nonlinearity partial effects provides accurate summary marginal effect population.\naverage individual-level partial effects across sample: \\[\nAPE = \\frac{1}{n} \\sum_{=1}^{n} f_\\epsilon(x_i \\hat{\\beta}) \\hat{\\beta}_j.\n\\]accounts nonlinearity partial effects provides accurate summary marginal effect population.Comparing Partial Effects Linear Nonlinear ModelsLinear Models:\nPartial effects constant: \\(APE = PEA\\).\ncoefficients directly represent marginal effects \\(E(y_i | x_i)\\).\nPartial effects constant: \\(APE = PEA\\).coefficients directly represent marginal effects \\(E(y_i | x_i)\\).Nonlinear Models:\nPartial effects constant due dependence \\(f_\\epsilon(x_i \\beta)\\).\nresult, \\(APE \\neq PEA\\) general.\nPartial effects constant due dependence \\(f_\\epsilon(x_i \\beta)\\).result, \\(APE \\neq PEA\\) general.","code":""},{"path":"linear-regression.html","id":"penalized-regularized-estimators","chapter":"5 Linear Regression","heading":"5.4 Penalized (Regularized) Estimators","text":"Penalized regularized estimators extensions Ordinary Least Squares (OLS) designed address limitations, particularly high-dimensional settings. Regularization methods introduce penalty term loss function prevent overfitting, handle multicollinearity, improve model interpretability.three popular regularization techniques (limited ):Ridge RegressionLasso RegressionElastic Net","code":""},{"path":"linear-regression.html","id":"motivation-for-penalized-estimators","chapter":"5 Linear Regression","heading":"5.4.1 Motivation for Penalized Estimators","text":"OLS minimizes Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n \\left( y_i - \\hat{y}_i \\right)^2 = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2,\n\\]:\\(y_i\\) observed outcome,\\(y_i\\) observed outcome,\\(x_i\\) vector predictors observation \\(\\),\\(x_i\\) vector predictors observation \\(\\),\\(\\beta\\) vector coefficients.\\(\\beta\\) vector coefficients.OLS works well ideal conditions (e.g., low dimensionality, multicollinearity), struggles :Multicollinearity: Predictors highly correlated, leading large variances \\(\\beta\\) estimates.Multicollinearity: Predictors highly correlated, leading large variances \\(\\beta\\) estimates.High Dimensionality: number predictors (\\(p\\)) exceeds approaches sample size (\\(n\\)), making OLS inapplicable unstable.High Dimensionality: number predictors (\\(p\\)) exceeds approaches sample size (\\(n\\)), making OLS inapplicable unstable.Overfitting: \\(p\\) large, OLS fits noise data, reducing generalizability.Overfitting: \\(p\\) large, OLS fits noise data, reducing generalizability.address issues, penalized regression modifies OLS loss function adding penalty term shrinks coefficients toward zero. discourages overfitting improves predictive performance.general form penalized loss function :\\[\nL(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda P(\\beta),\n\\]:\\(\\lambda \\geq 0\\): Tuning parameter controlling strength regularization.\\(\\lambda \\geq 0\\): Tuning parameter controlling strength regularization.\\(P(\\beta)\\): Penalty term quantifies model complexity.\\(P(\\beta)\\): Penalty term quantifies model complexity.Different choices \\(P(\\beta)\\) lead ridge regression, lasso regression, elastic net.","code":""},{"path":"linear-regression.html","id":"ridge-regression","chapter":"5 Linear Regression","heading":"5.4.2 Ridge Regression","text":"Ridge regression, also known L2 regularization, penalizes sum squared coefficients:\\[\nP(\\beta) = \\sum_{j=1}^p \\beta_j^2.\n\\]ridge objective function becomes:\\[\nL_{ridge}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\]:\\(\\lambda \\geq 0\\) controls degree shrinkage. Larger \\(\\lambda\\) leads greater shrinkage.Ridge regression closed-form solution:\\[\n\\hat{\\beta}_{ridge} = \\left( X'X + \\lambda \\right)^{-1} X'y,\n\\]\\(\\) \\(p \\times p\\) identity matrix.Key FeaturesShrinks coefficients set exactly zero.Handles multicollinearity effectively stabilizing coefficient estimates (Hoerl Kennard 1970).Works well predictors contribute response.Example Use CaseRidge regression ideal applications many correlated predictors, :Predicting housing prices based large set features (e.g., size, location, age house).","code":""},{"path":"linear-regression.html","id":"lasso-regression","chapter":"5 Linear Regression","heading":"5.4.3 Lasso Regression","text":"Lasso regression, L1 regularization, penalizes sum absolute coefficients:\\[\nP(\\beta) = \\sum_{j=1}^p |\\beta_j|.\n\\]lasso objective function :\\[\nL_{lasso}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]Key FeaturesUnlike ridge regression, lasso can set coefficients exactly zero, performing automatic feature selection.Encourages sparse models, making suitable high-dimensional data (Tibshirani 1996).OptimizationLasso closed-form solution due non-differentiability \\(|\\beta_j|\\) \\(\\beta_j = 0\\). requires iterative algorithms, :Coordinate Descent,Coordinate Descent,Least Angle Regression (LARS).Least Angle Regression (LARS).Example Use CaseLasso regression useful many predictors irrelevant, :Genomics, subset genes associated disease outcome.","code":""},{"path":"linear-regression.html","id":"elastic-net","chapter":"5 Linear Regression","heading":"5.4.4 Elastic Net","text":"Elastic Net combines penalties ridge lasso regression:\\[\nP(\\beta) = \\alpha \\sum_{j=1}^p |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^p \\beta_j^2,\n\\]:\\(0 \\leq \\alpha \\leq 1\\) determines balance lasso (L1) ridge (L2) penalties.\\(0 \\leq \\alpha \\leq 1\\) determines balance lasso (L1) ridge (L2) penalties.\\(\\lambda\\) controls overall strength regularization.\\(\\lambda\\) controls overall strength regularization.elastic net objective function :\\[\nL_{elastic\\ net}(\\beta) = \\sum_{=1}^n \\left( y_i - x_i'\\beta \\right)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^p |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^p \\beta_j^2 \\right).\n\\]Key FeaturesCombines strengths lasso (sparse models) ridge (stability correlated predictors) (H. Zou Hastie 2005).Effective predictors highly correlated \\(p > n\\).Example Use CaseElastic net ideal high-dimensional datasets correlated predictors, :Predicting customer churn using demographic behavioral features.","code":""},{"path":"linear-regression.html","id":"tuning-parameter-selection","chapter":"5 Linear Regression","heading":"5.4.5 Tuning Parameter Selection","text":"Choosing regularization parameter \\(\\lambda\\) (\\(\\alpha\\) elastic net) critical balancing model complexity (fit) regularization (parsimony). \\(\\lambda\\) large, coefficients overly shrunk (even set zero case L1 penalty), leading underfitting. \\(\\lambda\\) small, model might overfit coefficients penalized sufficiently. Hence, systematic approach needed determine optimal \\(\\lambda\\). elastic net, also choose appropriate \\(\\alpha\\) balance L1 L2 penalties.","code":""},{"path":"linear-regression.html","id":"cross-validation","chapter":"5 Linear Regression","heading":"5.4.5.1 Cross-Validation","text":"common approach selecting \\(\\lambda\\) (\\(\\alpha\\)) \\(K\\)-Fold Cross-Validation:Partition data \\(K\\) roughly equal-sized “folds.”Train model \\(K-1\\) folds validate remaining fold, computing validation error.Repeat process folds, compute average validation error across \\(K\\) folds.Select value \\(\\lambda\\) (\\(\\alpha\\) tuning ) minimizes cross-validated error.method helps us maintain good bias-variance trade-every point used training validation exactly .","code":""},{"path":"linear-regression.html","id":"information-criteria","chapter":"5 Linear Regression","heading":"5.4.5.2 Information Criteria","text":"Alternatively, one can use information criteria—like Akaike Information Criterion (AIC) Bayesian Information Criterion (BIC)—guide model selection. criteria reward goodness--fit penalizing model complexity, thereby helping selecting appropriately regularized model.","code":""},{"path":"linear-regression.html","id":"properties-of-penalized-estimators","chapter":"5 Linear Regression","heading":"5.4.6 Properties of Penalized Estimators","text":"Bias-Variance Tradeoff:\nRegularization introduces bias exchange reducing variance, often resulting better predictive performance new data.\nRegularization introduces bias exchange reducing variance, often resulting better predictive performance new data.Shrinkage:\nRidge shrinks coefficients toward zero usually retains predictors.\nLasso shrinks coefficients exactly zero, performing inherent feature selection.\nRidge shrinks coefficients toward zero usually retains predictors.Lasso shrinks coefficients exactly zero, performing inherent feature selection.Flexibility:\nElastic net allows continuum ridge lasso, can adapt different data structures (e.g., many correlated features high-dimensional feature spaces).\nElastic net allows continuum ridge lasso, can adapt different data structures (e.g., many correlated features high-dimensional feature spaces).plot, curve represents coefficient’s value function \\(\\lambda\\).plot, curve represents coefficient’s value function \\(\\lambda\\).\\(\\lambda\\) increases (moving left right log-scale default), coefficients shrink toward zero typically stay non-zero.\\(\\lambda\\) increases (moving left right log-scale default), coefficients shrink toward zero typically stay non-zero.Ridge regression tends shrink coefficients force exactly zero.Ridge regression tends shrink coefficients force exactly zero., \\(\\lambda\\) grows, several coefficient paths hit zero exactly, illustrating variable selection property lasso.Elastic net combines ridge lasso penalties. \\(\\lambda = 0.5\\), see partial shrinkage coefficients going zero.Elastic net combines ridge lasso penalties. \\(\\lambda = 0.5\\), see partial shrinkage coefficients going zero.model often helpful suspect group-wise shrinkage (like ridge) sparse solutions (like lasso) might beneficial.model often helpful suspect group-wise shrinkage (like ridge) sparse solutions (like lasso) might beneficial.can refine choice \\(\\lambda\\) performing cross-validation lasso model:plot displays cross-validated error (often mean-squared error deviance) y-axis versus \\(\\log(\\lambda)\\) x-axis.plot displays cross-validated error (often mean-squared error deviance) y-axis versus \\(\\log(\\lambda)\\) x-axis.Two vertical dotted lines typically appear:\n\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\n\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).\nTwo vertical dotted lines typically appear:\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\\(\\lambda.min\\): \\(\\lambda\\) achieves minimum cross-validated error.\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).\\(\\lambda.1se\\): largest \\(\\lambda\\) cross-validated error still within one standard error minimum. conservative choice favors higher regularization (simpler models).best_lambda prints numeric value \\(\\lambda.min\\). \\(\\lambda\\) gave lowest cross-validation error lasso model.best_lambda prints numeric value \\(\\lambda.min\\). \\(\\lambda\\) gave lowest cross-validation error lasso model.Interpretation:using cv.glmnet, systematically compare different values \\(\\lambda\\) terms predictive performance (cross-validation error).using cv.glmnet, systematically compare different values \\(\\lambda\\) terms predictive performance (cross-validation error).selected \\(\\lambda\\) typically balances smaller model (due regularization) retaining sufficient predictive power.selected \\(\\lambda\\) typically balances smaller model (due regularization) retaining sufficient predictive power.used real-world data, might also look performance metrics hold-test set ensure chosen \\(\\lambda\\) generalizes well.used real-world data, might also look performance metrics hold-test set ensure chosen \\(\\lambda\\) generalizes well.","code":"\n# Load required libraries\nlibrary(glmnet)\n\n# Simulate data\nset.seed(123)\nn <- 100   # Number of observations\np <- 20    # Number of predictors\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)  # Predictor matrix\ny <- rnorm(n)                                 # Response vector\n\n# Ridge regression (alpha = 0)\nridge_fit <- glmnet(X, y, alpha = 0)\nplot(ridge_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Ridge Regression\")\n# Lasso regression (alpha = 1)\nlasso_fit <- glmnet(X, y, alpha = 1)\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Lasso Regression\")\n# Elastic net (alpha = 0.5)\nelastic_net_fit <- glmnet(X, y, alpha = 0.5)\nplot(elastic_net_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"Coefficient Paths for Elastic Net (alpha = 0.5)\")\ncv_lasso <- cv.glmnet(X, y, alpha = 1)\nplot(cv_lasso)\nbest_lambda <- cv_lasso$lambda.min\nbest_lambda\n#> [1] 0.1449586"},{"path":"linear-regression.html","id":"robust-estimators","chapter":"5 Linear Regression","heading":"5.5 Robust Estimators","text":"Robust estimators statistical techniques designed provide reliable parameter estimates even assumptions underlying classical methods, Ordinary Least Squares (OLS), violated. Specifically, address issues caused outliers, non-normal errors, heavy-tailed distributions, can render OLS inefficient biased.goal robust estimation reduce sensitivity estimator extreme aberrant data points, thereby ensuring reliable accurate fit majority data.cover key robust estimation techniques, properties, applications, along practical examples mathematical derivations. focus include \\(M\\)-estimators, \\(R\\)-estimators, \\(L\\)-estimators, \\(LTS\\), \\(S\\)-estimators, \\(MM\\)-estimators, .","code":""},{"path":"linear-regression.html","id":"motivation-for-robust-estimation","chapter":"5 Linear Regression","heading":"5.5.1 Motivation for Robust Estimation","text":"OLS seeks minimize Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n (y_i - x_i'\\beta)^2,\n\\]:\\(y_i\\) observed response \\(\\)th observation,\\(x_i\\) vector predictors \\(\\)th observation,\\(\\beta\\) vector coefficients.OLS assumes:Errors normally distributed outliers data (A6 Normal Distribution).Homoscedasticity (constant variance errors) (A4 Homoskedasticity).real-world scenarios:Outliers \\(y\\) \\(x\\) can disproportionately affect estimates, leading biased inefficient results.Outliers \\(y\\) \\(x\\) can disproportionately affect estimates, leading biased inefficient results.Heavy-tailed distributions (e.g., Cauchy) violate normality assumption, making OLS inappropriate.Heavy-tailed distributions (e.g., Cauchy) violate normality assumption, making OLS inappropriate.example, P. J. Huber (1964) demonstrates single extreme observation can arbitrarily distort OLS estimates, Hampel et al. (2005) define breakdown point measure robustness. Robust estimators aim mitigate problems limiting influence problematic observations.OLS inherently squares residuals \\(e_i = y_i - x_i'\\beta\\), amplifying influence large residuals. example, single residual much larger others, squared value can dominate RSS, distorting estimated coefficients.Consider simple case \\(y_i = \\beta_0 + \\beta_1 x_i + e_i\\), \\(e_i \\sim N(0, \\sigma^2)\\) classical assumptions. Now introduce outlier: single observation unusually large \\(e_i\\). squared residual point dominate RSS pull estimated regression line towards , leading biased estimates \\(\\beta_0\\) \\(\\beta_1\\).breakdown point estimator proportion contamination (e.g., outliers) estimator can tolerate yielding arbitrarily large incorrect results. OLS, breakdown point \\(1/n\\), meaning even one outlier can cause substantial distortion estimates.","code":""},{"path":"linear-regression.html","id":"m-estimators","chapter":"5 Linear Regression","heading":"5.5.2 \\(M\\)-Estimators","text":"address sensitivity OLS, robust estimators minimize different objective function:\\[\n\\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]:\\(\\rho(\\cdot)\\) robust loss function grows slower quadratic function used OLS,\\(\\sigma\\) scale parameter normalize residuals.OLS, quadratic loss function \\(\\rho(z) = z^2\\) penalizes large residuals disproportionately. Robust estimators replace alternative \\(\\rho\\) functions limit penalty large residuals, thus reducing influence parameter estimates.robust \\(\\rho\\) function satisfy following properties:Bounded Influence: Large residuals contribute finite amount objective function.Symmetry: \\(\\rho(z) = \\rho(-z)\\) ensures positive negative residuals treated equally.Differentiability: computational tractability, \\(\\rho\\) smooth differentiable.","code":""},{"path":"linear-regression.html","id":"examples-of-robust-rho-functions","chapter":"5 Linear Regression","heading":"5.5.2.1 Examples of Robust \\(\\rho\\) Functions","text":"Huber’s Loss Function (P. J. Huber 1964)Huber’s loss function transitions quadratic linear growth:\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Key features:small residuals (\\(|z| \\leq c\\)), loss quadratic, mimicking OLS.large residuals (\\(|z| > c\\)), loss grows linearly, limiting influence.parameter \\(c\\) controls threshold loss function transitions quadratic linear. Smaller values \\(c\\) make estimator robust potentially less efficient normality.Tukey’s Bisquare Function (Beaton Tukey 1974)Tukey’s bisquare function completely bounds influence large residuals:\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Key features:Residuals larger \\(c\\) contribute constant value objective function, effectively excluding estimation process.approach achieves high robustness cost lower efficiency small residuals.Andrews’ Sine Function (D. F. Andrews 1974):\nSmoothly downweights extreme residuals: \\[ \\rho(z) = \\begin{cases}  c^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\ c^2/2 & \\text{} |z| > \\pi c. \\end{cases} \\]\nSmoothly downweights extreme residuals: \\[ \\rho(z) = \\begin{cases}  c^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\ c^2/2 & \\text{} |z| > \\pi c. \\end{cases} \\]","code":""},{"path":"linear-regression.html","id":"weighting-scheme-influence-functions","chapter":"5 Linear Regression","heading":"5.5.2.2 Weighting Scheme: Influence Functions","text":"critical concept robust estimation influence function, describes sensitivity estimator individual observations. \\(M\\)-estimators, influence function derived derivative loss function \\(\\rho(z)\\) respect \\(z\\):\\[\n\\psi(z) = \\frac{d}{dz} \\rho(z).\n\\]function plays crucial role downweighting large residuals. weight assigned residual proportional \\(\\psi(z)/z\\), decreases \\(|z|\\) increases robust estimators.Huber’s loss function, influence function :\\[\n\\psi(z) =\n\\begin{cases}\nz & \\text{} |z| \\leq c, \\\\\nc \\cdot \\text{sign}(z) & \\text{} |z| > c.\n\\end{cases}\n\\]small residuals, \\(\\psi(z) = z\\), matching OLS.large residuals, \\(\\psi(z)\\) constant, ensuring bounded influence.key consideration selecting robust estimator trade-robustness (resistance outliers) efficiency (performance ideal conditions). tuning parameters \\(\\rho\\) functions (e.g., \\(c\\) Huber’s loss) directly affect balance:Smaller \\(c\\) increases robustness reduces efficiency normality.Larger \\(c\\) improves efficiency normality decreases robustness outliers.trade-reflects fundamental goal robust estimation: achieve balance reliability precision across wide range data scenarios.","code":""},{"path":"linear-regression.html","id":"properties-of-m-estimators","chapter":"5 Linear Regression","heading":"5.5.2.3 Properties of \\(M\\)-Estimators","text":"Robust estimators, particularly \\(M\\)-estimators, possess following mathematical properties:Asymptotic Normality: mild regularity conditions, \\(M\\)-estimators asymptotically normal: \\[\n\\sqrt{n} (\\hat{\\beta} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\] \\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.Consistency: \\(n \\\\infty\\), \\(\\hat{\\beta} \\\\beta\\) probability, provided majority data satisfies model assumptions.Breakdown Point: \\(M\\)-estimators typically moderate breakdown point, sufficient handle reasonable proportion contamination.","code":""},{"path":"linear-regression.html","id":"r-estimators","chapter":"5 Linear Regression","heading":"5.5.3 \\(R\\)-Estimators","text":"\\(R\\)-estimators class robust estimators rely ranks residuals rather raw magnitudes. approach makes naturally resistant influence outliers highly effective scenarios involving ordinal data heavy-tailed error distributions. leveraging rank-based methods, \\(R\\)-estimators particularly useful situations classical assumptions data, normality homoscedasticity, hold.general form \\(R\\)-estimator can expressed :\\[\n\\hat{\\beta}_R = \\arg\\min_\\beta \\sum_{=1}^n w_i R_i \\left(y_i - x_i'\\beta\\right),\n\\]:\\(R_i\\) ranks residuals \\(e_i = y_i - x_i'\\beta\\),\\(w_i\\) rank-based weights determined chosen scoring function,\\(y_i\\) observed responses, \\(x_i\\) predictor values, \\(\\beta\\) vector coefficients.formulation differs \\(M\\)-estimators, directly minimize loss function \\(\\rho\\), instead using ordering residuals drive estimation.","code":""},{"path":"linear-regression.html","id":"ranks-and-scoring-function","chapter":"5 Linear Regression","heading":"5.5.3.1 Ranks and Scoring Function","text":"","code":""},{"path":"linear-regression.html","id":"definition-of-ranks","chapter":"5 Linear Regression","heading":"5.5.3.1.1 Definition of Ranks","text":"rank \\(R_i\\) residual \\(e_i\\) position sorted sequence residuals:\\[\nR_i = \\text{rank}(e_i) = \\sum_{j=1}^n \\mathbb{}(e_j \\leq e_i),\n\\]\\(\\mathbb{}(\\cdot)\\) indicator function, equal 1 condition true 0 otherwise. step transforms residuals ordinal scale, eliminating dependency magnitude.","code":""},{"path":"linear-regression.html","id":"scoring-function","chapter":"5 Linear Regression","heading":"5.5.3.1.2 Scoring Function","text":"weights \\(w_i\\) derived scoring function \\(S(R_i)\\), assigns importance rank. common choice Wilcoxon scoring function, defined :\\[\nS(R_i) = \\frac{R_i}{n + 1},\n\\]gives equal weight ranks, scaled position relative total number observations \\(n\\).scoring functions can emphasize different parts rank distribution:Normal Scores: Derived quantiles standard normal distribution.Logarithmic Scores: Weight lower ranks heavily.flexibility scoring function allows \\(R\\)-estimators adapt various data structures assumptions.","code":""},{"path":"linear-regression.html","id":"properties-of-r-estimators","chapter":"5 Linear Regression","heading":"5.5.3.2 Properties of \\(R\\)-Estimators","text":"","code":""},{"path":"linear-regression.html","id":"influence-function-and-robustness","chapter":"5 Linear Regression","heading":"5.5.3.2.1 Influence Function and Robustness","text":"key feature \\(R\\)-estimators bounded influence function, ensures robustness. estimator depends ranks residuals, extreme values \\(y\\) \\(x\\) disproportionately affect results.\\(R\\)-estimators, influence function \\(\\psi(e_i)\\) proportional derivative rank-based objective function:\\[\n\\psi(e_i) = S'(R_i),\n\\]\\(S'(R_i)\\) derivative scoring function. Since \\(R_i\\) depends ordering residuals, outliers data produce excessive changes \\(R_i\\), resulting bounded influence.","code":""},{"path":"linear-regression.html","id":"breakdown-point","chapter":"5 Linear Regression","heading":"5.5.3.2.2 Breakdown Point","text":"breakdown point \\(R\\)-estimators higher OLS comparable robust methods. means can tolerate larger proportion contaminated data without yielding unreliable results.","code":""},{"path":"linear-regression.html","id":"asymptotic-efficiency","chapter":"5 Linear Regression","heading":"5.5.3.2.3 Asymptotic Efficiency","text":"specific scoring functions, \\(R\\)-estimators achieve high asymptotic efficiency. example, Wilcoxon \\(R\\)-estimator performs nearly well OLS normality retaining robustness non-normality.","code":""},{"path":"linear-regression.html","id":"derivation-of-r-estimators-for-simple-linear-regression","chapter":"5 Linear Regression","heading":"5.5.3.3 Derivation of \\(R\\)-Estimators for Simple Linear Regression","text":"Consider simple linear regression model:\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i,\n\\]\\(e_i = y_i - (\\beta_0 + \\beta_1 x_i)\\) residuals.Rank Residuals: Compute residuals \\(e_i\\) observations rank smallest largest.Rank Residuals: Compute residuals \\(e_i\\) observations rank smallest largest.Assign Weights: Compute weights \\(w_i\\) residual rank based scoring function \\(S(R_i)\\).Assign Weights: Compute weights \\(w_i\\) residual rank based scoring function \\(S(R_i)\\).Minimize Rank-Based Objective: Solve following optimization problem:\n\\[\n\\hat{\\beta}_R = \\arg\\min_{\\beta_0, \\beta_1} \\sum_{=1}^n w_i R_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right).\n\\]\nminimization can performed iteratively using numerical methods, rank-based nature function makes direct analytic solutions challenging.Minimize Rank-Based Objective: Solve following optimization problem:\\[\n\\hat{\\beta}_R = \\arg\\min_{\\beta_0, \\beta_1} \\sum_{=1}^n w_i R_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right).\n\\]minimization can performed iteratively using numerical methods, rank-based nature function makes direct analytic solutions challenging.","code":""},{"path":"linear-regression.html","id":"comparison-to-m-estimators","chapter":"5 Linear Regression","heading":"5.5.3.4 Comparison to \\(M\\)-Estimators","text":"\\(M\\)-estimators downweight large residuals using robust loss functions, \\(R\\)-estimators completely avoid reliance magnitude residuals using ranks. distinction important implications:\\(R\\)-estimators naturally robust leverage points extreme outliers.performance \\(R\\)-estimators less sensitive choice scale parameter compared \\(M\\)-estimators.However, \\(R\\)-estimators may less efficient \\(M\\)-estimators normality use full information contained residual magnitudes.","code":""},{"path":"linear-regression.html","id":"l-estimators","chapter":"5 Linear Regression","heading":"5.5.4 \\(L\\)-Estimators","text":"\\(L\\)-estimators class robust estimators constructed linear combinations order statistics, order statistics simply sorted values dataset. estimators particularly appealing due intuitive nature computational simplicity. using relative ranks observations, \\(L\\)-estimators offer robustness outliers heavy-tailed distributions.Order statistics denoted \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\), \\(y_{()}\\) \\(\\)th smallest observation sample.general form \\(L\\)-estimator :\\[\n\\hat{\\theta}_L = \\sum_{=1}^n c_i y_{()},\n\\]:\\(y_{()}\\) order statistics (sorted observations),\\(c_i\\) coefficients (weights) determine contribution order statistic estimator.appropriately choosing weights \\(c_i\\), different types \\(L\\)-estimators can constructed suit specific needs, handling outliers capturing central tendencies robustly.Examples \\(L\\)-EstimatorsSample Median: sample median simple \\(L\\)-estimator middle order statistic contributes (odd \\(n\\)) average two middle order statistics contributes (even \\(n\\)):\n\\[\n\\hat{\\mu}_{\\text{median}} =\n\\begin{cases}\ny_{\\left(\\frac{n+1}{2}\\right)} & \\text{} n \\text{ odd}, \\\\\n\\frac{1}{2}\\left(y_{\\left(\\frac{n}{2}\\right)} + y_{\\left(\\frac{n}{2} + 1\\right)}\\right) & \\text{} n \\text{ even}.\n\\end{cases}\n\\]\nRobustness: median breakdown point \\(50\\%\\), meaning remains unaffected unless half data corrupted.\nEfficiency: normality, efficiency median lower mean (\\(64\\%\\)).\nSample Median: sample median simple \\(L\\)-estimator middle order statistic contributes (odd \\(n\\)) average two middle order statistics contributes (even \\(n\\)):\\[\n\\hat{\\mu}_{\\text{median}} =\n\\begin{cases}\ny_{\\left(\\frac{n+1}{2}\\right)} & \\text{} n \\text{ odd}, \\\\\n\\frac{1}{2}\\left(y_{\\left(\\frac{n}{2}\\right)} + y_{\\left(\\frac{n}{2} + 1\\right)}\\right) & \\text{} n \\text{ even}.\n\\end{cases}\n\\]Robustness: median breakdown point \\(50\\%\\), meaning remains unaffected unless half data corrupted.Efficiency: normality, efficiency median lower mean (\\(64\\%\\)).Trimmed Mean: trimmed mean excludes smallest largest \\(k\\%\\) observations averaging remaining values:\n\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()},\n\\]\n:\n\\(k\\) number observations trimmed tail,\n\\(n\\) sample size.\nRobustness: trimmed mean less sensitive extreme values sample mean.\nEfficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.\nTrimmed Mean: trimmed mean excludes smallest largest \\(k\\%\\) observations averaging remaining values:\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()},\n\\]:\\(k\\) number observations trimmed tail,\\(k\\) number observations trimmed tail,\\(n\\) sample size.\\(n\\) sample size.Robustness: trimmed mean less sensitive extreme values sample mean.Robustness: trimmed mean less sensitive extreme values sample mean.Efficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.Efficiency: retaining observations, trimmed mean achieves good balance robustness efficiency.Winsorized Mean: Similar trimmed mean, instead excluding extreme values, replaces nearest remaining observations:\n\\[\n\\hat{\\mu}_W = \\frac{1}{n} \\sum_{=1}^n y_{()}^*,\n\\]\n\\(y_{()}^*\\) “Winsorized” values: \\[\ny_{()}^* =\n\\begin{cases}\ny_{(k+1)} & \\text{} \\leq k, \\\\\ny_{()} & \\text{} k+1 \\leq \\leq n-k, \\\\\ny_{(n-k)} & \\text{} > n-k.\n\\end{cases}\n\\]\nRobustness: Winsorized mean reduces influence outliers without discarding data.\nEfficiency: Slightly less efficient trimmed mean normality.\nWinsorized Mean: Similar trimmed mean, instead excluding extreme values, replaces nearest remaining observations:\\[\n\\hat{\\mu}_W = \\frac{1}{n} \\sum_{=1}^n y_{()}^*,\n\\]\\(y_{()}^*\\) “Winsorized” values: \\[\ny_{()}^* =\n\\begin{cases}\ny_{(k+1)} & \\text{} \\leq k, \\\\\ny_{()} & \\text{} k+1 \\leq \\leq n-k, \\\\\ny_{(n-k)} & \\text{} > n-k.\n\\end{cases}\n\\]Robustness: Winsorized mean reduces influence outliers without discarding data.Efficiency: Slightly less efficient trimmed mean normality.Midrange: midrange average smallest largest observations:\n\\[\n\\hat{\\mu}_{\\text{midrange}} = \\frac{y_{(1)} + y_{(n)}}{2}.\n\\]\nRobustness: Poor robustness, depends entirely extreme observations.\nSimplicity: Highly intuitive computationally trivial.\nMidrange: midrange average smallest largest observations:\\[\n\\hat{\\mu}_{\\text{midrange}} = \\frac{y_{(1)} + y_{(n)}}{2}.\n\\]Robustness: Poor robustness, depends entirely extreme observations.Simplicity: Highly intuitive computationally trivial.","code":""},{"path":"linear-regression.html","id":"properties-of-l-estimators","chapter":"5 Linear Regression","heading":"5.5.4.1 Properties of \\(L\\)-Estimators","text":"Robustness Outliers: \\(L\\)-estimators gain robustness downweighting excluding extreme observations. instance:\ntrimmed mean completely removes outliers estimation process.\nWinsorized mean limits influence outliers bounding values.\ntrimmed mean completely removes outliers estimation process.Winsorized mean limits influence outliers bounding values.Breakdown Point:\nbreakdown point \\(L\\)-estimator depends many extreme observations excluded replaced.\nmedian highest possible breakdown point (\\(50\\%\\)), trimmed Winsorized means breakdown points proportional trimming percentage.\nbreakdown point \\(L\\)-estimator depends many extreme observations excluded replaced.median highest possible breakdown point (\\(50\\%\\)), trimmed Winsorized means breakdown points proportional trimming percentage.Efficiency:\nefficiency \\(L\\)-estimators varies depending underlying data distribution specific estimator.\nsymmetric distributions, trimmed mean Winsorized mean approach efficiency sample mean much robust.\nefficiency \\(L\\)-estimators varies depending underlying data distribution specific estimator.symmetric distributions, trimmed mean Winsorized mean approach efficiency sample mean much robust.Computational Simplicity:\n\\(L\\)-estimators involve simple operations like sorting averaging, making computationally efficient even large datasets.\n\\(L\\)-estimators involve simple operations like sorting averaging, making computationally efficient even large datasets.","code":""},{"path":"linear-regression.html","id":"derivation-of-the-trimmed-mean","chapter":"5 Linear Regression","heading":"5.5.4.2 Derivation of the Trimmed Mean","text":"understand robustness trimmed mean, consider dataset \\(n\\) observations. Sorting data gives \\(y_{(1)} \\leq y_{(2)} \\leq \\dots \\leq y_{(n)}\\). trimming smallest \\(k\\) largest \\(k\\) observations, remaining \\(n - 2k\\) observations used compute mean:\\[\n\\hat{\\mu}_T = \\frac{1}{n - 2k} \\sum_{=k+1}^{n-k} y_{()}.\n\\]Key observations:Impact \\(k\\): Larger \\(k\\) increases robustness removing extreme values reduces efficiency discarding data.Choosing \\(k\\): practice, \\(k\\) often chosen percentage total sample size, \\(10\\%\\) trimming (\\(k = 0.1n\\)).","code":""},{"path":"linear-regression.html","id":"least-trimmed-squares-lts","chapter":"5 Linear Regression","heading":"5.5.5 Least Trimmed Squares (LTS)","text":"Least Trimmed Squares (LTS) robust regression method minimizes sum smallest \\(h\\) squared residuals, rather using residuals Ordinary Least Squares (OLS). approach ensures large residuals, often caused outliers leverage points, influence parameter estimation.LTS estimator defined :\\[\n\\hat{\\beta}_{LTS} = \\arg\\min_\\beta \\sum_{=1}^h r_{[]}^2,\n\\]:\\(r_{[]}^2\\) ordered squared residuals, ranked smallest largest,\\(h\\) subset size residuals include minimization, typically chosen \\(h = \\lfloor n/2 \\rfloor + 1\\) (\\(n\\) sample size).trimming process ensures robustness focusing best-fitting \\(h\\) observations ignoring extreme residuals.","code":""},{"path":"linear-regression.html","id":"motivation-for-lts","chapter":"5 Linear Regression","heading":"5.5.5.1 Motivation for LTS","text":"OLS regression, objective minimize Residual Sum Squares (RSS):\\[\nRSS = \\sum_{=1}^n r_i^2,\n\\]\\(r_i = y_i - x_i'\\beta\\) residuals. However, method highly sensitive outliers even one large residual (\\(r_i^2\\)) can dominate RSS, distorting parameter estimates \\(\\beta\\).LTS addresses issue trimming largest residuals focusing \\(h\\) smallest ones, thus preventing extreme values affecting fit. approach provides robust estimate regression coefficients \\(\\beta\\).","code":""},{"path":"linear-regression.html","id":"properties-of-lts","chapter":"5 Linear Regression","heading":"5.5.5.2 Properties of LTS","text":"Objective Function: LTS objective function non-differentiable involves ordering squared residuals. Formally, ordered residuals denoted :\n\\[\nr_{[1]}^2 \\leq r_{[2]}^2 \\leq \\dots \\leq r_{[n]}^2,\n\\]\nobjective minimize:\n\\[\n\\sum_{=1}^h r_{[]}^2.\n\\]\nrequires sorting squared residuals, making computation complex OLS.Objective Function: LTS objective function non-differentiable involves ordering squared residuals. Formally, ordered residuals denoted :\\[\nr_{[1]}^2 \\leq r_{[2]}^2 \\leq \\dots \\leq r_{[n]}^2,\n\\]objective minimize:\\[\n\\sum_{=1}^h r_{[]}^2.\n\\]requires sorting squared residuals, making computation complex OLS.Choice \\(h\\): parameter \\(h\\) determines number residuals included minimization. common choice :\n\\[\nh = \\lfloor n/2 \\rfloor + 1,\n\\]\nensures high breakdown point (discussed ). Smaller values \\(h\\) increase robustness reduce efficiency, larger \\(h\\) values improve efficiency decrease robustness.Choice \\(h\\): parameter \\(h\\) determines number residuals included minimization. common choice :\\[\nh = \\lfloor n/2 \\rfloor + 1,\n\\]ensures high breakdown point (discussed ). Smaller values \\(h\\) increase robustness reduce efficiency, larger \\(h\\) values improve efficiency decrease robustness.Breakdown Point: LTS breakdown point approximately \\(50\\%\\), highest possible regression estimator. means LTS can handle \\(50\\%\\) contaminated data (e.g., outliers) without yielding unreliable estimates.Breakdown Point: LTS breakdown point approximately \\(50\\%\\), highest possible regression estimator. means LTS can handle \\(50\\%\\) contaminated data (e.g., outliers) without yielding unreliable estimates.Robustness: focusing \\(h\\) best-fitting observations, LTS naturally excludes outliers estimation process, making highly robust vertical outliers (extreme values \\(y\\)) leverage points (extreme values \\(x\\)).Robustness: focusing \\(h\\) best-fitting observations, LTS naturally excludes outliers estimation process, making highly robust vertical outliers (extreme values \\(y\\)) leverage points (extreme values \\(x\\)).","code":""},{"path":"linear-regression.html","id":"algorithm-for-lts","chapter":"5 Linear Regression","heading":"5.5.5.3 Algorithm for LTS","text":"Computing LTS estimator involves following steps:Initialization: Select initial subset \\(h\\) observations compute preliminary fit \\(\\beta\\).Initialization: Select initial subset \\(h\\) observations compute preliminary fit \\(\\beta\\).Residual Calculation: observation, compute squared residuals:\n\\[\nr_i^2 = \\left(y_i - x_i'\\beta\\right)^2.\n\\]Residual Calculation: observation, compute squared residuals:\\[\nr_i^2 = \\left(y_i - x_i'\\beta\\right)^2.\n\\]Trimming: Rank residuals smallest largest retain \\(h\\) smallest residuals.Trimming: Rank residuals smallest largest retain \\(h\\) smallest residuals.Refitting: Use \\(h\\) retained observations recompute regression coefficients \\(\\beta\\).Refitting: Use \\(h\\) retained observations recompute regression coefficients \\(\\beta\\).Iterative Refinement: Repeat process (residual calculation, trimming, refitting) convergence, typically \\(\\beta\\) stabilizes.Iterative Refinement: Repeat process (residual calculation, trimming, refitting) convergence, typically \\(\\beta\\) stabilizes.Efficient algorithms, Fast-LTS algorithm, used practice reduce computational complexity.","code":""},{"path":"linear-regression.html","id":"comparison-of-lts-with-ols","chapter":"5 Linear Regression","heading":"5.5.5.4 Comparison of LTS with OLS","text":"","code":""},{"path":"linear-regression.html","id":"s-estimators","chapter":"5 Linear Regression","heading":"5.5.6 \\(S\\)-Estimators","text":"\\(S\\)-estimators class robust estimators focus minimizing robust measure dispersion residuals. Unlike methods \\(M\\)-estimators, directly minimize loss function based residuals, \\(S\\)-estimators aim find parameter values \\(\\beta\\) produce residuals smallest robust scale. estimators particularly useful handling datasets outliers, heavy-tailed distributions, violations classical assumptions.scale \\(\\sigma\\) estimated solving following minimization problem:\\[\n\\hat{\\sigma}_S = \\arg\\min_\\sigma \\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]:\\(\\rho\\) robust loss function controls influence residuals,\\(y_i\\) observed responses, \\(x_i\\) predictors, \\(\\beta\\) vector regression coefficients,\\(\\sigma\\) represents robust scale residuals.\\(\\sigma\\) estimated, \\(S\\)-estimator \\(\\beta\\) obtained solving:\\[\n\\hat{\\beta}_S = \\arg\\min_\\beta \\hat{\\sigma}_S.\n\\]","code":""},{"path":"linear-regression.html","id":"motivation-for-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.1 Motivation for \\(S\\)-Estimators","text":"regression analysis, classical methods Ordinary Least Squares (OLS) rely minimizing Residual Sum Squares (RSS). However, OLS highly sensitive outliers even single extreme residual can dominate sum squared residuals, leading biased estimates \\(\\beta\\).\\(S\\)-estimators address limitation using robust scale \\(\\sigma\\) evaluate dispersion residuals. minimizing scale, \\(S\\)-estimators effectively downweight influence outliers, resulting parameter estimates resistant contamination data.","code":""},{"path":"linear-regression.html","id":"key-concepts-in-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.2 Key Concepts in \\(S\\)-Estimators","text":"Robust Scale Function: key idea \\(S\\)-estimators minimize robust measure scale. scale \\(\\sigma\\) computed residuals normalized \\(\\sigma\\) produce value close expected contribution well-behaved observations.\nFormally, \\(\\sigma\\) satisfies:\n\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta,\n\\]\n\\(\\delta\\) constant depends choice \\(\\rho\\) ensures consistency normality. equation balances residuals controls influence scale estimate.Robust Scale Function: key idea \\(S\\)-estimators minimize robust measure scale. scale \\(\\sigma\\) computed residuals normalized \\(\\sigma\\) produce value close expected contribution well-behaved observations.Formally, \\(\\sigma\\) satisfies:\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta,\n\\]\\(\\delta\\) constant depends choice \\(\\rho\\) ensures consistency normality. equation balances residuals controls influence scale estimate.Choice \\(\\rho\\)-Function: choice robust \\(\\rho\\) function critical determining behavior \\(S\\)-estimators. Common \\(\\rho\\) functions include:\nHuber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]\nTukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]\nAndrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]\nRobust \\(\\rho\\) functions grow slowly quadratic function used OLS, limiting impact large residuals.Choice \\(\\rho\\)-Function: choice robust \\(\\rho\\) function critical determining behavior \\(S\\)-estimators. Common \\(\\rho\\) functions include:Huber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]Huber’s \\(\\rho\\)-Function: \\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq c, \\\\\nc|z| - c^2/2 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Andrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]Andrews’ Sine: \\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\cos\\left(\\frac{z}{c}\\right)\\right)/2 & \\text{} |z| \\leq \\pi c, \\\\\nc^2/2 & \\text{} |z| > \\pi c.\n\\end{cases}\n\\]Robust \\(\\rho\\) functions grow slowly quadratic function used OLS, limiting impact large residuals.","code":""},{"path":"linear-regression.html","id":"properties-of-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.3 Properties of \\(S\\)-Estimators","text":"Breakdown Point: \\(S\\)-estimators breakdown point \\(50\\%\\), meaning can tolerate half data contaminated (e.g., outliers) without yielding unreliable estimates.Breakdown Point: \\(S\\)-estimators breakdown point \\(50\\%\\), meaning can tolerate half data contaminated (e.g., outliers) without yielding unreliable estimates.Efficiency: efficiency \\(S\\)-estimators depends choice \\(\\rho\\). highly robust, efficiency ideal conditions (e.g., normality) may lower OLS. Proper tuning \\(\\rho\\) can balance robustness efficiency.Efficiency: efficiency \\(S\\)-estimators depends choice \\(\\rho\\). highly robust, efficiency ideal conditions (e.g., normality) may lower OLS. Proper tuning \\(\\rho\\) can balance robustness efficiency.Influence Function: influence function measures sensitivity estimator small perturbation data. \\(S\\)-estimators, influence function bounded, ensuring robustness outliers.Influence Function: influence function measures sensitivity estimator small perturbation data. \\(S\\)-estimators, influence function bounded, ensuring robustness outliers.Consistency: mild regularity conditions, \\(S\\)-estimators consistent, meaning \\(\\hat{\\beta}_S \\\\beta\\) sample size \\(n \\\\infty\\).Consistency: mild regularity conditions, \\(S\\)-estimators consistent, meaning \\(\\hat{\\beta}_S \\\\beta\\) sample size \\(n \\\\infty\\).Asymptotic Normality: \\(S\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n}(\\hat{\\beta}_S - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.Asymptotic Normality: \\(S\\)-estimators asymptotically normal, :\\[\n\\sqrt{n}(\\hat{\\beta}_S - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\\(\\Sigma\\) depends choice \\(\\rho\\) distribution residuals.","code":""},{"path":"linear-regression.html","id":"algorithm-for-computing-s-estimators","chapter":"5 Linear Regression","heading":"5.5.6.4 Algorithm for Computing \\(S\\)-Estimators","text":"Initial Guess: Compute initial estimate \\(\\beta\\) using robust method (e.g., LTS \\(M\\)-estimator).Initial Guess: Compute initial estimate \\(\\beta\\) using robust method (e.g., LTS \\(M\\)-estimator).Scale Estimation: Compute robust estimate scale \\(\\hat{\\sigma}\\) solving:\n\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta.\n\\]Scale Estimation: Compute robust estimate scale \\(\\hat{\\sigma}\\) solving:\\[\n\\frac{1}{n} \\sum_{=1}^n \\rho\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) = \\delta.\n\\]Iterative Refinement:\nRecalculate residuals \\(r_i = y_i - x_i'\\beta\\).\nUpdate \\(\\beta\\) \\(\\sigma\\) iteratively convergence, typically using numerical optimization techniques.\nIterative Refinement:Recalculate residuals \\(r_i = y_i - x_i'\\beta\\).Update \\(\\beta\\) \\(\\sigma\\) iteratively convergence, typically using numerical optimization techniques.","code":""},{"path":"linear-regression.html","id":"mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7 \\(MM\\)-Estimators","text":"\\(MM\\)-estimators robust regression method combines strengths two powerful techniques: \\(S\\)-estimators \\(M\\)-estimators. designed achieve high breakdown point (\\(50\\%\\)) high efficiency ideal conditions (e.g., normality). combination makes \\(MM\\)-estimators one versatile widely used robust regression methods.process computing \\(MM\\)-estimators involves three main steps:Compute initial robust estimate scale using \\(S\\)-estimator.Use robust scale define weights \\(M\\)-estimator.Estimate regression coefficients solving weighted \\(M\\)-estimation problem.stepwise approach ensures robustness initial scale estimation leveraging efficiency \\(M\\)-estimators final parameter estimates.Step 1: Robust Scale EstimationThe first step estimate robust scale \\(\\sigma\\) using \\(S\\)-estimator. involves solving:\\[\n\\hat{\\sigma}_S = \\arg\\min_\\sigma \\frac{1}{n} \\sum_{=1}^n \\rho_S\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right),\n\\]\\(\\rho_S\\) robust loss function chosen control influence extreme residuals. Common choices \\(\\rho_S\\) include Huber’s Tukey’s bisquare functions. scale estimation provides robust baseline weighting residuals subsequent \\(M\\)-estimation step.Step 2: Weight Definition \\(M\\)-EstimationUsing robust scale \\(\\hat{\\sigma}_S\\) obtained Step 1, weights \\(M\\)-estimator defined based second loss function, \\(\\rho_M\\). weights downweight residuals proportional deviation relative \\(\\hat{\\sigma}_S\\). residual \\(r_i = y_i - x_i'\\beta\\), weight computed :\\[\nw_i = \\psi_M\\left(\\frac{r_i}{\\hat{\\sigma}_S}\\right) / \\frac{r_i}{\\hat{\\sigma}_S},\n\\]:\\(\\psi_M\\) derivative robust \\(\\rho_M\\) function, known influence function.\\(\\rho_M\\) often chosen provide high efficiency normality, Huber’s Hampel’s function.weights reduce impact large residuals preserving influence small, well-behaved residuals.Step 3: Final \\(M\\)-EstimationThe final step involves solving \\(M\\)-estimation problem using weights defined Step 2. coefficients \\(\\hat{\\beta}_{MM}\\) estimated minimizing weighted residuals:\\[\n\\hat{\\beta}_{MM} = \\arg\\min_\\beta \\sum_{=1}^n w_i \\rho_M\\left(\\frac{y_i - x_i'\\beta}{\\hat{\\sigma}_S}\\right).\n\\]ensures final estimates combine robustness initial \\(S\\)-estimator efficiency \\(M\\)-estimator.","code":""},{"path":"linear-regression.html","id":"properties-of-mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7.1 Properties of \\(MM\\)-Estimators","text":"High Breakdown Point:\n\\(S\\)-estimator first step ensures breakdown point \\(50\\%\\), meaning estimator can handle half data contaminated without producing unreliable results.\n\\(S\\)-estimator first step ensures breakdown point \\(50\\%\\), meaning estimator can handle half data contaminated without producing unreliable results.Asymptotic Efficiency:\nuse efficient \\(\\rho_M\\) function final \\(M\\)-estimation step ensures \\(MM\\)-estimators achieve high asymptotic efficiency normality, often close OLS.\nuse efficient \\(\\rho_M\\) function final \\(M\\)-estimation step ensures \\(MM\\)-estimators achieve high asymptotic efficiency normality, often close OLS.Robustness:\ncombination robust scale estimation downweighting large residuals makes \\(MM\\)-estimators highly robust outliers leverage points.\ncombination robust scale estimation downweighting large residuals makes \\(MM\\)-estimators highly robust outliers leverage points.Influence Function:\ninfluence function \\(MM\\)-estimators bounded, ensuring single observation can exert disproportionate influence parameter estimates.\ninfluence function \\(MM\\)-estimators bounded, ensuring single observation can exert disproportionate influence parameter estimates.Consistency:\n\\(MM\\)-estimators consistent, converging true parameter values sample size increases, provided majority data satisfies model assumptions.\n\\(MM\\)-estimators consistent, converging true parameter values sample size increases, provided majority data satisfies model assumptions.Asymptotic Normality:\n\\(MM\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.\n\\(MM\\)-estimators asymptotically normal, :\n\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\n\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.\\(MM\\)-estimators asymptotically normal, :\\[\n\\sqrt{n} (\\hat{\\beta}_{MM} - \\beta) \\xrightarrow{d} N(0, \\Sigma),\n\\]\\(\\Sigma\\) depends choice \\(\\rho_M\\) distribution residuals.","code":""},{"path":"linear-regression.html","id":"choice-of-rho-functions-for-mm-estimators","chapter":"5 Linear Regression","heading":"5.5.7.2 Choice of \\(\\rho\\)-Functions for \\(MM\\)-Estimators","text":"robustness efficiency \\(MM\\)-estimators depend choice \\(\\rho_S\\) (scale) \\(\\rho_M\\) (final estimation). Common choices include:Huber’s \\(\\rho\\)-Function: Combines quadratic linear growth balance robustness efficiency:\n\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Huber’s \\(\\rho\\)-Function: Combines quadratic linear growth balance robustness efficiency:\\[\n\\rho(z) =\n\\begin{cases}\n\\frac{z^2}{2} & \\text{} |z| \\leq c, \\\\\nc|z| - \\frac{c^2}{2} & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare Function: Provides high robustness completely bounding large residuals:\n\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Tukey’s Bisquare Function: Provides high robustness completely bounding large residuals:\\[\n\\rho(z) =\n\\begin{cases}\nc^2 \\left(1 - \\left(1 - \\left(\\frac{z}{c}\\right)^2\\right)^3\\right)/6 & \\text{} |z| \\leq c, \\\\\nc^2/6 & \\text{} |z| > c.\n\\end{cases}\n\\]Hampel’s Three-Part Redescending Function: limits influence large residuals assigning constant penalty beyond certain threshold.\n\\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq , \\\\\n|z| - ^2/2 & \\text{} < |z| \\leq b, \\\\\n\\text{constant} & \\text{} |z| > b.\n\\end{cases}\n\\]Hampel’s Three-Part Redescending Function: limits influence large residuals assigning constant penalty beyond certain threshold.\\[\n\\rho(z) =\n\\begin{cases}\nz^2/2 & \\text{} |z| \\leq , \\\\\n|z| - ^2/2 & \\text{} < |z| \\leq b, \\\\\n\\text{constant} & \\text{} |z| > b.\n\\end{cases}\n\\]","code":""},{"path":"linear-regression.html","id":"practical-considerations-1","chapter":"5 Linear Regression","heading":"5.5.8 Practical Considerations","text":"following table summarizes key properties, advantages, limitations robust estimators discussed:\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| Estimator | Key Features | Breakdown Point | Efficiency (Normality) | Applications | Advantages |\n+=================+======================================================================+=========================+==============================+==========================================================================+===============================================+\n| \\(M\\)-Estimators | Generalization OLS Robust \\(\\rho\\) reduces large residual influence | Moderate (\\(0.29\\)) | High proper tuning | Wide applicability regression moderate robustness | Balances robustness efficiency |\n| | | | | | |\n| | | | | | Flexible tuning via \\(\\rho\\)-function |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| \\(R\\)-Estimators | Rank-based method | High (depends ranks) | Moderate | Ordinal data heavily skewed distributions | Handles predictor response outliers |\n| | | | | | |\n| | Immune outliers \\(x\\) \\(y\\) | | | | Suitable ordinal rank-based data |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| \\(L\\)-Estimators | Linear combination order statistics | High (\\(50\\%\\)) | Moderate | Descriptive statistics, robust averages | Simple intuitive |\n| | | | | | |\n| | | | | | Easy compute, even large datasets |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| LTS | Minimizes smallest \\(h\\) squared residuals | High (\\(50\\%\\)) | Moderate | Data high contamination, fault detection | High robustness outliers |\n| | | | | | |\n| | | | | | Resistant leverage points |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| \\(S\\)-Estimators | Minimizes robust scale residuals | High (\\(50\\%\\)) | Low moderate | Outlier detection, data heavy-tailed distributions | Focus robust scale estimation |\n| | | | | | |\n| | | | | | Effective detecting extreme outliers |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+\n| \\(MM\\)-Estimators | High robustness (scale) + high efficiency (coefficients) | High (\\(50\\%\\)) | High | Real-world applications mixed contamination heavy-tailed errors | Combinesrobustness efficiency effectively |\n| | | | | | |\n| | | | | | Versatile flexible |\n+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+Notes Choosing Estimator\\(M\\)-Estimators: Best suited general-purpose robust regression, offering balance robustness efficiency moderate contamination.\\(R\\)-Estimators: Ideal rank-based data ordinal data, especially outliers present predictors responses.\\(L\\)-Estimators: Simple effective descriptive statistics data cleaning limited computational resources.LTS: Recommended datasets significant contamination leverage points due high breakdown point.\\(S\\)-Estimators: Focus robust scale estimation, suitable identifying mitigating influence extreme residuals.\\(MM\\)-Estimators: Combines robustness \\(S\\)-estimators efficiency \\(M\\)-estimators, making versatile choice heavily contaminated data.OLS coefficients highly influenced presence outliers. example, slope (x coefficient) intercept shifted fit outliers, resulting poor fit majority data.\\(M\\)-estimator reduces influence large residuals using Huber’s psi function. results coefficients less affected outliers compared OLS.LTS minimizes smallest squared residuals, ignoring extreme residuals. results robust fit, particularly presence vertical outliers leverage points.\\(MM\\)-estimators combine robust scale estimation (\\(S\\)-estimators) efficient coefficient estimation (\\(M\\)-estimators).\nachieves high robustness high efficiency normal conditions.Visualization shows differences regression fits:\n- OLS heavily influenced outliers provides poor fit majority data.\n)\n- \\(M\\)-estimator downweights large residuals, resulting better fit.\n- LTS regression ignores extreme residuals entirely, providing robust fit.\n- \\(MM\\)-estimators balance robustness efficiency, producing coefficients close LTS improved efficiency normality.table shows coefficients vary across methods:\n- OLS coefficients distorted outliers.\n- \\(M\\)-estimators \\(MM\\)-estimators provide coefficients less influenced extreme values.\n- LTS regression, trimming mechanism, produces robust coefficients excluding largest residuals.","code":"\n# Load necessary libraries\nlibrary(MASS)       # For robust regression functions like rlm\nlibrary(robustbase) # For LTS regression and MM-estimators\nlibrary(dplyr)      # For data manipulation\nlibrary(ggplot2)    # For visualization\n\n# Simulate dataset\nset.seed(123)\nn <- 100\nx <- rnorm(n, mean = 5, sd = 2)   # Predictor\ny <- 3 + 2 * x + rnorm(n, sd = 1) # Response\n\n# Introduce outliers\ny[95:100] <- y[95:100] + 20  # Vertical outliers\nx[90:95] <- x[90:95] + 10    # Leverage points\n\ndata <- data.frame(x, y)\n\n# Visualize the data\nggplot(data, aes(x, y)) +\n    geom_point() +\n    labs(title = \"Scatterplot of Simulated Data with Outliers\",\n         x = \"Predictor (x)\",\n         y = \"Response (y)\") +\n    theme_minimal()\n\n# Ordinary Least Squares (OLS)\nols_model <- lm(y ~ x, data = data)\nsummary(ols_model)\n#> \n#> Call:\n#> lm(formula = y ~ x, data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -12.6023  -2.4590  -0.5717   0.9247  24.4024 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   8.8346     1.1550   7.649 1.41e-11 ***\n#> x             0.9721     0.1749   5.558 2.36e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.583 on 98 degrees of freedom\n#> Multiple R-squared:  0.2396, Adjusted R-squared:  0.2319 \n#> F-statistic: 30.89 on 1 and 98 DF,  p-value: 2.358e-07\n# $M$-Estimators\nm_model <- rlm(y ~ x, data = data, psi = psi.huber)\nsummary(m_model)\n#> \n#> Call: rlm(formula = y ~ x, data = data, psi = psi.huber)\n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -18.43919  -0.97575  -0.03297   0.76967  21.85546 \n#> \n#> Coefficients:\n#>             Value   Std. Error t value\n#> (Intercept)  4.3229  0.2764    15.6421\n#> x            1.7250  0.0419    41.2186\n#> \n#> Residual standard error: 1.349 on 98 degrees of freedom\n# Least Trimmed Squares (LTS)\nlts_model <- ltsReg(y ~ x, data = data)\nlts_coefficients <- coef(lts_model)\n# $MM$-Estimators\nmm_model <- lmrob(y ~ x, data = data, setting = \"KS2014\")\nsummary(mm_model)\n#> \n#> Call:\n#> lmrob(formula = y ~ x, data = data, setting = \"KS2014\")\n#>  \\--> method = \"SMDM\"\n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -20.45989  -0.69436  -0.01455   0.73614  22.10173 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  3.02192    0.25850   11.69   <2e-16 ***\n#> x            1.96672    0.04538   43.34   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Robust residual standard error: 0.9458 \n#> Multiple R-squared:  0.9562, Adjusted R-squared:  0.9558 \n#> Convergence in 7 IRWLS iterations\n#> \n#> Robustness weights: \n#>  10 observations c(90,91,92,93,94,96,97,98,99,100)\n#>   are outliers with |weight| = 0 ( < 0.001); \n#>  67 weights are ~= 1. The remaining 23 ones are summarized as\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.2496  0.7969  0.9216  0.8428  0.9548  0.9943 \n#> Algorithmic parameters: \n#>       tuning.chi1       tuning.chi2       tuning.chi3       tuning.chi4 \n#>        -5.000e-01         1.500e+00                NA         5.000e-01 \n#>                bb       tuning.psi1       tuning.psi2       tuning.psi3 \n#>         5.000e-01        -5.000e-01         1.500e+00         9.500e-01 \n#>       tuning.psi4        refine.tol           rel.tol         scale.tol \n#>                NA         1.000e-07         1.000e-07         1.000e-10 \n#>         solve.tol          zero.tol       eps.outlier             eps.x \n#>         1.000e-07         1.000e-10         1.000e-03         3.223e-11 \n#> warn.limit.reject warn.limit.meanrw \n#>         5.000e-01         5.000e-01 \n#>      nResample         max.it       best.r.s       k.fast.s          k.max \n#>           1000            500             20              2           2000 \n#>    maxit.scale      trace.lev            mts     compute.rd      numpoints \n#>            200              0           1000              0             10 \n#> fast.s.large.n \n#>           2000 \n#>               setting                   psi           subsampling \n#>              \"KS2014\"                 \"lqq\"         \"nonsingular\" \n#>                   cov compute.outlier.stats \n#>             \".vcov.w\"                \"SMDM\" \n#> seed : int(0)\n# Visualizing results\ndata <- data %>%\n    mutate(\n        ols_fit = predict(ols_model, newdata = data),\n        m_fit = predict(m_model, newdata = data),\n        lts_fit = fitted(lts_model),\n        # Use `fitted()` for ltsReg objects\n        mm_fit = predict(mm_model, newdata = data)\n    )\n\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_line(\n        aes(y = ols_fit),\n        color = \"red\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"OLS\"\n    ) +\n    geom_line(\n        aes(y = m_fit),\n        color = \"blue\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"$M$-Estimator\"\n    ) +\n    geom_line(\n        aes(y = lts_fit),\n        color = \"green\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"LTS\"\n    ) +\n    geom_line(\n        aes(y = mm_fit),\n        color = \"purple\",\n        linetype = \"dashed\",\n        size = 1,\n        label = \"$MM$-Estimator\"\n    ) +\n    labs(title = \"Comparison of Regression Fits\",\n         x = \"Predictor (x)\",\n         y = \"Response (y)\") +\n    theme_minimal()\n# Comparing Coefficients\ncomparison <- data.frame(\n    Method = c(\"OLS\", \"$M$-Estimator\", \"LTS\", \"$MM$-Estimator\"),\n    Intercept = c(\n        coef(ols_model)[1],\n        coef(m_model)[1],\n        lts_coefficients[1],\n        coef(mm_model)[1]\n    ),\n    Slope = c(\n        coef(ols_model)[2],\n        coef(m_model)[2],\n        lts_coefficients[2],\n        coef(mm_model)[2]\n    )\n)\n\nprint(comparison)\n#>           Method Intercept     Slope\n#> 1            OLS  8.834553 0.9720994\n#> 2  $M$-Estimator  4.322869 1.7250441\n#> 3            LTS  2.954960 1.9777635\n#> 4 $MM$-Estimator  3.021923 1.9667208"},{"path":"linear-regression.html","id":"partial-least-squares","chapter":"5 Linear Regression","heading":"5.6 Partial Least Squares","text":"Partial Least Squares (PLS) dimensionality reduction technique used regression predictive modeling. particularly useful predictors highly collinear number predictors (\\(p\\)) exceeds number observations (\\(n\\)). Unlike methods Principal Component Regression (PCR), PLS simultaneously considers relationship predictors response variable.","code":""},{"path":"linear-regression.html","id":"motivation-for-pls","chapter":"5 Linear Regression","heading":"5.6.1 Motivation for PLS","text":"Limitations Classical MethodsMulticollinearity:\nOLS fails predictors highly correlated design matrix \\(X'X\\) becomes nearly singular, leading unstable estimates.\nOLS fails predictors highly correlated design matrix \\(X'X\\) becomes nearly singular, leading unstable estimates.High-Dimensional Data:\n\\(p > n\\), OLS directly applied \\(X'X\\) invertible.\n\\(p > n\\), OLS directly applied \\(X'X\\) invertible.Principal Component Regression (PCR):\nPCR addresses multicollinearity using principal components \\(X\\), account relationship predictors response variable \\(y\\) constructing components.\nPCR addresses multicollinearity using principal components \\(X\\), account relationship predictors response variable \\(y\\) constructing components.PLS overcomes limitations constructing components maximize covariance predictors \\(X\\) response \\(y\\). finds compromise explaining variance \\(X\\) predicting \\(y\\), making particularly suited regression high-dimensional collinear datasets.Let:\\(X\\) \\(n \\times p\\) matrix predictors,\\(X\\) \\(n \\times p\\) matrix predictors,\\(y\\) \\(n \\times 1\\) response vector,\\(y\\) \\(n \\times 1\\) response vector,\\(t_k\\) \\(k\\)-th latent component derived \\(X\\),\\(t_k\\) \\(k\\)-th latent component derived \\(X\\),\\(p_k\\) \\(q_k\\) loadings \\(X\\) \\(y\\), respectively.\\(p_k\\) \\(q_k\\) loadings \\(X\\) \\(y\\), respectively.PLS aims construct latent components \\(t_1, t_2, \\ldots, t_K\\) :\\(t_k\\) linear combination predictors: \\(t_k = X w_k\\), \\(w_k\\) weight vector. 2The covariance \\(t_k\\) \\(y\\) maximized: \\[\n   \\text{Maximize } Cov(t_k, y) = w_k' X' y.\n   \\]","code":""},{"path":"linear-regression.html","id":"steps-to-construct-pls-components","chapter":"5 Linear Regression","heading":"5.6.2 Steps to Construct PLS Components","text":"Compute Weights:\nweights \\(w_k\\) \\(k\\)-th component obtained solving: \\[\nw_k = \\frac{X'y}{\\|X'y\\|}.\n\\]\nweights \\(w_k\\) \\(k\\)-th component obtained solving: \\[\nw_k = \\frac{X'y}{\\|X'y\\|}.\n\\]Construct Latent Component:\nForm \\(k\\)-th latent component: \\[\nt_k = X w_k.\n\\]\nForm \\(k\\)-th latent component: \\[\nt_k = X w_k.\n\\]Deflate Predictors:\nextracting \\(t_k\\), predictors deflated remove information explained \\(t_k\\): \\[\nX \\leftarrow X - t_k p_k',\n\\] \\(p_k = \\frac{X't_k}{t_k't_k}\\) loadings \\(X\\).\nextracting \\(t_k\\), predictors deflated remove information explained \\(t_k\\): \\[\nX \\leftarrow X - t_k p_k',\n\\] \\(p_k = \\frac{X't_k}{t_k't_k}\\) loadings \\(X\\).Deflate Response:\nSimilarly, deflate \\(y\\) remove variance explained \\(t_k\\): \\[\ny \\leftarrow y - t_k q_k,\n\\] \\(q_k = \\frac{t_k'y}{t_k't_k}\\).\nSimilarly, deflate \\(y\\) remove variance explained \\(t_k\\): \\[\ny \\leftarrow y - t_k q_k,\n\\] \\(q_k = \\frac{t_k'y}{t_k't_k}\\).Repeat Components:\nRepeat steps \\(K\\) components extracted.\nRepeat steps \\(K\\) components extracted.constructing \\(K\\) components, response \\(y\\) modeled :\\[\ny = T C + \\epsilon,\n\\]:\\(T = [t_1, t_2, \\ldots, t_K]\\) matrix latent components,\\(T = [t_1, t_2, \\ldots, t_K]\\) matrix latent components,\\(C\\) vector regression coefficients.\\(C\\) vector regression coefficients.estimated coefficients original predictors :\\[\n\\hat{\\beta} = W (P' W)^{-1} C,\n\\]\\(W = [w_1, w_2, \\ldots, w_K]\\) \\(P = [p_1, p_2, \\ldots, p_K]\\).","code":""},{"path":"linear-regression.html","id":"properties-of-pls","chapter":"5 Linear Regression","heading":"5.6.3 Properties of PLS","text":"Dimensionality Reduction:\nPLS reduces \\(X\\) \\(K\\) components, \\(K \\leq \\min(n, p)\\).\nPLS reduces \\(X\\) \\(K\\) components, \\(K \\leq \\min(n, p)\\).Handles Multicollinearity:\nconstructing uncorrelated components, PLS avoids instability caused multicollinearity OLS.\nconstructing uncorrelated components, PLS avoids instability caused multicollinearity OLS.Supervised Dimensionality Reduction:\nUnlike PCR, PLS considers relationship \\(X\\) \\(y\\) constructing components.\nUnlike PCR, PLS considers relationship \\(X\\) \\(y\\) constructing components.Efficiency:\nPLS requires fewer components PCR achieve similar level predictive accuracy.\nPLS requires fewer components PCR achieve similar level predictive accuracy.Practical ConsiderationsNumber Components:\noptimal number components \\(K\\) can determined using cross-validation.\noptimal number components \\(K\\) can determined using cross-validation.Preprocessing:\nStandardizing predictors essential PLS, ensures variables scale.\nStandardizing predictors essential PLS, ensures variables scale.Comparison Methods:\nPLS outperforms OLS PCR cases multicollinearity \\(p > n\\), may less interpretable sparse methods like Lasso.\nPLS outperforms OLS PCR cases multicollinearity \\(p > n\\), may less interpretable sparse methods like Lasso.loadings provide contribution predictor PLS components. Higher absolute values indicate stronger contributions corresponding component.Summary Model:\nproportion variance explained indicates much variability predictors response captured PLS component.\ngoal retain enough components explain variance avoiding overfitting.\nSummary Model:proportion variance explained indicates much variability predictors response captured PLS component.proportion variance explained indicates much variability predictors response captured PLS component.goal retain enough components explain variance avoiding overfitting.goal retain enough components explain variance avoiding overfitting.Validation Plot:\nMean Squared Error Prediction (MSEP) curve used select optimal number components.\nAdding many components can lead overfitting, may underfit data.\nValidation Plot:Mean Squared Error Prediction (MSEP) curve used select optimal number components.Mean Squared Error Prediction (MSEP) curve used select optimal number components.Adding many components can lead overfitting, may underfit data.Adding many components can lead overfitting, may underfit data.Coefficients:\nextracted coefficients weights applied predictors final PLS model.\ncoefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.\nCoefficients:extracted coefficients weights applied predictors final PLS model.extracted coefficients weights applied predictors final PLS model.coefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.coefficients derived PLS components may differ OLS regression coefficients due dimensionality reduction.Actual vs Predicted Plot:\nvisualization evaluates well PLS model predicts response variable.\nPoints tightly clustered around diagonal indicate good performance.\nActual vs Predicted Plot:visualization evaluates well PLS model predicts response variable.visualization evaluates well PLS model predicts response variable.Points tightly clustered around diagonal indicate good performance.Points tightly clustered around diagonal indicate good performance.VIP Scores:\nVIP scores help identify important predictors PLS model.\nPredictors higher VIP scores contribute explaining response variable.\nVIP Scores:VIP scores help identify important predictors PLS model.VIP scores help identify important predictors PLS model.Predictors higher VIP scores contribute explaining response variable.Predictors higher VIP scores contribute explaining response variable.","code":"\n# Load required library\nlibrary(pls)\n\n# Step 1: Simulate data\nset.seed(123)  # Ensure reproducibility\nn <- 100       # Number of observations\np <- 10        # Number of predictors\nX <- matrix(rnorm(n * p), nrow = n, ncol = p)  # Design matrix (predictors)\nbeta <- runif(p)                               # True coefficients\ny <- X %*% beta + rnorm(n)                     # Response variable with noise\n\n# Step 2: Fit Partial Least Squares (PLS) Regression\npls_fit <- plsr(y ~ X, ncomp = 5, validation = \"CV\")\n\n# Step 3: Summarize the PLS Model\nsummary(pls_fit)\n#> Data:    X dimension: 100 10 \n#>  Y dimension: 100 1\n#> Fit method: kernelpls\n#> Number of components considered: 5\n#> \n#> VALIDATION: RMSEP\n#> Cross-validated using 10 random segments.\n#>        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps\n#> CV           1.339    1.123    1.086    1.090    1.088    1.087\n#> adjCV        1.339    1.112    1.078    1.082    1.080    1.080\n#> \n#> TRAINING: % variance explained\n#>    1 comps  2 comps  3 comps  4 comps  5 comps\n#> X    10.88    20.06    30.80    42.19    51.61\n#> y    44.80    48.44    48.76    48.78    48.78\n\n# Step 4: Perform Cross-Validation and Select Optimal Components\nvalidationplot(pls_fit, val.type = \"MSEP\")\n\n# Step 5: Extract Coefficients for Predictors\npls_coefficients <- coef(pls_fit)\nprint(pls_coefficients)\n#> , , 5 comps\n#> \n#>               y\n#> X1   0.30192935\n#> X2  -0.03161151\n#> X3   0.22392538\n#> X4   0.42315637\n#> X5   0.33000198\n#> X6   0.66228763\n#> X7   0.40452691\n#> X8  -0.05704037\n#> X9  -0.02699757\n#> X10  0.05944765\n\n# Step 6: Evaluate Model Performance\npredicted_y <- predict(pls_fit, X)\nactual_vs_predicted <- data.frame(\n  Actual = y,\n  Predicted = predicted_y[, , 5]  # Predicted values using 5 components\n)\n\n# Plot Actual vs Predicted\nlibrary(ggplot2)\nggplot(actual_vs_predicted, aes(x = Actual, y = Predicted)) +\n    geom_point() +\n    geom_abline(\n        intercept = 0,\n        slope = 1,\n        color = \"red\",\n        linetype = \"dashed\"\n    ) +\n    labs(title = \"Actual vs Predicted Values (PLS Regression)\",\n         x = \"Actual Values\",\n         y = \"Predicted Values\") +\n    theme_minimal()\n\n# Step 7: Extract and Interpret Variable Importance (Loadings)\nloadings_matrix <- as.matrix(unclass(loadings(pls_fit)))\nvariable_importance <- as.data.frame(loadings_matrix)\ncolnames(variable_importance) <-\n    paste0(\"Component_\", 1:ncol(variable_importance))\nrownames(variable_importance) <-\n    paste0(\"X\", 1:nrow(variable_importance))\n\n# Print variable importance\nprint(variable_importance)\n#>     Component_1 Component_2 Component_3 Component_4 Component_5\n#> X1  -0.04991097   0.5774569  0.24349681 -0.41550345 -0.02098351\n#> X2   0.08913192  -0.1139342 -0.17582957 -0.05709948 -0.06707863\n#> X3   0.13773357   0.1633338  0.07622919 -0.07248620 -0.61962875\n#> X4   0.40369572  -0.2730457  0.69994206 -0.07949013  0.35239113\n#> X5   0.50562681  -0.1788131 -0.27936562  0.36197480 -0.41919645\n#> X6   0.57044281   0.3358522 -0.38683260  0.17656349  0.31154275\n#> X7   0.36258623   0.1202109 -0.01753715 -0.12980483 -0.06919411\n#> X8   0.12975452  -0.1164935 -0.30479310 -0.65654861  0.49948167\n#> X9  -0.29521786   0.6170234 -0.32082508 -0.01041860  0.04904396\n#> X10  0.23930055  -0.3259554  0.20006888 -0.53547258 -0.17963372"},{"path":"linear-regression.html","id":"comparison-with-related-methods","chapter":"5 Linear Regression","heading":"5.6.4 Comparison with Related Methods","text":"","code":""},{"path":"non-linear-regression.html","id":"non-linear-regression","chapter":"6 Non-Linear Regression","heading":"6 Non-Linear Regression","text":"Non-linear regression models differ fundamentally linear regression models derivatives mean function respect parameters depend one parameters. dependence adds complexity also provides greater flexibility model intricate relationships.Linear Regression:Model Form Example: typical linear regression model looks like \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) \\(\\beta_1\\) parameters.Model Form Example: typical linear regression model looks like \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) \\(\\beta_1\\) parameters.Parameter Effect: influence parameter \\(y\\) constant. example, \\(\\beta_1\\) increases 1, change \\(y\\) always \\(x\\), regardless current value \\(\\beta_1\\).Parameter Effect: influence parameter \\(y\\) constant. example, \\(\\beta_1\\) increases 1, change \\(y\\) always \\(x\\), regardless current value \\(\\beta_1\\).Derivatives: partial derivatives \\(y\\) respect parameter (e.g., \\(\\frac{\\partial y}{\\partial \\beta_1} = x\\)) depend parameters \\(\\beta_0\\) \\(\\beta_1\\) —depend data \\(x\\). makes mathematics finding best-fit line straightforward.Derivatives: partial derivatives \\(y\\) respect parameter (e.g., \\(\\frac{\\partial y}{\\partial \\beta_1} = x\\)) depend parameters \\(\\beta_0\\) \\(\\beta_1\\) —depend data \\(x\\). makes mathematics finding best-fit line straightforward.Straightforward estimation via closed-form solutions like Ordinary Least Squares.Straightforward estimation via closed-form solutions like Ordinary Least Squares.Non-linear Regression:Model Form Example: Consider \\(y = \\alpha \\cdot e^{\\beta x}\\). , \\(\\alpha\\) \\(\\beta\\) parameters, relationship straight line.Model Form Example: Consider \\(y = \\alpha \\cdot e^{\\beta x}\\). , \\(\\alpha\\) \\(\\beta\\) parameters, relationship straight line.Parameter Effect: effect changing \\(\\alpha\\) \\(\\beta\\) \\(y\\) constant. instance, change \\(\\beta\\), impact \\(y\\) depends \\(x\\) current value \\(\\beta\\). makes predictions adjustments complex.Parameter Effect: effect changing \\(\\alpha\\) \\(\\beta\\) \\(y\\) constant. instance, change \\(\\beta\\), impact \\(y\\) depends \\(x\\) current value \\(\\beta\\). makes predictions adjustments complex.Derivatives: Taking partial derivative respect \\(\\beta\\) gives \\(\\frac{\\partial y}{\\partial \\beta} = \\alpha x e^{\\beta x}\\). Notice derivative depends \\(\\alpha\\), \\(\\beta\\), \\(x\\). Unlike linear regression, sensitivity \\(y\\) changes \\(\\beta\\) changes \\(\\beta\\) changes.Derivatives: Taking partial derivative respect \\(\\beta\\) gives \\(\\frac{\\partial y}{\\partial \\beta} = \\alpha x e^{\\beta x}\\). Notice derivative depends \\(\\alpha\\), \\(\\beta\\), \\(x\\). Unlike linear regression, sensitivity \\(y\\) changes \\(\\beta\\) changes \\(\\beta\\) changes.Estimation requires iterative algorithms like Gauss-Newton Algorithm, closed-form solutions feasible.Estimation requires iterative algorithms like Gauss-Newton Algorithm, closed-form solutions feasible.Summary Table: Linear vs. Non-Linear RegressionKey Features Non-linear regression:Complex Functional Forms: Non-linear regression allows relationships straight lines planes.Interpretability Challenges: Non-linear models can difficult interpret, especially functional forms complex.Practical Use Cases:\nGrowth curves\nHigh-order polynomials\nLinear approximations (e.g., Taylor expansions)\nCollections locally linear models basis functions (e.g., splines)\nGrowth curvesHigh-order polynomialsLinear approximations (e.g., Taylor expansions)Collections locally linear models basis functions (e.g., splines)approaches can approximate data, may suffer interpretability issues may generalize well data sparse. Hence, intrinsically non-linear models often preferred.Intrinsically Non-Linear ModelsThe general form intrinsically non-linear regression model :\\[\nY_i = f(\\mathbf{x}_i; \\mathbf{\\theta}) + \\epsilon_i\n\\]:\\(f(\\mathbf{x}_i; \\mathbf{\\theta})\\): non-linear function relates \\(E(Y_i)\\) independent variables \\(\\mathbf{x}_i\\).\\(\\mathbf{x}_i\\): \\(k \\times 1\\) vector independent variables (fixed).\\(\\mathbf{\\theta}\\): \\(p \\times 1\\) vector parameters.\\(\\epsilon_i\\): Independent identically distributed random errors, often assumed mean 0 constant variance \\(\\sigma^2\\). cases, \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).Example: Exponential Growth ModelA common non-linear model exponential growth function:\\[\ny = \\theta_1 e^{\\theta_2 x} + \\epsilon\n\\]:\\(\\theta_1\\): Initial value.\\(\\theta_2\\): Growth rate.\\(x\\): Independent variable (e.g., time).\\(\\epsilon\\): Random error.","code":""},{"path":"non-linear-regression.html","id":"inference","chapter":"6 Non-Linear Regression","heading":"6.1 Inference","text":"Since \\(Y_i = f(\\mathbf{x}_i, \\theta) + \\epsilon_i\\), \\(\\epsilon_i \\sim \\text{iid}(0, \\sigma^2)\\), can estimate parameters (\\(\\hat{\\theta}\\)) minimizing sum squared errors:\\[\n\\sum_{=1}^{n} \\big(Y_i - f(\\mathbf{x}_i, \\theta)\\big)^2\n\\]variance residuals estimated :\\[\ns^2 = \\hat{\\sigma}^2_{\\epsilon} = \\frac{\\sum_{=1}^{n} \\big(Y_i - f(\\mathbf{x}_i, \\hat{\\theta})\\big)^2}{n - p}\n\\]\\(p\\) number parameters \\(\\mathbf{\\theta}\\), \\(n\\) number observations.","code":""},{"path":"non-linear-regression.html","id":"linear-function-of-the-parameters","chapter":"6 Non-Linear Regression","heading":"6.1.1 Linear Function of the Parameters","text":"assume \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), asymptotic distribution parameter estimates \\(\\hat{\\theta}\\) can expressed :\\[\n\\hat{\\theta} \\sim (\\mathbf{\\theta}, \\sigma^2[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1})\n\\]:\\(\\) stands asymptotic normality.\\(\\mathbf{F}(\\theta)\\) Jacobian matrix partial derivatives \\(f(\\mathbf{x}_i, \\theta)\\) respect \\(\\mathbf{\\theta}\\), evaluated \\(\\hat{\\theta}\\).Asymptotic normality means sample size \\(n\\) becomes large, sampling distribution \\(\\hat{\\theta}\\) approaches normal distribution, enables inference parameters.Suppose interested linear combination parameters, \\(\\theta_1 - \\theta_2\\). Define contrast vector \\(\\mathbf{}\\) :\\[\n\\mathbf{} = (0, 1, -1)'\n\\]consider inference \\(\\mathbf{'\\theta}\\). Using rules expectation variance linear combination random vector \\(\\mathbf{Z}\\):\\[\n\\begin{aligned}\nE(\\mathbf{'Z}) &= \\mathbf{'}E(\\mathbf{Z}) \\\\\n\\text{Var}(\\mathbf{'Z}) &= \\mathbf{'} \\text{Var}(\\mathbf{Z}) \\mathbf{}\n\\end{aligned}\n\\]Applying \\(\\mathbf{'\\hat{\\theta}}\\), :\\[\n\\mathbf{'\\hat{\\theta}} \\sim \\big(\\mathbf{'\\theta}, \\sigma^2 \\mathbf{'[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1}}\\big)\n\\]indicates \\(\\mathbf{'\\hat{\\theta}}\\) asymptotically independent \\(s^2\\) (order \\(1/n\\)).Using \\(t\\)-distribution, \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mathbf{'\\theta}\\) given :\\[\n\\mathbf{'\\theta} \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{'[\\mathbf{F}(\\hat{\\theta})'\\mathbf{F}(\\hat{\\theta})]^{-1}}}\n\\]:\\(t_{(1-\\alpha/2, n-p)}\\) critical value \\(t\\)-distribution \\(n - p\\) degrees freedom.\\(s\\) estimated standard deviation residuals.focus single parameter \\(\\theta_j\\), let \\(\\mathbf{'} = (0, \\dots, 1, \\dots, 0)\\) (1 \\(j\\)-th position). , confidence interval \\(\\theta_j\\) becomes:\\[\n\\hat{\\theta}_j \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\hat{c}^j}\n\\]\\(\\hat{c}^j\\) \\(j\\)-th diagonal element \\([\\mathbf{F}(\\hat{\\theta})'\\mathbf{F}(\\hat{\\theta})]^{-1}\\).","code":""},{"path":"non-linear-regression.html","id":"nonlinear-functions-of-parameters","chapter":"6 Non-Linear Regression","heading":"6.1.2 Nonlinear Functions of Parameters","text":"\\(h(\\theta)\\) nonlinear function parameters, can use Taylor series expansion \\(\\theta\\) approximate \\(h(\\hat{\\theta})\\):\\[\nh(\\hat{\\theta}) \\approx h(\\theta) + \\mathbf{h}' [\\hat{\\theta} - \\theta]\n\\]:\\(\\mathbf{h} = \\left( \\frac{\\partial h}{\\partial \\theta_1}, \\frac{\\partial h}{\\partial \\theta_2}, \\dots, \\frac{\\partial h}{\\partial \\theta_p} \\right)'\\) gradient \\(h(\\theta)\\).Key Approximations:Expectation Variance \\(\\hat{\\theta}\\): \\[\n\\begin{aligned}\nE(\\hat{\\theta}) &\\approx \\theta, \\\\\n\\text{Var}(\\hat{\\theta}) &\\approx \\sigma^2 [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1}.\n\\end{aligned}\n\\]Expectation Variance \\(\\hat{\\theta}\\): \\[\n\\begin{aligned}\nE(\\hat{\\theta}) &\\approx \\theta, \\\\\n\\text{Var}(\\hat{\\theta}) &\\approx \\sigma^2 [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1}.\n\\end{aligned}\n\\]Expectation Variance \\(h(\\hat{\\theta})\\): \\[\n\\begin{aligned}\nE(h(\\hat{\\theta})) &\\approx h(\\theta), \\\\\n\\text{Var}(h(\\hat{\\theta})) &\\approx \\sigma^2 \\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}.\n\\end{aligned}\n\\]Expectation Variance \\(h(\\hat{\\theta})\\): \\[\n\\begin{aligned}\nE(h(\\hat{\\theta})) &\\approx h(\\theta), \\\\\n\\text{Var}(h(\\hat{\\theta})) &\\approx \\sigma^2 \\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}.\n\\end{aligned}\n\\]Approximate Distribution:Combining results, find:\\[\nh(\\hat{\\theta}) \\sim (h(\\theta), \\sigma^2 \\mathbf{h}' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}),\n\\]\\(\\) represents asymptotic normality.Confidence Interval \\(h(\\theta)\\):approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(h(\\theta)\\) :\\[\nh(\\hat{\\theta}) \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{h}'[\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{h}},\n\\]\\(\\mathbf{h}\\) \\(\\mathbf{F}(\\theta)\\) evaluated \\(\\hat{\\theta}\\).compute prediction interval new observation \\(Y_0\\) \\(x = x_0\\):Model Definition: \\[\nY_0 = f(x_0; \\theta) + \\epsilon_0, \\quad \\epsilon_0 \\sim N(0, \\sigma^2),\n\\] predicted value: \\[\n\\hat{Y}_0 = f(x_0, \\hat{\\theta}).\n\\]Model Definition: \\[\nY_0 = f(x_0; \\theta) + \\epsilon_0, \\quad \\epsilon_0 \\sim N(0, \\sigma^2),\n\\] predicted value: \\[\n\\hat{Y}_0 = f(x_0, \\hat{\\theta}).\n\\]Approximation \\(\\hat{Y}_0\\): \\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), : \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0, \\theta) + \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta],\n\\] : \\[\n\\mathbf{f}_0(\\theta) = \\left( \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_1}, \\dots, \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_p} \\right)'.\n\\]Approximation \\(\\hat{Y}_0\\): \\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), : \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0, \\theta) + \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta],\n\\] : \\[\n\\mathbf{f}_0(\\theta) = \\left( \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_1}, \\dots, \\frac{\\partial f(x_0, \\theta)}{\\partial \\theta_p} \\right)'.\n\\]Error Approximation: \\[\n\\begin{aligned}Y_0 - \\hat{Y}_0 &\\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\&= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\\end{aligned}\n\\]Error Approximation: \\[\n\\begin{aligned}Y_0 - \\hat{Y}_0 &\\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\&= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\\end{aligned}\n\\]Variance \\(Y_0 - \\hat{Y}_0\\): \\[\n\\begin{aligned}\n\\text{Var}(Y_0 - \\hat{Y}_0) &\\approx \\text{Var}(\\epsilon_0 - \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta]) \\\\\n&= \\sigma^2 + \\sigma^2 \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta) \\\\\n&= \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big).\n\\end{aligned}\n\\]Variance \\(Y_0 - \\hat{Y}_0\\): \\[\n\\begin{aligned}\n\\text{Var}(Y_0 - \\hat{Y}_0) &\\approx \\text{Var}(\\epsilon_0 - \\mathbf{f}_0(\\theta)' [\\hat{\\theta} - \\theta]) \\\\\n&= \\sigma^2 + \\sigma^2 \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta) \\\\\n&= \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big).\n\\end{aligned}\n\\]Approximate Distribution Prediction Error:error \\(Y_0 - \\hat{Y}_0\\) follows asymptotic normal distribution:\\[\nY_0 - \\hat{Y}_0 \\sim \\big(0, \\sigma^2 \\big(1 + \\mathbf{f}_0(\\theta)' [\\mathbf{F}(\\theta)' \\mathbf{F}(\\theta)]^{-1} \\mathbf{f}_0(\\theta)\\big)\\big).\n\\]Prediction Interval \\(Y_0\\):\\(100(1-\\alpha)\\%\\) prediction interval \\(Y_0\\) :\\[\n\\hat{Y}_0 \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{1 + \\mathbf{f}_0(\\hat{\\theta})' [\\mathbf{F}(\\hat{\\theta})' \\mathbf{F}(\\hat{\\theta})]^{-1} \\mathbf{f}_0(\\hat{\\theta})}.\n\\]confidence interval mean response \\(E(Y_i)\\) (different prediction interval) can obtained similarly, without including variance \\(\\epsilon_0\\) calculation. Specifically, mean response:\\[\nE(Y_0) \\approx f(x_0; \\theta),\n\\]confidence interval :\\[\nf(x_0, \\hat{\\theta}) \\pm t_{(1-\\alpha/2, n-p)} s \\sqrt{\\mathbf{f}_0(\\hat{\\theta})' [\\mathbf{F}(\\hat{\\theta})' \\mathbf{F}(\\hat{\\theta})]^{-1} \\mathbf{f}_0(\\hat{\\theta})}.\n\\]","code":""},{"path":"non-linear-regression.html","id":"non-linear-least-squares-estimation","chapter":"6 Non-Linear Regression","heading":"6.2 Non-linear Least Squares Estimation","text":"least squares (LS) estimate \\(\\theta\\), denoted \\(\\hat{\\theta}\\), minimizes residual sum squares:\\[\nS(\\hat{\\theta}) = SSE(\\hat{\\theta}) = \\sum_{=1}^{n} \\{Y_i - f(\\mathbf{x}_i; \\hat{\\theta})\\}^2\n\\]solve , consider partial derivatives \\(S(\\theta)\\) respect \\(\\theta_j\\) set zero, leading normal equations:\\[\n\\frac{\\partial S(\\theta)}{\\partial \\theta_j} = -2 \\sum_{=1}^{n} \\{Y_i - f(\\mathbf{x}_i; \\theta)\\} \\frac{\\partial f(\\mathbf{x}_i; \\theta)}{\\partial \\theta_j} = 0\n\\]However, equations inherently non-linear , cases, solved analytically. result, various estimation techniques employed approximate solutions efficiently. approaches include:Iterative Optimization – Methods refine estimates successive iterations minimize error.Derivative-Free Methods – Techniques rely gradient information, useful complex non-smooth functions.Stochastic Heuristic – Algorithms incorporate randomness, genetic algorithms simulated annealing, explore solution spaces.Linearization– Approximating non-linear models linear ones enable analytical numerical solutions.Hybrid Approaches – Combining multiple methods leverage respective strengths improved estimation.","code":""},{"path":"non-linear-regression.html","id":"iterative-optimization-nonlinear-regression","chapter":"6 Non-Linear Regression","heading":"6.2.1 Iterative Optimization","text":"","code":""},{"path":"non-linear-regression.html","id":"gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.1 Gauss-Newton Algorithm","text":"Gauss-Newton Algorithm iterative optimization method used estimate parameters nonlinear least squares problems. refines parameter estimates approximating Hessian matrix using first-order derivatives, making computationally efficient many practical applications (e.g., regression models finance marketing analytics). objective minimize Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} [Y_i - f_i(\\theta)]^2,\n\\]\\(\\mathbf{Y} = [Y_1, \\dots, Y_n]'\\) observed responses, \\(f_i(\\theta)\\) model-predicted values.Iterative Refinement via Taylor ExpansionThe Gauss-Newton algorithm iteratively refines initial estimate \\(\\hat{\\theta}^{(0)}\\) using Taylor series expansion \\(f(\\mathbf{x}_i; \\theta)\\) \\(\\hat{\\theta}^{(0)}\\). start observation model:\\[\nY_i = f(\\mathbf{x}_i; \\theta) + \\epsilon_i.\n\\]expanding \\(f(\\mathbf{x}_i; \\theta)\\) around \\(\\hat{\\theta}^{(0)}\\) ignoring higher-order terms (assuming remainder small), get:\\[\n\\begin{aligned}\nY_i &\\approx f(\\mathbf{x}_i; \\hat{\\theta}^{(0)})\n+ \\sum_{j=1}^{p} \\frac{\\partial f(\\mathbf{x}_i; \\theta)}{\\partial \\theta_j} \\bigg|_{\\theta = \\hat{\\theta}^{(0)}}\n\\bigl(\\theta_j - \\hat{\\theta}_j^{(0)}\\bigr)\n+ \\epsilon_i.\n\\end{aligned}\n\\]matrix form, let\\[\n\\mathbf{Y} =\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix},\n\\quad\n\\mathbf{f}(\\hat{\\theta}^{(0)}) =\n\\begin{bmatrix}\nf(\\mathbf{x}_1, \\hat{\\theta}^{(0)}) \\\\ \\vdots \\\\ f(\\mathbf{x}_n, \\hat{\\theta}^{(0)})\n\\end{bmatrix},\n\\]define Jacobian matrix partial derivatives\\[\n\\mathbf{F}(\\hat{\\theta}^{(0)}) =\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{x}_1, \\theta)}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_1, \\theta)}{\\partial \\theta_p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{x}_n, \\theta)}{\\partial \\theta_1} & \\cdots & \\frac{\\partial f(\\mathbf{x}_n, \\theta)}{\\partial \\theta_p}\n\\end{bmatrix}_{\\theta = \\hat{\\theta}^{(0)}}.\n\\],\\[\n\\mathbf{Y} \\approx \\mathbf{f}(\\hat{\\theta}^{(0)})\n+ \\mathbf{F}(\\hat{\\theta}^{(0)})\\,(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon,\n\\]\\(\\epsilon = [\\epsilon_1, \\dots, \\epsilon_n]'\\) assumed ..d. mean \\(0\\) variance \\(\\sigma^2\\).linear approximation,\\[\n\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(0)})\n\\approx \\mathbf{F}(\\hat{\\theta}^{(0)})\\,(\\theta - \\hat{\\theta}^{(0)}).\n\\]Solving \\(\\theta - \\hat{\\theta}^{(0)}\\) least squares sense gives Gauss increment \\(\\hat{\\delta}^{(1)}\\), can update:\\[\n\\hat{\\theta}^{(1)} = \\hat{\\theta}^{(0)} + \\hat{\\delta}^{(1)}.\n\\]Step--Step ProcedureInitialize: Start initial estimate \\(\\hat{\\theta}^{(0)}\\) set \\(j = 0\\).Compute Taylor Expansion: Calculate \\(\\mathbf{f}(\\hat{\\theta}^{(j)})\\) \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\).Solve Increment: Treating \\(\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(j)}) \\approx \\mathbf{F}(\\hat{\\theta}^{(j)})\\, (\\theta - \\hat{\\theta}^{(j)})\\) linear model, use Ordinary Least Squares compute \\(\\hat{\\delta}^{(j+1)}\\).Update Parameters: Set \\(\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)}\\).Check Convergence: convergence criteria met (see ), stop; otherwise, return Step 2.Estimate Variance: convergence, assume \\(\\epsilon \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{})\\). variance \\(\\sigma^2\\) can estimated \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\bigl(\\mathbf{Y} - \\mathbf{f}(\\mathbf{x}; \\hat{\\theta})\\bigr)' \\bigl(\\mathbf{Y} - \\mathbf{f}(\\mathbf{x}; \\hat{\\theta})\\bigr).\n\\]Convergence CriteriaCommon criteria deciding stop iterating include:Objective Function Change: \\[\n\\frac{\\bigl|SSE(\\hat{\\theta}^{(j+1)}) - SSE(\\hat{\\theta}^{(j)})\\bigr|}{SSE(\\hat{\\theta}^{(j)})} < \\gamma_1.\n\\]Parameter Change: \\[\n\\bigl|\\hat{\\theta}^{(j+1)} - \\hat{\\theta}^{(j)}\\bigr| < \\gamma_2.\n\\]Residual Projection Criterion: residuals satisfy convergence defined (Bates Watts 1981).Another way see update step viewing necessary condition minimum: gradient \\(SSE(\\theta)\\) respect \\(\\theta\\) zero. \\[\nSSE(\\theta) = \\sum_{=1}^{n} [Y_i - f_i(\\theta)]^2,\n\\]gradient \\[\n\\frac{\\partial SSE(\\theta)}{\\partial \\theta}\n= 2\\,\\mathbf{F}(\\theta)' \\bigl[\\mathbf{Y} - \\mathbf{f}(\\theta)\\bigr].\n\\]Using Gauss-Newton update rule iteration \\(j\\) \\(j+1\\):\\[\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)}\n&= \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)} \\\\\n&= \\hat{\\theta}^{(j)} + \\bigl[\\mathbf{F}(\\hat{\\theta}^{(j)})' \\,\\mathbf{F}(\\hat{\\theta}^{(j)})\\bigr]^{-1}\n\\,\\mathbf{F}(\\hat{\\theta}^{(j)})' \\bigl[\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(j)})\\bigr] \\\\\n&= \\hat{\\theta}^{(j)}\n- \\frac{1}{2} \\bigl[\\mathbf{F}(\\hat{\\theta}^{(j)})' \\,\\mathbf{F}(\\hat{\\theta}^{(j)})\\bigr]^{-1}\n\\, \\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\end{aligned}\n\\]:\\(\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector, pointing direction steepest ascent SSE.\\(\\bigl[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})\\bigr]^{-1}\\) determines step size, controlling far move direction improvement.factor \\(-\\tfrac{1}{2}\\) ensures movement direction steepest descent, helping minimize SSE.Gauss-Newton method works well nonlinear model can approximated accurately first-order Taylor expansion near solution. assumption near-linearity residual function \\(\\mathbf{r}(\\theta) = \\mathbf{Y} - \\mathbf{f}(\\theta)\\) violated, convergence may slow fail altogether. cases, robust methods like Levenberg-Marquardt Algorithm (modifies Gauss-Newton damping parameter) often preferred.defined model “nonlinear_model(theta, x)” returns Aexp(Bx).generated synthetic data using “true_theta” values added random noise.used nls.lm(...) minpack.lm package fit data:\npar = c(1, 0.1) initial parameter guess.\nfn = function(theta) y - nonlinear_model(theta, x) residual function, .e., observed minus predicted.\npar = c(1, 0.1) initial parameter guess.fn = function(theta) y - nonlinear_model(theta, x) residual function, .e., observed minus predicted.fit$par provides estimated parameters algorithm converges.","code":"\n# Load necessary libraries\nlibrary(minpack.lm)  # Provides nonlinear least squares functions\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    # theta is a vector of parameters: theta[1] = A, theta[2] = B\n    # x is the independent variable\n    # The model is A * exp(B * x)\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function for clarity\nsse <- function(theta, x, y) {\n    # SSE = sum of squared errors between actual y and model predictions\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)                     # for reproducibility\nx <- seq(0, 10, length.out = 100) # 100 points from 0 to 10\ntrue_theta <- c(2, 0.3)           # true parameter values\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Display the first few data points\nhead(data.frame(x, y))\n#>           x        y\n#> 1 0.0000000 1.719762\n#> 2 0.1010101 1.946445\n#> 3 0.2020202 2.904315\n#> 4 0.3030303 2.225593\n#> 5 0.4040404 2.322373\n#> 6 0.5050505 3.184724\n\n# Gauss-Newton optimization using nls.lm (Levenberg-Marquardt as extension).\n# Initial guess for theta: c(1, 0.1)\nfit <- nls.lm(\n    par = c(1, 0.1),\n    fn = function(theta)\n        y - nonlinear_model(theta, x)\n)\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B):\\n\")\n#> Estimated parameters (A, B):\nprint(fit$par)\n#> [1] 1.9934188 0.3008742\n# Visualize the data and the fitted model\nplot(\n  x,\n  y,\n  main = \"Data and Fitted Curve (Gauss-Newton/Levenberg-Marquardt)\",\n  xlab = \"x\",\n  ylab = \"y\",\n  pch = 19,\n  cex = 0.5\n)\ncurve(\n  nonlinear_model(fit$par, x),\n  from = 0,\n  to = 10,\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"modified-gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.2 Modified Gauss-Newton Algorithm","text":"Modified Gauss-Newton Algorithm introduces learning rate \\(\\alpha_j\\) control step size prevent overshooting local minimum. standard Gauss-Newton Algorithm assumes full step direction \\(\\hat{\\delta}^{(j+1)}\\) optimal, practice, especially highly nonlinear problems, can overstep minimum cause numerical instability. modification introduces step size reduction, making robust.redefine update step :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}, \\quad 0 < \\alpha_j < 1,\n\\]:\\(\\alpha_j\\) learning rate, controlling much step \\(\\hat{\\delta}^{(j+1)}\\) taken.\\(\\alpha_j = 1\\), recover standard Gauss-Newton method.\\(\\alpha_j\\) small, convergence slow; large, algorithm may diverge.learning rate \\(\\alpha_j\\) allows adaptive step size adjustments, helping prevent excessive parameter jumps ensuring SSE decreases iteration.common approach determine \\(\\alpha_j\\) step halving, ensuring iteration moves direction reduces SSE. Instead using fixed \\(\\alpha_j\\), iteratively reduce step size SSE decreases:\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\frac{1}{2^k}\\hat{\\delta}^{(j+1)},\n\\]:\\(k\\) smallest non-negative integer \\[\nSSE(\\hat{\\theta}^{(j)} + \\frac{1}{2^k} \\hat{\\delta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)}).\n\\]means start full step \\(\\hat{\\delta}^{(j+1)}\\), try \\(\\hat{\\delta}^{(j+1)}/2\\), \\(\\hat{\\delta}^{(j+1)}/4\\), , SSE reduced.Algorithm Step Halving:Compute Gauss-Newton step \\(\\hat{\\delta}^{(j+1)}\\).Set initial \\(\\alpha_j = 1\\).updated parameters \\(\\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}\\) increase SSE, divide \\(\\alpha_j\\) 2.Repeat SSE decreases.ensures monotonic SSE reduction, preventing divergence due overly aggressive step.Generalized Form Modified AlgorithmA general form update rule, incorporating step size control matrix \\(\\mathbf{}_j\\), :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_j \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\]:\\(\\mathbf{}_j\\) positive definite matrix preconditions update direction.\\(\\alpha_j\\) learning rate.\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient objective function \\(Q(\\theta)\\), typically SSE nonlinear regression.Connection Modified Gauss-Newton AlgorithmThe Modified Gauss-Newton Algorithm fits framework:\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} \\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}.\n\\], recognize:Objective function: \\(Q = SSE\\).Preconditioner matrix: \\([\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} = \\mathbf{}\\).Thus, standard Gauss-Newton method can interpreted special case broader optimization framework, preconditioned gradient descent approach.","code":"\n# Load required library\nlibrary(minpack.lm)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n  theta[1] * exp(theta[2] * x)\n}\n\n# Define the Sum of Squared Errors function\nsse <- function(theta, x, y) {\n  sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Gauss-Newton with Step Halving\ngauss_newton_modified <-\n  function(theta_init,\n           x,\n           y,\n           tol = 1e-6,\n           max_iter = 100) {\n    theta <- theta_init\n    for (j in 1:max_iter) {\n      # Compute Jacobian matrix numerically\n      epsilon <- 1e-6\n      F_matrix <-\n        matrix(0, nrow = length(y), ncol = length(theta))\n      for (p in 1:length(theta)) {\n        theta_perturb <- theta\n        theta_perturb[p] <- theta[p] + epsilon\n        F_matrix[, p] <-\n          (nonlinear_model(theta_perturb, x)-nonlinear_model(theta, x))/epsilon\n      }\n      \n      # Compute residuals\n      residuals <- y - nonlinear_model(theta, x)\n      \n      # Compute Gauss-Newton step\n      delta <-\n        solve(t(F_matrix) %*% F_matrix) %*% t(F_matrix) %*% residuals\n      \n      # Step Halving Implementation\n      alpha <- 1\n      k <- 0\n      while (sse(theta + alpha * delta, x, y) >= sse(theta, x, y) &&\n             k < 10) {\n        alpha <- alpha / 2\n        k <- k + 1\n      }\n      \n      # Update theta\n      theta_new <- theta + alpha * delta\n      \n      # Check for convergence\n      if (sum(abs(theta_new - theta)) < tol) {\n        break\n      }\n      \n      theta <- theta_new\n    }\n    return(theta)\n  }\n\n# Run Modified Gauss-Newton Algorithm\ntheta_init <- c(1, 0.1)  # Initial parameter guess\nestimated_theta <- gauss_newton_modified(theta_init, x, y)\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) with Modified Gauss-Newton:\\n\")\n#> Estimated parameters (A, B) with Modified Gauss-Newton:\nprint(estimated_theta)\n#>           [,1]\n#> [1,] 1.9934188\n#> [2,] 0.3008742\n\n# Plot data and fitted curve\nplot(\n  x,\n  y,\n  main = \"Modified Gauss-Newton: Data & Fitted Curve\",\n  pch = 19,\n  cex = 0.5,\n  xlab = \"x\",\n  ylab = \"y\"\n)\ncurve(\n  nonlinear_model(estimated_theta, x),\n  from = 0,\n  to = 10,\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"steepest-descent","chapter":"6 Non-Linear Regression","heading":"6.2.1.3 Steepest Descent (Gradient Descent)","text":"Steepest Descent Method, commonly known Gradient Descent, fundamental iterative optimization technique used finding parameter estimates minimize objective function \\(\\mathbf{Q}(\\theta)\\). special case Modified Gauss-Newton Algorithm, preconditioning matrix \\(\\mathbf{}_j\\) replaced identity matrix.update rule given :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_{p \\times p}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta},\n\\]:\\(\\alpha_j\\) learning rate, determining step size.\\(\\mathbf{}_{p \\times p}\\) identity matrix, meaning updates occur direction negative gradient.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector, provides direction steepest ascent; negation ensures movement toward minimum.Characteristics Steepest DescentSlow converge: algorithm moves direction gradient take account curvature, may result slow convergence, especially ill-conditioned problems.Moves rapidly initially: method can exhibit fast initial progress, approaches minimum, step sizes become small, leading slow convergence.Useful initialization: Due simplicity ease implementation, often used obtain starting values advanced methods like Newton’s method Gauss-Newton Algorithm.Comparison Gauss-NewtonThe key difference Steepest Descent considers gradient direction, Gauss-Newton Newton’s method incorporate curvature information.Choosing Learning Rate \\(\\alpha_j\\)well-chosen learning rate crucial success gradient descent:large: algorithm may overshoot minimum diverge.small: Convergence slow.Adaptive strategies:\nFixed step size: \\(\\alpha_j\\) constant.\nStep size decay: \\(\\alpha_j\\) decreases iterations (e.g., \\(\\alpha_j = \\frac{1}{j}\\)).\nLine search: Choose \\(\\alpha_j\\) minimizing \\(\\mathbf{Q}(\\theta^{(j+1)})\\) along gradient direction.\nFixed step size: \\(\\alpha_j\\) constant.Step size decay: \\(\\alpha_j\\) decreases iterations (e.g., \\(\\alpha_j = \\frac{1}{j}\\)).Line search: Choose \\(\\alpha_j\\) minimizing \\(\\mathbf{Q}(\\theta^{(j+1)})\\) along gradient direction.common approach backtracking line search, \\(\\alpha_j\\) reduced iteratively decrease \\(\\mathbf{Q}(\\theta)\\) observed.Steepest Descent (Gradient Descent) moves direction steepest descent, can lead zigzagging behavior.Slow convergence occurs curvature function varies significantly across dimensions.Learning rate tuning critical:\nlarge, algorithm diverges.\nsmall, progress slow.\nlarge, algorithm diverges.large, algorithm diverges.small, progress slow.small, progress slow.Useful initialization: often used get close optimal solution switching advanced methods like Gauss-Newton Algorithm Newton’s method.Several advanced techniques improve performance steepest descent:Momentum Gradient Descent: Adds momentum term smooth updates, reducing oscillations.Momentum Gradient Descent: Adds momentum term smooth updates, reducing oscillations.Adaptive Learning Rates:\nAdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.\nRMSprop: Uses moving average past gradients scale updates.\nAdam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.\nAdaptive Learning Rates:AdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.AdaGrad: Adjusts \\(\\alpha_j\\) per parameter based historical gradients.RMSprop: Uses moving average past gradients scale updates.RMSprop: Uses moving average past gradients scale updates.Adam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.Adam (Adaptive Moment Estimation): Combines momentum adaptive learning rates.practice, Adam widely used machine learning deep learning, Newton-based methods (including Gauss-Newton) preferred nonlinear regression.","code":"\n# Load necessary libraries\nlibrary(ggplot2)\n\n\n# Define the nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define the Sum of Squared Errors function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Define Gradient of SSE w.r.t theta (computed numerically)\ngradient_sse <- function(theta, x, y) {\n    n <- length(y)\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Safe Gradient Descent Implementation\ngradient_descent <-\n    function(theta_init,\n             x,\n             y,\n             alpha = 0.01,\n             tol = 1e-6,\n             max_iter = 500) {\n        theta <- theta_init\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            \n            # Check for NaN or Inf values in the gradient (prevents divergence)\n            if (any(is.na(grad)) || any(is.infinite(grad))) {\n                cat(\"Numerical instability detected at iteration\",\n                    j,\n                    \"\\n\")\n                break\n            }\n            \n            # Update step\n            theta_new <- theta - alpha * grad\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Check for convergence\n            if (!is.finite(sse_values[j])) {\n                cat(\"Divergence detected at iteration\", j, \"\\n\")\n                break\n            }\n            \n            if (sum(abs(theta_new - theta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Gradient Descent with a Safe Implementation\ntheta_init <- c(1, 0.1)  # Initial guess\nalpha <- 0.001           # Learning rate\nresult <- gradient_descent(theta_init, x, y, alpha)\n#> Divergence detected at iteration 1\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Gradient Descent:\\n\")\n#> Estimated parameters (A, B) using Gradient Descent:\nprint(estimated_theta)\n#> [1] 1.0 0.1\n\n# Plot convergence of SSE over iterations\n# Ensure sse_values has valid data\nsse_df <- data.frame(\n  Iteration = seq_along(sse_values),\n  SSE = sse_values\n)\n\n# Generate improved plot using ggplot()\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Gradient Descent Convergence\",\n    x = \"Iteration\",\n    y = \"SSE\"\n  ) +\n  theme_minimal()"},{"path":"non-linear-regression.html","id":"levenberg-marquardt","chapter":"6 Non-Linear Regression","heading":"6.2.1.4 Levenberg-Marquardt Algorithm","text":"Levenberg-Marquardt Algorithm widely used optimization method solving nonlinear least squares problems. adaptive technique blends Gauss-Newton Algorithm Steepest Descent (Gradient Descent), dynamically switching based problem conditions.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) + \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\tau\\) damping parameter, controlling whether step behaves like Gauss-Newton Algorithm Steepest Descent (Gradient Descent).\\(\\mathbf{}_{p \\times p}\\) identity matrix, ensuring numerical stability.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate, determining step size.Levenberg-Marquardt algorithm particularly useful Jacobian matrix \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) nearly singular, meaning Gauss-Newton Algorithm alone may fail.\\(\\tau\\) large, method behaves like Steepest Descent, ensuring stability.\\(\\tau\\) small, behaves like Gauss-Newton, accelerating convergence.Adaptive control \\(\\tau\\):\n\\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), reduce \\(\\tau\\): \\[\n\\tau \\gets \\tau / 10\n\\]\nOtherwise, increase \\(\\tau\\) stabilize: \\[\n\\tau \\gets 10\\tau\n\\]\n\\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), reduce \\(\\tau\\): \\[\n\\tau \\gets \\tau / 10\n\\]Otherwise, increase \\(\\tau\\) stabilize: \\[\n\\tau \\gets 10\\tau\n\\]adjustment ensures algorithm moves efficiently avoiding instability.Early Stability (Flat SSE)\nSSE remains near zero first iterations, suggests algorithm initially behaving stably.\nmight indicate initial parameter guess reasonable, updates small significantly affect SSE.\nEarly Stability (Flat SSE)SSE remains near zero first iterations, suggests algorithm initially behaving stably.SSE remains near zero first iterations, suggests algorithm initially behaving stably.might indicate initial parameter guess reasonable, updates small significantly affect SSE.might indicate initial parameter guess reasonable, updates small significantly affect SSE.Sudden Explosion SSE (Iteration ~8-9)\nsharp spike SSE iteration 9 indicates numerical instability divergence optimization process.\ndue :\nill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.\nsudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.\nFloating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.\n\nSudden Explosion SSE (Iteration ~8-9)sharp spike SSE iteration 9 indicates numerical instability divergence optimization process.sharp spike SSE iteration 9 indicates numerical instability divergence optimization process.due :\nill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.\nsudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.\nFloating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.\ndue :ill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.ill-conditioned Jacobian matrix: step direction poorly estimated, leading unstable jump.sudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.sudden large update (delta): damping parameter (tau) might reduced aggressively, causing uncontrolled step.Floating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.Floating-point issues: becomes nearly singular, solving \\ delta = residuals may produce excessively large values.Return Stability (Iteration 9)\nSSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.\nconsistent adaptive nature Levenberg-Marquardt:\nstep leads bad SSE increase, algorithm increases tau make next step conservative.\nnext step stabilizes, tau may reduced .\n\nReturn Stability (Iteration 9)SSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.SSE immediately returns low value spike, suggests damping parameter (tau) might increased detecting instability.consistent adaptive nature Levenberg-Marquardt:\nstep leads bad SSE increase, algorithm increases tau make next step conservative.\nnext step stabilizes, tau may reduced .\nconsistent adaptive nature Levenberg-Marquardt:step leads bad SSE increase, algorithm increases tau make next step conservative.step leads bad SSE increase, algorithm increases tau make next step conservative.next step stabilizes, tau may reduced .next step stabilizes, tau may reduced .","code":"\n# Load required libraries\nlibrary(minpack.lm)\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Robust Levenberg-Marquardt Optimization Implementation\nlevenberg_marquardt <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500,\n             tau_init = 1) {\n        theta <- theta_init\n        tau <- tau_init\n        lambda <- 1e-8  # Small regularization term\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            # Compute Jacobian matrix numerically\n            epsilon <- 1e-6\n            F_matrix <-\n                matrix(0, nrow = length(y), ncol = length(theta))\n            for (p in 1:length(theta)) {\n                theta_perturb <- theta\n                theta_perturb[p] <- theta[p] + epsilon\n                F_matrix[, p] <-\n                    (nonlinear_model(theta_perturb, x) \n                     - nonlinear_model(theta, x)) / epsilon\n            }\n            \n            # Compute residuals\n            residuals <- y - nonlinear_model(theta, x)\n            \n            # Compute Levenberg-Marquardt update\n            A <-\n                t(F_matrix) %*% F_matrix + tau * diag(length(theta)) \n            + lambda * diag(length(theta))  # Regularized A\n            delta <- tryCatch(\n                solve(A) %*% t(F_matrix) %*% residuals,\n                error = function(e) {\n                    cat(\"Singular matrix detected at iteration\",\n                        j,\n                        \"- Increasing tau\\n\")\n                    tau <<- tau * 10  # Increase tau to stabilize\n                    # Return zero delta to avoid NaN updates\n                    return(rep(0, length(theta)))  \n                }\n            )\n            \n            theta_new <- theta + delta\n            \n            # Compute new SSE\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Adjust tau dynamically\n            if (sse_values[j] < sse(theta, x, y)) {\n                # Reduce tau but prevent it from going too low\n                tau <-\n                    max(tau / 10, 1e-8)  \n            } else {\n                tau <- tau * 10  # Increase tau if SSE increases\n            }\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Levenberg-Marquardt\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- levenberg_marquardt(theta_init, x, y)\n#> Singular matrix detected at iteration 11 - Increasing tau\n#> Converged in 11 iterations.\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Levenberg-Marquardt:\\n\")\n#> Estimated parameters (A, B) using Levenberg-Marquardt:\nprint(estimated_theta)\n#>               [,1]\n#> [1,] -6.473440e-09\n#> [2,]  1.120637e+01\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", linewidth = 1) +\n    labs(title = \"Levenberg-Marquardt Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"newton-raphson","chapter":"6 Non-Linear Regression","heading":"6.2.1.5 Newton-Raphson Algorithm","text":"Newton-Raphson method second-order optimization technique used nonlinear least squares problems. Unlike first-order methods (Steepest Descent (Gradient Descent) Gauss-Newton Algorithm), Newton-Raphson uses first second derivatives objective function faster convergence.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\left[\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}\\right]^{-1} \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector (first derivative objective function).\\(\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}\\) Hessian matrix (second derivative objective function).\\(\\alpha_j\\) learning rate, controlling step size.Hessian matrix nonlinear least squares problems :\\[\n\\frac{\\partial^2 Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'} = 2 \\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) - 2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2 f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\n\\]:first term \\(2 \\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)})\\) Gauss-Newton Algorithm.second term \\(-2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2 f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\\) contains second-order derivatives.Key ObservationsGauss-Newton vs. Newton-Raphson:\nGauss-Newton approximates Hessian ignoring second term.\nNewton-Raphson explicitly incorporates second-order derivatives, making precise computationally expensive.\nGauss-Newton approximates Hessian ignoring second term.Newton-Raphson explicitly incorporates second-order derivatives, making precise computationally expensive.Challenges:\nHessian matrix may singular, making impossible invert.\nComputing second derivatives often difficult complex functions.\nHessian matrix may singular, making impossible invert.Computing second derivatives often difficult complex functions.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Compute Gradient (First Derivative) of SSE\ngradient_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Compute Hessian (Second Derivative) of SSE\nhessian_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Compute second derivatives\n    H_11 <- 2 * sum(exp(2 * theta[2] * x))\n    H_12 <- 2 * sum(x * exp(2 * theta[2] * x) * theta[1])\n    H_21 <- H_12\n    \n    term1 <- 2 * sum((x ^ 2) * exp(2 * theta[2] * x) * theta[1] ^ 2)\n    term2 <- 2 * sum(residuals * (x ^ 2) * exp(theta[2] * x))\n    \n    H_22 <- term1 - term2\n    \n    return(matrix(\n        c(H_11, H_12, H_21, H_22),\n        nrow = 2,\n        byrow = TRUE\n    ))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Newton-Raphson Optimization Implementation\nnewton_raphson <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500) {\n        theta <- theta_init\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            hessian <- hessian_sse(theta, x, y)\n            \n            # Check if Hessian is invertible\n            if (det(hessian) == 0) {\n                cat(\"Hessian is singular at iteration\",\n                    j,\n                    \"- Using identity matrix instead.\\n\")\n                # Replace with identity matrix if singular\n                hessian <-\n                    diag(length(theta))  \n            }\n            \n            # Compute Newton update\n            delta <- solve(hessian) %*% grad\n            theta_new <- theta - delta\n            sse_values[j] <- sse(theta_new, x, y)\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta_new, sse_values = sse_values[1:j]))\n            }\n            \n            theta <- theta_new\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Newton-Raphson\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- newton_raphson(theta_init, x, y)\n#> Converged in 222 iterations.\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B) using Newton-Raphson:\\n\")\n#> Estimated parameters (A, B) using Newton-Raphson:\nprint(estimated_theta)\n#>           [,1]\n#> [1,] 1.9934188\n#> [2,] 0.3008742\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", size = 1) +\n    labs(title = \"Newton-Raphson Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"quasi-newton-method","chapter":"6 Non-Linear Regression","heading":"6.2.1.6 Quasi-Newton Method","text":"Quasi-Newton method optimization technique approximates Newton’s method without requiring explicit computation Hessian matrix. Instead, iteratively constructs approximation \\(\\mathbf{H}_j\\) Hessian based first derivative information.update rule :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{H}_j^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]:\\(\\mathbf{H}_j\\) symmetric positive definite approximation Hessian matrix.\\(j \\\\infty\\), \\(\\mathbf{H}_j\\) gets closer true Hessian.\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate, controlling step size.Use Quasi-Newton Instead Newton-Raphson Method?Newton-Raphson requires computing Hessian matrix explicitly, computationally expensive may singular.Quasi-Newton avoids computing Hessian directly approximating iteratively.Among first-order methods (require gradients, Hessians), Quasi-Newton methods perform best.Hessian ApproximationInstead directly computing Hessian \\(\\mathbf{H}_j\\), Quasi-Newton methods update approximation \\(\\mathbf{H}_j\\) iteratively.One widely used formulas Broyden-Fletcher-Goldfarb-Shanno (BFGS) update:\\[\n\\mathbf{H}_{j+1} = \\mathbf{H}_j + \\frac{(\\mathbf{s}_j \\mathbf{s}_j')}{\\mathbf{s}_j' \\mathbf{y}_j} - \\frac{\\mathbf{H}_j \\mathbf{y}_j \\mathbf{y}_j' \\mathbf{H}_j}{\\mathbf{y}_j' \\mathbf{H}_j \\mathbf{y}_j}\n\\]:\\(\\mathbf{s}_j = \\hat{\\theta}^{(j+1)} - \\hat{\\theta}^{(j)}\\) (change parameters).\\(\\mathbf{y}_j = \\nabla Q(\\hat{\\theta}^{(j+1)}) - \\nabla Q(\\hat{\\theta}^{(j)})\\) (change gradient).\\(\\mathbf{H}_j\\) current inverse Hessian approximation.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x))^2)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Run BFGS Optimization using `optim()`\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- optim(\n    par = theta_init,\n    fn = function(theta) sse(theta, x, y),  # Minimize SSE\n    method = \"BFGS\",\n    control = list(trace = 0)  # Suppress optimization progress\n    # control = list(trace = 1, REPORT = 1)  # Print optimization progress\n)\n\n# Extract results\nestimated_theta <- result$par\nsse_final <- result$value\nconvergence_status <- result$convergence  # 0 means successful convergence\n\n# Display estimated parameters\ncat(\"\\n=== Optimization Results ===\\n\")\n#> \n#> === Optimization Results ===\ncat(\"Estimated parameters (A, B) using Quasi-Newton BFGS:\\n\")\n#> Estimated parameters (A, B) using Quasi-Newton BFGS:\nprint(estimated_theta)\n#> [1] 1.9954216 0.3007569\n\n# Display final SSE\ncat(\"\\nFinal SSE:\", sse_final, \"\\n\")\n#> \n#> Final SSE: 20.3227"},{"path":"non-linear-regression.html","id":"trust-region-reflective-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.1.7 Trust-Region Reflective Algorithm","text":"Trust-Region Reflective (TRR) algorithm optimization technique used nonlinear least squares problems. Unlike Newton’s method gradient-based approaches, TRR dynamically restricts updates trust region, ensuring stability preventing overshooting.goal minimize objective function \\(Q(\\theta)\\) (e.g., Sum Squared Errors, SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} Q(\\theta)\n\\]Instead taking full Newton step, TRR solves following quadratic subproblem:\\[\n\\min_{\\delta} m_j(\\delta) = Q(\\hat{\\theta}^{(j)}) + \\nabla Q(\\hat{\\theta}^{(j)})' \\delta + \\frac{1}{2} \\delta' \\mathbf{H}_j \\delta\n\\]subject :\\[\n\\|\\delta\\| \\leq \\Delta_j\n\\]:\\(\\mathbf{H}_j\\) approximation Hessian matrix.\\(\\mathbf{H}_j\\) approximation Hessian matrix.\\(\\nabla Q(\\hat{\\theta}^{(j)})\\) gradient vector.\\(\\nabla Q(\\hat{\\theta}^{(j)})\\) gradient vector.\\(\\Delta_j\\) trust-region radius, adjusted dynamically.\\(\\Delta_j\\) trust-region radius, adjusted dynamically.Trust-Region AdjustmentsThe algorithm modifies step size dynamically based ratio \\(\\rho_j\\):\\[\n\\rho_j = \\frac{Q(\\hat{\\theta}^{(j)}) - Q(\\hat{\\theta}^{(j)} + \\delta)}{m_j(0) - m_j(\\delta)}\n\\]\\(\\rho_j > 0.75\\) \\(\\|\\delta\\| = \\Delta_j\\), expand trust region: \\[\n\\Delta_{j+1} = 2 \\Delta_j\n\\]\\(\\rho_j > 0.75\\) \\(\\|\\delta\\| = \\Delta_j\\), expand trust region: \\[\n\\Delta_{j+1} = 2 \\Delta_j\n\\]\\(\\rho_j < 0.25\\), shrink trust region: \\[\n\\Delta_{j+1} = \\frac{1}{2} \\Delta_j\n\\]\\(\\rho_j < 0.25\\), shrink trust region: \\[\n\\Delta_{j+1} = \\frac{1}{2} \\Delta_j\n\\]\\(\\rho_j > 0\\), accept step; otherwise, reject .\\(\\rho_j > 0\\), accept step; otherwise, reject .step violates constraint, reflected back feasible region:\\[\n\\hat{\\theta}^{(j+1)} = \\max(\\hat{\\theta}^{(j)} + \\delta, \\theta_{\\min})\n\\]ensures optimization respects parameter bounds.","code":"\n# Load required libraries\nlibrary(ggplot2)\n\n\n# Define a nonlinear function (exponential model)\nnonlinear_model <- function(theta, x) {\n    theta[1] * exp(theta[2] * x)\n}\n\n# Define SSE function\nsse <- function(theta, x, y) {\n    sum((y - nonlinear_model(theta, x)) ^ 2)\n}\n\n# Compute Gradient (First Derivative) of SSE\ngradient_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Partial derivative w.r.t theta_1\n    grad_1 <- -2 * sum(residuals * exp(theta[2] * x))\n    \n    # Partial derivative w.r.t theta_2\n    grad_2 <- -2 * sum(residuals * theta[1] * x * exp(theta[2] * x))\n    \n    return(c(grad_1, grad_2))\n}\n\n# Compute Hessian Approximation of SSE\nhessian_sse <- function(theta, x, y) {\n    residuals <- y - nonlinear_model(theta, x)\n    \n    # Compute second derivatives\n    H_11 <- 2 * sum(exp(2 * theta[2] * x))\n    H_12 <- 2 * sum(x * exp(2 * theta[2] * x) * theta[1])\n    H_21 <- H_12\n    \n    term1 <- 2 * sum((x ^ 2) * exp(2 * theta[2] * x) * theta[1] ^ 2)\n    term2 <- 2 * sum(residuals * (x ^ 2) * exp(theta[2] * x))\n    \n    H_22 <- term1 - term2\n    \n    return(matrix(\n        c(H_11, H_12, H_21, H_22),\n        nrow = 2,\n        byrow = TRUE\n    ))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ntrue_theta <- c(2, 0.3)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.5)\n\n# Manual Trust-Region Reflective Optimization Implementation\ntrust_region_reflective <-\n    function(theta_init,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 500,\n             delta_max = 1.0) {\n        theta <- theta_init\n        delta_j <- 0.5  # Initial trust-region size\n        n <- length(theta)\n        sse_values <- numeric(max_iter)\n        \n        for (j in 1:max_iter) {\n            grad <- gradient_sse(theta, x, y)\n            hessian <- hessian_sse(theta, x, y)\n            \n            # Check if Hessian is invertible\n            if (det(hessian) == 0) {\n                cat(\"Hessian is singular at iteration\",\n                    j,\n                    \"- Using identity matrix instead.\\n\")\n                hessian <-\n                    diag(n)  # Replace with identity matrix if singular\n            }\n            \n            # Compute Newton step\n            delta_full <- -solve(hessian) %*% grad\n            \n            # Apply trust-region constraint\n            if (sqrt(sum(delta_full ^ 2)) > delta_j) {\n                delta <-\n                    (delta_j / sqrt(sum(delta_full ^ 2))) * delta_full  # Scale step\n            } else {\n                delta <- delta_full\n            }\n            \n            # Compute new theta and ensure it respects constraints\n            theta_new <-\n                pmax(theta + delta, c(0,-Inf))  # Reflect to lower bound\n            sse_new <- sse(theta_new, x, y)\n            \n            # Compute agreement ratio (rho_j)\n            predicted_reduction <-\n                -t(grad) %*% delta - 0.5 * t(delta) %*% hessian %*% delta\n            actual_reduction <- sse(theta, x, y) - sse_new\n            rho_j <- actual_reduction / predicted_reduction\n            \n            # Adjust trust region size\n            if (rho_j < 0.25) {\n                delta_j <- max(delta_j / 2, 1e-4)  # Shrink\n            } else if (rho_j > 0.75 &&\n                       sqrt(sum(delta ^ 2)) == delta_j) {\n                delta_j <- min(2 * delta_j, delta_max)  # Expand\n            }\n            \n            # Accept or reject step\n            if (rho_j > 0) {\n                theta <- theta_new  # Accept step\n            } else {\n                cat(\"Step rejected at iteration\", j, \"\\n\")\n            }\n            \n            sse_values[j] <- sse(theta, x, y)\n            \n            # Check for convergence\n            if (sum(abs(delta)) < tol) {\n                cat(\"Converged in\", j, \"iterations.\\n\")\n                return(list(theta = theta, sse_values = sse_values[1:j]))\n            }\n        }\n        \n        return(list(theta = theta, sse_values = sse_values))\n    }\n\n# Run Manual Trust-Region Reflective Algorithm\ntheta_init <- c(1, 0.1)  # Initial guess\nresult <- trust_region_reflective(theta_init, x, y)\n\n# Extract results\nestimated_theta <- result$theta\nsse_values <- result$sse_values\n\n# Plot convergence of SSE over iterations\nsse_df <-\n    data.frame(Iteration = seq_along(sse_values), SSE = sse_values)\n\nggplot(sse_df, aes(x = Iteration, y = SSE)) +\n    geom_line(color = \"blue\", size = 1) +\n    labs(title = \"Trust-Region Reflective Convergence\",\n         x = \"Iteration\",\n         y = \"SSE\") +\n    theme_minimal()"},{"path":"non-linear-regression.html","id":"derivative-free","chapter":"6 Non-Linear Regression","heading":"6.2.2 Derivative-Free","text":"","code":""},{"path":"non-linear-regression.html","id":"secant-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.1 Secant Method","text":"Secant Method root-finding algorithm approximates derivative using finite differences, making derivative-free alternative Newton’s method. particularly useful exact gradient (Jacobian case optimization problems) unavailable expensive compute.nonlinear optimization, apply Secant Method iteratively refine parameter estimates without explicitly computing second-order derivatives.one dimension, Secant Method approximates derivative :\\[\nf'(\\theta) \\approx \\frac{f(\\theta_{j}) - f(\\theta_{j-1})}{\\theta_{j} - \\theta_{j-1}}.\n\\]Using approximation, update step Secant Method follows:\\[\n\\theta_{j+1} = \\theta_j - f(\\theta_j) \\frac{\\theta_j - \\theta_{j-1}}{f(\\theta_j) - f(\\theta_{j-1})}.\n\\]Instead computing exact derivative (Newton’s method), use difference last two iterates approximate . makes Secant Method efficient cases gradient computation expensive infeasible.higher dimensions, Secant Method extends approximate Quasi-Newton Method, often referred Broyden’s Method. iteratively approximate inverse Hessian matrix using past updates.update formula vector-valued function \\(F(\\theta)\\) :\\[\n\\theta^{(j+1)} = \\theta^{(j)} - \\mathbf{B}^{(j)} F(\\theta^{(j)}),\n\\]\\(\\mathbf{B}^{(j)}\\) approximation inverse Jacobian matrix, updated step using:\\[\n\\mathbf{B}^{(j+1)} = \\mathbf{B}^{(j)} + \\frac{(\\Delta \\theta^{(j)} - \\mathbf{B}^{(j)} \\Delta F^{(j)}) (\\Delta \\theta^{(j)})'}{(\\Delta \\theta^{(j)})' \\Delta F^{(j)}},\n\\]:\\(\\Delta \\theta^{(j)} = \\theta^{(j+1)} - \\theta^{(j)}\\),\\(\\Delta F^{(j)} = F(\\theta^{(j+1)}) - F(\\theta^{(j)})\\).secant-based update approximates behavior true Jacobian inverse, reducing computational cost compared full Newton’s method.Algorithm: Secant Method Nonlinear OptimizationThe Secant Method nonlinear optimization follows steps:Initialize parameters \\(\\theta^{(0)}\\) \\(\\theta^{(1)}\\) (two starting points).Compute function values \\(F(\\theta^{(0)})\\) \\(F(\\theta^{(1)})\\).Use Secant update formula compute next iterate \\(\\theta^{(j+1)}\\).Update approximate inverse Jacobian \\(\\mathbf{B}^{(j)}\\).Repeat convergence.","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n\n# Define a nonlinear function (logistic model)\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    return(sum((y - nonlinear_model(theta, x)) ^ 2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Improved Secant Method with Line Search\nsecant_method_improved <-\n    function(theta0,\n             theta1,\n             x,\n             y,\n             tol = 1e-6,\n             max_iter = 100) {\n        theta_prev <- as.matrix(theta0)  # Convert to column vector\n        theta_curr <- as.matrix(theta1)\n        \n        alpha <- 1  # Initial step size\n        step_reduction_factor <- 0.5  # Reduce step if SSE increases\n        B_inv <- diag(length(theta0))  # Identity matrix initialization\n        \n        for (j in 1:max_iter) {\n            # Compute function values using numerical gradient\n            F_prev <-\n                as.matrix(grad(function(theta)\n                    sse(theta, x, y), theta_prev))\n            F_curr <-\n                as.matrix(grad(function(theta)\n                    sse(theta, x, y), theta_curr))\n            \n            # Compute secant step update (convert to column vectors)\n            delta_theta <- as.matrix(theta_curr - theta_prev)\n            delta_F <- as.matrix(F_curr - F_prev)\n            \n            # Prevent division by zero\n            if (sum(abs(delta_F)) < 1e-8) {\n                cat(\"Gradient difference is too small, stopping optimization.\\n\")\n                break\n            }\n            \n            # Ensure correct dimensions for Broyden update\n            numerator <-\n                (delta_theta - B_inv %*% delta_F) %*% t(delta_theta)\n            denominator <-\n                as.numeric(t(delta_theta) %*% delta_F)  # Convert scalar to numeric\n            \n            # Updated inverse Jacobian approximation\n            B_inv <-\n                B_inv + numerator / denominator  \n            \n            # Compute next theta using secant update\n            theta_next <- theta_curr - alpha * B_inv %*% F_curr\n            \n            # Line search: Reduce step size if SSE increases\n            while (sse(as.vector(theta_next), x, y) > sse(as.vector(theta_curr), x, y)) {\n                alpha <- alpha * step_reduction_factor\n                theta_next <- theta_curr - alpha * B_inv %*% F_curr\n                \n                # Print progress\n                # cat(\"Reducing step size to\", alpha, \"\\n\")\n            }\n            \n            # Print intermediate results for debugging\n            cat(\n                sprintf(\n                    \"Iteration %d: Theta = (%.4f, %.4f, %.4f), Alpha = %.4f\\n\",\n                    j,\n                    theta_next[1],\n                    theta_next[2],\n                    theta_next[3],\n                    alpha\n                )\n            )\n            \n            # Check convergence\n            if (sum(abs(theta_next - theta_curr)) < tol) {\n                cat(\"Converged successfully.\\n\")\n                break\n            }\n            \n            # Update iterates\n            theta_prev <- theta_curr\n            theta_curr <- theta_next\n        }\n        \n        return(as.vector(theta_curr))  # Convert back to numeric vector\n    }\n\n# Adjusted initial parameter guesses\ntheta0 <- c(2, 0.8,-1)   # Closer to true parameters\ntheta1 <- c(4, 1.2,-0.5)  # Slightly refined\n\n# Run Improved Secant Method\nestimated_theta <- secant_method_improved(theta0, theta1, x, y)\n#> Iteration 1: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0156\n#> Iteration 2: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0000\n#> Converged successfully.\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B, C) using Secant Method:\\n\")\n#> Estimated parameters (A, B, C) using Secant Method:\nprint(estimated_theta)\n#> [1] 3.85213912 1.30538435 0.00566417\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Secant Method: Data & Fitted Curve\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = -5,\n    to = 5,\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"grid-search","chapter":"6 Non-Linear Regression","heading":"6.2.2.2 Grid Search","text":"Grid Search (GS) brute-force optimization method systematically explores grid possible parameter values identify combination minimizes residual sum squares (RSS). Unlike gradient-based optimization, moves iteratively towards minimum, grid search evaluates predefined parameter combinations, making robust computationally expensive.Grid search particularly useful :function highly nonlinear, making gradient methods unreliable.function highly nonlinear, making gradient methods unreliable.parameter space small, allowing exhaustive search.parameter space small, allowing exhaustive search.global minimum needed, prior knowledge parameter ranges exists.global minimum needed, prior knowledge parameter ranges exists.goal Grid Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\\\Theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]Grid search discretizes search space \\(\\Theta\\) finite set candidate values parameter:\\[\n\\Theta = \\theta_1 \\times \\theta_2 \\times \\dots \\times \\theta_p.\n\\]accuracy solution depends grid resolution—finer grid leads better accuracy higher computational cost.Grid Search AlgorithmDefine grid possible values parameter.Evaluate SSE combination parameters.Select parameter set minimizes SSE.","code":"\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Grid Search Optimization\ngrid_search_optimization <- function(x, y, grid_size = 10) {\n    # Define grid of parameter values\n    A_values <- seq(2, 6, length.out = grid_size)\n    B_values <- seq(0.5, 3, length.out = grid_size)\n    C_values <- seq(-2, 2, length.out = grid_size)\n    \n    # Generate all combinations of parameters\n    param_grid <- expand.grid(A = A_values, B = B_values, C = C_values)\n    \n    # Evaluate SSE for each parameter combination\n    param_grid$SSE <- apply(param_grid, 1, function(theta) {\n        sse(as.numeric(theta[1:3]), x, y)\n    })\n    \n    # Select the best parameter set\n    best_params <- param_grid[which.min(param_grid$SSE), 1:3]\n    return(as.numeric(best_params))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Grid Search\nestimated_theta <- grid_search_optimization(x, y, grid_size = 20)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Grid Search:\\n\")\n#> Estimated parameters (A, B, C) using Grid Search:\nprint(estimated_theta)\n#> [1] 4.1052632 1.4210526 0.1052632\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Grid Search: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"nelder-mead","chapter":"6 Non-Linear Regression","heading":"6.2.2.3 Nelder-Mead (Simplex)","text":"Nelder-Mead algorithm, also known Simplex method, derivative-free optimization algorithm iteratively adjusts geometric shape (simplex) find minimum objective function. particularly useful nonlinear regression gradient-based methods fail due non-differentiability noisy function evaluations.Nelder-Mead particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low-dimensional.parameter space low-dimensional.goal Nelder-Mead find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Simplex RepresentationThe algorithm maintains simplex, geometric shape \\(p+1\\) vertices \\(p\\)-dimensional parameter space.vertex represents parameter vector:\\[\nS = \\{ \\theta_1, \\theta_2, \\dots, \\theta_{p+1} \\}.\n\\]2. Simplex OperationsAt iteration, algorithm updates simplex based objective function values vertex:Reflection: Reflect worst point \\(\\theta_h\\) across centroid.\n\\[\n\\theta_r = \\theta_c + \\alpha (\\theta_c - \\theta_h)\n\\]Reflection: Reflect worst point \\(\\theta_h\\) across centroid.\\[\n\\theta_r = \\theta_c + \\alpha (\\theta_c - \\theta_h)\n\\]Expansion: reflection improves objective, expand step.\n\\[\n\\theta_e = \\theta_c + \\gamma (\\theta_r - \\theta_c)\n\\]Expansion: reflection improves objective, expand step.\\[\n\\theta_e = \\theta_c + \\gamma (\\theta_r - \\theta_c)\n\\]Contraction: reflection worsens objective, contract towards centroid.\n\\[\n\\theta_c = \\theta_c + \\rho (\\theta_h - \\theta_c)\n\\]Contraction: reflection worsens objective, contract towards centroid.\\[\n\\theta_c = \\theta_c + \\rho (\\theta_h - \\theta_c)\n\\]Shrink: contraction fails, shrink simplex.\n\\[\n\\theta_i = \\theta_1 + \\sigma (\\theta_i - \\theta_1), \\quad \\forall > 1\n\\]Shrink: contraction fails, shrink simplex.\\[\n\\theta_i = \\theta_1 + \\sigma (\\theta_i - \\theta_1), \\quad \\forall > 1\n\\]:\\(\\alpha = 1\\) (reflection coefficient),\\(\\alpha = 1\\) (reflection coefficient),\\(\\gamma = 2\\) (expansion coefficient),\\(\\gamma = 2\\) (expansion coefficient),\\(\\rho = 0.5\\) (contraction coefficient),\\(\\rho = 0.5\\) (contraction coefficient),\\(\\sigma = 0.5\\) (shrink coefficient).\\(\\sigma = 0.5\\) (shrink coefficient).process continues convergence.Nelder-Mead AlgorithmInitialize simplex \\(p+1\\) vertices.Evaluate SSE vertex.Perform reflection, expansion, contraction, shrink operations.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Nelder-Mead Optimization\nnelder_mead_optimization <- function(x, y) {\n    # Initial guess for parameters\n    initial_guess <- c(2, 1, 0)\n    \n    # Run Nelder-Mead optimization\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x,\n        y = y,\n        method = \"Nelder-Mead\",\n        control = list(maxit = 500)\n    )\n    \n    return(result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Nelder-Mead Optimization\nestimated_theta <- nelder_mead_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Nelder-Mead:\\n\")\n#> Estimated parameters (A, B, C) using Nelder-Mead:\nprint(estimated_theta)\n#> [1] 4.06873116 1.42759898 0.01119379\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Nelder-Mead: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"powells-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.4 Powell’s Method","text":"Powell’s Method derivative-free optimization algorithm minimizes function without using gradients. works iteratively refining set search directions efficiently navigate parameter space. Unlike Nelder-Mead (Simplex), adapts simplex, Powell’s method builds orthogonal basis search directions.Powell’s method particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low--moderate dimensional.parameter space low--moderate dimensional.goal Powell’s Method find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Search DirectionsPowell’s method maintains set search directions \\(\\mathbf{d}_1, \\mathbf{d}_2, \\dots, \\mathbf{d}_p\\):\\[\nD = \\{ \\mathbf{d}_1, \\mathbf{d}_2, ..., \\mathbf{d}_p \\}.\n\\]Initially, directions chosen unit basis vectors (optimizing single parameter independently).2. Line MinimizationFor direction \\(\\mathbf{d}_j\\), Powell’s method performs 1D optimization:\\[\n\\theta' = \\theta + \\lambda \\mathbf{d}_j,\n\\]\\(\\lambda\\) chosen minimize \\(SSE(\\theta')\\).3. Updating Search DirectionsAfter optimizing along directions:largest improvement direction \\(\\mathbf{d}_{\\max}\\) replaced :\\[\n\\mathbf{d}_{\\text{new}} = \\theta_{\\text{final}} - \\theta_{\\text{initial}}.\n\\]new direction set orthogonalized using Gram-Schmidt.ensures efficient exploration parameter space.4. Convergence CriteriaPowell’s method stops function improvement tolerance level:\\[\n|SSE(\\theta_{t+1}) - SSE(\\theta_t)| < \\epsilon.\n\\]Powell’s Method AlgorithmInitialize search directions (standard basis vectors).Perform 1D line searches along direction.Update search directions based largest improvement.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Powell's Method Optimization\npowell_optimization <- function(x, y) {\n    # Initial guess for parameters\n    initial_guess <- c(2, 1, 0)\n    \n    # Run Powell’s optimization (via BFGS without gradient)\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x,\n        y = y,\n        method = \"BFGS\",\n        control = list(maxit = 500),\n        gr = NULL  # No gradient used\n    )\n    \n    return(result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Powell's Method Optimization\nestimated_theta <- powell_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Powell’s Method:\\n\")\n#> Estimated parameters (A, B, C) using Powell’s Method:\nprint(estimated_theta)\n#> [1] 4.06876538 1.42765687 0.01128753\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Powell's Method: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"random-search","chapter":"6 Non-Linear Regression","heading":"6.2.2.5 Random Search","text":"Random Search (RS) simple yet effective optimization algorithm explores search space randomly sampling candidate solutions. Unlike grid search, evaluates predefined parameter combinations, random search selects random subset, reducing computational cost.Random search particularly useful :search space large, making grid search impractical.search space large, making grid search impractical.Gradient-based methods fail due non-differentiability noisy data.Gradient-based methods fail due non-differentiability noisy data.optimal region unknown, making global exploration essential.optimal region unknown, making global exploration essential.goal Random Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\\\Theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]Unlike grid search, random search evaluate parameter combinations instead randomly samples subset:\\[\n\\Theta_{\\text{sampled}} \\subset \\Theta.\n\\]accuracy solution depends number random samples—higher number increases likelihood finding good solution.Random Search AlgorithmDefine random sampling range parameter.Randomly sample \\(N\\) parameter sets.Evaluate SSE sampled set.Select parameter set minimizes SSE.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n  return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n  return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n  predictions <- nonlinear_model(theta, x)\n  return(sum((y - predictions) ^ 2))\n}\n\n# Random Search Optimization\nrandom_search_optimization <- function(x, y, num_samples = 100) {\n  # Define parameter ranges\n  A_range <-\n    runif(num_samples, 2, 6)   # Random values between 2 and 6\n  B_range <-\n    runif(num_samples, 0.5, 3) # Random values between 0.5 and 3\n  C_range <-\n    runif(num_samples,-2, 2)  # Random values between -2 and 2\n  \n  # Initialize best parameters\n  best_theta <- NULL\n  best_sse <- Inf\n  \n  # Evaluate randomly sampled parameter sets\n  for (i in 1:num_samples) {\n    theta <- c(A_range[i], B_range[i], C_range[i])\n    current_sse <- sse(theta, x, y)\n    \n    if (current_sse < best_sse) {\n      best_sse <- current_sse\n      best_theta <- theta\n    }\n  }\n  \n  return(best_theta)\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Random Search\nestimated_theta <-\n  random_search_optimization(x, y, num_samples = 500)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Random Search:\\n\")\n#> Estimated parameters (A, B, C) using Random Search:\nprint(estimated_theta)\n#> [1] 4.0893431 1.4687456 0.1024474\n\n# Plot data and fitted curve\nplot(\n  x,\n  y,\n  main = \"Random Search: Nonlinear Regression Optimization\",\n  pch = 19,\n  cex = 0.5,\n  xlab = \"x\",\n  ylab = \"y\"\n)\ncurve(\n  nonlinear_model(estimated_theta, x),\n  from = min(x),\n  to = max(x),\n  add = TRUE,\n  col = \"red\",\n  lwd = 2\n)\nlegend(\n  \"topleft\",\n  legend = c(\"Data\", \"Fitted Curve\"),\n  pch = c(19, NA),\n  lty = c(NA, 1),\n  col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"hooke-jeeves","chapter":"6 Non-Linear Regression","heading":"6.2.2.6 Hooke-Jeeves Pattern Search","text":"Hooke-Jeeves Pattern Search derivative-free optimization algorithm searches optimal solution exploring adjusting parameter values iteratively. Unlike gradient-based methods, require differentiability, making effective non-smooth noisy functions.Hooke-Jeeves particularly useful :function non-differentiable noisy.function non-differentiable noisy.Gradient-based methods unreliable.Gradient-based methods unreliable.parameter space low--moderate dimensional.parameter space low--moderate dimensional.goal Hooke-Jeeves Pattern Search find optimal parameter set \\(\\hat{\\theta}\\) minimizes Sum Squared Errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Exploratory MovesAt iteration, algorithm perturbs parameter find lower SSE:\\[\n\\theta' = \\theta \\pm \\delta.\n\\]new parameter \\(\\theta'\\) reduces SSE, becomes new base point.2. Pattern MovesAfter improvement, algorithm accelerates movement promising direction:\\[\n\\theta_{\\text{new}} = \\theta_{\\text{old}} + (\\theta_{\\text{old}} - \\theta_{\\text{prev}}).\n\\]speeds convergence towards optimum.3. Step Size AdaptationIf improvement found, step size \\(\\delta\\) reduced:\\[\n\\delta_{\\text{new}} = \\beta \\cdot \\delta.\n\\]\\(\\beta < 1\\) reduction factor.4. Convergence CriteriaThe algorithm stops step size sufficiently small:\\[\n\\delta < \\epsilon.\n\\]Hooke-Jeeves AlgorithmInitialize starting point \\(\\theta\\) step size \\(\\delta\\).Perform exploratory moves parameter direction.improvement found, perform pattern moves.Reduce step size improvement found.Repeat convergence.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Hooke-Jeeves Pattern Search Optimization\nhooke_jeeves_optimization <-\n    function(x,\n             y,\n             step_size = 0.5,\n             tol = 1e-6,\n             max_iter = 500) {\n        # Initial guess for parameters\n        theta <- c(2, 1, 0)\n        best_sse <- sse(theta, x, y)\n        step <- step_size\n        iter <- 0\n        \n        while (step > tol & iter < max_iter) {\n            iter <- iter + 1\n            improved <- FALSE\n            new_theta <- theta\n            \n            # Exploratory move in each parameter direction\n            for (i in 1:length(theta)) {\n                for (delta in c(step,-step)) {\n                    theta_test <- new_theta\n                    theta_test[i] <- theta_test[i] + delta\n                    sse_test <- sse(theta_test, x, y)\n                    \n                    if (sse_test < best_sse) {\n                        best_sse <- sse_test\n                        new_theta <- theta_test\n                        improved <- TRUE\n                    }\n                }\n            }\n            \n            # Pattern move if improvement found\n            if (improved) {\n                theta <- 2 * new_theta - theta\n                best_sse <- sse(theta, x, y)\n            } else {\n                step <- step / 2  # Reduce step size\n            }\n        }\n        \n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Hooke-Jeeves Optimization\nestimated_theta <- hooke_jeeves_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Hooke-Jeeves:\\n\")\n#> Estimated parameters (A, B, C) using Hooke-Jeeves:\nprint(estimated_theta)\n#> [1] 4 1 0\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Hooke-Jeeves: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"bisection-method","chapter":"6 Non-Linear Regression","heading":"6.2.2.7 Bisection Method","text":"Bisection Method fundamental numerical technique primarily used root finding, can also adapted optimization problems goal minimize maximize nonlinear function.nonlinear regression, optimization often involves finding parameter values minimize sum squared errors (SSE):\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta)\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} \\left( y_i - f(x_i; \\theta) \\right)^2.\n\\]Since minimum function occurs derivative equals zero, apply Bisection Method derivative SSE function:\\[\n\\frac{d}{d\\theta} SSE(\\theta) = 0.\n\\]transforms optimization problem root-finding problem.Bisection Method based Intermediate Value Theorem, states:continuous function \\(f(\\theta)\\) satisfies \\(f(\\theta_a) \\cdot f(\\theta_b) < 0\\),\nexists least one root interval \\((\\theta_a, \\theta_b)\\).optimization, apply principle first derivative objective function \\(Q(\\theta)\\):\\[\nQ'(\\theta) = 0.\n\\]Step--Step Algorithm OptimizationChoose interval \\([\\theta_a, \\theta_b]\\) : \\[ Q'(\\theta_a) \\cdot Q'(\\theta_b) < 0. \\]Compute midpoint: \\[\n\\theta_m = \\frac{\\theta_a + \\theta_b}{2}.\n\\]Evaluate \\(Q'(\\theta_m)\\):\n\\(Q'(\\theta_m) = 0\\), \\(\\theta_m\\) optimal point.\n\\(Q'(\\theta_a) \\cdot Q'(\\theta_m) < 0\\), set \\(\\theta_b = \\theta_m\\).\nOtherwise, set \\(\\theta_a = \\theta_m\\).\n\\(Q'(\\theta_m) = 0\\), \\(\\theta_m\\) optimal point.\\(Q'(\\theta_a) \\cdot Q'(\\theta_m) < 0\\), set \\(\\theta_b = \\theta_m\\).Otherwise, set \\(\\theta_a = \\theta_m\\).Repeat convergence: \\[\n|\\theta_b - \\theta_a| < \\epsilon.\n\\]Determining Nature Critical PointSince Bisection Method finds stationary point, use second derivative test classify :\\(Q''(\\theta) > 0\\), point local minimum.\\(Q''(\\theta) < 0\\), point local maximum.nonlinear regression, expect \\(Q(\\theta) = SSE(\\theta)\\), solution found Bisection minimum.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Optimize all three parameters simultaneously\nfind_optimal_parameters <- function(x, y) {\n    # Initial guess for parameters (based on data)\n    initial_guess <- c(max(y), 1, median(x))  \n\n    # Bounds for parameters\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n\n    # Run optim() with L-BFGS-B (bounded optimization)\n    result <- optim(\n        par = initial_guess,\n        fn = sse,\n        x = x, y = y,\n        method = \"L-BFGS-B\",\n        lower = lower_bounds,\n        upper = upper_bounds\n    )\n    \n    return(result$par)  # Extract optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Find optimal parameters using optim()\nestimated_theta <- find_optimal_parameters(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using optim():\\n\")\n#> Estimated parameters (A, B, C) using optim():\nprint(estimated_theta)\n#> [1] 4.06876536 1.42765688 0.01128756\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Optim(): Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"stochastic-heuristic-nolinear-regression","chapter":"6 Non-Linear Regression","heading":"6.2.3 Stochastic Heuristic","text":"","code":""},{"path":"non-linear-regression.html","id":"differential-evolution-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.3.1 Differential Evolution Algorithm","text":"Differential Evolution (DE) Algorithm stochastic, population-based optimization algorithm widely used solving complex global optimization problems. Unlike gradient-based methods Newton’s method Secant method, DE require derivatives well-suited optimizing non-differentiable, nonlinear, multimodal functions.Key Features Differential EvolutionPopulation-based approach: Maintains population candidate solutions instead single point.Mutation crossover: Introduces variations explore search space.Selection mechanism: Retains best candidates next generation.Global optimization: Avoids local minima using stochastic search strategies.Mathematical Formulation Differential EvolutionDifferential Evolution operates population candidate solutions \\(\\{\\theta_i\\}\\), \\(\\theta_i\\) vector parameters. algorithm iteratively updates population using three main operations:1. MutationFor candidate solution \\(\\theta_i\\), mutant vector \\(\\mathbf{v}_i^{(j)}\\) generated :\\[\n\\mathbf{v}_i^{(j)} = \\mathbf{\\theta}_{r_1}^{(j)} + F \\cdot (\\mathbf{\\theta}_{r_2}^{(j)} - \\mathbf{\\theta}_{r_3}^{(j)})\n\\]:\\(\\mathbf{\\theta}_{r_1}, \\mathbf{\\theta}_{r_2}, \\mathbf{\\theta}_{r_3}\\) randomly selected distinct vectors population.\\(\\mathbf{\\theta}_{r_1}, \\mathbf{\\theta}_{r_2}, \\mathbf{\\theta}_{r_3}\\) randomly selected distinct vectors population.\\(F \\(0,2)\\) mutation factor controlling step size.\\(F \\(0,2)\\) mutation factor controlling step size.2. CrossoverA trial vector \\(\\mathbf{u}_i^{(j)}\\) generated combining mutant vector \\(\\mathbf{v}_i^{(j)}\\) original solution \\(\\mathbf{\\theta}_i^{(j)}\\):\\[\nu_{,k}^{(j)} =\n\\begin{cases}\nv_{,k}^{(j)}  & \\text{} rand_k \\leq C_r \\text{ } k = k_{\\text{rand}}, \\\\\n\\theta_{,k}^{(j)}  & \\text{otherwise}.\n\\end{cases}\n\\]:\\(C_r \\(0,1)\\) crossover probability.\\(C_r \\(0,1)\\) crossover probability.\\(rand_k\\) random value 0 1.\\(rand_k\\) random value 0 1.\\(k_{\\text{rand}}\\) ensures least one parameter mutated.\\(k_{\\text{rand}}\\) ensures least one parameter mutated.3. SelectionThe new candidate solution accepted improves objective function:\\[\n\\mathbf{\\theta}_i^{(j+1)} =\n\\begin{cases}\n\\mathbf{u}_i^{(j)} & \\text{} Q(\\mathbf{u}_i^{(j)}) < Q(\\mathbf{\\theta}_i^{(j)}), \\\\\n\\mathbf{\\theta}_i^{(j)} & \\text{otherwise}.\n\\end{cases}\n\\]\\(Q(\\theta)\\) objective function (e.g., sum squared errors regression problems).Algorithm: Differential Evolution Nonlinear OptimizationInitialize population candidate solutions.Evaluate objective function candidate.Mutate individuals using difference strategy.Apply crossover create trial solutions.Select individuals based fitness (objective function value).Repeat convergence (stopping criterion met).","code":"\n# Load required library\nlibrary(DEoptim)\n\n# Define a nonlinear function (logistic model)\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    return(sum((y - nonlinear_model(theta, x))^2))\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Define the objective function for DEoptim\nobjective_function <- function(theta) {\n    return(sse(theta, x, y))\n}\n\n# Define parameter bounds\nlower_bounds <- c(0, 0, -5)\nupper_bounds <- c(10, 5, 5)\n\n# Run Differential Evolution Algorithm\nde_result <-\n    DEoptim(\n        objective_function,\n        lower_bounds,\n        upper_bounds,\n        DEoptim.control(\n            NP = 50,\n            itermax = 100,\n            F = 0.8,\n            CR = 0.9, \n            trace = F\n        )\n    )\n\n# Extract optimized parameters\nestimated_theta <- de_result$optim$bestmem\n\n# Display estimated parameters\ncat(\"Estimated parameters (A, B, C) using Differential Evolution:\\n\")\n#> Estimated parameters (A, B, C) using Differential Evolution:\nprint(estimated_theta)\n#>       par1       par2       par3 \n#> 4.06876562 1.42765614 0.01128768\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Differential Evolution: Data & Fitted Curve\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = -5,\n    to = 5,\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"simulated-annealing","chapter":"6 Non-Linear Regression","heading":"6.2.3.2 Simulated Annealing","text":"Simulated Annealing (SA) probabilistic global optimization algorithm inspired annealing metallurgy, material heated slowly cooled remove defects. optimization, SA gradually refines solution exploring search space, allowing occasional jumps escape local minima, converging optimal solution.Simulated Annealing particularly useful :function highly nonlinear multimodal.function highly nonlinear multimodal.Gradient-based methods struggle due non-differentiability poor initialization.Gradient-based methods struggle due non-differentiability poor initialization.global minimum needed, rather local one.global minimum needed, rather local one.1. Energy Function (Objective Function)goal SA minimize objective function \\(Q(\\theta)\\). nonlinear regression, Sum Squared Errors (SSE):\\[\nQ(\\theta) = SSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]2. Probability AcceptanceAt step, SA randomly perturbs parameters \\(\\theta\\) create new candidate solution \\(\\theta'\\) evaluates change SSE:\\[\n\\Delta Q = Q(\\theta') - Q(\\theta).\n\\]Metropolis Criterion determines whether accept new solution:\\[\nP(\\text{accept}) =\n\\begin{cases}\n1, & \\text{} \\Delta Q < 0 \\quad \\text{(new solution improves fit)} \\\\\n\\exp\\left( -\\frac{\\Delta Q}{T} \\right), & \\text{} \\Delta Q \\geq 0 \\quad \\text{(accept probability)}.\n\\end{cases}\n\\]:\\(T\\) temperature gradually decreases iterations.\\(T\\) temperature gradually decreases iterations.Worse solutions accepted small probability escape local minima.Worse solutions accepted small probability escape local minima.3. Cooling ScheduleThe temperature follows cooling schedule:\\[\nT_k = \\alpha T_{k-1},\n\\]\\(\\alpha \\(0,1)\\) decay factor controls cooling speed.Simulated Annealing AlgorithmInitialize parameters \\(\\theta\\) randomly.Set initial temperature \\(T_0\\) cooling rate \\(\\alpha\\).Repeat max iterations:\nGenerate perturbed candidate \\(\\theta'\\).\nCompute \\(\\Delta Q = Q(\\theta') - Q(\\theta)\\).\nAccept \\(\\Delta Q < 0\\) probability \\(\\exp(-\\Delta Q / T)\\).\nReduce temperature: \\(T \\leftarrow \\alpha T\\).\nGenerate perturbed candidate \\(\\theta'\\).Compute \\(\\Delta Q = Q(\\theta') - Q(\\theta)\\).Accept \\(\\Delta Q < 0\\) probability \\(\\exp(-\\Delta Q / T)\\).Reduce temperature: \\(T \\leftarrow \\alpha T\\).Return best solution found.","code":"\n# Load required library\nlibrary(stats)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Simulated Annealing Algorithm\nsimulated_annealing <-\n    function(x,\n             y,\n             initial_theta,\n             T_init = 1.0,\n             alpha = 0.99,\n             max_iter = 5000) {\n        # Initialize parameters\n        theta <- initial_theta\n        best_theta <- theta\n        best_sse <- sse(theta, x, y)\n        T <- T_init  # Initial temperature\n        \n        for (iter in 1:max_iter) {\n            # Generate new candidate solution (small random perturbation)\n            theta_new <- theta + rnorm(length(theta), mean = 0, sd = T)\n            \n            # Compute new SSE\n            sse_new <- sse(theta_new, x, y)\n            \n            # Compute change in SSE\n            delta_Q <- sse_new - best_sse\n            \n            # Acceptance criteria\n            if (delta_Q < 0 || runif(1) < exp(-delta_Q / T)) {\n                theta <- theta_new\n                best_sse <- sse_new\n                best_theta <- theta_new\n            }\n            \n            # Reduce temperature\n            T <- alpha * T\n            \n            # Stopping condition (very low temperature)\n            if (T < 1e-6)\n                break\n        }\n        \n        return(best_theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Initial guess\ninitial_theta <-\n    c(runif(1, 1, 5), runif(1, 0.1, 3), runif(1,-2, 2))\n\n# Run Simulated Annealing\nestimated_theta <- simulated_annealing(x, y, initial_theta)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Simulated Annealing:\\n\")\n#> Estimated parameters (A, B, C) using Simulated Annealing:\nprint(estimated_theta)\n#> [1] 4.07180419 1.41457906 0.01422147\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Simulated Annealing: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"genetic-algorithm","chapter":"6 Non-Linear Regression","heading":"6.2.3.3 Genetic Algorithm","text":"Genetic Algorithms (GA) class evolutionary algorithms inspired principles natural selection genetics. Unlike deterministic optimization techniques, GA evolves population candidate solutions multiple generations, using genetic operators selection, crossover, mutation.GA particularly useful :function nonlinear, non-differentiable, highly multimodal.function nonlinear, non-differentiable, highly multimodal.Gradient-based methods fail due rugged function landscapes.Gradient-based methods fail due rugged function landscapes.global minimum required, rather local one.global minimum required, rather local one.goal Genetic Algorithm find optimal solution $\\hat{\\theta}$ minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Population RepresentationEach candidate solution (individual) represented chromosome, simply vector parameters: \\[\n\\theta = (\\theta_1, \\theta_2, \\theta_3)\n\\]entire population consists multiple solutions.2. SelectionEach individual’s fitness evaluated using:\\[\n\\text{Fitness}(\\theta) = -SSE(\\theta)\n\\]use Tournament Selection Roulette Wheel Selection choose parents reproduction.3. Crossover (Recombination)new solution \\(\\theta'\\) generated combining two parents:\\[\\theta' = \\alpha \\theta_{\\text{parent1}} + (1 - \\alpha) \\theta_{\\text{parent2}}, \\quad \\alpha \\sim U(0,1).\\]4. MutationRandom small changes introduced increase diversity:\\[\\theta_i' = \\theta_i + \\mathcal{N}(0, \\sigma),\\]\\(\\mathcal{N}(0, \\sigma)\\) small Gaussian perturbation.5. Evolutionary CycleThe algorithm iterates :SelectionSelectionCrossoverCrossoverMutationMutationSurvival fittestSurvival fittestTermination convergence reached.Termination convergence reached.","code":"\n# Load required library\nlibrary(GA)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Genetic Algorithm for Optimization\nga_optimization <- function(x, y) {\n    # Define fitness function (negative SSE for maximization)\n    fitness_function <- function(theta) {\n        # GA maximizes fitness, so we use negative SSE\n        return(-sse(theta, x, y))  \n    }\n    \n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n    \n    # Run GA optimization\n    ga_result <- ga(\n        type = \"real-valued\",\n        fitness = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        popSize = 50,\n        # Population size\n        maxiter = 200,\n        # Max generations\n        pmutation = 0.1,\n        # Mutation probability\n        monitor = FALSE\n    )\n    \n    return(ga_result@solution)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Genetic Algorithm\nestimated_theta <- ga_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Genetic Algorithm:\\n\")\n#> Estimated parameters (A, B, C) using Genetic Algorithm:\nprint(estimated_theta)\n#>            x1       x2         x3\n#> [1,] 4.066144 1.433886 0.00824126\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Genetic Algorithm: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"particle-swarm-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.3.4 Particle Swarm Optimization","text":"Particle Swarm Optimization (PSO) population-based global optimization algorithm inspired social behavior birds fish schools. Instead using genetic operators (like Genetic Algorithms), PSO models particles (solutions) flying search space, adjusting position based experience experience neighbors.PSO particularly useful :function nonlinear, noisy, lacks smooth gradients.function nonlinear, noisy, lacks smooth gradients.Gradient-based methods struggle due non-differentiability.Gradient-based methods struggle due non-differentiability.global minimum needed, rather local one.global minimum needed, rather local one.goal PSO find optimal solution \\(\\hat{\\theta}\\) minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Particle RepresentationEach particle represents candidate solution:\\[\n\\theta_i = (\\theta_{i1}, \\theta_{i2}, \\theta_{i3})\n\\]\\(\\theta_{ij}\\) \\(j^{th}\\) parameter particle \\(\\).2. Particle Velocity Position UpdatesEach particle moves search space velocity \\(v_i\\), updated :\\[\nv_i^{(t+1)} = \\omega v_i^{(t)} + c_1 r_1 (p_i - \\theta_i^{(t)}) + c_2 r_2 (g - \\theta_i^{(t)})\n\\]:\\(\\omega\\) inertia weight (controls exploration vs. exploitation),\\(\\omega\\) inertia weight (controls exploration vs. exploitation),\\(c_1, c_2\\) acceleration coefficients,\\(c_1, c_2\\) acceleration coefficients,\\(r_1, r_2 \\sim U(0,1)\\) random numbers,\\(r_1, r_2 \\sim U(0,1)\\) random numbers,\\(p_i\\) particle’s personal best position,\\(p_i\\) particle’s personal best position,\\(g\\) global best position.\\(g\\) global best position., position update :\\[\n\\theta_i^{(t+1)} = \\theta_i^{(t)} + v_i^{(t+1)}\n\\]process continues convergence criteria (like max number iterations minimum error) met.Particle Swarm Optimization AlgorithmInitialize particles randomly within search bounds.Set random initial velocities.Evaluate SSE particle.Update personal global best solutions.Update velocities positions using update equations.Repeat convergence.","code":"\n# Load required library\nlibrary(pso)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Particle Swarm Optimization (PSO) for Nonlinear Regression\npso_optimization <- function(x, y) {\n    # Define fitness function (minimize SSE)\n    fitness_function <- function(theta) {\n        return(sse(theta, x, y))\n    }\n    \n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n    \n    # Run PSO optimization\n    pso_result <- psoptim(\n        par = c(1, 1, 0),\n        # Initial guess\n        fn = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        control = list(maxit = 200, s = 50)  # 200 iterations, 50 particles\n    )\n    \n    return(pso_result$par)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Particle Swarm Optimization\nestimated_theta <- pso_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Particle Swarm Optimization:\\n\")\n#> Estimated parameters (A, B, C) using Particle Swarm Optimization:\nprint(estimated_theta)\n#> [1] 4.06876562 1.42765613 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Particle Swarm Optimization: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"evolutionary-strategies","chapter":"6 Non-Linear Regression","heading":"6.2.3.5 Evolutionary Strategies","text":"Evolutionary Strategies (ES) class evolutionary optimization algorithms improve solutions mutating selecting individuals based fitness. Unlike Genetic Algorithm, ES focuses self-adaptive mutation rates selection pressure rather crossover. makes ES particularly robust continuous optimization problems like nonlinear regression.ES particularly useful :function complex, noisy, lacks smooth gradients.function complex, noisy, lacks smooth gradients.Gradient-based methods fail due non-differentiability.Gradient-based methods fail due non-differentiability.adaptive approach exploration exploitation needed.adaptive approach exploration exploitation needed.goal ES find optimal solution \\(\\hat{\\theta}\\) minimizes objective function:\\[\n\\hat{\\theta} = \\arg\\min_{\\theta} SSE(\\theta),\n\\]:\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. Population RepresentationEach individual solution \\(\\theta_i\\) parameter space:\\[\n\\theta_i = (\\theta_{i1}, \\theta_{i2}, \\theta_{i3}).\n\\]population consists multiple individuals, representing different candidate parameters.2. MutationNew candidate solutions generated adding random noise:\\[\n\\theta'_i = \\theta_i + \\sigma \\mathcal{N}(0, ),\n\\]:\\(\\sigma\\) mutation step size, adapts time.\\(\\sigma\\) mutation step size, adapts time.\\(\\mathcal{N}(0, )\\) standard normal distribution.\\(\\mathcal{N}(0, )\\) standard normal distribution.3. SelectionES employs \\((\\mu, \\lambda)\\)-selection:\\((\\mu, \\lambda)\\)-ES: Select best \\(\\mu\\) solutions \\(\\lambda\\) offspring.\\((\\mu, \\lambda)\\)-ES: Select best \\(\\mu\\) solutions \\(\\lambda\\) offspring.\\((\\mu + \\lambda)\\)-ES: Combine parents offspring, selecting top \\(\\mu\\).\\((\\mu + \\lambda)\\)-ES: Combine parents offspring, selecting top \\(\\mu\\).4. Step-Size AdaptationMutation strength \\(\\sigma\\) self-adapts using 1/5 success rule:\\[\n\\sigma_{t+1} =\n\\begin{cases}\n\\sigma_t / c, & \\text{success rate } > 1/5 \\\\\n\\sigma_t \\cdot c, & \\text{success rate } < 1/5\n\\end{cases}\n\\]\\(c > 1\\) scaling factor.Evolutionary Strategies AlgorithmInitialize population \\(\\lambda\\) solutions random parameters.Set mutation step size \\(\\sigma\\).Repeat max iterations:\nGenerate \\(\\lambda\\) offspring mutating parent solutions.\nEvaluate fitness (SSE) offspring.\nSelect best \\(\\mu\\) solutions next generation.\nAdapt mutation step size based success rate.\nGenerate \\(\\lambda\\) offspring mutating parent solutions.Evaluate fitness (SSE) offspring.Select best \\(\\mu\\) solutions next generation.Adapt mutation step size based success rate.Return best solution found.","code":"\n# Load required library\nlibrary(DEoptim)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function for optimization\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions)^2))\n}\n\n# Evolutionary Strategies Optimization (Using Differential Evolution)\nes_optimization <- function(x, y) {\n    # Define fitness function (minimize SSE)\n    fitness_function <- function(theta) {\n        return(sse(theta, x, y))\n    }\n\n    # Set parameter bounds\n    lower_bounds <- c(0.1, 0.01, min(x))  # Ensure positive scaling\n    upper_bounds <- c(max(y) * 2, 10, max(x))\n\n    # Run Differential Evolution (mimicking ES)\n    es_result <- DEoptim(\n        fn = fitness_function,\n        lower = lower_bounds,\n        upper = upper_bounds,\n        # 50 individuals, 200 generations, suppress iteration output\n        DEoptim.control(NP = 50, itermax = 200, trace = F)  \n    )\n\n    return(es_result$optim$bestmem)  # Return optimized parameters\n}\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Evolutionary Strategies Optimization\nestimated_theta <- es_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Evolutionary Strategies:\\n\")\n#> Estimated parameters (A, B, C) using Evolutionary Strategies:\nprint(estimated_theta)\n#>       par1       par2       par3 \n#> 4.06876561 1.42765613 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x, y,\n    main = \"Evolutionary Strategies: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"linearization-nonlinear-regression-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4 Linearization","text":"","code":""},{"path":"non-linear-regression.html","id":"taylor-series-approximation-nonlinear-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4.1 Taylor Series Approximation","text":"Taylor Series Approximation fundamental tool nonlinear optimization, enabling local approximation complex functions using polynomial expansions. widely used linearize nonlinear models, facilitate derivative-based optimization, derive Newton-type methods.Taylor series approximation particularly useful :nonlinear function difficult compute directly.nonlinear function difficult compute directly.Optimization requires local gradient curvature information.Optimization requires local gradient curvature information.simpler, polynomial-based approximation improves computational efficiency.simpler, polynomial-based approximation improves computational efficiency.Given differentiable function \\(f(\\theta)\\), Taylor series expansion around point \\(\\theta_0\\) :\\[\nf(\\theta) = f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0) + \\frac{1}{2} f''(\\theta_0)(\\theta - \\theta_0)^2 + \\mathcal{O}((\\theta - \\theta_0)^3).\n\\]optimization, often use:First-order approximation (Linear Approximation): \\[\n   f(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0).\n   \\]Second-order approximation (Quadratic Approximation): \\[\nf(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)(\\theta - \\theta_0) + \\frac{1}{2} f''(\\theta_0)(\\theta - \\theta_0)^2.\n\\]gradient-based optimization, use Newton-Raphson update:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - [H_f(\\theta^{(k)})]^{-1} \\nabla f(\\theta^{(k)}),\n\\]:\\(\\nabla f(\\theta)\\) gradient (first derivative),\\(\\nabla f(\\theta)\\) gradient (first derivative),\\(H_f(\\theta)\\) Hessian matrix (second derivative).\\(H_f(\\theta)\\) Hessian matrix (second derivative).nonlinear regression, approximate Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]1. First-Order Approximation (Gradient Descent)gradient SSE w.r.t. parameters \\(\\theta\\) :\\[\n\\nabla SSE(\\theta) = -2 \\sum_{=1}^{n} (y_i - f(x_i; \\theta)) \\nabla f(x_i; \\theta).\n\\]Using first-order Taylor approximation, update parameters via gradient descent:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - \\alpha \\nabla SSE(\\theta^{(k)}),\n\\]\\(\\alpha\\) learning rate.2. Second-Order Approximation (Newton’s Method)Hessian matrix SSE :\\[\nH_{SSE}(\\theta) = 2 \\sum_{=1}^{n} \\nabla f(x_i; \\theta) \\nabla f(x_i; \\theta)^T - 2 \\sum_{=1}^{n} (y_i - f(x_i; \\theta)) H_f(x_i; \\theta).\n\\]Newton-Raphson update becomes:\\[\n\\theta^{(k+1)} = \\theta^{(k)} - H_{SSE}(\\theta)^{-1} \\nabla SSE(\\theta).\n\\]","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(is.na(x) |\n                      x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2, na.rm = TRUE))  # Avoid NA errors\n}\n\n# First-Order Approximation: Gradient Descent Optimization\ngradient_descent <-\n    function(x,\n             y,\n             alpha = 0.005,\n             tol = 1e-6,\n             max_iter = 5000) {\n        theta <- c(2, 1, 0)  # Initial guess\n        for (i in 1:max_iter) {\n            grad_sse <-\n                grad(function(t)\n                    sse(t, x, y), theta)  # Compute gradient\n            theta_new <-\n                theta - alpha * grad_sse  # Update parameters\n            \n            if (sum(abs(theta_new - theta)) < tol)\n                break  # Check convergence\n            theta <- theta_new\n        }\n        return(theta)\n    }\n\n# Second-Order Approximation: Newton's Method with Regularization\nnewton_method <-\n    function(x,\n             y,\n             tol = 1e-6,\n             max_iter = 100,\n             lambda = 1e-4) {\n        theta <- c(2, 1, 0)  # Initial guess\n        for (i in 1:max_iter) {\n            grad_sse <-\n                grad(function(t)\n                    sse(t, x, y), theta)  # Compute gradient\n            hessian_sse <-\n                hessian(function(t)\n                    sse(t, x, y), theta)  # Compute Hessian\n            \n            # Regularize Hessian to avoid singularity\n            hessian_reg <-\n                hessian_sse + lambda * diag(length(theta))\n            \n            # Ensure Hessian is invertible\n            if (is.na(det(hessian_reg)) ||\n                det(hessian_reg) < 1e-10) {\n                message(\"Singular Hessian encountered; increasing regularization.\")\n                lambda <- lambda * 10  # Increase regularization\n                next\n            }\n            \n            # Newton update\n            theta_new <- theta - solve(hessian_reg) %*% grad_sse\n            \n            if (sum(abs(theta_new - theta)) < tol)\n                break  # Check convergence\n            theta <- theta_new\n        }\n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Gradient Descent\nestimated_theta_gd <- gradient_descent(x, y)\n\n# Run Newton's Method with Regularization\nestimated_theta_newton <- newton_method(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Gradient Descent:\\n\")\n#> Estimated parameters (A, B, C) using Gradient Descent:\nprint(estimated_theta_gd)\n#> [1] 4.06876224 1.42766371 0.01128539\n\ncat(\"Estimated parameters (A, B, C) using Newton's Method:\\n\")\n#> Estimated parameters (A, B, C) using Newton's Method:\nprint(estimated_theta_newton)\n#>            [,1]\n#> [1,] 4.06876368\n#> [2,] 1.42766047\n#> [3,] 0.01128636\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Taylor Series Approximation: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta_gd, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"blue\",\n    lwd = 2,\n    lty = 2  # Dashed line to differentiate Gradient Descent\n)\ncurve(\n    nonlinear_model(estimated_theta_newton, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Gradient Descent\", \"Newton's Method (Regularized)\"),\n    pch = c(19, NA, NA),\n    lty = c(NA, 2, 1),\n    col = c(\"black\", \"blue\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"log-linearization-nonlinear-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.4.2 Log-Linearization","text":"Log-Linearization mathematical technique used transform nonlinear models linear models taking logarithm sides. transformation simplifies parameter estimation enables use linear regression techniques originally nonlinear functions.Log-linearization particularly useful :model exhibits exponential, power-law, logistic growth behavior.model exhibits exponential, power-law, logistic growth behavior.Linear regression methods preferred nonlinear optimization.Linear regression methods preferred nonlinear optimization.linearized version provides better interpretability computational efficiency.linearized version provides better interpretability computational efficiency.nonlinear model can often expressed form:\\[\ny = f(x; \\theta).\n\\]Applying log transformation, obtain:\\[\n\\log y = g(x; \\theta),\n\\]\\(g(x; \\theta)\\) now linear parameters. estimate \\(\\theta\\) using Ordinary Least Squares.Example 1: Exponential ModelConsider exponential growth model:\\[\ny = e^{Bx}.\n\\]Taking natural logarithm:\\[\n\\log y = \\log + Bx.\n\\]now linear \\(\\log y\\), allowing estimation via linear regression.Example 2: Power Law ModelFor power law function:\\[\ny = x^B.\n\\]Taking logs:\\[\n\\log y = \\log + B \\log x.\n\\], linearized, making solvable via OLS regression.Log-Linearization AlgorithmApply logarithm transformation dependent variable.Transform equation linear form.Use linear regression (OLS) estimate parameters.Convert parameters back original scale necessary.","code":"\n# Load required library\nlibrary(stats)\n\n# Generate synthetic data for an exponential model\nset.seed(123)\nx <- seq(1, 10, length.out = 100)\ntrue_A <- 2\ntrue_B <- 0.3\ny <- true_A * exp(true_B * x) + rnorm(length(x), sd = 0.5)\n\n# Apply logarithmic transformation\nlog_y <- log(y)\n\n# Fit linear regression model\nlog_linear_model <- lm(log_y ~ x)\n\n# Extract estimated parameters\nestimated_B <- coef(log_linear_model)[2]  # Slope in log-space\nestimated_A <-\n    exp(coef(log_linear_model)[1])  # Intercept (back-transformed)\n\n# Display results\ncat(\"Estimated parameters (A, B) using Log-Linearization:\\n\")\n#> Estimated parameters (A, B) using Log-Linearization:\nprint(c(estimated_A, estimated_B))\n#> (Intercept)           x \n#>   2.0012577   0.3001223\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Log-Linearization: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    estimated_A * exp(estimated_B * x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Log-Linear Model\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"hybrid-nonlinear-regression-optimization","chapter":"6 Non-Linear Regression","heading":"6.2.5 Hybrid","text":"","code":""},{"path":"non-linear-regression.html","id":"adaptive-levenberg-marquardt","chapter":"6 Non-Linear Regression","heading":"6.2.5.1 Adaptive Levenberg-Marquardt","text":"Levenberg-Marquardt Algorithm (LMA) powerful nonlinear least squares optimization method adaptively combines:Gauss-Newton Algorithm fast convergence near solution.Gauss-Newton Algorithm fast convergence near solution.Steepest Descent (Gradient Descent) stability far solution.Steepest Descent (Gradient Descent) stability far solution.Adaptive Levenberg-Marquardt Algorithm adjusts damping parameter \\(\\tau\\) dynamically, making efficient practice.Given objective function Sum Squared Errors (SSE):\\[\nSSE(\\theta) = \\sum_{=1}^{n} (y_i - f(x_i; \\theta))^2.\n\\]update rule LMA :\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})' \\mathbf{F}(\\hat{\\theta}^{(j)}) + \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}.\n\\]:\\(\\tau\\) adaptive damping parameter.\\(\\tau\\) adaptive damping parameter.\\(\\mathbf{}_{p \\times p}\\) identity matrix.\\(\\mathbf{}_{p \\times p}\\) identity matrix.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Jacobian matrix partial derivatives.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector.\\(\\alpha_j\\) learning rate.\\(\\alpha_j\\) learning rate.key adaptation rule \\(\\tau\\):new step decreases SSE, reduce \\(\\tau\\): \\[\n  \\tau \\gets \\tau / 10.\n  \\]new step decreases SSE, reduce \\(\\tau\\): \\[\n  \\tau \\gets \\tau / 10.\n  \\]Otherwise, increase \\(\\tau\\) ensure stability: \\[\n  \\tau \\gets 10\\tau.\n  \\]Otherwise, increase \\(\\tau\\) ensure stability: \\[\n  \\tau \\gets 10\\tau.\n  \\]adjustment ensures balance stability efficiency.Adaptive Levenberg-Marquardt AlgorithmInitialize parameters \\(\\theta_0\\), damping factor \\(\\tau\\).Compute Jacobian \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\).Compute step direction using modified Gauss-Newton update.Adjust \\(\\tau\\) dynamically:\nDecrease \\(\\tau\\) SSE improves.\nIncrease \\(\\tau\\) SSE worsens.\nDecrease \\(\\tau\\) SSE improves.Increase \\(\\tau\\) SSE worsens.Repeat convergence.","code":"\n# Load required libraries\nlibrary(numDeriv)\n\n# Define a numerically stable logistic function\nsafe_exp <- function(x) {\n    return(ifelse(x > 700, Inf, exp(pmin(x, 700))))  # Prevent overflow\n}\n\n# Define the logistic growth model\nnonlinear_model <- function(theta, x) {\n    return(theta[1] / (1 + safe_exp(-theta[2] * (x - theta[3]))))\n}\n\n# Define the Sum of Squared Errors (SSE) function\nsse <- function(theta, x, y) {\n    predictions <- nonlinear_model(theta, x)\n    return(sum((y - predictions) ^ 2))\n}\n\n# Adaptive Levenberg-Marquardt Optimization\nadaptive_lm_optimization <-\n    function(x, y, tol = 1e-6, max_iter = 100) {\n        theta <- c(2, 1, 0)  # Initial parameter guess\n        tau <- 1e-3  # Initial damping parameter\n        alpha <- 1  # Step size scaling\n        iter <- 0\n        \n        while (iter < max_iter) {\n            iter <- iter + 1\n            \n            # Compute Jacobian numerically\n            J <- jacobian(function(t)\n                nonlinear_model(t, x), theta)\n            \n            # Compute gradient of SSE\n            residuals <- y - nonlinear_model(theta, x)\n            grad_sse <- -2 * t(J) %*% residuals\n            \n            # Compute Hessian approximation\n            H <- 2 * t(J) %*% J + tau * diag(length(theta))\n            \n            # Compute parameter update step\n            delta_theta <- solve(H, grad_sse)\n            \n            # Trial step\n            theta_new <- theta - alpha * delta_theta\n            \n            # Compute SSE for new parameters\n            if (sse(theta_new, x, y) < sse(theta, x, y)) {\n                # Accept step, decrease tau\n                theta <- theta_new\n                tau <- tau / 10\n            } else {\n                # Reject step, increase tau\n                tau <- tau * 10\n            }\n            \n            # Check convergence\n            if (sum(abs(delta_theta)) < tol)\n                break\n        }\n        \n        return(theta)\n    }\n\n# Generate synthetic data\nset.seed(123)\nx <- seq(-5, 5, length.out = 100)\ntrue_theta <- c(4, 1.5, 0)  # True parameters (A, B, C)\ny <- nonlinear_model(true_theta, x) + rnorm(length(x), sd = 0.3)\n\n# Run Adaptive Levenberg-Marquardt Optimization\nestimated_theta <- adaptive_lm_optimization(x, y)\n\n# Display results\ncat(\"Estimated parameters (A, B, C) using Adaptive Levenberg-Marquardt:\\n\")\n#> Estimated parameters (A, B, C) using Adaptive Levenberg-Marquardt:\nprint(estimated_theta)\n#>            [,1]\n#> [1,] 4.06876562\n#> [2,] 1.42765612\n#> [3,] 0.01128767\n\n# Plot data and fitted curve\nplot(\n    x,\n    y,\n    main = \"Adaptive Levenberg-Marquardt: Nonlinear Regression Optimization\",\n    pch = 19,\n    cex = 0.5,\n    xlab = \"x\",\n    ylab = \"y\"\n)\ncurve(\n    nonlinear_model(estimated_theta, x),\n    from = min(x),\n    to = max(x),\n    add = TRUE,\n    col = \"red\",\n    lwd = 2\n)\nlegend(\n    \"topleft\",\n    legend = c(\"Data\", \"Fitted Curve\"),\n    pch = c(19, NA),\n    lty = c(NA, 1),\n    col = c(\"black\", \"red\")\n)"},{"path":"non-linear-regression.html","id":"comparison-of-nonlinear-optimizers","chapter":"6 Non-Linear Regression","heading":"6.2.6 Comparison of Nonlinear Optimizers","text":"","code":"\n# ALL-IN-ONE R SCRIPT COMPARING MULTIPLE NONLINEAR-REGRESSION OPTIMIZERS\n\nlibrary(minpack.lm) # nlsLM (Levenberg-Marquardt)\nlibrary(dfoptim)    # Powell (nmk), Hooke-Jeeves\nlibrary(nloptr)     # trust-region reflective\nlibrary(GA)         # genetic algorithm\nlibrary(DEoptim)    # differential evolution\nlibrary(GenSA)      # simulated annealing\nlibrary(pso)        # particle swarm\nlibrary(MASS)       # for ginv fallback\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# -- 1) DEFINE MODELS (SIMPLE VS COMPLEX) ---\n\n# 3-parameter logistic\nf_logistic <- function(theta, x) {\n  A <- theta[1]\n  B <- theta[2]\n  C <- theta[3]\n  A / (1 + exp(-B * (x - C)))\n}\nsse_logistic <-\n  function(theta, x, y)\n    sum((y - f_logistic(theta, x)) ^ 2)\n\n# 4-parameter \"extended\" model\nf_complex <- function(theta, x) {\n  A <- theta[1]\n  B <- theta[2]\n  C <- theta[3]\n  D <- theta[4]\n  A / (1 + exp(-B * (x - C))) + D * exp(-0.5 * x)\n}\nsse_complex <-\n  function(theta, x, y)\n    sum((y - f_complex(theta, x)) ^ 2)\n\n# Generate synthetic data\nset.seed(123)\nn <- 100\nx_data <- seq(-5, 5, length.out = n)\n# \"simple\" scenario\ntrue_theta_simple <- c(4, 1.5, 0)\ny_data_simple <-\n  f_logistic(true_theta_simple, x_data) + rnorm(n, sd = 0.3)\n# \"complex\" scenario\ntrue_theta_complex <- c(4, 1.2, -1, 0.5)\ny_data_complex <-\n  f_complex(true_theta_complex, x_data) + rnorm(n, sd = 0.3)\n\n# -- 2) OPTIMIZERS (EXCEPT BISECTION) ----\n#\n# All methods share signature:\n#   FUN(par, x, y, sse_fn, model_fn, lower=NULL, upper=NULL, ...)\n# Some do not strictly use lower/upper if unconstrained.\n\n# 2.1 Gauss–Newton\ngauss_newton_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL,\n                             max_iter = 100,\n                             tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    nP  <- length(theta)\n    Fmat <- matrix(0, nrow = length(x), ncol = nP)\n    for (p in seq_len(nP)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      Fmat[, p] <-\n        (model_fn(pert, x) - model_fn(theta, x)) / eps\n    }\n    r <- y - model_fn(theta, x)\n    delta <- tryCatch(\n      solve(t(Fmat) %*% Fmat, t(Fmat) %*% r),\n      error = function(e) {\n        # fallback to pseudoinverse\n        MASS::ginv(t(Fmat) %*% Fmat) %*% (t(Fmat) %*% r)\n      }\n    )\n    theta_new <- theta + delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.2 Modified Gauss-Newton (with step halving)\nmodified_gauss_newton_fit <- function(par,\n                                      x,\n                                      y,\n                                      sse_fn,\n                                      model_fn,\n                                      lower = NULL,\n                                      upper = NULL,\n                                      max_iter = 100,\n                                      tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    nP  <- length(theta)\n    Fmat <- matrix(0, nrow = length(x), ncol = nP)\n    for (p in seq_len(nP)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      Fmat[, p] <-\n        (model_fn(pert, x) - model_fn(theta, x)) / eps\n    }\n    r <- y - model_fn(theta, x)\n    lhs <- t(Fmat) %*% Fmat\n    rhs <- t(Fmat) %*% r\n    delta <- tryCatch(\n      solve(lhs, rhs),\n      error = function(e)\n        MASS::ginv(lhs) %*% rhs\n    )\n    sse_old <- sse_fn(theta, x, y)\n    alpha <- 1\n    for (k in 1:10) {\n      new_sse <- sse_fn(theta + alpha * delta, x, y)\n      if (new_sse < sse_old)\n        break\n      alpha <- alpha / 2\n    }\n    theta_new <- theta + alpha * delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.3 Steepest Descent (Gradient Descent)\nsteepest_descent_fit <- function(par,\n                                 x,\n                                 y,\n                                 sse_fn,\n                                 model_fn,\n                                 lower = NULL,\n                                 upper = NULL,\n                                 lr = 0.001,\n                                 max_iter = 5000,\n                                 tol = 1e-6)\n{\n  theta <- par\n  for (iter in seq_len(max_iter)) {\n    eps <- 1e-6\n    f0  <- sse_fn(theta, x, y)\n    grad <- numeric(length(theta))\n    for (p in seq_along(theta)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      grad[p] <- (sse_fn(pert, x, y) - f0) / eps\n    }\n    theta_new <- theta - lr * grad\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.4 Levenberg–Marquardt (nlsLM)\nlm_fit <- function(par,\n                   x,\n                   y,\n                   sse_fn,\n                   model_fn,\n                   lower = NULL,\n                   upper = NULL,\n                   form = c(\"simple\", \"complex\"))\n{\n  form <- match.arg(form)\n  if (form == \"simple\") {\n    fit <- nlsLM(y ~ A / (1 + exp(-B * (x - C))),\n                 start = list(A = par[1],\n                              B = par[2],\n                              C = par[3]))\n  } else {\n    fit <- nlsLM(y ~ A / (1 + exp(-B * (x - C))) + D * exp(-0.5 * x),\n                 start = list(\n                   A = par[1],\n                   B = par[2],\n                   C = par[3],\n                   D = par[4]\n                 ))\n  }\n  coef(fit)\n}\n\n# 2.5 Newton–Raphson (with numeric Hessian, fallback if singular)\nnewton_raphson_fit <- function(par,\n                               x,\n                               y,\n                               sse_fn,\n                               model_fn,\n                               lower = NULL,\n                               upper = NULL,\n                               max_iter = 50,\n                               tol = 1e-6)\n{\n  theta <- par\n  for (i in seq_len(max_iter)) {\n    eps <- 1e-6\n    f0  <- sse_fn(theta, x, y)\n    grad <- numeric(length(theta))\n    for (p in seq_along(theta)) {\n      pert <- theta\n      pert[p] <- pert[p] + eps\n      grad[p] <- (sse_fn(pert, x, y) - f0) / eps\n    }\n    Hess <- matrix(0, length(theta), length(theta))\n    for (p in seq_along(theta)) {\n      pert_p <- theta\n      pert_p[p] <- pert_p[p] + eps\n      f_p <- sse_fn(pert_p, x, y)\n      for (q in seq_along(theta)) {\n        pert_q <- pert_p\n        pert_q[q] <- pert_q[q] + eps\n        Hess[p, q] <- (sse_fn(pert_q, x, y) -\n                         f_p - (f0 - sse_fn(theta, x, y))) / (eps ^\n                                                                2)\n      }\n    }\n    delta <- tryCatch(\n      solve(Hess, grad),\n      error = function(e)\n        MASS::ginv(Hess) %*% grad\n    )\n    theta_new <- theta - delta\n    if (sum(abs(theta_new - theta)) < tol)\n      break\n    theta <- theta_new\n  }\n  theta\n}\n\n# 2.6 Quasi–Newton (BFGS via optim)\nquasi_newton_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- optim(par, fn, method = \"BFGS\")\n  res$par\n}\n\n# 2.7 Trust-region reflective (nloptr)\ntrust_region_fit <- function(par,\n                             x,\n                             y,\n                             sse_fn,\n                             model_fn,\n                             lower = NULL,\n                             upper = NULL)\n{\n  # numeric gradient\n  grad_numeric <- function(pp, eps = 1e-6) {\n    g  <- numeric(length(pp))\n    f0 <- sse_fn(pp, x, y)\n    for (i in seq_along(pp)) {\n      p2 <- pp\n      p2[i] <- p2[i] + eps\n      g[i] <- (sse_fn(p2, x, y) - f0) / eps\n    }\n    g\n  }\n  eval_f <- function(pp) {\n    val <- sse_fn(pp, x, y)\n    gr  <- grad_numeric(pp)\n    list(objective = val, gradient = gr)\n  }\n  lb <- if (is.null(lower))\n    rep(-Inf, length(par))\n  else\n    lower\n  ub <- if (is.null(upper))\n    rep(Inf, length(par))\n  else\n    upper\n  res <- nloptr(\n    x0 = par,\n    eval_f = eval_f,\n    lb = lb,\n    ub = ub,\n    opts = list(\n      algorithm = \"NLOPT_LD_TNEWTON\",\n      maxeval = 500,\n      xtol_rel = 1e-6\n    )\n  )\n  res$solution\n}\n\n# 2.8 Grid search\ngrid_search_fit <- function(par,\n                            x,\n                            y,\n                            sse_fn,\n                            model_fn,\n                            lower = NULL,\n                            upper = NULL,\n                            grid_defs = NULL)\n{\n  if (is.null(grid_defs))\n    stop(\"Must provide grid_defs for multi-parameter grid search.\")\n  g <- expand.grid(grid_defs)\n  g$SSE <-\n    apply(g, 1, function(rowp)\n      sse_fn(as.numeric(rowp), x, y))\n  best_idx <- which.min(g$SSE)\n  as.numeric(g[best_idx, seq_along(grid_defs)])\n}\n\n# 2.9 Nelder-Mead\nnelder_mead_fit <- function(par,\n                            x,\n                            y,\n                            sse_fn,\n                            model_fn,\n                            lower = NULL,\n                            upper = NULL)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- optim(par, fn, method = \"Nelder-Mead\")\n  res$par\n}\n\n# 2.10 Powell’s method (dfoptim::nmk for unconstrained)\npowell_fit <-\n  function(par,\n           x,\n           y,\n           sse_fn,\n           model_fn,\n           lower = NULL,\n           upper = NULL) {\n    fn <- function(pp)\n      sse_fn(pp, x, y)\n    dfoptim::nmk(par, fn)$par\n  }\n\n# 2.11 Hooke-Jeeves (dfoptim::hjkb)\nhooke_jeeves_fit <-\n  function(par,\n           x,\n           y,\n           sse_fn,\n           model_fn,\n           lower = NULL,\n           upper = NULL) {\n    fn <- function(pp)\n      sse_fn(pp, x, y)\n    dfoptim::hjkb(par, fn)$par\n  }\n\n# 2.12 Random Search\nrandom_search_fit <- function(par,\n                              x,\n                              y,\n                              sse_fn,\n                              model_fn,\n                              lower,\n                              upper,\n                              max_iter = 2000,\n                              ...)\n{\n  best_par <- NULL\n  best_sse <- Inf\n  dimp <- length(lower)\n  for (i in seq_len(max_iter)) {\n    candidate <- runif(dimp, min = lower, max = upper)\n    val <- sse_fn(candidate, x, y)\n    if (val < best_sse) {\n      best_sse <- val\n      best_par <- candidate\n    }\n  }\n  best_par\n}\n\n# 2.13 Differential Evolution (DEoptim)\ndiff_evo_fit <- function(par,\n                         x,\n                         y,\n                         sse_fn,\n                         model_fn,\n                         lower,\n                         upper,\n                         max_iter = 100,\n                         ...)\n{\n  fn <- function(v)\n    sse_fn(v, x, y)\n  out <- DEoptim(fn,\n                 lower = lower,\n                 upper = upper,\n                 DEoptim.control(NP = 50, itermax = max_iter, trace = F))\n  out$optim$bestmem\n}\n\n# 2.14 Simulated Annealing (GenSA)\nsim_anneal_fit <- function(par,\n                           x,\n                           y,\n                           sse_fn,\n                           model_fn,\n                           lower = NULL,\n                           upper = NULL,\n                           ...)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  lb <- if (is.null(lower))\n    rep(-Inf, length(par))\n  else\n    lower\n  ub <- if (is.null(upper))\n    rep(Inf, length(par))\n  else\n    upper\n  # GenSA requires: GenSA(par, fn, lower, upper, control=list(...))\n  out <-\n    GenSA(\n      par,\n      fn,\n      lower = lb,\n      upper = ub,\n      control = list(max.call = 10000)\n    )\n  out$par\n}\n\n# 2.15 Genetic Algorithm (GA)\ngenetic_fit <- function(par,\n                        x,\n                        y,\n                        sse_fn,\n                        model_fn,\n                        lower,\n                        upper,\n                        max_iter = 100,\n                        ...)\n{\n  fitness_fun <- function(pp)\n    - sse_fn(pp, x, y)\n  gares <- ga(\n    type = \"real-valued\",\n    fitness = fitness_fun,\n    lower = lower,\n    upper = upper,\n    popSize = 50,\n    maxiter = max_iter,\n    run = 50\n  )\n  gares@solution[1,]\n}\n\n# 2.16 Particle Swarm (pso)\nparticle_swarm_fit <- function(par,\n                               x,\n                               y,\n                               sse_fn,\n                               model_fn,\n                               lower,\n                               upper,\n                               max_iter = 100,\n                               ...)\n{\n  fn <- function(pp)\n    sse_fn(pp, x, y)\n  res <- psoptim(\n    par = (lower + upper) / 2,\n    fn = fn,\n    lower = lower,\n    upper = upper,\n    control = list(maxit = max_iter)\n  )\n  res$par\n}\n\n\n# -- 3) RUN METHOD WRAPPER ---\nrun_method <- function(method_name,\n                       FUN,\n                       par_init,\n                       x,\n                       y,\n                       sse_fn,\n                       model_fn,\n                       lower = NULL,\n                       upper = NULL,\n                       ...)\n{\n  mb <- microbenchmark(result = {\n    out <- FUN(par_init, x, y, sse_fn, model_fn, lower, upper, ...)\n    out\n  }, times = 1)\n  final_par <-\n    FUN(par_init, x, y, sse_fn, model_fn, lower, upper, ...)\n  if (is.null(final_par)) {\n    # e.g. placeholders that return NULL\n    return(data.frame(\n      Method = method_name,\n      Parameters = \"N/A\",\n      SSE = NA,\n      Time_ms = NA\n    ))\n  }\n  data.frame(\n    Method     = method_name,\n    Parameters = paste(round(final_par, 4), collapse = \", \"),\n    SSE        = round(sse_fn(final_par, x, y), 6),\n    Time_ms    = median(mb$time) / 1e6\n  )\n}\n\n# -- 4) MASTER FUNCTION TO COMPARE ALL METHODS (SIMPLE / COMPLEX) ---\n\ncompare_all_methods <- function(is_complex = FALSE) {\n  if (!is_complex) {\n    # SIMPLE (3-param logistic)\n    x <- x_data\n    y <- y_data_simple\n    sse_fn   <- sse_logistic\n    model_fn <- f_logistic\n    init_par <- c(3, 1, 0.5)\n    grid_defs <- list(\n      A = seq(2, 6, length.out = 10),\n      B = seq(0.5, 2, length.out = 10),\n      C = seq(-1, 1, length.out = 10)\n    )\n    lower <- c(1, 0.1,-3)\n    upper <- c(6, 3,  3)\n    lm_form <- \"simple\"\n  } else {\n    # COMPLEX (4-param model)\n    x <- x_data\n    y <- y_data_complex\n    sse_fn   <- sse_complex\n    model_fn <- f_complex\n    init_par <- c(3, 1,-0.5, 0.2)\n    grid_defs <- list(\n      A = seq(2, 6, length.out = 8),\n      B = seq(0.5, 2, length.out = 8),\n      C = seq(-2, 2, length.out = 8),\n      D = seq(0, 2, length.out = 8)\n    )\n    lower <- c(1, 0.1,-3, 0)\n    upper <- c(6, 3,  3, 2)\n    lm_form <- \"complex\"\n  }\n  \n  # RUN each method\n  out <- bind_rows(\n    run_method(\n      \"Gauss-Newton\",\n      gauss_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Modified Gauss-Newton\",\n      modified_gauss_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Steepest Descent\",\n      steepest_descent_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Levenberg-Marquardt (nlsLM)\",\n      lm_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      form = lm_form\n    ),\n    run_method(\n      \"Newton-Raphson\",\n      newton_raphson_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Quasi-Newton (BFGS)\",\n      quasi_newton_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Trust-region Reflective\",\n      trust_region_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper\n    ),\n    run_method(\n      \"Grid Search\",\n      grid_search_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      grid_defs = grid_defs\n    ),\n    run_method(\n      \"Nelder-Mead\",\n      nelder_mead_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\"Powell's method\",\n               powell_fit,\n               init_par,\n               x,\n               y,\n               sse_fn,\n               model_fn),\n    run_method(\n      \"Hooke-Jeeves\",\n      hooke_jeeves_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn\n    ),\n    run_method(\n      \"Random Search\",\n      random_search_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 1000\n    ),\n    run_method(\n      \"Differential Evolution\",\n      diff_evo_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    ),\n    run_method(\n      \"Simulated Annealing\",\n      sim_anneal_fit,\n      init_par,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper\n    ),\n    run_method(\n      \"Genetic Algorithm\",\n      genetic_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    ),\n    run_method(\n      \"Particle Swarm\",\n      particle_swarm_fit,\n      NULL,\n      x,\n      y,\n      sse_fn,\n      model_fn,\n      lower,\n      upper,\n      max_iter = 50\n    )\n  )\n  out\n}\n\n# -- 5) RUN & VISUALIZE ----\n\n# Compare \"simple\" logistic (3 params)\nresults_simple  <- compare_all_methods(is_complex = FALSE)\nresults_simple$Problem <- \"Simple\"\n\n# Compare \"complex\" (4 params)\nresults_complex <- compare_all_methods(is_complex = TRUE)\nresults_complex$Problem <- \"Complex\"\n\n# Combine\nall_results <- rbind(results_simple, results_complex)\n# print(all_results)\n# DT::datatable(all_results)\n\n# Example: SSE by method & problem\nggplot(all_results, aes(x = Method, y = log(SSE), fill = Problem)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal(base_size = 11) +\n  labs(title = \"Comparison of SSE by Method & Problem Complexity\",\n       x = \"\", y = \"Log(Sum of Squared Errors)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Example: Time (ms) by method & problem\nggplot(all_results, aes(x = Method, y = Time_ms, fill = Problem)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal(base_size = 11) +\n  labs(title = \"Comparison of Computation Time by Method & Problem Complexity\",\n       x = \"\", y = \"Time (ms)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"},{"path":"non-linear-regression.html","id":"practical-considerations-2","chapter":"6 Non-Linear Regression","heading":"6.3 Practical Considerations","text":"optimization algorithms converge, require good initial estimates parameters. choice starting values, constraints, complexity model play role whether optimization algorithm successfully finds suitable solution.","code":""},{"path":"non-linear-regression.html","id":"selecting-starting-values","chapter":"6 Non-Linear Regression","heading":"6.3.1 Selecting Starting Values","text":"Choosing good starting values can significantly impact efficiency success optimization algorithms. Several approaches can used:Prior theoretical information: prior knowledge parameters available, incorporated choice initial values.Grid search graphical inspection \\(SSE(\\theta)\\): Evaluating sum squared errors (SSE) across grid possible values can help identify promising starting points.Ordinary Least Squares estimates: linear approximation model exists, using OLS obtain initial estimates can effective.Model interpretation: Understanding structure behavior model can provide intuition reasonable starting values.Expected Value Parameterization: Reformulating model based expected values may improve interpretability numerical stability estimation.","code":""},{"path":"non-linear-regression.html","id":"grid-search-for-optimal-starting-values","chapter":"6 Non-Linear Regression","heading":"6.3.1.1 Grid Search for Optimal Starting Values","text":"Note: nls_multstart package can perform grid search efficiently without requiring manual looping.Visualizing Prediction IntervalsOnce model fitted, useful visualize prediction intervals assess model uncertainty.","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate x as 100 integers using seq function\nx <- seq(0, 100, 1)\n\n# Generate coefficients for exponential function\na <- runif(1, 0, 20)  # Random coefficient a\nb <- runif(1, 0.005, 0.075)  # Random coefficient b\nc <- runif(101, 0, 5)  # Random noise\n\n# Generate y as a * e^(b*x) + c\ny <- a * exp(b * x) + c\n\n# Print the generated parameters\ncat(\"Generated coefficients:\\n\")\n#> Generated coefficients:\ncat(\"a =\", a, \"\\n\")\n#> a = 5.75155\ncat(\"b =\", b, \"\\n\")\n#> b = 0.06018136\n\n# Define our data frame\ndatf <- data.frame(x, y)\n\n# Define our model function\nmod <- function(a, b, x) {\n  a * exp(b * x)\n}\n# Ensure all y values are positive (avoid log issues)\ny_adj <-\n  ifelse(y > 0, y, min(y[y > 0]) + 1e-3)  # Shift small values slightly\n\n# Create adjusted dataframe\ndatf_adj <- data.frame(x, y_adj)\n\n# Linearize by taking log(y)\nlin_mod <- lm(log(y_adj) ~ x, data = datf_adj)\n\n# Extract starting values\nastrt <-\n  exp(coef(lin_mod)[1])  # Convert intercept back from log scale\nbstrt <- coef(lin_mod)[2]  # Slope remains the same\ncat(\"Starting values for non-linear fit:\\n\")\nprint(c(astrt, bstrt))\n\n# Fit nonlinear model with these starting values\nnlin_mod <- nls(y ~ mod(a, b, x),\n                start = list(a = astrt, b = bstrt),\n                data = datf)\n\n# Model summary\nsummary(nlin_mod)\n\n# Plot original data\nplot(\n  x,\n  y,\n  main = \"Exponential Growth Fit\",\n  col = \"blue\",\n  pch = 16,\n  xlab = \"x\",\n  ylab = \"y\"\n)\n\n# Add fitted curve in red\nlines(x, predict(nlin_mod), col = \"red\", lwd = 2)\n\n# Add legend\nlegend(\n  \"topleft\",\n  legend = c(\"Original Data\", \"Fitted Model\"),\n  col = c(\"blue\", \"red\"),\n  pch = c(16, NA),\n  lwd = c(NA, 2)\n)\n# Define grid of possible parameter values\naseq <- seq(10, 18, 0.2)\nbseq <- seq(0.001, 0.075, 0.001)\n\nna <- length(aseq)\nnb <- length(bseq)\nSSout <- matrix(0, na * nb, 3)  # Matrix to store SSE values\ncnt <- 0\n\n# Evaluate SSE across grid\nfor (k in 1:na) {\n  for (j in 1:nb) {\n    cnt <- cnt + 1\n    ypred <-\n      # Evaluate model at these parameter values\n      mod(aseq[k], bseq[j], x)  \n    \n    # Compute SSE\n    ss <- sum((y - ypred) ^ 2)  \n    SSout[cnt, 1] <- aseq[k]\n    SSout[cnt, 2] <- bseq[j]\n    SSout[cnt, 3] <- ss\n  }\n}\n\n# Identify optimal starting values\nmn_indx <- which.min(SSout[, 3])\nastrt <- SSout[mn_indx, 1]\nbstrt <- SSout[mn_indx, 2]\n\n# Fit nonlinear model using optimal starting values\nnlin_modG <-\n  nls(y ~ mod(a, b, x), start = list(a = astrt, b = bstrt))\n\n# Display model results\nsummary(nlin_modG)\n#> \n#> Formula: y ~ mod(a, b, x)\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> a 5.889e+00  1.986e-02   296.6   <2e-16 ***\n#> b 5.995e-02  3.644e-05  1645.0   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.135 on 99 degrees of freedom\n#> \n#> Number of iterations to convergence: 4 \n#> Achieved convergence tolerance: 7.204e-06\n# Load necessary package\nlibrary(nlstools)\n\n# Plot fitted model with confidence and prediction intervals\nplotFit(\n  nlin_modG,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = datf\n)  "},{"path":"non-linear-regression.html","id":"using-programmed-starting-values-in-nls","chapter":"6 Non-Linear Regression","heading":"6.3.1.2 Using Programmed Starting Values in nls","text":"Many nonlinear models well-established functional forms, allowing programmed starting values nls function. example, models logistic growth asymptotic regression built-self-starting functions.explore available self-starting models R, use:command lists functions names starting SS, typically denote self-starting functions nonlinear regression.","code":"\napropos(\"^SS\")\n#>  [1] \"ss\"           \"SSasymp\"      \"SSasympOff\"   \"SSasympOrig\"  \"SSbiexp\"     \n#>  [6] \"SSD\"          \"sse\"          \"sse_complex\"  \"sse_df\"       \"sse_final\"   \n#> [11] \"sse_logistic\" \"sse_values\"   \"SSfol\"        \"SSfpl\"        \"SSgompertz\"  \n#> [16] \"SSlogis\"      \"SSmicmen\"     \"SSout\"        \"SSweibull\""},{"path":"non-linear-regression.html","id":"custom-self-starting-functions","chapter":"6 Non-Linear Regression","heading":"6.3.1.3 Custom Self-Starting Functions","text":"model match built-nls functions, can define self-starting function. Self-starting functions R automate process estimating initial values, helps fitting nonlinear models efficiently.needed, self-starting function :Define nonlinear equation.Define nonlinear equation.Implement method computing starting values.Implement method computing starting values.Return function structure appropriate format.Return function structure appropriate format.","code":""},{"path":"non-linear-regression.html","id":"handling-constrained-parameters","chapter":"6 Non-Linear Regression","heading":"6.3.2 Handling Constrained Parameters","text":"cases, parameters must satisfy constraints (e.g., \\(\\theta_i > \\) \\(< \\theta_i < b\\)). following strategies help address constrained parameter estimation:Fit model without constraints first: unconstrained parameter estimates satisfy desired constraints, action needed.Re-parameterization: estimated parameters violate constraints, consider re-parameterizing model naturally enforce required bounds.","code":""},{"path":"non-linear-regression.html","id":"failure-to-converge","chapter":"6 Non-Linear Regression","heading":"6.3.3 Failure to Converge","text":"Several factors can cause algorithm fail converge:“flat” SSE function: sum squared errors \\(SSE(\\theta)\\) relatively constant neighborhood minimum, algorithm may struggle locate optimal solution.Poor starting values: Trying different better initial values can help.Overly complex models: model complex relative data, consider simplifying .","code":""},{"path":"non-linear-regression.html","id":"convergence-to-a-local-minimum","chapter":"6 Non-Linear Regression","heading":"6.3.4 Convergence to a Local Minimum","text":"Linear least squares models well-defined, unique minimum SSE function quadratic:\\[ SSE(\\theta) = (Y - X\\beta)'(Y - X\\beta) \\]Nonlinear least squares models may multiple local minima.Testing different starting values can help identify global minimum.Graphing \\(SSE(\\theta)\\) function individual parameters (feasible) can provide insights.Alternative optimization algorithms Genetic Algorithm particle swarm optimization may useful non-convex problems.","code":""},{"path":"non-linear-regression.html","id":"model-adequacy-and-estimation-considerations","chapter":"6 Non-Linear Regression","heading":"6.3.5 Model Adequacy and Estimation Considerations","text":"Assessing adequacy nonlinear model involves checking nonlinearity, goodness fit, residual behavior. Unlike linear models, nonlinear models always direct equivalent \\(R^2\\), issues collinearity, leverage, residual heteroscedasticity must carefully evaluated.","code":""},{"path":"non-linear-regression.html","id":"components-of-nonlinearity","chapter":"6 Non-Linear Regression","heading":"6.3.5.1 Components of Nonlinearity","text":"Bates Watts (1980) defines two key aspects nonlinearity statistical modeling:Intrinsic NonlinearityMeasures bending twisting function \\(f(\\theta)\\).Assumes function relatively flat (planar) neighborhood \\(\\hat{\\theta}\\).severe, distribution residuals distorted.Leads :\nSlow convergence optimization algorithms.\nDifficulties identifying parameter estimates.\nSlow convergence optimization algorithms.Difficulties identifying parameter estimates.Solution approaches:\nHigher-order Taylor expansions estimation.\nBayesian methods parameter estimation.\nHigher-order Taylor expansions estimation.Bayesian methods parameter estimation.Parameter-Effects NonlinearityMeasures curvature (nonlinearity) depends parameterization.Measures curvature (nonlinearity) depends parameterization.Strong parameter effects nonlinearity can cause problems inference \\(\\hat{\\theta}\\).Strong parameter effects nonlinearity can cause problems inference \\(\\hat{\\theta}\\).Can assessed using:\nrms.curv function MASS.\nBootstrap-based inference.\nCan assessed using:rms.curv function MASS.rms.curv function MASS.Bootstrap-based inference.Bootstrap-based inference.Solution: Try reparameterization stabilize function.Solution: Try reparameterization stabilize function.","code":"\n# Check intrinsic curvature\nmodD <- deriv3(~ a * exp(b * x), c(\"a\", \"b\"), function(a, b, x) NULL)\n\nnlin_modD <- nls(y ~ modD(a, b, x),\n                 start = list(a = astrt, b = bstrt),\n                 data = datf)\n\nrms.curv(nlin_modD)  # Function from the MASS package to assess curvature\n#> Parameter effects: c^theta x sqrt(F) = 0.0564 \n#>         Intrinsic: c^iota  x sqrt(F) = 9e-04"},{"path":"non-linear-regression.html","id":"goodness-of-fit-in-nonlinear-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.2 Goodness of Fit in Nonlinear Models","text":"linear regression, use standard coefficient determination ($R^2$):\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\n\\]​:\\(SSR\\) = Regression Sum Squares\\(SSR\\) = Regression Sum Squares\\(SSE\\) = Error Sum Squares\\(SSE\\) = Error Sum Squares\\(SSTO\\) = Total Sum Squares\\(SSTO\\) = Total Sum SquaresHowever, nonlinear models, error model sum squares necessarily add total corrected sum squares:\\[\nSSR + SSE \\neq SST\n\\]Thus, \\(R^2\\) directly valid nonlinear case. Instead, use pseudo-\\(R^2\\):\\[\nR^2_{pseudo} = 1 - \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\]Unlike true \\(R^2\\), interpreted proportion variability explained model.Unlike true \\(R^2\\), interpreted proportion variability explained model.used relative model comparison (e.g., comparing different nonlinear models).used relative model comparison (e.g., comparing different nonlinear models).","code":""},{"path":"non-linear-regression.html","id":"residual-analysis-in-nonlinear-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.3 Residual Analysis in Nonlinear Models","text":"Residual plots help assess model adequacy, particularly intrinsic curvature small.nonlinear models, studentized residuals :\\[\nr_i = \\frac{e_i}{s \\sqrt{1-\\hat{c}_i}}\n\\]:\\(e_i\\) = residual observation \\(\\)\\(e_i\\) = residual observation \\(\\)\\(\\hat{c}_i\\) = \\(\\)th diagonal element tangent-plane hat matrix:\\(\\hat{c}_i\\) = \\(\\)th diagonal element tangent-plane hat matrix:\\[\n\\mathbf{\\hat{H} = F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\n\\]","code":"\n# Residual diagnostics for nonlinear models\nlibrary(nlstools)\nresid_nls <- nlsResiduals(nlin_modD)\n\n# Generate residual plots\nplot(resid_nls)"},{"path":"non-linear-regression.html","id":"potential-issues-in-nonlinear-regression-models","chapter":"6 Non-Linear Regression","heading":"6.3.5.4 Potential Issues in Nonlinear Regression Models","text":"","code":""},{"path":"non-linear-regression.html","id":"collinearity-1","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.1 Collinearity","text":"Measures correlated model’s predictors .Measures correlated model’s predictors .nonlinear models, collinearity assessed using condition number :nonlinear models, collinearity assessed using condition number :\\[\n\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\n\\]condition number > 30, collinearity concern.condition number > 30, collinearity concern.Solution: Consider reparameterization (Magel Hertsgaard 1987).Solution: Consider reparameterization (Magel Hertsgaard 1987).","code":""},{"path":"non-linear-regression.html","id":"leverage","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.2 Leverage","text":"Similar leverage Ordinary Least Squares.Similar leverage Ordinary Least Squares.nonlinear models, leverage assessed using tangent-plane hat matrix:nonlinear models, leverage assessed using tangent-plane hat matrix:\\[\n\\mathbf{\\hat{H} = F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\n\\]Solution: Identify influential points assess impact parameter estimates (St Laurent Cook 1992).","code":""},{"path":"non-linear-regression.html","id":"heterogeneous-errors","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.3 Heterogeneous Errors","text":"Non-constant variance across observations.Non-constant variance across observations.Solution: Use Weighted Nonlinear Least Squares (WNLS).Solution: Use Weighted Nonlinear Least Squares (WNLS).","code":""},{"path":"non-linear-regression.html","id":"correlated-errors","chapter":"6 Non-Linear Regression","heading":"6.3.5.4.4 Correlated Errors","text":"Residuals may autocorrelated.Residuals may autocorrelated.Solution approaches:\nGeneralized Nonlinear Least Squares (GNLS)\nNonlinear Mixed Models (NLMEM)\nBayesian Methods\nSolution approaches:Generalized Nonlinear Least Squares (GNLS)Generalized Nonlinear Least Squares (GNLS)Nonlinear Mixed Models (NLMEM)Nonlinear Mixed Models (NLMEM)Bayesian MethodsBayesian Methods","code":""},{"path":"non-linear-regression.html","id":"application","chapter":"6 Non-Linear Regression","heading":"6.4 Application","text":"","code":""},{"path":"non-linear-regression.html","id":"nonlinear-estimation-using-gauss-newton-algorithm","chapter":"6 Non-Linear Regression","heading":"6.4.1 Nonlinear Estimation Using Gauss-Newton Algorithm","text":"section demonstrates nonlinear parameter estimation using Gauss-Newton algorithm compares results nls(). model given :\\[\ny_i = \\frac{\\theta_0 + \\theta_1 x_i}{1 + \\theta_2 \\exp(0.4 x_i)} + \\epsilon_i\n\\]\\(= 1, \\dots ,n\\)\\(\\theta_0\\), \\(\\theta_1\\), \\(\\theta_2\\) unknown parameters.\\(\\epsilon_i\\) represents errors.Loading Visualizing DataDeriving Starting Values ParametersSince nonlinear optimization sensitive starting values, estimate reasonable initial values based model interpretation.Finding Maximum \\(Y\\) ValueWhen \\(y = 2.6722\\), corresponding \\(x = 0.0094\\).\\(y = 2.6722\\), corresponding \\(x = 0.0094\\).model equation: \\(\\theta_0 + 0.0094 \\theta_1 = 2.6722\\)model equation: \\(\\theta_0 + 0.0094 \\theta_1 = 2.6722\\)Estimating \\(\\theta_2\\) Median \\(y\\) ValueThe equation simplifies : \\(1 + \\theta_2 \\exp(0.4 x) = 2\\)yields equation: \\(83.58967 \\theta_2 = 1\\)Finding Value \\(\\theta_0\\) \\(\\theta_1\\)provides another equation: \\(\\theta_0 + \\theta_1 \\times 0.9895 - 2.164479 \\theta_2 = 1.457\\)Solving \\(\\theta_0, \\theta_1, \\theta_2\\)Implementing Gauss-Newton AlgorithmUsing estimates, manually implement Gauss-Newton optimization.Defining Model DerivativesIterative Gauss-Newton OptimizationChecking Convergence VarianceValidating nls()","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the dataset\nmy_data <- read.delim(\"images/S21hw1pr4.txt\", header = FALSE, sep = \"\") %>%\n  dplyr::rename(x = V1, y = V2)\n\n# Plot data\nggplot(my_data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Observed Data\", x = \"X\", y = \"Y\") +\n  theme_minimal()\nmax(my_data$y)\n#> [1] 2.6722\nmy_data$x[which.max(my_data$y)]\n#> [1] 0.0094\n# find mean y\nmean(my_data$y) \n#> [1] -0.0747864\n\n# find y closest to its mean\nmy_data$y[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] -0.0773\n\n\n# find x closest to the mean y\nmy_data$x[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] 11.0648\n# find value of x closet to 1\nmy_data$x[which.min(abs(my_data$x - 1))] \n#> [1] 0.9895\n\n# find index of x closest to 1\nmatch(my_data$x[which.min(abs(my_data$x - 1))], my_data$x) \n#> [1] 14\n\n# find y value\nmy_data$y[match(my_data$x[which.min(abs(my_data$x - 1))], my_data$x)]\n#> [1] 1.4577\nlibrary(matlib)\n\n# Define coefficient matrix\nA = matrix(\n  c(0, 0.0094, 0, 0, 0, 83.58967, 1, 0.9895, -2.164479),\n  nrow = 3,\n  ncol = 3,\n  byrow = T\n)\n\n# Define constant vector\nb <- c(2.6722, 1, 1.457)\n\n# Display system of equations\nshowEqn(A, b)\n#> 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 \n#> 0*x1      + 0*x2 + 83.58967*x3  =       1 \n#> 1*x1 + 0.9895*x2 - 2.164479*x3  =   1.457\n\n# Solve for parameters\ntheta_start <- Solve(A, b, fractions = FALSE)\n#> x1      =  -279.80879739 \n#>   x2    =   284.27659574 \n#>     x3  =      0.0119632\ntheta_start\n#> [1] \"x1      =  -279.80879739\" \"  x2    =   284.27659574\"\n#> [3] \"    x3  =      0.0119632\"\n# Starting values\ntheta_0_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[1]))\ntheta_1_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[2]))\ntheta_2_strt <- as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[3]))\n\n# Model function\nmod_4 <- function(theta_0, theta_1, theta_2, x) {\n  (theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x))\n}\n\n# Define function expression\nf_4 = expression((theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x)))\n\n# First derivatives\ndf_4.d_theta_0 <- D(f_4, 'theta_0')\ndf_4.d_theta_1 <- D(f_4, 'theta_1')\ndf_4.d_theta_2 <- D(f_4, 'theta_2')\n# Initialize\ntheta_vec <- matrix(c(theta_0_strt, theta_1_strt, theta_2_strt))\ndelta <- matrix(NA, nrow = 3, ncol = 1)\ni <- 1\n\n# Evaluate function at initial estimates\nf_theta <- as.matrix(eval(f_4, list(\n  x = my_data$x,\n  theta_0 = theta_vec[1, 1],\n  theta_1 = theta_vec[2, 1],\n  theta_2 = theta_vec[3, 1]\n)))\n\nrepeat {\n  # Compute Jacobian matrix\n  F_theta_0 <- as.matrix(cbind(\n    eval(df_4.d_theta_0, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    )),\n    eval(df_4.d_theta_1, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    )),\n    eval(df_4.d_theta_2, list(\n      x = my_data$x,\n      theta_0 = theta_vec[1, i],\n      theta_1 = theta_vec[2, i],\n      theta_2 = theta_vec[3, i]\n    ))\n  ))\n  \n  # Compute parameter updates\n  delta[, i] = (solve(t(F_theta_0)%*%F_theta_0))%*%t(F_theta_0)%*%(my_data$y-f_theta[,i])\n    \n  \n  # Update parameter estimates\n  theta_vec <- cbind(theta_vec, theta_vec[, i] + delta[, i])\n  theta_vec[, i + 1] = theta_vec[, i] + delta[, i]\n  \n  # Increment iteration counter\n  i <- i + 1\n  \n  # Compute new function values\n  f_theta <- cbind(f_theta, as.matrix(eval(f_4, list(\n    x = my_data$x,\n    theta_0 = theta_vec[1, i],\n    theta_1 = theta_vec[2, i],\n    theta_2 = theta_vec[3, i]\n  ))))\n  \n  delta = cbind(delta, matrix(NA, nrow = 3, ncol = 1))\n  \n  # Convergence criteria based on SSE\n  if (abs(sum((my_data$y - f_theta[, i])^2) - sum((my_data$y - f_theta[, i - 1])^2)) / \n      sum((my_data$y - f_theta[, i - 1])^2) < 0.001) {\n    break\n  }\n}\n\n# Final parameter estimates\ntheta_vec[, ncol(theta_vec)]\n#> [1]  3.6335135 -1.3055166  0.5043502\n# Final objective function value (SSE)\nsum((my_data$y - f_theta[, i])^2)\n#> [1] 19.80165\n\nsigma2 <- 1 / (nrow(my_data) - 3) * \n  (t(my_data$y - f_theta[, ncol(f_theta)]) %*% \n   (my_data$y - f_theta[, ncol(f_theta)]))  # p = 3\n\n# Asymptotic variance-covariance matrix\nas.numeric(sigma2)*as.matrix(solve(crossprod(F_theta_0)))\n#>             [,1]        [,2]        [,3]\n#> [1,]  0.11552571 -0.04817428  0.02685848\n#> [2,] -0.04817428  0.02100861 -0.01158212\n#> [3,]  0.02685848 -0.01158212  0.00703916\nnlin_4 <- nls(\n  y ~ mod_4(theta_0, theta_1, theta_2, x),\n  start = list(\n    theta_0 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[1])),\n    theta_1 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[2])),\n    theta_2 = as.numeric(gsub(\".*=\\\\s*\", \"\", theta_start[3]))\n  ),\n  data = my_data\n)\nsummary(nlin_4)\n#> \n#> Formula: y ~ mod_4(theta_0, theta_1, theta_2, x)\n#> \n#> Parameters:\n#>         Estimate Std. Error t value Pr(>|t|)    \n#> theta_0  3.63591    0.36528   9.954  < 2e-16 ***\n#> theta_1 -1.30639    0.15561  -8.395 3.65e-15 ***\n#> theta_2  0.50528    0.09215   5.483 1.03e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2831 on 247 degrees of freedom\n#> \n#> Number of iterations to convergence: 9 \n#> Achieved convergence tolerance: 2.294e-07"},{"path":"non-linear-regression.html","id":"logistic-growth-model","chapter":"6 Non-Linear Regression","heading":"6.4.2 Logistic Growth Model","text":"classic logistic growth model follows equation:\\[\nP = \\frac{K}{1 + \\exp(P_0 + r t)} + \\epsilon\n\\]:\\(P\\) = population time \\(t\\)\\(P\\) = population time \\(t\\)\\(K\\) = carrying capacity (maximum population)\\(K\\) = carrying capacity (maximum population)\\(r\\) = population growth rate\\(r\\) = population growth rate\\(P_0\\) = initial population log-ratio\\(P_0\\) = initial population log-ratioHowever, R’s built-SSlogis function uses slightly different parameterization:\\[\nP = \\frac{asym}{1 + \\exp\\left(\\frac{xmid - t}{scal}\\right)}\n\\]:\\(asym\\) = carrying capacity (\\(K\\))\\(asym\\) = carrying capacity (\\(K\\))\\(xmid\\) = \\(x\\)-value inflection point curve\\(xmid\\) = \\(x\\)-value inflection point curve\\(scal\\) = scaling parameter\\(scal\\) = scaling parameterThis gives parameter relationships:\\(K = asym\\)\\(K = asym\\)\\(r = -1 / scal\\)\\(r = -1 / scal\\)\\(P_0 = -r \\cdot xmid\\)\\(P_0 = -r \\cdot xmid\\)fit model using alternative parameterization (\\(K, r, P_0\\)), convert estimated coefficients:Visualizing Logistic Model Fit","code":"\n# Simulated time-series data\ntime <- c(1, 2, 3, 5, 10, 15, 20, 25, 30, 35)\npopulation <- c(2.8, 4.2, 3.5, 6.3, 15.7, 21.3, 23.7, 25.1, 25.8, 25.9)\n\n# Plot data points\nplot(time, population, las = 1, pch = 16, main = \"Logistic Growth Model\")\n\n# Fit the logistic growth model using programmed starting values\nlogisticModelSS <- nls(population ~ SSlogis(time, Asym, xmid, scal))\n\n# Model summary\nsummary(logisticModelSS)\n#> \n#> Formula: population ~ SSlogis(time, Asym, xmid, scal)\n#> \n#> Parameters:\n#>      Estimate Std. Error t value Pr(>|t|)    \n#> Asym  25.5029     0.3666   69.56 3.34e-11 ***\n#> xmid   8.7347     0.3007   29.05 1.48e-08 ***\n#> scal   3.6353     0.2186   16.63 6.96e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 1.908e-06\n\n# Extract parameter estimates\ncoef(logisticModelSS)\n#>      Asym      xmid      scal \n#> 25.502890  8.734698  3.635333\n# Convert parameter estimates to alternative logistic model parameters\nKs <- as.numeric(coef(logisticModelSS)[1])  # Carrying capacity (K)\nrs <- -1 / as.numeric(coef(logisticModelSS)[3])  # Growth rate (r)\nPos <- -rs * as.numeric(coef(logisticModelSS)[2])  # P_0\n\n# Fit the logistic model with the alternative parameterization\nlogisticModel <- nls(\n    population ~ K / (1 + exp(Po + r * time)),\n    start = list(Po = Pos, r = rs, K = Ks)\n)\n\n# Model summary\nsummary(logisticModel)\n#> \n#> Formula: population ~ K/(1 + exp(Po + r * time))\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> Po  2.40272    0.12702   18.92 2.87e-07 ***\n#> r  -0.27508    0.01654  -16.63 6.96e-07 ***\n#> K  25.50289    0.36665   69.56 3.34e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 0 \n#> Achieved convergence tolerance: 1.924e-06\n# Plot original data\nplot(time,\n     population,\n     las = 1,\n     pch = 16,\n     main = \"Logistic Growth Model Fit\")\n\n# Overlay the fitted logistic curve\nlines(time,\n      predict(logisticModel),\n      col = \"red\",\n      lwd = 2)"},{"path":"non-linear-regression.html","id":"nonlinear-plateau-model","chapter":"6 Non-Linear Regression","heading":"6.4.3 Nonlinear Plateau Model","text":"example based (Schabenberger Pierce 2001) demonstrates use plateau model estimate relationship soil nitrate (\\(NO_3\\)) concentration relative yield percent (RYP) two different depths (30 cm 60 cm).suggested nonlinear plateau model given :\\[\nE(Y_{ij}) = (\\beta_{0j} + \\beta_{1j}N_{ij})I_{N_{ij}\\le \\alpha_j} + (\\beta_{0j} + \\beta_{1j}\\alpha_j)I_{N_{ij} > \\alpha_j}\n\\]:\\(N_{ij}\\) represents soil nitrate (\\(NO_3\\)) concentration observation \\(\\) depth \\(j\\).\\(N_{ij}\\) represents soil nitrate (\\(NO_3\\)) concentration observation \\(\\) depth \\(j\\).\\(\\) indexes individual observations.\\(\\) indexes individual observations.\\(j = 1, 2\\) corresponds depths 30 cm 60 cm.\\(j = 1, 2\\) corresponds depths 30 cm 60 cm.model assumes linear increase threshold (\\(\\alpha_j\\)), beyond response levels (plateaus).Defining Plateau Model FunctionCreating Self-Starting Function nlsSince model piecewise linear, can estimate starting values using:linear regression first half sorted predictor values estimate \\(b_0\\) \\(b_1\\).linear regression first half sorted predictor values estimate \\(b_0\\) \\(b_1\\).last predictor value used regression plateau threshold (\\(\\alpha\\))last predictor value used regression plateau threshold (\\(\\alpha\\))Combining Model Self-Start FunctionThe nls function used estimate parameters separately soil depth (30 cm 60 cm).generate separate plots 30 cm 60 cm depths, showing confidence prediction intervals.Modeling Soil Depths Together Comparing ModelsInstead fitting separate models different soil depths, first fit combined model observations share common slope, intercept, plateau. test whether modeling two depths separately provides significantly better fit.Fitting Reduced (Combined) ModelThe reduced model assumes soil depths follow nonlinear relationship.Examining Residuals Combined ModelChecking residuals helps diagnose potential lack fit.pattern residuals (e.g., systematic deviations based soil depth), suggests separate model depth may necessary.Testing Whether Depths Require Separate ModelsTo formally test whether soil depth significantly affects model parameters, introduce parameterization depth-specific parameters increments baseline model (30 cm depth):\\[\n\\begin{aligned}\n\\beta_{02} &= \\beta_{01} + d_0 \\\\\n\\beta_{12} &= \\beta_{11} + d_1 \\\\\n\\alpha_{2} &= \\alpha_{1} + d_a\n\\end{aligned}\n\\]:\\(\\beta_{01}, \\beta_{11}, \\alpha_1\\) parameters 30 cm depth.\\(\\beta_{01}, \\beta_{11}, \\alpha_1\\) parameters 30 cm depth.\\(d_0, d_1, d_a\\) represent depth-specific differences 60 cm depth.\\(d_0, d_1, d_a\\) represent depth-specific differences 60 cm depth.\\(d_0, d_1, d_a\\) significantly different 0, two depths modeled separately.\\(d_0, d_1, d_a\\) significantly different 0, two depths modeled separately.Defining Full (Depth-Specific) ModelFitting Full (Depth-Specific) ModelThe starting values taken separately fitted models depth.Model Comparison: Depth Matter?\\(d_0, d_1, d_a\\) significantly different 0, depths modeled separately.\\(d_0, d_1, d_a\\) significantly different 0, depths modeled separately.p-values parameters indicate whether depth-specific modeling necessary.p-values parameters indicate whether depth-specific modeling necessary.","code":"\n# Load data\ndat <- read.table(\"images/dat.txt\", header = TRUE)\n\n# Plot NO3 concentration vs. relative yield percent, colored by depth\nlibrary(ggplot2)\ndat.plot <- ggplot(dat) + \n  geom_point(aes(x = no3, y = ryp, color = as.factor(depth))) +\n  labs(color = 'Depth (cm)') + \n  xlab('Soil NO3 Concentration') + \n  ylab('Relative Yield Percent') +\n  theme_minimal()\n\n# Display plot\ndat.plot\n# Define the nonlinear plateau model function\nnonlinModel <- function(predictor, b0, b1, alpha) {\n  ifelse(predictor <= alpha, \n         b0 + b1 * predictor,  # Linear growth below threshold\n         b0 + b1 * alpha)      # Plateau beyond threshold\n}\n# Define initialization function for self-starting plateau model\nnonlinModelInit <- function(mCall, LHS, data) {\n  # Sort data by increasing predictor value\n  xy <- sortedXyData(mCall[['predictor']], LHS, data)\n  n <- nrow(xy)\n  \n  # Fit a simple linear model using the first half of the sorted data\n  lmFit <- lm(xy[1:(n / 2), 'y'] ~ xy[1:(n / 2), 'x'])\n  \n  # Extract initial estimates\n  b0 <- coef(lmFit)[1]  # Intercept\n  b1 <- coef(lmFit)[2]  # Slope\n  alpha <- xy[(n / 2), 'x']  # Last x-value in the fitted linear range\n  \n  # Return initial parameter estimates\n  value <- c(b0, b1, alpha)\n  names(value) <- mCall[c('b0', 'b1', 'alpha')]\n  value\n}\n# Define a self-starting nonlinear model for nls\nSS_nonlinModel <- selfStart(nonlinModel,\n                            nonlinModelInit,\n                            c('b0', 'b1', 'alpha'))\n# Fit the model for depth = 30 cm\nsep30_nls <- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n                  data = dat[dat$depth == 30,])\n\n# Fit the model for depth = 60 cm\nsep60_nls <- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n                  data = dat[dat$depth == 60,])\n# Set plotting layout\npar(mfrow = c(1, 2))\n\n# Plot model fit for 30 cm depth\nplotFit(\n  sep30_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = dat[dat$depth == 30,],\n  main = \"Results at 30 cm Depth\",\n  ylab = \"Relative Yield Percent\",\n  xlab = \"Soil NO3 Concentration\",\n  xlim = c(0, 120)\n)\n\n# Plot model fit for 60 cm depth\nplotFit(\n  sep60_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightpink4\",\n  col.pred = \"lightpink2\",\n  data = dat[dat$depth == 60,],\n  main = \"Results at 60 cm Depth\",\n  ylab = \"Relative Yield Percent\",\n  xlab = \"Soil NO3 Concentration\",\n  xlim = c(0, 120)\n)\nsummary(sep30_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0     15.1943     2.9781   5.102 6.89e-07 ***\n#> b1      3.5760     0.1853  19.297  < 2e-16 ***\n#> alpha  23.1324     0.5098  45.373  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.258 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 6 \n#> Achieved convergence tolerance: 3.608e-09\nsummary(sep60_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      5.4519     2.9785    1.83   0.0684 .  \n#> b1      5.6820     0.2529   22.46   <2e-16 ***\n#> alpha  16.2863     0.2818   57.80   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.427 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 5 \n#> Achieved convergence tolerance: 8.571e-09\n# Fit the combined model (common parameters across all depths)\nred_nls <- nls(\n  ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), \n  data = dat\n)\n\n# Display model summary\nsummary(red_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      8.7901     2.7688   3.175   0.0016 ** \n#> b1      4.8995     0.2207  22.203   <2e-16 ***\n#> alpha  18.0333     0.3242  55.630   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.13 on 477 degrees of freedom\n#> \n#> Number of iterations to convergence: 7 \n#> Achieved convergence tolerance: 7.126e-09\n\n# Visualizing the combined model fit\npar(mfrow = c(1, 1))\nplotFit(\n  red_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightblue4\",\n  col.pred = \"lightblue2\",\n  data = dat,\n  main = 'Results for Combined Model',\n  ylab = 'Relative Yield Percent',\n  xlab = 'Soil NO3 Concentration'\n)\nlibrary(nlstools)\n\n# Residual diagnostics using nlstools\nresid <- nlsResiduals(red_nls)\n\n# Plot residuals\nplot(resid)\nnonlinModelF <- function(predictor, soildep, b01, b11, a1, d0, d1, da) {\n  \n  # Define parameters for 60 cm depth as increments from 30 cm parameters\n  b02 <- b01 + d0\n  b12 <- b11 + d1\n  a2 <- a1 + da\n  \n  # Compute model output for 30 cm depth\n  y1 <- ifelse(\n    predictor <= a1, \n    b01 + b11 * predictor, \n    b01 + b11 * a1\n  )\n  \n  # Compute model output for 60 cm depth\n  y2 <- ifelse(\n    predictor <= a2, \n    b02 + b12 * predictor, \n    b02 + b12 * a2\n  )\n  \n  # Assign correct model output based on depth\n  y <- y1 * (soildep == 30) + y2 * (soildep == 60)\n  \n  return(y)\n}\nSoil_full <- nls(\n  ryp ~ nonlinModelF(\n    predictor = no3,\n    soildep = depth,\n    b01,\n    b11,\n    a1,\n    d0,\n    d1,\n    da\n  ),\n  data = dat,\n  start = list(\n    b01 = 15.2,   # Intercept for 30 cm depth\n    b11 = 3.58,   # Slope for 30 cm depth\n    a1 = 23.13,   # Plateau cutoff for 30 cm depth\n    d0 = -9.74,   # Intercept difference (60 cm - 30 cm)\n    d1 = 2.11,    # Slope difference (60 cm - 30 cm)\n    da = -6.85    # Plateau cutoff difference (60 cm - 30 cm)\n  )\n)\n\n# Display model summary\nsummary(Soil_full)\n#> \n#> Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, \n#>     a1, d0, d1, da)\n#> \n#> Parameters:\n#>     Estimate Std. Error t value Pr(>|t|)    \n#> b01  15.1943     2.8322   5.365 1.27e-07 ***\n#> b11   3.5760     0.1762  20.291  < 2e-16 ***\n#> a1   23.1324     0.4848  47.711  < 2e-16 ***\n#> d0   -9.7424     4.2357  -2.300   0.0219 *  \n#> d1    2.1060     0.3203   6.575 1.29e-10 ***\n#> da   -6.8461     0.5691 -12.030  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.854 on 474 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 3.742e-06"},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"7 Generalized Linear Models","heading":"7 Generalized Linear Models","text":"Even though call generalized linear model, still paradigm non-linear regression, form regression model non-linear. name generalized linear model derived fact \\(\\mathbf{x'_i \\beta}\\) (linear form) model.","code":""},{"path":"generalized-linear-models.html","id":"logistic-regression-1","chapter":"7 Generalized Linear Models","heading":"7.1 Logistic Regression","text":"\\[\np_i = f(\\mathbf{x}_i ; \\beta) = \\frac{exp(\\mathbf{x_i'\\beta})}{1 + exp(\\mathbf{x_i'\\beta})}\n\\]Equivalently,\\[\nlogit(p_i) = log(\\frac{p_i}{1+p_i}) = \\mathbf{x_i'\\beta}\n\\]\\(\\frac{p_i}{1+p_i}\\)odds.form, model specified function mean response linear. Hence, Generalized Linear ModelsThe likelihood function\\[\nL(p_i) = \\prod_{=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}\n\\]\\(p_i = \\frac{\\mathbf{x'_i \\beta}}{1+\\mathbf{x'_i \\beta}}\\) \\(1-p_i = (1+ exp(\\mathbf{x'_i \\beta}))^{-1}\\)Hence, objective function \\[\nQ(\\beta) = log(L(\\beta)) = \\sum_{=1}^n Y_i \\mathbf{x'_i \\beta} - \\sum_{=1}^n  log(1+ exp(\\mathbf{x'_i \\beta}))\n\\]maximize function numerically using optimization method , allows us find numerical MLE \\(\\hat{\\beta}\\). can use standard asymptotic properties MLEs make inference.Property MLEs parameters asymptotically unbiased sample variance-covariance matrix given inverse Fisher information matrix\\[\n\\hat{\\beta} \\dot{\\sim} (\\beta,[\\mathbf{}(\\beta)]^{-1})\n\\]Fisher Information matrix, \\(\\mathbf{}(\\beta)\\) \\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= E[\\frac{\\partial \\log(L(\\beta))}{\\partial (\\beta)}\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta'}] \\\\\n&= E[(\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_i} \\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]regularity conditions, equivalent negative expected value Hessian Matrix\\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= -E[\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta \\partial \\beta'}] \\\\\n&= -E[(\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta_i \\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]Example:\\[\nx_i' \\beta = \\beta_0 + \\beta_1 x_i\n\\]\\[\n\\begin{aligned}\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_0} &= \\sum_{=1}^n \\frac{\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_1} &= \\sum_{=1}^n \\frac{x_i^2\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{x_i\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_i^2p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta_0 \\partial \\beta_1} &= \\sum_{=1}^n \\frac{x_i\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - x_i[\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_ip_i (1-p_i)\n\\end{aligned}\n\\]Hence,\\[\n\\mathbf{} (\\beta) =\n\\left[\n\\begin{array}\n{cc}\n\\sum_i p_i(1-p_i) & \\sum_i x_i p_i(1-p_i) \\\\\n\\sum_i x_i p_i(1-p_i) & \\sum_i x_i^2 p_i(1-p_i)\n\\end{array}\n\\right]\n\\]InferenceLikelihood Ratio TestsTo formulate test, let \\(\\beta = [\\beta_1', \\beta_2']'\\). interested testing hypothesis \\(\\beta_1\\), leave \\(\\beta_2\\) unspecified (called nuisance parameters). \\(\\beta_1\\) \\(\\beta_2\\) can either vector scalar, \\(\\beta_2\\) can null.Example: \\(H_0: \\beta_1 = \\beta_{1,0}\\) (\\(\\beta_{1,0}\\) specified) \\(\\hat{\\beta}_{2,0}\\) MLE \\(\\beta_2\\) restriction \\(\\beta_1 = \\beta_{1,0}\\). likelihood ratio test statistic \\[\n-2\\log\\Lambda = -2[\\log(L(\\beta_{1,0},\\hat{\\beta}_{2,0})) - \\log(L(\\hat{\\beta}_1,\\hat{\\beta}_2))]\n\\]wherethe first term value fo likelihood fitted restricted modelthe second term likelihood value fitted unrestricted modelUnder null,\\[\n-2 \\log \\Lambda \\sim \\chi^2_{\\upsilon}\n\\]\\(\\upsilon\\) dimension \\(\\beta_1\\)reject null \\(-2\\log \\Lambda > \\chi_{\\upsilon,1-\\alpha}^2\\)Wald StatisticsBased \\[\n\\hat{\\beta} \\sim (\\beta, [\\mathbf{}(\\beta)^{-1}])\n\\]\\[\nH_0: \\mathbf{L}\\hat{\\beta} = 0\n\\]\\(\\mathbf{L}\\) \\(q \\times p\\) matrix \\(q\\) linearly independent rows. \\[\nW = (\\mathbf{L\\hat{\\beta}})'(\\mathbf{L[(\\hat{\\beta})]^{-1}L'})^{-1}(\\mathbf{L\\hat{\\beta}})\n\\]null hypothesisConfidence interval\\[\n\\hat{\\beta}_i \\pm 1.96 \\hat{s}_{ii}^2\n\\]\\(\\hat{s}_{ii}^2\\) -th diagonal \\(\\mathbf{[(\\hat{\\beta})]}^{-1}\\)havelarge sample size, likelihood ratio Wald tests similar results.small sample size, likelihood ratio test better.Logistic Regression: Interpretation \\(\\beta\\)single regressor, model \\[\nlogit\\{\\hat{p}_{x_i}\\} \\equiv logit (\\hat{p}_i) = \\log(\\frac{\\hat{p}_i}{1 - \\hat{p}_i}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\\(x= x_i + 1\\)\\[\nlogit\\{\\hat{p}_{x_i +1}\\} = \\hat{\\beta}_0 + \\hat{\\beta}(x_i + 1) = logit\\{\\hat{p}_{x_i}\\} + \\hat{\\beta}_1\n\\],\\[\n\\begin{aligned}\nlogit\\{\\hat{p}_{x_i +1}\\} - logit\\{\\hat{p}_{x_i}\\} &= log\\{odds[\\hat{p}_{x_i +1}]\\} - log\\{odds[\\hat{p}_{x_i}]\\} \\\\\n&= log(\\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}) = \\hat{\\beta}_1\n\\end{aligned}\n\\]\\[\nexp(\\hat{\\beta}_1) = \\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}\n\\]estimated odds ratiothe estimated odds ratio, difference c units regressor x, \\(exp(c\\hat{\\beta}_1)\\). multiple covariates, \\(exp(\\hat{\\beta}_k)\\) estimated odds ratio variable \\(x_k\\), assuming variables held constant.Inference Mean ResponseLet \\(x_h = (1, x_{h1}, ...,x_{h,p-1})'\\). \\[\n\\hat{p}_h = \\frac{exp(\\mathbf{x'_h \\hat{\\beta}})}{1 + exp(\\mathbf{x'_h \\hat{\\beta}})}\n\\]\\(s^2(\\hat{p}_h) = \\mathbf{x'_h[(\\hat{\\beta})]^{-1}x_h}\\)new observation, can cutoff point decide whether y = 0 1.","code":""},{"path":"generalized-linear-models.html","id":"application-1","chapter":"7 Generalized Linear Models","heading":"7.1.1 Application","text":"Logistic Regression\\(x \\sim Unif(-0.5,2.5)\\). \\(\\eta = 0.5 + 0.75 x\\)Passing \\(\\eta\\)’s inverse-logit function, get\\[\np = \\frac{\\exp(\\eta)}{1+ \\exp(\\eta)}\n\\]\\(p \\[0,1]\\), generate \\(y \\sim Bernoulli(p)\\)Model FitBased odds ratio, \\(x = 0\\) , odds success 1.59\\(x = 1\\), odds success increase factor 2.19 (.e., 119.29% increase).Deviance Tests\\(H_0\\): variables related response (.e., model just intercept)\\(H_1\\): least one variable related responseSince see p-value 0, reject null variables related responseDeviance residualsHowever, plot informative. Hence, can can see residuals plots grouped bins based prediction values.can also see predicted value residuals.can also look binned plot logistic prediction versus true categoryFormal deviance testHosmer-Lemeshow testNull hypothesis: observed events match expected evens\\[\nX^2_{HL} = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{p}_j)^2}{m_j \\hat{p}_j(1-\\hat{p}_j)}\n\\]wherewithin j-th bin, \\(y_j\\) number successes\\(m_j\\) = number observations\\(\\hat{p}_j\\) = predicted probabilityUnder null hypothesis, \\(X^2_{HLL} \\sim \\chi^2_{J-1}\\)Since \\(p\\)-value = 0.99, reject null hypothesis (.e., model fitting well).","code":"\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(pscl)\nlibrary(ggplot2)\nlibrary(faraway)\nlibrary(nnet)\nlibrary(agridat)\nlibrary(nlstools)\nset.seed(23) #set seed for reproducibility\nx <- runif(1000, min = -0.5, max = 2.5)\neta1 <- 0.5 + 0.75 * x\np <- exp(eta1) / (1 + exp(eta1))\ny <- rbinom(1000, 1, p)\nBinData <- data.frame(X = x, Y = y)\nLogistic_Model <- glm(formula = Y ~ X,\n                      family = binomial, # family = specifies the response distribution\n                      data = BinData)\nsummary(Logistic_Model)\n#> \n#> Call:\n#> glm(formula = Y ~ X, family = binomial, data = BinData)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2317   0.4153   0.5574   0.7922   1.1469  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***\n#> X            0.78527    0.09296   8.447  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1106.7  on 999  degrees of freedom\n#> Residual deviance: 1027.4  on 998  degrees of freedom\n#> AIC: 1031.4\n#> \n#> Number of Fisher Scoring iterations: 4\nnlstools::confint2(Logistic_Model)\n#>                 2.5 %    97.5 %\n#> (Intercept) 0.2618709 0.6622204\n#> X           0.6028433 0.9676934\nOddsRatio <- coef(Logistic_Model) %>% exp\nOddsRatio \n#> (Intercept)           X \n#>    1.587318    2.192995\nTest_Dev <- Logistic_Model$null.deviance - Logistic_Model$deviance\np_val_dev <- 1 - pchisq(q = Test_Dev, df = 1)\nLogistic_Resids <- residuals(Logistic_Model, type = \"deviance\")\nplot(\n    y = Logistic_Resids,\n    x = BinData$X,\n    xlab = 'X',\n    ylab = 'Deviance Resids'\n)\nplot_bin <- function(Y,\n                     X,\n                     bins = 100,\n                     return.DF = FALSE) {\n    Y_Name <- deparse(substitute(Y))\n    X_Name <- deparse(substitute(X))\n    Binned_Plot <- data.frame(Plot_Y = Y, Plot_X = X)\n    Binned_Plot$bin <-\n        cut(Binned_Plot$Plot_X, breaks = bins) %>% as.numeric\n    Binned_Plot_summary <- Binned_Plot %>%\n        group_by(bin) %>%\n        summarise(\n            Y_ave = mean(Plot_Y),\n            X_ave = mean(Plot_X),\n            Count = n()\n        ) %>% as.data.frame\n    plot(\n        y = Binned_Plot_summary$Y_ave,\n        x = Binned_Plot_summary$X_ave,\n        ylab = Y_Name,\n        xlab = X_Name\n    )\n    if (return.DF)\n        return(Binned_Plot_summary)\n}\n\n\nplot_bin(Y = Logistic_Resids,\n         X = BinData$X,\n         bins = 100)\nLogistic_Predictions <- predict(Logistic_Model, type = \"response\")\nplot_bin(Y = Logistic_Resids, X = Logistic_Predictions, bins = 100)\nNumBins <- 10\nBinned_Data <- plot_bin(\n    Y = BinData$Y,\n    X = Logistic_Predictions,\n    bins = NumBins,\n    return.DF = TRUE\n)\nBinned_Data\n#>    bin     Y_ave     X_ave Count\n#> 1    1 0.5833333 0.5382095    72\n#> 2    2 0.5200000 0.5795887    75\n#> 3    3 0.6567164 0.6156540    67\n#> 4    4 0.7014925 0.6579674    67\n#> 5    5 0.6373626 0.6984765    91\n#> 6    6 0.7500000 0.7373341    72\n#> 7    7 0.7096774 0.7786747    93\n#> 8    8 0.8503937 0.8203819   127\n#> 9    9 0.8947368 0.8601232   133\n#> 10  10 0.8916256 0.9004734   203\nabline(0, 1, lty = 2, col = 'blue')\nHL_BinVals <-\n    (Binned_Data$Count * Binned_Data$Y_ave - Binned_Data$Count * Binned_Data$X_ave) ^ 2 /   Binned_Data$Count * Binned_Data$X_ave * (1 - Binned_Data$X_ave)\nHLpval <- pchisq(q = sum(HL_BinVals),\n                 df = NumBins,\n                 lower.tail = FALSE)\nHLpval\n#> [1] 0.9999989"},{"path":"generalized-linear-models.html","id":"probit-regression","chapter":"7 Generalized Linear Models","heading":"7.2 Probit Regression","text":"\\[\nE(Y_i) = p_i = \\Phi(\\mathbf{x_i'\\theta})\n\\]\\(\\Phi()\\) CDF \\(N(0,1)\\) random variable.models (e..g, t–distribution; log-log; complimentary log-log)let \\(Y_i = 1\\) success, \\(Y_i =0\\) success.assume \\(Y \\sim Ber\\) \\(p_i = P(Y_i =1)\\), success probability.assume \\(Y \\sim Ber\\) \\(p_i = P(Y_i =1)\\), success probability.consider logistic regression response function \\(logit(p_i) = x'_i \\beta\\)consider logistic regression response function \\(logit(p_i) = x'_i \\beta\\)Confusion matrixSensitivity: ability identify positive results\\[\n\\text{Sensitivity} = \\frac{TP}{TP + FN}\n\\]Specificity: ability identify negative results\\[\n\\text{Specificity} = \\frac{TN}{TN + FP}\n\\]False positive rate: Type error (1- specificity)\\[\n\\text{ False Positive Rate} = \\frac{FP}{TN+ FP}\n\\]False Negative Rate: Type II error (1-sensitivity)\\[\n\\text{False Negative Rate} = \\frac{FN}{TP + FN}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.3 Binomial Regression","text":"BinomialHere, cancer case = successes, control case = failures.","code":"\ndata(\"esoph\")\nhead(esoph, n = 3)\n#>   agegp     alcgp    tobgp ncases ncontrols\n#> 1 25-34 0-39g/day 0-9g/day      0        40\n#> 2 25-34 0-39g/day    10-19      0        10\n#> 3 25-34 0-39g/day    20-29      0         6\nplot(\n  esoph$ncases / (esoph$ncases + esoph$ncontrols) ~ esoph$alcgp,\n  ylab = \"Proportion\",\n  xlab = 'Alcohol consumption',\n  main = 'Esophageal Cancer data'\n)\nclass(esoph$agegp) <- \"factor\"\nclass(esoph$alcgp) <- \"factor\"\nclass(esoph$tobgp) <- \"factor\"\n#  only the alcohol consumption as a predictor\nmodel <- glm(cbind(ncases, ncontrols) ~ alcgp, data = esoph, family = binomial)\nsummary(model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -4.0759  -1.2037  -0.0183   1.0928   3.7336  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.5885     0.1925 -13.444  < 2e-16 ***\n#> alcgp40-79    1.2712     0.2323   5.472 4.46e-08 ***\n#> alcgp80-119   2.0545     0.2611   7.868 3.59e-15 ***\n#> alcgp120+     3.3042     0.3237  10.209  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 221.46  on 84  degrees of freedom\n#> AIC: 344.51\n#> \n#> Number of Fisher Scoring iterations: 5\n#Coefficient Odds\ncoefficients(model) %>% exp\n#> (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ \n#>  0.07512953  3.56527094  7.80261593 27.22570533\ndeviance(model)/df.residual(model)\n#> [1] 2.63638\nmodel$aic\n#> [1] 344.5109\n# alcohol consumption and age as predictors\nbetter_model <-\n    glm(cbind(ncases, ncontrols) ~ agegp + alcgp,\n        data = esoph,\n        family = binomial)\nsummary(better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2395  -0.7186  -0.2324   0.7930   3.3538  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -6.1472     1.0419  -5.900 3.63e-09 ***\n#> agegp35-44    1.6311     1.0800   1.510 0.130973    \n#> agegp45-54    3.4258     1.0389   3.297 0.000976 ***\n#> agegp55-64    3.9435     1.0346   3.811 0.000138 ***\n#> agegp65-74    4.3568     1.0413   4.184 2.87e-05 ***\n#> agegp75+      4.4242     1.0914   4.054 5.04e-05 ***\n#> alcgp40-79    1.4343     0.2448   5.859 4.64e-09 ***\n#> alcgp80-119   2.0071     0.2776   7.230 4.84e-13 ***\n#> alcgp120+     3.6800     0.3763   9.778  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 105.88  on 79  degrees of freedom\n#> AIC: 238.94\n#> \n#> Number of Fisher Scoring iterations: 6\nbetter_model$aic #smaller AIC is better\n#> [1] 238.9361\ncoefficients(better_model) %>% exp\n#>  (Intercept)   agegp35-44   agegp45-54   agegp55-64   agegp65-74     agegp75+ \n#>  0.002139482  5.109601844 30.748594216 51.596634690 78.005283850 83.448437749 \n#>   alcgp40-79  alcgp80-119    alcgp120+ \n#>  4.196747169  7.441782227 39.646885126\npchisq(\n    q = model$deviance - better_model$deviance,\n    df = model$df.residual - better_model$df.residual,\n    lower = FALSE\n)\n#> [1] 2.713923e-23\n# specify link function as probit\nProb_better_model <- glm(\n    cbind(ncases, ncontrols) ~ agegp + alcgp,\n    data = esoph,\n    family = binomial(link = probit)\n)\nsummary(Prob_better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1325  -0.6877  -0.1661   0.7654   3.3258  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -3.3741     0.4922  -6.855 7.13e-12 ***\n#> agegp35-44    0.8562     0.5081   1.685 0.092003 .  \n#> agegp45-54    1.7829     0.4904   3.636 0.000277 ***\n#> agegp55-64    2.1034     0.4876   4.314 1.61e-05 ***\n#> agegp65-74    2.3374     0.4930   4.741 2.13e-06 ***\n#> agegp75+      2.3694     0.5275   4.491 7.08e-06 ***\n#> alcgp40-79    0.8080     0.1330   6.076 1.23e-09 ***\n#> alcgp80-119   1.1399     0.1558   7.318 2.52e-13 ***\n#> alcgp120+     2.1204     0.2060  10.295  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 104.48  on 79  degrees of freedom\n#> AIC: 237.53\n#> \n#> Number of Fisher Scoring iterations: 6"},{"path":"generalized-linear-models.html","id":"poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.4 Poisson Regression","text":"Poisson distribution\\[\n\\begin{aligned}\nf(Y_i) &= \\frac{\\mu_i^{Y_i}exp(-\\mu_i)}{Y_i!}, Y_i = 0,1,.. \\\\\nE(Y_i) &= \\mu_i  \\\\\nvar(Y_i) &= \\mu_i\n\\end{aligned}\n\\]natural distribution counts. can see variance function mean. let \\(\\mu_i = f(\\mathbf{x_i; \\theta})\\), similar Logistic Regression since can choose \\(f()\\) \\(\\mu_i = \\mathbf{x_i'\\theta}, \\mu_i = \\exp(\\mathbf{x_i'\\theta}), \\mu_i = \\log(\\mathbf{x_i'\\theta})\\)","code":""},{"path":"generalized-linear-models.html","id":"application-2","chapter":"7 Generalized Linear Models","heading":"7.4.1 Application","text":"Count Data Poisson regressionResidual 1634 909 df isn’t great.see Pearson \\(\\chi^2\\)interaction terms, improvementsConsider \\(\\hat{\\phi} = \\frac{\\text{deviance}}{df}\\)evidence -dispersion. Likely cause missing variables. remedies either include variables consider random effects.quick fix force Poisson Regression include value \\(\\phi\\), model called “Quasi-Poisson”.directly rerun model asQuasi-Poisson recommended, Negative Binomial Regression extra parameter account -dispersion .","code":"\ndata(bioChemists, package = \"pscl\")\nbioChemists <- bioChemists %>%\n    rename(\n        Num_Article = art, #articles in last 3 years of PhD\n        Sex = fem, #coded 1 if female\n        Married = mar, #coded 1 if married\n        Num_Kid5 = kid5, #number of childeren under age 6\n        PhD_Quality = phd, #prestige of PhD program\n        Num_MentArticle = ment #articles by mentor in last 3 years\n    )\nhist(bioChemists$Num_Article, breaks = 25, main = 'Number of Articles')\nPoisson_Mod <- glm(Num_Article ~ ., family=poisson, bioChemists)\nsummary(Poisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.304617   0.102981   2.958   0.0031 ** \n#> SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***\n#> MarriedMarried   0.155243   0.061374   2.529   0.0114 *  \n#> Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***\n#> PhD_Quality      0.012823   0.026397   0.486   0.6271    \n#> Num_MentArticle  0.025543   0.002006  12.733  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5\nPredicted_Means <- predict(Poisson_Mod,type = \"response\")\nX2 <- sum((bioChemists$Num_Article - Predicted_Means)^2/Predicted_Means)\nX2\n#> [1] 1662.547\npchisq(X2,Poisson_Mod$df.residual, lower.tail = FALSE)\n#> [1] 7.849882e-47\nPoisson_Mod_All2way <- glm(Num_Article ~ .^2, family=poisson, bioChemists)\nPoisson_Mod_All3way <- glm(Num_Article ~ .^3, family=poisson, bioChemists)\nPoisson_Mod$deviance / Poisson_Mod$df.residual\n#> [1] 1.797988\nphi_hat = Poisson_Mod$deviance/Poisson_Mod$df.residual\nsummary(Poisson_Mod,dispersion = phi_hat)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.30462    0.13809   2.206  0.02739 *  \n#> SexWomen        -0.22459    0.07323  -3.067  0.00216 ** \n#> MarriedMarried   0.15524    0.08230   1.886  0.05924 .  \n#> Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***\n#> PhD_Quality      0.01282    0.03540   0.362  0.71715    \n#> Num_MentArticle  0.02554    0.00269   9.496  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1.797988)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5\nquasiPoisson_Mod <- glm(Num_Article ~ ., family=quasipoisson, bioChemists)"},{"path":"generalized-linear-models.html","id":"negative-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.5 Negative Binomial Regression","text":"can see dispersion 2.264 SE = 0.271, significantly different 1, indicating -dispersion. Check -Dispersion detail","code":"\nlibrary(MASS)\nNegBinom_Mod <- MASS::glm.nb(Num_Article ~ .,bioChemists)\nsummary(NegBinom_Mod)\n#> \n#> Call:\n#> MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n#>     link = log)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1678  -1.3617  -0.2806   0.4476   3.4524  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n#> SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n#> MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n#> Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n#> PhD_Quality      0.015271   0.035873   0.426 0.670326    \n#> Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n#> \n#>     Null deviance: 1109.0  on 914  degrees of freedom\n#> Residual deviance: 1004.3  on 909  degrees of freedom\n#> AIC: 3135.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.264 \n#>           Std. Err.:  0.271 \n#> \n#>  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"multinomial","chapter":"7 Generalized Linear Models","heading":"7.6 Multinomial","text":"two categories groups want model relative covariates (e.g., observations \\(= 1,…,n\\) groups/ covariates \\(j = 1,2,…,J\\)), multinomial candidate modelLet\\(p_{ij}\\) probability -th observation belongs j-th group\\(Y_{ij}\\) number observations individual group j; individual observations \\(Y_{i1},Y_{i2},…Y_{iJ}\\)assume probability observing response given multinomial distribution terms probabilities \\(p_{ij}\\), \\(\\sum_{j = 1}^J p_{ij} = 1\\) . interpretation, baseline category \\(p_{i1} = 1 - \\sum_{j = 2}^J p_{ij}\\)link mean response (probability) \\(p_{ij}\\) linear function covariates\\[\n\\eta_{ij} = \\mathbf{x'_i \\beta_j} = \\log \\frac{p_{ij}}{p_{i1}}, j = 2,..,J\n\\]compare \\(p_{ij}\\) baseline \\(p_{i1}\\), suggesting\\[\np_{ij} = \\frac{\\exp(\\eta_{ij})}{1 + \\sum_{=2}^J \\exp(\\eta_{ij})}\n\\]known multinomial logistic model.Note:Softmax coding multinomial logistic regression: rather selecting baseline class, treat \\(K\\) class symmetrically - equally important (baseline).\\[\nP(Y = k | X = x) = \\frac{exp(\\beta_{k1} + \\dots + \\beta_{k_p x_p})}{\\sum_{l = 1}^K exp(\\beta_{l0} + \\dots + \\beta_{l_p x_p})}\n\\]log odds ratio k-th k’-th classes \\[\n\\log (\\frac{P(Y=k|X=x)}{P(Y = k' | X=x)}) = (\\beta_{k0} - \\beta_{k'0}) + \\dots + (\\beta_{kp} - \\beta_{k'p}) x_p\n\\]try understand political strengthvisualize political strength variableFit multinomial logistic model:model political strength function age educationAlternatively, stepwise model selection based AICcompare best model full model based devianceWe see significant differencePlot fitted modelIf categories ordered (.e., ordinal data), must use another approach (still multinomial, use cumulative probabilities).Another example\\[\nY \\sim Gamma\n\\]Gamma non-negative opposed Normal. canonical Gamma link function inverse (reciprocal) link\\[\n\\begin{aligned}\n\\eta_{ij} &= \\beta_{0j} + \\beta_{1j}x_{ij} + \\beta_2x_{ij}^2 \\\\\nY_{ij} &= \\eta_{ij}^{-1}\n\\end{aligned}\n\\]linear predictor quadratic model fit j-th blocks. different model (fitted) one common slopes: glm(y ~ x + (x^2),…)predict new value \\(x\\)","code":"\nlibrary(faraway)\nlibrary(dplyr)\ndata(nes96, package=\"faraway\")\nhead(nes96,3)\n#>   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote\n#> 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole\n#> 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton\n#> 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton\ntable(nes96$PID)\n#> \n#>  strDem weakDem  indDem  indind  indRep weakRep  strRep \n#>     200     180     108      37      94     150     175\nnes96$Political_Strength <- NA\nnes96$Political_Strength[nes96$PID %in% c(\"strDem\", \"strRep\")] <-\n    \"Strong\"\nnes96$Political_Strength[nes96$PID %in% c(\"weakDem\", \"weakRep\")] <-\n    \"Weak\"\nnes96$Political_Strength[nes96$PID %in% c(\"indDem\", \"indind\", \"indRep\")] <-\n    \"Neutral\"\nnes96 %>% group_by(Political_Strength) %>% summarise(Count = n())\n#> # A tibble: 3 × 2\n#>   Political_Strength Count\n#>   <chr>              <int>\n#> 1 Neutral              239\n#> 2 Strong               375\n#> 3 Weak                 330\nlibrary(ggplot2)\nPlot_DF <- nes96 %>%\n    mutate(Age_Grp = cut_number(age, 4)) %>%\n    group_by(Age_Grp, Political_Strength) %>%\n    summarise(count = n()) %>%\n    group_by(Age_Grp) %>%\n    mutate(etotal = sum(count), proportion = count / etotal)\n\nAge_Plot <- ggplot(\n    Plot_DF,\n    aes(\n        x        = Age_Grp,\n        y        = proportion,\n        group    = Political_Strength,\n        linetype = Political_Strength,\n        color    = Political_Strength\n    )\n) +\n    geom_line(size = 2)\nAge_Plot\nlibrary(nnet)\nMultinomial_Model <-\n    multinom(Political_Strength ~ age + educ, nes96, trace = F)\nsummary(Multinomial_Model)\n#> Call:\n#> multinom(formula = Political_Strength ~ age + educ, data = nes96, \n#>     trace = F)\n#> \n#> Coefficients:\n#>        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4\n#> Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307\n#> Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634\n#>            educ^5     educ^6\n#> Strong -0.1664377 -0.1359449\n#> Weak   -0.1489030 -0.2173144\n#> \n#> Std. Errors:\n#>        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4\n#> Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776\n#> Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149\n#>           educ^5    educ^6\n#> Strong 0.2515012 0.2166774\n#> Weak   0.2643747 0.2199186\n#> \n#> Residual Deviance: 2024.596 \n#> AIC: 2056.596\nMultinomial_Step <- step(Multinomial_Model,trace = 0)\n#> trying - age \n#> trying - educ \n#> trying - age\nMultinomial_Step\n#> Call:\n#> multinom(formula = Political_Strength ~ age, data = nes96, trace = F)\n#> \n#> Coefficients:\n#>        (Intercept)          age\n#> Strong -0.01988977  0.009832916\n#> Weak    0.59497046 -0.005954348\n#> \n#> Residual Deviance: 2030.756 \n#> AIC: 2038.756\npchisq(q = deviance(Multinomial_Step) - deviance(Multinomial_Model),\ndf = Multinomial_Model$edf-Multinomial_Step$edf,lower=F)\n#> [1] 0.9078172\nPlotData <- data.frame(age = seq(from = 19, to = 91))\nPreds <-\n  PlotData %>% bind_cols(data.frame(predict(\n    object = Multinomial_Step,\n    PlotData, type = \"probs\"\n  )))\n\nplot(\n  x       = Preds$age,\n  y       = Preds$Neutral,\n  type    = \"l\",\n  ylim    = c(0.2, 0.6),\n  col     = \"black\",\n  ylab    = \"Proportion\",\n  xlab    = \"Age\"\n)\n\nlines(x   = Preds$age,\n      y   = Preds$Weak,\n      col = \"blue\")\nlines(x   = Preds$age,\n      y   = Preds$Strong,\n      col = \"red\")\n\nlegend(\n  'topleft',\n  legend  = c('Neutral', 'Weak', 'Strong'),\n  col     = c('black', 'blue', 'red'),\n  lty     = 1\n)\npredict(Multinomial_Step,data.frame(age = 34)) # predicted result (categoriy of political strength) of 34 year old\n#> [1] Weak\n#> Levels: Neutral Strong Weak\npredict(Multinomial_Step,data.frame(age = c(34,35)),type=\"probs\") # predicted result of the probabilities of each level of political strength for a 34 and 35\n#>     Neutral    Strong      Weak\n#> 1 0.2597275 0.3556910 0.3845815\n#> 2 0.2594080 0.3587639 0.3818281\nlibrary(agridat)\ndat <- agridat::streibig.competition\n# See Schaberger and Pierce, pages 370+\n# Consider only the mono-species barley data (no competition from Sinapis)\ngammaDat <- subset(dat, sseeds < 1)\ngammaDat <-\n    transform(gammaDat,\n              x = bseeds,\n              y = bdwt,\n              block = factor(block))\n# Inverse yield looks like it will be a good fit for Gamma's inverse link\nggplot(gammaDat, aes(x = x, y = 1 / y)) + \n    geom_point(aes(color = block, shape = block)) +\n    xlab('Seeding Rate') + \n    ylab('Inverse yield') + \n    ggtitle('Streibig Competion - Barley only')\n# linear predictor is quadratic, with separate intercept and slope per block\nm1 <-\n    glm(y ~ block + block * x + block * I(x ^ 2),\n        data = gammaDat,\n        family = Gamma(link = \"inverse\"))\nsummary(m1)\n#> \n#> Call:\n#> glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = \"inverse\"), \n#>     data = gammaDat)\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -1.21708  -0.44148   0.02479   0.17999   0.80745  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***\n#> blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    \n#> blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    \n#> x              -2.075e-03  1.099e-03  -1.888 0.072884 .  \n#> I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    \n#> blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    \n#> blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    \n#> blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    \n#> blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Gamma family taken to be 0.3232083)\n#> \n#>     Null deviance: 13.1677  on 29  degrees of freedom\n#> Residual deviance:  7.8605  on 21  degrees of freedom\n#> AIC: 225.32\n#> \n#> Number of Fisher Scoring iterations: 5\nnewdf <-\n    expand.grid(x = seq(0, 120, length = 50), block = factor(c('B1', 'B2', 'B3')))\n\nnewdf$pred <- predict(m1, new = newdf, type = 'response')\n\nggplot(gammaDat, aes(x = x, y = y)) + \n    geom_point(aes(color = block, shape = block)) +\n    xlab('Seeding Rate') + ylab('Inverse yield') + \n    ggtitle('Streibig Competion - Barley only Predictions') +\n    geom_line(data = newdf, aes(\n        x = x,\n        y = pred,\n        color = block,\n        linetype = block\n    ))"},{"path":"generalized-linear-models.html","id":"generalization","chapter":"7 Generalized Linear Models","heading":"7.7 Generalization","text":"can see Poisson regression looks similar logistic regression. Hence, can generalize class modeling. Thanks Nelder Wedderburn (1972), generalized linear models (GLMs). Estimation generalize models.Exponential Family\ntheory GLMs developed data distribution given y exponential family.\nform data distribution useful GLMs \\[\nf(y;\\theta, \\phi) = \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y, \\phi))\n\\]\\(\\theta\\) called natural parameter\\(\\phi\\) called dispersion parameterNote:family includes [Gamma], [Normal], [Poisson], . parameterization exponential family, check linkExampleif \\(Y \\sim N(\\mu, \\sigma^2)\\)\\[\n\\begin{aligned}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{(2\\pi \\sigma^2)^{1/2}}\\exp(-\\frac{1}{2\\sigma^2}(y- \\mu)^2) \\\\\n&= \\exp(-\\frac{1}{2\\sigma^2}(y^2 - 2y \\mu +\\mu^2)- \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{y \\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y , \\phi))\n\\end{aligned}\n\\]\\(\\theta = \\mu\\)\\(b(\\theta) = \\frac{\\mu^2}{2}\\)\\((\\phi) = \\sigma^2 = \\phi\\)\\(c(y , \\phi) = - \\frac{1}{2}(\\frac{y^2}{\\phi}+\\log(2\\pi \\sigma^2))\\)Properties GLM exponential families\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime”, transpose)\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime”, transpose)\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\n\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\n\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\\((), b(), c()\\) identifiable, derive expected value variance Y.\\((), b(), c()\\) identifiable, derive expected value variance Y.ExampleNormal distribution\\[\n\\begin{aligned}\nb'(\\theta) &= \\frac{\\partial b(\\mu^2/2)}{\\partial \\mu} = \\mu \\\\\nV(\\mu) &= \\frac{\\partial^2 (\\mu^2/2)}{\\partial \\mu^2} = 1 \\\\\n\\var(Y) &= (\\phi) = \\sigma^2\n\\end{aligned}\n\\]Poisson distribution\\[\n\\begin{aligned}\nf(y, \\theta, \\phi) &= \\frac{\\mu^y \\exp(-\\mu)}{y!} \\\\\n&= \\exp(y\\log(\\mu) - \\mu - \\log(y!)) \\\\\n&= \\exp(y\\theta - \\exp(\\theta) - \\log(y!))\n\\end{aligned}\n\\]\\(\\theta = \\log(\\mu)\\)\\((\\phi) = 1\\)\\(b(\\theta) = \\exp(\\theta)\\)\\(c(y, \\phi) = \\log(y!)\\)Hence,\\[\n\\begin{aligned}\nE(Y) = \\frac{\\partial b(\\theta)}{\\partial \\theta} = \\exp(\\theta) &= \\mu \\\\\nvar(Y) = \\frac{\\partial^2 b(\\theta)}{\\partial \\theta^2} &= \\mu\n\\end{aligned}\n\\]Since \\(\\mu = E(Y) = b'(\\theta)\\)GLM, take monotone function (typically nonlinear) \\(\\mu\\) linear set covariates\\[\ng(\\mu) = g(b'(\\theta)) = \\mathbf{x'\\beta}\n\\]Equivalently,\\[\n\\mu = g^{-1}(\\mathbf{x'\\beta})\n\\]\\(g(.)\\) link function since links mean response (\\(\\mu = E(Y)\\)) linear expression covariatesSome people use \\(\\eta = \\mathbf{x'\\beta}\\) \\(\\eta\\) = “linear predictor”GLM composed 2 componentsThe random component:distribution chosen model response variables \\(Y_1,...,Y_n\\)distribution chosen model response variables \\(Y_1,...,Y_n\\)specified choice fo \\((), b(), c()\\) exponential formis specified choice fo \\((), b(), c()\\) exponential formNotation:\nAssume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densities\nAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .\nNotation:Assume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densitiesAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .systematic componentis portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)consists 2 parts:\nlink function, \\(g(.)\\)\nlinear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)\nconsists 2 parts:link function, \\(g(.)\\)linear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)Notation:\nassume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)\nparameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)\nNotation:assume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)parameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)Canonical LinkTo choose \\(g(.)\\), can use canonical link function (Remember: Canonical link just special case link function)link function \\(g(.)\\) \\(g(\\mu_i) = \\eta_i = \\theta_i\\), natural parameter, \\(g(.)\\) canonical link.\\(b(\\theta)\\) = cumulant moment generating function\\(g(\\mu)\\) link function, relates linear predictor mean required monotone increasing, continuously differentiable invertible.Equivalently, can think canonical link function \\[\n\\gamma^{-1} \\circ g^{-1} = \n\\] identity. Hence,\\[\n\\theta = \\eta\n\\]inverse link\\(g^{-1}(.)\\) also known mean function, take linear predictor output (ranging \\(-\\infty\\) \\(\\infty\\)) transform different scale.Exponential: converts \\(\\mathbf{\\beta X}\\) curve restricted 0 \\(\\infty\\) (can see useful case want convert linear predictor non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\)Exponential: converts \\(\\mathbf{\\beta X}\\) curve restricted 0 \\(\\infty\\) (can see useful case want convert linear predictor non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\)Inverse Logit (also known logistic): converts \\(\\mathbf{\\beta X}\\) curve restricted 0 1, useful case want convert linear predictor probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\)\n\\(y\\) = linear predictor value\n\\(\\theta\\) = transformed value\nInverse Logit (also known logistic): converts \\(\\mathbf{\\beta X}\\) curve restricted 0 1, useful case want convert linear predictor probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\)\\(y\\) = linear predictor value\\(\\theta\\) = transformed valueThe identity link \\[\n\\begin{aligned}\n\\eta_i &= g(\\mu_i) = \\mu_i \\\\\n\\mu_i &= g^{-1}(\\eta_i) = \\eta_i\n\\end{aligned}\n\\]Table 15.1 Generalized Linear Models 15.1 Structure Generalized Linear ModelsMore example link functions inverses can found page 380ExampleNormal random componentMean Response: \\(\\mu_i = \\theta_i\\)Mean Response: \\(\\mu_i = \\theta_i\\)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Binomial random componentMean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Mean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Poisson random componentMean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Mean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Gamma random component:Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Inverse Gaussian randomCanonical Link: \\(g(\\mu_i) = \\frac{1}{\\mu_i^2}\\)","code":""},{"path":"generalized-linear-models.html","id":"estimation","chapter":"7 Generalized Linear Models","heading":"7.7.1 Estimation","text":"MLE parameters systematic component (\\(\\beta\\))Unification derivation computation (thanks exponential forms)unification estimation dispersion parameter (\\(\\phi\\))","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-beta","chapter":"7 Generalized Linear Models","heading":"7.7.1.1 Estimation of \\(\\beta\\)","text":"\\[\n\\begin{aligned}\nf(y_i ; \\theta_i, \\phi) &= \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi)) \\\\\nE(Y_i) &= \\mu_i = b'(\\theta) \\\\\nvar(Y_i) &= b''(\\theta)(\\phi) = V(\\mu_i)(\\phi) \\\\\ng(\\mu_i) &= \\mathbf{x}_i'\\beta = \\eta_i\n\\end{aligned}\n\\]log-likelihood single observation \\(l_i (\\beta,\\phi)\\). log-likelihood n observations \\[\n\\begin{aligned}\nl(\\beta,\\phi) &= \\sum_{=1}^n l_i (\\beta,\\phi) \\\\\n&= \\sum_{=1}^n (\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\end{aligned}\n\\]Using MLE find \\(\\beta\\), use chain rule get derivatives\\[\n\\begin{aligned}\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} &=  \\frac{\\partial l_i (\\beta, \\phi)}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i}\\times \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\\\\n&= \\sum_{=1}^{n}(\\frac{ y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij})\n\\end{aligned}\n\\]let\\[\nw_i \\equiv ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1}\n\\],\\[\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} = \\sum_{=1}^n (\\frac{y_i \\mu_i}{(\\phi)} \\times w_i \\times \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\times x_{ij})\n\\]can also get second derivatives using chain rule.Example:\\[Newton-Raphson\\] algorithm, need\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k})\n\\]\\((j,k)\\)-th element Fisher information matrix \\(\\mathbf{}(\\beta)\\)Hence,\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)}x_{ij}x_{ik}\n\\](j,k)th elementIf Bernoulli model logit link function (canonical link)\\[\n\\begin{aligned}\nb(\\theta) &= \\log(1 + \\exp(\\theta)) = \\log(1 + \\exp(\\mathbf{x'\\beta})) \\\\\n(\\phi) &= 1  \\\\\nc(y_i, \\phi) &= 0 \\\\\nE(Y) = b'(\\theta) &= \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\mu = p \\\\\n\\eta = g(\\mu) &= \\log(\\frac{\\mu}{1-\\mu}) = \\theta = \\log(\\frac{p}{1-p}) = \\mathbf{x'\\beta}\n\\end{aligned}\n\\]\\(Y_i\\), = 1,.., log-likelihood \\[\nl_i (\\beta, \\phi) = \\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) = y_i \\mathbf{x}'_i \\beta - \\log(1+ \\exp(\\mathbf{x'\\beta}))\n\\]Additionally,\\[\n\\begin{aligned}\nV(\\mu_i) &= \\mu_i(1-\\mu_i)= p_i (1-p_i) \\\\\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} &= p_i(1-p_i)\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\frac{\\partial l(\\beta, \\phi)}{\\partial \\beta_j} &= \\sum_{=1}^n[\\frac{y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)}\\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij}] \\\\\n&= \\sum_{=1}^n (y_i - p_i) \\times \\frac{1}{p_i(1-p_i)} \\times p_i(1-p_i) \\times x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - p_i) x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - \\frac{\\exp(\\mathbf{x'_i\\beta})}{1+ \\exp(\\mathbf{x'_i\\beta})})x_{ij}\n\\end{aligned}\n\\]\\[\nw_i = ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1} = p_i (1-p_i)\n\\]\\[\n\\mathbf{}_{jk}(\\mathbf{\\beta}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)} x_{ij}x_{ik} = \\sum_{=1}^n p_i (1-p_i)x_{ij}x_{ik}\n\\]Fisher-scoring algorithm MLE \\(\\mathbf{\\beta}\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m+1)}\n=\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m)} +\n\\mathbf{}^{-1}(\\mathbf{\\beta})\n\\left(\n\\begin{array}\n{c}\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_1} \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_2} \\\\\n. \\\\\n. \\\\\n. \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_p} \\\\\n\\end{array}\n\\right)|_{\\beta = \\beta^{(m)}}\n\\]Similar \\[Newton-Raphson\\] expect matrix second derivatives expected value second derivative matrix.matrix notation,\\[\n\\begin{aligned}\n\\frac{\\partial l }{\\partial \\beta} &= \\frac{1}{(\\phi)}\\mathbf{X'W\\Delta(y - \\mu)} \\\\\n&= \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}(y - \\mu)} \\\\\n\\end{aligned}\n\\]\\[\n\\mathbf{}(\\beta) = \\frac{1}{(\\phi)}\\mathbf{X'WX} = \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}F}\n\\]\\(\\mathbf{X}\\) \\(n \\times p\\) matrix covariates\\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\)\\(\\mathbf{\\Delta}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\)\\(\\mathbf{F} = \\mathbf{\\frac{\\partial \\mu}{\\partial \\beta}}\\) \\(n \\times p\\) matrix \\(\\)-th row \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = (\\frac{\\partial \\mu_i}{\\partial \\eta_i})\\mathbf{x}'_i\\)\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(V(\\mu_i)\\)Setting derivative log-likelihood equal 0, ML estimating equations \\[\n\\mathbf{F'V^{-1}y= F'V^{-1}\\mu}\n\\]components equation expect y depends parameters \\(\\beta\\)Special CasesIf one canonical link, estimating equations reduce \\[\n\\mathbf{X'y= X'\\mu}\n\\]one identity link, \\[\n\\mathbf{X'V^{-1}y = X'V^{-1}X\\hat{\\beta}}\n\\]gives generalized least squares estimatorGenerally, can rewrite Fisher-scoring algorithm \\[\n\\beta^{(m+1)} = \\beta^{(m)} + \\mathbf{(\\hat{F}'\\hat{V}^{-1}\\hat{F})^{-1}\\hat{F}'\\hat{V}^{-1}(y- \\hat{\\mu})}\n\\]Since \\(\\hat{F},\\hat{V}, \\hat{\\mu}\\) depend \\(\\beta\\), evaluate \\(\\beta^{(m)}\\)starting values \\(\\beta^{(0)}\\), can iterate convergence.Notes:\\((\\phi)\\) constant form \\(m_i \\phi\\) known \\(m_i\\), \\(\\phi\\) cancels.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-phi","chapter":"7 Generalized Linear Models","heading":"7.7.1.2 Estimation of \\(\\phi\\)","text":"2 approaches:MLE\\[\n\\frac{\\partial l_i}{\\partial \\phi} = \\frac{(\\theta_i y_i - b(\\theta_i)'(\\phi))}{^2(\\phi)} + \\frac{\\partial c(y_i,\\phi)}{\\partial \\phi}\n\\]MLE \\(\\phi\\) solves\\[\n\\frac{^2(\\phi)}{'(\\phi)}\\sum_{=1}^n \\frac{\\partial c(y_i, \\phi)}{\\partial \\phi} = \\sum_{=1}^n(\\theta_i y_i - b(\\theta_i))\n\\]Situation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleSituation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleEven canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Even canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Moment Estimation (“Bias Corrected \\(\\chi^2\\)”)\nMLE conventional approach estimation \\(\\phi\\) GLMS.\nexponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n\\begin{aligned}\n(\\phi) &= \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\end{aligned}\n\\] \\(p\\) dimension \\(\\beta\\)\nGLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]\nmethod estimator \\((\\phi)=\\phi\\) \nMoment Estimation (“Bias Corrected \\(\\chi^2\\)”)MLE conventional approach estimation \\(\\phi\\) GLMS.exponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n\\begin{aligned}\n(\\phi) &= \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\end{aligned}\n\\] \\(p\\) dimension \\(\\beta\\)GLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]method estimator \\((\\phi)=\\phi\\) \\[\n\\hat{\\phi} = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"inference-1","chapter":"7 Generalized Linear Models","heading":"7.7.2 Inference","text":"\\[\n\\hat{var}(\\beta) = (\\phi)(\\mathbf{\\hat{F}'\\hat{V}\\hat{F}})^{-1}\n\\]\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix diagonal elements given \\(V(\\mu_i)\\)\\(\\mathbf{F}\\) \\(n \\times p\\) matrix given \\(\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\\)\\(\\mathbf{V,F}\\) dependent mean \\(\\mu\\), thus \\(\\beta\\). Hence, estimates (\\(\\mathbf{\\hat{V},\\hat{F}}\\)) depend \\(\\hat{\\beta}\\).\\[\nH_0: \\mathbf{L\\beta = d}\n\\]\\(\\mathbf{L}\\) q x p matrix Wald test\\[\nW = \\mathbf{(L \\hat{\\beta}-d)'((\\phi)L(\\hat{F}'\\hat{V}^{-1}\\hat{F})L')^{-1}(L \\hat{\\beta}-d)}\n\\]follows \\(\\chi_q^2\\) distribution (asymptotically), \\(q\\) rank \\(\\mathbf{L}\\)simple case \\(H_0: \\beta_j = 0\\) gives \\(W = \\frac{\\hat{\\beta}^2_j}{\\hat{var}(\\hat{\\beta}_j)} \\sim \\chi^2_1\\) asymptoticallyLikelihood ratio test\\[\n\\Lambda = 2 (l(\\hat{\\beta}_f)-l(\\hat{\\beta}_r)) \\sim \\chi^2_q\n\\]\\(q\\) number constraints used fit reduced model \\(\\hat{\\beta}_r\\), \\(\\hat{\\beta}_r\\) fit full model.Wald test easier implement, likelihood ratio test better (especially small samples).","code":""},{"path":"generalized-linear-models.html","id":"deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3 Deviance","text":"Deviance necessary goodness fit, inference alternative estimation dispersion parameter. define consider Deviance likelihood ratio perspective.Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model”, saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model”, saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\](McCullagh 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] (McCullagh 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] \\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = devianceNote:random component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \n\\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\nrandom component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution \\(\\)-th observation.\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution \\(\\)-th observation.\\(D\\) used model selection\\(D\\) used model selection\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(d_i\\) used form deviance residuals\\(d_i\\) used form deviance residualsNormalWe \\[\n\\begin{aligned}\n\\theta &= \\mu \\\\\n\\phi &= \\sigma^2 \\\\\nb(\\theta) &= \\frac{1}{2} \\theta^2 \\\\\n(\\phi) &= \\phi\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\tilde{\\theta}_i &= y_i \\\\\n\\hat{\\theta}_i &= \\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i)\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nD &= 2 \\sum_{1=1}^n Y^2_i - y_i \\hat{\\mu}_i - \\frac{1}{2}y^2_i + \\frac{1}{2} \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n y_i^2 - 2y_i \\hat{\\mu}_i + \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n (y_i - \\hat{\\mu}_i)^2\n\\end{aligned}\n\\]residual sum squaresPoisson\\[\n\\begin{aligned}\nf(y) &= \\exp\\{y\\log(\\mu) - \\mu - \\log(y!)\\} \\\\\n\\theta &= \\log(\\mu) \\\\\nb(\\theta) &= \\exp(\\theta) \\\\\n(\\phi) &= 1 \\\\\n\\tilde{\\theta}_i &= \\log(y_i) \\\\\n\\hat{\\theta}_i &= \\log(\\hat{\\mu}_i) \\\\\n\\hat{\\mu}_i &= g^{-1}(\\hat{\\eta}_i)\n\\end{aligned}\n\\],\\[\n\\begin{aligned}\nD &= 2 \\sum_{= 1}^n y_i \\log(y_i) - y_i \\log(\\hat{\\mu}_i) - y_i + \\hat{\\mu}_i \\\\\n&= 2 \\sum_{= 1}^n y_i \\log(\\frac{y_i}{\\hat{\\mu}_i}) - (y_i - \\hat{\\mu}_i)\n\\end{aligned}\n\\]\\[\nd_i = 2\\{y_i \\log(\\frac{y_i}{\\hat{\\mu}})- (y_i - \\hat{\\mu}_i)\\}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"analysis-of-deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3.1 Analysis of Deviance","text":"difference deviance reduced full model, q difference number free parameters, asymptotic \\(\\chi^2_q\\). likelihood ratio test\\[\nD^*(\\mathbf{y;\\hat{\\mu}_r}) - D^*(\\mathbf{y;\\hat{\\mu}_f}) = 2\\{l(\\mathbf{y;\\hat{\\mu}_f})-l(\\mathbf{y;\\hat{\\mu}_r})\\}\n\\]comparison models Analysis Deviance. GLM uses analysis model selection.estimation \\(\\phi\\) \\[\n\\hat{\\phi} = \\frac{D(\\mathbf{y, \\hat{\\mu}})}{n - p}\n\\]\\(p\\) = number parameters fit.Excessive use \\(\\chi^2\\) test problematic since asymptotic (McCullagh 2019)","code":""},{"path":"generalized-linear-models.html","id":"deviance-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.2 Deviance Residuals","text":"\\(D = \\sum_{=1}^{n}d_i\\). , define deviance residuals\\[\nr_{D_i} = \\text{sign}(y_i -\\hat{\\mu}_i)\\sqrt{d_i}\n\\]Standardized version deviance residuals \\[\nr_{s,} = \\frac{y_i -\\hat{\\mu}}{\\hat{\\sigma}(1-h_{ii})^{1/2}}\n\\]Let \\(\\mathbf{H^{GLM} = W^{1/2}X(X'WX)^{-1}X'W^{-1/2}}\\), \\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\) (see Estimation \\(\\beta\\)). Standardized deviance residuals equivalently\\[\nr_{s, D_i} = \\frac{r_{D_i}}{\\{\\hat{\\phi}(1-h_{ii}^{glm}\\}^{1/2}}\n\\]\\(h_{ii}^{glm}\\) \\(\\)-th diagonal \\(\\mathbf{H}^{GLM}\\)","code":""},{"path":"generalized-linear-models.html","id":"pearson-chi-square-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.3 Pearson Chi-square Residuals","text":"Another \\(\\chi^2\\) statistic Pearson \\(\\chi^2\\) statistics: (assume \\(m_i = 1\\))\\[\nX^2 = \\sum_{=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]\\(\\hat{\\mu}_i\\) fitted mean response fo model interest.Scaled Pearson \\(\\chi^2\\) statistic given \\(\\frac{X^2}{\\phi} \\sim \\chi^2_{n-p}\\) p number parameters estimated. Hence, Pearson \\(\\chi^2\\) residuals \\[\nX^2_i = \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]following assumptions:Independent samplesNo -dispersion: \\(\\phi = 1\\), \\(\\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p}\\) \\(\\frac{X^2}{n-p}\\) value substantially larger 1 indicates improperly specified model overdispersionMultiple groupsthen \\(\\frac{X^2}{\\phi}\\) \\(D^*(\\mathbf{y; \\hat{\\mu}})\\) follow \\(\\chi^2_{n-p}\\)","code":""},{"path":"generalized-linear-models.html","id":"diagnostic-plots","chapter":"7 Generalized Linear Models","heading":"7.7.4 Diagnostic Plots","text":"Standardized residual Plots:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))\nStandardized residual Plots:plot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:plot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))see:\nTrend, means might wrong link function, choice scale\nSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)\nsee:Trend, means might wrong link function, choice scaleSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.","code":""},{"path":"generalized-linear-models.html","id":"goodness-of-fit","chapter":"7 Generalized Linear Models","heading":"7.7.5 Goodness of Fit","text":"assess goodness fit, can useDeviancePearson Chi-square ResidualsIn nested model, use likelihood-based information measures:\\[\n\\begin{aligned}\nAIC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p \\\\\nAICC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p(\\frac{n}{n-p-1}) \\\\\nBIC &= 2l(\\hat{\\mu}) + p \\log(n)\n\\end{aligned}\n\\]\\(l(\\hat{\\mu})\\) log-likelihood evaluated parameter estimates\\(p\\) number parameters\\(n\\) number observations.Note: use data model (.e., link function, random underlying random distribution). can different number parameters.Even though statisticians try come measures similar \\(R^2\\), practice, appropriate. example, compare log-likelihood fitted model model just intercept:\\[\nR^2_p = 1 - \\frac{l(\\hat{\\mu})}{l(\\hat{\\mu}_0)}\n\\]certain specific random components binary response model, rescaled generalized \\(R^2\\)\\[\n\\bar{R}^2 = \\frac{R^2_*}{\\max(R^2_*)} = \\frac{1-\\exp\\{-\\frac{2}{n}(l(\\hat{\\mu}) - l(\\hat{\\mu}_0) \\}}{1 - \\exp\\{\\frac{2}{n}l(\\hat{\\mu}_0)\\}}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"over-dispersion","chapter":"7 Generalized Linear Models","heading":"7.7.6 Over-Dispersion","text":"cases \\(\\phi = 1\\). Recall \\(b''(\\theta)= V(\\mu)\\) check Estimation \\(\\phi\\).find\\(\\phi >1\\): -dispersion (.e., much variation independent binomial Poisson distribution).\\(\\phi<1\\): -dispersion (.e., little variation independent binomial Poisson distribution).either -dispersion, means might unspecified random component, couldSelect different random component distribution can accommodate -dispersion (e.g., negative binomial, Conway-Maxwell Poisson)use Nonlinear Generalized Linear Mixed Models handle random effects generalized linear models.","code":""},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"dependent-data","chapter":"8 Linear Mixed Models","heading":"8.1 Dependent Data","text":"Forms dependent data:Multivariate measurements different individuals: (e.g., person’s blood pressure, fat, etc correlated)Clustered measurements: (e.g., blood pressure measurements people family can correlated).Repeated measurements: (e.g., measurement cholesterol time can correlated) “data collected repeatedly experimental material treatments applied initially, data repeated measure.” (Schabenberger Pierce 2001)Longitudinal data: (e.g., individual’s cholesterol tracked time correlated): “data collected repeatedly time observational study termed longitudinal.” (Schabenberger Pierce 2001)Spatial data: (e.g., measurement individuals living neighborhood correlated)Hence, like account correlations.Linear Mixed Model (LMM), also known Mixed Linear Model 2 components:Fixed effect (e.g, gender, age, diet, time)Fixed effect (e.g, gender, age, diet, time)Random effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsRandom effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsReview Two-Way Mixed Effects ANOVAWe choose model random subject-specific effect instead including dummy subject covariates model :reduction number parameters estimatewhen inference, make sense can infer population (.e., random effect).LLM MotivationIn repeated measurements analysis \\(Y_{ij}\\) response \\(\\)-th individual measured \\(j\\)-th time,\\(=1,…,N\\) ; \\(j = 1,…,n_i\\)\\[\n\\mathbf{Y}_i =\n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n. \\\\\n.\\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right)\n\\]measurements subject \\(\\).Stage 1: (Regression Model) response changes time \\(\\)-th subject\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]\\(Z_i\\) \\(n_i \\times q\\) matrix known covariates\\(\\beta_i\\) unknown \\(q \\times 1\\) vector subjective -specific coefficients (regression coefficients different subject)\\(\\epsilon_i\\) random errors (typically \\(\\sim N(0, \\sigma^2 )\\))notice two many \\(\\beta\\) estimate . Hence, motivation second stageStage 2: (Parameter Model)\\[\n\\mathbf{\\beta_i = K_i \\beta + b_i}\n\\]\\(K_i\\) \\(q \\times p\\) matrix known covariates\\(\\beta\\) \\(p \\times 1\\) vector unknown parameter\\(\\mathbf{b}_i\\) independent \\(N(0,D)\\) random variablesThis model explain observed variability subjects respect subject-specific regression coefficients, \\(\\beta_i\\). model different coefficient (\\(\\beta_i\\)) respect \\(\\beta\\).Example:Stage 1:\\[\nY_{ij} = \\beta_{1i} + \\beta_{2i}t_{ij} + \\epsilon_{ij}\n\\]\\(j = 1,..,n_i\\)matrix notation,\\[\n\\mathbf{Y_i} =\n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right); \\mathbf{Z}_i =\n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n. & . \\\\\n1 & t_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\beta_i =\n\\left(\n\\begin{array}\n{c}\n\\beta_{1i} \\\\\n\\beta_{2i}\n\\end{array}\n\\right); \\epsilon_i =\n\\left(\n\\begin{array}\n{c}\n\\epsilon_{i1} \\\\\n. \\\\\n\\epsilon_{in_i}\n\\end{array}\n\\right)\n\\]Thus,\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]Stage 2:\\[\n\\begin{aligned}\n\\beta_{1i} &= \\beta_0 + b_{1i} \\\\\n\\beta_{2i} &= \\beta_1 L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i}\n\\end{aligned}\n\\]\\(L_i, H_i, C_i\\) indicator variables defined 1 subject falls different categories.Subject specific intercepts depend upon treatment, \\(\\beta_0\\) (average response start treatment), \\(\\beta_1 , \\beta_2, \\beta_3\\) (average time effects three treatment groups).\\[\n\\begin{aligned}\n\\mathbf{K}_i &= \\left(\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i\n\\end{array}\n\\right) \\\\\n\\beta &= (\\beta_0 , \\beta_1, \\beta_2, \\beta_3)' \\\\\n\\mathbf{b}_i &=\n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i} \\\\\n\\end{array}\n\\right) \\\\\n\\beta_i &= \\mathbf{K_i \\beta + b_i}\n\\end{aligned}\n\\]get \\(\\hat{\\beta}\\), can fit model sequentially:Estimate \\(\\hat{\\beta_i}\\) first stageEstimate \\(\\hat{\\beta}\\) second stage replacing \\(\\beta_i\\) \\(\\hat{\\beta}_i\\)However, problems arise method:information lost summarizing vector \\(\\mathbf{Y}_i\\) solely \\(\\hat{\\beta}_i\\)need account variability replacing \\(\\beta_i\\) estimatedifferent subjects might different number observations.address problems, can use Linear Mixed Model (Laird Ware 1982)Substituting stage 2 stage 1:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]Let \\(\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i\\) \\(n_i \\times p\\) matrix . , LMM \\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]\\(= 1,..,N\\)\\(\\beta\\) fixed effects, common subjects\\(\\mathbf{b}_i\\) subject specific random effects. \\(\\mathbf{b}_i \\sim N_q (\\mathbf{0,D})\\)\\(\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) independent\\(\\mathbf{Z}_{(n_i \\times q})\\) \\(\\mathbf{X}_{(n_i \\times p})\\) matrices known covariates.Equivalently, hierarchical form, call conditional hierarchical formulation linear mixed model\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i) \\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D})\n\\end{aligned}\n\\]\\(= 1,..,N\\). denote respective functions \\(f(\\mathbf{Y}_i |\\mathbf{b}_i)\\) \\(f(\\mathbf{b}_i)\\)general,\\[\n\\begin{aligned}\nf(,B) &= f(|B)f(B) \\\\\nf() &= \\int f(,B)dB = \\int f(|B) f(B) dB\n\\end{aligned}\n\\]LMM, marginal density \\(\\mathbf{Y}_i\\) \\[\nf(\\mathbf{Y}_i) = \\int f(\\mathbf{Y}_i | \\mathbf{b}_i) f(\\mathbf{b}_i) d\\mathbf{b}_i\n\\]can shown\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X_i \\beta, Z_i DZ'_i + \\Sigma_i})\n\\]marginal formulation linear mixed modelNotes:longer \\(Z_i b_i\\) mean, add error variance (marginal dependence Y). kinda averaging common effect. Technically, shouldn’t call averaging error b (adding variance covariance matrix), called adding random effectContinue example\\[\nY_{ij} = (\\beta_0 + b_{1i}) + (\\beta_1L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i})t_{ij} + \\epsilon_{ij}\n\\]treatment group\\[\nY_{ik}=\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]Intercepts slopes subject specificDifferent treatment groups different slops, intercept.hierarchical model form\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i)\\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D})\n\\end{aligned}\n\\]X form \\[\n\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\n\\]\\[\n\\begin{aligned}\n\\mathbf{X}_i &= \\mathbf{Z}_i \\mathbf{K}_i \\\\\n&=\n\\left[\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2} \\\\\n. & . \\\\\n1 & t_{in_i}\n\\end{array}\n\\right]\n\\times\n\\left[\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i \\\\\n\\end{array}\n\\right] \\\\\n&=\n\\left[\n\\begin{array}\n{cccc}\n1 & t_{i1}L_i & t_{i1}H_i & T_{i1}C_i \\\\\n1 & t_{i2}L_i & t_{i2}H_i & T_{i2}C_i \\\\\n. &. &. &. \\\\\n1 & t_{in_i}L_i & t_{in_i}H_i & T_{in_i}C_i \\\\\n\\end{array}\n\\right]\\end{aligned}\n\\]\\[\n\\mathbf{b}_i =\n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i}\n\\end{array}\n\\right)\n\\]\\[\nD =\n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12}\\\\\nd_{12} & d_{22}\n\\end{array}\n\\right)\n\\]Assuming \\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}\\), called conditional independence, meaning response subject independent conditional \\(\\mathbf{b}_i\\) \\(\\beta\\)marginal model form\\[\nY_{ij} = \\beta_0 + \\beta_1 L_i t_{ij} + \\beta_2 H_i t_{ij} + \\beta_3 C_i t_{ij} + \\eta_{ij}\n\\]\\(\\eta_i \\sim N(\\mathbf{0},\\mathbf{Z}_i\\mathbf{DZ}_i'+ \\mathbf{\\Sigma}_i)\\)Equivalently,\\[\n\\mathbf{Y_i \\sim N(X_i \\beta, Z_i DZ_i' + \\Sigma_i})\n\\]case \\(n_i = 2\\)\\[\n\\begin{aligned}\n\\mathbf{Z_iDZ_i'} &=\n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2}\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12} \\\\\nd_{12} & d_{22}\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\n1 & 1 \\\\\nt_{i1} & t_{i2}\n\\end{array}\n\\right) \\\\\n&=\n\\left(\n\\begin{array}\n{cc}\nd_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\\\\nd_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2  \n\\end{array}\n\\right)\n\\end{aligned}\n\\]\\[\nvar(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \\sigma^2\n\\]top correlation errors, marginal implies variance function response quadratic time, positive curvature \\(d_{22}\\)","code":""},{"path":"linear-mixed-models.html","id":"random-intercepts-model","chapter":"8 Linear Mixed Models","heading":"8.1.1 Random-Intercepts Model","text":"remove random slopes,assumption variability subject-specific slopes can attributed treatment differencesthe model random-intercepts model. subject specific intercepts, slopes within treatment group.\\[\n\\begin{aligned}\n\\mathbf{Y}_i | b_i &\\sim N(\\mathbf{X}_i \\beta + 1 b_i , \\Sigma_i) \\\\\nb_i &\\sim N(0,d_{11})\n\\end{aligned}\n\\]marginal model (\\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}\\))\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, 11'd_{11} + \\sigma^2 \\mathbf{})\n\\]marginal covariance matrix \\[\n\\begin{aligned}\ncov(\\mathbf{Y}_i)  &= 11'd_{11} + \\sigma^2I \\\\\n&=\n\\left(\n\\begin{array}\n{cccc}\nd_{11}+ \\sigma^2 & d_{11} & ... & d_{11} \\\\\nd_{11} & d_{11} + \\sigma^2 & d_{11} & ... \\\\\n. & . & . & . \\\\\nd_{11} & ... & ... & d_{11} + \\sigma^2\n\\end{array}\n\\right)\n\\end{aligned}\n\\]associated correlation matrix \\[\ncorr(\\mathbf{Y}_i) =\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & \\rho & ... \\\\\n. & . & . & . \\\\\n\\rho & ... & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho \\equiv \\frac{d_{11}}{d_{11} + \\sigma^2}\\)Thu, haveconstant variance timeequal, positive correlation two measurements subjecta covariance structure called compound symmetry, \\(\\rho\\) called intra-class correlationthat \\(\\rho\\) large, inter-subject variability (\\(d_{11}\\)) large relative intra-subject variability (\\(\\sigma^2\\))","code":""},{"path":"linear-mixed-models.html","id":"covariance-models","chapter":"8 Linear Mixed Models","heading":"8.1.2 Covariance Models","text":"conditional independence assumption, (\\(\\mathbf{\\Sigma_i= \\sigma^2 I_{n_i}}\\)). Consider, \\(\\epsilon_i = \\epsilon_{(1)} + \\epsilon_{(2)}\\), \\(\\epsilon_{(1)}\\) “serial correlation” component. , part individual’s profile response time-varying stochastic processes.\\(\\epsilon_{(2)}\\) measurement error component, independent \\(\\epsilon_{(1)}\\)\\[\n\\mathbf{Y_i = X_i \\beta + Z_i b_i + \\epsilon_{(1)} + \\epsilon_{(2)}}\n\\]\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independent\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independentTo model structure \\(n_i \\times n_i\\) correlation (covariance ) matrix \\(\\mathbf{H}_i\\). Let (j,k)th element \\(\\mathbf{H}_i\\) \\(h_{ijk}= g(t_{ij}t_{ik})\\). function times \\(t_{ij}\\) \\(t_{ik}\\) , assumed function “distance’ times.\\[\nh_{ijk} = g(|t_{ij}-t_{ik}|)\n\\]decreasing function \\(g(.)\\) \\(g(0)=1\\) (correlation matrices).Examples type function:Exponential function: \\(g(|t_{ij}-t_{ik}|) = \\exp(-\\phi|t_{ij} - t_{ik}|)\\)Gaussian function: \\(g(|t_{ij} - t_{ik}|) = \\exp(-\\phi(t_{ij} - t_{ik})^2)\\)Similar structures also used \\(\\mathbf{D}\\) matrix (\\(\\mathbf{b}\\))Example: Autoregressive Covariance StructureA first order Autoregressive Model (AR(1)) form\\[\n\\alpha_t = \\phi \\alpha_{t-1} + \\eta_t\n\\]\\(\\eta_t \\sim iid N (0,\\sigma^2_\\eta)\\), covariance two observations \\[\ncov(\\alpha_t, \\alpha_{t+h}) = \\frac{\\sigma^2_\\eta \\phi^{|h|}}{1- \\phi^2}\n\\]\\(h = 0, \\pm 1, \\pm 2, ...; |\\phi|<1\\)Hence,\\[\ncorr(\\alpha_t, \\alpha_{t+h}) = \\phi^{|h|}\n\\]let \\(\\alpha_T = (\\alpha_1,...\\alpha_T)'\\), \\[\ncorr(\\alpha_T) =\n\\left[\n\\begin{array}\n{ccccc}\n1 & \\phi^1 & \\phi^2 & ... & \\phi^2 \\\\\n\\phi^1 & 1 & \\phi^1 & ... & \\phi^{T-1} \\\\\n\\phi^2 & \\phi^1 & 1 & ... & \\phi^{T-2} \\\\\n. & . & . & . &. \\\\\n\\phi^T & \\phi^{T-1} & \\phi^{T-2} & ... & 1\n\\end{array}\n\\right]\n\\]Notes:correlation decreases time lag increasesThis matrix structure known Toeplitz structureMore complicated covariance structures possible, critical component spatial random effects models time series models.Often, don’t need random effects \\(\\mathbf{b}\\) \\(\\epsilon_{(1)}\\)Time Series section","code":""},{"path":"linear-mixed-models.html","id":"estimation-1","chapter":"8 Linear Mixed Models","heading":"8.2 Estimation","text":"\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\beta, \\mathbf{b}_i, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) must obtain estimation data\\(\\mathbf{\\beta}, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) unknown, fixed, parameters, must estimated data\\(\\mathbf{b}_i\\) random variable. Thus, can’t estimate values, can predict . (.e., can’t estimate random thing).\\(\\hat{\\beta}\\) estimator \\(\\beta\\)\\(\\hat{\\mathbf{b}}_i\\) predictor \\(\\mathbf{b}_i\\),population average estimate \\(\\mathbf{Y}_i\\) \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta}\\)subject-specific prediction \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{b}_i\\)According (Henderson 1975), estimating equations known mixed model equations:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z +B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]\\[\n\\begin{aligned}\n\\mathbf{Y}\n&=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n. \\\\\n\\mathbf{y}_N\n\\end{array}\n\\right] ;\n\\mathbf{X}\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X}_1 \\\\\n. \\\\\n\\mathbf{X}_N\n\\end{array}\n\\right];\n\\mathbf{b} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b}_1 \\\\\n. \\\\\n\\mathbf{b}_N\n\\end{array}\n\\right] ;\n\\epsilon =\n\\left[\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n. \\\\\n\\epsilon_N\n\\end{array}\n\\right]\n\\\\\ncov(\\epsilon) &= \\mathbf{\\Sigma},\n\\mathbf{Z} =\n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{Z}_1 & 0 &  ... & 0 \\\\\n0 & \\mathbf{Z}_2 & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{Z}_n\n\\end{array}\n\\right],\n\\mathbf{B} =\n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{D} & 0 & ... & 0 \\\\\n0 & \\mathbf{D} & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{D}\n\\end{array}\n\\right]\n\\end{aligned}\n\\]model form\\[\n\\begin{aligned}\n\\mathbf{Y} &= \\mathbf{X \\beta + Z b + \\epsilon} \\\\\n\\mathbf{Y} &\\sim N(\\mathbf{X \\beta, ZBZ' + \\Sigma})\n\\end{aligned}\n\\]\\(\\mathbf{V = ZBZ' + \\Sigma}\\), solutions estimating equations can \\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'V^{-1}X)^{-1}X'V^{-1}Y} \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{BZ'V^{-1}(Y-X\\hat{\\beta}})\n\\end{aligned}\n\\]estimate \\(\\hat{\\beta}\\) generalized least squares estimate.predictor, \\(\\hat{\\mathbf{b}}\\) best linear unbiased predictor (BLUP), \\(\\mathbf{b}\\)\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= \\beta \\\\\nvar(\\hat{\\beta}) &= (\\mathbf{X'V^{-1}X})^{-1} \\\\\nE(\\hat{\\mathbf{b}}) &= 0\n\\end{aligned}\n\\]\\[\nvar(\\mathbf{\\hat{b}-b}) = \\mathbf{B-BZ'V^{-1}ZB + BZ'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}B}\n\\]variance variance prediction error (mean squared prediction error, MSPE), meaningful \\(var(\\hat{\\mathbf{b}})\\), since MSPE accounts variance bias prediction.derive mixed model equations, consider\\[\n\\mathbf{\\epsilon = Y - X\\beta - Zb}\n\\]Let \\(T = \\sum_{=1}^N n_i\\) total number observations (.e., length \\(\\mathbf{Y},\\epsilon\\)) \\(Nq\\) length \\(\\mathbf{b}\\). joint distribution \\(\\mathbf{b, \\epsilon}\\) \\[\nf(\\mathbf{b,\\epsilon})= \\frac{1}{(2\\pi)^{(T+ Nq)/2}}\n\\left|\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right| ^{-1/2}\n\\exp\n\\left(\n-\\frac{1}{2}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]\n\\right)\n\\]Maximization \\(f(\\mathbf{b},\\epsilon)\\) respect \\(\\mathbf{b}\\) \\(\\beta\\) requires minimization \\[\n\\begin{aligned}\nQ &=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right] \\\\\n&= \\mathbf{b'B^{-1}b+(Y-X \\beta-Zb)'\\Sigma^{-1}(Y-X \\beta-Zb)}\n\\end{aligned}\n\\]Setting derivatives Q respect \\(\\mathbf{b}\\) \\(\\mathbf{\\beta}\\) zero leads system equations:\\[\n\\begin{aligned}\n\\mathbf{X'\\Sigma^{-1}X\\beta + X'\\Sigma^{-1}Zb} &= \\mathbf{X'\\Sigma^{-1}Y}\\\\\n\\mathbf{(Z'\\Sigma^{-1}Z + B^{-1})b + Z'\\Sigma^{-1}X\\beta} &= \\mathbf{Z'\\Sigma^{-1}Y}\n\\end{aligned}\n\\]Rearranging\\[\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\beta \\\\\n\\mathbf{b}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Thus, solution mixed model equations give:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right] ^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Equivalently,Bayes’ theorem\\[\nf(\\mathbf{b}| \\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b})}{\\int f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b}) d\\mathbf{b}}\n\\]\\(f(\\mathbf{Y}|\\mathbf{b})\\) “likelihood”\\(f(\\mathbf{b})\\) priorthe denominator “normalizing constant”\\(f(\\mathbf{b}|\\mathbf{Y})\\) posterior distributionIn case\\[\n\\begin{aligned}\n\\mathbf{Y} | \\mathbf{b} &\\sim N(\\mathbf{X\\beta+Zb,\\Sigma}) \\\\\n\\mathbf{b} &\\sim N(\\mathbf{0,B})\n\\end{aligned}\n\\]posterior distribution form\\[\n\\mathbf{b}|\\mathbf{Y} \\sim N(\\mathbf{BZ'V^{-1}(Y-X\\beta),(Z'\\Sigma^{-1}Z + B^{-1})^{-1}})\n\\]Hence, best predictor (based squared error loss)\\[\nE(\\mathbf{b}|\\mathbf{Y}) = \\mathbf{BZ'V^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"estimating-mathbfv","chapter":"8 Linear Mixed Models","heading":"8.2.1 Estimating \\(\\mathbf{V}\\)","text":"\\(\\tilde{\\mathbf{V}}\\) (estimate \\(\\mathbf{V}\\)), can estimate:\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'\\tilde{V}^{-1}X)^{-1}X'\\tilde{V}^{-1}Y} \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{BZ'\\tilde{V}^{-1}(Y-X\\hat{\\beta})}\n\\end{aligned}\n\\]\\({\\mathbf{b}}\\) EBLUP (estimated BLUP) empirical Bayes estimateNote:\\(\\hat{var}(\\hat{\\beta})\\) consistent estimator \\(var(\\hat{\\beta})\\) \\(\\tilde{\\mathbf{V}}\\) consistent estimator \\(\\mathbf{V}\\)However, \\(\\hat{var}(\\hat{\\beta})\\) biased since variability arises estimating \\(\\mathbf{V}\\) accounted estimate.Hence, \\(\\hat{var}(\\hat{\\beta})\\) underestimates true variabilityWays estimate \\(\\mathbf{V}\\)Maximum Likelihood Estimation (MLE)Restricted Maximum Likelihood (REML)Estimated Generalized Least SquaresBayesian Hierarchical Models (BHM)","code":""},{"path":"linear-mixed-models.html","id":"maximum-likelihood-estimation-mle","chapter":"8 Linear Mixed Models","heading":"8.2.1.1 Maximum Likelihood Estimation (MLE)","text":"Grouping unknown parameters \\(\\Sigma\\) \\(B\\) parameter vector \\(\\theta\\). MLE, \\(\\hat{\\theta}\\) \\(\\hat{\\beta}\\) maximize likelihood \\(\\mathbf{y} \\sim N(\\mathbf{X\\beta, V(\\theta))}\\). Synonymously, \\(-2\\log L(\\mathbf{y;\\theta,\\beta})\\):\\[\n-2l(\\mathbf{\\beta,\\theta,y}) = \\log |\\mathbf{V(\\theta)}| + \\mathbf{(y-X\\beta)'V(\\theta)^{-1}(y-X\\beta)} + N \\log(2\\pi)\n\\]Step 1: Replace \\(\\beta\\) maximum likelihood (\\(\\theta\\) known \\(\\hat{\\beta}= (\\mathbf{X'V(\\theta)^{-1}X)^{-1}X'V(\\theta)^{-1}y}\\)Step 2: Minimize equation respect \\(\\theta\\) get estimator \\(\\hat{\\theta}_{MLE}\\)Step 3: Substitute \\(\\hat{\\theta}_{MLE}\\) back get \\(\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\theta_{MLE})^{-1}X)^{-1}X'V(\\theta_{MLE})^{-1}y}\\)Step 4: Get \\(\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B(\\hat{\\theta}_{MLE})Z'V(\\hat{\\theta}_{MLE})^{-1}(y-X\\hat{\\beta}_{MLE})}\\)Note:\\(\\hat{\\theta}\\) typically negatively biased due unaccounted fixed effects estimated, try account .","code":""},{"path":"linear-mixed-models.html","id":"restricted-maximum-likelihood-reml","chapter":"8 Linear Mixed Models","heading":"8.2.1.2 Restricted Maximum Likelihood (REML)","text":"REML accounts number estimated mean parameters adjusting objective function. Specifically, likelihood linear combination elements \\(\\mathbf{y}\\) accounted .\\(\\mathbf{K'y}\\), \\(\\mathbf{K}\\) \\(N \\times (N - p)\\) full-rank contrast matrix, columns orthogonal \\(\\mathbf{X}\\) matrix (\\(\\mathbf{K'X} = 0\\)). ,\\[\n\\mathbf{K'y} \\sim N(0,\\mathbf{K'V(\\theta)K})\n\\]\\(\\beta\\) longer distributionWe can proceed maximize likelihood contrasts get \\(\\hat{\\theta}_{REML}\\), depend choice \\(\\mathbf{K}\\). \\(\\hat{\\beta}\\) based \\(\\hat{\\theta}\\)Comparison REML MLEBoth methods based upon likelihood principle, desired properties estimates:\nconsistency\nasymptotic normality\nefficiency\nmethods based upon likelihood principle, desired properties estimates:consistencyconsistencyasymptotic normalityasymptotic normalityefficiencyefficiencyML estimation provides estimates fixed effects, REML can’tML estimation provides estimates fixed effects, REML can’tIn balanced models, REML identical ANOVAIn balanced models, REML identical ANOVAREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeChanging \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)Changing \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)REML less sensitive outliers MLEREML less sensitive outliers MLEMLE better REML regarding model comparisons (e.g., AIC BIC)MLE better REML regarding model comparisons (e.g., AIC BIC)","code":""},{"path":"linear-mixed-models.html","id":"estimated-generalized-least-squares","chapter":"8 Linear Mixed Models","heading":"8.2.1.3 Estimated Generalized Least Squares","text":"MLE REML rely upon Gaussian assumption. overcome issue, EGLS uses first second moments.\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\epsilon_i \\sim (\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i \\sim (\\mathbf{0,D})\\)\\(cov(\\epsilon_i, \\mathbf{b}_i) = 0\\)EGLS estimator \\[\n\\begin{aligned}\n\\hat{\\beta}_{GLS} &= \\{\\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}X_i}  \\}^{-1} \\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}Y_i} \\\\\n&=\\{\\mathbf{X'V(\\theta)^{-1}X} \\}^{-1} \\mathbf{X'V(\\theta)^{-1}Y}\n\\end{aligned}\n\\]depends first two moments\\(E(\\mathbf{Y}_i) = \\mathbf{X}_i \\beta\\)\\(var(\\mathbf{Y}_i)= \\mathbf{V}_i\\)EGLS use \\(\\hat{\\mathbf{V}}\\) \\(\\mathbf{V(\\theta)}\\)\\[\n\\hat{\\beta}_{EGLS} = \\{ \\mathbf{X'\\hat{V}^{-1}X} \\}^{-1} \\mathbf{X'\\hat{V}^{-1}Y}\n\\]Hence, fixed effects estimators MLE, REML, EGLS form, except estimate \\(\\mathbf{V}\\)case non-iterative approach, EGLS can appealing \\(\\mathbf{V}\\) can estimated without much computational burden.","code":""},{"path":"linear-mixed-models.html","id":"bayesian-hierarchical-models-bhm","chapter":"8 Linear Mixed Models","heading":"8.2.1.4 Bayesian Hierarchical Models (BHM)","text":"Joint distribution cane decomposed hierarchically terms product conditional distributions marginal distribution\\[\nf(,B,C) = f(|B,C) f(B|C)f(C)\n\\]Applying estimate \\(\\mathbf{V}\\)\\[\n\\begin{aligned}\nf(\\mathbf{Y, \\beta, b, \\theta}) &= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta,\\beta})f(\\mathbf{\\beta|\\theta})f(\\mathbf{\\theta}) & \\text{based probability decomposition} \\\\\n&= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta}) & \\text{based simplifying modeling assumptions}\n\\end{aligned}\n\\]elaborate second equality, assume conditional independence (e.g., given \\(\\theta\\), additional info \\(\\mathbf{b}\\) given knowing \\(\\beta\\)), can simply first equalityUsing Bayes’ rule\\[\nf(\\mathbf{\\beta, b, \\theta|Y}) \\propto f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta})\n\\]\\[\n\\begin{aligned}\n\\mathbf{Y| \\beta, b, \\theta} &\\sim \\mathbf{N(X\\beta+ Zb, \\Sigma(\\theta))} \\\\\n\\mathbf{b | \\theta} &\\sim \\mathbf{N(0, B(\\theta))}\n\\end{aligned}\n\\]also prior distributions \\(f(\\beta), f(\\theta)\\)normalizing constant, can obtain posterior distribution. Typically, can’t get analytical solution right away. Hence, can use Markov Chain Monte Carlo (MCMC) obtain samples posterior distribution.Bayesian Methods:account uncertainty parameters estimates accommodate propagation uncertainty modelcan adjust prior information (.e., priori) parametersCan extend beyond Gaussian distributionsbut hard implement algorithms might problem converging","code":""},{"path":"linear-mixed-models.html","id":"inference-2","chapter":"8 Linear Mixed Models","heading":"8.3 Inference","text":"","code":""},{"path":"linear-mixed-models.html","id":"parameters-beta","chapter":"8 Linear Mixed Models","heading":"8.3.1 Parameters \\(\\beta\\)","text":"","code":""},{"path":"linear-mixed-models.html","id":"wald-test-GLMM","chapter":"8 Linear Mixed Models","heading":"8.3.1.1 Wald test","text":"\\[\n\\begin{aligned}\n\\mathbf{\\hat{\\beta}(\\theta)} &= \\mathbf{\\{X'V^{-1}(\\theta) X\\}^{-1}X'V^{-1}(\\theta) Y} \\\\\nvar(\\hat{\\beta}(\\theta)) &= \\mathbf{\\{X'V^{-1}(\\theta) X\\}^{-1}}\n\\end{aligned}\n\\]can use \\(\\hat{\\theta}\\) place \\(\\theta\\) approximate Wald test\\[\nH_0: \\mathbf{\\beta =d}\n\\]\\[\nW = \\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}\n\\]\\(W \\sim \\chi^2_{rank()}\\) \\(H_0\\) true. However, take account variability using \\(\\hat{\\theta}\\) place \\(\\theta\\), hence standard errors underestimated","code":""},{"path":"linear-mixed-models.html","id":"f-test-1","chapter":"8 Linear Mixed Models","heading":"8.3.1.2 F-test","text":"Alternatively, can use modified F-test, suppose \\(var(\\mathbf{Y}) = \\sigma^2 \\mathbf{V}(\\theta)\\), \\[\nF^* = \\frac{\\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}}{\\hat{\\sigma}^2 \\text{rank}()}\n\\]\\(F^* \\sim f_{rank(), den(df)}\\) null hypothesis. den(df) needs approximated data either:Satterthwaite methodKenward-Roger approximationUnder balanced cases, Wald F tests similar. small sample sizes, can differ p-values. can reduced t-test single \\(\\beta\\)","code":""},{"path":"linear-mixed-models.html","id":"likelihood-ratio-test","chapter":"8 Linear Mixed Models","heading":"8.3.1.3 Likelihood Ratio Test","text":"\\[\nH_0: \\beta \\\\Theta_{\\beta,0}\n\\]\\(\\Theta_{\\beta, 0}\\) subspace parameter space, \\(\\Theta_{\\beta}\\) fixed effects \\(\\beta\\) . \\[\n-2\\log \\lambda_N = -2\\log\\{\\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}}\\}\n\\]\\(\\hat{L}_{ML,0}\\) , \\(\\hat{L}_{ML}\\) maximized likelihood obtained maximizing \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)\\(-2 \\log \\lambda_N \\dot{\\sim} \\chi^2_{df}\\) df difference dimension (.e., number parameters) \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)method applicable REML. REML can still used test covariance parameters nested models.","code":""},{"path":"linear-mixed-models.html","id":"variance-components","chapter":"8 Linear Mixed Models","heading":"8.3.2 Variance Components","text":"ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesFor ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesWald test variance components analogous fixed effects case (see 8.3.1.1 )\nHowever, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))\nTypically works better covariance parameter, variance parameters.\nWald test variance components analogous fixed effects case (see 8.3.1.1 )However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))Typically works better covariance parameter, variance parameters.Typically works better covariance parameter, variance parameters.likelihood ratio tests can also used ML REML estimates. However, problem parametersThe likelihood ratio tests can also used ML REML estimates. However, problem parameters","code":""},{"path":"linear-mixed-models.html","id":"information-criteria-1","chapter":"8 Linear Mixed Models","heading":"8.4 Information Criteria","text":"account likelihood number parameters assess model comparison.","code":""},{"path":"linear-mixed-models.html","id":"akaikes-information-criteria-aic","chapter":"8 Linear Mixed Models","heading":"8.4.1 Akaike’s Information Criteria (AIC)","text":"Derived estimator expected Kullback discrepancy true model fitted candidate model\\[\nAIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + 2q\n\\]\\(l(\\hat{\\theta}, \\hat{\\beta})\\) log-likelihoodq = effective number parameters; total fixed associated random effects (variance/covariance; estimated boundary constraint)Note:comparing models differ random effects, method advised due inability get correct number effective parameters).prefer smaller AIC values.program uses \\(l-q\\) prefer larger AIC values (rarely).can used mixed model section, (e.g., selection covariance structure), sample size must large adequate comparison based criterionCan large negative bias (e.g., sample size small number parameters large) due penalty term can’t approximate bias adjustment adequately","code":""},{"path":"linear-mixed-models.html","id":"corrected-aic-aicc","chapter":"8 Linear Mixed Models","heading":"8.4.2 Corrected AIC (AICC)","text":"developed (Hurvich Tsai 1989)correct small-sample adjustmentdepends candidate model classOnly fixed covariance structure, AICC justified, general covariance structure","code":""},{"path":"linear-mixed-models.html","id":"bayesian-information-criteria-bic","chapter":"8 Linear Mixed Models","heading":"8.4.3 Bayesian Information Criteria (BIC)","text":"\\[\nBIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + q \\log n\n\\]n = number observations.prefer smaller BIC valueBIC AIC used REML MLE mean structure. Otherwise, general, prefer MLEWith example presented beginning Linear Mixed Models,\\[\nY_{ik}=\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]\\(= 1,..,N\\)\\(j = 1,..,n_i\\) (measures time \\(t_{ij}\\))Note:subject-specific intercepts,\\[\n\\begin{aligned}\n\\mathbf{Y}_i |b_i &\\sim N(\\mathbf{X}_i \\beta + 1 b_i, \\sigma^2 \\mathbf{}) \\\\\nb_i &\\sim N(0,d_{11})\n\\end{aligned}\n\\], want estimate \\(\\beta, \\sigma^2, d_{11}\\) predict \\(b_i\\)","code":""},{"path":"linear-mixed-models.html","id":"split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5 Split-Plot Designs","text":"Typically used case two factors one needs much larger units .Example:: 3 levels (large units)B: 2 levels (small units)B levels randomized 4 blocks.differs Randomized Block Designs. block, one 6 (3x2) treatment combinations. Randomized Block Designs assign block randomly, split-plot randomize step.Moreover, needs applied large units, factor applied block B can applied multiple times.Hence, modelIf factor interest\\[\nY_{ij} = \\mu + \\rho_i + \\alpha_j + e_{ij}\n\\]\\(\\) = replication (block subject)\\(j\\) = level Factor \\(\\mu\\) = overall mean\\(\\rho_i\\) = variation due \\(\\)-th block\\(e_{ij} \\sim N(0, \\sigma^2_e)\\) = whole plot errorIf B factor interest\\[\nY_{ijk} = \\mu + \\phi_{ij} + \\beta_k + \\epsilon_{ijk}\n\\]\\(\\phi_{ij}\\) = variation due \\(ij\\)-th main plot\\(\\beta_k\\) = Factor B effect\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = subplot error\\(\\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\\)Together, split-plot model\\[\nY_{ijk} = \\mu + \\rho_i + \\alpha_j + e_{ij} + \\beta_k + (\\alpha \\beta)_{jk} + \\epsilon_{ijk}\n\\]\\(\\) = replicate (blocks subjects)\\(j\\) = level factor \\(k\\) = level factor B\\(\\mu\\) = overall mean\\(\\rho_i\\) = effect block\\(\\alpha_j\\) = main effect factor (fixed)\\(e_{ij} = (\\rho \\alpha)_{ij}\\) = block factor interaction (whole plot error, random)\\(\\beta_k\\) = main effect factor B (fixed)\\((\\alpha \\beta)_{jk}\\) = interaction factors B (fixed)\\(\\epsilon_{ijk}\\) = subplot error (random)can approach sub-plot analysis based onthe ANOVA perspective\nWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\n\nSub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\n\nANOVA perspectiveWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\nWhole plot comparisonsCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Sub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\nSub-plot comparisons:Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))mixed model perspectivethe mixed model perspective\\[\n\\mathbf{Y = X \\beta + Zb + \\epsilon}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"application-3","chapter":"8 Linear Mixed Models","heading":"8.5.1 Application","text":"","code":""},{"path":"linear-mixed-models.html","id":"example-1","chapter":"8 Linear Mixed Models","heading":"8.5.1.1 Example 1","text":"\\[\ny_{ijk} = \\mu + i_i + v_j + (iv)_{ij} + f_k + \\epsilon_{ijk}\n\\]\\(y_{ijk}\\) = observed yield\\(\\mu\\) = overall average yield\\(i_i\\) = irrigation effect\\(v_j\\) = variety effect\\((iv)_{ij}\\) = irrigation variety interaction\\(f_k\\) = random field (block) effect\\(\\epsilon_{ijk}\\) = residualbecause variety-field combination observed , can’t random interaction effects variety fieldSince p-value interaction term insignificant, consider fitting without .Since \\(p\\)-value \\(\\chi^2\\) test insignificant, can’t reject additive model already sufficient. Looking AIC BIC, can also see prefer additive modelRandom Effect ExaminationexactRLRT test\\(H_0\\): Var(random effect) (.e., \\(\\sigma^2\\))= 0\\(H_a\\): Var(random effect) (.e., \\(\\sigma^2\\)) > 0Since p-value significant, reject \\(H_0\\)","code":"\nlibrary(ggplot2)\ndata(irrigation, package = \"faraway\")\nsummary(irrigation)\n#>      field   irrigation variety     yield      \n#>  f1     :2   i1:4       v1:8    Min.   :34.80  \n#>  f2     :2   i2:4       v2:8    1st Qu.:37.60  \n#>  f3     :2   i3:4               Median :40.15  \n#>  f4     :2   i4:4               Mean   :40.23  \n#>  f5     :2                      3rd Qu.:42.73  \n#>  f6     :2                      Max.   :47.60  \n#>  (Other):4\nhead(irrigation, 4)\n#>   field irrigation variety yield\n#> 1    f1         i1      v1  35.4\n#> 2    f1         i1      v2  37.9\n#> 3    f2         i2      v1  36.7\n#> 4    f2         i2      v2  38.2\nggplot(irrigation,\n       aes(\n         x     = field,\n         y     = yield,\n         shape = irrigation,\n         color = variety\n       )) +\n  geom_point(size = 3)\nsp_model <-\n    lmerTest::lmer(yield ~ irrigation * variety \n                   + (1 |field), irrigation)\nsummary(sp_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ irrigation * variety + (1 | field)\n#>    Data: irrigation\n#> \n#> REML criterion at convergence: 45.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -0.7448 -0.5509  0.0000  0.5509  0.7448 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  field    (Intercept) 16.200   4.025   \n#>  Residual              2.107   1.452   \n#> Number of obs: 16, groups:  field, 8\n#> \n#> Fixed effects:\n#>                        Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***\n#> irrigationi2              1.200      4.279  4.487   0.280 0.791591    \n#> irrigationi3              0.700      4.279  4.487   0.164 0.877156    \n#> irrigationi4              3.500      4.279  4.487   0.818 0.454584    \n#> varietyv2                 0.600      1.452  4.000   0.413 0.700582    \n#> irrigationi2:varietyv2   -0.400      2.053  4.000  -0.195 0.855020    \n#> irrigationi3:varietyv2   -0.200      2.053  4.000  -0.097 0.927082    \n#> irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2\n#> irrigation2 -0.707                                          \n#> irrigation3 -0.707  0.500                                   \n#> irrigation4 -0.707  0.500  0.500                            \n#> varietyv2   -0.240  0.170  0.170  0.170                     \n#> irrgtn2:vr2  0.170 -0.240 -0.120 -0.120 -0.707              \n#> irrgtn3:vr2  0.170 -0.120 -0.240 -0.120 -0.707  0.500       \n#> irrgtn4:vr2  0.170 -0.120 -0.120 -0.240 -0.707  0.500  0.500\n\nanova(sp_model, ddf = c(\"Kenward-Roger\"))\n#> Type III Analysis of Variance Table with Kenward-Roger's method\n#>                    Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> irrigation         2.4545 0.81818     3     4  0.3882 0.7685\n#> variety            2.2500 2.25000     1     4  1.0676 0.3599\n#> irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612\nlibrary(lme4)\nsp_model_additive <- lmer(yield ~ irrigation + variety \n                          + (1 | field), irrigation)\nanova(sp_model_additive,sp_model,ddf = \"Kenward-Roger\")\n#> Data: irrigation\n#> Models:\n#> sp_model_additive: yield ~ irrigation + variety + (1 | field)\n#> sp_model: yield ~ irrigation * variety + (1 | field)\n#>                   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n#> sp_model_additive    7 83.959 89.368 -34.980   69.959                     \n#> sp_model            10 88.609 96.335 -34.305   68.609 1.3503  3     0.7172\nsp_model <- lme4::lmer(yield ~ irrigation * variety \n                       + (1 | field), irrigation)\nlibrary(RLRsim)\nexactRLRT(sp_model)\n#> \n#>  simulated finite sample distribution of RLRT.\n#>  \n#>  (p-value based on 10000 simulated values)\n#> \n#> data:  \n#> RLRT = 6.1118, p-value = 0.0087"},{"path":"linear-mixed-models.html","id":"repeated-measures-in-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.6 Repeated Measures in Mixed Models","text":"\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\delta_{(k)}+ \\epsilon_{ijk}\n\\]\\(\\)-th group (fixed)\\(j\\)-th (repeated measure) time effect (fixed)\\(k\\)-th subject\\(\\delta_{(k)} \\sim N(0,\\sigma^2_\\delta)\\) (k-th subject \\(\\)-th group) \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent error) random effects (\\(= 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\\))hence, variance-covariance matrix repeated observations k-th subject -th group, \\(\\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})'\\), \\[\n\\begin{aligned}\n\\mathbf{\\Sigma}_{subject} &=\n\\left(\n\\begin{array}\n{cccc}\n\\sigma^2_\\delta + \\sigma^2 & \\sigma^2_\\delta & ... & \\sigma^2_\\delta \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta +\\sigma^2 & ... & \\sigma^2_\\delta \\\\\n. & . & . & . \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta & ... & \\sigma^2_\\delta + \\sigma^2 \\\\\n\\end{array}\n\\right) \\\\\n&= (\\sigma^2_\\delta + \\sigma^2)\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & ... & \\rho \\\\\n. & . & . & . \\\\\n\\rho & \\rho & ... & 1 \\\\\n\\end{array}\n\\right)\n& \\text{product scalar correlation matrix}\n\\end{aligned}\n\\]\\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\), compound symmetry structure discussed Random-Intercepts ModelBut repeated measurements subject time, AR(1) structure might appropriateMixed model repeated measure\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) combines random error whole subplots.general,\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\epsilon \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) \\(\\mathbf{\\Sigma}\\) block diagonal random error covariance subjectThe variance covariance matrix AR(1) structure \\[\n\\mathbf{\\Sigma}_{subject} =\n\\sigma^2\n\\left(\n\\begin{array}\n{ccccc}\n1  & \\rho & \\rho^2 & ... & \\rho^{n_B-1} \\\\\n\\rho & 1 & \\rho & ... & \\rho^{n_B-2} \\\\\n. & . & . & . & . \\\\\n\\rho^{n_B-1} & \\rho^{n_B-2} & \\rho^{n_B-3} & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]Hence, mixed model repeated measure can written \\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) = random error whole subplotsGenerally,\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\epsilon \\sim N(0, \\mathbf{\\sigma^2 \\Sigma})\\) \\(\\Sigma\\) = block diagonal random error covariance subject.","code":""},{"path":"linear-mixed-models.html","id":"unbalanced-or-unequally-spaced-data","chapter":"8 Linear Mixed Models","heading":"8.7 Unbalanced or Unequally Spaced Data","text":"Consider model\\[\nY_{ikt} = \\beta_0 + \\beta_{0i} + \\beta_{1}t + \\beta_{1i}t + \\beta_{2} t^2 + \\beta_{2i} t^2 + \\epsilon_{ikt}\n\\]\\(= 1,2\\) (groups)\\(k = 1,…, n_i\\) ( individuals)\\(t = (t_1,t_2,t_3,t_4)\\) (times)\\(\\beta_{2i}\\) = common quadratic term\\(\\beta_{1i}\\) = common linear time trends\\(\\beta_{0i}\\) = common interceptsThen, assume variance-covariance matrix repeated measurements collected particular subject time form\\[\n\\mathbf{\\Sigma}_{ik} = \\sigma^2\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho^{t_2-t_1} & \\rho^{t_3-t_1} & \\rho^{t_4-t_1} \\\\\n\\rho^{t_2-t_1} & 1 & \\rho^{t_3-t_2} & \\rho^{t_4-t_2} \\\\\n\\rho^{t_3-t_1} & \\rho^{t_3-t_2} & 1 & \\rho^{t_4-t_3} \\\\\n\\rho^{t_4-t_1} & \\rho^{t_4-t_2} & \\rho^{t_4-t_3} & 1\n\\end{array}\n\\right)\n\\]called “power” covariance modelWe can consider \\(\\beta_{2i} , \\beta_{1i}, \\beta_{0i}\\) accordingly see whether terms needed final model","code":""},{"path":"linear-mixed-models.html","id":"application-4","chapter":"8 Linear Mixed Models","heading":"8.8 Application","text":"R Packages mixed modelsnlme\nnested structure\nflexible complex design\nuser-friendly\nnlmehas nested structurehas nested structureflexible complex designflexible complex designnot user-friendlynot user-friendlylme4\ncomputationally efficient\nuser-friendly\ncan handle non-normal response\ndetailed application, check Fitting Linear Mixed-Effects Models Using lme4\nlme4computationally efficientcomputationally efficientuser-friendlyuser-friendlycan handle non-normal responsecan handle non-normal responsefor detailed application, check Fitting Linear Mixed-Effects Models Using lme4for detailed application, check Fitting Linear Mixed-Effects Models Using lme4Others\nBayesian setting: MCMCglmm, brms\ngenetics: ASReml\nOthersBayesian setting: MCMCglmm, brmsBayesian setting: MCMCglmm, brmsFor genetics: ASRemlFor genetics: ASReml","code":""},{"path":"linear-mixed-models.html","id":"example-1-pulps","chapter":"8 Linear Mixed Models","heading":"8.8.1 Example 1 (Pulps)","text":"Model:\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\\(= 1,..,\\) groups random effect \\(\\alpha_i\\)\\(j = 1,...,n\\) individuals group\\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) random effects\\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) random effectsImply compound symmetry model intraclass correlation coefficient : \\(\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\epsilon}\\)factor \\(\\) explain much variation, low correlation within levels: \\(\\sigma^2_\\alpha \\0\\) \\(\\rho \\0\\)factor \\(\\) explain much variation, high correlation within levels \\(\\sigma^2_\\alpha \\\\infty\\) hence, \\(\\rho \\1\\)lmer applicationTo Satterthwaite approximation denominator df, use lmerTestIn example, can see confidence interval computed confint lmer package close confint lmerTest model.MCMglmm applicationunder Bayesian frameworkthis method offers confidence interval slightly positive lmer lmerTest","code":"\ndata(pulp, package = \"faraway\")\nplot(\n    y    = pulp$bright,\n    x    = pulp$operator,\n    xlab = \"Operator\",\n    ylab = \"Brightness\"\n)\npulp %>% dplyr::group_by(operator) %>% \n    dplyr::summarise(average = mean(bright))\n#> # A tibble: 4 × 2\n#>   operator average\n#>   <fct>      <dbl>\n#> 1 a           60.2\n#> 2 b           60.1\n#> 3 c           60.6\n#> 4 d           60.7\nlibrary(lme4)\nmixed_model <-\n    lmer(\n        # pipe (i..e, | ) denotes random-effect terms\n        formula = bright ~ 1 + (1 |operator), \n         data = pulp)\nsummary(mixed_model)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: bright ~ 1 + (1 | operator)\n#>    Data: pulp\n#> \n#> REML criterion at convergence: 18.6\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4666 -0.7595 -0.1244  0.6281  1.6012 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  operator (Intercept) 0.06808  0.2609  \n#>  Residual             0.10625  0.3260  \n#> Number of obs: 20, groups:  operator, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept)  60.4000     0.1494   404.2\ncoef(mixed_model)\n#> $operator\n#>   (Intercept)\n#> a    60.27806\n#> b    60.14088\n#> c    60.56767\n#> d    60.61340\n#> \n#> attr(,\"class\")\n#> [1] \"coef.mer\"\nfixef(mixed_model)   # fixed effects\n#> (Intercept) \n#>        60.4\nconfint(mixed_model) # confidence interval\n#>                 2.5 %     97.5 %\n#> .sig01       0.000000  0.6178987\n#> .sigma       0.238912  0.4821845\n#> (Intercept) 60.071299 60.7287012\nranef(mixed_model)   # random effects\n#> $operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n#> \n#> with conditional variances for \"operator\"\nVarCorr(mixed_model) # random effects standard deviation\n#>  Groups   Name        Std.Dev.\n#>  operator (Intercept) 0.26093 \n#>  Residual             0.32596\nre_dat = as.data.frame(VarCorr(mixed_model))\n\n# rho based on the above formula\nrho = re_dat[1, 'vcov'] / (re_dat[1, 'vcov'] + re_dat[2, 'vcov'])\nrho\n#> [1] 0.3905354\nlibrary(lmerTest)\nsummary(lmerTest::lmer(bright ~ 1 + (1 | operator), pulp))$coefficients\n#>             Estimate Std. Error df  t value     Pr(>|t|)\n#> (Intercept)     60.4  0.1494434  3 404.1664 3.340265e-08\nconfint(mixed_model)[3, ]\n#>   2.5 %  97.5 % \n#> 60.0713 60.7287\nlibrary(MCMCglmm)\nmixed_model_bayes <-\n    MCMCglmm(\n        bright ~ 1,\n        random =  ~ operator,\n        data = pulp,\n        verbose = FALSE\n    )\nsummary(mixed_model_bayes)$solutions\n#>             post.mean l-95% CI u-95% CI eff.samp pMCMC\n#> (Intercept)  60.40449  60.2055 60.66595     1000 0.001"},{"path":"linear-mixed-models.html","id":"prediction","chapter":"8 Linear Mixed Models","heading":"8.8.1.1 Prediction","text":"use bootMer() get bootstrap-based confidence intervals predictions.Another example using GLMM context blockingPenicillin dataExamine treatment effectSince p-value greater 0.05, can’t reject null hypothesis treatment effect.Since p-value greater 0.05, consistent previous observation, conclude can’t reject null hypothesis treatment effect.","code":"\n# random effects prediction (BLUPs)\nranef(mixed_model)$operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n\n# prediction for each categories\nfixef(mixed_model) + ranef(mixed_model)$operator \n#>   (Intercept)\n#> a    60.27806\n#> b    60.14088\n#> c    60.56767\n#> d    60.61340\n\n# equivalent to the above method\npredict(mixed_model, newdata = data.frame(operator = c('a', 'b', 'c', 'd'))) \n#>        1        2        3        4 \n#> 60.27806 60.14088 60.56767 60.61340\ndata(penicillin, package = \"faraway\")\nsummary(penicillin)\n#>  treat    blend       yield   \n#>  A:5   Blend1:4   Min.   :77  \n#>  B:5   Blend2:4   1st Qu.:81  \n#>  C:5   Blend3:4   Median :87  \n#>  D:5   Blend4:4   Mean   :86  \n#>        Blend5:4   3rd Qu.:89  \n#>                   Max.   :97\nlibrary(ggplot2)\nggplot(penicillin, aes(\n    y     = yield,\n    x     = treat,\n    shape = blend,\n    color = blend\n)) + \n    # treatment = fixed effect\n    # blend = random effects\n    geom_point(size = 3) +\n    xlab(\"Treatment\")\n\nlibrary(lmerTest) # for p-values\nmixed_model <- lmerTest::lmer(yield ~ treat + (1 | blend),\n                              data = penicillin)\nsummary(mixed_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ treat + (1 | blend)\n#>    Data: penicillin\n#> \n#> REML criterion at convergence: 103.8\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4152 -0.5017 -0.1644  0.6830  1.2836 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  blend    (Intercept) 11.79    3.434   \n#>  Residual             18.83    4.340   \n#> Number of obs: 20, groups:  blend, 5\n#> \n#> Fixed effects:\n#>             Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)   84.000      2.475 11.075  33.941 1.51e-12 ***\n#> treatB         1.000      2.745 12.000   0.364   0.7219    \n#> treatC         5.000      2.745 12.000   1.822   0.0935 .  \n#> treatD         2.000      2.745 12.000   0.729   0.4802    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>        (Intr) treatB treatC\n#> treatB -0.555              \n#> treatC -0.555  0.500       \n#> treatD -0.555  0.500  0.500\n\n#The BLUPs for the each blend\nranef(mixed_model)$blend\n#>        (Intercept)\n#> Blend1   4.2878788\n#> Blend2  -2.1439394\n#> Blend3  -0.7146465\n#> Blend4   1.4292929\n#> Blend5  -2.8585859\nanova(mixed_model) # p-value based on lmerTest\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>       Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> treat     70  23.333     3    12  1.2389 0.3387\nlibrary(pbkrtest)\n# REML is not appropriate for testing fixed effects, it should be ML\nfull_model <-\n    lmer(yield ~ treat + (1 | blend), \n         penicillin, \n         REML = FALSE) \nnull_model <- lmer(yield ~ 1 + (1 | blend), \n                   penicillin, \n                   REML = FALSE)\n\n# use  Kenward-Roger approximation for df\nKRmodcomp(full_model, null_model) \n#> large : yield ~ treat + (1 | blend)\n#> small : yield ~ 1 + (1 | blend)\n#>          stat     ndf     ddf F.scaling p.value\n#> Ftest  1.2389  3.0000 12.0000         1  0.3387"},{"path":"linear-mixed-models.html","id":"example-2-rats","chapter":"8 Linear Mixed Models","heading":"8.8.2 Example 2 (Rats)","text":"interested whether treatment effect induces changes time.Since p-value significant, can confident concluding treatment effect","code":"\nrats <- read.csv(\n    \"images/rats.dat\",\n    header = F,\n    sep = ' ',\n    col.names = c('Treatment', 'rat', 'age', 'y')\n)\n\n# log transformed age\nrats$t <- log(1 + (rats$age - 45) / 10) \nrat_model <-\n    # treatment = fixed effect, rat = random effects\n    lmerTest::lmer(y ~ t:Treatment + (1 | rat), \n                   data = rats) \nsummary(rat_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: y ~ t:Treatment + (1 | rat)\n#>    Data: rats\n#> \n#> REML criterion at convergence: 932.4\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.25574 -0.65898 -0.01163  0.58356  2.88309 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  rat      (Intercept) 3.565    1.888   \n#>  Residual             1.445    1.202   \n#> Number of obs: 252, groups:  rat, 50\n#> \n#> Fixed effects:\n#>                Estimate Std. Error       df t value Pr(>|t|)    \n#> (Intercept)     68.6074     0.3312  89.0275  207.13   <2e-16 ***\n#> t:Treatmentcon   7.3138     0.2808 247.2762   26.05   <2e-16 ***\n#> t:Treatmenthig   6.8711     0.2276 247.7097   30.19   <2e-16 ***\n#> t:Treatmentlow   7.5069     0.2252 247.5196   33.34   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) t:Trtmntc t:Trtmnth\n#> t:Tretmntcn -0.327                    \n#> t:Tretmnthg -0.340  0.111             \n#> t:Tretmntlw -0.351  0.115     0.119\nanova(rat_model)\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>             Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \n#> t:Treatment 3181.9  1060.6     3 223.21  734.11 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"example-3-agridat","chapter":"8 Linear Mixed Models","heading":"8.8.3 Example 3 (Agridat)","text":"Remove outliersPlot age specieslme function nlme packagelmer function lme4 packageNotes:|| double pipes= uncorrelated random effects|| double pipes= uncorrelated random effectsTo remove intercept term:\n(0+ti|tree)\n(ti-1|tree)\nremove intercept term:(0+ti|tree)(0+ti|tree)(ti-1|tree)(ti-1|tree)include structured covariance terms, can use following way","code":"\nlibrary(agridat)\nlibrary(latticeExtra)\ndat <- harris.wateruse\n# Compare to Schabenberger & Pierce, fig 7.23\nuseOuterStrips(\n    xyplot(\n        water ~ day | species * age,\n        dat,\n        as.table = TRUE,\n        group = tree,\n        type = c('p', 'smooth'),\n        main = \"harris.wateruse 2 species, 2 ages (10 trees each)\"\n    )\n)\ndat <- subset(dat, day!=268)\nxyplot(\n    water ~ day | tree,\n    dat,\n    subset   = age == \"A2\" & species == \"S2\",\n    as.table = TRUE,\n    type     = c('p', 'smooth'),\n    ylab     = \"Water use profiles of individual trees\",\n    main     = \"harris.wateruse (Age 2, Species 2)\"\n)\n# Rescale day for nicer output, \n# and convergence issues, add quadratic term\ndat <- transform(dat, ti = day / 100)\ndat <- transform(dat, ti2 = ti * ti)\n# Start with a subgroup: age 2, species 2\nd22 <- droplevels(subset(dat, age == \"A2\" & species == \"S2\"))\nlibrary(nlme)\n\n## We use pdDiag() to get uncorrelated random effects\nm1n <- lme(\n    water ~ 1 + ti + ti2,\n    #intercept, time and time-squared = fixed effects\n    data = d22,\n    na.action = na.omit,\n    random = list(tree = pdDiag(~ 1 + ti + ti2)) \n    # random intercept, time \n    # and time squared per tree = random effects\n)\nranef(m1n)\n#>     (Intercept)            ti           ti2\n#> T04   0.1985796  1.609864e-09  4.990101e-10\n#> T05   0.3492827  2.487690e-10 -4.845287e-11\n#> T19  -0.1978989 -7.681202e-10 -1.961453e-10\n#> T23   0.4519003 -3.270426e-10 -2.413583e-10\n#> T38  -0.6457494 -1.608770e-09 -3.298010e-10\n#> T40   0.3739432  3.264705e-10 -2.543109e-11\n#> T49   0.8620648  9.021831e-10 -5.402247e-12\n#> T53  -0.5655049 -8.279040e-10 -4.579291e-11\n#> T67  -0.4394623 -3.485113e-10  2.147434e-11\n#> T71  -0.3871552  7.930610e-10  3.718993e-10\nfixef(m1n)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nsummary(m1n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC     BIC    logLik\n#>   276.5142 300.761 -131.2571\n#> \n#> Random effects:\n#>  Formula: ~1 + ti + ti2 | tree\n#>  Structure: Diagonal\n#>         (Intercept)           ti          ti2  Residual\n#> StdDev:   0.5187869 1.438333e-05 3.864019e-06 0.3836614\n#> \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -10.798799 0.8814666 227 -12.25094       0\n#> ti           12.346704 0.7827112 227  15.77428       0\n#> ti2          -2.838503 0.1720614 227 -16.49704       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.979       \n#> ti2  0.970 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.07588246 -0.58531056  0.01210209  0.65402695  3.88777402 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10\nm1lmer <-\n    lmer(water ~ 1 + ti + ti2 + (ti + ti2 ||\n                                     tree),\n         data = d22,\n         na.action = na.omit)\nranef(m1lmer)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\nfixef(m1lmer)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm1l <-\n    lmer(water ~ 1 + ti + ti2 \n         + (1 | tree) + (0 + ti | tree) \n         + (0 + ti2 | tree), data = d22)\nranef(m1l)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\nfixef(m1l)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm2n <- lme(\n    water ~ 1 + ti + ti2,\n    data = d22,\n    random = ~ 1 | tree,\n    cor = corExp(form =  ~ day | tree),\n    na.action = na.omit\n)\nranef(m2n)\n#>     (Intercept)\n#> T04   0.1929971\n#> T05   0.3424631\n#> T19  -0.1988495\n#> T23   0.4538660\n#> T38  -0.6413664\n#> T40   0.3769378\n#> T49   0.8410043\n#> T53  -0.5528236\n#> T67  -0.4452930\n#> T71  -0.3689358\nfixef(m2n)\n#> (Intercept)          ti         ti2 \n#>  -11.223310   12.712094   -2.913682\nsummary(m2n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC      BIC   logLik\n#>   263.3081 284.0911 -125.654\n#> \n#> Random effects:\n#>  Formula: ~1 | tree\n#>         (Intercept)  Residual\n#> StdDev:   0.5154042 0.3925777\n#> \n#> Correlation Structure: Exponential spatial correlation\n#>  Formula: ~day | tree \n#>  Parameter estimate(s):\n#>    range \n#> 3.794624 \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -11.223310 1.0988725 227 -10.21348       0\n#> ti           12.712094 0.9794235 227  12.97916       0\n#> ti2          -2.913682 0.2148551 227 -13.56115       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.985       \n#> ti2  0.976 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.04861039 -0.55703950  0.00278101  0.62558762  3.80676991 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9 Nonlinear and Generalized Linear Mixed Models","text":"NLMMs extend nonlinear model include fixed effects random effectsGLMMs extend generalized linear model include fixed effects random effects.nonlinear mixed model form \\[\nY_{ij} = f(\\mathbf{x_{ij} , \\theta, \\alpha_i}) + \\epsilon_{ij}\n\\]j-th response cluster (subject) (\\(= 1,...,n\\)), \\(j = 1,...,n_i\\)\\(\\mathbf{\\theta}\\) fixed effects\\(\\mathbf{\\alpha}_i\\) random effects cluster \\(\\mathbf{x}_{ij}\\) regressors design variables\\(f(.)\\) nonlinear mean response functionA GLMM can written :assume\\[\ny_i |\\alpha_i \\sim \\text{indep } f(y_i | \\alpha)\n\\]\\(f(y_i | \\mathbf{\\alpha})\\) exponential family distribution,\\[\nf(y_i | \\alpha) = \\exp [\\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} - c(y_i, \\phi)]\n\\]conditional mean \\(y_i\\) related \\(\\theta_i\\)\\[\n\\mu_i = \\frac{\\partial b(\\theta_i)}{\\partial \\theta_i}\n\\]transformation mean give us desired linear model model fixed random effects.\\[\n\\begin{aligned}\nE(y_i |\\alpha) &= \\mu_i \\\\\ng(\\mu_i) &= \\mathbf{x_i' \\beta + z'_i \\alpha}\n\\end{aligned}\n\\]\\(g()\\) known link function \\(\\mu_i\\) conditional mean. can see similarity GLMWe also specify random effects distribution\\[\n\\alpha \\sim f(\\alpha)\n\\]similar specification mixed models.Moreover, law large number applies fixed effects know normal distribution. , can specify \\(\\alpha\\) subjectively.Hence, can show NLMM special case GLMM\\[\n\\begin{aligned}\n\\mathbf{Y}_i &= \\mathbf{f}(\\mathbf{x}_i, \\mathbf{\\theta, \\alpha}_i) + \\mathbf{\\epsilon}_i \\\\\n\\mathbf{Y}_i &= \\mathbf{g}^{-1} (\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}_i) + \\mathbf{\\epsilon}_i\n\\end{aligned}\n\\]inverse link function corresponds nonlinear transformation fixed random effects.Note:can’t derive analytical formulation marginal distribution nonlinear combination normal variables normally distributed, even case additive error (\\(e_i\\)) random effects (\\(\\alpha_i\\)) normal.Consequences random effectsThe marginal mean \\(y_i\\) \\[\nE(y_i) = E_\\alpha(E(y_i | \\alpha)) = E_\\alpha (\\mu_i) = E(g^{-1}(\\mathbf{x_i' \\beta + z_i' \\alpha}))\n\\]\\(g^{-1}()\\) nonlinear, simplified version can go .special cases log link (\\(g(\\mu) = \\log \\mu\\) \\(g^{-1}() = \\exp()\\)) \\[\nE(y_i) = E(\\exp(\\mathbf{x_i' \\beta + z_i' \\alpha})) = \\exp(\\mathbf{x'_i \\beta})E(\\exp(\\mathbf{z}_i'\\alpha))\n\\]moment generating function \\(\\alpha\\) evaluated \\(\\mathbf{z}_i\\)Marginal variance \\(y_i\\)\\[\n\\begin{aligned}\nvar(y_i) &= var_\\alpha (E(y_i | \\alpha)) + E_\\alpha (var(y_i | \\alpha)) \\\\\n&= var(\\mu_i) + E((\\phi) V(\\mu_i)) \\\\\n&= var(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})) + E((\\phi)V(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})))\n\\end{aligned}\n\\]Without specific assumption \\(g()\\) /conditional distribution \\(\\mathbf{y}\\), simplified version.Marginal covariance \\(\\mathbf{y}\\)linear mixed model, random effects introduce dependence among observations share random effect common\\[\n\\begin{aligned}\ncov(y_i, y_j) &= cov_{\\alpha}(E(y_i | \\mathbf{\\alpha}),E(y_j | \\mathbf{\\alpha})) + E_{\\alpha}(cov(y_i, y_j | \\mathbf{\\alpha})) \\\\\n&= cov(\\mu_i, \\mu_j) + E(0) \\\\\n&= cov(g^{-1}(\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}), g^{-1}(\\mathbf{x}'_j \\beta + \\mathbf{z}_j' \\mathbf{\\alpha}))\n\\end{aligned}\n\\]Important: conditioning induce covariabilityExample:Repeated measurements subjects.Let \\(y_{ij}\\) j-th count taken \\(\\)-th subject., model \\(y_{ij} | \\mathbf{\\alpha} \\sim \\text{indep } Pois(\\mu_{ij})\\). \\[\n\\log(\\mu_{ij}) = \\mathbf{x}_{ij}' \\beta + \\alpha_i\n\\]\\(\\alpha_i \\sim iid N(0,\\sigma^2_{\\alpha})\\)log-link random patient effect.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-2","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1 Estimation","text":"linear mixed models, marginal likelihood \\(\\mathbf{y}\\) integration random effects hierarchical formulation\\[\nf(\\mathbf{y}) = \\int f(\\mathbf{y}| \\alpha) f(\\alpha) d \\alpha\n\\]linear mixed models, assumed 2 component distributions Gaussian linear relationships, implied marginal distribution also linear Gaussian allows us solve integral analytically.hand, GLMMs, distribution \\(f(\\mathbf{y} | \\alpha)\\) Gaussian general, NLMMs, functional form mean response random (fixed) effects nonlinear. cases, can’t perform integral analytically, means solve itnumerically /ornumerically /orlinearize inverse link function.linearize inverse link function.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-numerical-integration","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.1 Estimation by Numerical Integration","text":"marginal likelihood \\[\nL(\\beta; \\mathbf{y}) = \\int f(\\mathbf{y} | \\alpha) f(\\alpha) d \\alpha\n\\]Estimation fo fixed effects requires \\(\\frac{\\partial l}{\\partial \\beta}\\), \\(l\\) log-likelihoodOne way obtain marginal inference numerically integrate random effects throughnumerical quadraturenumerical quadratureLaplace approximationLaplace approximationMonte Carlo methodsMonte Carlo methodsWhen dimension \\(\\mathbf{\\alpha}\\) relatively low, easy. dimension \\(\\alpha\\) high, additional approximation required.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-linearization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2 Estimation by Linearization","text":"Idea: Linearized version response (known working response, pseudo-response) called \\(\\tilde{y}_i\\) conditional mean \\[\nE(\\tilde{y}_i | \\alpha) = \\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\alpha\n\\]also estimate \\(var(\\tilde{y}_i | \\alpha)\\). , apply Linear Mixed Models estimation usual.difference linearization done (.e., expand \\(f(\\mathbf{x, \\theta, \\alpha})\\) inverse link function","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"penalized-quasi-likelihood","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.1 Penalized quasi-likelihood","text":"(PQL)popular method\\[\n\\tilde{y}_i^{(k)} = \\hat{\\eta}_i^{(k-1)} + ( y_i - \\hat{\\mu}_i^{(k-1)})\\frac{d \\eta}{d \\mu}| \\hat{\\eta}_i^{(k-1)}\n\\]\\(\\eta_i = g(\\mu_i)\\) linear predictor\\(\\eta_i = g(\\mu_i)\\) linear predictor\\(k\\) = iteration optimization algorithm\\(k\\) = iteration optimization algorithmThe algorithm updates \\(\\tilde{y}_i\\) linear mixed model fit using \\(E(\\tilde{y}_i | \\alpha)\\) \\(var(\\tilde{y}_i | \\alpha)\\)Comments:Easy implementEasy implementInference asymptotically correct due linearizatonInference asymptotically correct due linearizatonBiased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Biased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Hypothesis testing confidence intervals also problems.Hypothesis testing confidence intervals also problems.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"generalized-estimating-equations","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.2 Generalized Estimating Equations","text":"(GEE)Let marginal generalized linear model mean y function predictors, means linearize mean response function assume dependent error structureExample\nBinary data:\\[\nlogit (E(\\mathbf{y})) = \\mathbf{X} \\beta\n\\]assume “working covariance matrix”, \\(\\mathbf{V}\\) elements \\(\\mathbf{y}\\), maximum likelihood equations estimating \\(\\beta\\) \\[\n\\mathbf{X'V^{-1}y} = \\mathbf{X'V^{-1}} E(\\mathbf{y})\n\\]\\(\\mathbf{V}\\) correct, unbiased estimating equationsWe typically define \\(\\mathbf{V} = \\mathbf{}\\). Solutions unbiased estimating equation give consistent estimators.practice, assume covariance structure, logistic regression, calculate large sample varianceLet \\(y_{ij} , j = 1,..,n_i, = 1,..,K\\) j-th measurement \\(\\)-th subject.\\[\n\\mathbf{y}_i =\n\\left(\n\\begin{array}\n{c}\ny_{i1} \\\\\n. \\\\\ny_{in_i}\n\\end{array}\n\\right)\n\\]mean\\[\n\\mathbf{\\mu}_i =\n\\left(\n\\begin{array}\n{c}\n\\mu_{i1} \\\\\n. \\\\\n\\mu_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{x}_{ij} =\n\\left(\n\\begin{array}\n{c}\nX_{ij1} \\\\\n. \\\\\nX_{ijp}\n\\end{array}\n\\right)\n\\]Let \\(\\mathbf{V}_i = cov(\\mathbf{y}_i)\\), based (Liang Zeger 1986) GEE estimates \\(\\beta\\) can obtained solving equation:\\[\nS(\\beta) = \\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}_i'}{\\partial \\beta} \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{\\mu}_i) = 0\n\\]Let \\(\\mathbf{R}_i (\\mathbf{c})\\) \\(n_i \\times n_i\\) “working” correlation matrix specified parameters \\(\\mathbf{c}\\). , \\(\\mathbf{V}_i = (\\phi) \\mathbf{B}_i^{1/2}\\mathbf{R}(\\mathbf{c}) \\mathbf{B}_i^{1/2}\\), \\(\\mathbf{B}_i\\) \\(n_i \\times n_i\\) diagonal matrix \\(V(\\mu_{ij})\\) j-th diagonalIf \\(\\mathbf{R}(\\mathbf{c})\\) true correlation matrix \\(\\mathbf{y}_i\\), \\(\\mathbf{V}_i\\) true covariance matrixThe working correlation matrix must estimated iteratively fitting algorithm:Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Compute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Update \\(\\beta\\) according \n\\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Update \\(\\beta\\) according \\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Iterate algorithm convergesIterate algorithm convergesNote: Inference based likelihoods appropriate likelihood estimator","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-bayesian-hierarchical-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.3 Estimation by Bayesian Hierarchical Models","text":"Bayesian Estimation\\[\nf(\\mathbf{\\alpha}, \\mathbf{\\beta} | \\mathbf{y}) \\propto f(\\mathbf{y} | \\mathbf{\\alpha}, \\mathbf{\\beta}) f(\\mathbf{\\alpha})f(\\mathbf{\\beta})\n\\]Numerical techniques (e.g., MCMC) can used find posterior distribution. method best terms make simplifying approximation fully accounting uncertainty estimation prediction, complex, time-consuming, computationally intensive.Implementation Issues:valid joint distribution can constructed given conditional model random parametersNo valid joint distribution can constructed given conditional model random parametersThe mean/ variance relationship random effects lead constraints marginal covariance modelThe mean/ variance relationship random effects lead constraints marginal covariance modelDifficult fit computationallyDifficult fit computationally2 types estimation approaches:Approximate objective function (marginal likelihood) integral approximation\nLaplace methods\nQuadrature methods\nMonte Carlo integration\nApproximate objective function (marginal likelihood) integral approximationLaplace methodsLaplace methodsQuadrature methodsQuadrature methodsMonte Carlo integrationMonte Carlo integrationApproximate model (based Taylor series linearization)Approximate model (based Taylor series linearization)Packages RGLMM: MASS:glmmPQL lme4::glmer glmmTMBGLMM: MASS:glmmPQL lme4::glmer glmmTMBNLMM: nlme::nlme; lme4::nlmer brms::brmNLMM: nlme::nlme; lme4::nlmer brms::brmBayesian: MCMCglmm ; brms:brmBayesian: MCMCglmm ; brms:brmExample: Non-Gaussian Repeated measurementsWhen data Gaussian, Linear Mixed ModelsWhen data Gaussian, Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed Models","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"application-5","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2 Application","text":"","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-cbpp-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.1 Binomial (CBPP Data)","text":"PQLPro:Linearizes response pseudo-response mean response (like LMM)Linearizes response pseudo-response mean response (like LMM)computationally efficientcomputationally efficientCons:biased binary, Poisson data small countsbiased binary, Poisson data small countsrandom effects interpreted link scalerandom effects interpreted link scalecan’t interpret AIC/BIC valuecan’t interpret AIC/BIC valueis herd specific outcome odds varies.can interpret fixed effect coefficients just like GLM. use logit link function , can say log odds probability case period 2 -1.016 less period 1 (baseline).Numerical IntegrationPro:accurateCon:computationally expensivecomputationally expensivewon’t work complex models.won’t work complex models.small data set, difference two approaches minimalIn numerical integration, can set nAGQ > 1 switch method likelihood evaluation, might increase accuracyBayesian approach GLMMsassume fixed effects parameters distributionassume fixed effects parameters distributioncan handle models intractable result traditional methodscan handle models intractable result traditional methodscomputationally expensivecomputationally expensiveMCMCglmm fits residual variance component (useful dispersion issues)interpret Bayesian “credible intervals” similarly confidence intervalsMake sure make post-hoc diagnosesThere trend, well-mixedFor herd variable, lot 0, suggests problem. fix instability herd effect sampling, can eithermodify prior distribution herd variationmodify prior distribution herd variationincreases number iterationincreases number iterationTo change shape priors, MCMCglmm use:V controls location distribution (default = 1)V controls location distribution (default = 1)nu controls concentration around V (default = 0)nu controls concentration around V (default = 0)","code":"\ndata(cbpp,package = \"lme4\")\nhead(cbpp)\n#>   herd incidence size period\n#> 1    1         2   14      1\n#> 2    1         3   12      2\n#> 3    1         4    9      3\n#> 4    1         0    5      4\n#> 5    2         3   22      1\n#> 6    2         1   18      2\nlibrary(MASS)\npql_cbpp <-\n    glmmPQL(\n        cbind(incidence, size - incidence) ~ period,\n        random  = ~ 1 | herd,\n        data    = cbpp,\n        family  = binomial(link = \"logit\"),\n        verbose = F\n    )\nsummary(pql_cbpp)\n#> Linear mixed-effects model fit by maximum likelihood\n#>   Data: cbpp \n#>   AIC BIC logLik\n#>    NA  NA     NA\n#> \n#> Random effects:\n#>  Formula: ~1 | herd\n#>         (Intercept) Residual\n#> StdDev:   0.5563535 1.184527\n#> \n#> Variance function:\n#>  Structure: fixed weights\n#>  Formula: ~invwt \n#> Fixed effects:  cbind(incidence, size - incidence) ~ period \n#>                 Value Std.Error DF   t-value p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372  0.0000\n#> period2     -1.016126 0.3684079 38 -2.758156  0.0089\n#> period3     -1.149984 0.3937029 38 -2.920944  0.0058\n#> period4     -1.605217 0.5178388 38 -3.099839  0.0036\n#>  Correlation: \n#>         (Intr) perid2 perid3\n#> period2 -0.399              \n#> period3 -0.373  0.260       \n#> period4 -0.282  0.196  0.182\n#> \n#> Standardized Within-Group Residuals:\n#>        Min         Q1        Med         Q3        Max \n#> -2.0591168 -0.6493095 -0.2747620  0.5170492  2.6187632 \n#> \n#> Number of Observations: 56\n#> Number of Groups: 15\nexp(0.556)\n#> [1] 1.743684\nsummary(pql_cbpp)$tTable\n#>                 Value Std.Error DF   t-value      p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372 2.333216e-06\n#> period2     -1.016126 0.3684079 38 -2.758156 8.888179e-03\n#> period3     -1.149984 0.3937029 38 -2.920944 5.843007e-03\n#> period4     -1.605217 0.5178388 38 -3.099839 3.637000e-03\nlibrary(lme4)\nnumint_cbpp <-\n    glmer(\n        cbind(incidence, size - incidence) ~ \n            period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\")\n    )\nsummary(numint_cbpp)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(incidence, size - incidence) ~ period + (1 | herd)\n#>    Data: cbpp\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>    194.1    204.2    -92.0    184.1       51 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.3816 -0.7889 -0.2026  0.5142  2.8791 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  herd   (Intercept) 0.4123   0.6421  \n#> Number of obs: 56, groups:  herd, 15\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3983     0.2312  -6.048 1.47e-09 ***\n#> period2      -0.9919     0.3032  -3.272 0.001068 ** \n#> period3      -1.1282     0.3228  -3.495 0.000474 ***\n#> period4      -1.5797     0.4220  -3.743 0.000182 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr) perid2 perid3\n#> period2 -0.363              \n#> period3 -0.340  0.280       \n#> period4 -0.260  0.213  0.198\nlibrary(rbenchmark)\nbenchmark(\n    \"MASS\" = {\n        pql_cbpp <-\n            glmmPQL(\n                cbind(incidence, size - incidence) ~ period,\n                random = ~ 1 | herd,\n                data = cbpp,\n                family = binomial(link = \"logit\"),\n                verbose = F\n            )\n    },\n    \"lme4\" = {\n        glmer(\n            cbind(incidence, size - incidence) ~ period + (1 | herd),\n            data = cbpp,\n            family = binomial(link = \"logit\")\n        )\n    },\n    replications = 50,\n    columns = c(\"test\", \"replications\", \"elapsed\", \"relative\"),\n    order = \"relative\"\n)\n#>   test replications elapsed relative\n#> 1 MASS           50    3.79    1.000\n#> 2 lme4           50    7.79    2.055\nlibrary(lme4)\nnumint_cbpp_GH <-\n    glmer(\n        cbind(incidence, size - incidence) ~ period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\"),\n        nAGQ = 20\n    )\nsummary(numint_cbpp_GH)$coefficients[, 1] - \n    summary(numint_cbpp)$coefficients[, 1]\n#>   (Intercept)       period2       period3       period4 \n#> -0.0008808634  0.0005160912  0.0004066218  0.0002644629\nlibrary(MCMCglmm)\nBayes_cbpp <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random  = ~ herd,\n        data    = cbpp,\n        family  = \"multinomial2\",\n        verbose = FALSE\n    )\nsummary(Bayes_cbpp)\n#> \n#>  Iterations = 3001:12991\n#>  Thinning interval  = 10\n#>  Sample size  = 1000 \n#> \n#>  DIC: 537.9598 \n#> \n#>  G-structure:  ~herd\n#> \n#>      post.mean l-95% CI u-95% CI eff.samp\n#> herd   0.03246 1.03e-16   0.2073    105.7\n#> \n#>  R-structure:  ~units\n#> \n#>       post.mean l-95% CI u-95% CI eff.samp\n#> units     1.095   0.3017    2.281    306.9\n#> \n#>  Location effects: cbind(incidence, size - incidence) ~ period \n#> \n#>             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n#> (Intercept)   -1.5247  -2.1732  -0.9248    717.5 <0.001 ***\n#> period2       -1.2812  -2.3489  -0.3661    821.7  0.012 *  \n#> period3       -1.4152  -2.3443  -0.3088    691.5  0.004 ** \n#> period4       -1.9335  -3.2407  -0.8315    554.9 <0.001 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# explains less variability\napply(Bayes_cbpp$VCV,2,sd)\n#>      herd     units \n#> 0.1031743 0.5423514\nsummary(Bayes_cbpp)$solutions\n#>             post.mean  l-95% CI   u-95% CI eff.samp pMCMC\n#> (Intercept) -1.524731 -2.173223 -0.9247605 717.5157 0.001\n#> period2     -1.281212 -2.348887 -0.3660568 821.6596 0.012\n#> period3     -1.415170 -2.344293 -0.3087640 691.5463 0.004\n#> period4     -1.933501 -3.240745 -0.8314840 554.9365 0.001\nlibrary(lattice)\nxyplot(as.mcmc(Bayes_cbpp$Sol), layout = c(2, 2))\nxyplot(as.mcmc(Bayes_cbpp$VCV),layout=c(2,1))\nlibrary(MCMCglmm)\nBayes_cbpp2 <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random = ~ herd,\n        data   = cbpp,\n        family = \"multinomial2\",\n        nitt   = 20000,\n        burnin = 10000,\n        prior  = list(G = list(list(\n            V  = 1, nu = .1\n        ))),\n        verbose = FALSE\n    )\nxyplot(as.mcmc(Bayes_cbpp2$VCV), layout = c(2, 1))"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"count-owl-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.2 Count (Owl Data)","text":"typical Poisson model, \\(\\lambda\\) (Poisson mean), model \\(\\log(\\lambda) = \\mathbf{x'\\beta}\\) response rate (e.g., counts per BroodSize), model \\(\\log(\\lambda / b) = \\mathbf{x'\\beta}\\) , equivalently \\(\\log(\\lambda) = \\log(b) + \\mathbf{x'\\beta}\\) \\(b\\) BroodSize. Hence, “offset” mean log variable.nest explains relatively large proportion variability (standard deviation larger coefficients)nest explains relatively large proportion variability (standard deviation larger coefficients)model fit isn’t great (deviance 5202 594 df)model fit isn’t great (deviance 5202 594 df)improvement using negative binomial considering -dispersionTo account many 0s data, can use zero-inflated Poisson (ZIP) model.glmmTMB can handle ZIP GLMMs since adds automatic differentiation existing estimation strategies.can see ZIP GLMM arrival time covariate zero best.arrival time positive effect observing nonzero number callsarrival time positive effect observing nonzero number callsinteractions non significant, food treatment significant (fewer calls eating)interactions non significant, food treatment significant (fewer calls eating)nest variability large magnitude (without , parameter estimates change)nest variability large magnitude (without , parameter estimates change)","code":"\nlibrary(glmmTMB)\nlibrary(dplyr)\ndata(Owls, package = \"glmmTMB\")\nOwls <- Owls %>% \n    rename(Ncalls = SiblingNegotiation)\nowls_glmer <-\n    glmer(\n        Ncalls ~ offset(log(BroodSize)) \n        + FoodTreatment * SexParent +\n            (1 | Nest),\n        family = poisson,\n        data = Owls\n    )\nsummary(owls_glmer)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: poisson  ( log )\n#> Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +  \n#>     (1 | Nest)\n#>    Data: Owls\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>   5212.8   5234.8  -2601.4   5202.8      594 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.5529 -1.7971 -0.6842  1.2689 11.4312 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.2063   0.4542  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Fixed effects:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.65585    0.09567   6.855 7.12e-12 ***\n#> FoodTreatmentSatiated               -0.65612    0.05606 -11.705  < 2e-16 ***\n#> SexParentMale                       -0.03705    0.04501  -0.823   0.4104    \n#> FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) FdTrtS SxPrnM\n#> FdTrtmntStt -0.225              \n#> SexParentMl -0.292  0.491       \n#> FdTrtmS:SPM  0.170 -0.768 -0.605\n# Negative binomial model\nowls_glmerNB <-\n    glmer.nb(Ncalls ~ offset(log(BroodSize)) \n             + FoodTreatment * SexParent\n             + (1 | Nest), data = Owls)\n\nc(Deviance = round(summary(owls_glmerNB)$AICtab[\"deviance\"], 3),\n  df = summary(owls_glmerNB)$AICtab[\"df.resid\"])\n#> Deviance.deviance       df.df.resid \n#>          3483.616           593.000\nhist(Owls$Ncalls,breaks=30)\nlibrary(glmmTMB)\nowls_glmm <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 0,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\nowls_glmm_zi <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 1,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\n# Scale Arrival time to use as a covariate for zero-inflation parameter\nOwls$ArrivalTime <- scale(Owls$ArrivalTime)\nowls_glmm_zi_cov <- glmmTMB(\n    Ncalls ~ FoodTreatment * SexParent +\n        offset(log(BroodSize)) +\n        (1 | Nest),\n    ziformula =  ~ ArrivalTime,\n    family = nbinom2(link = \"log\"),\n    data = Owls\n)\nas.matrix(anova(owls_glmm, owls_glmm_zi))\n#>              Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm     6 3495.610 3521.981 -1741.805 3483.610       NA     NA\n#> owls_glmm_zi  7 3431.646 3462.413 -1708.823 3417.646 65.96373      1\n#>                Pr(>Chisq)\n#> owls_glmm              NA\n#> owls_glmm_zi 4.592983e-16\nas.matrix(anova(owls_glmm_zi, owls_glmm_zi_cov))\n#>                  Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm_zi      7 3431.646 3462.413 -1708.823 3417.646       NA     NA\n#> owls_glmm_zi_cov  8 3422.532 3457.694 -1703.266 3406.532 11.11411      1\n#>                    Pr(>Chisq)\n#> owls_glmm_zi               NA\n#> owls_glmm_zi_cov 0.0008567362\nsummary(owls_glmm_zi_cov)\n#>  Family: nbinom2  ( log )\n#> Formula:          \n#> Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)\n#> Zero inflation:          ~ArrivalTime\n#> Data: Owls\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>   3422.5   3457.7  -1703.3   3406.5      591 \n#> \n#> Random effects:\n#> \n#> Conditional model:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.07487  0.2736  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Dispersion parameter for nbinom2 family (): 2.22 \n#> \n#> Conditional model:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.84778    0.09961   8.511  < 2e-16 ***\n#> FoodTreatmentSatiated               -0.39529    0.13742  -2.877  0.00402 ** \n#> SexParentMale                       -0.07025    0.10435  -0.673  0.50079    \n#> FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero-inflation model:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3018     0.1261  -10.32  < 2e-16 ***\n#> ArrivalTime   0.3545     0.1074    3.30 0.000966 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.3 Binomial","text":"Fixed effects (\\(\\beta\\)) = genotypeFixed effects (\\(\\beta\\)) = genotypeRandom effects (\\(\\alpha\\)) = blockRandom effects (\\(\\alpha\\)) = blockEquivalently, can use MCMCglmm , Bayesian approach","code":"\nlibrary(agridat)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(spaMM)\ndata(gotway.hessianfly)\ndat <- gotway.hessianfly\ndat$prop <- dat$y / dat$n\n\nggplot(dat, aes(x = lat, y = long, fill = prop)) +\n    geom_tile() +\n    scale_fill_gradient(low = 'white', high = 'black') +\n    geom_text(aes(label = gen, color = block)) +\n    ggtitle('Gotway Hessian Fly')\nflymodel <-\n    glmer(\n        cbind(y, n - y) ~ gen + (1 | block),\n        data   = dat,\n        family = binomial,\n        nAGQ   = 5\n    )\nsummary(flymodel)\n#> Generalized linear mixed model fit by maximum likelihood (Adaptive\n#>   Gauss-Hermite Quadrature, nAGQ = 5) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(y, n - y) ~ gen + (1 | block)\n#>    Data: dat\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>    162.2    198.9    -64.1    128.2       47 \n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.38644 -1.01188  0.09631  1.03468  2.75479 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  block  (Intercept) 0.001022 0.03196 \n#> Number of obs: 64, groups:  block, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   1.5035     0.3914   3.841 0.000122 ***\n#> genG02       -0.1939     0.5302  -0.366 0.714644    \n#> genG03       -0.5408     0.5103  -1.060 0.289260    \n#> genG04       -1.4342     0.4714  -3.043 0.002346 ** \n#> genG05       -0.2037     0.5429  -0.375 0.707486    \n#> genG06       -0.9783     0.5046  -1.939 0.052533 .  \n#> genG07       -0.6041     0.5111  -1.182 0.237235    \n#> genG08       -1.6774     0.4907  -3.418 0.000630 ***\n#> genG09       -1.3984     0.4725  -2.960 0.003078 ** \n#> genG10       -0.6817     0.5333  -1.278 0.201181    \n#> genG11       -1.4630     0.4843  -3.021 0.002522 ** \n#> genG12       -1.4591     0.4918  -2.967 0.003010 ** \n#> genG13       -3.5528     0.6600  -5.383 7.31e-08 ***\n#> genG14       -2.5073     0.5264  -4.763 1.90e-06 ***\n#> genG15       -2.0872     0.4851  -4.302 1.69e-05 ***\n#> genG16       -2.9697     0.5383  -5.517 3.46e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(coda)\nBayes_flymodel <- MCMCglmm(\n    cbind(y, n - y) ~ gen ,\n    random  = ~ block,\n    data    = dat,\n    family  = \"multinomial2\",\n    verbose = FALSE\n)\nplot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1])\nautocorr.plot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1])"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"example-from-schabenberger_2001-section-8.4.1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.4 Example from (Schabenberger and Pierce 2001) section 8.4.1","text":"cumulative volume relates complementary diameter (subplots created based total tree height)proposed non-linear model:\\[\nV_{id_j} = (\\beta_0 + (\\beta_1 + b_{1i})\\frac{D^2_i H_i}{1000})(\\exp[-(\\beta_2 + b_{2i})t_{ij} \\exp(\\beta_3 t_{ij})]) + e_{ij}\n\\]\\(b_{1i}, b_{2i}\\) random effects\\(b_{1i}, b_{2i}\\) random effects\\(e_{ij}\\) random errors\\(e_{ij}\\) random errorsLittle different book different implementation nonlinear mixed models.red line = predicted observations based common fixed effectsteal line = tree-specific predictions random effects","code":"\ndat2 <- read.table(\"images/YellowPoplarData_r.txt\")\nnames(dat2) <- c('tn', 'k', 'dbh', 'totht',\n                 'dob', 'ht', 'maxd', 'cumv')\ndat2$t <- dat2$dob / dat2$dbh\ndat2$r <- 1 - dat2$dob / dat2$totht\nlibrary(ggplot2)\nlibrary(dplyr)\ndat2 <- dat2 %>% group_by(tn) %>% mutate(\n    z = case_when(\n        totht < 74 & totht >= 0 ~ 'a: 0-74ft',\n        totht < 88 & totht >= 74 ~ 'b: 74-88',\n        totht < 95 & totht >= 88 ~ 'c: 88-95',\n        totht < 99 & totht >= 95 ~ 'd: 95-99',\n        totht < 104 & totht >= 99 ~ 'e: 99-104',\n        totht < 109 & totht >= 104 ~ 'f: 104-109',\n        totht < 115 & totht >= 109 ~ 'g: 109-115',\n        totht < 120 & totht >= 115 ~ 'h: 115-120',\n        totht < 140 & totht >= 120 ~ 'i: 120-150',\n    )\n)\nggplot(dat2, aes(x = r, y = cumv)) + \n    geom_point(size = 0.5) + \n    facet_wrap(vars(z))\nlibrary(nlme)\ntmp <-\n    nlme(\n        cumv ~ (b0 + (b1 + u1) *\n                    (dbh * dbh * totht / 1000)) *\n            (exp(-(b2 + u2) * (t / 1000) * exp(b3 * t))), \n        data = dat2,\n        fixed = b0 + b1 + b2 + b3 ~ 1,\n        # 1 on the right hand side of the formula indicates \n        # a single fixed effects for the corresponding parameters\n        random = list(pdDiag(u1 + u2 ~ 1)),\n        #uncorrelated random effects\n        groups = ~ tn,\n        #group on trees so each tree w/ have u1 and u2\n        start = list(fixed = c(\n            b0 = 0.25,\n            b1 = 2.3,\n            b2 = 2.87,\n            b3 = 6.7\n        ))\n    )\nsummary(tmp)\n#> Nonlinear mixed-effects model fit by maximum likelihood\n#>   Model: cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht/1000)) * (exp(-(b2 +      u2) * (t/1000) * exp(b3 * t))) \n#>   Data: dat2 \n#>        AIC      BIC    logLik\n#>   31103.73 31151.33 -15544.86\n#> \n#> Random effects:\n#>  Formula: list(u1 ~ 1, u2 ~ 1)\n#>  Level: tn\n#>  Structure: Diagonal\n#>                u1       u2 Residual\n#> StdDev: 0.1508094 0.447829 2.226361\n#> \n#> Fixed effects:  b0 + b1 + b2 + b3 ~ 1 \n#>       Value  Std.Error   DF  t-value p-value\n#> b0 0.249386 0.12894687 6297   1.9340  0.0532\n#> b1 2.288832 0.01266804 6297 180.6776  0.0000\n#> b2 2.500497 0.05606685 6297  44.5985  0.0000\n#> b3 6.848871 0.02140677 6297 319.9395  0.0000\n#>  Correlation: \n#>    b0     b1     b2    \n#> b1 -0.639              \n#> b2  0.054  0.056       \n#> b3 -0.011 -0.066 -0.850\n#> \n#> Standardized Within-Group Residuals:\n#>           Min            Q1           Med            Q3           Max \n#> -6.694575e+00 -3.081861e-01 -8.904304e-05  3.469469e-01  7.855665e+00 \n#> \n#> Number of Observations: 6636\n#> Number of Groups: 336\nnlme::intervals(tmp)\n#> Approximate 95% confidence intervals\n#> \n#>  Fixed effects:\n#>           lower      est.     upper\n#> b0 -0.003318061 0.2493855 0.5020892\n#> b1  2.264006036 2.2888322 2.3136584\n#> b2  2.390620340 2.5004973 2.6103743\n#> b3  6.806919342 6.8488713 6.8908232\n#> \n#>  Random Effects:\n#>   Level: tn \n#>            lower      est.     upper\n#> sd(u1) 0.1376084 0.1508094 0.1652768\n#> sd(u2) 0.4056209 0.4478290 0.4944291\n#> \n#>  Within-group standard error:\n#>    lower     est.    upper \n#> 2.187258 2.226361 2.266162\nlibrary(cowplot)\nnlmmfn <- function(fixed,rand,dbh,totht,t){\n  b0 <- fixed[1]\n  b1 <- fixed[2]\n  b2 <- fixed[3]\n  b3 <- fixed[4]\n  u1 <- rand[1]\n  u2 <- rand[2]\n  #just made so we can predict w/o random effects\n  return((b0+(b1+u1)*(dbh*dbh*totht/1000))*(exp(-(b2+u2)*(t/1000)*exp(b3*t))))\n}\n\n\n\n#Tree 1\npred1 <- data.frame(seq(1, 24, length.out = 100))\nnames(pred1) <- 'dob'\npred1$tn <- 1\npred1$dbh <- unique(dat2[dat2$tn == 1, ]$dbh)\npred1$t <- pred1$dob / pred1$dbh\npred1$totht <- unique(dat2[dat2$tn == 1, ]$totht)\npred1$r <- 1 - pred1$dob / pred1$totht\n\n\npred1$test <- predict(tmp, pred1)\npred1$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred1$dbh,\n        pred1$totht,\n        pred1$t\n    )\n\np1 <-\n    ggplot(pred1) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 1, ], aes(x = r, y = cumv)) + \n    ggtitle('Tree 1') + theme(legend.position = \"none\")\n\n\n#Tree 151\npred151        <- data.frame(seq(1, 21, length.out = 100))\nnames(pred151) <- 'dob'\npred151$tn     <- 151\npred151$dbh    <- unique(dat2[dat2$tn == 151, ]$dbh)\npred151$t      <- pred151$dob / pred151$dbh\npred151$totht  <- unique(dat2[dat2$tn == 151, ]$totht)\npred151$r      <- 1 - pred151$dob / pred151$totht\n\n\npred151$test <- predict(tmp, pred151)\npred151$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred151$dbh,\n        pred151$totht,\n        pred151$t\n    )\n\np2 <-\n    ggplot(pred151) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 151,], aes(x = r, y = cumv)) + \n    ggtitle('Tree 151') + \n    theme(legend.position = \"none\")\n\n\n#Tree 279\npred279        <- data.frame(seq(1, 9, length.out = 100))\nnames(pred279) <- 'dob'\npred279$tn     <- 279\npred279$dbh    <- unique(dat2[dat2$tn == 279, ]$dbh)\npred279$t      <- pred279$dob / pred279$dbh\npred279$totht  <- unique(dat2[dat2$tn == 279, ]$totht)\npred279$r      <- 1 - pred279$dob / pred279$totht\n\n\npred279$test <- predict(tmp, pred279)\npred279$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred279$dbh,\n        pred279$totht,\n        pred279$t\n    )\n\np3 <-\n    ggplot(pred279) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 279, ], aes(x = r, y = cumv)) + \n    ggtitle('Tree 279') + \n    theme(legend.position = \"none\")\n\nplot_grid(p1, p2, p3)"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"summary-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.3 Summary","text":"","code":""},{"path":"model-specification.html","id":"model-specification","chapter":"10 Model Specification","heading":"10 Model Specification","text":"Test whether underlying assumptions hold trueNested Model (A1/A3)Non-Nested Model (A1/A3)Heteroskedasticity (A4)","code":""},{"path":"model-specification.html","id":"nested-model","chapter":"10 Model Specification","heading":"10.1 Nested Model","text":"\\[\n\\begin{aligned}\ny &= \\beta_0 + x_1\\beta_1 + x_2\\beta-2 + x_3\\beta_3 + \\epsilon & \\text{unrestricted model} \\\\\ny &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{restricted model}\n\\end{aligned}\n\\]Unrestricted model always longer restricted model\nrestricted model “nested” within unrestricted model\ndetermine variables included exclude, use Wald TestAdjusted \\(R^2\\)\\(R^2\\) always increase variables includedAdjusted \\(R^2\\) tries correct penalizing inclusion unnecessary variables.\\[\n\\begin{aligned}\n{R}^2 &= 1 - \\frac{SSR/n}{SST/n} \\\\\n{R}^2_{adj} &= 1 - \\frac{SSR/(n-k)}{SST/(n-1)} \\\\\n&= 1 - \\frac{(n-1)(1-R^2)}{(n-k)}\n\\end{aligned}\n\\]\\({R}^2_{adj}\\) increases t-statistic additional variable greater 1 absolute value.\\({R}^2_{adj}\\) valid models heteroskedasticitythere fore used determining variables included model (t F-tests appropriate)","code":""},{"path":"model-specification.html","id":"chow-test","chapter":"10 Model Specification","heading":"10.1.1 Chow test","text":"run two different regressions two groups?","code":""},{"path":"model-specification.html","id":"non-nested-model","chapter":"10 Model Specification","heading":"10.2 Non-Nested Model","text":"compare models different non-nested specifications","code":""},{"path":"model-specification.html","id":"davidson-mackinnon-test","chapter":"10 Model Specification","heading":"10.2.1 Davidson-Mackinnon test","text":"","code":""},{"path":"model-specification.html","id":"independent-variable","chapter":"10 Model Specification","heading":"10.2.1.1 Independent Variable","text":"independent variables logged? (decide non-nested alternatives)\\[\n\\begin{aligned}\ny =  \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(level eq)} \\\\\ny =  \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(log eq)}\n\\end{aligned}\n\\]Obtain predict outcome estimating model log equation \\(\\check{y}\\) estimate following auxiliary equation,\\[\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]evaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)Obtain predict outcome estimating model level equation \\(\\hat{y}\\), estimate following auxiliary equation,\\[\ny = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]evaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)reject null (1) step fail reject null second step, log equation preferred.fail reject null (1) step reject null (2) step , level equation preferred.reject steps, statistical evidence neither model used re-evaluate functional form model.fail reject steps, sufficient evidence prefer one model . can compare \\(R^2_{adj}\\) choose two models.\\[\n\\begin{aligned}\ny &= \\beta_0 + ln(x)\\beta_1 + \\epsilon \\\\\ny &= \\beta_0 + x(\\beta_1) + x^2\\beta_2 + \\epsilon\n\\end{aligned}\n\\]Compare better fits dataCompare standard \\(R^2\\) unfair second model less parsimonious (parameters estimate)\\(R_{adj}^2\\) penalize second model less parsimonious + valid heteroskedasticity (A4 holds)compare Davidson-Mackinnon test","code":""},{"path":"model-specification.html","id":"dependent-variable","chapter":"10 Model Specification","heading":"10.2.1.2 Dependent Variable","text":"\\[\n\\begin{aligned}\ny &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{level eq} \\\\\nln(y) &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{log eq} \\\\\n\\end{aligned}\n\\]level model, regardless big y , x constant effect (.e., one unit change \\(x_1\\) results \\(\\beta_1\\) unit change y)log model, larger y , effect x stronger (.e., one unit change \\(x_1\\) increase y 1 \\(1+\\beta_1\\) 100 100+100x\\(\\beta_1\\))compare \\(R^2\\) \\(R^2_{adj}\\) outcomes complement different, scaling different (SST different)need “un-transform” \\(ln(y)\\) back scale y compare,Estimate model log equation obtain predicted outcome \\(\\hat{ln(y)}\\)“Un-transform” predicted outcome\\[\n\\hat{m} = exp(\\hat{ln(y)})\n\\]Estimate following model (without intercept)\\[\ny = \\alpha\\hat{m} + error\n\\]obtain predicted outcome \\(\\hat{y}\\)take square correlation \\(\\hat{y}\\) y scaled version \\(R^2\\) log model can now compare usual \\(R^2\\) level model.","code":""},{"path":"model-specification.html","id":"heteroskedasticity","chapter":"10 Model Specification","heading":"10.3 Heteroskedasticity","text":"Using roust standard errors always validUsing roust standard errors always validIf significant evidence heteroskedasticity implying A4 hold\nGauss-Markov Theorem longer holds, OLS BLUE.\nconsider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)\nsignificant evidence heteroskedasticity implying A4 holdGauss-Markov Theorem longer holds, OLS BLUE.consider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)","code":""},{"path":"model-specification.html","id":"breusch-pagan-test","chapter":"10 Model Specification","heading":"10.3.1 Breusch-Pagan test","text":"A4 implies\\[\nE(\\epsilon_i^2|\\mathbf{x_i})=\\sigma^2\n\\]\\[\n\\epsilon_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]determining whether \\(\\mathbf{x}_i\\) predictive valueif \\(\\mathbf{x}_i\\) predictive value, variance changes levels \\(\\mathbf{x}_i\\) evidence heteroskedasticityif \\(\\mathbf{x}_i\\) predictive value, variance constant levels \\(\\mathbf{x}_i\\)Breusch-Pagan test heteroskedasticity compute F-test total significance following model\\[\ne_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]low p-value means reject null homoskedasticityHowever, Breusch-Pagan test detect heteroskedasticity non-linear form","code":""},{"path":"model-specification.html","id":"white-test","chapter":"10 Model Specification","heading":"10.3.2 White test","text":"test heteroskedasticity allow non-linear relationship computing F-test total significance following model (assume three independent random variables)\\[\n\\begin{aligned}\ne_i^2 &= \\gamma_0 + x_i \\gamma_1 + x_{i2}\\gamma_2 + x_{i3}\\gamma_3 \\\\\n&+ x_{i1}^2\\gamma_4 + x_{i2}^2\\gamma_5 + x_{i3}^2\\gamma_6 \\\\\n&+ (x_{i1} \\times x_{i2})\\gamma_7 + (x_{i1} \\times x_{i3})\\gamma_8 + (x_{i2} \\times x_{i3})\\gamma_9 + error\n\\end{aligned}\n\\]low p-value means reject null homoskedasticityEquivalently, can compute LM \\(LM = nR^2_{e^2}\\) \\(R^2_{e^2}\\) come regression squared residual outcomeThe LM statistic [\\(\\chi_k^2\\)][Chi-squared] distribution","code":""},{"path":"imputation-missing-data.html","id":"imputation-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11 Imputation (Missing Data)","text":"","code":""},{"path":"imputation-missing-data.html","id":"introduction-to-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.1 Introduction to Missing Data","text":"Missing data common problem statistical analyses data science, impacting quality reliability insights derived datasets. One widely used approach address issue imputation, missing data replaced reasonable estimates.","code":""},{"path":"imputation-missing-data.html","id":"types-of-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.1.1 Types of Imputation","text":"Imputation can categorized :Unit Imputation: Replacing entire missing observation (.e., features single data point missing).Item Imputation: Replacing missing values specific variables (features) within dataset.imputation offers means make use incomplete datasets, historically viewed skeptically. skepticism arises :Frequent misapplication imputation techniques, can introduce significant bias estimates.Limited applicability, imputation works well certain assumptions missing data mechanism research objectives.Biases imputation can arise various factors, including:Imputation method: chosen method can influence results introduce biases.Imputation method: chosen method can influence results introduce biases.Missing data mechanism: nature missing data—whether Missing Completely Random (MCAR) Missing Random (MAR)—affects accuracy imputation.Missing data mechanism: nature missing data—whether Missing Completely Random (MCAR) Missing Random (MAR)—affects accuracy imputation.Proportion missing data: amount missing data significantly impacts reliability imputation.Proportion missing data: amount missing data significantly impacts reliability imputation.Available information dataset: Limited information reduces robustness imputed values.Available information dataset: Limited information reduces robustness imputed values.","code":""},{"path":"imputation-missing-data.html","id":"when-and-why-to-use-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.1.2 When and Why to Use Imputation","text":"appropriateness imputation depends nature missing data research goal:Missing Data Outcome Variable (\\(y\\)): Imputation cases generally problematic, can distort statistical models lead misleading conclusions. example, imputing outcomes regression classification problems can alter underlying relationship dependent independent variables.Missing Data Outcome Variable (\\(y\\)): Imputation cases generally problematic, can distort statistical models lead misleading conclusions. example, imputing outcomes regression classification problems can alter underlying relationship dependent independent variables.Missing Data Predictive Variables (\\(x\\)): Imputation commonly applied , especially non-random missing data. Properly handled, imputation can enable use incomplete datasets minimizing bias.Missing Data Predictive Variables (\\(x\\)): Imputation commonly applied , especially non-random missing data. Properly handled, imputation can enable use incomplete datasets minimizing bias.","code":""},{"path":"imputation-missing-data.html","id":"objectives-of-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.1.2.1 Objectives of Imputation","text":"utility imputation methods differs substantially depending whether goal analysis inference/explanation prediction. goal distinct priorities tolerances bias, variance, assumptions missing data mechanism:","code":""},{"path":"imputation-missing-data.html","id":"inferenceexplanation","chapter":"11 Imputation (Missing Data)","heading":"11.1.2.1.1 Inference/Explanation","text":"causal inference explanatory analyses, primary objective ensure valid statistical inference, emphasizing unbiased estimation parameters accurate representation uncertainty. treatment missing data must align closely assumptions mechanism behind missing data—whether Missing Completely Random (MCAR), Missing Random (MAR), Missing Random (MNAR):Bias Sensitivity: Inference analyses require imputed data preserve integrity relationships among variables. Poorly executed imputation can introduce bias, even addresses missingness superficially.Bias Sensitivity: Inference analyses require imputed data preserve integrity relationships among variables. Poorly executed imputation can introduce bias, even addresses missingness superficially.Variance Confidence Intervals: inference, quality standard errors, confidence intervals, test statistics critical. Naive imputation methods (e.g., mean imputation) often fail appropriately reflect uncertainty due missingness, leading overconfidence parameter estimates.Variance Confidence Intervals: inference, quality standard errors, confidence intervals, test statistics critical. Naive imputation methods (e.g., mean imputation) often fail appropriately reflect uncertainty due missingness, leading overconfidence parameter estimates.Mechanism Considerations: Imputation methods, multiple imputation (MI), attempt generate values consistent observed data distribution accounting missing data uncertainty. However, MI’s performance depends heavily validity MAR assumption. missingness mechanism MNAR addressed adequately, imputed data yield biased parameter estimates, undermining purpose inference.Mechanism Considerations: Imputation methods, multiple imputation (MI), attempt generate values consistent observed data distribution accounting missing data uncertainty. However, MI’s performance depends heavily validity MAR assumption. missingness mechanism MNAR addressed adequately, imputed data yield biased parameter estimates, undermining purpose inference.","code":""},{"path":"imputation-missing-data.html","id":"prediction-1","chapter":"11 Imputation (Missing Data)","heading":"11.1.2.1.2 Prediction","text":"predictive modeling, primary goal maximize model accuracy (e.g., minimizing mean squared error continuous outcomes maximizing classification accuracy). , focus shifts optimizing predictive performance rather ensuring unbiased parameter estimates:Loss Information: Missing data reduces amount usable information dataset. Imputation allows model leverage available data, rather excluding incomplete cases via listwise deletion, can significantly reduce sample size model performance.Loss Information: Missing data reduces amount usable information dataset. Imputation allows model leverage available data, rather excluding incomplete cases via listwise deletion, can significantly reduce sample size model performance.Impact Model Fit: predictive contexts, imputation can reduce standard errors predictions stabilize model coefficients incorporating plausible estimates missing values.Impact Model Fit: predictive contexts, imputation can reduce standard errors predictions stabilize model coefficients incorporating plausible estimates missing values.Flexibility Mechanism: Predictive models less sensitive missing data mechanism inferential models, long imputed values help reduce variability align patterns observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, even machine learning models (e.g., random forests imputation) can valuable, regardless strict adherence MAR MCAR assumptions.Flexibility Mechanism: Predictive models less sensitive missing data mechanism inferential models, long imputed values help reduce variability align patterns observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, even machine learning models (e.g., random forests imputation) can valuable, regardless strict adherence MAR MCAR assumptions.Trade-offs: Overimputation, much noise complexity introduced imputation process, can harm prediction introducing artifacts degrade model generalizability.Trade-offs: Overimputation, much noise complexity introduced imputation process, can harm prediction introducing artifacts degrade model generalizability.","code":""},{"path":"imputation-missing-data.html","id":"key-takeaways-1","chapter":"11 Imputation (Missing Data)","heading":"11.1.2.1.3 Key Takeaways","text":"usefulness imputation depends whether goal analysis inference prediction:Inference/Explanation: primary concern valid statistical inference, biased estimates unacceptable. Imputation often limited value purpose, may address underlying missing data mechanism appropriately (Rubin 1996).Inference/Explanation: primary concern valid statistical inference, biased estimates unacceptable. Imputation often limited value purpose, may address underlying missing data mechanism appropriately (Rubin 1996).Prediction: Imputation can useful predictive modeling, reduces loss information incomplete cases. leveraging observed data, imputation can lower standard errors improve model accuracy.Prediction: Imputation can useful predictive modeling, reduces loss information incomplete cases. leveraging observed data, imputation can lower standard errors improve model accuracy.","code":""},{"path":"imputation-missing-data.html","id":"importance-of-missing-data-treatment-in-statistical-modeling","chapter":"11 Imputation (Missing Data)","heading":"11.1.3 Importance of Missing Data Treatment in Statistical Modeling","text":"Proper handling missing data ensures:Unbiased Estimates: Avoiding distortions parameter estimates.Accurate Standard Errors: Ensuring valid hypothesis testing confidence intervals.Adequate Statistical Power: Maximizing use available data.Ignoring mishandling missing data can lead :Bias: Systematic errors parameter estimates, especially MAR MNAR mechanisms.Loss Power: Reduced sample size leads larger standard errors weaker statistical significance.Misleading Conclusions: -simplistic imputation methods (e.g., mean substitution) can distort relationships among variables.","code":""},{"path":"imputation-missing-data.html","id":"prevalence-of-missing-data-across-domains","chapter":"11 Imputation (Missing Data)","heading":"11.1.4 Prevalence of Missing Data Across Domains","text":"Missing data affects virtually fields:Business: Non-responses customer surveys, incomplete sales records, transactional errors.Healthcare: Missing data electronic health records (EHRs) due incomplete patient histories inconsistent data entry.Social Sciences: Non-responses partial responses large-scale surveys, leading biased conclusions.","code":""},{"path":"imputation-missing-data.html","id":"practical-considerations-for-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.1.5 Practical Considerations for Imputation","text":"Diagnostic Checks: Always examine patterns mechanisms missing data applying imputation (Diagnosing Missing Data Mechanism).Model Selection: Align imputation method missing data mechanism research goal.Validation: Assess impact imputation results sensitivity analyses cross-validation.","code":""},{"path":"imputation-missing-data.html","id":"theoretical-foundations-of-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.2 Theoretical Foundations of Missing Data","text":"","code":""},{"path":"imputation-missing-data.html","id":"definition-and-classification-of-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.2.1 Definition and Classification of Missing Data","text":"Missing data refers absence values variables dataset. mechanisms underlying missingness significantly impact validity statistical analyses choice handling methods. mechanisms classified three categories:Missing Random (MNAR): Missingness depends unobserved variables missing values .","code":""},{"path":"imputation-missing-data.html","id":"missing-completely-at-random-mcar","chapter":"11 Imputation (Missing Data)","heading":"11.2.1.1 Missing Completely at Random (MCAR)","text":"MCAR occurs probability missingness entirely random unrelated either observed unobserved variables. mechanism, missing data introduce bias parameter estimates ignored, although statistical efficiency reduced due smaller sample size.Mathematical Definition: missingness independent data, observed unobserved:\\[\nP(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}})\n\\]Characteristics MCAR:Missingness completely unrelated observed unobserved data.Analyses remain unbiased even missing data ignored, though may lack efficiency due reduced sample size.missing data points represent random subset overall data.Examples:sensor randomly fails specific time points, unrelated environmental operational conditions.Survey participants randomly omit responses certain questions without systematic pattern.Methods Testing MCAR:Little’s MCAR Test: formal statistical test assess whether data MCAR. significant result suggests deviation MCAR.Little’s MCAR Test: formal statistical test assess whether data MCAR. significant result suggests deviation MCAR.Mean Comparison Tests:\nT-tests similar approaches compare observed missing data groups variables. Significant differences indicate potential bias.\nFailure reject null hypothesis difference confirm MCAR suggests consistency MCAR assumption.\nMean Comparison Tests:T-tests similar approaches compare observed missing data groups variables. Significant differences indicate potential bias.Failure reject null hypothesis difference confirm MCAR suggests consistency MCAR assumption.Handling MCAR:Since MCAR data introduce bias, can handled using following techniques:Complete Case Analysis (Listwise Deletion):\nAnalyses performed cases complete data. unbiased MCAR, method reduces sample size efficiency.\nAnalyses performed cases complete data. unbiased MCAR, method reduces sample size efficiency.Universal Singular Value Thresholding (USVT):\ntechnique effective MCAR data recovery can recover mean structure, entire true distribution (Chatterjee 2015).\ntechnique effective MCAR data recovery can recover mean structure, entire true distribution (Chatterjee 2015).SoftImpute:\nmatrix completion method useful missing data problems less effective missingness MCAR (Hastie et al. 2015).\nmatrix completion method useful missing data problems less effective missingness MCAR (Hastie et al. 2015).Synthetic Nearest Neighbor Imputation:\nrobust method imputing missing data. primarily designed MCAR, can also handle certain cases missing random (MNAR) (Agarwal et al. 2023). Available GitHub: syntheticNN.\nrobust method imputing missing data. primarily designed MCAR, can also handle certain cases missing random (MNAR) (Agarwal et al. 2023). Available GitHub: syntheticNN.Notes:“missingness” one variable can correlated “missingness” another variable without violating MCAR assumption.Absence evidence bias (e.g., failing reject t-test) confirm data MCAR.","code":""},{"path":"imputation-missing-data.html","id":"missing-at-random-mar","chapter":"11 Imputation (Missing Data)","heading":"11.2.1.2 Missing at Random (MAR)","text":"Missing Random (MAR) occurs missingness depends observed variables missing values . mechanism assumes observed data provide sufficient information explain missingness. words, systematic relationship propensity missing values observed data, missing data.Mathematical Definition:probability missingness conditional observed data:\\[\nP(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}} | X)\n\\]implies whether observation missing unrelated missing values related observed values variables.Characteristics MAR:Missingness systematically related observed variables.propensity data point missing related missing data related observed data.Analyses must account observed data mitigate bias.Examples:Women less likely disclose weight, gender recorded. case, weight MAR.Missing income data correlated education, observed. example, individuals higher education levels might less likely reveal income.Challenges MAR:MAR weaker Missing Completely Random (MCAR).impossible directly test MAR. Evidence MAR relies domain expertise indirect statistical checks rather direct tests.Handling MAR:Common methods handling MAR include:Multiple Imputation Chained Equations (MICE): Iteratively imputes missing values based observed data.Multiple Imputation Chained Equations (MICE): Iteratively imputes missing values based observed data.Maximum Likelihood Estimation: Estimates model parameters directly accounting MAR assumptions.Maximum Likelihood Estimation: Estimates model parameters directly accounting MAR assumptions.Regression-Based Imputation: Predicts missing values using observed covariates.Regression-Based Imputation: Predicts missing values using observed covariates.methods assume observed variables fully explain missingness. Effective handling MAR requires careful modeling often domain-specific knowledge validate assumptions underlying analysis.","code":""},{"path":"imputation-missing-data.html","id":"missing-not-at-random-mnar","chapter":"11 Imputation (Missing Data)","heading":"11.2.1.3 Missing Not at Random (MNAR)","text":"Missing Random (MNAR) complex missing data mechanism. , missingness depends unobserved variables values missing data . makes MNAR particularly challenging, ignoring dependency introduces significant bias analyses.Mathematical Definition:probability missingness depends missing values:\\[\nP(Y_{\\text{missing}} | Y, X) \\neq P(Y_{\\text{missing}} | X)\n\\]Characteristics MNAR:Missingness fully explained observed data.cause missingness directly related unobserved values.Ignoring MNAR introduces significant bias parameter estimates, often leading invalid conclusions.Examples:High-income individuals less likely disclose income, income unobserved.Patients severe symptoms drop clinical study, leaving health outcomes unrecorded.Challenges MNAR:MNAR difficult missingness mechanism address missing data mechanism must explicitly modeled.Identifying MNAR often requires domain knowledge auxiliary information beyond observed dataset.Handling MNAR:MNAR requires explicit modeling missingness mechanism. Common approaches include:Heckman Selection Models: models explicitly account selection process leading missing data, adjusting potential bias (James J. Heckman 1976).Heckman Selection Models: models explicitly account selection process leading missing data, adjusting potential bias (James J. Heckman 1976).Instrumental Variables: Variables predictive missingness unrelated outcome can used mitigate bias (B. Sun et al. 2018; E. J. Tchetgen Tchetgen Wirth 2017).Instrumental Variables: Variables predictive missingness unrelated outcome can used mitigate bias (B. Sun et al. 2018; E. J. Tchetgen Tchetgen Wirth 2017).Pattern-Mixture Models: models separate data groups (patterns) based missingness model group separately. particularly useful relationship missingness missing values complex.Pattern-Mixture Models: models separate data groups (patterns) based missingness model group separately. particularly useful relationship missingness missing values complex.Sensitivity Analysis: Examines conclusions change different assumptions missing data mechanism.Sensitivity Analysis: Examines conclusions change different assumptions missing data mechanism.Use Auxiliary Data\nAuxiliary data refers external data sources variables can help explain missingness mechanism.\nSurrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.\nLinking External Datasets: Merging datasets different sources can provide additional context predictors missingness.\nApplications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.\nUse Auxiliary DataAuxiliary data refers external data sources variables can help explain missingness mechanism.Surrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.Surrogate Variables: Adding variables correlate missing data can improve imputation accuracy mitigate MNAR challenge.Linking External Datasets: Merging datasets different sources can provide additional context predictors missingness.Linking External Datasets: Merging datasets different sources can provide additional context predictors missingness.Applications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.Applications Business: marketing, customer demographics transaction histories often serve auxiliary data predict missing responses surveys.Additionally, data collection strategies, follow-surveys targeted sampling, can help mitigate MNAR effects collecting information directly addresses missingness mechanism. However, approaches can resource-intensive require careful planning.","code":""},{"path":"imputation-missing-data.html","id":"missing-data-mechanisms","chapter":"11 Imputation (Missing Data)","heading":"11.2.2 Missing Data Mechanisms","text":"","code":""},{"path":"imputation-missing-data.html","id":"relationship-between-mechanisms-and-ignorability","chapter":"11 Imputation (Missing Data)","heading":"11.2.3 Relationship Between Mechanisms and Ignorability","text":"concept ignorability central determining whether missingness process must explicitly modeled. Ignorability impacts choice methods handling missing data whether missing data mechanism can safely disregarded must explicitly accounted .","code":""},{"path":"imputation-missing-data.html","id":"ignorable-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.2.3.1 Ignorable Missing Data","text":"Missing data ignorable following conditions:missing data mechanism MAR MCAR.parameters governing missing data process unrelated parameters interest analysis.cases ignorable missing data, need model missingness mechanism explicitly unless aim improve efficiency precision parameter estimates. Common imputation techniques, multiple imputation maximum likelihood estimation, rely assumption ignorability produce unbiased parameter estimates.Practical Considerations Ignorable MissingnessEven though ignorable mechanisms simplify analysis, researchers must rigorously assess whether missingness mechanism meets MAR MCAR criteria. Violations can lead biased results, even unintentionally overlooked.example: survey income may assume MAR missingness associated respondent age (observed variable) income (unobserved variable). However, income directly influences nonresponse, assumption MAR violated.","code":""},{"path":"imputation-missing-data.html","id":"non-ignorable","chapter":"11 Imputation (Missing Data)","heading":"11.2.3.2 Non-Ignorable Missing Data","text":"Missing data non-ignorable :missingness mechanism depends values missing data unobserved variables.missing data mechanism related parameters interest, resulting bias mechanism modeled explicitly.type missingness (.e., Missing Random (MNAR) requires modeling missing data mechanism directly produce unbiased estimates.Characteristics Non-Ignorable MissingnessDependence Missing Values: likelihood missingness associated missing values .\nExample: study health, individuals severe conditions likely drop , leading underrepresentation sickest individuals data.\nExample: study health, individuals severe conditions likely drop , leading underrepresentation sickest individuals data.Bias Complete Case Analysis: Analyses based solely complete cases can lead substantial bias.\nExample: income surveys, wealthier individuals less likely report income, estimated mean income systematically lower true population mean.\nExample: income surveys, wealthier individuals less likely report income, estimated mean income systematically lower true population mean.Need Explicit Modeling: address MNAR, analyst must model missing data mechanism. often involves specifying relationships observed data, missing data, missingness process .","code":""},{"path":"imputation-missing-data.html","id":"implications-of-non-ignorable-missingness","chapter":"11 Imputation (Missing Data)","heading":"11.2.3.3 Implications of Non-Ignorable Missingness","text":"Non-ignorable mechanisms often associated sensitive personal data:Examples:\nIndividuals lower education levels may omit education information.\nParticipants controversial stigmatized health conditions might opt surveys entirely.\nExamples:Individuals lower education levels may omit education information.Individuals lower education levels may omit education information.Participants controversial stigmatized health conditions might opt surveys entirely.Participants controversial stigmatized health conditions might opt surveys entirely.Impact Policy Decision-Making:\nBiases introduced MNAR can serious consequences policymaking, underestimating prevalence poverty mischaracterizing population health needs.\nImpact Policy Decision-Making:Biases introduced MNAR can serious consequences policymaking, underestimating prevalence poverty mischaracterizing population health needs.explicitly addressing non-ignorable missingness, researchers can mitigate biases ensure findings accurately reflect underlying population.","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-the-missing-data-mechanism","chapter":"11 Imputation (Missing Data)","heading":"11.3 Diagnosing the Missing Data Mechanism","text":"Understanding mechanism behind missing data critical choosing appropriate methods handling . three main mechanisms missing data MCAR (Missing Completely Random), MAR (Missing Random), MNAR (Missing Random). section discusses methods diagnosing mechanisms, including descriptive inferential approaches.","code":""},{"path":"imputation-missing-data.html","id":"descriptive-methods","chapter":"11 Imputation (Missing Data)","heading":"11.3.1 Descriptive Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"visualizing-missing-data-patterns","chapter":"11 Imputation (Missing Data)","heading":"11.3.1.1 Visualizing Missing Data Patterns","text":"Visualization tools essential detecting patterns missing data. Heatmaps correlation plots can help identify systematic missingness provide insights underlying mechanism.Heatmaps: Highlight missingness occurs dataset.Heatmaps: Highlight missingness occurs dataset.Correlation Plots: Show relationships missingness indicators different variables.Correlation Plots: Show relationships missingness indicators different variables.Exploring Univariate Multivariate MissingnessUnivariate Analysis: Calculate proportion missing data variable.Multivariate Analysis: Examine whether missingness one variable related others. can visualized using scatterplots observed vs. missing values.","code":"\n# Example: Visualizing missing data\nlibrary(Amelia)\nmissmap(\n    airquality,\n    main = \"Missing Data Heatmap\",\n    col = c(\"yellow\", \"black\"),\n    legend = TRUE\n)\n# Example: Proportion of missing values\nmissing_proportions <- colSums(is.na(airquality)) / nrow(airquality)\nprint(missing_proportions)\n#>      Ozone    Solar.R       Wind       Temp      Month        Day \n#> 0.24183007 0.04575163 0.00000000 0.00000000 0.00000000 0.00000000\n# Example: Missingness correlation\nlibrary(naniar)\nvis_miss(airquality)\ngg_miss_upset(airquality) # Displays a missingness upset plot"},{"path":"imputation-missing-data.html","id":"statistical-tests-for-missing-data-mechanisms","chapter":"11 Imputation (Missing Data)","heading":"11.3.2 Statistical Tests for Missing Data Mechanisms","text":"","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-mcar-littles-test","chapter":"11 Imputation (Missing Data)","heading":"11.3.2.1 Diagnosing MCAR: Little’s Test","text":"Little’s test hypothesis test determine missing data mechanism MCAR. tests whether means observed missing data significantly different. null hypothesis data MCAR.\\[\n\\chi^2 = \\sum_{=1}^n \\frac{(O_i - E_i)^2}{E_i}\n\\]:\\(O_i\\)= Observed frequency\\(O_i\\)= Observed frequency\\(E_i\\)= Expected frequency MCAR\\(E_i\\)= Expected frequency MCAR","code":"\n# Example: Little's test\nnaniar::mcar_test(airquality)\n#> # A tibble: 1 × 4\n#>   statistic    df p.value missing.patterns\n#>       <dbl> <dbl>   <dbl>            <int>\n#> 1      35.1    14 0.00142                4\nmisty::na.test(airquality)\n#>  Little's MCAR Test\n#> \n#>     n nIncomp nPattern  chi2 df  pval \n#>   153      42        4 35.15 14 0.001"},{"path":"imputation-missing-data.html","id":"diagnosing-mcar-via-dummy-variables","chapter":"11 Imputation (Missing Data)","heading":"11.3.2.2 Diagnosing MCAR via Dummy Variables","text":"Creating binary indicator missingness allows test whether presence missing data related observed data. instance:Create dummy variable:\n1 = Missing\n0 = Observed\nCreate dummy variable:1 = Missing1 = Missing0 = Observed0 = ObservedConduct chi-square test t-test:\nChi-square: Compare proportions missingness across groups.\nT-test: Compare means () observed variables missingness indicators.\nConduct chi-square test t-test:Chi-square: Compare proportions missingness across groups.Chi-square: Compare proportions missingness across groups.T-test: Compare means () observed variables missingness indicators.T-test: Compare means () observed variables missingness indicators.","code":"\n# Example: Chi-square test\nairquality$missing_var <- as.factor(ifelse(is.na(airquality$Ozone), 1, 0))\n# Across groups of months\ntable(airquality$missing_var, airquality$Month)\n#>    \n#>      5  6  7  8  9\n#>   0 26  9 26 26 29\n#>   1  5 21  5  5  1\nchisq.test(table(airquality$missing_var, airquality$Month))\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  table(airquality$missing_var, airquality$Month)\n#> X-squared = 44.751, df = 4, p-value = 4.48e-09\n\n# Example: T-test (of other variable)\nt.test(Wind ~ missing_var, data = airquality)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  Wind by missing_var\n#> t = -0.60911, df = 63.646, p-value = 0.5446\n#> alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.6893132  0.8999377\n#> sample estimates:\n#> mean in group 0 mean in group 1 \n#>        9.862069       10.256757"},{"path":"imputation-missing-data.html","id":"assessing-mar-and-mnar","chapter":"11 Imputation (Missing Data)","heading":"11.3.3 Assessing MAR and MNAR","text":"","code":""},{"path":"imputation-missing-data.html","id":"sensitivity-analysis","chapter":"11 Imputation (Missing Data)","heading":"11.3.3.1 Sensitivity Analysis","text":"Sensitivity analysis involves simulating different scenarios missing data assessing results change. example, imputing missing values different assumptions can provide insight whether data MAR MNAR.","code":""},{"path":"imputation-missing-data.html","id":"proxy-variables-and-external-data","chapter":"11 Imputation (Missing Data)","heading":"11.3.3.2 Proxy Variables and External Data","text":"Using proxy variables external data sources can help assess whether missingness depends unobserved variables (MNAR). example, surveys, follow-ups non-respondents can reveal systematic differences.","code":""},{"path":"imputation-missing-data.html","id":"practical-challenges-in-distinguishing-mar-from-mnar","chapter":"11 Imputation (Missing Data)","heading":"11.3.3.3 Practical Challenges in Distinguishing MAR from MNAR","text":"Distinguishing Missing Random (MAR) Missing Random (MNAR) critical challenging task data analysis. Properly identifying nature missing data significant implications choice imputation strategies, model robustness, validity conclusions. statistical tests can sometimes aid determination, process often relies heavily domain knowledge, intuition, exploratory analysis. , discuss key considerations examples highlight challenges:Sensitive Topics: Missing data related sensitive stigmatized topics, income, drug use, health conditions, often MNAR. example, individuals higher incomes might deliberately choose report earnings due privacy concerns. Similarly, participants health survey may avoid answering questions smoking perceive social disapproval. cases, probability missingness directly related unobserved value , making MNAR likely.Sensitive Topics: Missing data related sensitive stigmatized topics, income, drug use, health conditions, often MNAR. example, individuals higher incomes might deliberately choose report earnings due privacy concerns. Similarly, participants health survey may avoid answering questions smoking perceive social disapproval. cases, probability missingness directly related unobserved value , making MNAR likely.Field-Specific Norms: Understanding norms typical data collection practices specific field can provide insights missingness patterns. instance, marketing surveys, respondents may skip questions spending habits consider questions intrusive. Prior research historical data domain can help infer whether missingness likely MAR (e.g., random skipping due survey fatigue) MNAR (e.g., deliberate omission higher spenders).Field-Specific Norms: Understanding norms typical data collection practices specific field can provide insights missingness patterns. instance, marketing surveys, respondents may skip questions spending habits consider questions intrusive. Prior research historical data domain can help infer whether missingness likely MAR (e.g., random skipping due survey fatigue) MNAR (e.g., deliberate omission higher spenders).Analyzing Auxiliary Variables: Leveraging auxiliary variables—correlated missing variable—can help infer missingness mechanism. example, missing income data strongly correlates employment status, suggests MAR mechanism, missingness depends observed variables. However, missingness persists even accounting observable predictors, MNAR might play.Analyzing Auxiliary Variables: Leveraging auxiliary variables—correlated missing variable—can help infer missingness mechanism. example, missing income data strongly correlates employment status, suggests MAR mechanism, missingness depends observed variables. However, missingness persists even accounting observable predictors, MNAR might play.Experimental Design Follow-: longitudinal studies, dropout rates can signal MAR MNAR patterns. example, dropouts occur disproportionately among participants reporting lower satisfaction early surveys, indicates MNAR mechanism. Designing follow-surveys specifically investigate dropout reasons can clarify missingness patterns.Experimental Design Follow-: longitudinal studies, dropout rates can signal MAR MNAR patterns. example, dropouts occur disproportionately among participants reporting lower satisfaction early surveys, indicates MNAR mechanism. Designing follow-surveys specifically investigate dropout reasons can clarify missingness patterns.Sensitivity Analysis: account uncertainty missingness mechanism, researchers can conduct sensitivity analyses comparing results different assumptions (e.g., imputing data using MAR MNAR approaches). process helps quantify potential impact misclassifying missingness mechanism study conclusions.Sensitivity Analysis: account uncertainty missingness mechanism, researchers can conduct sensitivity analyses comparing results different assumptions (e.g., imputing data using MAR MNAR approaches). process helps quantify potential impact misclassifying missingness mechanism study conclusions.Real-World Examples:\ncustomer feedback surveys, higher ratings might overrepresented due non-response bias. Customers negative experiences might less likely complete surveys, leading MNAR scenario.\nfinancial reporting, missing audit data might correlate companies financial distress, classic MNAR case missingness depends unobserved financial health metrics.\nReal-World Examples:customer feedback surveys, higher ratings might overrepresented due non-response bias. Customers negative experiences might less likely complete surveys, leading MNAR scenario.financial reporting, missing audit data might correlate companies financial distress, classic MNAR case missingness depends unobserved financial health metrics.SummaryMCAR: pattern missingness; use Little’s test dummy variable analysis.MCAR: pattern missingness; use Little’s test dummy variable analysis.MAR: Missingness related observed data; requires modeling assumptions proxy analysis.MAR: Missingness related observed data; requires modeling assumptions proxy analysis.MNAR: Missingness depends unobserved data; requires external validation sensitivity analysis.MNAR: Missingness depends unobserved data; requires external validation sensitivity analysis.","code":""},{"path":"imputation-missing-data.html","id":"methods-for-handling-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.4 Methods for Handling Missing Data","text":"","code":""},{"path":"imputation-missing-data.html","id":"basic-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.1 Basic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"complete-case-analysis-listwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.4.1.1 Complete Case Analysis (Listwise Deletion)","text":"Listwise deletion retains cases complete data features, discarding rows missing values.Advantages:Universally applicable various statistical tests (e.g., SEM, multilevel regression).data Missing Completely Random (MCAR), parameter estimates standard errors unbiased.specific Missing Random (MAR) conditions, probability missing data depends independent variables, listwise deletion can still yield unbiased estimates. instance, model \\(y = \\beta_{0} + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\), missingness \\(X_1\\) independent \\(y\\) depends \\(X_1\\) \\(X_2\\), estimates remain unbiased (Little 1992).\naligns principles stratified sampling, bias estimates.\nlogistic regression, missing data depend dependent variable independent variables, listwise deletion produces consistent slope estimates, though intercept may biased (Vach Vach 1994).\naligns principles stratified sampling, bias estimates.logistic regression, missing data depend dependent variable independent variables, listwise deletion produces consistent slope estimates, though intercept may biased (Vach Vach 1994).regression analysis, listwise deletion robust Maximum Likelihood (ML) Multiple Imputation (MI) MAR assumption violated.Disadvantages:Results larger standard errors compared advanced methods.data MAR MCAR, biased estimates can occur.non-regression contexts, sophisticated methods often outperform listwise deletion.","code":""},{"path":"imputation-missing-data.html","id":"available-case-analysis-pairwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.4.1.2 Available Case Analysis (Pairwise Deletion)","text":"Pairwise deletion calculates estimates using available data pair variables, without requiring complete cases. particularly suitable methods like linear regression, factor analysis, SEM, rely correlation covariance matrices.Advantages:MCAR, pairwise deletion produces consistent unbiased estimates large samples.Compared listwise deletion (Glasser 1964):\nvariable correlations low, pairwise deletion provides efficient estimates.\ncorrelations high, listwise deletion becomes efficient.\nvariable correlations low, pairwise deletion provides efficient estimates.correlations high, listwise deletion becomes efficient.Disadvantages:Yields biased estimates MAR conditions.small samples, covariance matrices might positive definite, rendering coefficient estimation infeasible.Software implementation varies sample size handled, potentially affecting standard errors.Note: Carefully review software documentation understand sample size treated, influences standard error calculations.","code":""},{"path":"imputation-missing-data.html","id":"indicator-method-dummy-variable-adjustment","chapter":"11 Imputation (Missing Data)","heading":"11.4.1.3 Indicator Method (Dummy Variable Adjustment)","text":"Also known Missing Indicator Method, approach introduces additional variable indicate missingness dataset.Implementation:Create indicator variable:\\[\nD =\n\\begin{cases}\n1 & \\text{data } X \\text{ missing} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]Modify original variable accommodate missingness:\\[\nX^* =\n\\begin{cases}\nX & \\text{data available} \\\\\nc & \\text{data missing}\n\\end{cases}\n\\]Note: common choice \\(c\\) mean \\(X\\).Interpretation:coefficient \\(D\\) represents difference expected value \\(Y\\) cases missing data without.coefficient \\(X^*\\) reflects effect \\(X\\) \\(Y\\) cases observed data.Disadvantages:Produces biased estimates coefficients, even MCAR conditions (Jones 1996).May lead overinterpretation “missingness effect,” complicating model interpretation.","code":""},{"path":"imputation-missing-data.html","id":"advantages-and-limitations-of-basic-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.1.4 Advantages and Limitations of Basic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"single-imputation-techniques","chapter":"11 Imputation (Missing Data)","heading":"11.4.2 Single Imputation Techniques","text":"Single imputation methods replace missing data single value, generating complete dataset can analyzed using standard techniques. convenient, single imputation generally underestimates variability risks biasing results.","code":""},{"path":"imputation-missing-data.html","id":"deterministic-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.1 Deterministic Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"mean-median-mode-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.1.1 Mean, Median, Mode Imputation","text":"method replaces missing values mean, median, mode observed data.Advantages:Simplicity ease implementation.Useful quick exploratory data analysis.Disadvantages:Bias Variances Relationships: Mean imputation reduces variance disrupts relationships among variables, leading biased estimates variances covariances (Haitovsky 1968).Underestimated Standard Errors: Results overly optimistic conclusions increased risk Type errors.Dependency Structure Ignored: Particularly problematic high-dimensional data, fails capture dependencies among features.","code":""},{"path":"imputation-missing-data.html","id":"forward-and-backward-filling-time-series-contexts","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.1.2 Forward and Backward Filling (Time Series Contexts)","text":"Used time series analysis, method replaces missing values using preceding (forward filling) succeeding (backward filling) values.Advantages:Simple preserves temporal ordering.Suitable datasets adjacent values strongly correlated.Disadvantages:Biased missingness spans long gaps occurs systematically.capture trends changes underlying process.","code":""},{"path":"imputation-missing-data.html","id":"statistical-prediction-models","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.2 Statistical Prediction Models","text":"","code":""},{"path":"imputation-missing-data.html","id":"linear-regression-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.2.1 Linear Regression Imputation","text":"Missing values variable imputed based linear regression model using observed values variables.Advantages:Preserves relationships variables.sophisticated mean median imputation.Disadvantages:Assumes linear relationships, may hold datasets.Fails capture variability, leading downwardly biased standard errors.","code":""},{"path":"imputation-missing-data.html","id":"logistic-regression-for-categorical-variables","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.2.2 Logistic Regression for Categorical Variables","text":"Similar linear regression imputation used categorical variables. missing category predicted using logistic regression model.Advantages:Useful binary multinomial categorical data.Preserves relationships variables.Disadvantages:Assumes underlying logistic model appropriate.account uncertainty imputed values.","code":""},{"path":"imputation-missing-data.html","id":"non-parametric-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.3 Non-Parametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"hot-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.3.1 Hot Deck Imputation","text":"Hot Deck Imputation method handling missing data missing values replaced observed values “donor” cases similar characteristics. technique widely used survey data, including organizations like U.S. Census Bureau, due flexibility ability maintain observed data distributions.Advantages Hot Deck ImputationRetains observed data distributions: Since missing values imputed using actual observed data, overall distribution remains realistic.Flexible: method applicable categorical continuous variables.Constrained imputations: Imputed values always feasible, come observed cases.Adds variability: randomly selecting donors, method introduces variability, can aid robust standard error estimation.Disadvantages Hot Deck ImputationSensitivity similarity definitions: quality imputed values depends criteria used define similarity cases.Computational intensity: Identifying similar cases randomly selecting donors can computationally expensive, especially large datasets.Subjectivity: Deciding define “similar” can introduce subjectivity bias.Algorithm Hot Deck ImputationLet \\(n_1\\) represent number cases complete data variable \\(Y\\), \\(n_0\\) represent number cases missing data \\(Y\\). steps follows:\\(n_1\\) cases complete data, take random sample (replacement) \\(n_1\\) cases.sampled pool, take another random sample (replacement) size \\(n_0\\).Assign values sampled \\(n_0\\) cases cases missing data \\(Y\\).Repeat process every variable dataset.multiple imputation, repeat four steps multiple times create multiple imputed datasets.Variations ConsiderationsSkipping Step 1: Step 1 skipped, variability imputed values reduced. approach might fully account uncertainty missing data, can underestimate standard errors.Defining similarity: major challenge method deciding constitutes “similarity” cases. Common approaches include matching based distance metrics (e.g., Euclidean distance) grouping cases strata clusters.Practical ExampleThe U.S. Census Bureau employs approximate Bayesian bootstrap variation Hot Deck Imputation. approach:Similar cases identified based shared characteristics grouping variables.Similar cases identified based shared characteristics grouping variables.randomly chosen value similar individual sample used replace missing value.randomly chosen value similar individual sample used replace missing value.method ensures imputed values plausible incorporating variability.Key NotesGood aspects:\nImputed values constrained observed possibilities.\nRandom selection introduces variability, helpful multiple imputation scenarios.\nImputed values constrained observed possibilities.Random selection introduces variability, helpful multiple imputation scenarios.Challenges:\nDefining operationalizing “similarity” remains critical step applying method effectively.\nDefining operationalizing “similarity” remains critical step applying method effectively.example code snippet illustrating Hot Deck Imputation R:code randomly imputes missing values Age column based observed data using Hmisc package’s impute function.","code":"\nlibrary(Hmisc)\n\n# Example dataset with missing values\ndata <- data.frame(\n  ID = 1:10,\n  Age = c(25, 30, NA, 40, NA, 50, 60, NA, 70, 80),\n  Gender = c(\"M\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\")\n)\n\n# Perform Hot Deck Imputation using Hmisc::impute\ndata$Age_imputed <- impute(data$Age, \"random\")\n\n# Display the imputed dataset\nprint(data)\n#>    ID Age Gender Age_imputed\n#> 1   1  25      M          25\n#> 2   2  30      F          30\n#> 3   3  NA      F          25\n#> 4   4  40      M          40\n#> 5   5  NA      M          70\n#> 6   6  50      F          50\n#> 7   7  60      M          60\n#> 8   8  NA      F          25\n#> 9   9  70      M          70\n#> 10 10  80      F          80"},{"path":"imputation-missing-data.html","id":"cold-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.3.2 Cold Deck Imputation","text":"Cold Deck Imputation systematic variant Hot Deck Imputation donor pool predefined. Instead selecting donors dynamically within dataset, Cold Deck Imputation relies external reference dataset, historical data high-quality external sources.Advantages Cold Deck ImputationUtilizes high-quality external data: method particularly useful reliable external reference datasets available, allowing accurate consistent imputations.Consistency: donor pool used across multiple datasets, imputations remain consistent, can advantageous longitudinal studies standardized processes.Disadvantages Cold Deck ImputationLack adaptability: External data may adequately reflect unique characteristics variability current dataset.Potential systematic bias: donor pool significantly different target dataset, imputations may introduce bias.Reduces variability: Unlike Hot Deck Imputation, Cold Deck Imputation systematically selects values, removes random variation. can affect estimation standard errors inferential statistics.Key CharacteristicsSystematic Selection: Cold Deck Imputation selects donor values systematically based predefined rules matching criteria, rather using random sampling.External Donor Pool: Donors typically drawn separate dataset historical records.Algorithm Cold Deck ImputationIdentify external reference dataset predefined donor pool.Define matching criteria find “similar” cases donor pool current dataset (e.g., based covariates stratification).Systematically assign values donor pool missing values current dataset based matching criteria.Repeat process variable missing data.Practical ConsiderationsCold Deck Imputation works well external data closely resemble target dataset. However, significant differences distributions relationships variables, imputations may biased unrealistic.method less useful datasets without access reliable external reference data.Suppose current dataset missing values historical dataset similar variables. following example demonstrates Cold Deck Imputation can implemented:Comparison Hot Deck ImputationThis method suits situations external reference datasets trusted representative. However, careful consideration required ensure alignment donor pool target dataset avoid systematic biases.","code":"\n# Current dataset with missing values\ncurrent_data <- data.frame(\n  ID = 1:5,\n  Age = c(25, 30, NA, 45, NA),\n  Gender = c(\"M\", \"F\", \"F\", \"M\", \"M\")\n)\n\n# External reference dataset (donor pool)\nreference_data <- data.frame(\n  Age = c(28, 35, 42, 50),\n  Gender = c(\"M\", \"F\", \"F\", \"M\")\n)\n\n# Perform Cold Deck Imputation\nlibrary(dplyr)\n\n# Define a matching function to find closest donor\nimpute_cold_deck <- function(missing_row, reference_data) {\n  # Filter donors with the same gender\n  possible_donors <- reference_data %>%\n    filter(Gender == missing_row$Gender)\n  \n  # Return the mean age of matching donors as an example of systematic imputation\n  return(mean(possible_donors$Age, na.rm = TRUE))\n}\n\n# Apply Cold Deck Imputation to the missing rows\ncurrent_data <- current_data %>%\n  rowwise() %>%\n  mutate(\n    Age_imputed = ifelse(\n      is.na(Age),\n      impute_cold_deck(cur_data(), reference_data),\n      Age\n    )\n  )\n\n# Display the imputed dataset\nprint(current_data)\n#> # A tibble: 5 × 4\n#> # Rowwise: \n#>      ID   Age Gender Age_imputed\n#>   <int> <dbl> <chr>        <dbl>\n#> 1     1    25 M             25  \n#> 2     2    30 F             30  \n#> 3     3    NA F             38.8\n#> 4     4    45 M             45  \n#> 5     5    NA M             38.8"},{"path":"imputation-missing-data.html","id":"random-draw-from-observed-distribution","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.3.3 Random Draw from Observed Distribution","text":"imputation method replaces missing values randomly sampling observed distribution variable missing data. simple, non-parametric approach retains variability original data.AdvantagesPreserves variability:\nrandomly drawing values observed data, method ensures imputed values reflect inherent variability variable.\nrandomly drawing values observed data, method ensures imputed values reflect inherent variability variable.Computational simplicity:\nprocess straightforward require model fitting complex calculations.\nprocess straightforward require model fitting complex calculations.DisadvantagesIgnores relationships among variables:\nSince imputation based solely observed distribution variable, consider relationships dependencies variables.\nSince imputation based solely observed distribution variable, consider relationships dependencies variables.May align trends:\nImputed values random may fail align patterns trends present data, time series structures interactions.\nImputed values random may fail align patterns trends present data, time series structures interactions.Steps Random Draw ImputationIdentify observed (non-missing) values variable.missing value, randomly sample one value observed distribution without replacement.Replace missing value randomly sampled value.following example demonstrates use random draw imputation fill missing values:ConsiderationsWhen Use:\nmethod suitable exploratory analysis quick way handle missing data univariate contexts.\nUse:method suitable exploratory analysis quick way handle missing data univariate contexts.Limitations:\nRandom draws may result values fit well broader context dataset, especially cases variable strong relationships others.\nLimitations:Random draws may result values fit well broader context dataset, especially cases variable strong relationships others.method quick computationally efficient way address missing data best complemented sophisticated methods relationships variables important.","code":"\n# Example dataset with missing values\nset.seed(123)\ndata <- data.frame(\n  ID = 1:10,\n  Value = c(10, 20, NA, 30, 40, NA, 50, 60, NA, 70)\n)\n\n# Perform random draw imputation\nrandom_draw_impute <- function(data, variable) {\n  observed_values <- data[[variable]][!is.na(data[[variable]])] # Observed values\n  data[[variable]][is.na(data[[variable]])] <- sample(observed_values, \n                                                      sum(is.na(data[[variable]])), \n                                                      replace = TRUE)\n  return(data)\n}\n\n# Apply the imputation\nimputed_data <- random_draw_impute(data, variable = \"Value\")\n\n# Display the imputed dataset\nprint(imputed_data)\n#>    ID Value\n#> 1   1    10\n#> 2   2    20\n#> 3   3    70\n#> 4   4    30\n#> 5   5    40\n#> 6   6    70\n#> 7   7    50\n#> 8   8    60\n#> 9   9    30\n#> 10 10    70"},{"path":"imputation-missing-data.html","id":"semi-parametric-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.4 Semi-Parametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"predictive-mean-matching-pmm","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.4.1 Predictive Mean Matching (PMM)","text":"Predictive Mean Matching (PMM) imputes missing values finding observed values closest predicted value (based regression model) missing data. donor values used fill gaps.Advantages:Maintains observed variability data.Maintains observed variability data.Ensures imputed values realistic since drawn observed data.Ensures imputed values realistic since drawn observed data.Disadvantages:Requires suitable predictive model.Requires suitable predictive model.Computationally intensive large datasets.Computationally intensive large datasets.Steps PMM:Regress \\(Y\\) \\(X\\) (matrix covariates) \\(n_1\\) (non-missing cases) estimate coefficients \\(\\hat{b}\\) residual variance \\(s^2\\).Draw posterior predictive distribution residual variance: \\[s^2_{[1]} = \\frac{(n_1-k)s^2}{\\chi^2},\\] \\(\\chi^2\\) random draw \\(\\chi^2_{n_1-k}\\).Randomly sample posterior distribution \\(\\hat{b}\\): \\[b_{[1]} \\sim MVN(\\hat{b}, s^2_{[1]}(X'X)^{-1}).\\]Standardize residuals \\(n_1\\) cases: \\[e_i = \\frac{y_i - \\hat{b}x_i}{\\sqrt{s^2(1-k/n_1)}}.\\]Randomly draw sample (replacement) \\(n_0\\) residuals Step 4.Calculate imputed values \\(n_0\\) missing cases: \\[y_i = b_{[1]}x_i + s_{[1]}e_i.\\]Repeat Steps 2–6 (except Step 4) create multiple imputations.Notes:PMM can handle heteroskedasticityPMM can handle heteroskedasticityworks multiple variables, imputing using others predictors.works multiple variables, imputing using others predictors.Example:Example Statistics GlobeExample UCLA Statistical Consultingrr = number observations pairs values observedrm = number observations variables missing valuesmr = number observations first variable’s value (e.g. row variable) observed second (column) variable missingmm = number observations second variable’s value (e.g. col variable) observed first (row) variable missing","code":"\nset.seed(1) # Seed\nN  <- 100                                    # Sample size\ny  <- round(runif(N,-10, 10))                 # Target variable Y\nx1 <- y + round(runif(N, 0, 50))              # Auxiliary variable 1\nx2 <- round(y + 0.25 * x1 + rnorm(N,-3, 15))  # Auxiliary variable 2\nx3 <- round(0.1 * x1 + rpois(N, 2))           # Auxiliary variable 3\n# (categorical variable)\nx4 <- as.factor(round(0.02 * y + runif(N)))   # Auxiliary variable 4 \n\n# Insert 20% missing data in Y\ny[rbinom(N, 1, 0.2) == 1] <- NA               \n\ndata <- data.frame(y, x1, x2, x3, x4)         # Store data in dataset\nhead(data) # First 6 rows of our data\n#>    y x1  x2 x3 x4\n#> 1 NA 28 -10  5  0\n#> 2 NA 15  -2  2  1\n#> 3  1 15 -12  6  1\n#> 4  8 58  22 10  1\n#> 5 NA 26 -12  7  0\n#> 6 NA 19  36  5  1\n\nlibrary(\"mice\") # Load mice package\n\n##### Impute data via predictive mean matching (single imputation)#####\n\nimp_single <- mice(data, m = 1, method = \"pmm\") # Impute missing values\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_imp_single <- complete(imp_single)         # Store imputed data\n# head(data_imp_single)\n\n# Since single imputation underestiamtes stnadard errors, \n# we use multiple imputaiton\n\n##### Predictive mean matching (multiple imputation) #####\n\n# Impute missing values multiple times\nimp_multi <- mice(data, m = 5, method = \"pmm\")  \n#> \n#>  iter imp variable\n#>   1   1  y\n#>   1   2  y\n#>   1   3  y\n#>   1   4  y\n#>   1   5  y\n#>   2   1  y\n#>   2   2  y\n#>   2   3  y\n#>   2   4  y\n#>   2   5  y\n#>   3   1  y\n#>   3   2  y\n#>   3   3  y\n#>   3   4  y\n#>   3   5  y\n#>   4   1  y\n#>   4   2  y\n#>   4   3  y\n#>   4   4  y\n#>   4   5  y\n#>   5   1  y\n#>   5   2  y\n#>   5   3  y\n#>   5   4  y\n#>   5   5  y\ndata_imp_multi_all <-\n    # Store multiply imputed data\n    complete(imp_multi,       \n             \"repeated\",\n             include = TRUE)\n\ndata_imp_multi <-\n    # Combine imputed Y and X1-X4 (for convenience)\n    data.frame(data_imp_multi_all[, 1:6], data[, 2:5])\n\nhead(data_imp_multi)\n#>   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4\n#> 1  NA  -1   6  -1  -3   3 28 -10  5  0\n#> 2  NA -10  10   4   0   2 15  -2  2  1\n#> 3   1   1   1   1   1   1 15 -12  6  1\n#> 4   8   8   8   8   8   8 58  22 10  1\n#> 5  NA   0  -1  -6   2   0 26 -12  7  0\n#> 6  NA   4   0   3   3   3 19  36  5  1\nlibrary(mice)\nlibrary(VIM)\nlibrary(lattice)\nlibrary(ggplot2)\n## set observations to NA\nanscombe <- within(anscombe, {\n    y1[1:3] <- NA\n    y4[3:5] <- NA\n})\n## view\nhead(anscombe)\n#>   x1 x2 x3 x4   y1   y2    y3   y4\n#> 1 10 10 10  8   NA 9.14  7.46 6.58\n#> 2  8  8  8  8   NA 8.14  6.77 5.76\n#> 3 13 13 13  8   NA 8.74 12.74   NA\n#> 4  9  9  9  8 8.81 8.77  7.11   NA\n#> 5 11 11 11  8 8.33 9.26  7.81   NA\n#> 6 14 14 14  8 9.96 8.10  8.84 7.04\n\n## check missing data patterns\nmd.pattern(anscombe)#>   x1 x2 x3 x4 y2 y3 y1 y4  \n#> 6  1  1  1  1  1  1  1  1 0\n#> 2  1  1  1  1  1  1  1  0 1\n#> 2  1  1  1  1  1  1  0  1 1\n#> 1  1  1  1  1  1  1  0  0 2\n#>    0  0  0  0  0  0  3  3 6\n\n## Number of observations per patterns for all pairs of variables\np <- md.pairs(anscombe)\np \n#> $rr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1 11 11 11 11  8 11 11  8\n#> x2 11 11 11 11  8 11 11  8\n#> x3 11 11 11 11  8 11 11  8\n#> x4 11 11 11 11  8 11 11  8\n#> y1  8  8  8  8  8  8  8  6\n#> y2 11 11 11 11  8 11 11  8\n#> y3 11 11 11 11  8 11 11  8\n#> y4  8  8  8  8  6  8  8  8\n#> \n#> $rm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  3  0  0  3\n#> x2  0  0  0  0  3  0  0  3\n#> x3  0  0  0  0  3  0  0  3\n#> x4  0  0  0  0  3  0  0  3\n#> y1  0  0  0  0  0  0  0  2\n#> y2  0  0  0  0  3  0  0  3\n#> y3  0  0  0  0  3  0  0  3\n#> y4  0  0  0  0  2  0  0  0\n#> \n#> $mr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  3  3  3  3  0  3  3  2\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  3  3  3  3  2  3  3  0\n#> \n#> $mm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  0  0  0  0  3  0  0  1\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  0  0  0  0  1  0  0  3\n## Margin plot of y1 and y4\nmarginplot(anscombe[c(5, 8)], col = c(\"blue\", \"red\", \"orange\"))\n\n## 5 imputations for all missing values\nimp1 <- mice(anscombe, m = 5)\n#> \n#>  iter imp variable\n#>   1   1  y1  y4\n#>   1   2  y1  y4\n#>   1   3  y1  y4\n#>   1   4  y1  y4\n#>   1   5  y1  y4\n#>   2   1  y1  y4\n#>   2   2  y1  y4\n#>   2   3  y1  y4\n#>   2   4  y1  y4\n#>   2   5  y1  y4\n#>   3   1  y1  y4\n#>   3   2  y1  y4\n#>   3   3  y1  y4\n#>   3   4  y1  y4\n#>   3   5  y1  y4\n#>   4   1  y1  y4\n#>   4   2  y1  y4\n#>   4   3  y1  y4\n#>   4   4  y1  y4\n#>   4   5  y1  y4\n#>   5   1  y1  y4\n#>   5   2  y1  y4\n#>   5   3  y1  y4\n#>   5   4  y1  y4\n#>   5   5  y1  y4\n\n## linear regression for each imputed data set - 5 regression are run\nfitm <- with(imp1, lm(y1 ~ y4 + x1))\nsummary(fitm)\n#> # A tibble: 15 × 6\n#>    term        estimate std.error statistic p.value  nobs\n#>    <chr>          <dbl>     <dbl>     <dbl>   <dbl> <int>\n#>  1 (Intercept)    7.33      2.44       3.01  0.0169    11\n#>  2 y4            -0.416     0.223     -1.86  0.0996    11\n#>  3 x1             0.371     0.141      2.63  0.0302    11\n#>  4 (Intercept)    7.27      2.90       2.51  0.0365    11\n#>  5 y4            -0.435     0.273     -1.59  0.150     11\n#>  6 x1             0.387     0.160      2.41  0.0422    11\n#>  7 (Intercept)    6.54      2.80       2.33  0.0479    11\n#>  8 y4            -0.322     0.255     -1.26  0.243     11\n#>  9 x1             0.362     0.156      2.32  0.0491    11\n#> 10 (Intercept)    5.93      3.08       1.92  0.0907    11\n#> 11 y4            -0.286     0.282     -1.02  0.339     11\n#> 12 x1             0.418     0.176      2.37  0.0451    11\n#> 13 (Intercept)    8.16      2.67       3.05  0.0158    11\n#> 14 y4            -0.489     0.251     -1.95  0.0867    11\n#> 15 x1             0.326     0.151      2.17  0.0622    11\n\n## pool coefficients and standard errors across all 5 regression models\npool(fitm)\n#> Class: mipo    m = 5 \n#>          term m   estimate       ubar           b          t dfcom       df\n#> 1 (Intercept) 5  7.0445966 7.76794670 0.719350800 8.63116766     8 5.805314\n#> 2          y4 5 -0.3896685 0.06634920 0.006991497 0.07473900     8 5.706243\n#> 3          x1 5  0.3727865 0.02473847 0.001134293 0.02609962     8 6.178032\n#>          riv     lambda       fmi\n#> 1 0.11112601 0.10001207 0.3044313\n#> 2 0.12644909 0.11225460 0.3161877\n#> 3 0.05502168 0.05215218 0.2586992\n\n## output parameter estimates\nsummary(pool(fitm))\n#>          term   estimate std.error statistic       df    p.value\n#> 1 (Intercept)  7.0445966 2.9378849  2.397846 5.805314 0.05483678\n#> 2          y4 -0.3896685 0.2733843 -1.425350 5.706243 0.20638512\n#> 3          x1  0.3727865 0.1615538  2.307508 6.178032 0.05923999"},{"path":"imputation-missing-data.html","id":"stochastic-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.4.2 Stochastic Imputation","text":"Stochastic Imputation enhancement regression imputation introduces randomness imputation process adding random residual predicted values regression model. approach aims retain variability original data reducing bias introduced deterministic regression imputation.Stochastic Imputation can described :\\[\n\\text{Imputed Value} = \\text{Predicted Value (regression)} + \\text{Random Residual}\n\\]method commonly used foundation multiple imputation techniques.Advantages Stochastic ImputationRetains benefits regression imputation:\nPreserves relationships variables dataset.\nUtilizes information observed data inform imputations.\nPreserves relationships variables dataset.Utilizes information observed data inform imputations.Introduces randomness:\nAdds variability including random residual term, making imputed values realistic better representing uncertainty missing data.\nAdds variability including random residual term, making imputed values realistic better representing uncertainty missing data.Supports multiple imputation:\ngenerating different random residuals iteration, facilitates creation multiple plausible datasets robust statistical analysis.\ngenerating different random residuals iteration, facilitates creation multiple plausible datasets robust statistical analysis.Disadvantages Stochastic ImputationImplausible values:\nDepending random residuals, imputed values may fall outside plausible range (e.g., negative values variables like age income).\nDepending random residuals, imputed values may fall outside plausible range (e.g., negative values variables like age income).handle heteroskedasticity:\ndata exhibit heteroskedasticity (.e., non-constant variance residuals), randomness added stochastic imputation may accurately reflect underlying variability.\ndata exhibit heteroskedasticity (.e., non-constant variance residuals), randomness added stochastic imputation may accurately reflect underlying variability.Steps Stochastic ImputationFit regression model using cases complete data variable missing values.Predict missing values using fitted model.Generate random residuals based distribution residuals regression model.Add random residuals predicted values impute missing values.NotesMultiple Imputation: multiple imputation methods extensions stochastic regression imputation. repeating imputation process different random seeds, multiple datasets can generated account uncertainty imputed values.Multiple Imputation: multiple imputation methods extensions stochastic regression imputation. repeating imputation process different random seeds, multiple datasets can generated account uncertainty imputed values.Dealing Implausible Values: Additional constraints transformations (e.g., truncating imputed values plausible range) may necessary address issue implausible values.Dealing Implausible Values: Additional constraints transformations (e.g., truncating imputed values plausible range) may necessary address issue implausible values.Comparison Deterministic Regression ImputationSingle stochastic regression imputationSingle predictive mean matchingStochastic regression imputation contains negative valuesEvidence heteroskadastic dataSingle stochastic regression imputationSingle predictive mean matchingComparison predictive mean matching stochastic regression imputation","code":"\n# Example dataset with missing values\nset.seed(123)\ndata <- data.frame(\n  X = rnorm(10, mean = 50, sd = 10),\n  Y = c(100, 105, 110, NA, 120, NA, 130, 135, 140, NA)\n)\n\n# Perform stochastic imputation\nstochastic_impute <- function(data, predictor, target) {\n  # Subset data with complete cases\n  complete_data <- data[!is.na(data[[target]]), ]\n  \n  # Fit a regression model\n  model <- lm(as.formula(paste(target, \"~\", predictor)), data = complete_data)\n  \n  # Predict missing values\n  missing_data <- data[is.na(data[[target]]), ]\n  predictions <- predict(model, newdata = missing_data)\n  \n  # Add random residuals\n  residual_sd <- sd(model$residuals, na.rm = TRUE)\n  stochastic_values <- predictions + rnorm(length(predictions), mean = 0, sd = residual_sd)\n  \n  # Impute missing values\n  data[is.na(data[[target]]), target] <- stochastic_values\n  return(data)\n}\n\n# Apply stochastic imputation\nimputed_data <- stochastic_impute(data, predictor = \"X\", target = \"Y\")\n\n# Display the imputed dataset\nprint(imputed_data)\n# Income data\nset.seed(1)                              # Set seed\nN <- 1000                                    # Sample size\n\nincome <-\n  round(rnorm(N, 0, 500))            # Create some synthetic income data\nincome[income < 0] <- income[income < 0] * (-1)\n\nx1 <- income + rnorm(N, 1000, 1500)          # Auxiliary variables\nx2 <- income + rnorm(N,-5000, 2000)\n\n\n# Create 10% missingness in income\nincome[rbinom(N, 1, 0.1) == 1] <- NA\n\ndata_inc_miss <- data.frame(income, x1, x2)\nimp_inc_sri  <- mice(data_inc_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_sri <- complete(imp_inc_sri)\nimp_inc_pmm  <- mice(data_inc_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_pmm <- complete(imp_inc_pmm)\ndata_inc_sri$income[data_inc_sri$income < 0]\n#>  [1]  -23.85404  -58.37790  -61.86396  -57.47909  -21.29221  -73.26549\n#>  [7]  -61.76194  -42.45942 -351.02991 -317.69090\n# No values below 0\ndata_inc_pmm$income[data_inc_pmm$income < 0] \n#> numeric(0)\n# Heteroscedastic data\n \nset.seed(1)                             # Set seed\nN <- 1:1000                                  # Sample size\n \na <- 0\nb <- 1\nsigma2 <- N^2\neps <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n \ny <- a + b * N + eps                         # Heteroscedastic variable\nx <- 30 * N + rnorm(N[length(N)], 1000, 200) # Correlated variable\n \ny[rbinom(N[length(N)], 1, 0.3) == 1] <- NA   # 30% missing\n \ndata_het_miss <- data.frame(y, x)\nimp_het_sri  <- mice(data_het_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_sri <- complete(imp_het_sri)\nimp_het_pmm  <- mice(data_het_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_pmm <- complete(imp_het_pmm)\npar(mfrow = c(1, 2))                              # Both plots in one graphic\n\n# Plot of observed values\nplot(x[!is.na(data_het_sri$y)],\n     data_het_sri$y[!is.na(data_het_sri$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n# Plot of missing values\npoints(x[is.na(y)], data_het_sri$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Stochastic Regression Imputation\",        \n      line = 0.5)\n\n# Regression line\nabline(lm(y ~ x, data_het_sri),                   \n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\n# Plot of observed values\nplot(x[!is.na(data_het_pmm$y)],\n     data_het_pmm$y[!is.na(data_het_pmm$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n\n\n# Plot of missing values\npoints(x[is.na(y)], data_het_pmm$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Predictive Mean Matching\",\n      line = 0.5)\nabline(lm(y ~ x, data_het_pmm),\n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\nmtext(\n  \"Imputation of Heteroscedastic Data\",\n  # Main title of plot\n  side = 3,\n  line = -1.5,\n  outer = TRUE,\n  cex = 2\n)"},{"path":"imputation-missing-data.html","id":"matrix-completion","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.5 Matrix Completion","text":"Matrix completion method used impute missing data feature matrix accounting dependence features. approach leverages principal components approximate data matrix, process referred matrix completion (James et al. 2013, Sec 12.3).Problem SetupConsider \\(n \\times p\\) feature matrix \\(\\mathbf{X}\\), element \\(x_{ij}\\) represents value \\(\\)th observation \\(j\\)th feature. elements \\(\\mathbf{X}\\) missing, aim impute missing values.Similar process described 22.2, matrix \\(\\mathbf{X}\\) can approximated using leading principal components. Specifically, consider \\(M\\) principal components minimize following objective:\\[\n\\underset{\\mathbf{} \\\\mathbb{R}^{n \\times M}, \\mathbf{B} \\\\mathbb{R}^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\]\\(\\mathcal{O}\\) set observed indices \\((,j)\\), subset total \\(n \\times p\\) pairs. : - \\(\\mathbf{}\\) \\(n \\times M\\) matrix principal component scores. - \\(\\mathbf{B}\\) \\(p \\times M\\) matrix principal component loadings.Imputation Missing ValuesAfter solving minimization problem:Missing observations \\(x_{ij}\\) can imputed using formula: \\[\n   \\hat{x}_{ij} = \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n   \\] \\(\\hat{}_{im}\\) \\(\\hat{b}_{jm}\\) estimated elements \\(\\mathbf{}\\) \\(\\mathbf{B}\\), respectively.leading \\(M\\) principal component scores loadings can approximately recovered, done complete data scenarios.Iterative AlgorithmThe eigen-decomposition used standard principal component analysis applicable missing values. Instead, iterative algorithm, described (James et al. 2013, Alg 12.1), employed:Initialize Complete Matrix: Construct initial complete matrix \\(\\tilde{\\mathbf{X}}\\) dimension \\(n \\times p\\) : \\[\n\\tilde{x}_{ij} =\n\\begin{cases}\nx_{ij} & \\text{} (,j) \\\\mathcal{O} \\\\\n\\bar{x}_j & \\text{} (,j) \\notin \\mathcal{O}\n\\end{cases}\n\\] , \\(\\bar{x}_j\\) mean observed values \\(j\\)th variable incomplete data matrix \\(\\mathbf{X}\\). \\(\\mathcal{O}\\) indexes observed elements \\(\\mathbf{X}\\).Initialize Complete Matrix: Construct initial complete matrix \\(\\tilde{\\mathbf{X}}\\) dimension \\(n \\times p\\) : \\[\n\\tilde{x}_{ij} =\n\\begin{cases}\nx_{ij} & \\text{} (,j) \\\\mathcal{O} \\\\\n\\bar{x}_j & \\text{} (,j) \\notin \\mathcal{O}\n\\end{cases}\n\\] , \\(\\bar{x}_j\\) mean observed values \\(j\\)th variable incomplete data matrix \\(\\mathbf{X}\\). \\(\\mathcal{O}\\) indexes observed elements \\(\\mathbf{X}\\).Iterative Steps: Repeat following steps convergence:\nMinimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).\nUpdate Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]\nRecalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]\nIterative Steps: Repeat following steps convergence:Minimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).Minimize Objective: Solve problem: \\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\] computing principal components current \\(\\tilde{\\mathbf{X}}\\).Update Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]Update Missing Values: missing element \\((,j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\n\\]Recalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]Recalculate Objective: Compute objective: \\[\n\\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]Return Imputed Values: algorithm converges, return estimated missing entries \\(\\tilde{x}_{ij}\\) \\((,j) \\notin \\mathcal{O}\\).Return Imputed Values: algorithm converges, return estimated missing entries \\(\\tilde{x}_{ij}\\) \\((,j) \\notin \\mathcal{O}\\).Key ConsiderationsThis approach assumes missing data missing random (MAR).Convergence criteria iterative algorithm often involve achieving threshold change objective function limiting number iterations.choice \\(M\\), number principal components, can guided cross-validation model selection techniques.","code":""},{"path":"imputation-missing-data.html","id":"comparison-of-single-imputation-techniques","chapter":"11 Imputation (Missing Data)","heading":"11.4.2.6 Comparison of Single Imputation Techniques","text":"Single imputation techniques straightforward accessible, often underestimate uncertainty fail fully leverage relationships among variables. limitations make less ideal rigorous analyses compared multiple imputation model-based approaches.","code":""},{"path":"imputation-missing-data.html","id":"machine-learning-and-modern-approaches","chapter":"11 Imputation (Missing Data)","heading":"11.4.3 Machine Learning and Modern Approaches","text":"","code":""},{"path":"imputation-missing-data.html","id":"tree-based-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.1 Tree-Based Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"random-forest-imputation-missforest","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.1.1 Random Forest Imputation (missForest)","text":"Random Forest Imputation uses iterative process random forest model predicts missing values one variable time, treating variables predictors. process continues convergence.Mathematical Framework:\nvariable \\(X_j\\) missing values, treat \\(X_j\\) response variable.\nFit random forest model \\(f(X_{-j})\\) using variables \\(X_{-j}\\) predictors.\nPredict missing values \\(\\hat{X}_j = f(X_{-j})\\).\nRepeat variables missing data imputed values stabilize.\nvariable \\(X_j\\) missing values, treat \\(X_j\\) response variable.Fit random forest model \\(f(X_{-j})\\) using variables \\(X_{-j}\\) predictors.Predict missing values \\(\\hat{X}_j = f(X_{-j})\\).Repeat variables missing data imputed values stabilize.Advantages:\nCaptures complex interactions non-linearities.\nHandles mixed data types seamlessly.\nCaptures complex interactions non-linearities.Handles mixed data types seamlessly.Limitations:\nComputationally intensive large datasets.\nSensitive quality data relationships.\nComputationally intensive large datasets.Sensitive quality data relationships.","code":""},{"path":"imputation-missing-data.html","id":"gradient-boosting-machines-gbm","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.1.2 Gradient Boosting Machines (GBM)","text":"Gradient Boosting Machines iteratively build models minimize loss functions. imputation, missing values treated target variable predicted.Mathematical Framework: GBM algorithm minimizes loss function: \\[\n  L = \\sum_{=1}^n \\ell(y_i, f(x_i)),\n  \\] \\(\\ell\\) loss function (e.g., mean squared error), \\(y_i\\) observed values, \\(f(x_i)\\) predictions.Mathematical Framework: GBM algorithm minimizes loss function: \\[\n  L = \\sum_{=1}^n \\ell(y_i, f(x_i)),\n  \\] \\(\\ell\\) loss function (e.g., mean squared error), \\(y_i\\) observed values, \\(f(x_i)\\) predictions.Missing values treated \\(y_i\\) predicted iteratively.Missing values treated \\(y_i\\) predicted iteratively.Advantages:\nHighly accurate predictions.\nCaptures variable importance.\nAdvantages:Highly accurate predictions.Captures variable importance.Limitations:\nOverfitting risks.\nRequires careful parameter tuning.\nLimitations:Overfitting risks.Requires careful parameter tuning.","code":""},{"path":"imputation-missing-data.html","id":"neural-network-based-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.2 Neural Network-Based Imputation","text":"","code":""},{"path":"imputation-missing-data.html","id":"autoencoders","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.2.1 Autoencoders","text":"Autoencoders unsupervised neural networks compress reconstruct data. Missing values estimated reconstruction.Mathematical Framework: autoencoder consists :\nencoder function: \\(h = g(Wx + b)\\), compresses input \\(x\\).\ndecoder function: \\(\\hat{x} = g'(W'h + b')\\), reconstructs data.\nMathematical Framework: autoencoder consists :encoder function: \\(h = g(Wx + b)\\), compresses input \\(x\\).decoder function: \\(\\hat{x} = g'(W'h + b')\\), reconstructs data.network minimizes reconstruction loss: \\[\n  L = \\sum_{=1}^n (x_i - \\hat{x}_i)^2.\n  \\]network minimizes reconstruction loss: \\[\n  L = \\sum_{=1}^n (x_i - \\hat{x}_i)^2.\n  \\]Advantages:\nHandles high-dimensional non-linear data.\nUnsupervised learning.\nAdvantages:Handles high-dimensional non-linear data.Unsupervised learning.Limitations:\nComputationally demanding.\nRequires large datasets effective training.\nLimitations:Computationally demanding.Requires large datasets effective training.","code":""},{"path":"imputation-missing-data.html","id":"generative-adversarial-networks-gans-for-data-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.2.2 Generative Adversarial Networks (GANs) for Data Imputation","text":"GANs consist generator discriminator. imputation, generator fills missing values, discriminator evaluates quality imputations.Mathematical Framework: GAN training involves optimizing: \\[\n  \\min_G \\max_D \\mathbb{E}[\\log D(x)] + \\mathbb{E}[\\log(1 - D(G(z)))].\n  \\]\n\\(D(x)\\): Discriminator’s probability \\(x\\) real.\n\\(G(z)\\): Generator’s output latent input \\(z\\).\n\\(D(x)\\): Discriminator’s probability \\(x\\) real.\\(G(z)\\): Generator’s output latent input \\(z\\).Advantages:\nRealistic imputations reflect underlying distributions.\nHandles complex data types.\nRealistic imputations reflect underlying distributions.Handles complex data types.Limitations:\nDifficult train tune.\nComputationally intensive.\nDifficult train tune.Computationally intensive.","code":""},{"path":"imputation-missing-data.html","id":"matrix-factorization-and-matrix-completion","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.3 Matrix Factorization and Matrix Completion","text":"","code":""},{"path":"imputation-missing-data.html","id":"singular-value-decomposition-svd","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.3.1 Singular Value Decomposition (SVD)","text":"SVD decomposes matrix \\(\\) three matrices: \\[\n= U\\Sigma V^T,\n\\] \\(U\\) \\(V\\) orthogonal matrices, \\(\\Sigma\\) contains singular values. Missing values estimated reconstructing \\(\\) using low-rank approximation: \\[\n\\hat{} = U_k \\Sigma_k V_k^T.\n\\]Advantages:\nCaptures global patterns.\nEfficient structured data.\nCaptures global patterns.Efficient structured data.Limitations:\nAssumes linear relationships.\nSensitive sparsity.\nAssumes linear relationships.Sensitive sparsity.","code":""},{"path":"imputation-missing-data.html","id":"collaborative-filtering-approaches","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.3.2 Collaborative Filtering Approaches","text":"Collaborative filtering uses similarities rows (users) columns (items) impute missing data. instance, value \\(X_{ij}\\) predicted : \\[\n\\hat{X}_{ij} = \\frac{\\sum_{k \\N()} w_{ik} X_{kj}}{\\sum_{k \\N()} w_{ik}},\n\\] \\(w_{ik}\\) represents similarity weights \\(N()\\) set neighbors.","code":""},{"path":"imputation-missing-data.html","id":"k-nearest-neighbor-knn-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.4 K-Nearest Neighbor (KNN) Imputation","text":"KNN identifies \\(k\\) nearest observations based distance metric imputes missing values using weighted average (continuous variables) mode (categorical variables).Mathematical Framework: missing value \\(x\\), imputed value : \\[\n  \\hat{x} = \\frac{\\sum_{=1}^k w_i x_i}{\\sum_{=1}^k w_i},\n  \\] \\(w_i = \\frac{1}{d(x, x_i)}\\) \\(d(x, x_i)\\) distance metric (e.g., Euclidean Manhattan).Mathematical Framework: missing value \\(x\\), imputed value : \\[\n  \\hat{x} = \\frac{\\sum_{=1}^k w_i x_i}{\\sum_{=1}^k w_i},\n  \\] \\(w_i = \\frac{1}{d(x, x_i)}\\) \\(d(x, x_i)\\) distance metric (e.g., Euclidean Manhattan).Advantages:\nSimple interpretable.\nNon-parametric.\nAdvantages:Simple interpretable.Non-parametric.Limitations:\nComputationally expensive large datasets.\nLimitations:Computationally expensive large datasets.","code":""},{"path":"imputation-missing-data.html","id":"hybrid-methods","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.5 Hybrid Methods","text":"Hybrid methods combine statistical machine learning approaches. example, mean imputation followed fine-tuning machine learning models. methods aim leverage strengths multiple techniques.","code":""},{"path":"imputation-missing-data.html","id":"summary-table","chapter":"11 Imputation (Missing Data)","heading":"11.4.3.6 Summary Table","text":"","code":""},{"path":"imputation-missing-data.html","id":"multiple-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.4 Multiple Imputation","text":"Multiple Imputation (MI) statistical technique handling missing data creating several plausible datasets imputation, analyzing dataset separately, combining results account uncertainty imputations. MI operates assumption missing data either Missing Completely Random (MCAR) Missing Random (MAR).Unlike Single Imputation Techniques, MI reflects uncertainty inherent missing data introducing variability imputed values. avoids biases introduced ad hoc methods produces reliable statistical inferences.three fundamental steps MI :Imputation: Replace missing values set plausible values create multiple “completed” datasets.Analysis: Perform desired statistical analysis imputed dataset.Combination: Combine results using rules account within- -imputation variability.","code":""},{"path":"imputation-missing-data.html","id":"why-multiple-imputation-is-important","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.1 Why Multiple Imputation is Important","text":"Imputed values estimates inherently include random error. However, estimates treated exact values subsequent analysis, software may overlook additional error. oversight results underestimated standard errors overly small p-values, leading misleading conclusions.Multiple imputation addresses issue generating multiple estimates missing value. estimates differ slightly due random component, reintroduces variation. variation helps software incorporate uncertainty imputed values, resulting :Unbiased parameter estimatesUnbiased parameter estimatesAccurate standard errorsAccurate standard errorsImproved p-valuesImproved p-valuesMultiple imputation significant breakthrough statistics approximately 20 years ago. provides solutions many missing data issues (though ) , applied correctly, leads reliable parameter estimates.proportion missing data small (e.g., 2-3%), choice imputation method less critical.","code":""},{"path":"imputation-missing-data.html","id":"goals-of-multiple-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.2 Goals of Multiple Imputation","text":"primary goals missing data technique, including multiple imputation, :Unbiased parameter estimates: Ensuring accurate regression coefficients, group means, odds ratios, etc.Unbiased parameter estimates: Ensuring accurate regression coefficients, group means, odds ratios, etc.Accurate standard errors: leads reliable p-values appropriate statistical inferences.Accurate standard errors: leads reliable p-values appropriate statistical inferences.Adequate power: detect meaningful significant parameter values.Adequate power: detect meaningful significant parameter values.","code":""},{"path":"imputation-missing-data.html","id":"overview-of-rubins-framework","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.3 Overview of Rubin’s Framework","text":"Rubin’s Framework provides theoretical foundation MI. uses Bayesian model-based approach generating imputations frequentist approach evaluating results. central goals Rubin’s framework ensure imputations:Retain statistical relationships present data.Reflect uncertainty true values missing data.Rubin’s framework, MI offers following advantages:Generalizability: Unlike Maximum Likelihood Estimation (MLE), MI can applied wide range models.Statistical Properties: data MAR MCAR, MI estimates consistent, asymptotically normal, efficient.Rubin also emphasized importance using multiple imputations, single imputations fail account variability imputed values, leading underestimated standard errors overly optimistic test statistics.","code":""},{"path":"imputation-missing-data.html","id":"multivariate-imputation-via-chained-equations-mice","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.4 Multivariate Imputation via Chained Equations (MICE)","text":"Multivariate Imputation via Chained Equations (MICE) widely used algorithm implementing MI, particularly datasets mixed variable types. steps MICE include:Initialization: Replace missing values initial guesses, mean median observed data.Iterative Imputation:\nvariable missing values, regress variables (subset relevant predictors).\nUse regression model predict missing values, adding random error term drawn residual distribution.\nvariable missing values, regress variables (subset relevant predictors).Use regression model predict missing values, adding random error term drawn residual distribution.Convergence: Repeat imputation process parameter estimates stabilize.MICE offers flexibility specifying regression models variable, accommodating continuous, categorical, binary data.","code":""},{"path":"imputation-missing-data.html","id":"bayesian-ridge-regression-for-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.5 Bayesian Ridge Regression for Imputation","text":"Bayesian ridge regression advanced imputation method incorporates prior distributions regression coefficients, making particularly useful :Predictors highly correlated.Sample sizes small.Missingness substantial.method treats regression coefficients random variables samples posterior distribution, introducing variability imputation process. Bayesian ridge regression computationally intensive simpler methods like MICE offers greater robustness.","code":""},{"path":"imputation-missing-data.html","id":"combining-results-from-mi-rubins-rules","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.6 Combining Results from MI (Rubin’s Rules)","text":"multiple datasets imputed analyzed, Rubin’s Rules used combine results. goal properly account uncertainty introduced missing data. parameter interest \\(\\theta\\):Estimate Combination: \\[\n\\bar{\\theta} = \\frac{1}{M} \\sum_{m=1}^M \\theta_m\n\\] \\(\\theta_m\\) estimate \\(m\\)th imputed dataset, \\(M\\) number imputations.Estimate Combination: \\[\n\\bar{\\theta} = \\frac{1}{M} \\sum_{m=1}^M \\theta_m\n\\] \\(\\theta_m\\) estimate \\(m\\)th imputed dataset, \\(M\\) number imputations.Variance Combination: \\[\nT = \\bar{W} + \\left(1 + \\frac{1}{M}\\right) B\n\\] :\n\\(\\bar{W}\\) average within-imputation variance.\n\\(B\\) -imputation variance: \\[\nB = \\frac{1}{M-1} \\sum_{m=1}^M (\\theta_m - \\bar{\\theta})^2\n\\]\nVariance Combination: \\[\nT = \\bar{W} + \\left(1 + \\frac{1}{M}\\right) B\n\\] :\\(\\bar{W}\\) average within-imputation variance.\\(B\\) -imputation variance: \\[\nB = \\frac{1}{M-1} \\sum_{m=1}^M (\\theta_m - \\bar{\\theta})^2\n\\]formulas adjust final variance reflect uncertainty within across imputations.","code":""},{"path":"imputation-missing-data.html","id":"challenges","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.6.1 Challenges","text":"Stochastic Variability: MI results vary slightly runs due reliance random draws. ensure reproducibility, always set random seed.Convergence: Iterative algorithms like MICE may struggle converge, especially high proportions missing data.Assumption MAR: MI assumes missing data MAR. data Missing Random (MNAR), MI can produce biased results.","code":""},{"path":"imputation-missing-data.html","id":"best-practices","chapter":"11 Imputation (Missing Data)","heading":"11.4.4.6.2 Best Practices","text":"Algorithm Selection:\nUse Multiple Imputation Chained Equations (MICE) datasets mixed data types relationships variables complex.\nApply Bayesian Ridge Regression small datasets predictors highly correlated.\nUse Multiple Imputation Chained Equations (MICE) datasets mixed data types relationships variables complex.Apply Bayesian Ridge Regression small datasets predictors highly correlated.Diagnostic Checks:\nEvaluate quality imputations assess convergence using trace plots diagnostic statistics ensure reliable results.\nEvaluate quality imputations assess convergence using trace plots diagnostic statistics ensure reliable results.Data Transformations:\nskewed proportion data, consider applying log logit transformations imputation inverse-transforming afterward preserve data’s original scale.\nskewed proportion data, consider applying log logit transformations imputation inverse-transforming afterward preserve data’s original scale.Handling Non-Linear Relationships:\nnon-linear relationships interactions, stratify imputations levels categorical variable involved ensure accurate estimates.\nnon-linear relationships interactions, stratify imputations levels categorical variable involved ensure accurate estimates.Number Imputations:\nUse least 20 imputations small datasets datasets high missingness. ensures robust reliable results downstream analyses.\nUse least 20 imputations small datasets datasets high missingness. ensures robust reliable results downstream analyses.Avoid Rounding Imputations Dummy Variables:\nMany imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even dummy variables. historically recommended round imputed values 0 1 binary variables, research shows introduces bias parameter estimates. Instead, leave imputed values fractional, even though may seem counter-intuitive.\nMany imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even dummy variables. historically recommended round imputed values 0 1 binary variables, research shows introduces bias parameter estimates. Instead, leave imputed values fractional, even though may seem counter-intuitive.Transform Skewed Variables Imputation:\nTransforming variables meet normality assumptions imputation can distort relationships variables, leading biased imputations possibly introducing outliers. better directly impute skewed variable.\nTransforming variables meet normality assumptions imputation can distort relationships variables, leading biased imputations possibly introducing outliers. better directly impute skewed variable.Use Imputations:\nTraditional advice suggests 5–10 imputations sufficient unbiased estimates, inconsistencies may arise repeated analyses. [@Bodner_2008] suggests using number imputations equal percentage missing data. additional imputations generally significantly increase computational workload, using imputations prudent choice.\nTraditional advice suggests 5–10 imputations sufficient unbiased estimates, inconsistencies may arise repeated analyses. [@Bodner_2008] suggests using number imputations equal percentage missing data. additional imputations generally significantly increase computational workload, using imputations prudent choice.Create Multiplicative Terms Imputation:\nmodel includes interaction quadratic terms, generate terms imputing missing values. Imputing first generating terms can introduce bias regression parameters, highlighted [@von_Hippel_2009].\nmodel includes interaction quadratic terms, generate terms imputing missing values. Imputing first generating terms can introduce bias regression parameters, highlighted [@von_Hippel_2009].","code":""},{"path":"imputation-missing-data.html","id":"evaluation-of-imputation-methods","chapter":"11 Imputation (Missing Data)","heading":"11.5 Evaluation of Imputation Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"statistical-metrics-for-assessing-imputation-quality","chapter":"11 Imputation (Missing Data)","heading":"11.5.1 Statistical Metrics for Assessing Imputation Quality","text":"evaluate quality imputed data, several statistical metrics commonly used. metrics compare imputed values observed values (cases missingness simulated artificially introduced) assess overall impact imputation quality subsequent analyses. Key metrics include:Root Mean Squared Error (RMSE): RMSE calculated : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2}\n\\] measures average magnitude errors true imputed values. Lower RMSE indicates better imputation accuracy.Root Mean Squared Error (RMSE): RMSE calculated : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2}\n\\] measures average magnitude errors true imputed values. Lower RMSE indicates better imputation accuracy.Mean Absolute Error (MAE): MAE measures average absolute difference observed imputed values: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\] MAE provides straightforward assessment imputation performance less sensitive outliers RMSE.Mean Absolute Error (MAE): MAE measures average absolute difference observed imputed values: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\] MAE provides straightforward assessment imputation performance less sensitive outliers RMSE.Log-Likelihood Deviance Measures: Log-likelihood can used evaluate well imputation model fits data. Deviance measures, based likelihood comparisons, assess relative goodness fit imputation models. particularly useful evaluating methods like maximum likelihood estimation.Log-Likelihood Deviance Measures: Log-likelihood can used evaluate well imputation model fits data. Deviance measures, based likelihood comparisons, assess relative goodness fit imputation models. particularly useful evaluating methods like maximum likelihood estimation.practice, metrics may combined graphical methods density plots residual analysis understand imputation performance thoroughly.","code":""},{"path":"imputation-missing-data.html","id":"bias-variance-tradeoff-in-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.5.2 Bias-Variance Tradeoff in Imputation","text":"Imputation methods must balance bias variance achieve reliable results. Simpler methods, mean mode imputation, often lead biased parameter estimates, particularly missingness mechanism non-random. methods underestimate variability, shrinking standard errors potentially leading overconfidence statistical inferences.Conversely, advanced methods like Multiple Imputation Full Information Maximum Likelihood (FIML) typically yield unbiased estimates appropriately calibrated variances. However, methods may increase computational complexity require careful tuning assumptions parameters.tradeoff summarized follows:High Bias, Low Variance: Simpler methods (e.g., single imputation, mean imputation).High Bias, Low Variance: Simpler methods (e.g., single imputation, mean imputation).Low Bias, Moderate Variance: Advanced methods (e.g., MI, FIML, Bayesian methods).Low Bias, Moderate Variance: Advanced methods (e.g., MI, FIML, Bayesian methods).","code":""},{"path":"imputation-missing-data.html","id":"sensitivity-analysis-1","chapter":"11 Imputation (Missing Data)","heading":"11.5.3 Sensitivity Analysis","text":"Sensitivity analysis crucial assess robustness imputation methods varying assumptions. Two primary areas focus include:Assessing Robustness Assumptions: Imputation models often rely assumptions missingness mechanism (See Definition Classification Missing Data). Sensitivity analysis involves testing results vary assumptions slightly relaxed modified.Assessing Robustness Assumptions: Imputation models often rely assumptions missingness mechanism (See Definition Classification Missing Data). Sensitivity analysis involves testing results vary assumptions slightly relaxed modified.Impact Downstream Analysis: quality imputation also evaluated based influence downstream analyses (Objectives Imputation). instance:\nimputation affect causal inference regression models?\nconclusions hypothesis testing predictive modeling robust imputation technique?\nImpact Downstream Analysis: quality imputation also evaluated based influence downstream analyses (Objectives Imputation). instance:imputation affect causal inference regression models?conclusions hypothesis testing predictive modeling robust imputation technique?","code":""},{"path":"imputation-missing-data.html","id":"validation-using-simulated-data-and-real-world-case-studies","chapter":"11 Imputation (Missing Data)","heading":"11.5.4 Validation Using Simulated Data and Real-World Case Studies","text":"Validation imputation methods best performed combination simulated data real-world examples:Simulated Data: - Create datasets known missingness patterns true values. - Apply various imputation methods assess performance using RMSE, MAE, metrics.Real-World Case Studies:\nUse datasets actual studies, customer transaction data marketing financial data portfolio analysis.\nEvaluate impact imputation actionable outcomes (e.g., market segmentation, risk assessment).\nUse datasets actual studies, customer transaction data marketing financial data portfolio analysis.Evaluate impact imputation actionable outcomes (e.g., market segmentation, risk assessment).Combining approaches ensures methods generalize well across different contexts data structures.","code":""},{"path":"imputation-missing-data.html","id":"criteria-for-choosing-an-effective-approach","chapter":"11 Imputation (Missing Data)","heading":"11.6 Criteria for Choosing an Effective Approach","text":"Choosing appropriate imputation method depends following criteria:Unbiased Parameter Estimates: technique ensure key estimates, means, variances, regression coefficients, unbiased, particularly presence MAR MNAR data.Unbiased Parameter Estimates: technique ensure key estimates, means, variances, regression coefficients, unbiased, particularly presence MAR MNAR data.Adequate Power: method preserve statistical power, enabling robust hypothesis testing model estimation. ensures important effects missed due inflated type II error.Adequate Power: method preserve statistical power, enabling robust hypothesis testing model estimation. ensures important effects missed due inflated type II error.Accurate Standard Errors: Accurate estimation standard errors critical reliable p-values confidence intervals. Methods like single imputation often underestimate standard errors, leading overconfident conclusions.Accurate Standard Errors: Accurate estimation standard errors critical reliable p-values confidence intervals. Methods like single imputation often underestimate standard errors, leading overconfident conclusions.Preferred Methods: Multiple Imputation Full Information Maximum LikelihoodMultiple Imputation (MI):MI replaces missing values multiple plausible values drawn predictive distribution. generates multiple complete datasets, analyzes dataset, combines results.MI replaces missing values multiple plausible values drawn predictive distribution. generates multiple complete datasets, analyzes dataset, combines results.Pros: Handles uncertainty well, provides valid standard errors, robust MAR.Pros: Handles uncertainty well, provides valid standard errors, robust MAR.Cons: Computationally intensive, sensitive model mis-specification.Cons: Computationally intensive, sensitive model mis-specification.Full Information Maximum Likelihood (FIML):FIML uses available data estimate parameters directly, avoiding need impute missing values explicitly.FIML uses available data estimate parameters directly, avoiding need impute missing values explicitly.Pros: Efficient, unbiased MAR, computationally elegant.Pros: Efficient, unbiased MAR, computationally elegant.Cons: Requires correctly specified models may sensitive MNAR data.Cons: Requires correctly specified models may sensitive MNAR data.Methods AvoidSingle Imputation (e.g., Mean, Mode):\nLeads biased estimates underestimates variability.\nLeads biased estimates underestimates variability.Listwise Deletion:\nDiscards rows missing data, reducing sample size potentially introducing bias data MCAR.\nDiscards rows missing data, reducing sample size potentially introducing bias data MCAR.Practical ConsiderationsComputational efficiency ease implementation.Compatibility downstream analysis methods.Alignment data’s missingness mechanism.","code":""},{"path":"imputation-missing-data.html","id":"challenges-and-ethical-considerations","chapter":"11 Imputation (Missing Data)","heading":"11.7 Challenges and Ethical Considerations","text":"","code":""},{"path":"imputation-missing-data.html","id":"challenges-in-high-dimensional-data","chapter":"11 Imputation (Missing Data)","heading":"11.7.1 Challenges in High-Dimensional Data","text":"High-dimensional data, number variables exceeds number observations, poses unique challenges missing data analysis.Curse Dimensionality: Standard imputation methods, mean regression imputation, struggle high-dimensional spaces due sparse data distribution.Curse Dimensionality: Standard imputation methods, mean regression imputation, struggle high-dimensional spaces due sparse data distribution.Regularized Methods: Techniques LASSO, Ridge Regression, Elastic Net can used handle high-dimensional missing data. methods shrink model coefficients, preventing overfitting.Regularized Methods: Techniques LASSO, Ridge Regression, Elastic Net can used handle high-dimensional missing data. methods shrink model coefficients, preventing overfitting.Matrix Factorization: Methods like Principal Component Analysis (PCA) Singular Value Decomposition (SVD) often adapted impute missing values high-dimensional datasets reducing dimensionality first.Matrix Factorization: Methods like Principal Component Analysis (PCA) Singular Value Decomposition (SVD) often adapted impute missing values high-dimensional datasets reducing dimensionality first.","code":""},{"path":"imputation-missing-data.html","id":"missing-data-in-big-data-contexts","chapter":"11 Imputation (Missing Data)","heading":"11.7.2 Missing Data in Big Data Contexts","text":"advent big data introduces additional complexities missing data handling, including computational scalability storage constraints.","code":""},{"path":"imputation-missing-data.html","id":"distributed-imputation-techniques","chapter":"11 Imputation (Missing Data)","heading":"11.7.2.1 Distributed Imputation Techniques","text":"MapReduce Frameworks: Algorithms like k-nearest neighbor (KNN) imputation multiple imputation can adapted distributed environments using MapReduce similar frameworks.MapReduce Frameworks: Algorithms like k-nearest neighbor (KNN) imputation multiple imputation can adapted distributed environments using MapReduce similar frameworks.Federated Learning: scenarios data distributed across multiple locations (e.g., healthcare banking), federated learning allows imputation without centralizing data, ensuring privacy.Federated Learning: scenarios data distributed across multiple locations (e.g., healthcare banking), federated learning allows imputation without centralizing data, ensuring privacy.","code":""},{"path":"imputation-missing-data.html","id":"cloud-based-implementations","chapter":"11 Imputation (Missing Data)","heading":"11.7.2.2 Cloud-Based Implementations","text":"Cloud-Native Algorithms: Cloud platforms like AWS, Google Cloud, Azure provide scalable solutions implementing advanced imputation algorithms large datasets.Cloud-Native Algorithms: Cloud platforms like AWS, Google Cloud, Azure provide scalable solutions implementing advanced imputation algorithms large datasets.AutoML Integration: Automated Machine Learning (AutoML) pipelines often include missing data handling preprocessing step, leveraging cloud-based computational power.AutoML Integration: Automated Machine Learning (AutoML) pipelines often include missing data handling preprocessing step, leveraging cloud-based computational power.Real-Time Imputation: e-commerce, cloud-based solutions enable real-time imputation recommendation systems fraud detection, ensuring seamless user experiences.Real-Time Imputation: e-commerce, cloud-based solutions enable real-time imputation recommendation systems fraud detection, ensuring seamless user experiences.","code":""},{"path":"imputation-missing-data.html","id":"ethical-concerns","chapter":"11 Imputation (Missing Data)","heading":"11.7.3 Ethical Concerns","text":"","code":""},{"path":"imputation-missing-data.html","id":"bias-amplification","chapter":"11 Imputation (Missing Data)","heading":"11.7.3.1 Bias Amplification","text":"Introduction Systematic Bias: Imputation methods can inadvertently reinforce existing biases. example, imputing salary data based demographic variables may propagate societal inequalities.Introduction Systematic Bias: Imputation methods can inadvertently reinforce existing biases. example, imputing salary data based demographic variables may propagate societal inequalities.Business Implications: credit scoring, biased imputation missing financial data can lead unfair credit decisions, disproportionately affecting marginalized groups.Business Implications: credit scoring, biased imputation missing financial data can lead unfair credit decisions, disproportionately affecting marginalized groups.Mitigation Strategies: Techniques fairness-aware machine learning bias auditing can help identify reduce bias introduced imputation.Mitigation Strategies: Techniques fairness-aware machine learning bias auditing can help identify reduce bias introduced imputation.","code":""},{"path":"imputation-missing-data.html","id":"transparency-in-reporting-imputation-decisions","chapter":"11 Imputation (Missing Data)","heading":"11.7.3.2 Transparency in Reporting Imputation Decisions","text":"Reproducibility Documentation: Transparent reporting imputation methods assumptions essential reproducibility. Analysts provide clear documentation imputation pipeline.Reproducibility Documentation: Transparent reporting imputation methods assumptions essential reproducibility. Analysts provide clear documentation imputation pipeline.Stakeholder Communication: business settings, communicating imputation decisions stakeholders ensures informed decision-making trust results.Stakeholder Communication: business settings, communicating imputation decisions stakeholders ensures informed decision-making trust results.Ethical Frameworks: Ethical guidelines, provided European Union’s GDPR industry-specific codes, emphasize importance transparency data handling.Ethical Frameworks: Ethical guidelines, provided European Union’s GDPR industry-specific codes, emphasize importance transparency data handling.","code":""},{"path":"imputation-missing-data.html","id":"emerging-trends-in-missing-data-handling","chapter":"11 Imputation (Missing Data)","heading":"11.8 Emerging Trends in Missing Data Handling","text":"","code":""},{"path":"imputation-missing-data.html","id":"advances-in-neural-network-approaches","chapter":"11 Imputation (Missing Data)","heading":"11.8.1 Advances in Neural Network Approaches","text":"Neural networks transformed landscape missing data imputation, offering flexible, scalable, powerful solutions go beyond traditional methods.","code":""},{"path":"imputation-missing-data.html","id":"variational-autoencoders-vaes","chapter":"11 Imputation (Missing Data)","heading":"11.8.1.1 Variational Autoencoders (VAEs)","text":"Overview: Variational Autoencoders (VAEs) generative models encode data latent space reconstruct , filling missing values reconstruction.Overview: Variational Autoencoders (VAEs) generative models encode data latent space reconstruct , filling missing values reconstruction.Advantages:\nHandle complex, non-linear relationships variables.\nScalable high-dimensional datasets.\nGenerate probabilistic imputations, reflecting uncertainty.\nAdvantages:Handle complex, non-linear relationships variables.Scalable high-dimensional datasets.Generate probabilistic imputations, reflecting uncertainty.Applications:\nmarketing, VAEs can impute missing customer behavior data accounting seasonal demographic variations.\nfinance, VAEs assist imputing missing stock price data modeling dependencies among assets.\nApplications:marketing, VAEs can impute missing customer behavior data accounting seasonal demographic variations.finance, VAEs assist imputing missing stock price data modeling dependencies among assets.","code":""},{"path":"imputation-missing-data.html","id":"gans-for-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.8.1.2 GANs for Missing Data","text":"Generative Adversarial Networks (GANs): GANs consist generator discriminator, generator imputing missing data discriminator evaluating quality.Generative Adversarial Networks (GANs): GANs consist generator discriminator, generator imputing missing data discriminator evaluating quality.Advantages:\nPreserve data distributions avoid -smoothing.\nSuitable imputation datasets complex patterns multi-modal distributions.\nAdvantages:Preserve data distributions avoid -smoothing.Suitable imputation datasets complex patterns multi-modal distributions.Applications:\nhealthcare, GANs used impute missing patient records preserving patient privacy data integrity.\nretail, GANs can model missing sales data predict trends optimize inventory.\nApplications:healthcare, GANs used impute missing patient records preserving patient privacy data integrity.retail, GANs can model missing sales data predict trends optimize inventory.","code":""},{"path":"imputation-missing-data.html","id":"integration-with-reinforcement-learning","chapter":"11 Imputation (Missing Data)","heading":"11.8.2 Integration with Reinforcement Learning","text":"Reinforcement learning (RL) increasingly integrated missing data strategies, particularly dynamic sequential data environments.Markov Decision Processes (MDPs): RL models missing data handling MDP, actions (imputations) optimized based rewards (accuracy predictions decisions).Markov Decision Processes (MDPs): RL models missing data handling MDP, actions (imputations) optimized based rewards (accuracy predictions decisions).Active Imputation:\nRL can used actively query missing data points, prioritizing highest impact downstream tasks.\nExample: customer churn prediction, RL can optimize imputation high-value customer records.\nActive Imputation:RL can used actively query missing data points, prioritizing highest impact downstream tasks.Example: customer churn prediction, RL can optimize imputation high-value customer records.Applications:\nFinancial forecasting: RL models used impute missing transaction data dynamically, optimizing portfolio decisions.\nSmart cities: RL-based models handle missing sensor data enhance real-time decision-making traffic management.\nApplications:Financial forecasting: RL models used impute missing transaction data dynamically, optimizing portfolio decisions.Smart cities: RL-based models handle missing sensor data enhance real-time decision-making traffic management.","code":""},{"path":"imputation-missing-data.html","id":"synthetic-data-generation-for-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.8.3 Synthetic Data Generation for Missing Data","text":"Synthetic data generation emerged robust solution address missing data, providing flexibility privacy.Data Augmentation: Synthetic data generated augment datasets missing values, reducing biases introduced imputation.Data Augmentation: Synthetic data generated augment datasets missing values, reducing biases introduced imputation.Techniques:\nSimulations: Monte Carlo simulations create plausible data points based observed distributions.\nGenerative Models: GANs VAEs generate realistic synthetic data aligns existing patterns.\nTechniques:Simulations: Monte Carlo simulations create plausible data points based observed distributions.Generative Models: GANs VAEs generate realistic synthetic data aligns existing patterns.Applications:\nfraud detection, synthetic datasets balance impact missing values anomaly detection.\ninsurance, synthetic data supports pricing models filling gaps incomplete policyholder records.\nApplications:fraud detection, synthetic datasets balance impact missing values anomaly detection.insurance, synthetic data supports pricing models filling gaps incomplete policyholder records.","code":""},{"path":"imputation-missing-data.html","id":"federated-learning-and-privacy-preserving-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.8.4 Federated Learning and Privacy-Preserving Imputation","text":"Federated learning gained traction method collaborative analysis preserving data privacy.Federated Imputation:\nDistributed imputation algorithms operate decentralized data, ensuring sensitive information remains local.\nExample: Hospitals collaboratively impute missing patient data without sharing individual records.\nDistributed imputation algorithms operate decentralized data, ensuring sensitive information remains local.Example: Hospitals collaboratively impute missing patient data without sharing individual records.Privacy Mechanisms:\nDifferential privacy adds noise imputed values, protecting individual-level data.\nHomomorphic encryption allows computations encrypted data, ensuring privacy throughout imputation process.\nDifferential privacy adds noise imputed values, protecting individual-level data.Homomorphic encryption allows computations encrypted data, ensuring privacy throughout imputation process.Applications:\nHealthcare: Federated learning imputes missing diagnostic data across clinics.\nBanking: Collaborative imputation financial transaction data supports risk modeling adhering regulations.\nHealthcare: Federated learning imputes missing diagnostic data across clinics.Banking: Collaborative imputation financial transaction data supports risk modeling adhering regulations.","code":""},{"path":"imputation-missing-data.html","id":"imputation-in-streaming-and-online-data-environments","chapter":"11 Imputation (Missing Data)","heading":"11.8.5 Imputation in Streaming and Online Data Environments","text":"increasing use streaming data business technology requires real-time imputation methods ensure uninterrupted analysis.Challenges:\nImputation must occur dynamically data streams .\nLow latency high accuracy essential maintain real-time decision-making.\nImputation must occur dynamically data streams .Low latency high accuracy essential maintain real-time decision-making.Techniques:\nOnline Learning Algorithms: Update imputation models incrementally new data arrives.\nSliding Window Methods: Use recent data estimate impute missing values real time.\nOnline Learning Algorithms: Update imputation models incrementally new data arrives.Sliding Window Methods: Use recent data estimate impute missing values real time.Applications:\nIoT devices: Imputation sensor networks smart homes industrial monitoring ensures continuous operation despite data transmission issues.\nFinancial markets: Streaming imputation models predict fill gaps real-time stock price feeds inform trading algorithms.\nIoT devices: Imputation sensor networks smart homes industrial monitoring ensures continuous operation despite data transmission issues.Financial markets: Streaming imputation models predict fill gaps real-time stock price feeds inform trading algorithms.","code":""},{"path":"imputation-missing-data.html","id":"application-of-imputation-in-r","chapter":"11 Imputation (Missing Data)","heading":"11.9 Application of Imputation in R","text":"section demonstrates visualize missing data handle using different imputation techniques.","code":""},{"path":"imputation-missing-data.html","id":"visualizing-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.9.1 Visualizing Missing Data","text":"Visualizing missing data essential first step understanding patterns extent missingness dataset.details, read Missing Book Nicholas Tierney & Allison Horst.","code":"\nlibrary(visdat)\nlibrary(naniar)\nlibrary(ggplot2)\n\n# Visualizing missing data\nvis_miss(airquality)\n\n# Missingness patterns using an upset plot\ngg_miss_upset(airquality)\n# Scatter plot of missing data with faceting\nggplot(airquality, aes(x, y)) +\n  geom_miss_point() +\n  facet_wrap(~ group)\n\n# Missing values by variable\ngg_miss_var(data, facet = group)\n\n# Missingness in relation to factors\ngg_miss_fct(x = variable1, fct = variable2)"},{"path":"imputation-missing-data.html","id":"how-many-imputations","chapter":"11 Imputation (Missing Data)","heading":"11.9.2 How Many Imputations?","text":"Usually, 5 imputations sufficient unless extremely high proportion missing data. High proportions require revisiting data collection processes.Rubin’s Rule Relative EfficiencyAccording Rubin, relative efficiency estimate based \\(m\\) imputations (relative infinite imputations) given :\\[\n\\text{Relative Efficiency} = ( 1 + \\frac{\\lambda}{m})^{-1}\n\\]\\(\\lambda\\) rate missing data.example, 50% missing data (\\(\\lambda = 0.5\\)), standard deviation estimate based 5 imputations 5% wider infinite imputations:\\[\n\\sqrt{1 + \\frac{0.5}{5}} = 1.049\n\\]","code":""},{"path":"imputation-missing-data.html","id":"generating-missing-data-for-demonstration","chapter":"11 Imputation (Missing Data)","heading":"11.9.3 Generating Missing Data for Demonstration","text":"","code":"\nlibrary(missForest)\n\n# Load the data\ndata <- iris\n\n# Generate 10% missing values at random\nset.seed(1)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n# Remove categorical variables for numeric imputation\niris.mis.cat <- iris.mis\niris.mis <- subset(iris.mis, select = -c(Species))"},{"path":"imputation-missing-data.html","id":"imputation-with-mean-median-and-mode","chapter":"11 Imputation (Missing Data)","heading":"11.9.4 Imputation with Mean, Median, and Mode","text":"Mean, median, mode imputation simple yet commonly used technique.Checking AccuracyAccuracy can checked comparing predictions actual values.","code":"\n# Imputation for the entire dataset\ne1071::impute(iris.mis, what = \"mean\")        # Replace with mean\ne1071::impute(iris.mis, what = \"median\")      # Replace with median\n\n# Imputation by variable\nHmisc::impute(iris.mis$Sepal.Length, mean)    # Replace with mean\nHmisc::impute(iris.mis$Sepal.Length, median)  # Replace with median\nHmisc::impute(iris.mis$Sepal.Length, 0)       # Replace with a specific value\n# Example data\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- rep(mean(iris$Sepal.Width, na.rm = TRUE), length(actuals))\n\n# Using MLmetrics package\nlibrary(MLmetrics)\n\nMAE(predicteds, actuals)\n#> [1] 0.2870303\nMSE(predicteds, actuals)\n#> [1] 0.1301598\nRMSE(predicteds, actuals)\n#> [1] 0.3607767"},{"path":"imputation-missing-data.html","id":"k-nearest-neighbors-knn-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.9.5 K-Nearest Neighbors (KNN) Imputation","text":"KNN sophisticated method, leveraging similar observations fill missing values.KNN typically improves upon mean median imputation terms predictive accuracy.","code":"\nlibrary(DMwR2)\nknnOutput <- knnImputation(data = iris.mis.cat, meth = \"median\")\nanyNA(knnOutput)  # Check for remaining missing values\n#> [1] FALSE\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- knnOutput[is.na(iris.mis$Sepal.Width), \"Sepal.Width\"]\n# Using MLmetrics package\nlibrary(MLmetrics)\n\nMAE(predicteds, actuals)\n#> [1] 0.2318182\nMSE(predicteds, actuals)\n#> [1] 0.1038636\nRMSE(predicteds, actuals)\n#> [1] 0.3222788"},{"path":"imputation-missing-data.html","id":"imputation-with-decision-trees-rpart","chapter":"11 Imputation (Missing Data)","heading":"11.9.6 Imputation with Decision Trees (rpart)","text":"Decision trees, implemented rpart, effective numeric categorical variables.","code":"\nlibrary(rpart)\n\n# Imputation for a categorical variable\nclass_mod <- rpart(\n  Species ~ . - Sepal.Length,\n  data = iris.mis.cat[!is.na(iris.mis.cat$Species), ],\n  method = \"class\",\n  na.action = na.omit\n)\n\n# Imputation for a numeric variable\nanova_mod <- rpart(\n  Sepal.Width ~ . - Sepal.Length,\n  data = iris.mis[!is.na(iris.mis$Sepal.Width), ],\n  method = \"anova\",\n  na.action = na.omit\n)\n\n# Predictions\nspecies_pred <- predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species), ])\nwidth_pred <- predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width), ])"},{"path":"imputation-missing-data.html","id":"mice-multivariate-imputation-via-chained-equations","chapter":"11 Imputation (Missing Data)","heading":"11.9.7 MICE (Multivariate Imputation via Chained Equations)","text":"MICE assumes data Missing Random (MAR). imputes data variable specifying imputation model tailored variable type.","code":""},{"path":"imputation-missing-data.html","id":"how-mice-works","chapter":"11 Imputation (Missing Data)","heading":"11.9.7.1 How MICE Works","text":"dataset variables \\(X_1, X_2, \\dots, X_k\\):\\(X_1\\) missing data, regressed variables.\\(X_1\\) missing data, regressed variables.process repeated variables missing data, using previously predicted values needed.process repeated variables missing data, using previously predicted values needed.default:Continuous variables use linear regression.Continuous variables use linear regression.Categorical variables use logistic regression.Categorical variables use logistic regression.","code":""},{"path":"imputation-missing-data.html","id":"methods-available-in-mice","chapter":"11 Imputation (Missing Data)","heading":"11.9.7.2 Methods Available in MICE","text":"pmm (Predictive Mean Matching): numeric variables.logreg (Logistic Regression): binary variables (2 levels).polyreg (Bayesian polytomous regression): factor variables (≥2 levels).Proportional Odds Model: ordered factor variables (≥2 levels).Imputing DataEvaluating Imputed DataAccessing Using Imputed DataRegression Model Imputed Dataset","code":"\n# Load packages\nlibrary(mice)\nlibrary(VIM)\n\n# Check missing values pattern\nmd.pattern(iris.mis)#>     Sepal.Width Sepal.Length Petal.Length Petal.Width   \n#> 100           1            1            1           1  0\n#> 15            1            1            1           0  1\n#> 8             1            1            0           1  1\n#> 2             1            1            0           0  2\n#> 11            1            0            1           1  1\n#> 1             1            0            1           0  2\n#> 1             1            0            0           1  2\n#> 1             1            0            0           0  3\n#> 7             0            1            1           1  1\n#> 3             0            1            0           1  2\n#> 1             0            0            1           1  2\n#>              11           15           15          19 60\n\n# Plot missing values\naggr(\n  iris.mis,\n  col = c('navyblue', 'yellow'),\n  numbers = TRUE,\n  sortVars = TRUE,\n  labels = names(iris.mis),\n  cex.axis = 0.7,\n  gap = 3,\n  ylab = c(\"Missing data\", \"Pattern\")\n)#> \n#>  Variables sorted by number of missings: \n#>      Variable      Count\n#>   Petal.Width 0.12666667\n#>  Sepal.Length 0.10000000\n#>  Petal.Length 0.10000000\n#>   Sepal.Width 0.07333333\n# Perform multiple imputation using MICE\nimputed_Data <- mice(\n  iris.mis,\n  m = 5,             # Number of imputed datasets\n  maxit = 10,        # Number of iterations\n  method = 'pmm',    # Imputation method\n  seed = 500         # Random seed for reproducibility\n)\n# Summary of imputed data\nsummary(imputed_Data)\n#> Class: mids\n#> Number of multiple imputations:  5 \n#> Imputation methods:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>        \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\" \n#> PredictorMatrix:\n#>              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> Sepal.Length            0           1            1           1\n#> Sepal.Width             1           0            1           1\n#> Petal.Length            1           1            0           1\n#> Petal.Width             1           1            1           0\n\n# Density plot: compare imputed values (red) with observed values (blue)\ndensityplot(imputed_Data)\n# Access the complete datasets\ncompleteData1 <- complete(imputed_Data, 1)  # First imputed dataset\ncompleteData2 <- complete(imputed_Data, 2)  # Second imputed dataset\n# Fit a regression model using imputed datasets\nfit <- with(data = imputed_Data, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width))\n\n# Combine results of all 5 models\ncombine <- pool(fit)\nsummary(combine)\n#>           term   estimate  std.error statistic        df      p.value\n#> 1  (Intercept)  1.9054698 0.33454626  5.695684 105.12438 1.127064e-07\n#> 2 Sepal.Length  0.2936285 0.07011405  4.187870  88.69066 6.625536e-05\n#> 3  Petal.Width -0.4742921 0.08138313 -5.827892  46.94941 4.915270e-07"},{"path":"imputation-missing-data.html","id":"amelia","chapter":"11 Imputation (Missing Data)","heading":"11.9.8 Amelia","text":"Amelia uses bootstrap-based Expectation-Maximization Bootstrapping (EMB) algorithm imputation, making faster suitable cross-sectional time-series data.","code":""},{"path":"imputation-missing-data.html","id":"assumptions","chapter":"11 Imputation (Missing Data)","heading":"11.9.8.1 Assumptions","text":"variables must follow Multivariate Normal Distribution (MVN). Transformations may required non-normal data.variables must follow Multivariate Normal Distribution (MVN). Transformations may required non-normal data.Data must Missing Random (MAR).Data must Missing Random (MAR).","code":""},{"path":"imputation-missing-data.html","id":"comparison-amelia-vs.-mice","chapter":"11 Imputation (Missing Data)","heading":"11.9.8.2 Comparison: Amelia vs. MICE","text":"MICE imputes variable--variable basis using separate models.MICE imputes variable--variable basis using separate models.Amelia uses joint modeling approach based MVN.Amelia uses joint modeling approach based MVN.MICE handles multiple data types, Amelia requires variables approximate normality.MICE handles multiple data types, Amelia requires variables approximate normality.","code":""},{"path":"imputation-missing-data.html","id":"imputation-with-amelia","chapter":"11 Imputation (Missing Data)","heading":"11.9.8.3 Imputation with Amelia","text":"Amelia’s workflow includes bootstrapping multiple imputations generate robust estimates means variances. process ensures flexibility speed large datasets.","code":"\nlibrary(Amelia)\ndata(\"iris\")\n\n# Seed 10% missing values\nset.seed(123)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n# Specify columns and run Amelia\namelia_fit <- amelia(\n  iris.mis,\n  m = 5,                      # Number of imputations\n  parallel = \"multicore\",     # Use multicore processing\n  noms = \"Species\"            # Nominal variables\n)\n#> -- Imputation 1 --\n#> \n#>   1  2  3  4  5  6  7\n#> \n#> -- Imputation 2 --\n#> \n#>   1  2  3  4  5\n#> \n#> -- Imputation 3 --\n#> \n#>   1  2  3  4  5\n#> \n#> -- Imputation 4 --\n#> \n#>   1  2  3  4  5  6\n#> \n#> -- Imputation 5 --\n#> \n#>   1  2  3  4  5  6  7  8  9 10\n\n# Access imputed outputs\n# amelia_fit$imputations[[1]]"},{"path":"imputation-missing-data.html","id":"missforest","chapter":"11 Imputation (Missing Data)","heading":"11.9.9 missForest","text":"missForest package provides robust non-parametric imputation method using Random Forest algorithm. versatile, handling continuous categorical variables without requiring assumptions underlying functional forms.Key Features missForestNon-Parametric: assumptions functional form.Variable-Specific Models: Builds random forest model variable impute missing values.Error Estimates: Provides --bag (OOB) imputation error estimates.\nNRMSE (Normalized Root Mean Squared Error) continuous variables.\nPFC (Proportion Falsely Classified) categorical variables.\nNRMSE (Normalized Root Mean Squared Error) continuous variables.PFC (Proportion Falsely Classified) categorical variables.High Control: Offers customizable parameters like mtry ntree.","code":"\nlibrary(missForest)\n\n# Impute missing values using default parameters\niris.imp <- missForest(iris.mis)\n\n# Check imputed values\n# View the imputed dataset\n# iris.imp$ximp\n# Out-of-bag error estimates\niris.imp$OOBerror\n#>      NRMSE        PFC \n#> 0.14004144 0.02877698\n\n# Compare imputed data with original data to calculate error\niris.err <- mixError(iris.imp$ximp, iris.mis, iris)\niris.err\n#>      NRMSE        PFC \n#> 0.14420833 0.09090909"},{"path":"imputation-missing-data.html","id":"hmisc","chapter":"11 Imputation (Missing Data)","heading":"11.9.10 Hmisc","text":"Hmisc package provides suite tools imputing missing data, offering simple methods (like mean median imputation) advanced approaches like aregImpute.Features Hmiscimpute(): Simple imputation using user-defined methods like mean, median, random value.impute(): Simple imputation using user-defined methods like mean, median, random value.aregImpute():\nCombines additive regression, bootstrapping, predictive mean matching.\nHandles continuous categorical variables.\nAutomatically recognizes variable types applies appropriate methods.\naregImpute():Combines additive regression, bootstrapping, predictive mean matching.Combines additive regression, bootstrapping, predictive mean matching.Handles continuous categorical variables.Handles continuous categorical variables.Automatically recognizes variable types applies appropriate methods.Automatically recognizes variable types applies appropriate methods.AssumptionsLinearity variables predicted.Linearity variables predicted.Fisher’s optimum scoring used categorical variable prediction.Fisher’s optimum scoring used categorical variable prediction.Note: missForest often outperforms Hmisc terms accuracy, latter useful datasets simpler requirements.","code":"library(Hmisc)\n\n# Impute using mean\niris.mis$imputed_SepalLength <- with(iris.mis, impute(Sepal.Length, mean))\n\n# Impute using random value\niris.mis$imputed_SepalLength2 <- with(iris.mis, impute(Sepal.Length, 'random'))\n\n# Advanced imputation using aregImpute\nimpute_arg <- aregImpute(\n  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species,\n  data = iris.mis,\n  n.impute = 5\n)\n#> Iteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \n\n# Check R-squared values for predicted missing values\nimpute_arg\n#> \n#> Multiple Imputation using Bootstrap and PMM\n#> \n#> aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + \n#>     Petal.Width + Species, data = iris.mis, n.impute = 5)\n#> \n#> n: 150   p: 5    Imputations: 5      nk: 3 \n#> \n#> Number of NAs:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>           17           19           12           16           11 \n#> \n#>              type d.f.\n#> Sepal.Length    s    2\n#> Sepal.Width     s    2\n#> Petal.Length    s    2\n#> Petal.Width     s    2\n#> Species         c    2\n#> \n#> Transformation of Target Variables Forced to be Linear\n#> \n#> R-squares for Predicting Non-Missing Values for Each Variable\n#> Using Last Imputations of Predictors\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>        0.895        0.536        0.987        0.967        0.984\n\n# Access imputed values for Sepal.Length\nimpute_arg$imputed$Sepal.Length\n#>     [,1] [,2] [,3] [,4] [,5]\n#> 13   4.4  4.9  4.9  5.0  4.9\n#> 14   4.8  4.4  5.0  4.5  4.5\n#> 23   4.8  5.1  5.1  5.1  4.8\n#> 26   5.0  4.8  4.9  4.9  5.0\n#> 34   5.0  5.8  6.0  5.7  5.8\n#> 39   4.4  4.9  5.0  4.5  4.6\n#> 41   5.2  5.1  4.8  5.0  4.8\n#> 69   5.8  6.0  6.3  6.0  6.1\n#> 72   5.6  5.7  5.7  5.8  6.1\n#> 89   6.1  5.7  5.7  5.6  6.9\n#> 90   5.5  6.2  5.2  6.0  5.8\n#> 91   5.7  6.9  6.0  6.4  6.4\n#> 116  5.9  6.8  6.4  6.6  6.9\n#> 118  7.9  7.9  7.9  7.9  7.9\n#> 135  6.7  6.7  6.7  6.9  6.7\n#> 141  7.0  6.3  5.9  6.7  7.0\n#> 143  5.7  6.7  5.8  6.3  5.4"},{"path":"imputation-missing-data.html","id":"mi","chapter":"11 Imputation (Missing Data)","heading":"11.9.11 mi","text":"mi package powerful tool imputation, using Bayesian methods providing rich diagnostics model evaluation convergence.Features miGraphical Diagnostics: Visualize imputation models convergence.Graphical Diagnostics: Visualize imputation models convergence.Bayesian Regression: Handles separation issues data.Bayesian Regression: Handles separation issues data.Irregularity Detection: Automatically detects issues like high collinearity.Irregularity Detection: Automatically detects issues like high collinearity.Noise Addition: Adds noise address additive constraints.Noise Addition: Adds noise address additive constraints.","code":"\nlibrary(mi)\n\n# Perform imputation using mi\nmi_data <- mi(iris.mis, seed = 1)\n\n# Summary of the imputation process\nsummary(mi_data)\n#> $Sepal.Length\n#> $Sepal.Length$is_missing\n#> missing\n#> FALSE  TRUE \n#>   133    17 \n#> \n#> $Sepal.Length$imputed\n#>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n#> -0.6355172 -0.0703238 -0.0005039 -0.0052716  0.0765631  0.3731257 \n#> \n#> $Sepal.Length$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.90110 -0.47329 -0.04549  0.00000  0.32120  1.23792 \n#> \n#> \n#> $Sepal.Width\n#> $Sepal.Width$is_missing\n#> missing\n#> FALSE  TRUE \n#>   131    19 \n#> \n#> $Sepal.Width$imputed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -2.1083 -0.4216 -0.1925 -0.1940  0.1589  0.7330 \n#> \n#> $Sepal.Width$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -1.01272 -0.30642 -0.07099  0.00000  0.39988  1.34161 \n#> \n#> \n#> $Petal.Length\n#> $Petal.Length$is_missing\n#> missing\n#> FALSE  TRUE \n#>   138    12 \n#> \n#> $Petal.Length$imputed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.86312 -0.58453  0.23556  0.04176  0.48870  0.77055 \n#> \n#> $Petal.Length$observed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.7797 -0.6088  0.1459  0.0000  0.3880  0.9006 \n#> \n#> \n#> $Petal.Width\n#> $Petal.Width$is_missing\n#> missing\n#> FALSE  TRUE \n#>   134    16 \n#> \n#> $Petal.Width$imputed\n#>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n#> -0.9116177 -0.0000042  0.2520468  0.1734543  0.5147010  0.8411324 \n#> \n#> $Petal.Width$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.69624 -0.56602  0.08503  0.00000  0.41055  0.86629 \n#> \n#> \n#> $Species\n#> $Species$crosstab\n#>             \n#>              observed imputed\n#>   setosa          180      16\n#>   versicolor      192      11\n#>   virginica       184      17\n#> \n#> \n#> $imputed_SepalLength\n#> $imputed_SepalLength$is_missing\n#> [1] \"all values observed\"\n#> \n#> $imputed_SepalLength$observed\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.9574 -0.4379  0.0000  0.0000  0.3413  1.3152 \n#> \n#> \n#> $imputed_SepalLength2\n#> $imputed_SepalLength2$is_missing\n#> [1] \"all values observed\"\n#> \n#> $imputed_SepalLength2$observed\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -0.90570 -0.48398 -0.06225  0.00000  0.35947  1.20292"},{"path":"data.html","id":"data","chapter":"12 Data","heading":"12 Data","text":"multiple ways categorize data. example,Qualitative vs. Quantitative:","code":""},{"path":"data.html","id":"cross-sectional","chapter":"12 Data","heading":"12.1 Cross-Sectional","text":"","code":""},{"path":"data.html","id":"time-series","chapter":"12 Data","heading":"12.2 Time Series","text":"\\[\ny_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + ... + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t\n\\]ExamplesStatic Model\n\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)\nStatic Model\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)Finite Distributed Lag model\n\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)\nLong Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)\nFinite Distributed Lag model\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)Long Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)Dynamic Model\n\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)\nDynamic Model\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)Finite Sample Properties Time Series:A1-A3: OLS unbiasedA1-A4: usual standard errors consistent Gauss-Markov Theorem holds (OLS BLUE)A1-A6, A6: Finite Sample Wald Test (t-test F-test) validA3 might hold time series settingSpurious Time Trend - solvableStrict vs Contemporaneous Exogeneity - solvableIn time series data, many processes:Autoregressive model order p: AR(p)Moving average model order q: MA(q)Autoregressive model order p moving average model order q: ARMA(p,q)Autoregressive conditional heteroskedasticity model order p: ARCH(p)Generalized Autoregressive conditional heteroskedasticity orders p q; GARCH(p.q)","code":""},{"path":"data.html","id":"deterministic-time-trend","chapter":"12 Data","heading":"12.2.1 Deterministic Time trend","text":"dependent independent variables trending timeSpurious Time Series Regression\\[\ny_t = \\alpha_0 + t\\alpha_1 + v_t\n\\]x takes form\\[\nx_t = \\lambda_0 + t\\lambda_1 + u_t\n\\]\\(\\alpha_1 \\neq 0\\) \\(\\lambda_1 \\neq 0\\)\\(v_t\\) \\(u_t\\) independentthere relationship \\(y_t\\) \\(x_t\\)estimate regression,\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]true \\(\\beta_1=0\\)Inconsistent: \\(plim(\\hat{\\beta}_1)=\\frac{\\alpha_1}{\\lambda_1}\\)Invalid Inference: \\(|t| \\^d \\infty\\) \\(H_0: \\beta_1=0\\), always reject null \\(n \\\\infty\\)Uninformative \\(R^2\\): \\(plim(R^2) = 1\\) able perfectly predict \\(n \\\\infty\\)can rewrite equation \\[\n\\begin{aligned}\ny_t &=\\beta_0 + \\beta_1x_t+\\epsilon_t \\\\\n\\epsilon_t &= \\alpha_1t + v_t\n\\end{aligned}\n\\]\\(\\beta_0 = \\alpha_0\\) \\(\\beta_1=0\\). Since \\(x_t\\) deterministic function time, \\(\\epsilon_t\\) correlated \\(x_t\\) usual omitted variable bias.\nEven \\(y_t\\) \\(x_t\\) related (\\(\\beta_1 \\neq 0\\)) trending time, still get spurious results simple regression \\(y_t\\) \\(x_t\\)Solutions Spurious TrendInclude time trend \\(t\\) additional control\nconsistent parameter estimates valid inference\nInclude time trend \\(t\\) additional controlconsistent parameter estimates valid inferenceDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))\nDetrending partialing [Frisch-Waugh-Lovell Theorem]\nallow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\n\nDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))Detrending partialing [Frisch-Waugh-Lovell Theorem]\nallow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\nDetrending partialing [Frisch-Waugh-Lovell Theorem]allow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)Allow seasonality including indicators relevant “seasons” (quarters, months, weeks).A3 hold :Feedback Effect\n\\(\\epsilon_t\\) influences next period’s independent variables\nFeedback Effect\\(\\epsilon_t\\) influences next period’s independent variablesDynamic Specification\ninclude last time period outcome explanatory variable\nDynamic Specificationinclude last time period outcome explanatory variableDynamically Complete\nfinite distrusted lag model, number lags needs absolutely correct.\nDynamically CompleteFor finite distrusted lag model, number lags needs absolutely correct.","code":""},{"path":"data.html","id":"feedback-effect","chapter":"12 Data","heading":"12.2.2 Feedback Effect","text":"\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]A3\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, \\(y_t\\) likely influence \\(x_{t+1},..,x_T\\)A3 violated require error uncorrelated time observation independent regressors (strict exogeneity)","code":""},{"path":"data.html","id":"dynamic-specification","chapter":"12 Data","heading":"12.2.3 Dynamic Specification","text":"\\[\ny_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T)\n\\]equal 0, \\(y_t\\) \\(\\epsilon_t\\) inherently correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Dynamic Specification allowed A3","code":""},{"path":"data.html","id":"dynamically-complete","chapter":"12 Data","heading":"12.2.4 Dynamically Complete","text":"\\[\ny_t = \\beta_0 + x_t\\delta_0 + x_{t-1}\\delta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, include enough lags, \\(x_{t-2}\\) \\(\\epsilon_t\\) correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Can corrected including lags (stop? )Without A3OLS biasedGauss-Markov TheoremFinite Sample Properties invalidthen, canFocus Large Sample PropertiesCan use [A3a] instead A3[A3a] time series become\\[\nA3a: E(\\mathbf{x}_t'\\epsilon_t)= 0\n\\]regressors time period need independent error time period (Contemporaneous Exogeneity)\\(\\epsilon_t\\) can correlated \\(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\\)can dynamic specification \\(y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\\)Deriving Large Sample Properties Time SeriesAssumptions A1, A2, [A3a]Assumptions A1, A2, [A3a][Weak Law] Central Limit Theorem depend A5\n\\(x_t\\) \\(\\epsilon_t\\) dependent t\nwithout [Weak Law] Central Limit Theorem depend A5, Large Sample Properties OLS\nInstead A5, consider [A5a]\n[Weak Law] Central Limit Theorem depend A5\\(x_t\\) \\(\\epsilon_t\\) dependent twithout [Weak Law] Central Limit Theorem depend A5, Large Sample Properties OLSInstead A5, consider [A5a]Derivation Asymptotic Variance depends A4\ntime series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)\nDerivation Asymptotic Variance depends A4time series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)A1, A2, [A3a], [A5a], OLS estimator consistent, asymptotically normal","code":""},{"path":"data.html","id":"highly-persistent-data","chapter":"12 Data","heading":"12.2.5 Highly Persistent Data","text":"\\(y_t, \\mathbf{x}_t\\) weakly dependent stationary process\\(y_t\\) \\(y_{t-h}\\) almost independent large h\\(y_t\\) \\(y_{t-h}\\) almost independent large h[A5a] hold OLS consistent limiting distribution.[A5a] hold OLS consistent limiting distribution.Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\)Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\)Solution First difference stationary process\\[\ny_t - y_{t-1} = u_t\n\\]\\(u_t\\) weakly dependent process (also called integrated order 0) \\(y_t\\) said difference-stationary process (integrated order 1)regression, \\(\\{y_t, \\mathbf{x}_t \\}\\) random walks (integrated order 1), can consistently estimate first difference equation\\[\n\\begin{aligned}\ny_t - y_{t-1} &= (\\mathbf{x}_t - \\mathbf{x}_{t-1}\\beta + \\epsilon_t - \\epsilon_{t-1}) \\\\\n\\Delta y_t &= \\Delta \\mathbf{x}\\beta + \\Delta u_t\n\\end{aligned}\n\\]Unit Root Test\\[\ny_t = \\alpha + \\alpha y_{t-1} + u_t\n\\]tests \\(\\rho=1\\) (integrated order 1)null \\(H_0: \\rho = 1\\), OLS consistent asymptotically normal.alternative \\(H_a: \\rho < 1\\), OLS consistent asymptotically normal.usual t-test valid, need use transformed equation produce valid test.Dickey-Fuller Test \\[\n\\Delta y_t= \\alpha + \\theta y_{t-1} + v_t\n\\] \\(\\theta = \\rho -1\\)\\(H_0: \\theta = 0\\) \\(H_a: \\theta < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) .Dickey Fuller derived non-normal asymptotic distribution. reject null \\(y_t\\) random walk.Concerns standard Dickey Fuller Test\n1. considers fairly simplistic dynamic relationship\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta_{t-1} + ..+ \\gamma_p \\Delta_{t-p} +v_t\n\\]one additional lag, null \\(\\Delta_{y_t}\\) AR(1) process alternative \\(y_t\\) AR(2) process.Solution: include lags \\(\\Delta_{y_t}\\) controls.allow time trend \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t\n\\]allows \\(y_t\\) quadratic relationship \\(t\\)Solution: include time trend (changes critical values).Adjusted Dickey-Fuller Test \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + v_t\n\\] \\(\\theta = 1 - \\rho\\)\\(H_0: \\theta_1 = 0\\) \\(H_a: \\theta_1 < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) notCritical values different time trend, reject null \\(y_t\\) random walk.","code":""},{"path":"data.html","id":"newey-west-standard-errors","chapter":"12 Data","heading":"12.2.5.0.1 Newey West Standard Errors","text":"A4 hold, can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent)\\[\n\\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x'_tx_t} + \\sum_{h=1}^{g}(1-\\frac{h}{g+1})T^{-1}\\sum_{t=h+1}^{T} e_t e_{t-h}(\\mathbf{x_t'x_{t-h}+ x_{t-h}'x_t})\n\\]estimates covariances distance g partestimates covariances distance g partdownweights insure \\(\\hat{B}\\) PSDdownweights insure \\(\\hat{B}\\) PSDHow choose g:\nyearly data: \\(g = 1\\) 2 likely account correlation\nquarterly monthly data: g larger ($g = 4$ 8 quarterly \\(g = 12\\) 14 monthly)\ncan also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)\nchoose g:yearly data: \\(g = 1\\) 2 likely account correlationFor quarterly monthly data: g larger ($g = 4$ 8 quarterly \\(g = 12\\) 14 monthly)can also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)Testing Serial CorrelationRun OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Reject null serial correlation coefficient significant 5% level.\nTest using heteroskedastic robust standard errors\ncan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)\nReject null serial correlation coefficient significant 5% level.Test using heteroskedastic robust standard errorscan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)","code":""},{"path":"data.html","id":"repeated-cross-sections","chapter":"12 Data","heading":"12.3 Repeated Cross Sections","text":"time point (day, month, year, etc.), set data sampled. set data can different among different time points.example, can sample different groups students time survey.Allowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i\n\\]Dummy variables one time periodallows different intercept time periodallows outcome change average time periodAllowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows effects change based time period (structural break)Interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"pooled-cross-section","chapter":"12 Data","heading":"12.3.1 Pooled Cross Section","text":"\\[\ny_i=\\mathbf{x_i\\beta +x_i \\times y1\\gamma_1 + ...+ x_i \\times yT\\gamma_T + \\delta_1y_1+...+ \\delta_Ty_T + \\epsilon_i}\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows different slopes time periodallows effect change based time period (structural break)\ninteracting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.\nallows effect change based time period (structural break)interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"panel-data","chapter":"12 Data","heading":"12.4 Panel Data","text":"Detail notes R can found hereFollows individual T time periods.Panel data structure like n samples time series dataCharacteristicsInformation across individuals time (cross-sectional time-series)Information across individuals time (cross-sectional time-series)N individuals T time periodsN individuals T time periodsData can either\nBalanced: individuals observed time periods\nUnbalanced: individuals observed time periods.\nData can eitherBalanced: individuals observed time periodsUnbalanced: individuals observed time periods.Assume correlation (clustering) time given individual, independence individuals.Assume correlation (clustering) time given individual, independence individuals.TypesShort panel: many individuals time periods.Long panel: many time periods individualsBoth: many time periods many individualsTime Trends Time EffectsNonlinearSeasonalityDiscontinuous shocksRegressorsTime-invariant regressors \\(x_{}=x_i\\) t (e.g., gender, race, education) zero within variationIndividual-invariant regressors \\(x_{}=x_{t}\\) (e.g., time trend, economy trends) zero variationVariation dependent variable regressorsOverall variation: variation time individuals.variation: variation individualsWithin variation: variation within individuals (time).Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\)Since n observation time period t, can control time effect separately including time dummies (time effects)\\[\ny_{}=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + \\epsilon_{}\n\\]Note: use many time dummies time series data time series data, n 1. Hence, variation, sometimes enough data compared variables estimate coefficients.Unobserved Effects Model Similar group clustering, assume random effect captures differences across individuals constant time.\\[\ny_it=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + c_i + u_{}\n\\]\\(c_i + u_{} = \\epsilon_{}\\)\\(c_i\\) unobserved individual heterogeneity (effect)\\(u_{}\\) idiosyncratic shock\\(\\epsilon_{}\\) unobserved error term.","code":""},{"path":"data.html","id":"pooled-ols-estimator","chapter":"12 Data","heading":"12.4.1 Pooled OLS Estimator","text":"\\(c_i\\) uncorrelated \\(x_{}\\)\\[\nE(\\mathbf{x_{}'}(c_i+u_{})) = 0\n\\][A3a] still holds. Pooled OLS consistent.A4 hold, OLS still consistent, efficient, need cluster robust SE.Sufficient [A3a] hold, needExogeneity \\(u_{}\\) [A3a] (contemporaneous exogeneity): \\(E(\\mathbf{x_{}'}u_{})=0\\) time varying errorRandom Effect Assumption (time constant error): \\(E(\\mathbf{x_{}'}c_{})=0\\)Pooled OLS give consistent coefficient estimates A1, A2, [A3a] (\\(u_{}\\) RE assumption), A5 (randomly sampling across ).","code":""},{"path":"data.html","id":"individual-specific-effects-model","chapter":"12 Data","heading":"12.4.2 Individual-specific effects model","text":"believe unobserved heterogeneity across individual (e.g., unobserved ability individual affects \\(y\\)), individual-specific effects correlated regressors, Fixed Effects Estimator. correlated Random Effects Estimator.","code":""},{"path":"data.html","id":"random-effects-estimator","chapter":"12 Data","heading":"12.4.2.1 Random Effects Estimator","text":"Random Effects estimator Feasible GLS estimator assumes \\(u_{}\\) serially uncorrelated homoskedasticUnder A1, A2, [A3a] (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.\nA4 holds \\(u_{}\\), RE efficient estimator\nA4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.\nA1, A2, [A3a] (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.A4 holds \\(u_{}\\), RE efficient estimatorIf A4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.","code":""},{"path":"data.html","id":"fixed-effects-estimator","chapter":"12 Data","heading":"12.4.2.2 Fixed Effects Estimator","text":"also known Within Estimator uses within variation (time)RE assumption hold (\\(E(\\mathbf{x_{}'}c_i) \\neq 0\\)), A3a hold (\\(E(\\mathbf{x_{}'}\\epsilon_i) \\neq 0\\)).Hence, OLS RE inconsistent/biased (omitted variable bias)However, FE can fix bias due time-invariant factors (observables unobservables) correlated treatment (time-variant factors correlated treatment).traditional FE technique flawed lagged dependent variables included model. (Nickell 1981) (Narayanan Nair 2013)measurement error independent, FE exacerbate errors---variables bias.","code":""},{"path":"data.html","id":"demean-approach","chapter":"12 Data","heading":"12.4.2.2.1 Demean Approach","text":"deal violation \\(c_i\\), \\[\ny_{}= \\mathbf{x_{} \\beta} + c_i + u_{}\n\\]\\[\n\\bar{y_i}=\\bar{\\mathbf{x_i}} \\beta + c_i + \\bar{u_i}\n\\]second equation time averaged equationusing within transformation, \\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})}\\beta + u_{} - \\bar{u_i}\n\\]\\(c_i\\) time constant.Fixed Effects estimator uses POLS transformed equation\\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})} \\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + u_{} - \\bar{u_i}\n\\]need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.Variables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.\ninterested effects time-invariant variables, consider OLS estimator\nVariables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.interested effects time-invariant variables, consider OLS estimatorIt’s recommended still use cluster robust standard errors.’s recommended still use cluster robust standard errors.","code":""},{"path":"data.html","id":"dummy-approach","chapter":"12 Data","heading":"12.4.2.2.2 Dummy Approach","text":"Equivalent within transformation (.e., mathematically equivalent Demean Approach), can fixed effect estimator dummy regression\\[\ny_{} = x_{}\\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + c_1\\gamma_1 + ... + c_{n-1}\\gamma_{n-1} + u_{}\n\\]\\[\nc_i\n=\n\\begin{cases}\n1 &\\text{observation } \\\\\n0 &\\text{otherwise} \\\\\n\\end{cases}\n\\]standard error incorrectly calculated.FE within transformation controlling difference across individual allowed correlated observables.","code":""},{"path":"data.html","id":"first-difference-approach","chapter":"12 Data","heading":"12.4.2.2.3 First-difference Approach","text":"Economists typically use approach\\[\ny_{} - y_{(t-1)} = (\\mathbf{x}_{} - \\mathbf{x}_{(t-1)}) \\beta +  + (u_{} - u_{(t-1)})\n\\]","code":""},{"path":"data.html","id":"fixed-effects-summary","chapter":"12 Data","heading":"12.4.2.2.4 Fixed Effects Summary","text":"three approaches almost equivalent.\nDemean Approach mathematically equivalent Dummy Approach\n1 period, 3 .\nthree approaches almost equivalent.Demean Approach mathematically equivalent Dummy ApproachDemean Approach mathematically equivalent Dummy ApproachIf 1 period, 3 .1 period, 3 .Since fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.\nHence, small number changes standard error \\(\\beta\\) explode\nSince fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.Hence, small number changes standard error \\(\\beta\\) explodeStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers.\nTreatment effect typically non-directional.\ncan give parameter direction needed.\nStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers.Treatment effect typically non-directional.Treatment effect typically non-directional.can give parameter direction needed.can give parameter direction needed.Issues:\nfundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.\nfixed effects focus bias reduction, might larger variance (typically, fixed effects less df)\nIssues:fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.can estimate FE different units (just individuals).can estimate FE different units (just individuals).FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)Recall\\[\nY_{} = \\beta_0 + X_{}\\beta_1 + \\alpha_i + u_{}\n\\]\\(\\epsilon_{} = \\alpha_i + u_{}\\)\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{SSR_{OLS}}{NT - K}\n\\]\\[\n\\hat{\\sigma}^2_u = \\frac{SSR_{FE}}{NT - (N+K)} = \\frac{SSR_{FE}}{N(T-1)-K}\n\\]’s ambiguous whether variance error changes SSR can increase denominator decreases.FE can unbiased, consistent (.e., converging true effect)","code":""},{"path":"data.html","id":"fe-examples","chapter":"12 Data","heading":"12.4.2.2.5 FE Examples","text":"","code":""},{"path":"data.html","id":"blau1999","chapter":"12 Data","heading":"12.4.2.2.6 Blau (1999)","text":"Intergenerational mobilityIntergenerational mobilityIf transfer resources low income family, can generate upward mobility (increase ability)?transfer resources low income family, can generate upward mobility (increase ability)?Mechanisms intergenerational mobilityGenetic (policy can’t affect) (.e., ability endowment)Environmental indirectEnvironmental direct\\[\n\\frac{\\% \\Delta \\text{Human capital}}{\\% \\Delta \\text{income}}\n\\]Financial transferIncome measures:Total household incomeWage incomeNon-wage incomeAnnual versus permanent incomeCore control variables:Bad controls jointly determined dependent variableControl mother = choice motherUncontrolled mothers:mother racemother racelocation birthlocation birtheducation parentseducation parentshousehold structure age 14household structure age 14\\[\nY_{ijt} = X_{jt} \\beta_i + I_{jt} \\alpha_i + \\epsilon_{ijt}\n\\]\\(\\) = test\\(\\) = test\\(j\\) = individual (child)\\(j\\) = individual (child)\\(t\\) = time\\(t\\) = timeGrandmother’s modelSince child nested within mother mother nested within grandmother, fixed effect child included fixed effect mother, included fixed-effect grandmother\\[\nY_{ijgmt} = X_{} \\beta_{} + I_{jt} \\alpha_i + \\gamma_g + u_{ijgmt}\n\\]\\(\\) = test, \\(j\\) = kid, \\(m\\) = mother, \\(g\\) = grandmother\\(\\) = test, \\(j\\) = kid, \\(m\\) = mother, \\(g\\) = grandmotherwhere \\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)\\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)Grandma fixed-effectPros:control genetics + fixed characteristics mother raisedcontrol genetics + fixed characteristics mother raisedcan estimate effect parameter incomecan estimate effect parameter incomeCon:Might sufficient controlCommon cluster fixed-effect level (common correlated component)Fixed effect exaggerates attenuation biasError rate survey can help fix (plug number , uncertainty associated number).","code":""},{"path":"data.html","id":"babcock2010","chapter":"12 Data","heading":"12.4.2.2.7 Babcock (2010)","text":"\\[\nT_{ijct} = \\alpha_0 + S_{jct} \\alpha_1 + X_{ijct} \\alpha_2 + u_{ijct}\n\\]\\(S_{jct}\\) average class expectation\\(S_{jct}\\) average class expectation\\(X_{ijct}\\alpha_2\\) individual characteristics\\(X_{ijct}\\alpha_2\\) individual characteristics\\(\\) student\\(\\) student\\(j\\) instructor\\(j\\) instructor\\(c\\) course\\(c\\) course\\(t\\) time\\(t\\) time\\[\nT_{ijct} = \\beta_0+ S_{jct} \\beta_1+ X_{ijct} \\beta_2 +\\mu_{jc} + \\epsilon_{ijct}\n\\]\\(\\mu_{jc}\\) instructor course fixed effect (unique id), different \\((\\theta_j + \\delta_c)\\)Decrease course shopping conditioned available information (\\(\\mu_{jc}\\)) (class grade instructor’s info).Grade expectation change even though class materials stay sameIdentification strategy isUnder (fixed) time-varying factor bias coefficient (simultaneity)\\[\nY_{ijt} = X_{} \\beta_1 + \\text{Teacher Experience}_{jt} \\beta_2 + \\text{Teacher education}_{jt} \\beta_3 + \\text{Teacher score}_{}\\beta_4 + \\dots + \\epsilon_{ijt}\n\\]Drop teacher characteristics, include teacher dummy effect\\[\nY_{ijt} = X_{} \\alpha + \\Gamma_{} \\theta_j + u_{ijt}\n\\]\\(\\alpha\\) within teacher (conditional teacher fixed effect) \\(j = 1 \\(J-1)\\)Nuisance sense don’t interpretation \\(\\alpha\\)least can say \\(\\theta_j\\) teacher effect conditional student test score.\\[\nY_{ijt} = X_{} \\gamma + \\epsilon_{ijt}\n\\]\\(\\gamma\\) within (unconditional) \\(e_{ijt}\\) prediction error\\[\ne_{ijt} = T_{} \\delta_j + \\tilde{e}_{ijt}\n\\]\\(\\delta_j\\) mean group\\[\nY_{ijkt} = Y_{ijkt-1} + X_{} \\beta + T_{} \\tau_j + (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(Y_{ijkt-1}\\) = lag control\\(Y_{ijkt-1}\\) = lag control\\(\\tau_j\\) = teacher fixed time\\(\\tau_j\\) = teacher fixed time\\(W_i\\) student fixed effect\\(W_i\\) student fixed effect\\(P_k\\) school fixed effect\\(P_k\\) school fixed effect\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)worry selection class schoolBias \\(\\tau\\) (1 teacher) \\[\n\\frac{1}{N_j} \\sum_{= 1}^N (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(N_j\\) = number student class teacher \\(j\\)can \\(P_k + \\frac{1}{N_j} \\sum_{= 1}^{N_j} (W_i + \\epsilon_{ijkt})\\)Shocks small class can bias \\(\\tau\\)\\[\n\\frac{1}{N_j} \\sum_{= 1}^{N_j} \\epsilon_{ijkt} \\neq 0\n\\]inflate teacher fixed effectEven create random teacher fixed effect put model, still contains bias mentioned can still \\(\\tau\\) (know way affect - whether positive negative).teachers switch schools, can estimate teacher school fixed effect (mobility web thin vs. thick)Mobility web refers web switchers (.e., one status another).\\[\nY_{ijkt} = Y_{ijk(t-1)} \\alpha + X_{}\\beta + T_{} \\tau + P_k + \\epsilon_{ijkt}\n\\]demean (fixed-effect), \\(\\tau\\) (teacher fixed effect) go awayIf want examine teacher fixed effect, include teacher fixed effectControl school, article argues selection biasFor \\(\\frac{1}{N_j} \\sum_{=1}^{N_j} \\epsilon_{ijkt}\\) (teacher-level average residuals), \\(var(\\tau)\\) change \\(N_j\\) (Figure 2 paper). words, quality teachers function number studentsIf \\(var(\\tau) =0\\) means teacher quality matterSpin-Measurement Error: Sampling error estimation error\\[\n\\hat{\\tau}_j = \\tau_j + \\lambda_j\n\\]\\[\nvar(\\hat{\\tau}) = var(\\tau + \\lambda)\n\\]Assume \\(cov(\\tau_j, \\lambda_j)=0\\) (reasonable) words, randomness getting children correlation teacher quality.Hence,\\[\n\\begin{aligned}\nvar(\\hat{\\tau}) &= var(\\tau) + var(\\lambda) \\\\\nvar(\\tau) &= var(\\hat{\\tau}) - var(\\lambda) \\\\\n\\end{aligned}\n\\]\\(var(\\hat{\\tau})\\) need estimate \\(var(\\lambda)\\)\\[\nvar(\\lambda) = \\frac{1}{J} \\sum_{j=1}^J \\hat{\\sigma}^2_j\n\\] \\(\\hat{\\sigma}^2_j\\) squared standard error teacher \\(j\\) (function \\(n\\))Hence,\\[\n\\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{reliability} = \\text{true variance signal}\n\\] also known much noise \\(\\hat{\\tau}\\) \\[\n1 - \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{noise}\n\\]Even cases true relationship \\(\\tau\\) function \\(N_j\\), recovery method \\(\\lambda\\) still affectedTo examine assumption\\[\n\\hat{\\tau}_j = \\beta_0 + X_j \\beta_1 + \\epsilon_j\n\\]Regressing teacher fixed-effect teacher characteristics give us \\(R^2\\) close 0, teacher characteristics predict sampling error (\\(\\hat{\\tau}\\) contain sampling error)","code":""},{"path":"data.html","id":"tests-for-assumptions","chapter":"12 Data","heading":"12.4.3 Tests for Assumptions","text":"typically don’t test heteroskedasticity use robust covariance matrix estimation anyway.Dataset","code":"\nlibrary(\"plm\")\ndata(\"EmplUK\", package=\"plm\")\ndata(\"Produc\", package=\"plm\")\ndata(\"Grunfeld\", package=\"plm\")\ndata(\"Wages\", package=\"plm\")"},{"path":"data.html","id":"poolability","chapter":"12 Data","heading":"12.4.3.1 Poolability","text":"also known F test stability (Chow test) coefficients\\(H_0\\): individuals coefficients (.e., equal coefficients individuals).\\(H_a\\) Different individuals different coefficients.Notes:within (.e., fixed) model, different intercepts individual assumedUnder random model, intercept assumedHence, reject null hypothesis coefficients stable. , use random model.","code":"\nlibrary(plm)\nplm::pooltest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  F statistic\n#> \n#> data:  inv ~ value + capital\n#> F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10\n#> alternative hypothesis: unstability"},{"path":"data.html","id":"individual-and-time-effects","chapter":"12 Data","heading":"12.4.3.2 Individual and time effects","text":"use Lagrange multiplier test test presence individual time (.e., individual time).Types:honda: (Honda 1985) Defaultbp: (Breusch Pagan 1980) unbalanced panelskw: (M. L. King Wu 1997) unbalanced panels, two-way effectsghm: (Gourieroux, Holly, Monfort 1982): two-way effects","code":"\npFtest(inv~value+capital, data=Grunfeld, effect=\"twoways\")\n#> \n#>  F test for twoways effects\n#> \n#> data:  inv ~ value + capital\n#> F = 17.403, df1 = 28, df2 = 169, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"individual\")\n#> \n#>  F test for individual effects\n#> \n#> data:  inv ~ value + capital\n#> F = 49.177, df1 = 9, df2 = 188, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"time\")\n#> \n#>  F test for time effects\n#> \n#> data:  inv ~ value + capital\n#> F = 0.23451, df1 = 19, df2 = 178, p-value = 0.9997\n#> alternative hypothesis: significant effects"},{"path":"data.html","id":"cross-sectional-dependencecontemporaneous-correlation","chapter":"12 Data","heading":"12.4.3.3 Cross-sectional dependence/contemporaneous correlation","text":"Null hypothesis: residuals across entities correlated.","code":""},{"path":"data.html","id":"global-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.1 Global cross-sectional dependence","text":"","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  inv ~ value + capital\n#> z = 4.6612, p-value = 3.144e-06\n#> alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"local-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.2 Local cross-sectional dependence","text":"use command, supply matrix w argument.","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  inv ~ value + capital\n#> z = 4.6612, p-value = 3.144e-06\n#> alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"serial-correlation-1","chapter":"12 Data","heading":"12.4.3.4 Serial Correlation","text":"Null hypothesis: serial correlationNull hypothesis: serial correlationusually seen macro panels long time series (large N T), seen micro panels (small T large N)usually seen macro panels long time series (large N T), seen micro panels (small T large N)Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Can \nmarginal test: 1 two dependence (can biased towards rejection)\njoint test: dependencies (don’t know one causing problem)\nconditional test: assume correctly specify one dependence structure, test whether departure present.\nCan bemarginal test: 1 two dependence (can biased towards rejection)marginal test: 1 two dependence (can biased towards rejection)joint test: dependencies (don’t know one causing problem)joint test: dependencies (don’t know one causing problem)conditional test: assume correctly specify one dependence structure, test whether departure present.conditional test: assume correctly specify one dependence structure, test whether departure present.","code":""},{"path":"data.html","id":"unobserved-effect-test","chapter":"12 Data","heading":"12.4.3.4.1 Unobserved effect test","text":"semi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.\nnull, covariance matrix residuals = diagonal (-diagonal = 0)\nsemi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.null, covariance matrix residuals = diagonal (-diagonal = 0)robust unobserved effects constant within every group, kind serial correlation.robust unobserved effects constant within every group, kind serial correlation., reject null hypothesis unobserved effects residuals. Hence, exclude using pooled OLS.","code":"\npwtest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc)\n#> \n#>  Wooldridge's test for unobserved individual effects\n#> \n#> data:  formula\n#> z = 3.9383, p-value = 8.207e-05\n#> alternative hypothesis: unobserved effect"},{"path":"data.html","id":"locally-robust-tests-for-random-effects-and-serial-correlation","chapter":"12 Data","heading":"12.4.3.4.2 Locally robust tests for random effects and serial correlation","text":"joint LM test random effects serial correlation assuming normality homoskedasticity idiosyncratic errors [Baltagi Li (1991)](Baltagi Li 1995), reject null hypothesis presence serial correlation, random effects. still know whether serial correlation, random effects bothTo know departure null assumption, can use Bera, Sosa-Escudero, Yoon (2001)’s test first-order serial correlation random effects (normality homoskedasticity assumption error).BSY serial correlationBSY random effectsSince BSY locally robust, “know” serial correlation, test based LM test superior:hand, know random effects, test serial correlation, use (Breusch 1978)-(Godfrey 1978)’s testIf “know” random effects, use (Baltagi Li 1995)’s. test serial correlation AR(1) MA(1) processes.\\(H_0\\): Uncorrelated errors.Note:one-sided power positive serial correlation.applicable balanced panels.General serial correlation testsapplicable random effects model, OLS, FE (large T, also known long panel).can also test higher-order serial correlationin case short panels (small T large n), can use","code":"\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n         data = Produc,\n         test = \"j\")\n#> \n#>  Baltagi and Li AR-RE joint test\n#> \n#> data:  formula\n#> chisq = 4187.6, df = 2, p-value < 2.2e-16\n#> alternative hypothesis: AR(1) errors or random effects\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n         data = Produc)\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test\n#> \n#> data:  formula\n#> chisq = 52.636, df = 1, p-value = 4.015e-13\n#> alternative hypothesis: AR(1) errors sub random effects\npbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, \n         data=Produc, \n         test=\"re\")\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test (one-sided)\n#> \n#> data:  formula\n#> z = 57.914, p-value < 2.2e-16\n#> alternative hypothesis: random effects sub AR(1) errors\nplmtest(inv ~ value + capital, data = Grunfeld, \n        type = \"honda\")\n#> \n#>  Lagrange Multiplier Test - (Honda)\n#> \n#> data:  inv ~ value + capital\n#> normal = 28.252, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\nlmtest::bgtest()\npbltest(\n    log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n    data = Produc,\n    alternative = \"onesided\"\n)\n#> \n#>  Baltagi and Li one-sided LM test\n#> \n#> data:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\n#> z = 21.69, p-value < 2.2e-16\n#> alternative hypothesis: AR(1)/MA(1) errors in RE panel model\nplm::pbgtest(plm::plm(inv ~ value + capital,\n                      data = Grunfeld,\n                      model = \"within\"),\n             order = 2)\n#> \n#>  Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n#> \n#> data:  inv ~ value + capital\n#> chisq = 42.587, df = 2, p-value = 5.655e-10\n#> alternative hypothesis: serial correlation in idiosyncratic errors\npwartest(log(emp) ~ log(wage) + log(capital), data=EmplUK)\n#> \n#>  Wooldridge's test for serial correlation in FE panels\n#> \n#> data:  plm.model\n#> F = 312.3, df1 = 1, df2 = 889, p-value < 2.2e-16\n#> alternative hypothesis: serial correlation"},{"path":"data.html","id":"unit-rootsstationarity","chapter":"12 Data","heading":"12.4.3.5 Unit roots/stationarity","text":"Dickey-Fuller test stochastic trends.Null hypothesis: series non-stationary (unit root)want test less critical value (p<.5) evidence unit roots.","code":""},{"path":"data.html","id":"heteroskedasticity-1","chapter":"12 Data","heading":"12.4.3.6 Heteroskedasticity","text":"Breusch-Pagan testBreusch-Pagan testNull hypothesis: data homoskedasticNull hypothesis: data homoskedasticIf evidence heteroskedasticity, robust covariance matrix advised.evidence heteroskedasticity, robust covariance matrix advised.control heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)\n“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.\n“white2” - “white1” restricted common variance within groups. Recommended random effects.\n“arellano” - heteroskedasticity serial correlation. Recommended fixed effects\ncontrol heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.“white2” - “white1” restricted common variance within groups. Recommended random effects.“arellano” - heteroskedasticity serial correlation. Recommended fixed effects","code":""},{"path":"data.html","id":"model-selection","chapter":"12 Data","heading":"12.4.4 Model Selection","text":"","code":""},{"path":"data.html","id":"pols-vs.-re","chapter":"12 Data","heading":"12.4.4.1 POLS vs. RE","text":"continuum RE (used FGLS assumption ) POLS check back section FGLSBreusch-Pagan LM testTest random effect model based OLS residualNull hypothesis: variances across entities zero. another word, panel effect.test significant, RE preferable compared POLS","code":""},{"path":"data.html","id":"fe-vs.-re","chapter":"12 Data","heading":"12.4.4.2 FE vs. RE","text":"RE require strict exogeneity consistency (feedback effect residual covariates)Hausman TestFor Hausman test run, need assume thatstrict exogeneity holdA4 hold \\(u_{}\\),Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})'(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) \\(n(X)\\) number parameters time-varying regressors.low p-value means reject null hypothesis prefer FEA high p-value means reject null hypothesis consider RE estimator.Violation EstimatorBasic EstimatorInstrumental variable EstimatorVariable Coefficients estimatorGeneralized Method Moments estimatorGeneral FGLS estimatorMeans groups estimatorCCEMGEstimator limited dependent variables","code":"\ngw <- plm(inv ~ value + capital, data = Grunfeld, model = \"within\")\ngr <- plm(inv ~ value + capital, data = Grunfeld, model = \"random\")\nphtest(gw, gr)\n#> \n#>  Hausman Test\n#> \n#> data:  inv ~ value + capital\n#> chisq = 2.3304, df = 2, p-value = 0.3119\n#> alternative hypothesis: one model is inconsistent"},{"path":"data.html","id":"summary-2","chapter":"12 Data","heading":"12.4.5 Summary","text":"three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,POLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 hold, use cluster robust SE POLS efficient\nPOLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 hold, use cluster robust SE POLS efficientRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 (\\(u_{}\\)) holds usual SE valid RE efficient\nA4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)\nRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 (\\(u_{}\\)) holds usual SE valid RE efficientIf A4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)FE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)\nestimate effects time constant variables\nA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE needed\nFE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)estimate effects time constant variablesA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE neededNote: A5 individual (time dimension) implies [A5a] entire data set.Based table provided Ani Katchova","code":""},{"path":"data.html","id":"application-6","chapter":"12 Data","heading":"12.4.6 Application","text":"","code":""},{"path":"data.html","id":"plm-package","chapter":"12 Data","heading":"12.4.6.1 plm package","text":"Recommended application plm can found Yves CroissantAdvancedOther methods estimate random model:\"swar\": default (Swamy Arora 1972)\"walhus\": (Wallace Hussain 1969)\"amemiya\": (Amemiya 1971)\"nerlove\"” (Nerlove 1971)effects:Individual effects: defaultTime effects: \"time\"Individual time effects: \"twoways\"Note: random two-ways effect model random.method = \"nerlove\"call estimation variance error componentsCheck unbalancedness. Closer 1 indicates balanced data (Ahrens Pincus 1981)Instrumental variable\"bvk\": default (Balestra Varadharajan-Krishnakumar 1987)\"baltagi\": (Baltagi 1981)\"\" (Amemiya MaCurdy 1986)\"bms\": (Breusch, Mizon, Schmidt 1989)","code":"\n#install.packages(\"plm\")\nlibrary(\"plm\")\n\nlibrary(foreign)\nPanel <- read.dta(\"http://dss.princeton.edu/training/Panel101.dta\")\n\nattach(Panel)\nY <- cbind(y)\nX <- cbind(x1, x2, x3)\n\n# Set data as panel data\npdata <- pdata.frame(Panel, index = c(\"country\", \"year\"))\n\n# Pooled OLS estimator\npooling <- plm(Y ~ X, data = pdata, model = \"pooling\")\nsummary(pooling)\n\n# Between estimator\nbetween <- plm(Y ~ X, data = pdata, model = \"between\")\nsummary(between)\n\n# First differences estimator\nfirstdiff <- plm(Y ~ X, data = pdata, model = \"fd\")\nsummary(firstdiff)\n\n# Fixed effects or within estimator\nfixed <- plm(Y ~ X, data = pdata, model = \"within\")\nsummary(fixed)\n\n# Random effects estimator\nrandom <- plm(Y ~ X, data = pdata, model = \"random\")\nsummary(random)\n\n# LM test for random effects versus OLS\n# Accept Null, then OLS, Reject Null then RE\nplmtest(pooling, effect = \"individual\", type = c(\"bp\")) \n# other type: \"honda\", \"kw\",\" \"ghm\"; other effect : \"time\" \"twoways\"\n\n\n# B-P/LM and Pesaran CD (cross-sectional dependence) test\n# Breusch and Pagan's original LM statistic\npcdtest(fixed, test = c(\"lm\")) \n# Pesaran's CD statistic\npcdtest(fixed, test = c(\"cd\")) \n\n# Serial Correlation\npbgtest(fixed)\n\n# stationary\nlibrary(\"tseries\")\nadf.test(pdata$y, k = 2)\n\n# LM test for fixed effects versus OLS\npFtest(fixed, pooling)\n\n# Hausman test for fixed versus random effects model\nphtest(random, fixed)\n\n# Breusch-Pagan heteroskedasticity\nlibrary(lmtest)\nbptest(y ~ x1 + factor(country), data = pdata)\n\n# If there is presence of heteroskedasticity\n## For RE model\ncoeftest(random) #orginal coef\n\n# Heteroskedasticity consistent coefficients\ncoeftest(random, vcovHC) \n\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(\n        vcovHC(random, type = x)\n    )))) #show HC SE of the coef\n# HC0 - heteroskedasticity consistent. The default.\n# HC1,HC2, HC3 – Recommended for small samples. \n# HC3 gives less weight to influential observations.\n# HC4 - small samples with influential observations\n# HAC - heteroskedasticity and autocorrelation consistent\n\n## For FE model\ncoeftest(fixed) # Original coefficients\ncoeftest(fixed, vcovHC) # Heteroskedasticity consistent coefficients\n\n# Heteroskedasticity consistent coefficients (Arellano)\ncoeftest(fixed, vcovHC(fixed, method = \"arellano\")) \n\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(\n        vcovHC(fixed, type = x)\n    )))) #show HC SE of the coef\namemiya <-\n    plm(\n        Y ~ X,\n        data = pdata,\n        model = \"random\",\n        random.method = \"amemiya\",\n        effect = \"twoways\"\n    )\nercomp(Y ~ X,\n       data = pdata,\n       method = \"amemiya\",\n       effect = \"twoways\")\npunbalancedness(random)\ninstr <-\n    plm(\n        Y ~ X | X_ins,\n        data = pdata,\n        random.method = \"ht\",\n        model = \"random\",\n        inst.method = \"baltagi\"\n    )"},{"path":"data.html","id":"other-estimators","chapter":"12 Data","heading":"12.4.6.1.1 Other Estimators","text":"","code":""},{},{},{},{"path":"data.html","id":"fixest-package","chapter":"12 Data","heading":"12.4.6.2 fixest package","text":"Available functionsfeols: linear modelsfeols: linear modelsfeglm: generalized linear modelsfeglm: generalized linear modelsfemlm: maximum likelihood estimationfemlm: maximum likelihood estimationfeNmlm: non-linear RHS parametersfeNmlm: non-linear RHS parametersfepois: Poisson fixed-effectfepois: Poisson fixed-effectfenegbin: negative binomial fixed-effectfenegbin: negative binomial fixed-effectNotescan work fixest objectExamples package’s authorsFor multiple estimation","code":"\nlibrary(fixest)\ndata(airquality)\n\n# Setting a dictionary\nsetFixest_dict(\n    c(\n        Ozone   = \"Ozone (ppb)\",\n        Solar.R = \"Solar Radiation (Langleys)\",\n        Wind    = \"Wind Speed (mph)\",\n        Temp    = \"Temperature\"\n    )\n)\n\n\n# On multiple estimations: see the dedicated vignette\nest = feols(\n    Ozone ~ Solar.R + sw0(Wind + Temp) | csw(Month, Day),\n    data = airquality,\n    cluster = ~ Day\n)\n\netable(est)\n#>                                         est.1              est.2\n#> Dependent Var.:                   Ozone (ppb)        Ozone (ppb)\n#>                                                                 \n#> Solar Radiation (Langleys) 0.1148*** (0.0234)   0.0522* (0.0202)\n#> Wind Speed (mph)                              -3.109*** (0.7986)\n#> Temperature                                    1.875*** (0.3671)\n#> Fixed-Effects:             ------------------ ------------------\n#> Month                                     Yes                Yes\n#> Day                                        No                 No\n#> __________________________ __________________ __________________\n#> S.E.: Clustered                       by: Day            by: Day\n#> Observations                              111                111\n#> R2                                    0.31974            0.63686\n#> Within R2                             0.12245            0.53154\n#> \n#>                                        est.3              est.4\n#> Dependent Var.:                  Ozone (ppb)        Ozone (ppb)\n#>                                                                \n#> Solar Radiation (Langleys) 0.1078** (0.0329)   0.0509* (0.0236)\n#> Wind Speed (mph)                             -3.289*** (0.7777)\n#> Temperature                                   2.052*** (0.2415)\n#> Fixed-Effects:             ----------------- ------------------\n#> Month                                    Yes                Yes\n#> Day                                      Yes                Yes\n#> __________________________ _________________ __________________\n#> S.E.: Clustered                      by: Day            by: Day\n#> Observations                             111                111\n#> R2                                   0.58018            0.81604\n#> Within R2                            0.12074            0.61471\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# in latex\netable(est, tex = T)\n#> \\begingroup\n#> \\centering\n#> \\begin{tabular}{lcccc}\n#>    \\tabularnewline \\midrule \\midrule\n#>    Dependent Variable: & \\multicolumn{4}{c}{Ozone (ppb)}\\\\\n#>    Model:                     & (1)            & (2)            & (3)            & (4)\\\\  \n#>    \\midrule\n#>    \\emph{Variables}\\\\\n#>    Solar Radiation (Langleys) & 0.1148$^{***}$ & 0.0522$^{**}$  & 0.1078$^{***}$ & 0.0509$^{**}$\\\\   \n#>                               & (0.0234)       & (0.0202)       & (0.0329)       & (0.0236)\\\\   \n#>    Wind Speed (mph)           &                & -3.109$^{***}$ &                & -3.289$^{***}$\\\\   \n#>                               &                & (0.7986)       &                & (0.7777)\\\\   \n#>    Temperature                &                & 1.875$^{***}$  &                & 2.052$^{***}$\\\\   \n#>                               &                & (0.3671)       &                & (0.2415)\\\\   \n#>    \\midrule\n#>    \\emph{Fixed-effects}\\\\\n#>    Month                      & Yes            & Yes            & Yes            & Yes\\\\  \n#>    Day                        &                &                & Yes            & Yes\\\\  \n#>    \\midrule\n#>    \\emph{Fit statistics}\\\\\n#>    Observations               & 111            & 111            & 111            & 111\\\\  \n#>    R$^2$                      & 0.31974        & 0.63686        & 0.58018        & 0.81604\\\\  \n#>    Within R$^2$               & 0.12245        & 0.53154        & 0.12074        & 0.61471\\\\  \n#>    \\midrule \\midrule\n#>    \\multicolumn{5}{l}{\\emph{Clustered (Day) standard-errors in parentheses}}\\\\\n#>    \\multicolumn{5}{l}{\\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\\\\n#> \\end{tabular}\n#> \\par\\endgroup\n\n\n# get the fixed-effects coefficients for 1 model\nfixedEffects = fixef(est[[1]])\nsummary(fixedEffects)\n#> Fixed_effects coefficients\n#> Number of fixed-effects for variable Month is 5.\n#>  Mean = 19.6 Variance = 272\n#> \n#> COEFFICIENTS:\n#>   Month:     5     6     7     8     9\n#>          3.219 8.288 34.26 40.12 12.13\n\n# see the fixed effects for one dimension\nfixedEffects$Month\n#>         5         6         7         8         9 \n#>  3.218876  8.287899 34.260812 40.122257 12.130971\n\nplot(fixedEffects)\n# set up\nlibrary(fixest)\n\n# let R know the base dataset (the biggest/ultimate \n# dataset that includes everything in your analysis)\nbase = iris\n\n# rename variables\nnames(base) = c(\"y1\", \"y2\", \"x1\", \"x2\", \"species\")\n\nres_multi = feols(\n    c(y1, y2) ~ x1 + csw(x2, x2 ^ 2) |\n        sw0(species),\n    data = base,\n    fsplit = ~ species,\n    lean = TRUE,\n    vcov = \"hc1\" # can also clustered at the fixed effect level\n)\n# it's recommended to use vcov at \n# estimation stage, not summary stage\n\nsummary(res_multi, \"compact\")\n#>         sample   fixef lhs               rhs     (Intercept)                x1\n#> 1  Full sample 1        y1 x1 + x2           4.19*** (0.104)  0.542*** (0.076)\n#> 2  Full sample 1        y1 x1 + x2 + I(x2^2) 4.27*** (0.101)  0.719*** (0.082)\n#> 3  Full sample 1        y2 x1 + x2           3.59*** (0.103) -0.257*** (0.066)\n#> 4  Full sample 1        y2 x1 + x2 + I(x2^2) 3.68*** (0.097)    -0.030 (0.078)\n#> 5  Full sample species  y1 x1 + x2                            0.906*** (0.076)\n#> 6  Full sample species  y1 x1 + x2 + I(x2^2)                  0.900*** (0.077)\n#> 7  Full sample species  y2 x1 + x2                              0.155* (0.073)\n#> 8  Full sample species  y2 x1 + x2 + I(x2^2)                    0.148. (0.075)\n#> 9  setosa      1        y1 x1 + x2           4.25*** (0.474)     0.399 (0.325)\n#> 10 setosa      1        y1 x1 + x2 + I(x2^2) 4.00*** (0.504)     0.405 (0.325)\n#> 11 setosa      1        y2 x1 + x2           2.89*** (0.416)     0.247 (0.305)\n#> 12 setosa      1        y2 x1 + x2 + I(x2^2) 2.82*** (0.423)     0.248 (0.304)\n#> 13 setosa      species  y1 x1 + x2                               0.399 (0.325)\n#> 14 setosa      species  y1 x1 + x2 + I(x2^2)                     0.405 (0.325)\n#> 15 setosa      species  y2 x1 + x2                               0.247 (0.305)\n#> 16 setosa      species  y2 x1 + x2 + I(x2^2)                     0.248 (0.304)\n#> 17 versicolor  1        y1 x1 + x2           2.38*** (0.423)  0.934*** (0.166)\n#> 18 versicolor  1        y1 x1 + x2 + I(x2^2)   0.323 (1.44)   0.901*** (0.164)\n#> 19 versicolor  1        y2 x1 + x2           1.25*** (0.275)     0.067 (0.095)\n#> 20 versicolor  1        y2 x1 + x2 + I(x2^2)   0.097 (1.01)      0.048 (0.099)\n#> 21 versicolor  species  y1 x1 + x2                            0.934*** (0.166)\n#> 22 versicolor  species  y1 x1 + x2 + I(x2^2)                  0.901*** (0.164)\n#> 23 versicolor  species  y2 x1 + x2                               0.067 (0.095)\n#> 24 versicolor  species  y2 x1 + x2 + I(x2^2)                     0.048 (0.099)\n#> 25 virginica   1        y1 x1 + x2             1.05. (0.539)  0.995*** (0.090)\n#> 26 virginica   1        y1 x1 + x2 + I(x2^2)   -2.39 (2.04)   0.994*** (0.088)\n#> 27 virginica   1        y2 x1 + x2             1.06. (0.572)     0.149 (0.107)\n#> 28 virginica   1        y2 x1 + x2 + I(x2^2)    1.10 (1.76)      0.149 (0.108)\n#> 29 virginica   species  y1 x1 + x2                            0.995*** (0.090)\n#> 30 virginica   species  y1 x1 + x2 + I(x2^2)                  0.994*** (0.088)\n#> 31 virginica   species  y2 x1 + x2                               0.149 (0.107)\n#> 32 virginica   species  y2 x1 + x2 + I(x2^2)                     0.149 (0.108)\n#>                  x2          I(x2^2)\n#> 1   -0.320. (0.170)                 \n#> 2  -1.52*** (0.307) 0.348*** (0.075)\n#> 3    0.364* (0.142)                 \n#> 4  -1.18*** (0.313) 0.446*** (0.074)\n#> 5    -0.006 (0.163)                 \n#> 6     0.290 (0.408)   -0.088 (0.117)\n#> 7  0.623*** (0.114)                 \n#> 8    0.951* (0.472)   -0.097 (0.125)\n#> 9    0.712. (0.418)                 \n#> 10    2.51. (1.47)     -2.91 (2.10) \n#> 11    0.702 (0.560)                 \n#> 12     1.27 (2.39)    -0.911 (3.28) \n#> 13   0.712. (0.418)                 \n#> 14    2.51. (1.47)     -2.91 (2.10) \n#> 15    0.702 (0.560)                 \n#> 16     1.27 (2.39)    -0.911 (3.28) \n#> 17   -0.320 (0.364)                 \n#> 18     3.01 (2.31)     -1.24 (0.841)\n#> 19 0.929*** (0.244)                 \n#> 20    2.80. (1.65)    -0.695 (0.583)\n#> 21   -0.320 (0.364)                 \n#> 22     3.01 (2.31)     -1.24 (0.841)\n#> 23 0.929*** (0.244)                 \n#> 24    2.80. (1.65)    -0.695 (0.583)\n#> 25    0.007 (0.205)                 \n#> 26    3.50. (2.09)    -0.870 (0.519)\n#> 27 0.535*** (0.122)                 \n#> 28    0.503 (1.56)     0.008 (0.388)\n#> 29    0.007 (0.205)                 \n#> 30    3.50. (2.09)    -0.870 (0.519)\n#> 31 0.535*** (0.122)                 \n#> 32    0.503 (1.56)     0.008 (0.388)\n\n# call the first 3 estimated models only\netable(res_multi[1:3],\n       \n       # customize the headers\n       headers = c(\"mod1\", \"mod2\", \"mod3\")) \n#>                   res_multi[1:3].1   res_multi[1:3].2    res_multi[1:3].3\n#>                               mod1               mod2                mod3\n#> Dependent Var.:                 y1                 y1                  y2\n#>                                                                          \n#> Constant         4.191*** (0.1037)  4.266*** (0.1007)   3.587*** (0.1031)\n#> x1              0.5418*** (0.0761) 0.7189*** (0.0815) -0.2571*** (0.0664)\n#> x2               -0.3196. (0.1700) -1.522*** (0.3072)    0.3640* (0.1419)\n#> x2 square                          0.3479*** (0.0748)                    \n#> _______________ __________________ __________________ ___________________\n#> S.E. type       Heteroskedas.-rob. Heteroskedas.-rob. Heteroskedast.-rob.\n#> Observations                   150                150                 150\n#> R2                         0.76626            0.79456             0.21310\n#> Adj. R2                    0.76308            0.79034             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"multiple-estimation-left-hand-side","chapter":"12 Data","heading":"12.4.6.2.1 Multiple estimation (Left-hand side)","text":"multiple interested dependent variablesTo input list dependent variable","code":"\netable(feols(c(y1, y2) ~ x1 + x2, base))\n#>                 feols(c(y1, y2)..1 feols(c(y1, y2) ..2\n#> Dependent Var.:                 y1                  y2\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> x1              0.5418*** (0.0693) -0.2571*** (0.0669)\n#> x2               -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndepvars <- c(\"y1\", \"y2\")\n\nres <- lapply(depvars, function(var) {\n    res <- feols(xpd(..lhs ~ x1 + x2, ..lhs = var), data = base)\n    # summary(res)\n})\netable(res)\n#>                            model 1             model 2\n#> Dependent Var.:                 y1                  y2\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> x1              0.5418*** (0.0693) -0.2571*** (0.0669)\n#> x2               -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"multiple-estimation-right-hand-side","chapter":"12 Data","heading":"12.4.6.2.2 Multiple estimation (Right-hand side)","text":"Options write functionssw (stepwise): sequentially analyze elements\ny ~ sw(x1, x2) estimated y ~ x1 y ~ x2\nsw (stepwise): sequentially analyze elementsy ~ sw(x1, x2) estimated y ~ x1 y ~ x2sw0 (stepwise 0): similar sw also estimate model without elements set first\ny ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2\nsw0 (stepwise 0): similar sw also estimate model without elements set firsty ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2csw (cumulative stepwise): sequentially add element set formula\ny ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2\ncsw (cumulative stepwise): sequentially add element set formulay ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2csw0 (cumulative stepwise 0): similar csw also estimate model without elements set first\ny ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2\ncsw0 (cumulative stepwise 0): similar csw also estimate model without elements set firsty ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2mvsw (multiverse stepwise): possible combination elements set (get large quick).\nmvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)\nmvsw (multiverse stepwise): possible combination elements set (get large quick).mvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)","code":""},{"path":"data.html","id":"split-sample-estimation","chapter":"12 Data","heading":"12.4.6.2.3 Split sample estimation","text":"","code":"\netable(feols(y1 ~ x1 + x2, fsplit = ~ species, data = base))\n#>                  feols(y1 ~ x1 +..1 feols(y1 ~ x1 ..2 feols(y1 ~ x1 +..3\n#> Sample (species)        Full sample            setosa         versicolor\n#> Dependent Var.:                  y1                y1                 y1\n#>                                                                         \n#> Constant          4.191*** (0.0970) 4.248*** (0.4114)  2.381*** (0.4493)\n#> x1               0.5418*** (0.0693)   0.3990 (0.2958) 0.9342*** (0.1693)\n#> x2                -0.3196* (0.1605)   0.7121 (0.4874)   -0.3200 (0.4024)\n#> ________________ __________________ _________________ __________________\n#> S.E. type                       IID               IID                IID\n#> Observations                    150                50                 50\n#> R2                          0.76626           0.11173            0.57432\n#> Adj. R2                     0.76308           0.07393            0.55620\n#> \n#>                  feols(y1 ~ x1 +..4\n#> Sample (species)          virginica\n#> Dependent Var.:                  y1\n#>                                    \n#> Constant            1.052* (0.5139)\n#> x1               0.9946*** (0.0893)\n#> x2                  0.0071 (0.1795)\n#> ________________ __________________\n#> S.E. type                       IID\n#> Observations                     50\n#> R2                          0.74689\n#> Adj. R2                     0.73612\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"standard-errors-1","chapter":"12 Data","heading":"12.4.6.2.4 Standard Errors","text":"iid: errors homoskedastic independent identically distributediid: errors homoskedastic independent identically distributedhetero: errors heteroskedastic using White correctionhetero: errors heteroskedastic using White correctioncluster: errors correlated within cluster groupscluster: errors correlated within cluster groupsnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.\nvcov = newey_west ~ id + period id subject id period time period panel.\nspecify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.\nnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.vcov = newey_west ~ id + period id subject id period time period panel.vcov = newey_west ~ id + period id subject id period time period panel.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.driscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.\nvcov = discoll_kraay ~ period\ndriscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.vcov = discoll_kraay ~ periodconley: (Conley 1999) cross-section data. Errors spatially correlated\nvcov = conley ~ latitude + longitude\nspecify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.\nconley: (Conley 1999) cross-section data. Errors spatially correlatedvcov = conley ~ latitude + longitudevcov = conley ~ latitude + longitudeto specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.hc: sandwich package\nvcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))\nhc: sandwich packagevcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))let R know SE estimation want use, insert vcov = vcov_type ~ variables","code":""},{"path":"data.html","id":"small-sample-correction","chapter":"12 Data","heading":"12.4.6.2.5 Small sample correction","text":"specify R needs use small sample correction addssc = ssc(adj = T, cluster.adj = T)","code":""},{"path":"variable-transformation.html","id":"variable-transformation","chapter":"13 Variable Transformation","heading":"13 Variable Transformation","text":"trafo vignette","code":""},{"path":"variable-transformation.html","id":"continuous-variables","chapter":"13 Variable Transformation","heading":"13.1 Continuous Variables","text":"Purposes:change scale variablesTo change scale variablesTo transform skewed data distribution normal distributionTo transform skewed data distribution normal distribution","code":""},{"path":"variable-transformation.html","id":"standardization","chapter":"13 Variable Transformation","heading":"13.1.1 Standardization","text":"\\[\nx_i' = \\frac{x_i - \\bar{x}}{s}\n\\]large numbers","code":""},{"path":"variable-transformation.html","id":"min-max-scaling","chapter":"13 Variable Transformation","heading":"13.1.2 Min-max scaling","text":"\\[\nx_i' = \\frac{x_i - x_{max}}{x_{max} - x_{min}}\n\\]dependent min max values, makes sensitive outliers.best use values fixed interval.","code":""},{"path":"variable-transformation.html","id":"square-rootcube-root","chapter":"13 Variable Transformation","heading":"13.1.3 Square Root/Cube Root","text":"variables positive skewness residuals positive heteroskasticity.variables positive skewness residuals positive heteroskasticity.Frequency counts variableFrequency counts variableData many 0 extremely small values.Data many 0 extremely small values.","code":""},{"path":"variable-transformation.html","id":"logarithmic","chapter":"13 Variable Transformation","heading":"13.1.4 Logarithmic","text":"Variables positively skewed distributionFor general case \\(\\log(x_i + c)\\), choosing constant rather tricky.choice constant critically important, especially want inference. can dramatically change model fit (see (Ekwaru Veugelers 2018) independent variable case).J. Chen Roth (2023) show causal inference problem, \\(\\log\\) transformation values meaningful 0 problematic. solutions approach (e.g., , IV).However, assuming 0s ofCensoringCensoringNo measurement errors (stemming measurement tools)measurement errors (stemming measurement tools)can proceed choosing c (’s okay 0’s represent really small values).","code":"\ndata(cars)\ncars$speed %>% head()\n#> [1] 4 4 7 7 8 9\n\nlog(cars$speed) %>% head()\n#> [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225\n\n# log(x+1)\nlog1p(cars$speed) %>% head()\n#> [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585"},{"path":"variable-transformation.html","id":"exponential","chapter":"13 Variable Transformation","heading":"13.1.5 Exponential","text":"Negatively skewed dataNegatively skewed dataUnderlying logarithmic trend (e.g., survival, decay)Underlying logarithmic trend (e.g., survival, decay)","code":""},{"path":"variable-transformation.html","id":"power-1","chapter":"13 Variable Transformation","heading":"13.1.6 Power","text":"Variables negatively skewed distribution","code":""},{"path":"variable-transformation.html","id":"inversereciprocal","chapter":"13 Variable Transformation","heading":"13.1.7 Inverse/Reciprocal","text":"Variables platykurtic distributionVariables platykurtic distributionData positively skewedData positively skewedRatio dataRatio data","code":"\ndata(cars)\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\nplot(cars$dist)\nplot(1/(cars$dist))"},{"path":"variable-transformation.html","id":"hyperbolic-arcsine","chapter":"13 Variable Transformation","heading":"13.1.8 Hyperbolic arcsine","text":"Variables positively skewed distribution","code":""},{"path":"variable-transformation.html","id":"ordered-quantile-norm","chapter":"13 Variable Transformation","heading":"13.1.9 Ordered Quantile Norm","text":"(Bartlett 1947)\\[\nx_i' = \\Phi^{-1} (\\frac{rank(x_i) - 1/2}{length(x)})\n\\]","code":"\nord_dist <- bestNormalize::orderNorm(cars$dist)\nord_dist\n#> orderNorm Transformation with 50 nonmissing obs and ties\n#>  - 35 unique values \n#>  - Original quantiles:\n#>   0%  25%  50%  75% 100% \n#>    2   26   36   56  120\nord_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"arcsinh","chapter":"13 Variable Transformation","heading":"13.1.10 Arcsinh","text":"Proportion variable (0-1)\\[\narcsinh(Y) = \\log(\\sqrt{1 + Y^2} + Y)\n\\]simple regression model, \\(Y = \\beta X\\)\\(Y\\) \\(X\\) transformed, coefficient estimate represents elasticity, indicating percentage change \\(Y\\) 1% change \\(X\\).\\(Y\\) transformed \\(X\\) raw form, coefficient estimate represents percentage change \\(Y\\) one-unit change \\(X\\).","code":"\ncars$dist %>% hist()\n# cars$dist %>% MASS::truehist()\n\nas_dist <- bestNormalize::arcsinh_x(cars$dist)\nas_dist\n#> Standardized asinh(x) Transformation with 50 nonmissing obs.:\n#>  Relevant statistics:\n#>  - mean (before standardization) = 4.230843 \n#>  - sd (before standardization) = 0.7710887\nas_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"lambert-w-x-f-transformation","chapter":"13 Variable Transformation","heading":"13.1.11 Lambert W x F Transformation","text":"LambertW packageUsing moments normalize data.Using moments normalize data.Usually need compare Box-Cox Transformation Yeo-Johnson TransformationUsually need compare Box-Cox Transformation Yeo-Johnson TransformationCan handle skewness, heavy-tailed.Can handle skewness, heavy-tailed.","code":"\ndata(cars)\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\ncars$dist %>% hist()\n\n\nl_dist <- LambertW::Gaussianize(cars$dist)\n# small fix\nl_dist %>% hist()"},{"path":"variable-transformation.html","id":"inverse-hyperbolic-sine-ihs-transformation","chapter":"13 Variable Transformation","heading":"13.1.12 Inverse Hyperbolic Sine (IHS) transformation","text":"Proposed (N. L. Johnson 1949)Proposed (N. L. Johnson 1949)Can applied real numbers.Can applied real numbers.\\[\n\\begin{aligned}\nf(x,\\theta) &= \\frac{\\sinh^{-1} (\\theta x)}{\\theta} \\\\\n&= \\frac{\\log(\\theta x + (\\theta^2 x^2 + 1)^{1/2})}{\\theta}\n\\end{aligned}\n\\]","code":""},{"path":"variable-transformation.html","id":"box-cox-transformation","chapter":"13 Variable Transformation","heading":"13.1.13 Box-Cox Transformation","text":"\\[\ny^\\lambda = \\beta x+ \\epsilon\n\\]fix non-linearity error termswork well (-3,3) (.e., small transformation).independent variables\\[\nx_i'^\\lambda =\n\\begin{cases}\n\\frac{x_i^\\lambda-1}{\\lambda} & \\text{} \\lambda \\neq 0\\\\\n\\log(x_i) & \\text{} \\lambda = 0\n\\end{cases}\n\\]two-parameter version \\[\nx_i' (\\lambda_1, \\lambda_2) =\n\\begin{cases}\n\\frac{(x_i + \\lambda_2)^{\\lambda_1}-1}{} & \\text{} \\lambda_1 \\neq 0 \\\\\n\\log(x_i + \\lambda_2) & \\text{} \\lambda_1 = 0\n\\end{cases}\n\\]advances(Manly 1976)(Manly 1976)(Bickel Doksum 1981; Box Cox 1981)(Bickel Doksum 1981; Box Cox 1981)","code":"\nlibrary(MASS)\ndata(cars)\nmod <- lm(cars$speed ~ cars$dist, data = cars)\n# check residuals\nplot(mod)\n\nbc <- boxcox(mod, lambda = seq(-3, 3))\n\n# best lambda\nbc$x[which(bc$y == max(bc$y))]\n#> [1] 1.242424\n\n# model with best lambda\nmod_lambda = lm(cars$speed ^ (bc$x[which(bc$y == max(bc$y))]) ~ cars$dist, \n                data = cars)\nplot(mod_lambda)\n\n# 2-parameter version\ntwo_bc = geoR::boxcoxfit(cars$speed)\ntwo_bc\n#> Fitted parameters:\n#>    lambda      beta   sigmasq \n#>  1.028798 15.253008 31.935297 \n#> \n#> Convergence code returned by optim: 0\nplot(two_bc)\n\n\n# bestNormalize\nbc_dist <- bestNormalize::boxcox(cars$dist)\nbc_dist\n#> Standardized Box Cox Transformation with 50 nonmissing obs.:\n#>  Estimated statistics:\n#>  - lambda = 0.4950628 \n#>  - mean (before standardization) = 10.35636 \n#>  - sd (before standardization) = 3.978036\nbc_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"yeo-johnson-transformation","chapter":"13 Variable Transformation","heading":"13.1.14 Yeo-Johnson Transformation","text":"Similar Box-Cox Transformation (\\(\\lambda = 1\\)), allows negative value\\[\nx_i'^\\lambda =\n\\begin{cases}\n\\frac{(x_i+1)^\\lambda -1}{\\lambda} & \\text{} \\lambda \\neq0, x_i \\ge 0 \\\\\n\\log(x_i + 1) & \\text{} \\lambda = 0, x_i \\ge 0 \\\\\n\\frac{-[(-x_i+1)^{2-\\lambda}-1]}{2 - \\lambda} & \\text{} \\lambda \\neq 2, x_i <0 \\\\\n-\\log(-x_i + 1) & \\text{} \\lambda = 2, x_i <0\n\\end{cases}\n\\]","code":"\ndata(cars)\nyj_speed <- bestNormalize::yeojohnson(cars$speed)\nyj_speed$x.t %>% hist()"},{"path":"variable-transformation.html","id":"rankgauss","chapter":"13 Variable Transformation","heading":"13.1.15 RankGauss","text":"Turn values ranks, ranks values normal distribution.","code":""},{"path":"variable-transformation.html","id":"summary-3","chapter":"13 Variable Transformation","heading":"13.1.16 Summary","text":"Automatically choose best method normalize data (code bestNormalize)","code":"\nbestdist <- bestNormalize::bestNormalize(cars$dist)\nbestdist$x.t %>% hist()\n\nboxplot(log10(bestdist$oos_preds), yaxt = \"n\")\n# axis(2, at = log10(c(.1, .5, 1, 2, 5, 10)), \n#      labels = c(.1, .5, 1, 2, 5, 10))"},{"path":"variable-transformation.html","id":"categorical-variables","chapter":"13 Variable Transformation","heading":"13.2 Categorical Variables","text":"PurposesTo transform continuous variable (machine learning models) (e.g., encoding/ embedding text mining)Approaches:One-hot encodingOne-hot encodingLabel encodingLabel encodingFeature hashingFeature hashingBinary encodingBinary encodingBase N encodingBase N encodingFrequency encodingFrequency encodingTarget encodingTarget encodingOrdinal encodingOrdinal encodingHelmert encodingHelmert encodingMean encodingMean encodingWeight evidence encodingWeight evidence encodingProbability ratio encodingProbability ratio encodingBackward difference encodingBackward difference encodingLeave one encodingLeave one encodingJames-Stein encodingJames-Stein encodingM-estimator encodingM-estimator encodingThermometer encodingThermometer encoding","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"14 Hypothesis Testing","heading":"14 Hypothesis Testing","text":"Error types:Type Error (False Positive):\nReality: nope\nDiagnosis/Analysis: yes\nType Error (False Positive):Reality: nopeDiagnosis/Analysis: yesType II Error (False Negative):\nReality: yes\nDiagnosis/Analysis: nope\nType II Error (False Negative):Reality: yesDiagnosis/Analysis: nopePower: probability rejecting null hypothesis actually falseNote:Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Sometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\n\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\n\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\nSometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]Assuming null hypothesis true, (asymptotic) distribution estimatorAssuming null hypothesis true, (asymptotic) distribution estimatorTwo-sidedTwo-sided\\[\n\\begin{aligned}\n&H_0: \\beta_j = 0 \\\\\n&H_1: \\beta_j \\neq 0\n\\end{aligned}\n\\]null, OLS estimator following distribution\\[\nA1-A3a, A5: \\sqrt{n} \\hat{\\beta_j}  \\sim  N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]one-sided test, null set values, now choose worst case single value hardest prove derive distribution nullOne-sided\\[\n\\begin{aligned}\n&H_0: \\beta_j\\ge 0 \\\\\n&H_1: \\beta_j < 0\n\\end{aligned}\n\\]hardest null value prove \\(H_0: \\beta_j=0\\). specific null, OLS estimator following asymptotic distribution\\[\nA1-A3a, A5: \\sqrt{n}\\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]","code":""},{"path":"hypothesis-testing.html","id":"types-of-hypothesis-testing","chapter":"14 Hypothesis Testing","heading":"14.1 Types of hypothesis testing","text":"\\(H_0 : \\theta = \\theta_0\\)\\(H_1 : \\theta \\neq \\theta_0\\)far away / extreme \\(\\theta\\) can null hypothesis trueAssume likelihood function q \\(L(q) = q^{30}(1-q)^{70}\\) Likelihood functionLog-Likelihood functionFigure (Fox 1997)typically, likelihood ratio test (Lagrange Multiplier (Score)) performs better small moderate sample sizes, Wald test requires one maximization (full model).","code":"\nq = seq(0, 1, length = 100)\nL = function(q) {\n    q ^ 30 * (1 - q) ^ 70\n}\n\nplot(q,\n     L(q),\n     ylab = \"L(q)\",\n     xlab = \"q\",\n     type = \"l\")\nq = seq(0, 1, length = 100)\nl = function(q) {\n    30 * log(q) + 70 * log(1 - q)\n}\nplot(q,\n     l(q) - l(0.3),\n     ylab = \"l(q) - l(qhat)\",\n     xlab = \"q\",\n     type = \"l\")\nabline(v = 0.2)"},{"path":"hypothesis-testing.html","id":"wald-test","chapter":"14 Hypothesis Testing","heading":"14.2 Wald test","text":"\\[\n\\begin{aligned}\nW &= (\\hat{\\theta}-\\theta_0)'[cov(\\hat{\\theta})]^{-1}(\\hat{\\theta}-\\theta_0) \\\\\nW &\\sim \\chi_q^2\n\\end{aligned}\n\\]\\(cov(\\hat{\\theta})\\) given inverse Fisher Information matrix evaluated \\(\\hat{\\theta}\\) q rank \\(cov(\\hat{\\theta})\\), number non-redundant parameters \\(\\theta\\)Alternatively,\\[\nt_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{(\\theta_0)^{-1}} \\sim \\chi^2_{(v)}\n\\]v degree freedom.Equivalently,\\[\ns_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{(\\hat{\\theta})^{-1}}} \\sim Z\n\\]far away distribution sample estimate hypothesized population parameter.null value, probability obtained realization “extreme” “worse” estimate actually obtained?Significance Level (\\(\\alpha\\)) Confidence Level (\\(1-\\alpha\\))significance level benchmark probability low reject nullThe confidence level probability sets bounds far away realization estimator reject null.Test StatisticsStandardized (transform) estimator null value test statistic always distributionTest Statistic OLS estimator single hypothesis\\[\nT = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]Equivalently,\\[\nT = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]test statistic another random variable function data null hypothesis.T denotes random variable test statistict denotes single realization test statisticEvaluating Test Statistic: determine whether reject fail reject null hypothesis given significance / confidence levelThree equivalent waysCritical ValueCritical ValueP-valueP-valueConfidence IntervalConfidence IntervalCritical ValueCritical ValueFor given significance level, determine critical value \\((c)\\)One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\nP(T<c|H_0)=\\alpha\n\\]Reject null \\(t<c\\)One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\nP(T>c|H_0)=\\alpha\n\\]Reject null \\(t>c\\)Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\nP(|T|>c|H_0)=\\alpha\n\\]Reject null \\(|t|>c\\)p-valueCalculate probability test statistic worse realization haveOne-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\n\\text{p-value} = P(T<t|H_0)\n\\]One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\n\\text{p-value} = P(T>t|H_0)\n\\]Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\n\\text{p-value} = P(|T|<t|H_0)\n\\]reject null p-value \\(< \\alpha\\)Confidence IntervalUsing critical value associated null hypothesis significance level, create interval\\[\nCI(\\hat{\\beta}_j)_{\\alpha} = [\\hat{\\beta}_j-(c \\times SE(\\hat{\\beta}_j)),\\hat{\\beta}_j+(c \\times SE(\\hat{\\beta}_j))]\n\\]null set lies outside interval reject null.testing whether true population value close estimate, testing given field true population value parameter, like observed estimate.Can interpreted believe \\((1-\\alpha)\\times 100 \\%\\) probability confidence interval captures true parameter value.stronger assumption (A1-A6), consider Finite Sample Properties\\[\nT = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k)\n\\]distributional derivation strongly dependent A4 A5T student t-distribution numerator normal denominator \\(\\chi^2\\).Critical value p-values calculated student t-distribution rather standard normal distribution.\\(n \\\\infty\\), \\(T(n-k)\\) asymptotically standard normal.Rule thumbif \\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k<120\\)\n(A1-A6) hold t-test exact finite distribution test\n(A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))\n\\(n-k<120\\)(A1-A6) hold t-test exact finite distribution testif (A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))","code":""},{"path":"hypothesis-testing.html","id":"multiple-hypothesis","chapter":"14 Hypothesis Testing","heading":"14.2.1 Multiple Hypothesis","text":"test multiple parameters time\n\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\n\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)\ntest multiple parameters time\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).test statistic based restriction written matrix form.test statistic based restriction written matrix form.\\[\ny=\\beta_0+x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon\n\\]Null hypothesis \\(H_0: \\beta_1 = 0\\) & \\(\\beta_2=0\\) can rewritten \\(H_0: \\mathbf{R}\\beta -\\mathbf{q}=0\\) \\(\\mathbf{R}\\) \\(m \\times k\\) matrix m number restrictions \\(k\\) number parameters. \\(\\mathbf{q}\\) \\(k \\times 1\\) vector\\(\\mathbf{R}\\) “picks ” relevant parameters \\(\\mathbf{q}\\) null value parameter\\[\n\\mathbf{R}=\n\\left(\n\\begin{array}{cccc}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right),\n\\mathbf{q} =\n\\left(\n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right)\n\\]Test Statistic OLS estimator multiple hypothesis\\[\nF = \\frac{(\\mathbf{R\\hat{\\beta}-q})\\hat{\\Sigma}^{-1}(\\mathbf{R\\hat{\\beta}-q})}{m} \\sim^F(m,n-k)\n\\]\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrix\nA4 holds, homoskedastic heteroskedastic versions produce valid estimator\nA4 hold, heteroskedastic version produces valid estimators.\n\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrixif A4 holds, homoskedastic heteroskedastic versions produce valid estimatorIf A4 hold, heteroskedastic version produces valid estimators.\\(m = 1\\), single restriction, \\(F\\)-statistic \\(t\\)-statistic squared.\\(m = 1\\), single restriction, \\(F\\)-statistic \\(t\\)-statistic squared.\\(F\\) distribution strictly positive, check [F-Distribution] details.\\(F\\) distribution strictly positive, check [F-Distribution] details.","code":""},{"path":"hypothesis-testing.html","id":"linear-combination","chapter":"14 Hypothesis Testing","heading":"14.2.2 Linear Combination","text":"Testing multiple parameters time\\[\n\\begin{aligned}\nH_0&: \\beta_1 -\\beta_2 = 0 \\\\\nH_0&: \\beta_1 - \\beta_2 > 0 \\\\\nH_0&: \\beta_1 - 2\\times\\beta_2 =0\n\\end{aligned}\n\\]single restriction function parameters.Null hypothesis:\\[\nH_0: \\beta_1 -\\beta_2 = 0\n\\]can rewritten \\[\nH_0: \\mathbf{R}\\beta -\\mathbf{q}=0\n\\]\\(\\mathbf{R}\\)=(0 1 -1 0 0) \\(\\mathbf{q}=0\\)","code":""},{"path":"hypothesis-testing.html","id":"estimate-difference-in-coefficients","chapter":"14 Hypothesis Testing","heading":"14.2.3 Estimate Difference in Coefficients","text":"package estimate difference two coefficients CI, simple function created Katherine Zee can used calculate difference. modifications might needed don’t use standard lm model R.","code":"\ndifftest_lm <- function(x1, x2, model) {\n    diffest <-\n        summary(model)$coef[x1, \"Estimate\"] - summary(model)$coef[x2, \"Estimate\"]\n    \n    vardiff <- (summary(model)$coef[x1, \"Std. Error\"] ^ 2 +\n                    summary(model)$coef[x2, \"Std. Error\"] ^ 2) - (2 * (vcov(model)[x1, x2]))\n    # variance of x1 + variance of x2 - 2*covariance of x1 and x2\n    diffse <- sqrt(vardiff)\n    tdiff <- (diffest) / (diffse)\n    ptdiff <- 2 * (1 - pt(abs(tdiff), model$df, lower.tail = T))\n    upr <-\n        # will usually be very close to 1.96\n        diffest + qt(.975, df = model$df) * diffse \n    lwr <- diffest + qt(.025, df = model$df) * diffse\n    df <- model$df\n    return(list(\n        est = round(diffest, digits = 2),\n        t = round(tdiff, digits = 2),\n        p = round(ptdiff, digits = 4),\n        lwr = round(lwr, digits = 2),\n        upr = round(upr, digits = 2),\n        df = df\n    ))\n}"},{"path":"hypothesis-testing.html","id":"application-7","chapter":"14 Hypothesis Testing","heading":"14.2.4 Application","text":"","code":"\nlibrary(\"car\")\n\n# Multiple hypothesis\nmod.davis <- lm(weight ~ repwt, data=Davis)\nlinearHypothesis(mod.davis, c(\"(Intercept) = 0\", \"repwt = 1\"),white.adjust = TRUE)\n#> Linear hypothesis test\n#> \n#> Hypothesis:\n#> (Intercept) = 0\n#> repwt = 1\n#> \n#> Model 1: restricted model\n#> Model 2: weight ~ repwt\n#> \n#> Note: Coefficient covariance matrix supplied.\n#> \n#>   Res.Df Df      F  Pr(>F)  \n#> 1    183                    \n#> 2    181  2 3.3896 0.03588 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Linear Combination\nmod.duncan <- lm(prestige ~ income + education, data=Duncan)\nlinearHypothesis(mod.duncan, \"1*income - 1*education = 0\")\n#> Linear hypothesis test\n#> \n#> Hypothesis:\n#> income - education = 0\n#> \n#> Model 1: restricted model\n#> Model 2: prestige ~ income + education\n#> \n#>   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n#> 1     43 7518.9                           \n#> 2     42 7506.7  1    12.195 0.0682 0.7952"},{"path":"hypothesis-testing.html","id":"nonlinear","chapter":"14 Hypothesis Testing","heading":"14.2.5 Nonlinear","text":"Suppose q nonlinear functions parameters\\[\n\\mathbf{h}(\\theta) = \\{ h_1 (\\theta), ..., h_q (\\theta)\\}'\n\\],n, Jacobian matrix (\\(\\mathbf{H}(\\theta)\\)), rank q \\[\n\\mathbf{H}_{q \\times p}(\\theta) =\n\\left(\n\\begin{array}\n{ccc}\n\\frac{\\partial h_1(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_1(\\theta)}{\\partial \\theta_p} \\\\\n. & . & . \\\\\n\\frac{\\partial h_q(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_q(\\theta)}{\\partial \\theta_p}\n\\end{array}\n\\right)\n\\]null hypothesis \\(H_0: \\mathbf{h} (\\theta) = 0\\) can tested 2-sided alternative Wald statistic\\[\nW = \\frac{\\mathbf{h(\\hat{\\theta})'\\{H(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}H(\\hat{\\theta})'\\}^{-1}h(\\hat{\\theta})}}{s^2q} \\sim F_{q,n-p}\n\\]","code":""},{"path":"hypothesis-testing.html","id":"the-likelihood-ratio-test","chapter":"14 Hypothesis Testing","heading":"14.3 The likelihood ratio test","text":"\\[\nt_{LR} = 2[l(\\hat{\\theta})-l(\\theta_0)] \\sim \\chi^2_v\n\\]v degree freedom.Compare height log-likelihood sample estimate relation height log-likelihood hypothesized population parameterAlternatively,test considers ratio two maximizations,\\[\n\\begin{aligned}\nL_r &= \\text{maximized value likelihood $H_0$ (reduced model)} \\\\\nL_f &= \\text{maximized value likelihood $H_0 \\cup H_a$ (full model)}\n\\end{aligned}\n\\], likelihood ratio :\\[\n\\Lambda = \\frac{L_r}{L_f}\n\\]can’t exceed 1 (since \\(L_f\\) always least large \\(L-r\\) \\(L_r\\) result maximization restricted set parameter values).likelihood ratio statistic :\\[\n\\begin{aligned}\n-2ln(\\Lambda) &= -2ln(L_r/L_f) = -2(l_r - l_f) \\\\\n\\lim_{n \\\\infty}(-2ln(\\Lambda)) &\\sim \\chi^2_v\n\\end{aligned}\n\\]\\(v\\) number parameters full model minus number parameters reduced model.\\(L_r\\) much smaller \\(L_f\\) (likelihood ratio exceeds \\(\\chi_{\\alpha,v}^2\\)), reject reduced model accept full model \\(\\alpha \\times 100 \\%\\) significance level","code":""},{"path":"hypothesis-testing.html","id":"lagrange-multiplier-score","chapter":"14 Hypothesis Testing","heading":"14.4 Lagrange Multiplier (Score)","text":"\\[\nt_S= \\frac{S(\\theta_0)^2}{(\\theta_0)} \\sim \\chi^2_v\n\\]\\(v\\) degree freedom.Compare slope log-likelihood sample estimate relation slope log-likelihood hypothesized population parameter","code":""},{"path":"hypothesis-testing.html","id":"two-one-sided-tests-tost-equivalence-testing","chapter":"14 Hypothesis Testing","heading":"14.5 Two One-Sided Tests (TOST) Equivalence Testing","text":"good way test whether population effect size within range practical interest (e.g., effect size 0).","code":"\nlibrary(TOSTER)"},{"path":"marginal-effects.html","id":"marginal-effects","chapter":"15 Marginal Effects","heading":"15 Marginal Effects","text":"cases without polynomials interactions, can easy interpret marginal effect.example,\\[\nY = \\beta_1 X_1 + \\beta_2 X_2\n\\]\\(\\beta\\) marginal effects.Numerical derivation easier analytical derivation.need choose values variables calculate marginal effect \\(X\\)Analytical derivation\\[\nf'(x) \\equiv \\lim_{h \\0} \\frac{f(x+h) - f(x)}{h}\n\\]E.g., \\(f(x) = X^2\\)\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h} \\\\\n&= \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\\n&= \\frac{2xh + h^2}{h} \\\\\n&= 2x + h \\\\\n&= 2x\n\\end{aligned}\n\\]numerically approach, “just” need find small \\(h\\) plug function. However, also need large enough \\(h\\) numerically accurate computation (Gould, Pitblado, Poi 2010, chap. 1)Numerically approachOne-sided derivative\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h}  \\\\\n& \\approx \\frac{f(x+h) -f(x)}{h}\n\\end{aligned}\n\\]Alternatively, two-sided derivative\\[\nf'_2(x) \\approx \\frac{f(x+h) - f(x- h)}{2h}\n\\]Marginal effects fordiscrete variables (also known incremental effects) change \\(E[Y|X]\\) one unit change \\(X\\)discrete variables (also known incremental effects) change \\(E[Y|X]\\) one unit change \\(X\\)continuous variables change \\(E[Y|X]\\) small changes \\(X\\) (unit changes), ’s derivative, limit \\(h \\0\\)continuous variables change \\(E[Y|X]\\) small changes \\(X\\) (unit changes), ’s derivative, limit \\(h \\0\\)","code":""},{"path":"marginal-effects.html","id":"delta-method","chapter":"15 Marginal Effects","heading":"15.1 Delta Method","text":"approximate mean variance function random variables using first-order Taylor approximationA semi-parametric methodAlternative approaches:\nAnalytically derive probability function margin\nSimulation/Bootstrapping\nAnalytically derive probability function marginAnalytically derive probability function marginSimulation/BootstrappingSimulation/BootstrappingResources:\nAdvanced: modmarg\nIntermediate: UCLA stat\nSimple: Another one\nAdvanced: modmargAdvanced: modmargIntermediate: UCLA statIntermediate: UCLA statSimple: Another oneSimple: Another oneLet \\(G(\\beta)\\) function parameters \\(\\beta\\), \\[\nvar(G(\\beta)) \\approx \\nabla G(\\beta) cov (\\beta) \\nabla G(\\beta)'\n\\]\\(\\nabla G(\\beta)\\) = gradient partial derivatives \\(G(\\beta)\\) (also known Jacobian)","code":""},{"path":"marginal-effects.html","id":"average-marginal-effect-algorithm","chapter":"15 Marginal Effects","heading":"15.2 Average Marginal Effect Algorithm","text":"one-sided derivative \\(\\frac{\\partial p(\\mathbf{X},\\beta)}{\\partial X}\\) probability scaleEstimate modelFor observation \\(\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed values\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\n\nCalculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.\nCalculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) discrete, \\(h = 1\\)\\(X\\) discrete, \\(h = 1\\)Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)Average numerical derivative \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\)Two-sided derivativesEstimate modelFor observation \\(\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed values\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\n\nCalculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.\nCalculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.\nCalculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) discrete, \\(h = 1\\)\\(X\\) discrete, \\(h = 1\\)Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)Average numerical derivative \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\)","code":"\nlibrary(margins)\nlibrary(tidyverse)\n\ndata(\"mtcars\")\nmod <- lm(mpg ~ cyl * disp * hp, data = mtcars)\nmargins::margins(mod) %>% summary()\n#>  factor     AME     SE       z      p    lower   upper\n#>     cyl -4.0592 3.7614 -1.0792 0.2805 -11.4313  3.3130\n#>    disp -0.0350 0.0132 -2.6473 0.0081  -0.0610 -0.0091\n#>      hp -0.0284 0.0185 -1.5348 0.1248  -0.0647  0.0079\n\n# function for variable\nget_mae <- function(mod, var = \"disp\") {\n    data = mod$model\n    \n    pred <- predict(mod)\n    \n    if (class(mod$model[[{\n        {\n            var\n        }\n    }]]) == \"numeric\") {\n        h = (abs(mean(mod$model[[var]])) + 0.01) * 0.01\n    } else {\n        h = 1\n    }\n    \n    data[[{\n        {\n            var\n        }\n    }]] <- data[[{\n{\nvar\n}\n}]] - h\n\n    pred_new <- predict(mod, newdata = data)\n\n    mean(pred - pred_new) / h\n}\n\nget_mae(mod, var = \"disp\")\n#> [1] -0.03504546"},{"path":"marginal-effects.html","id":"packages","chapter":"15 Marginal Effects","heading":"15.3 Packages","text":"","code":""},{"path":"marginal-effects.html","id":"marginaleffects","chapter":"15 Marginal Effects","heading":"15.3.1 MarginalEffects","text":"MarginalEffects package successor margins emtrends (faster, efficient, adaptable). Hence, advocated used.limitation readily function correct multiple comparisons. Hence, one can use p.adjust function overcome disadvantage.Definitions package:Marginal effects slopes derivatives (.e., effect changes variable outcome)\nmargins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”\nMarginal effects slopes derivatives (.e., effect changes variable outcome)margins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”Marginal means averages integrals (.e., marginalizing across rows prediction grid)Marginal means averages integrals (.e., marginalizing across rows prediction grid)customize plot using plot_cme (ggplot class), can check post author MarginalEffects packageCausal inference parametric g-formulaBecause plug-g estimator equivalent average contrast marginaleffects package.get predicted values","code":"\nlibrary(marginaleffects)\nlibrary(tidyverse)\ndata(mtcars)\n\nmod <- lm(mpg ~ hp * wt * am, data = mtcars)\npredictions(mod) %>% head()\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      22.5      0.884 25.4   <0.001 471.7  20.8   24.2\n#>      20.8      1.194 17.4   <0.001 223.3  18.5   23.1\n#>      25.3      0.709 35.7   <0.001 922.7  23.9   26.7\n#>      20.3      0.704 28.8   <0.001 601.5  18.9   21.6\n#>      17.0      0.712 23.9   <0.001 416.2  15.6   18.4\n#>      19.7      0.875 22.5   <0.001 368.8  17.9   21.4\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am\n# for specific reference values\npredictions(mod, newdata = datagrid(am = 0, wt = c(2, 4)))\n#> \n#>  am wt Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp\n#>   0  2     22.0       2.04 10.8   <0.001  87.4  18.0   26.0 147\n#>   0  4     16.6       1.08 15.3   <0.001 173.8  14.5   18.7 147\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt\nplot_cap(mod, condition = c(\"hp\",\"wt\"))\n# Average Margianl Effects\nmfx <- marginaleffects(mod, variables = c(\"hp\", \"wt\"))\nsummary(mfx)\n#> \n#>  Term    Contrast Estimate Std. Error     z Pr(>|z|)   2.5 % 97.5 %\n#>    hp mean(dY/dX)  -0.0381     0.0128 -2.98  0.00291 -0.0631 -0.013\n#>    wt mean(dY/dX)  -3.9391     1.0858 -3.63  < 0.001 -6.0672 -1.811\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n# Group-Average Marginal Effects\nmarginaleffects::marginaleffects(mod, by = \"hp\", variables = \"am\")\n#> \n#>  Term          Contrast  hp Estimate Std. Error      z Pr(>|z|)   S  2.5 %\n#>    am mean(1) - mean(0)  52    3.976       5.20  0.764    0.445 1.2  -6.22\n#>    am mean(1) - mean(0)  62   -2.774       2.51 -1.107    0.268 1.9  -7.68\n#>    am mean(1) - mean(0)  65    2.999       4.13  0.725    0.468 1.1  -5.10\n#>    am mean(1) - mean(0)  66    2.025       3.48  0.582    0.561 0.8  -4.80\n#>    am mean(1) - mean(0)  91    1.858       2.76  0.674    0.500 1.0  -3.54\n#>    am mean(1) - mean(0)  93    1.201       2.35  0.511    0.609 0.7  -3.40\n#>    am mean(1) - mean(0)  95   -1.832       1.97 -0.931    0.352 1.5  -5.69\n#>    am mean(1) - mean(0)  97    0.708       2.04  0.347    0.728 0.5  -3.28\n#>    am mean(1) - mean(0) 105   -2.682       2.37 -1.132    0.258 2.0  -7.32\n#>    am mean(1) - mean(0) 109   -0.237       1.59 -0.149    0.881 0.2  -3.35\n#>    am mean(1) - mean(0) 110   -0.640       1.57 -0.407    0.684 0.5  -3.73\n#>    am mean(1) - mean(0) 113    4.081       3.94  1.037    0.300 1.7  -3.63\n#>    am mean(1) - mean(0) 123   -2.098       2.10 -0.998    0.318 1.7  -6.22\n#>    am mean(1) - mean(0) 150   -1.429       1.90 -0.753    0.452 1.1  -5.15\n#>    am mean(1) - mean(0) 175   -0.416       1.56 -0.266    0.790 0.3  -3.48\n#>    am mean(1) - mean(0) 180   -1.381       2.47 -0.560    0.576 0.8  -6.22\n#>    am mean(1) - mean(0) 205   -2.873       6.24 -0.460    0.645 0.6 -15.11\n#>    am mean(1) - mean(0) 215   -2.534       6.95 -0.364    0.716 0.5 -16.16\n#>    am mean(1) - mean(0) 230   -1.477       7.07 -0.209    0.835 0.3 -15.34\n#>    am mean(1) - mean(0) 245    1.115       2.28  0.488    0.625 0.7  -3.36\n#>    am mean(1) - mean(0) 264    2.106       2.29  0.920    0.358 1.5  -2.38\n#>    am mean(1) - mean(0) 335    4.027       3.24  1.243    0.214 2.2  -2.32\n#>  97.5 %\n#>   14.18\n#>    2.14\n#>   11.10\n#>    8.85\n#>    7.26\n#>    5.80\n#>    2.02\n#>    4.70\n#>    1.96\n#>    2.87\n#>    2.45\n#>   11.79\n#>    2.02\n#>    2.29\n#>    2.64\n#>    3.46\n#>    9.36\n#>   11.09\n#>   12.39\n#>    5.59\n#>    6.59\n#>   10.38\n#> \n#> Columns: term, contrast, hp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted\n\n# Marginal effects at representative values\nmarginaleffects::marginaleffects(mod, \n                                 newdata = datagrid(am = 0, \n                                                    wt = c(2, 4)))\n#> \n#>  Term Contrast am wt Estimate Std. Error      z Pr(>|z|)   S   2.5 %   97.5 %\n#>    am    1 - 0  0  2   2.5465     2.7860  0.914   0.3607 1.5 -2.9139  8.00694\n#>    am    1 - 0  0  4  -2.9661     3.0381 -0.976   0.3289 1.6 -8.9207  2.98852\n#>    hp    dY/dX  0  2  -0.0598     0.0283 -2.115   0.0344 4.9 -0.1153 -0.00439\n#>    hp    dY/dX  0  4  -0.0309     0.0187 -1.654   0.0981 3.3 -0.0676  0.00572\n#>    wt    dY/dX  0  2  -2.6762     1.4194 -1.885   0.0594 4.1 -5.4582  0.10587\n#>    wt    dY/dX  0  4  -2.6762     1.4199 -1.885   0.0595 4.1 -5.4591  0.10676\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, predicted_lo, predicted_hi, predicted, mpg, hp\n\n# Marginal Effects at the Mean\nmarginaleffects::marginaleffects(mod, newdata = \"mean\")\n#> \n#>  Term Contrast Estimate Std. Error      z Pr(>|z|)    S  2.5 %  97.5 %\n#>    am    1 - 0  -0.8086    1.52383 -0.531  0.59568  0.7 -3.795  2.1781\n#>    hp    dY/dX  -0.0323    0.00956 -3.375  < 0.001 10.4 -0.051 -0.0135\n#>    wt    dY/dX  -3.7959    1.21310 -3.129  0.00175  9.2 -6.174 -1.4183\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am\n# counterfactual\ncomparisons(mod, variables = list(am = 0:1)) %>% summary()\n#> \n#>  Term          Contrast Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %\n#>    am mean(1) - mean(0)  -0.0481       1.85 -0.026    0.979 -3.68   3.58\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"marginal-effects.html","id":"margins","chapter":"15 Marginal Effects","heading":"15.3.2 margins","text":"Marginal effects partial derivative regression equation respect variable model unit dataAverage Partial Effects: contribution variable outcome scale, conditional variables involved link function transformation linear predictorAverage Partial Effects: contribution variable outcome scale, conditional variables involved link function transformation linear predictorAverage Marginal Effects: marginal contribution variable scale linear predictor.Average Marginal Effects: marginal contribution variable scale linear predictor.Average marginal effects mean unit-specific partial derivatives sampleAverage marginal effects mean unit-specific partial derivatives samplemargins package gives marginal effects models (replication margins command Stata).prediction package gives unit-specific sample average predictions models (similar predictive margins Stata).cases interaction polynomial terms, coefficient estimates interpreted marginal effects X Y. Hence, want know average marginal effects variable thenMarginal effects mean (MEM):Marginal effects mean values sampleFor discrete variables, ’s called average discrete change (ADC)Average Marginal Effect (AME)average marginal effects value sampleMarginal Effects representative values (MER)","code":"\nlibrary(margins)\n\n# examples by the package's authors\nmod <- lm(mpg ~ cyl * hp + wt, data = mtcars)\nsummary(mod)\n#> \n#> Call:\n#> lm(formula = mpg ~ cyl * hp + wt, data = mtcars)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.3440 -1.4144 -0.6166  1.2160  4.2815 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 52.017520   4.916935  10.579 4.18e-11 ***\n#> cyl         -2.742125   0.800228  -3.427  0.00197 ** \n#> hp          -0.163594   0.052122  -3.139  0.00408 ** \n#> wt          -3.119815   0.661322  -4.718 6.51e-05 ***\n#> cyl:hp       0.018954   0.006645   2.852  0.00823 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.242 on 27 degrees of freedom\n#> Multiple R-squared:  0.8795, Adjusted R-squared:  0.8616 \n#> F-statistic: 49.25 on 4 and 27 DF,  p-value: 5.065e-12\nsummary(margins(mod))\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236\n\n# equivalently \nmargins_summary(mod)\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236\n\nplot(margins(mod))\nmargins(mod, at = list(hp = 150))\n#>  at(hp)    cyl       hp    wt\n#>     150 0.1009 -0.04632 -3.12\n\nmargins(mod, at = list(hp = 150)) %>% summary()\n#>  factor       hp     AME     SE       z      p   lower   upper\n#>     cyl 150.0000  0.1009 0.6128  0.1647 0.8692 -1.1001  1.3019\n#>      hp 150.0000 -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt 150.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236"},{"path":"marginal-effects.html","id":"mfx","chapter":"15 Marginal Effects","heading":"15.3.3 mfx","text":"Works well Generalized Linear Models/glm packageFor technical details, see package vignetteThis package can give marginal effect variable glm model, average marginal effect might look .","code":"\nlibrary(mfx)\ndata(\"mtcars\")\npoissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> Call:\n#> poissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> \n#> Marginal Effects:\n#>                    dF/dx   Std. Err.       z  P>|z|\n#> mpg           1.4722e-03  8.7531e-03  0.1682 0.8664\n#> cyl           6.6420e-03  3.9263e-02  0.1692 0.8657\n#> disp          1.5899e-04  9.4555e-04  0.1681 0.8665\n#> mpg:cyl      -3.4698e-04  2.0564e-03 -0.1687 0.8660\n#> mpg:disp     -7.6794e-06  4.5545e-05 -0.1686 0.8661\n#> cyl:disp     -3.3837e-05  1.9919e-04 -0.1699 0.8651\n#> mpg:cyl:disp  1.6812e-06  9.8919e-06  0.1700 0.8650"},{"path":"prediction-and-estimation.html","id":"prediction-and-estimation","chapter":"16 Prediction and Estimation","heading":"16 Prediction and Estimation","text":"Prediction Estimation (Causal Inference) serve distinct roles understanding modeling data.","code":""},{"path":"prediction-and-estimation.html","id":"prediction-2","chapter":"16 Prediction and Estimation","heading":"16.1 Prediction","text":"Definition: Prediction, denoted \\(\\hat{y}\\), creating algorithm predicting outcome variable \\(y\\) predictors \\(x\\).Definition: Prediction, denoted \\(\\hat{y}\\), creating algorithm predicting outcome variable \\(y\\) predictors \\(x\\).Goal: primary goal loss minimization, aiming model accuracy unseen data:\n\\[\n\\hat{f} \\approx \\min E_{(y,x)} L(f(x), y)\n\\]Goal: primary goal loss minimization, aiming model accuracy unseen data:\\[\n\\hat{f} \\approx \\min E_{(y,x)} L(f(x), y)\n\\]Applications Economics:\nMeasure variables.\nEmbed prediction tasks within parameter estimation treatment effects.\nControl observed confounders.\nApplications Economics:Measure variables.Embed prediction tasks within parameter estimation treatment effects.Control observed confounders.","code":""},{"path":"prediction-and-estimation.html","id":"parameter-estimation","chapter":"16 Prediction and Estimation","heading":"16.2 Parameter Estimation","text":"Definition: Parameter estimation, represented \\(\\hat{\\beta}\\), focuses estimating relationship \\(y\\) \\(x\\).Definition: Parameter estimation, represented \\(\\hat{\\beta}\\), focuses estimating relationship \\(y\\) \\(x\\).Goal: aim consistency, ensuring models perform well training data:\n\\[\nE[\\hat{f}] = f\n\\]Goal: aim consistency, ensuring models perform well training data:\\[\nE[\\hat{f}] = f\n\\]Challenges:\nHigh-dimensional spaces can lead covariance among variables multicollinearity.\nleads bias-variance tradeoff (Hastie et al. 2009).\nChallenges:High-dimensional spaces can lead covariance among variables multicollinearity.leads bias-variance tradeoff (Hastie et al. 2009).","code":""},{"path":"prediction-and-estimation.html","id":"causation-versus-prediction","chapter":"16 Prediction and Estimation","heading":"16.3 Causation versus Prediction","text":"Understanding relationship causation prediction crucial statistical modeling.Let \\(Y\\) outcome variable dependent \\(X\\), aim manipulate \\(X\\) maximize payoff function \\(\\pi(X, Y)\\) (Kleinberg et al. 2015). decision \\(X\\) hinges :\\[\n\\begin{aligned}\n\\frac{d\\pi(X, Y)}{d X} &= \\frac{\\partial \\pi}{\\partial X} (Y) + \\frac{\\partial \\pi}{\\partial Y} \\frac{\\partial Y}{\\partial X} \\\\\n&= \\frac{\\partial \\pi}{\\partial X} \\text{(Prediction)} + \\frac{\\partial \\pi}{\\partial Y} \\text{(Causation)}\n\\end{aligned}\n\\]Empirical work essential estimating derivatives equation:\\(\\frac{\\partial Y}{\\partial X}\\) required causal inference determine \\(X\\)’s effect \\(Y\\),\\(\\frac{\\partial Y}{\\partial X}\\) required causal inference determine \\(X\\)’s effect \\(Y\\),\\(\\frac{\\partial \\pi}{\\partial X}\\) required prediction \\(Y\\).\\(\\frac{\\partial \\pi}{\\partial X}\\) required prediction \\(Y\\).","code":""},{"path":"moderation.html","id":"moderation","chapter":"17 Moderation","heading":"17 Moderation","text":"Spotlight Analysis: Compare mean dependent two groups (treatment control) every value (Simple Slopes Analysis)Floodlight Analysis: spotlight analysis whole range moderator (Johnson-Neyman intervals)Resources:BANOVAL : floodlight analysis Bayesian ANOVA modelsBANOVAL : floodlight analysis Bayesian ANOVA modelscSEM : doFloodlightAnalysis SEM modelcSEM : doFloodlightAnalysis SEM model(Spiller et al. 2013)(Spiller et al. 2013)Terminology:Main effects (slopes): coefficients involve interaction termsMain effects (slopes): coefficients involve interaction termsSimple slope: continuous independent variable interact moderating variable, slope particular level moderating variableSimple slope: continuous independent variable interact moderating variable, slope particular level moderating variableSimple effect: categorical independent variable interacts moderating variable, effect particular level moderating variable.Simple effect: categorical independent variable interacts moderating variable, effect particular level moderating variable.Example:\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 M + \\beta_3 X \\times M\n\\]\\(\\beta_0\\) = intercept\\(\\beta_0\\) = intercept\\(\\beta_1\\) = simple effect (slope) \\(X\\) (independent variable)\\(\\beta_1\\) = simple effect (slope) \\(X\\) (independent variable)\\(\\beta_2\\) = simple effect (slope) \\(M\\) (moderating variable)\\(\\beta_2\\) = simple effect (slope) \\(M\\) (moderating variable)\\(\\beta_3\\) = interaction \\(X\\) \\(M\\)\\(\\beta_3\\) = interaction \\(X\\) \\(M\\)Three types interactions:Continuous continuousContinuous categoricalCategorical categoricalWhen interpreting three-way interactions, one can use slope difference test (Dawson Richter 2006)","code":""},{"path":"moderation.html","id":"emmeans-package","chapter":"17 Moderation","heading":"17.1 emmeans package","text":"Data set UCLA seminar gender prog categorical","code":"\ninstall.packages(\"emmeans\")\nlibrary(emmeans)\ndat <- readRDS(\"data/exercise.rds\") %>%\n    mutate(prog = factor(prog, labels = c(\"jog\", \"swim\", \"read\"))) %>%\n    mutate(gender = factor(gender, labels = c(\"male\", \"female\")))"},{"path":"moderation.html","id":"continuous-by-continuous","chapter":"17 Moderation","heading":"17.1.1 Continuous by continuous","text":"Simple slopes continuous continuous modelSpotlight analysis (Aiken West 2005): usually pick 3 values moderating variable:Mean Moderating Variable + \\(\\sigma \\times\\) (Moderating variable)Mean Moderating Variable + \\(\\sigma \\times\\) (Moderating variable)Mean Moderating VariableMean Moderating VariableMean Moderating Variable - \\(\\sigma \\times\\) (Moderating variable)Mean Moderating Variable - \\(\\sigma \\times\\) (Moderating variable)3 p-values interaction term.publication, use","code":"\ncontcont <- lm(loss~hours*effort,data=dat)\nsummary(contcont)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * effort, data = dat)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -29.52 -10.60  -1.78  11.13  34.51 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   7.79864   11.60362   0.672   0.5017  \n#> hours        -9.37568    5.66392  -1.655   0.0982 .\n#> effort       -0.08028    0.38465  -0.209   0.8347  \n#> hours:effort  0.39335    0.18750   2.098   0.0362 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 13.56 on 896 degrees of freedom\n#> Multiple R-squared:  0.07818,    Adjusted R-squared:  0.07509 \n#> F-statistic: 25.33 on 3 and 896 DF,  p-value: 9.826e-16\neffar <- round(mean(dat$effort) + sd(dat$effort), 1)\neffr  <- round(mean(dat$effort), 1)\neffbr <- round(mean(dat$effort) - sd(dat$effort), 1)\n# specify list of points\nmylist <- list(effort = c(effbr, effr, effar))\n\n# get the estimates\nemtrends(contcont, ~ effort, var = \"hours\", at = mylist)\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.352 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.308 896    1.745     6.88\n#> \n#> Confidence level used: 0.95\n\n# plot\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\nemmip(contcont, effort ~ hours, at = mylist, CIs = TRUE)\n\n# statistical test for slope difference\nemtrends(\n    contcont,\n    pairwise ~ effort,\n    var = \"hours\",\n    at = mylist,\n    adjust = \"none\"\n)\n#> $emtrends\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.352 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.308 896    1.745     6.88\n#> \n#> Results are averaged over the levels of: hours \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast                estimate    SE  df t.ratio p.value\n#>  effort24.5 - effort29.7    -2.05 0.975 896  -2.098  0.0362\n#>  effort24.5 - effort34.8    -4.05 1.931 896  -2.098  0.0362\n#>  effort29.7 - effort34.8    -2.01 0.956 896  -2.098  0.0362\n#> \n#> Results are averaged over the levels of: hours\nlibrary(ggplot2)\n\n# data\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\ncontcontdat <-\n    emmip(contcont,\n          effort ~ hours,\n          at = mylist,\n          CIs = TRUE,\n          plotit = FALSE)\ncontcontdat$feffort <- factor(contcontdat$effort)\nlevels(contcontdat$feffort) <- c(\"low\", \"med\", \"high\")\n\n# plot\np  <-\n    ggplot(data = contcontdat, \n           aes(x = hours, y = yvar, color = feffort)) +  \n    geom_line()\np1 <-\n    p + \n    geom_ribbon(aes(ymax = UCL, ymin = LCL, fill = feffort), \n                    alpha = 0.4)\np1  + labs(x = \"Hours\",\n           y = \"Weight Loss\",\n           color = \"Effort\",\n           fill = \"Effort\")"},{"path":"moderation.html","id":"continuous-by-categorical","chapter":"17 Moderation","heading":"17.1.2 Continuous by categorical","text":"Get simple slopes level categorical moderator","code":"\n# use Female as basline\ndat$gender <- relevel(dat$gender, ref = \"female\")\n\ncontcat <- lm(loss ~ hours * gender, data = dat)\nsummary(contcat)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * gender, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -27.118 -11.350  -1.963  10.001  42.376 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)         3.335      2.731   1.221    0.222  \n#> hours               3.315      1.332   2.489    0.013 *\n#> gendermale          3.571      3.915   0.912    0.362  \n#> hours:gendermale   -1.724      1.898  -0.908    0.364  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 14.06 on 896 degrees of freedom\n#> Multiple R-squared:  0.008433,   Adjusted R-squared:  0.005113 \n#> F-statistic:  2.54 on 3 and 896 DF,  p-value: 0.05523\nemtrends(contcat, ~ gender, var = \"hours\")\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95\n\n# test difference in slopes\nemtrends(contcat, pairwise ~ gender, var = \"hours\")\n#> $emtrends\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast      estimate  SE  df t.ratio p.value\n#>  female - male     1.72 1.9 896   0.908  0.3639\n# which is the same as the interaction term\n# plot\n(mylist <- list(\n    hours = seq(0, 4, by = 0.4),\n    gender = c(\"female\", \"male\")\n))\n#> $hours\n#>  [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0\n#> \n#> $gender\n#> [1] \"female\" \"male\"\nemmip(contcat, gender ~ hours, at = mylist, CIs = TRUE)"},{"path":"moderation.html","id":"categorical-by-categorical","chapter":"17 Moderation","heading":"17.1.3 Categorical by categorical","text":"Simple effectsPlotBar graph","code":"\n# relevel baseline\ndat$prog   <- relevel(dat$prog, ref = \"read\")\ndat$gender <- relevel(dat$gender, ref = \"female\")\ncatcat <- lm(loss ~ gender * prog, data = dat)\nsummary(catcat)\n#> \n#> Call:\n#> lm(formula = loss ~ gender * prog, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -19.1723  -4.1894  -0.0994   3.7506  27.6939 \n#> \n#> Coefficients:\n#>                     Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          -3.6201     0.5322  -6.802 1.89e-11 ***\n#> gendermale           -0.3355     0.7527  -0.446    0.656    \n#> progjog               7.9088     0.7527  10.507  < 2e-16 ***\n#> progswim             32.7378     0.7527  43.494  < 2e-16 ***\n#> gendermale:progjog    7.8188     1.0645   7.345 4.63e-13 ***\n#> gendermale:progswim  -6.2599     1.0645  -5.881 5.77e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.519 on 894 degrees of freedom\n#> Multiple R-squared:  0.7875, Adjusted R-squared:  0.7863 \n#> F-statistic: 662.5 on 5 and 894 DF,  p-value: < 2.2e-16\nemcatcat <- emmeans(catcat, ~ gender*prog)\n\n# differences in predicted values\ncontrast(emcatcat, \n         \"revpairwise\", \n         by = \"prog\", \n         adjust = \"bonferroni\")\n#> prog = read:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -0.335 0.753 894  -0.446  0.6559\n#> \n#> prog = jog:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female    7.483 0.753 894   9.942  <.0001\n#> \n#> prog = swim:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -6.595 0.753 894  -8.762  <.0001\nemmip(catcat, prog ~ gender,CIs=TRUE)\ncatcatdat <- emmip(catcat,\n                   gender ~ prog,\n                   CIs = TRUE,\n                   plotit = FALSE)\np <-\n    ggplot(data = catcatdat,\n           aes(x = prog, y = yvar, fill = gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")\n\np1 <-\n    p + geom_errorbar(\n        position = position_dodge(.9),\n        width = .25,\n        aes(ymax = UCL, ymin = LCL),\n        alpha = 0.3\n    )\np1  + labs(x = \"Program\", y = \"Weight Loss\", fill = \"Gender\")"},{"path":"moderation.html","id":"probmod-package","chapter":"17 Moderation","heading":"17.2 probmod package","text":"recommend: package serious problem subscript.","code":"\ninstall.packages(\"probemod\")\nlibrary(probemod)\n\nmyModel <-\n    lm(loss ~ hours * gender, data = dat %>% \n           select(loss, hours, gender))\njnresults <- jn(myModel,\n                dv = 'loss',\n                iv = 'hours',\n                mod = 'gender')\n\n\npickapoint(\n    myModel,\n    dv = 'loss',\n    iv = 'hours',\n    mod = 'gender',\n    alpha = .01\n)\n\nplot(jnresults)"},{"path":"moderation.html","id":"interactions-package","chapter":"17 Moderation","heading":"17.3 interactions package","text":"Recommend","code":"\ninstall.packages(\"interactions\")"},{"path":"moderation.html","id":"continuous-interaction","chapter":"17 Moderation","heading":"17.3.1 Continuous interaction","text":"(least one two variables continuous)continuous moderator, three values chosen :-1 SD mean-1 SD meanThe meanThe mean-1 SD mean-1 SD meanTo include weights regression inn plotPartial Effect PlotCheck linearity assumption modelPlot lines based subsample (red line), whole sample (black line)","code":"\nlibrary(interactions)\nlibrary(jtools) # for summ()\nstates <- as.data.frame(state.x77)\nfiti <- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)\nsumm(fiti)\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              \n              # if you don't want the plot to mean-center\n              # centered = \"none\", \n              \n              # exclude the mean value of the moderator\n              # modx.values = \"plus-minus\", \n              \n              # split moderator's distribution into 3 groups\n              # modx.values = \"terciles\" \n              \n              plot.points = T, # overlay data\n              \n              \n              # different shape for differennt levels of the moderator\n              point.shape = T, \n              \n              # if two data points are on top one another, \n              # this moves them apart by little\n              jitter = 0.1, \n              \n              # other appearance option\n              x.label = \"X label\", \n              y.label = \"Y label\",\n              main.title = \"Title\",\n              legend.main = \"Legend Title\",\n              colors = \"blue\",\n              \n              # include confidence band\n              interval = TRUE, \n              int.width = 0.9, \n              robust = TRUE # use robust SE\n              ) \nfiti <- lm(Income ~ Illiteracy * Murder,\n           data = states,\n           weights = Population)\n\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              plot.points = TRUE)\nlibrary(ggplot2)\ndata(cars)\nfitc <- lm(cty ~ year + cyl * displ + class + fl + drv, \n           data = mpg)\nsumm(fitc)\n\ninteract_plot(\n    fitc,\n    pred = displ,\n    modx = cyl,\n    # the observed data is based on displ, cyl, and model error\n    partial.residuals = TRUE, \n    modx.values = c(4, 5, 6, 8)\n)\nx_2 <- runif(n = 200, min = -3, max = 3)\nw   <- rbinom(n = 200, size = 1, prob = 0.5)\nerr <- rnorm(n = 200, mean = 0, sd = 4)\ny_2 <- 2.5 - x_2 ^ 2 - 5 * w + 2 * w * (x_2 ^ 2) + err\n\ndata_2 <- as.data.frame(cbind(x_2, y_2, w))\n\nmodel_2 <- lm(y_2 ~ x_2 * w, data = data_2)\nsumm(model_2)\ninteract_plot(\n    model_2,\n    pred = x_2,\n    modx = w,\n    linearity.check = TRUE,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"simple-slopes-analysis","chapter":"17 Moderation","heading":"17.3.1.1 Simple Slopes Analysis","text":"continuous continuous variable interaction (still work binary)continuous continuous variable interaction (still work binary)conditional slope variable interest (.e., slope \\(X\\) hold \\(M\\) constant value)conditional slope variable interest (.e., slope \\(X\\) hold \\(M\\) constant value)Using sim_slopes willmean-center variables except variable interestmean-center variables except variable interestFor moderator \nContinuous, pick mean, plus/minus 1 SD\nCategorical, use factor\nmoderator isContinuous, pick mean, plus/minus 1 SDContinuous, pick mean, plus/minus 1 SDCategorical, use factorCategorical, use factorsim_slopes requiresA regression model interaction term)regression model interaction term)Variable interest (pred =)Variable interest (pred =)Moderator: (modx =)Moderator: (modx =)Table 17.1:  ","code":"\nsim_slopes(fiti,\n           pred = Illiteracy,\n           modx = Murder,\n           johnson_neyman = FALSE)\n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -71.59   268.65    -0.27   0.79\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -437.12   175.82    -2.49   0.02\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -802.66   145.72    -5.51   0.00\n\n# plot the coefficients\nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\nplot(ss)\n\n# table \nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\nlibrary(huxtable)\nas_huxtable(ss)"},{"path":"moderation.html","id":"johnson-neyman-intervals","chapter":"17 Moderation","heading":"17.3.1.2 Johnson-Neyman intervals","text":"know values moderator slope variable interest statistically significant, can use Johnson-Neyman interval (P. O. Johnson Neyman 1936)Even though kind know alpha level implementing Johnson-Neyman interval correct (Bauer Curran 2005), recently correction type II errors (Esarey Sumner 2018).Since Johnson-Neyman inflates type error (comparisons across values moderator)plotting, can use johnson_neymanNote:y-axis conditional slope variable interest","code":"\nsim_slopes(\n    fiti,\n    pred = Illiteracy,\n    modx = Murder,\n    johnson_neyman = TRUE,\n    control.fdr = TRUE,\n    # correction for type I and II\n    \n    # include conditional intecepts\n    # cond.int = TRUE, \n    \n    robust = \"HC3\",\n    # rubust SE\n    \n    # don't mean-centered non-focal variables\n    # centered = \"none\",\n    jnalpha = 0.05\n)\n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When Murder is OUTSIDE the interval [-11.70, 8.75], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.33 \n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -71.59   256.60    -0.28   0.78\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -437.12   191.07    -2.29   0.03\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -802.66   178.75    -4.49   0.00\njohnson_neyman(fiti,\n               pred = Illiteracy,\n               modx = Murder,\n               \n               # correction for type I and II\n               control.fdr = TRUE, \n               alpha = .05)\n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When Murder is OUTSIDE the interval [-22.57, 8.52], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.33"},{"path":"moderation.html","id":"way-interaction","chapter":"17 Moderation","heading":"17.3.1.3 3-way interaction","text":"Johnson-Neyman 3-way interactionReportTable 17.2:  ","code":"\n# fita3 <-\n#     lm(rating ~ privileges * critical * learning, \n#        data = attitude)\n# \n# probe_interaction(\n#     fita3,\n#     pred = critical,\n#     modx = learning,\n#     mod2 = privileges,\n#     alpha = .1\n# )\n\n\nmtcars$cyl <- factor(mtcars$cyl,\n                     labels = c(\"4 cylinder\", \"6 cylinder\", \"8 cylinder\"))\nfitc3 <- lm(mpg ~ hp * wt * cyl, data = mtcars)\ninteract_plot(fitc3,\n              pred = hp,\n              modx = wt,\n              mod2 = cyl) +\n    theme_apa(legend.pos = \"bottomright\")\nlibrary(survey)\ndata(api)\n\ndstrat <- svydesign(\n    id = ~ 1,\n    strata = ~ stype,\n    weights = ~ pw,\n    data = apistrat,\n    fpc = ~ fpc\n)\n\nregmodel3 <-\n    survey::svyglm(api00 ~ avg.ed * growth * enroll, design = dstrat)\n\nsim_slopes(\n    regmodel3,\n    pred = growth,\n    modx = avg.ed,\n    mod2 = enroll,\n    jnplot = TRUE\n)\n#> ███████████████ While enroll (2nd moderator) =  153.0518 (- 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   1.25   0.32     3.86   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.39   0.22     1.75   0.08\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.48   0.35    -1.37   0.17\n#> \n#> ████████████████ While enroll (2nd moderator) =  595.2821 (Mean) ███████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.72   0.22     3.29   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.34   0.16     2.16   0.03\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.04   0.24    -0.16   0.87\n#> \n#> ███████████████ While enroll (2nd moderator) = 1037.5125 (+ 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> The Johnson-Neyman interval could not be found. Is the p value for your\n#> interaction term below the specified alpha?\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.18   0.31     0.58   0.56\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.29   0.20     1.49   0.14\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.40   0.27     1.49   0.14\nss3 <-\n    sim_slopes(regmodel3,\n               pred = growth,\n               modx = avg.ed,\n               mod2 = enroll)\nplot(ss3)\nas_huxtable(ss3)"},{"path":"moderation.html","id":"categorical-interaction","chapter":"17 Moderation","heading":"17.3.2 Categorical interaction","text":"","code":"\nlibrary(ggplot2)\nmpg2 <- mpg %>% \n    mutate(cyl = factor(cyl))\n\nmpg2[\"auto\"] <- \"auto\"\nmpg2$auto[mpg2$trans %in% c(\"manual(m5)\", \"manual(m6)\")] <- \"manual\"\nmpg2$auto <- factor(mpg2$auto)\nmpg2[\"fwd\"] <- \"2wd\"\nmpg2$fwd[mpg2$drv == \"4\"] <- \"4wd\"\nmpg2$fwd <- factor(mpg2$fwd)\n## Drop the two cars with 5 cylinders (rest are 4, 6, or 8)\nmpg2 <- mpg2[mpg2$cyl != \"5\", ]\n## Fit the model\nfit3 <- lm(cty ~ cyl * fwd * auto, data = mpg2)\n\nlibrary(jtools) # for summ()\nsumm(fit3)\ncat_plot(fit3,\n         pred = cyl,\n         modx = fwd,\n         plot.points = T)\n#line plots\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"line\",\n    point.shape = TRUE,\n    # colors = \"Set2\", # choose color\n    vary.lty = TRUE\n)\n\n\n# bar plot\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"bar\",\n    interval = T,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"interactionr-package","chapter":"17 Moderation","heading":"17.4 interactionR package","text":"publication purposesFollowing\n(Knol VanderWeele 2012) presentation\n(Hosmer Lemeshow 1992) confidence intervals based delta method\n(G. Y. Zou 2008) variance recovery “mover” method\n(Assmann et al. 1996) bootstrapping\n(Knol VanderWeele 2012) presentation(Knol VanderWeele 2012) presentation(Hosmer Lemeshow 1992) confidence intervals based delta method(Hosmer Lemeshow 1992) confidence intervals based delta method(G. Y. Zou 2008) variance recovery “mover” method(G. Y. Zou 2008) variance recovery “mover” method(Assmann et al. 1996) bootstrapping(Assmann et al. 1996) bootstrapping","code":"\ninstall.packages(\"interactionR\")"},{"path":"moderation.html","id":"sjplot-package","chapter":"17 Moderation","heading":"17.5 sjPlot package","text":"publication purposes (recommend, advanced)publication purposes (recommend, advanced)linklink","code":""},{"path":"causal-inference.html","id":"causal-inference","chapter":"18 Causal Inference","heading":"18 Causal Inference","text":"mambo jumbo learned far, want now talk concept causality. usually say correlation causation. , causation?\nOne favorite books explained concept beautifully (Pearl Mackenzie 2018). just going quickly summarize gist understanding. hope can give initial grasp concept later can continue read develop deeper understanding.’s important deep understanding regarding method research. However, one needs aware limitation. mentioned various sections throughout book, see need ask experts number baseline visit literature gain insight past research., dive conceptual side statistical analysis whole, regardless particular approach.probably heard scientists say correlation doesn’t mean causation. ridiculous spurious correlations give firm grip previous phrase means. pioneer tried use regression infer causation social science Yule (1899) (fatal attempt found relief policy increases poverty). make causal inference statistics, equation (function form) must stable intervention (.e., variables manipulated). Statistics used causality-free enterprise past.development path analysis Sewall Wright 1920s discipline started pay attention causation. , remained dormant Causal Revolution (quoted Judea Pearl’s words). revolution introduced calculus causation includes (1) causal diagrams), (2) symbolic languageThe world using \\(P(Y|X)\\) (statistics use derive ), want compare difference \\(P(Y|(X))\\): treatment group\\(P(Y|(X))\\): treatment group\\(P(Y|(-X))\\): control group\\(P(Y|(-X))\\): control groupHence, can see clear difference \\(P(Y|X) \\neq P(Y|(X))\\)conclusion want make data counterfactuals: happened X?teach robot make inference, need inference engineLevels cognitive ability causal learner:SeeingDoingImaginingLadder causation (associated levels cognitive ability well):Association: conditional probability, correlation, regressionInterventionCounterfactualsAssociation\\(P(y|x)\\)?seeing X change belief Y?Intervention\\(P(y|(x),z)\\)DoingInterveningWhat ?X?Counterfactuals\\(P(y_x|x',y')\\)?\nX caused Y?acted differentlyTable (Pearl 2019, 57)define causation probability aloneIf say X causes Y X raises probability Y.” surface, might sound intuitively right. translate probability notation: \\(P(Y|X) >P(Y)\\) , can’t wrong. Just seeing X (1st level), doesn’t mean probability Y increases.either (1) X causes Y, (2) Z affects X Y. Hence, people might use control variables, translate: \\(P(Y|X, Z=z) > P(Y|Z=z)\\), can confident probabilistic observation. However, question can choose \\(Z\\)invention -operator, now can represent X causes Y \\[\nP(Y|(X)) > P(Y)\n\\]help causal diagram, now can answer questions 2nd level (Intervention)Note: people econometrics might still use “Granger causality” “vector autoregression” use probability language represent causality (’s ).7 tools Structural Causal Model framework (Pearl 2019):Encoding Causal Assumptions - transparency testability (graphical representation)Encoding Causal Assumptions - transparency testability (graphical representation)-calculus control confounding: “back-door”-calculus control confounding: “back-door”algorithmization CounterfactualsThe algorithmization CounterfactualsMediation Analysis Assessment Direct Indirect EffectsMediation Analysis Assessment Direct Indirect EffectsAdaptability, External validity Sample Selection Bias: still researched “domain adaptation”, “transfer learning”Adaptability, External validity Sample Selection Bias: still researched “domain adaptation”, “transfer learning”Recovering missing dataRecovering missing dataCausal Discovery:\nd-separation\nFunctional decomposition (Hoyer et al. 2008)\nSpontaneous local changes (Pearl 2014)\nCausal Discovery:d-separationd-separationFunctional decomposition (Hoyer et al. 2008)Functional decomposition (Hoyer et al. 2008)Spontaneous local changes (Pearl 2014)Spontaneous local changes (Pearl 2014)List packages causal inference RSimpson’s Paradox:statistical association seen entire population reversed sub-population.Structural Causal Model accompanies graphical causal model create efficient language represent causalityStructural Causal Model solution curse dimensionality (.e., large numbers variable \\(p\\), small dataset \\(n\\)) thanks product decomposition. allows us solve problems without knowing function, parameters, distributions error terms.Suppose causal chain \\(X \\Y \\Z\\):\\[\nP(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)\n\\]Criticisms quasi-experimental versus experimental designs:Quasi-experimental methods don’t approximate well experimental results. example,\nLaLonde (1986) shows Matching Methods, Difference--differences, Tobit-2 (Heckman-type) can’t approximate experimental estimates.\nQuasi-experimental methods don’t approximate well experimental results. example,LaLonde (1986) shows Matching Methods, Difference--differences, Tobit-2 (Heckman-type) can’t approximate experimental estimates.Tools hierarchical orderExperimental Design: Randomized Control Trials (Gold standard): Tier 1Experimental Design: Randomized Control Trials (Gold standard): Tier 1Quasi-experimental\nRegression Discontinuity\nSynthetic Difference--Differences\nDifference--Differences\nSynthetic Control\nEvent Studies\nFixed Effects Estimator 12.4.2.2\nEndogenous Treatment: mostly Instrumental Variables\nMatching Methods\nInterrupted Time Series\nEndogenous Sample Selection 33.2: mostly Heckman’s correction\nQuasi-experimentalRegression DiscontinuityRegression DiscontinuitySynthetic Difference--DifferencesSynthetic Difference--DifferencesDifference--DifferencesDifference--DifferencesSynthetic ControlSynthetic ControlEvent StudiesEvent StudiesFixed Effects Estimator 12.4.2.2Fixed Effects Estimator 12.4.2.2Endogenous Treatment: mostly Instrumental VariablesEndogenous Treatment: mostly Instrumental VariablesMatching MethodsMatching MethodsInterrupted Time SeriesInterrupted Time SeriesEndogenous Sample Selection 33.2: mostly Heckman’s correctionEndogenous Sample Selection 33.2: mostly Heckman’s correctionInternal vs. External ValidityInternal Validity: Economists applied scientists largely care .Internal Validity: Economists applied scientists largely care .External Validity: Localness might affect external validity.External Validity: Localness might affect external validity.many economic policies, difference treatment intention treat.example, might effective vaccine (.e., intention treat), mean everybody take (.e., treatment).four types subjects deal :Non-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .\nAlways takers\nNever takers\nNon-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .Always takersAlways takersNever takersNever takersSwitchers\nCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\n\nDefiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\n\nSwitchersCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\nCompliers: defined respect intervention.care compliers introduce intervention, something. don’t interventions, won’t .care compliers introduce intervention, something. don’t interventions, won’t .Tools used identify causal impact intervention compliersTools used identify causal impact intervention compliersIf compliers dataset, intention treatment = treatment effect.compliers dataset, intention treatment = treatment effect.Defiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\nDefiers: go opposite direction treatment.typically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.Directional Bias due selection treatment comes 2 general opposite sourcesMitigation-based: select treatment combat problemPreference-based: select treatment units like kind treatment.","code":""},{"path":"causal-inference.html","id":"treatment-effect-types","chapter":"18 Causal Inference","heading":"18.1 Treatment effect types","text":"section based Paul Testa’s noteTerminology:Quantities causal interest (.e., treatment effect types)Quantities causal interest (.e., treatment effect types)Estimands: parameters interestEstimands: parameters interestEstimators: procedures calculate hesitates parameters interestEstimators: procedures calculate hesitates parameters interestSources bias (according prof. Luke Keele)\\[\n\\begin{aligned}\n&\\text{Estimator - True Causal Effect} \\\\\n&= \\text{Hidden bias + Misspecification bias + Statistical Noise} \\\\\n&= \\text{Due design + Due modeling + Due finite sample}\n\\end{aligned}\n\\]","code":""},{"path":"causal-inference.html","id":"average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.1 Average Treatment Effects","text":"Average treatment effect (ATE) difference means treated control groupsRandomization Experimental Design can provide unbiased estimate ATE.Let \\(Y_i(1)\\) denote outcome individual \\(\\) treatment \\(Y_i(0)\\) denote outcome individual \\(\\) controlThen, treatment effect individual \\(\\) difference outcome treatment control\\[\n\\tau_i = Y_i(1) - Y_i(0)\n\\]Without time machine dimension portal, can observe one two event: either individual \\(\\) experiences treatment doesn’t., ATE quantity interest can come handy since can observe across individuals\\[\nATE = \\frac{1}{N} \\sum_{=1}^N \\tau_i = \\frac{\\sum_1^N Y_i(1)}{N} - \\frac{\\sum_i^N Y_i(0)}{N}\n\\]random assignment (.e., treatment assignment independent potential outcome observables unobservables), observed means difference two groups unbiased estimator average treatment effect\\[\nE(Y_i (1) |D = 1) = E(Y_i(1)|D=0) = E(Y_i(1)) \\\\\nE(Y_i(0) |D = 1) = E(Y_i(0)|D = 0 ) = E(Y_i(0))\n\\]\\[\nATE = E(Y_i(1)) - E(Y_i(0))\n\\]Alternatively, can write potential outcomes model regression form\\[\nY_i = Y_i(0)  + [Y_i (1) - Y_i(0)] D_i\n\\]Let \\(\\beta_{0i} = Y_i (0) ; \\beta_{1i} = Y_i(1) - Y_i(0)\\), \\[\nY_i = \\beta_{0i} + \\beta_{1i} D_i\n\\]\\(\\beta_{0i}\\) = outcome unit receive treatment\\(\\beta_{0i}\\) = outcome unit receive treatment\\(\\beta_{1i}\\) = treatment effect (.e., random coefficients unit \\(\\))\\(\\beta_{1i}\\) = treatment effect (.e., random coefficients unit \\(\\))understand endogeneity (.e., nonrandom treatment assignment), can examine standard linear model\\[\n\\begin{aligned}\nY_i &= \\beta_{0i} + \\beta_{1i} D_i \\\\\n&= ( \\bar{\\beta}_{0} + \\epsilon_{0i} ) + (\\bar{\\beta}_{1} + \\epsilon_{1i} )D_i \\\\\n&=  \\bar{\\beta}_{0} + \\epsilon_{0i} + \\bar{\\beta}_{1} D_i + \\epsilon_{1i} D_i\n\\end{aligned}\n\\]random assignment, \\(E(\\epsilon_{0i}) = E(\\epsilon_{1i}) = 0\\)selection bias: \\(D_i \\perp e_{0i}\\)selection bias: \\(D_i \\perp e_{0i}\\)Treatment effect independent treatment assignment: \\(D_i \\perp e_{1i}\\)Treatment effect independent treatment assignment: \\(D_i \\perp e_{1i}\\)otherwise, residuals can correlate \\(D_i\\)estimation,\\(\\hat{\\beta}_1^{OLS}\\) identical difference means (.e., \\(Y_i(1) - Y_i(0)\\))\\(\\hat{\\beta}_1^{OLS}\\) identical difference means (.e., \\(Y_i(1) - Y_i(0)\\))case heteroskedasticity (.e., \\(\\epsilon_{0i} + D_i \\epsilon_{1i} \\neq 0\\) ), residual’s variance depends \\(X\\) heterogeneous treatment effects (.e., \\(\\epsilon_{1i} \\neq 0\\))\nRobust SE still give consistent estimate \\(\\hat{\\beta}_1\\) case\nAlternatively, one can use two-sample t-test difference means unequal variances.\ncase heteroskedasticity (.e., \\(\\epsilon_{0i} + D_i \\epsilon_{1i} \\neq 0\\) ), residual’s variance depends \\(X\\) heterogeneous treatment effects (.e., \\(\\epsilon_{1i} \\neq 0\\))Robust SE still give consistent estimate \\(\\hat{\\beta}_1\\) caseRobust SE still give consistent estimate \\(\\hat{\\beta}_1\\) caseAlternatively, one can use two-sample t-test difference means unequal variances.Alternatively, one can use two-sample t-test difference means unequal variances.","code":""},{"path":"causal-inference.html","id":"conditional-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.2 Conditional Average Treatment Effects","text":"Treatment effects can different different groups people. words, treatment effects can vary across subgroups.examine heterogeneity across groups (e.g., men vs. women), can estimate conditional average treatment effects (CATE) subgroup\\[\nCATE = E(Y_i(1) - Y_i(0) |D_i, X_i))\n\\]","code":""},{"path":"causal-inference.html","id":"intent-to-treat-effects","chapter":"18 Causal Inference","heading":"18.1.3 Intent-to-treat Effects","text":"encounter non-compliance (either people suppose receive treatment don’t receive , people suppose control group receive treatment), treatment receipt independent potential outcomes confounders.case, difference observed means treatment control groups Average Treatment Effects, Intent--treat Effects (ITT). words, ITT treatment effect receive treatment","code":""},{"path":"causal-inference.html","id":"local-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.4 Local Average Treatment Effects","text":"Instead estimating treatment effects receive treatment (.e., Intent--treat Effects), want estimate treatment effect actually comply treatment. local average treatment effects (LATE) complier average causal effects (CACE). assume don’t use CATE denote complier average treatment effect reserved conditional average treatment effects.Using random treatment assignment instrument, can recover effect treatment compliers.percent compliers increases, Intent--treat Effects Local Average Treatment Effects convergeAs percent compliers increases, Intent--treat Effects Local Average Treatment Effects convergeRule thumb: SE(LATE) = SE(ITT)/(share compliers)Rule thumb: SE(LATE) = SE(ITT)/(share compliers)LATE estimate always greater ITT estimateLATE estimate always greater ITT estimateLATE can also estimated using pure placebo group (Gerber et al. 2010).LATE can also estimated using pure placebo group (Gerber et al. 2010).Partial compliance hard study, IV/2SLS estimator biased, use Bayesian (Long, Little, Lin 2010; Jin Rubin 2009, 2008).Partial compliance hard study, IV/2SLS estimator biased, use Bayesian (Long, Little, Lin 2010; Jin Rubin 2009, 2008).","code":""},{"path":"causal-inference.html","id":"one-sided-noncompliance","chapter":"18 Causal Inference","heading":"18.1.4.1 One-sided noncompliance","text":"One-sided noncompliance sample, compliers never-takersOne-sided noncompliance sample, compliers never-takersWith exclusion restriction (.e., excludability), never-takers results treatment control group (.e., never treated)exclusion restriction (.e., excludability), never-takers results treatment control group (.e., never treated)random assignment, can number never-takers treatment control groupsWith random assignment, can number never-takers treatment control groupsHence,Hence,\\[\nLATE = \\frac{ITT}{\\text{share compliers}}\n\\]","code":""},{"path":"causal-inference.html","id":"two-sided-noncompliance","chapter":"18 Causal Inference","heading":"18.1.4.2 Two-sided noncompliance","text":"Two-sided noncompliance sample, compliers, never-takers, always-takersTwo-sided noncompliance sample, compliers, never-takers, always-takersTo estimate LATE, beyond excludability like One-sided noncompliance case, need assume defiers (.e., monotonicity assumption) (excusable practical studies)estimate LATE, beyond excludability like One-sided noncompliance case, need assume defiers (.e., monotonicity assumption) (excusable practical studies)\\[\nLATE = \\frac{ITT}{\\text{share compliers}}\n\\]","code":""},{"path":"causal-inference.html","id":"population-vs.-sample-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.5 Population vs. Sample Average Treatment Effects","text":"See (Imai, King, Stuart 2008) sample average treatment effect (SATE) diverges population average treatment effect (PATE).stay consistent, section uses notations (Imai, King, Stuart 2008)’s paper.finite population \\(N\\), observe \\(n\\) observations (\\(N>>n\\)), half control half treatment group.unknown data generating process, \\[\nI_i =\n\\begin{cases}\n1 \\text{ unit sample} \\\\\n0 \\text{ otherwise}\n\\end{cases}\n\\]\\[\nT_i =\n\\begin{cases}\n1 \\text{ unit treatment group} \\\\\n0 \\text{ unit control group}\n\\end{cases}\n\\]\\[\n\\text{potential outcome} =\n\\begin{cases}\nY_i(1) \\text{ } T_i = 1 \\\\\nY_i(0) \\text{ } T_i = 0\n\\end{cases}\n\\]Observed outcome \\[\nY_i | I_i = 1= T_i Y_i(1) + (1-T_i)Y_i(0)\n\\]Since can never observed outcome individual, treatment effect always unobserved unit \\(\\)\\[\nTE_i = Y_i(1) - Y_i(0)\n\\]Sample average treatment effect \\[\nSATE = \\frac{1}{n}\\sum_{\\\\{I_i = 1\\}} TE_i\n\\]Population average treatment effect \\[\nPATE = \\frac{1}{N}\\sum_{=1}^N TE_i\n\\]Let \\(X_i\\) observables \\(U_i\\) unobservables unit \\(\\)baseline estimator SATE PATE \\[\n\\begin{aligned}\nD &= \\frac{1}{n/2} \\sum_{\\(I_i = 1, T_i = 1)} Y_i - \\frac{1}{n/2} \\sum_{\\(I_i = 1 , T_i = 0)} Y_i \\\\\n&= \\text{observed sample mean treatment group} \\\\\n&- \\text{observed sample mean control group}\n\\end{aligned}\n\\]Let \\(\\Delta\\) estimation error (deviation truth), additive model\\[\nY_i(t) = g_t(X_i) + h_t(U_i)\n\\]decomposition estimation error \\[\n\\begin{aligned}\nPATE - D = \\Delta &= \\Delta_S + \\Delta_T \\\\\n&= (PATE - SATE) + (SATE - D)\\\\\n&= \\text{sample selection}+ \\text{treatment imbalance} \\\\\n&= (\\Delta_{S_X} + \\Delta_{S_U}) + (\\Delta_{T_X} + \\Delta_{T_U}) \\\\\n&= \\text{(selection observed + selection unobserved)} \\\\\n&+ (\\text{treatment imbalance observed + unobserved})\n\\end{aligned}\n\\]","code":""},{"path":"causal-inference.html","id":"estimation-error-from-sample-selection","chapter":"18 Causal Inference","heading":"18.1.5.1 Estimation Error from Sample Selection","text":"Also known sample selection error\\[\n\\Delta_S = PATE - SATE = \\frac{N - n}{N}(NATE - SATE)\n\\]NATE non-sample average treatment effect (.e., average treatment effect population sample:\\[\nNATE = \\sum_{\\(I_i = 0)} \\frac{TE_i}{N-n}\n\\]equation, zero sample selection error (.e., \\(\\Delta_S = 0\\)), can eitherGet \\(N = n\\) redefining sample population interestGet \\(N = n\\) redefining sample population interest\\(NATE = SATE\\) (e.g., \\(TE_i\\) constant \\(\\) selected sample, population select)\\(NATE = SATE\\) (e.g., \\(TE_i\\) constant \\(\\) selected sample, population select)NoteWhen heterogeneous treatment effects, random sampling can warrant sample selection bias, sample selection error.heterogeneous treatment effects, random sampling can warrant sample selection bias, sample selection error.Since can rarely know true underlying distributions observables (\\(X\\)) unobservables (\\(U\\)), verify whether empirical distributions observables unobservables sample identical population (reduce \\(\\Delta_S\\)). special case,\nSay census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))\nSay willing assume \\(TE_i\\) constant \n\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\n\\(U_i\\), \\(\\Delta_{U}=0\\)\n\nSince can rarely know true underlying distributions observables (\\(X\\)) unobservables (\\(U\\)), verify whether empirical distributions observables unobservables sample identical population (reduce \\(\\Delta_S\\)). special case,Say census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))Say census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))Say willing assume \\(TE_i\\) constant \n\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\n\\(U_i\\), \\(\\Delta_{U}=0\\)\nSay willing assume \\(TE_i\\) constant \\(X_i\\), \\(\\Delta_{S_X} = 0\\)\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\\(U_i\\), \\(\\Delta_{U}=0\\)\\(U_i\\), \\(\\Delta_{U}=0\\)","code":""},{"path":"causal-inference.html","id":"estimation-error-from-treatment-imbalance","chapter":"18 Causal Inference","heading":"18.1.5.2 Estimation Error from Treatment Imbalance","text":"Also known treatment imbalance error\\[\n\\Delta_T = SATE - D\n\\]\\(\\Delta_T \\0\\) treatment control groups balanced (.e., identical empirical distributions) observables (\\(X\\)) unobservables (\\(U\\))However, reality, can readjust observables, unobservables.","code":""},{"path":"causal-inference.html","id":"average-treatment-effects-on-the-treated-and-control","chapter":"18 Causal Inference","heading":"18.1.6 Average Treatment Effects on the Treated and Control","text":"Average Effect treatment Treated (ATT) \\[\n\\begin{aligned}\nATT &= E(Y_i(1) - Y_i(0)|D_i = 1) \\\\\n&= E(Y_i(1)|D_i = 1) - E(Y_i(0) |D_i = 1)\n\\end{aligned}\n\\]Average Effect treatment Control (ATC) (.e., effect weren’t treated) \\[\n\\begin{aligned}\nATC &= E(Y_i(1) - Y_i (0) |D_i =0) \\\\\n&= E(Y_i(1)|D_i = 0) - E(Y_i(0)|D_i = 0)\n\\end{aligned}\n\\]random assignment full compliance,\\[\nATE = ATT = ATC\n\\]Sample average treatment effect treated \\[\nSATT = \\frac{1}{n} \\sum_i TE_i\n\\]\\(TE_i\\) treatment effect unit \\(\\)\\(TE_i\\) treatment effect unit \\(\\)\\(n\\) number treated units sample\\(n\\) number treated units sample\\(\\) belongs subset (.e., sample) population interest treated.\\(\\) belongs subset (.e., sample) population interest treated.Population average treatment effect treated \\[\nPATT = \\frac{1}{N} \\sum_i TE_i\n\\]\\(TE_i\\) treatment effect unit \\(\\)\\(TE_i\\) treatment effect unit \\(\\)\\(N\\) number treated units population\\(N\\) number treated units population\\(\\) belongs population interest treated.\\(\\) belongs population interest treated.","code":""},{"path":"causal-inference.html","id":"quantile-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.7 Quantile Average Treatment Effects","text":"Instead middle point estimate (ATE), can also understand changes distribution outcome variable due treatment.Using quantile regression assumptions (Abadie, Angrist, Imbens 2002; Chernozhukov Hansen 2005), can consistent estimate quantile treatment effects (QTE), can make inference regarding given quantile.","code":""},{"path":"causal-inference.html","id":"mediation-effects","chapter":"18 Causal Inference","heading":"18.1.8 Mediation Effects","text":"additional assumptions (.e., sequential ignorability (Imai, Keele, Tingley 2010; Bullock Ha 2011)), can examine mechanism treatment outcome.causal framework,indirect effect treatment via mediator called average causal mediation effect (ACME)indirect effect treatment via mediator called average causal mediation effect (ACME)direct effect treatment outcome average direct effect (ADE)direct effect treatment outcome average direct effect (ADE)Mediation Section 36","code":""},{"path":"causal-inference.html","id":"log-odds-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.9 Log-odds Treatment Effects","text":"binary outcome variable, might interested log-odds success. See (Freedman 2008) estimate consistent causal effect.Alternatively, attributable effects (Rosenbaum 2002) can also appropriate binary outcome.","code":""},{"path":"experimental-design.html","id":"experimental-design","chapter":"19 Experimental Design","heading":"19 Experimental Design","text":"Randomized Control Trials (RCT) Experiments always likely continue future holy grail causal inference, \nunbiased estimates\nelimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))\nunbiased estimatesunbiased estimateselimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))elimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))RCT means two group treatment (experimental) gorp control group. Hence, introduce treatment (exogenous variable) treatment group, expected difference outcomes two group due treatment.Subjects population randomly assigned either treatment control group. random assignment give us confidence changes outcome variable due treatment, source (variable).can easier hard science RCT can introduce treatment, control environments. ’s hard social scientists subjects usually human, treatment can hard introduce, environments uncontrollable. Hence, social scientists develop different tools (Quasi-experimental) recover causal inference recreate treatment control group environment.RCT, can easily establish internal validityEven though random assignment thing ceteris paribus (.e., holding everything else constant), effect (.e., random manipulation, things equal can observed, average, across treatment control groups).Selection ProblemAssume havebinary treatment \\(D_i =(0,1)\\)binary treatment \\(D_i =(0,1)\\)outcome interest \\(Y_i\\) individual \\(\\)\n\\(Y_{0i}\\) treated\n\\(Y_{1i}\\) treated\noutcome interest \\(Y_i\\) individual \\(\\)\\(Y_{0i}\\) treated\\(Y_{0i}\\) treated\\(Y_{1i}\\) treated\\(Y_{1i}\\) treated\\[\n\\text{Potential Outcome} =\n\\begin{cases}\nY_{1i} \\text{ } D_i = 1 \\\\\nY_{0i} \\text{ } D_i = 0\n\\end{cases}\n\\], observe outcome variable \\[\nY_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i\n\\]’s likely \\(Y_{1i}\\) \\(Y_{0i}\\) distributions (.e., different treatment effect different people). Since can’t see outcomes individual (unless time machine), can make inference regarding average outcome treated .\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n&= (E[Y_{1i}-Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n\\text{Observed difference treatment} &= \\text{Average treatment effect treated} + \\text{Selection bias}\n\\end{aligned}\n\\]average treatment effect average person treated person (another parallel universe) treatedThe average treatment effect average person treated person (another parallel universe) treatedThe selection bias difference treated weren’t treatedThe selection bias difference treated weren’t treatedWith random assignment treatment (\\(D_i\\)) Experimental Design, can \\(D_i\\) independent potential outcomes\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i|D_i = 0] &= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\\\\n&= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] && D_i \\perp Y_i \\\\\n&= E[Y_{1i} - Y_{0i}|D_i = 1] \\\\\n&= E[Y_{1i} - Y_{0i}]\n\\end{aligned}\n\\]Another representation regressionSuppose know effect \\[\nY_{1i} - Y_{0i} = \\rho\n\\]observed outcome variable (individual) can rewritten \\[\n\\begin{aligned}\nY_i &= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\\\\n&= \\alpha + \\rho D_i + \\eta_i\n\\end{aligned}\n\\]\\(\\eta_i\\) = random variation \\(Y_{0i}\\)Hence, conditional expectation individual outcome treatment status \\[\n\\begin{aligned}\nE[Y_i |D_i = 1] &= \\alpha + \\rho &+ E[\\eta_i |D_i = 1] \\\\\nE[Y_i |D_i = 0] &= \\alpha &+ E[\\eta_i |D_i = 0]\n\\end{aligned}\n\\]Thus,\\[\nE[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \\rho + E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\n\\]\\(E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\\) selection bias - correlation regression error term (\\(\\eta_i\\)), regressor (\\(D_i\\))regression, \\[\nE[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0]\n\\]difference outcomes weren’t treated get treated weren’t treated stay untreatedSay control variables (\\(X_i\\)), uncorrelated treatment (\\(D_i\\)), can include model, won’t (principle) affect estimate treatment effect (\\(\\rho\\)) added benefit reducing residual variance, subsequently reduces standard error estimates.\\[\nY_i = \\alpha + \\rho D_i + X_i'\\gamma + \\eta_i\n\\]Examples:Bertrand Mullainathan (2004) randomly assign race job application study effect race callbacks.","code":""},{"path":"experimental-design.html","id":"notes","chapter":"19 Experimental Design","heading":"19.1 Notes","text":"outcomes 0s, can’t use log-like transformation, ’s sensitive outcome unit (J. Chen Roth 2023). info issue, check [Zero-valued Outcomes]. use:Percentage changes Average: using Poisson QMLE, can interpret coefficients effect treatment treated group relative mean control group.Percentage changes Average: using Poisson QMLE, can interpret coefficients effect treatment treated group relative mean control group.Extensive vs. Intensive Margins: Distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1).\nget bounds intensive-margin, use Lee (2009) (assuming treatment monotonic effect outcome)\nExtensive vs. Intensive Margins: Distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1).get bounds intensive-margin, use Lee (2009) (assuming treatment monotonic effect outcome)Percentage changes AverageTo calculate proportional effectHence, conclude treatment effect 1215% higher treated group compared control group.Extensive vs. Intensive MarginsHere, can estimate intensive-margin treatment effect (.e., treatment effect “always-takers”).Since case, bounds contains 0, can’t say much intensive margin always-takers.aim examine sensitivity always-takers, consider scenarios average outcome compliers \\(100 \\times c\\%\\) lower higher always-takers.assume \\(E(Y(1)|Complier) = (1-c)E(Y(1)|Always-taker)\\)assume \\(c = 0.1\\) (.e., treatment, compliers outcome equal 10% outcome always-takers), intensive-margin effect always-takers 6.6 unit outcome.assume \\(c = 0.1\\) (.e., treatment, compliers outcome equal 10% outcome always-takers), intensive-margin effect always-takers 6.6 unit outcome.assume \\(c = 0.5\\) (.e., treatment, compliers outcome equal 50% outcome always-takers), intensive-margin effect always-takers 2.54 unit outcome.assume \\(c = 0.5\\) (.e., treatment, compliers outcome equal 50% outcome always-takers), intensive-margin effect always-takers 2.54 unit outcome.","code":"\nset.seed(123) # For reproducibility\nlibrary(tidyverse)\n\nn <- 1000 # Number of observations\np_treatment <- 0.5 # Probability of being treated\n\n# Step 1: Generate the treatment variable D\nD <- rbinom(n, 1, p_treatment)\n\n# Step 2: Generate potential outcomes\n# Untreated potential outcome (mostly zeroes)\nY0 <- rnorm(n, mean = 0, sd = 1) * (runif(n) < 0.3)\n\n# Treated potential outcome (shifting both the probability of being positive - extensive margin and its magnitude - intensive margin)\nY1 <- Y0 + rnorm(n, mean = 2, sd = 1) * (runif(n) < 0.7)\n\n# Step 3: Combine effects based on treatment\nY_observed <- (1 - D) * Y0 + D * Y1\n\n# Add explicit zeroes to model situations with no effect\nY_observed[Y_observed < 0] <- 0\n\n\ndata <-\n    data.frame(\n        ID = 1:n,\n        Treatment = D,\n        Outcome = Y_observed,\n        X = rnorm(n)\n    ) |>\n    # whether outcome is positive\n    dplyr::mutate(positive = Outcome > 0)\n\n# Viewing the first few rows of the dataset\nhead(data)\n#>   ID Treatment   Outcome          X positive\n#> 1  1         0 0.0000000  1.4783345    FALSE\n#> 2  2         1 2.2369379 -1.4067867     TRUE\n#> 3  3         0 0.0000000 -1.8839721    FALSE\n#> 4  4         1 3.2192276 -0.2773662     TRUE\n#> 5  5         1 0.6649693  0.4304278     TRUE\n#> 6  6         0 0.0000000 -0.1287867    FALSE\n\nhist(data$Outcome)\nlibrary(fixest)\nres_pois <-\n    fepois(\n        fml = Outcome ~ Treatment + X,\n        data = data, \n        vcov = \"hetero\"\n    )\netable(res_pois)\n#>                           res_pois\n#> Dependent Var.:            Outcome\n#>                                   \n#> Constant        -2.223*** (0.1440)\n#> Treatment        2.579*** (0.1494)\n#> X                  0.0235 (0.0406)\n#> _______________ __________________\n#> S.E. type       Heteroskedas.-rob.\n#> Observations                 1,000\n#> Squared Cor.               0.33857\n#> Pseudo R2                  0.26145\n#> BIC                        1,927.9\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# proportional effect\nexp(coefficients(res_pois)[\"Treatment\"]) - 1\n#> Treatment \n#>  12.17757\n\n# SE\nexp(coefficients(res_pois)[\"Treatment\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment\", \"Treatment\"])\n#> Treatment \n#>  1.968684\nres <- causalverse::lee_bounds(\n    df = data,\n    d = \"Treatment\",\n    m = \"positive\",\n    y = \"Outcome\",\n    numdraws = 10\n) |> \n    causalverse::nice_tab(2)\nprint(res)\n#>          term estimate std.error\n#> 1 Lower bound    -0.22      0.09\n#> 2 Upper bound     2.77      0.14\nset.seed(1)\nc_values = c(.1, .5, .7)\n\ncombined_res <- bind_rows(lapply(c_values, function(c) {\n    res <- causalverse::lee_bounds(\n        df = data,\n        d = \"Treatment\",\n        m = \"positive\",\n        y = \"Outcome\",\n        numdraws = 10,\n        c_at_ratio = c\n    )\n    \n    res$c_value <- as.character(c)\n    return(res)\n}))\n\ncombined_res |> \n    dplyr::select(c_value, everything()) |> \n    causalverse::nice_tab()\n#>   c_value           term estimate std.error\n#> 1     0.1 Point estimate     6.60      0.71\n#> 2     0.5 Point estimate     2.54      0.13\n#> 3     0.7 Point estimate     1.82      0.08"},{"path":"experimental-design.html","id":"semi-random-experiment","chapter":"19 Experimental Design","heading":"19.2 Semi-random Experiment","text":"Chicago Open Enrollment Program (Cullen, Jacob, Levitt 2005)Students can apply “choice” schoolsStudents can apply “choice” schoolsMany schools oversubscribed (Demand > Supply)Many schools oversubscribed (Demand > Supply)Resolve scarcity via random lotteriesResolve scarcity via random lotteriesNon-random enrollment, random lottery mean aboveNon-random enrollment, random lottery mean aboveLet\\[\n\\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1)\n\\]\\[\n\\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1)\n\\]Hence, can clearly see \\(\\delta_j \\neq \\theta_j\\) can enroll, ensure win. Thus, intention treat different treatment effect.Non-random enrollment, random lottery means can estimate \\(\\theta_j\\)recover true treatment effect, can use\\[\n\\delta_j = \\frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)}\n\\]\\(\\delta_j\\) = treatment effect\\(\\delta_j\\) = treatment effect\\(W\\) = Whether students win lottery\\(W\\) = Whether students win lottery\\(\\) = Whether student apply lottery\\(\\) = Whether student apply lotteryi = applicationi = applicationj = schoolj = schoolSay have10 win10 loseIntent treatment = Average effect give option choose\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=1; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\\\\n&= 0.25\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=0; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\\\\n&= 0.05\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\text{Intent treatment} &= 0.25 - 0.05 = 0.2 \\\\\n\\text{Treatment effect} &= 1\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nP(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &= \\frac{1+2}{10} = 0.3 \\\\\nP(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &= \\frac{1}{10} = 0.1\n\\end{aligned}\n\\]\\[\n\\text{Treatment effect} = \\frac{0.2}{0.3-0.1} = 1\n\\]knowing recover treatment effect, turn attention main model\\[\nY_{ia} = \\delta W_{ia} + \\lambda L_{ia} + e_{ia}\n\\]\\(W\\) = whether student wins lottery\\(W\\) = whether student wins lottery\\(L\\) = whether student enrolls lottery\\(L\\) = whether student enrolls lottery\\(\\delta\\) = intent treat\\(\\delta\\) = intent treatHence,Conditional lottery, \\(\\delta\\) validConditional lottery, \\(\\delta\\) validBut without lottery, \\(\\delta\\) randomBut without lottery, \\(\\delta\\) randomWinning losing identified within lotteryWinning losing identified within lotteryEach lottery multiple entries. Thus, can within estimatorEach lottery multiple entries. Thus, can within estimatorWe can also include control variables (\\(X_i \\theta\\))\\[\nY_{ia} = \\delta_1 W_{ia} + \\lambda_1 L_{ia} + X_i \\theta + u_{ia}\n\\]\\[\n\\begin{aligned}\nE(\\delta) &= E(\\delta_1) \\\\\nE(\\lambda) &\\neq E(\\lambda_1) && \\text{choosing lottery random}\n\\end{aligned}\n\\]Including \\((X_i \\theta)\\) just shifts around control variables (.e., reweighting lottery), affect treatment effect \\(E(\\delta)\\)","code":""},{"path":"experimental-design.html","id":"rerandomization","chapter":"19 Experimental Design","heading":"19.3 Rerandomization","text":"Since randomization balances baseline covariates average, imbalance variables due random chance can still happen.Since randomization balances baseline covariates average, imbalance variables due random chance can still happen.case “bad” randomization (.e., imbalance important baseline covariates), (Morgan Rubin 2012) introduce idea rerandomization.case “bad” randomization (.e., imbalance important baseline covariates), (Morgan Rubin 2012) introduce idea rerandomization.Rerandomization checking balance randomization process (experiment), eliminate bad allocation (.e., unacceptable balance).Rerandomization checking balance randomization process (experiment), eliminate bad allocation (.e., unacceptable balance).greater number variables, greater likelihood least one covariate imbalanced across treatment groups.\nExample: 10 covariates, probability significant difference \\(\\alpha = .05\\) least one covariate \\(1 - (1-.05)^{10} = 0.4 = 40\\%\\)\ngreater number variables, greater likelihood least one covariate imbalanced across treatment groups.Example: 10 covariates, probability significant difference \\(\\alpha = .05\\) least one covariate \\(1 - (1-.05)^{10} = 0.4 = 40\\%\\)Rerandomization increase treatment effect estimate precision covariates correlated outcome.\nImprovement precision treatment effect estimate depends (1) improvement covariate balance (2) correlation covariates outcome.\nRerandomization increase treatment effect estimate precision covariates correlated outcome.Improvement precision treatment effect estimate depends (1) improvement covariate balance (2) correlation covariates outcome.also need take account rerandomization analysis making inference.also need take account rerandomization analysis making inference.Rerandomization equivalent increasing sample size.Rerandomization equivalent increasing sample size.Alternatives include\nStratified randomization (Johansson Schultzberg 2022)\nMatched randomization (Greevy et al. 2004; Kapelner Krieger 2014)\nMinimization (Pocock Simon 1975)\nAlternatives includeStratified randomization (Johansson Schultzberg 2022)Stratified randomization (Johansson Schultzberg 2022)Matched randomization (Greevy et al. 2004; Kapelner Krieger 2014)Matched randomization (Greevy et al. 2004; Kapelner Krieger 2014)Minimization (Pocock Simon 1975)Minimization (Pocock Simon 1975)Rerandomization CriterionAcceptable randomization based function covariate matrix \\(\\mathbf{X}\\) vector treatment assignments \\(\\mathbf{W}\\)\\[\nW_i =\n\\begin{cases}\n1 \\text{ treated} \\\\\n0 \\text{ control}\n\\end{cases}\n\\]Mahalanobis Distance, \\(M\\), can used criteria acceptable balanceLet \\(M\\) multivariate distance groups means\\[\n\\begin{aligned}\nM &= (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' cov(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C) \\\\\n&= (\\frac{1}{n_T}+ \\frac{1}{n_C})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' cov(\\mathbf{X})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)\n\\end{aligned}\n\\]large sample size “pure” randomization \\(M \\sim \\chi^2_k\\) \\(k\\) number covariates balancedThen let \\(p_a\\) probability accepting randomization. Choosing appropriate \\(p_a\\) tradeoff balance time.rule thumb re-randomize \\(M > \\)","code":""},{"path":"experimental-design.html","id":"two-stage-randomized-experiments-with-interference-and-noncompliance","chapter":"19 Experimental Design","heading":"19.4 Two-Stage Randomized Experiments with Interference and Noncompliance","text":"(Imai, Jiang, Malani 2021)","code":""},{"path":"sampling.html","id":"sampling","chapter":"20 Sampling","heading":"20 Sampling","text":"","code":""},{"path":"sampling.html","id":"simple-sampling","chapter":"20 Sampling","heading":"20.1 Simple Sampling","text":"Simple (random) SamplingIdentify missing points sample collected data","code":"\nlibrary(dplyr)\niris_df <- iris\nset.seed(1)\nsample_n(iris_df, 10)\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#> 1           5.8         2.7          4.1         1.0 versicolor\n#> 2           6.4         2.8          5.6         2.1  virginica\n#> 3           4.4         3.2          1.3         0.2     setosa\n#> 4           4.3         3.0          1.1         0.1     setosa\n#> 5           7.0         3.2          4.7         1.4 versicolor\n#> 6           5.4         3.0          4.5         1.5 versicolor\n#> 7           5.4         3.4          1.7         0.2     setosa\n#> 8           7.6         3.0          6.6         2.1  virginica\n#> 9           6.1         2.8          4.7         1.2 versicolor\n#> 10          4.6         3.4          1.4         0.3     setosa\nlibrary(sampling)\n# set unique id number for each row\niris_df$id = 1:nrow(iris_df)\n\n# Simple random sampling with replacement\nsrswor(10, length(iris_df$id))\n#>   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n#>  [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n#> [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#> [149] 0 0\n\n# Simple random sampling without replacement (sequential method)\nsrswor1(10, length(iris_df$id))\n#>   [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#> [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n#> [149] 0 0\n\n# Simple random sampling with replacement\nsrswr(10, length(iris_df$id))\n#>   [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n#> [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n#> [149] 0 0\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~1)\nlibrary(sampler)\nrsamp(albania,\n      n = 260,\n      over = 0.1, # desired oversampling proportion\n      rep = F)\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nrmissing(sampdf = alsample,\n         colldf = alreceived,\n         col_name = qvKod)"},{"path":"sampling.html","id":"stratified-sampling","chapter":"20 Sampling","heading":"20.2 Stratified Sampling","text":"stratum subset population least one common characteristic.Steps:Identify relevant stratums representation population.Randomly sample select sufficient number subjects stratum.Stratified sampling reduces sampling error.Identify number missing points strata sample collected data","code":"\nlibrary(dplyr)\n# by number of rows\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_n(5)\nsample_iris\n#> # A tibble: 15 × 5\n#> # Groups:   Species [3]\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n#>  1          4.4         3            1.3         0.2 setosa    \n#>  2          5.2         3.5          1.5         0.2 setosa    \n#>  3          5.1         3.8          1.5         0.3 setosa    \n#>  4          5.2         3.4          1.4         0.2 setosa    \n#>  5          4.5         2.3          1.3         0.3 setosa    \n#>  6          5.5         2.5          4           1.3 versicolor\n#>  7          7           3.2          4.7         1.4 versicolor\n#>  8          6.7         3            5           1.7 versicolor\n#>  9          6.1         2.9          4.7         1.4 versicolor\n#> 10          5.5         2.4          3.8         1.1 versicolor\n#> 11          6.4         2.7          5.3         1.9 virginica \n#> 12          6.4         2.8          5.6         2.1 virginica \n#> 13          6.4         3.2          5.3         2.3 virginica \n#> 14          6.8         3.2          5.9         2.3 virginica \n#> 15          7.2         3.6          6.1         2.5 virginica\n\n# by fraction\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_frac(size = .15)\nsample_iris\n#> # A tibble: 24 × 5\n#> # Groups:   Species [3]\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n#>  1          5.5         4.2          1.4         0.2 setosa    \n#>  2          5           3            1.6         0.2 setosa    \n#>  3          5.2         4.1          1.5         0.1 setosa    \n#>  4          4.6         3.1          1.5         0.2 setosa    \n#>  5          5.1         3.7          1.5         0.4 setosa    \n#>  6          4.8         3.4          1.9         0.2 setosa    \n#>  7          5.1         3.3          1.7         0.5 setosa    \n#>  8          5.5         3.5          1.3         0.2 setosa    \n#>  9          5           2.3          3.3         1   versicolor\n#> 10          5.6         2.9          3.6         1.3 versicolor\n#> # ℹ 14 more rows\nlibrary(sampler)\n# Stratified sample using proportional allocation without replacement\nssamp(df=albania, n=360, strata=qarku, over=0.1)\n#> # A tibble: 395 × 45\n#>    qarku  Q_ID bashkia   BAS_ID zaz   njesiaAdministrative COM_ID qvKod zgjedhes\n#>    <fct> <int> <fct>      <int> <fct> <fct>                 <int> <fct>    <int>\n#>  1 Berat     1 Berat         11 ZAZ … \"Berat \"               1101 \"\\\"3…      558\n#>  2 Berat     1 Berat         11 ZAZ … \"Berat \"               1101 \"\\\"3…      815\n#>  3 Berat     1 Berat         11 ZAZ … \"Sinje\"                1108 \"\\\"3…      419\n#>  4 Berat     1 Kucove        13 ZAZ … \"Lumas\"                1104 \"\\\"3…      237\n#>  5 Berat     1 Kucove        13 ZAZ … \"Kucove\"               1201 \"\\\"3…      562\n#>  6 Berat     1 Skrapar       17 ZAZ … \"Corovode\"             1303 \"\\\"3…      829\n#>  7 Berat     1 Berat         11 ZAZ … \"Roshnik\"              1107 \"\\\"3…      410\n#>  8 Berat     1 Ura Vajg…     19 ZAZ … \"Ura Vajgurore\"        1110 \"\\\"3…      708\n#>  9 Berat     1 Kucove        13 ZAZ … \"Perondi\"              1203 \"\\\"3…      835\n#> 10 Berat     1 Kucove        13 ZAZ … \"Kucove\"               1201 \"\\\"3…      907\n#> # ℹ 385 more rows\n#> # ℹ 36 more variables: meshkuj <int>, femra <int>, totalSeats <int>,\n#> #   vendndodhja <fct>, ambienti <fct>, totalVoters <int>, femVoters <int>,\n#> #   maleVoters <int>, unusedBallots <int>, damagedBallots <int>,\n#> #   ballotsCast <int>, invalidVotes <int>, validVotes <int>, lsi <int>,\n#> #   ps <int>, pkd <int>, sfida <int>, pr <int>, pd <int>, pbdksh <int>,\n#> #   adk <int>, psd <int>, ad <int>, frd <int>, pds <int>, pdiu <int>, …\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nsmissing(\n    sampdf = alsample,\n    colldf = alreceived,\n    strata = qarku,\n    col_name = qvKod\n)"},{"path":"sampling.html","id":"unequal-probability-sampling","chapter":"20 Sampling","heading":"20.3 Unequal Probability Sampling","text":"","code":"\nUPbrewer()\nUPmaxentropy()\nUPmidzuno()\nUPmidzunopi2()\nUPmultinomial()\nUPpivotal()\nUPrandompivotal()\nUPpoisson()\nUPsampford()\nUPsystematic()\nUPrandomsystematic()\nUPsystematicpi2()\nUPtille()\nUPtillepi2()"},{"path":"sampling.html","id":"balanced-sampling","chapter":"20 Sampling","heading":"20.4 Balanced Sampling","text":"Purpose: get means population sample auxiliary variablesPurpose: get means population sample auxiliary variablesBalanced sampling different purposive selectionBalanced sampling different purposive selectionBalancing equations\\[\n\\sum_{k \\S} \\frac{\\mathbf{x}_k}{\\pi_k} = \\sum_{k \\U} \\mathbf{x}_k\n\\]\\(\\mathbf{x}_k\\) vector auxiliary variables","code":""},{"path":"sampling.html","id":"cube","chapter":"20 Sampling","heading":"20.4.1 Cube","text":"flight phaseflight phaselanding phaselanding phase","code":"\nsamplecube()\nfastflightcube()\nlandingcube()"},{"path":"sampling.html","id":"stratification","chapter":"20 Sampling","heading":"20.4.2 Stratification","text":"Try replicate population based original multivariate histogram","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        strata = ~stype,\n                        id = ~1)\nbalancedstratification()"},{"path":"sampling.html","id":"cluster","chapter":"20 Sampling","heading":"20.4.3 Cluster","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus1,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~dnum)\nbalancedcluster()"},{"path":"sampling.html","id":"two-stage","chapter":"20 Sampling","heading":"20.4.4 Two-stage","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus2, \n                        fpc = ~fpc1 + fpc2, \n                        id = ~ dnum + snum)\nbalancedtwostage()"},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21 Analysis of Variance (ANOVA)","text":"ANOVA using underlying mechanism linear regression. However, angle ANOVA chooses look slightly different traditional linear regression. can useful case qualitative variables designed experiments.Experimental DesignFactor: explanatory predictor variable studied investigationTreatment (Factor Level): “value” factor applied experimental unitExperimental Unit: person, animal, piece material, etc. subjected treatment(s) provides responseSingle Factor Experiment: one explanatory variable consideredMultifactor Experiment: one explanatory variableClassification Factor: factor control experimenter (observational data)Experimental Factor: assigned experimenterBasics experimental design:Choices statistician make:\nset treatments\nset experimental units\ntreatment assignment (selection bias)\nmeasurement (measurement bias, blind experiments)\nChoices statistician make:set treatmentsset experimental unitstreatment assignment (selection bias)measurement (measurement bias, blind experiments)Advancements experimental design:\nFactorial Experiments:\nconsider multiple factors time (interaction)\nReplication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\n\nRandomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\n\nLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\n\nAdvancements experimental design:Factorial Experiments:\nconsider multiple factors time (interaction)Factorial Experiments:\nconsider multiple factors time (interaction)Replication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\nReplication: repetition experimentassess mean squared errorcontrol precision experiment (power)Randomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\nRandomizationBefore R.. Fisher (1900s), treatments assigned systematically subjectivelyrandomization: assign treatments experimental units random, averages systematic effects control investigatorLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\nLocal control: Blocking StratificationReduce experimental errors increase power placing restrictions randomization treatments experimental units.Randomization may also eliminate correlations due time space.","code":""},{"path":"analysis-of-variance-anova.html","id":"completely-randomized-design-crd","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1 Completely Randomized Design (CRD)","text":"Treatment factor \\(\\ge2\\) treatments levels. Experimental units randomly assigned treatment. number experimental units group can beequal (balanced): nunequal (unbalanced): \\(n_i\\) -th group (= 1,…,).total sample size \\(N=\\sum_{=1}^{}n_i\\)Possible assignments units treatments \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\)probability 1/k selected. experimental unit measured response \\(Y_{ij}\\), j denotes unit denotes treatment.Treatmentwhere \\(\\bar{Y_{.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\)\\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\)grand mean \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{}\\sum_{j}Y_{ij}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-fixed-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1 Single Factor Fixed Effects Model","text":"also known Single Factor (One-Way) ANOVA ANOVA Type model.Partitioning VarianceThe total variability \\(Y_{ij}\\) observation can measured deviation \\(Y_{ij}\\) around overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\)can rewritten :\\[\n\\begin{aligned}\nY_{ij} - \\bar{Y_{..}}&=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{.}} - \\bar{Y_{.}} \\\\\n&= (\\bar{Y_{.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{.}})\n\\end{aligned}\n\\]wherethe first term treatment differences (.e., deviation treatment mean overall mean)second term within treatment differences (.e., deviation observation around treatment mean)\\[\n\\begin{aligned}\n\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &=  \\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})^2+\\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\nSSTO &= SSTR + SSE \\\\\ntotal~SS &= treatment~SS + error~SS \\\\\n(N-1)~d.f. &= (-1)~d.f. + (N - ) ~ d.f.\n\\end{aligned}\n\\]lose d.f. total corrected SSTO estimation mean (\\(\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\))\n, SSTR \\(\\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})=0\\)Accordingly, \\(MSTR= \\frac{SST}{-1}\\) \\(MSR=\\frac{SSE}{N-}\\)ANOVA TableLinear Model Explanation ANOVA","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.1 Cell means model","text":"\\[\nY_{ij}=\\mu_i+\\epsilon\\_{ij}\n\\]\\(Y_{ij}\\) response variable \\(j\\)-th subject \\(\\)-th treatment\\(Y_{ij}\\) response variable \\(j\\)-th subject \\(\\)-th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)observations varianceAll observations varianceExample:\\(= 3\\) (3 treatments) \\(n_1=n_2=n_3=2\\)\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\]\\(X_{k,ij}=1\\) \\(k\\)-th treatment used\\(X_{k,ij}=0\\) OtherwiseNote: intercept term.\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\nn_1 & 0 & 0\\\\\n0 & n_2 & 0\\\\\n0 & 0 & n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_1\\\\\nY_2\\\\\nY_3\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\\begin{array}{c}\n\\bar{Y_1}\\\\\n\\bar{Y_2}\\\\\n\\bar{Y_3}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\tag{21.1}\n\\end{equation}\\]BLUE (best linear unbiased estimator) \\(\\beta=[\\mu_1 \\mu_2\\mu_3]'\\)\\[\nE(\\mathbf{b})=\\beta\n\\]\\[\nvar(\\mathbf{b})=\\sigma^2(\\mathbf{X'X})^{-1}=\\sigma^2\n\\left[\\begin{array}{ccc}\n1/n_1 & 0 & 0\\\\\n0 & 1/n_2 & 0\\\\\n0 & 0 & 1/n_3\\\\\n\\end{array}\\right]\n\\]\\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X'X})^{-1})\\)\\[\n\\begin{aligned}\nMSE &= \\frac{1}{N-} \\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\n    &= \\frac{1}{N-} \\sum_{}[(n_i-1)\\frac{\\sum_{}(Y_{ij}-\\bar{Y_{.}})^2}{n_i-1}] \\\\\n    &= \\frac{1}{N-} \\sum_{}(n_i-1)s_1^2\n\\end{aligned}\n\\]\\(E(s_i^2)=\\sigma^2\\)\\(E(MSE)=\\frac{1}{N-}\\sum_{}(n_i-1)\\sigma^2=\\sigma^2\\)Hence, MSE unbiased estimator \\(\\sigma^2\\), regardless whether treatment means equal .\\(E(MSTR)=\\sigma^2+\\frac{\\sum_{}n_i(\\mu_i-\\mu_.)^2}{-1}\\)\n\\(\\mu_.=\\frac{\\sum_{=1}^{}n_i\\mu_i}{\\sum_{=1}^{}n_i}\\)\ntreatment means equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\).can use \\(F\\)-test equality treatment means:\\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\]\\[H_a: ~al l~ \\mu_i ~ ~ equal \\]\\(F=\\frac{MSTR}{MSE}\\)\nlarge values F support \\(H_a\\) (since MSTR tend exceed MSE \\(H_a\\) holds)\nF near 1 support \\(H_0\\) (upper tail test)Equivalently, \\(H_0\\) true, \\(F \\sim f_{(-1,N-)}\\)\\(F \\leq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)\\(F \\geq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)Note: \\(= 2\\) (2 treatments), \\(F\\)-test = two sample \\(t\\)-test","code":""},{"path":"analysis-of-variance-anova.html","id":"treatment-effects-factor-effects","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2 Treatment Effects (Factor Effects)","text":"Besides Cell means model, another way formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] \\(Y_{ij}\\) \\(j\\)-th response \\(\\)-th treatment\\(\\tau_i\\) \\(\\)-th treatment effect\\(\\mu\\) constant component, common observations\\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\)example, \\(= 3\\), \\(n_1=n_2=n_3=2\\)\\[\\begin{equation}\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{cccc}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_1 \\\\\n\\tau_2 \\\\\n\\tau_3\\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\tag{21.2}\n\\end{equation}\\]However,\\[\n\\mathbf{X'X} =\n\\left(\n\\begin{array}\n{cccc}\n\\sum_{}n_i & n_1 & n_2 & n_3 \\\\\nn_1 & n_1 & 0 & 0 \\\\\nn_2 & 0 & n_2 & 0 \\\\\nn_3 & 0 & 0 & n_3 \\\\\n\\end{array}\n\\right)\n\\]singular thus exist, \\(\\mathbf{b}\\) insolvable (infinite solutions)Hence, impose restrictions parameters model matrix \\(\\mathbf{X}\\) full rank.Whatever restriction use, still :\\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ ~ -th ~ treatment\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-sum-of-tau","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2.1 Restriction on sum of tau","text":"\\(\\sum_{=1}^{}\\tau_i=0\\)implies\\[\n\\mu= \\mu +\\frac{1}{}\\sum_{=1}^{}(\\mu+\\tau_i)\n\\]average treatment mean (grand mean) (overall mean)\\[\n\\begin{aligned}\n\\tau_i  &=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\\n        &= \\text{treatment  mean} - \\text{grand~mean} \\\\\n        &= \\text{treatment  effect}\n\\end{aligned}\n\\]\\[\n\\tau_a=-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, mean -th treatment \\[\n\\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, model need “” parameters:\\[\n\\mu,\\tau_1,\\tau_2,..,\\tau_{-1}\n\\]Equation (21.2) becomes\\[\\begin{equation}\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & -1 & -1 \\\\\n1 & -1 & -1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_1 \\\\\n\\tau_2 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\end{equation}\\]\\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]'\\)Equation (21.1) \\(\\sum_{}\\tau_i=0\\) becomes\\[\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_1} \\\\\n\\hat{\\tau_2} \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_1-n_3 & n_2-n_3\\\\\nn_1-n_3 & n_1+n_3 & n_3\\\\\nn_2-n_3 & n_3 & n_2-n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{1.}-Y_{3.}\\\\\nY_{2.}-Y_{3.}\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\\begin{array}{c}\n\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{1.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{2.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\end{array}\\right]\\\\\n& =\n\\left[\\begin{array}{c}\n\\hat{\\mu}\\\\\n\\hat{\\tau_1}\\\\\n\\hat{\\tau_2}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\]\\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{}\\bar{Y_{.}}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-first-tau","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2.2 Restriction on first tau","text":"R, lm() uses restriction \\(\\tau_1=0\\)previous example, \\(n_1=n_2=n_3=2\\), \\(\\tau_1=0\\).treatment means can written :\\[\n\\begin{aligned}\n\\mu_1 &= \\mu + \\tau_1 = \\mu + 0 = \\mu  \\\\\n\\mu_2 &= \\mu + \\tau_2 \\\\\n\\mu_3 &= \\mu + \\tau_3\n\\end{aligned}\n\\]Hence, \\(\\mu\\) mean response first treatmentIn matrix form,\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\]\\(\\beta = [\\mu,\\tau_2,\\tau_3]'\\)\\[\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_2} \\\\\n\\hat{\\tau_3} \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_2 & n_3\\\\\nn_2 & n_2 & 0\\\\\nn_3 & 0 & n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{2.}\\\\\nY_{3.}\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\n\\begin{array}{c}\n\\bar{Y_{1.}} \\\\\n\\bar{Y_{2.}} - \\bar{Y_{1.}} \\\\\n\\bar{Y_{3.}} - \\bar{Y_{1.}}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\]\\[\nE(\\mathbf{b})= \\beta =\n\\left[\\begin{array}{c}\n{\\mu}\\\\\n{\\tau_2}\\\\\n{\\tau_3}\\\\\n\\end{array}\\right]\n=\n\\left[\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2-\\mu_1\\\\\n\\mu_3-\\mu_1\\\\\n\\end{array}\\right]\n\\]\\[\n\\begin{aligned}\nvar(\\mathbf{b}) &= \\sigma^2(\\mathbf{X'X})^{-1} \\\\\nvar(\\hat{\\mu}) &= var(\\bar{Y_{1.}})=\\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_2}) &= var(\\bar{Y_{2.}}-\\bar{Y_{1.}}) = \\sigma^2/n_2 + \\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_3}) &= var(\\bar{Y_{3.}}-\\bar{Y_{1.}}) = \\sigma^2/n_3 + \\sigma^2/n_1\n\\end{aligned}\n\\]Note three parameterization, ANOVA table sameModel 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\)Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\sum_{} \\tau_i=0\\)Model 3: \\(Y_{ij}= \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\tau_1=0\\)models calculation \\(\\hat{Y}\\) \\[\n\\mathbf{\\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}\n\\]ANOVA TableError(within treatments)\\(\\mathbf{P_1} = \\frac{1}{n}\\mathbf{J}\\)\\(F\\)-statistic \\((-1,N-)\\) degrees freedom, gives value three parameterization, hypothesis test written bit different:\\[\n\\begin{aligned}\n&H_0 : \\mu_1 = \\mu_2 = ... = \\mu_a \\\\\n&H_0 : \\mu + \\tau_1 = \\mu + \\tau_2 = ... = \\mu + \\tau_a \\\\\n&H_0 : \\tau_1 = \\tau_2 = ...= \\tau_a\n\\end{aligned}\n\\]\\(F\\)-test serves preliminary analysis, see difference different factors. -depth analysis, consider different testing treatment effects.","code":""},{"path":"analysis-of-variance-anova.html","id":"testing-of-treatment-effects","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3 Testing of Treatment Effects","text":"Single Treatment Mean \\(\\mu_i\\)Differences Treatment MeansA Contrast Among Treatment MeansA Linear Combination Treatment Means","code":""},{"path":"analysis-of-variance-anova.html","id":"single-treatment-mean","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.1 Single Treatment Mean","text":"\\(\\hat{\\mu_i}=\\bar{Y_{.}}\\) \\(E(\\bar{Y_{.}})=\\mu_i\\)\\(var(\\bar{Y_{}})=\\sigma^2/n_i\\) estimated \\(s^2(\\bar{Y_{.}})=MSE / n_i\\)Since \\(\\frac{\\bar{Y_{.}}-\\mu_i}{s(\\bar{Y_{.}})} \\sim t_{N-}\\) confidence interval \\(\\mu_i\\) \\(\\bar{Y_{.}} \\pm t_{1-\\alpha/2;N-}s(\\bar{Y_{.}})\\),\ncan t-test means difference constant \\(c\\)\\[\n\\begin{aligned}\n&H_0: \\mu_i = c \\\\\n&H_1: \\mu_i \\neq c\n\\end{aligned}\n\\]\\[\nT =\\frac{\\bar{Y_{.}}-c}{s(\\bar{Y_{.}})}\n\\]follows \\(t_{N-}\\) \\(H_0\\) true.\n\\(|T| > t_{1-\\alpha/2;N-}\\), can reject \\(H_0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"differences-between-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.2 Differences Between Treatment Means","text":"Let \\(D=\\mu_i - \\mu_i'\\), also known pairwise comparison\\(D\\) can estimated \\(\\hat{D}=\\bar{Y_{}}-\\bar{Y_{}}'\\) unbiased (\\(E(\\hat{D})=\\mu_i-\\mu_i'\\))Since \\(\\bar{Y_{}}\\) \\(\\bar{Y_{}}'\\) independent, \\[\nvar(\\hat{D})=var(\\bar{Y_{}}) + var(\\bar{Y_{'}}) = \\sigma^2(1/n_i + 1/n_i')\n\\]can estimated \\[\ns^2(\\hat{D}) = MSE(1/n_i + 1/n_i')\n\\]single treatment inference,\\[\n\\frac{\\hat{D}-D}{s(\\hat{D})} \\sim t_{N-}\n\\]hence,\\[\n\\hat{D} \\pm t_{(1-\\alpha/2;N-)}s(\\hat{D})\n\\]Hypothesis tests:\\[\n\\begin{aligned}\n&H_0: \\mu_i = \\mu_i' \\\\\n&H_a: \\mu_i \\neq \\mu_i'\n\\end{aligned}\n\\]can tested following statistic\\[\nT = \\frac{\\hat{D}}{s(\\hat{D})} \\sim t_{1-\\alpha/2;N-}\n\\]reject \\(H_0\\) \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"contrast-among-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.3 Contrast Among Treatment Means","text":"generalize comparison two means, contrastsA contrast linear combination treatment means:\\[\nL = \\sum_{=1}^{}c_i \\mu_i\n\\]\\(c_i\\) non-random constant sum 0:\\[\n\\sum_{=1}^{} c_i = 0\n\\]unbiased estimator contrast L \\[\n\\hat{L} = \\sum_{=1}^{}c_i \\bar{Y}_{.}\n\\]\\(E(\\hat{L}) = L\\). Since \\(\\bar{Y}_{.}\\), = 1,…, independent.\\[\n\\begin{aligned}\nvar(\\hat{L}) &= var(\\sum_{=1}^c_i \\bar{Y}_{.}) = \\sum_{=1}^var(c_i \\bar{Y}_i)  \\\\\n&= \\sum_{=1}^c_i^2 var(\\bar{Y}_i) = \\sum_{=1}^c_i^2 \\sigma^2 /n_i \\\\\n&= \\sigma^2 \\sum_{=1}^{} c_i^2 /n_i\n\\end{aligned}\n\\]Estimation variance:\\[\ns^2(\\hat{L}) = MSE \\sum_{=1}^{} \\frac{c_i^2}{n_i}\n\\]\\(\\hat{L}\\) normally distributed (since linear combination independent normal random variables)., since \\(SSE/\\sigma^2\\) \\(\\chi_{N-}^2\\)\\[\n\\frac{\\hat{L}-L}{s(\\hat{L})} \\sim t_{N-}\n\\]\\(1-\\alpha\\) confidence limits given \\[\n\\hat{L} \\pm t_{1-\\alpha/2; N-}s(\\hat{L})\n\\]Hypothesis testing\\[\n\\begin{aligned}\n&H_0: L = 0 \\\\\n&H_a: L \\neq 0\n\\end{aligned}\n\\]\\[\nT = \\frac{\\hat{L}}{s(\\hat{L})}\n\\]reject \\(H_0\\) \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"linear-combination-of-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.4 Linear Combination of Treatment Means","text":"just like contrast \\(L = \\sum_{=1}^c_i \\mu_i\\) restrictions \\(c_i\\) coefficients.Tests single treatment mean, two treatment means, contrasts can considered form perspective.\\[\n\\begin{aligned}\n&H_0: \\sum c_i \\mu_i = c \\\\\n&H_a: \\sum c_i \\mu_i \\neq c\n\\end{aligned}\n\\]test statistics ( \\(t\\)-stat) can considered equivalently \\(F\\)-tests; \\(F = (T)^2\\) \\(F \\sim F_{1,N-}\\). Since numerator degrees freedom always 1 cases, refer single-degree--freedom tests.Multiple ContrastsTo test simultaneously \\(k \\ge 2\\) contrasts, let \\(T_1,...,T_k\\) t-stat. joint distribution random variables multivariate t-distribution (tests dependent since re based data).Limitations comparing multiple contrasts:confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 \\(t\\)-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 \\(t\\)-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.\noften, results experiment suggest important (.e.,..g, potential significant) relationships.\nprocess studying effects suggests data called data snooping\nconfidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.often, results experiment suggest important (.e.,..g, potential significant) relationships.process studying effects suggests data called data snoopingMultiple Comparison Procedures:TukeyScheffeBonferroni","code":""},{},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"multiple-comparisons-with-a-control","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.5 Multiple comparisons with a control","text":"","code":""},{},{"path":"analysis-of-variance-anova.html","id":"summary-4","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.6 Summary","text":"choosing multiple contrast method:Pairwise\nEqual groups sizes: Tukey\nUnequal groups sizes: Tukey, Scheffe\nPairwiseEqual groups sizes: TukeyUnequal groups sizes: Tukey, ScheffeNot pairwise\ncontrol: Dunnett\ngeneral: Bonferroni, Scheffe\npairwisewith control: Dunnettgeneral: Bonferroni, Scheffe","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-random-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2 Single Factor Random Effects Model","text":"Also known ANOVA Type II models.Treatments chosen larger population. extend inference treatments population restrict inference treatments happened selected study.","code":""},{"path":"analysis-of-variance-anova.html","id":"random-cell-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1 Random Cell Means","text":"\\[\nY_{ij} = \\mu_i + \\epsilon_{ij}\n\\]\\(\\mu_i \\sim N(\\mu, \\sigma^2_{\\mu})\\) independent\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent\\(\\mu_i\\) \\(\\epsilon_{ij}\\) mutually independent \\(=1,...,; j = 1,...,n\\)treatment sample sizes equal\\[\n\\begin{aligned}\nE(Y_{ij}) &= E(\\mu_i) = \\mu \\\\\nvar(Y_{ij}) &= var(\\mu_i) + var(\\epsilon_i) = \\sigma^2_{\\mu} + \\sigma^2\n\\end{aligned}\n\\]Since \\(Y_{ij}\\) independent\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\\\\n&= E(\\mu_i^2 + \\mu_i \\epsilon_{ij'} + \\mu_i \\epsilon_{ij} + \\epsilon_{ij}\\epsilon_{ij'}) - \\mu^2 \\\\\n&= \\sigma^2_{\\mu} + \\mu^2 - \\mu^2 & \\text{} j \\neq j' \\\\\n&= \\sigma^2_{\\mu} & \\text{} j \\neq j'\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{'j'}) &= E(\\mu_i \\mu_{'} + \\mu_i \\epsilon_{'j'}+ \\mu_{'}\\epsilon_{ij}+ \\epsilon_{ij}\\epsilon_{'j'}) - \\mu^2 \\\\\n&= \\mu^2 - \\mu^2 & \\text{} \\neq ' \\\\\n&= 0 \\\\\n\\end{aligned}\n\\]Hence,observations varianceany two observations treatment covariance \\(\\sigma^2_{\\mu}\\)correlation two responses treatment:\\[\n\\begin{aligned}\n\\rho(Y_{ij},Y_{ij'}) &= \\frac{\\sigma^2_{\\mu}}{\\sigma^2_{\\mu}+ \\sigma^2} && \\text{$j \\neq j'$}\n\\end{aligned}\n\\]InferenceIntraclass Correlation Coefficient\\[\n\\frac{\\sigma^2_{\\mu}}{\\sigma^2 + \\sigma^2_{\\mu}}\n\\]measures proportion total variability \\(Y_{ij}\\) accounted variance \\(\\mu_i\\)\\[\n\\begin{aligned}\n&H_0: \\sigma_{\\mu}^2 = 0 \\\\\n&H_a: \\sigma_{\\mu}^2 \\neq 0\n\\end{aligned}\n\\]\\(H_0\\) implies \\(\\mu_i = \\mu\\) , can tested F-test ANOVA.understandings Single Factor Fixed Effects Model Single Factor Random Effects Model different, ANOVA one factor model. difference expected mean squaresIf \\(\\sigma^2_\\mu\\), MSE MSTR expectation (\\(\\sigma^2\\)). Otherwise, \\(E(MSTR) >E(MSE)\\). Large values statistic\\[\nF = \\frac{MSTR}{MSE}\n\\]suggest reject \\(H_0\\).Since \\(F \\sim F_{(-1,(n-1))}\\) \\(H_0\\) holds. \\(F > f_{(1-\\alpha;-1,(n-1))}\\) reject \\(H_0\\).sample sizes equal, \\(F\\)-test can still used, df \\(-1\\) \\(N-\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-mu","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.1 Estimation of \\(\\mu\\)","text":"unbiased estimator \\(E(Y_{ij})=\\mu\\) grand mean: \\(\\hat{\\mu} = \\hat{Y}_{..}\\)variance estimator \\[\n\\begin{aligned}\nvar(\\bar{Y}_{..}) &= var(\\sum_i \\bar{Y}_{.}/) \\\\\n&= \\frac{1}{^2}\\sum_ivar(\\bar{Y}_{.}) \\\\\n&= \\frac{1}{^2}\\sum_i(\\sigma^2_\\mu+\\sigma^2/n) \\\\\n&= \\frac{1}{^2}(\\sigma^2_{\\mu}+\\sigma^2/n) \\\\\n&= \\frac{n\\sigma^2_{\\mu}+ \\sigma^2}{}\n\\end{aligned}\n\\]unbiased estimator variance \\(s^2(\\bar{Y})=\\frac{MSTR}{}\\). Thus \\(\\frac{\\bar{Y}_{..}-\\mu}{s(\\bar{Y}_{..})} \\sim t_{-1}\\)\\(1-\\alpha\\) confidence interval \\(\\bar{Y}_{..} \\pm t_{(1-\\alpha/2;-1)}s(\\bar{Y}_{..})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_musigma2_musigma2","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.2 Estimation of \\(\\sigma^2_\\mu/(\\sigma^2_{\\mu}+\\sigma^2)\\)","text":"random fixed effects model, MSTR MSE independent. sample sizes equal (\\(n_i = n\\) ),\\[\n\\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\sim f_{(-1,(n-1))}\n\\]\\[\nP(f_{(\\alpha/2;-1,(n-1))}\\le \\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\le f_{(1-\\alpha/2;-1,(n-1))}) = 1-\\alpha\n\\]\\[\n\\begin{aligned}\nL &= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(1-\\alpha/2;-1,(n-1))}})-1) \\\\\nU &= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(\\alpha/2;-1,(n-1))}})-1)\n\\end{aligned}\n\\]lower upper \\((L^*,U^*)\\) confidence limits \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\)\\[\n\\begin{aligned}\nL^* &= \\frac{L}{1+L} \\\\\nU^* &= \\frac{U}{1+U}\n\\end{aligned}\n\\]lower limit \\(\\frac{\\sigma^2_\\mu}{\\sigma^2}\\) negative, customary set \\(L = 0\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.3 Estimation of \\(\\sigma^2\\)","text":"\\((n-1)MSE/\\sigma^2 \\sim \\chi^2_{(n-1)}\\), \\((1-\\alpha)\\) confidence interval \\(\\sigma^2\\):\\[\n\\frac{(n-1)MSE}{\\chi^2_{1-\\alpha/2;(n-1)}} \\le \\sigma^2 \\le \\frac{(n-1)MSE}{\\chi^2_{\\alpha/2;(n-1)}}\n\\]can also used case sample sizes equal - df N-.","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_mu","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.4 Estimation of \\(\\sigma^2_\\mu\\)","text":"\\(E(MSE) = \\sigma^2\\) \\(E(MSTR) = \\sigma^2 + n\\sigma^2_\\mu\\). Hence,\\[\n\\sigma^2_{\\mu} = \\frac{E(MSTR)- E(MSE)}{n}\n\\]unbiased estimator \\(\\sigma^2_\\mu\\) given \\[\ns^2_\\mu =\\frac{MSTR-MSE}{n}\n\\]\\(s^2_\\mu < 0\\), set \\(s^2_\\mu = 0\\)sample sizes equal,\\[\ns^2_\\mu = \\frac{MSTR - MSE}{n'}\n\\]\\(n' = \\frac{1}{-1}(\\sum_i n_i- \\frac{\\sum_i n^2_i}{\\sum_i n_i})\\)exact confidence intervals \\(\\sigma^2_\\mu\\), can approximate intervals.Satterthewaite Procedure can used construct approximate confidence intervals linear combination expected mean squares\nlinear combination:\\[\n\\sigma^2_\\mu = \\frac{1}{n} E(MSTR) + (-\\frac{1}{n}) E(MSE)\n\\]\\[\nS = d_1 E(MS_1) + ..+ d_h E(MS_h)\n\\]\\(d_i\\) coefficients.unbiased estimator S \\[\n\\hat{S} = d_1 MS_1 + ...+ d_h  MS_h\n\\]Let \\(df_i\\) degrees freedom associated mean square \\(MS_i\\). Satterthwaite approximation:\\[\n\\frac{(df)\\hat{S}}{S} \\sim \\chi^2_{df}\n\\]\\[\ndf = \\frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}\n\\]approximate \\(1-\\alpha\\) confidence interval S:\\[\n\\frac{(df)\\hat{S}}{\\chi^2_{1-\\alpha/2;df}} \\le S \\le \\frac{(df)\\hat{S}}{\\chi^2_{\\alpha/2;df}}\n\\]single factor random effects model\\[\n\\frac{(df)s^2_\\mu}{\\chi^2_{1-\\alpha/2;df}} \\le \\sigma^2_\\mu \\le \\frac{(df)s^2_\\mu}{\\chi^2_{\\alpha/2;df}}\n\\]\\[\ndf = \\frac{(sn^2_\\mu)^2}{\\frac{(MSTR)^2}{-1}+ \\frac{(MSE)^2}{(n-1)}}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"random-treatment-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.2 Random Treatment Effects Model","text":"\\[\n\\tau_i = \\mu_i - E(\\mu_i) = \\mu_i - \\mu\n\\]\\(\\mu_i = \\mu + \\tau_i\\) \\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]\\(\\mu\\) = constant, common observations\\(\\tau_i \\sim N(0,\\sigma^2_\\tau)\\) independent (random variables)\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent.\\(\\tau_{}, \\epsilon_{ij}\\) independent (=1,…,; j =1,..,n)model concerned balanced single factor ANOVA.Diagnostics MeasuresNon-constant error variance (plots, Levene test, Hartley test).Non-independence errors (plots, Durban-Watson test).Outliers (plots, regression methods).Non-normality error terms (plots, Shapiro-Wilk, Anderson-Darling).Omitted Variable Bias (plots)RemedialWeighted Least Squares[Transformations]Non-parametric Procedures.NoteFixed effect ANOVA relatively robust \nnon-normality\nunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.\nFixed effect ANOVA relatively robust tonon-normalityunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.Lack independence can seriously affect fixed random effect ANVOA.Lack independence can seriously affect fixed random effect ANVOA.","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-fixed-effect-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3 Two Factor Fixed Effect ANOVA","text":"multi-factor experiment ismore efficientprovides infogives validity findings.","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1 Balanced","text":"Assumption:treatment sample sizes equalAll treatment means equal importanceAssume:Factor \\(\\) levels Factor \\(B\\) b levels. \\(\\times b\\) factor levels considered.number treatments level n. \\(N = abn\\) observations study.","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1.1 Cell Means Model","text":"\\[\nY_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{ij}\\) fixed parameters (cell means)\\(= 1,...,\\) = levels Factor \\(j = 1,...,b\\) = levels Factor B.\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(= 1,...,\\), \\(j = 1,..,b\\) \\(k = 1,..,n\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{ij} \\\\\nvar(Y_{ijk}) &= var(\\epsilon_{ijk}) = \\sigma^2\n\\end{aligned}\n\\]Hence,\\[\nY_{ijk} \\sim \\text{indep } N(\\mu_{ij},\\sigma^2)\n\\]model \\[\n\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\n\\]Thus,\\[\n\\begin{aligned}\nE(\\mathbf{Y}) &= \\mathbf{X}\\beta \\\\\nvar(\\mathbf{Y}) &= \\sigma^2 \\mathbf{}\n\\end{aligned}\n\\]Interaction\\[\n(\\alpha \\beta)_{ij} = \\mu_{ij} - (\\mu_{..}+ \\alpha_i + \\beta_j)\n\\]\\(\\mu_{..} = \\sum_i \\sum_j \\mu_{ij}/ab\\) grand mean\\(\\alpha_i = \\mu_{.}-\\mu_{..}\\) main effect factor \\(\\) \\(\\)-th level\\(\\beta_j = \\mu_{.j} - \\mu_{..}\\) main effect factor \\(B\\) \\(j\\)-th level\\((\\alpha \\beta)_{ij}\\) interaction effect factor \\(\\) \\(\\)-th level factor \\(B\\) \\(j\\)-th level.\\((\\alpha \\beta)_{ij} = \\mu_{ij} - \\mu_{.}-\\mu_{.j}+ \\mu_{..}\\)Examine interactions:Examine whether \\(\\mu_{ij}\\) can expressed sums \\(\\mu_{..} + \\alpha_i + \\beta_j\\)Examine whether difference mean responses two levels factor \\(B\\) levels factor \\(\\).Examine whether difference mean response two levels factor \\(\\) levels factor \\(B\\)Examine whether treatment mean curves different factor levels treatment plot parallel.\\(j = 1,...,b\\)\\[\n\\begin{aligned}\n\\sum_i(\\alpha \\beta)_{ij} &= \\sum_i (\\mu_{ij} - \\mu_{..} - \\alpha_i - \\beta_j) \\\\\n&= \\sum_i \\mu_{ij} - \\mu_{..} - \\sum_i \\alpha_i - \\beta_j \\\\\n&= \\mu_{.j} - \\mu_{..}- \\sum_i (\\mu_{.} - \\mu_{..}) - (\\mu_{.j}-\\mu_{..}) \\\\\n&= \\mu_{.j} - \\mu_{..} - \\mu_{..}+ \\mu_{..} - (\\mu_{.j} - \\mu_{..}) \\\\\n&= 0\n\\end{aligned}\n\\]Similarly, \\(\\sum_j (\\alpha \\beta) = 0, = 1,...,\\) \\(\\sum_i \\sum_j (\\alpha \\beta)_{ij} =0\\), \\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"factor-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1.2 Factor Effects Model","text":"\\[\n\\begin{aligned}\n\\mu_{ij} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\end{aligned}\n\\]\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i=0\\)\\(\\beta_j\\) constants subject restriction \\(\\sum_j \\beta_j = 0\\)\\((\\alpha \\beta)_{ij}\\) constants subject restriction \\(\\sum_i(\\alpha \\beta)_{ij} = 0\\) \\(j=1,...,b\\) \\(\\sum_j(\\alpha \\beta)_{ij} = 0\\) \\(= 1,...,\\)\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(k = 1,..,n\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\\\\nvar(Y_{ijk}) &= \\sigma^2 \\\\\nY_{ijk} &\\sim N (\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}, \\sigma^2)\n\\end{aligned}\n\\]\\(1++b+ab\\) parameters. \\(ab\\) parameters Cell Means Model. Factor Effects Model, restrictions limit number parameters can estimated:\\[\n\\begin{aligned}\n1 &\\text{ } \\mu_{..} \\\\\n(-1) &\\text{ } \\alpha_i \\\\\n(b-1) &\\text{ } \\beta_j \\\\\n(-1)(b-1) &\\text{ } (\\alpha \\beta)_{ij}\n\\end{aligned}\n\\]Hence, \\[\n1 + - 1 + b - 1 + ab - - b + 1 = ab\n\\]parameters model.can several restrictions considering model form \\(\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\\)One way:\\[\n\\begin{aligned}\n\\alpha_a  &= \\alpha_1 - \\alpha_2 - ... - \\alpha_{-1} \\\\\n\\beta_b &= -\\beta_1 - \\beta_2 - ... - \\beta_{b-1} \\\\\n(\\alpha \\beta)_{ib} &= -(\\alpha \\beta)_{i1} -(\\alpha \\beta)_{i2} -...-(\\alpha \\beta)_{,b-1} ; = 1,..,\\\\\n(\\alpha \\beta)_{aj}& = -(\\alpha \\beta)_{1j}-(\\alpha \\beta)_{2j} - ... -(\\alpha \\beta)_{-1,j}; j = 1,..,b\n\\end{aligned}\n\\]can fit model least squares maximum likelihoodCell Means Model\nminimize\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk}-\\mu_{ij})^2\n\\]estimators\\[\n\\begin{aligned}\n\\hat{\\mu}_{ij} &= \\bar{Y}_{ij} \\\\\n\\hat{Y}_{ijk} &= \\bar{Y}_{ij} \\\\\ne_{ijk} = Y_{ijk} - \\hat{Y}_{ijk} &= Y_{ijk} - \\bar{Y}_{ij}\n\\end{aligned}\n\\]Factor Effects Model\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{..}-\\alpha_i = \\beta_j - (\\alpha \\beta)_{ij})^2\n\\]subject restrictions\\[\n\\begin{aligned}\n\\sum_i \\alpha_i &= 0 \\\\\n\\sum_j \\beta_j &= 0 \\\\\n\\sum_i (\\alpha \\beta)_{ij} &= 0 \\\\\n\\sum_j (\\alpha \\beta)_{ij} &= 0\n\\end{aligned}\n\\]estimators\\[\n\\begin{aligned}\n\\hat{\\mu}_{..} &= \\bar{Y}_{...} \\\\\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\beta}_j &= \\bar{Y}_{.j.}-\\bar{Y}_{...} \\\\\n(\\hat{\\alpha \\beta})_{ij} &= \\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.}+ \\bar{Y}_{...}\n\\end{aligned}\n\\]fitted values\\[\n\\hat{Y}_{ijk} = \\bar{Y}_{...}+ (\\bar{Y}_{..}- \\bar{Y}_{...})+ (\\bar{Y}_{.j.}- \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{..}-\\bar{Y}_{.j.}+\\bar{Y}_{...}) = \\bar{Y}_{ij.}\n\\]\\[\n\\begin{aligned}\ne_{ijk} &= Y_{ijk} - \\bar{Y}_{ij.} \\\\\ne_{ijk} &\\sim \\text{ indep } (0,\\sigma^2)\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ns^2_{\\hat{\\mu}..} &= \\frac{MSE}{nab} \\\\\ns^2_{\\hat{\\alpha}_i} &= MSE(\\frac{1}{nb} - \\frac{1}{nab}) \\\\\ns^2_{\\hat{\\beta}_j} &= MSE(\\frac{1}{na} - \\frac{1}{nab}) \\\\\ns^2_{(\\hat{\\alpha\\beta})_{ij}} &= MSE (\\frac{1}{n} - \\frac{1}{na}- \\frac{1}{nb} + \\frac{1}{nab})\n\\end{aligned}\n\\]","code":""},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"unbalanced","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.2 Unbalanced","text":"unequal numbers replications treatment combinations:Observational studiesDropouts designed studiesLarger sample sizes inexpensive treatmentsSample sizes match population makeup.Assume factor combination least 1 observation (empty cells)Consider model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]sample sizes : \\(n_{ij}\\):\\[\n\\begin{aligned}\nn_{.} &= \\sum_j n_{ij} \\\\\nn_{.j} &= \\sum_i n_{ij} \\\\\nn_T &= \\sum_i \\sum_j n_{ij}\n\\end{aligned}\n\\]Problem \\[\nSSTO \\neq SSA + SSB + SSAB + SSE\n\\](design non-orthogonal)\\(= 1,...,-1,\\)\\[\nu_i = \\begin{cases} +1 & \\text{obs -th level Factor 1} \\\\ -1 & \\text{obs -th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\ \\end{cases}\n\\]\\(j=1,...,b-1\\)\\[\nv_i =\n\\begin{cases} +1 & \\text{obs j-th level Factor 1} \\\\ -1 & \\text{obs b-th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\\n\\end{cases}\n\\]can use indicator variables predictor variables \\(\\mu_{..}, \\alpha_i ,\\beta_j, (\\alpha \\beta)_{ij}\\) unknown parameters.\\[\nY = \\mu_{..} + \\sum_{=1}^{-1} \\alpha_i u_i + \\sum_{j=1}^{b-1} \\beta_j v_j + \\sum_{=1}^{-1} \\sum_{j=1}^{b-1}(\\alpha \\beta)_{ij} u_i v_j + \\epsilon\n\\]test hypotheses, use extra sum squares idea.interaction effects\\[\n\\begin{aligned}\n&H_0: (\\alpha \\beta)_{ij} = 0 \\\\\n&H_a: \\text{}(\\alpha \\beta)_{ij} =0\n\\end{aligned}\n\\]test\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\\n&H_a: \\text{} \\beta_j = 0\n\\end{aligned}\n\\]Analysis Factor Means(e.g., contrasts) analogous balanced case, modifications formulas means standard errors account unequal sample sizes., can fit cell means model consider regression perspectiveIf empty cells (.e., factor combinations observation), equivalent regression approach can’t used. can still partial analyses","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-random-effects-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.4 Two-Way Random Effects ANOVA","text":"\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i \\sim N(0,\\sigma^2_{\\alpha}), = 1,..,\\) (independent)\\(\\beta_j \\sim N(0,\\sigma^2_{\\beta}), j = 1,..,b\\) (independent)\\((\\alpha \\beta)_{ij} \\sim N(0,\\sigma^2_{\\alpha \\beta}),=1,...,,j=1,..,b\\) (independent)\\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent)\\(\\alpha_i, \\beta_j, (\\alpha \\beta)_{ij}\\) pairwise independentTheoretical means, variances, covariances \\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} \\\\\nvar(Y_{ijk}) &= \\sigma^2_Y= \\sigma^2_\\alpha + \\sigma^2_\\beta +  \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\end{aligned}\n\\]\\(Y_{ijk} \\sim N(\\mu_{..},\\sigma^2_\\alpha + \\sigma^2_\\beta + \\sigma^2_{\\alpha \\beta} + \\sigma^2)\\)\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{ij'k'}) &= \\sigma^2_{\\alpha}, j \\neq j' \\\\\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_{\\beta}, \\neq '\\\\\ncov(Y_{ijk},Y_{ijk'}) &= \\sigma^2_\\alpha + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= , \\neq ', j \\neq j'\n\\end{aligned}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-mixed-effects-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5 Two-Way Mixed Effects ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5.1 Balanced","text":"One fixed factor, random treatment levels, mixed effects model mixed modelRestricted mixed model 2-way ANOVA:\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i\\): fixed effects constraints subject restriction \\(\\sum \\alpha_i = 0\\)\\(\\beta_j \\sim indep N(0,\\sigma^2_\\beta)\\)\\((\\alpha \\beta)_{ij} \\sim N(0,\\frac{-1}{}\\sigma^2_{\\alpha \\beta})\\) subject restriction \\(\\sum_i (\\alpha \\beta)_{ij} = 0\\) j, variance written proportion convenience; makes expected mean squares simpler (assumed \\(var((\\alpha \\beta)_{ij}= \\sigma^2_{\\alpha \\beta}\\))\\(cov((\\alpha \\beta)_{ij},(\\alpha \\beta)_{'j'}) = - \\frac{1}{} \\sigma^2_{\\alpha \\beta}, \\neq '\\)\\(\\epsilon_{ijk}\\sim indepN(0,\\sigma^2)\\)\\(\\beta_j, (\\alpha \\beta)_{ij}, \\epsilon_{ijk}\\) pairwise independentTwo-way mixed models written “unrestricted” form, restrictions interaction effects \\((\\alpha \\beta)_{ij}\\), pairwise independent.Let \\(\\beta^*, (\\alpha \\beta)^*_{ij}\\) unrestricted random effects, \\((\\bar{\\alpha \\beta})_{ij}^*\\) means averaged fixed factor level random factor B.\\[\n\\begin{aligned}\n\\beta_j &= \\beta_j^* + (\\bar{\\alpha \\beta})_{ij}^* \\\\\n(\\alpha \\beta)_{ij} &= (\\alpha \\beta)_{ij}^* - (\\bar{\\alpha \\beta})_{ij}^*\n\\end{aligned}\n\\]consider restricted model general. consider restricted form.\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) &= \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\end{aligned}\n\\]Responses random factor \\((B)\\) level correlated\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{ijk'}) &= E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\\\\n&= \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} , k \\neq k'\n\\end{aligned}\n\\]Similarly,\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_\\beta - \\frac{1}{} \\sigma^2_{\\alpha\\ \\beta}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= 0,  j \\neq j'\n\\end{aligned}\n\\]Hence, can see way don’t dependence \\(Y\\) don’t share random effect.advantage restricted mixed model 2 observations random factor b level can positively negatively correlated. unrestricted model, can positively correlated.Fixed ANOVA(, B Fixed)Random ANOVA(,B random)Mixed ANVOA(fixed, B random)fixed, random, mixed models (balanced), ANOVA table sums squares calculations identical. (also true df mean squares). difference expected mean squares, thus test statistics.Random ANOVA, test\\[\n\\begin{aligned}\n&H_0: \\sigma^2 = 0 \\\\\n&H_a: \\sigma^2 > 0\n\\end{aligned}\n\\]considering \\(F= \\frac{MSA}{MSAB} \\sim F_{-1;(-1)(b-1)}\\)test statistic used mixed models, case testing null hypothesis \\(\\alpha_i = 0\\)test statistic different null hypothesis fixed effects model.Fixed ANOVA(&B fixed)Random ANOVA(&B random)Mixed ANOVA(fixed, B random)Estimation Variance ComponentsIn random mixed effects models, interested estimating variance components\nVariance component \\(\\sigma^2_\\beta\\) mixed ANOVA.\\[\nE(\\sigma^2_\\beta) = \\frac{E(MSB)-E(MSE)}{na} = \\frac{\\sigma^2 + na \\sigma^2_\\beta - \\sigma^2}{na} = \\sigma^2_\\beta\n\\]can estimated \\[\n\\hat{\\sigma}^2_\\beta = \\frac{MSB - MSE}{na}\n\\]Confidence intervals variance components can constructed (approximately) using Satterthwaite procedure MLS procedure (like 1-way random effects)Estimation Fixed Effects Mixed Models\\[\n\\begin{aligned}\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\mu}_{.} &= \\bar{Y}_{...} + (\\bar{Y}_{..}- \\bar{Y}_{...}) = \\bar{Y}_{..}  \\\\\n\\sigma^2(\\hat{\\alpha}_i) &= \\frac{\\sigma^2 + n \\sigma^2_{\\alpha \\beta}}{bn} = \\frac{E(MSAB)}{bn} \\\\\ns^2(\\hat{\\alpha}_i) &= \\frac{MSAB}{bn}\n\\end{aligned}\n\\]Contrasts Fixed Effects\\[\n\\begin{aligned}\nL &= \\sum c_i \\alpha_i \\\\\n\\sum c_i &= 0 \\\\\n\\hat{L} &= \\sum c_i \\hat{\\alpha}_i \\\\\n\\sigma^2(\\hat{L}) &= \\sum c^2_i \\sigma^2 (\\hat{\\alpha}_i) \\\\\ns^2(\\hat{L}) &= \\frac{MSAB}{bn} \\sum c^2_i\n\\end{aligned}\n\\]Confidence intervals tests can constructed usual","code":""},{"path":"analysis-of-variance-anova.html","id":"unbalanced-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5.2 Unbalanced","text":"mixed model = 2, b = 4\\[\n\\begin{aligned}\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\\\\nvar(\\beta_j)&= \\sigma^2_\\beta \\\\\nvar((\\alpha \\beta)_{ij})&= \\frac{2-1}{2}\\sigma^2_{\\alpha \\beta} = \\frac{\\sigma^2_{\\alpha \\beta}}{2} \\\\\nvar(\\epsilon_{ijk}) &= \\sigma^2 \\\\\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) &= \\sigma^2_{\\beta} + \\frac{\\sigma^2_{\\alpha \\beta}}{2} + \\sigma^2 \\\\\ncov(Y_{ijk},Y_{ijk'}) &= \\sigma^2 + \\frac{\\sigma^2_{\\alpha \\beta}}{2}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_{\\beta} - \\frac{\\sigma^2_{\\alpha \\beta}}{2}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= 0, j \\neq j'\n\\end{aligned}\n\\]assume\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\beta, M)\n\\]\\(M\\) block diagonaldensity function\\[\nf(\\mathbf{Y}) = \\frac{1}{(2\\pi)^{N/2}|M|^{1/2}}exp(-\\frac{1}{2}\\mathbf{(Y - X \\beta)' M^{-1}(Y-X\\beta)})\n\\]knew variance components, use GLS:\\[\n\\hat{\\beta}_{GLS} = \\mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}\n\\]usually don’t know variance components \\(\\sigma^2, \\sigma^2_\\beta, \\sigma^2_{\\alpha \\beta}\\) make \\(M\\)\nAnother way get estimates Maximum likelihood estimationwe try maximize log\\[\n\\ln L = - \\frac{N}{2} \\ln (2\\pi) - \\frac{1}{2}\\ln|M| - \\frac{1}{2} \\mathbf{(Y-X \\beta)'\\Sigma^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"nonparametric-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2 Nonparametric ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"kruskal-wallis","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2.1 Kruskal-Wallis","text":"Generalization independent samples Wilcoxon Rank sum test 2 independent samples (like F-test one-way ANOVA generalization several independent samples two sample t-test)Consider one-way case:\\(\\ge2\\) treatments\\(n_i\\) sample size \\(\\)-th treatment\\(Y_{ij}\\) \\(j\\)-th observation \\(\\)-th treatment.make assumption normalityWe assume observations \\(\\)-th treatment random sample continuous CDF \\(F_i\\), = 1,..,n, mutually independent.\\[\n\\begin{aligned}\n&H_0: F_1 = F_2 = ... = F_a \\\\\n&H_a: F_i < F_j \\text{ } \\neq j\n\\end{aligned}\n\\]distribution location-scale family, \\(H_0: \\theta_1 = \\theta_2 = ... = \\theta_a\\))ProcedureRank \\(N = \\sum_{=1}^n_i\\) observations ascending order. Let \\(r_{ij} = rank(Y_{ij})\\), note \\(\\sum_i \\sum_j r_{ij} = 1 + 2 .. + N = \\frac{N(N+1)}{2}\\)Calculate rank sums averages:\\[\nr_{.} = \\sum_{j=1}^{n_i} r_{ij}\n\\] \\[\n\\bar{r}_{.} = \\frac{r_{.}}{n_i}, = 1,..,\n\\]Calculate test statistic ranks: \\[\n\\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}}\n\\] \\(SSTR = \\sum n_i (\\bar{r}_{.}- \\bar{r}_{..})^2\\) \\(SSTO = \\sum \\sum (\\bar{r}_{ij}- \\bar{r}_{..})^2\\)large \\(n_i\\) (\\(\\ge 5\\) observations) Kruskal-Wallis statistic approximated \\(\\chi^2_{-1}\\) distribution treatment means equal. Hence, reject \\(H_0\\) \\(\\chi^2_{KW} > \\chi^2_{(1-\\alpha;-1)}\\).sample sizes small, one can exhaustively work possible distinct ways assigning N ranks observations treatments calculate value KW statistic case (\\(\\frac{N!}{n_1!..n_a!}\\) possible combinations). \\(H_0\\) assignments equally likely.","code":""},{"path":"analysis-of-variance-anova.html","id":"friedman-test","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2.2 Friedman Test","text":"responses \\(Y_{ij} = 1,..,n, j = 1,..,r\\) randomized complete block design normally distributed (constant variance), nonparametric test helpful.distribution-free rank-based test comparing treatments setting Friedman test. Let \\(F_{ij}\\) CDF random \\(Y_{ij}\\), corresponding observed value \\(y_{ij}\\)null hypothesis, \\(F_{ij}\\) identical treatments j separately block .\\[\n\\begin{aligned}\n&H_0: F_{i1} = F_{i2} = ... = F_{ir}  \\text{ } \\\\\n&H_a: F_{ij} < F_{ij'} \\text{ } j \\neq j' \\text{ } \n\\end{aligned}\n\\]location parameter distributions, treatment effects can tested:\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = ... = \\tau_r \\\\\n&H_a: \\tau_j > \\tau_{j'} \\text{ } j \\neq j'\n\\end{aligned}\n\\]ProcedureRank observations r treatments separately within block (ascending order; ties, tied observation given mean ranks involved). Let ranks called \\(r_{ij}\\)Calculate Friedman test statistic\\[\n\\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}}\n\\] \\[\n\\begin{aligned}\nSSTR &= n \\sum (\\bar{r}_{.j}-\\bar{r}_{..})^2 \\\\\nSSE &= \\sum \\sum (r_{ij} - \\bar{r}_{.j})^2 \\\\\n\\bar{r}_{.j} &= \\frac{\\sum_i r_{ij}}{n}\\\\\n\\bar{r}_{..} &= \\frac{r+1}{2}\n\\end{aligned}\n\\]ties, can rewritten \\[\n\\chi^2_{F} = [\\frac{12}{nr(n+1)}\\sum_j r_{.j}^2] - 3n(r+1)\n\\]large number blocks, \\(\\chi^2_F\\) approximately \\(\\chi^2_{r-1}\\) \\(H_0\\). Hence, reject \\(H_0\\) \\(\\chi^2_F > \\chi^2_{(1-\\alpha;r-1)}\\)\nexact null distribution \\(\\chi^2_F\\) can derived since r! possible ways assigning ranks 1,2,…,r r observations within block. n blocks thus \\((r!)^n\\) possible assignments ranks, equally likely \\(H_0\\) true.","code":""},{"path":"analysis-of-variance-anova.html","id":"sample-size-planning-for-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3 Sample Size Planning for ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1 Balanced Designs","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-studies","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.1 Single Factor Studies","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"fixed-cell-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.1.1 Fixed cell means","text":"\\[\nP(F>f_{(1-\\alpha;-1,N-)}|\\phi) = 1 - \\beta\n\\]\\(\\phi\\) non-centrality parameter (measures unequal treatment means \\(\\mu_i\\) )\\[\n\\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n}{}\\sum_i (\\mu_i - \\mu_.)^2} , (n_i \\equiv n)\n\\]\\[\n\\mu_. = \\frac{\\sum \\mu_i}{}\n\\]decide power probabilities use non-central F distribution.use power table directly effects fixed design balanced using minimum range factor level means desired differences\\[\n\\Delta = \\max(\\mu_i) - \\min(\\mu_i)\n\\]Hence, need\\(\\alpha\\) level\\(\\Delta\\)\\(\\sigma\\)\\(\\beta\\)Notes:\\(\\Delta/\\sigma\\) small greatly affects sample size, \\(\\Delta/\\sigma\\) large.Reducing \\(\\alpha\\) \\(\\beta\\) increases required sample sizes.Error estimating \\(\\sigma\\) can make large difference.","code":""},{"path":"analysis-of-variance-anova.html","id":"multi-factor-studies","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.2 Multi-factor Studies","text":"noncentral \\(F\\) tables can used hereFor two-factor fixed effect modelTest interactions:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\alpha \\beta_{ij})^2}{(-1)(b-1)+1}} = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\mu_{ij}- \\mu_{.} - \\mu_{.j} + \\mu_{..})^2}{(-1)(b-1)+1}} \\\\\n\\upsilon_1 &= (-1)(b-1) \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Test Factor \\(\\) main effects:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum \\alpha_i^2}{}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nb \\sum (\\mu_{.}- \\mu_{..})^2}{}} \\\\\n\\upsilon_1 &= -1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Test Factor \\(B\\) main effects:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum \\beta_j^2}{b}} = \\frac{1}{\\sigma}\\sqrt{\\frac{na \\sum (\\mu_{.j}- \\mu_{..})^2}{b}} \\\\\n\\upsilon_1 &= b-1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Procedure:Specify minimum range Factor \\(\\) meansObtain sample sizes \\(r = \\). resulting sample size \\(bn\\), \\(n\\) can obtained.Repeat first 2 steps Factor \\(B\\) minimum range.Choose greater number sample size \\(\\) \\(B\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-experiments","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.2 Randomized Block Experiments","text":"Analogous completely randomized designs . power F-test treatment effects randomized block design uses non-centrality parameter completely randomized design:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{r} \\sum (\\mu_i - \\mu_.)^2}\n\\]However, power level different randomized block design becauseerror variance \\(\\sigma^2\\) differentdf(MSE) different.","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.4 Randomized Block Designs","text":"improve precision treatment comparisons, can reduce variability among experimental units. can group experimental units blocks block contains relatively homogeneous units.Within block, random assignment treatments units (separate random assignment block)number units per block multiple number factor combinations.Commonly, use treatment block.Benefits BlockingReduction variability estimators treatment means\nImproved power t-tests F-tests\nNarrower confidence intervals\nSmaller MSE\nReduction variability estimators treatment meansImproved power t-tests F-testsNarrower confidence intervalsSmaller MSECompare treatments different conditions (related different blocks).Compare treatments different conditions (related different blocks).Loss Blocking (little lose)don’t blocking well, waste df negligible block effects used estimate \\(\\sigma^2\\)Hence, df \\(t\\)-tests denominator df \\(F\\)-tests reduced without reducing MSE small loss power tests.Consider\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]\\(= 1, 2, \\dots, n\\)\\(j = 1, 2, \\dots, r\\)\\(\\mu_{..}\\): overall mean response, averaging across blocks treatments\\(\\rho_i\\): block effect, average difference response -th block (\\(\\sum \\rho_i =0\\))\\(\\tau_j\\) treatment effect, average across blocks (\\(\\sum \\tau_j = 0\\))\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random experimental error., assume block treatment effects additive. difference average response pair treatments within block\\[\n(\\mu_{..} +  \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j') = \\tau_j - \\tau_j'\n\\]\\(=1,..,n\\) blocks\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\bar{Y}_{..} \\\\\n\\hat{\\rho}_i &= \\bar{Y}_{.} - \\bar{Y}_{..} \\\\\n\\hat{\\tau}_j &= \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\hat{Y}_{ij} &= \\bar{Y}_{..} + (\\bar{Y}_{.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j}- \\bar{Y}_{..}) = \\bar{Y}_{.} + \\bar{Y}_{.j} - \\bar{Y}_{..} \\\\\ne_{ij} &= Y_{ij} - \\hat{Y}_{ij} = Y_{ij}- \\bar{Y}_{.} - \\bar{Y}_{.j} + \\bar{Y}_{..}\n\\end{aligned}\n\\]ANOVA tableFixed TreatmentsE(MS)Random TreatmentsE(MS)F-tests\\[\n\\begin{aligned}\nH_0: \\tau_1 = \\tau_2 = ... = \\tau_r = 0 && \\text{Fixed Treatment Effects} \\\\\nH_a: \\text{} \\tau_j = 0 \\\\\n\\\\\nH_0: \\sigma^2_{\\tau} = 0 && \\text{Random Treatment Effects} \\\\\nH_a: \\sigma^2_{\\tau} \\neq 0\n\\end{aligned}\n\\]cases \\(F = \\frac{MSTR}{MSE}\\), reject \\(H_0\\) \\(F > f_{(1-\\alpha; r-1,(n-1)(r-1))}\\)don’t use F-test compare blocks, becauseWe priori blocs differentRandomization done “within” block.estimate efficiency gained blocking (relative completely randomized design).\\[\n\\begin{aligned}\n\\hat{\\sigma}^2_{CR} &= \\frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\\\\n\\hat{\\sigma}^2_{RB} &= MSE \\\\\n\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}} &= \\text{1} \\\\\n\\end{aligned}\n\\]completely randomized experiment \\[\n(\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}}-1)\\%%\n\\]observations randomized block design get MSEIf batches randomly selected random effects. , experiment repeated, new sample batches selected,d yielding new values \\(\\rho_1, \\rho_2,...,\\rho_i\\) .\\[\n\\rho_1, \\rho_2,...,\\rho_j \\sim N(0,\\sigma^2_\\rho)\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) fixed\\(\\rho_i\\): random iid \\(N(0,\\sigma^2_p)\\)\\(\\tau_j\\) fixed (random) \\(\\sum \\tau_j = 0\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\)Fixed Treatment\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) &= \\sigma^2_{\\rho} + \\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= \\sigma^2 , j \\neq j' \\text{ treatments within block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) &= 0 , \\neq ' , j \\neq j'\n\\end{aligned}\n\\]Correlation 2 observations block\\[\n\\frac{\\sigma^2_{\\rho}}{\\sigma^2 + \\sigma^2_{\\rho}}\n\\]expected MS additive fixed treatment effect, random block effect isInteractions Blocks\nwithout replications within block treatment, can’t consider interaction block treatment block effect fixed. Hence, random block effect, \\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + (\\rho \\tau)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) constant\\(\\rho_i \\sim idd N(0,\\sigma^2_{\\rho})\\) random\\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\))\\((\\rho \\tau)_{ij} \\sim N(0,\\frac{r-1}{r}\\sigma^2_{\\rho \\tau})\\) \\(\\sum_j (\\rho \\tau)_{ij}=0\\) \\(cov((\\rho \\tau)_{ij},(\\rho \\tau)_{ij'})= -\\frac{1}{r} \\sigma^2_{\\rho \\tau}\\) \\(j \\neq j'\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\) randomNote: special case mixed 2-factor model 1 observation per “cell”\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) &= \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, j \\neq j' \\text{ obs block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) &= 0, \\neq ', j \\neq j' \\text{ obs different blocks independent}\n\\end{aligned}\n\\]sum squares degrees freedom interaction model additive model. difference exists expected mean squaresNo exact test possible block effects interaction present (important blocks used primarily reduce experimental error variability)\\(E(MSE) = \\sigma^2 + \\sigma^2_{\\rho \\tau}\\) error term variance interaction variance \\(\\sigma^2_{\\rho \\tau}\\). can’t estimate components separately model. two confounded.1 observation per treatment block combination, one can consider interaction fixed block effects, called generalized randomized block designs (multifactor analysis).","code":""},{"path":"analysis-of-variance-anova.html","id":"tukey-test-of-additivity","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.4.1 Tukey Test of Additivity","text":"(Tukey’s 1 df test additivity)formal test interaction effects blocks treatments randomized block design. can also considered testing additivity 2-way analyses one observation per cell.consider less restricted interaction term\\[\n(\\rho \\tau)_{ij} = D\\rho_i \\tau_j \\text{(D: Constant)}\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + D\\rho_i \\tau_j + \\epsilon_{ij}\n\\]least square estimate MLE D\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j \\rho_i \\tau_j Y_{ij}}{\\sum_i \\rho_i^2 \\sum_j \\tau^2_j}\n\\]replacing parameters estimates\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j (\\bar{Y}_{.}- \\bar{Y}_{..})(\\bar{Y}_{.j}- \\bar{Y}_{..})Y_{ij}}{\\sum_i (\\bar{Y}_{.}- \\bar{Y}_{..})^2 \\sum_j(\\bar{Y}_{.j}- \\bar{Y}_{..})^2}\n\\]Thus, interaction sum squares\\[\nSSint = \\sum_i \\sum_j \\hat{D}^2(\\bar{Y}_{.}- \\bar{Y}_{..})^2(\\bar{Y}_{.j}- \\bar{Y}_{..})^2\n\\]ANOVA decomposition\\[\nSSTO = SSBL + SSTR + SSint + SSRem\n\\]\\(SSRem\\): remainder sum squares\\[\nSSRem = SSTO - SSBL - SSTR - SSint\n\\]\\(D = 0\\) (.e., interactions type \\(D \\rho_i \\tau_j\\)). \\(SSint\\) \\(SSRem\\) independent \\(\\chi^2_{1,rn-r-n}\\).\\(D = 0\\),\\[\nF = \\frac{SSint/1}{SSRem/(rn-r-n)} \\sim f_{(1-\\alpha;rn-r-n)}\n\\]\\[\n\\begin{aligned}\n&H_0: D = 0 \\text{ interaction present} \\\\\n&H_a: D \\neq 0 \\text{ interaction form $D \\rho_i \\tau_j$ present}\n\\end{aligned}\n\\]reject \\(H_0\\) \\(F > f_{(1-\\alpha;1,nr-r-n)}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"nested-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.5 Nested Designs","text":"Let \\(\\mu_{ij}\\) mean response factor -th level factor B j-th level.\nfactors crossed, \\(j\\)-th level B levels .\nfactor B nested within , j-th level B level 1 nothing common j-th level B level 2.Factors can’t manipulated designated classification factors, opposed experimental factors (.e., assign experimental units).","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-nested-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.5.1 Two-Factor Nested Designs","text":"Consider B nested within .factors fixedAll treatment means equally important.Mean responses\\[\n\\mu_{.} = \\sum_j \\mu_{ij}/b\n\\]Main effect factor \\[\n\\alpha_i = \\mu_{.} - \\mu_{..}\n\\]\\(\\mu_{..} = \\frac{\\mu_{ij}}{ab} = \\frac{\\sum_i \\mu_{.}}{}\\) \\(\\sum_i \\alpha_i = 0\\)Individual effects \\(B\\) denoted \\(\\beta_{j()}\\) \\(j()\\) indicates \\(j\\)-th level factor \\(B\\) nested within -h level factor \\[\n\\begin{aligned}\n\\beta_{j()} &= \\mu_{ij} - \\mu_{.} \\\\\n&= \\mu_{ij} - \\alpha_i - \\mu_{..} \\\\\n\\sum_j \\beta_{j()}&=0 , = 1,...,\n\\end{aligned}\n\\]\\(\\beta_{j()}\\) specific effect \\(j\\)-th level factor \\(B\\) nested within \\(\\)-th level factor \\(\\). Hence,\\[\n\\mu_{ij} \\equiv \\mu_{..} + \\alpha_i + \\beta_{j()} \\equiv \\mu_{..} + (\\mu_{.} - \\mu_{..}) + (\\mu_{ij} - \\mu_{.})\n\\]Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk}\n\\]\\(Y_{ijk}\\) response \\(k\\)-th treatment factor \\(\\) \\(\\)-th level factor \\(B\\) \\(j\\)-th level \\((= 1,..,; j = 1,..,b; k = 1,..n)\\)\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i = 0\\)\\(\\beta_{j()}\\) constants subject restriction \\(\\sum_j \\beta_{j()} = 0\\) \\(\\)\\(\\epsilon_{ijk} \\sim iid N(0,\\sigma^2)\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_{j()} \\\\\nvar(Y_{ijk}) &= \\sigma^2\n\\end{aligned}\n\\]interaction term nested modelANOVA Two-Factor Nested DesignsLeast Squares MLE estimatesresidual \\(e_{ijk} = Y_{ijk} - \\bar{Y}_{ijk}\\)\\[\n\\begin{aligned}\nSSTO &= SSA + SSB() + SSE \\\\\n\\sum_i \\sum_j \\sum_k (Y_{ijk}- \\bar{Y}_{...})^2 &= bn \\sum_i (\\bar{Y}_{..}- \\bar{Y}_{...})^2 + n \\sum_i \\sum_j (\\bar{Y}_{ij.}- \\bar{Y}_{..})^2  \\\\\n&+ \\sum_i \\sum_j \\sum_k (Y_{ijk} -\\bar{Y}_{ij.})^2\n\\end{aligned}\n\\]ANOVA TableTests Factor Effects\\[\n\\begin{aligned}\n&H_0: \\text{ } \\alpha_i =0 \\\\\n&H_a: \\text{ } \\alpha_i = 0\n\\end{aligned}\n\\]\\(F = \\frac{MSA}{MSE} \\sim f_{(1-\\alpha;-1,(n-1)ab)}\\) reject \\(F > f\\)\\[\n\\begin{aligned}\n&H_0: \\text{ } \\beta_{j()} =0 \\\\\n&H_a: \\text{ } \\beta_{j()} = 0\n\\end{aligned}\n\\]\\(F = \\frac{MSB()}{MSE} \\sim f_{(1-\\alpha;(b-1),(n-1)ab)}\\) reject \\(F>f\\)Testing Factor Effect Contrasts\\(L = \\sum c_i \\mu_i\\) \\(\\sum c_i =0\\)\\[\n\\begin{aligned}\n\\hat{L} &= \\sum c_i \\bar{Y}_{..} \\\\\n\\hat{L} &\\pm t_{(1-\\alpha/2;df)}s(\\hat{L})\n\\end{aligned}\n\\]\\(s^2(\\hat{L}) = \\sum c_i^2 s^2(\\bar{Y}_{..})\\), \\(s^2(\\bar{Y}_{..}) = \\frac{MSE}{bn}, df = ab(n-1)\\)Testing Treatment Means\\(L = \\sum c_i \\mu_{.j}\\) estimated \\(\\hat{L} = \\sum c_i \\bar{Y}_{ij}\\) confidence limits:\\[\n\\hat{L} \\pm t_{(1-\\alpha/2;(n-1)ab)}s(\\hat{L})\n\\]\\[\ns^2(\\hat{L}) = \\frac{MSE}{n}\\sum c^2_i\n\\]Unbalanced Nested Two-Factor DesignsIf different number levels factor \\(B\\) different levels factor \\(\\), design called unbalancedThe model\\[\n\\begin{aligned}\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk} \\\\\n\\sum_{=1}^2 \\alpha_i &=0 \\\\\n\\sum_{j=1}^3 \\beta_{j(1)} &= 0 \\\\\n\\sum_{j=1}^2 \\beta_{j(2)}&=0\n\\end{aligned}\n\\]\\(= 1,2;j =1,..,b_i;k=1,..,n_{ij}\\)\\(= 1,2;j =1,..,b_i;k=1,..,n_{ij}\\)\\(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\)\\(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\)\\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) parameters.\\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) parameters.constraints: \\(\\alpha_2 = - \\alpha_1, \\beta_{3(1)}= - \\beta_{1(1)}-\\beta_{2(1)}, \\beta_{2(2)}=-\\beta_{1(2)}\\)4 indicator variables\\[\\begin{equation}\nX_1 =\n\\begin{cases}\n1&\\text{obs school 1}\\\\\n-1&\\text{obs school 2}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_2 =\n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_3 =\n\\begin{cases}\n1&\\text{obs instructor 2 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_4 =\n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 2 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]Regression Full Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_1 X_{ijk1} + \\beta_{1(1)}X_{ijk2} + \\beta_{2(1)}X_{ijk3} + \\beta_{1(2)}X_{ijk4} + \\epsilon_{ijk}\n\\]Random Factor EffectsIf\\[\n\\begin{aligned}\n\\alpha_1 &\\sim iid N(0,\\sigma^2_\\alpha) \\\\\n\\beta_{j()} &\\sim iid N(0,\\sigma^2_\\beta)\n\\end{aligned}\n\\]Expected Mean SquaresA fixed, B randomExpected Mean SquaresA random, B randomTest StatisticsAnother way increase precision treatment comparisons reducing variability use regression models adjust differences among experimental units (also known analysis covariance).","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-covariance-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.6 Single Factor Covariance Model","text":"\\[\nY_{ij} = \\mu_{.} + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) + \\epsilon_{ij}\n\\]\\(= 1,...,r;j=1,..,n_i\\)\\(\\mu_.\\) overall mean\\(\\tau_i\\): fixed treatment effects (\\(\\sum \\tau_i =0\\))\\(\\gamma\\): fixed regression coefficient effect X Y\\(X_{ij}\\) covariate (random)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random errorsIf just use \\(\\gamma X_{ij}\\) regression term (rather \\(\\gamma(X_{ij}-\\bar{X}_{..})\\)), \\(\\mu_.\\) longer overall mean; thus need centered mean.\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_. + \\tau_i + \\gamma(X_{ij}-\\bar{X}_{..}) \\\\\nvar(Y_{ij}) &= \\sigma^2\n\\end{aligned}\n\\]\\(Y_{ij} \\sim N(\\mu_{ij},\\sigma^2)\\),\\[\n\\begin{aligned}\n\\mu_{ij} &= \\mu_. + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) \\\\\n\\sum \\tau_i &=0\n\\end{aligned}\n\\]Thus, mean response (\\(\\mu_{ij}\\)) regression line intercept \\(\\mu_. + \\tau_i\\) slope \\(\\gamma\\) treatment $$.Assumption:treatment regression lines slopewhen treatment interact covariate \\(X\\) (non-parallel slopes), covariance analysis appropriate. case use separate regression lines.complicated regression features (e.g., quadratic, cubic) additional covariates e.g.,\\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma_1(X_{ij1}-\\bar{X}_{..2}) + \\gamma_2(X_{ij2}-\\bar{X}_{..2}) + \\epsilon_{ij}\n\\]Regression FormulationWe can use indicator variables treatments\\[\nl_1 =\n\\begin{cases}\n1 & \\text{case treatment 1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n\\]\\[\n.\n\\]\\[\n.\n\\]\\[\nl_{r-1} =\n\\begin{cases}\n1 & \\text{case treatment r-1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n\\]Let \\(x_{ij} = X_{ij}- \\bar{X}_{..}\\). regression model \\[\nY_{ij} = \\mu_. + \\tau_1l_{ij,1} + .. + \\tau_{r-1}l_{ij,r-1} + \\gamma x_{ij}+\\epsilon_{ij}\n\\]\\(I_{ij,1}\\) indicator variable \\(l_1\\) j-th case treatment . treatment effect \\(\\tau_1,..\\tau_{r-1}\\) just regression coefficients indicator variables.use diagnostic tools case.InferenceTreatment effects\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = ...= 0 \\\\\n&H_a: \\text{} \\tau_i =0\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n&\\text{Full Model}: Y_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} +\\epsilon_{ij}  \\\\\n&\\text{Reduced Model}: Y_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij}\n\\end{aligned}\n\\]\\[\nF = \\frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \\frac{SSE(F)}{N-(r+1)} \\sim F_{(r-1,N-(r+1))}\n\\]interested comparisons treatment effects.\nexample, r - 3. estimate \\(\\tau_1,\\tau_2, \\tau_3 = -\\tau_1 - \\tau_2\\)Testing Parallel SlopesExample:r = 3\\[\nY_{ij} = \\mu_{.} + \\tau_1 I_{ij,1} + \\tau_2 I_{ij,2} + \\gamma X_{ij} + \\beta_1 I_{ij,1}X_{ij} + \\beta_2 I_{ij,2}X_{ij} + \\epsilon_{ij}\n\\]\\(\\beta_1,\\beta_2\\): interaction coefficients.\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0 \\\\\n&H_a: \\text{least one} \\beta \\neq 0\n\\end{aligned}\n\\]can’t reject \\(H_0\\) using F-test evidence slopes parallelAdjusted MeansThe means response adjusting covariate effect\\[\nY_{.}(adj) = \\bar{Y}_{.} - \\hat{\\gamma}(\\bar{X}_{.} - \\bar{X}_{..})\n\\]","code":""},{"path":"multivariate-methods.html","id":"multivariate-methods","chapter":"22 Multivariate Methods","heading":"22 Multivariate Methods","text":"\\(y_1,...,y_p\\) possibly correlated random variables means \\(\\mu_1,...,\\mu_p\\)\\[\n\\mathbf{y} =\n\\left(\n\\begin{array}\n{c}\ny_1 \\\\\n. \\\\\ny_p \\\\\n\\end{array}\n\\right)\n\\]\\[\nE(\\mathbf{y}) =\n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n. \\\\\n\\mu_p \\\\\n\\end{array}\n\\right)\n\\]Let \\(\\sigma_{ij} = cov(y_i, y_j)\\) \\(,j = 1,…,p\\)\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij}) =\n\\left(\n\\begin{array}\n{cccc}\n\\sigma_{11} & \\sigma_{22} & ... &  \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p} \\\\\n. & . & . & . \\\\\n\\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n\\end{array}\n\\right)\n\\]\\(\\mathbf{\\Sigma}\\) (symmetric) variance-covariance dispersion matrixLet \\(\\mathbf{u}_{p \\times 1}\\) \\(\\mathbf{v}_{q \\times 1}\\) random vectors means \\(\\mu_u\\) \\(\\mu_v\\) . \\[\n\\mathbf{\\Sigma}_{uv} = cov(\\mathbf{u,v}) = E[(\\mathbf{u} - \\mu_u)(\\mathbf{v} - \\mu_v)']\n\\]\\(\\mathbf{\\Sigma}_{uv} \\neq \\mathbf{\\Sigma}_{vu}\\) \\(\\mathbf{\\Sigma}_{uv} = \\mathbf{\\Sigma}_{vu}'\\)Properties Covariance MatricesSymmetric \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\)Non-negative definite \\(\\mathbf{'\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\), equivalent eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0\\)\\(|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 ... \\lambda_p \\ge 0\\) (generalized variance) (bigger number , variation \\(trace(\\mathbf{\\Sigma}) = tr(\\mathbf{\\Sigma}) = \\lambda_1 + ... + \\lambda_p = \\sigma_{11} + ... + \\sigma_{pp} =\\) sum variance (total variance)Note:\\(\\mathbf{\\Sigma}\\) typically required positive definite, means eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\) \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma} = \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma \\Sigma}^{-1}\\)Correlation Matrices\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n\\]\\[\n\\mathbf{R} =\n\\left(\n\\begin{array}\n{cccc}\n\\rho_{11} & \\rho_{12} & ... & \\rho_{1p} \\\\\n\\rho_{21} & \\rho_{22} & ... & \\rho_{2p} \\\\\n. & . & . &. \\\\\n\\rho_{p1} & \\rho_{p2} & ... & \\rho_{pp} \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho_{ij}\\) correlation, \\(\\rho_{ii} = 1\\) iAlternatively,\\[\n\\mathbf{R} = [diag(\\mathbf{\\Sigma})]^{-1/2}\\mathbf{\\Sigma}[diag(\\mathbf{\\Sigma})]^{-1/2}\n\\]\\(diag(\\mathbf{\\Sigma})\\) matrix \\(\\sigma_{ii}\\)’s diagonal 0’s elsewhereand \\(\\mathbf{}^{1/2}\\) (square root symmetric matrix) symmetric matrix \\(\\mathbf{} = \\mathbf{}^{1/2}\\mathbf{}^{1/2}\\)EqualitiesLet\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constantsThen\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)Multivariate Normal DistributionLet \\(\\mathbf{y}\\) multivariate normal (MVN) random variable mean \\(\\mu\\) variance \\(\\mathbf{\\Sigma}\\). density \\(\\mathbf{y}\\) \\[\nf(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2} \\mathbf{(y-\\mu)'\\Sigma^{-1}(y-\\mu)} )\n\\]\\(\\mathbf{y} \\sim N_p(\\mu, \\mathbf{\\Sigma})\\)","code":""},{"path":"multivariate-methods.html","id":"properties-of-mvn","chapter":"22 Multivariate Methods","heading":"22.0.1 Properties of MVN","text":"Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)Define partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \n\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\n\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\n\\(p_1 + p_2 = p\\)\n\\(p_1,p_2 \\ge 1\\) \nDefine partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(p_1 + p_2 = p\\)\\(p_1 + p_2 = p\\)\\(p_1,p_2 \\ge 1\\) \\(p_1,p_2 \\ge 1\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n\\mathbf{y}_2 \\\\\n\\end{array}\n\\right)\n\\sim\nN\n\\left(\n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n\\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\\n\\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22}\\\\\n\\end{array}\n\\right)\n\\right)\n\\]marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)conditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\nanalogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\nconditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)formula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)Multiple Regression\\[\n\\left(\n\\begin{array}\n{c}\nY \\\\\n\\mathbf{x}\n\\end{array}\n\\right)\n\\sim\nN_{p+1}\n\\left(\n\\left[\n\\begin{array}\n{c}\n\\mu_y \\\\\n\\mathbf{\\mu}_x\n\\end{array}\n\\right]\n,\n\\left[\n\\begin{array}\n{cc}\n\\sigma^2_Y & \\mathbf{\\Sigma}_{yx} \\\\\n\\mathbf{\\Sigma}_{yx} & \\mathbf{\\Sigma}_{xx}\n\\end{array}\n\\right]\n\\right)\n\\]conditional distribution Y given x follows univariate normal distribution \\[\n\\begin{aligned}\nE(Y| \\mathbf{x}) &= \\mu_y + \\mathbf{\\Sigma}_{yx} \\Sigma_{xx}^{-1} (\\mathbf{x}- \\mu_x) \\\\\n&= \\mu_y - \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mu_x + \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mathbf{x} \\\\\n&= \\beta_0 + \\mathbf{\\beta'x}\n\\end{aligned}\n\\]\\(\\beta = (\\beta_1,...,\\beta_p)' = \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{yx}'\\) (e.g., analogous \\(\\mathbf{(x'x)^{-1}x'y}\\) consider \\(Y_i\\) \\(\\mathbf{x}_i\\), \\(= 1,..,n\\) use empirical covariance formula: \\(var(Y|\\mathbf{x}) = \\sigma^2_Y - \\mathbf{\\Sigma_{yx}\\Sigma^{-1}_{xx} \\Sigma'_{yx}}\\))Samples Multivariate Normal PopulationsA random sample size n, \\(\\mathbf{y}_1,.., \\mathbf{y}_n\\) \\(N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). ThenSince \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)Since \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\n\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\n\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).Large Sample Properties\\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) random sample population mean \\(\\mathbf{\\mu}\\) variance-covariance matrix \\(\\mathbf{\\Sigma}\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Maximum Likelihood Estimation MVNSuppose iid \\(\\mathbf{y}_1 ,... \\mathbf{y}_n \\sim N_p (\\mu, \\mathbf{\\Sigma})\\), likelihood function data \\[\n\\begin{aligned}\nL(\\mu, \\mathbf{\\Sigma}) &= \\prod_{j=1}^n (\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2}(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)) \\\\\n&= \\frac{1}{(2\\pi)^{np/2}|\\mathbf{\\Sigma}|^{n/2}} \\exp(-\\frac{1}{2} \\sum_{j=1}^n(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)\n\\end{aligned}\n\\], MLEs \\[\n\\hat{\\mu} = \\bar{\\mathbf{y}}\n\\]\\[\n\\hat{\\mathbf{\\Sigma}} = \\frac{n-1}{n} \\mathbf{S}\n\\]using derivatives log likelihood function respect \\(\\mu\\) \\(\\mathbf{\\Sigma}\\)Properties MLEsInvariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Invariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Consistency: MLEs consistent estimators, usually biasedConsistency: MLEs consistent estimators, usually biasedEfficiency: MLEs efficient estimators (estimator smaller variance large samples)Efficiency: MLEs efficient estimators (estimator smaller variance large samples)Asymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\n\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\ncan estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)\nAsymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)Likelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio test\nstatistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)\nlarge n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)\nLikelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio testThe statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)Test Multivariate NormalityCheck univariate normality trait (X) separately\nCan check \\[Normality Assessment\\]\ngood thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.\nHowever, marginal normality traits imply joint MVN\nEasily rule multivariate normality, easy prove \nCheck univariate normality trait (X) separatelyCan check \\[Normality Assessment\\]Can check \\[Normality Assessment\\]good thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.good thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.However, marginal normality traits imply joint MVNHowever, marginal normality traits imply joint MVNEasily rule multivariate normality, easy prove itEasily rule multivariate normality, easy prove itMardia’s tests multivariate normality\nMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\n\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\nMultivariate kurtosis defined \n\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\nMVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)\nsample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\n\n(Mardia 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n\nMardia’s tests multivariate normalityMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]Multivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)Multivariate kurtosis defined asMultivariate kurtosis defined \\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)sample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\nsample size n, can estimate\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance(Mardia 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n(Mardia 1970) shows large n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)Alternatively, Doornik-Hansen test multivariate normality (Doornik Hansen 2008)Alternatively, Doornik-Hansen test multivariate normality (Doornik Hansen 2008)Chi-square Q-Q plot\nLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\n\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\nplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degree\nrequires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.\nChi-square Q-Q plotLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)plot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeit requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.data normal, can\nignore \nuse nonparametric methods\nuse models based upon approximate distribution (e.g., GLMM)\ntry performing transformation\ndata normal, canignore itignore ituse nonparametric methodsuse nonparametric methodsuse models based upon approximate distribution (e.g., GLMM)use models based upon approximate distribution (e.g., GLMM)try performing transformationtry performing transformation","code":"\nlibrary(heplots)\nlibrary(ICSNP)\nlibrary(MVN)\nlibrary(tidyverse)\n\ntrees = read.table(\"images/trees.dat\")\nnames(trees) <- c(\"Nitrogen\",\"Phosphorous\",\"Potassium\",\"Ash\",\"Height\")\nstr(trees)\n#> 'data.frame':    26 obs. of  5 variables:\n#>  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...\n#>  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...\n#>  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...\n#>  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...\n#>  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...\n\nsummary(trees)\n#>     Nitrogen      Phosphorous       Potassium           Ash        \n#>  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  \n#>  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  \n#>  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  \n#>  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  \n#>  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  \n#>  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  \n#>      Height     \n#>  Min.   : 65.0  \n#>  1st Qu.:122.5  \n#>  Median :181.0  \n#>  Mean   :196.6  \n#>  3rd Qu.:276.0  \n#>  Max.   :373.0\ncor(trees, method = \"pearson\") # correlation matrix\n#>              Nitrogen Phosphorous Potassium       Ash    Height\n#> Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641\n#> Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656\n#> Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683\n#> Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771\n#> Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000\n\n# qq-plot \ngg <- trees %>%\n    pivot_longer(everything(), names_to = \"Var\", values_to = \"Value\") %>%\n    ggplot(aes(sample = Value)) +\n    geom_qq() +\n    geom_qq_line() +\n    facet_wrap(\"Var\", scales = \"free\")\ngg\n\n# Univariate normality\nsw_tests <- apply(trees, MARGIN = 2, FUN = shapiro.test)\nsw_tests\n#> $Nitrogen\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.96829, p-value = 0.5794\n#> \n#> \n#> $Phosphorous\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.93644, p-value = 0.1104\n#> \n#> \n#> $Potassium\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.95709, p-value = 0.3375\n#> \n#> \n#> $Ash\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.92071, p-value = 0.04671\n#> \n#> \n#> $Height\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.94107, p-value = 0.1424\n# Kolmogorov-Smirnov test \nks_tests <- map(trees, ~ ks.test(scale(.x),\"pnorm\"))\nks_tests\n#> $Nitrogen\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.12182, p-value = 0.8351\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Phosphorous\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.17627, p-value = 0.3944\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Potassium\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.10542, p-value = 0.9348\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Ash\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.14503, p-value = 0.6449\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Height\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.1107, p-value = 0.9076\n#> alternative hypothesis: two-sided\n\n# Mardia's test, need large sample size for power\nmardia_test <-\n    mvn(\n        trees,\n        mvnTest = \"mardia\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\n\nmardia_test$multivariateNormality\n#>              Test         Statistic            p value Result\n#> 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES\n#> 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281    YES\n#> 3             MVN              <NA>               <NA>    YES\n\n# Doornik-Hansen's test \ndh_test <-\n    mvn(\n        trees,\n        mvnTest = \"dh\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\ndh_test$multivariateNormality\n#>             Test        E df      p value MVN\n#> 1 Doornik-Hansen 161.9446 10 1.285352e-29  NO\n\n# Henze-Zirkler's test \nhz_test <-\n    mvn(\n        trees,\n        mvnTest = \"hz\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nhz_test$multivariateNormality\n#>            Test        HZ   p value MVN\n#> 1 Henze-Zirkler 0.7591525 0.6398905 YES\n# The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05.\n\n# Royston's test\n# can only apply for 3 < obs < 5000 (because of Shapiro-Wilk's test)\nroyston_test <-\n    mvn(\n        trees,\n        mvnTest = \"royston\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nroyston_test$multivariateNormality\n#>      Test        H    p value MVN\n#> 1 Royston 9.064631 0.08199215 YES\n\n\n# E-statistic\nestat_test <-\n    mvn(\n        trees,\n        mvnTest = \"energy\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nestat_test$multivariateNormality\n#>          Test Statistic p value MVN\n#> 1 E-statistic  1.091101   0.532 YES"},{"path":"multivariate-methods.html","id":"mean-vector-inference","chapter":"22 Multivariate Methods","heading":"22.0.2 Mean Vector Inference","text":"univariate normal distribution, test \\(H_0: \\mu =\\mu_0\\) using\\[\nT = \\frac{\\bar{y}- \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}\n\\]null hypothesis. reject null \\(|T|\\) large relative \\(t_{(1-\\alpha/2,n-1)}\\) means seeing value large observed rare null trueEquivalently,\\[\nT^2 = \\frac{(\\bar{y}- \\mu_0)^2}{s^2/n} = n(\\bar{y}- \\mu_0)(s^2)^{-1}(\\bar{y}- \\mu_0) \\sim f_{(1,n-1)}\n\\]","code":""},{"path":"multivariate-methods.html","id":"natural-multivariate-generalization","chapter":"22 Multivariate Methods","heading":"22.0.2.1 Natural Multivariate Generalization","text":"\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0 \\\\\n&H_a: \\mathbf{\\mu} \\neq \\mathbf{\\mu}_0\n\\end{aligned}\n\\]Define Hotelling’s \\(T^2\\) \\[\nT^2 = n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)\n\\]can viewed generalized distance \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{\\mu}_0\\)assumption normality,\\[\nF = \\frac{n-p}{(n-1)p} T^2 \\sim f_{(p,n-p)}\n\\]reject null hypothesis \\(F > f_{(1-\\alpha, p, n-p)}\\)\\(T^2\\) test invariant changes measurement units.\n\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\n\\(T^2\\) test invariant changes measurement units.\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)","code":""},{"path":"multivariate-methods.html","id":"confidence-intervals","chapter":"22 Multivariate Methods","heading":"22.0.2.2 Confidence Intervals","text":"","code":""},{"path":"multivariate-methods.html","id":"confidence-region","chapter":"22 Multivariate Methods","heading":"22.0.2.2.1 Confidence Region","text":"“exact” \\(100(1-\\alpha)\\%\\) confidence region \\(\\mathbf{\\mu}\\) set vectors, \\(\\mathbf{v}\\), “close enough” observed mean vector, \\(\\bar{\\mathbf{y}}\\) satisfy\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\le \\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)}\n\\]\\(\\mathbf{v}\\) just mean vectors rejected \\(T^2\\) test \\(\\mathbf{\\bar{y}}\\) observed.case 2 parameters, confidence region “hyper-ellipsoid”.region, consists \\(\\mathbf{\\mu}_0\\) vectors \\(T^2\\) test reject \\(H_0\\) significance level \\(\\alpha\\)Even though confidence region better assesses joint knowledge concerning plausible values \\(\\mathbf{\\mu}\\) , people typically include confidence statement individual component means. ’d like separate confidence statements hold simultaneously specified high probability. Simultaneous confidence intervals: intervals statement incorrect","code":""},{},{},{"path":"multivariate-methods.html","id":"general-hypothesis-testing","chapter":"22 Multivariate Methods","heading":"22.0.3 General Hypothesis Testing","text":"","code":""},{"path":"multivariate-methods.html","id":"one-sample-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.1 One-sample Tests","text":"\\[\nH_0: \\mathbf{C \\mu= 0}\n\\]\\(\\mathbf{C}\\) \\(c \\times p\\) matrix rank c \\(c \\le p\\)can test hypothesis using following statistic\\[\nF = \\frac{n - c}{(n-1)c} T^2\n\\]\\(T^2 = n(\\mathbf{C\\bar{y}})' (\\mathbf{CSC'})^{-1} (\\mathbf{C\\bar{y}})\\)Example:\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_p\n\\]Equivalently,\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0 \\\\\n&\\vdots \\\\\n\\mu_{p-1} - \\mu_p &= 0\n\\end{aligned}\n\\]total \\(p-1\\) tests. Hence, \\(\\mathbf{C}\\) \\(p - 1 \\times p\\) matrix\\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & 1 & -1\n\\end{array}\n\\right)\n\\]number rows = \\(c = p -1\\)Equivalently, can also compare means first mean. , test \\(\\mu_1 - \\mu_2 = 0, \\mu_1 - \\mu_3 = 0,..., \\mu_1 - \\mu_p = 0\\), \\((p-1) \\times p\\) matrix \\(\\mathbf{C}\\) \\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n-1 & 1 & 0 & \\ldots & 0 \\\\\n-1 & 0 & 1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n-1 & 0 & \\ldots & 0 & 1\n\\end{array}\n\\right)\n\\]value \\(T^2\\) invariant equivalent choices \\(\\mathbf{C}\\)often used repeated measures designs, subject receives treatment successive periods time (treatments administered unit).Example:Let \\(y_{ij}\\) response subject time j \\(= 1,..,n, j = 1,...,T\\). case, \\(\\mathbf{y}_i = (y_{i1}, ..., y_{})', = 1,...,n\\) random sample \\(N_T (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(n=8\\) subjects, \\(T = 6\\). interested \\(\\mu_1, .., \\mu_6\\)\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_6\n\\]Equivalently,\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0 \\\\\n\\mu_2 - \\mu_3 &= 0 \\\\\n&... \\\\\n\\mu_5  - \\mu_6 &= 0\n\\end{aligned}\n\\]can test orthogonal polynomials 4 equally spaced time points. test example null hypothesis quadratic cubic effects jointly equal 0, define \\(\\mathbf{C}\\)\\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{cccc}\n1 & -1 & -1 & 1 \\\\\n-1 & 3 & -3 & 1\n\\end{array}\n\\right)\n\\]","code":""},{"path":"multivariate-methods.html","id":"two-sample-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.2 Two-Sample Tests","text":"Consider analogous two sample multivariate tests.Example: data two independent random samples, one sample two populations\\[\n\\begin{aligned}\n\\mathbf{y}_{1i} &\\sim N_p (\\mathbf{\\mu_1, \\Sigma}) \\\\\n\\mathbf{y}_{2j} &\\sim N_p (\\mathbf{\\mu_2, \\Sigma})\n\\end{aligned}\n\\]assumenormalitynormalityequal variance-covariance matricesequal variance-covariance matricesindependent random samplesindependent random samplesWe can summarize data using sufficient statistics \\(\\mathbf{\\bar{y}}_1, \\mathbf{S}_1, \\mathbf{\\bar{y}}_2, \\mathbf{S}_2\\) respective sample sizes, \\(n_1,n_2\\)Since assume \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\), compute pooled estimate variance-covariance matrix \\(n_1 + n_2 - 2\\) df\\[\n\\mathbf{S} = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2-1) \\mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}\n\\]\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 \\\\\n&H_a: \\mathbf{\\mu}_1 \\neq \\mathbf{\\mu}_2\n\\end{aligned}\n\\]least one element mean vectors differentWe use\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)\nNote: assume two populations independent, covariance\n\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)Note: assume two populations independent, covariance\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)Reject \\(H_0\\) \\[\n\\begin{aligned}\nT^2 &= (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} (\\frac{1}{n_1} + \\frac{1}{n_2})\\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n&= \\frac{n_1 n_2}{n_1 +n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} \\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n& \\ge \\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \\alpha,n_1 + n_2 - p -1)}\n\\end{aligned}\n\\]equivalently, \\[\nF = \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\ge f_{(1- \\alpha, p , n_1 + n_2 -p -1)}\n\\]\\(100(1-\\alpha) \\%\\) confidence region \\(\\mu_1 - \\mu_2\\) consists vector \\(\\delta\\) satisfy\\[\n\\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta})' \\mathbf{S}^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta}) \\le \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\\alpha, p , n_1 + n_2 - p -1)}\n\\]simultaneous confidence intervals linear combinations \\(\\mu_1 - \\mu_2\\) form\\[\n\\mathbf{'}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\pm \\sqrt{\\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\\alpha, p, n_1 + n_2 -p -1)} \\times \\sqrt{\\mathbf{'Sa}(\\frac{1}{n_1} + \\frac{1}{n_2})}\n\\]Bonferroni intervals, k combinations\\[\n(\\bar{y}_{1i} - \\bar{y}_{2i}) \\pm t_{(1-\\alpha/2k, n_1 + n_2 - 2)}\\sqrt{(\\frac{1}{n_1}  + \\frac{1}{n_2})s_{ii}}\n\\]","code":""},{"path":"multivariate-methods.html","id":"model-assumptions","chapter":"22 Multivariate Methods","heading":"22.0.3.3 Model Assumptions","text":"model assumption metUnequal Covariance Matrices\n\\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample test\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflated\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction power\nUnequal Covariance MatricesIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerSample Normal\nType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributions\nOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.\nIntuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.\nSolutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\n\nSample NormalType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.One sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Solutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\nSolutions:Transform make data normalTransform make data normalLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)","code":""},{"path":"multivariate-methods.html","id":"equal-covariance-matrices-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.3.1 Equal Covariance Matrices Tests","text":"independent random samples k populations \\(p\\)-dimensional vectors. compute sample covariance matrix , \\(\\mathbf{S}_i\\), \\(= 1,...,k\\)\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\ldots = \\mathbf{\\Sigma}_k = \\mathbf{\\Sigma} \\\\\n&H_a: \\text{least 2 different}\n\\end{aligned}\n\\]Assume \\(H_0\\) true, use pooled estimate common covariance matrix, \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{S} = \\frac{\\sum_{=1}^k (n_i -1)\\mathbf{S}_i}{\\sum_{=1}^k (n_i - 1)}\n\\]\\(\\sum_{=1}^k (n_i -1)\\)","code":""},{},{"path":"multivariate-methods.html","id":"two-sample-repeated-measurements","chapter":"22 Multivariate Methods","heading":"22.0.3.4 Two-Sample Repeated Measurements","text":"Define \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TDefine \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TAssume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)Assume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)test statistic formThe test statistic form\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\mathbf{C}'(\\mathbf{CSC}')^{-1}\\mathbf{C} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\n\\]\\(\\mathbf{S}\\) pooled covariance estimate. ,\\[\nF = \\frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \\sim f_{(c, n_1 + n_2 - c-1)}\n\\]\\(H_0\\) trueIf null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) rejected. weaker hypothesis profiles two groups parallel.\\[\n\\begin{aligned}\n\\mu_{11} - \\mu_{21} &= \\mu_{12} - \\mu_{22} \\\\\n&\\vdots \\\\\n\\mu_{1t-1} - \\mu_{2t-1} &= \\mu_{1t} - \\mu_{2t}\n\\end{aligned}\n\\]null hypothesis matrix term \\(H_0: \\mathbf{C}(\\mu_1 - \\mu_2) = \\mathbf{0}_c\\) , \\(c = t - 1\\) \\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & -1\n\\end{array}\n\\right)_{(t-1) \\times t}\n\\]can’t reject null hypothesized vector meansreject null two labs’ measurements equalreject null. Hence, difference means bivariate normal distributions","code":"\n# One-sample Hotelling's T^2 test\n#  Create data frame\nplants <- data.frame(\n    y1 = c(2.11, 2.36, 2.13, 2.78, 2.17),\n    y2 = c(10.1, 35.0, 2.0, 6.0, 2.0),\n    y3 = c(3.4, 4.1, 1.9, 3.8, 1.7)\n)\n\n# Center the data with \n# the hypothesized means and make a matrix\nplants_ctr <- plants %>%\n    transmute(y1_ctr = y1 - 2.85,\n              y2_ctr = y2 - 15.0,\n              y3_ctr = y3 - 6.0) %>%\n    as.matrix()\n\n# Use anova.mlm to calculate Wilks' lambda\nonesamp_fit <- anova(lm(plants_ctr ~ 1), test = \"Wilks\")\nonesamp_fit\n#> Analysis of Variance Table\n#> \n#>             Df    Wilks approx F num Df den Df  Pr(>F)  \n#> (Intercept)  1 0.054219   11.629      3      2 0.08022 .\n#> Residuals    4                                          \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Paired-Sample Hotelling's T^2 test\nlibrary(ICSNP)\n\n#  Create data frame\nwaste <- data.frame(\n    case = 1:11,\n    com_y1 = c(6, 6, 18, 8, 11, 34, 28, 71, 43, 33, 20),\n    com_y2 = c(27, 23, 64, 44, 30, 75, 26, 124, 54, 30, 14),\n    state_y1 = c(25, 28, 36, 35, 15, 44, 42, 54, 34, 29, 39),\n    state_y2 = c(15, 13, 22, 29, 31, 64, 30, 64, 56, 20, 21)\n)\n\n# Calculate the difference between commercial and state labs\nwaste_diff <- waste %>%\n    transmute(y1_diff = com_y1 - state_y1,\n              y2_diff = com_y2 - state_y2)\n# Run the test\npaired_fit <- HotellingsT2(waste_diff)\n# value T.2 in the output corresponds to \n# the approximate F-value in the output from anova.mlm\npaired_fit \n#> \n#>  Hotelling's one sample T2-test\n#> \n#> data:  waste_diff\n#> T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083\n#> alternative hypothesis: true location is not equal to c(0,0)\n# Independent-Sample Hotelling's T^2 test with Bartlett's test\n\n# Read in data\nsteel <- read.table(\"images/steel.dat\")\nnames(steel) <- c(\"Temp\", \"Yield\", \"Strength\")\nstr(steel)\n#> 'data.frame':    12 obs. of  3 variables:\n#>  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...\n#>  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...\n#>  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...\n\n# Plot the data\nggplot(steel, aes(x = Yield, y = Strength)) +\n    geom_text(aes(label = Temp), size = 5) +\n    geom_segment(aes(\n        x = 33,\n        y = 57.5,\n        xend = 42,\n        yend = 65\n    ), col = \"red\")\n\n\n# Bartlett's test for equality of covariance matrices\n# same thing as Box's M test in the multivariate setting\nbart_test <- boxM(steel[, -1], steel$Temp)\nbart_test # fail to reject the null of equal covariances \n#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  steel[, -1]\n#> Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n\n# anova.mlm\ntwosamp_fit <-\n    anova(lm(cbind(Yield, Strength) ~ factor(Temp), \n             data = steel), \n          test = \"Wilks\")\ntwosamp_fit\n#> Analysis of Variance Table\n#> \n#>              Df    Wilks approx F num Df den Df    Pr(>F)    \n#> (Intercept)   1 0.001177   3818.1      2      9 6.589e-14 ***\n#> factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** \n#> Residuals    10                                              \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ICSNP package\ntwosamp_fit2 <-\n    HotellingsT2(cbind(steel$Yield, steel$Strength) ~ \n                     factor(steel$Temp))\ntwosamp_fit2\n#> \n#>  Hotelling's two sample T2-test\n#> \n#> data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)\n#> T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\n#> alternative hypothesis: true location difference is not equal to c(0,0)"},{"path":"multivariate-methods.html","id":"manova","chapter":"22 Multivariate Methods","heading":"22.1 MANOVA","text":"Multivariate Analysis VarianceOne-way MANOVACompare treatment means h different populationsPopulation 1: \\(\\mathbf{y}_{11}, \\mathbf{y}_{12}, \\dots, \\mathbf{y}_{1n_1} \\sim idd N_p (\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\)\\(\\vdots\\)Population h: \\(\\mathbf{y}_{h1}, \\mathbf{y}_{h2}, \\dots, \\mathbf{y}_{hn_h} \\sim idd N_p (\\mathbf{\\mu}_h, \\mathbf{\\Sigma})\\)AssumptionsIndependent random samples \\(h\\) different populationsCommon covariance matricesEach population multivariate normalCalculate summary statistics \\(\\mathbf{\\bar{y}}_i, \\mathbf{S}\\) pooled estimate covariance matrix \\(\\mathbf{S}\\)Similar univariate one-way ANVOA, can use effects model formulation \\(\\mathbf{\\mu}_i = \\mathbf{\\mu} + \\mathbf{\\tau}_i\\), \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.one-way model: \\(\\mathbf{y}_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(= 1,..,h; j = 1,..., n_i\\) \\(\\epsilon_{ij} \\sim N_p(\\mathbf{0, \\Sigma})\\)However, model -parameterized (.e., infinite number ways define \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\tau}_i\\)’s add \\(\\mu_i\\). Thus can constrain \\[\n\\sum_{=1}^h n_i \\tau_i = 0\n\\]\\[\n\\mathbf{\\tau}_h = 0\n\\]observational equivalent effects model \\[\n\\begin{aligned}\n\\mathbf{y}_{ij} &= \\mathbf{\\bar{y}} + (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}}) + (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i) \\\\\n&= \\text{overall sample mean} + \\text{treatement effect} + \\text{residual} \\text{ (univariate ANOVA)}\n\\end{aligned}\n\\]manipulation\\[\n\\sum_{= 1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})' = \\sum_{= 1}^h n_i (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})' + \\sum_{=1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}}_i)'\n\\]LHS = Total corrected sums squares cross products (SSCP) matrixRHS =1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)2nd term = residual (within subject) SSCP matrix denoted (E;W)2nd term = residual (within subject) SSCP matrix denoted (E;W)Note:\\[\n\\mathbf{E} = (n_1 - 1)\\mathbf{S}_1  + ... + (n_h -1) \\mathbf{S}_h = (n-h) \\mathbf{S}\n\\]MANOVA tableMONOVA table\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h = \\mathbf{0}\n\\]consider relative “sizes” \\(\\mathbf{E}\\) \\(\\mathbf{H+E}\\)Wilk’s LambdaDefine Wilk’s Lambda\\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H+E}|}\n\\]Properties:Wilk’s Lambda equivalent F-statistic univariate caseWilk’s Lambda equivalent F-statistic univariate caseThe exact distribution \\(\\Lambda^*\\) can determined especial cases.exact distribution \\(\\Lambda^*\\) can determined especial cases.large sample sizes, reject \\(H_0\\) ifFor large sample sizes, reject \\(H_0\\) \\[\n-(\\sum_{=1}^h n_i - 1 - \\frac{p+h}{2}) \\log(\\Lambda^*) > \\chi^2_{(1-\\alpha, p(h-1))}\n\\]","code":""},{"path":"multivariate-methods.html","id":"testing-general-hypotheses","chapter":"22 Multivariate Methods","heading":"22.1.1 Testing General Hypotheses","text":"\\(h\\) different treatments\\(h\\) different treatmentswith -th treatmentwith -th treatmentapplied \\(n_i\\) subjects thatapplied \\(n_i\\) subjects thatare observed \\(p\\) repeated measures.observed \\(p\\) repeated measures.Consider \\(p\\) dimensional obs random sample \\(h\\) different treatment populations.\\[\n\\mathbf{y}_{ij} = \\mathbf{\\mu} + \\mathbf{\\tau}_i + \\mathbf{\\epsilon}_{ij}\n\\]\\(= 1,..,h\\) \\(j = 1,..,n_i\\)Equivalently,\\[\n\\mathbf{Y} = \\mathbf{XB} + \\mathbf{\\epsilon}\n\\]\\(n = \\sum_{= 1}^h n_i\\) restriction \\(\\mathbf{\\tau}_h = 0\\)\\[\n\\mathbf{Y}_{(n \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_{11}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{1n_1}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{hn_h}'\n\\end{array}\n\\right],\n\\mathbf{B}_{(h \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{\\mu}' \\\\\n\\mathbf{\\tau}_1' \\\\\n\\vdots \\\\\n\\mathbf{\\tau}_{h-1}'\n\\end{array}\n\\right],\n\\mathbf{\\epsilon}_{(n \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\epsilon_{11}' \\\\\n\\vdots \\\\\n\\epsilon_{1n_1}' \\\\\n\\vdots \\\\\n\\epsilon_{hn_h}'\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{X}_{(n \\times h)} =\n\\left[\n\\begin{array}\n{ccccc}\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0\n\\end{array}\n\\right]\n\\]Estimation\\[\n\\mathbf{\\hat{B}} = (\\mathbf{X'X})^{-1} \\mathbf{X'Y}\n\\]Rows \\(\\mathbf{Y}\\) independent (.e., \\(var(\\mathbf{Y}) = \\mathbf{}_n \\otimes \\mathbf{\\Sigma}\\) , \\(np \\times np\\) matrix, \\(\\otimes\\) Kronecker product).\\[\n\\begin{aligned}\n&H_0: \\mathbf{LBM} = 0 \\\\\n&H_a: \\mathbf{LBM} \\neq 0\n\\end{aligned}\n\\]\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traits\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traitsThe general treatment corrected sums squares cross product \\[\n\\mathbf{H} = \\mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}\n\\]null hypothesis \\(H_0: \\mathbf{LBM} = \\mathbf{D}\\)\\[\n\\mathbf{H} = (\\mathbf{\\hat{LBM}} - \\mathbf{D})'[\\mathbf{X(X'X)^{-1}L}]^{-1}(\\mathbf{\\hat{LBM}} - \\mathbf{D})\n\\]general matrix residual sums squares cross product\\[\n\\mathbf{E} = \\mathbf{M'Y'[-X(X'X)^{-1}X']YM} = \\mathbf{M'[Y'Y - \\hat{B}'(X'X)^{-1}\\hat{B}]M}\n\\]can compute following statistic eigenvalues \\(\\mathbf{}^{-1}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)\\(H_0\\) true n large, \\(-(n-1- \\frac{p+h}{2})\\ln \\Lambda^* \\sim \\chi^2_{p(h-1)}\\). special values p h can give exact F-dist \\(H_0\\)reject null equal multivariate mean vectors three admmission groupsIf independent = time 3 levels -> univariate ANOVA (require sphericity assumption (.e., variances differences equal))level independent time separate variable -> MANOVA (require sphericity assumption)can’t reject null hypothesis sphericity, hence univariate ANOVA also appropriate.also see linear significant time effect, quadratic time effectreject null hypothesis difference means treatmentsthere significant difference means control bww9 drugthere significant difference means ax23 drug treatment rest treatments","code":"\n# One-way MANOVA\n\nlibrary(car)\nlibrary(emmeans)\nlibrary(profileR)\nlibrary(tidyverse)\n\n## Read in the data\ngpagmat <- read.table(\"images/gpagmat.dat\")\n\n## Change the variable names\nnames(gpagmat) <- c(\"y1\", \"y2\", \"admit\")\n\n## Check the structure\nstr(gpagmat)\n#> 'data.frame':    85 obs. of  3 variables:\n#>  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...\n#>  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...\n#>  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...\n\n\n## Plot the data\ngg <- ggplot(gpagmat, aes(x = y1, y = y2)) +\n    geom_text(aes(label = admit, col = as.character(admit))) +\n    scale_color_discrete(name = \"Admission\",\n                         labels = c(\"Admit\", \"Do not admit\", \"Borderline\")) +\n    scale_x_continuous(name = \"GPA\") +\n    scale_y_continuous(name = \"GMAT\")\n\n## Fit one-way MANOVA\noneway_fit <- manova(cbind(y1, y2) ~ admit, data = gpagmat)\nsummary(oneway_fit, test = \"Wilks\")\n#>           Df  Wilks approx F num Df den Df    Pr(>F)    \n#> admit      1 0.6126   25.927      2     82 1.881e-09 ***\n#> Residuals 83                                            \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Repeated Measures MANOVA\n\n\n## Create data frame\nstress <- data.frame(\n    subject = 1:8,\n    begin = c(3, 2, 5, 6, 1, 5, 1, 5),\n    middle = c(3, 4, 3, 7, 4, 7, 1, 2),\n    final = c(6, 7, 4, 7, 6, 7, 3, 5)\n)\n## MANOVA\nstress_mod <- lm(cbind(begin, middle, final) ~ 1, data = stress)\nidata <-\n    data.frame(time = factor(\n        c(\"begin\", \"middle\", \"final\"),\n        levels = c(\"begin\", \"middle\", \"final\")\n    ))\nrepeat_fit <-\n    Anova(\n        stress_mod,\n        idata = idata,\n        idesign = ~ time,\n        icontrasts = \"contr.poly\"\n    )\nsummary(repeat_fit) \n#> \n#> Type III Repeated Measures MANOVA Tests:\n#> \n#> ------------------------------------------\n#>  \n#> Term: (Intercept) \n#> \n#>  Response transformation matrix:\n#>        (Intercept)\n#> begin            1\n#> middle           1\n#> final            1\n#> \n#> Sum of squares and products for the hypothesis:\n#>             (Intercept)\n#> (Intercept)        1352\n#> \n#> Multivariate Tests: (Intercept)\n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.896552 60.66667      1      7 0.00010808 ***\n#> Wilks             1  0.103448 60.66667      1      7 0.00010808 ***\n#> Hotelling-Lawley  1  8.666667 60.66667      1      7 0.00010808 ***\n#> Roy               1  8.666667 60.66667      1      7 0.00010808 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> ------------------------------------------\n#>  \n#> Term: time \n#> \n#>  Response transformation matrix:\n#>               time.L     time.Q\n#> begin  -7.071068e-01  0.4082483\n#> middle -7.850462e-17 -0.8164966\n#> final   7.071068e-01  0.4082483\n#> \n#> Sum of squares and products for the hypothesis:\n#>           time.L   time.Q\n#> time.L 18.062500 6.747781\n#> time.Q  6.747781 2.520833\n#> \n#> Multivariate Tests: time\n#>                  Df test stat approx F num Df den Df   Pr(>F)  \n#> Pillai            1 0.7080717 7.276498      2      6 0.024879 *\n#> Wilks             1 0.2919283 7.276498      2      6 0.024879 *\n#> Hotelling-Lawley  1 2.4254992 7.276498      2      6 0.024879 *\n#> Roy               1 2.4254992 7.276498      2      6 0.024879 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n#> \n#>             Sum Sq num Df Error SS den Df F value    Pr(>F)    \n#> (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***\n#> time         20.58      2    24.75     14  5.8215 0.0144578 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> \n#> Mauchly Tests for Sphericity\n#> \n#>      Test statistic p-value\n#> time         0.7085 0.35565\n#> \n#> \n#> Greenhouse-Geisser and Huynh-Feldt Corrections\n#>  for Departure from Sphericity\n#> \n#>       GG eps Pr(>F[GG])  \n#> time 0.77429    0.02439 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>         HF eps Pr(>F[HF])\n#> time 0.9528433 0.01611634\n## Polynomial contrasts\n# What is the reference for the marginal means?\nref_grid(stress_mod, mult.name = \"time\")\n#> 'emmGrid' object with variables:\n#>     1 = 1\n#>     time = multivariate response levels: begin, middle, final\n\n# marginal means for the levels of time\ncontr_means <- emmeans(stress_mod, ~ time, mult.name = \"time\")\ncontrast(contr_means, method = \"poly\")\n#>  contrast  estimate    SE df t.ratio p.value\n#>  linear        2.12 0.766  7   2.773  0.0276\n#>  quadratic     1.38 0.944  7   1.457  0.1885\n# MANOVA\n\n\n## Read in Data\nheart <- read.table(\"images/heart.dat\")\nnames(heart) <- c(\"drug\", \"y1\", \"y2\", \"y3\", \"y4\")\n## Create a subject ID nested within drug\nheart <- heart %>%\n    group_by(drug) %>%\n    mutate(subject = row_number()) %>%\n    ungroup()\nstr(heart)\n#> tibble [24 × 6] (S3: tbl_df/tbl/data.frame)\n#>  $ drug   : chr [1:24] \"ax23\" \"ax23\" \"ax23\" \"ax23\" ...\n#>  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...\n#>  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...\n#>  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...\n#>  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...\n#>  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...\n\n## Create means summary for profile plot,\n# pivot longer for plotting with ggplot\nheart_means <- heart %>%\n    group_by(drug) %>%\n    summarize_at(vars(starts_with(\"y\")), mean) %>%\n    ungroup() %>%\n    pivot_longer(-drug, names_to = \"time\", values_to = \"mean\") %>%\n    mutate(time = as.numeric(as.factor(time)))\ngg_profile <- ggplot(heart_means, aes(x = time, y = mean)) +\n    geom_line(aes(col = drug)) +\n    geom_point(aes(col = drug)) +\n    ggtitle(\"Profile Plot\") +\n    scale_y_continuous(name = \"Response\") +\n    scale_x_discrete(name = \"Time\")\ngg_profile\n## Fit model\nheart_mod <- lm(cbind(y1, y2, y3, y4) ~ drug, data = heart)\nman_fit <- car::Anova(heart_mod)\nsummary(man_fit)\n#> \n#> Type II MANOVA Tests:\n#> \n#> Sum of squares and products for error:\n#>        y1      y2      y3     y4\n#> y1 641.00 601.750 535.250 426.00\n#> y2 601.75 823.875 615.500 534.25\n#> y3 535.25 615.500 655.875 555.25\n#> y4 426.00 534.250 555.250 674.50\n#> \n#> ------------------------------------------\n#>  \n#> Term: drug \n#> \n#> Sum of squares and products for the hypothesis:\n#>        y1       y2       y3    y4\n#> y1 567.00 335.2500  42.7500 387.0\n#> y2 335.25 569.0833 404.5417 367.5\n#> y3  42.75 404.5417 391.0833 171.0\n#> y4 387.00 367.5000 171.0000 316.0\n#> \n#> Multivariate Tests: drug\n#>                  Df test stat  approx F num Df den Df     Pr(>F)    \n#> Pillai            2  1.283456  8.508082      8     38 1.5010e-06 ***\n#> Wilks             2  0.079007 11.509581      8     36 6.3081e-08 ***\n#> Hotelling-Lawley  2  7.069384 15.022441      8     34 3.9048e-09 ***\n#> Roy               2  6.346509 30.145916      4     19 5.4493e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Contrasts\nheart$drug <- factor(heart$drug)\nL <- matrix(c(0, 2,\n              1, -1,-1, -1), nrow = 3, byrow = T)\ncolnames(L) <- c(\"bww9:ctrl\", \"ax23:rest\")\nrownames(L) <- unique(heart$drug)\ncontrasts(heart$drug) <- L\ncontrasts(heart$drug)\n#>      bww9:ctrl ax23:rest\n#> ax23         0         2\n#> bww9         1        -1\n#> ctrl        -1        -1\n\n# do not set contrast L if you do further analysis (e.g., Anova, lm)\n# do M matrix instead\n\nM <- matrix(c(1, -1, 0, 0,\n              0, 1, -1, 0,\n              0, 0, 1, -1), nrow = 4)\n## update model to test contrasts\nheart_mod2 <- update(heart_mod)\ncoef(heart_mod2)\n#>                  y1         y2        y3    y4\n#> (Intercept)   75.00 78.9583333 77.041667 74.75\n#> drugbww9:ctrl  4.50  5.8125000  3.562500  4.25\n#> drugax23:rest -2.25  0.7708333  1.979167 -0.75\n\n# Hypothesis test for bww9 vs control after transformation M\n# same as linearHypothesis(heart_mod, hypothesis.matrix = c(0,1,-1), P = M)\nbww9vctrl <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 1, 0),\n                     P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\n\nbww9vctrl <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(0, 1, -1),\n                     P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\n# Hypothesis test for ax23 vs rest after transformation M\naxx23vrest <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 0, 1),\n                     P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  438.0208  175.20833 -395.7292\n#> [2,]  175.2083   70.08333 -158.2917\n#> [3,] -395.7292 -158.29167  357.5208\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.855364 37.45483      3     19 3.5484e-08 ***\n#> Wilks             1  0.144636 37.45483      3     19 3.5484e-08 ***\n#> Hotelling-Lawley  1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> Roy               1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naxx23vrest <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(2, -1, 1),\n                     P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  402.5208  127.41667 -390.9375\n#> [2,]  127.4167   40.33333 -123.7500\n#> [3,] -390.9375 -123.75000  379.6875\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.842450 33.86563      3     19 7.9422e-08 ***\n#> Wilks             1  0.157550 33.86563      3     19 7.9422e-08 ***\n#> Hotelling-Lawley  1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> Roy               1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multivariate-methods.html","id":"profile-analysis","chapter":"22 Multivariate Methods","heading":"22.1.2 Profile Analysis","text":"Examine similarities treatment effects (subjects), useful longitudinal analysis. Null treatments average effect.\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_h\n\\]Equivalently,\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h\n\\]exact nature similarities differences treatments can examined analysis.Sequential steps profile analysis:profiles parallel? (.e., interaction treatment time)profiles coincidental? (.e., profiles identical?)profiles horizontal? (.e., differences time points?)reject null hypothesis profiles parallel, can testAre differences among groups within subset total time points?differences among groups within subset total time points?differences among time points particular group (groups)?differences among time points particular group (groups)?differences within subset total time points particular group (groups)?differences within subset total time points particular group (groups)?Example4 times (p = 4)4 times (p = 4)3 treatments (h=3)3 treatments (h=3)","code":""},{"path":"multivariate-methods.html","id":"parallel-profile","chapter":"22 Multivariate Methods","heading":"22.1.2.1 Parallel Profile","text":"profiles population identical expect mean shift?\\[\n\\begin{aligned}\nH_0: \\mu_{11} - \\mu_{21} - \\mu_{12} - \\mu_{22} = &\\dots = \\mu_{1t} - \\mu_{2t} \\\\\n\\mu_{11} - \\mu_{31} - \\mu_{12} - \\mu_{32} = &\\dots = \\mu_{1t} - \\mu_{3t} \\\\\n&\\dots\n\\end{aligned}\n\\]\\(h-1\\) equationsEquivalently,\\[\nH_0: \\mathbf{LBM = 0}\n\\]\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{ccc}\n1 & -1 & 0 \\\\\n1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n\\mu_{11} & \\dots & \\mu_{14} \\\\\n\\mu_{21} & \\dots & \\mu_{24} \\\\\n\\mu_{31} & \\dots & \\mu_{34}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n=\n\\mathbf{0}\n\\]cell means parameterization \\(\\mathbf{B}\\)multiplication first 2 matrices \\(\\mathbf{LB}\\) \\[\n\\left[\n\\begin{array}\n{cccc}\n\\mu_{11} - \\mu_{21} & \\mu_{12} - \\mu_{22} & \\mu_{13} - \\mu_{23} & \\mu_{14} - \\mu_{24}\\\\\n\\mu_{11} - \\mu_{31} & \\mu_{12} - \\mu_{32} & \\mu_{13} - \\mu_{33} & \\mu_{14} - \\mu_{34}\n\\end{array}\n\\right]\n\\]differences treatment means timeMultiplying \\(\\mathbf{M}\\), get comparison across time\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{21}) - (\\mu_{12} - \\mu_{22}) & (\\mu_{11} - \\mu_{21}) -(\\mu_{13} - \\mu_{23}) & (\\mu_{11} - \\mu_{21}) - (\\mu_{14} - \\mu_{24}) \\\\\n(\\mu_{11} - \\mu_{31}) - (\\mu_{12} - \\mu_{32}) & (\\mu_{11} - \\mu_{31}) - (\\mu_{13} - \\mu_{33}) & (\\mu_{11} - \\mu_{31}) -(\\mu_{14} - \\mu_{34})\n\\end{array}\n\\right]\n\\]Alternatively, can also use effects parameterization\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{cccc}\n0 & 1 & -1 & 0 \\\\\n0 & 1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\mu' \\\\\n\\tau'_1 \\\\\n\\tau_2' \\\\\n\\tau_3'\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \\mathbf{0}\n\\]parameterizations, \\(rank(\\mathbf{L}) = h-1\\) \\(rank(\\mathbf{M}) = p-1\\)also choose \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) forms\\[\n\\mathbf{L} = \\left[\n\\begin{array}\n{cccc}\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]still obtain result.","code":""},{"path":"multivariate-methods.html","id":"coincidental-profiles","chapter":"22 Multivariate Methods","heading":"22.1.2.2 Coincidental Profiles","text":"evidence profiles parallel (.e., fail reject parallel profile test), can ask whether identical?Given profiles parallel, sums components \\(\\mu_i\\) identical treatments, profiles identical.\\[\nH_0: \\mathbf{1'}_p \\mu_1 = \\mathbf{1'}_p \\mu_2 = \\dots = \\mathbf{1'}_p \\mu_h\n\\]Equivalently,\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]cell means parameterization\\[\n\\mathbf{L} =\n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} =\n\\left[\n\\begin{array}\n{cccc}\n1 & 1 & 1 & 1\n\\end{array}\n\\right]'\n\\]multiplication yields\\[\n\\left[\n\\begin{array}\n{c}\n(\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\\\\n(\\mu_{21} + \\mu_{22} + \\mu_{23} + \\mu_{24}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34})\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0\n\\end{array}\n\\right]\n\\]Different choices \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) can yield result","code":""},{"path":"multivariate-methods.html","id":"horizontal-profiles","chapter":"22 Multivariate Methods","heading":"22.1.2.3 Horizontal Profiles","text":"Given can’t reject null hypothesis \\(h\\) profiles , can ask whether elements common profile equal? (.e., horizontal)\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]\\[\n\\mathbf{L} =\n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]hence,\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{12}) & (\\mu_{12} - \\mu_{13}) & (\\mu_{13} + \\mu_{14})\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{ccc}\n0 & 0 & 0\n\\end{array}\n\\right]\n\\]Note:fail reject 3 hypotheses, fail reject null hypotheses difference treatments differences traits.","code":"\nprofile_fit <-\n    pbg(\n        data = as.matrix(heart[, 2:5]),\n        group = as.matrix(heart[, 1]),\n        original.names = TRUE,\n        profile.plot = FALSE\n    )\nsummary(profile_fit)\n#> Call:\n#> pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, \n#>     1]), original.names = TRUE, profile.plot = FALSE)\n#> \n#> Hypothesis Tests:\n#> $`Ho: Profiles are parallel`\n#>   Multivariate.Test Statistic  Approx.F num.df den.df      p.value\n#> 1             Wilks 0.1102861 12.737599      6     38 7.891497e-08\n#> 2            Pillai 1.0891707  7.972007      6     40 1.092397e-05\n#> 3  Hotelling-Lawley 6.2587852 18.776356      6     36 9.258571e-10\n#> 4               Roy 5.9550887 39.700592      3     20 1.302458e-08\n#> \n#> $`Ho: Profiles have equal levels`\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> group        2  328.7  164.35   5.918 0.00915 **\n#> Residuals   21  583.2   27.77                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> $`Ho: Profiles are flat`\n#>          F df1 df2      p-value\n#> 1 14.30928   3  19 4.096803e-05\n# reject null hypothesis of parallel profiles\n# reject the null hypothesis of coincidental profiles\n# reject the null hypothesis that the profiles are flat"},{"path":"multivariate-methods.html","id":"summary-5","chapter":"22 Multivariate Methods","heading":"22.1.3 Summary","text":"","code":""},{"path":"multivariate-methods.html","id":"principal-components","chapter":"22 Multivariate Methods","heading":"22.2 Principal Components","text":"Unsupervised learningfind important featuresreduce dimensions data set“decorrelate” multivariate vectors dependence.uses eigenvector/eigvenvalue decomposition covariance (correlation) matrices.According “spectral decomposition theorem”, \\(\\mathbf{\\Sigma}_{p \\times p}\\) s positive semi-definite, symmetric, real matrix, exists orthogonal matrix \\(\\mathbf{}\\) \\(\\mathbf{'\\Sigma } = \\Lambda\\) \\(\\Lambda\\) diagonal matrix containing eigenvalues \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{\\Lambda} =\n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{} =\n\\left(\n\\begin{array}\n{cccc}\n\\mathbf{}_1 & \\mathbf{}_2 & \\ldots & \\mathbf{}_p\n\\end{array}\n\\right)\n\\]-th column \\(\\mathbf{}\\) , \\(\\mathbf{}_i\\), -th \\(p \\times 1\\) eigenvector \\(\\mathbf{\\Sigma}\\) corresponds eigenvalue, \\(\\lambda_i\\) , \\(\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_p\\) . Alternatively, express matrix decomposition:\\[\n\\mathbf{\\Sigma} = \\mathbf{\\Lambda }'\n\\]\\[\n\\mathbf{\\Sigma} = \\mathbf{}\n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\mathbf{}'\n= \\sum_{=1}^p \\lambda_i \\mathbf{}_i \\mathbf{}_i'\n\\]outer product \\(\\mathbf{}_i \\mathbf{}_i'\\) \\(p \\times p\\) matrix rank 1.example,\\(\\mathbf{x} \\sim N_2(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\[\n\\mathbf{\\mu} =\n\\left(\n\\begin{array}\n{c}\n5 \\\\\n12\n\\end{array}\n\\right);\n\\mathbf{\\Sigma} =\n\\left(\n\\begin{array}\n{cc}\n4 & 1 \\\\\n1 & 2\n\\end{array}\n\\right)\n\\],\\[\n\\mathbf{} =\n\\left(\n\\begin{array}\n{cc}\n0.9239 & -0.3827 \\\\\n0.3827 & 0.9239 \\\\\n\\end{array}\n\\right)\n\\]Columns \\(\\mathbf{}\\) eigenvectors decompositionUnder matrix multiplication (\\(\\mathbf{'\\Sigma }\\) \\(\\mathbf{'}\\) ), -diagonal elements equal 0Multiplying data matrix (.e., projecting data onto orthogonal axes); distribution resulting data (.e., “scores”) \\[\nN_2 (\\mathbf{'\\mu,'\\Sigma }) = N_2 (\\mathbf{'\\mu, \\Lambda})\n\\]Equivalently,\\[\n\\mathbf{y} = \\mathbf{'x} \\sim N\n\\left[\n\\left(\n\\begin{array}\n{c}\n9.2119 \\\\\n9.1733\n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n4.4144 & 0 \\\\\n0 & 1.5859\n\\end{array}\n\\right)\n\\right]\n\\]dependence data structure, plotNotes:-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto “new principal component axesThe values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto “new principal component axesVariances \\(y_1\\) greater possible projectionVariances \\(y_1\\) greater possible projectionCovariance matrix decomposition projection onto orthogonal axes = PCA","code":"\nlibrary(MASS)\nmu = as.matrix(c(5, 12))\nSigma = matrix(c(4, 1, 1, 2), nrow = 2, byrow = T)\nsim <- mvrnorm(n = 1000, mu = mu, Sigma = Sigma)\nplot(sim[, 1], sim[, 2])\nA_matrix = matrix(c(0.9239, -0.3827, 0.3827, 0.9239),\n                  nrow = 2,\n                  byrow = T)\nt(A_matrix) %*% A_matrix\n#>          [,1]     [,2]\n#> [1,] 1.000051 0.000000\n#> [2,] 0.000000 1.000051\n\nsim1 <-\n    mvrnorm(\n        n = 1000,\n        mu = t(A_matrix) %*% mu,\n        Sigma = t(A_matrix) %*% Sigma %*% A_matrix\n    )\nplot(sim1[, 1], sim1[, 2])"},{"path":"multivariate-methods.html","id":"population-principal-components","chapter":"22 Multivariate Methods","heading":"22.2.1 Population Principal Components","text":"\\(p \\times 1\\) vectors \\(\\mathbf{x}_1, \\dots , \\mathbf{x}_n\\) iid \\(var(\\mathbf{x}_i) = \\mathbf{\\Sigma}\\)first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)continues \\(y_i\\) \\(y_p\\)continues \\(y_i\\) \\(y_p\\)\\(\\mathbf{}_i\\)’s make matrix \\(\\mathbf{}\\) symmetric decomposition \\(\\mathbf{'\\Sigma } = \\mathbf{\\Lambda}\\) , \\(var(y_1) = \\lambda_1, \\dots , var(y_p) = \\lambda_p\\) total variance \\(\\mathbf{x}\\) \\[\n\\begin{aligned}\nvar(x_1) + \\dots + var(x_p) &= tr(\\Sigma) = \\lambda_1 + \\dots + \\lambda_p \\\\\n&= var(y_1) + \\dots + var(y_p)\n\\end{aligned}\n\\]Data ReductionTo reduce dimension data p (original) k dimensions without much “loss information”, can use properties population principal componentsSuppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)Suppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).proportion total variance accounted j-th principal component isThe proportion total variance accounted j-th principal component \\[\n\\frac{var(y_j)}{\\sum_{=1}^p var(y_i)} = \\frac{\\lambda_j}{\\sum_{=1}^p \\lambda_i}\n\\]proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal componentAbove example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal component","code":""},{"path":"multivariate-methods.html","id":"sample-principal-components","chapter":"22 Multivariate Methods","heading":"22.2.2 Sample Principal Components","text":"Since \\(\\mathbf{\\Sigma}\\) unknown, use\\[\n\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})'\n\\]Let \\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots \\ge \\hat{\\lambda}_p \\ge 0\\) eigenvalues \\(\\mathbf{S}\\) \\(\\hat{\\mathbf{}}_1, \\hat{\\mathbf{}}_2, \\dots, \\hat{\\mathbf{}}_p\\) denote eigenvectors \\(\\mathbf{S}\\), -th sample principal component score (principal component score) \\[\n\\hat{y}_{ij} = \\sum_{k=1}^p \\hat{}_{ik}x_{kj} = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\n\\]Properties Sample Principal ComponentsThe estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)estimated correlation \\(\\)-th principal component score \\(l\\)-th attribute \\(\\mathbf{x}\\) isThe estimated correlation \\(\\)-th principal component score \\(l\\)-th attribute \\(\\mathbf{x}\\) \\[\nr_{x_l , \\hat{y}_i} = \\frac{\\hat{}_{il}\\sqrt{\\lambda_i}}{\\sqrt{s_{ll}}}\n\\]correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According R. . Johnson, Wichern, et al. (2002), pp.433-434, \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According R. . Johnson, Wichern, et al. (2002), pp.433-434, \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”use k principal components, must calculate scores data vector sample\\[\n\\mathbf{y}_j =\n\\left(\n\\begin{array}\n{c}\ny_{1j} \\\\\ny_{2j} \\\\\n\\vdots \\\\\ny_{kj}\n\\end{array}\n\\right) =\n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\mathbf{x}_j \\\\\n\\hat{\\mathbf{}}_2' \\mathbf{x}_j \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k' \\mathbf{x}_j\n\\end{array}\n\\right) =\n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\\\\n\\hat{\\mathbf{}}_2' \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k'\n\\end{array}\n\\right) \\mathbf{x}_j\n\\]Issues:Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.PC invariant changes scale (Exception: trait rescaled multiplying constant, feet inches).\nPCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)\nPCA correlation matrix just rescaling trait unit variance\nTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCA\ntransformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)\nPCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\n\nmany PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\n\nPC invariant changes scale (Exception: trait rescaled multiplying constant, feet inches).PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA correlation matrix just rescaling trait unit variancePCA correlation matrix just rescaling trait unit varianceTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCATransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCAAfter transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )use \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.scale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.many PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\nmany PCs use can guided byScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.Scree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.Kaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocCompare eigenvalue scree plot data scree plot data randomized.Compare eigenvalue scree plot data scree plot data randomized.","code":""},{"path":"multivariate-methods.html","id":"application-8","chapter":"22 Multivariate Methods","heading":"22.2.3 Application","text":"PCA covariance matrix usually preferred due fact PCA invariant changes scale. Hence, PCA correlation matrix preferredThis also addresses problem multicollinearityThe eigvenvectors may differ multiplication -1 different implementation, interpretation.Covid ExampleTo reduce collinearity problem dataset, can use principal components regressors.MSE PC-based model larger regular regression, models large degree collinearity can still perform well.pcr function pls can used fitting PC regression (select optimal number components model).","code":"\nlibrary(tidyverse)\n## Read in and check data\nstock <- read.table(\"images/stock.dat\")\nnames(stock) <- c(\"allied\", \"dupont\", \"carbide\", \"exxon\", \"texaco\")\nstr(stock)\n#> 'data.frame':    100 obs. of  5 variables:\n#>  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...\n#>  $ dupont : num  0 -0.04485 0.06077 0.02995 -0.00379 ...\n#>  $ carbide: num  0 -0.00303 0.08815 0.06681 -0.03979 ...\n#>  $ exxon  : num  0.0395 -0.0145 0.0862 0.0135 -0.0186 ...\n#>  $ texaco : num  0 0.0435 0.0781 0.0195 -0.0242 ...\n\n## Covariance matrix of data\ncov(stock)\n#>               allied       dupont      carbide        exxon       texaco\n#> allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715\n#> dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431\n#> carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767\n#> exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734\n#> texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370\n\n## Correlation matrix of data\ncor(stock)\n#>            allied    dupont   carbide     exxon    texaco\n#> allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781\n#> dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534\n#> carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266\n#> exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293\n#> texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000\n\n# cov(scale(stock)) # give the same result\n\n## PCA with covariance\ncov_pca <- prcomp(stock) \n# uses singular value decomposition for calculation and an N -1 divisor\n# alternatively, princomp can do PCA via spectral decomposition, \n# but it has worse numerical accuracy\n\n# eigen values\ncov_results <- data.frame(eigen_values = cov_pca$sdev ^ 2)\ncov_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion)) \n#>   eigen_values proportion cumulative\n#> 1 0.0035953867 0.60159252  0.6015925\n#> 2 0.0007921798 0.13255027  0.7341428\n#> 3 0.0007364426 0.12322412  0.8573669\n#> 4 0.0005086686 0.08511218  0.9424791\n#> 5 0.0003437707 0.05752091  1.0000000\n# first 2 PCs account for 73% variance in the data\n\n# eigen vectors\ncov_pca$rotation # prcomp calls rotation\n#>               PC1         PC2        PC3         PC4         PC5\n#> allied  0.5605914  0.73884565 -0.1260222  0.28373183 -0.20846832\n#> dupont  0.4698673 -0.09286987 -0.4675066 -0.68793190  0.28069055\n#> carbide 0.5473322 -0.65401929 -0.1140581  0.50045312 -0.09603973\n#> exxon   0.2908932 -0.11267353  0.6099196 -0.43808002 -0.58203935\n#> texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638\n# princomp calls loadings.\n\n# first PC = overall average\n# second PC compares Allied to Carbide\n\n## PCA with correlation\n#same as scale(stock) %>% prcomp\ncor_pca <- prcomp(stock, scale = T)\n\n\n\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\ncor_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion))\n#>   eigen_values proportion cumulative\n#> 1    2.8564869 0.57129738  0.5712974\n#> 2    0.8091185 0.16182370  0.7331211\n#> 3    0.5400440 0.10800880  0.8411299\n#> 4    0.4513468 0.09026936  0.9313992\n#> 5    0.3430038 0.06860076  1.0000000\n\n# first egiven values corresponds to less variance \n# than PCA based on the covariance matrix\n\n# eigen vectors\ncor_pca$rotation\n#>               PC1        PC2        PC3        PC4        PC5\n#> allied  0.4635405 -0.2408499  0.6133570 -0.3813727  0.4532876\n#> dupont  0.4570764 -0.5090997 -0.1778996 -0.2113068 -0.6749814\n#> carbide 0.4699804 -0.2605774 -0.3370355  0.6640985  0.3957247\n#> exxon   0.4216770  0.5252647 -0.5390181 -0.4728036  0.1794482\n#> texaco  0.4213291  0.5822416  0.4336029  0.3812273 -0.3874672\n# interpretation of PC2 is different from above: \n# it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco \nload('images/MOcovid.RData')\ncovidpca <- prcomp(ndat[,-1],scale = T,center = T)\n\ncovidpca$rotation[,1:2]\n#>                                                          PC1         PC2\n#> X..Population.in.Rural.Areas                      0.32865838  0.05090955\n#> Area..sq..miles.                                  0.12014444 -0.28579183\n#> Population.density..sq..miles.                   -0.29670124  0.28312922\n#> Literacy.rate                                    -0.12517700 -0.08999542\n#> Families                                         -0.25856941  0.16485752\n#> Area.of.farm.land..sq..miles.                     0.02101106 -0.31070363\n#> Number.of.farms                                  -0.03814582 -0.44809679\n#> Average.value.of.all.property.per.farm..dollars. -0.05410709  0.14404306\n#> Estimation.of.rurality..                         -0.19040210  0.12089501\n#> Male..                                            0.02182394 -0.09568768\n#> Number.of.Physcians.per.100.000                  -0.31451606  0.13598026\n#> average.age                                       0.29414708  0.35593459\n#> X0.4.age.proportion                              -0.11431336 -0.23574057\n#> X20.44.age.proportion                            -0.32802128 -0.22718550\n#> X65.and.over.age.proportion                       0.30585033  0.32201626\n#> prop..White..nonHisp                              0.35627561 -0.14142646\n#> prop..Hispanic                                   -0.16655381 -0.15105342\n#> prop..Black                                      -0.33333359  0.24405802\n\n\n# Variability of each principal component: pr.var\npr.var <- covidpca$sdev ^ 2\n# Variance explained by each principal component: pve\npve <- pr.var / sum(pr.var)\nplot(\n    pve,\n    xlab = \"Principal Component\",\n    ylab = \"Proportion of Variance Explained\",\n    ylim = c(0, 0.5),\n    type = \"b\"\n)\n\nplot(\n    cumsum(pve),\n    xlab = \"Principal Component\",\n    ylab = \"Cumulative Proportion of Variance Explained\",\n    ylim = c(0, 1),\n    type = \"b\"\n)\n\n# the first six principe account for around 80% of the variance. \n\n\n#using base lm function for PC regression\npcadat <- data.frame(covidpca$x[, 1:6])\npcadat$y <- ndat$Y\npcr.man <- lm(log(y) ~ ., pcadat)\nmean(pcr.man$residuals ^ 2)\n#> [1] 0.03453371\n\n#comparison to lm w/o prin comps\nlm.fit <- lm(log(Y) ~ ., data = ndat)\nmean(lm.fit$residuals ^ 2)\n#> [1] 0.02335128"},{"path":"multivariate-methods.html","id":"factor-analysis","chapter":"22 Multivariate Methods","heading":"22.3 Factor Analysis","text":"PurposeUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsSimilar PCA, factor analysis model basedSimilar PCA, factor analysis model basedMore details can found PSU stat UMN statLet \\(\\mathbf{y}\\) set \\(p\\) measured variables\\(E(\\mathbf{y}) = \\mathbf{\\mu}\\)\\(var(\\mathbf{y}) = \\mathbf{\\Sigma}\\)\\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&=\n\\left(\n\\begin{array}\n{c}\nl_{11}f_1 + l_{12}f_2 + \\dots + l_{tm}f_m \\\\\n\\vdots \\\\\nl_{p1}f_1 + l_{p2}f_2 + \\dots + l_{pm} f_m\n\\end{array}\n\\right)\n+\n\\left(\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n\\vdots \\\\\n\\epsilon_p\n\\end{array}\n\\right)\n\\end{aligned}\n\\]\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).want \\(m\\) (number factors) much smaller \\(p\\) (number measured attributes)Restrictions model\\(E(\\epsilon) = \\mathbf{0}\\)\\(E(\\epsilon) = \\mathbf{0}\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independent\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independentAdditional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)Additional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)\\[\n\\begin{aligned}\nvar(\\mathbf{y}) = \\mathbf{\\Sigma} &=  var(\\mathbf{Lf} + \\mathbf{\\epsilon}) \\\\\n&= var(\\mathbf{Lf}) + var(\\epsilon) \\\\\n&= \\mathbf{L} var(\\mathbf{f}) \\mathbf{L}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LIL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LL}' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Since \\(\\mathbf{\\Psi}\\) diagonal, -diagonal elements \\(\\mathbf{LL}'\\) \\(\\sigma_{ij}\\), co variances \\(\\mathbf{\\Sigma}\\), means \\(cov(y_i, y_j) = \\sum_{k=1}^m l_{ik}l_{jk}\\) covariance \\(\\mathbf{y}\\) completely determined m factors ( \\(m <<p\\))\\(var(y_i) = \\sum_{k=1}^m l_{ik}^2 + \\psi_i\\) \\(\\psi_i\\) specific variance summation term -th communality (.e., portion variance -th variable contributed \\(m\\) common factors (\\(h_i^2 = \\sum_{k=1}^m l_{ik}^2\\))factor model uniquely determined orthogonal transformation factors.Let \\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix \\(\\mathbf{TT}' = \\mathbf{T'T} = \\mathbf{}\\) \\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&= \\mathbf{LTT'f} + \\epsilon \\\\\n&= \\mathbf{L}^*(\\mathbf{T'f}) + \\epsilon & \\text{} \\mathbf{L}^* = \\mathbf{LT}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\mathbf{LL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LTT'L} + \\mathbf{\\Psi} \\\\\n&= (\\mathbf{L}^*)(\\mathbf{L}^*)' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Hence, orthogonal transformation factors equally good description correlations among observed traits.Let \\(\\mathbf{y} = \\mathbf{Cx}\\) , \\(\\mathbf{C}\\) diagonal matrix, \\(\\mathbf{L}_y = \\mathbf{CL}_x\\) \\(\\mathbf{\\Psi}_y = \\mathbf{C\\Psi}_x\\mathbf{C}\\)Hence, can see factor analysis also invariant changes scale","code":""},{"path":"multivariate-methods.html","id":"methods-of-estimation","chapter":"22 Multivariate Methods","heading":"22.3.1 Methods of Estimation","text":"estimate \\(\\mathbf{L}\\)Principal Component MethodPrincipal Factor Method22.3.1.3","code":""},{"path":"multivariate-methods.html","id":"principal-component-method","chapter":"22 Multivariate Methods","heading":"22.3.1.1 Principal Component Method","text":"Spectral decomposition\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\lambda_1 \\mathbf{}_1 \\mathbf{}_1' + \\dots + \\lambda_p \\mathbf{}_p \\mathbf{}_p' \\\\\n&= \\mathbf{\\Lambda }' \\\\\n&= \\sum_{k=1}^m \\lambda+k \\mathbf{}_k \\mathbf{}_k' + \\sum_{k= m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k' \\\\\n&= \\sum_{k=1}^m l_k l_k' + \\sum_{k=m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k'\n\\end{aligned}\n\\]\\(l_k = \\mathbf{}_k \\sqrt{\\lambda_k}\\) second term diagonal general.Assume\\[\n\\psi_i = \\sigma_{ii} - \\sum_{k=1}^m l_{ik}^2 = \\sigma_{ii} -  \\sum_{k=1}^m \\lambda_i a_{ik}^2\n\\]\\[\n\\mathbf{\\Sigma} \\approx \\mathbf{LL}' + \\mathbf{\\Psi}\n\\]estimate \\(\\mathbf{L}\\) \\(\\Psi\\) , use expected eigenvalues eigenvectors \\(\\mathbf{S}\\) \\(\\mathbf{R}\\)estimated factor loadings don’t change number actors increasesThe estimated factor loadings don’t change number actors increasesThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedWe select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)","code":""},{"path":"multivariate-methods.html","id":"principal-factor-method","chapter":"22 Multivariate Methods","heading":"22.3.1.2 Principal Factor Method","text":"Consider modeling correlation matrix, \\(\\mathbf{R} = \\mathbf{L} \\mathbf{L}' + \\mathbf{\\Psi}\\) . \\[\n\\mathbf{L} \\mathbf{L}' = \\mathbf{R} - \\mathbf{\\Psi} =\n\\left(\n\\begin{array}\n{cccc}\nh_1^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & h_2^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & h_p^2\n\\end{array}\n\\right)\n\\]\\(h_i^2 = 1- \\psi_i\\) (communality)Suppose initial estimates available communalities, \\((h_1^*)^2,(h_2^*)^2, \\dots , (h_p^*)^2\\), can regress trait others, use \\(r^2\\) \\(h^2\\)estimate \\(\\mathbf{R} - \\mathbf{\\Psi}\\) step k \\[\n(\\mathbf{R} - \\mathbf{\\Psi})_k =\n\\left(\n\\begin{array}\n{cccc}\n(h_1^*)^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & (h_2^*)^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & (h_p^*)^2\n\\end{array}\n\\right) =\n\\mathbf{L}_k^*(\\mathbf{L}_k^*)'\n\\]\\[\n\\mathbf{L}_k^* = (\\sqrt{\\hat{\\lambda}_1^*\\hat{\\mathbf{}}_1^* , \\dots \\hat{\\lambda}_m^*\\hat{\\mathbf{}}_m^*})\n\\]\\[\n\\hat{\\psi}_{,k}^* = 1 - \\sum_{j=1}^m \\hat{\\lambda}_i^* (\\hat{}_{ij}^*)^2\n\\]used spectral decomposition estimated matrix \\((\\mathbf{R}- \\mathbf{\\Psi})\\) calculate \\(\\hat{\\lambda}_i^* s\\) \\(\\mathbf{\\hat{}}_i^* s\\)updating values \\((\\hat{h}_i^*)^2 = 1 - \\hat{\\psi}_{,k}^*\\) use form new \\(\\mathbf{L}_{k+1}^*\\) via another spectral decomposition. Repeat processNotes:matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, can\nfix communality greater 1 1 continues.\ncontinue iterations regardless size communalities. However, results can outside fo parameter space.\n\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, canfix communality greater 1 1 continues.fix communality greater 1 1 continues.continue iterations regardless size communalities. However, results can outside fo parameter space.continue iterations regardless size communalities. However, results can outside fo parameter space.","code":""},{"path":"multivariate-methods.html","id":"maximum-likelihood-method-factor-analysis","chapter":"22 Multivariate Methods","heading":"22.3.1.3 Maximum Likelihood Method","text":"Since need likelihood function, make additional (critical) assumption \\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)restriction\\(\\mathbf{L}' \\mathbf{\\Psi}^{-1}\\mathbf{L} = \\mathbf{\\Delta}\\) \\(\\mathbf{\\Delta}\\) diagonal matrix. (since factor loading matrix unique, need restriction).Notes:Finding MLE can computationally expensiveFinding MLE can computationally expensivewe typically use methods exploratory data analysiswe typically use methods exploratory data analysisLikelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)Likelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)","code":""},{"path":"multivariate-methods.html","id":"factor-rotation","chapter":"22 Multivariate Methods","heading":"22.3.2 Factor Rotation","text":"\\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix property \\[\n\\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}} = \\hat{\\mathbf{L}}^*(\\hat{\\mathbf{L}}^*)' + \\hat{\\mathbf{\\Psi}}\n\\]\\(\\mathbf{L}^* = \\mathbf{LT}\\)means estimated specific variances communalities altered orthogonal transformation.Since infinite number choices \\(\\mathbf{T}\\), selection criterion necessaryFor example, can find orthogonal transformation maximizes objective function\\[\n\\sum_{j = 1}^m [\\frac{1}{p}\\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 - \\{\\frac{\\gamma}{p} \\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 \\}^2]\n\\]\\(\\frac{l_{ij}^{*2}}{h_i}\\) “scaled loadings”, gives variables small communalities influence.Different choices \\(\\gamma\\) objective function correspond different orthogonal rotation found literature;Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Quartimax \\(\\gamma = 0\\)Quartimax \\(\\gamma = 0\\)Equimax \\(\\gamma = m/2\\)Equimax \\(\\gamma = m/2\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Promax: non-orthogonal olique transformationsPromax: non-orthogonal olique transformationsHarris-Kaiser (HK): non-orthogonal oblique transformationsHarris-Kaiser (HK): non-orthogonal oblique transformations","code":""},{"path":"multivariate-methods.html","id":"estimation-of-factor-scores","chapter":"22 Multivariate Methods","heading":"22.3.3 Estimation of Factor Scores","text":"Recall\\[\n(\\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}_{p \\times m}\\mathbf{f}_j + \\epsilon_j\n\\]factor model correct \\[\nvar(\\epsilon_j) = \\mathbf{\\Psi} = diag (\\psi_1, \\dots , \\psi_p)\n\\]Thus consider using weighted least squares estimate \\(\\mathbf{f}_j\\) , vector factor scores j-th sampled unit \\[\n\\begin{aligned}\n\\hat{\\mathbf{f}} &= (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\mu}) \\\\\n& \\approx (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\end{aligned}\n\\]","code":""},{"path":"multivariate-methods.html","id":"the-regression-method","chapter":"22 Multivariate Methods","heading":"22.3.3.1 The Regression Method","text":"Alternatively, can use regression method estimate factor scoresConsider joint distribution \\((\\mathbf{y}_j - \\mathbf{\\mu})\\) \\(\\mathbf{f}_j\\) assuming multivariate normality, maximum likelihood approach. ,\\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_j - \\mathbf{\\mu} \\\\\n\\mathbf{f}_j\n\\end{array}\n\\right) \\sim\nN_{p + m}\n\\left(\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{LL}' + \\mathbf{\\Psi} & \\mathbf{L} \\\\\n\\mathbf{L}' & \\mathbf{}_{m\\times m}\n\\end{array}\n\\right]\n\\right)\n\\]\\(m\\) factor model correctHence,\\[\nE(\\mathbf{f}_j | \\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}(\\mathbf{y}_j - \\mathbf{\\mu})\n\\]notice \\(\\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}\\) \\(m \\times p\\) matrix regression coefficientsThen, use estimated conditional mean vector estimate factor scores\\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}})^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]Alternatively, reduce effect possible incorrect determination fo number factors \\(m\\) using \\(\\mathbf{S}\\) substitute \\(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}}\\) \\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'\\mathbf{S}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]\\(j = 1,\\dots,n\\)","code":""},{"path":"multivariate-methods.html","id":"model-diagnostic","chapter":"22 Multivariate Methods","heading":"22.3.4 Model Diagnostic","text":"PlotsPlotsCheck outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check multivariate normality assumptionCheck multivariate normality assumptionUse univariate tests normality check factor scoresUse univariate tests normality check factor scoresConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fitConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fit","code":""},{"path":"multivariate-methods.html","id":"application-9","chapter":"22 Multivariate Methods","heading":"22.3.5 Application","text":"psych package,h2 = communalitiesh2 = communalitiesu2 = uniquenessu2 = uniquenesscom = complexitycom = complexityThe output info null hypothesis common factors statement “degrees freedom null model ..”output info null hypothesis number factors sufficient statement “total number observations …”One factor enough, two sufficient, enough data 3 factors (df -2 NA p-value). Hence, use 2-factor model.","code":"\nlibrary(psych)\nlibrary(tidyverse)\n## Load the data from the psych package\ndata(Harman.5)\nHarman.5\n#>         population schooling employment professional housevalue\n#> Tract1        5700      12.8       2500          270      25000\n#> Tract2        1000      10.9        600           10      10000\n#> Tract3        3400       8.8       1000           10       9000\n#> Tract4        3800      13.6       1700          140      25000\n#> Tract5        4000      12.8       1600          140      25000\n#> Tract6        8200       8.3       2600           60      12000\n#> Tract7        1200      11.4        400           10      16000\n#> Tract8        9100      11.5       3300           60      14000\n#> Tract9        9900      12.5       3400          180      18000\n#> Tract10       9600      13.7       3600          390      25000\n#> Tract11       9600       9.6       3300           80      12000\n#> Tract12       9400      11.4       4000          100      13000\n\n# Correlation matrix\ncor_mat <- cor(Harman.5)\ncor_mat\n#>              population  schooling employment professional housevalue\n#> population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157\n#> schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009\n#> employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599\n#> professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425\n#> housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000\n\n## Principal Component Method with Correlation\ncor_pca <- prcomp(Harman.5, scale = T)\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\n\ncor_results <- cor_results %>%\n    mutate(\n        proportion = eigen_values / sum(eigen_values),\n        cumulative = cumsum(proportion),\n        number = row_number()\n    )\ncor_results\n#>   eigen_values  proportion cumulative number\n#> 1   2.87331359 0.574662719  0.5746627      1\n#> 2   1.79666009 0.359332019  0.9339947      2\n#> 3   0.21483689 0.042967377  0.9769621      3\n#> 4   0.09993405 0.019986811  0.9969489      4\n#> 5   0.01525537 0.003051075  1.0000000      5\n\n# Scree plot of Eigenvalues\nscree_gg <- ggplot(cor_results, aes(x = number, y = eigen_values)) +\n    geom_line(alpha = 0.5) +\n    geom_text(aes(label = number)) +\n    scale_x_continuous(name = \"Number\") +\n    scale_y_continuous(name = \"Eigenvalue\") +\n    theme_bw()\nscree_gg\n\nscreeplot(cor_pca, type = 'lines')\n\n## Keep 2 factors based on scree plot and eigenvalues\nfactor_pca <- principal(Harman.5, nfactors = 2, rotate = \"none\")\nfactor_pca\n#> Principal Components Analysis\n#> Call: principal(r = Harman.5, nfactors = 2, rotate = \"none\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               PC1   PC2   h2    u2 com\n#> population   0.58  0.81 0.99 0.012 1.8\n#> schooling    0.77 -0.54 0.89 0.115 1.8\n#> employment   0.67  0.73 0.98 0.021 2.0\n#> professional 0.93 -0.10 0.88 0.120 1.0\n#> housevalue   0.79 -0.56 0.94 0.062 1.8\n#> \n#>                        PC1  PC2\n#> SS loadings           2.87 1.80\n#> Proportion Var        0.57 0.36\n#> Cumulative Var        0.57 0.93\n#> Proportion Explained  0.62 0.38\n#> Cumulative Proportion 0.62 1.00\n#> \n#> Mean item complexity =  1.7\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.03 \n#>  with the empirical chi square  0.29  with prob <  0.59 \n#> \n#> Fit based upon off diagonal values = 1\n\n# factor 1 = overall socioeconomic health\n# factor 2 = contrast of the population and employment against school and house value\n\n\n## Ssquared multiple correlation (SMC) prior, no rotation\nfactor_pca_smc <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_pca_smc\n#> Factor Analysis using method =  pa\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               PA1   PA2   h2      u2 com\n#> population   0.62  0.78 1.00 -0.0027 1.9\n#> schooling    0.70 -0.53 0.77  0.2277 1.9\n#> employment   0.70  0.68 0.96  0.0413 2.0\n#> professional 0.88 -0.15 0.80  0.2017 1.1\n#> housevalue   0.78 -0.60 0.96  0.0361 1.9\n#> \n#>                        PA1  PA2\n#> SS loadings           2.76 1.74\n#> Proportion Var        0.55 0.35\n#> Cumulative Var        0.55 0.90\n#> Proportion Explained  0.61 0.39\n#> Cumulative Proportion 0.61 1.00\n#> \n#> Mean item complexity =  1.7\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.34 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.03 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.02  with prob <  0.88 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.596\n#> RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n#> BIC =  -0.04\n#> Fit based upon off diagonal values = 1\n\n## SMC prior, Promax rotation\nfactor_pca_smc_pro <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"Promax\",\n    SMC = TRUE\n)\nfactor_pca_smc_pro\n#> Factor Analysis using method =  pa\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"Promax\", SMC = TRUE, \n#>     fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                PA1   PA2   h2      u2 com\n#> population   -0.11  1.02 1.00 -0.0027 1.0\n#> schooling     0.90 -0.11 0.77  0.2277 1.0\n#> employment    0.02  0.97 0.96  0.0413 1.0\n#> professional  0.75  0.33 0.80  0.2017 1.4\n#> housevalue    1.01 -0.14 0.96  0.0361 1.0\n#> \n#>                        PA1  PA2\n#> SS loadings           2.38 2.11\n#> Proportion Var        0.48 0.42\n#> Cumulative Var        0.48 0.90\n#> Proportion Explained  0.53 0.47\n#> Cumulative Proportion 0.53 1.00\n#> \n#>  With factor correlations of \n#>      PA1  PA2\n#> PA1 1.00 0.25\n#> PA2 0.25 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.34 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.03 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.02  with prob <  0.88 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.596\n#> RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n#> BIC =  -0.04\n#> Fit based upon off diagonal values = 1\n\n## SMC prior, varimax rotation\nfactor_pca_smc_var <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"varimax\",\n    SMC = TRUE\n)\n## Make a data frame of the loadings for ggplot2\nfactors_df <-\n    bind_rows(\n        data.frame(\n            y = rownames(factor_pca_smc$loadings),\n            unclass(factor_pca_smc$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_pro$loadings),\n            unclass(factor_pca_smc_pro$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_var$loadings),\n            unclass(factor_pca_smc_var$loadings)\n        ),\n        .id = \"Rotation\"\n    )\nflag_gg <- ggplot(factors_df) +\n    geom_vline(aes(xintercept = 0)) +\n    geom_hline(aes(yintercept = 0)) +\n    geom_point(aes(\n        x = PA2,\n        y = PA1,\n        col = y,\n        shape = y\n    ), size = 2) +\n    scale_x_continuous(name = \"Factor 2\", limits = c(-1.1, 1.1)) +\n    scale_y_continuous(name = \"Factor1\", limits = c(-1.1, 1.1)) +\n    facet_wrap(\"Rotation\", labeller = labeller(Rotation = c(\n        \"1\" = \"Original\", \"2\" = \"Promax\", \"3\" = \"Varimax\"\n    ))) +\n    coord_fixed(ratio = 1) # make aspect ratio of each facet 1\n\nflag_gg\n\n# promax and varimax did a good job to assign trait to a particular factor\n\nfactor_mle_1 <- fa(\n    Harman.5,\n    nfactors = 1,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_1\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 1, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               ML1    h2     u2 com\n#> population   0.97 0.950 0.0503   1\n#> schooling    0.14 0.021 0.9791   1\n#> employment   1.00 0.995 0.0049   1\n#> professional 0.51 0.261 0.7388   1\n#> housevalue   0.12 0.014 0.9864   1\n#> \n#>                 ML1\n#> SS loadings    2.24\n#> Proportion Var 0.45\n#> \n#> Mean item complexity =  1\n#> Test of the hypothesis that 1 factor is sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 5  and the objective function was  3.14 \n#> \n#> The root mean square of the residuals (RMSR) is  0.41 \n#> The df corrected root mean square of the residuals is  0.57 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  39.41  with prob <  2e-07 \n#> The total n.obs was  12  with Likelihood Chi Square =  24.56  with prob <  0.00017 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.022\n#> RMSEA index =  0.564  and the 90 % confidence intervals are  0.374 0.841\n#> BIC =  12.14\n#> Fit based upon off diagonal values = 0.5\n#> Measures of factor score adequacy             \n#>                                                    ML1\n#> Correlation of (regression) scores with factors   1.00\n#> Multiple R square of scores with factors          1.00\n#> Minimum correlation of possible factor scores     0.99\n\nfactor_mle_2 <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_2\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                ML2  ML1   h2    u2 com\n#> population   -0.03 1.00 1.00 0.005 1.0\n#> schooling     0.90 0.04 0.81 0.193 1.0\n#> employment    0.09 0.98 0.96 0.036 1.0\n#> professional  0.78 0.46 0.81 0.185 1.6\n#> housevalue    0.96 0.05 0.93 0.074 1.0\n#> \n#>                        ML2  ML1\n#> SS loadings           2.34 2.16\n#> Proportion Var        0.47 0.43\n#> Cumulative Var        0.47 0.90\n#> Proportion Explained  0.52 0.48\n#> Cumulative Proportion 0.52 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.31 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.05 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.05  with prob <  0.82 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.22  with prob <  0.14 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.658\n#> RMSEA index =  0.307  and the 90 % confidence intervals are  0 0.945\n#> BIC =  -0.26\n#> Fit based upon off diagonal values = 1\n#> Measures of factor score adequacy             \n#>                                                    ML2  ML1\n#> Correlation of (regression) scores with factors   0.98 1.00\n#> Multiple R square of scores with factors          0.95 1.00\n#> Minimum correlation of possible factor scores     0.91 0.99\n\nfactor_mle_3 <- fa(\n    Harman.5,\n    nfactors = 3,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_3\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 3, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                ML2  ML1   ML3   h2     u2 com\n#> population   -0.12 0.98 -0.11 0.98 0.0162 1.1\n#> schooling     0.89 0.15  0.29 0.90 0.0991 1.3\n#> employment    0.00 1.00  0.04 0.99 0.0052 1.0\n#> professional  0.72 0.52 -0.10 0.80 0.1971 1.9\n#> housevalue    0.97 0.13 -0.09 0.97 0.0285 1.1\n#> \n#>                        ML2  ML1  ML3\n#> SS loadings           2.28 2.26 0.11\n#> Proportion Var        0.46 0.45 0.02\n#> Cumulative Var        0.46 0.91 0.93\n#> Proportion Explained  0.49 0.49 0.02\n#> Cumulative Proportion 0.49 0.98 1.00\n#> \n#> Mean item complexity =  1.2\n#> Test of the hypothesis that 3 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are -2  and the objective function was  0 \n#> \n#> The root mean square of the residuals (RMSR) is  0 \n#> The df corrected root mean square of the residuals is  NA \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0  with prob <  NA \n#> The total n.obs was  12  with Likelihood Chi Square =  0  with prob <  NA \n#> \n#> Tucker Lewis Index of factoring reliability =  1.318\n#> Fit based upon off diagonal values = 1\n#> Measures of factor score adequacy             \n#>                                                    ML2  ML1  ML3\n#> Correlation of (regression) scores with factors   0.99 1.00 0.82\n#> Multiple R square of scores with factors          0.98 1.00 0.68\n#> Minimum correlation of possible factor scores     0.96 0.99 0.36"},{"path":"multivariate-methods.html","id":"discriminant-analysis","chapter":"22 Multivariate Methods","heading":"22.4 Discriminant Analysis","text":"Suppose two different populations observations come . Discriminant analysis seeks determine possible population observation comes making mistakes possibleThis alternative logistic approaches following advantages:\nclear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches suffer\nX normal classes sample size small, discriminant approaches can accurate\nalternative logistic approaches following advantages:clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferwhen clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferIf X normal classes sample size small, discriminant approaches can accurateIf X normal classes sample size small, discriminant approaches can accurateNotationSimilar MANOVA, let \\(\\mathbf{y}_{j1},\\mathbf{y}_{j2},\\dots, \\mathbf{y}_{in_j} \\sim iid f_j (\\mathbf{y})\\) \\(j = 1,\\dots, h\\)Let \\(f_j(\\mathbf{y})\\) density function population j . Note vector \\(\\mathbf{y}\\) contain measurements \\(p\\) traitsAssume observation one \\(h\\) possible populations.want form discriminant rule allocate observation \\(\\mathbf{y}\\) population j \\(\\mathbf{y}\\) fact population","code":""},{"path":"multivariate-methods.html","id":"known-populations","chapter":"22 Multivariate Methods","heading":"22.4.1 Known Populations","text":"maximum likelihood discriminant rule assigning observation \\(\\mathbf{y}\\) one \\(h\\) populations allocates \\(\\mathbf{y}\\) population gives largest likelihood \\(\\mathbf{y}\\)Consider likelihood single observation \\(\\mathbf{y}\\), form \\(f_j (\\mathbf{y})\\) j true population.Since \\(j\\) unknown, make likelihood large possible, choose value j causes \\(f_j (\\mathbf{y})\\) large possibleConsider simple univariate example. Suppose data one two binomial populations.first population \\(n= 10\\) trials success probability \\(p = .5\\)first population \\(n= 10\\) trials success probability \\(p = .5\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)population assign observation \\(y = 7\\)population assign observation \\(y = 7\\)Note:\n\\(f(y = 7|n = 10, p = .5) = .117\\)\n\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\nHence, choose second population\nNote:\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.Hence, choose second populationHence, choose second populationAnother exampleWe 2 populations, whereFirst population: \\(N(\\mu_1, \\sigma^2_1)\\)First population: \\(N(\\mu_1, \\sigma^2_1)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)likelihood single observation \\[\nf_j (y) = (2\\pi \\sigma^2_j)^{-1/2} \\exp\\{ -\\frac{1}{2}(\\frac{y - \\mu_j}{\\sigma_j})^2\\}\n\\]Consider likelihood ratio rule\\[\n\\begin{aligned}\n\\Lambda &= \\frac{\\text{likelihood y pop 1}}{\\text{likelihood y pop 2}} \\\\\n&= \\frac{f_1(y)}{f_2(y)} \\\\\n&= \\frac{\\sigma_2}{\\sigma_1} \\exp\\{-\\frac{1}{2}[(\\frac{y - \\mu_1}{\\sigma_1})^2- (\\frac{y - \\mu_2}{\\sigma_2})^2] \\}\n\\end{aligned}\n\\]Hence, classify intopop 1 \\(\\Lambda >1\\)pop 1 \\(\\Lambda >1\\)pop 2 \\(\\Lambda <1\\)pop 2 \\(\\Lambda <1\\)ties, flip coinfor ties, flip coinAnother way think:classify population 1 “standardized distance” y \\(\\mu_1\\) less “standardized distance” y \\(\\mu_2\\) referred quadratic discriminant rule.(Significant simplification occurs th special case \\(\\sigma_1 = \\sigma_2 = \\sigma^2\\))Thus, classify population 1 \\[\n(y - \\mu_2)^2 > (y - \\mu_1)^2\n\\]\\[\n|y- \\mu_2| > |y - \\mu_1|\n\\]\\[\n-2 \\log (\\Lambda) = -2y  \\frac{(\\mu_1 - \\mu_2)}{\\sigma^2} + \\frac{(\\mu_1^2 - \\mu_2^2)}{\\sigma^2} = \\beta y + \\alpha\n\\]Thus, classify population 1 less 0.Discriminant classification rule linear y case.","code":""},{"path":"multivariate-methods.html","id":"multivariate-expansion","chapter":"22 Multivariate Methods","heading":"22.4.1.1 Multivariate Expansion","text":"Suppose 2 populations\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\[\n\\begin{aligned}\n-2 \\log(\\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})}) &= \\log|\\mathbf{\\Sigma}_1| + (\\mathbf{x} - \\mathbf{\\mu}_1)' \\mathbf{\\Sigma}^{-1}_1 (\\mathbf{x} - \\mathbf{\\mu}_1) \\\\\n&- [\\log|\\mathbf{\\Sigma}_2|+ (\\mathbf{x} - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}_2 (\\mathbf{x} - \\mathbf{\\mu}_2) ]\n\\end{aligned}\n\\], classify population 1 less 0, otherwise, population 2. like univariate case non-equal variances, quadratic discriminant rule.covariance matrices equal: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}_1\\) classify population 1 \\[\n(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\ge 0\n\\]linear discriminant rule also referred Fisher’s linear discriminant functionBy assuming covariance matrices equal, assume shape orientation fo two populations must (can strong restriction)words, variable, can different mean variance.Note: LDA Bayes decision boundary linear. Hence, quadratic decision boundary might lead better classification. Moreover, assumption variance/covariance matrix across classes Gaussian densities imposes linear rule, allow predictors class follow MVN distribution class-specific mean vectors variance/covariance matrices, Quadratic Discriminant Analysis. , parameters estimate (gives flexibility LDA) cost variance (bias -variance tradeoff).\\(\\mathbf{\\mu}_1, \\mathbf{\\mu}_2, \\mathbf{\\Sigma}\\) known, probability misclassification can determined:\\[\n\\begin{aligned}\nP(2|1) &= P(\\text{calssify pop 2| x pop 1}) \\\\\n&= P((\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} \\mathbf{x} \\le \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)|\\mathbf{x} \\sim N(\\mu_1, \\mathbf{\\Sigma}) \\\\\n&= \\Phi(-\\frac{1}{2} \\delta)\n\\end{aligned}\n\\]\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\Phi\\) standard normal CDF\\(\\Phi\\) standard normal CDFSuppose \\(h\\) possible populations, distributed \\(N_p (\\mathbf{\\mu}_p, \\mathbf{\\Sigma})\\). , maximum likelihood (linear) discriminant rule allocates \\(\\mathbf{y}\\) population j j minimizes squared Mahalanobis distance\\[\n(\\mathbf{y} - \\mathbf{\\mu}_j)' \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{\\mu}_j)\n\\]","code":""},{"path":"multivariate-methods.html","id":"bayes-discriminant-rules","chapter":"22 Multivariate Methods","heading":"22.4.1.2 Bayes Discriminant Rules","text":"know population j prior probabilities \\(\\pi_j\\) (assume \\(\\pi_j >0\\)) can form Bayes discriminant rule.rule allocates observation \\(\\mathbf{y}\\) population \\(\\pi_j f_j (\\mathbf{y})\\) maximized.Note:Maximum likelihood discriminant rule special case Bayes discriminant rule, sets \\(\\pi_j = 1/h\\)Optimal Properties Bayes Discriminant Ruleslet \\(p_{ii}\\) probability correctly assigning observation population ilet \\(p_{ii}\\) probability correctly assigning observation population ithen one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .rule better alternative called admissibleA rule better alternative called admissibleBayes Discriminant Rules admissibleBayes Discriminant Rules admissibleIf utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)Bayes Discriminant Rules largest possible posterior probability correct allocation respect priorBayes Discriminant Rules largest possible posterior probability correct allocation respect priorThese properties show Bayes Discriminant rule best approach.properties show Bayes Discriminant rule best approach.Unequal CostWe want consider cost misallocation\nDefine \\(c_{ij}\\) cost associated allocation member population j population .\nwant consider cost misallocationDefine \\(c_{ij}\\) cost associated allocation member population j population .Assume \n\\(c_{ij} >0\\) \\(\\neq j\\)\n\\(c_{ij} = 0\\) \\(= j\\)\nAssume \\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} = 0\\) \\(= j\\)\\(c_{ij} = 0\\) \\(= j\\)determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)Example:Two binomial populations, size 10, probabilities \\(p_1 = .5\\) \\(p_2 = .7\\)probability first population .9However, suppose cost inappropriately allocating first population 1 cost incorrectly allocating second population 5.case, pick population 1 population 2In general, consider two regions, \\(R_1\\) \\(R_2\\) associated population 1 2:\\[\nR_1: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} \\ge \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\[\nR_2: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} < \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\(c_{12}\\) cost assigning member population 2 population 1.","code":""},{"path":"multivariate-methods.html","id":"discrimination-under-estimation","chapter":"22 Multivariate Methods","heading":"22.4.1.3 Discrimination Under Estimation","text":"Suppose know form distributions populations interests, still estimate parameters.Example:know distributions multivariate normal, estimate means variancesThe maximum likelihood discriminant rule allocates observation \\(\\mathbf{y}\\) population j j maximizes function\\[\nf_j (\\mathbf{y} |\\hat{\\theta})\n\\]\\(\\hat{\\theta}\\) maximum likelihood estimates unknown parametersFor instance, 2 multivariate normal populations distinct means, common variance covariance matrixMLEs \\(\\mathbf{\\mu}_1\\) \\(\\mathbf{\\mu}_2\\) \\(\\mathbf{\\bar{y}}_1\\) \\(\\mathbf{\\bar{y}}_2\\)common \\(\\mathbf{\\Sigma}\\) \\(\\mathbf{S}\\).Thus, estimated discriminant rule formed substituting sample values population values","code":""},{"path":"multivariate-methods.html","id":"native-bayes","chapter":"22 Multivariate Methods","heading":"22.4.1.4 Native Bayes","text":"challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .Naive Bayes makes one assumption: within k-th class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)Naive Bayes makes one assumption: within k-th class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)\n\\]\\(f_{kj}\\) density function j-th predictor among observation k-th class.assumption allows use joint distribution without need account dependence observations. However, (native) assumption can unrealistic, still works well cases number sample (n) large relative number features (p).assumption, \\[\nP(Y=k|X=x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1)\\times \\dots f_{lp}(x_p)}\n\\]need estimate one-dimensional density function \\(f_{kj}\\) either approaches:\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.\\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.","code":""},{"path":"multivariate-methods.html","id":"comparison-of-classification-methods","chapter":"22 Multivariate Methods","heading":"22.4.1.5 Comparison of Classification Methods","text":"Assuming K classes K baseline (James , Witten, Hastie, Tibshirani book)Comparing log odds relative K class","code":""},{"path":"multivariate-methods.html","id":"logistic-regression-2","chapter":"22 Multivariate Methods","heading":"22.4.1.5.1 Logistic Regression","text":"\\[\n\\log(\\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj}x_j\n\\]","code":""},{"path":"multivariate-methods.html","id":"lda","chapter":"22 Multivariate Methods","heading":"22.4.1.5.2 LDA","text":"\\[\n\\log(\\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \\sum_{j=1}^p b_{kj} x_j\n\\]\\(a_k\\) \\(b_{kj}\\) functions \\(\\pi_k, \\pi_K, \\mu_k , \\mu_K, \\mathbf{\\Sigma}\\)Similar logistic regression, LDA assumes log odds linear \\(x\\)Even though look like form, parameters logistic regression estimated MLE, LDA linear parameters specified prior normal distributionsWe expect LDA outperform logistic regression normality assumption (approximately) holds, logistic regression perform better ","code":""},{"path":"multivariate-methods.html","id":"qda","chapter":"22 Multivariate Methods","heading":"22.4.1.5.3 QDA","text":"\\[\n\\log(\\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \\sum_{j=1}^{p}b_{kj}x_{j} + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl}x_j x_l\n\\]\\(a_k, b_{kj}, c_{kjl}\\) functions \\(\\pi_k , \\pi_K, \\mu_k, \\mu_K ,\\mathbf{\\Sigma}_k, \\mathbf{\\Sigma}_K\\)","code":""},{"path":"multivariate-methods.html","id":"naive-bayes","chapter":"22 Multivariate Methods","heading":"22.4.1.5.4 Naive Bayes","text":"\\[\n\\log (\\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \\sum_{j=1}^p g_{kj} (x_j)\n\\]\\(a_k = \\log (\\pi_k / \\pi_K)\\) \\(g_{kj}(x_j) = \\log(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\\) form generalized additive model","code":""},{"path":"multivariate-methods.html","id":"summary-6","chapter":"22 Multivariate Methods","heading":"22.4.1.5.5 Summary","text":"LDA special case QDALDA special case QDALDA robust comes high dimensionsLDA robust comes high dimensionsAny classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).Compare non-parametric method (KNN)KNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN also limited high-dimensions due curse dimensionalityKNN also limited high-dimensions due curse dimensionalitySince QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methods KNN classification. QDA can fewer training observations KNN flexible.Since QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methods KNN classification. QDA can fewer training observations KNN flexible.simulation:like linear regression, can also introduce flexibility including transformed features \\(\\sqrt{X}, X^2, X^3\\)","code":""},{"path":"multivariate-methods.html","id":"probabilities-of-misclassification","chapter":"22 Multivariate Methods","heading":"22.4.2 Probabilities of Misclassification","text":"distribution exactly known, can determine misclassification probabilities exactly. however, need estimate population parameters, estimate probability misclassificationNaive method\nPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.\ntend optimistic number samples one populations small.\nNaive methodPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.Plugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.tend optimistic number samples one populations small.tend optimistic number samples one populations small.Resubstitution method\nUse proportion samples population allocated another population estimate misclassification probability\nalso optimistic number samples small\nResubstitution methodUse proportion samples population allocated another population estimate misclassification probabilityUse proportion samples population allocated another population estimate misclassification probabilityBut also optimistic number samples smallBut also optimistic number samples smallJack-knife estimates:\ntwo methods use observation estimate parameters also misclassification probabilities based upon discriminant rule\nAlternatively, determine discriminant rule based upon data except k-th observation j-th population\n, determine k-th observation misclassified rule\nperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassified\nrepeat process \\(\\neq j\\) populations\nmethod reliable others, also computationally intensive\nJack-knife estimates:two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleThe two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleAlternatively, determine discriminant rule based upon data except k-th observation j-th populationAlternatively, determine discriminant rule based upon data except k-th observation j-th populationthen, determine k-th observation misclassified rulethen, determine k-th observation misclassified ruleperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassifiedperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassifiedrepeat process \\(\\neq j\\) populationsrepeat process \\(\\neq j\\) populationsThis method reliable others, also computationally intensiveThis method reliable others, also computationally intensiveCross-ValidationCross-ValidationSummaryConsider group-specific densities \\(f_j (\\mathbf{x})\\) multivariate vector \\(\\mathbf{x}\\).Assume equal misclassifications costs, Bayes classification probability \\(\\mathbf{x}\\) belonging j-th population \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j f_j (\\mathbf{x})}{\\sum_{k=1}^h \\pi_k f_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)\\(h\\) possible groups.classify group probability membership largestAlternatively, can write terms generalized squared distance formation\\[\nD_j^2 (\\mathbf{x}) = d_j^2 (\\mathbf{x})+ g_1(j) + g_2 (j)\n\\]\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \n\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\n\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\n\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriateand\\[\ng_1(j) =\n\\begin{cases}\n\\ln |\\mathbf{S}_j| & \\text{within group covariances equal} \\\\\n0 & \\text{pooled covariance}\n\\end{cases}\n\\]\\[\ng_2(j) =\n\\begin{cases}\n-2 \\ln \\pi_j & \\text{prior probabilities equal} \\\\\n0 & \\text{prior probabilities equal}\n\\end{cases}\n\\], posterior probability belonging group j \\[\np(j| \\mathbf{x})  = \\frac{\\exp(-.5 D_j^2(\\mathbf{x}))}{\\sum_{k=1}^h \\exp(-.5 D^2_k (\\mathbf{x}))}\n\\]\\(j = 1,\\dots , h\\)\\(\\mathbf{x}\\) classified group j \\(p(j | \\mathbf{x})\\) largest \\(j = 1,\\dots,h\\) (, \\(D_j^2(\\mathbf{x})\\) smallest).","code":""},{"path":"multivariate-methods.html","id":"assessing-classification-performance","chapter":"22 Multivariate Methods","heading":"22.4.2.1 Assessing Classification Performance","text":"binary classification, confusion matrixand table 4.6 (James et al. 2013)ROC curve (receiver Operating Characteristics) graphical comparison sensitivity (true positive) specificity ( = 1 - false positive)y-axis = true positive ratex-axis = false positive rateas change threshold rate classifying observation 0 1AUC (area ROC) ideally equal 1, bad classifier AUC = 0.5 (pure chance)","code":""},{"path":"multivariate-methods.html","id":"unknown-populations-nonparametric-discrimination","chapter":"22 Multivariate Methods","heading":"22.4.3 Unknown Populations/ Nonparametric Discrimination","text":"multivariate data Gaussian, known distributional form , can use following methods","code":""},{"path":"multivariate-methods.html","id":"kernel-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.1 Kernel Methods","text":"approximate \\(f_j (\\mathbf{x})\\) kernel density estimate\\[\n\\hat{f}_j(\\mathbf{x}) = \\frac{1}{n_j} \\sum_{= 1}^{n_j} K_j (\\mathbf{x} - \\mathbf{x}_i)\n\\]\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.Thus, finding \\(\\hat{f}_j (\\mathbf{x})\\) \\(h\\) populations, posterior probability group membership \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j \\hat{f}_j (\\mathbf{x})}{\\sum_{k-1}^h \\pi_k \\hat{f}_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)different choices kernel function:UniformUniformNormalNormalEpanechnikovEpanechnikovBiweightBiweightTriweightTriweightWe kernels, pick “radius” (variance, width, window width, bandwidth) kernel, smoothing parameter (larger radius, smooth kernel estimate density).select smoothness parameter, can use following methodIf believe populations close multivariate normal, \\[\nR = (\\frac{4/(2p+1)}{n_j})^{1/(p+1}\n\\]since know sure, might choose several different values select one vies best sample cross-validation discrimination.Moreover, also decide whether use different kernel smoothness different populations, similar individual pooled covariances classical methodology.","code":""},{"path":"multivariate-methods.html","id":"nearest-neighbor-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.2 Nearest Neighbor Methods","text":"nearest neighbor (also known k-nearest neighbor) method performs classification new observation vector based group membership nearest neighbors. practice, find\\[\nd_{ij}^2 (\\mathbf{x}, \\mathbf{x}_i) = (\\mathbf{x}, \\mathbf{x}_i) V_j^{-1}(\\mathbf{x}, \\mathbf{x}_i)\n\\]distance vector \\(\\mathbf{x}\\) \\(\\)-th observation group \\(j\\)consider different choices \\(\\mathbf{V}_j\\)example,\\[\n\\begin{aligned}\n\\mathbf{V}_j &= \\mathbf{S}_p \\\\\n\\mathbf{V}_j &= \\mathbf{S}_j \\\\\n\\mathbf{V}_j &= \\mathbf{} \\\\\n\\mathbf{V}_j &= diag (\\mathbf{S}_p)\n\\end{aligned}\n\\]find \\(k\\) observations closest \\(\\mathbf{x}\\) (users pick \\(k\\)). classify common population, weighted prior.","code":""},{"path":"multivariate-methods.html","id":"modern-discriminant-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.3 Modern Discriminant Methods","text":"Note:Logistic regression (without random effects) flexible model-based procedure classification two populations.extension logistic regression multi-group setting polychotomous logistic regression (, mulinomial regression).machine learning pattern recognition growing strong focus nonlinear discriminant analysis methods :radial basis function networksradial basis function networkssupport vector machinessupport vector machinesmultiplayer perceptrons (neural networks)multiplayer perceptrons (neural networks)general framework\\[\ng_j (\\mathbf{x}) = \\sum_{l = 1}^m w_{jl}\\phi_l (\\mathbf{x}; \\mathbf{\\theta}_l) + w_{j0}\n\\]\\(j = 1,\\dots, h\\)\\(j = 1,\\dots, h\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)assign \\(\\mathbf{x}\\) \\(j\\)-th population \\(g_j(\\mathbf{x})\\) maximum \\(j = 1,\\dots, h\\)Development usually focuses choice estimation basis functions, \\(\\phi_l\\) estimation weights \\(w_{jl}\\)details can found (Webb, Copsey, Cawley 2011)","code":""},{"path":"multivariate-methods.html","id":"application-10","chapter":"22 Multivariate Methods","heading":"22.4.4 Application","text":"","code":"\nlibrary(class)\nlibrary(klaR)\nlibrary(MASS)\nlibrary(tidyverse)\n\n## Read in the data\ncrops <- read.table(\"images/crops.txt\")\nnames(crops) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops)\n#> 'data.frame':    36 obs. of  5 variables:\n#>  $ crop: chr  \"Corn\" \"Corn\" \"Corn\" \"Corn\" ...\n#>  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...\n#>  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...\n#>  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...\n#>  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...\n\n\n## Read in test data\ncrops_test <- read.table(\"images/crops_test.txt\")\nnames(crops_test) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops_test)\n#> 'data.frame':    5 obs. of  5 variables:\n#>  $ crop: chr  \"Corn\" \"Soybeans\" \"Cotton\" \"Sugarbeets\" ...\n#>  $ y1  : int  16 21 29 54 32\n#>  $ y2  : int  27 25 24 23 32\n#>  $ y3  : int  31 23 26 21 62\n#>  $ y4  : int  33 24 28 54 16"},{"path":"multivariate-methods.html","id":"lda-1","chapter":"22 Multivariate Methods","heading":"22.4.4.1 LDA","text":"Default prior proportional sample size lda qda fit constant intercept termLDA didn’t well within sample --sample data.","code":"\n## Linear discriminant analysis\nlda_mod <- lda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\nlda_mod\n#> Call:\n#> lda(crop ~ y1 + y2 + y3 + y4, data = crops)\n#> \n#> Prior probabilities of groups:\n#>     Clover       Corn     Cotton   Soybeans Sugarbeets \n#>  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 \n#> \n#> Group means:\n#>                  y1       y2       y3       y4\n#> Clover     46.36364 32.63636 34.18182 36.63636\n#> Corn       15.28571 22.71429 27.42857 33.14286\n#> Cotton     34.50000 32.66667 35.00000 39.16667\n#> Soybeans   21.00000 27.00000 23.50000 29.66667\n#> Sugarbeets 31.00000 32.16667 20.00000 40.50000\n#> \n#> Coefficients of linear discriminants:\n#>              LD1          LD2         LD3          LD4\n#> y1 -6.147360e-02  0.009215431 -0.02987075 -0.014680566\n#> y2 -2.548964e-02  0.042838972  0.04631489  0.054842132\n#> y3  1.642126e-02 -0.079471595  0.01971222  0.008938745\n#> y4  5.143616e-05 -0.013917423  0.05381787 -0.025717667\n#> \n#> Proportion of trace:\n#>    LD1    LD2    LD3    LD4 \n#> 0.7364 0.1985 0.0576 0.0075\n\n## Look at accuracy on the training data\nlda_fitted <- predict(lda_mod,newdata = crops)\n# Contingency table\nlda_table <- table(truth = crops$crop, fitted = lda_fitted$class)\nlda_table\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          6    0      3        0          2\n#>   Corn            0    6      0        1          0\n#>   Cotton          3    0      1        2          0\n#>   Soybeans        0    1      1        3          1\n#>   Sugarbeets      1    1      0        2          2\n# accuracy of 0.5 is just random (not good)\n\n## Posterior probabilities of membership\ncrops_post <- cbind.data.frame(crops,\n                               crop_pred = lda_fitted$class,\n                               lda_fitted$posterior)\ncrops_post <- crops_post %>%\n    mutate(missed = crop != crop_pred)\nhead(crops_post)\n#>   crop y1 y2 y3 y4 crop_pred     Clover      Corn    Cotton  Soybeans\n#> 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845\n#> 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101\n#> 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105\n#> 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477\n#> 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696\n#> 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924\n#>   Sugarbeets missed\n#> 1 0.08971545  FALSE\n#> 2 0.07219340  FALSE\n#> 3 0.11573442  FALSE\n#> 4 0.09546233  FALSE\n#> 5 0.03980738  FALSE\n#> 6 0.10109590   TRUE\n# posterior shows that posterior of corn membership is much higher than the prior\n\n## LOOCV\n# leave-one-out cross validation for linear discriminant analysis\n# cannot run the predict function using the object with CV = TRUE \n# because it returns the within sample predictions\nlda_cv <- lda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nlda_table_cv <- table(truth = crops$crop, fitted = lda_cv$class)\nlda_table_cv\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          4    3      1        0          3\n#>   Corn            0    4      1        2          0\n#>   Cotton          3    0      0        2          1\n#>   Soybeans        0    1      1        3          1\n#>   Sugarbeets      2    1      0        2          1\n\n## Predict the test data\nlda_pred <- predict(lda_mod, newdata = crops_test)\n\n## Make a contingency table with truth and most likely class\ntable(truth=crops_test$crop, predict=lda_pred$class)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          0    0      1        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      0        1          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      1    0      0        0          0"},{"path":"multivariate-methods.html","id":"qda-1","chapter":"22 Multivariate Methods","heading":"22.4.4.2 QDA","text":"","code":"\n## Quadratic discriminant analysis\nqda_mod <- qda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\n\n## Look at accuracy on the training data\nqda_fitted <- predict(qda_mod, newdata = crops)\n# Contingency table\nqda_table <- table(truth = crops$crop, fitted = qda_fitted$class)\nqda_table\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          9    0      0        0          2\n#>   Corn            0    7      0        0          0\n#>   Cotton          0    0      6        0          0\n#>   Soybeans        0    0      0        6          0\n#>   Sugarbeets      0    0      1        1          4\n\n## LOOCV\nqda_cv <- qda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nqda_table_cv <- table(truth = crops$crop, fitted = qda_cv$class)\nqda_table_cv\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          9    0      0        0          2\n#>   Corn            3    2      0        0          2\n#>   Cotton          3    0      2        0          1\n#>   Soybeans        3    0      0        2          1\n#>   Sugarbeets      3    0      1        1          1\n\n## Predict the test data\nqda_pred <- predict(qda_mod, newdata = crops_test)\n## Make a contingency table with truth and most likely class\ntable(truth = crops_test$crop, predict = qda_pred$class)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      1        0          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1"},{"path":"multivariate-methods.html","id":"knn","chapter":"22 Multivariate Methods","heading":"22.4.4.3 KNN","text":"knn uses design matrices features.","code":"\n## Design matrices\nX_train <- crops %>%\n    dplyr::select(-crop)\nX_test <- crops_test %>%\n    dplyr::select(-crop)\nY_train <- crops$crop\nY_test <- crops_test$crop\n\n## Nearest neighbors with 2 neighbors\nknn_2 <- knn(X_train, X_train, Y_train, k = 2)\ntable(truth = Y_train, fitted = knn_2)\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          7    0      2        1          1\n#>   Corn            0    7      0        0          0\n#>   Cotton          0    0      4        0          2\n#>   Soybeans        0    0      0        4          2\n#>   Sugarbeets      1    0      2        0          3\n\n## Accuracy\nmean(Y_train==knn_2)\n#> [1] 0.6944444\n\n## Performance on test data\nknn_2_test <- knn(X_train, X_test, Y_train, k = 2)\ntable(truth = Y_test, predict = knn_2_test)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      0        0          1\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1\n\n## Accuracy\nmean(Y_test==knn_2_test)\n#> [1] 0.8\n\n## Nearest neighbors with 3 neighbors\nknn_3 <- knn(X_train, X_train, Y_train, k = 3)\ntable(truth = Y_train, fitted = knn_3)\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          8    0      1        1          1\n#>   Corn            0    4      1        2          0\n#>   Cotton          1    1      3        0          1\n#>   Soybeans        0    1      1        4          0\n#>   Sugarbeets      0    0      0        2          4\n\n## Accuracy\nmean(Y_train==knn_3)\n#> [1] 0.6388889\n\n## Performance on test data\nknn_3_test <- knn(X_train, X_test, Y_train, k = 3)\ntable(truth = Y_test, predict = knn_3_test)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      1        0          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1\n\n## Accuracy\nmean(Y_test==knn_3_test)\n#> [1] 1"},{"path":"multivariate-methods.html","id":"stepwise","chapter":"22 Multivariate Methods","heading":"22.4.4.4 Stepwise","text":"Stepwise discriminant analysis using stepclass function klaR package.Iris Data","code":"\nstep <- stepclass(\n    crop ~ y1 + y2 + y3 + y4,\n    data = crops,\n    method = \"qda\",\n    improvement = 0.15\n)\n#> correctness rate: 0.45;  in: \"y1\";  variables (1): y1 \n#> \n#>  hr.elapsed min.elapsed sec.elapsed \n#>        0.00        0.00        0.16\n\nstep$process\n#>    step var varname result.pm\n#> 0 start   0      --      0.00\n#> 1    in   1      y1      0.45\n\nstep$performance.measure\n#> [1] \"correctness rate\"\n\nlibrary(dplyr)\ndata('iris')\nset.seed(1)\nsamp <-\n    sample.int(nrow(iris), size = floor(0.70 * nrow(iris)), replace = F)\n\ntrain.iris <- iris[samp,] %>% mutate_if(is.numeric,scale)\ntest.iris <- iris[-samp,] %>% mutate_if(is.numeric,scale)\n\nlibrary(ggplot2)\niris.model <- lda(Species ~ ., data = train.iris)\n#pred\npred.lda <- predict(iris.model, test.iris)\ntable(truth = test.iris$Species, prediction = pred.lda$class)\n#>             prediction\n#> truth        setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         17         0\n#>   virginica       0          0        13\n\nplot(iris.model)\n\niris.model.qda <- qda(Species~.,data=train.iris)\n#pred\npred.qda <- predict(iris.model.qda,test.iris)\ntable(truth=test.iris$Species,prediction=pred.qda$class)\n#>             prediction\n#> truth        setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         16         1\n#>   virginica       0          0        13"},{"path":"multivariate-methods.html","id":"pca-with-discriminant-analysis","chapter":"22 Multivariate Methods","heading":"22.4.4.5 PCA with Discriminant Analysis","text":"can use PCA dimension reduction discriminant analysis","code":"\nzeros <- as.matrix(read.table(\"images/mnist0_train_b.txt\"))\nnines <- as.matrix(read.table(\"images/mnist9_train_b.txt\"))\ntrain <- rbind(zeros[1:1000, ], nines[1:1000, ])\ntrain <- train / 255 #divide by 255 per notes (so ranges from 0 to 1)\ntrain <- t(train) #each column is an observation\nimage(matrix(train[, 1], nrow = 28), main = 'Example image, unrotated')\n\n\ntest <- rbind(zeros[2501:3000, ], nines[2501:3000, ])\ntest <- test / 255\ntest <- t(test)\ny.train <- c(rep(0, 1000), rep(9, 1000))\ny.test <- c(rep(0, 500), rep(9, 500))\n\n\nlibrary(MASS)\npc <- prcomp(t(train))\ntrain.large <- data.frame(cbind(y.train, pc$x[, 1:10]))\nlarge <- lda(y.train ~ ., data = train.large)\n#the test data set needs to be constucted w/ the same 10 princomps\ntest.large <- data.frame(cbind(y.test, predict(pc, t(test))[, 1:10]))\npred.lda <- predict(large, test.large)\ntable(truth = test.large$y.test, prediction = pred.lda$class)\n#>      prediction\n#> truth   0   9\n#>     0 491   9\n#>     9   5 495\n\nlarge.qda <- qda(y.train~.,data=train.large)\n#prediction\npred.qda <- predict(large.qda,test.large)\ntable(truth=test.large$y.test,prediction=pred.qda$class)\n#>      prediction\n#> truth   0   9\n#>     0 493   7\n#>     9   3 497"},{"path":"quasi-experimental.html","id":"quasi-experimental","chapter":"23 Quasi-experimental","heading":"23 Quasi-experimental","text":"cases, means pre- post-intervention data.Great resources causal inference include Causal Inference Mixtape Recent Advances Micro, especially like read history causal inference field well (codes Stata, R, Python).Libraries R:EconometricsEconometricsCausal InferenceCausal InferenceIdentification strategy quasi-experiment (ways prove formal statistical test, can provide plausible argument evidence)exogenous variation comes (argument institutional knowledge)Exclusion restriction: Evidence variation exogenous shock outcome due factors\nstable unit treatment value assumption (SUTVA) states treatment unit \\(\\) affect outcome unit \\(\\) (.e., spillover control groups)\nstable unit treatment value assumption (SUTVA) states treatment unit \\(\\) affect outcome unit \\(\\) (.e., spillover control groups)quasi-experimental methods involve tradeoff power support exogeneity assumption (.e., discard variation data exogenous).Consequently, don’t usually look \\(R^2\\) (Ebbes, Papies, Van Heerde 2011). can even misleading use \\(R^2\\) basis model comparison.Clustering based design, expectations correlation (Abadie et al. 2023). small sample, use wild bootstrap procedure (Cameron, Gelbach, Miller 2008) correct downward bias (see (Cai et al. 2022)additional assumptions).Typical robustness check: recommended (Goldfarb, Tucker, Wang 2022)Different controls: show models without controls. Typically, want see change estimate interest. See (Altonji, Elder, Taber 2005) formal assessment based Rosenbaum bounds (.e., changes estimate threat Omitted variables estimate). specific applications marketing, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012)Different controls: show models without controls. Typically, want see change estimate interest. See (Altonji, Elder, Taber 2005) formal assessment based Rosenbaum bounds (.e., changes estimate threat Omitted variables estimate). specific applications marketing, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012)Different functional formsDifferent functional formsDifferent window time (longitudinal setting)Different window time (longitudinal setting)Different dependent variables (related) different measures dependent variablesDifferent dependent variables (related) different measures dependent variablesDifferent control group size (matched vs. un-matched samples)Different control group size (matched vs. un-matched samples)Placebo tests: see placebo test setting .Placebo tests: see placebo test setting .Showing mechanism:Mediation analysisMediation analysisModeration analysis\nEstimate model separately (different groups)\nAssess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.\nModeration analysisEstimate model separately (different groups)Estimate model separately (different groups)Assess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.Assess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.External Validity:Assess representative sample isAssess representative sample isExplain limitation design.Explain limitation design.Use quasi-experimental results conjunction structural models: see (J. E. Anderson, Larch, Yotov 2015; Einav, Finkelstein, Levin 2010; Chung, Steenburgh, Sudhir 2014)Use quasi-experimental results conjunction structural models: see (J. E. Anderson, Larch, Yotov 2015; Einav, Finkelstein, Levin 2010; Chung, Steenburgh, Sudhir 2014)LimitationWhat identifying assumptions identification strategyWhat threats validity assumptions?address ? maybe future research can address .","code":""},{"path":"quasi-experimental.html","id":"natural-experiments","chapter":"23 Quasi-experimental","heading":"23.1 Natural Experiments","text":"Reusing natural experiments research, particularly employing identical methods determine treatment effect given setting, can pose problems hypothesis testing.Simulations show \\(N_{\\text{Outcome}} >> N_{\\text{True effect}}\\), 50% statistically significant findings may false positives (Heath et al. 2023, 2331).Solutions:Bonferroni correctionBonferroni correctionRomano Wolf (2005) Romano Wolf (2016) correction: recommendedRomano Wolf (2005) Romano Wolf (2016) correction: recommendedBenjamini Yekutieli (2001) correctionBenjamini Yekutieli (2001) correctionAlternatively, refer rules thumb Table AI (Heath et al. 2023, 2356).Alternatively, refer rules thumb Table AI (Heath et al. 2023, 2356).applying multiple testing corrections, can either use (give similar results anyway (Heath et al. 2023, 2335)):Chronological Sequencing: Outcomes ordered date first reported, multiple testing corrections applied sequence. method progressively raises statistical significance threshold outcomes reviewed time.Chronological Sequencing: Outcomes ordered date first reported, multiple testing corrections applied sequence. method progressively raises statistical significance threshold outcomes reviewed time.Best Foot Forward Policy: Outcomes ordered least likely rejected based experimental data. Used primarily clinical trials, approach gives priority intended treatment effects, subjected less stringent statistical requirements. New outcomes added sequence linked primary treatment effect.Best Foot Forward Policy: Outcomes ordered least likely rejected based experimental data. Used primarily clinical trials, approach gives priority intended treatment effects, subjected less stringent statistical requirements. New outcomes added sequence linked primary treatment effect.tests, one can use multtest::mt.rawp2adjp includes:BonferroniHolm (1979)Šidák (1967)Hochberg (1988)Benjamini Hochberg (1995)Benjamini Yekutieli (2001)Adaptive Benjamini Hochberg (2000)Two-stage Benjamini, Krieger, Yekutieli (2006)Permutation adjusted p-values simple multiple testing procedures","code":"# Romano-Wolf correction\nlibrary(fixest)\nlibrary(wildrwolf)\n\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\n\nfit1 <- feols(Sepal.Width ~ Sepal.Length , data = iris)\nfit2 <- feols(Petal.Length ~ Sepal.Length, data = iris)\nfit3 <- feols(Petal.Width ~ Sepal.Length, data = iris)\n\nres <- rwolf(\n  models = list(fit1, fit2, fit3), \n  param = \"Sepal.Length\",  \n  B = 500\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\nres\n#>   model   Estimate Std. Error   t value     Pr(>|t|) RW Pr(>|t|)\n#> 1     1 -0.0618848 0.04296699 -1.440287    0.1518983 0.139720559\n#> 2     2   1.858433 0.08585565  21.64602 1.038667e-47 0.001996008\n#> 3     3  0.7529176 0.04353017  17.29645 2.325498e-37 0.001996008\n# BiocManager::install(\"multtest\")\nlibrary(multtest)\n\nprocs <-\n    c(\"Bonferroni\",\n      \"Holm\",\n      \"Hochberg\",\n      \"SidakSS\",\n      \"SidakSD\",\n      \"BH\",\n      \"BY\",\n      \"ABH\",\n      \"TSBH\")\n\nmt.rawp2adjp(\n    # p-values\n    runif(10),\n    procs) |> causalverse::nice_tab()\n#>    adjp.rawp adjp.Bonferroni adjp.Holm adjp.Hochberg adjp.SidakSS adjp.SidakSD\n#> 1       0.12               1         1          0.75         0.72         0.72\n#> 2       0.22               1         1          0.75         0.92         0.89\n#> 3       0.24               1         1          0.75         0.94         0.89\n#> 4       0.29               1         1          0.75         0.97         0.91\n#> 5       0.36               1         1          0.75         0.99         0.93\n#> 6       0.38               1         1          0.75         0.99         0.93\n#> 7       0.44               1         1          0.75         1.00         0.93\n#> 8       0.59               1         1          0.75         1.00         0.93\n#> 9       0.65               1         1          0.75         1.00         0.93\n#> 10      0.75               1         1          0.75         1.00         0.93\n#>    adjp.BH adjp.BY adjp.ABH adjp.TSBH_0.05 index h0.ABH h0.TSBH\n#> 1     0.63       1     0.63           0.63     2     10      10\n#> 2     0.63       1     0.63           0.63     6     10      10\n#> 3     0.63       1     0.63           0.63     8     10      10\n#> 4     0.63       1     0.63           0.63     3     10      10\n#> 5     0.63       1     0.63           0.63    10     10      10\n#> 6     0.63       1     0.63           0.63     1     10      10\n#> 7     0.63       1     0.63           0.63     7     10      10\n#> 8     0.72       1     0.72           0.72     9     10      10\n#> 9     0.72       1     0.72           0.72     5     10      10\n#> 10    0.75       1     0.75           0.75     4     10      10"},{"path":"regression-discontinuity.html","id":"regression-discontinuity","chapter":"24 Regression Discontinuity","heading":"24 Regression Discontinuity","text":"regression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).\nRunning variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .\nregression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).Running variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .Review paper (G. Imbens Lemieux 2008; Lee Lemieux 2010)Review paper (G. Imbens Lemieux 2008; Lee Lemieux 2010)readings:\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf\nreadings:https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf(Thistlethwaite Campbell 1960): first paper use RD context merit awards future academic outcomes.(Thistlethwaite Campbell 1960): first paper use RD context merit awards future academic outcomes.RD localized experiment cutoff point\nHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”\nRD localized experiment cutoff pointHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018); Mathematica). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018); Mathematica). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Tradeoff efficiency biasTradeoff efficiency biasRegression discontinuity framework Instrumental Variable (structural IV) argued (J. D. Angrist Lavy 1999) special case Matching Methods (matching one point) argued (James J. Heckman, LaLonde, Smith 1999).Regression discontinuity framework Instrumental Variable (structural IV) argued (J. D. Angrist Lavy 1999) special case Matching Methods (matching one point) argued (James J. Heckman, LaLonde, Smith 1999).hard part find setting can apply, find one, ’s easy applyThe hard part find setting can apply, find one, ’s easy applyWe can also multiple cutoff lines. However, cutoff line, can one breakup pointWe can also multiple cutoff lines. However, cutoff line, can one breakup pointRD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.RD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better Interrupted Time SeriesAs running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better Interrupted Time SeriesMultiple alternative model specifications produce consistent results reliable (parametric - linear regression polynomials terms, non-parametric - local linear regression). according (Lee Lemieux 2010), one straightforward method ease linearity assumption incorporating polynomial functions forcing variable. choice polynomial terms can determined based data.\n. According (Gelman Imbens 2019), accounting global high-order polynomials presents three issues: (1) imprecise estimates due noise, (2) sensitivity polynomial’s degree, (3) inadequate coverage confidence intervals. address , researchers instead employ estimators rely local linear quadratic polynomials smooth functions.\nMultiple alternative model specifications produce consistent results reliable (parametric - linear regression polynomials terms, non-parametric - local linear regression). according (Lee Lemieux 2010), one straightforward method ease linearity assumption incorporating polynomial functions forcing variable. choice polynomial terms can determined based data.. According (Gelman Imbens 2019), accounting global high-order polynomials presents three issues: (1) imprecise estimates due noise, (2) sensitivity polynomial’s degree, (3) inadequate coverage confidence intervals. address , researchers instead employ estimators rely local linear quadratic polynomials smooth functions.RD viewed description data generating process, rather method approach (similar randomized experiment)RD viewed description data generating process, rather method approach (similar randomized experiment)RD close \nquasi-experimental methods sense ’s based discontinuity threshold\nrandomized experiments sense ’s local randomization.\nRD close toother quasi-experimental methods sense ’s based discontinuity thresholdother quasi-experimental methods sense ’s based discontinuity thresholdrandomized experiments sense ’s local randomization.randomized experiments sense ’s local randomization.several types Regression Discontinuity:Sharp RD: Change treatment probability cutoff point 1\nKink design: Instead discontinuity level running variable, discontinuity slope function (function/level can remain continuous) (Nielsen, Sørensen, Taber 2010). See (Böckerman, Kanninen, Suoniemi 2018) application, (Card et al. 2015) theory.\nSharp RD: Change treatment probability cutoff point 1Kink design: Instead discontinuity level running variable, discontinuity slope function (function/level can remain continuous) (Nielsen, Sørensen, Taber 2010). See (Böckerman, Kanninen, Suoniemi 2018) application, (Card et al. 2015) theory.Kink RDKink RDFuzzy RD: Change treatment probability less 1Fuzzy RD: Change treatment probability less 1Fuzzy Kink RDFuzzy Kink RDRDiT: running variable time.RDiT: running variable time.Others:Multiple cutoffMultiple cutoffMultiple ScoresMultiple ScoresGeographic RDGeographic RDDynamic TreatmentsDynamic TreatmentsContinuous TreatmentsContinuous TreatmentsConsider\\[\nD_i = 1_{X_i > c}\n\\]\\[\nD_i =\n\\begin{cases}\nD_i = 1 \\text{ } X_i > C \\\\\nD_i = 0 \\text{ } X_i < C\n\\end{cases}\n\\]\\(D_i\\) = treatment effect\\(D_i\\) = treatment effect\\(X_i\\) = score variable (continuous)\\(X_i\\) = score variable (continuous)\\(c\\) = cutoff point\\(c\\) = cutoff pointIdentification (Identifying assumptions) RD:Average Treatment Effect cutoff (Continuity-based)\\[\n\\begin{aligned}\n\\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\\\\n&= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\\\\n&= \\lim_{x \\c^+} E[Y_{1i}|X_i = c] - \\lim_{x \\c^=} E[Y_{0i}|X_i = c]\n\\end{aligned}\n\\]Average Treatment Effect neighborhood (Local Randomization-based):\\[\n\\begin{aligned}\n\\alpha_{LR} &= E[Y_{1i} - Y_{0i}|X_i \\W] \\\\\n&= \\frac{1}{N_1} \\sum_{X_i \\W, T_i = 1}Y_i - \\frac{1}{N_0}\\sum_{X_i \\W, T_i =0} Y_i\n\\end{aligned}\n\\]RDD estimates local average treatment effect (LATE), cutoff point individual population levels.Since researchers typically care internal validity, external validity, localness affects external validity.Assumptions:Independent assignmentIndependent assignmentContinuity conditional regression functions\n\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.\nContinuity conditional regression functions\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.RD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableRD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableOnly treatment(s) (e.g., joint distribution multiple treatments) cause discontinuity jump outcome variableOnly treatment(s) (e.g., joint distribution multiple treatments) cause discontinuity jump outcome variableAll factors smooth cutoff (.e., threshold) value. (can also test assumption seeing discontinuity factors). “jump”, bias causal estimateAll factors smooth cutoff (.e., threshold) value. (can also test assumption seeing discontinuity factors). “jump”, bias causal estimateThreats RDVariables (treatment) change discontinuously cutoff\ncan test jumps variables (including pre-treatment outcome)\nVariables (treatment) change discontinuously cutoffWe can test jumps variables (including pre-treatment outcome)Multiple discontinuities assignment variableMultiple discontinuities assignment variableManipulation assignment variable\ncutoff point, check continuity density assignment variable.\nManipulation assignment variableAt cutoff point, check continuity density assignment variable.","code":""},{"path":"regression-discontinuity.html","id":"estimation-and-inference","chapter":"24 Regression Discontinuity","heading":"24.1 Estimation and Inference","text":"","code":""},{"path":"regression-discontinuity.html","id":"local-randomization-based","chapter":"24 Regression Discontinuity","heading":"24.1.1 Local Randomization-based","text":"Additional Assumption: Local Randomization approach assumes inside chosen window \\(W = [c-w, c+w]\\) assigned treatment good random:Joint probability distribution scores units inside chosen window \\(W\\) knownPotential outcomes affected value scoreThis approach stronger Continuity-based assume regressions continuously \\(c\\) unaffected running variable within window \\(W\\)can choose window \\(W\\) (within random assignment plausible), sample size can typically small.choose window \\(W\\), can base eitherwhere pre-treatment covariate-balance observedindependent tests outcome scoredomain knowledgeTo make inference, can either use(Fisher) randomization inference(Fisher) randomization inference(Neyman) design-based(Neyman) design-based","code":""},{"path":"regression-discontinuity.html","id":"continuity-based","chapter":"24 Regression Discontinuity","heading":"24.1.2 Continuity-based","text":"also known local polynomial method\nname suggests, global polynomial regression recommended (lack robustness, -fitting Runge’s phenomenon)\nalso known local polynomial methodas name suggests, global polynomial regression recommended (lack robustness, -fitting Runge’s phenomenon)Step estimate local polynomial regressionChoose polynomial order weighting schemeChoose bandwidth optimal MSE coverage errorEstimate parameter interestExamine robust bias-correct inference","code":""},{"path":"regression-discontinuity.html","id":"specification-checks","chapter":"24 Regression Discontinuity","heading":"24.2 Specification Checks","text":"Balance ChecksSorting/Bunching/ManipulationPlacebo TestsSensitivity Bandwidth Choice","code":""},{"path":"regression-discontinuity.html","id":"balance-checks","chapter":"24 Regression Discontinuity","heading":"24.2.1 Balance Checks","text":"Also known checking Discontinuities Average CovariatesAlso known checking Discontinuities Average CovariatesNull Hypothesis: average effect covariates pseudo outcomes (.e., qualitatively affected treatment) 0.Null Hypothesis: average effect covariates pseudo outcomes (.e., qualitatively affected treatment) 0.hypothesis rejected, better good reason can cast serious doubt RD design.hypothesis rejected, better good reason can cast serious doubt RD design.","code":""},{"path":"regression-discontinuity.html","id":"sortingbunchingmanipulation","chapter":"24 Regression Discontinuity","heading":"24.2.2 Sorting/Bunching/Manipulation","text":"Also known checking Discontinuity Distribution Forcing VariableAlso known checking Discontinuity Distribution Forcing VariableAlso known clustering density testAlso known clustering density testFormal test McCrary sorting test (McCrary 2008) (Cattaneo, Idrobo, Titiunik 2019)Formal test McCrary sorting test (McCrary 2008) (Cattaneo, Idrobo, Titiunik 2019)Since human subjects can manipulate running variable just cutoff (assuming running variable manipulable), especially cutoff point known advance subjects, can result discontinuity distribution running variable cutoff (.e., see “bunching” behavior right cutoff)>\nPeople like sort treatment ’s desirable. density running variable 0 just threshold\nPeople like treatment ’s undesirable\nSince human subjects can manipulate running variable just cutoff (assuming running variable manipulable), especially cutoff point known advance subjects, can result discontinuity distribution running variable cutoff (.e., see “bunching” behavior right cutoff)>People like sort treatment ’s desirable. density running variable 0 just thresholdPeople like sort treatment ’s desirable. density running variable 0 just thresholdPeople like treatment ’s undesirablePeople like treatment ’s undesirable(McCrary 2008) proposes density test (.e., formal test manipulation assignment variable).\n\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\n\\(H_a\\): jump density function point\nEven though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.\n(McCrary 2008) proposes density test (.e., formal test manipulation assignment variable).\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\\(H_a\\): jump density function point\\(H_a\\): jump density function pointEven though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.Even though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.(J. L. Zhang Rubin 2003; Lee 2009; Aronow, Baron, Pinson 2019) offers guide know warrant manipulation(J. L. Zhang Rubin 2003; Lee 2009; Aronow, Baron, Pinson 2019) offers guide know warrant manipulationUsually ’s better know research design inside can suspect manipulation attempts.\nsuspect direction manipulation. typically, ’s one-way manipulation. cases might ways, theoretically cancel .\nUsually ’s better know research design inside can suspect manipulation attempts.suspect direction manipulation. typically, ’s one-way manipulation. cases might ways, theoretically cancel .also observe partial manipulation reality (e.g., subjects can imperfectly manipulate). typically, treat like fuzzy RD, identification problems. complete manipulation lead serious identification issues.also observe partial manipulation reality (e.g., subjects can imperfectly manipulate). typically, treat like fuzzy RD, identification problems. complete manipulation lead serious identification issues.Remember: even cases fail reject null hypothesis density test, rule completely identification problem exists (just like hypotheses)Remember: even cases fail reject null hypothesis density test, rule completely identification problem exists (just like hypotheses)Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Review paper (Kleven 2016)Review paper (Kleven 2016)test can detect manipulation changes distribution running variable. can choose cutoff point 2-sided manipulation, test fail detect .test can detect manipulation changes distribution running variable. can choose cutoff point 2-sided manipulation, test fail detect .Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)can also use bunching method study individuals’ firm’s responsiveness changes policy.can also use bunching method study individuals’ firm’s responsiveness changes policy.RD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.\nBunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).\nfraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.\nRD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)\nRD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)running variable outcome variable simultaneously determined, can use modified RDD estimator consistent estimate. (Bajari et al. 2011)Assumptions:\nManipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1\nManipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)\nAssumptions:Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)Steps:Identify window running variable contains bunching behavior. can step empirically based Bosch, Dekker, Strohmaier (2020). Additionally robustness test needed (.e., varying manipulation window).Estimate manipulation-free counterfactualCalculating standard errors inference can follow (Chetty, Hendren, Katz 2016) bootstrap re-sampling residuals estimation counts individuals within bins (large data can render step unnecessary).pass bunching test, can move Placebo TestMcCrary (2008) testA jump density threshold (.e., discontinuity) hold can serve evidence sorting around cutoff pointCattaneo, Idrobo, Titiunik (2019) test","code":"\nlibrary(rdd)\n\n# you only need the runing variable and the cutoff point\n\n# Example by the package's authors\n#No discontinuity\nx<-runif(1000,-1,1)\nDCdensity(x,0)#> [1] 0.6126802\n\n#Discontinuity\nx<-runif(1000,-1,1)\nx<-x+2*(runif(1000,-1,1)>0&x<0)\nDCdensity(x,0)#> [1] 0.0008519227\nlibrary(rddensity)\n\n# Example by the package's authors\n# Continuous Density\nset.seed(1)\nx <- rnorm(2000, mean = -0.5)\nrdd <- rddensity(X = x, vce = \"jackknife\")\nsummary(rdd)\n#> \n#> Manipulation testing using local polynomial density estimation.\n#> \n#> Number of obs =       2000\n#> Model =               unrestricted\n#> Kernel =              triangular\n#> BW method =           estimated\n#> VCE method =          jackknife\n#> \n#> c = 0                 Left of c           Right of c          \n#> Number of obs         1376                624                 \n#> Eff. Number of obs    354                 345                 \n#> Order est. (p)        2                   2                   \n#> Order bias (q)        3                   3                   \n#> BW est. (h)           0.514               0.609               \n#> \n#> Method                T                   P > |T|             \n#> Robust                -0.6798             0.4966              \n#> \n#> \n#> P-values of binomial tests (H0: p=0.5).\n#> \n#> Window Length / 2          <c     >=c    P>|T|\n#> 0.036                      28      20    0.3123\n#> 0.072                      46      39    0.5154\n#> 0.107                      68      59    0.4779\n#> 0.143                      94      79    0.2871\n#> 0.179                     122     103    0.2301\n#> 0.215                     145     130    0.3986\n#> 0.250                     163     156    0.7370\n#> 0.286                     190     176    0.4969\n#> 0.322                     214     200    0.5229\n#> 0.358                     249     218    0.1650\n\n# you have to specify your own plot (read package manual)"},{"path":"regression-discontinuity.html","id":"placebo-tests","chapter":"24 Regression Discontinuity","heading":"24.2.3 Placebo Tests","text":"Also known Discontinuities Average Outcomes ValuesAlso known Discontinuities Average Outcomes ValuesWe see jumps values (either \\(X_i <c\\) \\(X_i \\ge c\\))\nUse bandwidth use cutoff, move along running variable: testing jump conditional mean outcome median running variable.\nsee jumps values (either \\(X_i <c\\) \\(X_i \\ge c\\))Use bandwidth use cutoff, move along running variable: testing jump conditional mean outcome median running variable.Also known falsification checksAlso known falsification checksBefore cutoff point, can run placebo test see whether X’s different).cutoff point, can run placebo test see whether X’s different).placebo test expect coefficients different 0.placebo test expect coefficients different 0.test can used \nTesting discontinuity predetermined variables:\nTesting discontinuities\nPlacebo outcomes: see changes outcomes shouldn’t changed.\nInclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.\ntest can used forTesting discontinuity predetermined variables:Testing discontinuity predetermined variables:Testing discontinuitiesTesting discontinuitiesPlacebo outcomes: see changes outcomes shouldn’t changed.Placebo outcomes: see changes outcomes shouldn’t changed.Inclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.Inclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.analogous Experimental Design test whether observables similar treatment control groups (reject , don’t random assignment), test unobservables.analogous Experimental Design test whether observables similar treatment control groups (reject , don’t random assignment), test unobservables.Balance observable characteristics sides\\[\nZ_i = \\alpha_0 + \\alpha_1 f(x_i) + [(x_i \\ge c)] \\alpha_2 + [f(x_i) \\times (x_i \\ge c)]\\alpha_3 + u_i\n\\]\\(x_i\\) running variable\\(x_i\\) running variable\\(Z_i\\) characteristics people (e.g., age, etc)\\(Z_i\\) characteristics people (e.g., age, etc)Theoretically, \\(Z_i\\) affected treatment. Hence, \\(E(\\alpha_2) = 0\\)Moreover, multiple \\(Z_i\\), typically simulate joint distribution (avoid significant coefficient based chance).way don’t need generate joint distribution \\(Z_i\\)’s independent (unlikely reality).RD, shouldn’t Matching Methods. just like random assignment, need make balanced dataset cutoff. balancing, RD assumptions probably wrong first place.","code":""},{"path":"regression-discontinuity.html","id":"sensitivity-to-bandwidth-choice","chapter":"24 Regression Discontinuity","heading":"24.2.4 Sensitivity to Bandwidth Choice","text":"Methods bandwidth selection\nAd-hoc substantively driven\nData driven: cross validation\nConservative approach: (Calonico, Cattaneo, Farrell 2020)\nMethods bandwidth selectionAd-hoc substantively drivenAd-hoc substantively drivenData driven: cross validationData driven: cross validationConservative approach: (Calonico, Cattaneo, Farrell 2020)Conservative approach: (Calonico, Cattaneo, Farrell 2020)objective minimize mean squared error estimated actual treatment effects.objective minimize mean squared error estimated actual treatment effects., need see sensitive results dependent choice bandwidth., need see sensitive results dependent choice bandwidth.cases, best bandwidth testing covariates may best bandwidth treating , may close.cases, best bandwidth testing covariates may best bandwidth treating , may close.","code":"\n# find optimal bandwidth by Imbens-Kalyanaraman\nrdd::IKbandwidth(running_var,\n                 outcome_var,\n                 cutpoint = \"\",\n                 kernel = \"triangular\") # can also pick other kernels"},{"path":"regression-discontinuity.html","id":"manipulation-robust-regression-discontinuity-bounds","chapter":"24 Regression Discontinuity","heading":"24.2.5 Manipulation Robust Regression Discontinuity Bounds","text":"McCrary (2008) linked density jumps cutoffs RD studies potential manipulation.\njump detected, researchers proceed RD analysis; detected, halt using cutoff inference.\nstudies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.\nFalse negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.\nEven correct rejections manipulation may overlook data can still informative despite modest manipulation.\nGerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.\n\nMcCrary (2008) linked density jumps cutoffs RD studies potential manipulation.jump detected, researchers proceed RD analysis; detected, halt using cutoff inference.jump detected, researchers proceed RD analysis; detected, halt using cutoff inference.studies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.\nFalse negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.\nEven correct rejections manipulation may overlook data can still informative despite modest manipulation.\nGerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.\nstudies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.False negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.False negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.Even correct rejections manipulation may overlook data can still informative despite modest manipulation.Even correct rejections manipulation may overlook data can still informative despite modest manipulation.Gerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.Gerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.model introduces two types unobservable units RD designs:\nalways-assigned units, always one side cutoff,\npotentially-assigned units, fit traditional RD assumptions.\nstandard RD model subset broader model, assumes always-assigned units.\n\nmodel introduces two types unobservable units RD designs:always-assigned units, always one side cutoff,always-assigned units, always one side cutoff,potentially-assigned units, fit traditional RD assumptions.\nstandard RD model subset broader model, assumes always-assigned units.\npotentially-assigned units, fit traditional RD assumptions.standard RD model subset broader model, assumes always-assigned units.Identifying assumption: manipulation occurs one-sided selection.Identifying assumption: manipulation occurs one-sided selection.approach make binary decision manipulation RD designs assesses extent worst-case impact.approach make binary decision manipulation RD designs assesses extent worst-case impact.Two steps used:Determining proportion always-assigned units using discontinuity cutoffBounding treatment effects based extreme feasible outcomes units.sharp RD designs, bounds established trimming extreme outcomes near cutoff; fuzzy designs, process involves complex adjustments due additional model constraints.sharp RD designs, bounds established trimming extreme outcomes near cutoff; fuzzy designs, process involves complex adjustments due additional model constraints.Extensions study use covariate information economic behavior assumptions refine bounds identify covariate distributions among unit types cutoff.Extensions study use covariate information economic behavior assumptions refine bounds identify covariate distributions among unit types cutoff.SetupIndependent data points \\((X_i, Y_i, D_i)\\), \\(X_i\\) running variable, \\(Y_i\\) outcome, \\(D_i\\) indicates treatment status (1 treated, 0 otherwise). Treatment assigned based \\(X_i \\geq c\\).design sharp \\(D_i = (X_i \\geq c)\\) fuzzy otherwise.population divided :Potentially-assigned units (\\(M_i = 0\\)): Follow standard RD framework, potential outcomes \\(Y_i(d)\\) potential treatment states \\(D_i(x)\\).Potentially-assigned units (\\(M_i = 0\\)): Follow standard RD framework, potential outcomes \\(Y_i(d)\\) potential treatment states \\(D_i(x)\\).Always-assigned units (\\(M_i = 1\\)): units require potential outcomes states, always \\(X_i\\) values beyond cutoff.Always-assigned units (\\(M_i = 1\\)): units require potential outcomes states, always \\(X_i\\) values beyond cutoff.AssumptionsLocal Independence Continuity:\n\\(P(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0)\\)\ndefiers: \\(P(D^+ \\geq D^-|X = c, M = 0) = 1\\)\nContinuity potential outcomes states \\(c\\).\n\\(F_{X|M=0}(x)\\) differentiable \\(c\\), positive derivative.\n\\(P(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0)\\)defiers: \\(P(D^+ \\geq D^-|X = c, M = 0) = 1\\)Continuity potential outcomes states \\(c\\).\\(F_{X|M=0}(x)\\) differentiable \\(c\\), positive derivative.Smoothness Running Variable among Potentially-Assigned Units:\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).Restrictions Always-Assigned Units:\n\\(P(X \\geq c|M = 1) = 1\\) \\(F_{X|M=1}(x)\\) right-differentiable (left-differentiable) \\(c\\).\n(local) one-sided manipulation assumption allows identification proportion always-assigned units among units close cutoff.\n\\(P(X \\geq c|M = 1) = 1\\) \\(F_{X|M=1}(x)\\) right-differentiable (left-differentiable) \\(c\\).(local) one-sided manipulation assumption allows identification proportion always-assigned units among units close cutoff.always-assigned unit exist, RD design fuzzy haveTreated untreated units among potentially-assigned (cutoff)Always-assigned units (cutoff).Causal Effects Interestcausal effects among potentially-assigned units:\\[\n\\Gamma = E[Y(1) - Y(0) | X = c, D^+ > D^-, M = 0]\n\\]parameter represents local average treatment effect (LATE) subgroup “compliers”—units receive treatment running variable \\(X_i\\) exceeds certain cutoff.parameter \\(\\Gamma\\) captures causal effect changes cutoff level treatment status among potentially-assigned compliers.Focuses actual observations cutoff, hypothetical true values.Provides direct observable estimate causal effects, without reliance hypothetical constructs.Exclude observations around cutoff use extrapolation trends outside excluded range infer causal effects cutoffAssumes hypothetical population existing counterfactual scenario without manipulation.Requires strong assumptions nature manipulation minimal impact extrapolation biases.Identification \\(\\tau\\) RD DesignsIdentification challenges arise due inability distinguish always-assigned potentially-assigned units, thus Γ point identified. establish sharp bounds ΓIdentification challenges arise due inability distinguish always-assigned potentially-assigned units, thus Γ point identified. establish sharp bounds ΓThese bounds supported stochastic dominance potential outcome CDFs observed distributions.bounds supported stochastic dominance potential outcome CDFs observed distributions.Unit Types Notation:\\(C_0\\): Potentially-assigned compliers.\\(A_0\\): Potentially-assigned always-takers.\\(N_0\\): Potentially-assigned never-takers.\\(T_1\\): Always-assigned treated units.\\(U_1\\): Always-assigned untreated units.measure \\(\\tau\\) , representing proportion always-assigned units near cutoff, point identified discontinuity observed running variable density \\(f_X\\) cutoffSharp RD:Units left cutoff potentially assigned units. distribution observed outcomes (\\(Y\\)) outcomes \\(Y(0)\\) potentially-assigned compliers (\\(C_0\\)) cutoff.Units left cutoff potentially assigned units. distribution observed outcomes (\\(Y\\)) outcomes \\(Y(0)\\) potentially-assigned compliers (\\(C_0\\)) cutoff.determine bounds treatment effect (\\(\\Gamma\\)), need assess distribution treated outcomes (\\(Y(1)\\)) potentially-assigned compliers cutoff.determine bounds treatment effect (\\(\\Gamma\\)), need assess distribution treated outcomes (\\(Y(1)\\)) potentially-assigned compliers cutoff.Information regarding treated outcomes (\\(Y(1)\\)) comes exclusively subpopulation treated units, includes potentially-assigned compliers (\\(C_0\\)) always assigned units (\\(T_1\\)).Information regarding treated outcomes (\\(Y(1)\\)) comes exclusively subpopulation treated units, includes potentially-assigned compliers (\\(C_0\\)) always assigned units (\\(T_1\\)).\\(\\tau\\) point identified, can estimate sharp bounds \\(\\Gamma\\).\\(\\tau\\) point identified, can estimate sharp bounds \\(\\Gamma\\).Fuzzy RD:Note: Table page 848 (Gerard, Rokkanen, Rothe 2020)Unit Types Combinations: five distinct unit types four combinations treatment assignments decisions relevant analysis. distinctions important affect potential outcomes analyzed bounded.Unit Types Combinations: five distinct unit types four combinations treatment assignments decisions relevant analysis. distinctions important affect potential outcomes analyzed bounded.Outcome Distributions: analysis involves estimating distribution potential outcomes (treated untreated) among potentially-assigned compliers cutoff.Outcome Distributions: analysis involves estimating distribution potential outcomes (treated untreated) among potentially-assigned compliers cutoff.Three-Step Process:\nPotential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.\nPotential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.\nBounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.\nThree-Step Process:Potential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.Potential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.Potential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.Potential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.Bounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.Bounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.Extreme Value Consideration: bounds treatment effects based “extreme” scenarios worst-case assumptions distribution potential outcomes, making sharp empirically relevant within data constraints.Extreme Value Consideration: bounds treatment effects based “extreme” scenarios worst-case assumptions distribution potential outcomes, making sharp empirically relevant within data constraints.Extensions:Quantile Treatment Effects: alternative average effects focusing different quantiles outcome distribution, less affected extreme values.Quantile Treatment Effects: alternative average effects focusing different quantiles outcome distribution, less affected extreme values.Applicability Discrete OutcomesApplicability Discrete OutcomesBehavioral Assumptions Impact: Assuming high likelihood treatment among always-assigned units can narrow bounds treatment effects refining analysis potential outcomes.Behavioral Assumptions Impact: Assuming high likelihood treatment among always-assigned units can narrow bounds treatment effects refining analysis potential outcomes.Utilization Covariates: Incorporating covariates measured prior treatment can refine bounds treatment effects help target policies identifying covariate distributions among different unit types.Utilization Covariates: Incorporating covariates measured prior treatment can refine bounds treatment effects help target policies identifying covariate distributions among different unit types.Notes:Quantile Treatment Effects (QTEs): QTE bounds less sensitive tails outcome distribution, making tighter ATE bounds.\nInference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.\nInference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.\nQuantile Treatment Effects (QTEs): QTE bounds less sensitive tails outcome distribution, making tighter ATE bounds.Inference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.Inference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.Inference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.Inference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.Alternative Inference Strategy manipulation believed unlikely. Try different hypothetical values \\(\\tau\\)Alternative Inference Strategy manipulation believed unlikely. Try different hypothetical values \\(\\tau\\)","code":"\ndevtools::install_github(\"francoisgerard/rdbounds/R\")\nlibrary(formattable)\nlibrary(data.table)\nlibrary(rdbounds)\nset.seed(123)\ndf <- rdbounds_sampledata(10000, covs = FALSE)\n#> [1] \"True tau: 0.117999815082062\"\n#> [1] \"True treatment effect on potentially-assigned: 2\"\n#> [1] \"True treatment effect on right side of cutoff: 2.35399944524618\"\nhead(df)\n#>            x        y treatment\n#> 1 -1.2532616 3.489563         0\n#> 2 -0.5146925 3.365232         0\n#> 3  3.4853777 6.193533         0\n#> 4  0.1576616 8.820440         1\n#> 5  0.2890962 4.791972         0\n#> 6  3.8350019 7.316907         0\n\nrdbounds_est <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        \n        # for median effect use \n        # type = \"qte\", \n        # percentiles = .5, \n        \n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209\"\n#> [1] \"2024-05-13 19:12:33 Estimating CDFs for point estimates\"\n#> [1] \"2024-05-13 19:12:33 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:35 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2024-05-13 19:12:35 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:38 Beginning parallelized output by bootstrap..\"\n#> [1] \"2024-05-13 19:12:42 Computing Confidence Intervals\"\n#> [1] \"2024-05-13 19:12:51 Time taken:0.3 minutes\"\nrdbounds_summary(rdbounds_est, title_prefix = \"Sample Data Results\")\n#> [1] \"Time taken: 0.3 minutes\"\n#> [1] \"Sample size: 10000\"\n#> [1] \"Local Average Treatment Effect:\"\n#> $tau_hat\n#> [1] 0.04209028\n#> \n#> $tau_hat_CI\n#> [1] 0.1671043 0.7765031\n#> \n#> $takeup_increase\n#> [1] 0.7521208\n#> \n#> $takeup_increase_CI\n#> [1] 0.7065353 0.7977063\n#> \n#> $TE_SRD_naive\n#> [1] 1.770963\n#> \n#> $TE_SRD_naive_CI\n#> [1] 1.541314 2.000612\n#> \n#> $TE_SRD_bounds\n#> [1] 1.569194 1.912681\n#> \n#> $TE_SRD_CI\n#> [1] -0.1188634  3.5319468\n#> \n#> $TE_SRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_SRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_FRD_naive\n#> [1] 2.356601\n#> \n#> $TE_FRD_naive_CI\n#> [1] 1.995430 2.717772\n#> \n#> $TE_FRD_bounds\n#> [1] 1.980883 2.362344\n#> \n#> $TE_FRD_CI\n#> [1] -0.6950823  4.6112538\n#> \n#> $TE_FRD_bounds_refinementA\n#> [1] 1.980883 2.357499\n#> \n#> $TE_FRD_refinementA_CI\n#> [1] -0.6950823  4.6112538\n#> \n#> $TE_FRD_bounds_refinementB\n#> [1] 1.980883 2.351411\n#> \n#> $TE_FRD_refinementB_CI\n#> [1] -0.6152215  4.2390830\n#> \n#> $TE_FRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_FRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_SRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_FRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_SRD_right_bounds\n#> [1] 1.376392 2.007746\n#> \n#> $TE_SRD_right_CI\n#> [1] -5.036752  5.889137\n#> \n#> $TE_FRD_right_bounds\n#> [1] 1.721121 2.511504\n#> \n#> $TE_FRD_right_CI\n#> [1] -6.663269  7.414185\nrdbounds_est_tau <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        potential_taus = c(.025, .05, .1, .2),\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209\"\n#> [1] \"2024-05-13 19:12:52 Estimating CDFs for point estimates\"\n#> [1] \"2024-05-13 19:12:52 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:53 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2024-05-13 19:12:53 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:56 Beginning parallelized output by bootstrap..\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.025\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.05\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.1\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.2\"\n#> [1] \"2024-05-13 19:13:03 Beginning parallelized output by bootstrap x fixed tau..\"\n#> [1] \"2024-05-13 19:13:09 Computing Confidence Intervals\"\n#> [1] \"2024-05-13 19:13:19 Time taken:0.46 minutes\"\ncausalverse::plot_rd_aa_share(rdbounds_est_tau) # For SRD (default)\n# causalverse::plot_rd_aa_share(rdbounds_est_tau, rd_type = \"FRD\")  # For FRD"},{"path":"regression-discontinuity.html","id":"fuzzy-rd-design","chapter":"24 Regression Discontinuity","heading":"24.3 Fuzzy RD Design","text":"cutoff perfectly determine treatment, creates discontinuity likelihood receiving treatment, need another instrumentFor close cutoff, create instrument \\(D_i\\)\\[\nZ_i=\n\\begin{cases}\n1 & \\text{} X_i \\ge c \\\\\n0 & \\text{} X_c < c\n\\end{cases}\n\\], can estimate effect treatment compliers (.e., treatment \\(D_i\\) depends \\(Z_i\\))LATE parameter\\[\n\\lim_{c - \\epsilon \\le X \\le c + \\epsilon, \\epsilon \\0}( \\frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)})\n\\]equivalently, canonical parameter:\\[\n\\frac{lim_{x \\downarrow c}E(Y|X = x) - \\lim_{x \\uparrow c} E(Y|X = x)}{\\lim_{x \\downarrow c } E(D |X = x) - \\lim_{x \\uparrow c}E(D |X=x)}\n\\]Two equivalent ways estimateFirst\nSharp RDD \\(Y\\)\nSharp RDD \\(D\\)\nTake estimate step 1 divide step 2\nFirstSharp RDD \\(Y\\)Sharp RDD \\(Y\\)Sharp RDD \\(D\\)Sharp RDD \\(D\\)Take estimate step 1 divide step 2Take estimate step 1 divide step 2Second: Subset observations close \\(c\\) run instrumental variable \\(Z\\)Second: Subset observations close \\(c\\) run instrumental variable \\(Z\\)","code":""},{"path":"regression-discontinuity.html","id":"regression-kink-design","chapter":"24 Regression Discontinuity","heading":"24.4 Regression Kink Design","text":"slope treatment intensity changes cutoff (instead level treatment assignment), can regression kink designIf slope treatment intensity changes cutoff (instead level treatment assignment), can regression kink designExample: unemployment benefitsExample: unemployment benefitsSharp Kink RD parameter\\[\n\\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}b(x) - \\lim_{x \\uparrow c} \\frac{d}{dx}b(x)}\n\\]\\(b(x)\\) known function inducing “kink”Fuzzy Kink RD parameter\\[\n\\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}E[D_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[D_i |X_i = x]}\n\\]","code":""},{"path":"regression-discontinuity.html","id":"multi-cutoff","chapter":"24 Regression Discontinuity","heading":"24.5 Multi-cutoff","text":"\\[\n\\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c]\n\\]","code":""},{"path":"regression-discontinuity.html","id":"multi-score","chapter":"24 Regression Discontinuity","heading":"24.6 Multi-score","text":"Multi-score (multiple dimensions) (e.g., math English cutoff certain honor class):\\[\n\\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x]\n\\]","code":""},{"path":"regression-discontinuity.html","id":"steps-for-sharp-rd","chapter":"24 Regression Discontinuity","heading":"24.7 Steps for Sharp RD","text":"Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Run regression sides cutoff get treatment effectRun regression sides cutoff get treatment effectRobustness checks:\nAssess possible jumps variables around cutoff\nHypothesis testing bunching\nPlacebo tests\nVarying bandwidth\nRobustness checks:Assess possible jumps variables around cutoffAssess possible jumps variables around cutoffHypothesis testing bunchingHypothesis testing bunchingPlacebo testsPlacebo testsVarying bandwidthVarying bandwidth","code":""},{"path":"regression-discontinuity.html","id":"steps-for-fuzzy-rd","chapter":"24 Regression Discontinuity","heading":"24.8 Steps for Fuzzy RD","text":"Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph probability treatmentGraph probability treatmentEstimate treatment effect using 2SLSEstimate treatment effect using 2SLSRobustness checks:\nAssess possible jumps variables around cutoff\nHypothesis testing bunching\nPlacebo tests\nVarying bandwidth\nRobustness checks:Assess possible jumps variables around cutoffAssess possible jumps variables around cutoffHypothesis testing bunchingHypothesis testing bunchingPlacebo testsPlacebo testsVarying bandwidthVarying bandwidth","code":""},{"path":"regression-discontinuity.html","id":"steps-for-rdit-regression-discontinuity-in-time","chapter":"24 Regression Discontinuity","heading":"24.9 Steps for RDiT (Regression Discontinuity in Time)","text":"Notes:Additional assumption: Time-varying confounders change smoothly across cutoff dateTypically used policy implementation date subjects, can also used cases implementation dates different subjects. second case, researchers typically use different RDiT specification time series.Sometimes date implementation randomly assigned chosen strategically. Hence, RDiT thought “discontinuity threshold” interpretation RD (“local randomization”). (C. Hausman Rapson 2018, 8)Normal RD uses variation \\(N\\) dimension, RDiT uses variation \\(T\\) dimensionChoose polynomials based BIC typically. can either global polynomial pre-period post-period polynomial time series (usually global one perform better)use augmented local linear outlined (C. Hausman Rapson 2018, 12), estimate model control first take residuals include model RDiT treatment (remember use bootstrapping method account first-stage variance second stage).Pros:can overcome cases cross-sectional variation treatment implementation (feasible)\npapers use RDiT (1) see differential treatment effects across individuals/ space (Auffhammer Kellogg 2011) (2) compare 2 estimates control group’s validity questionable (Gallego, Montero, Salas 2013).\ncan overcome cases cross-sectional variation treatment implementation (feasible)papers use RDiT (1) see differential treatment effects across individuals/ space (Auffhammer Kellogg 2011) (2) compare 2 estimates control group’s validity questionable (Gallego, Montero, Salas 2013).Better pre/post comparison can include flexible controlsBetter pre/post comparison can include flexible controlsBetter event studies can use long-time horizons (may relevant now since development long-time horizon event studies), can use higher-order polynomials time control variables.Better event studies can use long-time horizons (may relevant now since development long-time horizon event studies), can use higher-order polynomials time control variables.Cons:Taking observation threshold (time) can bias estimates unobservables time-series properties data generating process.Taking observation threshold (time) can bias estimates unobservables time-series properties data generating process.(McCrary 2008) test possible (see Sorting/Bunching/Manipulation) density running (time) uniform, can’t use test.(McCrary 2008) test possible (see Sorting/Bunching/Manipulation) density running (time) uniform, can’t use test.Time-varying unobservables may impact dependent variable discontinuouslyTime-varying unobservables may impact dependent variable discontinuouslyError terms likely include persistence (serially correlated errors)Error terms likely include persistence (serially correlated errors)Researchers model time-varying treatment RDiT\nsmall enough window, local linear specification fine, global polynomials can either big small (C. Hausman Rapson 2018)\nResearchers model time-varying treatment RDiTIn small enough window, local linear specification fine, global polynomials can either big small (C. Hausman Rapson 2018)BiasesTime-Varying treatment Effects\nincrease sample size either \ngranular data (greater frequency): increase power problem serial correlation\nincreasing time window: increases bias confounders\n\n2 additional assumption:\nModel correctly specified (confoudners global polynomial approximation)\nTreatment effect correctly specified (whether ’s smooth constant, varies)\n2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)\n\nusually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)\nTime-Varying treatment Effectsincrease sample size either \ngranular data (greater frequency): increase power problem serial correlation\nincreasing time window: increases bias confounders\nincrease sample size either bymore granular data (greater frequency): increase power problem serial correlationmore granular data (greater frequency): increase power problem serial correlationincreasing time window: increases bias confoundersincreasing time window: increases bias confounders2 additional assumption:\nModel correctly specified (confoudners global polynomial approximation)\nTreatment effect correctly specified (whether ’s smooth constant, varies)\n2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)\n2 additional assumption:Model correctly specified (confoudners global polynomial approximation)Model correctly specified (confoudners global polynomial approximation)Treatment effect correctly specified (whether ’s smooth constant, varies)Treatment effect correctly specified (whether ’s smooth constant, varies)2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)usually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)usually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)Autoregression (serial dependence)\nNeed use clustered standard errors account serial dependence residuals\ncase serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)\ncase serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)\nAutoregression (serial dependence)Need use clustered standard errors account serial dependence residualsNeed use clustered standard errors account serial dependence residualsIn case serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)case serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)case serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)case serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)Sorting Anticipation Effects\nrun (McCrary 2008) density time running variable uniform\nCan still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)\nHence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behavior\nSorting Anticipation EffectsCannot run (McCrary 2008) density time running variable uniformCannot run (McCrary 2008) density time running variable uniformCan still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)Can still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)Hence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behaviorHence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behaviorRecommendations robustness check following (C. Hausman Rapson 2018, 549)Plot raw data residuals (removing confounders trend). varying polynomial local linear controls, inconsistent results can sign time-varying treatment effects.Using global polynomial, overfit, show polynomial different order alternative local linear bandwidths. results consistent, ’re okayPlacebo Tests: estimate another RD (1) another location subject (receive treatment) (2) use another date.Plot RD discontinuity continuous controlsDonut RD see avoiding selection close cutoff yield better results (Barreca et al. 2011)Test auto-regression (using pre-treatment data). evidence autoregression, include lagged dependent variableAugmented local linear (need use global polynomial avoid -fitting)\nUse full sample exclude effect important predictors\nEstimate conditioned second stage smaller sample bandwidth\nUse full sample exclude effect important predictorsUse full sample exclude effect important predictorsEstimate conditioned second stage smaller sample bandwidthEstimate conditioned second stage smaller sample bandwidthExamples (C. Hausman Rapson 2018, 534) inecon(Davis 2008): Air quality(Davis 2008): Air quality(Auffhammer Kellogg 2011): Air quality(Auffhammer Kellogg 2011): Air quality(H. Chen et al. 2018): Air quality(H. Chen et al. 2018): Air quality(De Paola, Scoppa, Falcone 2013): car accidents(De Paola, Scoppa, Falcone 2013): car accidents(Gallego, Montero, Salas 2013): air quality(Gallego, Montero, Salas 2013): air quality(Bento et al. 2014): Traffic(Bento et al. 2014): Traffic(M. L. Anderson 2014): Traffic(M. L. Anderson 2014): Traffic(Burger, Kaffine, Yu 2014): Car accidents(Burger, Kaffine, Yu 2014): Car accidents(Brodeur et al. 2021): Covid19 lock-downs well-(Brodeur et al. 2021): Covid19 lock-downs well-beingmarketingM. R. Busse et al. (2013): Vehicle pricesM. R. Busse et al. (2013): Vehicle prices(X. Chen et al. 2009): Customer Satisfaction(X. Chen et al. 2009): Customer Satisfaction(M. R. Busse, Simester, Zettelmeyer 2010): Vehicle prices(M. R. Busse, Simester, Zettelmeyer 2010): Vehicle prices(Davis Kahn 2010): vehicle prices(Davis Kahn 2010): vehicle prices","code":""},{"path":"regression-discontinuity.html","id":"evaluation-of-an-rd","chapter":"24 Regression Discontinuity","heading":"24.10 Evaluation of an RD","text":"Evidence (either formal tests graphs)\nTreatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .\nmanipulation assignment variable.\nEvidence (either formal tests graphs)Treatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .Treatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .manipulation assignment variable.manipulation assignment variable.Results robust various functional forms forcing variableResults robust various functional forms forcing variableIs (unobserved) confound cause discontinuous change cutoff (.e., multiple forcing variables / bundling institutions)?(unobserved) confound cause discontinuous change cutoff (.e., multiple forcing variables / bundling institutions)?External Validity: likely result cutoff generalize?External Validity: likely result cutoff generalize?General Model\\[\nY_i = \\beta_0 + f(x_i) \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]\\(f(x_i)\\) functional form \\(x_i\\)Simple caseWhen \\(f(x_i) = x_i\\) (linear function)\\[\nY_i = \\beta_0 + x_i \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]RD gives \\(\\beta_2\\) (causal effect) \\(X\\) \\(Y\\) cutoff pointIn practice, everyone \\[\nY_i = \\alpha_0 + f(x) \\alpha _1 + [(x_i \\ge c)]\\alpha_2 + [f(x_i)\\times [(x_i \\ge c)]\\alpha_3 + u_i\n\\]estimate different slope different sides lineand estimate \\(\\alpha_3\\) different 0 return simple caseNotes:Sparse data can make \\(\\alpha_3\\) large differential effectSparse data can make \\(\\alpha_3\\) large differential effectPeople skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) good. However, still insist, non-parametric estimation can best bet.People skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) good. However, still insist, non-parametric estimation can best bet.Bandwidth \\(c\\) (window)Closer \\(c\\) can give lower bias, also efficiencyCloser \\(c\\) can give lower bias, also efficiencyWider \\(c\\) can increase bias, higher efficiency.Wider \\(c\\) can increase bias, higher efficiency.Optimal bandwidth controversial, usually appendix research article anyway.Optimal bandwidth controversial, usually appendix research article anyway.can either\ndrop observations outside bandwidth \nweight depends far close \\(c\\)\ncan eitherdrop observations outside bandwidth ordrop observations outside bandwidth orweight depends far close \\(c\\)weight depends far close \\(c\\)","code":""},{"path":"regression-discontinuity.html","id":"applications","chapter":"24 Regression Discontinuity","heading":"24.11 Applications","text":"Examples marketing:(Narayanan Kalyanam 2015)(Narayanan Kalyanam 2015)(Hartmann, Nair, Narayanan 2011): nonparametric estimation guide identifying causal marketing mix effects(Hartmann, Nair, Narayanan 2011): nonparametric estimation guide identifying causal marketing mix effectsPackages R (see (Thoemmes, Liao, Jin 2017) detailed comparisons): can handle sharp fuzzy RDrddrddrdrobust estimation, inference plotrdrobust estimation, inference plotrddensity discontinuity density tests (Sorting/Bunching/Manipulation) using local polynomials binomial testrddensity discontinuity density tests (Sorting/Bunching/Manipulation) using local polynomials binomial testrdlocrand covariate balance, binomial tests, window selectionrdlocrand covariate balance, binomial tests, window selectionrdmulti multiple cutoffs multiple scoresrdmulti multiple cutoffs multiple scoresrdpower power, sample selectionrdpower power, sample selectionrddtoolsrddtools(Calonico, Cattaneo, Farrell 2020)(G. Imbens Kalyanaraman 2012)(Calonico, Cattaneo, Titiunik 2014)Kernel functionsTriangularTriangularRectangularRectangularEpanechnikovGaussianIncludeResidualsMcCrary sortingEquality covariates distribution meanbased table 1 (Thoemmes, Liao, Jin 2017) (p. 347)","code":""},{"path":"regression-discontinuity.html","id":"example-1-1","chapter":"24 Regression Discontinuity","heading":"24.11.1 Example 1","text":"Example Leihua Ye\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i\n\\]\\[\nX_i =\n\\begin{cases}\n1, W_i \\ge c \\\\\n0, W_i < c\n\\end{cases}\n\\]","code":"\n#cutoff point = 3.5\nGPA <- runif(1000, 0, 4)\nfuture_success <- 10 + 2 * GPA + 10 * (GPA >= 3.5) + rnorm(1000)\n#install and load the package ‘rddtools’\n#install.packages(“rddtools”)\nlibrary(rddtools)\ndata <- rdd_data(future_success, GPA, cutpoint = 3.5)\n# plot the dataset\nplot(\n    data,\n    col =  \"red\",\n    cex = 0.1,\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)\n# estimate the sharp RDD model\nrdd_mod <- rdd_reg_lm(rdd_object = data, slope =  \"same\")\nsummary(rdd_mod)\n#> \n#> Call:\n#> lm(formula = y ~ ., data = dat_step1, weights = weights)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.90364 -0.70348  0.00278  0.66828  3.00603 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 16.90704    0.06637  254.75   <2e-16 ***\n#> D           10.09058    0.11063   91.21   <2e-16 ***\n#> x            1.97078    0.03281   60.06   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9908 on 997 degrees of freedom\n#> Multiple R-squared:  0.9654, Adjusted R-squared:  0.9654 \n#> F-statistic: 1.392e+04 on 2 and 997 DF,  p-value: < 2.2e-16\n# plot the RDD model along with binned observations\nplot(\n    rdd_mod,\n    cex = 0.1,\n    col =  \"red\",\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)"},{"path":"regression-discontinuity.html","id":"example-2","chapter":"24 Regression Discontinuity","heading":"24.11.2 Example 2","text":"Bowblis Smith (2021)Occupational licensing can either increase decrease market efficiency:information means efficiencyMore information means efficiencyIncreased entry barriers (.e., friction) increase efficiencyIncreased entry barriers (.e., friction) increase efficiencyComponents RDRunning variableCutoff: 120 beds aboveTreatment: treatment cutoff point.OLS\\[\nY_i = \\alpha_0 + X_i \\alpha_1 + LW_i \\alpha_2 + \\epsilon_i\n\\]\\(LW_i\\) Licensed/certified workers (fraction format center).\\(LW_i\\) Licensed/certified workers (fraction format center).\\(Y_i\\) = Quality service\\(Y_i\\) = Quality serviceBias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Preference-based: places higher quality staff want keep high quality staffs.Preference-based: places higher quality staff want keep high quality staffs.RD\\[\n\\begin{aligned}\nY_{ist} &= \\beta_0 + [(Bed \\ge121)_{ist}]\\beta_1 + f(Size_{ist}) \\beta_2\\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\beta_3 \\\\\n&+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\end{aligned}\n\\]\\(s\\) = state\\(s\\) = state\\(t\\) = year\\(t\\) = year\\(\\) = hospital\\(\\) = hospitalThis RD fuzzyIf right near threshold (bandwidth), states different sorting (.e., non-random), need fixed-effect state \\(s\\). RD assumption wrong anyway, won’t first placeIf right near threshold (bandwidth), states different sorting (.e., non-random), need fixed-effect state \\(s\\). RD assumption wrong anyway, won’t first placeTechnically, also run fixed-effect regression, ’s lower causal inference hierarchy. Hence, don’t .Technically, also run fixed-effect regression, ’s lower causal inference hierarchy. Hence, don’t .Moreover, RD framework, don’t include \\(t\\) treatment (FE include )Moreover, RD framework, don’t include \\(t\\) treatment (FE include )include \\(\\pi_i\\) hospital, don’t variation causal estimates (hardly hospital changes bed size panel)include \\(\\pi_i\\) hospital, don’t variation causal estimates (hardly hospital changes bed size panel)\\(\\beta_1\\) intent treat (treatment effect coincide intent treat)\\(\\beta_1\\) intent treat (treatment effect coincide intent treat)take fuzzy cases , introduce selection bias.take fuzzy cases , introduce selection bias.Note drop cases based behavioral choice (exclude non-compliers), can drop particular behaviors ((e.g., people like round numbers).Note drop cases based behavioral choice (exclude non-compliers), can drop particular behaviors ((e.g., people like round numbers).Thus, use Instrument variable 33.1.3.1Stage 1:\\[\n\\begin{aligned}\nQSW_{ist} &= \\alpha_0 + [(Bed \\ge121)_{ist}]\\alpha_1 + f(Size_{ist}) \\alpha_2\\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\alpha_3 \\\\\n&+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\end{aligned}\n\\](Note: different fixed effects error term - \\(\\delta, \\gamma_s, \\theta_t, \\epsilon_{ist}\\) first equation, ran Greek letters)Stage 2:\\[\n\\begin{aligned}\nY_{ist} &= \\gamma_0 + \\gamma_1 \\hat{QWS}_{ist} + f(Size_{ist}) \\delta_2 \\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)] \\delta_3 \\\\\n&+ X_{} \\lambda + \\eta_s + \\tau_t + u_{ist}\n\\end{aligned}\n\\]bigger jump (discontinuity), similar 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) \\(\\gamma_1\\) average treatment effect (exposing policy)bigger jump (discontinuity), similar 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) \\(\\gamma_1\\) average treatment effect (exposing policy)\\(\\beta_1\\) always closer 0 \\(\\gamma_1\\)\\(\\beta_1\\) always closer 0 \\(\\gamma_1\\)Figure 1 shows bunching every 5 units cutoff, 120 still .Figure 1 shows bunching every 5 units cutoff, 120 still .manipulable bunching, decrease 130If manipulable bunching, decrease 130Since limited number mass points (round numbers), clustered standard errors mass pointSince limited number mass points (round numbers), clustered standard errors mass point","code":""},{"path":"regression-discontinuity.html","id":"example-3","chapter":"24 Regression Discontinuity","heading":"24.11.3 Example 3","text":"Replication (Carpenter Dobkin 2009) Philipp Leppert, dataset ","code":""},{"path":"regression-discontinuity.html","id":"example-4","chapter":"24 Regression Discontinuity","heading":"24.11.4 Example 4","text":"detailed application, see (Thoemmes, Liao, Jin 2017) use rdd, rdrobust, rddtools","code":""},{"path":"synthetic-difference-in-differences.html","id":"synthetic-difference-in-differences","chapter":"25 Synthetic Difference-in-Differences","heading":"25 Synthetic Difference-in-Differences","text":"(Arkhangelsky et al. 2021)also known weighted double-differencing estimatorsSetting: Researchers use panel data study effects policy changes.\nPanel data: repeated observations across time various units.\nunits exposed policy different times others.\nSetting: Researchers use panel data study effects policy changes.Panel data: repeated observations across time various units.Panel data: repeated observations across time various units.units exposed policy different times others.units exposed policy different times others.Policy changes often aren’t random across units time.\nChallenge: Observed covariates might lead credible conclusions confounding (G. W. Imbens Rubin 2015)\nPolicy changes often aren’t random across units time.Challenge: Observed covariates might lead credible conclusions confounding (G. W. Imbens Rubin 2015)estimate effects, either\nDifference--differences () method widely used applied economics.\nSynthetic Control (SC) methods offer alternative approach comparative case studies.\nestimate effects, eitherDifference--differences () method widely used applied economics.Difference--differences () method widely used applied economics.Synthetic Control (SC) methods offer alternative approach comparative case studies.Synthetic Control (SC) methods offer alternative approach comparative case studies.Difference SC:\n: used many policy-exposed units; relies “parallel trends” assumption.\nSC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.\nDifference SC:: used many policy-exposed units; relies “parallel trends” assumption.: used many policy-exposed units; relies “parallel trends” assumption.SC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.SC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.New proposition: Synthetic Difference Differences (SDID).\nCombines features SC.\nReweights matches pre-exposure trends (similar SC).\nInvariant additive unit-level shifts, valid large-panel inference (like ).\nNew proposition: Synthetic Difference Differences (SDID).Combines features SC.Combines features SC.Reweights matches pre-exposure trends (similar SC).Reweights matches pre-exposure trends (similar SC).Invariant additive unit-level shifts, valid large-panel inference (like ).Invariant additive unit-level shifts, valid large-panel inference (like ).Attractive features:\nSDID provides consistent asymptotically normal estimates.\nSDID performs par better traditional settings.\ncan handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.\n\nSimilarly, SDID good better SC traditional SC settings.\nUniformly random treatment assignment results unbiased outcomes methods, SDID precise.\nSDID reduces bias effectively non-uniformly random assignments.\nSDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).\nmuch similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)\nAttractive features:SDID provides consistent asymptotically normal estimates.SDID provides consistent asymptotically normal estimates.SDID performs par better traditional settings.\ncan handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.\nSDID performs par better traditional settings.can handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.Similarly, SDID good better SC traditional SC settings.Similarly, SDID good better SC traditional SC settings.Uniformly random treatment assignment results unbiased outcomes methods, SDID precise.Uniformly random treatment assignment results unbiased outcomes methods, SDID precise.SDID reduces bias effectively non-uniformly random assignments.SDID reduces bias effectively non-uniformly random assignments.SDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).SDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).much similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)much similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)Ideal case use SDID estimator \\(N_{ctr} \\approx T_{pre}\\)\\(N_{ctr} \\approx T_{pre}\\)Small \\(T_{post}\\)Small \\(T_{post}\\)\\(N_{tr} <\\sqrt{N_{ctr}}\\)\\(N_{tr} <\\sqrt{N_{ctr}}\\)Applications marketing:Lambrecht, Tucker, Zhang (2024): TV ads online browsing sales.Lambrecht, Tucker, Zhang (2024): TV ads online browsing sales.Keller, Guyt, Grewal (2024): soda tax marketing effectiveness.Keller, Guyt, Grewal (2024): soda tax marketing effectiveness.","code":""},{"path":"synthetic-difference-in-differences.html","id":"understanding","chapter":"25 Synthetic Difference-in-Differences","heading":"25.1 Understanding","text":"Consider traditional time-series cross-sectional dataLet \\(Y_{}\\) denote outcome unit \\(\\) period \\(t\\)balanced panel \\(N\\) units \\(T\\) time periods\\(W_{} \\\\{0, 1\\}\\) binary treatment\\(W_{} \\\\{0, 1\\}\\) binary treatment\\(N_c\\) never-treated units (control)\\(N_c\\) never-treated units (control)\\(N_t\\) treated units time \\(T_{pre}\\)\\(N_t\\) treated units time \\(T_{pre}\\)Steps:Find unit weights \\(\\hat{w}^{sdid}\\) \\(\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} Y_{} \\approx N_t^{-1} \\sum_{= N_c + 1}^N Y_{} \\forall t = 1, \\dots, T_{pre}\\) (.e., pre-treatment trends outcome treated similar control units) (similar SC).Find time weights \\(\\hat{\\lambda}_t\\) balanced window (.e., posttreatment outcomes control units differ consistently weighted average pretreatment outcomes).Estimate average causal effect treatment\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_ t - W_{} \\tau)^2 \\hat{w}_i^{sdid} \\hat{\\lambda}_t^{sdid} \\}\n\\]Better estimator \\(\\tau^{}\\) consider time unit weights\\[\n(\\hat{\\tau}^{}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_ t - W_{} \\tau)^2 \\}\n\\]Better SC estimator \\(\\tau^{sc}\\) lacks unit fixed effete time weights\\[\n(\\hat{\\tau}^{sc}, \\hat{\\mu}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\beta_ t - W_{} \\tau)^2 \\hat{w}_i^{sdid}  \\}\n\\]Alternatively, think parameter interest :\\[\n\\hat{\\tau} = \\hat{\\delta}_t - \\sum_{= 1}^{N_c} \\hat{w}_i \\hat{\\delta}_i\n\\]\\(\\hat{\\delta}_t = \\frac{1}{N_t} \\sum_{= N_c + 1}^N \\hat{\\delta}_i\\)SDID estimator uses weights:\nMakes two-way fixed effect regression “local.”\nEmphasizes units similar past treated units.\nPrioritizes periods resembling treated periods.\nSDID estimator uses weights:Makes two-way fixed effect regression “local.”Makes two-way fixed effect regression “local.”Emphasizes units similar past treated units.Emphasizes units similar past treated units.Prioritizes periods resembling treated periods.Prioritizes periods resembling treated periods.Benefits localization:\nRobustness: Using similar units periods boosts estimator’s robustness.\nImproved Precision: Weights can eliminate predictable outcome components.\nSEs SDID smaller SC \nCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .\n\nBenefits localization:Robustness: Using similar units periods boosts estimator’s robustness.Robustness: Using similar units periods boosts estimator’s robustness.Improved Precision: Weights can eliminate predictable outcome components.\nSEs SDID smaller SC \nCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .\nImproved Precision: Weights can eliminate predictable outcome components.SEs SDID smaller SC DIDThe SEs SDID smaller SC DIDCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .Caveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .Weight Design:\nUnit Weights: Makes average outcome treated units roughly parallel weighted average control units.\nTime Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.\nWeight Design:Unit Weights: Makes average outcome treated units roughly parallel weighted average control units.Unit Weights: Makes average outcome treated units roughly parallel weighted average control units.Time Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.Time Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.Weights enhance ’s plausibility:\nRaw data often lacks parallel time trends treated/control units.\nSimilar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).\nSDID automates process, applying similar logic weight units time periods.\nWeights enhance ’s plausibility:Raw data often lacks parallel time trends treated/control units.Raw data often lacks parallel time trends treated/control units.Similar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).Similar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).SDID automates process, applying similar logic weight units time periods.SDID automates process, applying similar logic weight units time periods.Time Weights SDID:\nRemoves bias boosts precision (.e., minimizes influence time periods vastly different posttreatment periods).\nTime Weights SDID:Removes bias boosts precision (.e., minimizes influence time periods vastly different posttreatment periods).Argument Unit Fixed Effects:\nFlexibility: Increases model flexibility thereby bolsters robustness.\nEnhanced Precision: Unit fixed effects explain significant portion outcome variation.\nArgument Unit Fixed Effects:Flexibility: Increases model flexibility thereby bolsters robustness.Flexibility: Increases model flexibility thereby bolsters robustness.Enhanced Precision: Unit fixed effects explain significant portion outcome variation.Enhanced Precision: Unit fixed effects explain significant portion outcome variation.SC Weighting & Unit Fixed Effects:\ncertain conditions, SC weighting can inherently account unit fixed effects.\nexample, weighted average outcome control units pretreatment treated units. (unlikely reality)\n\nuse unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)\nSC Weighting & Unit Fixed Effects:certain conditions, SC weighting can inherently account unit fixed effects.\nexample, weighted average outcome control units pretreatment treated units. (unlikely reality)\ncertain conditions, SC weighting can inherently account unit fixed effects.example, weighted average outcome control units pretreatment treated units. (unlikely reality)use unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)use unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)details applicationChoose unit weightsRegularization Parameter:\nEqual size typical one-period outcome change control units pre-period, multiplied scaling factor (Arkhangelsky et al. 2021, 4092).\nRegularization Parameter:Equal size typical one-period outcome change control units pre-period, multiplied scaling factor (Arkhangelsky et al. 2021, 4092).Relation SC Weights:\nSDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:\nInclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\n\nRegularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\n\n\nweights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.\nRelation SC Weights:SDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:\nInclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\n\nRegularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\n\nSDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:Inclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\nInclusion Intercept Term:weights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.weights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.flexibility comes use unit fixed effects, can absorb consistent differences units.flexibility comes use unit fixed effects, can absorb consistent differences units.Regularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\nRegularization Penalty:Adopted Doudchenko Imbens (2016) .Adopted Doudchenko Imbens (2016) .Enhances dispersion ensures uniqueness weights.Enhances dispersion ensures uniqueness weights.weights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.weights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.Choose time weightsAlso include intercept term, regularization (correlated observations within time periods unit plausible, across units within period).Note: account time-varying variables weights, one can use residuals regression observed outcome time-varying variables, instead observed outcomes (\\(Y_{}^{res} = Y_{} - X_{} \\hat{\\beta}\\), \\(\\hat{\\beta}\\) come \\(Y = \\beta X_{}\\)).SDID method can account systematic effects, often referred unit effects unit heterogeneity, influence treatment assignment (.e., treatment assignment correlated systematic effects). Consequently, provides unbiased estimates, especially valuable ’s suspicion treatment might influenced persistent, unit-specific attributes.Even cases completely random assignment, SDID, , SC unbiased, SynthDiD smallest SE.","code":""},{"path":"synthetic-difference-in-differences.html","id":"application-11","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2 Application","text":"SDID AlgorithmCompute regularization parameter \\(\\zeta\\)\\[\n\\zeta = (N_{t}T_{post})^{1/4} \\hat{\\sigma}\n\\]\\[\n\\hat{\\sigma}^2 = \\frac{1}{N_c(T_{pre}- 1)} \\sum_{= 1}^{N_c} \\sum_{t = 1}^{T_{re}-1}(\\Delta_{} - \\hat{\\Delta})^2\n\\]\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)Compute unit weights \\(\\hat{w}^{sdid}\\)\\[\n(\\hat{w}_0, \\hat{w}^{sidid}) = \\arg \\min_{w_0 \\R, w \\\\Omega}l_{unit}(w_0, w)\n\\]\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)Compute time weights \\(\\hat{\\lambda}^{sdid}\\)\\[\n(\\hat{\\lambda}_0 , \\hat{\\lambda}^{sdid}) = \\arg \\min_{\\lambda_0 \\R, \\lambda \\\\Lambda} l_{time}(\\lambda_0, \\lambda)\n\\]\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)Compute SDID estimator\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta}\\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid}\\hat{\\lambda}_t^{sdid}\n\\]SE EstimationUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUsing asymptotic variance, conventional confidence intervals can applied SDID.Using asymptotic variance, conventional confidence intervals can applied SDID.\\[\n\\tau \\\\hat{\\tau}^{sdid} \\pm z_{\\alpha/2}\\sqrt{\\hat{V}_\\tau}\n\\]3 approaches variance estimation confidence intervals:\nClustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\n\nJackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\n\nPlacebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\n\n3 approaches variance estimation confidence intervals:Clustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\nClustered Bootstrap (Efron 1992):Independently resample units.Independently resample units.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Disadvantage: Computationally expensive.Disadvantage: Computationally expensive.Jackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\nJackknife (Miller 1974):Applied weighted SDID regression fixed weights.Applied weighted SDID regression fixed weights.Generally conservative precise treated control units sufficiently similar.Generally conservative precise treated control units sufficiently similar.recommended methods, like SC estimator, due potential biases.recommended methods, like SC estimator, due potential biases.Appropriate jackknifing without random weights.Appropriate jackknifing without random weights.Placebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\nPlacebo Variance Estimation:Can used cases one treated unit large panels.Can used cases one treated unit large panels.Placebo evaluations swap treated unit untreated ones estimate noise.Placebo evaluations swap treated unit untreated ones estimate noise.Relies homoskedasticity across units.Relies homoskedasticity across units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).algorithms Arkhangelsky et al. (2021), p. 4109:Bootstrap Variance EstimationFor \\(b\\) \\(1 \\B\\):\nSample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.\nsample lacks treated control units, resample.\nCalculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).\n\\(b\\) \\(1 \\B\\):Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.sample lacks treated control units, resample.sample lacks treated control units, resample.Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Jackknife Variance EstimationFor \\(\\) \\(1 \\N\\):\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)Calculate: \\(\\hat{V}_{\\tau} = (N - 1) N^{-1} \\sum_{= 1}^N (\\hat{\\tau}^{(-)} - \\hat{\\tau})^2\\)Placebo Variance EstimationFor \\(b\\) \\(1 \\B\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatment\nConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controls\nCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatmentConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controlsCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)Calculate \\(\\hat{V}_\\tau = \\frac{1}{B}\\sum_{b = 1}^B (\\hat{\\tau}^b - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)","code":""},{"path":"synthetic-difference-in-differences.html","id":"block-treatment","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2.1 Block Treatment","text":"Code provided synthdid package","code":"\nlibrary(synthdid)\nlibrary(tidyverse)\n\n# Estimate the effect of California Proposition 99 on cigarette consumption\ndata('california_prop99')\n\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\ntau.hat = synthdid::synthdid_estimate(setup$Y, setup$N0, setup$T0)\n\n# se = sqrt(vcov(tau.hat, method = 'placebo'))\n\nplot(tau.hat) + causalverse::ama_theme()\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\n# Run for specific estimators\nresults_selected = causalverse::panel_estimate(setup,\n                                               selected_estimators = c(\"synthdid\", \"did\", \"sc\"))\n\nresults_selected\n#> $synthdid\n#> $synthdid$estimate\n#> synthdid: -15.604 +- NA. Effective N0/N0 = 16.4/38~0.4. Effective T0/T0 = 2.8/19~0.1. N1,T1 = 1,12. \n#> \n#> $synthdid$std.error\n#> [1] 10.05324\n#> \n#> \n#> $did\n#> $did$estimate\n#> synthdid: -27.349 +- NA. Effective N0/N0 = 38.0/38~1.0. Effective T0/T0 = 19.0/19~1.0. N1,T1 = 1,12. \n#> \n#> $did$std.error\n#> [1] 15.81479\n#> \n#> \n#> $sc\n#> $sc$estimate\n#> synthdid: -19.620 +- NA. Effective N0/N0 = 3.8/38~0.1. Effective T0/T0 = Inf/19~Inf. N1,T1 = 1,12. \n#> \n#> $sc$std.error\n#> [1] 11.16422\n\n# to access more details in the estimate object\nsummary(results_selected$did$estimate)\n#> $estimate\n#> [1] -27.34911\n#> \n#> $se\n#>      [,1]\n#> [1,]   NA\n#> \n#> $controls\n#>                estimate 1\n#> Wyoming             0.026\n#> Wisconsin           0.026\n#> West Virginia       0.026\n#> Virginia            0.026\n#> Vermont             0.026\n#> Utah                0.026\n#> Texas               0.026\n#> Tennessee           0.026\n#> South Dakota        0.026\n#> South Carolina      0.026\n#> Rhode Island        0.026\n#> Pennsylvania        0.026\n#> Oklahoma            0.026\n#> Ohio                0.026\n#> North Dakota        0.026\n#> North Carolina      0.026\n#> New Mexico          0.026\n#> New Hampshire       0.026\n#> Nevada              0.026\n#> Nebraska            0.026\n#> Montana             0.026\n#> Missouri            0.026\n#> Mississippi         0.026\n#> Minnesota           0.026\n#> Maine               0.026\n#> Louisiana           0.026\n#> Kentucky            0.026\n#> Kansas              0.026\n#> Iowa                0.026\n#> Indiana             0.026\n#> Illinois            0.026\n#> Idaho               0.026\n#> Georgia             0.026\n#> Delaware            0.026\n#> Connecticut         0.026\n#> \n#> $periods\n#>      estimate 1\n#> 1988      0.053\n#> 1987      0.053\n#> 1986      0.053\n#> 1985      0.053\n#> 1984      0.053\n#> 1983      0.053\n#> 1982      0.053\n#> 1981      0.053\n#> 1980      0.053\n#> 1979      0.053\n#> 1978      0.053\n#> 1977      0.053\n#> 1976      0.053\n#> 1975      0.053\n#> 1974      0.053\n#> 1973      0.053\n#> 1972      0.053\n#> 1971      0.053\n#> \n#> $dimensions\n#>           N1           N0 N0.effective           T1           T0 T0.effective \n#>            1           38           38           12           19           19\n\ncausalverse::process_panel_estimate(results_selected)\n#>     Method Estimate    SE\n#> 1 SYNTHDID   -15.60 10.05\n#> 2      DID   -27.35 15.81\n#> 3       SC   -19.62 11.16"},{"path":"synthetic-difference-in-differences.html","id":"staggered-adoption","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2.2 Staggered Adoption","text":"apply staggered adoption settings using SDID estimator (see examples Arkhangelsky et al. (2021), p. 4115 similar Ben-Michael, Feller, Rothstein (2022)), can:Apply SDID estimator repeatedly, every adoption date.Apply SDID estimator repeatedly, every adoption date.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Create multiple samples splitting data time periods. sample consistent adoption date.Create multiple samples splitting data time periods. sample consistent adoption date.formal note special case, see Porreca (2022). compares outcomes using SynthDiD estimators:Two-Way Fixed Effects (TWFE),Two-Way Fixed Effects (TWFE),group time average treatment effect estimator Callaway Sant’Anna (2021),group time average treatment effect estimator Callaway Sant’Anna (2021),partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.findings reveal SynthDiD produces different estimate average treatment effect compared methods.\nSimulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.\nfindings reveal SynthDiD produces different estimate average treatment effect compared methods.Simulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.explore heterogeneity treatment effect, can subgroup analysis (Berman Israeli 2022, 1092)Split data separate subsets subgroup.Compute synthetic effects subset.Use control group consisting non-adopters balanced panel cohort analysis.Switch treatment units subgroup analyzed.Perform synthdid analysis.Use data estimate synthetic control weights.Compute treatment effects using treated subgroup units treatment units.Plot different estimators","code":"\nlibrary(tidyverse)\ndf <- fixest::base_stagg |>\n   dplyr::mutate(treatvar = if_else(time_to_treatment >= 0, 1, 0)) |>\n   dplyr::mutate(treatvar = as.integer(if_else(year_treated > (5 + 2), 0, treatvar)))\n\n\nest <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\"\n)\n#> adoption_cohort: 5 \n#> Treated units: 5 Control units: 65 \n#> adoption_cohort: 6 \n#> Treated units: 5 Control units: 60 \n#> adoption_cohort: 7 \n#> Treated units: 5 Control units: 55\n\ndata.frame(\n    Period = names(est$TE_mean_w),\n    ATE    = est$TE_mean_w,\n    SE     = est$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2 -0.05 0.22\n#> 2      -1  0.05 0.22\n#> 3       0 -5.07 0.80\n#> 4       1 -4.68 0.51\n#> 5       2 -3.70 0.79\n#> 6 cumul.0 -5.07 0.80\n#> 7 cumul.1 -4.87 0.55\n#> 8 cumul.2 -4.48 0.53\n\n\ncausalverse::synthdid_plot_ate(est)\nest_sub <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\",\n  # a vector of subgroup id (from unit id)\n  subgroup           =  c(\n    # some are treated\n    \"11\", \"30\", \"49\" ,\n    # some are control within this period\n    \"20\", \"25\", \"21\")\n)\n#> adoption_cohort: 5 \n#> Treated units: 3 Control units: 65 \n#> adoption_cohort: 6 \n#> Treated units: 0 Control units: 60 \n#> adoption_cohort: 7 \n#> Treated units: 0 Control units: 55\n\ndata.frame(\n    Period = names(est_sub$TE_mean_w),\n    ATE = est_sub$TE_mean_w,\n    SE = est_sub$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2  0.32 0.44\n#> 2      -1 -0.32 0.44\n#> 3       0 -4.29 1.68\n#> 4       1 -4.00 1.52\n#> 5       2 -3.44 2.90\n#> 6 cumul.0 -4.29 1.68\n#> 7 cumul.1 -4.14 1.52\n#> 8 cumul.2 -3.91 1.82\n\ncausalverse::synthdid_plot_ate(est)\nlibrary(causalverse)\nmethods <- c(\"synthdid\", \"did\", \"sc\", \"sc_ridge\", \"difp\", \"difp_ridge\")\n\nestimates <- lapply(methods, function(method) {\n  synthdid_est_ate(\n    data               = df,\n    adoption_cohorts   = 5:7,\n    lags               = 2,\n    leads              = 2,\n    time_var           = \"year\",\n    unit_id_var        = \"id\",\n    treated_period_var = \"year_treated\",\n    treat_stat_var     = \"treatvar\",\n    outcome_var        = \"y\",\n    method = method\n  )\n})\n\nplots <- lapply(seq_along(estimates), function(i) {\n  causalverse::synthdid_plot_ate(estimates[[i]],\n                                 title = methods[i],\n                                 theme = causalverse::ama_theme(base_size = 6))\n})\n\ngridExtra::grid.arrange(grobs = plots, ncol = 2)"},{"path":"difference-in-differences.html","id":"difference-in-differences","chapter":"26 Difference-in-differences","heading":"26 Difference-in-differences","text":"List packagesExamples marketing(Liaukonyte, Teixeira, Wilbur 2015): TV ad online shopping(Yanwen Wang, Lewis, Schweidel 2018): political ad source message tone vote shares turnout using discontinuities level political ads borders(Datta, Knox, Bronnenberg 2018): streaming service total music consumption using timing users adoption music streaming service(Janakiraman, Lim, Rishika 2018): data breach announcement affect customer spending using timing data breach variation whether customer info breached event(Israeli 2018): digital monitoring enforcement violations using enforcement min ad price policies(Ramani Srinivasan 2019): firms respond foreign direct investment liberalization using India’s reform 1991.(Pattabhiramaiah, Sriram, Manchanda 2019): paywall affects readership(Akca Rao 2020): aggregators airlines business effect(Lim et al. 2020): nutritional labels nutritional quality brands category using variation timing adoption nutritional labels across categories(Guo, Sriram, Manchanda 2020): payment disclosure laws effect physician prescription behavior using Timing Massachusetts open payment law exogenous shock(S. , Hollenbeck, Proserpio 2022): using Amazon policy change examine causal impact fake reviews sales, average ratings.(Peukert et al. 2022): using European General data protection Regulation, examine impact policy change website usage.Examples econ:(Rosenzweig Wolpin 2000)(Rosenzweig Wolpin 2000)(J. D. Angrist Krueger 2001)(J. D. Angrist Krueger 2001)(Fuchs-Schündeln Hassan 2016): macro(Fuchs-Schündeln Hassan 2016): macroShow mechanism viaMediation analysis: see (Habel, Alavi, Linsenmayer 2021)Mediation analysis: see (Habel, Alavi, Linsenmayer 2021)Moderation analysis: see (Goldfarb Tucker 2011)Moderation analysis: see (Goldfarb Tucker 2011)Steps trust :Visualize treatment rollout (e.g., panelView).Visualize treatment rollout (e.g., panelView).Document number treated units cohort (e.g., control treated).Document number treated units cohort (e.g., control treated).Visualize trajectory average outcomes across cohorts (multiple periods).Visualize trajectory average outcomes across cohorts (multiple periods).Parallel Trends Conduct event-study analysis without covariates.Parallel Trends Conduct event-study analysis without covariates.case covariates, check overlap covariates treated control groups ensure control group validity (e.g., control relatively small treated group, might overlap, make extrapolation).case covariates, check overlap covariates treated control groups ensure control group validity (e.g., control relatively small treated group, might overlap, make extrapolation).Conduct sensitivity analysis parallel trend violations (e.g., honestDiD).Conduct sensitivity analysis parallel trend violations (e.g., honestDiD).","code":""},{"path":"difference-in-differences.html","id":"visualization","chapter":"26 Difference-in-differences","heading":"26.1 Visualization","text":"","code":"\nlibrary(panelView)\nlibrary(fixest)\nlibrary(tidyverse)\nbase_stagg <- fixest::base_stagg |>\n    # treatment status\n    dplyr::mutate(treat_stat = dplyr::if_else(time_to_treatment < 0, 0, 1)) |> \n    select(id, year, treat_stat, y)\n\nhead(base_stagg)\n#>   id year treat_stat           y\n#> 2 90    1          0  0.01722971\n#> 3 89    1          0 -4.58084528\n#> 4 88    1          0  2.73817174\n#> 5 87    1          0 -0.65103066\n#> 6 86    1          0 -5.33381664\n#> 7 85    1          0  0.49562631\n\npanelView::panelview(\n    y ~ treat_stat,\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# alternatively specification\npanelView::panelview(\n    Y = \"y\",\n    D = \"treat_stat\",\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# Average outcomes for each cohort\npanelView::panelview(\n    data = base_stagg, \n    Y = \"y\",\n    D = \"treat_stat\",\n    index = c(\"id\", \"year\"),\n    by.timing = T,\n    display.all = F,\n    type = \"outcome\", \n    by.cohort = T\n)\n#> Number of unique treatment histories: 10"},{"path":"difference-in-differences.html","id":"simple-dif-n-dif","chapter":"26 Difference-in-differences","heading":"26.2 Simple Dif-n-dif","text":"tool developed intuitively study “natural experiment”, uses much broader.tool developed intuitively study “natural experiment”, uses much broader.Fixed Effects Estimator foundation DIDFixed Effects Estimator foundation DIDWhy dif--dif attractive? Identification strategy: Inter-temporal variation groups\nCross-sectional estimator helps avoid omitted (unobserved) common trends\nTime-series estimator helps overcome omitted (unobserved) cross-sectional differences\ndif--dif attractive? Identification strategy: Inter-temporal variation groupsCross-sectional estimator helps avoid omitted (unobserved) common trendsCross-sectional estimator helps avoid omitted (unobserved) common trendsTime-series estimator helps overcome omitted (unobserved) cross-sectional differencesTime-series estimator helps overcome omitted (unobserved) cross-sectional differencesConsider\\(D_i = 1\\) treatment group\\(D_i = 1\\) treatment group\\(D_i = 0\\) control group\\(D_i = 0\\) control group\\(T= 1\\) treatment\\(T= 1\\) treatment\\(T =0\\) treatment\\(T =0\\) treatmentmissing \\(E[Y_{0i}(1)|D=1]\\)Average Treatment Effect Treated (ATT)\\[\n\\begin{aligned}\nE[Y_1(1) - Y_0(1)|D=1] &= \\{E[Y(1)|D=1] - E[Y(1)|D=0] \\} \\\\\n&- \\{E[Y(0)|D=1] - E[Y(0)|D=0] \\}\n\\end{aligned}\n\\]elaboration:treatment group, isolate difference treated treated. untreated group affected different way, design estimate tell us nothing.Alternatively, can’t observe treatment variation control group, can’t say anything treatment effect group.ExtensionMore 2 groups (multiple treatments multiple controls), 2 period (pre post)\\[\nY_{igt} = \\alpha_g + \\gamma_t + \\beta I_{gt} + \\delta X_{igt} + \\epsilon_{igt}\n\\]\\(\\alpha_g\\) group-specific fixed effect\\(\\alpha_g\\) group-specific fixed effect\\(\\gamma_t\\) = time specific fixed effect\\(\\gamma_t\\) = time specific fixed effect\\(\\beta\\) = dif--dif effect\\(\\beta\\) = dif--dif effect\\(I_{gt}\\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity time)\\(I_{gt}\\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity time)specification “two-way fixed effects ” - TWFE (.e., 2 sets fixed effects: group + time).However, Staggered Dif-n-dif (.e., treatment applied different times different groups). TWFE really bad.Long-term EffectsTo examine dynamic treatment effects (rollout/staggered design), can create centered time variable,Last period right treatment periodRemember use period reference groupBy interacting factor variable, can examine dynamic effect treatment (.e., whether ’s fading intensifying)\\[\n\\begin{aligned}\nY &= \\alpha_0 + \\alpha_1 Group + \\alpha_2 Time  \\\\\n&+ \\beta_{-T_1} Treatment+  \\beta_{-(T_1 -1)} Treatment + \\dots +  \\beta_{-1} Treatment \\\\\n&+ \\beta_1 + \\dots + \\beta_{T_2} Treatment\n\\end{aligned}\n\\]\\(\\beta_0\\) used reference group (.e., drop model)\\(\\beta_0\\) used reference group (.e., drop model)\\(T_1\\) pre-treatment period\\(T_1\\) pre-treatment period\\(T_2\\) post-treatment period\\(T_2\\) post-treatment periodWith variables (.e., interaction terms), coefficients estimates can less precise (.e., higher SE).relationship, levels. Technically, can apply research design variables, also coefficients estimates regression models policy implemented.Goal:Pre-treatment coefficients non-significant \\(\\beta_{-T_1}, \\dots, \\beta_{-1} = 0\\) (similar Placebo Test)Post-treatment coefficients expected significant \\(\\beta_1, \\dots, \\beta_{T_2} \\neq0\\)\ncan now examine trend post-treatment coefficients (.e., increasing decreasing)\ncan now examine trend post-treatment coefficients (.e., increasing decreasing)","code":"\nlibrary(tidyverse)\nlibrary(fixest)\n\nod <- causaldata::organ_donations %>%\n    \n    # Treatment variable\n    dplyr::mutate(California = State == 'California') %>%\n    # centered time variable\n    dplyr::mutate(center_time = as.factor(Quarter_Num - 3))  \n# where 3 is the reference period precedes the treatment period\n\nclass(od$California)\n#> [1] \"logical\"\nclass(od$State)\n#> [1] \"character\"\n\ncali <- feols(Rate ~ i(center_time, California, ref = 0) |\n                  State + center_time,\n              data = od)\n\netable(cali)\n#>                                              cali\n#> Dependent Var.:                              Rate\n#>                                                  \n#> California x center_time = -2    -0.0029 (0.0051)\n#> California x center_time = -1   0.0063** (0.0023)\n#> California x center_time = 1  -0.0216*** (0.0050)\n#> California x center_time = 2  -0.0203*** (0.0045)\n#> California x center_time = 3    -0.0222* (0.0100)\n#> Fixed-Effects:                -------------------\n#> State                                         Yes\n#> center_time                                   Yes\n#> _____________________________ ___________________\n#> S.E.: Clustered                         by: State\n#> Observations                                  162\n#> R2                                        0.97934\n#> Within R2                                 0.00979\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\niplot(cali, pt.join = T)\ncoefplot(cali)"},{"path":"difference-in-differences.html","id":"notes-1","chapter":"26 Difference-in-differences","heading":"26.3 Notes","text":"Matching Methods\nMatch treatment control based pre-treatment observables\nModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effect\nWhereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.\ncareful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.\nMatching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.\nMatching MethodsMatch treatment control based pre-treatment observablesMatch treatment control based pre-treatment observablesModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effectModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effectWhereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.Whereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.careful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.careful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.’s always good show results without controls \ncontrols fixed within group within time, absorbed fixed effects\ncontrols dynamic across group across, parallel trends assumption plausible.\n’s always good show results without controls becauseIf controls fixed within group within time, absorbed fixed effectsIf controls fixed within group within time, absorbed fixed effectsIf controls dynamic across group across, parallel trends assumption plausible.controls dynamic across group across, parallel trends assumption plausible.causal inference, \\(R^2\\) important.causal inference, \\(R^2\\) important.count data, one can use fixed-effects Poisson pseudo-maximum likelihood estimator (PPML) Puhani (2012) (applied papers, see Burtch, Carnahan, Greenwood (2018) management C. et al. (2021) marketing). also allows robust standard errors -dispersion (Wooldridge 1999).estimator outperforms log OLS data many 0s(Silva Tenreyro 2011), since log-OLS can produce biased estimates (O’Hara Kotze 2010) heteroskedascity (Silva Tenreyro 2006).estimator outperforms log OLS data many 0s(Silva Tenreyro 2011), since log-OLS can produce biased estimates (O’Hara Kotze 2010) heteroskedascity (Silva Tenreyro 2006).thinking negative binomial fixed effects, isn’t estimator right now (Allison Waterman 2002).thinking negative binomial fixed effects, isn’t estimator right now (Allison Waterman 2002).[Zero-valued Outcomes], distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). Alternatively, can either focus onProportional treatment effects: \\(\\theta_{ATT\\%} = \\frac{E(Y_{}(1) | D_i = 1, Post_t = 1) - E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) | D_i = 1 , Post_t = 1}\\) (.e., percentage change treated group’s average post-treatment outcome). Instead relying parallel trends assumption levels, also rely parallel trends assumption ratio (Wooldridge 2023).\ncan use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).\nexamine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).\nEven see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).\nProportional treatment effects: \\(\\theta_{ATT\\%} = \\frac{E(Y_{}(1) | D_i = 1, Post_t = 1) - E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) | D_i = 1 , Post_t = 1}\\) (.e., percentage change treated group’s average post-treatment outcome). Instead relying parallel trends assumption levels, also rely parallel trends assumption ratio (Wooldridge 2023).can use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).can use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).examine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).examine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).Even see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).Even see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).Log Effects Calibrated Extensive-margin value: due problem mean value interpretation proportional treatment effects outcomes heavy-tailed, might interested extensive margin effect. , can explicit model much weight put intensive vs. extensive margin (J. Chen Roth 2023, 39).Log Effects Calibrated Extensive-margin value: due problem mean value interpretation proportional treatment effects outcomes heavy-tailed, might interested extensive margin effect. , can explicit model much weight put intensive vs. extensive margin (J. Chen Roth 2023, 39).Proportional treatment effectsIn example, coefficient significant. However, say ’s significant, can interpret coefficient 3 percent increase posttreatment period due treatment.parallel trend “ratio” version Wooldridge (2023) :\\[\n\\frac{E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) |D_i = 1, Post_t = 0)} = \\frac{E(Y_{}(0) |D_i = 0, Post_t = 1)}{E(Y_{}(0) |D_i =0, Post_t = 0)}\n\\]means without treatment, average percentage change mean outcome treated group identical control group.Log Effects Calibrated Extensive-margin valueIf want study treatment effect concave transformation outcome less influenced distribution’s tail, can perform analysis.Steps:Normalize outcomes 1 represents minimum non-zero positve value (.e., divide outcome minimum non-zero positive value).Estimate treatment effects new outcome\\[\nm(y) =\n\\begin{cases}\n\\log(y) & \\text{} y >0 \\\\\n-x & \\text{} y = 0\n\\end{cases}\n\\]choice \\(x\\) depends researcher interested :dynamic treatment effects different hypothesized extensive-margin value \\(x \\(0, .1, .5, 1, 3, 5)\\)first column zero-valued outcome equal \\(y_{min, y>0}\\) (.e., different minimum outcome zero outcome - \\(x = 0\\))particular example, extensive margin increases, see increase effect magnitude. second column assume extensive-margin change 0 \\(y_{min, y >0}\\) equivalent 10 (.e., \\(0.1 \\times 100\\)) log point change along intensive margin.","code":"\nset.seed(123) # For reproducibility\n\nn <- 500 # Number of observations per group (treated and control)\n# Generating IDs for a panel setup\nID <- rep(1:n, times = 2)\n\n# Defining groups and periods\nGroup <- rep(c(\"Control\", \"Treated\"), each = n)\nTime <- rep(c(\"Before\", \"After\"), times = n)\nTreatment <- ifelse(Group == \"Treated\", 1, 0)\nPost <- ifelse(Time == \"After\", 1, 0)\n\n# Step 1: Generate baseline outcomes with a zero-inflated model\nlambda <- 20 # Average rate of occurrence\nzero_inflation <- 0.5 # Proportion of zeros\nY_baseline <-\n    ifelse(runif(2 * n) < zero_inflation, 0, rpois(2 * n, lambda))\n\n# Step 2: Apply DiD treatment effect on the treated group in the post-treatment period\nTreatment_Effect <- Treatment * Post\nY_treatment <-\n    ifelse(Treatment_Effect == 1, rpois(n, lambda = 2), 0)\n\n# Incorporating a simple time trend, ensuring outcomes are non-negative\nTime_Trend <- ifelse(Time == \"After\", rpois(2 * n, lambda = 1), 0)\n\n# Step 3: Combine to get the observed outcomes\nY_observed <- Y_baseline + Y_treatment + Time_Trend\n\n# Ensure no negative outcomes after the time trend\nY_observed <- ifelse(Y_observed < 0, 0, Y_observed)\n\n# Create the final dataset\ndata <-\n    data.frame(\n        ID = ID,\n        Treatment = Treatment,\n        Period = Post,\n        Outcome = Y_observed\n    )\n\n# Viewing the first few rows of the dataset\nhead(data)\n#>   ID Treatment Period Outcome\n#> 1  1         0      0       0\n#> 2  2         0      1      25\n#> 3  3         0      0       0\n#> 4  4         0      1      20\n#> 5  5         0      0      19\n#> 6  6         0      1       0\nlibrary(fixest)\nres_pois <-\n    fepois(Outcome ~ Treatment + Period + Treatment * Period,\n           data = data,\n           vcov = \"hetero\")\netable(res_pois)\n#>                             res_pois\n#> Dependent Var.:              Outcome\n#>                                     \n#> Constant           2.249*** (0.0717)\n#> Treatment           0.1743. (0.0932)\n#> Period               0.0662 (0.0960)\n#> Treatment x Period   0.0314 (0.1249)\n#> __________________ _________________\n#> S.E. type          Heteroskeda.-rob.\n#> Observations                   1,000\n#> Squared Cor.                 0.01148\n#> Pseudo R2                    0.00746\n#> BIC                         15,636.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Average percentage change\nexp(coefficients(res_pois)[\"Treatment:Period\"]) - 1\n#> Treatment:Period \n#>       0.03191643\n\n# SE using delta method\nexp(coefficients(res_pois)[\"Treatment:Period\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment:Period\", \"Treatment:Period\"])\n#> Treatment:Period \n#>        0.1288596\nlibrary(fixest)\n\nbase_did_log0 <- base_did |> \n    mutate(y = if_else(y > 0, y, 0))\n\nres_pois_es <-\n    fepois(y ~ x1 + i(period, treat, 5) | id + period,\n           data = base_did_log0,\n           vcov = \"hetero\")\n\netable(res_pois_es)\n#>                            res_pois_es\n#> Dependent Var.:                      y\n#>                                       \n#> x1                  0.1895*** (0.0108)\n#> treat x period = 1    -0.2769 (0.3545)\n#> treat x period = 2    -0.2699 (0.3533)\n#> treat x period = 3     0.1737 (0.3520)\n#> treat x period = 4    -0.2381 (0.3249)\n#> treat x period = 6     0.3724 (0.3086)\n#> treat x period = 7    0.7739* (0.3117)\n#> treat x period = 8    0.5028. (0.2962)\n#> treat x period = 9   0.9746** (0.3092)\n#> treat x period = 10  1.310*** (0.3193)\n#> Fixed-Effects:      ------------------\n#> id                                 Yes\n#> period                             Yes\n#> ___________________ __________________\n#> S.E. type           Heteroskedas.-rob.\n#> Observations                     1,080\n#> Squared Cor.                   0.51131\n#> Pseudo R2                      0.34836\n#> BIC                            5,868.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\niplot(res_pois_es)\nlibrary(fixest)\nbase_did_log0_cali <- base_did_log0 |> \n    # get min \n    mutate(min_y = min(y[y > 0])) |> \n    \n    # normalized the outcome \n    mutate(y_norm = y / min_y)\n\nmy_regression <-\n    function(x) {\n        base_did_log0_cali <-\n            base_did_log0_cali %>% mutate(my = ifelse(y_norm == 0,-x,\n                                                      log(y_norm)))\n        my_reg <-\n            feols(\n                fml = my ~ x1 + i(period, treat, 5) | id + period,\n                data = base_did_log0_cali,\n                vcov = \"hetero\"\n            )\n        \n        return(my_reg)\n    }\n\nxvec <- c(0, .1, .5, 1, 3)\nreg_list <- purrr::map(.x = xvec, .f = my_regression)\n\n\niplot(reg_list, \n      pt.col =  1:length(xvec),\n      pt.pch = 1:length(xvec))\nlegend(\"topleft\", \n       col = 1:length(xvec),\n       pch = 1:length(xvec),\n       legend = as.character(xvec))\n\n\netable(\n    reg_list,\n    headers = list(\"Extensive-margin value (x)\" = as.character(xvec)),\n    digits = 2,\n    digits.stats = 2\n)\n#>                                   model 1        model 2        model 3\n#> Extensive-margin value (x)              0            0.1            0.5\n#> Dependent Var.:                        my             my             my\n#>                                                                        \n#> x1                         0.43*** (0.02) 0.44*** (0.02) 0.46*** (0.03)\n#> treat x period = 1           -0.92 (0.67)   -0.94 (0.69)    -1.0 (0.73)\n#> treat x period = 2           -0.41 (0.66)   -0.42 (0.67)   -0.43 (0.71)\n#> treat x period = 3           -0.34 (0.67)   -0.35 (0.68)   -0.38 (0.73)\n#> treat x period = 4            -1.0 (0.67)    -1.0 (0.68)    -1.1 (0.73)\n#> treat x period = 6            0.44 (0.66)    0.44 (0.67)    0.45 (0.72)\n#> treat x period = 7            1.1. (0.64)    1.1. (0.65)    1.2. (0.70)\n#> treat x period = 8            1.1. (0.64)    1.1. (0.65)     1.1 (0.69)\n#> treat x period = 9           1.7** (0.65)   1.7** (0.66)    1.8* (0.70)\n#> treat x period = 10         2.4*** (0.62)  2.4*** (0.63)  2.5*** (0.68)\n#> Fixed-Effects:             -------------- -------------- --------------\n#> id                                    Yes            Yes            Yes\n#> period                                Yes            Yes            Yes\n#> __________________________ ______________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080          1,080\n#> R2                                   0.43           0.43           0.43\n#> Within R2                            0.26           0.26           0.25\n#> \n#>                                   model 4        model 5\n#> Extensive-margin value (x)              1              3\n#> Dependent Var.:                        my             my\n#>                                                         \n#> x1                         0.49*** (0.03) 0.62*** (0.04)\n#> treat x period = 1            -1.1 (0.79)     -1.5 (1.0)\n#> treat x period = 2           -0.44 (0.77)   -0.51 (0.99)\n#> treat x period = 3           -0.43 (0.78)    -0.60 (1.0)\n#> treat x period = 4            -1.2 (0.78)     -1.5 (1.0)\n#> treat x period = 6            0.45 (0.77)     0.46 (1.0)\n#> treat x period = 7             1.2 (0.75)     1.3 (0.97)\n#> treat x period = 8             1.2 (0.74)     1.3 (0.96)\n#> treat x period = 9            1.8* (0.75)    2.1* (0.97)\n#> treat x period = 10         2.7*** (0.73)  3.2*** (0.94)\n#> Fixed-Effects:             -------------- --------------\n#> id                                    Yes            Yes\n#> period                                Yes            Yes\n#> __________________________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080\n#> R2                                   0.42           0.41\n#> Within R2                            0.25           0.24\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"difference-in-differences.html","id":"standard-errors-2","chapter":"26 Difference-in-differences","heading":"26.4 Standard Errors","text":"Serial correlation big problem (Bertrand, Duflo, Mullainathan 2004)often uses long time seriesOutcomes often highly positively serially correlatedMinimal variation treatment variable time within group (e.g., state).overcome problem:Using parametric correction (standard AR correction) good.Using nonparametric (e.g., block bootstrap- keep obs group state together) good number groups large.Remove time series dimension (.e., aggregate data 2 periods: pre post). still works small number groups (See (Donald Lang 2007) notes small-sample aggregation).Empirical arbitrary variance-covariance matrix corrections work large samples.","code":""},{"path":"difference-in-differences.html","id":"examples","chapter":"26 Difference-in-differences","heading":"26.5 Examples","text":"Example Philipp Leppert replicating Card Krueger (1994)Example Anthony Schmidt","code":""},{"path":"difference-in-differences.html","id":"example-by-doleac2020unintended","chapter":"26 Difference-in-differences","heading":"26.5.1 Example by Doleac and Hansen (2020)","text":"purpose banning checking box ex-criminal banned thought gives access felonsThe purpose banning checking box ex-criminal banned thought gives access felonsEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based raceEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based race3 types ban boxPublic employer onlyPrivate employer government contractAll employersMain identification strategyIf county Metropolitan Statistical Area (MSA) adopts ban box, means whole MSA treated. state adopts “ban ban,” every county treatedUnder Simple Dif-n-dif\\[ Y_{} = \\beta_0 + \\beta_1 Post_t + \\beta_2 treat_i + \\beta_2 (Post_t \\times Treat_i) + \\epsilon_{} \\]common post time, use Staggered Dif-n-dif\\[ \\begin{aligned} E_{imrt} &= \\alpha + \\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\\\  &+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + \\delta_m\\times f(t) \\beta_7 + e_{imrt} \\end{aligned} \\]\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., Midwest) ; \\(r\\) = region; \\(t\\) = year\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., Midwest) ; \\(r\\) = region; \\(t\\) = year\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\delta_m\\) = dummy MSI\\(\\delta_m\\) = dummy MSI\\(D_{imt}\\) = control people\\(D_{imt}\\) = control people\\(\\lambda_{rt}\\) = region time fixed effect\\(\\lambda_{rt}\\) = region time fixed effect\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)put \\(\\lambda_r - \\lambda_t\\) (separately) broad fixed effect, \\(\\lambda_{rt}\\) give us deeper narrower fixed effect.running model, drop races. \\(\\beta_1, \\beta_2, \\beta_3\\) collinear interaction terms \\(BTB_{mt}\\)just want estimate model black men, modify \\[ E_{imrt} = \\alpha + BTB_{mt} \\beta_1 + \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt} \\]\\[ \\begin{aligned} E_{imrt} &= \\alpha + BTB_{m (t - 3t)} \\theta_1 + BTB_{m(t-2)} \\theta_2 + BTB_{mt} \\theta_4 \\\\ &+ BTB_{m(t+1)}\\theta_5 + BTB_{m(t+2)}\\theta_6 + BTB_{m(t+3t)}\\theta_7 \\\\ &+ [\\delta_m + D_{imt}\\beta_5 + \\lambda_r + (\\delta_m \\times (f(t))\\beta_7 + e_{imrt}] \\end{aligned} \\]leave \\(BTB_{m(t-1)}\\theta_3\\) category perfect collinearitySo year BTB (\\(\\theta_1, \\theta_2, \\theta_3\\)) similar (.e., pre-trend). Remember, run places BTB.\\(\\theta_2\\) statistically different \\(\\theta_3\\) (baseline), problem, also make sense pre-trend announcement.","code":""},{"path":"difference-in-differences.html","id":"example-from-princeton","chapter":"26 Difference-in-differences","heading":"26.5.2 Example from Princeton","text":"estimate estimatorThe coefficient differences--differences estimator. Treat negative effect","code":"\nlibrary(foreign)\nmydata = read.dta(\"http://dss.princeton.edu/training/Panel101.dta\") %>%\n    # create a dummy variable to indicate the time when the treatment started\n    dplyr::mutate(time = ifelse(year >= 1994, 1, 0)) %>%\n    # create a dummy variable to identify the treatment group\n    dplyr::mutate(treated = ifelse(country == \"E\" |\n                                country == \"F\" | country == \"G\" ,\n                            1,\n                            0)) %>%\n    # create an interaction between time and treated\n    dplyr::mutate(did = time * treated)\ndidreg = lm(y ~ treated + time + did, data = mydata)\nsummary(didreg)\n#> \n#> Call:\n#> lm(formula = y ~ treated + time + did, data = mydata)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -9.768e+09 -1.623e+09  1.167e+08  1.393e+09  6.807e+09 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  3.581e+08  7.382e+08   0.485   0.6292  \n#> treated      1.776e+09  1.128e+09   1.575   0.1200  \n#> time         2.289e+09  9.530e+08   2.402   0.0191 *\n#> did         -2.520e+09  1.456e+09  -1.731   0.0882 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.953e+09 on 66 degrees of freedom\n#> Multiple R-squared:  0.08273,    Adjusted R-squared:  0.04104 \n#> F-statistic: 1.984 on 3 and 66 DF,  p-value: 0.1249"},{"path":"difference-in-differences.html","id":"example-by-card1993minimum","chapter":"26 Difference-in-differences","heading":"26.5.3 Example by Card and Krueger (1993)","text":"found increase minimum wage increases employmentExperimental Setting:New Jersey (treatment) increased minimum wageNew Jersey (treatment) increased minimum wagePenn (control) increase minimum wagePenn (control) increase minimum wagewhereA - B = treatment effect + effect time (additive)- B = treatment effect + effect time (additive)C - D = effect timeC - D = effect time(- B) - (C - D) = dif-n-dif(- B) - (C - D) = dif-n-difThe identifying assumptions:Can’t switchersCan’t switchersPA control group\ngood counter factual\nNJ look like hadn’t treatment\nPA control groupis good counter factualis good counter factualis NJ look like hadn’t treatmentis NJ look like hadn’t treatment\\[\nY_{jt} = \\beta_0 + NJ_j \\beta_1 + POST_t \\beta_2 + (NJ_j \\times POST_t)\\beta_3+ X_{jt}\\beta_4 + \\epsilon_{jt}\n\\]\\(j\\) = restaurant\\(j\\) = restaurant\\(NJ\\) = dummy \\(1 = NJ\\), \\(0 = PA\\)\\(NJ\\) = dummy \\(1 = NJ\\), \\(0 = PA\\)\\(POST\\) = dummy \\(1 = post\\), \\(0 = pre\\)\\(POST\\) = dummy \\(1 = post\\), \\(0 = pre\\)Notes:don’t need \\(\\beta_4\\) model unbiased \\(\\beta_3\\), including give coefficients efficiencyWe don’t need \\(\\beta_4\\) model unbiased \\(\\beta_3\\), including give coefficients efficiencyIf use \\(\\Delta Y_{jt}\\) dependent variable, don’t need \\(POST_t \\beta_2\\) anymoreIf use \\(\\Delta Y_{jt}\\) dependent variable, don’t need \\(POST_t \\beta_2\\) anymoreAlternative model specification authors use NJ high wage restaurant control group (still choose close border)Alternative model specification authors use NJ high wage restaurant control group (still choose close border)reason can’t control everything (PA + NJ high wage) ’s hard interpret causal treatmentThe reason can’t control everything (PA + NJ high wage) ’s hard interpret causal treatmentDif-n-dif utilizes similarity pretrend dependent variables. However, neither necessary sufficient identifying assumption.\n’s sufficient can multiple treatments (technically, include control, treatment can’t interact)\n’s necessary trends can parallel treatment\nDif-n-dif utilizes similarity pretrend dependent variables. However, neither necessary sufficient identifying assumption.’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s necessary trends can parallel treatmentIt’s necessary trends can parallel treatmentHowever, can’t never certain; just try find evidence consistent theory dif-n-dif can work.However, can’t never certain; just try find evidence consistent theory dif-n-dif can work.Notice don’t need treatment levels dependent variable (e.g., wage average NJ PA), dif-n-dif needs pre-trend (.e., slope) two groups.Notice don’t need treatment levels dependent variable (e.g., wage average NJ PA), dif-n-dif needs pre-trend (.e., slope) two groups.","code":""},{"path":"difference-in-differences.html","id":"example-by-butcher2014effects","chapter":"26 Difference-in-differences","heading":"26.5.4 Example by Butcher, McEwan, and Weerapana (2014)","text":"Theory:Highest achieving students usually hard science. ?\nHard give students students benefit doubt hard science\nunpleasant easy get job. Degrees lower market value typically want make feel pleasant\nHighest achieving students usually hard science. ?Hard give students students benefit doubt hard scienceHard give students students benefit doubt hard scienceHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantUnder OLS\\[\nE_{ij} = \\beta_0 + X_i \\beta_1 + G_j \\beta_2 + \\epsilon_{ij}\n\\]\\(X_i\\) = student attributes\\(X_i\\) = student attributes\\(\\beta_2\\) = causal estimate (grade change)\\(\\beta_2\\) = causal estimate (grade change)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(G_j\\) = grade given major \\(j\\)\\(G_j\\) = grade given major \\(j\\)Examine \\(\\hat{\\beta}_2\\)Negative bias: Endogenous response department lower enrollment rate give better gradeNegative bias: Endogenous response department lower enrollment rate give better gradePositive bias: hard science already best students (.e., ability), don’t grades can even lowerPositive bias: hard science already best students (.e., ability), don’t grades can even lowerUnder dif-n-dif\\[\nY_{idt} = \\beta_0 + POST_t \\beta_1 + Treat_d \\beta_2 + (POST_t \\times Treat_d)\\beta_3 + X_{idt} + \\epsilon_{idt}\n\\]\\(Y_{idt}\\) = grade averageA general specification dif-n-dif \\[\nY_{idt} = \\alpha_0 + (POST_t \\times Treat_d) \\alpha_1 + \\theta_d + \\delta_t + X_{idt} + u_{idt}\n\\]\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)","code":""},{"path":"difference-in-differences.html","id":"one-difference","chapter":"26 Difference-in-differences","heading":"26.6 One Difference","text":"regression formula follows (Liaukonytė, Tuchman, Zhu 2023):\\[\ny_{ut} = \\beta \\text{Post}_t + \\gamma_u + \\gamma_w(t) + \\gamma_l + \\gamma_g(u)p(t) + \\epsilon_{ut}\n\\]\\(y_{ut}\\): Outcome interest unit u time t.\\(\\text{Post}_t\\): Dummy variable representing specific post-event period.\\(\\beta\\): Coefficient measuring average change outcome event relative pre-period.\\(\\gamma_u\\): Fixed effects unit.\\(\\gamma_w(t)\\): Time-specific fixed effects account periodic variations.\\(\\gamma_l\\): Dummy variable specific significant period (e.g., major event change).\\(\\gamma_g(u)p(t)\\): Group x period fixed effects flexible trends may vary across different categories (e.g., geographical regions) periods.\\(\\epsilon_{ut}\\): Error term.model can used analyze impact event outcome interest controlling various fixed effects time-specific variations, using units pre-treatment controls.","code":""},{"path":"difference-in-differences.html","id":"two-way-fixed-effects","chapter":"26 Difference-in-differences","heading":"26.7 Two-way Fixed-effects","text":"generalization dif-n-dif model two-way fixed-effects models multiple groups time effects. designed-based, non-parametric causal estimator (Imai Kim 2021)applying TWFE multiple groups multiple periods, supposedly causal coefficient weighted average two-group/two-period estimators data weights can negative. specifically, weights proportional group sizes treatment indicator’s variation pair, units middle panel highest weight.canonical/standard TWFE works whenEffects homogeneous across units across time periods (.e., dynamic changes effects treatment). See (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021) details. Similarly, relies assumption linear additive effects (Imai Kim 2021)\nargue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.\nNeed manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.\nTreatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).\nEffects homogeneous across units across time periods (.e., dynamic changes effects treatment). See (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021) details. Similarly, relies assumption linear additive effects (Imai Kim 2021)argue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.argue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.Need manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.Need manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.Treatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).Treatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).2 time periods.2 time periods.Within setting, TWFE works , using baseline (e.g., control units treatment status unchanged across time periods), comparison can beGood \nNewly treated units vs. control\nNewly treated units vs -yet treated\nGood forNewly treated units vs. controlNewly treated units vs. controlNewly treated units vs -yet treatedNewly treated units vs -yet treatedBad \nNewly treated vs. already treated (already treated serve potential outcome newly treated).\nStrict exogeneity (.e., time-varying confounders, feedback past outcome treatment) (Imai Kim 2019)\nSpecific functional forms (.e., treatment effect homogeneity carryover effects anticipation effects) (Imai Kim 2019)\nBad forNewly treated vs. already treated (already treated serve potential outcome newly treated).Strict exogeneity (.e., time-varying confounders, feedback past outcome treatment) (Imai Kim 2019)Specific functional forms (.e., treatment effect homogeneity carryover effects anticipation effects) (Imai Kim 2019)Note: Notation section consistent (2020)\\[\nY_{} = \\alpha_i + \\lambda_t + \\tau W_{} + \\beta X_{} + \\epsilon_{}\n\\]\\(Y_{}\\) outcome\\(Y_{}\\) outcome\\(\\alpha_i\\) unit FE\\(\\alpha_i\\) unit FE\\(\\lambda_t\\) time FE\\(\\lambda_t\\) time FE\\(\\tau\\) causal effect treatment\\(\\tau\\) causal effect treatment\\(W_{}\\) treatment indicator\\(W_{}\\) treatment indicator\\(X_{}\\) covariates\\(X_{}\\) covariatesWhen \\(T = 2\\), TWFE traditional modelUnder following assumption, \\(\\hat{\\tau}_{OLS}\\) unbiased:homogeneous treatment effectparallel trends assumptionslinear additive effects (Imai Kim 2021)Remedies TWFE’s shortcomings(Goodman-Bacon 2021): diagnostic robustness tests TWFE identify influential observations estimate (Goodman-Bacon Decomposition)(Goodman-Bacon 2021): diagnostic robustness tests TWFE identify influential observations estimate (Goodman-Bacon Decomposition)(Callaway Sant’Anna 2021): 2-step estimation bootstrap procedure can account autocorrelation clustering,\nparameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)\nComparing post-treatment outcomes fo groups treated period similar group never treated (using matching).\nTreatment status switch (treated, stay treated rest panel)\nPackage: \n(Callaway Sant’Anna 2021): 2-step estimation bootstrap procedure can account autocorrelation clustering,parameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)parameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)Comparing post-treatment outcomes fo groups treated period similar group never treated (using matching).Comparing post-treatment outcomes fo groups treated period similar group never treated (using matching).Treatment status switch (treated, stay treated rest panel)Treatment status switch (treated, stay treated rest panel)Package: didPackage: (L. Sun Abraham 2021): specialization (Callaway Sant’Anna 2021) event-study context.\ninclude lags leads design\ncohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)\npropose “interaction-weighted” estimator.\nPackage: fixest\n(L. Sun Abraham 2021): specialization (Callaway Sant’Anna 2021) event-study context.include lags leads designThey include lags leads designhave cohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)cohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)propose “interaction-weighted” estimator.propose “interaction-weighted” estimator.Package: fixestPackage: fixest(Imai Kim 2021)\nDifferent (Callaway Sant’Anna 2021) allow units switch treatment.\nBased matching methods, weighted TWFE\nPackage: wfe PanelMatch\n(Imai Kim 2021)Different (Callaway Sant’Anna 2021) allow units switch treatment.Different (Callaway Sant’Anna 2021) allow units switch treatment.Based matching methods, weighted TWFEBased matching methods, weighted TWFEPackage: wfe PanelMatchPackage: wfe PanelMatch(Gardner 2022): two-stage \ndid2s\n(Gardner 2022): two-stage DiDdid2sIn cases unaffected unit (.e., never-treated), using exposure-adjusted difference--differences estimators can recover average treatment effect (Clément De Chaisemartin d’Haultfoeuille 2020). However, want see treatment effect heterogeneity (cases true heterogeneous treatment effects vary exposure rate), exposure-adjusted still fails (L. Sun Shapiro 2022).cases unaffected unit (.e., never-treated), using exposure-adjusted difference--differences estimators can recover average treatment effect (Clément De Chaisemartin d’Haultfoeuille 2020). However, want see treatment effect heterogeneity (cases true heterogeneous treatment effects vary exposure rate), exposure-adjusted still fails (L. Sun Shapiro 2022).(2020): see (2020): see belowTo robust againsttime- unit-varying effectsWe can use reshaped inverse probability weighting (RIPW)- TWFE estimatorWith following assumptions:SUTVASUTVABinary treatment: \\(\\mathbf{W}_i = (W_{i1}, \\dots, W_{})\\) \\(\\mathbf{W}_i \\sim \\mathbf{\\pi}_i\\) generalized propensity score (.e., person treatment likelihood follow \\(\\pi\\) regardless period)Binary treatment: \\(\\mathbf{W}_i = (W_{i1}, \\dots, W_{})\\) \\(\\mathbf{W}_i \\sim \\mathbf{\\pi}_i\\) generalized propensity score (.e., person treatment likelihood follow \\(\\pi\\) regardless period), unit-time specific effect \\(\\tau_{} = Y_{}(1) - Y_{}(0)\\)Doubly Average Treatment Effect (DATE) \\[\n\\tau(\\xi) = \\sum_{T=1}^T \\xi_t \\left(\\frac{1}{n} \\sum_{= 1}^n \\tau_{} \\right)\n\\]\\(\\frac{1}{n} \\sum_{= 1}^n \\tau_{}\\) unweighted effect treatment across units (.e., time-specific ATE).\\(\\frac{1}{n} \\sum_{= 1}^n \\tau_{}\\) unweighted effect treatment across units (.e., time-specific ATE).\\(\\xi = (\\xi_1, \\dots, \\xi_t)\\) user-specific weights time period.\\(\\xi = (\\xi_1, \\dots, \\xi_t)\\) user-specific weights time period.estimand called DATE ’s weighted (averaged) across time units.estimand called DATE ’s weighted (averaged) across time units.special case DATE time unit-weights equal\\[\n\\tau_{eq} = \\frac{1}{nT} \\sum_{t=1}^T \\sum_{= 1}^n \\tau_{}\n\\]Borrowing idea inverse propensity-weighted least squares estimator cross-sectional case reweight objective function via treatment assignment mechanism:\\[\n\\hat{\\tau} \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n (Y_i -\\mu - W_i \\tau)^2 \\frac{1}{\\pi_i (W_i)}\n\\]wherethe first term least squares objectivethe first term least squares objectivethe second term propensity scorethe second term propensity scoreIn panel data case, IPW estimator \\[\n\\hat{\\tau}_{IPW} \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n \\sum_{t =1}^T (Y_{t}-\\alpha_i - \\lambda_t - W_{} \\tau)^2 \\frac{1}{\\pi_i (W_i)}\n\\], DATE users can specify structure time weight, use reshaped IPW estimator (2020)\\[\n\\hat{\\tau}_{RIPW} (\\Pi) \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n \\sum_{t =1}^T (Y_{t}-\\alpha_i - \\lambda_t - W_{} \\tau)^2 \\frac{\\Pi(W_i)}{\\pi_i (W_i)}\n\\]’s function data-independent distribution \\(\\Pi\\) depends support treatment path \\(\\mathbb{S} = \\cup_i Supp(W_i)\\)generalization can transform toIPW-TWFE estimator \\(\\Pi \\sim Unif(\\mathbb{S})\\)IPW-TWFE estimator \\(\\Pi \\sim Unif(\\mathbb{S})\\)randomized experiment \\(\\Pi = \\pi_i\\)randomized experiment \\(\\Pi = \\pi_i\\)choose \\(\\Pi\\), don’t need data, just need possible assignments setting.practical problems (, staggered, transient), closed form solutionsFor practical problems (, staggered, transient), closed form solutionsFor generic solver, can use nonlinear programming (e..g, BFGS algorithm)generic solver, can use nonlinear programming (e..g, BFGS algorithm)argued (Imai Kim 2021) TWFE non-parametric approach, can subjected incorrect model assumption (.e., model dependence).Hence, advocate matching methods time-series cross-sectional data (Imai Kim 2021)Hence, advocate matching methods time-series cross-sectional data (Imai Kim 2021)Use wfe PanelMatch apply paper.Use wfe PanelMatch apply paper.package based (Somaini Wolak 2016)Standard errors estimation optionsAlternatively, can also manually plm package, careful SEs estimatedAs can see, differences stem SE estimation, coefficient estimate.","code":"\n# dataset\nlibrary(bacondecomp)\ndf <- bacondecomp::castle\n# devtools::install_github(\"paulosomaini/xtreg2way\")\n\nlibrary(xtreg2way)\n# output <- xtreg2way(y,\n#                     data.frame(x1, x2),\n#                     iid,\n#                     tid,\n#                     w,\n#                     noise = \"1\",\n#                     se = \"1\")\n\n# equilvalently\noutput <- xtreg2way(l_homicide ~ post,\n                    df,\n                    iid = df$state, # group id\n                    tid = df$year, # time id\n                    # w, # vector of weight\n                    se = \"1\")\noutput$betaHat\n#>                  [,1]\n#> l_homicide 0.08181162\noutput$aVarHat\n#>             [,1]\n#> [1,] 0.003396724\n\n# to save time, you can use your structure in the \n# last output for a new set of variables\n# output2 <- xtreg2way(y, x1, struc=output$struc)\nlibrary(multiwayvcov) # get vcov matrix \nlibrary(lmtest) # robust SEs estimation\n\n# manual\noutput3 <- lm(l_homicide ~ post + factor(state) + factor(year),\n              data = df)\n\n# get variance-covariance matrix\nvcov_tw <- multiwayvcov::cluster.vcov(output3,\n                        cbind(df$state, df$year),\n                        use_white = F,\n                        df_correction = F)\n\n# get coefficients\ncoeftest(output3, vcov_tw)[2,] \n#>   Estimate Std. Error    t value   Pr(>|t|) \n#> 0.08181162 0.05671410 1.44252696 0.14979397\n# using the plm package\nlibrary(plm)\n\noutput4 <- plm(l_homicide ~ post, \n               data = df, \n               index = c(\"state\", \"year\"), \n               model = \"within\", \n               effect = \"twoways\")\n\n# get coefficients\ncoeftest(output4, vcov = vcovHC, type = \"HC1\")\n#> \n#> t test of coefficients:\n#> \n#>      Estimate Std. Error t value Pr(>|t|)\n#> post 0.081812   0.057748  1.4167   0.1572"},{"path":"difference-in-differences.html","id":"multiple-periods-and-variation-in-treatment-timing","chapter":"26 Difference-in-differences","heading":"26.8 Multiple periods and variation in treatment timing","text":"extension framework settings havemore 2 time periodsmore 2 time periodsdifferent treatment timingdifferent treatment timingWhen treatment effects heterogeneous across time units, standard Two-way Fixed-effects inappropriate.Notation consistent package (Callaway Sant’Anna 2021)\\(Y_{}(0)\\) potential outcome unit \\(\\)\\(Y_{}(0)\\) potential outcome unit \\(\\)\\(Y_{}(g)\\) potential outcome unit \\(\\) time period \\(t\\) ’s treated period \\(g\\)\\(Y_{}(g)\\) potential outcome unit \\(\\) time period \\(t\\) ’s treated period \\(g\\)\\(Y_{}\\) observed outcome unit \\(\\) time period \\(t\\)\\(Y_{}\\) observed outcome unit \\(\\) time period \\(t\\)\\[\nY_{} =\n\\begin{cases}\nY_{} = Y_{}(0) & \\forall \\\\text{never-treated group} \\\\\nY_{} = 1\\{G_i > t\\} Y_{}(0) +  1\\{G_i \\le t \\}Y_{}(G_i) & \\forall \\\\text{groups}\n\\end{cases}\n\\]\\(G_i\\) time period \\(\\) treated\\(G_i\\) time period \\(\\) treated\\(C_i\\) dummy \\(\\) belongs never-treated group\\(C_i\\) dummy \\(\\) belongs never-treated group\\(D_{}\\) dummy whether \\(\\) treated period \\(t\\)\\(D_{}\\) dummy whether \\(\\) treated period \\(t\\)Assumptions:Staggered treatment adoption: treated, unit untreated (revert)Staggered treatment adoption: treated, unit untreated (revert)Parallel trends assumptions (conditional covariates):\nBased never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)\nWithout treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.\n\nBased -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)\n-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)\nAdditional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)\n\nParallel trends assumptions (conditional covariates):Based never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)\nWithout treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.\nBased never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)Without treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.Based -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)\n-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)\nAdditional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)\nBased -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)Additional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)Additional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)Random samplingRandom samplingIrreversibility treatment (treated, untreated)Irreversibility treatment (treated, untreated)Overlap (treatment propensity \\(e \\[0,1]\\))Overlap (treatment propensity \\(e \\[0,1]\\))Group-Time ATEThis equivalent average treatment effect standard case (2 groups, 2 periods) multiple time periods.\\[\nATT(g,t) = E[Y_t(g) - Y_t(0) |G = g]\n\\]average treatment effect group \\(g\\) period \\(t\\)Identification: parallel trends assumption based \nNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)\n-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)\nIdentification: parallel trends assumption based onNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)Identification: parallel trends assumption holds conditional covariates based \nNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)\n-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)\nplausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).\nIdentification: parallel trends assumption holds conditional covariates based onNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)plausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).plausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).Possible parameters interest :Average treatment effect per group\\[\n\\theta_S(g) = \\frac{1}{\\tau - g + 1} \\sum_{t = 2}^\\tau \\mathbb{1} \\{ \\le t \\} ATT(g,t)\n\\]Average treatment effect across groups (treated) (similar average treatment effect treated canonical case)\\[\n\\theta_S^O := \\sum_{g=2}^\\tau \\theta_S(g) P(G=g)\n\\]Average treatment effect dynamics (.e., average treatment effect groups exposed treatment \\(e\\) time periods):\\[\n\\theta_D(e) := \\sum_{g=2}^\\tau \\mathbb{1} \\{g + e \\le \\tau \\}ATT(g,g + e) P(G = g|G + e \\le \\tau)\n\\]Average treatment effect period \\(t\\) groups treated period \\(t\\))\\[\n\\theta_C(t) = \\sum_{g=2}^\\tau \\mathbb{1}\\{g \\le t\\} ATT(g,t) P(G = g|g \\le t)\n\\]Average treatment effect calendar time\\[\n\\theta_C = \\frac{1}{\\tau-1}\\sum_{t=2}^\\tau \\theta_C(t)\n\\]","code":""},{"path":"difference-in-differences.html","id":"staggered-dif-n-dif","chapter":"26 Difference-in-differences","heading":"26.9 Staggered Dif-n-dif","text":"See Wing et al. (2024) checklist.Recommendations Baker, Larcker, Wang (2022)TWFE regressions suitable single treatment periods treatment effects homogeneous, provided ’s solid rationale effect homogeneity.TWFE regressions suitable single treatment periods treatment effects homogeneous, provided ’s solid rationale effect homogeneity.TWFE staggered , researchers evaluate bias risks, plot treatment timings check variations, use decompositions like Goodman-Bacon (2021) possible. decompositions aren’t feasible (e.g., unbalanced panel), percentage never-treated units can indicate bias severity. Expected treatment effect variability also discussed.TWFE staggered , researchers evaluate bias risks, plot treatment timings check variations, use decompositions like Goodman-Bacon (2021) possible. decompositions aren’t feasible (e.g., unbalanced panel), percentage never-treated units can indicate bias severity. Expected treatment effect variability also discussed.TWFE staggered event studies, avoid binning time periods without evidence uniform effects. Use full relative-time indicators, justify reference periods, wary multicollinearity causing bias.TWFE staggered event studies, avoid binning time periods without evidence uniform effects. Use full relative-time indicators, justify reference periods, wary multicollinearity causing bias.address treatment timing bias concerns, use alternative estimators like stacked regressions, L. Sun Abraham (2021), Callaway Sant’Anna (2021), separate regressions event “clean” controls.address treatment timing bias concerns, use alternative estimators like stacked regressions, L. Sun Abraham (2021), Callaway Sant’Anna (2021), separate regressions event “clean” controls.Justify selection comparison groups (-yet treated, last treated, never treated) ensure parallel-trends assumption holds, especially anticipating effects certain groups.Justify selection comparison groups (-yet treated, last treated, never treated) ensure parallel-trends assumption holds, especially anticipating effects certain groups.Notes:subjects treated different point time (variation treatment timing across units), use staggered (also known event study dynamic ).design treatment applied units exposed treatment time afterward, see (Athey Imbens 2022)example, basic design (Stevenson Wolfers 2006)\\[\n\\begin{aligned}\nY_{} &= \\sum_k \\beta_k Treatment_{}^k + \\sum_i \\eta_i  State_i \\\\\n&+ \\sum_t \\lambda_t Year_t + Controls_{} + \\epsilon_{}\n\\end{aligned}\n\\]\\(Treatment_{}^k\\) series dummy variables equal 1 state \\(\\) treated \\(k\\) years ago period \\(t\\)\\(Treatment_{}^k\\) series dummy variables equal 1 state \\(\\) treated \\(k\\) years ago period \\(t\\)SE usually clustered group level (occasionally time level).SE usually clustered group level (occasionally time level).avoid collinearity, period right treatment usually chosen drop.avoid collinearity, period right treatment usually chosen drop.general form TWFE (L. Sun Abraham 2021):First, define relative period bin indicator \\[\nD_{}^l = \\mathbf{1}(t - E_i = l)\n\\]’s indicator function unit \\(\\) \\(l\\) periods first treatment time \\(t\\)Static specification\\[\nY_{} = \\alpha_i + \\lambda_t + \\mu_g \\sum_{l \\ge0} D_{}^l + \\epsilon_{}\n\\]\\(\\alpha_i\\) unit FE\\(\\alpha_i\\) unit FE\\(\\lambda_t\\) time FE\\(\\lambda_t\\) time FE\\(\\mu_g\\) coefficient interest \\(g = [0,T)\\)\\(\\mu_g\\) coefficient interest \\(g = [0,T)\\)exclude periods first adoption.exclude periods first adoption.Dynamic specification\\[\nY_{} = \\alpha_i + \\lambda_t + \\sum_{\\substack{l = -K \\\\ l \\neq -1}}^{L} \\mu_l D_{}^l + \\epsilon_{}\n\\]exclude relative periods avoid multicollinearity problem (e.g., either period right treatment, treatment period).setting, try show treatment control groups statistically different (.e., coefficient estimates treatment different 0) show pre-treatment parallel trends.However, two-way fixed effects design criticized L. Sun Abraham (2021); Callaway Sant’Anna (2021); Goodman-Bacon (2021). researchers include leads lags treatment see long-term effects treatment, leads lags can biased effects periods, pre-trends can falsely arise due treatment effects heterogeneity.Applying new proposed method, finance accounting researchers find many cases, causal estimates turn null (Baker, Larcker, Wang 2022).Assumptions Staggered DIDRollout Exogeneity (.e., exogeneity treatment adoption): treatment randomly implemented time (.e., unrelated variables also affect dependent variables)\nEvidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)\nEvidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\n\nRollout Exogeneity (.e., exogeneity treatment adoption): treatment randomly implemented time (.e., unrelated variables also affect dependent variables)Evidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)Evidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .Treatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .confounding eventsNo confounding eventsExclusion restrictions\n-anticipation assumption: future treatment time affect current outcomes\nInvariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.\nExclusion restrictionsNo-anticipation assumption: future treatment time affect current outcomesNo-anticipation assumption: future treatment time affect current outcomesInvariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.Invariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.assumptions listed Multiple periods variation treatment timingAnd assumptions listed Multiple periods variation treatment timingAuxiliary assumptions:\nConstant treatment effects across units\nConstant treatment effect time\nRandom sampling\nEffect Additivity\nAuxiliary assumptions:Constant treatment effects across unitsConstant treatment effects across unitsConstant treatment effect timeConstant treatment effect timeRandom samplingRandom samplingEffect AdditivityEffect AdditivityRemedies staggered (Baker, Larcker, Wang 2022):treated cohort compared appropriate controls (-yet-treated, never-treated)\n(Goodman-Bacon 2021)\n(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)\n(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))\n\n(Clément De Chaisemartin d’Haultfoeuille 2020)\n(Borusyak, Jaravel, Spiess 2021)\ntreated cohort compared appropriate controls (-yet-treated, never-treated)(Goodman-Bacon 2021)(Goodman-Bacon 2021)(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)\n(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))\n(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))(Clément De Chaisemartin d’Haultfoeuille 2020)(Clément De Chaisemartin d’Haultfoeuille 2020)(Borusyak, Jaravel, Spiess 2021)(Borusyak, Jaravel, Spiess 2021)Stacked (biased simple):\n(Gormley Matsa 2011)\n(Cengiz et al. 2019)\n(Deshpande Li 2019)\nStacked (biased simple):(Gormley Matsa 2011)(Gormley Matsa 2011)(Cengiz et al. 2019)(Cengiz et al. 2019)(Deshpande Li 2019)(Deshpande Li 2019)","code":""},{"path":"difference-in-differences.html","id":"stacked-did","chapter":"26 Difference-in-differences","heading":"26.9.1 Stacked DID","text":"Notations following slides\\[\nY_{} = \\beta_{FE} D_{} + A_i + B_t + \\epsilon_{}\n\\]\\(A_i\\) group fixed effects\\(A_i\\) group fixed effects\\(B_t\\) period fixed effects\\(B_t\\) period fixed effectsStepsChoose Event WindowEnumerate Sub-experimentsDefine Inclusion CriteriaStack DataSpecify Estimating EquationEvent WindowLet\\(\\kappa_a\\) length pre-event window\\(\\kappa_a\\) length pre-event window\\(\\kappa_b\\) length post-event window\\(\\kappa_b\\) length post-event windowBy setting common event window analysis, essentially exclude events meet criteria.Sub-experimentsLet \\(T_1\\) earliest period dataset\\(T_T\\) last period datasetThen, collection policy adoption periods event window \\[\n\\Omega_A = \\{ A_i |T_1 + \\kappa_a \\le A_i \\le T_T - \\kappa_b\\}\n\\]events existat least \\(\\kappa_a\\) periods earliest periodat least \\(\\kappa_a\\) periods earliest periodat least \\(\\kappa_b\\) periods last periodat least \\(\\kappa_b\\) periods last periodLet \\(d = 1, \\dots, D\\) index column sub-experiments \\(\\Omega_A\\)\\(\\omega_d\\) event date d-th sub-experiment (e.g., \\(\\omega_1\\) = adoption date 1st experiment)Inclusion CriteriaValid treated Units\nWithin sub-experiment \\(d\\), treated units adoption date\nmakes sure unit can serve treated unit 1 sub-experiment\nWithin sub-experiment \\(d\\), treated units adoption dateWithin sub-experiment \\(d\\), treated units adoption dateThis makes sure unit can serve treated unit 1 sub-experimentThis makes sure unit can serve treated unit 1 sub-experimentClean controls\nunits satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment d\nensures controls \nnever treated units\nunits treated far future\n\nunit can control unit multiple sub-experiments (need correct SE)\nunits satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment dOnly units satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment dThis ensures controls \nnever treated units\nunits treated far future\nensures controls onlynever treated unitsnever treated unitsunits treated far futureunits treated far futureBut unit can control unit multiple sub-experiments (need correct SE)unit can control unit multiple sub-experiments (need correct SE)Valid Time Periods\nobservations within sub-experiment d time periods within sub-experiment’s event window\nensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) included\nobservations within sub-experiment d time periods within sub-experiment’s event windowAll observations within sub-experiment d time periods within sub-experiment’s event windowThis ensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) includedThis ensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) includedStack DataEstimating Equation\\[\nY_{itd} = \\beta_0 + \\beta_1 T_{id} + \\beta_2 P_{td} + \\beta_3 (T_{id} \\times P_{td}) + \\epsilon_{itd}\n\\]\\(T_{id}\\) = 1 unit \\(\\) treated sub-experiment \\(d\\), 0 control\\(T_{id}\\) = 1 unit \\(\\) treated sub-experiment \\(d\\), 0 control\\(P_{td}\\) = 1 ’s period treatment sub-experiment \\(d\\)\\(P_{td}\\) = 1 ’s period treatment sub-experiment \\(d\\)Equivalently,\\[\nY_{itd} = \\beta_3 (T_{id} \\times P_{td}) + \\theta_{id} + \\gamma_{td} + \\epsilon_{itd}\n\\]\\(\\beta_3\\) averages time-varying effects single number (can’t see time-varying effects)Stacked Event StudyLet \\(YSE_{td} = t - \\omega_d\\) “time since event” variable sub-experiment \\(d\\), \\(YSE_{td} = -\\kappa_a, \\dots, 0, \\dots, \\kappa_b\\) every sub-experimentIn sub-experiment, can fit\\[\nY_{}^d = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j^d \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j^d (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_i^d + \\epsilon_{}^d\n\\]Different set event study coefficients sub-experiment\\[\nY_{itd} = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_{id} + \\epsilon_{itd}\n\\]ClusteringClustered unit x sub-experiment level (Cengiz et al. 2019)Clustered unit x sub-experiment level (Cengiz et al. 2019)Clustered unit level (Deshpande Li 2019)Clustered unit level (Deshpande Li 2019)","code":"\nlibrary(did)\nlibrary(tidyverse)\nlibrary(fixest)\n\ndata(base_stagg)\n\n# first make the stacked datasets\n# get the treatment cohorts\ncohorts <- base_stagg %>%\n    select(year_treated) %>%\n    # exclude never-treated group\n    filter(year_treated != 10000) %>%\n    unique() %>%\n    pull()\n\n# make formula to create the sub-datasets\ngetdata <- function(j, window) {\n    #keep what we need\n    base_stagg %>%\n        # keep treated units and all units not treated within -5 to 5\n        # keep treated units and all units not treated within -window to window\n        filter(year_treated == j | year_treated > j + window) %>%\n        # keep just year -window to window\n        filter(year >= j - window & year <= j + window) %>%\n        # create an indicator for the dataset\n        mutate(df = j)\n}\n\n# get data stacked\nstacked_data <- map_df(cohorts, ~ getdata(., window = 5)) %>%\n    mutate(rel_year = if_else(df == year_treated, time_to_treatment, NA_real_)) %>%\n    fastDummies::dummy_cols(\"rel_year\", ignore_na = TRUE) %>%\n    mutate(across(starts_with(\"rel_year_\"), ~ replace_na(., 0)))\n\n# get stacked value\nstacked <-\n    feols(\n        y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` +\n            `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 +\n            rel_year_4 + rel_year_5 |\n            id ^ df + year ^ df,\n        data = stacked_data\n    )$coefficients\n\nstacked_se = feols(\n    y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` +\n        `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 +\n        rel_year_4 + rel_year_5 |\n        id ^ df + year ^ df,\n    data = stacked_data\n)$se\n\n# add in 0 for omitted -1\nstacked <- c(stacked[1:4], 0, stacked[5:10])\nstacked_se <- c(stacked_se[1:4], 0, stacked_se[5:10])\n\n\ncs_out <- att_gt(\n    yname = \"y\",\n    data = base_stagg,\n    gname = \"year_treated\",\n    idname = \"id\",\n    # xformla = \"~x1\",\n    tname = \"year\"\n)\ncs <-\n    aggte(\n        cs_out,\n        type = \"dynamic\",\n        min_e = -5,\n        max_e = 5,\n        bstrap = FALSE,\n        cband = FALSE\n    )\n\n\n\nres_sa20 = feols(y ~ sunab(year_treated, year) |\n                     id + year, base_stagg)\nsa = tidy(res_sa20)[5:14, ] %>% pull(estimate)\nsa = c(sa[1:4], 0, sa[5:10])\n\nsa_se = tidy(res_sa20)[6:15, ] %>% pull(std.error)\nsa_se = c(sa_se[1:4], 0, sa_se[5:10])\n\ncompare_df_est = data.frame(\n    period = -5:5,\n    cs = cs$att.egt,\n    sa = sa,\n    stacked = stacked\n)\n\ncompare_df_se = data.frame(\n    period = -5:5,\n    cs = cs$se.egt,\n    sa = sa_se,\n    stacked = stacked_se\n)\n\ncompare_df_longer <- compare_df_est %>%\n    pivot_longer(!period, names_to = \"estimator\", values_to = \"est\") %>%\n    \n    full_join(compare_df_se %>% \n                  pivot_longer(!period, names_to = \"estimator\", values_to = \"se\")) %>%\n    \n    mutate(upper = est +  1.96 * se,\n           lower = est - 1.96 * se)\n\n\nggplot(compare_df_longer) +\n    geom_ribbon(aes(\n        x = period,\n        ymin = lower,\n        ymax = upper,\n        group = estimator\n    )) +\n    geom_line(aes(\n        x = period,\n        y = est,\n        group = estimator,\n        col = estimator\n    ),\n    linewidth = 1) + \n    causalverse::ama_theme()"},{"path":"difference-in-differences.html","id":"goodman-bacon-decomposition","chapter":"26 Difference-in-differences","heading":"26.9.2 Goodman-Bacon Decomposition","text":"Paper: (Goodman-Bacon 2021)excellent explanation slides author, seeTakeaways:pairwise (\\(\\tau\\)) gets weight change close middle study windowA pairwise (\\(\\tau\\)) gets weight change close middle study windowA pairwise (\\(\\tau\\)) gets weight includes observations.pairwise (\\(\\tau\\)) gets weight includes observations.Code bacondecomp vignetteTwo-way Fixed effect estimateHence, naive TWFE fixed effect equals weighted average Bacon decomposition (= 0.08).time-varying controls can identify variation within-treatment timing group, ”early vs. late” “late vs. early” estimates collapse just one estimate (.e., treated).","code":"\nlibrary(bacondecomp)\nlibrary(tidyverse)\ndata(\"castle\")\ncastle <- bacondecomp::castle %>% \n    dplyr::select(\"l_homicide\", \"post\", \"state\", \"year\")\nhead(castle)\n#>   l_homicide post   state year\n#> 1   2.027356    0 Alabama 2000\n#> 2   2.164867    0 Alabama 2001\n#> 3   1.936334    0 Alabama 2002\n#> 4   1.919567    0 Alabama 2003\n#> 5   1.749841    0 Alabama 2004\n#> 6   2.130440    0 Alabama 2005\n\n\ndf_bacon <- bacon(\n    l_homicide ~ post,\n    data = castle,\n    id_var = \"state\",\n    time_var = \"year\"\n)\n#>                       type  weight  avg_est\n#> 1 Earlier vs Later Treated 0.05976 -0.00554\n#> 2 Later vs Earlier Treated 0.03190  0.07032\n#> 3     Treated vs Untreated 0.90834  0.08796\n\n# weighted average of the decomposition\nsum(df_bacon$estimate * df_bacon$weight)\n#> [1] 0.08181162\nlibrary(broom)\nfit_tw <- lm(l_homicide ~ post + factor(state) + factor(year),\n             data = bacondecomp::castle)\nhead(tidy(fit_tw))\n#> # A tibble: 6 × 5\n#>   term                    estimate std.error statistic   p.value\n#>   <chr>                      <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)               1.95      0.0624    31.2   2.84e-118\n#> 2 post                      0.0818    0.0317     2.58  1.02e-  2\n#> 3 factor(state)Alaska      -0.373     0.0797    -4.68  3.77e-  6\n#> 4 factor(state)Arizona      0.0158    0.0797     0.198 8.43e-  1\n#> 5 factor(state)Arkansas    -0.118     0.0810    -1.46  1.44e-  1\n#> 6 factor(state)California  -0.108     0.0810    -1.34  1.82e-  1\nlibrary(ggplot2)\n\nggplot(df_bacon) +\n    aes(\n        x = weight,\n        y = estimate,\n        # shape = factor(type),\n        color = type\n    ) +\n    labs(x = \"Weight\", y = \"Estimate\", shape = \"Type\") +\n    geom_point() +\n    causalverse::ama_theme()"},{"path":"difference-in-differences.html","id":"did-with-in-and-out-treatment-condition","chapter":"26 Difference-in-differences","heading":"26.9.3 DID with in and out treatment condition","text":"","code":""},{"path":"difference-in-differences.html","id":"panel-match","chapter":"26 Difference-in-differences","heading":"26.9.3.1 Panel Match","text":"Imai Kim (2021)case generalizes staggered adoption setting, allowing units vary treatment time. \\(N\\) units across \\(T\\) time periods (potentially unbalanced panels), let \\(X_{}\\) represent treatment \\(Y_{}\\) outcome unit \\(\\) time \\(t\\). use two-way linear fixed effects model:\\[\nY_{} = \\alpha_i + \\gamma_t + \\beta X_{} + \\epsilon_{}\n\\]\\(= 1, \\dots, N\\) \\(t = 1, \\dots, T\\). , \\(\\alpha_i\\) \\(\\gamma_t\\) unit time fixed effects. capture time-invariant unit-specific unit-invariant time-specific unobserved confounders, respectively. can express \\(\\alpha_i = h(\\mathbf{U}_i)\\) \\(\\gamma_t = f(\\mathbf{V}_t)\\), \\(\\mathbf{U}_i\\) \\(\\mathbf{V}_t\\) confounders. model doesn’t assume specific form \\(h(.)\\) \\(f(.)\\), ’re additive separable given binary treatment.least squares estimate \\(\\beta\\) leverages covariance outcome treatment (Imai Kim 2021, 406). Specifically, uses within-unit within-time variations. Many researchers prefer two fixed effects (2FE) estimator adjusts types unobserved confounders without specific functional-form assumptions, wrong (Imai Kim 2019). need functional-form assumption (.e., linearity assumption) 2FE work (Imai Kim 2021, 406)Two-Way Matching Estimator:\ncan lead mismatches; units treatment status get matched estimating counterfactual outcomes.\nObservations need matched opposite treatment status correct causal effects estimation.\nMismatches can cause attenuation bias.\n2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.\nTwo-Way Matching Estimator:can lead mismatches; units treatment status get matched estimating counterfactual outcomes.can lead mismatches; units treatment status get matched estimating counterfactual outcomes.Observations need matched opposite treatment status correct causal effects estimation.Observations need matched opposite treatment status correct causal effects estimation.Mismatches can cause attenuation bias.Mismatches can cause attenuation bias.2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.Weighting 2FE:\nObservation \\((,t)\\) weighted based often acts control unit.\nweighted 2FE estimator still mismatches, fewer standard 2FE estimator.\nAdjustments made based observations neither belong unit time period matched observation.\nmeans challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.\nWeighting 2FE:Observation \\((,t)\\) weighted based often acts control unit.Observation \\((,t)\\) weighted based often acts control unit.weighted 2FE estimator still mismatches, fewer standard 2FE estimator.weighted 2FE estimator still mismatches, fewer standard 2FE estimator.Adjustments made based observations neither belong unit time period matched observation.Adjustments made based observations neither belong unit time period matched observation.means challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.means challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.Equivalence & Assumptions:\nEquivalence 2FE estimator estimator dependent linearity assumption.\nmulti-period estimator described average two-time-period, two-group estimators applied changes control treatment.\nEquivalence & Assumptions:Equivalence 2FE estimator estimator dependent linearity assumption.Equivalence 2FE estimator estimator dependent linearity assumption.multi-period estimator described average two-time-period, two-group estimators applied changes control treatment.multi-period estimator described average two-time-period, two-group estimators applied changes control treatment.Comparison :\nsimple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.\ndoesn’t hold multi-period designs units change treatment status multiple times different intervals.\nContrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.\nmulti-period can equivalent weighted 2FE, control observations may negative regression weights.\nComparison :simple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.simple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.doesn’t hold multi-period designs units change treatment status multiple times different intervals.doesn’t hold multi-period designs units change treatment status multiple times different intervals.Contrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.Contrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.multi-period can equivalent weighted 2FE, control observations may negative regression weights.multi-period can equivalent weighted 2FE, control observations may negative regression weights.Conclusion:\nJustifying 2FE estimator estimator isn’t warranted without imposing linearity assumption.\nConclusion:Justifying 2FE estimator estimator isn’t warranted without imposing linearity assumption.Application (Imai, Kim, Wang 2021)Matching Methods:\nEnhance validity causal inference.\nReduce model dependence provide intuitive diagnostics (Ho et al. 2007)\nRarely utilized analyzing time series cross-sectional data.\nproposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specified\nBetter synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.\nMatching Methods:Enhance validity causal inference.Enhance validity causal inference.Reduce model dependence provide intuitive diagnostics (Ho et al. 2007)Reduce model dependence provide intuitive diagnostics (Ho et al. 2007)Rarely utilized analyzing time series cross-sectional data.Rarely utilized analyzing time series cross-sectional data.proposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specifiedThe proposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specifiedBetter synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.Better synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.Notes:\nPotential carryover effects (treatment may long-term effect), leading post-treatment bias.\nNotes:Potential carryover effects (treatment may long-term effect), leading post-treatment bias.Proposed Approach:\nTreated observations matched control observations units time period treatment history specified number lags.\nStandard matching weighting techniques employed refine matched set.\nApply estimator adjust time trend.\ngoal treated matched control observations similar covariate values.\nProposed Approach:Treated observations matched control observations units time period treatment history specified number lags.Treated observations matched control observations units time period treatment history specified number lags.Standard matching weighting techniques employed refine matched set.Standard matching weighting techniques employed refine matched set.Apply estimator adjust time trend.Apply estimator adjust time trend.goal treated matched control observations similar covariate values.goal treated matched control observations similar covariate values.Assessment:\nquality matches evaluated covariate balancing.\nAssessment:quality matches evaluated covariate balancing.Estimation:\nshort-term long-term average treatment effects treated (ATT) estimated.\nEstimation:short-term long-term average treatment effects treated (ATT) estimated.Treatment Variation plotVisualize variation treatment across space timeVisualize variation treatment across space timeAids discerning whether treatment fluctuates adequately time units variation primarily clustered subset data.Aids discerning whether treatment fluctuates adequately time units variation primarily clustered subset data.Select \\(F\\) (.e., number leads - time periods treatment). Driven authors interested estimating:\\(F = 0\\) contemporaneous effect (short-term effect)\\(F = 0\\) contemporaneous effect (short-term effect)\\(F = n\\) treatment effect outcome two time periods treatment. (cumulative long-term effect)\\(F = n\\) treatment effect outcome two time periods treatment. (cumulative long-term effect)Select \\(L\\) (number lags adjust).Driven identification assumption.Driven identification assumption.Balances bias-variance tradeoff.Balances bias-variance tradeoff.Higher \\(L\\) values increase credibility reduce efficiency limiting potential matches.Higher \\(L\\) values increase credibility reduce efficiency limiting potential matches.Model assumption:spillover effect assumed.spillover effect assumed.Carryover effect allowed \\(L\\) periods.Carryover effect allowed \\(L\\) periods.Potential outcome unit depends neither others’ treatment status past treatment \\(L\\) periods.Potential outcome unit depends neither others’ treatment status past treatment \\(L\\) periods.defining causal quantity parameters \\(L\\) \\(F\\).Focus average treatment effect treatment status change.\\(\\delta(F,L)\\) average causal effect treatment change (ATT), \\(F\\) periods post-treatment, considering treatment history \\(L\\) periods.Causal quantity considers potential future treatment reversals, meaning treatment revert control outcome measurement.Also possible estimate average treatment effect treatment reversal reversed (ART).Choose \\(L,F\\) based specific needs.large \\(L\\) value:\nIncreases credibility limited carryover effect assumption.\nAllows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).\nMight reduce number matches lead less precise estimates.\nlarge \\(L\\) value:Increases credibility limited carryover effect assumption.Increases credibility limited carryover effect assumption.Allows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).Allows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).Might reduce number matches lead less precise estimates.Might reduce number matches lead less precise estimates.Selecting appropriate number lags\nResearchers base choice substantive knowledge.\nSensitivity empirical results choice examined.\nSelecting appropriate number lagsResearchers base choice substantive knowledge.Researchers base choice substantive knowledge.Sensitivity empirical results choice examined.Sensitivity empirical results choice examined.choice \\(F\\) :\nSubstantively motivated.\nDecides whether interest lies short-term long-term causal effects.\nlarge \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.\nchoice \\(F\\) :Substantively motivated.Substantively motivated.Decides whether interest lies short-term long-term causal effects.Decides whether interest lies short-term long-term causal effects.large \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.large \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.Identification AssumptionParallel trend assumption conditioned treatment, outcome (excluding immediate lag), covariate histories.Parallel trend assumption conditioned treatment, outcome (excluding immediate lag), covariate histories.Doesn’t require strong unconfoundedness assumption.Doesn’t require strong unconfoundedness assumption.account unobserved time-varying confounders.account unobserved time-varying confounders.Essential examine outcome time trends.\nCheck ’re parallel treated matched control units using pre-treatment data\nEssential examine outcome time trends.Check ’re parallel treated matched control units using pre-treatment dataConstructing Matched Sets:\ntreated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).\nMatching based treatment history helps control carryover effects.\nPast treatments often act major confounders, method can correct .\nExact matching time period adjusts time-specific unobserved confounders.\nUnlike staggered adoption methods, units can change treatment status multiple times.\nMatched set allows treatment switching treatment\nConstructing Matched Sets:treated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).treated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).Matching based treatment history helps control carryover effects.Matching based treatment history helps control carryover effects.Past treatments often act major confounders, method can correct .Past treatments often act major confounders, method can correct .Exact matching time period adjusts time-specific unobserved confounders.Exact matching time period adjusts time-specific unobserved confounders.Unlike staggered adoption methods, units can change treatment status multiple times.Unlike staggered adoption methods, units can change treatment status multiple times.Matched set allows treatment switching treatmentMatched set allows treatment switching treatmentRefining Matched Sets:\nInitially, matched sets adjust treatment history.\nParallel trend assumption requires adjustments confounders like past outcomes covariates.\nMatching methods:\nMatch treated observation \\(J\\) control units.\nDistance measures like Mahalanobis distance propensity score can used.\nMatch based estimated propensity score, considering pretreatment covariates.\nRefined matched set selects similar control units based observed confounders.\n\nWeighting methods:\nAssign weight control unit matched set.\nWeights prioritize similar units.\nInverse propensity score weighting method can applied.\nWeighting generalized method matching.\n\nRefining Matched Sets:Initially, matched sets adjust treatment history.Initially, matched sets adjust treatment history.Parallel trend assumption requires adjustments confounders like past outcomes covariates.Parallel trend assumption requires adjustments confounders like past outcomes covariates.Matching methods:\nMatch treated observation \\(J\\) control units.\nDistance measures like Mahalanobis distance propensity score can used.\nMatch based estimated propensity score, considering pretreatment covariates.\nRefined matched set selects similar control units based observed confounders.\nMatching methods:Match treated observation \\(J\\) control units.Match treated observation \\(J\\) control units.Distance measures like Mahalanobis distance propensity score can used.Distance measures like Mahalanobis distance propensity score can used.Match based estimated propensity score, considering pretreatment covariates.Match based estimated propensity score, considering pretreatment covariates.Refined matched set selects similar control units based observed confounders.Refined matched set selects similar control units based observed confounders.Weighting methods:\nAssign weight control unit matched set.\nWeights prioritize similar units.\nInverse propensity score weighting method can applied.\nWeighting generalized method matching.\nWeighting methods:Assign weight control unit matched set.Assign weight control unit matched set.Weights prioritize similar units.Weights prioritize similar units.Inverse propensity score weighting method can applied.Inverse propensity score weighting method can applied.Weighting generalized method matching.Weighting generalized method matching.Difference--Differences Estimator:Using refined matched sets, ATT (Average Treatment Effect Treated) policy change estimated.Using refined matched sets, ATT (Average Treatment Effect Treated) policy change estimated.treated observation, estimate counterfactual outcome using weighted average control units refined set.treated observation, estimate counterfactual outcome using weighted average control units refined set.estimate ATT computed treated observation, averaged across observations.estimate ATT computed treated observation, averaged across observations.noncontemporaneous treatment effects \\(F > 0\\):\nATT doesn’t specify future treatment sequence.\nMatched control units might units receiving treatment time \\(t\\) \\(t + F\\).\ntreated units return control conditions times.\nnoncontemporaneous treatment effects \\(F > 0\\):ATT doesn’t specify future treatment sequence.ATT doesn’t specify future treatment sequence.Matched control units might units receiving treatment time \\(t\\) \\(t + F\\).Matched control units might units receiving treatment time \\(t\\) \\(t + F\\).treated units return control conditions times.treated units return control conditions times.Checking Covariate Balance:proposed methodology offers advantage checking covariate balance treated matched control observations.proposed methodology offers advantage checking covariate balance treated matched control observations.check helps see treated matched control observations comparable respect observed confounders.check helps see treated matched control observations comparable respect observed confounders.matched sets refined, covariate balance examination becomes straightforward.matched sets refined, covariate balance examination becomes straightforward.Examine mean difference covariate treated observation matched controls pretreatment time period.Examine mean difference covariate treated observation matched controls pretreatment time period.Standardize difference using standard deviation covariate across treated observations dataset.Standardize difference using standard deviation covariate across treated observations dataset.Aggregate covariate balance measure across treated observations covariate pretreatment time period.Aggregate covariate balance measure across treated observations covariate pretreatment time period.Examine balance lagged outcome variables multiple pretreatment periods time-varying covariates.\nhelps evaluate validity parallel trend assumption underlying proposed estimator.\nExamine balance lagged outcome variables multiple pretreatment periods time-varying covariates.helps evaluate validity parallel trend assumption underlying proposed estimator.Relations Linear Fixed Effects Regression Estimators:standard estimator equivalent linear two-way fixed effects regression estimator :\ntwo time periods exist.\nTreatment given units exclusively second period.\nstandard estimator equivalent linear two-way fixed effects regression estimator :two time periods exist.two time periods exist.Treatment given units exclusively second period.Treatment given units exclusively second period.equivalence doesn’t extend multiperiod designs, :\ntwo time periods considered.\nUnits might receive treatment multiple times.\nequivalence doesn’t extend multiperiod designs, :two time periods considered.two time periods considered.Units might receive treatment multiple times.Units might receive treatment multiple times.Despite , many researchers relate use two-way fixed effects estimator design.Despite , many researchers relate use two-way fixed effects estimator design.Standard Error Calculation:Approach:\nCondition weights implied matching process.\nweights denote often observation utilized matching (G. W. Imbens Rubin 2015)\nApproach:Condition weights implied matching process.Condition weights implied matching process.weights denote often observation utilized matching (G. W. Imbens Rubin 2015)weights denote often observation utilized matching (G. W. Imbens Rubin 2015)Context:\nAnalogous conditional variance seen regression models.\nResulting standard errors don’t factor uncertainties around matching procedure.\ncan viewed measure uncertainty conditional upon matching process (Ho et al. 2007).\nContext:Analogous conditional variance seen regression models.Analogous conditional variance seen regression models.Resulting standard errors don’t factor uncertainties around matching procedure.Resulting standard errors don’t factor uncertainties around matching procedure.can viewed measure uncertainty conditional upon matching process (Ho et al. 2007).can viewed measure uncertainty conditional upon matching process (Ho et al. 2007).Key Findings:Even conditions favoring OLS, proposed matching estimator displayed higher robustness omitted relevant lags linear regression model fixed effects.Even conditions favoring OLS, proposed matching estimator displayed higher robustness omitted relevant lags linear regression model fixed effects.robustness offered matching came cost - reduced statistical power.robustness offered matching came cost - reduced statistical power.emphasizes classic statistical tradeoff bias (matching advantage) variance (regression models might efficient).emphasizes classic statistical tradeoff bias (matching advantage) variance (regression models might efficient).Data RequirementsThe treatment variable binary:\n0 signifies “assignment” control.\n1 signifies assignment treatment.\ntreatment variable binary:0 signifies “assignment” control.0 signifies “assignment” control.1 signifies assignment treatment.1 signifies assignment treatment.Variables identifying units data must : Numeric integer.Variables identifying units data must : Numeric integer.Variables identifying time periods : Consecutive numeric/integer data.Variables identifying time periods : Consecutive numeric/integer data.Data format requirement: Must provided standard data.frame object.Data format requirement: Must provided standard data.frame object.Basic functions:Utilize treatment histories create matching sets treated control units.Utilize treatment histories create matching sets treated control units.Refine matched sets determining weights control unit set.\nUnits higher weights larger influence estimations.\nRefine matched sets determining weights control unit set.Units higher weights larger influence estimations.Matching Treatment History:Goal match units transitioning untreated treated status control units similar past treatment histories.Goal match units transitioning untreated treated status control units similar past treatment histories.Setting Quantity Interest (qoi =)\natt average treatment effect treated units\natc average treatment effect treatment control units\nart average effect treatment reversal units experience treatment reversal\nate average treatment effect\nSetting Quantity Interest (qoi =)att average treatment effect treated unitsatt average treatment effect treated unitsatc average treatment effect treatment control unitsatc average treatment effect treatment control unitsart average effect treatment reversal units experience treatment reversalart average effect treatment reversal units experience treatment reversalate average treatment effectate average treatment effectControl units treated unit identical treatment histories lag window (1988-1991)set limited first one, can still see exact past histories.Refining Matched Sets\nRefinement involves assigning weights control units.\nUsers must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\n\nRefining Matched SetsRefinement involves assigning weights control units.Refinement involves assigning weights control units.Users must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\nUsers must:Specify method calculating unit similarity/distance.Specify method calculating unit similarity/distance.Choose variables similarity/distance calculations.Choose variables similarity/distance calculations.Select Refinement Method\nUsers determine refinement method via refinement.method argument.\nOptions include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\n\nMethods “match” name Mahalanobis assign equal weights similar control units.\n“Weighting” methods give higher weights control units similar treated units.\nSelect Refinement MethodUsers determine refinement method via refinement.method argument.Users determine refinement method via refinement.method argument.Options include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\nOptions include:mahalanobismahalanobisps.matchps.matchCBPS.matchCBPS.matchps.weightps.weightCBPS.weightCBPS.weightps.msm.weightps.msm.weightCBPS.msm.weightCBPS.msm.weightnonenoneMethods “match” name Mahalanobis assign equal weights similar control units.Methods “match” name Mahalanobis assign equal weights similar control units.“Weighting” methods give higher weights control units similar treated units.“Weighting” methods give higher weights control units similar treated units.Variable Selection\nUsers need define covariates used covs.formula argument, one-sided formula object.\nVariables right side formula used calculations.\n“Lagged” versions variables can included using format: (lag(name..var, 0:n)).\nVariable SelectionUsers need define covariates used covs.formula argument, one-sided formula object.Users need define covariates used covs.formula argument, one-sided formula object.Variables right side formula used calculations.Variables right side formula used calculations.“Lagged” versions variables can included using format: (lag(name..var, 0:n)).“Lagged” versions variables can included using format: (lag(name..var, 0:n)).Understanding PanelMatch matched.set objects\nPanelMatch function returns PanelMatch object.\ncrucial element within PanelMatch object matched.set object.\nWithin PanelMatch object, matched.set object names like att, art, atc.\nqoi = ate, two matched.set objects: att atc.\nUnderstanding PanelMatch matched.set objectsThe PanelMatch function returns PanelMatch object.PanelMatch function returns PanelMatch object.crucial element within PanelMatch object matched.set object.crucial element within PanelMatch object matched.set object.Within PanelMatch object, matched.set object names like att, art, atc.Within PanelMatch object, matched.set object names like att, art, atc.qoi = ate, two matched.set objects: att atc.qoi = ate, two matched.set objects: att atc.Matched.set Object Details\nmatched.set named list added attributes.\nAttributes include:\nLag\nNames treatment\nUnit time variables\n\nlist entry represents matched set treated control units.\nNaming follows structure: [id variable].[time variable].\nlist element vector control unit ids match treated unit mentioned element name.\nSince ’s matching method, weights given size.match similar control units based distance calculations.\nMatched.set Object Detailsmatched.set named list added attributes.matched.set named list added attributes.Attributes include:\nLag\nNames treatment\nUnit time variables\nAttributes include:LagLagNames treatmentNames treatmentUnit time variablesUnit time variablesEach list entry represents matched set treated control units.list entry represents matched set treated control units.Naming follows structure: [id variable].[time variable].Naming follows structure: [id variable].[time variable].list element vector control unit ids match treated unit mentioned element name.list element vector control unit ids match treated unit mentioned element name.Since ’s matching method, weights given size.match similar control units based distance calculations.Since ’s matching method, weights given size.match similar control units based distance calculations.Visualizing Matched Sets plot methodUsers can visualize distribution matched set sizes.Users can visualize distribution matched set sizes.red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).Plot adjustments can made using graphics::plot.Plot adjustments can made using graphics::plot.Comparing Methods RefinementUsers encouraged :\nUse substantive knowledge experimentation evaluation.\nConsider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\n\nNote: Large numbers small matched sets can lead larger standard errors estimation stage.\nCovariates aren’t well balanced can lead undesirable comparisons treated control units.\nAspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\n\nUsers encouraged :Use substantive knowledge experimentation evaluation.Use substantive knowledge experimentation evaluation.Consider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\nConsider following configuring PanelMatch:number matched sets.number matched sets.number controls matched treated unit.number controls matched treated unit.Achieving covariate balance.Achieving covariate balance.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Aspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\nAspects consider include:Refinement method.Refinement method.Variables weight calculation.Variables weight calculation.Size lag window.Size lag window.Procedures addressing missing data (refer match.missing listwise.delete arguments).Procedures addressing missing data (refer match.missing listwise.delete arguments).Maximum size matched sets (matching methods).Maximum size matched sets (matching methods).Supportive Features:\nprint, plot, summary methods assist understanding matched sets sizes.\nget_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\n\nSupportive Features:print, plot, summary methods assist understanding matched sets sizes.print, plot, summary methods assist understanding matched sets sizes.get_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\nget_covariate_balance helps evaluate covariate balance:Lower values covariate balance calculations preferred.get_covariate_balance Function Options:Allows generation plots displaying covariate balance using plot = TRUE.Allows generation plots displaying covariate balance using plot = TRUE.Plots can customized using arguments typically used base R plot method.Plots can customized using arguments typically used base R plot method.Option set use.equal.weights = TRUE :\nObtaining balance unrefined sets.\nFacilitating understanding refinement’s impact.\nOption set use.equal.weights = TRUE :Obtaining balance unrefined sets.Obtaining balance unrefined sets.Facilitating understanding refinement’s impact.Facilitating understanding refinement’s impact.PanelEstimateStandard Error Calculation Methods\ndifferent methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\n\nqoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\n\nStandard Error Calculation MethodsThere different methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\ndifferent methods available:Bootstrap (default method 1000 iterations).Bootstrap (default method 1000 iterations).Conditional: Assumes independence across units, time.Conditional: Assumes independence across units, time.Unconditional: Doesn’t make assumptions independence across units time.Unconditional: Doesn’t make assumptions independence across units time.qoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\nqoi values set att, art, atc (Imai, Kim, Wang 2021):can use analytical methods calculating standard errors, include “conditional” “unconditional” methods.Moderating VariablesTo write journal submission, can follow following report:study, closely aligned research (Acemoglu et al. 2019), two key effects democracy economic growth estimated: impact democratization authoritarian reversal. treatment variable, \\(X_{}\\), defined one country \\(\\) democratic year \\(t\\), zero otherwise.Average Treatment Effect Treated (ATT) democratization formulated follows:\\[\n\\begin{aligned}\n\\delta(F, L) &= \\mathbb{E} \\left\\{ Y_{, t + F} (X_{} = 1, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 0, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) | X_{} = 1, X_{, t - 1} = 0 \\right\\}\n\\end{aligned}\n\\]framework, treated observations countries transition authoritarian regime \\(X_{-1} = 0\\) democratic one \\(X_{} = 1\\). variable \\(F\\) represents number leads, denoting time periods following treatment, \\(L\\) signifies number lags, indicating time periods preceding treatment.ATT authoritarian reversal given :\\[\n\\begin{aligned}\n&\\mathbb{E} \\left[ Y_{, t + F} (X_{} = 0, X_{, t - 1} = 1, \\{ X_{, t - l}\\}_{l=2}^L ) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 1, X_{-1} = 1, \\{X_{, t - l} \\}_{l=2}^L ) | X_{} = 0, X_{, t - 1} = 1 \\right]\n\\end{aligned}\n\\]ATT calculated conditioning 4 years lags (\\(L = 4\\)) 4 years following policy change \\(F = 1, 2, 3, 4\\). Matched sets treated observation constructed based treatment history, number matched control units generally decreasing considering 4-year treatment history compared 1-year history.enhance quality matched sets, methods Mahalanobis distance matching, propensity score matching, propensity score weighting utilized. approaches enable us evaluate effectiveness refinement method. process matching, employ --five --ten matching investigate sensitive empirical results maximum number allowed matches. information refinement process, please see Web AppendixThe Mahalanobis distance expressed specific formula. aim pair treated unit maximum \\(J\\) control units, permitting replacement, denoted \\(| \\mathcal{M}_{} \\le J|\\). average Mahalanobis distance treated control unit time computed :\\[ S_{} (') = \\frac{1}{L} \\sum_{l = 1}^L \\sqrt{(\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})^T \\mathbf{\\Sigma}_{, t - l}^{-1} (\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})} \\]matched control unit \\(' \\\\mathcal{M}_{}\\), \\(\\mathbf{V}_{'}\\) represents time-varying covariates adjust , \\(\\mathbf{\\Sigma}_{'}\\) sample covariance matrix \\(\\mathbf{V}_{'}\\). Essentially, calculate standardized distance using time-varying covariates average across different time intervals.context propensity score matching, employ logistic regression model balanced covariates derive propensity score. Defined conditional likelihood treatment given pre-treatment covariates (Rosenbaum Rubin 1983), propensity score estimated first creating data subset comprised treated matched control units year. logistic regression model fitted follows:\\[ \\begin{aligned} & e_{} (\\{\\mathbf{U}_{, t - l} \\}^L_{l = 1}) \\\\ &= Pr(X_{} = 1| \\mathbf{U}_{, t -1}, \\ldots, \\mathbf{U}_{, t - L}) \\\\ &= \\frac{1}{1 = \\exp(- \\sum_{l = 1}^L \\beta_l^T \\mathbf{U}_{, t - l})} \\end{aligned} \\]\\(\\mathbf{U}_{'} = (X_{'}, \\mathbf{V}_{'}^T)^T\\). Given model, estimated propensity score treated matched control units computed. enables adjustment lagged covariates via matching calculated propensity score, resulting following distance measure:\\[ S_{} (') = | \\text{logit} \\{ \\hat{e}_{} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\} - \\text{logit} \\{ \\hat{e}_{'t}( \\{ \\mathbf{U}_{', t - l} \\}^L_{l = 1})\\} | \\], \\(\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\) represents estimated propensity score matched control unit \\(' \\\\mathcal{M}_{}\\).distance measure \\(S_{} (')\\) determined control units original matched set, fine-tune set selecting \\(J\\) closest control units, meet researcher-defined caliper constraint \\(C\\). control units receive zero weight. results refined matched set treated unit \\((, t)\\):\\[ \\mathcal{M}_{}^* = \\{' : ' \\\\mathcal{M}_{}, S_{} (') < C, S_{} \\le S_{}^{(J)}\\} \\]\\(S_{}^{(J)}\\) \\(J\\)th smallest distance among control units original set \\(\\mathcal{M}_{}\\).refinement using weighting, weight assigned control unit \\('\\) matched set corresponding treated unit \\((, t)\\), greater weight accorded similar units. utilize inverse propensity score weighting, based propensity score model mentioned earlier:\\[ w_{}^{'} \\propto \\frac{\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )}{1 - \\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )} \\]model, \\(\\sum_{' \\\\mathcal{M}_{}} w_{}^{'} = 1\\) \\(w_{}^{'} = 0\\) \\(' \\notin \\mathcal{M}_{}\\). model fitted complete sample treated matched control units.Checking Covariate Balance distinct advantage proposed methodology regression methods ability offers researchers inspect covariate balance treated matched control observations. facilitates evaluation whether treated matched control observations comparable regarding observed confounders. investigate mean difference covariate (e.g., \\(V_{'j}\\), representing \\(j\\)-th variable \\(\\mathbf{V}_{'}\\)) treated observation matched control observation pre-treatment time period (.e., \\(t' < t\\)), standardize difference. given pretreatment time period, adjust standard deviation covariate across treated observations dataset. Thus, mean difference quantified terms standard deviation units. Formally, treated observation \\((,t)\\) \\(D_{} = 1\\), define covariate balance variable \\(j\\) pretreatment time period \\(t - l\\) : \\[\\begin{equation}\nB_{}(j, l) = \\frac{V_{, t- l,j}- \\sum_{' \\\\mathcal{M}_{}}w_{}^{'}V_{', t-l,j}}{\\sqrt{\\frac{1}{N_1 - 1} \\sum_{'=1}^N \\sum_{t' = L+1}^{T-F}D_{'t'}(V_{', t'-l, j} - \\bar{V}_{t' - l, j})^2}}\n\\label{eq:covbalance}\n\\end{equation}\\] \\(N_1 = \\sum_{'= 1}^N \\sum_{t' = L+1}^{T-F} D_{'t'}\\) denotes total number treated observations \\(\\bar{V}_{t-l,j} = \\sum_{=1}^N D_{,t-l,j}/N\\). aggregate covariate balance measure across treated observations covariate pre-treatment time period: \\[\\begin{equation}\n\\bar{B}(j, l) = \\frac{1}{N_1} \\sum_{=1}^N \\sum_{t = L+ 1}^{T-F}D_{} B_{}(j,l)\n\\label{eq:aggbalance}\n\\end{equation}\\] Lastly, evaluate balance lagged outcome variables several pre-treatment periods time-varying covariates. examination aids assessing validity parallel trend assumption integral estimator justification.Figure ??, demonstrate enhancement covariate balance thank refinement matched sets. scatter plot contrasts absolute standardized mean difference, detailed Equation (??), (horizontal axis) (vertical axis) refinement. Points 45-degree line indicate improved standardized mean balance certain time-varying covariates post-refinement. majority variables benefit refinement process. Notably, propensity score weighting (bottom panel) shows significant improvement, whereas Mahalanobis matching (top panel) yields modest improvement.can either sequentaillyor parallelNote: Scatter plots display standardized mean difference covariate \\(j\\) lag year \\(l\\) defined Equation (??) (x-axis) (y-axis) matched set refinement. plot includes varying numbers possible matches matching method. Rows represent different matching/weighting methods, columns indicate adjustments various lag lengths.exportNote: graph displays standardized mean difference, outlined Equation (??), plotted vertical axis across pre-treatment duration four years represented horizontal axis. leftmost column illustrates balance prior refinement, subsequent three columns depict covariate balance post application distinct refinement techniques. individual line signifies balance specific variable pre-treatment phase.red line tradewb blue line lagged outcome variable.Figure ??, observe marked improvement covariate balance due implemented matching procedures pre-treatment period. analysis prioritizes methods adjust time-varying covariates span four years preceding treatment initiation. two rows delineate standardized mean balance treatment modalities, individual lines representing balance covariate.Across scenarios, refinement attributed matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances confounders. degree imbalance remains evident Mahalanobis distance propensity score matching techniques, standardized mean difference lagged outcome remains stable throughout pre-treatment phase. consistency lends credence validity proposed estimator.Estimation ResultsWe now detail estimated ATTs derived matching techniques. Figure offers visual representations impacts treatment initiation (upper panel) treatment reversal (lower panel) outcome variable duration 5 years post-transition, specifically, (F = 0, 1, …, 4). Across five methods (columns), becomes evident point estimates effects associated treatment initiation consistently approximate zero 5-year window. contrast, estimated outcomes treatment reversal notably negative maintain statistical significance refinement techniques initial year transition 1 4 years follow, provided treatment reversal permissible. effects notably pronounced, pointing estimated reduction roughly X% outcome variable.Collectively, findings indicate transition treated state absence doesn’t invariably lead heightened outcome. Instead, transition treated state back absence exerts considerable negative effect outcome variable short intermediate terms. Hence, positive effect treatment (use traditional ) actually driven negative effect treatment reversal.export","code":"\nlibrary(PanelMatch)\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    \n    hide.x.tick.label = TRUE, hide.y.tick.label = TRUE, \n    # dense.plot = TRUE,\n    data = dem\n)\nlibrary(PanelMatch)\n# All examples follow the package's vignette\n# Create the matched sets\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# visualize the treated unit and matched controls\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    data = dem,\n    matched.set = PM.results.none$att[1],\n    # highlight the particular set\n    show.set.only = TRUE\n)\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    data = dem,\n    matched.set = PM.results.none$att[2],\n    # highlight the particular set\n    show.set.only = TRUE\n)\n# PanelMatch without any refinement\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# Extract the matched.set object\nmsets.none <- PM.results.none$att\n\n# PanelMatch with refinement\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\", # use Mahalanobis distance\n        data = dem,\n        match.missing = TRUE,\n        covs.formula = ~ tradewb,\n        size.match = 5,\n        qoi = \"att\" ,\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nmsets.maha <- PM.results.maha$att\n# these 2 should be identical because weights are not shown\nmsets.none |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\nmsets.maha |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\n# summary(msets.none)\n# summary(msets.maha)\nplot(msets.none)\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\",\n        data = dem,\n        match.missing = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# listwise deletion used for missing data\nPM.results.listwise <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\",\n        data = dem,\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# propensity score based weighting method\nPM.results.ps.weight <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"ps.weight\",\n        data = dem,\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE\n    )\n\nget_covariate_balance(\n    PM.results.none$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb            y\n#> t_4 -0.07245466  0.291871990\n#> t_3 -0.20930129  0.208654876\n#> t_2 -0.24425207  0.107736647\n#> t_1 -0.10806125 -0.004950238\n\nget_covariate_balance(\n    PM.results.maha$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4  0.04558637 0.09701606\n#> t_3 -0.03312750 0.10844046\n#> t_2 -0.01396793 0.08890753\n#> t_1  0.10474894 0.06618865\n\n\nget_covariate_balance(\n    PM.results.listwise$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4  0.05634922 0.05223623\n#> t_3 -0.01104797 0.05217896\n#> t_2  0.01411473 0.03094133\n#> t_1  0.06850180 0.02092209\n\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4 0.014362590 0.04035905\n#> t_3 0.005529734 0.04188731\n#> t_2 0.009410044 0.04195008\n#> t_1 0.027907540 0.03975173\n# Use equal weights\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    use.equal.weights = TRUE,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = TRUE,\n    # visualize by setting plot to TRUE\n    ylim = c(-1, 1)\n)\n\n# Compare covariate balance to refined sets\n# See large improvement in balance\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = TRUE,\n    # visualize by setting plot to TRUE\n    ylim = c(-1, 1)\n)\n\n\nbalance_scatter(\n    matched_set_list = list(PM.results.maha$att,\n                            PM.results.ps.weight$att),\n    data = dem,\n    covariates = c(\"y\", \"tradewb\")\n)\nPE.results <- PanelEstimate(\n    sets              = PM.results.ps.weight,\n    data              = dem,\n    se.method         = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level  = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.6399349 1.0304938 1.3825265 1.7625951 2.1672629\n\n\n# use conditional method\nPE.results <- PanelEstimate(\n    sets             = PM.results.ps.weight,\n    data             = dem,\n    se.method        = \"conditional\",\n    confidence.level = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143\n\nsummary(PE.results)\n#> Weighted Difference-in-Differences with Propensity Score\n#> Matches created with 4 lags\n#> \n#> Standard errors computed with conditional  method\n#> \n#> Estimate of Average Treatment Effect on the Treated (ATT) by Period:\n#> $summary\n#>      estimate std.error       2.5%    97.5%\n#> t+0 0.2609565 0.4844805 -0.6886078 1.210521\n#> t+1 0.9630847 0.8170604 -0.6383243 2.564494\n#> t+2 1.2851017 1.1171942 -0.9045586 3.474762\n#> t+3 1.7370930 1.4116879 -1.0297644 4.503950\n#> t+4 1.4871846 1.7172143 -1.8784937 4.852863\n#> \n#> $lag\n#> [1] 4\n#> \n#> $qoi\n#> [1] \"att\"\n\nplot(PE.results)\n# moderating variable\ndem$moderator <- 0\ndem$moderator <- ifelse(dem$wbcode2 > 100, 1, 2)\n\nPM.results <-\n    PanelMatch(\n        lag                          = 4,\n        time.id                      = \"year\",\n        unit.id                      = \"wbcode2\",\n        treatment                    = \"dem\",\n        refinement.method            = \"mahalanobis\",\n        data                         = dem,\n        match.missing                = TRUE,\n        covs.formula                 = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match                   = 5,\n        qoi                          = \"att\",\n        outcome.var                  = \"y\",\n        lead                         = 0:4,\n        forbid.treatment.reversal    = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPE.results <-\n    PanelEstimate(sets      = PM.results,\n                  data      = dem,\n                  moderator = \"moderator\")\n\n# Each element in the list corresponds to a level in the moderator\nplot(PE.results[[1]])\n\nplot(PE.results[[2]])\nlibrary(PanelMatch)\nlibrary(causalverse)\n\nrunPanelMatch <- function(method, lag, size.match=NULL, qoi=\"att\") {\n    \n    # Default parameters for PanelMatch\n    common.args <- list(\n        lag = lag,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        data = dem,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi = qoi,\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        size.match = size.match  # setting size.match here for all methods\n    )\n    \n    if(method == \"mahalanobis\") {\n        common.args$refinement.method <- \"mahalanobis\"\n        common.args$match.missing <- TRUE\n        common.args$use.diagonal.variance.matrix <- TRUE\n    } else if(method == \"ps.match\") {\n        common.args$refinement.method <- \"ps.match\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    } else if(method == \"ps.weight\") {\n        common.args$refinement.method <- \"ps.weight\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    }\n    \n    return(do.call(PanelMatch, common.args))\n}\n\nmethods <- c(\"mahalanobis\", \"ps.match\", \"ps.weight\")\nlags <- c(1, 4)\nsizes <- c(5, 10)\nres_pm <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm[[name]] <- runPanelMatch(method, lag, size)\n        }\n    }\n}\n\n# Now, you can access res_pm using res_pm[[\"mahalanobis.1lag.5m\"]] etc.\n\n# for treatment reversal\nres_pm_rev <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm_rev[[name]] <- runPanelMatch(method, lag, size, qoi = \"art\")\n        }\n    }\n}\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(cores = 4)\n# Initialize an empty list to store results\nres_pm <- list()\n\n# Replace nested for-loops with foreach\nresults <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <- runPanelMatch(method, lag, size)\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results)) {\n  res_pm[[name]] <- results[[name]]\n}\n\n# Treatment reversal\n# Initialize an empty list to store results\nres_pm_rev <- list()\n\n# Replace nested for-loops with foreach\nresults_rev <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <-\n          runPanelMatch(method, lag, size, qoi = \"art\")\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results_rev)) {\n  res_pm_rev[[name]] <- results_rev[[name]]\n}\n\n\nstopImplicitCluster()\nlibrary(gridExtra)\n\n# Updated plotting function\ncreate_balance_plot <- function(method, lag, sizes, res_pm, dem) {\n    matched_set_lists <- lapply(sizes, function(size) {\n        res_pm[[paste0(method, \".\", lag, \"lag.\", size, \"m\")]]$att\n    })\n    \n    return(\n        balance_scatter_custom(\n            matched_set_list = matched_set_lists,\n            legend.title = \"Possible Matches\",\n            set.names = as.character(sizes),\n            legend.position = c(0.2, 0.8),\n            \n            # for compiled plot, you don't need x,y, or main labs\n            x.axis.label = \"\",\n            y.axis.label = \"\",\n            main = \"\",\n            data = dem,\n            dot.size = 5,\n            # show.legend = F,\n            them_use = causalverse::ama_theme(base_size = 32),\n            covariates = c(\"y\", \"tradewb\")\n        )\n    )\n}\n\nplots <- list()\n\nfor (method in methods) {\n    for (lag in lags) {\n        plots[[paste0(method, \".\", lag, \"lag\")]] <-\n            create_balance_plot(method, lag, sizes, res_pm, dem)\n    }\n}\n\n# # Arranging plots in a 3x2 grid\n# grid.arrange(plots[[\"mahalanobis.1lag\"]],\n#              plots[[\"mahalanobis.4lag\"]],\n#              plots[[\"ps.match.1lag\"]],\n#              plots[[\"ps.match.4lag\"]],\n#              plots[[\"ps.weight.1lag\"]],\n#              plots[[\"ps.weight.4lag\"]],\n#              ncol=2, nrow=3)\n\n\n# Standardized Mean Difference of Covariates\nlibrary(gridExtra)\nlibrary(grid)\n\n# Create column and row labels using textGrob\ncol_labels <- c(\"1-year Lag\", \"4-year Lag\")\nrow_labels <- c(\"Maha Matching\", \"PS Matching\", \"PS Weigthing\")\n\nmajor.axes.fontsize = 40\nminor.axes.fontsize = 30\n\npng(\n    file.path(getwd(), \"images\", \"did_balance_scatter.png\"),\n    width = 1200,\n    height = 1000\n)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(textGrob(\n        row_labels[1],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"mahalanobis.1lag\"]], plots[[\"mahalanobis.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[2],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.match.1lag\"]], plots[[\"ps.match.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[3],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.weight.1lag\"]], plots[[\"ps.weight.4lag\"]])\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\ngrid.arrange(\n    grobs = grobs,\n    ncol = 3,\n    nrow = 4,\n    widths = c(0.15, 0.42, 0.42),\n    heights = c(0.15, 0.28, 0.28, 0.28)\n)\n\ngrid.text(\n    \"Before Refinement\",\n    x = 0.5,\n    y = 0.03,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"After Refinement\",\n    x = 0.03,\n    y = 0.5,\n    rot = 90,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ndev.off()\n#> png \n#>   2\n# Step 1: Define configurations\nconfigurations <- list(\n    list(refinement.method = \"none\", qoi = \"att\"),\n    list(refinement.method = \"none\", qoi = \"art\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"att\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"art\"),\n    list(refinement.method = \"ps.match\", qoi = \"att\"),\n    list(refinement.method = \"ps.match\", qoi = \"art\"),\n    list(refinement.method = \"ps.weight\", qoi = \"att\"),\n    list(refinement.method = \"ps.weight\", qoi = \"art\")\n)\n\n# Step 2: Use lapply or loop to generate results\nresults <- lapply(configurations, function(config) {\n    PanelMatch(\n        lag                       = 4,\n        time.id                   = \"year\",\n        unit.id                   = \"wbcode2\",\n        treatment                 = \"dem\",\n        data                      = dem,\n        match.missing             = FALSE,\n        listwise.delete           = TRUE,\n        size.match                = 5,\n        outcome.var               = \"y\",\n        lead                      = 0:4,\n        forbid.treatment.reversal = FALSE,\n        refinement.method         = config$refinement.method,\n        covs.formula              = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi                       = config$qoi\n    )\n})\n\n# Step 3: Get covariate balance and plot\nplots <- mapply(function(result, config) {\n    df <- get_covariate_balance(\n        if (config$qoi == \"att\")\n            result$att\n        else\n            result$art,\n        data = dem,\n        covariates = c(\"tradewb\", \"y\"),\n        plot = F\n    )\n    causalverse::plot_covariate_balance_pretrend(df, main = \"\", show_legend = F)\n}, results, configurations, SIMPLIFY = FALSE)\n\n# Set names for plots\nnames(plots) <- sapply(configurations, function(config) {\n    paste(config$qoi, config$refinement.method, sep = \".\")\n})\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <-\n    c(\"None\",\n      \"Mahalanobis\",\n      \"Propensity Score Matching\",\n      \"Propensity Score Weighting\")\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_covariate_balance.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(\n        textGrob(\n            row_labels[1],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$att.none,\n        plots$att.mahalanobis,\n        plots$att.ps.match,\n        plots$att.ps.weight\n    ),\n    \n    list(\n        textGrob(\n            row_labels[2],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$art.none,\n        plots$art.mahalanobis,\n        plots$art.ps.match,\n        plots$art.ps.weight\n    )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n    grobs   = grobs,\n    ncol    = 5,\n    nrow    = 3,\n    widths  = c(0.1, 0.225, 0.225, 0.225, 0.225),\n    heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n    \"Refinement Methods\",\n    x  = 0.5,\n    y  = 0.01,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"Quantities of Interest\",\n    x   = 0.02,\n    y   = 0.5,\n    rot = 90,\n    gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_covariate_balance.png\"))\n# sequential\n# Step 1: Apply PanelEstimate function\n\n# Initialize an empty list to store results\nres_est <- vector(\"list\", length(res_pm))\n\n# Iterate over each element in res_pm\nfor (i in 1:length(res_pm)) {\n  res_est[[i]] <- PanelEstimate(\n    res_pm[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est list\n  names(res_est)[i] <- names(res_pm)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function\n\n# Initialize an empty list to store plot results\nres_est_plot <- vector(\"list\", length(res_est))\n\n# Iterate over each element in res_est\nfor (i in 1:length(res_est)) {\n    res_est_plot[[i]] <-\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n    # Transfer the name of the current element to the res_est_plot list\n    names(res_est_plot)[i] <- names(res_est)[i]\n}\n\n# check results\n# res_est_plot$mahalanobis.1lag.5m\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev\n\n# Initialize an empty list to store results\nres_est_rev <- vector(\"list\", length(res_pm_rev))\n\n# Iterate over each element in res_pm_rev\nfor (i in 1:length(res_pm_rev)) {\n  res_est_rev[[i]] <- PanelEstimate(\n    res_pm_rev[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est_rev list\n  names(res_est_rev)[i] <- names(res_pm_rev)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev\n\n# Initialize an empty list to store plot results\nres_est_plot_rev <- vector(\"list\", length(res_est_rev))\n\n# Iterate over each element in res_est_rev\nfor (i in 1:length(res_est_rev)) {\n    res_est_plot_rev[[i]] <-\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n  # Transfer the name of the current element to the res_est_plot_rev list\n  names(res_est_plot_rev)[i] <- names(res_est_rev)[i]\n}\n# parallel\nlibrary(doParallel)\nlibrary(foreach)\n\n# Detect the number of cores to use for parallel processing\nnum_cores <- 4\n\n# Register the parallel backend\ncl <- makeCluster(num_cores)\nregisterDoParallel(cl)\n\n# Step 1: Apply PanelEstimate function in parallel\nres_est <-\n    foreach(i = 1:length(res_pm), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm to res_est\nnames(res_est) <- names(res_pm)\n\n# Step 2: Apply plot_PanelEstimate function in parallel\nres_est_plot <-\n    foreach(\n        i = 1:length(res_est),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est to res_est_plot\nnames(res_est_plot) <- names(res_est)\n\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev in parallel\nres_est_rev <-\n    foreach(i = 1:length(res_pm_rev), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm_rev[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm_rev to res_est_rev\nnames(res_est_rev) <- names(res_pm_rev)\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev in parallel\nres_est_plot_rev <-\n    foreach(\n        i = 1:length(res_est_rev),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est_rev to res_est_plot_rev\nnames(res_est_plot_rev) <- names(res_est_rev)\n\n# Stop the cluster\nstopCluster(cl)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <- c(\"Mahalanobis 5m\", \n                \"Mahalanobis 10m\", \n                \"PS Matching 5m\", \n                \"PS Matching 10m\", \n                \"PS Weighting 5m\")\n\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n  list(\n    nullGrob(),\n    textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[5], gp = gpar(fontsize = minor.axes.fontsize))\n  ),\n  \n  list(\n    textGrob(row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot$mahalanobis.1lag.5m,\n    res_est_plot$mahalanobis.1lag.10m,\n    res_est_plot$ps.match.1lag.5m,\n    res_est_plot$ps.match.1lag.10m,\n    res_est_plot$ps.weight.1lag.5m\n  ),\n  \n  list(\n    textGrob(row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot_rev$mahalanobis.1lag.5m,\n    res_est_plot_rev$mahalanobis.1lag.10m,\n    res_est_plot_rev$ps.match.1lag.5m,\n    res_est_plot_rev$ps.match.1lag.10m,\n    res_est_plot_rev$ps.weight.1lag.5m\n  )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n  grobs   = grobs,\n  ncol    = 6,\n  nrow    = 3,\n  widths  = c(0.1, 0.18, 0.18, 0.18, 0.18, 0.18),\n  heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n  \"Methods\",\n  x  = 0.5,\n  y  = 0.02,\n  gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n  \"\",\n  x   = 0.02,\n  y   = 0.5,\n  rot = 90,\n  gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"))"},{"path":"difference-in-differences.html","id":"counterfactual-estimators","chapter":"26 Difference-in-differences","heading":"26.9.3.2 Counterfactual Estimators","text":"Also known imputation approach (Liu, Wang, Xu 2022)class estimator consider observation treatment missing data. Models built using data control units impute conterfactuals treated observations.’s called counterfactual estimators predict outcomes treated observations received treatment.Advantages:\nAvoids negative weights biases using treated observations modeling applying uniform weights.\nSupports various models, including may relax strict exogeneity assumptions.\nAvoids negative weights biases using treated observations modeling applying uniform weights.Supports various models, including may relax strict exogeneity assumptions.Methods including\nFixed-effects conterfactual estimator (FEct) (special case):\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\n\nInteractive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\n\nMatrix completion (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\n\n[Synthetic Controls] (case studies)\nFixed-effects conterfactual estimator (FEct) (special case):\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).Interactive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.Generalized Synthetic Controls subset IFEct treatments don’t revert.Matrix completion (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.latent factors (unobservables) strong sparse, IFEct outperforms MC.[Synthetic Controls] (case studies)Identifying Assumptions:Function Form: Additive separability observables, unobservables, idiosyncratic error term.\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).Strict Exogeneity: Conditional observables unobservables, potential outcomes independent treatment assignment (.e., baseline quasi-randomization)\n, unobservables = unit + time FEs, assumption parallel trends assumption\n, unobservables = unit + time FEs, assumption parallel trends assumptionLow-dimensional Decomposition (Feasibility Assumption): Unobservable effects can decomposed low-dimension.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.Estimation Procedure:Using control observations, estimate functions observable unobservable variables (relying Assumptions 1 3).Predict counterfactual outcomes treated unit using obtained functions.Calculate difference treatment effect treated individual.averaging treated individuals, can obtain Average Treatment Effect Treated (ATT).Notes:Use jackknife number treated units small (Liu, Wang, Xu 2022, 166).","code":""},{"path":"difference-in-differences.html","id":"imputation-method","chapter":"26 Difference-in-differences","heading":"26.9.3.2.1 Imputation Method","text":"Liu, Wang, Xu (2022) can also account treatment reversals heterogeneous treatment effects.imputation estimators include[@gardner2022two @borusyak2021revisiting][@gardner2022two @borusyak2021revisiting]N. Brown, Butts, Westerlund (2023)N. Brown, Butts, Westerlund (2023)F-test \\(H_0\\): residual averages pre-treatment periods = 0To see treatment reversal effects","code":"\nlibrary(fect)\n\nPanelMatch::dem\n\nmodel.fect <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        parallel = TRUE,\n        seed = 1234,\n        # twfe\n        force = \"two-way\"\n    )\nprint(model.fect$est.avg)\n\nplot(model.fect)\n\nplot(model.fect, stats = \"F.p\")\nplot(model.fect, stats = \"F.p\", type = 'exit')"},{"path":"difference-in-differences.html","id":"placebo-test","chapter":"26 Difference-in-differences","heading":"26.9.3.2.2 Placebo Test","text":"selecting part data excluding observations within specified range improve model fitting, evaluate whether estimated Average Treatment Effect (ATT) within range significantly differs zero. approach helps us analyze periods treatment.test fails, either functional form strict exogeneity assumption problematic.","code":"\nout.fect.p <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        placeboTest = TRUE,\n        # using 3 periods\n        placebo.period = c(-2, 0)\n    )\nplot(out.fect.p, proportion = 0.1, stats = \"placebo.p\")"},{"path":"difference-in-differences.html","id":"no-carryover-effects-test","chapter":"26 Difference-in-differences","heading":"26.9.3.2.3 (No) Carryover Effects Test","text":"placebo test can adapted assess carryover effects masking several post-treatment periods instead pre-treatment ones. carryover effects present, average prediction error approximate zero. carryover test, set carryoverTest = TRUE. Specify post-treatment period range carryover.period exclude observations model fitting, evaluate estimated ATT significantly deviates zero.Even carryover effects, cases staggered adoption setting, researchers interested cumulative effects, aggregated treatment effects, ’s okay.evidence carryover effects.","code":"\nout.fect.c <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        carryoverTest = TRUE,\n        # how many periods of carryover\n        carryover.period = c(1, 3)\n    )\nplot(out.fect.c,  stats = \"carryover.p\")"},{"path":"difference-in-differences.html","id":"matrix-completion-1","chapter":"26 Difference-in-differences","heading":"26.9.3.3 Matrix Completion","text":"Applications marketing:Bronnenberg, Dubé, Sanders (2020)estimate average causal effects panel data units exposed treatment intermittently, two literatures pivotal:Unconfoundedness (G. W. Imbens Rubin 2015): Imputes missing potential control outcomes treated units using observed outcomes similar control units previous periods.Unconfoundedness (G. W. Imbens Rubin 2015): Imputes missing potential control outcomes treated units using observed outcomes similar control units previous periods.Synthetic Control (Abadie, Diamond, Hainmueller 2010): Imputes missing control outcomes treated units using weighted averages control units, matching lagged outcomes treated control units.Synthetic Control (Abadie, Diamond, Hainmueller 2010): Imputes missing control outcomes treated units using weighted averages control units, matching lagged outcomes treated control units.exploit missing potential outcomes different assumptions:Unconfoundedness assumes time patterns stable across units.Unconfoundedness assumes time patterns stable across units.Synthetic control assumes unit patterns stable time.Synthetic control assumes unit patterns stable time.regularization applied, approaches applicable similar settings (Athey et al. 2021).Matrix Completion method, nesting , based matrix factorization, focusing imputing missing matrix elements assuming:Complete matrix = low-rank matrix + noise.Missingness completely random.’s distinguished imposing factorization restrictions utilizing regularization define estimator, particularly effective nuclear norm regularizer complex missing patterns (Athey et al. 2021).Contributions Athey et al. (2021) matrix completion include:Recognizing structured missing patterns allowing time correlation, enabling staggered adoption.Modifying estimators unregularized unit time fixed effects.Performing well across various \\(T\\) \\(N\\) sizes, unlike unconfoundedness synthetic control, falter \\(T >> N\\) \\(N >> T\\), respectively.Identifying Assumptions:SUTVA: Potential outcomes indexed unit’s contemporaneous treatment.dynamic effects (’s okay staggered adoption, gives different interpretation estimand).Setup:\\(Y_{}(0)\\) \\(Y_{}(1)\\) represent potential outcomes \\(Y_{}\\).\\(W_{}\\) binary treatment indicator.Aim estimate average effect treated:\\[\n\\tau = \\frac{\\sum_{(,t): W_{} = 1}[Y_{}(1) - Y_{}(0)]}{\\sum_{,t}W_{}}\n\\]observe relevant values \\(Y_{}(1)\\)want impute missing entries \\(Y(0)\\) matrix treated units \\(W_{} = 1\\).Define \\(\\mathcal{M}\\) set pairs indices \\((,t)\\), \\(\\N\\) \\(t \\T\\), corresponding missing entries \\(W_{} = 1\\); \\(\\mathcal{O}\\) set pairs indices corresponding observed entries \\(Y(0)\\) \\(W_{} = 0\\).Data conceptualized two \\(N \\times T\\) matrices, one incomplete one complete:\\[\nY = \\begin{pmatrix}\nY_{11} & Y_{12} & ? & \\cdots & Y_{1T} \\\\\n? & ? & Y_{23} & \\cdots & ? \\\\\nY_{31} & ? & Y_{33} & \\cdots & ? \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nY_{N1} & ? & Y_{N3} & \\cdots & ?\n\\end{pmatrix},\n\\]\\[\nW = \\begin{pmatrix}\n0 & 0 & 1 & \\cdots & 0 \\\\\n1 & 1 & 0 & \\cdots & 1 \\\\\n0 & 1 & 0 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 1 & 0 & \\cdots & 1\n\\end{pmatrix},\n\\]\\[\nW_{} =\n\\begin{cases}\n1 & \\text{} (,t) \\\\mathcal{M}, \\\\\n0 & \\text{} (,t) \\\\mathcal{O},\n\\end{cases}\n\\]indicator event corresponding component \\(Y\\), \\(Y_{}\\), missing.Patterns missing data \\(\\mathbf{Y}\\):Block (treatment) structure 2 special cases\nSingle-treated-period block structure (G. W. Imbens Rubin 2015)\nSingle-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)\nBlock (treatment) structure 2 special casesSingle-treated-period block structure (G. W. Imbens Rubin 2015)Single-treated-period block structure (G. W. Imbens Rubin 2015)Single-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)Single-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)Staggered AdoptionStaggered AdoptionShape matrix \\(\\mathbf{Y}\\):Thin (\\(N >> T\\))Thin (\\(N >> T\\))Fat (\\(T >> N\\))Fat (\\(T >> N\\))Square (\\(N \\approx T\\))Square (\\(N \\approx T\\))Combinations patterns missingness shape create different literatures:Horizontal Regression = Thin matrix + single-treated-period block (focusing cross-section correlation patterns)Horizontal Regression = Thin matrix + single-treated-period block (focusing cross-section correlation patterns)Vertical Regression = Fat matrix + single-treated-unit block (focusing time-series correlation patterns)Vertical Regression = Fat matrix + single-treated-unit block (focusing time-series correlation patterns)TWFE = Square matrixTWFE = Square matrixTo combine, can exploit stable patterns time, across units (e.g., TWFE, interactive FEs matrix completion).factor model\\[\n\\mathbf{Y = UV}^T + \\mathbf{\\epsilon}\n\\]\\(\\mathbf{U}\\) \\(N \\times R\\) \\(\\mathbf{V}\\) \\(T\\times R\\)interactive FE literature focuses fixed number factors \\(R\\) \\(\\mathbf{U, V}\\), matrix completion focuses impute \\(\\mathbf{Y}\\) using forms regularization (e.g., nuclear norm).can also estimate number factors \\(R\\) Moon Weidner (2015)use nuclear norm minimization estimator, must add penalty term regularize objective function. However, , need explicitly estimate time (\\(\\lambda_t\\)) unit (\\(\\mu_i\\)) fixed effects implicitly embedded missing data matrix reduce bias regularization term.Specifically,\\[\nY_{}  =L_{} + \\sum_{p = 1}^P \\sum_{q= 1}^Q X_{ip} H_{pq}Z_{qt} + \\mu_i + \\lambda_t + V_{} \\beta + \\epsilon_{}\n\\]\\(X_{ip}\\) matrix \\(p\\) variables unit \\(\\)\\(X_{ip}\\) matrix \\(p\\) variables unit \\(\\)\\(Z_{qt}\\) matrix \\(q\\) variables time \\(t\\)\\(Z_{qt}\\) matrix \\(q\\) variables time \\(t\\)\\(V_{}\\) matrix time-varying variables.\\(V_{}\\) matrix time-varying variables.Lasso-type \\(l_1\\) norm (\\(||H|| = \\sum_{p = 1}^p \\sum_{q = 1}^Q |H_{pq}|\\)) used shrink \\(H \\0\\)several options regularize \\(L\\):Frobenius (.e., Ridge): informative since imputes missing values 0.Nuclear Norm (.e., Lasso): computationally feasible (using SOFT-IMPUTE algorithm (Mazumder, Hastie, Tibshirani 2010)).Rank (.e., Subset selection): computationally feasibleThis method allows touse covariatesuse covariatesleverage data treated units (can used treatment effect constant pattern missing complex).leverage data treated units (can used treatment effect constant pattern missing complex).autocorrelated errorshave autocorrelated errorshave weighted loss function (.e., take account probability outcomes unit missing)weighted loss function (.e., take account probability outcomes unit missing)","code":""},{"path":"difference-in-differences.html","id":"gardner2022two-and-borusyak2021revisiting","chapter":"26 Difference-in-differences","heading":"26.9.4 Gardner (2022) and Borusyak, Jaravel, and Spiess (2021)","text":"Estimate time unit fixed effects separatelyEstimate time unit fixed effects separatelyKnown imputation method (Borusyak, Jaravel, Spiess 2021) two-stage (Gardner 2022)Known imputation method (Borusyak, Jaravel, Spiess 2021) two-stage (Gardner 2022)Borusyak, Jaravel, Spiess (2021) didimputationThis version currently working","code":"\n# remotes::install_github(\"kylebutts/did2s\")\nlibrary(did2s)\nlibrary(ggplot2)\nlibrary(fixest)\nlibrary(tidyverse)\ndata(base_stagg)\n\n\nest <- did2s(\n    data = base_stagg |> mutate(treat = if_else(time_to_treatment >= 0, 1, 0)),\n    yname = \"y\",\n    first_stage = ~ x1 | id + year,\n    second_stage = ~ i(time_to_treatment, ref = c(-1,-1000)),\n    treatment = \"treat\" ,\n    cluster_var = \"id\"\n)\n\nfixest::esttable(est)\n#>                                       est\n#> Dependent Var.:                         y\n#>                                          \n#> time_to_treatment = -9  0.3518** (0.1332)\n#> time_to_treatment = -8  -0.3130* (0.1213)\n#> time_to_treatment = -7    0.0894 (0.2367)\n#> time_to_treatment = -6    0.0312 (0.2176)\n#> time_to_treatment = -5   -0.2079 (0.1519)\n#> time_to_treatment = -4   -0.1152 (0.1438)\n#> time_to_treatment = -3   -0.0127 (0.1483)\n#> time_to_treatment = -2    0.1503 (0.1440)\n#> time_to_treatment = 0  -5.139*** (0.3680)\n#> time_to_treatment = 1  -3.480*** (0.3784)\n#> time_to_treatment = 2  -2.021*** (0.3055)\n#> time_to_treatment = 3   -0.6965. (0.3947)\n#> time_to_treatment = 4    1.070** (0.3501)\n#> time_to_treatment = 5   2.173*** (0.4456)\n#> time_to_treatment = 6   4.449*** (0.3680)\n#> time_to_treatment = 7   4.864*** (0.3698)\n#> time_to_treatment = 8   6.187*** (0.2702)\n#> ______________________ __________________\n#> S.E. type                          Custom\n#> Observations                          950\n#> R2                                0.62486\n#> Adj. R2                           0.61843\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfixest::iplot(\n    est,\n    main = \"Event study\",\n    xlab = \"Time to treatment\",\n    ref.line = -1\n)\n\ncoefplot(est)\nmult_est <- did2s::event_study(\n    data = fixest::base_stagg |>\n        dplyr::mutate(year_treated = dplyr::if_else(year_treated == 10000, 0, year_treated)),\n    gname = \"year_treated\",\n    idname = \"id\",\n    tname = \"year\",\n    yname = \"y\",\n    estimator = \"all\"\n)\n#> Error in purrr::map(., function(y) { : ℹ In index: 1.\n#> ℹ With name: y.\n#> Caused by error in `.subset2()`:\n#> ! no such index at level 1\ndid2s::plot_event_study(mult_est)\nlibrary(didimputation)\nlibrary(fixest)\ndata(\"base_stagg\")\n\ndid_imputation(\n    data = base_stagg,\n    yname = \"y\",\n    gname = \"year_treated\",\n    tname = \"year\",\n    idname = \"id\"\n)"},{"path":"difference-in-differences.html","id":"de2020two","chapter":"26 Difference-in-differences","heading":"26.9.5 Clément De Chaisemartin and d’Haultfoeuille (2020)","text":"use twowayfeweights GitHub (Clément De Chaisemartin d’Haultfoeuille 2020)Average instant treatment effect changes treatment\nrelaxes -carryover-effect assumption.\nAverage instant treatment effect changes treatmentThis relaxes -carryover-effect assumption.Drawbacks:\nobserve treatment effects manifest time.\nDrawbacks:observe treatment effects manifest time.still isn’t good package estimator.don’t recommend TwoWayFEWeights since gives aggregated average treatment effect post-treatment periods, period.","code":"\n# remotes::install_github(\"shuo-zhang-ucsb/did_multiplegt\") \nlibrary(DIDmultiplegt)\nlibrary(fixest)\nlibrary(tidyverse)\n\ndata(\"base_stagg\")\n\nres <-\n    did_multiplegt(\n        df = base_stagg |>\n            dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n        Y        = \"y\",\n        G        = \"year_treated\",\n        T        = \"year\",\n        D        = \"treatment\",\n        controls = \"x1\",\n        # brep     = 20, # getting SE will take forever\n        placebo  = 5,\n        dynamic  = 5, \n        average_effect = \"simple\"\n    )\n\nhead(res)\n#> $effect\n#> treatment \n#> -5.214207 \n#> \n#> $N_effect\n#> [1] 675\n#> \n#> $N_switchers_effect\n#> [1] 45\n#> \n#> $dynamic_1\n#> [1] -3.63556\n#> \n#> $N_dynamic_1\n#> [1] 580\n#> \n#> $N_switchers_effect_1\n#> [1] 40\nlibrary(TwoWayFEWeights)\n\nres <- twowayfeweights(\n    data = base_stagg |> dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n    Y = \"y\",\n    G = \"year_treated\",\n    T = \"year\",\n    D = \"treatment\", \n    summary_measures = T\n)\n\nprint(res)\n#> Under the common trends assumption, beta estimates a weighted sum of 45 ATTs.\n#> 41 ATTs receive a positive weight, and 4 receive a negative weight.\n#> \n#> ────────────────────────────────────────── \n#> Treat. var: treatment    ATTs    Σ weights \n#> ────────────────────────────────────────── \n#> Positive weights           41       1.0238 \n#> Negative weights            4      -0.0238 \n#> ────────────────────────────────────────── \n#> Total                      45            1 \n#> ──────────────────────────────────────────\n#> \n#> Summary Measures:\n#>   TWFE Coefficient (β_fe): -3.4676\n#>   min σ(Δ) compatible with β_fe and Δ_TR = 0: 4.8357\n#>   min σ(Δ) compatible with β_fe and Δ_TR of a different sign: 36.1549\n#>   Reference: Corollary 1, de Chaisemartin, C and D'Haultfoeuille, X (2020a)\n#> \n#> The development of this package was funded by the European Union (ERC, REALLYCREDIBLE,GA N. 101043899)."},{"path":"difference-in-differences.html","id":"callaway2021difference","chapter":"26 Difference-in-differences","heading":"26.9.6 Callaway and Sant’Anna (2021)","text":"staggered packagestaggered packageGroup-time average treatment effectGroup-time average treatment effectFisher’s Randomization Test (.e., permutation test)\\(H_0\\): \\(TE = 0\\)","code":"\nlibrary(staggered) \nlibrary(fixest)\ndata(\"base_stagg\")\n\n# simple weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245\n\n# cohort weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"cohort\"\n)\n#>    estimate        se se_neyman\n#> 1 -2.724242 0.2701093 0.2701745\n\n# calendar weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"calendar\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.5861831 0.1768297 0.1770729\n\nres <- staggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"eventstudy\", \n    eventTime = -9:8\n)\nhead(res)\n#>      estimate        se se_neyman eventTime\n#> 1  0.20418779 0.1045821 0.1045821        -9\n#> 2 -0.06215104 0.1669703 0.1670886        -8\n#> 3  0.02744671 0.1413273 0.1420377        -7\n#> 4 -0.02131747 0.2203695 0.2206338        -6\n#> 5 -0.30690897 0.2015697 0.2036412        -5\n#> 6  0.05594029 0.1908101 0.1921745        -4\n\n\nggplot(\n    res |> mutate(\n        ymin_ptwise = estimate + 1.96 * se,\n        ymax_ptwise = estimate - 1.96 * se\n    ),\n    aes(x = eventTime, y = estimate)\n) +\n    geom_pointrange(aes(ymin = ymin_ptwise, ymax = ymax_ptwise)) +\n    geom_hline(yintercept = 0) +\n    xlab(\"Event Time\") +\n    ylab(\"Estimate\") +\n    causalverse::ama_theme()\n# Callaway and Sant'Anna estimator for the simple weighted average\nstaggered_cs(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7994889 0.4484987 0.4486122\n\n# Sun and Abraham estimator for the simple weighted average\nstaggered_sa(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7551901 0.4407818 0.4409525\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\",\n    compute_fisher = T,\n    num_fisher_permutations = 100\n)\n#>     estimate        se se_neyman fisher_pval fisher_pval_se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245           0                     0\n#>   num_fisher_permutations\n#> 1                     100"},{"path":"difference-in-differences.html","id":"sun2021estimating","chapter":"26 Difference-in-differences","heading":"26.9.7 L. Sun and Abraham (2021)","text":"paper utilizes Cohort Average Treatment Effects Treated (CATT), measures cohort-specific average difference outcomes relative never treated, offering detailed analysis Goodman-Bacon (2021). scenarios lacking never-treated group, method designates last cohort treated control group.Parameter interest cohort-specific ATT \\(l\\) periods int ital treatment period \\(e\\)\\[\nCATT = E[Y_{, e + } - Y_{, e + }^\\infty|E_i = e]\n\\]paper uses interaction-weighted estimator panel data setting, original paper Gibbons, Suárez Serrato, Urbancic (2018) used idea cross-sectional setting.Callaway Sant’Anna (2021) explores group-time average treatment effects, employing cohorts yet treated controls, permits conditioning time-varying covariates.Callaway Sant’Anna (2021) explores group-time average treatment effects, employing cohorts yet treated controls, permits conditioning time-varying covariates.Athey Imbens (2022) examines treatment effect relation counterfactual outcome always-treated group, diverging conventional focus never-treated.Athey Imbens (2022) examines treatment effect relation counterfactual outcome always-treated group, diverging conventional focus never-treated.Borusyak, Jaravel, Spiess (2021) presumes uniform treatment effect across cohorts, effectively simplifying CATT ATT.Borusyak, Jaravel, Spiess (2021) presumes uniform treatment effect across cohorts, effectively simplifying CATT ATT.Identifying Assumptions dynamic TWFE:Parallel Trends: Baseline outcomes follow parallel trends across cohorts treatment.\ngives us CATT (including , included bins, excluded bins)\nParallel Trends: Baseline outcomes follow parallel trends across cohorts treatment.gives us CATT (including , included bins, excluded bins)Anticipatory Behavior: effect treatment pre-treatment periods, indicating outcomes influenced anticipation treatment.Anticipatory Behavior: effect treatment pre-treatment periods, indicating outcomes influenced anticipation treatment.Treatment Effect Homogeneity: treatment effect consistent across cohorts relative period. adoption cohort path treatment effects. words, trajectory treatment cohort similar. Compare designs:\nAthey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.\nBorusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.\nCallaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.\nClement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.\nGoodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.\nL. Sun Abraham (2021) allows treatment effect heterogeneity across units time.\nTreatment Effect Homogeneity: treatment effect consistent across cohorts relative period. adoption cohort path treatment effects. words, trajectory treatment cohort similar. Compare designs:Athey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.Athey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.Borusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.Borusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.Callaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.Callaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.Clement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.Clement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.Goodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.Goodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.L. Sun Abraham (2021) allows treatment effect heterogeneity across units time.L. Sun Abraham (2021) allows treatment effect heterogeneity across units time.Sources Heterogeneous Treatment EffectsAdoption cohorts can differ based certain covariates. Similarly, composition units within adoption cohort different.Adoption cohorts can differ based certain covariates. Similarly, composition units within adoption cohort different.response treatment varies among cohorts units self-select initial treatment timing based anticipated treatment effects. However, self-selection still compatible parallel trends assumption. true units choose based evaluation baseline outcomes - , baseline outcomes similar (following parallel trends), might see selection treatment based evaluation baseline outcome.response treatment varies among cohorts units self-select initial treatment timing based anticipated treatment effects. However, self-selection still compatible parallel trends assumption. true units choose based evaluation baseline outcomes - , baseline outcomes similar (following parallel trends), might see selection treatment based evaluation baseline outcome.Treatment effects can vary across cohorts due calendar time-varying effects, changes economic conditions.Treatment effects can vary across cohorts due calendar time-varying effects, changes economic conditions.Notes:TWFE, actually drop 2 terms avoid multicollinearity:\nPeriod right treatment (one known paper)\nDrop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.\nTWFE, actually drop 2 terms avoid multicollinearity:Period right treatment (one known paper)Period right treatment (one known paper)Drop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.Drop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.Contamination treatment effect estimates excluded periods type “normalization”. avoid , assume pre-treatment periods CATT.\nL. Sun Abraham (2021) estimation method gives reasonable weights CATT (..e, weights sum 1, non negative). estimate weighted average CATT weights shares cohorts experience least \\(l\\) periods treatment, normalized size total periods \\(g\\).\nContamination treatment effect estimates excluded periods type “normalization”. avoid , assume pre-treatment periods CATT.L. Sun Abraham (2021) estimation method gives reasonable weights CATT (..e, weights sum 1, non negative). estimate weighted average CATT weights shares cohorts experience least \\(l\\) periods treatment, normalized size total periods \\(g\\).Aggregation CATT similar Callaway Sant’Anna (2021)Aggregation CATT similar Callaway Sant’Anna (2021)Applicationcan use fixest r sunab functionUsing syntax fixest","code":"\nlibrary(fixest)\ndata(\"base_stagg\")\nres_sa20 = feols(y ~ x1 + sunab(year_treated, year) | id + year, base_stagg)\niplot(res_sa20)\n\nsummary(res_sa20, agg = \"att\")\n#> OLS estimation, Dep. Var.: y\n#> Observations: 950 \n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>      Estimate Std. Error  t value  Pr(>|t|)    \n#> x1   0.994678   0.018378 54.12293 < 2.2e-16 ***\n#> ATT -1.133749   0.205070 -5.52858 2.882e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n\nsummary(res_sa20, agg = c(\"att\" = \"year::[^-]\")) \n#> OLS estimation, Dep. Var.: y\n#> Observations: 950 \n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>                      Estimate Std. Error   t value   Pr(>|t|)    \n#> x1                   0.994678   0.018378 54.122928  < 2.2e-16 ***\n#> year::-9:cohort::10  0.351766   0.359073  0.979649 3.2977e-01    \n#> year::-8:cohort::9   0.033914   0.471437  0.071937 9.4281e-01    \n#> year::-8:cohort::10 -0.191932   0.352896 -0.543876 5.8781e-01    \n#> year::-7:cohort::8  -0.589387   0.736910 -0.799809 4.2584e-01    \n#> year::-7:cohort::9   0.872995   0.493427  1.769249 8.0096e-02 .  \n#> year::-7:cohort::10  0.019512   0.603411  0.032336 9.7427e-01    \n#> year::-6:cohort::7  -0.042147   0.865736 -0.048683 9.6127e-01    \n#> year::-6:cohort::8  -0.657571   0.573257 -1.147078 2.5426e-01    \n#> year::-6:cohort::9   0.877743   0.533331  1.645775 1.0315e-01    \n#> year::-6:cohort::10 -0.403635   0.347412 -1.161832 2.4825e-01    \n#> year::-5:cohort::6  -0.658034   0.913407 -0.720418 4.7306e-01    \n#> year::-5:cohort::7  -0.316974   0.697939 -0.454158 6.5076e-01    \n#> year::-5:cohort::8  -0.238213   0.469744 -0.507113 6.1326e-01    \n#> year::-5:cohort::9   0.301477   0.604201  0.498968 6.1897e-01    \n#> year::-5:cohort::10 -0.564801   0.463214 -1.219308 2.2578e-01    \n#> year::-4:cohort::5  -0.983453   0.634492 -1.549984 1.2451e-01    \n#> year::-4:cohort::6   0.360407   0.858316  0.419900 6.7552e-01    \n#> year::-4:cohort::7  -0.430610   0.661356 -0.651102 5.1657e-01    \n#> year::-4:cohort::8  -0.895195   0.374901 -2.387816 1.8949e-02 *  \n#> year::-4:cohort::9  -0.392478   0.439547 -0.892914 3.7418e-01    \n#> year::-4:cohort::10  0.519001   0.597880  0.868069 3.8757e-01    \n#> year::-3:cohort::4   0.591288   0.680169  0.869324 3.8688e-01    \n#> year::-3:cohort::5  -1.000650   0.971741 -1.029749 3.0577e-01    \n#> year::-3:cohort::6   0.072188   0.652641  0.110609 9.1216e-01    \n#> year::-3:cohort::7  -0.836820   0.804275 -1.040465 3.0079e-01    \n#> year::-3:cohort::8  -0.783148   0.701312 -1.116691 2.6697e-01    \n#> year::-3:cohort::9   0.811285   0.564470  1.437251 1.5397e-01    \n#> year::-3:cohort::10  0.527203   0.320051  1.647250 1.0285e-01    \n#> year::-2:cohort::3   0.036941   0.673771  0.054828 9.5639e-01    \n#> year::-2:cohort::4   0.832250   0.859544  0.968246 3.3541e-01    \n#> year::-2:cohort::5  -1.574086   0.525563 -2.995051 3.5076e-03 ** \n#> year::-2:cohort::6   0.311758   0.832095  0.374666 7.0875e-01    \n#> year::-2:cohort::7  -0.558631   0.871993 -0.640638 5.2332e-01    \n#> year::-2:cohort::8   0.429591   0.305270  1.407250 1.6265e-01    \n#> year::-2:cohort::9   1.201899   0.819186  1.467188 1.4566e-01    \n#> year::-2:cohort::10 -0.002429   0.682087 -0.003562 9.9717e-01    \n#> att                 -1.133749   0.205070 -5.528584 2.8820e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n# alternatively\nsummary(res_sa20, agg = c(\"att\" = \"year::[012345678]\")) |> \n    etable(digits = 2)\n#>                         summary(res_..\n#> Dependent Var.:                      y\n#>                                       \n#> x1                      0.99*** (0.02)\n#> year = -9 x cohort = 10    0.35 (0.36)\n#> year = -8 x cohort = 9     0.03 (0.47)\n#> year = -8 x cohort = 10   -0.19 (0.35)\n#> year = -7 x cohort = 8    -0.59 (0.74)\n#> year = -7 x cohort = 9    0.87. (0.49)\n#> year = -7 x cohort = 10    0.02 (0.60)\n#> year = -6 x cohort = 7    -0.04 (0.87)\n#> year = -6 x cohort = 8    -0.66 (0.57)\n#> year = -6 x cohort = 9     0.88 (0.53)\n#> year = -6 x cohort = 10   -0.40 (0.35)\n#> year = -5 x cohort = 6    -0.66 (0.91)\n#> year = -5 x cohort = 7    -0.32 (0.70)\n#> year = -5 x cohort = 8    -0.24 (0.47)\n#> year = -5 x cohort = 9     0.30 (0.60)\n#> year = -5 x cohort = 10   -0.56 (0.46)\n#> year = -4 x cohort = 5    -0.98 (0.63)\n#> year = -4 x cohort = 6     0.36 (0.86)\n#> year = -4 x cohort = 7    -0.43 (0.66)\n#> year = -4 x cohort = 8   -0.90* (0.37)\n#> year = -4 x cohort = 9    -0.39 (0.44)\n#> year = -4 x cohort = 10    0.52 (0.60)\n#> year = -3 x cohort = 4     0.59 (0.68)\n#> year = -3 x cohort = 5     -1.0 (0.97)\n#> year = -3 x cohort = 6     0.07 (0.65)\n#> year = -3 x cohort = 7    -0.84 (0.80)\n#> year = -3 x cohort = 8    -0.78 (0.70)\n#> year = -3 x cohort = 9     0.81 (0.56)\n#> year = -3 x cohort = 10    0.53 (0.32)\n#> year = -2 x cohort = 3     0.04 (0.67)\n#> year = -2 x cohort = 4     0.83 (0.86)\n#> year = -2 x cohort = 5   -1.6** (0.53)\n#> year = -2 x cohort = 6     0.31 (0.83)\n#> year = -2 x cohort = 7    -0.56 (0.87)\n#> year = -2 x cohort = 8     0.43 (0.31)\n#> year = -2 x cohort = 9      1.2 (0.82)\n#> year = -2 x cohort = 10  -0.002 (0.68)\n#> att                     -1.1*** (0.21)\n#> Fixed-Effects:          --------------\n#> id                                 Yes\n#> year                               Yes\n#> _______________________ ______________\n#> S.E.: Clustered                 by: id\n#> Observations                       950\n#> R2                             0.90982\n#> Within R2                      0.87641\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# devtools::install_github(\"kylebutts/fwlplot\")\nlibrary(fwlplot)\nfwl_plot(y ~ x1, data = base_stagg)\n\nfwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100)\n\nfwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100, fsplit = ~ treated)"},{"path":"difference-in-differences.html","id":"wooldridge2022simple","chapter":"26 Difference-in-differences","heading":"26.9.8 Wooldridge (2022)","text":"use etwfe(Extended two-way Fixed Effects) (Wooldridge 2022)","code":""},{"path":"difference-in-differences.html","id":"doubly-robust-did","chapter":"26 Difference-in-differences","heading":"26.9.9 Doubly Robust DiD","text":"Also known locally efficient doubly robust (Sant’Anna Zhao 2020)Code example authorsThe package (method) rather limited application:Use OLS (handle glm)Use OLS (handle glm)Canonical (handle DDD).Canonical (handle DDD).","code":"\nlibrary(DRDID)\ndata(\"nsw_long\")\neval_lalonde_cps <-\n    subset(nsw_long, nsw_long$treated == 0 | nsw_long$sample == 2)\nhead(eval_lalonde_cps)\n#>   id year treated age educ black married nodegree dwincl      re74 hisp\n#> 1  1 1975      NA  42   16     0       1        0     NA     0.000    0\n#> 2  1 1978      NA  42   16     0       1        0     NA     0.000    0\n#> 3  2 1975      NA  20   13     0       0        0     NA  2366.794    0\n#> 4  2 1978      NA  20   13     0       0        0     NA  2366.794    0\n#> 5  3 1975      NA  37   12     0       1        0     NA 25862.322    0\n#> 6  3 1978      NA  37   12     0       1        0     NA 25862.322    0\n#>   early_ra sample experimental         re\n#> 1       NA      2            0     0.0000\n#> 2       NA      2            0   100.4854\n#> 3       NA      2            0  3317.4678\n#> 4       NA      2            0  4793.7451\n#> 5       NA      2            0 22781.8555\n#> 6       NA      2            0 25564.6699\n\n\n# locally efficient doubly robust DiD Estimators for the ATT\nout <-\n    drdid(\n        yname = \"re\",\n        tname = \"year\",\n        idname = \"id\",\n        dname = \"experimental\",\n        xformla = ~ age + educ + black + married + nodegree + hisp + re74,\n        data = eval_lalonde_cps,\n        panel = TRUE\n    )\nsummary(out)\n#>  Call:\n#> drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n#>     xformla = ~age + educ + black + married + nodegree + hisp + \n#>         re74, data = eval_lalonde_cps, panel = TRUE)\n#> ------------------------------------------------------------------\n#>  Further improved locally efficient DR DID estimator for the ATT:\n#>  \n#>    ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n#> -901.2703   393.6247   -2.2897     0.022    -1672.7747  -129.766 \n#> ------------------------------------------------------------------\n#>  Estimator based on panel data.\n#>  Outcome regression est. method: weighted least squares.\n#>  Propensity score est. method: inverse prob. tilting.\n#>  Analytical standard error.\n#> ------------------------------------------------------------------\n#>  See Sant'Anna and Zhao (2020) for details.\n\n\n\n# Improved locally efficient doubly robust DiD estimator \n# for the ATT, with panel data\n# drdid_imp_panel()\n\n# Locally efficient doubly robust DiD estimator for the ATT, \n# with panel data\n# drdid_panel()\n\n# Locally efficient doubly robust DiD estimator for the ATT, \n# with repeated cross-section data\n# drdid_rc()\n\n# Improved locally efficient doubly robust DiD estimator for the ATT, \n# with repeated cross-section data\n# drdid_imp_rc()"},{"path":"difference-in-differences.html","id":"augmentedforward-did","chapter":"26 Difference-in-differences","heading":"26.9.10 Augmented/Forward DID","text":"Methods Limited Pre-Treatment Periods:Augmented (K. T. Li Van den Bulte 2023)Forward (K. T. Li 2024)","code":""},{"path":"difference-in-differences.html","id":"multiple-treatments","chapter":"26 Difference-in-differences","heading":"26.10 Multiple Treatments","text":"2 treatments setting, always try model one regression see whether significantly different.Never use one treated groups control , run separate regression.check answer\\[\n\\begin{aligned}\nY_{} &= \\alpha + \\gamma_1 Treat1_{} + \\gamma_2 Treat2_{} + \\lambda Post_t  \\\\\n&+ \\delta_1(Treat1_i \\times Post_t) + \\delta_2(Treat2_i \\times Post_t) + \\epsilon_{}\n\\end{aligned}\n\\](Fricke 2017)(Clement De Chaisemartin D’haultfœuille 2023) video code","code":""},{"path":"difference-in-differences.html","id":"mediation-under-did","chapter":"26 Difference-in-differences","heading":"26.11 Mediation Under DiD","text":"Check post","code":""},{"path":"difference-in-differences.html","id":"assumptions-1","chapter":"26 Difference-in-differences","heading":"26.12 Assumptions","text":"Parallel Trends: Difference treatment control groups remain constant treatment.\nused cases \nobserve event\ntreatment control groups\n\ncases \ntreatment random\nconfounders.\n\nsupport use\nPlacebo test\nPrior Parallel Trends Test\n\nParallel Trends: Difference treatment control groups remain constant treatment.used cases \nobserve event\ntreatment control groups\nused cases whereyou observe eventyou observe eventyou treatment control groupsyou treatment control groupsnot cases \ntreatment random\nconfounders.\ncases wheretreatment randomtreatment randomconfounders.confounders.support use\nPlacebo test\nPrior Parallel Trends Test\nsupport usePlacebo testPlacebo testPrior Parallel Trends TestPrior Parallel Trends TestLinear additive effects (group/unit specific time-specific):\nadditively interact, use weighted 2FE estimator (Imai Kim 2021)\nTypically seen Staggered Dif-n-dif\nLinear additive effects (group/unit specific time-specific):additively interact, use weighted 2FE estimator (Imai Kim 2021)additively interact, use weighted 2FE estimator (Imai Kim 2021)Typically seen Staggered Dif-n-difTypically seen Staggered Dif-n-difNo anticipation: causal effect treatment implementation.anticipation: causal effect treatment implementation.Possible issuesEstimate dependent functional form:\nsize response depends (nonlinearly) size intervention, might want look difference group high intensity vs. low.\nEstimate dependent functional form:size response depends (nonlinearly) size intervention, might want look difference group high intensity vs. low.Selection (time–varying) unobservables\nCan use overall sensitivity coefficient estimates hidden bias using Rosenbaum Bounds\nSelection (time–varying) unobservablesCan use overall sensitivity coefficient estimates hidden bias using Rosenbaum BoundsLong-term effects\nParallel trends likely observed shorter period (window observation)\nLong-term effectsParallel trends likely observed shorter period (window observation)Heterogeneous effects\nDifferent intensity (e.g., doses) different groups.\nHeterogeneous effectsDifferent intensity (e.g., doses) different groups.Ashenfelter dip (Ashenfelter Card 1985) (job training program participant likely experience earning drop prior enrolling programs)\nParticipants systemically different nonparticipants treatment, leading question permanent transitory changes.\nfix transient endogeneity calculate long-run differences (exclude number periods symmetrically around adoption/ implementation date). see sustained impact, strong evidence causal impact policy. (Proserpio Zervas 2017b) (James J. Heckman Smith 1999) (Jepsen, Troske, Coomes 2014) (X. Li, Gan, Hu 2011)\nAshenfelter dip (Ashenfelter Card 1985) (job training program participant likely experience earning drop prior enrolling programs)Participants systemically different nonparticipants treatment, leading question permanent transitory changes.fix transient endogeneity calculate long-run differences (exclude number periods symmetrically around adoption/ implementation date). see sustained impact, strong evidence causal impact policy. (Proserpio Zervas 2017b) (James J. Heckman Smith 1999) (Jepsen, Troske, Coomes 2014) (X. Li, Gan, Hu 2011)Response event might immediate (can’t observed right away dependent variable)\nUsing lagged dependent variable \\(Y_{-1}\\) might appropriate (Blundell Bond 1998)\nResponse event might immediate (can’t observed right away dependent variable)Using lagged dependent variable \\(Y_{-1}\\) might appropriate (Blundell Bond 1998)factors affect difference trends two groups (.e., treatment control) bias estimation.factors affect difference trends two groups (.e., treatment control) bias estimation.Correlated observations within group timeCorrelated observations within group timeIncidental parameters problems (Lancaster 2000): ’s always better use individual time fixed effect.Incidental parameters problems (Lancaster 2000): ’s always better use individual time fixed effect.examining effects variation treatment timing, careful negative weights (per group) can negative heterogeneity treatment effects time. Example: [Athey Imbens (2022)](Borusyak, Jaravel, Spiess 2021)(Goodman-Bacon 2021). case use new estimands proposed @callaway2021difference(Clément De Chaisemartin d’Haultfoeuille 2020), package. expect lags leads, see (L. Sun Abraham 2021)examining effects variation treatment timing, careful negative weights (per group) can negative heterogeneity treatment effects time. Example: [Athey Imbens (2022)](Borusyak, Jaravel, Spiess 2021)(Goodman-Bacon 2021). case use new estimands proposed @callaway2021difference(Clément De Chaisemartin d’Haultfoeuille 2020), package. expect lags leads, see (L. Sun Abraham 2021)(Gibbons, Suárez Serrato, Urbancic 2018) caution suspect treatment effect treatment variance vary across groups(Gibbons, Suárez Serrato, Urbancic 2018) caution suspect treatment effect treatment variance vary across groups","code":""},{"path":"difference-in-differences.html","id":"prior-parallel-trends-test","chapter":"26 Difference-in-differences","heading":"26.12.1 Prior Parallel Trends Test","text":"Plot average outcomes time treatment control group treatment time.Statistical test difference trends (using data treatment period)\\[\nY = \\alpha_g + \\beta_1 T + \\beta_2 T\\times G + \\epsilon\n\\]\\(Y\\) = outcome variable\\(Y\\) = outcome variable\\(\\alpha_g\\) = group fixed effects\\(\\alpha_g\\) = group fixed effects\\(T\\) = time (e.g., specific year, month)\\(T\\) = time (e.g., specific year, month)\\(\\beta_2\\) = different time trends group\\(\\beta_2\\) = different time trends groupHence, \\(\\beta_2 =0\\) provides evidence differences trend two groups prior time treatment.can also use different functional forms (e..g, polynomial nonlinear).\\(\\beta_2 \\neq 0\\) statistically, possible reasons can :Statistical significance can driven large sampleStatistical significance can driven large sampleOr trends consistent, just one period deviation can throw trends. Hence, statistical statistical significance.trends consistent, just one period deviation can throw trends. Hence, statistical statistical significance.Technically, can still salvage research including time fixed effects, instead just --time fixed effect (actually, researchers mechanically anyway nowadays). However, side effect can time fixed effects can also absorb part treatment effect well, especially cases treatment effects vary time (.e., stronger weaker time) (Wolfers 2003).Debate:(Kahn-Lang Lang 2020) argue plausible treatment control groups similar trends, also levels. observe dissimilar levels prior treatment, okay think affect future trends?\nShow plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).\ncase don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.\n(Kahn-Lang Lang 2020) argue plausible treatment control groups similar trends, also levels. observe dissimilar levels prior treatment, okay think affect future trends?Show plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).Show plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).case don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.case don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.Pre-trend statistical tests: (Roth 2022) provides evidence test usually powered.\nSee PretrendsPower pretrends packages correcting .\nPre-trend statistical tests: (Roth 2022) provides evidence test usually powered.See PretrendsPower pretrends packages correcting .Parallel trends assumption specific transformation units outcome (Roth Sant’Anna 2023)\nSee falsification test (\\(H_0\\): parallel trends insensitive functional form).\nParallel trends assumption specific transformation units outcome (Roth Sant’Anna 2023)See falsification test (\\(H_0\\): parallel trends insensitive functional form).alarming since one periods significantly different 0, means parallel trends assumption plausible.cases might violations parallel trends assumption, check (Rambachan Roth 2023)Impose restrictions different post-treatment violations parallel trends can pre-trends.Impose restrictions different post-treatment violations parallel trends can pre-trends.Partial identification causal parameterPartial identification causal parameterSensitivity analysisSensitivity analysisAlternatively, Ban Kedagni (2022) propose method information set (.e., pre-treatment covariates), assumption selection bias post-treatment period (.e., lies within convex hull selection biases), can still identify set ATT, stricter assumption selection bias policymakers perspective, can also point estimate.Alternatively, can use pretrends package examine assumptions (Roth 2022)","code":"\nlibrary(tidyverse)\nlibrary(fixest)\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    filter(Quarter_Num <= 3) %>% \n    # Treatment variable\n    dplyr::mutate(California = State == 'California')\n\n# use my package\ncausalverse::plot_par_trends(\n    data = od,\n    metrics_and_names = list(\"Rate\" = \"Rate\"),\n    treatment_status_var = \"California\",\n    time_var = list(Quarter_Num = \"Time\"),\n    display_CI = F\n)\n#> [[1]]\n\n# do it manually\n# always good but plot the dependent out\nod |>\n    # group by treatment status and time\n    dplyr::group_by(California, Quarter) |>\n    dplyr::summarize_all(mean) |>\n    dplyr::ungroup() |>\n    # view()\n    \n    ggplot2::ggplot(aes(x = Quarter_Num, y = Rate, color = California)) +\n    ggplot2::geom_line() +\n    causalverse::ama_theme()\n\n\n# but it's also important to use statistical test\nprior_trend <- fixest::feols(Rate ~ i(Quarter_Num, California) | State + Quarter,\n               data = od)\n\nfixest::coefplot(prior_trend, grid = F)\nfixest::iplot(prior_trend, grid = F)\n# https://github.com/asheshrambachan/HonestDiD\n# remotes::install_github(\"asheshrambachan/HonestDiD\")\n# library(HonestDiD)"},{"path":"difference-in-differences.html","id":"placebo-test-1","chapter":"26 Difference-in-differences","heading":"26.12.2 Placebo Test","text":"Procedure:Sample data period treatment time.Consider different fake cutoff time, either\nTry whole sequence time\nGenerate random treatment period, use randomization inference account sampling distribution fake effect.\nTry whole sequence timeTry whole sequence timeGenerate random treatment period, use randomization inference account sampling distribution fake effect.Generate random treatment period, use randomization inference account sampling distribution fake effect.Estimate model post-time = 1 fake cutoffA significant coefficient means violate parallel trends! big problem.Alternatively,data multiple control groups, drop treated group, assign another control group “fake” treated group. even fails (.e., find significant effect) among control groups, can still fine. However, method used Synthetic ControlCode theeffectbook.netWe like “supposed” insignificant.","code":"\nlibrary(tidyverse)\nlibrary(fixest)\n\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    dplyr::filter(Quarter_Num <= 3) %>%\n    \n    # Create fake treatment variables\n    dplyr::mutate(\n        FakeTreat1 = State == 'California' &\n            Quarter %in% c('Q12011', 'Q22011'),\n        FakeTreat2 = State == 'California' &\n            Quarter == 'Q22011'\n    )\n\n\nclfe1 <- fixest::feols(Rate ~ FakeTreat1 | State + Quarter,\n               data = od)\nclfe2 <- fixest::feols(Rate ~ FakeTreat2 | State + Quarter,\n               data = od)\n\nfixest::etable(clfe1,clfe2)\n#>                           clfe1            clfe2\n#> Dependent Var.:            Rate             Rate\n#>                                                 \n#> FakeTreat1TRUE  0.0061 (0.0051)                 \n#> FakeTreat2TRUE                  -0.0017 (0.0028)\n#> Fixed-Effects:  --------------- ----------------\n#> State                       Yes              Yes\n#> Quarter                     Yes              Yes\n#> _______________ _______________ ________________\n#> S.E.: Clustered       by: State        by: State\n#> Observations                 81               81\n#> R2                      0.99377          0.99376\n#> Within R2               0.00192          0.00015\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"difference-in-differences.html","id":"assumption-violations","chapter":"26 Difference-in-differences","heading":"26.12.3 Assumption Violations","text":"Endogenous TimingIf timing units can influenced strategic decisions analysis, instrumental variable approach control function can used control endogeneity timing.Questionable CounterfactualsIn situations control units may serve reliable counterfactual treated units, matching methods propensity score matching generalized random forest can utilized. Additional methods can found Matching Methods.","code":""},{"path":"difference-in-differences.html","id":"robustness-checks","chapter":"26 Difference-in-differences","heading":"26.12.4 Robustness Checks","text":"Placebo (estimate \\(\\neq 0\\), parallel trend violated, original biased):\nGroup: Use fake treatment groups: population affect treatment\nTime: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).\nPlacebo (estimate \\(\\neq 0\\), parallel trend violated, original biased):Group: Use fake treatment groups: population affect treatmentGroup: Use fake treatment groups: population affect treatmentTime: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).Time: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).Possible alternative control group: Expected results similarPossible alternative control group: Expected results similarTry different windows (away treatment point, factors can creep nullify effect).Try different windows (away treatment point, factors can creep nullify effect).Treatment Reversal (don’t see treatment event)Treatment Reversal (don’t see treatment event)Higher-order polynomial time trend (relax linearity assumption)Higher-order polynomial time trend (relax linearity assumption)Test whether dependent variables affected event indeed unaffected.\nUse control treatment period (\\(\\neq0\\), problem)\nTest whether dependent variables affected event indeed unaffected.Use control treatment period (\\(\\neq0\\), problem)triple-difference strategy involves examining interaction treatment variable probability affected program, group-level participation rate. identification assumption differential trends high low participation groups early versus late implementing countries.triple-difference strategy involves examining interaction treatment variable probability affected program, group-level participation rate. identification assumption differential trends high low participation groups early versus late implementing countries.","code":""},{"path":"changes-in-changes.html","id":"changes-in-changes","chapter":"27 Changes-in-Changes","heading":"27 Changes-in-Changes","text":"Introduction\nChanges--Changes (CiC) estimator, introduced Athey Imbens (2006), alternative Difference--Differences () strategy.\nUnlike traditional , estimates Average Treatment Effect Treated (ATT), CiC focuses Quantile Treatment Effect Treated (QTT).\nQTT captures difference potential outcome distributions treated units specific quantile.\nBeyond Averages: Policymakers often look beyond average program impacts, considering benefits distributed across different groups.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.\nTraditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.\nQTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.\n\nQTE vs. ATE: QTEs provide detailed insights distributional impacts, also allow recovery ATEs. However, ATEs usually identified weaker assumptions, making QTEs suitable exploring shape treatment effects rather just central tendency.\nIntroductionThe Changes--Changes (CiC) estimator, introduced Athey Imbens (2006), alternative Difference--Differences () strategy.Unlike traditional , estimates Average Treatment Effect Treated (ATT), CiC focuses Quantile Treatment Effect Treated (QTT).QTT captures difference potential outcome distributions treated units specific quantile.Beyond Averages: Policymakers often look beyond average program impacts, considering benefits distributed across different groups.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.\nTraditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.\nQTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.Traditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.QTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.QTE vs. ATE: QTEs provide detailed insights distributional impacts, also allow recovery ATEs. However, ATEs usually identified weaker assumptions, making QTEs suitable exploring shape treatment effects rather just central tendency.Key Concepts\nQuantile Treatment Effect Treated (QTT): Difference quantiles treated units’ potential outcome distributions.\nRank Preservation: Assumes unit’s rank remains constant across potential outcome distributions—strong assumption.\nCounterfactual Distribution: Estimation focuses determining distribution treated units period 1.\nKey ConceptsQuantile Treatment Effect Treated (QTT): Difference quantiles treated units’ potential outcome distributions.Rank Preservation: Assumes unit’s rank remains constant across potential outcome distributions—strong assumption.Counterfactual Distribution: Estimation focuses determining distribution treated units period 1.Estimating QTT\nCiC uses four distributions 2x2 design:\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\n\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\n\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\n\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.\n\nQTT defined difference inverses \\(F_{Y(1),11}\\) counterfactual distribution \\(F_{Y(0),11}\\) quantile \\(q\\):\n\\[\n  \\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta)\n  \\]Estimating QTTCiC uses four distributions 2x2 design:\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\n\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\n\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\n\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.QTT defined difference inverses \\(F_{Y(1),11}\\) counterfactual distribution \\(F_{Y(0),11}\\) quantile \\(q\\):\\[\n  \\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta)\n  \\]Estimation Process\nCounterfactual CDF:\n\\[\n  \\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right)\n  \\]\nEquivalent Expression:\n\\[\n  \\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right)\n  \\]\nTreatment Effect Estimate:\n\\[\n  \\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta)\n  \\]\nEquivalently:\n\\(\\Delta^{CIC}_{\\theta}\\) difference two QTE estimates:\n\\[\n  \\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta',0}\n  \\]\n:\n\\(\\Delta^{QTT}_{\\theta,1}\\) = change time \\(y\\) quantile \\(\\theta\\) \\(D = 1\\) group.\n\\(\\Delta^{QTU}_{\\theta',0}\\) = change time \\(y\\) quantile \\(\\theta'\\) \\(D = 0\\) group, \\(q'\\) quantile \\(D = 0, T = 0\\) distribution corresponding value \\(y\\) associated quantile \\(\\theta\\) \\(D = 1, T = 0\\) distribution.\nEstimation ProcessCounterfactual CDF:\\[\n  \\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right)\n  \\]Equivalent Expression:\\[\n  \\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right)\n  \\]Treatment Effect Estimate:\\[\n  \\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta)\n  \\]Equivalently:\\(\\Delta^{CIC}_{\\theta}\\) difference two QTE estimates:\\[\n  \\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta',0}\n  \\]:\\(\\Delta^{QTT}_{\\theta,1}\\) = change time \\(y\\) quantile \\(\\theta\\) \\(D = 1\\) group.\\(\\Delta^{QTU}_{\\theta',0}\\) = change time \\(y\\) quantile \\(\\theta'\\) \\(D = 0\\) group, \\(q'\\) quantile \\(D = 0, T = 0\\) distribution corresponding value \\(y\\) associated quantile \\(\\theta\\) \\(D = 1, T = 0\\) distribution.Marketing Example\nSuppose company implements new online marketing strategy aimed improving customer retention rates.\nQTT: goal estimate effect strategy customer retention rates different quantiles (e.g., median retention rate).\nRank Preservation: Assumes customers’ rank retention distribution remains , regardless strategy—assumption strong carefully considered.\nCounterfactual: CiC helps estimate retention rates changed without new strategy comparing control group.\nMarketing ExampleSuppose company implements new online marketing strategy aimed improving customer retention rates.QTT: goal estimate effect strategy customer retention rates different quantiles (e.g., median retention rate).Rank Preservation: Assumes customers’ rank retention distribution remains , regardless strategy—assumption strong carefully considered.Counterfactual: CiC helps estimate retention rates changed without new strategy comparing control group.References\nAthey Imbens (2006)\nFrölich Melly (2013): IV-based\nCallaway Li (2019): panel data\nM. Huber, Schelker, Strittmatter (2022)\nReferencesAthey Imbens (2006)Frölich Melly (2013): IV-basedCallaway Li (2019): panel dataM. Huber, Schelker, Strittmatter (2022)Additional Resources\nCode examples available Stata.\nAdditional ResourcesCode examples available Stata.","code":""},{"path":"changes-in-changes.html","id":"application-12","chapter":"27 Changes-in-Changes","heading":"27.1 Application","text":"","code":""},{"path":"changes-in-changes.html","id":"ecic-package","chapter":"27 Changes-in-Changes","heading":"27.1.1 ECIC package","text":"","code":"\nlibrary(ecic)\ndata(dat, package = \"ecic\")\nmod =\n  ecic(\n    yvar  = lemp,         # dependent variable\n    gvar  = first.treat,  # group indicator\n    tvar  = year,         # time indicator\n    ivar  = countyreal,   # unit ID\n    dat   = dat,          # dataset\n    boot  = \"weighted\",   # bootstrap proceduce (\"no\", \"normal\", or \"weighted\")\n    nReps = 3            # number of bootstrap runs\n    )\nmod_res <- summary(mod)\nmod_res\n#>   perc    coefs          se\n#> 1  0.1 1.206140 0.021351711\n#> 2  0.2 1.316599 0.009225026\n#> 3  0.3 1.449963 0.001859468\n#> 4  0.4 1.583415 0.015296156\n#> 5  0.5 1.739932 0.011240454\n#> 6  0.6 1.915558 0.013060348\n#> 7  0.7 2.114966 0.014482208\n#> 8  0.8 2.363105 0.005173865\n#> 9  0.9 2.779202 0.020831180\n\necic_plot(mod_res)"},{"path":"changes-in-changes.html","id":"qte-package","chapter":"27 Changes-in-Changes","heading":"27.1.2 QTE package","text":"QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.-like modelsWith distributional difference--differences assumption Callaway Li (2019), extension parallel trends assumption, can estimate QTET.2 periods, distributional assumption can partially identify QTET bounds (Fan Yu 2012)restrictive assumption difference quantiles distribution potential outcomes treated untreated groups values quantiles, can mean modelOn top distributional assumption, need copula stability assumption (.e., , treatment, units highest outcomes improving , expect see improving current period .) models:","code":"\nlibrary(qte)\ndata(lalonde)\n\n# randomized setting\n# qte is identical to qtet\njt.rand <-\n    ci.qtet(\n        re78 ~ treat,\n        data = lalonde.exp,\n        iters = 10\n    )\nsummary(jt.rand)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05    0.00    0.00\n#> 0.1     0.00    0.00\n#> 0.15    0.00    0.00\n#> 0.2     0.00   18.33\n#> 0.25  338.65  377.74\n#> 0.3   846.40  470.45\n#> 0.35 1451.51  515.86\n#> 0.4  1177.72  869.19\n#> 0.45 1396.08  918.39\n#> 0.5  1123.55  925.74\n#> 0.55 1181.54  938.82\n#> 0.6  1466.51  951.64\n#> 0.65 2115.04  892.16\n#> 0.7  1795.12  842.66\n#> 0.75 2347.49  678.45\n#> 0.8  2278.12  971.21\n#> 0.85 2178.28  973.90\n#> 0.9  3239.60 1889.23\n#> 0.95 3979.62 2872.52\n#> \n#> Average Treatment Effect:    1794.34\n#>   Std. Error:        665.59\nggqte(jt.rand)\n# conditional independence assumption (CIA)\njt.cia <- ci.qte(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.cia)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1       0.00        0.00\n#> 0.15  -4433.18      710.76\n#> 0.2   -8219.15      419.90\n#> 0.25 -10435.74      793.20\n#> 0.3  -12232.03     1037.03\n#> 0.35 -12428.30     1425.39\n#> 0.4  -14195.24     1793.20\n#> 0.45 -14248.66     1907.98\n#> 0.5  -15538.67     2095.11\n#> 0.55 -16550.71     2329.67\n#> 0.6  -15595.02     2686.45\n#> 0.65 -15827.52     2745.62\n#> 0.7  -16090.32     3390.26\n#> 0.75 -16091.49     3376.67\n#> 0.8  -17864.76     3245.52\n#> 0.85 -16756.71     3533.91\n#> 0.9  -17914.99     2305.10\n#> 0.95 -23646.22     2003.55\n#> \n#> Average Treatment Effect:    -13435.40\n#>   Std. Error:        1259.01\nggqte(jt.cia)\n\njt.ciat <- ci.qtet(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.ciat)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1   -1018.15      614.29\n#> 0.15  -3251.00     1557.37\n#> 0.2   -7240.86     1433.54\n#> 0.25  -8379.94      475.33\n#> 0.3   -8758.82      345.53\n#> 0.35  -9897.44      606.54\n#> 0.4  -10239.57      747.91\n#> 0.45 -10751.39      736.39\n#> 0.5  -10570.14      899.75\n#> 0.55 -11348.96      898.80\n#> 0.6  -11550.84      687.20\n#> 0.65 -12203.56      780.92\n#> 0.7  -13277.72      979.47\n#> 0.75 -14011.74      993.28\n#> 0.8  -14373.95      706.69\n#> 0.85 -14499.18     1048.62\n#> 0.9  -15008.63     2201.11\n#> 0.95 -15954.05     2655.30\n#> \n#> Average Treatment Effect:    4266.19\n#>   Std. Error:        600.51\nggqte(jt.ciat)\n# distributional DiD assumption\njt.pqtet <- panel.qtet(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tmin2 = 1974,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.pqtet)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05  4779.21     1222.37\n#> 0.1   1987.35      776.82\n#> 0.15   842.95     3332.09\n#> 0.2  -7366.04     4852.87\n#> 0.25 -8449.96     3522.70\n#> 0.3  -7992.15     1201.51\n#> 0.35 -7429.21     1161.43\n#> 0.4  -6597.37     1288.64\n#> 0.45 -5519.45     1391.04\n#> 0.5  -4702.88     1129.80\n#> 0.55 -3904.52     1131.23\n#> 0.6  -2741.80     1157.60\n#> 0.65 -1507.31     1223.03\n#> 0.7   -771.12     1264.45\n#> 0.75   707.81     1280.34\n#> 0.8    580.00      793.09\n#> 0.85   821.75      969.38\n#> 0.9   -250.77     1662.49\n#> 0.95 -1874.54     2706.67\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        795.44\nggqte(jt.pqtet)\nres_bound <-\n    bounds(\n        re ~ treat,\n        t = 1978,\n        tmin1 = 1975,\n        data = lalonde.psid.panel,\n        idname = \"id\",\n        tname = \"year\"\n    )\nsummary(res_bound)\n#> \n#> Bounds on the Quantile Treatment Effect on the Treated:\n#>      \n#> tau  Lower Bound Upper Bound\n#>         tau  Lower Bound Upper Bound\n#>        0.05       -51.72           0\n#>         0.1     -1220.84           0\n#>        0.15      -1881.9           0\n#>         0.2     -2601.32           0\n#>        0.25     -2916.38      485.23\n#>         0.3     -3080.16      943.05\n#>        0.35     -3327.89     1505.98\n#>         0.4     -3240.59     2133.59\n#>        0.45     -2982.51     2616.84\n#>         0.5     -3108.01      2566.2\n#>        0.55     -3342.66     2672.82\n#>         0.6      -3491.4      3065.7\n#>        0.65     -3739.74     3349.74\n#>         0.7     -4647.82     2992.03\n#>        0.75     -4826.78     3219.32\n#>         0.8      -5801.7     2702.33\n#>        0.85     -6588.61     2499.41\n#>         0.9     -8953.84     2020.84\n#>        0.95    -14283.61      397.04\n#> \n#> Average Treatment Effect on the Treated: 2326.51\nplot(res_bound)\njt.mdid <- ddid2(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.mdid)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05 10616.61      744.99\n#> 0.1   5019.83      447.82\n#> 0.15  2388.12      334.57\n#> 0.2   1033.23      365.01\n#> 0.25   485.23      445.95\n#> 0.3    943.05      631.10\n#> 0.35   931.45      756.72\n#> 0.4    945.35      888.69\n#> 0.45  1205.88      903.88\n#> 0.5   1362.11      778.89\n#> 0.55  1279.05      871.73\n#> 0.6   1618.13      734.08\n#> 0.65  1834.30      674.83\n#> 0.7   1326.06      793.46\n#> 0.75  1586.35      714.42\n#> 0.8   1256.09      591.37\n#> 0.85   723.10      871.86\n#> 0.9    251.36     1703.13\n#> 0.95 -1509.92     2033.88\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        514.81\nplot(jt.mdid)\njt.qdid <- QDiD(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)\n\njt.cic <- CiC(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)"},{"path":"synthetic-control.html","id":"synthetic-control","chapter":"28 Synthetic Control","heading":"28 Synthetic Control","text":"Examples marketing:(Tirunillai Tellis 2017): offline TV ad Online Chatter(Yanwen Wang, Wu, Zhu 2019): mobile hailing technology adoption drivers’ hourly earnings(Guo, Sriram, Manchanda 2020): payment disclosure laws effect physician prescription behavior using Timing Massachusetts open payment law exogenous shock(Adalja et al. 2023): mandatory GMO labels impact consumer demand (Using Vermont mandatory state)NotesThe SC method provides asymptotically normal estimators various linear panel data models, given sufficiently large pre-treatment periods, making natural alternative Difference--differences model (Arkhangelsky Hirshberg 2023).SC method provides asymptotically normal estimators various linear panel data models, given sufficiently large pre-treatment periods, making natural alternative Difference--differences model (Arkhangelsky Hirshberg 2023).SCM superior Matching Methods matches covariates (.e., pre-treatment variables), also outcomes.SCM superior Matching Methods matches covariates (.e., pre-treatment variables), also outcomes.review method, see (Abadie 2021)review method, see (Abadie 2021)SCMs can also used Bayesian framework (Bayesian Synthetic Control) impose restrictive priori (S. Kim, Lee, Gupta 2020)SCMs can also used Bayesian framework (Bayesian Synthetic Control) impose restrictive priori (S. Kim, Lee, Gupta 2020)Different Matching Methods SCMs match pre-treatment outcomes period Matching Methods match number covariates.Different Matching Methods SCMs match pre-treatment outcomes period Matching Methods match number covariates.data driven procedure construct comparable control groups (.e., black box).data driven procedure construct comparable control groups (.e., black box).causal inference control treatment group using Matching Methods, typically similar covariates control treated groups. However, don’t methods like Propensity Scores can perform rather poorly (.e., large bias).causal inference control treatment group using Matching Methods, typically similar covariates control treated groups. However, don’t methods like Propensity Scores can perform rather poorly (.e., large bias).Advantages Difference--differencesMaximization observable similarity control treatment (maybe also unobservables)Can also used cases untreated case similar matching dimensions treated casesObjective selection controls.Advantages linear regressionRegression weights estimator outside [0,1] (regression allows extrapolation), sparse (.e., can less 0).Regression weights estimator outside [0,1] (regression allows extrapolation), sparse (.e., can less 0).extrapolation SCMsNo extrapolation SCMsExplicitly state fit (.e., weight)Explicitly state fit (.e., weight)Can estimated without post-treatment outcomes control group (can’t p-hack)Can estimated without post-treatment outcomes control group (can’t p-hack)Advantages:selection criteria, researchers can understand relative importance candidatePost-intervention outcomes used synthetic. Hence, can’t retro-fit.Observable similarity control treatment cases maximizedDisadvantages:’s hard argue weights use create “synthetic control”SCM recommended whenSocial events evaluate large-scale program policyOnly one treated case several control candidates.AssumptionsDonor subject good match synthetic control (.e., gap dependent donor subject synthetic control 0 treatment)Donor subject good match synthetic control (.e., gap dependent donor subject synthetic control 0 treatment)treated subject undergoes treatment subjects donor pool.treated subject undergoes treatment subjects donor pool.changes subjects whole window.changes subjects whole window.counterfactual outcome treatment group can imputed linear combination control groups.counterfactual outcome treatment group can imputed linear combination control groups.Identification: exclusion restriction met conditional pre-treatment outcomes.Synth provides algorithm finds weighted combination comparison units weights chosen best resembles values predictors outcome variable affected units interventionSetting (notation followed professor Alberto Abadie)\\(J + 1\\) units periods \\(1, \\dots, T\\)\\(J + 1\\) units periods \\(1, \\dots, T\\)first unit treated one \\(T_0 + 1, \\dots, T\\)first unit treated one \\(T_0 + 1, \\dots, T\\)\\(J\\) units called donor pool\\(J\\) units called donor pool\\(Y_{}^\\) outcome unit \\(\\) ’s exposed treatment \\(T_0 + 1 , \\dots T\\)\\(Y_{}^\\) outcome unit \\(\\) ’s exposed treatment \\(T_0 + 1 , \\dots T\\)\\(Y_{}^N\\) outcome unit \\(\\) ’s exposed treatment\\(Y_{}^N\\) outcome unit \\(\\) ’s exposed treatmentWe try estimate effect treatment treated unit\\[\n\\tau_{1t} = Y_{1t}^- Y_{1t}^N\n\\]observe first treated unit already \\(Y_{1t}^= Y_{1t}\\)construct synthetic control unit, find appropriate weight donor donor pool finding \\(\\mathbf{W} = (w_2, \\dots, w_{J=1})'\\) \\(w_j \\ge 0\\) \\(j = 2, \\dots, J+1\\)\\(w_j \\ge 0\\) \\(j = 2, \\dots, J+1\\)\\(w_2 + \\dots + w_{J+1} = 1\\)\\(w_2 + \\dots + w_{J+1} = 1\\)“appropriate” vector \\(\\mathbf{W}\\) constrained \\[\n\\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}||\n\\]\\(\\mathbf{X}_1\\) \\(k \\times 1\\) vector pre-treatment characteristics treated unit\\(\\mathbf{X}_1\\) \\(k \\times 1\\) vector pre-treatment characteristics treated unit\\(\\mathbf{X}_0\\) \\(k \\times J\\) matrix pre-treatment characteristics untreated units\\(\\mathbf{X}_0\\) \\(k \\times J\\) matrix pre-treatment characteristics untreated unitsFor simplicity, researchers usually use\\[\n\\begin{aligned}\n&\\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}|| \\\\\n&= (\\sum_{h=1}^k v_h(X_{h1}- w_2 X-{h2} - \\dots - w_{J+1} X_{hJ +1})^{1/2}\n\\end{aligned}\n\\]\\(v_1, \\dots, v_k\\) vector positive constants represent predictive power \\(k\\) predictors \\(Y_{1t}^N\\) (.e., potential outcome treated without treatment) can chosen either explicitly researcher data-driven methodsFor penalized synthetic control (Abadie L’hour 2021), minimization problem becomes\\[\n\\min_{\\mathbf{W}} ||\\mathbf{X}_1 - \\sum_{j=2}^{J + 1}W_j \\mathbf{X}_j ||^2 + \\lambda \\sum_{j=2}^{J+1} W_j ||\\mathbf{X}_1 - \\mathbf{X}_j||^2\n\\]\\(W_j \\ge 0\\) \\(\\sum_{j=2}^{J+1} W_j = 1\\)\\(W_j \\ge 0\\) \\(\\sum_{j=2}^{J+1} W_j = 1\\)\\(\\lambda >0\\) balances -fitting treated minimize sum pairwise distances\n\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\n\\(\\lambda \\\\infty\\): nearest neighbor matching\n\\(\\lambda >0\\) balances -fitting treated minimize sum pairwise distances\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\\(\\lambda \\\\infty\\): nearest neighbor matching\\(\\lambda \\\\infty\\): nearest neighbor matchingAdvantages:\\(\\lambda >0\\), unique sparse solutionFor \\(\\lambda >0\\), unique sparse solutionReduces interpolation bias averaging dissimilar unitsReduces interpolation bias averaging dissimilar unitsPenalized SC never uses dissimilar unitsPenalized SC never uses dissimilar unitsThen synthetic control estimator \\[\n\\hat{\\tau}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt}\n\\]\\(Y_{jt}\\) outcome unit \\(j\\) time \\(t\\)ConsiderationUnder factor model (Abadie, Diamond, Hainmueller 2010)\\[\nY_{}^N = \\mathbf{\\theta}_t \\mathbf{Z}_i + \\mathbf{\\lambda}_t \\mathbf{\\mu}_i + \\epsilon_{}\n\\]\\(Z_i\\) = observables\\(Z_i\\) = observables\\(\\mu_i\\) = unobservables\\(\\mu_i\\) = unobservables\\(\\epsilon_{}\\) = unit-level transitory shock (.e., random noise)\\(\\epsilon_{}\\) = unit-level transitory shock (.e., random noise)assumptions \\(\\mathbf{W}^*\\) \\[\n\\begin{aligned}\n\\sum_{j=2}^{J+1} w_j^* \\mathbf{Z}_j  &= \\mathbf{Z}_1 \\\\\n&\\dots \\\\\n\\sum_{j=2}^{J+1} w_j^* Y_{j1} &= Y_{11} \\\\\n\\sum_{j=2}^{J+1} w_j^* Y_{jT_0} &= Y_{1T_0}\n\\end{aligned}\n\\]Basically, assume synthetic control good counterfactual treated unit exposed treatment.,bias bound depends close fit, controlled ratio \\(\\epsilon_{}\\) (transitory shock) \\(T_0\\) (number pre-treatment periods). words, good fit \\(Y_{1t}\\) pre-treatment period (.e., \\(T_0\\) large small variance \\(\\epsilon_{}\\))bias bound depends close fit, controlled ratio \\(\\epsilon_{}\\) (transitory shock) \\(T_0\\) (number pre-treatment periods). words, good fit \\(Y_{1t}\\) pre-treatment period (.e., \\(T_0\\) large small variance \\(\\epsilon_{}\\))poor fit, use bias correction version synthetic control. See Ben-Michael, Feller, Rothstein (2020)poor fit, use bias correction version synthetic control. See Ben-Michael, Feller, Rothstein (2020)Overfitting can result small \\(T_0\\) (number pre-treatment periods), large \\(J\\) (number units donor pool), large \\(\\epsilon_{}\\) (noise)\nMitigation: put similar units (treated one) donor pool\nOverfitting can result small \\(T_0\\) (number pre-treatment periods), large \\(J\\) (number units donor pool), large \\(\\epsilon_{}\\) (noise)Mitigation: put similar units (treated one) donor poolTo make inference, create permutation distribution (iteratively reassigning treatment units donor pool estimate placebo effects iteration). say effect treatment magnitude value treatment effect treated unit extreme relative permutation distribution.’s recommended use one-sided inference. permutation distribution superior p-values alone (sampling-based inference hard SCMs either undefined sampling mechanism sample population).benchmark (permutation) distribution (e.g., uniform), see (Firpo Possebom 2018)","code":""},{"path":"synthetic-control.html","id":"applications-1","chapter":"28 Synthetic Control","heading":"28.1 Applications","text":"","code":""},{"path":"synthetic-control.html","id":"example-1-2","chapter":"28 Synthetic Control","heading":"28.1.1 Example 1","text":"Danilo Freiresimulate data 10 states 30 years. State receives treatment T = 20 year 15.Gaps plot:Alternatively, gsynth provides options estimate iterative fixed effects, handle multiple treated units tat time., use two=way fixed effects bootstrapped standard errors","code":"\n# install.packages(\"Synth\")\n# install.packages(\"gsynth\")\nlibrary(\"Synth\")\nlibrary(\"gsynth\")\nset.seed(1)\nyear         <- rep(1:30, 10)\nstate        <- rep(LETTERS[1:10], each = 30)\nX1           <- round(rnorm(300, mean = 2, sd = 1), 2)\nX2           <- round(rbinom(300, 1, 0.5) + rnorm(300), 2)\nY            <- round(1 + 2 * X1 + rnorm(300), 2)\ndf           <- as.data.frame(cbind(Y, X1, X2, state, year))\ndf$Y         <- as.numeric(as.character(df$Y))\ndf$X1        <- as.numeric(as.character(df$X1))\ndf$X2        <- as.numeric(as.character(df$X2))\ndf$year      <- as.numeric(as.character(df$year))\ndf$state.num <- rep(1:10, each = 30)\ndf$state     <- as.character(df$state)\ndf$`T`       <- ifelse(df$state == \"A\" & df$year >= 15, 1, 0)\ndf$Y         <- ifelse(df$state == \"A\" & df$year >= 15, \n                       df$Y + 20, df$Y)\nstr(df)\n#> 'data.frame':    300 obs. of  7 variables:\n#>  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...\n#>  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...\n#>  $ X2       : num  1.96 0.4 -0.75 -0.56 -0.45 1.06 0.51 -2.1 0 0.54 ...\n#>  $ state    : chr  \"A\" \"A\" \"A\" \"A\" ...\n#>  $ year     : num  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ T        : num  0 0 0 0 0 0 0 0 0 0 ...\ndataprep.out <-\n    dataprep(\n        df,\n        predictors            = c(\"X1\", \"X2\"),\n        dependent             = \"Y\",\n        unit.variable         = \"state.num\",\n        time.variable         = \"year\",\n        unit.names.variable   = \"state\",\n        treatment.identifier  = 1,\n        controls.identifier   = c(2:10),\n        time.predictors.prior = c(1:14),\n        time.optimize.ssr     = c(1:14),\n        time.plot             = c(1:30)\n    )\n\n\nsynth.out <- synth(dataprep.out)\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 9.831789 \n#> \n#> solution.v:\n#>  0.3888387 0.6111613 \n#> \n#> solution.w:\n#>  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853\nprint(synth.tables   <- synth.tab(\n        dataprep.res = dataprep.out,\n        synth.res    = synth.out)\n      )\n#> $tab.pred\n#>    Treated Synthetic Sample Mean\n#> X1   2.028     2.028       2.017\n#> X2   0.513     0.513       0.394\n#> \n#> $tab.v\n#>    v.weights\n#> X1 0.389    \n#> X2 0.611    \n#> \n#> $tab.w\n#>    w.weights unit.names unit.numbers\n#> 2      0.112          B            2\n#> 3      0.183          C            3\n#> 4      0.103          D            4\n#> 5      0.312          E            5\n#> 6      0.061          F            6\n#> 7      0.035          G            7\n#> 8      0.059          H            8\n#> 9      0.057          I            9\n#> 10     0.078          J           10\n#> \n#> $tab.loss\n#>            Loss W   Loss V\n#> [1,] 9.761708e-12 9.831789\npath.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Y\"),\n          Xlab         = c(\"Year\"),\n          Legend       = c(\"State A\",\"Synthetic State A\"),\n          Legend.position = c(\"topleft\")\n)\n\nabline(v   = 15,\n       lty = 2)\ngaps.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Gap\"),\n          Xlab         = c(\"Year\"),\n          Ylim         = c(-30, 30),\n          Main         = \"\"\n)\n\nabline(v   = 15,\n       lty = 2)gsynth.out <- gsynth(\n  Y ~ `T` + X1 + X2,\n  data = df,\n  index = c(\"state\", \"year\"),\n  force = \"two-way\",\n  CV = TRUE,\n  r = c(0, 5),\n  se = TRUE,\n  inference = \"parametric\",\n  nboots = 1000,\n  parallel = F # TRUE\n)\n#> Cross-validating ... \n#>  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502\n#>  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375\n#>  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*\n#>  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319\n#>  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301\n#>  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596\n#> \n#>  r* = 2\n#> \n#> \nSimulating errors .............\nBootstrapping ...\n#> ..........\nplot(gsynth.out)\nplot(gsynth.out, type = \"counterfactual\")\nplot(gsynth.out, type = \"counterfactual\", raw = \"all\") \n# shows estimations for the control cases"},{"path":"synthetic-control.html","id":"example-2-1","chapter":"28 Synthetic Control","heading":"28.1.2 Example 2","text":"Leihua Yetransform data used synth()\\(X_1\\) = control case treatment\\(X_1\\) = control case treatment\\(X_0\\) = control cases treatment\\(X_0\\) = control cases treatment\\(Z_1\\): treatment case treatment\\(Z_1\\): treatment case treatment\\(Z_0\\): treatment case treatment\\(Z_0\\): treatment case treatmentCalculate difference real basque region synthetic controlRelative importance unitDoubly Robust Difference--DifferencesExample DRDID packageEstimate Average Treatment Effect Treated using Improved Locally Efficient Doubly Robust estimator","code":"\n\nlibrary(Synth)\ndata(\"basque\")\ndim(basque) #774*17\n#> [1] 774  17\nhead(basque)\n#>   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry\n#> 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA\n#> 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA\n#> 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA\n#> 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA\n#> 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA\n#> 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA\n#>   sec.construction sec.services.venta sec.services.nonventa school.illit\n#> 1               NA                 NA                    NA           NA\n#> 2               NA                 NA                    NA           NA\n#> 3               NA                 NA                    NA           NA\n#> 4               NA                 NA                    NA           NA\n#> 5               NA                 NA                    NA           NA\n#> 6               NA                 NA                    NA           NA\n#>   school.prim school.med school.high school.post.high popdens invest\n#> 1          NA         NA          NA               NA      NA     NA\n#> 2          NA         NA          NA               NA      NA     NA\n#> 3          NA         NA          NA               NA      NA     NA\n#> 4          NA         NA          NA               NA      NA     NA\n#> 5          NA         NA          NA               NA      NA     NA\n#> 6          NA         NA          NA               NA      NA     NA\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op =  \"mean\",\n    # the operator\n    time.predictors.prior = 1964:1969,\n    #the entire time frame from the #beginning to the end\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969,  \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969,  \"mean\")\n    ),\n    dependent =  \"gdpcap\",\n    # dv\n    unit.variable =  \"regionno\",\n    #identifying unit numbers\n    unit.names.variable =  \"regionname\",\n    #identifying unit names\n    time.variable =  \"year\",\n    #time-periods\n    treatment.identifier = 17,\n    #the treated case\n    controls.identifier = c(2:16, 18),\n    #the control cases; all others #except number 17\n    time.optimize.ssr = 1960:1969,\n    #the time-period over which to optimize\n    time.plot = 1955:1997\n) #the entire time period before/after the treatment\nsynth.out = synth(data.prep.obj = dataprep.out, method = \"BFGS\")\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 0.008864606 \n#> \n#> solution.v:\n#>  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n#> \n#> solution.w:\n#>  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps = dataprep.out$Y1plot - (dataprep.out$Y0plot \n                                     %*% synth.out$solution.w)\ngaps[1:3,1]\n#>       1955       1956       1957 \n#> 0.15023473 0.09168035 0.03716475\nsynth.tables = synth.tab(dataprep.res = dataprep.out,\n                         synth.res = synth.out)\nnames(synth.tables)\n#> [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\nsynth.tables$tab.pred[1:13,]\n#>                                          Treated Synthetic Sample Mean\n#> school.illit                              39.888   256.337     170.786\n#> school.prim                             1031.742  2730.104    1127.186\n#> school.med                                90.359   223.340      76.260\n#> school.high                               25.728    63.437      24.235\n#> school.post.high                          13.480    36.153      13.478\n#> invest                                    24.647    21.583      21.424\n#> special.gdpcap.1960.1969                   5.285     5.271       3.581\n#> special.sec.agriculture.1961.1969          6.844     6.179      21.353\n#> special.sec.energy.1961.1969               4.106     2.760       5.310\n#> special.sec.industry.1961.1969            45.082    37.636      22.425\n#> special.sec.construction.1961.1969         6.150     6.952       7.276\n#> special.sec.services.venta.1961.1969      33.754    41.104      36.528\n#> special.sec.services.nonventa.1961.1969    4.072     5.371       7.111\nsynth.tables$tab.w[8:14, ]\n#>    w.weights            unit.names unit.numbers\n#> 9      0.000    Castilla-La Mancha            9\n#> 10     0.851              Cataluna           10\n#> 11     0.000  Comunidad Valenciana           11\n#> 12     0.000           Extremadura           12\n#> 13     0.000               Galicia           13\n#> 14     0.149 Madrid (Comunidad De)           14\n#> 15     0.000    Murcia (Region de)           15\n# plot the changes before and after the treatment \npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita gdp (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab =  \"gap in real per - capita GDP (1986 USD, thousand)\",\n    Xlab =  \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)\nlibrary(DRDID)\ndata(nsw_long)\n# Form the Lalonde sample with CPS comparison group\neval_lalonde_cps <- subset(nsw_long, nsw_long$treated == 0 | \n                               nsw_long$sample == 2)\nout <-\n    drdid(\n        yname = \"re\",\n        tname = \"year\",\n        idname = \"id\",\n        dname = \"experimental\",\n        xformla = ~ age + educ + black + married + nodegree + hisp + re74,\n        data = eval_lalonde_cps,\n        panel = TRUE\n    )\nsummary(out)\n#>  Call:\n#> drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n#>     xformla = ~age + educ + black + married + nodegree + hisp + \n#>         re74, data = eval_lalonde_cps, panel = TRUE)\n#> ------------------------------------------------------------------\n#>  Further improved locally efficient DR DID estimator for the ATT:\n#>  \n#>    ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n#> -901.2703   393.6247   -2.2897     0.022    -1672.7747  -129.766 \n#> ------------------------------------------------------------------\n#>  Estimator based on panel data.\n#>  Outcome regression est. method: weighted least squares.\n#>  Propensity score est. method: inverse prob. tilting.\n#>  Analytical standard error.\n#> ------------------------------------------------------------------\n#>  See Sant'Anna and Zhao (2020) for details."},{"path":"synthetic-control.html","id":"example-3-1","chapter":"28 Synthetic Control","heading":"28.1.3 Example 3","text":"Synth package’s authorssynth() requires\\(X_1\\) vector treatment predictors\\(X_1\\) vector treatment predictors\\(X_0\\) matrix variables control group\\(X_0\\) matrix variables control group\\(Z_1\\) vector outcome variable treatment group\\(Z_1\\) vector outcome variable treatment group\\(Z_0\\) matrix outcome variable control group\\(Z_0\\) matrix outcome variable control groupuse dataprep() prepare data format can used throughout Synth packagefind optimal weights identifies synthetic control treatment groupYou also run placebo tests","code":"\nlibrary(Synth)\ndata(\"basque\")\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op = \"mean\",\n    time.predictors.prior = 1964:1969,\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969 , \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969, \"mean\")\n    ),\n    dependent = \"gdpcap\",\n    unit.variable = \"regionno\",\n    unit.names.variable = \"regionname\",\n    time.variable = \"year\",\n    treatment.identifier = 17,\n    controls.identifier = c(2:16, 18),\n    time.optimize.ssr = 1960:1969,\n    time.plot = 1955:1997\n)\nsynth.out <- synth(data.prep.obj = dataprep.out, method = \"BFGS\")\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 0.008864606 \n#> \n#> solution.v:\n#>  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n#> \n#> solution.w:\n#>  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps <- dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w)\ngaps[1:3, 1]\n#>       1955       1956       1957 \n#> 0.15023473 0.09168035 0.03716475\nsynth.tables <-\n    synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nnames(synth.tables) # you can pick tables to see \n#> [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"gap in real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)"},{"path":"synthetic-control.html","id":"example-4-1","chapter":"28 Synthetic Control","heading":"28.1.4 Example 4","text":"Michael Robbins Steven Davenport authors MicroSynth following improvements:Standardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsStandardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsOmnibus statistic (set omnibus.var ) multiple outcome variablesOmnibus statistic (set omnibus.var ) multiple outcome variablesincorporate multiple follow-periods end.postincorporate multiple follow-periods end.postNotes:predictors outcome used match units intervention\nOutcome variable time-variant\nPredictors time-invariant\npredictors outcome used match units interventionOutcome variable time-variantOutcome variable time-variantPredictors time-invariantPredictors time-invariant","code":"\n# right now the package is not availabe for R version 4.2\nlibrary(microsynth)\ndata(\"seattledmi\")\n\n\ncov.var <-\n    c(\n        \"TotalPop\",\n        \"BLACK\",\n        \"HISPANIC\",\n        \"Males_1521\",\n        \"HOUSEHOLDS\",\n        \"FAMILYHOUS\",\n        \"FEMALE_HOU\",\n        \"RENTER_HOU\",\n        \"VACANT_HOU\"\n    )\nmatch.out <- c(\"i_felony\", \"i_misdemea\", \"i_drugs\", \"any_crime\")\nsea1 <- microsynth(\n    seattledmi,\n    idvar       = \"ID\",\n    timevar     = \"time\",\n    intvar      = \"Intervention\",\n    start.pre   = 1,\n    end.pre     = 12,\n    end.post    = 16,\n    match.out   = match.out, # outcome variable will be matched on exactly\n    match.covar = cov.var, # specify covariates will be matched on exactly\n    result.var  = match.out, # used to report results\n    omnibus.var = match.out, # feature in the omnibus p-value\n    test        = \"lower\",\n    n.cores     = min(parallel::detectCores(), 2)\n)\nsea1\nsummary(sea1)\nplot_microsynth(sea1)\nsea2 <- microsynth(\n    seattledmi,\n    idvar = \"ID\",\n    timevar = \"time\",\n    intvar = \"Intervention\",\n    start.pre = 1,\n    end.pre = 12,\n    end.post = c(14, 16),\n    match.out = match.out,\n    match.covar = cov.var,\n    result.var = match.out,\n    omnibus.var = match.out,\n    test = \"lower\",\n    perm = 250,\n    jack = TRUE,\n    n.cores = min(parallel::detectCores(), 2)\n)"},{"path":"synthetic-control.html","id":"augmented-synthetic-control-method","chapter":"28 Synthetic Control","heading":"28.2 Augmented Synthetic Control Method","text":"package: augsynth (Ben-Michael, Feller, Rothstein 2021)","code":""},{"path":"synthetic-control.html","id":"synthetic-control-with-staggered-adoption","chapter":"28 Synthetic Control","heading":"28.3 Synthetic Control with Staggered Adoption","text":"references: https://ebenmichael.github.io/assets/research/jamboree.pdf (Ben-Michael, Feller, Rothstein 2022) package: augsynth","code":""},{"path":"synthetic-control.html","id":"bayesian-synthetic-control","chapter":"28 Synthetic Control","heading":"28.4 Bayesian Synthetic Control","text":"S. Kim, Lee, Gupta (2020)Pang, Liu, Xu (2022)","code":""},{"path":"synthetic-control.html","id":"generalized-synthetic-control","chapter":"28 Synthetic Control","heading":"28.5 Generalized Synthetic Control","text":"reference: (Xu 2017)Bootstrap procedure biased (K. T. Li Sonnier 2023). Hence, need follow K. T. Li Sonnier (2023) terms SEs estimation.","code":""},{"path":"synthetic-control.html","id":"other-advances","chapter":"28 Synthetic Control","heading":"28.6 Other Advances","text":"L. Sun, Ben-Michael, Feller (2023) Using Multiple Outcomes Improve SCMCommon Weights Across Outcomes: paper proposes using single set synthetic control weights across multiple outcomes, rather estimating separate weights outcome.Reduced Bias Low-Rank Factor Model: balancing vector index outcomes, approach yields lower bias bounds low-rank factor model, improvements number outcomes increases.Reduced Bias Low-Rank Factor Model: balancing vector index outcomes, approach yields lower bias bounds low-rank factor model, improvements number outcomes increases.Evidence: re-analysis Flint water crisis’s impact educational outcome.Evidence: re-analysis Flint water crisis’s impact educational outcome.","code":""},{"path":"event-studies.html","id":"event-studies","chapter":"29 Event Studies","heading":"29 Event Studies","text":"earliest paper used event study (Dolley 1933)(Campbell et al. 1998) introduced method, based efficient markets theory (Fama 1970)Review:(McWilliams Siegel 1997): management(McWilliams Siegel 1997): management(. Sorescu, Warren, Ertekin 2017): marketing(. Sorescu, Warren, Ertekin 2017): marketingPrevious marketing studies:Firm-initiated activities(Horsky Swyngedouw 1987): name change(Horsky Swyngedouw 1987): name change(Chaney, Devinney, Winer 1991) new product announcements(Chaney, Devinney, Winer 1991) new product announcements(Agrawal Kamakura 1995): celebrity endorsement(Agrawal Kamakura 1995): celebrity endorsement(Lane Jacobson 1995): brand extensions(Lane Jacobson 1995): brand extensions(Houston Johnson 2000): joint venture(Houston Johnson 2000): joint venture(Geyskens, Gielens, Dekimpe 2002): Internet channel (newspapers)(Geyskens, Gielens, Dekimpe 2002): Internet channel (newspapers)(Cornwell, Pruitt, Clark 2005): sponsorship announcements(Cornwell, Pruitt, Clark 2005): sponsorship announcements(Elberse 2007): casting announcements(Elberse 2007): casting announcements(. B. Sorescu, Chandy, Prabhu 2007): M&(. B. Sorescu, Chandy, Prabhu 2007): M&(Sood Tellis 2009): innovation payoff(Sood Tellis 2009): innovation payoff(Wiles Danielova 2009): product placements movies(Wiles Danielova 2009): product placements movies(Joshi Hanssens 2009): movie releases(Joshi Hanssens 2009): movie releases(Wiles et al. 2010): Regulatory Reports Deceptive Advertising(Wiles et al. 2010): Regulatory Reports Deceptive Advertising(Boyd, Chandy, Cunha Jr 2010): new CMO appointments(Boyd, Chandy, Cunha Jr 2010): new CMO appointments(Karniouchina, Uslay, Erenburg 2011): product placement(Karniouchina, Uslay, Erenburg 2011): product placement(Wiles, Morgan, Rego 2012): Brand Acquisition Disposal(Wiles, Morgan, Rego 2012): Brand Acquisition Disposal(Kalaignanam Bahadir 2013): corporate brand name change(Kalaignanam Bahadir 2013): corporate brand name change(Raassens, Wuyts, Geyskens 2012): new product development outsourcing(Raassens, Wuyts, Geyskens 2012): new product development outsourcing(Mazodier Rezaee 2013): sports announcements(Mazodier Rezaee 2013): sports announcements(Borah Tellis 2014): make, buy ally innovations(Borah Tellis 2014): make, buy ally innovations(Homburg, Vollmayr, Hahn 2014): channel expansions(Homburg, Vollmayr, Hahn 2014): channel expansions(Fang, Lee, Yang 2015): Co-development agreements(Fang, Lee, Yang 2015): Co-development agreements(Wu et al. 2015): horizontal collaboration new product development(Wu et al. 2015): horizontal collaboration new product development(Fama et al. 1969): stock split(Fama et al. 1969): stock splitNon-firm-initiated activities(. B. Sorescu, Chandy, Prabhu 2003): FDA approvals(. B. Sorescu, Chandy, Prabhu 2003): FDA approvals(Pandey, Shanahan, Hansen 2005): diversity elite list(Pandey, Shanahan, Hansen 2005): diversity elite list(Balasubramanian, Mathur, Thakur 2005): high-quality achievements(Balasubramanian, Mathur, Thakur 2005): high-quality achievements(Tellis Johnson 2007): quality reviews Walter Mossberg(Tellis Johnson 2007): quality reviews Walter Mossberg(Fornell et al. 2006): customer satisfaction(Fornell et al. 2006): customer satisfaction(Gielens et al. 2008): Walmart’s entry UK market(Gielens et al. 2008): Walmart’s entry UK market(Boyd Spekman 2008): indirect ties(Boyd Spekman 2008): indirect ties(R. S. Rao, Chandy, Prabhu 2008): FDA approvals(R. S. Rao, Chandy, Prabhu 2008): FDA approvals(Ittner, Larcker, Taylor 2009): customer satisfaction(Ittner, Larcker, Taylor 2009): customer satisfaction(Tipton, Bharadwaj, Robertson 2009): Deceptive advertising(Tipton, Bharadwaj, Robertson 2009): Deceptive advertising(Y. Chen, Ganesan, Liu 2009): product recalls(Y. Chen, Ganesan, Liu 2009): product recalls(Jacobson Mizik 2009): satisfaction score release(Jacobson Mizik 2009): satisfaction score release(Karniouchina, Moore, Cooney 2009): Mad money Jim Cramer(Karniouchina, Moore, Cooney 2009): Mad money Jim Cramer(Wiles et al. 2010): deceptive advertising(Wiles et al. 2010): deceptive advertising(Y. Chen, Liu, Zhang 2012): third-party movie reviews(Y. Chen, Liu, Zhang 2012): third-party movie reviews(Xiong Bharadwaj 2013): positive negative news(Xiong Bharadwaj 2013): positive negative news(Gao et al. 2015): product recall(Gao et al. 2015): product recall(Malhotra Kubowicz Malhotra 2011): data breach(Malhotra Kubowicz Malhotra 2011): data breach(Bhagat, Bizjak, Coles 1998): litigation(Bhagat, Bizjak, Coles 1998): litigationPotential avenues:Ad campaignsAd campaignsMarket entryMarket entryproduct failure/recallsproduct failure/recallsPatentsPatentsPros:Better accounting based measures (e.g., profits) managers can manipulate profits (Benston 1985)Better accounting based measures (e.g., profits) managers can manipulate profits (Benston 1985)Easy doEasy doFun fact:(Dubow Monteiro 2006) came way gauge ‘clean’ market . based measure much prices seemed move way suggested insider knowledge, release important regulatory announcements affect stock prices. price shifts might suggest insider trading occurring. Essentially, watching unusual price changes day announcement.Events can beInternal (e.g., stock repurchase)Internal (e.g., stock repurchase)External (e.g., macroeconomic variables)External (e.g., macroeconomic variables)Assumptions:Efficient market theoryShareholders important group among stakeholdersThe event sharply affects share priceExpected return calculated appropriatelySteps:Event Identification: (e.g., dividends, M&, stock buyback, laws regulation, privatization vs. nationalization, celebrity endorsements, name changes, brand extensions etc. see list events US international, see WRDS S&P Capital IQ Key Developments). Events must affect either cash flows discount rate firms (. Sorescu, Warren, Ertekin 2017, 191)\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\n\nSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date\n(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date\nLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).\n\nEvent window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)\nOne day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)\nTwo days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)\n10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)\n\nPost Event window: \\(T_2 \\T_3\\)\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\n\nSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date\n(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date\nLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).Recommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).(Gielens et al. 2008) 260 10 days 300 46 days (Gielens et al. 2008) 260 10 days 300 46 days (Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.Similarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event dateSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event dateLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).Leakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).Event window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)\nOne day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)\nTwo days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)\n10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)\nEvent window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)One day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)One day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)Two days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)Two days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)Post Event window: \\(T_2 \\T_3\\)Post Event window: \\(T_2 \\T_3\\)Normal vs. Abnormal returns\\[\n\\epsilon_{}^* = \\frac{P_{} - E(P_{})}{P_{-1}} = R_{} - E(R_{}|X_t)\n\\]\\(\\epsilon_{}^*\\) = abnormal return\\(\\epsilon_{}^*\\) = abnormal return\\(R_{}\\) = realized (actual) return\\(R_{}\\) = realized (actual) return\\(P\\) = dividend-adjusted price stock\\(P\\) = dividend-adjusted price stock\\(E(R_{}|X_t)\\) normal expected return\\(E(R_{}|X_t)\\) normal expected returnThere several model calculate expected returnA. Statistical Models: assumes jointly multivariate normal iid time (need distributional assumptions valid finite-sample estimation) rather robust (hence, recommended)Constant Mean Return ModelMarket ModelAdjusted Market Return ModelFactor ModelB. Economic Model (strong assumption regarding investor behavior)Capital Asset Pricing Model (CAPM)Arbitrage Pricing Theory (APT)","code":""},{"path":"event-studies.html","id":"other-issues","chapter":"29 Event Studies","heading":"29.1 Other Issues","text":"","code":""},{"path":"event-studies.html","id":"event-studies-in-marketing","chapter":"29 Event Studies","heading":"29.1.1 Event Studies in marketing","text":"(Skiera, Bayer, Schöler 2017) dependent variable marketing-related event studies?Based valuation theory, Shareholder value = value operating business + non-operating asset - debt (Schulze, Skiera, Wiesel 2012)\nMany marketing events affect operating business value, non-operating assets debt\nBased valuation theory, Shareholder value = value operating business + non-operating asset - debt (Schulze, Skiera, Wiesel 2012)Many marketing events affect operating business value, non-operating assets debtIgnoring differences firm-specific leverage effects dual effects:\ninflates impact observation pertaining firms large debt\ndeflates pertaining firms large non-operating asset.\nIgnoring differences firm-specific leverage effects dual effects:inflates impact observation pertaining firms large debtinflates impact observation pertaining firms large debtdeflates pertaining firms large non-operating asset.deflates pertaining firms large non-operating asset.’s recommended marketing papers report \\(CAR^{OB}\\) \\(CAR^{SHV}\\) argue whichever one appropriate.’s recommended marketing papers report \\(CAR^{OB}\\) \\(CAR^{SHV}\\) argue whichever one appropriate.paper, two previous event studies control financial structure: (Gielens et al. 2008) (Chaney, Devinney, Winer 1991)paper, two previous event studies control financial structure: (Gielens et al. 2008) (Chaney, Devinney, Winer 1991)Definitions:Cumulative abnormal percentage return shareholder value (\\(CAR^{SHV}\\))\nShareholder value refers firm’s market capitalization = share price x # shares.\nCumulative abnormal percentage return shareholder value (\\(CAR^{SHV}\\))Shareholder value refers firm’s market capitalization = share price x # shares.Cumulative abnormal percentage return value operating business (\\(CAR^{OB}\\))\n\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)\nLeverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).\nValue operating business = shareholder value - non-operating assets + debt\nLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm size\ndebt = long-term + short-term debt; long-term debt\nfirm size = book value equity; market cap; total assets; debt + equity\n\nCumulative abnormal percentage return value operating business (\\(CAR^{OB}\\))\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)Leverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).Leverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).Value operating business = shareholder value - non-operating assets + debtValue operating business = shareholder value - non-operating assets + debtLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm size\ndebt = long-term + short-term debt; long-term debt\nfirm size = book value equity; market cap; total assets; debt + equity\nLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm sizedebt = long-term + short-term debt; long-term debtdebt = long-term + short-term debt; long-term debtfirm size = book value equity; market cap; total assets; debt + equityfirm size = book value equity; market cap; total assets; debt + equityOperating assets used firm core business operations (e..g, property, plant, equipment, natural resources, intangible asset)Operating assets used firm core business operations (e..g, property, plant, equipment, natural resources, intangible asset)Non–operating assets (redundant assets), play role firm’s operations, still generate form return (e.g., excess cash , marketable securities - commercial papers, market instruments)Non–operating assets (redundant assets), play role firm’s operations, still generate form return (e.g., excess cash , marketable securities - commercial papers, market instruments)Marketing events usually influence value firm’s operating assets (specifically intangible assets). , changes value operating business can impact shareholder value.Three rare instances marketing events can affect non-operating assets debt\n(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt\n(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt\n(Bhaduri 2002) production unique products.\nThree rare instances marketing events can affect non-operating assets debt(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt(Bhaduri 2002) production unique products.(Bhaduri 2002) production unique products.marketing-related event can either influencevalue components firm’s value (= firm’s operating business, non-operating assets debt)value components firm’s value (= firm’s operating business, non-operating assets debt)operating business.operating business.Replication leverage effect\\[\n\\begin{aligned}\n\\text{leverage effect} &= \\frac{\\text{operating business}}{\\text{shareholder value}} \\\\\n&= \\frac{\\text{(shareholder value - non-operating assets + debt)}}{\\text{shareholder value}} \\\\\n&= \\frac{prcc_f \\times csho - ivst + dd1 + dltt + pstk}{prcc_f \\times csho}\n\\end{aligned}\n\\]Compustat Data Itemshort-term investments(Non-operating assets)Since WRDS longer maintains S&P 500 list time writing, can’t replicate list used (Skiera, Bayer, Schöler 2017) paper.","code":"\nlibrary(tidyverse)\ndf_leverage_effect <- read.csv(\"data/leverage_effect.csv.gz\") %>% \n    \n    # get active firms only\n    filter(costat == \"A\") %>% \n    \n    # drop missing values\n    drop_na() %>% \n    \n    # create the leverage effect variable\n    mutate(le = (prcc_f * csho - ivst + dd1 + dltt + pstk)/ (prcc_f * csho)) %>% \n    \n    # get shareholder value\n    mutate(shv = prcc_f * csho) %>% \n    \n    # remove Infinity value for leverage effect (i.e., shareholder value = 0)\n    filter_all(all_vars(!is.infinite(.))) %>% \n    \n    # positive values only \n    filter_all(all_vars(. > 0)) %>% \n    \n    # get the within coefficient of variation\n    group_by(gvkey) %>% \n    mutate(within_var_mean_le = mean(le),\n           within_var_sd_le = sd(le)) %>% \n    ungroup()\n\n\n# get the mean and standard deviation\nmean(df_leverage_effect$le)\n#> [1] 150.1087\nmax(df_leverage_effect$le)\n#> [1] 183629.6\nhist(df_leverage_effect$le)\n\n# coefficient of variation \nsd(df_leverage_effect$le) / mean(df_leverage_effect$le) * 100\n#> [1] 2749.084\n\n# Within-firm variation (similar to fig 3a)\ndf_leverage_effect %>% \n    group_by(gvkey) %>% \n    slice(1) %>% \n    ungroup() %>% \n    dplyr::select(within_var_mean_le, within_var_sd_le) %>% \n    dplyr::mutate(cv = within_var_sd_le/ within_var_mean_le) %>% \n    dplyr::select(cv) %>% \n    pull() %>% \n    hist()"},{"path":"event-studies.html","id":"economic-significance","chapter":"29 Event Studies","heading":"29.1.2 Economic significance","text":"Total wealth gain (loss) event\\[\n\\Delta W_t = CAR_t \\times MKTVAL_0\n\\]\\(\\Delta W_t\\) = gain (loss)\\(\\Delta W_t\\) = gain (loss)\\(CAR_t\\) = cumulative residuals date \\(t\\)\\(CAR_t\\) = cumulative residuals date \\(t\\)\\(MKTVAL_0\\) market value firm event window\\(MKTVAL_0\\) market value firm event window","code":""},{"path":"event-studies.html","id":"statistical-power","chapter":"29 Event Studies","heading":"29.1.3 Statistical Power","text":"increases withmore firmsmore firmsless days event window (avoiding potential contamination confounds)less days event window (avoiding potential contamination confounds)","code":""},{"path":"event-studies.html","id":"testing","chapter":"29 Event Studies","heading":"29.2 Testing","text":"","code":""},{"path":"event-studies.html","id":"parametric-test","chapter":"29 Event Studies","heading":"29.2.1 Parametric Test","text":"(S. J. Brown Warner 1985) provide evidence even presence non-normality, parametric tests still perform well. Since proportion positive negative abnormal returns tends equal sample (least 5 securities). excess returns coverage normality sample size increases. Hence, parametric test advocated non-parametric one.Low power detect significance (Kothari Warner 1997)Power = f(sample, size, actual size abnormal returns, variance abnormal returns across firms)","code":""},{"path":"event-studies.html","id":"t-test","chapter":"29 Event Studies","heading":"29.2.1.1 T-test","text":"Applying CLT\\[\n\\begin{aligned}\nt_{CAR} &= \\frac{\\bar{CAR_{}}}{\\sigma (CAR_{})/\\sqrt{n}} \\\\\nt_{BHAR} &= \\frac{\\bar{BHAR_{}}}{\\sigma (BHAR_{})/\\sqrt{n}}\n\\end{aligned}\n\\]AssumeAbnormal returns normally distributedAbnormal returns normally distributedVar(abnormal returns) equal across firmsVar(abnormal returns) equal across firmsNo cross-correlation abnormal returns.cross-correlation abnormal returns.Hence, misspecified suspectedHeteroskedasticityHeteroskedasticityCross-sectional dependenceCross-sectional dependenceTechnically, abnormal returns follow non-normal distribution (design abnormal returns calculation, typically forces distribution normal)Technically, abnormal returns follow non-normal distribution (design abnormal returns calculation, typically forces distribution normal)address concerns, Patell Standardized Residual (PSR) can sometimes help.","code":""},{"path":"event-studies.html","id":"patell-standardized-residual-psr","chapter":"29 Event Studies","heading":"29.2.1.2 Patell Standardized Residual (PSR)","text":"(Patell 1976)Since market model uses observations outside event window, abnormal returns contain prediction errors top true residuals , standardized:\\[\nAR_{} = \\frac{\\hat{u}_{}}{s_i \\sqrt{C_{}}}\n\\]\\(\\hat{u}_{}\\) = estimated residual\\(\\hat{u}_{}\\) = estimated residual\\(s_i\\) = standard deviation estimate residuals (estimation period)\\(s_i\\) = standard deviation estimate residuals (estimation period)\\(C_{}\\) = correction account prediction’s increased variation outside estimation period (Strong 1992)\\(C_{}\\) = correction account prediction’s increased variation outside estimation period (Strong 1992)\\[\nC_{} = 1 + \\frac{1}{T} + \\frac{(R_{mt} - \\bar{R}_m)^2}{\\sum_t (R_{mt} - \\bar{R}_m)^2}\n\\]\\(T\\) = number observations (estimation period)\\(T\\) = number observations (estimation period)\\(R_{mt}\\) = average rate return stocks trading stock market time \\(t\\)\\(R_{mt}\\) = average rate return stocks trading stock market time \\(t\\)\\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^T R_{mt}\\)\\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^T R_{mt}\\)","code":""},{"path":"event-studies.html","id":"non-parametric-test","chapter":"29 Event Studies","heading":"29.2.2 Non-parametric Test","text":"assumptions return distributionNo assumptions return distributionSign Test (assumes symmetry returns)\nbinom.test()\nSign Test (assumes symmetry returns)binom.test()Wilcoxon Signed-Rank Test (allows non-symmetry returns)\nUse wilcox.test(sample)\nWilcoxon Signed-Rank Test (allows non-symmetry returns)Use wilcox.test(sample)Gen Sign TestGen Sign TestCorrado Rank TestCorrado Rank Test","code":""},{"path":"event-studies.html","id":"sample","chapter":"29 Event Studies","heading":"29.3 Sample","text":"Sample can relative small\n(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcements\nCan range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)\nSample can relative small(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcements(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcementsCan range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)Can range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)","code":""},{"path":"event-studies.html","id":"confounders","chapter":"29 Event Studies","heading":"29.3.1 Confounders","text":"Avoid confounding events: earnings announcements, key executive changes, unexpected stock buybacks, changes dividends within two-trading day window surrounding event, mergers acquisitions, spin-offers, stock splits, management changes, joint ventures, unexpected dividend, IPO, debt defaults, dividend cancellations (McWilliams Siegel 1997)According (Fornell et al. 2006), need control:one-day event period = day Wall Street Journal publish ACSI announcement.one-day event period = day Wall Street Journal publish ACSI announcement.5 days event rule news (PR Newswires, Dow Jones, Business Wires)\nM&, Spin-offs, stock splits\nCEO CFO changes,\nLayoffs, restructurings, earnings announcements, lawsuits\nCapital IQ - Key Developments: covers almost important events don’t search news.\n5 days event rule news (PR Newswires, Dow Jones, Business Wires)M&, Spin-offs, stock splitsM&, Spin-offs, stock splitsCEO CFO changes,CEO CFO changes,Layoffs, restructurings, earnings announcements, lawsuitsLayoffs, restructurings, earnings announcements, lawsuitsCapital IQ - Key Developments: covers almost important events don’t search news.Capital IQ - Key Developments: covers almost important events don’t search news.(. Sorescu, Warren, Ertekin 2017) examine confounding events short-term windows:RavenPack, 3982 US publicly traded firms, press releases (2000-2013)RavenPack, 3982 US publicly traded firms, press releases (2000-2013)3-day window around event dates3-day window around event datesThe difference sample full observations sample without confounded events negligible (non-significant).difference sample full observations sample without confounded events negligible (non-significant).Conclusion: excluding confounded observations may unnecessary short-term event studies.\nBiases can stem researchers pick choose events exclude\ntime progresses, events need exclude can infeasible.\nConclusion: excluding confounded observations may unnecessary short-term event studies.Biases can stem researchers pick choose events excludeBiases can stem researchers pick choose events excludeAs time progresses, events need exclude can infeasible.time progresses, events need exclude can infeasible.illustrate point, let’s quick simulation exerciseIn example, explore three types events:Focal eventsFocal eventsCorrelated events (.e., events correlated focal events; presence correlated events can follow presence focal event)Correlated events (.e., events correlated focal events; presence correlated events can follow presence focal event)Uncorrelated events (.e., events dates might randomly coincide focal events, correlated ).Uncorrelated events (.e., events dates might randomly coincide focal events, correlated ).ability control strength correlation focal correlated events study, well number unrelated events wish examine.Let’s examine implications including excluding correlated uncorrelated events estimates focal events.depicted plot, inclusion correlated events demonstrates minimal impact estimation focal events. Conversely, excluding correlated events can diminish statistical power. true cases pronounced correlation.However, consequences excluding unrelated events notably significant. becomes evident omitting around 40 unrelated events study, lose ability accurately identify true effects focal events. reality within research, often rely Key Developments database, excluding 150 events, practice can substantially impair capacity ascertain authentic impact focal events.little experiment really drives home point – better darn good reason exclude event study (make super convincing)!","code":"\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tidyverse)\n\n# Parameters\nn                  <- 100000         # Number of observations\nn_focal            <- round(n * 0.2) # Number of focal events\noverlap_correlated <- 0.5            # Overlapping percentage between focal and correlated events\n\n# Function to compute mean and confidence interval\nmean_ci <- function(x) {\n    m <- mean(x)\n    ci <- qt(0.975, length(x)-1) * sd(x) / sqrt(length(x)) # 95% confidence interval\n    list(mean = m, lower = m - ci, upper = m + ci)\n}\n\n# Simulate data\nset.seed(42)\ndata <- tibble(\n    date       = seq.Date(from = as.Date(\"2010-01-01\"), by = \"day\", length.out = n), # Date sequence\n    focal      = rep(0, n),\n    correlated = rep(0, n),\n    ab_ret     = rnorm(n)\n)\n\n\n# Define focal events\nfocal_idx <- sample(1:n, n_focal)\ndata$focal[focal_idx] <- 1\n\ntrue_effect <- 0.25\n\n# Adjust the ab_ret for the focal events to have a mean of true_effect\ndata$ab_ret[focal_idx] <- data$ab_ret[focal_idx] - mean(data$ab_ret[focal_idx]) + true_effect\n\n\n\n# Determine the number of correlated events that overlap with focal and those that don't\nn_correlated_overlap <- round(length(focal_idx) * overlap_correlated)\nn_correlated_non_overlap <- n_correlated_overlap\n\n# Sample the overlapping correlated events from the focal indices\ncorrelated_idx <- sample(focal_idx, size = n_correlated_overlap)\n\n# Get the remaining indices that are not part of focal\nremaining_idx <- setdiff(1:n, focal_idx)\n\n# Check to ensure that we're not attempting to sample more than the available remaining indices\nif (length(remaining_idx) < n_correlated_non_overlap) {\n    stop(\"Not enough remaining indices for non-overlapping correlated events\")\n}\n\n# Sample the non-overlapping correlated events from the remaining indices\ncorrelated_non_focal_idx <- sample(remaining_idx, size = n_correlated_non_overlap)\n\n# Combine the two to get all correlated indices\nall_correlated_idx <- c(correlated_idx, correlated_non_focal_idx)\n\n# Set the correlated events in the data\ndata$correlated[all_correlated_idx] <- 1\n\n\n# Inflate the effect for correlated events to have a mean of \ncorrelated_non_focal_idx <- setdiff(all_correlated_idx, focal_idx) # Fixing the selection of non-focal correlated events\ndata$ab_ret[correlated_non_focal_idx] <- data$ab_ret[correlated_non_focal_idx] - mean(data$ab_ret[correlated_non_focal_idx]) + 1\n\n\n# Define the numbers of uncorrelated events for each scenario\nnum_uncorrelated <- c(5, 10, 20, 30, 40)\n\n# Define uncorrelated events\nfor (num in num_uncorrelated) {\n    for (i in 1:num) {\n        data[paste0(\"uncorrelated_\", i)] <- 0\n        uncorrelated_idx <- sample(1:n, round(n * 0.1))\n        data[uncorrelated_idx, paste0(\"uncorrelated_\", i)] <- 1\n    }\n}\n\n\n# Define uncorrelated columns and scenarios\nunc_cols <- paste0(\"uncorrelated_\", 1:num_uncorrelated)\nresults <- tibble(\n    Scenario = c(\"Include Correlated\", \"Correlated Effects\", \"Exclude Correlated\", \"Exclude Correlated and All Uncorrelated\"),\n    MeanEffect = c(\n        mean_ci(data$ab_ret[data$focal == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$mean\n    ),\n    LowerCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$lower\n    ),\n    UpperCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$upper\n    )\n)\n\n# Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated\nfor (num in num_uncorrelated) {\n    unc_cols <- paste0(\"uncorrelated_\", 1:num)\n    results <- results %>%\n        add_row(\n            Scenario = paste(\"Exclude\", num, \"Uncorrelated\"),\n            MeanEffect = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$mean,\n            LowerCI = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$lower,\n            UpperCI = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$upper\n        )\n}\n\n\nggplot(results,\n       aes(\n           x = factor(Scenario, levels = Scenario),\n           y = MeanEffect,\n           ymin = LowerCI,\n           ymax = UpperCI\n       )) +\n    geom_pointrange() +\n    coord_flip() +\n    ylab(\"Mean Effect\") +\n    xlab(\"Scenario\") +\n    ggtitle(\"Mean Effect of Focal Events under Different Scenarios\") +\n    geom_hline(yintercept = true_effect,\n               linetype = \"dashed\",\n               color = \"red\") "},{"path":"event-studies.html","id":"biases","chapter":"29 Event Studies","heading":"29.4 Biases","text":"Different closing time obscure estimation abnormal returns, check (Campbell et al. 1998)Different closing time obscure estimation abnormal returns, check (Campbell et al. 1998)Upward bias aggregating CAR + transaction prices (bid ask)Upward bias aggregating CAR + transaction prices (bid ask)Cross-sectional dependence returns bias standard deviation estimates downward, inflates test statistics events share common dates (MacKinlay 1997). Hence, (Jaffe 1974) Calendar-time Portfolio Abnormal Returns (CTARs) used correct bias.Cross-sectional dependence returns bias standard deviation estimates downward, inflates test statistics events share common dates (MacKinlay 1997). Hence, (Jaffe 1974) Calendar-time Portfolio Abnormal Returns (CTARs) used correct bias.(Wiles, Morgan, Rego 2012): events confined relatively industries, cross-sectional dependence returns can bias SD estimate downward, inflating associated test statistics” (p. 47). control potential cross-sectional correlation abnormal returns, can use time-series standard deviation test statistic (S. J. Brown Warner 1980)(Wiles, Morgan, Rego 2012): events confined relatively industries, cross-sectional dependence returns can bias SD estimate downward, inflating associated test statistics” (p. 47). control potential cross-sectional correlation abnormal returns, can use time-series standard deviation test statistic (S. J. Brown Warner 1980)Sample selection bias (self-selection firms event treatment) similar omitted variable bias omitted variable private info leads firm take action.\nSee Endogenous Sample Selection methods correct bias.\nUse Heckman model (Acharya 1993)\nhard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)\nCan estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).\nExamples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)\n\nCounterfactual observations\nPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\n\nSwitching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)\n\nSample selection bias (self-selection firms event treatment) similar omitted variable bias omitted variable private info leads firm take action.See Endogenous Sample Selection methods correct bias.See Endogenous Sample Selection methods correct bias.Use Heckman model (Acharya 1993)\nhard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)\nCan estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).\nExamples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)\nUse Heckman model (Acharya 1993)hard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)hard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)Can estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).Can estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).Examples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)Examples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)Counterfactual observations\nPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\n\nSwitching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)\nCounterfactual observationsPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\nPropensity score matching:Finance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)Finance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)Marketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)Marketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)Switching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)Switching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)","code":""},{"path":"event-studies.html","id":"long-run-event-studies","chapter":"29 Event Studies","heading":"29.5 Long-run event studies","text":"Usually make assumption distribution abnormal returns events mean 0 (. Sorescu, Warren, Ertekin 2017, 192). (. Sorescu, Warren, Ertekin 2017) provide evidence events examine results samples without confounding events differ.Usually make assumption distribution abnormal returns events mean 0 (. Sorescu, Warren, Ertekin 2017, 192). (. Sorescu, Warren, Ertekin 2017) provide evidence events examine results samples without confounding events differ.Long-horizon event studies face challenges due systematic errors time sensitivity model choice.Long-horizon event studies face challenges due systematic errors time sensitivity model choice.Two main approaches used measure long-term abnormal stock returns\nBuy Hold Abnormal Returns (BHAR)\nLong-term Cumulative Abnormal Returns (LCARs)\nCalendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecification\nTwo main approaches used measure long-term abnormal stock returnsBuy Hold Abnormal Returns (BHAR)Buy Hold Abnormal Returns (BHAR)Long-term Cumulative Abnormal Returns (LCARs)Long-term Cumulative Abnormal Returns (LCARs)Calendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecificationCalendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecificationTwo types:\nUnexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.\nComplex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)\nTwo types:Unexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.Unexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.Complex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)Complex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)12 - 60 months event window: (Loughran Ritter 1995) (Brav Gompers 1997)12 - 60 months event window: (Loughran Ritter 1995) (Brav Gompers 1997)Example: (Dutta et al. 2018)Example: (Dutta et al. 2018)","code":"\nlibrary(crseEventStudy)\n\n# example by the package's author\ndata(demo_returns)\nSAR <-\n    sar(event = demo_returns$EON,\n        control = demo_returns$RWE,\n        logret = FALSE)\nmean(SAR)\n#> [1] 0.006870196"},{"path":"event-studies.html","id":"buy-and-hold-abnormal-returns-bhar","chapter":"29 Event Studies","heading":"29.5.1 Buy and Hold Abnormal Returns (BHAR)","text":"Classic references: (Loughran Ritter 1995) (Barber Lyon 1997) (Lyon, Barber, Tsai 1999)Use portfolio stocks close matches current firm period benchmark, see difference firm return portfolio.technical note measures returns buying stocks event-experiencing firms shorting stocks similar non-event firms within time.high cross-sectional correlations, BHARs’ t-stat can inflated, rank order affected (Markovitch Golder 2008; . B. Sorescu, Chandy, Prabhu 2007)construct portfolio, use similarsizebook--marketmomentumMatching Procedure (Barber Lyon 1997):year July June, common stocks CRSP database categorized ten groups (deciles) based market capitalization previous June.year July June, common stocks CRSP database categorized ten groups (deciles) based market capitalization previous June.Within deciles, firms sorted five groups (quintiles) based book--market ratios December previous year earlier, considering possible delays financial statement reporting.Within deciles, firms sorted five groups (quintiles) based book--market ratios December previous year earlier, considering possible delays financial statement reporting.Benchmark portfolios designed exclude firms specific events include firms can classified characteristic-based portfolios.Benchmark portfolios designed exclude firms specific events include firms can classified characteristic-based portfolios.Similarly, Wiles et al. (2010) uses following matching procedure:firms two-digit SIC code market values 50% 150% focal firms selectedFrom list, 10 firms comparable book--market ratios chosen serve matched portfolio (matched portfolio can less 10 firms).Calculations:\\[\nAR_{} = R_{} - E(R_{}|X_t)\n\\]Cumulative Abnormal Return (CAR):\\[\nCAR_{} = \\sum_{t=1}^T (R_{} - E(R_{}))\n\\]Buy--Hold Abnormal Return (BHAR)\\[\nBHAR_{t = 1}^T = \\Pi_{t=1}^T(1 + R_{}) - \\Pi_{t = 1}^T (1 + E(R_{}))\n\\]CAR arithmetic sum, BHAR geometric sum.short-term event studies, differences CAR BHAR often minimal. However, long-term studies, difference significantly skew results. (Barber Lyon 1997) shows BHAR usually slightly lower annual CAR, dramatically surpasses CAR annual BHAR exceeds 28%.calculate long-run return (\\(\\Pi_{t=1}^T (1 + E(R_{}))\\)) benchmark portfolio, can:annual rebalance: period, portfolio re-balanced compound mean stock returns portfolio given period:\\[\n\\Pi_{t = 1}^T (1 + E(R_{})) = \\Pi_{t}^T (1 + \\sum_{= s}^{n_t}w_{} R_{})\n\\]\\(n_t\\) number firms period \\(t\\), \\(w_{}\\) (1) \\(1/n_t\\) (2) value-weight firm \\(\\) period \\(t\\).avoid favoring recent events, cross-sectional event studies, researchers usually treat events equally studying impact stock market time. approach helps identify abnormal changes stock prices, especially dealing series unplanned events.Potential problems:Solution first: Form benchmark portfolios never change constituent firms (Mitchell Stafford 2000), problems:\nNewly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).\nRegularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).\nValue-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.\nSolution first: Form benchmark portfolios never change constituent firms (Mitchell Stafford 2000), problems:Newly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).Newly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).Regularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).Regularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).Value-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.Value-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.Without annual rebalance: Compounding returns securities comprising portfolio, followed calculating average across securities\\[\n\\Pi_{t = s}^{T} (1 + E(R_{})) = \\sum_{=s}^{n_t} (w_{} \\Pi_{t=1}^T (1 + R_{}))\n\\]\\(t\\) investment period, \\(R_{}\\) return security \\(\\), \\(n_i\\) number securities, \\(w_{}\\) either \\(1/n_s\\) value-weight factor security \\(\\) initial period \\(s\\). portfolio’s profits come simple investment included stocks given equal importance, weighted according market value, specific past period (period s). means doesn’t consider stocks listed period, adjust portfolio month. However, one problem method value assigned stock, based market size, needs corrected. make sure recent stocks don’t end much influence.Fortunately, WRDS, give types BHAR (2x2) (equal-weighted vs. value-weighted annual rebalance without annual rebalance)“MINWIN” smallest number months company trades event included study.“MAXWIN” months study considers calculations.\nCompanies aren’t excluded less MAXWIN months, unless also fewer MINWIN months.\n“MAXWIN” months study considers calculations.Companies aren’t excluded less MAXWIN months, unless also fewer MINWIN months.term “MONTH” signifies chosen months (typically 12, 24, 36) used work BHAR.\nmonthly returns missing set period, matching portfolio returns fill gaps.\nterm “MONTH” signifies chosen months (typically 12, 24, 36) used work BHAR.monthly returns missing set period, matching portfolio returns fill gaps.","code":""},{"path":"event-studies.html","id":"long-term-cumulative-abnormal-returns-lcars","chapter":"29 Event Studies","heading":"29.5.2 Long-term Cumulative Abnormal Returns (LCARs)","text":"Formula LCARs \\((1,T)\\) postevent horizon (. B. Sorescu, Chandy, Prabhu 2007)\\[\nLCAR_{pT} = \\sum_{t = 1}^{t = T} (R_{} - R_{pt})\n\\]\\(R_{}\\) rate return stock \\(\\) month \\(t\\)\\(R_{pt}\\) rate return counterfactual portfolio month \\(t\\)","code":""},{"path":"event-studies.html","id":"calendar-time-portfolio-abnormal-returns-ctars","chapter":"29 Event Studies","heading":"29.5.3 Calendar-time Portfolio Abnormal Returns (CTARs)","text":"section follows strictly procedure (Wiles et al. 2010)portfolio every day calendar time (including securities experience event time).portfolio, securities returns equally weightedFor portfolios, average abnormal return calculated \\[\nAAR_{Pt} = \\frac{\\sum_{=1}^S AR_i}{S}\n\\]\\(S\\) number securities portfolio \\(P\\)\\(AR_i\\) abnormal return stock \\(\\) portfolioFor every portfolio \\(P\\), time series estimate \\(\\sigma(AAR_{Pt})\\) calculated preceding \\(k\\) days, assuming \\(AAR_{Pt}\\) independent time.portfolio’s average abnormal return standardized\\[\nSAAR_{Pt} = \\frac{AAR_{Pt}}{SD(AAR_{Pt})}\n\\]Average standardized residual across portfolio’s calendar time\\[\nASAAR = \\frac{1}{n}\\sum_{=1}^{255} SAAR_{Pt} \\times D_t\n\\]\\(D_t = 1\\) least one security portfolio \\(t\\)\\(D_t = 1\\) least one security portfolio \\(t\\)\\(D_t = 0\\) security portfolio \\(t\\)\\(D_t = 0\\) security portfolio \\(t\\)\\(n\\) number days portfolio least one security \\(n = \\sum_{= 1}^{255}D_t\\)\\(n\\) number days portfolio least one security \\(n = \\sum_{= 1}^{255}D_t\\)cumulative average standardized average abnormal returns \\[\nCASSAR_{S_1, S_2} = \\sum_{=S_1}^{S_2} ASAAR\n\\]ASAAR independent time, standard deviation estimate \\(\\sqrt{S_2 - S_1 + 1}\\), test statistics \\[\nt = \\frac{CASAAR_{S_1,S_2}}{\\sqrt{S_2 - S_1 + 1}}\n\\]LimitationsCannot examine individual stock difference, can see difference portfolio level.\nOne can construct multiple portfolios (based metrics interest) firms portfolio shares characteristics. , one can compare intercepts portfolio.\nexamine individual stock difference, can see difference portfolio level.One can construct multiple portfolios (based metrics interest) firms portfolio shares characteristics. , one can compare intercepts portfolio.Low power (Loughran Ritter 2000), type II error likely.Low power (Loughran Ritter 2000), type II error likely.","code":""},{"path":"event-studies.html","id":"aggregation","chapter":"29 Event Studies","heading":"29.6 Aggregation","text":"","code":""},{"path":"event-studies.html","id":"over-time","chapter":"29 Event Studies","heading":"29.6.1 Over Time","text":"calculate cumulative abnormal (CAR) event windows\\(H_0\\): Standardized cumulative abnormal return stock \\(\\) 0 (effect events stock performance)\\(H_1\\): SCAR 0 (effect events stock performance)","code":""},{"path":"event-studies.html","id":"across-firms-over-time","chapter":"29 Event Studies","heading":"29.6.2 Across Firms + Over Time","text":"Additional assumptions: Abnormal returns different socks uncorrelated (rather strong), ’s valid event windows different stocks overlap. windows different overlap, follow (Bernard 1987) Schipper Smith (1983)\\(H_0\\): mean abnormal returns across firms 0 (effect)\\(H_1\\): mean abnormal returns across firms different form 0 (effect)Parametric (empirically either one works fine) (assume abnormal returns normally distributed) :Aggregate CAR stocks (Use true abnormal variance greater stocks higher variance)Aggregate SCAR stocks (Use true abnormal return constant across stocks)Non-parametric (parametric assumptions):Sign test:\nAssume abnormal returns CAR independent across stocks\nAssume 50% positive abnormal returns 50% negative abnormal return\nnull positive abnormal return correlated event (want alternative negative relationship)\nskewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might better\nAssume abnormal returns CAR independent across stocksAssume abnormal returns CAR independent across stocksAssume 50% positive abnormal returns 50% negative abnormal returnAssume 50% positive abnormal returns 50% negative abnormal returnThe null positive abnormal return correlated event (want alternative negative relationship)null positive abnormal return correlated event (want alternative negative relationship)skewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might betterWith skewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might betterRank test\nNull: abnormal return event window\nNull: abnormal return event window","code":""},{"path":"event-studies.html","id":"heterogeneity-in-the-event-effect","chapter":"29 Event Studies","heading":"29.7 Heterogeneity in the event effect","text":"\\[\ny = X \\theta + \\eta\n\\]\\(y\\) = CAR\\(y\\) = CAR\\(X\\) = Characteristics lead heterogeneity event effect (.e., abnormal returns) (e.g., firm event specific)\\(X\\) = Characteristics lead heterogeneity event effect (.e., abnormal returns) (e.g., firm event specific)\\(\\eta\\) = error term\\(\\eta\\) = error termNote:cases selection bias (firm characteristics investor anticipation event: larger firms might enjoy great positive effect event, investors endogenously anticipate effect overvalue stock), use White’s \\(t\\)-statistics lower bounds true significance estimates.technique employed even average CAR significantly different 0, especially CAR variance high (Boyd, Chandy, Cunha Jr 2010)","code":""},{"path":"event-studies.html","id":"common-variables-in-marketing","chapter":"29 Event Studies","heading":"29.7.1 Common variables in marketing","text":"(. Sorescu, Warren, Ertekin 2017) Table 4Firm size negatively correlated abnormal return finance (. Sorescu, Warren, Ertekin 2017), mixed results marketing.Firm size negatively correlated abnormal return finance (. Sorescu, Warren, Ertekin 2017), mixed results marketing.# event occurrences# event occurrencesR&D expenditureR&D expenditureAdvertising expenseAdvertising expenseMarketing investment (SG&)Marketing investment (SG&)Industry concentration (HHI, # competitors)Industry concentration (HHI, # competitors)Financial leverageFinancial leverageMarket shareMarket shareMarket size (total sales volume within firm’s SIC code)Market size (total sales volume within firm’s SIC code)marketing capabilitymarketing capabilityBook market valueBook market valueROAROAFree cash flowFree cash flowSales growthSales growthFirm ageFirm age","code":""},{"path":"event-studies.html","id":"expected-return-calculation","chapter":"29 Event Studies","heading":"29.8 Expected Return Calculation","text":"","code":""},{"path":"event-studies.html","id":"statistical-models","chapter":"29 Event Studies","heading":"29.8.1 Statistical Models","text":"based statistical assumptions behavior returns (e..g, multivariate normality)based statistical assumptions behavior returns (e..g, multivariate normality)need assume stable distributions (Owen Rabinovitch 1983)need assume stable distributions (Owen Rabinovitch 1983)","code":""},{"path":"event-studies.html","id":"constant-mean-return-model","chapter":"29 Event Studies","heading":"29.8.1.1 Constant Mean Return Model","text":"expected normal return mean real returns\\[\nRa_{} = R_{} - \\bar{R}_i\n\\]Assumption:returns revert mean (questionable)basic mean returns model generally delivers similar findings complex models since variance abnormal returns decreased considerably (S. J. Brown Warner 1985)","code":""},{"path":"event-studies.html","id":"market-model","chapter":"29 Event Studies","heading":"29.8.1.2 Market Model","text":"\\[\nR_{} = \\alpha_i + \\beta R_{mt} + \\epsilon_{}\n\\]\\(R_{}\\) = stock return \\(\\) period \\(t\\)\\(R_{}\\) = stock return \\(\\) period \\(t\\)\\(R_{mt}\\) = market return\\(R_{mt}\\) = market return\\(\\epsilon_{}\\) = zero mean (\\(E(e_{}) = 0\\)) error term variance \\(\\sigma^2\\)\\(\\epsilon_{}\\) = zero mean (\\(E(e_{}) = 0\\)) error term variance \\(\\sigma^2\\)Notes:People typically use S&P 500, CRSP value-weighed equal-weighted index market portfolio.People typically use S&P 500, CRSP value-weighed equal-weighted index market portfolio.\\(\\beta =0\\), Market Model Constant Mean Return ModelWhen \\(\\beta =0\\), Market Model Constant Mean Return Modelbetter fit market-model, less variance abnormal return, easy detect event’s effectbetter fit market-model, less variance abnormal return, easy detect event’s effectrecommend generalized method moments robust auto-correlation heteroskedasticityrecommend generalized method moments robust auto-correlation heteroskedasticity","code":""},{"path":"event-studies.html","id":"fama-french-model","chapter":"29 Event Studies","heading":"29.8.1.3 Fama-French Model","text":"Please note difference just taking return versus taking excess return dependent variable.correct way use excess return firm market (Fama French 2010, 1917).\\(\\alpha_i\\) “average return left unexplained benchmark model” (.e., abnormal return)","code":""},{"path":"event-studies.html","id":"ff3","chapter":"29 Event Studies","heading":"29.8.1.3.1 FF3","text":"(Fama French 1993)\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} = \\alpha_i &+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t\n\\end{aligned}\n\\]\\(r_{ft}\\) risk-free rate (e.g., 3-month Treasury bill)\\(r_{ft}\\) risk-free rate (e.g., 3-month Treasury bill)\\(R_{mt}\\) market-rate (e.g., S&P 500)\\(R_{mt}\\) market-rate (e.g., S&P 500)SML: returns small (size) portfolio minus returns big portfolioSML: returns small (size) portfolio minus returns big portfolioHML: returns high (B/M) portfolio minus returns low portfolio.HML: returns high (B/M) portfolio minus returns low portfolio.","code":""},{"path":"event-studies.html","id":"ff4","chapter":"29 Event Studies","heading":"29.8.1.3.2 FF4","text":"(. Sorescu, Warren, Ertekin 2017, 195) suggest use Market Model marketing short-term window Fama-French Model long-term window (statistical properties model examined daily setting).(Carhart 1997)\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} = \\alpha_i &+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t\n\\end{aligned}\n\\]\\(UMD_t\\) momentum factor (difference high low prior return stock portfolios) day \\(t\\).","code":""},{"path":"event-studies.html","id":"economic-model","chapter":"29 Event Studies","heading":"29.8.2 Economic Model","text":"difference CAPM APT APT multiple factors (including factors beyond focal company)Economic models put limits statistical model come assumed behavior derived theory.","code":""},{"path":"event-studies.html","id":"capital-asset-pricing-model-capm","chapter":"29 Event Studies","heading":"29.8.2.1 Capital Asset Pricing Model (CAPM)","text":"\\[\nE(R_i) = R_f + \\beta_i (E(R_m) - R_f)\n\\]\\(E(R_i)\\) = expected firm return\\(E(R_i)\\) = expected firm return\\(R_f\\) = risk free rate\\(R_f\\) = risk free rate\\(E(R_m - R_f)\\) = market risk premium\\(E(R_m - R_f)\\) = market risk premium\\(\\beta_i\\) = firm sensitivity\\(\\beta_i\\) = firm sensitivity","code":""},{"path":"event-studies.html","id":"arbitrage-pricing-theory-apt","chapter":"29 Event Studies","heading":"29.8.2.2 Arbitrage Pricing Theory (APT)","text":"\\[\nR = R_f + \\Lambda f + \\epsilon\n\\]\\(\\epsilon \\sim N(0, \\Psi)\\)\\(\\epsilon \\sim N(0, \\Psi)\\)\\(\\Lambda\\) = factor loadings\\(\\Lambda\\) = factor loadings\\(f \\sim N(\\mu, \\Omega)\\) = general factor model\n\\(\\mu\\) = expected risk premium vector\n\\(\\Omega\\) = factor covariance matrix\n\\(f \\sim N(\\mu, \\Omega)\\) = general factor model\\(\\mu\\) = expected risk premium vector\\(\\mu\\) = expected risk premium vector\\(\\Omega\\) = factor covariance matrix\\(\\Omega\\) = factor covariance matrix","code":""},{"path":"event-studies.html","id":"application-13","chapter":"29 Event Studies","heading":"29.9 Application","text":"Packages:eventstudieseventstudiesererererEventStudyEventStudyAbnormalReturnsAbnormalReturnsEvent Study ToolsEvent Study Toolsestudy2estudy2PerformanceAnalyticsPerformanceAnalyticsIn practice, people usually sort portfolio sure whether FF model specified correctly.Steps:Sort returns CRSP 10 deciles based size.decile, sort returns 10 decides based BMGet average return 100 portfolios period (.e., expected returns stocks given decile - characteristics)stock event study: Compare return stock corresponding portfolio based size BM.Notes:Sorting produces outcomes often conservative (e.g., FF abnormal returns can greater used sorting).Sorting produces outcomes often conservative (e.g., FF abnormal returns can greater used sorting).results change B/M first size vice versa, results robust (extends just two characteristics - e.g., momentum).results change B/M first size vice versa, results robust (extends just two characteristics - e.g., momentum).Examples:Forestry:(Mei Sun 2008) M&financial performance (forest product)(Mei Sun 2008) M&financial performance (forest product)(C. Sun Liao 2011) litigation firm values(C. Sun Liao 2011) litigation firm valuesExample Ana Julia Akaishi Padula, Pedro Albuquerque (posted LAMFO)Example AbnormalReturns package","code":"\nlibrary(erer)\n\n# example by the package's author\ndata(daEsa)\nhh <- evReturn(\n    y = daEsa,       # dataset\n    firm = \"wpp\",    # firm name\n    y.date = \"date\", # date in y \n    index = \"sp500\", # index\n    est.win = 250,   # estimation window wedith in days\n    digits = 3, \n    event.date = 19990505, # firm event dates \n    event.win = 5          # one-side event window wdith in days (default = 3, where 3 before + 1 event date + 3 days after = 7 days)\n)\nhh; plot(hh)\n#> \n#> === Regression coefficients by firm =========\n#>   N firm event.date alpha.c alpha.e alpha.t alpha.p alpha.s beta.c beta.e\n#> 1 1  wpp   19990505  -0.135   0.170  -0.795   0.428          0.665  0.123\n#>   beta.t beta.p beta.s\n#> 1  5.419  0.000    ***\n#> \n#> === Abnormal returns by date ================\n#>    day Ait.wpp    HNt\n#> 1   -5   4.564  4.564\n#> 2   -4   0.534  5.098\n#> 3   -3  -1.707  3.391\n#> 4   -2   2.582  5.973\n#> 5   -1  -0.942  5.031\n#> 6    0  -3.247  1.784\n#> 7    1  -0.646  1.138\n#> 8    2  -2.071 -0.933\n#> 9    3   0.368 -0.565\n#> 10   4   4.141  3.576\n#> 11   5   0.861  4.437\n#> \n#> === Average abnormal returns across firms ===\n#>      name estimate error t.value p.value sig\n#> 1 CiT.wpp    4.437 8.888   0.499   0.618    \n#> 2     GNT    4.437 8.888   0.499   0.618"},{"path":"event-studies.html","id":"eventus","chapter":"29 Event Studies","heading":"29.9.1 Eventus","text":"2 types output:Using different estimation methods (e.g., market model calendar-time approach)\ninclude event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.\nUsing different estimation methods (e.g., market model calendar-time approach)Using different estimation methods (e.g., market model calendar-time approach)include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.Cross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)\nSince stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.\nCross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)Since stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.","code":""},{"path":"event-studies.html","id":"basic-event-study","chapter":"29 Event Studies","heading":"29.9.1.1 Basic Event Study","text":"Input text file contains firm identifier (e.g., PERMNO, CUSIP) event dateChoose market indices: equally weighted value weighted index (.e., weighted market capitalization). check Fama-French Carhart factors.Estimation options\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.\nUse “autodate” options: first trading event date used event falls weekend holiday\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.Estimation period: ESTLEN = 100 convention estimation impacted outliers.Use “autodate” options: first trading event date used event falls weekend holidayUse “autodate” options: first trading event date used event falls weekend holidayAbnormal returns window: depends specific eventChoose test: either parametric (including Patell Standardized Residual (PSR)) non-parametric","code":""},{"path":"event-studies.html","id":"cross-sectional-analysis-of-eventus","chapter":"29 Event Studies","heading":"29.9.1.2 Cross-sectional Analysis of Eventus","text":"Similar Basic Event Study, now can event-specific abnormal returns.","code":""},{"path":"event-studies.html","id":"evenstudies","chapter":"29 Event Studies","heading":"29.9.2 Evenstudies","text":"package use Fama-French model, market models.example author package","code":"\nlibrary(eventstudies)\n# firm and date data\ndata(\"SplitDates\")\nhead(SplitDates)\n\n# stock price data \ndata(\"StockPriceReturns\")\nhead(StockPriceReturns)\nclass(StockPriceReturns)\n\nes <-\n    eventstudy(\n        firm.returns = StockPriceReturns,\n        event.list = SplitDates,\n        event.window = 5,\n        type = \"None\",\n        to.remap = TRUE,\n        remap = \"cumsum\",\n        inference = TRUE,\n        inference.strategy = \"bootstrap\"\n    )\n\nplot(es)"},{"path":"event-studies.html","id":"eventstudy","chapter":"29 Event Studies","heading":"29.9.3 EventStudy","text":"pay API key. (’s $10/month).Example authors packageData PrepReference market Germany DAXCreate files01_RequestFile.csv02_FirmData.csv03_MarketData.csvCalculating abnormal returns","code":"\nlibrary(EventStudy)\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(\"Quandl\")\nlibrary(\"quantmod\")\nQuandl.auth(\"LDqWhYXzVd2omw4zipN2\")\nTWTR <- Quandl(\"NSE/OIL\",type =\"xts\")\ncandleChart(TWTR)\naddSMA(col=\"red\") #Adding a Simple Moving Average\naddEMA() #Adding an Exponential Moving Average\n# Index Data\n# indexName <- c(\"DAX\")\n\nindexData <- tq_get(\"^GDAXI\", from = \"2014-05-01\", to = \"2015-12-31\") %>%\n    mutate(date = format(date, \"%d.%m.%Y\")) %>%\n    mutate(symbol = \"DAX\")\n\nhead(indexData)\n# get & set parameters for abnormal return Event Study\n# we use a garch model and csv as return\n# Attention: fitting a GARCH(1, 1) model is compute intensive\nesaParams <- EventStudy::ARCApplicationInput$new()\nesaParams$setResultFileType(\"csv\")\nesaParams$setBenchmarkModel(\"garch\")\n\n\ndataFiles <-\n    c(\n        \"request_file\" = file.path(getwd(), \"data\", \"EventStudy\", \"01_requestFile.csv\"),\n        \"firm_data\"    = file.path(getwd(), \"data\", \"EventStudy\", \"02_firmDataPrice.csv\"),\n        \"market_data\"  = file.path(getwd(), \"data\", \"EventStudy\", \"03_marketDataPrice.csv\")\n    )\n\n# check data files, you can do it also in our R6 class\nEventStudy::checkFiles(dataFiles)\narEventStudy <- estSetup$performEventStudy(estParams     = esaParams, \n                                      dataFiles     = dataFiles, \n                                      downloadFiles = T)\nlibrary(EventStudy)\n\napiUrl <- \"https://api.eventstudytools.com\"\nSys.setenv(EventStudyapiKey = \"\")\n\n# The URL is already set by default\noptions(EventStudy.URL = apiUrl)\noptions(EventStudy.KEY = Sys.getenv(\"EventStudyapiKey\"))\n\n# use EventStudy estAPIKey function\nestAPIKey(Sys.getenv(\"EventStudyapiKey\"))\n\n# initialize object\nestSetup <- EventStudyAPI$new()\nestSetup$authentication(apiKey = Sys.getenv(\"EventStudyapiKey\"))"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"30 Instrumental Variables","heading":"30 Instrumental Variables","text":"Similar RCT, try introduce randomization (random assignment treatment) treatment variable using variation instrument.Logic using instrument:Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)See relationship treatment outcome terms residual variations exogenous omitted variables.See relationship treatment outcome terms residual variations exogenous omitted variables.Notes:Instruments can used remove attenuation bias errors--variables.Instruments can used remove attenuation bias errors--variables.careful F-test standard errors 2SLS hand (need correct ).careful F-test standard errors 2SLS hand (need correct ).Repeated use related IVs across different studies can collectively invalidate instruments, primarily violation exclusion restriction (Gallen 2020). One needs test invalid instruments (Hausman-like test).\nMellon (2023) shows widespread use weather instrument social sciences (289 studies linking weather 195 variables) demonstrates significant exclusion violations can overturn many IV results.\nRepeated use related IVs across different studies can collectively invalidate instruments, primarily violation exclusion restriction (Gallen 2020). One needs test invalid instruments (Hausman-like test).Mellon (2023) shows widespread use weather instrument social sciences (289 studies linking weather 195 variables) demonstrates significant exclusion violations can overturn many IV results.[Zero-valued Outcomes], can’t directly interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change. percentage change interpretation, can either :\nProportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,\nRegress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.\nGet estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\n\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\nSE can obtained non-parametric bootstrap.\nspecific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).\n\nLee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.\n[Zero-valued Outcomes], can’t directly interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change. percentage change interpretation, can either :Proportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,\nRegress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.\nGet estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\n\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\nSE can obtained non-parametric bootstrap.\nspecific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).\nProportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,Regress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.Regress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.Get estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)Get estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.SE can obtained non-parametric bootstrap.SE can obtained non-parametric bootstrap.specific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).specific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).Lee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.Lee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.Notes First-stage:Always use OLS regression first stage (regardless type endogenous variables - e.g., continuous discreet) (suggested (J. D. Angrist Pischke 2009). Estimates IV can still consistent regardless form endogenous variables (discreet vs. continuous).\nAlternatively, use “biprobit” model, applicable cases dependent endogenous variables binary.\nAlways use OLS regression first stage (regardless type endogenous variables - e.g., continuous discreet) (suggested (J. D. Angrist Pischke 2009). Estimates IV can still consistent regardless form endogenous variables (discreet vs. continuous).Alternatively, use “biprobit” model, applicable cases dependent endogenous variables binary.still want continue use logit probit models first stage binary variables, “forbidden regression” (also 1, 2) (.e., incorrect extension 2SLS nonlinear case).still want continue use logit probit models first stage binary variables, “forbidden regression” (also 1, 2) (.e., incorrect extension 2SLS nonlinear case).several ways understand problem:Identification strategy: identification strategy instrumental variables analysis relies fact instrumental variable affects outcome variable effect endogenous variable. However, endogenous variable binary, relationship instrumental variable endogenous variable continuous. means instrumental variable can affect endogenous variable discrete jumps, rather continuous change. result, identification causal effect endogenous variable outcome variable may possible probit logit regression first stage.Identification strategy: identification strategy instrumental variables analysis relies fact instrumental variable affects outcome variable effect endogenous variable. However, endogenous variable binary, relationship instrumental variable endogenous variable continuous. means instrumental variable can affect endogenous variable discrete jumps, rather continuous change. result, identification causal effect endogenous variable outcome variable may possible probit logit regression first stage.Model assumptions: models assume error term specific distribution (normal logistic), probability binary outcome function linear combination regressors.\nendogenous variable binary, however, distribution error term specified, continuous relationship endogenous variable outcome variable. means assumptions probit logit models may hold, resulting estimates may reliable interpretable.Model assumptions: models assume error term specific distribution (normal logistic), probability binary outcome function linear combination regressors.endogenous variable binary, however, distribution error term specified, continuous relationship endogenous variable outcome variable. means assumptions probit logit models may hold, resulting estimates may reliable interpretable.Issue weak instruments: instrument weak, variance inverse Mills ratio (used correct endogeneity instrumental variables analysis) can large. case binary endogenous variables, inverse Mills ratio consistently estimated using probit logit regression, can lead biased inconsistent estimates causal effect endogenous variable outcome variable.Issue weak instruments: instrument weak, variance inverse Mills ratio (used correct endogeneity instrumental variables analysis) can large. case binary endogenous variables, inverse Mills ratio consistently estimated using probit logit regression, can lead biased inconsistent estimates causal effect endogenous variable outcome variable.Problems weak instruments (Bound, Jaeger, Baker 1995):Weak instrumental variables can produce (finite-sample) biased inconsistent estimates causal effect endogenous variable outcome variable (even presence large sample size)Weak instrumental variables can produce (finite-sample) biased inconsistent estimates causal effect endogenous variable outcome variable (even presence large sample size)finite sample, instrumental variables (IV) estimates can biased direction ordinary least squares (OLS) estimates. Additionally, bias IV estimates approaches OLS estimates correlation (R2) instruments endogenous explanatory variable approaches zero. means correlation instruments endogenous variable weak, bias IV estimates can similar OLS estimates.finite sample, instrumental variables (IV) estimates can biased direction ordinary least squares (OLS) estimates. Additionally, bias IV estimates approaches OLS estimates correlation (R2) instruments endogenous explanatory variable approaches zero. means correlation instruments endogenous variable weak, bias IV estimates can similar OLS estimates.Weak instruments problematic enough variation fully capture variation endogenous variable, leading measurement error sources noise estimates.Weak instruments problematic enough variation fully capture variation endogenous variable, leading measurement error sources noise estimates.Using weak instruments can produce large standard errors low t-ratio. feedback (reverse causality) strong, bias IV even greater OLS (C. Nelson Startz 1988).Using weak instruments can produce large standard errors low t-ratio. feedback (reverse causality) strong, bias IV even greater OLS (C. Nelson Startz 1988).Using lagged dependent variables instruments current values depends serial correlations, typically low (C. Nelson Startz 1988).Using lagged dependent variables instruments current values depends serial correlations, typically low (C. Nelson Startz 1988).Using multiple covariates artificially increase first-stage \\(R^2\\) solve weak instrument problem (C. Nelson Startz 1988).Using multiple covariates artificially increase first-stage \\(R^2\\) solve weak instrument problem (C. Nelson Startz 1988).Solutions:\nuse multiple instruments\nuse instrumental variables higher correlation\nuse alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.\nSolutions:use multiple instrumentsuse multiple instrumentsuse instrumental variables higher correlationuse instrumental variables higher correlationuse alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.use alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.Instrument Validity:Random assignment (Exogeneity Assumption).effect instrument outcome must endogenous variable (Relevance Assumption).","code":""},{"path":"instrumental-variables.html","id":"framework","chapter":"30 Instrumental Variables","heading":"30.1 Framework","text":"\\(D_i \\sim Bern\\) Dummy Treatment\\(D_i \\sim Bern\\) Dummy Treatment\\(Y_{0i}, Y_{1i}\\) potential outcomes\\(Y_{0i}, Y_{1i}\\) potential outcomes\\(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\\) observed outcome\\(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\\) observed outcome\\(Z_i \\perp Y_{0i}, Y_{1i}\\) Instrumental variables (also correlate \\(D_i\\))\\(Z_i \\perp Y_{0i}, Y_{1i}\\) Instrumental variables (also correlate \\(D_i\\))constant-effects linear (\\(Y_{1i} - Y_{0i}\\) everyone)\\[ \\begin{aligned} Y_{0i} &= \\alpha + \\eta_i \\\\ Y_{1i} - Y_{0i} &= \\rho \\\\ Y_i &= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\\\ &= \\alpha + \\eta_i  + D_i \\rho \\\\ &= \\alpha + \\rho D_i + \\eta_i \\end{aligned} \\]\\(\\eta_i\\) individual differences\\(\\eta_i\\) individual differences\\(\\rho\\) difference treated outcome untreated outcome. assume constant everyone\\(\\rho\\) difference treated outcome untreated outcome. assume constant everyoneHowever, problem OLS \\(D_i\\) correlated \\(\\eta_i\\) unitBut \\(Z_i\\) can come rescue, causal estimate can written \\[ \\begin{aligned} \\rho &= \\frac{Cov( Y_i, Z_i)}{Cov(D_i, Z_i)} \\\\ &= \\frac{Cov(Y_i, Z_i) / V(Z_i) }{Cov( D_i, Z_i) / V(Z_i)} = \\frac{Reduced form}{First-stage} \\\\ &= \\frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]} \\end{aligned} \\]heterogeneous treatment effect (\\(Y_{1i} - Y_{0i}\\) different everyone) LATE framework\\(Y_i(d,z)\\) denotes potential outcome unit \\(\\) treatment \\(D_i = d\\) instrument \\(Z_i = z\\)Observed treatment status\\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}) \\]\\(D_{1i}\\) treatment status unit \\(\\) \\(z_i = 1\\)\\(D_{1i}\\) treatment status unit \\(\\) \\(z_i = 1\\)\\(D_{0i}\\) treatment status unit \\(\\) \\(z_i = 0\\)\\(D_{0i}\\) treatment status unit \\(\\) \\(z_i = 0\\)\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\)\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\)AssumptionsIndependence: instrument randomly assigned (.e., independent potential outcomes potential treatments)\n\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)\nassumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)\n\\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &= E[D_{1i} - D_{0i}] \\end{aligned} \\]\nassumption also sufficient causal interpretation reduced form, see effect instrument outcome.\nIndependence: instrument randomly assigned (.e., independent potential outcomes potential treatments)\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)assumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)assumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)\\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &= E[D_{1i} - D_{0i}] \\end{aligned} \\]assumption also sufficient causal interpretation reduced form, see effect instrument outcome.\\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \\]Exclusion (.e., existence instruments (G. W. Imbens Angrist 1994)\ntreatment \\(D_i\\) fully mediates effect \\(Z_i\\) \\(Y_i\\)\n\\[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\\\  Y_{0i} = Y_i (0,1) = Y_i (0, 0) \\]\nassumption, observed outcome \\(Y_i\\) can thought (assume \\(Y_{1i}, Y_{0i}\\) already satisfy independence assumption)\n\\[ \\begin{aligned} Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\ &= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \\end{aligned} \\]\nassumption let us go reduced-form causal effects treatment effects (J. D. Angrist Imbens 1995)\nExclusion (.e., existence instruments (G. W. Imbens Angrist 1994)treatment \\(D_i\\) fully mediates effect \\(Z_i\\) \\(Y_i\\)\\[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\\\  Y_{0i} = Y_i (0,1) = Y_i (0, 0) \\]assumption, observed outcome \\(Y_i\\) can thought (assume \\(Y_{1i}, Y_{0i}\\) already satisfy independence assumption)\\[ \\begin{aligned} Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\ &= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \\end{aligned} \\]assumption let us go reduced-form causal effects treatment effects (J. D. Angrist Imbens 1995)Monotonicity: \\(D_{1i} > D_{0i} \\forall \\)\nassumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)\nassumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)\nassumption used solve problem shifts participation status back non-participation status.\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.\nthird solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).\n\nMonotonicity: \\(D_{1i} > D_{0i} \\forall \\)assumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)assumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)assumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)assumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)assumption used solve problem shifts participation status back non-participation status.\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.\nthird solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).\nassumption used solve problem shifts participation status back non-participation status.Alternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.Alternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).three assumptions, LATE theorem (J. D. Angrist Pischke 2009, 4.4.1)\\[ \\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} > D_{0i}] \\]LATE assumptions allow us go back types subjects Causal InferenceSwitchers:\nCompliers: \\(D_{1i} > D_{0i}\\)\nSwitchers:Compliers: \\(D_{1i} > D_{0i}\\)Non-switchers:\nAlways-takers: \\(D_{1i} = D_{0i} = 1\\)\nNever-takers: \\(D_{1i} = D_{0i} = 0\\)\nNon-switchers:Always-takers: \\(D_{1i} = D_{0i} = 1\\)Always-takers: \\(D_{1i} = D_{0i} = 1\\)Never-takers: \\(D_{1i} = D_{0i} = 0\\)Never-takers: \\(D_{1i} = D_{0i} = 0\\)Instrumental Variables can’t say anything non-switchers treatment status \\(D_i\\) effects (similar fixed effects models).groups , come back constant-effects world.Treatment effects treated weighted average always-takers compliers.special case IV randomized trials, compliance problem (compliance voluntary), treated always take treatment (.e., might selection bias).Intention--treat analysis valid, contaminated non-complianceIntention--treat analysis valid, contaminated non-complianceIV case (\\(Z_i\\) = random assignment treatment; \\(D_i\\) = whether unit actually received/took treatment) can solve problem.IV case (\\(Z_i\\) = random assignment treatment; \\(D_i\\) = whether unit actually received/took treatment) can solve problem.certain assumptions (.e., SUTVA, random assignment, exclusion restriction, defiers, monotinicity), analysis can give causal interpreation LATE ’s average causal effect compliers .\nWithout assumptions, ’s ratio intention--treat.\ncertain assumptions (.e., SUTVA, random assignment, exclusion restriction, defiers, monotinicity), analysis can give causal interpreation LATE ’s average causal effect compliers .Without assumptions, ’s ratio intention--treat.Without always-takers case, LATE = Treatment effects treatedWithout always-takers case, LATE = Treatment effects treatedSee proof Bloom (1984) examples Bloom et al. (1997) Sherman Berk (1984)\\[ \\frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \\frac{\\text{Intention--treat effect}}{\\text{Compliance rate}} \\\\ = E[Y_{1i} - Y_{0i} |D_i = 1] \\]","code":""},{"path":"instrumental-variables.html","id":"estimation-3","chapter":"30 Instrumental Variables","heading":"30.2 Estimation","text":"","code":""},{"path":"instrumental-variables.html","id":"sls-estimation","chapter":"30 Instrumental Variables","heading":"30.2.1 2SLS Estimation","text":"special case IV-GMMExamples authors fixest packageDefault statisticsF-test first-stage (weak instrument test)Wu-Hausman endogeneity testOver-identifying restriction (Sargan) J-testTo set default printingTo see results different stages","code":"\nlibrary(fixest)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\n# est_iv = feols(y ~ x1 | x_endo_1  ~ x_inst_1 , base)\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nest_iv\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\nfitstat(\n    est_iv,\n    type = c(\n        \"n\", \"ll\", \"aic\", \"bic\", \"rmse\", # ll means log-likelihood\n        \n        \"my\", # mean dependent var\n\n        \"g\", # degrees of freedom used to compute the t-test\n\n        \"r2\", \"ar2\", \"wr2\", \"awr2\", \"pr2\", \"apr2\", \"wpr2\", \"awpr2\",\n\n        \"theta\", # over-dispersion parameter in Negative Binomial models\n\n        \"f\", \"wf\", # F-tests of nullity of the coefficients\n\n        \"wald\", # Wald test of joint nullity of the coefficients\n\n        \"ivf\",\n        \n        \"ivf1\",\n\n        \"ivf2\",\n\n        \"ivfall\",\n        \n        \"ivwald\", \"ivwald1\", \"ivwald2\", \"ivwaldall\",\n\n        \"cd\"\n        \n        # \"kpr\"\n        \n        \n        ),\n    cluster = 'fe'\n)\n#>                 Observations: 150\n#>               Log-Likelihood: -75.0\n#>                          AIC: 157.9\n#>                          BIC: 170.0\n#>                         RMSE: 0.398842\n#>               Dep. Var. mean: 5.84333\n#>                            G: 3\n#>                           R2: 0.766452\n#>                      Adj. R2: 0.761653\n#>                    Within R2: NA\n#>                         awr2: NA\n#>                    Pseudo R2: 0.592684\n#>               Adj. Pseudo R2: 0.576383\n#>             Within Pseudo R2: NA\n#>                        awpr2: NA\n#>              Over-dispersion: NA\n#>                       F-test: stat =       1.80769, p = 0.375558, on 3 and 2 DoF.\n#>           F-test (projected): NA\n#>         Wald (joint nullity): stat = 539,363.2    , p < 2.2e-16 , on 3 and 146 DoF, VCOV: Clustered (fe).\n#> F-test (1st stage), x_endo_1: stat =     903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =       3.25828, p = 0.041268, on 2 and 146 DoF.\n#>           F-test (2nd stage): stat =     194.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#>             F-test (IV only): stat =     194.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> Wald (1st stage), x_endo_1  : stat =   1,482.6    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#> Wald (1st stage), x_endo_2  : stat =       2.22157, p = 0.112092, on 2 and 146 DoF, VCOV: Clustered (fe).\n#>             Wald (2nd stage): stat = 539,363.2    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#>               Wald (IV only): stat = 539,363.2    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#>                 Cragg-Donald: 3.11162\n# always add second-stage Wald test\nsetFixest_print(fitstat = ~ . + ivwald2)\nest_iv\n# first-stage\nsummary(est_iv, stage = 1)\n\n# second-stage\nsummary(est_iv, stage = 2)\n\n# both stages\netable(summary(est_iv, stage = 1:2), fitstat = ~ . + ivfall + ivwaldall.p)\netable(summary(est_iv, stage = 2:1), fitstat = ~ . + ivfall + ivwaldall.p)\n# .p means p-value, not statistic\n# `all` means IV only"},{"path":"instrumental-variables.html","id":"iv-gmm","chapter":"30 Instrumental Variables","heading":"30.2.2 IV-GMM","text":"general framework.2SLS Estimation special case IV-GMM estimator\\[\nY = X \\beta + u, u \\sim (0, \\Omega)\n\\]\\(X\\) matrix endogenous variables (\\(N\\times k\\))use matrix instruments \\(X\\) \\(N \\times l\\) dimensions (\\(l \\ge k\\)), can set \\(l\\) moments:\\[\ng_i (\\beta) = Z_i' u_i = Z_i' (Y_i - X_i \\beta)\n\\]\\(\\(1,N)\\)\\(l\\) moment equation sample moment, can estimated averaging \\(N\\)\\[\n\\bar{g}(\\beta) = \\frac{1}{N} \\sum_{= 1}^N Z_i (Y_i - X_i \\beta) = \\frac{1}{N} Z'u\n\\]GMM estimate \\(\\beta\\) \\(\\bar{g}(\\hat{\\beta}_{GMM}) = 0\\)\\(l = k\\) unique solution system equations (equivalent IV estimator)\\[\n\\hat{\\beta}_{IV} = (Z'X)^{-1}Z'Y\n\\]\\(l > k\\), set \\(k\\) instruments\\[\n\\hat{X} = Z(Z'Z)^{-1} Z' X = P_ZX\n\\]can use 2SLS estimator\\[\n\\begin{aligned}\n\\hat{\\beta}_{2SLS} &= (\\hat{X}'X)^{-1} \\hat{X}' Y \\\\\n&= (X'P_Z X)^{-1}X' P_Z Y\n\\end{aligned}\n\\]Differences 2SLS IV-GMM:2SLS method, instruments available actually needed estimation, address , matrix created includes necessary instruments, simplifies calculation.2SLS method, instruments available actually needed estimation, address , matrix created includes necessary instruments, simplifies calculation.IV-GMM method uses available instruments, applies weighting system prioritize instruments relevant. approach useful instruments necessary, can make calculation complex. IV-GMM method uses criterion function weight estimates improve accuracy.IV-GMM method uses available instruments, applies weighting system prioritize instruments relevant. approach useful instruments necessary, can make calculation complex. IV-GMM method uses criterion function weight estimates improve accuracy.short, always use IV-GMM overid problemsIn short, always use IV-GMM overid problemsGMM estimator minimizes\\[\nJ (\\hat{\\beta}_{GMM} ) = N \\bar{g}(\\hat{\\beta}_{GMM})' W \\bar{g} (\\hat{\\beta}_{GMM})\n\\]\\(W\\) symmetric weighting matrix \\(l \\times l\\)overid equation, solving set FOCs IV-GMM estimator, \\[\n\\hat{\\beta}_{GMM} = (X'ZWZ' X)^{-1} X'ZWZ'Y\n\\]identical \\(W\\) matrices. optimal \\(W = S^{-1}\\) (L. P. Hansen 1982) \\(S\\) covariance matrix moment conditions produce efficient estimator:\\[\nS = E[Z'uu'Z] = \\lim_{N \\\\infty} N^{-1}[Z' \\Omega Z]\n\\]consistent estimator \\(S\\) 2SLS residuals, feasible IV-GMM estimator can defined \\[\n\\hat{\\beta}_{FEGMM} = (X'Z \\hat{S}^{-1} Z' X)^{-1} X'Z \\hat{S}^{-1} Z'Y\n\\]cases \\(\\Omega\\) (.e., vcov error process \\(u\\)) satisfy classical assumptionsIID\\(S = \\sigma^2_u I_N\\)optimal weighting matrix proportional identity matrixThen, IV-GMM estimator standard IV (2SLS) estimator.IV-GMM, also additional test overid restrictions: GMM distance (also known Hayashi C statistic)account clustering, one can use code provided blog","code":""},{"path":"instrumental-variables.html","id":"inference-3","chapter":"30 Instrumental Variables","heading":"30.3 Inference","text":"just-identified instrument variable model, \\[\nY = \\beta X + u\n\\]\\(corr(u, Z) = 0\\) (relevant assumption) \\(corr(Z,X) \\neq 0\\) (exogenous assumption)t-ratio approach construct 95 CIs \\[\n\\hat{\\beta} \\pm 1.96 \\sqrt{\\hat{V}_N(\\hat{\\beta})}\n\\]wrong, long recognized understand “weak instruments” problem Dufour (1997)test null hypothesis \\(\\beta = \\beta_0\\) (Lee et al. 2022) \\[ \\frac{(\\hat{\\beta} - \\beta_0)^2}{\\hat{V}_N(\\hat{\\beta})} = \\hat{t}^2 = \\hat{t}^2_{AR} \\times \\frac{1}{1 - \\hat{\\rho} \\frac{\\hat{t}_{AR}}{\\hat{f}} + \\frac{\\hat{t}^2_{AR}}{\\hat{f}^2}} \\] \\(\\hat{t}_{AR}^2 \\sim \\chi^2(1)\\) (even weak instruments) (T. W. Anderson Rubin 1949)\\[\n\\hat{t}_{AR} = \\frac{\\hat{\\pi}(\\hat{\\beta} - \\beta_0)}{\\sqrt{\\hat{V}_N (\\hat{\\pi} (\\hat{\\beta} - \\beta_0))}} \\sim N(0,1)\n\\]\\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\sim N\\)\\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\sim N\\)\\(\\hat{\\pi}\\) = 1st-stage coefficient\\(\\hat{\\pi}\\) = 1st-stage coefficient\\(\\hat{\\rho} = COV(Zv, Zu)\\) = correlation 1st-stage residual estimate \\(u\\)\\(\\hat{\\rho} = COV(Zv, Zu)\\) = correlation 1st-stage residual estimate \\(u\\)Even large samples, \\(\\hat{t}^2 \\neq \\hat{t}^2_{AR}\\) right-hand term degenerate distribution. Thus, normal t critical values wouldn’t work.t-ratios match standard normal, matches proposed density Staiger Stock (1997) J. H. Stock Yogo (2005) .deviation \\(\\hat{t}^2 , \\hat{t}^2_{AR}\\) depends \\(\\pi\\) (.e., correlation instrument endogenous variable)\\(E(F)\\) (.e., strength first-stage)Magnitude \\(|\\rho|\\) (.e., degree endogeneity)Hence, can think several scenarios:Worst case: weak first stage (\\(\\pi = 0\\)) high degree endogeneity (\\(|\\rho |= 1\\)).interval \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) contain true parameter \\(\\beta\\).interval \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) contain true parameter \\(\\beta\\).5 percent significance test conditions incorrectly reject null hypothesis (\\(\\beta = \\beta_0\\)) 100% time.5 percent significance test conditions incorrectly reject null hypothesis (\\(\\beta = \\beta_0\\)) 100% time.Best case: endogeneity (\\(\\rho =0\\)) large \\(\\hat{f}\\) (strong first-stage)interval \\(\\hat{\\beta} \\pm 1.96 \\times SD\\) accurately contains \\(\\beta\\) least 95% time.Intermediate case: performance interval lies two extremes.Solutions: valid inference \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) using t-ratio (\\(\\hat{t}^2 \\approx \\hat{t}^2_{AR}\\)), can eitherAssume problem away\nAssume \\(E(F) > 142.6\\) (Lee et al. 2022) (much assumption since can observe first-stage F-stat empirically).\nAssume \\(|\\rho| < 0.565\\) Lee et al. (2022), defeats motivation use IV first place think strong endogeneity bias, ’s trying correct (circular argument).\nAssume \\(E(F) > 142.6\\) (Lee et al. 2022) (much assumption since can observe first-stage F-stat empirically).Assume \\(|\\rho| < 0.565\\) Lee et al. (2022), defeats motivation use IV first place think strong endogeneity bias, ’s trying correct (circular argument).Deal head \nAR approach (T. W. Anderson Rubin 1949)\ntF Procedure (Lee et al. 2022)\nAK approach (J. Angrist Kolesár 2023)\nAR approach (T. W. Anderson Rubin 1949)tF Procedure (Lee et al. 2022)AK approach (J. Angrist Kolesár 2023)Common Practices & Challenges:t-ratio test preferred many researchers pitfalls:\nKnown -reject (equivalently, -cover confidence intervals), especially weak instruments Dufour (1997).\nt-ratio test preferred many researchers pitfalls:Known -reject (equivalently, -cover confidence intervals), especially weak instruments Dufour (1997).address :\nfirst-stage F-statistic used indicator weak instruments.\nJ. H. Stock Yogo (2005) provided framework understand correct distortions.\naddress :first-stage F-statistic used indicator weak instruments.first-stage F-statistic used indicator weak instruments.J. H. Stock Yogo (2005) provided framework understand correct distortions.J. H. Stock Yogo (2005) provided framework understand correct distortions.Misinterpretations:Common errors application:\nUsing rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).\nMislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)\nCommon errors application:Using rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).Using rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).Mislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)Mislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)Pretesting weak instruments might exacerbate -rejection t-ratio test mentioned (. R. Hall, Rudebusch, Wilcox 1996).Pretesting weak instruments might exacerbate -rejection t-ratio test mentioned (. R. Hall, Rudebusch, Wilcox 1996).Selective model specification (.e., dropping certain specification) based F-statistics also leads significant distortions (. Andrews, Stock, Sun 2019).Selective model specification (.e., dropping certain specification) based F-statistics also leads significant distortions (. Andrews, Stock, Sun 2019).","code":""},{"path":"instrumental-variables.html","id":"ar-approach","chapter":"30 Instrumental Variables","heading":"30.3.1 AR approach","text":"Validity Anderson-Rubin Test (notated AR) (T. W. Anderson Rubin 1949):Gives accurate results even non-normal homoskedastic errors (Staiger Stock 1997).Gives accurate results even non-normal homoskedastic errors (Staiger Stock 1997).Maintains validity across diverse error structures (J. H. Stock Wright 2000).Maintains validity across diverse error structures (J. H. Stock Wright 2000).Minimizes type II error among several alternative tests, cases :\nHomoskedastic errors M. J. Moreira (2009).\nGeneralized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).\nMinimizes type II error among several alternative tests, cases :Homoskedastic errors M. J. Moreira (2009).Homoskedastic errors M. J. Moreira (2009).Generalized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).Generalized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).","code":"\nlibrary(ivDiag)\n\n# AR test (robust to weak instruments)\n# example by the package's authors\nivDiag::AR_test(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    # treatment\n    D = \"lm_pob_mesa\",\n    # instruments\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    CI = FALSE\n)\n#> $Fstat\n#>         F       df1       df2         p \n#>   50.5097    1.0000 4350.0000    0.0000\n\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$AR\n#> $Fstat\n#>         F       df1       df2         p \n#>   50.5097    1.0000 4350.0000    0.0000 \n#> \n#> $ci.print\n#> [1] \"[-1.2545, -0.7156]\"\n#> \n#> $ci\n#> [1] -1.2545169 -0.7155854\n#> \n#> $bounded\n#> [1] TRUE\nivDiag::plot_coef(g)"},{"path":"instrumental-variables.html","id":"tf-procedure","chapter":"30 Instrumental Variables","heading":"30.3.2 tF Procedure","text":"Lee et al. (2022) propose new method aligned better traditional econometric training AR, called tF procedure. incorporates 1st-stage F-stat 2SLS \\(t\\)-value. method applicable single instrumental variable (.e., just-identified model), includingRandomized trials imperfect compliance (G. W. Imbens Angrist 1994).Randomized trials imperfect compliance (G. W. Imbens Angrist 1994).Fuzzy Regression Discontinuity designs (Lee Lemieux 2010).Fuzzy Regression Discontinuity designs (Lee Lemieux 2010).Fuzzy regression kink designs (Card et al. 2015).Fuzzy regression kink designs (Card et al. 2015).See . Andrews, Stock, Sun (2019) comparison AR approach tF Procedure.tF Procedure:Adjusts t-ratio based first-stage F-statistic.Adjusts t-ratio based first-stage F-statistic.Rather fixed pretesting threshold, applies adjustment factor 2SLS standard errors.Rather fixed pretesting threshold, applies adjustment factor 2SLS standard errors.Adjustment factors provided 95% 99% confidence levels.Adjustment factors provided 95% 99% confidence levels.Advantages tF Procedure:Smooth Adjustment:\nGives usable finite confidence intervals smaller F statistic values.\n95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.\nSmooth Adjustment:Gives usable finite confidence intervals smaller F statistic values.Gives usable finite confidence intervals smaller F statistic values.95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.Clear Confidence Levels:\nlevels incorporate effects basing inference first-stage F.\nMirrors AR zero distortion procedures.\nClear Confidence Levels:levels incorporate effects basing inference first-stage F.levels incorporate effects basing inference first-stage F.Mirrors AR zero distortion procedures.Mirrors AR zero distortion procedures.Robustness:\nRobust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).\nadjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).\nRobustness:Robust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).Robust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).adjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).adjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).Comparison AR:\nSurprisingly, \\(F > 3.84\\), AR’s expected interval length infinite, tF’s finite (.e., better).\nComparison AR:Surprisingly, \\(F > 3.84\\), AR’s expected interval length infinite, tF’s finite (.e., better).Applicability:\ntF adjustment can re-evaluate published studies first-stage F-statistic available.\nOriginal data access needed.\nApplicability:tF adjustment can re-evaluate published studies first-stage F-statistic available.tF adjustment can re-evaluate published studies first-stage F-statistic available.Original data access needed.Original data access needed.Impacts Applied Research:Lee et al. (2022) examined recent single-instrument specification studies American Economic Review (AER).Lee et al. (2022) examined recent single-instrument specification studies American Economic Review (AER).Observations:\nleast 25% studied specifications, using tF increased confidence interval lengths :\n49% (5% significance level).\n136% (1% significance level).\n\nspecifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.\nConclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.\nObservations:least 25% studied specifications, using tF increased confidence interval lengths :\n49% (5% significance level).\n136% (1% significance level).\nleast 25% studied specifications, using tF increased confidence interval lengths :49% (5% significance level).49% (5% significance level).136% (1% significance level).136% (1% significance level).specifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.specifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.Conclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.Conclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.Notation\\(Y = X \\beta + W \\gamma + u\\)\\(X = Z \\pi + W \\xi + \\nu\\)\\(W\\): Additional covariates, possibly including intercept term.\\(X\\): variable interest\\(Z\\): instrumentsKey Statistics:\\(t\\)-ratio instrumental variable estimator: \\(\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\\)\\(t\\)-ratio instrumental variable estimator: \\(\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\\)\\(t\\)-ratio first-stage coefficient: \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\\)\\(t\\)-ratio first-stage coefficient: \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\\)\\(\\hat{F} = \\hat{f}^2\\)\\(\\hat{F} = \\hat{f}^2\\)\\(\\hat{\\beta}\\): Instrumental variable estimator.\\(\\hat{V}_N (\\hat{\\beta})\\): Estimated variance \\(\\hat{\\beta}\\), possibly robust deal non-iid errors.\\(\\hat{t}\\): \\(t\\)-ratio null hypothesis.\\(\\hat{f}\\): \\(t\\)-ratio null hypothesis \\(\\pi=0\\).Traditional \\(t\\) Inference:large samples, \\(\\hat{t}^2 \\^d t^2\\)Standard normal critical values \\(\\pm 1.96\\) 5% significance level testing.Distortions Inference case IV:Use standard normal can lead distorted inferences even large samples.\nDespite large samples, t-distribution might normal.\nDespite large samples, t-distribution might normal.magnitude distortion can quantified.\nJ. H. Stock Yogo (2005) provides formula Wald test statistics using 2SLS.\n\\(t^2\\) formula allows quantification inference distortions.\njust-identified case one endogenous regressor \\(t^2 = f + t_{AR} + \\rho f t_{AR}\\) (J. H. Stock Yogo 2005)\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\n\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\n\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))\n\nJ. H. Stock Yogo (2005) provides formula Wald test statistics using 2SLS.\\(t^2\\) formula allows quantification inference distortions.just-identified case one endogenous regressor \\(t^2 = f + t_{AR} + \\rho f t_{AR}\\) (J. H. Stock Yogo 2005)\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\n\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\n\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))Implications \\(t^2\\) formula:Varies rejection rates depending \\(\\rho\\) value.\n\\(\\rho \\(0,0.5]\\) (low) t-ratio rejects probability nominal \\(0.05\\) rate\n\\(\\rho = 0.8\\) (high) rejection rate can \\(0.13\\)\n\\(\\rho \\(0,0.5]\\) (low) t-ratio rejects probability nominal \\(0.05\\) rate\\(\\rho = 0.8\\) (high) rejection rate can \\(0.13\\)short, incorrect test size relying solely \\(t^2\\) (based traditional econometric understanding)correct , one canEstimate usually 2SLS standard errorsMultiply SE adjustment factor based observed first-stage \\(\\hat{F}\\) statOne can go back traditional hypothesis using either t-ratio confidence intervalsLee et al. (2022) call adjusted SE “0.05 tF SE”.","code":"\nlibrary(ivDiag)\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$tF\n#>         F        cF      Coef        SE         t    CI2.5%   CI97.5%   p-value \n#> 8598.3264    1.9600   -0.9835    0.1540   -6.3872   -1.2853   -0.6817    0.0000\n# example in fixest package\nlibrary(fixest)\nlibrary(tidyverse)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nest_iv\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\n\nres_est_iv <- est_iv$coeftable |> \n    rownames_to_column()\n\n\ncoef_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Estimate\"]\nse_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Std. Error\"]\nfstat_1st <- fitstat(est_iv, type = \"ivf1\")[[1]]$stat\n\n# To get the correct SE based on 1st-stage F-stat (This result is similar without adjustment since F is large)\n# the results are the new CIS and p.value\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = fstat_1st) |> \n    causalverse::nice_tab(5)\n#>          F   cF    Coef      SE        t  CI2.5. CI97.5. p.value\n#> 1 903.1628 1.96 0.44498 0.02209 20.14744 0.40169 0.48827       0\n\n# We can try to see a different 1st-stage F-stat and how it changes the results\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = 2) |> \n    causalverse::nice_tab(5)\n#>   F    cF    Coef      SE        t  CI2.5. CI97.5. p.value\n#> 1 2 18.66 0.44498 0.02209 20.14744 0.03285 0.85711 0.03432"},{"path":"instrumental-variables.html","id":"ak-approach","chapter":"30 Instrumental Variables","heading":"30.3.3 AK approach","text":"(J. Angrist Kolesár 2023)","code":""},{"path":"instrumental-variables.html","id":"testing-assumptions","chapter":"30 Instrumental Variables","heading":"30.4 Testing Assumptions","text":"\\[\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\\(X_1\\) exogenous variables\\(X_1\\) exogenous variables\\(X_2\\) endogenous variables\\(X_2\\) endogenous variables\\(Z\\) instrumental variables\\(Z\\) instrumental variablesIf \\(Z\\) satisfies relevance condition, means \\(Cov(Z, X_2) \\neq 0\\)important need able estimate \\(\\beta_2\\) \\[\n\\beta_2 = \\frac{Cov(Z,Y)}{Cov(Z, X_2)}\n\\]\\(Z\\) satisfies exogeneity condition, \\(E[Z\\epsilon]=0\\), can achieve \\(Z\\) direct effect \\(Y\\) except \\(X_2\\)\\(Z\\) direct effect \\(Y\\) except \\(X_2\\)presence omitted variable, \\(Z\\) uncorrelated variable.presence omitted variable, \\(Z\\) uncorrelated variable.just want know effect \\(Z\\) \\(Y\\) (reduced form) coefficient \\(Z\\) \\[\n\\rho = \\frac{Cov(Y, Z)}{Var(Z)}\n\\]effect \\(X_2\\) (exclusion restriction assumption).can also consistently estimate effect \\(Z\\) \\(X\\) (first stage) coefficient \\(X_2\\) \\[\n\\pi = \\frac{Cov(X_2, Z)}{Var(Z)}\n\\]IV estimate \\[\n\\beta_2 = \\frac{Cov(Y,Z)}{Cov(X_2, Z)} = \\frac{\\rho}{\\pi}\n\\]","code":""},{"path":"instrumental-variables.html","id":"relevance-assumption","chapter":"30 Instrumental Variables","heading":"30.4.1 Relevance Assumption","text":"Weak instruments: can explain little variation endogenous regressor\nCoefficient estimate endogenous variable inaccurate.\ncases weak instruments unavoidable, M. J. Moreira (2003) proposes conditional likelihood ratio test robust inference. test considered approximately optimal weak instrument scenarios (D. W. Andrews, Moreira, Stock 2008; D. W. Andrews Marmer 2008).\nWeak instruments: can explain little variation endogenous regressorCoefficient estimate endogenous variable inaccurate.cases weak instruments unavoidable, M. J. Moreira (2003) proposes conditional likelihood ratio test robust inference. test considered approximately optimal weak instrument scenarios (D. W. Andrews, Moreira, Stock 2008; D. W. Andrews Marmer 2008).Rule thumb:\nCompute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)\nuse linearHypothesis() see instrument coefficients.\nRule thumb:Compute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)Compute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)use linearHypothesis() see instrument coefficients.use linearHypothesis() see instrument coefficients.First-Stage F-TestIn context two-stage least squares (2SLS) setup estimating equation:\\[\nY = X \\beta + \\epsilon\n\\]\\(X\\) endogenous, typically estimate first-stage regression :\\[\nX = Z \\pi + u\n\\]𝑍Z instrument.first-stage F-test evaluates joint significance instruments first stage:\\[\nF = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/ (n - k - 1)}\n\\]:\\(SSR_r\\) sum squared residuals restricted model (instruments, just constant).\\(SSR_r\\) sum squared residuals restricted model (instruments, just constant).\\(SSR_{ur}\\) sum squared residuals unrestricted model (instruments).\\(SSR_{ur}\\) sum squared residuals unrestricted model (instruments).\\(q\\) number instruments excluded main equation.\\(q\\) number instruments excluded main equation.\\(n\\) number observations.\\(n\\) number observations.\\(k\\) number explanatory variables excluding instruments.\\(k\\) number explanatory variables excluding instruments.Cragg-Donald TestThe Cragg-Donald statistic essentially Wald statistic joint significance instruments first stage, ’s used specifically multiple endogenous regressors. ’s calculated :\\[\nCD = n \\times (R_{ur}^2 - R_r^2)\n\\]:\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(n\\) number observations.\\(n\\) number observations.one endogenous variable, Cragg-Donald test results align closely Stock Yogo. Anderson canonical correlation test, likelihood ratio test, also works similar conditions, contrasting Cragg-Donald’s Wald statistic approach. valid one endogenous variable least one instrument.Stock-Yogo Weak IV TestThe Stock-Yogo test directly compute statistic like F-test Cragg-Donald, rather uses pre-computed critical values assess strength instruments. often uses eigenvalues derived concentration matrix:\\[\nS = \\frac{1}{n} (Z' X) (X'Z)\n\\]\\(Z\\) matrix instruments \\(X\\) matrix endogenous regressors.Stock Yogo provide critical values different scenarios (bias, size distortion) given number instruments endogenous regressors, based smallest eigenvalue \\(S\\). test compares eigenvalues critical values correspond thresholds permissible bias size distortion 2SLS estimator.Critical Values Test Conditions: critical values derived Stock Yogo depend level acceptable bias, number endogenous regressors, number instruments. example, 5% maximum acceptable bias, one endogenous variable, three instruments, critical value sufficient first stage F-statistic 13.91. Note framework requires least two overidentifying degree freedom.ComparisonAll mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors independently identically distributed. assumption violated, Kleinbergen-Paap test robust violations iid assumption can applied even single endogenous variable instrument, provided model properly identified (Baum Schaffer 2021).","code":""},{"path":"instrumental-variables.html","id":"weak-instrument-tests","chapter":"30 Instrumental Variables","heading":"30.4.1.1 Weak Instrument Tests","text":"","code":""},{"path":"instrumental-variables.html","id":"cragg-donald","chapter":"30 Instrumental Variables","heading":"30.4.1.2 Cragg-Donald","text":"(Cragg Donald 1993)Similar first-stage F-statisticLarge CD statistic implies instruments strong, case . judge critical value, look Stock-Yogo","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\n\ncragg_donald(\n    # control variables\n    X = ~ 1, \n    # endogeneous variables\n    D = ~ x, \n    # instrument variables \n    Z = ~ z, \n    data = WeakInstrument\n)\n#> Cragg-Donald test for weak instruments:\n#> \n#>      Data:                        WeakInstrument \n#>      Controls:                    ~1 \n#>      Treatments:                  ~x \n#>      Instruments:                 ~z \n#> \n#>      Cragg-Donald Statistic:        4.566136 \n#>      Df:                                 198"},{"path":"instrumental-variables.html","id":"stock-yogo","chapter":"30 Instrumental Variables","heading":"30.4.1.3 Stock-Yogo","text":"J. H. Stock Yogo (2002) set critical values bias less 10% (default)\\(H_0:\\) Instruments weak\\(H_1:\\) Instruments weakThe CD statistic bigger set critical value considered strong instruments.","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\nstock_yogo_test(\n    # control variables\n    X = ~ 1,\n    # endogeneous variables\n    D = ~ x,\n    # instrument variables\n    Z = ~ z,\n    size_bias = \"bias\",\n    data = WeakInstrument\n)"},{"path":"instrumental-variables.html","id":"anderson-rubin","chapter":"30 Instrumental Variables","heading":"30.4.1.4 Anderson-Rubin","text":"","code":""},{"path":"instrumental-variables.html","id":"stock-wright","chapter":"30 Instrumental Variables","heading":"30.4.1.5 Stock-Wright","text":"","code":""},{"path":"instrumental-variables.html","id":"exogeneity-assumption","chapter":"30 Instrumental Variables","heading":"30.4.2 Exogeneity Assumption","text":"local average treatment effect (LATE) defined :\\[\n\\text{LATE} = \\frac{\\text{reduced form}}{\\text{first stage}} = \\frac{\\rho}{\\phi}\n\\]implies reduced form (\\(\\rho\\)) product first stage (\\(\\phi\\)) LATE:\\[\n\\rho = \\phi \\times \\text{LATE}\n\\]Thus, first stage (\\(\\phi\\)) 0, reduced form (\\(\\rho\\)) also 0.statistically significant reduced form estimate without corresponding first stage indicates issue, suggesting alternative channel linking instruments outcomes direct effect IV outcome.Direct Effect: direct effect 0 first stage 0, reduced form 0.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.Direct Effect: direct effect IV outcome, reduced form can significantly different 0, even first stage 0.\nviolates exogeneity assumption, IV affect outcome treatment variable.\nviolates exogeneity assumption, IV affect outcome treatment variable.test validity exogeneity assumption, can use sanity test:Identify groups effects instruments treatment variable small significantly different 0. reduced form estimate groups also 0. “-first-stage samples” provide evidence whether exogeneity assumption violated.","code":"\n# Load necessary libraries\nlibrary(shiny)\nlibrary(AER)  # for ivreg\nlibrary(ggplot2)  # for visualization\nlibrary(dplyr)  # for data manipulation\n\n# Function to simulate the dataset\nsimulate_iv_data <- function(n, beta, phi, direct_effect) {\n  Z <- rnorm(n)\n  epsilon_x <- rnorm(n)\n  epsilon_y <- rnorm(n)\n  X <- phi * Z + epsilon_x\n  Y <- beta * X + direct_effect * Z + epsilon_y\n  data <- data.frame(Y = Y, X = X, Z = Z)\n  return(data)\n}\n\n# Function to run the simulations and calculate the effects\nrun_simulation <- function(n, beta, phi, direct_effect) {\n  # Simulate the data\n  simulated_data <- simulate_iv_data(n, beta, phi, direct_effect)\n  \n  # Estimate first-stage effect (phi)\n  first_stage <- lm(X ~ Z, data = simulated_data)\n  phi <- coef(first_stage)[\"Z\"]\n  phi_ci <- confint(first_stage)[\"Z\", ]\n  \n  # Estimate reduced-form effect (rho)\n  reduced_form <- lm(Y ~ Z, data = simulated_data)\n  rho <- coef(reduced_form)[\"Z\"]\n  rho_ci <- confint(reduced_form)[\"Z\", ]\n  \n  # Estimate LATE using IV regression\n  iv_model <- ivreg(Y ~ X | Z, data = simulated_data)\n  iv_late <- coef(iv_model)[\"X\"]\n  iv_late_ci <- confint(iv_model)[\"X\", ]\n  \n  # Calculate LATE as the ratio of reduced-form and first-stage coefficients\n  calculated_late <- rho / phi\n  calculated_late_se <- sqrt(\n    (rho_ci[2] - rho)^2 / phi^2 + (rho * (phi_ci[2] - phi) / phi^2)^2\n  )\n  calculated_late_ci <- c(calculated_late - 1.96 * calculated_late_se, \n                          calculated_late + 1.96 * calculated_late_se)\n  \n  # Return a list of results\n  list(phi = phi, \n       phi_ci = phi_ci,\n       rho = rho, \n       rho_ci = rho_ci,\n       direct_effect = direct_effect,\n       direct_effect_ci = c(direct_effect, direct_effect),  # Placeholder for direct effect CI\n       iv_late = iv_late, \n       iv_late_ci = iv_late_ci,\n       calculated_late = calculated_late, \n       calculated_late_ci = calculated_late_ci,\n       true_effect = beta,\n       true_effect_ci = c(beta, beta))  # Placeholder for true effect CI\n}\n\n# Define UI for the sliders\nui <- fluidPage(\n  titlePanel(\"IV Model Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"beta\", \"True Effect of X on Y (beta):\", min = 0, max = 1.0, value = 0.5, step = 0.1),\n      sliderInput(\"phi\", \"First Stage Effect (phi):\", min = 0, max = 1.0, value = 0.7, step = 0.1),\n      sliderInput(\"direct_effect\", \"Direct Effect of Z on Y:\", min = -0.5, max = 0.5, value = 0, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"dotPlot\")\n    )\n  )\n)\n\n# Define server logic to run the simulation and generate the plot\nserver <- function(input, output) {\n  output$dotPlot <- renderPlot({\n    # Run simulation\n    results <- run_simulation(n = 1000, beta = input$beta, phi = input$phi, direct_effect = input$direct_effect)\n    \n    # Prepare data for plotting\n    plot_data <- data.frame(\n      Effect = c(\"First Stage (phi)\", \"Reduced Form (rho)\", \"Direct Effect\", \"LATE (Ratio)\", \"LATE (IV)\", \"True Effect\"),\n      Value = c(results$phi, results$rho, results$direct_effect, results$calculated_late, results$iv_late, results$true_effect),\n      CI_Lower = c(results$phi_ci[1], results$rho_ci[1], results$direct_effect_ci[1], results$calculated_late_ci[1], results$iv_late_ci[1], results$true_effect_ci[1]),\n      CI_Upper = c(results$phi_ci[2], results$rho_ci[2], results$direct_effect_ci[2], results$calculated_late_ci[2], results$iv_late_ci[2], results$true_effect_ci[2])\n    )\n    \n    # Create dot plot with confidence intervals\n    ggplot(plot_data, aes(x = Effect, y = Value)) +\n      geom_point(size = 3) +\n      geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +\n      labs(title = \"IV Model Effects\",\n           y = \"Coefficient Value\") +\n      coord_cartesian(ylim = c(-1, 1)) +  # Limits the y-axis to -1 to 1 but allows CI beyond\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"},{"path":"instrumental-variables.html","id":"overid-tests","chapter":"30 Instrumental Variables","heading":"30.4.2.1 Overid Tests","text":"Wald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenous\nPeople might prefer Wald test Hausman test.\nWald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenousPeople might prefer Wald test Hausman test.Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)tests can provide evidence fo validity -identifying restrictions sufficient necessary validity moment conditions (.e., assumption tested). (Deaton 2010; Parente Silva 2012)-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)overid tests hold value/info?Overidentifying restrictions valid irrespective instruments’ validity\nWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)\nOveridentifying restrictions valid irrespective instruments’ validityWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)Overidentifying restriction invalid instrument valid\neffect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)\nOveridentifying restriction invalid instrument validWhen effect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)tests (overidentifying restrictions) used check whether different instruments identify parameters interest, check validity(J. . Hausman 1983; Parente Silva 2012)","code":""},{"path":"instrumental-variables.html","id":"wald-test-1","chapter":"30 Instrumental Variables","heading":"30.4.2.1.1 Wald Test","text":"Assuming \\(Z\\) exogenous (valid instrument), want know whether \\(X_2\\) exogenous1st stage:\\[\nX_2 = \\hat{\\alpha} Z + \\hat{\\epsilon}\n\\]2nd stage:\\[\nY = \\delta_0 X_1 + \\delta_1 X_2 + \\delta_2 \\hat{\\epsilon} + u\n\\]\\(\\hat{\\epsilon}\\) residuals 1st stageThe Wald test exogeneity assumes\\[\nH_0: \\delta_2 = 0 \\\\\nH_1: \\delta_2 \\neq 0\n\\]one endogenous variable one instrument, \\(\\delta_2\\) vector residuals first-stage equations. null hypothesis jointly equal 0.reject hypothesis, means \\(X_2\\) endogenous. Hence, test, want reject null hypothesis.test sacrificially significant, might just don’t enough information reject null.valid instrument \\(Z\\), whether \\(X_2\\) endogenous exogenous, coefficient estimates \\(X_2\\) still consistent. \\(X_2\\) exogenous, 2SLS inefficient (.e., larger standard errors).Intuition:\\(\\hat{\\epsilon}\\) supposed endogenous part \\(X_2\\), regress \\(Y\\) \\(\\hat{\\epsilon}\\) observe coefficient different 0. means exogenous part \\(X_2\\) can explain well impact \\(Y\\), endogenous part.","code":""},{"path":"instrumental-variables.html","id":"hausmans-test","chapter":"30 Instrumental Variables","heading":"30.4.2.1.2 Hausman’s Test","text":"Similar Wald Test identical Wald Test homoskedasticity (.e., homogeneity variances). assumption, ’s used less often Wald Test","code":""},{"path":"instrumental-variables.html","id":"hansens-j","chapter":"30 Instrumental Variables","heading":"30.4.2.1.3 Hansen’s J","text":"(L. P. Hansen 1982)(L. P. Hansen 1982)J-test (-identifying restrictions test): test whether additional instruments exogenous\nCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\nAssume least one instrument within \\(Z\\) exogenous\nJ-test (-identifying restrictions test): test whether additional instruments exogenousCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\\(dim(Z) > dim(X_2)\\)Assume least one instrument within \\(Z\\) exogenousProcedure IV-GMM:Obtain residuals 2SLS estimationRegress residuals instruments exogenous variables.Test joint hypothesis coefficients residuals across instruments 0 (.e., true instruments exogenous).\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).\nexogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).Compute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.reject hypothesis, can \nfirst sets instruments invalid\nsecond sets instruments invalid\nsets instruments invalid\nfirst sets instruments invalidThe first sets instruments invalidThe second sets instruments invalidThe second sets instruments invalidBoth sets instruments invalidBoth sets instruments invalidNote: test true residuals homoskedastic.heteroskedasticity-robust \\(J\\)-statistic, see (Carrasco Doukali 2022; H. Li et al. 2022)","code":""},{"path":"instrumental-variables.html","id":"sargan-test","chapter":"30 Instrumental Variables","heading":"30.4.2.1.4 Sargan Test","text":"(Sargan 1958)Similar Hansen’s J, assumes homoskedasticityHave careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).one heteroskedasticity design, Sargan test invalid (Pitt 2011})one heteroskedasticity design, Sargan test invalid (Pitt 2011})","code":""},{"path":"instrumental-variables.html","id":"negative-r2","chapter":"30 Instrumental Variables","heading":"30.5 Negative \\(R^2\\)","text":"’s okay negative \\(R^2\\) 2nd stage. care consistent coefficient estimates.\\(R^2\\) statistical meaning instrumental variable regression 2 3SLS\\[\nR^2 = \\frac{MSS}{TSS}\n\\]whereMSS = model sum squares (TSS- RSS)TSS = total sum squares (\\(\\sum(y - \\bar{y})^2\\))RSS = residual sum squares (\\(\\sum (y - Xb)^2\\))\\(TSS > RSS\\), negative RSS negative \\(R^2\\). Since predicted values endogenous variables different endogenous variables , error used calculate RSS can different error second stage, RSS second stage can less TSS. information, see .","code":""},{"path":"instrumental-variables.html","id":"treatment-intensity","chapter":"30 Instrumental Variables","heading":"30.6 Treatment Intensity","text":"Two-Stage Least Squares (TSLS) can used estimate average causal effect variable treatment intensity, “identifies weighted average per-unit treatment effects along length causal response function” (J. D. Angrist Imbens 1995, 431). exampleDrug dosageDrug dosageHours exam prep score (Powers Swinton 1984)Hours exam prep score (Powers Swinton 1984)Cigarette smoking birth weights (Permutt Hebel 1989)Cigarette smoking birth weights (Permutt Hebel 1989)Years educationYears educationClass size test score (J. D. Angrist Lavy 1999)Class size test score (J. D. Angrist Lavy 1999)Sibship size earning (Lavy, Angrist, Schlosser 2006)Sibship size earning (Lavy, Angrist, Schlosser 2006)Social Media AdoptionSocial Media AdoptionThe average causal effect refers conditional expectation difference outcomes treated happened counterfactual world.Notes:need linearity assumption relationships dependent variable, treatment intensities, instruments.ExampleIn original paper, J. D. Angrist Imbens (1995) take example schooling effect earnings quarters birth instrumental variable.additional year schooling, can increase earnings, additional year can heterogeneous (sense grade 9th grade 10th qualitatively different one can change different school).\\[\nY = \\gamma_0 + \\gamma_1 X_1 + \\rho S + \\epsilon\n\\]\\(S\\) years schooling (.e., endogenous regressor)\\(S\\) years schooling (.e., endogenous regressor)\\(\\rho\\) return year schooling\\(\\rho\\) return year schooling\\(X_1\\) matrix exogenous covariates\\(X_1\\) matrix exogenous covariatesSchooling can also related exogenous variable \\(X_1\\)\\[\nS = \\delta_0 + X_1 \\delta_1 + X_2 \\delta_2 + \\eta\n\\]\\(X_2\\) exogenous instrument\\(X_2\\) exogenous instrument\\(\\delta_2\\) coefficient instrument\\(\\delta_2\\) coefficient instrumentby using fitted value second, TSLS can give consistent estimate effect schooling earning\\[\nY = \\gamma_0 + X_1 \\gamma-1 + \\rho \\hat{S} + \\nu\n\\]give \\(\\rho\\) causal interpretation,first SUTVA (stable unit treatment value assumption), potential outcomes person different years schooling independent.\\(\\rho\\) probability limit equal weighted average \\(E[Y_j - Y_{j-1}] \\forall j\\)Even though first bullet point trivial, time don’t defend much research article, second bullet point harder one argue apply certain cases.","code":""},{"path":"instrumental-variables.html","id":"control-function","chapter":"30 Instrumental Variables","heading":"30.7 Control Function","text":"Also known two-stage residual inclusionResources:Binary outcome binary endogenous variable application (E. Tchetgen Tchetgen 2014)\nrare events: use logistic model 2nd stage\nnon-rare events: use risk ratio regression 2nd stage\nBinary outcome binary endogenous variable application (E. Tchetgen Tchetgen 2014)rare events: use logistic model 2nd stageIn rare events: use logistic model 2nd stageIn non-rare events: use risk ratio regression 2nd stageIn non-rare events: use risk ratio regression 2nd stageApplication marketing consumer choice model (Petrin Train 2010)Application marketing consumer choice model (Petrin Train 2010)NotesThis approach better suited models nonadditive errors (e.g., discrete choice models), binary endogenous model, binary response variable, etc.\\[\nY = g(X) + U \\\\\nX = \\pi(Z) + V \\\\\nE(U |Z,V) = E(U|V) \\\\\nE(V|Z) = 0\n\\]control function approach,\\[\nE(Y|Z,V) = g(X) + E(U|Z,V) \\\\\n= g(X) + E(U|V) \\\\\n= g(X) + h(V)\n\\]\\(h(V)\\) control function models endogeneityLinear parametersLinear Endogenous Variables:\ncontrol function function approach identical usual 2SLS estimator\ncontrol function function approach identical usual 2SLS estimatorNonlinear Endogenous Variables:\ncontrol function different 2SLS estimator\ncontrol function different 2SLS estimatorNonlinear parameters:\nCF function superior 2SLS estimator\nCF function superior 2SLS estimator","code":""},{"path":"instrumental-variables.html","id":"simulation","chapter":"30 Instrumental Variables","heading":"30.7.1 Simulation","text":"Linear parameter linear endogenous variableNonlinear endogenous variableNonlinear parameters","code":"\nlibrary(fixest)\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Set the seed for reproducibility\nset.seed(123)\nn = 10000\n# Generate the exogenous variable from a normal distribution\nexogenous <- rnorm(n, mean = 5, sd = 1)\n\n# Generate the omitted variable as a function of the exogenous variable\nomitted <- rnorm(n, mean = 2, sd = 1)\n\n# Generate the endogenous variable as a function of the omitted variable and the exogenous variable\nendogenous <- 5 * omitted + 2 * exogenous + rnorm(n, mean = 0, sd = 1)\n\n# nonlinear endogenous variable\nendogenous_nonlinear <- 5 * omitted^2 + 2 * exogenous + rnorm(100, mean = 0, sd = 1)\n\nunrelated <- rexp(n, rate = 1)\n\n# Generate the response variable as a function of the endogenous variable and the omitted variable\nresponse <- 4 +  3 * endogenous + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear <- 4 +  3 * endogenous_nonlinear + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear_para <- 4 +  3 * endogenous ^ 2 + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\n\n# Combine the variables into a data frame\nmy_data <-\n    data.frame(\n        exogenous,\n        omitted,\n        endogenous,\n        response,\n        unrelated,\n        response,\n        response_nonlinear,\n        response_nonlinear_para\n    )\n\n# View the first few rows of the data frame\n# head(my_data)\n\nwo_omitted <- feols(response ~ endogenous + sw0(unrelated), data = my_data)\nw_omitted  <- feols(response ~ endogenous + omitted + unrelated, data = my_data)\n\n\n# ivreg::ivreg(response ~ endogenous + unrelated | exogenous, data = my_data)\niv <- feols(response ~ 1 + sw0(unrelated) | endogenous ~ exogenous, data = my_data)\n\netable(\n    wo_omitted,\n    w_omitted,\n    iv, \n    digits = 2\n    # vcov = list(\"each\", \"iid\", \"hetero\")\n)\n#>                   wo_omitted.1   wo_omitted.2      w_omitted           iv.1\n#> Dependent Var.:       response       response       response       response\n#>                                                                            \n#> Constant        -3.9*** (0.10) -4.0*** (0.10)  4.0*** (0.05) 15.7*** (0.59)\n#> endogenous      4.0*** (0.005) 4.0*** (0.005) 3.0*** (0.004)  3.0*** (0.03)\n#> unrelated                         0.03 (0.03)  0.002 (0.010)               \n#> omitted                                        6.0*** (0.02)               \n#> _______________ ______________ ______________ ______________ ______________\n#> S.E. type                  IID            IID            IID            IID\n#> Observations            10,000         10,000         10,000         10,000\n#> R2                     0.98566        0.98567        0.99803        0.92608\n#> Adj. R2                0.98566        0.98566        0.99803        0.92607\n#> \n#>                           iv.2\n#> Dependent Var.:       response\n#>                               \n#> Constant        15.6*** (0.59)\n#> endogenous       3.0*** (0.03)\n#> unrelated         0.10. (0.06)\n#> omitted                       \n#> _______________ ______________\n#> S.E. type                  IID\n#> Observations            10,000\n#> R2                     0.92610\n#> Adj. R2                0.92608\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# manual\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -77.683 -14.374  -0.107  14.289  78.274 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     15.6743     2.0819   7.529 5.57e-14 ***\n#> new_endogenous   3.0142     0.1039  29.025  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 21.26 on 9998 degrees of freedom\n#> Multiple R-squared:  0.07771,    Adjusted R-squared:  0.07762 \n#> F-statistic: 842.4 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response ~ endogenous + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response ~ endogenous + residual, data = new_data_cf)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.360 -1.016  0.003  1.023  5.201 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 15.674265   0.149350   105.0   <2e-16 ***\n#> endogenous   3.014202   0.007450   404.6   <2e-16 ***\n#> residual     1.140920   0.008027   142.1   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.525 on 9997 degrees of freedom\n#> Multiple R-squared:  0.9953, Adjusted R-squared:  0.9953 \n#> F-statistic: 1.048e+06 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous_nonlinear ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous_nonlinear = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -94.43 -52.10 -15.29  36.50 446.08 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               15.3390    11.8175   1.298    0.194    \n#> new_endogenous_nonlinear   3.0174     0.3376   8.938   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 69.51 on 9998 degrees of freedom\n#> Multiple R-squared:  0.007927,   Adjusted R-squared:  0.007828 \n#> F-statistic: 79.89 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ endogenous_nonlinear + residual, \n#>     data = new_data_cf)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -17.5437  -0.8348   0.4614   1.4424   4.8154 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          15.33904    0.38459   39.88   <2e-16 ***\n#> endogenous_nonlinear  3.01737    0.01099  274.64   <2e-16 ***\n#> residual              0.24919    0.01104   22.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.262 on 9997 degrees of freedom\n#> Multiple R-squared:  0.9989, Adjusted R-squared:  0.9989 \n#> F-statistic: 4.753e+06 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear_para ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1536.5  -452.4   -80.7   368.4  3780.9 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    -1089.943     61.706  -17.66   <2e-16 ***\n#> new_endogenous   119.829      3.078   38.93   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 630.2 on 9998 degrees of freedom\n#> Multiple R-squared:  0.1316, Adjusted R-squared:  0.1316 \n#> F-statistic:  1516 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear_para ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ endogenous_nonlinear + \n#>     residual, data = new_data_cf)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -961.00 -139.32  -16.02  135.57 1403.62 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          678.1593     9.9177   68.38   <2e-16 ***\n#> endogenous_nonlinear  17.7884     0.2759   64.46   <2e-16 ***\n#> residual              52.5016     1.1552   45.45   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 231.9 on 9997 degrees of freedom\n#> Multiple R-squared:  0.8824, Adjusted R-squared:  0.8824 \n#> F-statistic: 3.751e+04 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))"},{"path":"instrumental-variables.html","id":"new-advances","chapter":"30 Instrumental Variables","heading":"30.8 New Advances","text":"Combine ML IV (Singh, Hosanagar, Gandhi 2020)","code":""},{"path":"matching-methods.html","id":"matching-methods","chapter":"31 Matching Methods","heading":"31 Matching Methods","text":"Matching process aims close back doors - potential sources bias - constructing comparison groups similar according set matching variables. helps ensure observed differences outcomes treatment comparison groups can confidently attributed treatment , rather factors may differ groups.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.Matching useful, general solution causal problems (J. . Smith Todd 2005)Assumption: Observables can identify selection treatment control groupsIdentification: exclusion restriction can met conditional observablesMotivationEffect college quality earningsThey ultimately estimate treatment effect treated attending top (high ACT) versus bottom (low ACT) quartile collegeExampleAaronson, Barrow, Sander (2007)teachers qualifications (causally) affect student test scores?Step 1:\\[\nY_{ijt} = \\delta_0 + Y_{ij(t-1)} \\delta_1 + X_{} \\delta_2 + Z_{jt} \\delta_3 + \\epsilon_{ijt}\n\\]can always another variableAny observable sorting imperfectStep 2:\\[\nY_{ijst} = \\alpha_0 + Y_{ij(t-1)}\\alpha_1 + X_{} \\alpha_2 + Z_{jt} \\alpha_3 + \\gamma_s + u_{isjt}\n\\]\\(\\delta_3 >0\\)\\(\\delta_3 >0\\)\\(\\delta_3 > \\alpha_3\\)\\(\\delta_3 > \\alpha_3\\)\\(\\gamma_s\\) = school fixed effect\\(\\gamma_s\\) = school fixed effectSorting less within school. Hence, can introduce school fixed effectStep 3:Find schools look like putting students class randomly (good random) + run step 2\\[\n\\begin{aligned}\nY_{isjt} = Y_{isj(t-1)} \\lambda &+ X_{} \\alpha_1 +Z_{jt} \\alpha_{21} \\\\\n&+ (Z_{jt} \\times D_i)\\alpha_{22}+ \\gamma_5 + u_{isjt}\n\\end{aligned}\n\\]\\(D_{}\\) element \\(X_{}\\)\\(D_{}\\) element \\(X_{}\\)\\(Z_{}\\) = teacher experience\\(Z_{}\\) = teacher experience\\[\nD_{}=\n\\begin{cases}\n1 & \\text{ high poverty} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\\(H_0:\\) \\(\\alpha_{22} = 0\\) test effect heterogeneity whether effect teacher experience (\\(Z_{jt}\\)) differentFor low poverty \\(\\alpha_{21}\\)low poverty \\(\\alpha_{21}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)Matching selection observables works good observables.Sufficient identification assumption Selection observable/ back-door criterion (based Bernard Koch’s presentation)Strong conditional ignorability\n\\(Y(0),Y(1) \\perp T|X\\)\nhidden confounders\nStrong conditional ignorability\\(Y(0),Y(1) \\perp T|X\\)\\(Y(0),Y(1) \\perp T|X\\)hidden confoundersNo hidden confoundersOverlap\n\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\ntreatments non-zero probability observed\nOverlap\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)treatments non-zero probability observedAll treatments non-zero probability observedSUTVA/ Consistency\nTreatment outcomes different subjects independent\nSUTVA/ ConsistencyTreatment outcomes different subjects independentRelative OLSMatching makes common support explicit (changes default “ignore” “enforce”)Relaxes linear function form. Thus, less parametric.also helps high ratio controls treatments.detail summary (Stuart 2010)Matching defined “method aims equate (”balance”) distribution covariates treated control groups.” (Stuart 2010, 1)Equivalently, matching selection observables identifications strategy.think OLS estimate biased, matching estimate (almost surely) .Unconditionally, consider\\[\n\\begin{aligned}\nE(Y_i^T | T) - E(Y_i^C |C) &+ E(Y_i^C | T) - E(Y_i^C | T) \\\\\n= E(Y_i^T - Y_i^C | T) &+ [E(Y_i^C | T) - E(Y_i^C |C)] \\\\\n= E(Y_i^T - Y_i^C | T) &+ \\text{selection bias}\n\\end{aligned}\n\\]\\(E(Y_i^T - Y_i^C | T)\\) causal inference want know.Randomization eliminates selection bias.don’t randomization, \\(E(Y_i^C | T) \\neq E(Y_i^C |C)\\)Matching tries selection observables \\(E(Y_i^C | X, T) = E(Y_i^C|X, C)\\)Propensity Scores basically \\(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\\)Matching standard errors exceed OLS standard errorsThe treatment larger predictive power control use treatment pick control (control pick treatment).average treatment effect (ATE) \\[\n\\frac{1}{N_T} \\sum_{=1}^{N_T} (Y_i^T - \\frac{1}{N_{C_T}} \\sum_{=1}^{N_{C_T}} Y_i^C)\n\\]Since closed-form solution standard error average treatment effect, use bootstrapping get standard error.Professor Gary King advocates instead using word “matching”, use “pruning” (.e., deleting observations). preprocessing step prunes nonmatches make control variables less important analysis.Without MatchingImbalance data leads model dependence lead lot researcher discretion leads biasWith MatchingWe balance data essentially erase human discretionTable @ref(tab:Gary King - International Methods Colloquium talk 2015)Fully blocked superior onimbalanceimbalancemodel dependencemodel dependencepowerpowerefficiencyefficiencybiasbiasresearch costsresearch costsrobustnessrobustnessMatching used whenOutcomes available select subjects follow-upOutcomes available select subjects follow-upOutcomes available improve precision estimate (.e., reduce bias)Outcomes available improve precision estimate (.e., reduce bias)Hence, can observe one outcome unit (either treated control), can think problem missing data well. Thus, section closely related Imputation (Missing Data)observational studies, randomize treatment effect. Subjects select treatments, introduce selection bias (.e., systematic differences group differences confound effects response variable differences).Matching used toreduce model dependencereduce model dependencediagnose balance datasetdiagnose balance datasetAssumptions matching:treatment assignment independent potential outcomes given covariates\n\\(T \\perp (Y(0),Y(1))|X\\)\nknown ignorability, ignorable, hidden bias, unconfounded.\ntypically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\n\ntreatment assignment independent potential outcomes given covariates\\(T \\perp (Y(0),Y(1))|X\\)\\(T \\perp (Y(0),Y(1))|X\\)known ignorability, ignorable, hidden bias, unconfounded.known ignorability, ignorable, hidden bias, unconfounded.typically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\ntypically satisfy assumption unobserved covariates correlated observed covariates.unobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)positive probability receiving treatment X\n\\(0 < P(T=1|X)<1 \\forall X\\)\npositive probability receiving treatment X\\(0 < P(T=1|X)<1 \\forall X\\)Stable Unit Treatment value Assumption (SUTVA)\nOutcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\n\nStable Unit Treatment value Assumption (SUTVA)Outcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\nOutcomes affected treatment B.hard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.Generalization\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_c\\): control population -> \\(N_c\\): random sample control\\(P_c\\): control population -> \\(N_c\\): random sample control\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(T_j\\) = treatment assignment\\(T_j\\) = treatment assignment\\(Y_j\\) = observed outcome\\(Y_j\\) = observed outcomeAssume: \\(N_t < N_c\\)Assume: \\(N_t < N_c\\)Treatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \n\\(R_1(x) = E(Y(1)|X)\\)\n\\(R_0(x) = E(Y(0)|X)\\)\nTreatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \\(R_1(x) = E(Y(1)|X)\\)\\(R_1(x) = E(Y(1)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)Assume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)\nparallel trends assumed, average effect can estimated.\nAssume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)parallel trends assumed, average effect can estimated.Common estimands:\nAverage effect treatment treated (ATT): effects treatment group\nAverage treatment effect (ATE): effect treatment control\nCommon estimands:Average effect treatment treated (ATT): effects treatment groupAverage effect treatment treated (ATT): effects treatment groupAverage treatment effect (ATE): effect treatment controlAverage treatment effect (ATE): effect treatment controlSteps:Define “closeness”: decide distance measure used\nvariables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\n\ndistance measures: \nDefine “closeness”: decide distance measure usedWhich variables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\nvariables include:Ignorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\nIgnorability (unobserved differences treatment control)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)include variables affected treatment.include variables affected treatment.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.distance measures: belowWhich distance measures: belowMatching methods\nNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\n\nSubclassification, Full Matching Weighting\nNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\n\nAssessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\n\nMatching methodsNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\nNearest neighbor matchingSimple (greedy) matching: performs poorly competition controls.Simple (greedy) matching: performs poorly competition controls.Optimal matching: considers global distance measureOptimal matching: considers global distance measureRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).Ratio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).Subclassification, Full Matching Weighting\nNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\nSubclassification, Full Matching WeightingNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.Subclassification: distribution multiple subclass (e.g., 5-10)Subclassification: distribution multiple subclass (e.g., 5-10)Full matching: optimal ly minimize average distances treated unit control unit within matched set.Full matching: optimal ly minimize average distances treated unit control unit within matched set.Weighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Kernel weighting (e.g., economics) averages multiple units control group.Kernel weighting (e.g., economics) averages multiple units control group.Assessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\nAssessing Common Supportcommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.Assessing quality matched samples (Diagnose)\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\n\nNumerical Diagnostics\nstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\n\nGraphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\n\nAssessing quality matched samples (Diagnose)Balance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.Numerical Diagnostics\nstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\nNumerical Diagnosticsstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.standardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.standardized difference means propensity score (< 0.25) (Rubin 2001)standardized difference means propensity score (< 0.25) (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)covariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.covariate, ratio fo variance residuals orthogonal propensity score treated control groups.Note: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.Graphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\nGraphical DiagnosticsQQ plotsQQ plotsEmpirical Distribution PlotEmpirical Distribution PlotEstimate treatment effect\nk:1\nNeed account weights use matching replacement.\n\nSubclassification Full Matching\nWeighting subclass estimates number treated units subclass ATT\nWeighting overall number individual subclass ATE.\n\nVariance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)\nEstimate treatment effectAfter k:1\nNeed account weights use matching replacement.\nk:1Need account weights use matching replacement.Subclassification Full Matching\nWeighting subclass estimates number treated units subclass ATT\nWeighting overall number individual subclass ATE.\nSubclassification Full MatchingWeighting subclass estimates number treated units subclass ATTWeighting subclass estimates number treated units subclass ATTWeighting overall number individual subclass ATE.Weighting overall number individual subclass ATE.Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Notes:missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)Violation ignorable treatment assignment (.e., unobservables affect treatment outcome). control \nmeasure pre-treatment measure outcome variable\nfind difference outcomes multiple control groups. significant difference, evidence violation.\nfind range correlations unobservables treatment assignment outcome nullify significant effect.\nViolation ignorable treatment assignment (.e., unobservables affect treatment outcome). control bymeasure pre-treatment measure outcome variablemeasure pre-treatment measure outcome variablefind difference outcomes multiple control groups. significant difference, evidence violation.find difference outcomes multiple control groups. significant difference, evidence violation.find range correlations unobservables treatment assignment outcome nullify significant effect.find range correlations unobservables treatment assignment outcome nullify significant effect.Choosing methods\nsmallest standardized difference mean across largest number covariates\nminimize standardized difference means particularly prognostic covariates\nfest number large standardized difference means (> 0.25)\n(Diamond Sekhon 2013) automates process\nChoosing methodssmallest standardized difference mean across largest number covariatessmallest standardized difference mean across largest number covariatesminimize standardized difference means particularly prognostic covariatesminimize standardized difference means particularly prognostic covariatesfest number large standardized difference means (> 0.25)fest number large standardized difference means (> 0.25)(Diamond Sekhon 2013) automates process(Diamond Sekhon 2013) automates processIn practice\nATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT instead\nATT, ask controls across full range treated group\npracticeIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATT, ask controls across full range treated groupIf ATT, ask controls across full range treated groupChoose matching method\nATE, use IPTW full matching\nATT, controls treated (least 3 times), k:1 nearest neighbor without replacement\nATT, controls , use subclassification, full matching, weighting odds\nChoose matching methodIf ATE, use IPTW full matchingIf ATE, use IPTW full matchingIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls , use subclassification, full matching, weighting oddsIf ATT, controls , use subclassification, full matching, weighting oddsDiagnostic\nbalance, use regression matched samples\nimbalance covariates, treat Mahalanobis\nimbalance many covariates, try k:1 matching replacement\nDiagnosticIf balance, use regression matched samplesIf balance, use regression matched samplesIf imbalance covariates, treat MahalanobisIf imbalance covariates, treat MahalanobisIf imbalance many covariates, try k:1 matching replacementIf imbalance many covariates, try k:1 matching replacementWays define distance \\(D_{ij}\\)Exact\\[\nD_{ij} =\n\\begin{cases}\n0, \\text{ } X_i = X_j, \\\\\n\\infty, \\text{ } X_i \\neq X_j\n\\end{cases}\n\\]advanced Coarsened Exact MatchingMahalanobis\\[\nD_{ij} = (X_i - X_j)'\\Sigma^{-1} (X_i - X_j)\n\\]\\(\\Sigma\\) = variance covariance matrix X thecontrol group ATT interestedcontrol group ATT interestedpolled treatment control groups ATE interestedpolled treatment control groups ATE interestedPropensity score:\\[\nD_{ij} = |e_i - e_j|\n\\]\\(e_k\\) = propensity score individual kAn advanced Prognosis score (B. B. Hansen 2008), know (.e., specify) relationship covariates outcome.Linear propensity score\\[\nD_{ij} = |logit(e_i) - logit(e_j)|\n\\]exact Mahalanobis good high dimensional non normally distributed X’s cases.can combine Mahalanobis matching propensity score calipers (Rubin Thomas 2000)advanced methods longitudinal settingsmarginal structural models (Robins, Hernan, Brumback 2000)marginal structural models (Robins, Hernan, Brumback 2000)balanced risk set matching (Y. P. Li, Propert, Rosenbaum 2001)balanced risk set matching (Y. P. Li, Propert, Rosenbaum 2001)matching methods based (ex-post)propensity scorepropensity scoredistance metricdistance metriccovariatescovariatesPackagescem Coarsened exact matchingcem Coarsened exact matchingMatching Multivariate propensity score matching balance optimizationMatching Multivariate propensity score matching balance optimizationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2017)MatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2017)optmatchoptimal matching variable ratio, optimal full matchingoptmatchoptimal matching variable ratio, optimal full matchingPSAgraphics Propensity score graphicsPSAgraphics Propensity score graphicsrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptionrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptiontwang weighting analysis non-equivalent groupstwang weighting analysis non-equivalent groupsCBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.CBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.PanelMatch based Imai, Kim, Wang (2018)PanelMatch based Imai, Kim, Wang (2018)Easier asses whether ’s workingEasier explainallows nice visualization evaluationHowever, problem omitted variables (.e., affect outcome whether observation treated) - unobserved confounders still present matching methods.Difference matching regression following Pischke’s lectureSuppose want estimate effect treatment treated\\[\n\\begin{aligned}\n\\delta_{TOT} &= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\\\\n&= E\\{E[Y_{1i} | X_i, D_i = 1] \\\\\n& - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\\} && \\text{law itereated expectations}\n\\end{aligned}\n\\]conditional independence\\[\nE[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1]\n\\]\\[\n\\begin{aligned}\n\\delta_{TOT} &= E \\{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\\} \\\\\n&= E\\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\\} \\\\\n&= E[\\delta_X |D_i = 1]\n\\end{aligned}\n\\]\\(\\delta_X\\) X-specific difference means covariate value \\(X_i\\)\\(X_i\\) discrete, matching estimand \\[\n\\delta_M = \\sum_x \\delta_x P(X_i = x |D_i = 1)\n\\]\\(P(X_i = x |D_i = 1)\\) probability mass function \\(X_i\\) given \\(D_i = 1\\)According Bayes rule,\\[\nP(X_i = x | D_i = 1) = \\frac{P(D_i = 1 | X_i = x) \\times P(X_i = x)}{P(D_i = 1)}\n\\]hence,\\[\n\\begin{aligned}\n\\delta_M &= \\frac{\\sum_x \\delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}\n\\end{aligned}\n\\]hand, suppose regression\\[\ny_i = \\sum_x d_{ix} \\beta_x + \\delta_R D_i + \\epsilon_i\n\\]\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\delta_R\\) = regression estimand \\(\\delta_R\\) = regression estimand \\[\n\\begin{aligned}\n\\delta_R &= \\frac{\\sum_x \\delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}\n\\end{aligned}\n\\]difference regression matching estimand weights use combine covariate specific treatment effect \\(\\delta_x\\)\\(P(D_i = 1|X_i = x)\\)fraction treated observations covariate cell (.e., mean \\(D_i\\))\\(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\\)variance \\(D_i\\) covariate cellThe goal matching produce covariate balance (.e., distributions covariates treatment control groups approximately similar successful randomized experiment).","code":""},{"path":"matching-methods.html","id":"selection-on-observables","chapter":"31 Matching Methods","heading":"31.1 Selection on Observables","text":"","code":""},{"path":"matching-methods.html","id":"matchit","chapter":"31 Matching Methods","heading":"31.1.1 MatchIt","text":"Procedure typically involves (proposed Noah Freifer using MatchIt)planningmatchingchecking (balance)estimating treatment effectexamine treat re78Planningselect type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select target populationselect target populationselect variables match/balance (Austin 2011) (T. J. VanderWeele 2019)select variables match/balance (Austin 2011) (T. J. VanderWeele 2019)Check Initial ImbalanceMatchingCheck balanceSometimes make trade-balance sample size.Try Full Match (.e., every treated matches one control, every control one treated).Checking balance againExact MatchingSubclassficationOptimal MatchingGenetic MatchingEstimating Treatment Effecttreat coefficient = estimated ATTWhen reporting, remember mentionthe matching specification (method, additional options)distance measure (e.g., propensity score)methods, rationale final chosen method.balance statistics matched dataset.number matched, unmatched, discardedestimation method treatment effect.","code":"\nlibrary(MatchIt)\ndata(\"lalonde\")\n# No matching; constructing a pre-match matchit object\nm.out0 <- matchit(\n    formula(treat ~ age + educ + race \n            + married + nodegree + re74 + re75, env = lalonde),\n    data = data.frame(lalonde),\n    method = NULL,\n    # assess balance before matching\n    distance = \"glm\" # logistic regression\n)\n\n# Checking balance prior to matching\nsummary(m.out0)\n# 1:1 NN PS matching w/o replacement\nm.out1 <- matchit(treat ~ age + educ,\n                  data = lalonde,\n                  method = \"nearest\",\n                  distance = \"glm\")\nm.out1\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# Checking balance after NN matching\nsummary(m.out1, un = FALSE)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ, data = lalonde, method = \"nearest\", \n#>     distance = \"glm\")\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3080        0.3077          0.0094     0.9963    0.0033\n#> age            25.8162       25.8649         -0.0068     1.0300    0.0050\n#> educ           10.3459       10.2865          0.0296     0.5886    0.0253\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0432          0.0146\n#> age        0.0162          0.0597\n#> educ       0.1189          0.8146\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           429     185\n#> Matched       185     185\n#> Unmatched     244       0\n#> Discarded       0       0\n\n# examine visually\nplot(m.out1, type = \"jitter\", interactive = FALSE)\n\nplot(\n    m.out1,\n    type = \"qq\",\n    interactive = FALSE,\n    which.xs = c(\"age\")\n)\n# Full matching on a probit PS\nm.out2 <- matchit(treat ~ age + educ, \n                  data = lalonde,\n                  method = \"full\", \n                  distance = \"glm\", \n                  link = \"probit\")\nm.out2\n#> A matchit object\n#>  - method: Optimal full matching\n#>  - distance: Propensity score\n#>              - estimated with probit regression\n#>  - number of obs.: 614 (original), 614 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# Checking balance after full matching\nsummary(m.out2, un = FALSE)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ, data = lalonde, method = \"full\", \n#>     distance = \"glm\", link = \"probit\")\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3082        0.3081          0.0023     0.9815    0.0028\n#> age            25.8162       25.8035          0.0018     0.9825    0.0062\n#> educ           10.3459       10.2315          0.0569     0.4390    0.0481\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0270          0.0382\n#> age        0.0249          0.1110\n#> educ       0.1300          0.9805\n#> \n#> Sample Sizes:\n#>               Control Treated\n#> All            429.       185\n#> Matched (ESS)  145.23     185\n#> Matched        429.       185\n#> Unmatched        0.         0\n#> Discarded        0.         0\n\nplot(summary(m.out2))\n# Full matching on a probit PS\nm.out3 <-\n    matchit(\n        treat ~ age + educ,\n        data = lalonde,\n        method = \"exact\"\n    )\nm.out3\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 614 (original), 332 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out4 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"subclass\"\n)\nm.out4\n#> A matchit object\n#>  - method: Subclassification (6 subclasses)\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 614 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n\n# Or you can use in conjunction with \"nearest\"\nm.out4 <- matchit(\n    treat ~ age + educ,\n    data = lalonde,\n    method = \"nearest\",\n    option = \"subclass\"\n)\nm.out4\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out5 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"optimal\",\n    ratio = 2\n)\nm.out5\n#> A matchit object\n#>  - method: 2:1 optimal pair matching\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 555 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out6 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"genetic\"\n)\nm.out6\n#> A matchit object\n#>  - method: 1:1 genetic matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# get matched data\nm.data1 <- match.data(m.out1)\n\nhead(m.data1)\n#>      treat age educ   race married nodegree re74 re75       re78  distance\n#> NSW1     1  37   11  black       1        1    0    0  9930.0460 0.2536942\n#> NSW2     1  22    9 hispan       0        1    0    0  3595.8940 0.3245468\n#> NSW3     1  30   12  black       0        0    0    0 24909.4500 0.2881139\n#> NSW4     1  27   11  black       0        1    0    0  7506.1460 0.3016672\n#> NSW5     1  33    8  black       0        1    0    0   289.7899 0.2683025\n#> NSW6     1  22    9  black       0        1    0    0  4056.4940 0.3245468\n#>      weights subclass\n#> NSW1       1        1\n#> NSW2       1       98\n#> NSW3       1      109\n#> NSW4       1      120\n#> NSW5       1      131\n#> NSW6       1      142\nlibrary(\"lmtest\") #coeftest\nlibrary(\"sandwich\") #vcovCL\n\n# imbalance matched dataset\nfit1 <- lm(re78 ~ treat + age + educ ,\n           data = m.data1, \n           weights = weights)\n\ncoeftest(fit1, vcov. = vcovCL, cluster = ~subclass)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  -174.902   2445.013 -0.0715 0.943012   \n#> treat       -1139.085    780.399 -1.4596 0.145253   \n#> age           153.133     55.317  2.7683 0.005922 **\n#> educ          358.577    163.860  2.1883 0.029278 * \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# balance matched dataset \nm.data2 <- match.data(m.out2)\n\nfit2 <- lm(re78 ~ treat + age + educ , \n           data = m.data2, weights = weights)\n\ncoeftest(fit2, vcov. = vcovCL, cluster = ~subclass)\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept) 2151.952   3141.152  0.6851  0.49355  \n#> treat       -725.184    703.297 -1.0311  0.30289  \n#> age          120.260     53.933  2.2298  0.02612 *\n#> educ         175.693    241.694  0.7269  0.46755  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"matching-methods.html","id":"designmatch","chapter":"31 Matching Methods","heading":"31.1.2 designmatch","text":"package includesdistmatch optimal distance matchingdistmatch optimal distance matchingbmatch optimal bipartile matchingbmatch optimal bipartile matchingcardmatch optimal cardinality matchingcardmatch optimal cardinality matchingprofmatch optimal profile matchingprofmatch optimal profile matchingnmatch optimal nonbipartile matchingnmatch optimal nonbipartile matching","code":"\nlibrary(designmatch)"},{"path":"matching-methods.html","id":"matchingfrontier","chapter":"31 Matching Methods","heading":"31.1.3 MatchingFrontier","text":"mentioned MatchIt, make trade-(also known bias-variance trade-) balance sample size. automated procedure optimize trade-implemented MatchingFrontier (G. King, Lucas, Nielsen 2017), solves joint optimization problem.Following MatchingFrontier guide","code":"\n# library(devtools)\n# install_github('ChristopherLucas/MatchingFrontier')\nlibrary(MatchingFrontier)\ndata(\"lalonde\")\n# choose var to match on\nmatch.on <-\n    colnames(lalonde)[!(colnames(lalonde) %in% c('re78', 'treat'))]\nmatch.on\n\n# Mahanlanobis frontier (default)\nmahal.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = \"treat\",\n        match.on = match.on\n    )\nmahal.frontier\n\n# L1 frontier\nL1.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = 'treat',\n        match.on = match.on,\n        QOI = 'SATT',\n        metric = 'L1',\n        ratio = 'fixed'\n    )\nL1.frontier\n\n# estimate effects along the frontier\n\n# Set base form\nmy.form <-\n    as.formula(re78 ~ treat + age + black + education \n               + hispanic + married + nodegree + re74 + re75)\n\n# Estimate effects for the mahalanobis frontier\nmahal.estimates <-\n    estimateEffects(\n        mahal.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Estimate effects for the L1 frontier\nL1.estimates <-\n    estimateEffects(\n        L1.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Plot covariates means \n# plotPrunedMeans()\n\n\n# Plot estimates (deprecated)\n# plotEstimates(\n#     L1.estimates,\n#     ylim = c(-10000, 3000),\n#     cex.lab = 1.4,\n#     cex.axis = 1.4,\n#     panel.first = grid(NULL, NULL, lwd = 2,)\n# )\n\n# Plot estimates\nplotMeans(L1.frontier)\n\n\n# parallel plot\nparallelPlot(\n    L1.frontier,\n    N = 400,\n    variables = c('age', 're74', 're75', 'black'),\n    treated.col = 'blue',\n    control.col = 'gray'\n)\n\n# export matched dataset\n# take 400 units\nmatched.data <- generateDataset(L1.frontier, N = 400) "},{"path":"matching-methods.html","id":"propensity-scores","chapter":"31 Matching Methods","heading":"31.1.4 Propensity Scores","text":"Even though mention propensity scores matching method , longer recommended use method research publication (G. King Nielsen 2019) increasesimbalanceimbalanceinefficiencyinefficiencymodel dependence: small changes model specification lead big changes model resultsmodel dependence: small changes model specification lead big changes model resultsbiasbias(Abadie Imbens 2016)noteThe initial estimation propensity score influences large sample distribution estimators.initial estimation propensity score influences large sample distribution estimators.Adjustments made large sample variances estimators ATE ATT.\nadjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.\nATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.\nAdjustments made large sample variances estimators ATE ATT.adjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.adjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.ATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.ATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.PSM tries accomplish complete randomization methods try achieve fully blocked. Hence, probably better use methods.Propensity “probability receiving treatment given observed covariates.” (Rosenbaum Rubin 1985)Equivalently, can understood probability treated.\\[\ne_i (X_i) = P(T_i = 1 | X_i)\n\\]Estimation usinglogistic regressionlogistic regressionNon parametric methods:\nboosted CART\ngeneralized boosted models (gbm)\nNon parametric methods:boosted CARTboosted CARTgeneralized boosted models (gbm)generalized boosted models (gbm)Steps Gary King’s slidesreduce k elements X scalarreduce k elements X scalar\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)match treated unit nearest control unitmatch treated unit nearest control unitcontrol units: reused; pruned unusedcontrol units: reused; pruned unusedprune matches distances > caliperprune matches distances > caliperIn best case scenario, randomly prune, increases imbalanceOther methods dominate try match exactly hence\\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)\\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)Notes:include/control irrelevant covariates leads PSM random, hence imbalanceDo include/control irrelevant covariates leads PSM random, hence imbalanceDo include (Bhattacharya Vogt 2007) instrumental variable predictor set propensity score matching estimator. generally, using variables control potential confounders, even predictive treatment, can result biased estimatesDo include (Bhattacharya Vogt 2007) instrumental variable predictor set propensity score matching estimator. generally, using variables control potential confounders, even predictive treatment, can result biased estimatesWhat left pruning important start throw .Diagnostics:balance covariatesbalance covariatesno need concern collinearityno need concern collinearitycan’t use c-stat stepwise model fit stat applycan’t use c-stat stepwise model fit stat apply","code":""},{"path":"matching-methods.html","id":"look-ahead-propensity-score-matching","chapter":"31 Matching Methods","heading":"31.1.4.1 Look Ahead Propensity Score Matching","text":"(Bapna, Ramaprasad, Umyarov 2018)","code":""},{"path":"matching-methods.html","id":"mahalanobis-distance","chapter":"31 Matching Methods","heading":"31.1.5 Mahalanobis Distance","text":"Approximates fully blocked experimentDistance \\((X_c,X_t)\\) = \\(\\sqrt{(X_c - X_t)'S^{-1}(X_c - X_t)}\\)\\(S^{-1}\\) standardize distanceIn application use Euclidean distance.Prune unused control units, prune matches distance > caliper","code":""},{"path":"matching-methods.html","id":"coarsened-exact-matching","chapter":"31 Matching Methods","heading":"31.1.6 Coarsened Exact Matching","text":"Steps Gray King’s slides International Methods Colloquium talk 2015Temporarily coarsen \\(X\\)Temporarily coarsen \\(X\\)Apply exact matching coarsened \\(X, C(X)\\)\nsort observation strata, unique values \\(C(X)\\)\nprune stratum 0 treated 0 control units\nApply exact matching coarsened \\(X, C(X)\\)sort observation strata, unique values \\(C(X)\\)sort observation strata, unique values \\(C(X)\\)prune stratum 0 treated 0 control unitsprune stratum 0 treated 0 control unitsPass original (uncoarsened) units except prunedPass original (uncoarsened) units except prunedProperties:Monotonic imbalance bounding (MIB) matching method\nmaximum imbalance treated control chosen ex ante\nMonotonic imbalance bounding (MIB) matching methodmaximum imbalance treated control chosen ex antemeets congruence principlemeets congruence principlerobust measurement errorrobust measurement errorcan implemented multiple imputationcan implemented multiple imputationworks well multi-category treatmentsworks well multi-category treatmentsAssumptions:Ignorability (.e., omitted variable bias)detail (Iacus, King, Porro 2012)Example package’s authorsautomated coarseningcoarsening explicit user choiceCan also use progressive coarsening method control number matches.Can also use progressive coarsening method control number matches.cem can also handle missingness.cem can also handle missingness.","code":"\nlibrary(cem)\ndata(LeLonde)\n\nLe <- data.frame(na.omit(LeLonde)) # remove missing data\n# treated and control groups\ntr <- which(Le$treated==1)\nct <- which(Le$treated==0)\nntr <- length(tr)\nnct <- length(ct)\n\n# unadjusted, biased difference in means\nmean(Le$re78[tr]) - mean(Le$re78[ct])\n#> [1] 759.0479\n\n# pre-treatment covariates\nvars <-\n    c(\n        \"age\",\n        \"education\",\n        \"black\",\n        \"married\",\n        \"nodegree\",\n        \"re74\",\n        \"re75\",\n        \"hispanic\",\n        \"u74\",\n        \"u75\",\n        \"q1\"\n    )\n\n# overall imbalance statistics\nimbalance(group=Le$treated, data=Le[vars]) # L1 = 0.902\n#> \n#> Multivariate Imbalance Measure: L1=0.902\n#> Percentage of local common support: LCS=5.8%\n#> \n#> Univariate Imbalance Measures:\n#> \n#>               statistic   type           L1 min 25%      50%       75%\n#> age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n#> education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n#> black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n#> married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n#> nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n#> re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n#> re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n#> hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n#> u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n#> u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n#> q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n#>                  max\n#> age          -6.0000\n#> education     1.0000\n#> black         0.0000\n#> married       0.0000\n#> nodegree      0.0000\n#> re74      -2139.0195\n#> re75        490.3945\n#> hispanic      0.0000\n#> u74           0.0000\n#> u75           0.0000\n#> q1                NA\n\n# drop other variables that are not pre - treatmentt matching variables\ntodrop <- c(\"treated\", \"re78\")\nimbalance(group=Le$treated, data=Le, drop=todrop)\n#> \n#> Multivariate Imbalance Measure: L1=0.902\n#> Percentage of local common support: LCS=5.8%\n#> \n#> Univariate Imbalance Measures:\n#> \n#>               statistic   type           L1 min 25%      50%       75%\n#> age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n#> education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n#> black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n#> married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n#> nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n#> re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n#> re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n#> hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n#> u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n#> u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n#> q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n#>                  max\n#> age          -6.0000\n#> education     1.0000\n#> black         0.0000\n#> married       0.0000\n#> nodegree      0.0000\n#> re74      -2139.0195\n#> re75        490.3945\n#> hispanic      0.0000\n#> u74           0.0000\n#> u75           0.0000\n#> q1                NA\nmat <-\n    cem(\n        treatment = \"treated\",\n        data = Le,\n        drop = \"re78\",\n        keep.all = TRUE\n    )\n#> \n#> Using 'treated'='1' as baseline group\nmat\n#>            G0  G1\n#> All       392 258\n#> Matched    95  84\n#> Unmatched 297 174\n\n# mat$w\n# categorial variables\nlevels(Le$q1) # grouping option\n#> [1] \"agree\"             \"disagree\"          \"neutral\"          \n#> [4] \"no opinion\"        \"strongly agree\"    \"strongly disagree\"\nq1.grp <-\n    list(\n        c(\"strongly agree\", \"agree\"),\n        c(\"neutral\", \"no opinion\"),\n        c(\"strongly disagree\", \"disagree\")\n    ) # if you want ordered categories\n\n# continuous variables\ntable(Le$education)\n#> \n#>   3   4   5   6   7   8   9  10  11  12  13  14  15 \n#>   1   5   4   6  12  55 106 146 173 113  19   9   1\neducut <- c(0, 6.5, 8.5, 12.5, 17)  # use cutpoints\n\nmat1 <-\n    cem(\n        treatment = \"treated\",\n        data = Le,\n        drop = \"re78\",\n        cutpoints = list(education = educut),\n        grouping = list(q1 = q1.grp)\n    )\n#> \n#> Using 'treated'='1' as baseline group\nmat1\n#>            G0  G1\n#> All       392 258\n#> Matched   158 115\n#> Unmatched 234 143"},{"path":"matching-methods.html","id":"genetic-matching","chapter":"31 Matching Methods","heading":"31.1.7 Genetic Matching","text":"GM uses iterative checking process propensity scores, combines propensity scores Mahalanobis distance.\nGenMatch (Diamond Sekhon 2013)\nGM uses iterative checking process propensity scores, combines propensity scores Mahalanobis distance.GenMatch (Diamond Sekhon 2013)GM arguably “superior” method nearest neighbor full matching imbalanced dataGM arguably “superior” method nearest neighbor full matching imbalanced dataUse genetic search algorithm find weights covariate optimal balance.Use genetic search algorithm find weights covariate optimal balance.Implementation\nuse replacement\nbalance can based \npaired \\(t\\)-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\n\nImplementationcould use replacementcould use replacementbalance can based \npaired \\(t\\)-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\nbalance can based onpaired \\(t\\)-tests (dichotomous variables)paired \\(t\\)-tests (dichotomous variables)Kolmogorov-Smirnov (multinomial continuous)Kolmogorov-Smirnov (multinomial continuous)PackagesMatching","code":"\nlibrary(Matching)\ndata(lalonde)\nattach(lalonde)\n\n#The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n\n#The covariates we want to obtain balance on\nBalanceMat <-\n    cbind(age,\n          educ,\n          black,\n          hisp,\n          married,\n          nodegr,\n          u74,\n          u75,\n          re75,\n          re74,\n          I(re74 * re75))\n\n#\n#Let's call GenMatch() to find the optimal weight to give each\n#covariate in 'X' so as we have achieved balance on the covariates in\n#'BalanceMat'. This is only an example so we want GenMatch to be quick\n#so the population size has been set to be only 16 via the 'pop.size'\n#option. This is *WAY* too small for actual problems.\n#For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf.\n#\ngenout <-\n    GenMatch(\n        Tr = treat,\n        X = X,\n        BalanceMatrix = BalanceMat,\n        estimand = \"ATE\",\n        M = 1,\n        pop.size = 16,\n        max.generations = 10,\n        wait.generations = 1\n    )\n\n#The outcome variable\nY=re78/1000\n\n#\n# Now that GenMatch() has found the optimal weights, let's estimate\n# our causal effect of interest using those weights\n#\nmout <-\n    Match(\n        Y = Y,\n        Tr = treat,\n        X = X,\n        estimand = \"ATE\",\n        Weight.matrix = genout\n    )\nsummary(mout)\n\n#                        \n#Let's determine if balance has actually been obtained on the variables of interest\n#                        \nmb <-\n    MatchBalance(\n        treat ~ age + educ + black + hisp + married + nodegr \n        + u74 + u75 + re75 + re74 + I(re74 * re75),\n        match.out = mout,\n        nboots = 500\n    )"},{"path":"matching-methods.html","id":"entropy-balancing","chapter":"31 Matching Methods","heading":"31.1.8 Entropy Balancing","text":"(Hainmueller 2012)Entropy balancing method achieving covariate balance observational studies binary treatments.Entropy balancing method achieving covariate balance observational studies binary treatments.uses maximum entropy reweighting scheme ensure treatment control groups balanced based sample moments.uses maximum entropy reweighting scheme ensure treatment control groups balanced based sample moments.method adjusts inequalities covariate distributions, reducing dependence model used estimating treatment effects.method adjusts inequalities covariate distributions, reducing dependence model used estimating treatment effects.Entropy balancing improves balance across included covariate moments removes need repetitive balance checking iterative model searching.Entropy balancing improves balance across included covariate moments removes need repetitive balance checking iterative model searching.","code":""},{"path":"matching-methods.html","id":"matching-for-high-dimensional-data","chapter":"31 Matching Methods","heading":"31.1.9 Matching for high-dimensional data","text":"One reduce number dimensions using methods :Lasso (Gordon et al. 2019)Lasso (Gordon et al. 2019)Penalized logistic regression (Eckles Bakshy 2021)Penalized logistic regression (Eckles Bakshy 2021)PCA (Principal Component Analysis)PCA (Principal Component Analysis)Locality Preserving Projections (LPP) (S. Li et al. 2016)Locality Preserving Projections (LPP) (S. Li et al. 2016)Random projectionRandom projectionAutoencoders (Ramachandra 2018)Autoencoders (Ramachandra 2018)Additionally, one jointly dimension reduction balancing distributions control treated groups (Yao et al. 2018).","code":""},{"path":"matching-methods.html","id":"matching-for-time-series-cross-section-data","chapter":"31 Matching Methods","heading":"31.1.10 Matching for time series-cross-section data","text":"Examples: (Scheve Stasavage 2012) (Acemoglu et al. 2019)Identification strategy:Within-unit -time variationWithin-unit -time variationwithin-time across-units variationwithin-time across-units variationSee treatment condition details method","code":""},{"path":"matching-methods.html","id":"matching-for-multiple-treatments","chapter":"31 Matching Methods","heading":"31.1.11 Matching for multiple treatments","text":"cases multiple treatment groups, want matching, ’s important baseline (control) group. details, see(McCaffrey et al. 2013)(McCaffrey et al. 2013)(Lopez Gutman 2017)(Lopez Gutman 2017)(Zhao et al. 2021): also continuous treatment(Zhao et al. 2021): also continuous treatmentIf insist using MatchIt package, see answer","code":""},{"path":"matching-methods.html","id":"matching-for-multi-level-treatments","chapter":"31 Matching Methods","heading":"31.1.12 Matching for multi-level treatments","text":"See (Yang et al. 2016)Package R shuyang1987/multilevelMatching Github","code":""},{"path":"matching-methods.html","id":"matching-for-repeated-treatments","chapter":"31 Matching Methods","heading":"31.1.13 Matching for repeated treatments","text":"https://cran.r-project.org/web/packages/twang/vignettes/iptw.pdfpackage R twang","code":""},{"path":"matching-methods.html","id":"selection-on-unobservables","chapter":"31 Matching Methods","heading":"31.2 Selection on Unobservables","text":"several ways one can deal selection unobservables:Rosenbaum BoundsRosenbaum BoundsEndogenous Sample Selection (.e., Heckman-style correction): examine \\(\\lambda\\) term see whether ’s significant (sign endogenous selection)Endogenous Sample Selection (.e., Heckman-style correction): examine \\(\\lambda\\) term see whether ’s significant (sign endogenous selection)Relative Correlation RestrictionsRelative Correlation RestrictionsCoefficient-stability BoundsCoefficient-stability Bounds","code":""},{"path":"matching-methods.html","id":"rosenbaum-bounds","chapter":"31 Matching Methods","heading":"31.2.1 Rosenbaum Bounds","text":"Examples marketing(Oestreicher-Singer Zalmanson 2013): range 1.5 1.8 important effect level community participation users willingness pay premium services.(Oestreicher-Singer Zalmanson 2013): range 1.5 1.8 important effect level community participation users willingness pay premium services.(M. Sun Zhu 2013): factor 1.5 essential understanding relationship launch ad revenue-sharing program popularity content.(M. Sun Zhu 2013): factor 1.5 essential understanding relationship launch ad revenue-sharing program popularity content.(Manchanda, Packard, Pattabhiramaiah 2015): factor 1.6 required social dollar effect nullified.(Manchanda, Packard, Pattabhiramaiah 2015): factor 1.6 required social dollar effect nullified.(Sudhir Talukdar 2015): factor 1.9 needed adoption impact labor productivity, 2.2 adoption affect floor productivity.(Sudhir Talukdar 2015): factor 1.9 needed adoption impact labor productivity, 2.2 adoption affect floor productivity.(Proserpio Zervas 2017b): factor 2 necessary firm’s use management responses influence online reputation.(Proserpio Zervas 2017b): factor 2 necessary firm’s use management responses influence online reputation.(S. Zhang et al. 2022): factor 1.55 critical acquisition verified images drive demand Airbnb properties.(S. Zhang et al. 2022): factor 1.55 critical acquisition verified images drive demand Airbnb properties.(Chae, Ha, Schweidel 2023): factor 27 (typo) significant paywall suspensions affect subsequent subscription decisions.(Chae, Ha, Schweidel 2023): factor 27 (typo) significant paywall suspensions affect subsequent subscription decisions.GeneralMatching Methods favored estimating treatment effects observational data, offering advantages regression methods \nreduces reliance functional form assumptions.\nAssumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.\nMatching Methods favored estimating treatment effects observational data, offering advantages regression methods becauseIt reduces reliance functional form assumptions.reduces reliance functional form assumptions.Assumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.Assumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.Concerns arise potentially relevant covariates unmeasured.\nRosenbaum Bounds assess overall sensitivity coefficient estimates hidden bias (Rosenbaum Rosenbaum 2002) without knowledge (e.g., direction) bias. unboservables cause hidden bias affect selection treatment factor \\(\\Gamma\\) predictive outcome, method also known worst case analyses (DiPrete Gangl 2004).\nConcerns arise potentially relevant covariates unmeasured.Rosenbaum Bounds assess overall sensitivity coefficient estimates hidden bias (Rosenbaum Rosenbaum 2002) without knowledge (e.g., direction) bias. unboservables cause hidden bias affect selection treatment factor \\(\\Gamma\\) predictive outcome, method also known worst case analyses (DiPrete Gangl 2004).Can’t provide precise bounds estimates treatment effects (see Relative Correlation Restrictions)Can’t provide precise bounds estimates treatment effects (see Relative Correlation Restrictions)Typically, show p-value H-L point estimate level gamma \\(\\Gamma\\)Typically, show p-value H-L point estimate level gamma \\(\\Gamma\\)random treatment assignment, can use non-parametric test (Wilcoxon signed rank test) see treatment effect.Without random treatment assignment (.e., observational data), use test. Selection Observables, can use test believe unmeasured confounders. Rosenbaum (2002) can come talk believability notion.layman’s terms, consider treatment assignment based method odds treatment unit control differ multiplier \\(\\Gamma\\)example, \\(\\Gamma = 1\\) means odds assignment identical, indicating random treatment assignment.Another example, \\(\\Gamma = 2\\), matched pair, one unit twice likely receive treatment (due unobservables).Since can’t know \\(\\Gamma\\) certainty, run sensitivity analysis see results change different values \\(\\Gamma\\)bias product unobservable influences treatment selection outcome factor \\(\\Gamma\\) (omitted variable bias)technical terms,Treatment Assignment Probability:\nConsider unit \\(j\\) probability \\(\\pi_j\\) receiving treatment, unit \\(\\) \\(\\pi_i\\).\nIdeally, matching, ’s hidden bias, ’d \\(\\pi_i = \\pi_j\\).\nHowever, observing \\(\\pi_i \\neq \\pi_j\\) raises questions potential biases affecting inference. evaluated using odds ratio.\nConsider unit \\(j\\) probability \\(\\pi_j\\) receiving treatment, unit \\(\\) \\(\\pi_i\\).Ideally, matching, ’s hidden bias, ’d \\(\\pi_i = \\pi_j\\).However, observing \\(\\pi_i \\neq \\pi_j\\) raises questions potential biases affecting inference. evaluated using odds ratio.Odds Ratio Hidden Bias:\nodds treatment unit \\(j\\) defined \\(\\frac{\\pi_j}{1 - \\pi_j}\\).\nodds ratio two matched units \\(\\) \\(j\\) constrained \\(\\frac{1}{\\Gamma} \\le \\frac{\\pi_i / (1- \\pi_i)}{\\pi_j/ (1- \\pi_j)} \\le \\Gamma\\).\n\\(\\Gamma = 1\\), implies absence hidden bias.\n\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.\n\nodds treatment unit \\(j\\) defined \\(\\frac{\\pi_j}{1 - \\pi_j}\\).odds ratio two matched units \\(\\) \\(j\\) constrained \\(\\frac{1}{\\Gamma} \\le \\frac{\\pi_i / (1- \\pi_i)}{\\pi_j/ (1- \\pi_j)} \\le \\Gamma\\).\n\\(\\Gamma = 1\\), implies absence hidden bias.\n\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.\n\\(\\Gamma = 1\\), implies absence hidden bias.\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.Sensitivity Analysis Using Gamma:\nvalue \\(\\Gamma\\) helps measure potential departure bias-free study.\nSensitivity analysis involves varying \\(\\Gamma\\) examine inferences might change presence hidden biases.\nvalue \\(\\Gamma\\) helps measure potential departure bias-free study.Sensitivity analysis involves varying \\(\\Gamma\\) examine inferences might change presence hidden biases.Incorporating Unobserved Covariates:\nConsider scenario unit \\(\\) observed covariates \\(x_i\\) unobserved covariate \\(u_i\\), affect outcome.\nlogistic regression model link odds assignment covariates: \\(\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\kappa x_i + \\gamma u_i\\), \\(\\gamma\\) represents impact unobserved covariate.\nConsider scenario unit \\(\\) observed covariates \\(x_i\\) unobserved covariate \\(u_i\\), affect outcome.logistic regression model link odds assignment covariates: \\(\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\kappa x_i + \\gamma u_i\\), \\(\\gamma\\) represents impact unobserved covariate.Steps Sensitivity Analysis (create table different levels \\(\\Gamma\\) assess magnitude biases can affect evidence treatment effect (estimate):\nSelect range values \\(\\Gamma\\) (e.g., \\(1 \\2\\)).\nAssess p-value magnitude treatment effect (Hodges Jr Lehmann 2011) (details, see (Hollander, Wolfe, Chicken 2013)) changes varying \\(\\Gamma\\) values.\nEmploy specific randomization tests based type outcome establish bounds inferences.\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)\n\nSelect range values \\(\\Gamma\\) (e.g., \\(1 \\2\\)).Assess p-value magnitude treatment effect (Hodges Jr Lehmann 2011) (details, see (Hollander, Wolfe, Chicken 2013)) changes varying \\(\\Gamma\\) values.Employ specific randomization tests based type outcome establish bounds inferences.\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)Notes:treatment assignment clustered (e.g., within school, within state) need adjust bounds clustered treatment assignment (B. B. Hansen, Rosenbaum, Small 2014) (similar clustered standard errors).Packagesrbounds (Keele 2010)rbounds (Keele 2010)sensitivitymv (Rosenbaum 2015)sensitivitymv (Rosenbaum 2015)Since typically assess estimate sensitivity unboservables matching, first matching.multiple control group matchingsensitivitymw faster sensitivitymw. sensitivitymw can match matched sets can differing numbers controls (Rosenbaum 2015).","code":"\nlibrary(MatchIt)\nlibrary(Matching)\ndata(\"lalonde\")\n\nmatched <- MatchIt::matchit(\n    treat ~ age + educ,\n    data = lalonde,\n    method = \"nearest\"\n)\nsummary(matched)\n#> \n#> Call:\n#> MatchIt::matchit(formula = treat ~ age + educ, data = lalonde, \n#>     method = \"nearest\")\n#> \n#> Summary of Balance for All Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.4203        0.4125          0.1689     1.2900    0.0431\n#> age            25.8162       25.0538          0.1066     1.0278    0.0254\n#> educ           10.3459       10.0885          0.1281     1.5513    0.0287\n#>          eCDF Max\n#> distance   0.1251\n#> age        0.0652\n#> educ       0.1265\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.4203        0.4179          0.0520     1.1691    0.0105\n#> age            25.8162       25.5081          0.0431     1.1518    0.0148\n#> educ           10.3459       10.2811          0.0323     1.5138    0.0224\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0595          0.0598\n#> age        0.0486          0.5628\n#> educ       0.0757          0.3602\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           260     185\n#> Matched       185     185\n#> Unmatched      75       0\n#> Discarded       0       0\nmatched_data <- match.data(matched)\n\ntreatment_group <- subset(matched_data, treat == 1)\ncontrol_group <- subset(matched_data, treat == 0)\n\n\nlibrary(rbounds)\n\n# p-value sensitivity \npsens_res <-\n    psens(treatment_group$re78,\n          control_group$re78,\n          Gamma = 2,\n          GammaInc = .1)\n\npsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n#>  \n#> Unconfounded estimate ....  0.0058 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0      0.0058      0.0058\n#>    1.1      0.0011      0.0235\n#>    1.2      0.0002      0.0668\n#>    1.3      0.0000      0.1458\n#>    1.4      0.0000      0.2599\n#>    1.5      0.0000      0.3967\n#>    1.6      0.0000      0.5378\n#>    1.7      0.0000      0.6664\n#>    1.8      0.0000      0.7723\n#>    1.9      0.0000      0.8523\n#>    2.0      0.0000      0.9085\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \n\n# Hodges-Lehmann point estimate sensitivity\n# median difference between treatment and control\nhlsens_res <-\n    hlsens(treatment_group$re78,\n           control_group$re78,\n           Gamma = 2,\n           GammaInc = .1)\nhlsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n#>  \n#> Unconfounded estimate ....  1745.843 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0 1745.800000      1745.8\n#>    1.1 1139.100000      1865.6\n#>    1.2  830.840000      2160.9\n#>    1.3  533.740000      2462.4\n#>    1.4  259.940000      2793.8\n#>    1.5   -0.056912      3059.3\n#>    1.6 -144.960000      3297.8\n#>    1.7 -380.560000      3535.7\n#>    1.8 -554.360000      3751.0\n#>    1.9 -716.360000      4012.1\n#>    2.0 -918.760000      4224.3\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \nlibrary(Matching)\nlibrary(MatchIt)\n\nn_ratio <- 2\nmatched <- MatchIt::matchit(treat ~ age + educ ,\n                   method = \"nearest\", ratio = n_ratio)\nsummary(matched)\nmatched_data <- match.data(matched)\n\nmcontrol_res <- rbounds::mcontrol(\n    y          = matched_data$re78,\n    grp.id     = matched_data$subclass,\n    treat.id   = matched_data$treat,\n    group.size = n_ratio + 1,\n    Gamma      = 2.5,\n    GammaInc   = .1\n)\n\nmcontrol_res\nlibrary(sensitivitymv)\ndata(lead150)\nhead(lead150)\n#>      [,1] [,2] [,3] [,4] [,5] [,6]\n#> [1,] 1.40 1.23 2.24 0.96 1.90 1.14\n#> [2,] 0.63 0.99 0.87 1.90 0.67 1.40\n#> [3,] 1.98 0.82 0.66 0.58 1.00 1.30\n#> [4,] 1.45 0.53 1.43 1.70 0.85 1.50\n#> [5,] 1.60 1.70 0.63 1.05 1.08 0.92\n#> [6,] 1.13 0.31 0.71 1.10 0.86 1.14\nsenmv(lead150,gamma=2,trim=2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524\n\nlibrary(sensitivitymw)\nsenmw(lead150,gamma=2,trim=2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524"},{"path":"matching-methods.html","id":"relative-correlation-restrictions","chapter":"31 Matching Methods","heading":"31.2.2 Relative Correlation Restrictions","text":"Examples marketing(Manchanda, Packard, Pattabhiramaiah 2015): 3.23 social dollar effect nullified(Manchanda, Packard, Pattabhiramaiah 2015): 3.23 social dollar effect nullified(Chae, Ha, Schweidel 2023): 6.69 (.e., much stronger selection unobservables compared selection observables negate result) paywall suspensions affect subsequent subscription decisions(Chae, Ha, Schweidel 2023): 6.69 (.e., much stronger selection unobservables compared selection observables negate result) paywall suspensions affect subsequent subscription decisions(M. Sun Zhu 2013)(M. Sun Zhu 2013)GeneralProposed Altonji, Elder, Taber (2005)Proposed Altonji, Elder, Taber (2005)Generalized Krauth (2016)Generalized Krauth (2016)Estimate bounds treatment effects due unobserved selection.Estimate bounds treatment effects due unobserved selection.\\[\nY_i = X_i \\beta  + C_i \\gamma + \\epsilon_i\n\\]\\(\\beta\\) effect interest\\(\\beta\\) effect interest\\(C_i\\) control variable\\(C_i\\) control variableUsing OLS, \\(cor(X_i, \\epsilon_i) = 0\\)Using OLS, \\(cor(X_i, \\epsilon_i) = 0\\)RCR analysis, assume\\[\ncor(X_i, \\epsilon_i) = \\lambda cor(X_i, C_i \\gamma)\n\\]\\(\\lambda \\(\\lambda_l, \\lambda_h)\\)Choice \\(\\lambda\\)Strong assumption omitted variable bias (smallStrong assumption omitted variable bias (smallIf \\(\\lambda = 0\\), \\(cor(X_i, \\epsilon_i) = 0\\)\\(\\lambda = 0\\), \\(cor(X_i, \\epsilon_i) = 0\\)\\(\\lambda = 1\\), \\(cor(X_i, \\epsilon_i) = cor(X_i, C_i \\gamma)\\)\\(\\lambda = 1\\), \\(cor(X_i, \\epsilon_i) = cor(X_i, C_i \\gamma)\\)typically examine \\(\\lambda \\(0, 1)\\)typically examine \\(\\lambda \\(0, 1)\\)","code":"\n# remotes::install_github(\"bvkrauth/rcr/r/rcrbounds\")\nlibrary(rcrbounds)\n# rcrbounds::install_rcrpy()\ndata(\"ChickWeight\")\n\nrcr_res <-\n    rcrbounds::rcr(weight ~ Time |\n                       Diet, ChickWeight, rc_range = c(0, 10))\nrcr_res\n#> \n#> Call:\n#> rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, \n#>     rc_range = c(0, 10))\n#> \n#> Coefficients:\n#>     rcInf effectInf       rc0   effectL   effectH \n#> 34.676505 71.989336 34.741955  7.447713  8.750492\nsummary(rcr_res)\n#> \n#> Call:\n#> rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, \n#>     rc_range = c(0, 10))\n#> \n#> Coefficients:\n#>            Estimate  Std. Error    t value      Pr(>|t|)\n#> rcInf     34.676505  50.1295005  0.6917385  4.891016e-01\n#> effectInf 71.989336 112.5711682  0.6395007  5.224973e-01\n#> rc0       34.741955  58.7169195  0.5916856  5.540611e-01\n#> effectL    7.447713   2.4276246  3.0679014  2.155677e-03\n#> effectH    8.750492   0.2607671 33.5567355 7.180405e-247\n#> ---\n#> conservative confidence interval:\n#>          2.5  %  97.5  %\n#> effect 2.689656 9.261586\n\n# hypothesis test for the coefficient\nrcrbounds::effect_test(rcr_res, h0 = 0)\n#> [1] 0.001234233\nplot(rcr_res)"},{"path":"matching-methods.html","id":"coefficient-stability-bounds","chapter":"31 Matching Methods","heading":"31.2.3 Coefficient-stability Bounds","text":"Developed Oster (2019)Assess robustness omitted variable bias observing:\nChanges coefficient interest\nShifts model \\(R^2\\)\nChanges coefficient interestChanges coefficient interestShifts model \\(R^2\\)Shifts model \\(R^2\\)Refer Masten Poirier (2022) reverse sign problem.","code":""},{"path":"interrupted-time-series.html","id":"interrupted-time-series","chapter":"32 Interrupted Time Series","heading":"32 Interrupted Time Series","text":"Regression Discontinuity TimeRegression Discontinuity TimeControl \nSeasonable trends\nConcurrent events\nControl forSeasonable trendsSeasonable trendsConcurrent eventsConcurrent eventsPros (Penfold Zhang 2013)\ncontrol long-term trends\nPros (Penfold Zhang 2013)control long-term trendsCons\nMin 8 data points 8 intervention\nMultiple events hard distinguish\nConsMin 8 data points 8 interventionMin 8 data points 8 interventionMultiple events hard distinguishMultiple events hard distinguishNotes:subgroup analysis (heterogeneity effect size), see (Harper Bruckner 2017)interpret control variables, see (Bottomley, Scott, Isham 2019)Interrupted time series used whenlongitudinal data (outcome time - observations intervention)full population affected one specific point time (can stacked based intervention)framework, can 4 possible scenarios outcome interventionNo effectsNo effectsImmediate effectImmediate effectSustained (long-term) effect (smooth)Sustained (long-term) effect (smooth)immediate sustained effectBoth immediate sustained effect\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 P + \\epsilon\n\\]\\(Y\\) outcome variable\n\\(\\beta_0\\) baseline level outcome\n\\(Y\\) outcome variable\\(\\beta_0\\) baseline level outcome\\(T\\) time variable (e.g., days, weeks, etc.) passed start observation period\n\\(\\beta_1\\) slope line intervention\n\\(T\\) time variable (e.g., days, weeks, etc.) passed start observation period\\(\\beta_1\\) slope line intervention\\(D\\) treatment variable \\(1\\) intervention \\(0\\) intervention.\n\\(\\beta_2\\) immediate effect intervention\n\\(D\\) treatment variable \\(1\\) intervention \\(0\\) intervention.\\(\\beta_2\\) immediate effect intervention\\(P\\) time variable indicating time passed since intervention (intervention, value set 0) (examine sustained effect).\n\\(\\beta_3\\) sustained effect = difference slope line prior intervention slope line subsequent intervention\n\\(P\\) time variable indicating time passed since intervention (intervention, value set 0) (examine sustained effect).\\(\\beta_3\\) sustained effect = difference slope line prior intervention slope line subsequent interventionExampleCreate fictitious dataset know true data generating process\\[\nOutcome = 10 \\times time + 20 \\times treatment + 25 \\times timesincetreatment + noise\n\\]VisualizeInterpretationTime coefficient shows -intervention outcome trend. Positive significant, indicating rising trend. Every day adds 15 points.Time coefficient shows -intervention outcome trend. Positive significant, indicating rising trend. Every day adds 15 points.treatment coefficient shows immediate increase outcome. Immediate effect positive significant, increasing outcome 20 points.treatment coefficient shows immediate increase outcome. Immediate effect positive significant, increasing outcome 20 points.time since treatment coefficient reflects change trend subsequent intervention. sustained effect positive statistically significant, showing outcome increases 25 points per day intervention.time since treatment coefficient reflects change trend subsequent intervention. sustained effect positive statistically significant, showing outcome increases 25 points per day intervention.See Lee Rodgers, Beasley, Schuelke (2014) suggestionsPlot counterfactualPossible threats validity interrupted time series analysis (Baicker Svoronos 2019)Delayed effects (Rodgers, John, Coleman 2005) (may make assess time intervention - assess immediate dates).Delayed effects (Rodgers, John, Coleman 2005) (may make assess time intervention - assess immediate dates).confounding events Linden (2017)confounding events Linden (2017)Intervention introduced later withdrawn (Linden 2015)Intervention introduced later withdrawn (Linden 2015)Autocorrelation (every time series data): might cause underestimation standard errors (.e., overestimating statistical significance treatment effect)Autocorrelation (every time series data): might cause underestimation standard errors (.e., overestimating statistical significance treatment effect)Regression mean: short-term shock outcome, individuals can revert back initial states.Regression mean: short-term shock outcome, individuals can revert back initial states.Selection bias: certain individuals affected treatment (use Multiple Groups).Selection bias: certain individuals affected treatment (use Multiple Groups).","code":"\n# number of days\nn = 365\n\n\n# intervention at day\ninterven = 200\n\n# time index from 1 to 365\ntime = c(1:n)\n\n# treatment variable: before internvation = day 1 to 200, \n# after intervention = day 201 to 365\ntreatment = c(rep(0, interven), rep(1, n - interven))\n\n# time since treatment\ntimesincetreat = c(rep(0, interven), c(1:(n - interven)))\n\n# outcome\noutcome = 10 + 15 * time + 20 * treatment + \n    25 * timesincetreat + rnorm(n, mean = 0, sd = 1)\n\ndf = data.frame(outcome, time, treatment, timesincetreat)\n\nhead(df, 10)\n#>      outcome time treatment timesincetreat\n#> 1   25.79832    1         0              0\n#> 2   42.08680    2         0              0\n#> 3   55.55952    3         0              0\n#> 4   68.54228    4         0              0\n#> 5   82.75827    5         0              0\n#> 6  100.82867    6         0              0\n#> 7  114.41550    7         0              0\n#> 8  131.06942    8         0              0\n#> 9  145.22532    9         0              0\n#> 10 161.08298   10         0              0\nplot(df$time, df$outcome)\n\n# intervention date\nabline(v = interven, col = \"blue\")\n\n# regression line\nts <- lm(outcome ~ time + treatment + timesincetreat, data = df)\nlines(df$time, ts$fitted.values, col = \"red\")\nsummary(ts)\n#> \n#> Call:\n#> lm(formula = outcome ~ time + treatment + timesincetreat, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.58812 -0.67771  0.03995  0.63623  2.82507 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)     9.705206   0.135820    71.46   <2e-16 ***\n#> time           15.002674   0.001172 12802.61   <2e-16 ***\n#> treatment      19.852727   0.201416    98.57   <2e-16 ***\n#> timesincetreat 24.996424   0.001954 12791.27   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9568 on 361 degrees of freedom\n#> Multiple R-squared:      1,  Adjusted R-squared:      1 \n#> F-statistic: 1.042e+09 on 3 and 361 DF,  p-value: < 2.2e-16\n# treatment prediction\npred <- predict(ts, df)\n\n# counterfactual dataset\nnew_df <-\n    as.data.frame(cbind(\n        time = time,\n        # treatment = 0 means counterfactual\n        treatment = rep(0, n),\n        # time since treatment = 0 means counterfactual\n        timesincetreat = rep(0)\n    ))\n\n# counterfactual predictions\npred_cf <- predict(ts, new_df)\n\n# plot\nplot(\n    outcome,\n    col = gray(0.2, 0.2),\n    pch = 19,\n    xlim  = c(1,365),\n    ylim = c(0, 10000),\n    xlab = \"xlab\",\n    ylab = \"ylab\"\n)\n\n# regression line before treatment\nlines(rep(1:interven), pred[1:interven], col = \"blue\", lwd = 3)\n\n# regression line after treatment\nlines(rep((interven + 1):n), pred[(interven + 1):n], \n      col = \"blue\", lwd = 3)\n\n# regression line after treatment (counterfactual)\nlines(\n    rep(interven:n),\n    pred_cf[(interven):n],\n    col = \"yellow\",\n    lwd = 3,\n    lty = 5\n)\n\nabline(v = interven, col = \"red\", lty = 2)"},{"path":"interrupted-time-series.html","id":"autocorrelation","chapter":"32 Interrupted Time Series","heading":"32.1 Autocorrelation","text":"Assess autocorrelation residualThis best example since created dataset. residuals autocorrelation, see patterns (.e., points randomly distributed plot)formally test autocorrelation, can use Durbin-Watson testFrom p-value, know autocorrelation time seriesA solution problem use advanced time series analysis (e.g., ARIMA - coming book) adjust seasonality dependency.","code":"\n# simple regression on time \nsimple_ts <- lm(outcome ~ time, data = df)\n\nplot(resid(simple_ts))\n\n# alternatively\nacf(resid(simple_ts))\nlmtest::dwtest(df$outcome ~ df$time)\n#> \n#>  Durbin-Watson test\n#> \n#> data:  df$outcome ~ df$time\n#> DW = 0.00037607, p-value < 2.2e-16\n#> alternative hypothesis: true autocorrelation is greater than 0\nforecast::auto.arima(df$outcome, xreg = as.matrix(df[,-1]))\n#> Series: df$outcome \n#> Regression with ARIMA(3,0,2) errors \n#> \n#> Coefficients:\n#>          ar1      ar2     ar3      ma1     ma2  intercept     time  treatment\n#>       0.1904  -0.9672  0.0925  -0.1327  0.9557     9.7122  15.0026    19.8588\n#> s.e.  0.0693   0.0356  0.0543   0.0467  0.0338     0.1446   0.0012     0.2141\n#>       timesincetreat\n#>              24.9965\n#> s.e.          0.0021\n#> \n#> sigma^2 = 0.91:  log likelihood = -496.34\n#> AIC=1012.67   AICc=1013.3   BIC=1051.67"},{"path":"interrupted-time-series.html","id":"multiple-groups","chapter":"32 Interrupted Time Series","heading":"32.2 Multiple Groups","text":"suspect might confounding events selection bias, can add control group experience treatment (much similar Difference--differences)model becomes\\[\n\\begin{aligned}\nY = \\beta_0 &+ \\beta_1 time+ \\beta_2 treatment +\\beta_3 \\times timesincetreat \\\\\n&+\\beta_4 group + \\beta_5 group \\times time + \\beta_6 group \\times treatment \\\\\n&+ \\beta_7 group \\times timesincetreat\n\\end{aligned}\n\\]whereGroup = 1 observation treatment 0 controlGroup = 1 observation treatment 0 control\\(\\beta_4\\) = baseline difference treatment control group\\(\\beta_4\\) = baseline difference treatment control group\\(\\beta_5\\) = slope difference treatment control group treatment\\(\\beta_5\\) = slope difference treatment control group treatment\\(\\beta_6\\) = baseline difference treatment control group associated treatment.\\(\\beta_6\\) = baseline difference treatment control group associated treatment.\\(\\beta_7\\) = difference sustained effect treatment control group treatment.\\(\\beta_7\\) = difference sustained effect treatment control group treatment.","code":""},{"path":"endogeneity.html","id":"endogeneity","chapter":"33 Endogeneity","heading":"33 Endogeneity","text":"RefresherA general model framework\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{X} = n \\times k\\)\\(\\mathbf{X} = n \\times k\\)\\(\\beta = k \\times 1\\)\\(\\beta = k \\times 1\\)\\(\\epsilon = n \\times 1\\)\\(\\epsilon = n \\times 1\\), OLS estimates coefficients \\[\n\\begin{aligned}\n\\hat{\\beta}_{OLS} &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{Y}) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'(\\mathbf{X \\beta + \\epsilon})) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{X}) \\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon}) \\\\\n\\hat{\\beta}_{OLS} & \\\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\n\\end{aligned}\n\\]unbiased estimates, get rid second part \\((\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\\)2 conditions achieve unbiased estimates:\\(E(\\epsilon |X) = 0\\) (easy, putting intercept can solve issue)\\(Cov(\\mathbf{X}, \\epsilon) = 0\\) (hard part)care omitted variableUsually, problem stem Omitted Variables Bias, care omitted variable bias whenOmitted variables correlate variables care (\\(X\\)). OMV correlate \\(X\\), don’t care, random assignment makes correlation goes 0)Omitted variables correlates outcome/ dependent variableThere types endogeneity listed .Types endogeneity (See Hill et al. (2021) review management):Endogenous TreatmentOmitted Variables Bias\nMotivation\nAbility/talent\nSelf-selection\nOmitted Variables BiasMotivationAbility/talentSelf-selectionFeedback Effect (Simultaneity): also known bidirectionalityFeedback Effect (Simultaneity): also known bidirectionalityReverse Causality: Subtle difference Simultaneity: Technically, two variables affect sequentially, big enough time frame, (e.g., monthly, yearly), coefficient biased just like simultaneity.Reverse Causality: Subtle difference Simultaneity: Technically, two variables affect sequentially, big enough time frame, (e.g., monthly, yearly), coefficient biased just like simultaneity.Measurement ErrorMeasurement ErrorEndogenous Sample SelectionTo deal problem, toolbox (mentioned previous chapter 18)Using control variables regression “selection observables” identification strategy.words, believe omitted variable, can measure , including regression model solves problem. uninterested variables called control variables model.However, rarely case (problem don’t measurements). Hence, need elaborate methods:Endogenous TreatmentEndogenous TreatmentEndogenous Sample SelectionEndogenous Sample SelectionBefore get methods deal bias arises omitted variables, consider cases measurements variable, measurement error (bias).","code":""},{"path":"endogeneity.html","id":"endogenous-treatment","chapter":"33 Endogeneity","heading":"33.1 Endogenous Treatment","text":"","code":""},{"path":"endogeneity.html","id":"measurement-error","chapter":"33 Endogeneity","heading":"33.1.1 Measurement Error","text":"Data error can stem \nCoding errors\nReporting errors\nData error can stem fromCoding errorsCoding errorsReporting errorsReporting errorsTwo forms measurement error:Random (stochastic) (indeterminate error) (Classical Measurement Errors): noise measurement errors show consistent predictable way.Systematic (determinate error) (Non-classical Measurement Errors): measurement error consistent predictable across observations.\nInstrument errors (e.g., faulty scale) -> calibration adjustment\nMethod errors (e.g., sampling errors) -> better method development + study design\nHuman errors (e.g., judgement)\nInstrument errors (e.g., faulty scale) -> calibration adjustmentMethod errors (e.g., sampling errors) -> better method development + study designHuman errors (e.g., judgement)Usually systematic measurement error bigger issue introduces “bias” estimates, random error introduces noise estimatesNoise -> regression estimate 0Bias -> can pull estimate upward downward.","code":""},{"path":"endogeneity.html","id":"classical-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.1 Classical Measurement Errors","text":"","code":""},{"path":"endogeneity.html","id":"right-hand-side","chapter":"33 Endogeneity","heading":"33.1.1.1.1 Right-hand side","text":"Right-hand side measurement error: measurement covariates, endogeneity problem.Say know true model \\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\]don’t observe \\(X_i\\), observe\\[\n\\tilde{X}_i = X_i + e_i\n\\]known classical measurement errors assume \\(e_i\\) uncorrelated \\(X_i\\) (.e., \\(E(X_i e_i) = 0\\)), estimate observed variables, (substitute \\(X_i\\) \\(\\tilde{X}_i - e_i\\) ):\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 (\\tilde{X}_i - e_i)+ u_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + u_i - \\beta_1 e_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + v_i\n\\end{aligned}\n\\]words, measurement error \\(X_i\\) now part error term regression equation \\(v_i\\). Hence, endogeneity bias.Endogeneity arises \\[\n\\begin{aligned}\nE(\\tilde{X}_i v_i) &= E((X_i + e_i )(u_i - \\beta_1 e_i)) \\\\\n&= -\\beta_1 Var(e_i) \\neq 0\n\\end{aligned}\n\\]Since \\(\\tilde{X}_i\\) \\(e_i\\) positively correlated, leads toa negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea positive bias \\(\\beta_1\\) negativea positive bias \\(\\beta_1\\) negativeIn words, measurement errors cause attenuation bias, inter turn pushes coefficient towards 0As \\(Var(e_i)\\) increases \\(\\frac{Var(e_i)}{Var(\\tilde{X})} \\1\\) \\(e_i\\) random (noise) \\(\\beta_1 \\0\\) (random variable \\(\\tilde{X}\\) relation \\(Y_i\\))Technical note:size bias OLS-estimator \\[\n\\hat{\\beta}_{OLS} = \\frac{ cov(\\tilde{X}, Y)}{var(\\tilde{X})} = \\frac{cov(X + e, \\beta X + u)}{var(X + e)}\n\\]\\[\nplim \\hat{\\beta}_{OLS} = \\beta \\frac{\\sigma^2_X}{\\sigma^2_X + \\sigma^2_e} = \\beta \\lambda\n\\]\\(\\lambda\\) reliability signal--total variance ratio attenuation factorReliability affect extent measurement error attenuates \\(\\hat{\\beta}\\). attenuation bias \\[\n\\hat{\\beta}_{OLS} - \\beta = -(1-\\lambda)\\beta\n\\]Thus, \\(\\hat{\\beta}_{OLS} < \\beta\\) (unless \\(\\lambda = 1\\), case don’t even measurement error).Note:Data transformation worsen (magnify) measurement error\\[\ny= \\beta x + \\gamma x^2 + \\epsilon\n\\], attenuation factor \\(\\hat{\\gamma}\\) square attenuation factor \\(\\hat{\\beta}\\) (.e., \\(\\lambda_{\\hat{\\gamma}} = \\lambda_{\\hat{\\beta}}^2\\))Adding covariates increases attenuation biasTo fix classical measurement error problem, canFind estimates either \\(\\sigma^2_X, \\sigma^2_\\epsilon\\) \\(\\lambda\\) validation studies, survey data.Endogenous Treatment Use instrument \\(Z\\) correlated \\(X\\) uncorrelated \\(\\epsilon\\)Abandon project","code":""},{"path":"endogeneity.html","id":"left-hand-side","chapter":"33 Endogeneity","heading":"33.1.1.1.2 Left-hand side","text":"measurement outcome variable, econometricians causal scientists care still unbiased estimate coefficients (zero conditional mean assumption violated, hence don’t endogeneity). However, statisticians might care might inflate uncertainty coefficient estimates (.e., higher standard errors).\\[\n\\tilde{Y} = Y + v\n\\]model estimate \\[\n\\tilde{Y} = \\beta X + u + v\n\\]Since \\(v\\) uncorrelated \\(X\\), \\(\\hat{\\beta}\\) consistently estimated OLSIf measurement error \\(Y_i\\), pass \\(\\beta_1\\) go \\(u_i\\)","code":""},{"path":"endogeneity.html","id":"non-classical-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.2 Non-classical Measurement Errors","text":"Relaxing assumption \\(X\\) \\(\\epsilon\\) uncorrelatedRecall true model true estimate \\[\n\\hat{\\beta} = \\frac{cov(X + \\epsilon, \\beta X + u)}{var(X + \\epsilon)}\n\\]without assumption, \\[\n\\begin{aligned}\nplim \\hat{\\beta} &= \\frac{\\beta (\\sigma^2_X + \\sigma_{X \\epsilon})}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}} \\\\\n&= (1 - \\frac{\\sigma^2_{\\epsilon} + \\sigma_{X \\epsilon}}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}}) \\beta \\\\\n&= (1 - b_{\\epsilon \\tilde{X}}) \\beta\n\\end{aligned}\n\\]\\(b_{\\epsilon \\tilde{X}}\\) covariance \\(\\tilde{X}\\) \\(\\epsilon\\) (also regression coefficient regression \\(\\epsilon\\) \\(\\tilde{X}\\))Hence, Classical Measurement Errors just special case Non-classical Measurement Errors \\(b_{\\epsilon \\tilde{X}} = 1 - \\lambda\\)\\(\\sigma_{X \\epsilon} = 0\\) (Classical Measurement Errors), increasing covariance \\(b_{\\epsilon \\tilde{X}}\\) increases covariance increases attenuation factor half variance \\(\\tilde{X}\\) measurement error, decreases attenuation factor otherwise. also known mean reverting measurement error Bound, Brown, Mathiowetz (2001)general framework right-hand side left-hand side measurement error (Bound, Brown, Mathiowetz 2001):consider true model\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1}\\tilde{X} \\tilde{Y}} \\\\\n&= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (\\tilde{X} \\beta - U \\beta + v + \\epsilon )} \\\\\n&= \\mathbf{\\beta + (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (-U \\beta + v + \\epsilon)} \\\\\nplim \\hat{\\beta} &= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' ( -U\\beta + v) \\\\\n&= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' W\n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\end{aligned}\n\\]Since collect measurement errors matrix \\(W = [U|v]\\), \\[\n( -U\\beta + v) = W\n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\]Hence, general, biases coefficients \\(\\beta\\) regression coefficients regressing measurement errors mis-measured \\(\\tilde{X}\\)Notes:Instrumental Variable can help fix problemInstrumental Variable can help fix problemThere can also measurement error dummy variables can still use Instrumental Variable fix .can also measurement error dummy variables can still use Instrumental Variable fix .","code":""},{"path":"endogeneity.html","id":"solution-to-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.3 Solution to Measurement Errors","text":"","code":""},{"path":"endogeneity.html","id":"correlation","chapter":"33 Endogeneity","heading":"33.1.1.3.1 Correlation","text":"\\[\n\\begin{aligned}\nP(\\rho | data) &= \\frac{P(data|\\rho)P(\\rho)}{P(data)} \\\\\n\\text{Posterior Probability} &\\propto \\text{Likelihood} \\times \\text{Prior Probability}\n\\end{aligned}\n\\] \\(\\rho\\) correlation coefficient\\(P(data|\\rho)\\) likelihood function evaluated \\(\\rho\\)\\(P(\\rho)\\) prior probability\\(P(data)\\) normalizing constantWith sample correlation coefficient \\(r\\):\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\n\\] posterior density approximation \\(\\rho\\) (Schisterman et al. 2003, 3)\\[\nP(\\rho| x, y)  \\propto P(\\rho) \\frac{(1- \\rho^2)^{(n-1)/2}}{(1- \\rho \\times r)^{n - (3/2)}}\n\\]\\(\\rho = \\tanh \\xi\\) \\(\\xi \\sim N(z, 1/n)\\)\\(r = \\tanh z\\)posterior density follow normal distribution whereMean\\[\n\\mu_{posterior} = \\sigma^2_{posterior} \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood})\n\\]variance\\[\n\\sigma^2_{posterior} = \\frac{1}{n_{prior} + n_{Likelihood}}\n\\]simplify integration process, choose prior \\[\nP(\\rho) \\propto (1 - \\rho^2)^c\n\\] \\(c\\) weight prior estimation (.e., \\(c = 0\\) prior info, hence \\(P(\\rho) \\propto 1\\))Example:Current study: \\(r_{xy} = 0.5, n = 200\\)Previous study: \\(r_{xy} = 0.2765, (n=50205)\\)Combining two, posterior following normal distribution variance \\[\n\\sigma^2_{posterior} =  \\frac{1}{n_{prior} + n_{Likelihood}} = \\frac{1}{200 + 50205} = 0.0000198393\n\\]Mean\\[\n\\begin{aligned}\n\\mu_{Posterior} &= \\sigma^2_{Posterior}  \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood}) \\\\\n&= 0.0000198393 \\times (50205 \\times \\tanh^{-1} 0.2765 + 200 \\times \\tanh^{-1}0.5 )\\\\\n&= 0.2849415\n\\end{aligned}\n\\]Hence, \\(Posterior \\sim N(0.691, 0.0009)\\), means correlation coefficient \\(\\tanh(0.691) = 0.598\\) 95% CI \\[\n\\mu_{posterior} \\pm 1.96 \\times \\sqrt{\\sigma^2_{Posterior}} = 0.2849415 \\pm 1.96 \\times (0.0000198393)^{1/2} = (0.2762115, 0.2936714)\n\\]Hence, interval posterior \\(\\rho\\) \\((0.2693952, 0.2855105)\\)future authors suspect haveLarge sampling variationMeasurement error either measures correlation, attenuates relationship two variablesApplying Bayesian correction can give better estimate correlation two.implement calculation R, see ","code":"\nn_new              <- 200\nr_new              <- 0.5\nalpha              <- 0.05\n\nupdate_correlation <- function(n_new, r_new, alpha) {\n    n_meta             <- 50205\n    r_meta             <- 0.2765\n    \n    # Variance\n    var_xi         <- 1 / (n_new + n_meta)\n    format(var_xi, scientific = FALSE)\n    \n    # mean\n    mu_xi          <- var_xi * (n_meta * atanh(r_meta) + n_new * (atanh(r_new)))\n    format(mu_xi, scientific  = FALSE)\n    \n    # confidence interval\n    upper_xi       <- mu_xi + qnorm(1 - alpha / 2) * sqrt(var_xi)\n    lower_xi       <- mu_xi - qnorm(1 - alpha / 2) * sqrt(var_xi)\n    \n    # rho\n    mean_rho       <- tanh(mu_xi)\n    upper_rho      <- tanh(upper_xi)\n    lower_rho      <- tanh(lower_xi)\n    \n    # return a list\n    return(\n        list(\n            \"mu_xi\" = mu_xi,\n            \"var_xi\" = var_xi,\n            \"upper_xi\" = upper_xi,\n            \"lower_xi\" = lower_xi,\n            \"mean_rho\" = mean_rho,\n            \"upper_rho\" = upper_rho,\n            \"lower_rho\" = lower_rho\n        )\n    )\n}\n\n\n\n\n# Old confidence interval\nr_new + qnorm(1 - alpha / 2) * sqrt(1/n_new)\n#> [1] 0.6385904\nr_new - qnorm(1 - alpha / 2) * sqrt(1/n_new)\n#> [1] 0.3614096\n\ntesting = update_correlation(n_new = n_new, r_new = r_new, alpha = alpha)\n\n# Updated rho\ntesting$mean_rho\n#> [1] 0.2774723\n\n# Updated confidence interval\ntesting$upper_rho\n#> [1] 0.2855105\ntesting$lower_rho\n#> [1] 0.2693952"},{"path":"endogeneity.html","id":"simultaneity","chapter":"33 Endogeneity","heading":"33.1.2 Simultaneity","text":"independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).Examples: quantity price demand supply, investment productivity, sales advertisementExamples: quantity price demand supply, investment productivity, sales advertisementGeneral Simultaneous (Structural) Equations\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + u_i \\\\\nX_i &= \\alpha_0 + \\alpha_1 Y_i + v_i\n\\end{aligned}\n\\]Hence, solutions \\[\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 v_i + u_i}{1 - \\alpha_1 \\beta_1} \\\\\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}\n\\end{aligned}\n\\]run one regression, biased estimators (simultaneity bias):\\[\n\\begin{aligned}\nCov(X_i, u_i) &= Cov(\\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}, u_i) \\\\\n&= \\frac{\\alpha_1}{1- \\alpha_1 \\beta_1} Var(u_i)\n\\end{aligned}\n\\]even general model\\[\n\\begin{cases}\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + u_i \\\\\nX_i = \\alpha_0 + \\alpha_1 Y_i + \\alpha_2 Z_i + v_i\n\\end{cases}\n\\]\\(X_i, Y_i\\) endogenous variables determined within system\\(X_i, Y_i\\) endogenous variables determined within system\\(T_i, Z_i\\) exogenous variables\\(T_i, Z_i\\) exogenous variablesThen, reduced form model \\[\n\\begin{cases}\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 \\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{u}_i \\\\\n&= B_0 + B_1 Z_i + B_2 T_i + \\tilde{u}_i\n\\end{aligned}\n\\\\\n\\begin{aligned}\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\alpha_1\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{v}_i \\\\\n&= A_0 + A_1 Z_i + A_2 T_i + \\tilde{v}_i\n\\end{aligned}\n\\end{cases}\n\\], now can get consistent estimates reduced form parametersAnd get original parameter estimates\\[\n\\begin{aligned}\n\\frac{B_1}{A_1} &= \\beta_1 \\\\\nB_2 (1 - \\frac{B_1 A_2}{A_1B_2}) &= \\beta_2 \\\\\n\\frac{A_2}{B_2} &= \\alpha_1 \\\\\nA_1 (1 - \\frac{B_1 A_2}{A_1 B_2}) &= \\alpha_2\n\\end{aligned}\n\\]Rules IdentificationOrder Condition (necessary sufficient)\\[\nK - k \\ge m - 1\n\\]\\(M\\) = number endogenous variables model\\(M\\) = number endogenous variables modelK = number exogenous variables int modelK = number exogenous variables int model\\(m\\) = number endogenous variables given\\(m\\) = number endogenous variables given\\(k\\) = number exogenous variables given equation\\(k\\) = number exogenous variables given equationThis actually general framework instrumental variables","code":""},{"path":"endogeneity.html","id":"endogenous-treatment-solutions","chapter":"33 Endogeneity","heading":"33.1.3 Endogenous Treatment Solutions","text":"Using OLS estimates reference point","code":"\nlibrary(AER)\nlibrary(REndo)\nset.seed(421)\ndata(\"CASchools\")\nschool <- CASchools\nschool$stratio <- with(CASchools, students / teachers)\nm1.ols <-\n    lm(read ~ stratio + english + lunch \n       + grades + income + calworks + county,\n       data = school)\nsummary(m1.ols)$coefficients[1:7,]\n#>                 Estimate Std. Error     t value      Pr(>|t|)\n#> (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e-218\n#> stratio      -0.30035544 0.25797023  -1.1643027  2.450536e-01\n#> english      -0.20550107 0.03765408  -5.4576041  8.871666e-08\n#> lunch        -0.38684059 0.03700982 -10.4523759  1.427370e-22\n#> gradesKK-08  -1.91291321 1.35865394  -1.4079474  1.599886e-01\n#> income        0.71615378 0.09832843   7.2832829  1.986712e-12\n#> calworks     -0.05273312 0.06154758  -0.8567863  3.921191e-01"},{"path":"endogeneity.html","id":"instrumental-variable","chapter":"33 Endogeneity","heading":"33.1.3.1 Instrumental Variable","text":"[A3a] requires \\(\\epsilon_i\\) uncorrelated \\(\\mathbf{x}_i\\)Assume A1 , A2, A5\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x_i'x_i})]^{-1}E(\\mathbf{x_i'}\\epsilon_i)\n\\][A3a] weakest assumption needed OLS consistentA3 fails \\(x_{ik}\\) correlated \\(\\epsilon_i\\)Omitted Variables Bias: \\(\\epsilon_i\\) includes factors may influence dependent variable (linearly)Simultaneity Demand prices simultaneously determined.Endogenous Sample Selection iid sampleMeasurement ErrorNoteOmitted Variable: omitted variable variable, omitted model (\\(\\epsilon_i\\)) unobserved predictive power towards outcome.Omitted Variable Bias: bias (inconsistency looking large sample properties) OLS estimator omitted variable.cam positive negative selection bias (depends story )structural equation used emphasize interested understanding causal relationship\\[\ny_{i1} = \\beta_0 + \\mathbf{z}_i1 \\beta_1 + y_{i2}\\beta_2 +  \\epsilon_i\n\\]\\(y_{}\\) outcome variable (inherently correlated \\(\\epsilon_i\\))\\(y_{i2}\\) endogenous covariate (presumed correlated \\(\\epsilon_i\\))\\(\\beta_1\\) represents causal effect \\(y_{i2}\\) \\(y_{i1}\\)\\(\\mathbf{z}_{i1}\\) exogenous controls (uncorrelated \\(\\epsilon_i\\)) (\\(E(z_{1i}'\\epsilon_i) = 0\\))OLS inconsistent estimator causal effect \\(\\beta_2\\)endogeneity\\(E(y_{i2}'\\epsilon_i) = 0\\)exogenous variation \\(y_{i2}\\) identifies causal effectIf endogeneityAny wiggle \\(y_{i2}\\) shift simultaneously \\(\\epsilon_i\\)\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\n\\]\\(\\beta\\) causal effect\\([E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\\) endogenous effectHence \\(\\hat{\\beta}_{OLS}\\) can either positive negative true causal effect.Motivation Two Stage Least Squares (2SLS)\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]want understand movement \\(y_{i2}\\) effects movement \\(y_{i1}\\), whenever move \\(y_{i2}\\), \\(\\epsilon_i\\) also moves.Solution\nneed way move \\(y_{i2}\\) independently \\(\\epsilon_i\\), can analyze response \\(y_{i1}\\) causal effectFind instrumental variable(s) \\(z_{i2}\\)\nInstrument Relevance: ** \\(z_{i2}\\) moves \\(y_{i2}\\) also moves\nInstrument Exogeneity: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\nFind instrumental variable(s) \\(z_{i2}\\)Instrument Relevance: ** \\(z_{i2}\\) moves \\(y_{i2}\\) also movesInstrument Exogeneity: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)Finding Instrumental variable:Random Assignment: + Effect class size educational outcomes: instrument initial randomRelation’s Choice + Effect Education Fertility: instrument parent’s educational levelEligibility + Trade-IRA 401K retirement savings: instrument 401k eligibilityExampleReturn Collegeeducation correlated ability - endogenouseducation correlated ability - endogenousNear 4year instrument\nInstrument Relevance: near moves education also moves\nInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.\nNear 4year instrumentInstrument Relevance: near moves education also movesInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.potential instruments; near 2-year college. Parent’s Education. Owning Library CardOther potential instruments; near 2-year college. Parent’s Education. Owning Library Card\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]First Stage (Reduced Form) Equation:\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i\n\\]\\(\\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2}\\) exogenous variation \\(v_i\\) endogenous variationThis called reduced form equationNot interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)Instrumental variable \\(z_{i2}\\)Instrument Relevance: \\(\\pi_2 \\neq 0\\)Instrument Exogeneity: \\(E(\\mathbf{z_{i2}\\epsilon_i})=0\\)Moving exogenous part \\(y_i2\\) moving\\[\n\\tilde{y}_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1 + z_{i2}\\pi_2}\n\\]two Stage Least Squares (2SLS)\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ y_{i2}\\beta_2 + \\epsilon_i\n\\]\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]Equivalently,\\(\\tilde{y}_{i2} =\\pi_0 + \\mathbf{z_{i2}\\pi_2}\\)\\(u_i = v_i \\beta_2+ \\epsilon_i\\)(33.1) holds A1, A5A2 holds instrument relevant \\(\\pi_2 \\neq 0\\) + \\(y_{i1} = \\beta_0 + \\mathbf{z_{i1}\\beta_1 + (\\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2)}\\beta_2 + u_i\\)[A3a] holds instrument exogenous \\(E(\\mathbf{z}_{i2}\\epsilon_i)=0\\)\\[\n\\begin{aligned}\nE(\\tilde{y}_{i2}'u_i) &= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})(v_i\\beta_2 + \\epsilon_i)) \\\\\n&= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})( \\epsilon_i)) \\\\\n&= E(\\epsilon_i)\\pi_0 + E(\\epsilon_iz_{i1})\\pi_1 + E(\\epsilon_iz_{i2}) \\\\\n&=0\n\\end{aligned}\n\\]Hence, (33.1) consistentThe 2SLS Estimator\n1. Estimate first stage using OLS\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]obtained estimated value \\(\\hat{y}_{i2}\\)Estimate altered equation using OLS\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ \\hat{y}_{i2}\\beta_2 + \\epsilon_i\n\\]Properties 2SLS EstimatorUnder A1, A2, [A3a] (\\(z_{i1}\\)), A5 instrument satisfies following two conditions, + Instrument Relevance: \\(\\pi_2 \\neq 0\\) + Instrument Exogeneity: \\(E(\\mathbf{z}_{i2}'\\epsilon_i) = 0\\) 2SLS estimator consistentCan handle one endogenous variable one instrumental variable\\[\n\\begin{aligned}\ny_{i1} &= \\beta_0 + z_{i1}\\beta_1 + y_{i2}\\beta_2 + y_{i3}\\beta_3 + \\epsilon_i \\\\\ny_{i2} &= \\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2 + z_{i3}\\pi_3 + z_{i4}\\pi_4 + v_{i2} \\\\\ny_{i3} &= \\gamma_0 + z_{i1}\\gamma_1 + z_{i2}\\gamma_2 + z_{i3}\\gamma_3 + z_{i4}\\gamma_4 + v_{i3}\n\\end{aligned}\n\\]Standard errors produced second step correct\nknow \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variation\nproblem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.\nA4 hold, need report robust standard errors.\nStandard errors produced second step correctBecause know \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variationWe problem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.A4 hold, need report robust standard errors.2SLS less efficient OLS always larger standard errors.\nFirst, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)\nSecond, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)\n2SLS less efficient OLS always larger standard errors.First, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)Second, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)number instruments need least many number endogenous variables.number instruments need least many number endogenous variables.Note2SLS can combined FGLS make estimator efficient: first-stage, second-stage, instead using OLS, can use FLGS weight matrix \\(\\hat{w}\\)Generalized Method Moments can efficient 2SLS.second-stage 2SLS, can also use MLE, making assumption distribution outcome variable, endogenous variable, relationship (joint distribution).","code":"    + **IV estimator**: one endogenous variable with a single instrument \n    + **2SLS estimator**: one endogenous variable with multiple instruments \n    + **GMM estimator**: multiple endogenous variables with multiple instruments\n    "},{"path":"endogeneity.html","id":"testing-assumptions-1","chapter":"33 Endogeneity","heading":"33.1.3.1.1 Testing Assumptions","text":"Endogeneity Test: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Endogeneity Test: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Exogeneity (always test can might informative)Exogeneity (always test can might informative)Relevancy (need avoid “weak instruments”)Relevancy (need avoid “weak instruments”)","code":""},{},{},{},{"path":"endogeneity.html","id":"checklist","chapter":"33 Endogeneity","heading":"33.1.3.1.2 Checklist","text":"Regress dependent variable instrument (reduced form). Since OLS, unbiased estimate, coefficient estimate significant (make sure sign makes sense)Report F-stat excluded instruments. F-stat < 10 means weak instrument (J. H. Stock, Wright, Yogo 2002).Present \\(R^2\\) including instrument (Rossi 2014)models multiple instrument, present firs-t second-stage result instrument separately. Overid test conducted (e.g., Sargan-Hansen J)Hausman test OLS 2SLS (don’t confuse test evidence endogeneity irrelevant - invalid IV, test useless)Compare 2SLS limited information ML. different, evidence weak instruments.","code":""},{"path":"endogeneity.html","id":"good-instruments","chapter":"33 Endogeneity","heading":"33.1.3.2 Good Instruments","text":"Exogeneity Relevancy necessary sufficient IV produce consistent estimates.Without theory possible explanation, can always create new variable correlated \\(X\\) uncorrelated \\(\\epsilon\\)example, want estimate effect price quantity (Reiss 2011, 960)\\[\n\\begin{aligned}\nQ &= \\beta_1 P + \\beta_2 X + \\epsilon \\\\\nP &= \\pi_1 X + \\eta\n\\end{aligned}\n\\]\\(\\epsilon\\) \\(\\eta\\) jointly determined, \\(X \\perp \\epsilon, \\eta\\)Without theory, can just create new variable \\(Z = X + u\\) \\(E(u) = 0; u \\perp X, \\epsilon, \\eta\\), \\(Z\\) satisfied conditions:Relevancy: \\(X\\) correlates \\(P\\) \\(\\rightarrow\\) \\(Z\\) correlates \\(P\\)Relevancy: \\(X\\) correlates \\(P\\) \\(\\rightarrow\\) \\(Z\\) correlates \\(P\\)Exogeneity: \\(u \\perp \\epsilon\\) (random noise)Exogeneity: \\(u \\perp \\epsilon\\) (random noise)obviously, ’s valid instrument (intuitively). theoretically, relevance exogeneity sufficient identify \\(\\beta\\) unsatisfied rank condition identification.Moreover, functional form instrument also plays role choosing good instrument. Hence, always need check robustness instrument.IV methods even valid instruments can still poor sampling properties (finite sample bias, large sampling errors) (Rossi 2014)weak instrument, ’s important report appropriately. problem exacerbated multiple instruments (Larcker Rusticus 2010).","code":""},{"path":"endogeneity.html","id":"lagged-dependent-variable","chapter":"33 Endogeneity","heading":"33.1.3.2.1 Lagged dependent variable","text":"time series data sets, can use lagged dependent variable instrument influenced current shocks. example, Chetty, Friedman, Rockoff (2014) used lagged dependent variable econ.","code":""},{"path":"endogeneity.html","id":"lagged-explanatory-variable","chapter":"33 Endogeneity","heading":"33.1.3.2.2 Lagged explanatory variable","text":"Common practice applied economics: Replace suspected simultaneously determined explanatory variable lagged value Bellemare, Masaki, Pepinsky (2017).\npractice avoid simultaneity bias.\nEstimates using method still inconsistent.\nHypothesis testing becomes invalid approach.\nLagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.\nCommon practice applied economics: Replace suspected simultaneously determined explanatory variable lagged value Bellemare, Masaki, Pepinsky (2017).practice avoid simultaneity bias.practice avoid simultaneity bias.Estimates using method still inconsistent.Estimates using method still inconsistent.Hypothesis testing becomes invalid approach.Hypothesis testing becomes invalid approach.Lagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.Lagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.Key conditions appropriate use (Bellemare, Masaki, Pepinsky 2017):\nunobserved confounding:\ndynamics among unobservables.\nlagged variable \\(X\\) stationary autoregressive process.\n\nunobserved confounding:\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).\nReverse causality contemporaneous, one-period lag effect.\nReverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).\n\nKey conditions appropriate use (Bellemare, Masaki, Pepinsky 2017):unobserved confounding:\ndynamics among unobservables.\nlagged variable \\(X\\) stationary autoregressive process.\ndynamics among unobservables.lagged variable \\(X\\) stationary autoregressive process.unobserved confounding:\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).\nReverse causality contemporaneous, one-period lag effect.\nReverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).Reverse causality contemporaneous, one-period lag effect.Reverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).Alternative approach: Use lagged values endogenous variable IV estimation. However, IV estimation effective (Reed 2015):\nLagged values belong estimating equation.\nLagged values sufficiently correlated simultaneously determined explanatory variable.\nLagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).\nAlternative approach: Use lagged values endogenous variable IV estimation. However, IV estimation effective (Reed 2015):Lagged values belong estimating equation.Lagged values belong estimating equation.Lagged values sufficiently correlated simultaneously determined explanatory variable.Lagged values sufficiently correlated simultaneously determined explanatory variable.Lagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).Lagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).","code":""},{"path":"endogeneity.html","id":"internal-instrumental-variable","chapter":"33 Endogeneity","heading":"33.1.3.3 Internal instrumental variable","text":"(also known instrument free methods). section based Raluca Gui’s guide(also known instrument free methods). section based Raluca Gui’s guidealternative external instrumental variable approachesalternative external instrumental variable approachesAll approaches assume continuous dependent variableAll approaches assume continuous dependent variable","code":""},{"path":"endogeneity.html","id":"non-hierarchical-data-cross-classified","chapter":"33 Endogeneity","heading":"33.1.3.3.1 Non-hierarchical Data (Cross-classified)","text":"\\[\nY_t = \\beta_0 + \\beta_1 P_t + \\beta_2 X_t + \\epsilon_t\n\\]\\(t = 1, .., T\\) (indexes either time cross-sectional units)\\(Y_t\\) \\(k \\times 1\\) response variable\\(X_t\\) \\(k \\times n\\) exogenous regressor\\(P_t\\) \\(k \\times 1\\) continuous endogenous regressor\\(\\epsilon_t\\) structural error term \\(\\mu_\\epsilon =0\\) \\(E(\\epsilon^2) = \\sigma^2\\)\\(\\beta\\) model parametersThe endogeneity problem arises correlation \\(P_t\\) \\(\\epsilon_t\\):\\[\nP_t = \\gamma Z_t + v_t\n\\]\\(Z_t\\) \\(l \\times 1\\) vector internal instrumental variables\\(ν_t\\) random error \\(\\mu_{v_t}, E(v^2) = \\sigma^2_v, E(\\epsilon v) = \\sigma_{\\epsilon v}\\)\\(Z_t\\) assumed stochastic distribution \\(G\\)\\(ν_t\\) assumed density \\(h(·)\\)","code":""},{},{},{},{},{"path":"endogeneity.html","id":"hierarchical-data","chapter":"33 Endogeneity","heading":"33.1.3.3.2 Hierarchical Data","text":"Multiple independent assumptions involving various random components different levels mean moderate correlation predictors random component error term can result significant bias coefficients variance components. (J.-S. Kim Frees 2007) proposed generalized method moments uses , within variations exogenous variables, assumes within variation variables endogenous.Assumptionsthe errors level \\(\\sim iid N\\)slope variables exogenousthe level-1 \\(\\epsilon \\perp X, P\\). case, additional, external instruments necessaryHierarchical Model\\[\n\\begin{aligned}\nY_{cst} &= Z_{cst}^1 \\beta_{cs}^1 + X_{cst}^1 \\beta_1 + \\epsilon_{cst}^1 \\\\\n\\beta^1_{cs} &= Z_{cs}^2 \\beta_{c}^2 + X_{cst}^2 \\beta_2 + \\epsilon_{cst}^2 \\\\\n\\beta^2_{c} &= X^3_c \\beta_3 + \\epsilon_c^3\n\\end{aligned}\n\\]Bias stem :errors higher two levels (\\(\\epsilon_c^3,\\epsilon_{cst}^2\\)) correlated regressorsonly third level errors (\\(\\epsilon_c^3\\)) correlated regressors(J.-S. Kim Frees 2007) proposedWhen variables assumed exogenous, proposed estimator equals random effects estimatorWhen variables assumed endogenous, equals fixed effects estimatoralso use omitted variable test (based Hausman-test (J. . Hausman 1978) panel data), allows comparison robust estimator estimator efficient null hypothesis omitted variables comparison two robust estimators different levels.Another example using simulated datalevel-1 regressors: \\(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\\), \\(X_{15}\\) correlated level-2 error (.e., endogenous).level-2 regressors: \\(X_{21}, X_{22}, X_{23}, X_{24}\\)level-3 regressors: \\(X_{31}, X_{32}, X_{33}\\)estimate three-level model X15 assumed endogenous. three-level hierarchy, multilevelIV() returns five estimators, robust omitted variables (FE_L2), efficient (REF) (.e. lowest mean squared error).random effects estimator (REF) efficient assuming omitted variablesThe fixed effects estimator (FE) unbiased asymptotically normal even presence omitted variables.efficiency, random effects estimator preferable think omitted. variablesThe robust estimator preferable think omitted variables.True \\(\\beta_{X_{15}} =-1\\). can see estimators bias \\(X_{15}\\) correlated level-two error, FE_L2 GMM_L2 robustTo select appropriate estimator, use omitted variable test.three-level setting, can different estimator comparisons:Fixed effects vs. random effects estimators: Test omitted level-two level-three omitted effects, simultaneously, one compares FE_L2 REF. know omitted variables exist.Fixed effects vs. GMM estimators: existence omitted effects established sure level, test level-2 omitted effects comparing FE_L2 vs GMM_L3. reject null, omitted variables level-2 accomplished testing FE_L2 vs. GMM_L2, since latter consistent omitted effects level-2.Fixed effects vs. fixed effects estimators: can test omitted level-2 effects, allowing omitted level-3 effects comparing FE_L2 vs. FE_L3 since FE_L2 robust level-2 level-3 omitted effects FE_L3 robust level-3 omitted variables.Summary, use omitted variable test comparing REF vs. FE_L2 first.null hypothesis rejected, omitted variables either level-2 level-3If null hypothesis rejected, omitted variables either level-2 level-3Next, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:\nFE_L2 vs. FE_L3\nFE_L2 vs. GMM_L2\nNext, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:FE_L2 vs. FE_L3FE_L2 vs. GMM_L2If omitted variables level-2 found, test omitted level-3 effects comparing either\nFE_L3 vs. GMM_L3\nGMM_L2 vs. GMM_L3\nomitted variables level-2 found, test omitted level-3 effects comparing eitherFE_L3 vs. GMM_L3GMM_L2 vs. GMM_L3Since null hypothesis rejected (p = 0.000139), bias random effects estimator.test level-2 omitted effects (regardless level-3 omitted effects), compare FE_L2 versus FE_L3The null hypothesis omitted level-2 effects rejected (\\(p = 3.92e − 05\\)). Hence, omitted effects level-two. use FE_L2 consistent underlying data generated (level-2 error correlated \\(X_15\\), leads biased FE_L3 coefficients.omitted variable test FE_L2 GMM_L2 reject null hypothesis omitted level-2 effects (p-value 0).assume endogenous variable exogenous, RE GMM estimators biased wrong set internal instrumental variables. increase confidence, compare omitted variable tests variable considered endogenous vs. exogenous get sense whether variable truly endogenous.","code":"\n# function 'cholmod_factor_ldetA' not provided by package 'Matrix'\nset.seed(113)\nschool$gr08 <- school$grades == \"KK-06\"\nm7.multilevel <-\n    multilevelIV(read ~ stratio + english + lunch + income + gr08 +\n                     calworks + (1 | county) | endo(stratio),\n                 data = school)\nsummary(m7.multilevel)$coefficients[1:7,]\n# function 'cholmod_factor_ldetA' not provided by package 'Matrix'’\ndata(dataMultilevelIV)\nset.seed(114)\nformula1 <-\n    y ~ X11 + X12 + X13 + X14 + X15 + X21 + X22 + X23 + X24 +\n    X31 + X32 + X33 + (1 | CID) + (1 | SID) | endo(X15)\nm8.multilevel <-\n    multilevelIV(formula = formula1, data = dataMultilevelIV)\ncoef(m8.multilevel)\n\nsummary(m8.multilevel, \"REF\")\nsummary(m8.multilevel, \"REF\")\n# compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE_L2 (the most robust estimator), equivalently we are testing simultaneously for level-2 and level-3 omitted effects. \nsummary(m8.multilevel,\"FE_L2\")"},{"path":"endogeneity.html","id":"proxy-variables","chapter":"33 Endogeneity","heading":"33.1.3.4 Proxy Variables","text":"Can place omitted variableCan place omitted variablewill able estimate effect omitted variablewill able estimate effect omitted variablewill able reduce endogeneity caused bye omitted variablewill able reduce endogeneity caused bye omitted variablebut can Measurement Error. Hence, extremely careful using proxies.can Measurement Error. Hence, extremely careful using proxies.Criteria proxy variable:proxy correlated omitted variable.omitted variable regression solve problem endogeneityThe variation omitted variable unexplained proxy uncorrelated independent variables, including proxy.IQ test can proxy ability regression wage explained education.third requirement\\[\nability = \\gamma_0 + \\gamma_1 IQ + \\epsilon\n\\]\\(\\epsilon\\) uncorrelated education IQ test.","code":""},{"path":"endogeneity.html","id":"endogenous-sample-selection","chapter":"33 Endogeneity","heading":"33.2 Endogenous Sample Selection","text":"Also known sample selection self-selection problem incidental truncation.Also known sample selection self-selection problem incidental truncation.omitted variable people selected sampleThe omitted variable people selected sampleSome disciplines consider nonresponse bias selection bias sample selection.unobservable factors affect sample independent unobservable factors affect outcome, sample selection endogenous. Hence, sample selection ignorable estimator ignores sample selection still consistent.unobservable factors affect included sample correlated unobservable factors affect outcome, sample selection endogenous ignorable, estimators ignore endogenous sample selection consistent (don’t know part observable outcome related causal relationship part due different people selected treatment control groups).Assumptions: - unobservables affect treatment selection outcome jointly distributed bivariate normal.Notes:don’t strong exclusion restriction, identification driven assumed non linearity functional form (inverse Mills ratio). E.g., estimate depend bivariate normal distribution error structure:\nstrong exclusion restriction covariate correction equation, variation variable can help identify control selection\nweak exclusion restriction, variable exists steps, ’s assumed error structure identifies control selection (J. Heckman Navarro-Lozano 2004).\ndon’t strong exclusion restriction, identification driven assumed non linearity functional form (inverse Mills ratio). E.g., estimate depend bivariate normal distribution error structure:strong exclusion restriction covariate correction equation, variation variable can help identify control selectionWith weak exclusion restriction, variable exists steps, ’s assumed error structure identifies control selection (J. Heckman Navarro-Lozano 2004).management, Wolfolds Siegel (2019) found papers valid exclusion conditions, without , simulations show results using Heckman method less reliable obtained OLS.management, Wolfolds Siegel (2019) found papers valid exclusion conditions, without , simulations show results using Heckman method less reliable obtained OLS.differences Heckman Sample Selection vs. Heckman-type correctionThere differences Heckman Sample Selection vs. Heckman-type correctionTo deal [Sample Selection], canRandomization: participants randomly selected treatment control.Instruments determine treatment status (.e., treatment vs. control) outcome (\\(Y\\))Functional form selection outcome processes: originated (James J. Heckman 1976), later generalize (Amemiya 1984)main model\\[\n\\mathbf{y^* = xb + \\epsilon}\n\\]However, pattern missingness (.e., censored) related unobserved (latent) process:\\[\n\\mathbf{z^* = w \\gamma + u}\n\\]\\[\nz_i =\n\\begin{cases}\n1& \\text{} z_i^*>0 \\\\\n0&\\text{} z_i^*\\le0\\\\\n\\end{cases}\n\\]Equivalently, \\(z_i = 1\\) (\\(y_i\\) observed) \\[\nu_i \\ge -w_i \\gamma\n\\]Hence, probability observed \\(y_i\\) \\[\n\\begin{aligned}\nP(u_i \\ge -w_i \\gamma) &= 1 - \\Phi(-w_i \\gamma) \\\\\n&= \\Phi(w_i \\gamma) & \\text{symmetry standard normal distribution}\n\\end{aligned}\n\\]assumethe error term selection \\(\\mathbf{u \\sim N(0,)}\\)\\(Var(u_i) = 1\\) identification purposesVisually, \\(P(u_i \\ge -w_i \\gamma)\\) shaded area.Hence observed model, seeand joint distribution selection model (\\(u_i\\)), observed equation (\\(\\epsilon_i\\)) \\[\n\\left[\n\\begin{array}\n{c}\nu \\\\\n\\epsilon \\\\\n\\end{array}\n\\right]\n\\sim^{iid}N\n\\left(\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right],\n\\left[\n\\begin{array}\n{cc}\n1 & \\rho \\\\\n\\rho & \\sigma^2_{\\epsilon} \\\\\n\\end{array}\n\\right]\n\\right)\n\\]relation observed selection models:\\[\n\\begin{aligned}\nE(y_i | y_i \\text{ observed}) &= E(y_i| z^*>0) \\\\\n&= E(y_i| -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + E(\\epsilon_i | u_i > -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + \\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\n\\end{aligned}\n\\]\\(\\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\\) Inverse Mills Ratio. \\(\\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)} \\ge 0\\)property IMR: derivative : \\(IMR'(x) = -x IMR(x) - IMR(x)^2\\)Great visualization special cases correlation patterns among data errors professor Rob HickNote:(Bareinboim Pearl 2014) excellent summary cases can still causal inference case selection bias. ’ll try summarize idea :Let \\(X\\) action, \\(Y\\) outcome, S binary indicator entry data pool (\\(S = 1 =\\) sample, \\(S = 0 =\\) sample) Q conditional distribution \\(Q = P(y|x)\\).Usually want understand , \\(S\\), \\(P(y, x|S = 1)\\). Hence, ’d like recover \\(P(y|x)\\) \\(P(y, x|S = 1)\\)X Y affect S, can’t unbiasedly estimate \\(P(y|x)\\)case Omitted variable bias (\\(U\\)) sample selection bias (\\(S\\)), unblocked extraneous “flow” information X \\(Y\\), causes spurious correlation \\(X\\) \\(Y\\). Traditionally, recover \\(Q\\) parametric assumption ofThe data generating process (e.g., Heckman 2-step)Type data-generating model (e..g, treatment-dependent outcome-dependent)Selection’s probability \\(P(S = 1|P a_s)\\) non-parametrically based causal graphical models, authors proposed robust way model misspecification regardless type data-generating model, require selection’s probability. Hence, can recover Q\nWithout external data\nexternal data\nCausal effects Selection-backdoor criterion\nWithout external dataWith external dataCausal effects Selection-backdoor criterion","code":"\nx = seq(-3, 3, length = 200)\ny = dnorm(x, mean = 0, sd = 1)\nplot(x,\n     y,\n     type = \"l\",\n     main = bquote(\"Probabibility distribution of\" ~ u[i]))\nx = seq(0.3, 3, length = 100)\ny = dnorm(x, mean = 0, sd = 1)\npolygon(c(0.3, x, 3), c(0, y, 0), col = \"gray\")\ntext(1, 0.1, bquote(1 - Phi ~ (-w[i] ~ gamma)))\narrows(-0.5, 0.1, 0.3, 0, length = .15)\ntext(-0.5, 0.12, bquote(-w[i] ~ gamma))\nlegend(\n    \"topright\",\n    \"Gray = Prob of Observed\",\n    pch = 1,\n    title = \"legend\",\n    inset = .02\n)"},{"path":"endogeneity.html","id":"tobit-2","chapter":"33 Endogeneity","heading":"33.2.1 Tobit-2","text":"also known Heckman’s standard sample selection model\nAssumption: joint normality errorsData taken Mroz (1984).want estimate log(wage) married women, education, experience, experience squared, dummy variable living big city. can observe wage women working, means lot married women 1975 labor force unaccounted . Hence, OLS estimate wage equation bias due sample selection. Since data non-participants (.e., working pay), can correct selection process.Tobit-2 estimates consistent","code":""},{"path":"endogeneity.html","id":"example-1-3","chapter":"33 Endogeneity","heading":"33.2.1.1 Example 1","text":"2-stage Heckman’s model:probit equation estimates selection process (labor force?)results 1st stage used construct variable captures selection effect wage equation. correction variable called inverse Mills ratio.Use variables affect selection process selection equation. Technically, selection equation equation interest set regressors. recommended use variables (least one) selection equation affect selection process, wage process (.e., instruments). , variable kids fulfill role: women kids may likely stay home, working moms kids wages change.Alternatively,ManualSimilarly,Rho estimate correlation errors selection wage equations. lower panel, estimated coefficient inverse Mills ratio given Heckman model. fact statistically different zero consistent idea selection bias serious problem case.estimated coefficient inverse Mills ratio Heckman model statistically different zero, selection bias serious problem.","code":"\nlibrary(sampleSelection)\nlibrary(dplyr)\n# 1975 data on married women’s pay and labor-force participation \n# from the Panel Study of Income Dynamics (PSID)\ndata(\"Mroz87\") \nhead(Mroz87)\n#>   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage\n#> 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288\n#> 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416\n#> 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807\n#> 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417\n#> 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000\n#> 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106\n#>   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll\n#> 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE\n#> 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE\n#> 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE\n#> 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE\n#> 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE\n#> 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE\nMroz87 = Mroz87 %>%\n    mutate(kids = kids5 + kids618)\n\nlibrary(nnet)\nlibrary(ggplot2)\nlibrary(reshape2)\n# OLS: log wage regression on LF participants only\nols1 = lm(log(wage) ~ educ + exper + I(exper ^ 2) + city, \n          data = subset(Mroz87, lfp == 1))\n# Heckman's Two-step estimation with LFP selection equation\nheck1 = heckit(\n    selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ,\n    # the selection process, l\n    # fp = 1 if the woman is participating in the labor force\n    outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city,\n    data = Mroz87\n)\n\nsummary(heck1$probit)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                  Estimate  Std. error t value   Pr(> t)    \n#> XS(Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> XSage          0.18608901  0.06517476  2.8552  0.004301 ** \n#> XSI(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> XSkids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> XShuswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> XSeduc         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\nsummary(heck1$lm)\n#> \n#> Call:\n#> lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> XO(Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> XOeduc         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> XOexper        0.0419205  0.0136176   3.078  0.00222 ** \n#> XOI(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> XOcity         0.0510492  0.0692414   0.737  0.46137    \n#> imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.7734, Adjusted R-squared:  0.7702 \n#> F-statistic:   240 on 6 and 422 DF,  p-value: < 2.2e-16\n# ML estimation of selection model\nml1 = selection(\n    selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ,\n    outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city,\n    data = Mroz87\n) \nsummary(ml1)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 3 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -914.0777 \n#> 753 observations (325 censored and 428 observed)\n#> 13 free parameters (df = 740)\n#> Probit selection equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.1484037  1.4109302  -2.940 0.003382 ** \n#> age          0.1842132  0.0658041   2.799 0.005253 ** \n#> I(age^2)    -0.0023925  0.0007664  -3.122 0.001868 ** \n#> kids        -0.1488158  0.0384888  -3.866 0.000120 ***\n#> huswage     -0.0434253  0.0123229  -3.524 0.000451 ***\n#> educ         0.1255639  0.0229229   5.478 5.91e-08 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.5814781  0.3052031  -1.905  0.05714 .  \n#> educ         0.1078481  0.0172998   6.234 7.63e-10 ***\n#> exper        0.0415752  0.0133269   3.120  0.00188 ** \n#> I(exper^2)  -0.0008125  0.0003974  -2.044  0.04129 *  \n#> city         0.0522990  0.0682652   0.766  0.44385    \n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.66326    0.02309  28.729   <2e-16 ***\n#> rho    0.05048    0.23169   0.218    0.828    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n# summary(ml1$twoStep)\nmyprob <- probit(lfp ~ age + I( age^2 ) + kids + huswage + educ, \n                 # x = TRUE, \n                 # iterlim = 30, \n                 data = Mroz87)\nsummary(myprob)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                Estimate  Std. error t value   Pr(> t)    \n#> (Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> age          0.18608901  0.06517476  2.8552  0.004301 ** \n#> I(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> kids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> huswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> educ         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\n\nimr <- invMillsRatio(myprob)\nMroz87$IMR1 <- imr$IMR1\n\nmanually_est <- lm(log(wage) ~ educ + exper + I( exper^2 ) + city + IMR1,\n                   data = Mroz87, \n                   subset = (lfp == 1))\n\nsummary(manually_est)\n#> \n#> Call:\n#> lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, \n#>     data = Mroz87, subset = (lfp == 1))\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> IMR1         0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.1582, Adjusted R-squared:  0.1482 \n#> F-statistic: 15.86 on 5 and 422 DF,  p-value: 2.505e-14\nprobit_selection <-\n    glm(lfp ~ age + I( age^2 ) + kids + huswage + educ,\n        data = Mroz87,\n        family = binomial(link = 'probit'))\n\n# library(fixest)\n# probit_selection <-\n#     fixest::feglm(lfp ~ age + I( age^2 ) + kids + huswage + educ,\n#         data = Mroz87,\n#         family = binomial(link = 'probit'))\n\nprobit_lp <- -predict(probit_selection)\ninv_mills <- dnorm(probit_lp) / (1 - pnorm(probit_lp))\nMroz87$inv_mills <- inv_mills\n\n\nprobit_outcome <-\n    glm(\n        log(wage) ~ educ + exper + I(exper ^ 2) + city + inv_mills,\n        data = Mroz87,\n        subset = (lfp == 1)\n    )\nsummary(probit_outcome)\n#> \n#> Call:\n#> glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + \n#>     inv_mills, data = Mroz87, subset = (lfp == 1))\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -3.09494  -0.30953   0.05341   0.36530   2.34770  \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143383  0.3768798  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> inv_mills    0.0551179  0.2111918   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.4454809)\n#> \n#>     Null deviance: 223.33  on 427  degrees of freedom\n#> Residual deviance: 187.99  on 422  degrees of freedom\n#> AIC: 876.49\n#> \n#> Number of Fisher Scoring iterations: 2\nlibrary(\"stargazer\")\nlibrary(\"Mediana\")\nlibrary(\"plm\")\n# function to calculate corrected SEs for regression \ncse = function(reg) {\n  rob = sqrt(diag(vcovHC(reg, type = \"HC1\")))\n  return(rob)\n}\n\n# stargazer table\nstargazer(\n    # ols1,\n    heck1,\n    ml1,\n    # manually_est,\n    \n    se = list(cse(ols1), NULL, NULL),\n    title = \"Married women's wage regressions\",\n    type = \"text\",\n    df = FALSE,\n    digits = 4,\n    selection.equation = T\n)\n#> \n#> Married women's wage regressions\n#> ===================================================\n#>                           Dependent variable:      \n#>                     -------------------------------\n#>                                   lfp              \n#>                         Heckman        selection   \n#>                        selection                   \n#>                           (1)             (2)      \n#> ---------------------------------------------------\n#> age                    0.1861***       0.1842***   \n#>                                        (0.0658)    \n#>                                                    \n#> I(age2)                 -0.0024       -0.0024***   \n#>                                        (0.0008)    \n#>                                                    \n#> kids                  -0.1496***      -0.1488***   \n#>                                        (0.0385)    \n#>                                                    \n#> huswage                 -0.0430       -0.0434***   \n#>                                        (0.0123)    \n#>                                                    \n#> educ                    0.1250         0.1256***   \n#>                        (0.0130)        (0.0229)    \n#>                                                    \n#> Constant              -4.1815***      -4.1484***   \n#>                        (0.2032)        (1.4109)    \n#>                                                    \n#> ---------------------------------------------------\n#> Observations              753             753      \n#> R2                      0.1582                     \n#> Adjusted R2             0.1482                     \n#> Log Likelihood                         -914.0777   \n#> rho                     0.0830      0.0505 (0.2317)\n#> Inverse Mills Ratio 0.0551 (0.2099)                \n#> ===================================================\n#> Note:                   *p<0.1; **p<0.05; ***p<0.01\n\n\nstargazer(\n    ols1,\n    # heck1,\n    # ml1,\n    manually_est,\n    \n    se = list(cse(ols1), NULL, NULL),\n    title = \"Married women's wage regressions\",\n    type = \"text\",\n    df = FALSE,\n    digits = 4,\n    selection.equation = T\n)\n#> \n#> Married women's wage regressions\n#> ================================================\n#>                         Dependent variable:     \n#>                     ----------------------------\n#>                              log(wage)          \n#>                          (1)            (2)     \n#> ------------------------------------------------\n#> educ                  0.1057***      0.1092***  \n#>                        (0.0130)      (0.0197)   \n#>                                                 \n#> exper                 0.0411***      0.0419***  \n#>                        (0.0154)      (0.0136)   \n#>                                                 \n#> I(exper2)              -0.0008*      -0.0008**  \n#>                        (0.0004)      (0.0004)   \n#>                                                 \n#> city                    0.0542        0.0510    \n#>                        (0.0653)      (0.0692)   \n#>                                                 \n#> IMR1                                  0.0551    \n#>                                      (0.2112)   \n#>                                                 \n#> Constant              -0.5308***      -0.6143   \n#>                        (0.2032)      (0.3769)   \n#>                                                 \n#> ------------------------------------------------\n#> Observations             428            428     \n#> R2                      0.1581        0.1582    \n#> Adjusted R2             0.1501        0.1482    \n#> Residual Std. Error     0.6667        0.6674    \n#> F Statistic           19.8561***    15.8635***  \n#> ================================================\n#> Note:                *p<0.1; **p<0.05; ***p<0.01"},{"path":"endogeneity.html","id":"example-2-2","chapter":"33 Endogeneity","heading":"33.2.1.2 Example 2","text":"code R package sampleSelectionwithout exclusion restriction, generate yo using xs instead xo.can see estimates still unbiased standard errors substantially larger. exclusion restriction (.e., independent information selection process) certain identifying power desire. Hence, ’s better different set variable selection process interested equation. Without exclusion restriction, solely rely functional form identification.","code":"\nset.seed(0)\nlibrary(\"sampleSelection\")\nlibrary(\"mvtnorm\")\n# bivariate normal disturbances\neps <-\n    rmvnorm(500, c(0, 0), matrix(c(1, -0.7, -0.7, 1), 2, 2)) \n\n# uniformly distributed explanatory variable \n# (vectors of explanatory variables for the selection)\nxs <- runif(500)\n\n# probit data generating process\nys <- xs + eps[, 1] > 0 \n\n# vectors of explanatory variables for outcome equation\nxo <- runif(500) \nyoX <- xo + eps[, 2] # latent outcome\nyo <- yoX * (ys > 0) # observable outcome\n# true intercepts = 0 and our true slopes = 1\n# xs and xo are independent. \n# Hence, exclusion restriction is fulfilled\nsummary(selection(ys ~ xs, yo ~ xo))\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 5 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -712.3163 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  \n#> xs            1.3377     0.2014   6.642 8.18e-11 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.0002265  0.1294178  -0.002    0.999    \n#> xo           0.7299070  0.1635925   4.462 1.01e-05 ***\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma   0.9190     0.0574  16.009  < 2e-16 ***\n#> rho    -0.5392     0.1521  -3.544 0.000431 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nyoX <- xs + eps[,2]\nyo <- yoX*(ys > 0)\nsummary(selection(ys ~ xs, yo ~ xs))\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 14 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -712.8298 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  \n#> xs            1.2907     0.2085   6.191 1.25e-09 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  -0.5499     0.5644  -0.974  0.33038   \n#> xs            1.3987     0.4482   3.120  0.00191 **\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.85091    0.05352  15.899   <2e-16 ***\n#> rho   -0.13226    0.72684  -0.182    0.856    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"endogeneity.html","id":"tobit-5","chapter":"33 Endogeneity","heading":"33.2.2 Tobit-5","text":"Also known switching regression model\nCondition: least one variable X selection process included observed process. Used separate models participants, non-participants.exclusion restriction fulfilled \\(x\\)’s independent.estimates close true values.Example functional form misspecificationAlthough still exclusion restriction (xo1 xo2 independent), now problems intercepts (.e., statistically significantly different true values zero), convergence problems.don’t exclusion restriction, larger variance xsUsually converge. Even , results may seriously biased.NoteThe log-likelihood function models might globally concave. Hence, might converge, converge local maximum. combat , can useDifferent starting valueDifferent maximization methods.refer [Non-linear Least Squares] suggestions.","code":"\nset.seed(0)\nvc <- diag(3)\nvc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)\nvc[upper.tri(vc)] <- vc[lower.tri(vc)]\n\n# 3 disturbance vectors by a 3-dimensional normal distribution\neps <- rmvnorm(500, c(0,0,0), vc) \nxs <- runif(500) # uniformly distributed on [0, 1]\nys <- xs + eps[,1] > 0\nxo1 <- runif(500) # uniformly distributed on [0, 1]\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(500) # uniformly distributed on [0, 1]\nyo2 <- xo2 + eps[,3]\n# one selection equation and a list of two outcome equations\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2))) \n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 11 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -895.8201 \n#> 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)\n#> 10 free parameters (df = 490)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1550     0.1051  -1.474    0.141    \n#> xs            1.1408     0.1785   6.390 3.86e-10 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02708    0.16395   0.165    0.869    \n#> xo1          0.83959    0.14968   5.609  3.4e-08 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.1583     0.1885   0.840    0.401    \n#> xo2           0.8375     0.1707   4.908 1.26e-06 ***\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.93191    0.09211  10.118   <2e-16 ***\n#> sigma2  0.90697    0.04434  20.455   <2e-16 ***\n#> rho1    0.88988    0.05353  16.623   <2e-16 ***\n#> rho2    0.17695    0.33139   0.534    0.594    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nset.seed(5)\neps <- rmvnorm(1000, rep(0, 3), vc)\neps <- eps^2 - 1 # subtract 1 in order to get the mean zero disturbances\n\n# interval [−1, 0] to get an asymmetric distribution over observed choices\nxs <- runif(1000, -1, 0) \nys <- xs + eps[,1] > 0\nxo1 <- runif(1000)\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(1000)\nyo2 <- xo2 + eps[,3]\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim=20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 3: Last step could not find a value above the current.\n#> Boundary of parameter space?  \n#> Consider switching to a more robust optimisation method temporarily.\n#> Log-Likelihood: -1665.936 \n#> 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.53698    0.05808  -9.245  < 2e-16 ***\n#> xs           0.31268    0.09395   3.328 0.000906 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.70679    0.03573  -19.78   <2e-16 ***\n#> xo1          0.91603    0.05626   16.28   <2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   0.1446        NaN     NaN      NaN  \n#> xo2           1.1196     0.5014   2.233   0.0258 *\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.67770    0.01760   38.50   <2e-16 ***\n#> sigma2  2.31432    0.07615   30.39   <2e-16 ***\n#> rho1   -0.97137        NaN     NaN      NaN    \n#> rho2    0.17039        NaN     NaN      NaN    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nset.seed(6)\nxs <- runif(1000, -1, 1)\nys <- xs + eps[,1] > 0\nyo1 <- xs + eps[,2]\nyo2 <- xs + eps[,3]\nsummary(tmp <- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 16 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -1936.431 \n#> 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.3528     0.0424  -8.321 2.86e-16 ***\n#> xs            0.8354     0.0756  11.050  < 2e-16 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.55448    0.06339  -8.748   <2e-16 ***\n#> xs           0.81764    0.06048  13.519   <2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   0.6457     0.4994   1.293    0.196\n#> xs            0.3520     0.3197   1.101    0.271\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.59187    0.01853  31.935   <2e-16 ***\n#> sigma2  1.97257    0.07228  27.289   <2e-16 ***\n#> rho1    0.15568    0.15914   0.978    0.328    \n#> rho2   -0.01541    0.23370  -0.066    0.947    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"endogeneity.html","id":"pattern-mixture-models","chapter":"33 Endogeneity","heading":"33.2.2.0.1 Pattern-Mixture Models","text":"compared Heckman’s model assumes value missing data predetermined, pattern-mixture models assume missingness affect distribution variable interest (e.g., Y)read , can check NCSU, stefvanbuuren.","code":""},{"path":"other-biases.html","id":"other-biases","chapter":"34 Other Biases","heading":"34 Other Biases","text":"econometrics, main objective often uncover causal relationships. However, coefficient estimates can affected various biases. ’s list common biases can affect coefficient estimates:’ve covered far (see Linear Regression Endogeneity):Omitted Variable Bias (OVB):\nArises variable affects dependent variable correlated independent variable left regression.\nOmitted Variable Bias (OVB):Arises variable affects dependent variable correlated independent variable left regression.Endogeneity Bias:\nOccurs error term correlated independent variable. can due :\nSimultaneity: dependent variable simultaneously affects independent variable.\nOmitted variables.\nMeasurement error independent variable.\n\nEndogeneity Bias:Occurs error term correlated independent variable. can due :\nSimultaneity: dependent variable simultaneously affects independent variable.\nOmitted variables.\nMeasurement error independent variable.\nOccurs error term correlated independent variable. can due :Simultaneity: dependent variable simultaneously affects independent variable.Simultaneity: dependent variable simultaneously affects independent variable.Omitted variables.Omitted variables.Measurement error independent variable.Measurement error independent variable.Measurement Error:\nBias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.\nMeasurement Error:Bias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.Sample Selection Bias:\nArises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.\nSample Selection Bias:Arises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.Simultaneity Bias (Reverse Causality):\nHappens dependent variable causes changes independent variable, leading two-way causation.\nSimultaneity Bias (Reverse Causality):Happens dependent variable causes changes independent variable, leading two-way causation.Multicollinearity:\nbias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.\nMulticollinearity:bias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.Specification Errors:\nArise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.\nSpecification Errors:Arise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.Autocorrelation (Serial Correlation):\nOccurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.\nAutocorrelation (Serial Correlation):Occurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.Heteroskedasticity:\nOccurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.\nHeteroskedasticity:Occurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.section, mention biases may encounter conducting researchIntroduced data aggregated, analysis conducted aggregate level rather individual level.[Survivorship Bias] (much related Sample Selection):Arises sample includes “survivors” “passed” certain threshold. Common finance funds firms “survive” analyzed.bias econometric estimation per se, relevant context empirical studies. refers tendency journals publish significant positive results, leading overrepresentation results literature.","code":""},{"path":"other-biases.html","id":"aggregation-bias","chapter":"34 Other Biases","heading":"34.1 Aggregation Bias","text":"Aggregation bias, also known ecological fallacy, refers error introduced data aggregated analysis conducted aggregate level, rather individual level. can especially problematic econometrics, analysts often concerned understanding individual behavior.relationship variables different aggregate level individual level, aggregation bias can result. bias arises inferences individual behaviors made based aggregate data.Example: Suppose data individuals’ incomes personal consumption. individual level, ’s possible income rises, consumption also rises. However, aggregate data , say, neighborhood level, neighborhoods diverse income levels might similar average consumption due unobserved factors.Step 1: Create individual level dataThis show significant positive relationship income consumption.Step 2: Aggregate data ‘neighborhood’ levelIf aggregation bias present, coefficient income aggregate regression might different coefficient individual regression, even individual relationship significant strong.plots, can see relationship individual level, neighborhood colored differently first plot. second plot shows aggregate data, point now represents whole neighborhood.Direction Bias: direction aggregation bias isn’t predetermined. depends underlying relationship data distribution. cases, aggregation might attenuate (reduce) relationship, cases, might exaggerate .Relation Biases: Aggregation bias closely related several biases econometrics:Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.Measurement Error: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.Measurement Error: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.","code":"\nset.seed(123)\n\n# Generate data for 1000 individuals\nn <- 1000\n\nincome <- rnorm(n, mean = 50, sd = 10)\nconsumption <- 0.5 * income + rnorm(n, mean = 0, sd = 5)\n\n# Individual level regression\nindividual_lm <- lm(consumption ~ income)\nsummary(individual_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -15.1394  -3.4572   0.0213   3.5436  16.4557 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1.99596    0.82085  -2.432   0.0152 *  \n#> income       0.54402    0.01605  33.888   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.032 on 998 degrees of freedom\n#> Multiple R-squared:  0.535,  Adjusted R-squared:  0.5346 \n#> F-statistic:  1148 on 1 and 998 DF,  p-value: < 2.2e-16\n# Assume 100 neighborhoods with 10 individuals each\nn_neighborhoods <- 100\n\ndf <- data.frame(income, consumption)\ndf$neighborhood <- rep(1:n_neighborhoods, each = n / n_neighborhoods)\n\naggregate_data <- aggregate(. ~ neighborhood, data = df, FUN = mean)\n\n# Aggregate level regression\naggregate_lm <- lm(consumption ~ income, data = aggregate_data)\nsummary(aggregate_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income, data = aggregate_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.4517 -0.9322 -0.0826  1.0556  3.5728 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.94338    2.60699  -1.896   0.0609 .  \n#> income       0.60278    0.05188  11.618   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.54 on 98 degrees of freedom\n#> Multiple R-squared:  0.5794, Adjusted R-squared:  0.5751 \n#> F-statistic:   135 on 1 and 98 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Individual scatterplot\np1 <- ggplot(df, aes(x = income, y = consumption)) +\n    geom_point(aes(color = neighborhood), alpha = 0.6) +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Individual Level Data\") +\n    causalverse::ama_theme()\n\n# Aggregate scatterplot\np2 <- ggplot(aggregate_data, aes(x = income, y = consumption)) +\n    geom_point(color = \"red\") +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Aggregate Level Data\") +\n    causalverse::ama_theme()\n\n# print(p1)\n# print(p2)\n\ngridExtra::grid.arrange(grobs = list(p1, p2), ncol = 2)"},{"path":"other-biases.html","id":"simpsons-paradox","chapter":"34 Other Biases","heading":"34.1.1 Simpson’s Paradox","text":"Simpson’s Paradox, also known Yule-Simpson effect, phenomenon probability statistics trend appears different groups data disappears reverses groups combined. ’s striking example aggregated data can sometimes provide misleading representation actual situation.Illustration Simpson’s Paradox:Consider hypothetical scenario involving two hospitals: Hospital Hospital B. want analyze success rates treatments hospitals. break data severity cases (.e., minor cases vs. major cases):Hospital :\nMinor cases: 95% success rate\nMajor cases: 80% success rate\nHospital :Minor cases: 95% success rateMinor cases: 95% success rateMajor cases: 80% success rateMajor cases: 80% success rateHospital B:\nMinor cases: 90% success rate\nMajor cases: 85% success rate\nHospital B:Minor cases: 90% success rateMinor cases: 90% success rateMajor cases: 85% success rateMajor cases: 85% success rateFrom breakdown, Hospital appears better treating minor major cases since higher success rate categories.However, let’s consider overall success rates without considering case severity:Hospital : 83% overall success rateHospital : 83% overall success rateHospital B: 86% overall success rateHospital B: 86% overall success rateSuddenly, Hospital B seems better overall. surprising reversal happens two hospitals might handle different proportions minor major cases. example, Hospital treats many major cases (lower success rates) Hospital B, can drag overall success rate.Causes:Simpson’s Paradox can arise due :lurking confounding variable wasn’t initially considered (example, severity medical cases).lurking confounding variable wasn’t initially considered (example, severity medical cases).Different group sizes, one group might much larger , influencing aggregate results.Different group sizes, one group might much larger , influencing aggregate results.Implications:Simpson’s Paradox highlights dangers interpreting aggregated data without considering potential underlying sub-group structures. underscores importance disaggregating data aware context ’s analyzed.Relation Aggregation BiasIn extreme case, aggregation bias can reverse coefficient sign relationship interest (.e., Simpson’s Paradox).Example: Suppose studying effect new study technique student grades. two groups students: used new technique (treatment = 1) (treatment = 0). want see using new study technique related higher grades.Let’s assume grades influenced starting ability students. Perhaps sample, many high-ability students didn’t use new technique (felt didn’t need ), many low-ability students .’s setup:High-ability students tend high grades regardless technique.High-ability students tend high grades regardless technique.new technique positive effect grades, masked fact many low-ability students use .new technique positive effect grades, masked fact many low-ability students use .simulation:overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.classic example Simpson’s Paradox: within ability group, technique appears beneficial, data aggregated, effect seems negative distribution technique across ability groups.","code":"\nset.seed(123)\n\n# Generate data for 1000 students\nn <- 1000\n\n# 500 students are of high ability, 500 of low ability\nability <- c(rep(\"high\", 500), rep(\"low\", 500))\n\n# High ability students are less likely to use the technique\ntreatment <-\n  ifelse(ability == \"high\", rbinom(500, 1, 0.2), rbinom(500, 1, 0.8))\n\n# Grades are influenced by ability and treatment (new technique),\n# but the treatment has opposite effects based on ability.\ngrades <-\n  ifelse(\n    ability == \"high\",\n    rnorm(500, mean = 85, sd = 5) + treatment * -3,\n    rnorm(500, mean = 60, sd = 5) + treatment * 5\n  )\n\ndf <- data.frame(ability, treatment, grades)\n\n# Regression without considering ability\noverall_lm <- lm(grades ~ factor(treatment), data = df)\nsummary(overall_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -33.490  -4.729   0.986   6.368  25.607 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         80.0133     0.4373   183.0   <2e-16 ***\n#> factor(treatment)1 -11.7461     0.6248   -18.8   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.877 on 998 degrees of freedom\n#> Multiple R-squared:  0.2615, Adjusted R-squared:  0.2608 \n#> F-statistic: 353.5 on 1 and 998 DF,  p-value: < 2.2e-16\n\n# Regression within ability groups\nhigh_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"high\",])\nlow_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"low\",])\nsummary(high_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"high\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -14.2156  -3.4813   0.1186   3.4952  13.2919 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         85.1667     0.2504 340.088  < 2e-16 ***\n#> factor(treatment)1  -3.9489     0.5776  -6.837 2.37e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.046 on 498 degrees of freedom\n#> Multiple R-squared:  0.08581,    Adjusted R-squared:  0.08398 \n#> F-statistic: 46.75 on 1 and 498 DF,  p-value: 2.373e-11\nsummary(low_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"low\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.3717  -3.5413   0.1097   3.3531  17.0568 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         59.8950     0.4871 122.956   <2e-16 ***\n#> factor(treatment)1   5.2979     0.5474   9.679   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.968 on 498 degrees of freedom\n#> Multiple R-squared:  0.1583, Adjusted R-squared:  0.1566 \n#> F-statistic: 93.68 on 1 and 498 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Scatterplot for overall data\np1 <-\n  ggplot(df, aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Overall Effect of Study Technique on Grades\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for high-ability students\np2 <-\n  ggplot(df[df$ability == \"high\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (High Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for low-ability students\np3 <-\n  ggplot(df[df$ability == \"low\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (Low Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# print(p1)\n# print(p2)\n# print(p3)\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), ncol = 1)"},{"path":"other-biases.html","id":"contamination-bias","chapter":"34 Other Biases","heading":"34.2 Contamination Bias","text":"Goldsmith-Pinkham, Hull, Kolesár (2022) show regressions multiple treatments flexible controls often fail estimate convex averages heterogeneous treatment effects, resulting contamination non-convex averages treatments’ effects.3 estimation methods avoid bias find significant contamination bias observational studies, experimental studies showing less due lower variability propensity scores.","code":""},{"path":"other-biases.html","id":"survivorship-bias","chapter":"34 Other Biases","heading":"34.3 Survivorship Bias","text":"Survivorship bias refers logical error concentrating entities made past selection process overlooking didn’t, typically lack visibility. can skew results lead overly optimistic conclusions.Example: analyze success companies based ones still business today, ’d miss insights failed. give distorted view makes successful company, wouldn’t account attributes didn’t succeed.Relation Biases:Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Using histogram visualize distribution earnings, highlighting “survivors”.plot, “True Avg” might lower “Survivor Avg”, indicating looking survivors, overestimate average earnings.Remedies:Awareness: Recognizing potential survivorship bias first step.Awareness: Recognizing potential survivorship bias first step.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.Sensitivity Analysis: Test sensitive results assumptions non-survivors.Sensitivity Analysis: Test sensitive results assumptions non-survivors.","code":"\nset.seed(42)\n\n# Generating data for 100 companies\nn <- 100\n\n# Randomly generate earnings; assume true average earnings is 50\nearnings <- rnorm(n, mean = 50, sd = 10)\n\n# Threshold for bankruptcy\nthreshold <- 40\n\n# Only companies with earnings above the threshold \"survive\"\nsurvivor_earnings <- earnings[earnings > threshold]\n\n# Average earnings for all companies vs. survivors\ntrue_avg <- mean(earnings)\nsurvivor_avg <- mean(survivor_earnings)\n\ntrue_avg\n#> [1] 50.32515\nsurvivor_avg\n#> [1] 53.3898\nlibrary(ggplot2)\n\ndf <- data.frame(earnings)\n\np <- ggplot(df, aes(x = earnings)) +\n  geom_histogram(\n    binwidth = 2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(aes(xintercept = true_avg, color = \"True Avg\"),\n             linetype = \"dashed\",\n             size = 1) +\n  geom_vline(\n    aes(xintercept = survivor_avg, color = \"Survivor Avg\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = c(\"True Avg\" = \"blue\", \"Survivor Avg\" = \"red\"),\n                     name = \"Average Type\") +\n  labs(title = \"Distribution of Company Earnings\",\n       x = \"Earnings\",\n       y = \"Number of Companies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"other-biases.html","id":"publication-bias","chapter":"34 Other Biases","heading":"34.4 Publication Bias","text":"Publication bias occurs results studies influence likelihood published. Typically, studies significant, positive, sensational results likely published non-significant negative results. can skew perceived effectiveness results researchers conduct meta-analyses literature reviews, leading draw inaccurate conclusions.Example: Imagine pharmaceutical research. 10 studies done new drug, 2 show positive effect 8 show effect, 2 positive studies get published, later review literature might erroneously conclude drug effective.Relation Biases:Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Let’s simulate experiment new treatment. ’ll assume treatment effect, due random variation, studies show significant positive negative effects.Using histogram visualize distribution study results, highlighting “published” studies.plot might show “True Avg Effect” around zero, “Published Avg Effect” likely higher lower, depending studies happen significant results simulation.Remedies:Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).","code":"\nset.seed(42)\n\n# Number of studies\nn <- 100\n\n# Assuming no real effect (effect size = 0)\ntrue_effect <- 0\n\n# Random variation in results\nresults <- rnorm(n, mean = true_effect, sd = 1)\n\n# Only \"significant\" results get published \n# (arbitrarily defining significant as abs(effect) > 1.5)\npublished_results <- results[abs(results) > 1.5]\n\n# Average effect for all studies vs. published studies\ntrue_avg_effect <- mean(results)\npublished_avg_effect <- mean(published_results)\n\ntrue_avg_effect\n#> [1] 0.03251482\npublished_avg_effect\n#> [1] -0.3819601\nlibrary(ggplot2)\n\ndf <- data.frame(results)\n\np <- ggplot(df, aes(x = results)) +\n  geom_histogram(\n    binwidth = 0.2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(\n    aes(xintercept = true_avg_effect,\n        color = \"True Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  geom_vline(\n    aes(xintercept = published_avg_effect,\n        color = \"Published Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\n      \"True Avg Effect\" = \"blue\",\n      \"Published Avg Effect\" = \"red\"\n    ),\n    name = \"Effect Type\"\n  ) +\n  labs(title = \"Distribution of Study Results\",\n       x = \"Effect Size\",\n       y = \"Number of Studies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"controls.html","id":"controls","chapter":"35 Controls","heading":"35 Controls","text":"section follows (Cinelli, Forney, Pearl 2022) codeTraditional literature usually considers adding additional control variables harmless analysis.specifically, problem prevalent review process. Reviewers ask authors add variables “control” variable, can asked limited rationale. Rarely ever see reviewer asks author remove variables see behavior variable interest (also related Coefficient stability).However, adding controls good limited cases.","code":"\nlibrary(dagitty)\nlibrary(ggdag)"},{"path":"controls.html","id":"bad-controls","chapter":"35 Controls","heading":"35.1 Bad Controls","text":"","code":""},{"path":"controls.html","id":"m-bias","chapter":"35 Controls","heading":"35.1.1 M-bias","text":"Traditional textbooks (G. W. Imbens Rubin 2015; J. D. Angrist Pischke 2009) consider \\(Z\\) good control ’s pre-treatment variable, correlates treatment outcome.prevalent Matching Methods, recommended include “pre-treatment” variables.However, bad control opens back-door path \\(Z \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\)Even though \\(Z\\) can correlate \\(X\\) \\(Y\\) well, ’s confounder.Controlling \\(Z\\) can bias \\(X \\Y\\) estimate, opens colliding path \\(X \\leftarrow U_1 \\rightarrow Z \\leftarrow U_2 \\leftarrow Y\\)Table 35.1:  Another worse variation isYou can’t much case.don’t control \\(Z\\), open back-door path \\(X \\leftarrow U_1 \\Z \\Y\\), unadjusted estimate biasedIf don’t control \\(Z\\), open back-door path \\(X \\leftarrow U_1 \\Z \\Y\\), unadjusted estimate biasedIf control \\(Z\\), open backdoor path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), adjusted estimate also biasedIf control \\(Z\\), open backdoor path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), adjusted estimate also biasedHence, identify causal effect case.can sensitivity analyses examine (Cinelli et al. 2019; Cinelli Hazlett 2020)plausible bounds strength direct effect \\(Z \\Y\\)strength effects latent variables","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u1->x; u1->z; u2->z; u2->y}\")\n\n# set u as latent\nlatents(model) <- c(\"u1\", \"u2\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(x = c(\n    x = 1,\n    u1 = 1,\n    z = 2,\n    u2 = 3,\n    y = 3\n),\ny = c(\n    x = 1,\n    u1 = 2,\n    z = 1.5,\n    u2 = 2,\n    y = 1\n))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nu1 <- rnorm(n)\nu2 <- rnorm(n)\nz <- u1 + u2 + rnorm(n)\nx <- u1 + rnorm(n)\ncausal_coef <- 2\ny <- causal_coef * x - 4*u2 + rnorm(n)\n\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u1->x; u1->z; u2->z; u2->y; z->y}\")\n\n# set u as latent\nlatents(model) <- c(\"u1\", \"u2\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, u1=1, z=2, u2=3, y=3),\n  y = c(x=1, u1=2, z=1.5, u2=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()"},{"path":"controls.html","id":"bias-amplification-1","chapter":"35 Controls","heading":"35.1.2 Bias Amplification","text":"Controlling Z amplifies omitted variable biasTable 35.2:  ","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->x; u->y; z->x}\")\n\n# set u as latent\nlatents(model) <- c(\"u\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(z=1, x=2, u=3, y=4),\n  y = c(z=1, x=1, u=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nu <- rnorm(n)\nx <- 2*z + u + rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"overcontrol-bias","chapter":"35 Controls","heading":"35.1.3 Overcontrol bias","text":"Sometimes, similar controlling variables proxy dependent variable.X proxy Z (.e., mediator Z Y), controlling Z badTable 35.3:  Now see \\(Z\\) significant, technically true, interested causal coefficient \\(X\\) \\(Y\\).Another setting overcontrol bias isTable 35.4:  Another setting bias isTable 35.5:  total effect \\(X\\) \\(Y\\) biased (.e., \\(1.01 \\approx 1.48 - 0.47\\)).Controlling Z fail identify direct effect \\(X\\) \\(Y\\) opens biasing path \\(X \\rightarrow Z \\leftarrow U \\rightarrow Y\\)","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->z; z->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=1, z=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nz <- x + rnorm(n)\ny <- z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->m; m->z; m->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, m=2, z=2, y=3),\n  y = c(x=2, m=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nm <- x + rnorm(n)\nz <- m + rnorm(n)\ny <- m + rnorm(n)\n\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->z; z->y; u->z; u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, y=4),\n  y = c(x=1, z=1, u=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nset.seed(1)\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u + rnorm(n)\ny <- z + u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"selection-bias","chapter":"35 Controls","heading":"35.1.4 Selection Bias","text":"Also known “collider stratification bias”Adjusting \\(Z\\) opens colliding path \\(X \\Z \\leftarrow U \\Y\\)Table 35.6:  Another setting isControlling \\(Z\\) opens colliding path \\(X \\Z \\leftarrow Y\\)Table 35.7:  ","code":"\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; u->z;u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=2, y=3),\n  y = c(x=3, z=2, u=4, y=3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u +  rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; y->z}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- x + y + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"case-control-bias","chapter":"35 Controls","heading":"35.1.5 Case-control Bias","text":"Controlling \\(Z\\) opens virtual collider (descendant collider).However, \\(X\\) truly causal effect \\(Y\\). , controlling \\(Z\\) valid testing whether effect \\(X\\) \\(Y\\) 0 X d-separated \\(Y\\) regardless adjusting \\(Z\\)Table 35.8:  ","code":"\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; y->z}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- y + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"good-controls","chapter":"35 Controls","heading":"35.2 Good Controls","text":"","code":""},{"path":"controls.html","id":"omitted-variable-bias-correction","chapter":"35 Controls","heading":"35.2.1 Omitted Variable Bias Correction","text":"\\(Z\\) can block back-door paths.Unadjusted estimate biasedadjusting \\(Z\\) blocks backdoor pathTable 35.9:  Unadjusted estimate biasedadjusting \\(Z\\) blocks backdoor door path due \\(U\\)Table 35.10:  Even though \\(Z\\) significant, give causal interpretation.Table 17.1:  Even though \\(Z\\) significant, give causal interpretation.Summary","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{x->y; z->x; z->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, y=3, z=2),\n  y = c(x=1, y=1, z=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\ncausal_coef = 2\nbeta2 = 3\nx <- z + rnorm(n)\ny <- causal_coef * x + beta2 * z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Draw DAG\n\n# specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->x; u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\ncausal_coef = 2\nx <- z + rnorm(n)\ny <- causal_coef * x + u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Draw DAG\n\n# specify edges\nmodel <- dagitty(\"dag{x->y; u->z; u->x; z->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=3, u=2, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- u + rnorm(n)\ncausal_coef <- 2\ny     <- causal_coef * x + z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Model 1 \n\nmodel1 <- dagitty(\"dag{x->y; z->x; z->y}\")\n\n## coordinates for plotting\ncoordinates(model1) <-  list(\n  x = c(x=1, y=3, z=2),\n  y = c(x=1, y=1, z=2))\n\n\n\n# Model 2\n\n# specify edges\nmodel2 <- dagitty(\"dag{x->y; u->z; z->x; u->y}\")\n\n# set u as latent\nlatents(model2) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model2) <-  list(\n  x = c(x=1, z=2, u=3, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n\n\n# Model 3\n\n# specify edges\nmodel3 <- dagitty(\"dag{x->y; u->z; u->x; z->y}\")\n\n# set u as latent\nlatents(model3) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model3) <-  list(\n  x = c(x=1, z=3, u=2, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\npar(mfrow=c(1,3))\n\n## ggplot\nggdag(model1) + theme_dag()\n\n## ggplot\nggdag(model2) + theme_dag()\n\n## ggplot\nggdag(model3) + theme_dag()"},{"path":"controls.html","id":"omitted-variable-bias-in-mediation-correction","chapter":"35 Controls","heading":"35.2.2 Omitted Variable Bias in Mediation Correction","text":"Common causes \\(X\\) mediator (\\(X\\) \\(Y\\)) confound effect \\(X\\) \\(Y\\)\\(Z\\) confounder mediator \\(M\\) \\(X\\)Table 35.11:  Table 17.2:  Table 35.12:  Summary","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->x; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, m=3, y=4),\n  y = c(x=1, z=2, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nz     <- rnorm(n)\nx     <- z + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + z + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->x; x->m; u->m; m->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- z + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + u + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->m; x->m; u->x; m->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=3, u=2, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- u + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + z + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# model 4\nmodel4 <- dagitty(\"dag{x->y; z->x; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model4) <-  list(\n  x = c(x=1, z=2, m=3, y=4),\n  y = c(x=1, z=2, m=1, y=1))\n\n\n# model 5\nmodel5 <- dagitty(\"dag{x->y; u->z; z->x; x->m; u->m; m->y}\")\n\n# set u as latent\nlatents(model5) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model5) <-  list(\n  x = c(x=1, z=2, u=3, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n\n# model 6\n\nmodel6 <- dagitty(\"dag{x->y; u->z; z->m; x->m; u->x; m->y}\")\n\n# set u as latent\nlatents(model6) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model6) <-  list(\n  x = c(x=1, z=3, u=2, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\npar(mfrow=c(1,3))\n\n## ggplot\nggdag(model4) + theme_dag()\n\n## ggplot\nggdag(model5) + theme_dag()\n\n## ggplot\nggdag(model6) + theme_dag()"},{"path":"controls.html","id":"neutral-controls","chapter":"35 Controls","heading":"35.3 Neutral Controls","text":"","code":""},{"path":"controls.html","id":"good-predictive-controls","chapter":"35 Controls","heading":"35.3.1 Good Predictive Controls","text":"Good precisionControlling \\(Z\\) help hurt identification, can increase precision (.e., reducing SE)Table 35.13:  Similar coefficients, smaller SE controlling \\(Z\\)Another variation isTable 35.14:  Controlling \\(Z\\) can reduce SE","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\ny <- x + 2 * z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, m=2, y=3),\n  y = c(x=1, z=2, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\nm <- 2 * z + rnorm(n)\ny <- x + 2 * m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"good-selection-bias","chapter":"35 Controls","heading":"35.3.2 Good Selection Bias","text":"Unadjusted estimate unbiasedControlling Z can increase SEControlling Z W can help identify XTable 35.15:  ","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; z->w; u->w;u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, w=3, u=3, y=5),\n  y = c(x=3, z=2, w=1, u=4, y=3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + rnorm(n)\nw <- z + u + rnorm(n)\ny <- x - 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + w), lm(y ~ x + z + w))"},{"path":"controls.html","id":"bad-predictive-controls","chapter":"35 Controls","heading":"35.3.3 Bad Predictive Controls","text":"Table 35.16:  Similar coefficients, greater SE controlling \\(Z\\)Another variation isTable 35.17:  Worse SE controlling \\(Z\\) (\\(0.02 < 0.05\\))","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->x}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=1, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- 2 * z + rnorm(n)\ny <- x + 2 * rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=1, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nset.seed(1)\nn <- 1e4\nx <- rnorm(n)\nz <- 2 * x + rnorm(n)\ny <- x + 2 * rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"bad-selection-bias","chapter":"35 Controls","heading":"35.3.4 Bad Selection Bias","text":"post-treatment variables bad.Controlling \\(Z\\) neutral, might hurt precision causal effect.","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()"},{"path":"controls.html","id":"choosing-controls","chapter":"35 Controls","heading":"35.4 Choosing Controls","text":"providing causal diagram, deciding appropriateness controls automated.FusionFusionDAGittyDAGittyGuide choose confounders: T. J. VanderWeele (2019)cases ’s hard determine plausibility controls, might need analysis.sensemakr provides tools.simple cases, can follow simple rules thumb provided Steinmetz Block (2022) (p. 614, Fig 2)","code":"\nlibrary(pcalg)\nlibrary(dagitty)\nlibrary(causaleffect)\nlibrary(sensemakr)"},{"path":"mediation.html","id":"mediation","chapter":"36 Mediation","heading":"36 Mediation","text":"","code":""},{"path":"mediation.html","id":"traditional-approach","chapter":"36 Mediation","heading":"36.1 Traditional Approach","text":"Baron Kenny (1986) outdated step 1, still see original idea.3 regressionsStep 1: \\(X \\Y\\)Step 1: \\(X \\Y\\)Step 2: \\(X \\M\\)Step 2: \\(X \\M\\)Step 3: \\(X + M \\Y\\)Step 3: \\(X + M \\Y\\)\\(X\\) = independent (causal) variable\\(X\\) = independent (causal) variable\\(Y\\) = dependent (outcome) variable\\(Y\\) = dependent (outcome) variable\\(M\\) = mediating variable\\(M\\) = mediating variableNote: Originally, first path \\(X \\Y\\) suggested (Baron Kenny 1986) needs significant. cases indirect \\(X\\) \\(Y\\) without significant direct effect \\(X\\) \\(Y\\) (e.g., effect absorbed M, two counteracting effects \\(M_1, M_2\\) cancel effect).\\(c\\) total effectwhere\\(c'\\) = direct effect (effect \\(X\\) \\(Y\\) accounting indirect path)\\(c'\\) = direct effect (effect \\(X\\) \\(Y\\) accounting indirect path)\\(ab\\) = indirect effect\\(ab\\) = indirect effectHence,\\[\n\\begin{aligned}\n\\text{total effect} &= \\text{direct effect} + \\text{indirect effect} \\\\\nc &= c' + ab\n\\end{aligned}\n\\]However, simple equation hold cases ofModels latent variablesLogistic models (approximately). Hence, can calculate \\(c\\) total effect \\(c' + ab\\)Multi-level models (Bauer, Preacher, Gil 2006)measure mediation (.e., indirect effect),\\(1 - \\frac{c'}{c}\\) highly unstable (D. P. MacKinnon, Warsi, Dwyer 1995), especially cases \\(c\\) small (re* recommended)Product method: \\(\\times b\\)Difference method: \\(c- c'\\)linear models, following assumptions:unmeasured confound \\(X-Y\\), \\(X-M\\) \\(M-Y\\) relationships.unmeasured confound \\(X-Y\\), \\(X-M\\) \\(M-Y\\) relationships.\\(X \\\\rightarrow C\\) \\(C\\) confounder \\(M-Y\\) relationship\\(X \\\\rightarrow C\\) \\(C\\) confounder \\(M-Y\\) relationshipReliability: errors measurement \\(M\\) (also known reliability assumption) (can consider errors--variables models)Reliability: errors measurement \\(M\\) (also known reliability assumption) (can consider errors--variables models)Mathematically,\\[\nY = b_0 + b_1 X + \\epsilon\n\\]\\(b_1\\) need significant.examine effect \\(X\\) \\(M\\). step requires significant effect \\(X\\) \\(M\\) continue analysisMathematically,\\[\nM = b_0 + b_2 X + \\epsilon\n\\]\\(b_2\\) needs significant.step, want effect \\(M\\) \\(Y\\) “absorbs” direct effect \\(X\\) \\(Y\\) (least makes effect smaller).Mathematically,\\[\nY = b_0 + b_4 X + b_3 M + \\epsilon\n\\]\\(b_4\\) needs either smaller insignificant.Examine mediation effect (.e., whether significant)Sobel Test (Sobel 1982)Sobel Test (Sobel 1982)Joint Significance TestJoint Significance TestBootstrapping Shrout Bolger (2002) (preferable)Bootstrapping Shrout Bolger (2002) (preferable)Notes:Proximal mediation (\\(> b\\)) can lead multicollinearity reduce statistical power, whereas distal mediation (\\(b > \\)) preferred maximizing test power.Proximal mediation (\\(> b\\)) can lead multicollinearity reduce statistical power, whereas distal mediation (\\(b > \\)) preferred maximizing test power.ideal balance maximizing power mediation analysis involves slightly distal mediators (.e., path \\(b\\) somewhat larger path \\(\\)) (Hoyle 1999).ideal balance maximizing power mediation analysis involves slightly distal mediators (.e., path \\(b\\) somewhat larger path \\(\\)) (Hoyle 1999).Tests direct effects (c c’) lower power compared indirect effect (ab), making possible ab significant c , even cases seems complete mediation statistical evidence direct cause-effect relationship X Y without considering M (Kenny Judd 2014).Tests direct effects (c c’) lower power compared indirect effect (ab), making possible ab significant c , even cases seems complete mediation statistical evidence direct cause-effect relationship X Y without considering M (Kenny Judd 2014).testing \\(ab\\) offers power advantage \\(c’\\) effectively combines two tests. However, claims complete mediation based solely non-significance \\(c’\\) approached caution, emphasizing need sufficient sample size power, especially assessing partial mediation. one never make complete mediation claim (Hayes Scharkow 2013)testing \\(ab\\) offers power advantage \\(c’\\) effectively combines two tests. However, claims complete mediation based solely non-significance \\(c’\\) approached caution, emphasizing need sufficient sample size power, especially assessing partial mediation. one never make complete mediation claim (Hayes Scharkow 2013)","code":""},{"path":"mediation.html","id":"assumptions-2","chapter":"36 Mediation","heading":"36.1.1 Assumptions","text":"","code":""},{"path":"mediation.html","id":"direction","chapter":"36 Mediation","heading":"36.1.1.1 Direction","text":"Quick fix convincing: Measure \\(X\\) \\(M\\) \\(Y\\) prevent \\(M\\) \\(Y\\) causing \\(X\\); measure \\(M\\) \\(Y\\) avoid \\(Y\\) causing \\(M\\).Quick fix convincing: Measure \\(X\\) \\(M\\) \\(Y\\) prevent \\(M\\) \\(Y\\) causing \\(X\\); measure \\(M\\) \\(Y\\) avoid \\(Y\\) causing \\(M\\).\\(Y\\) may cause \\(M\\) feedback model.\nAssuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.\nE. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .\n\\(Y\\) may cause \\(M\\) feedback model.Assuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.Assuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.E. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .E. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .","code":""},{"path":"mediation.html","id":"interaction","chapter":"36 Mediation","heading":"36.1.1.2 Interaction","text":"M interact X affect Y, M mediator mediator (Baron Kenny 1986).M interact X affect Y, M mediator mediator (Baron Kenny 1986).Interaction \\(XM\\) always estimated.Interaction \\(XM\\) always estimated.interpretation interaction, see (T. VanderWeele 2015)interpretation interaction, see (T. VanderWeele 2015)","code":""},{"path":"mediation.html","id":"reliability","chapter":"36 Mediation","heading":"36.1.1.3 Reliability","text":"mediator contains measurement errors, \\(b, c'\\) biased. Possible fix: mediator = latent variable (loss power) (Ledgerwood Shrout 2011)\n\\(b\\) attenuated (closer 0)\n\\(c'\\) \noverestimated \\(ab >0\\)\nunderestiamted \\(ab<0\\)\n\nmediator contains measurement errors, \\(b, c'\\) biased. Possible fix: mediator = latent variable (loss power) (Ledgerwood Shrout 2011)\\(b\\) attenuated (closer 0)\\(b\\) attenuated (closer 0)\\(c'\\) \noverestimated \\(ab >0\\)\nunderestiamted \\(ab<0\\)\n\\(c'\\) isoverestimated \\(ab >0\\)overestimated \\(ab >0\\)underestiamted \\(ab<0\\)underestiamted \\(ab<0\\)treatment contains measurement errors, \\(,b\\) biased\n\\(\\) attenuated\n\\(b\\) \noverestimated \\(ac'>0\\)\nunderestimated \\(ac' <0\\)\n\ntreatment contains measurement errors, \\(,b\\) biased\\(\\) attenuated\\(\\) attenuated\\(b\\) \noverestimated \\(ac'>0\\)\nunderestimated \\(ac' <0\\)\n\\(b\\) isoverestimated \\(ac'>0\\)overestimated \\(ac'>0\\)underestimated \\(ac' <0\\)underestimated \\(ac' <0\\)outcome contains measurement errors,\nunstandardized, bias\nstandardized, attenuation bias\noutcome contains measurement errors,unstandardized, biasIf unstandardized, biasIf standardized, attenuation biasIf standardized, attenuation bias","code":""},{"path":"mediation.html","id":"confounding","chapter":"36 Mediation","heading":"36.1.1.4 Confounding","text":"Omitted variable bias can happen pair relationshipsOmitted variable bias can happen pair relationshipsTo deal problem, one can either use\nDesign Strategies\nStatistical Strategies\ndeal problem, one can either useDesign StrategiesDesign StrategiesStatistical StrategiesStatistical Strategies","code":""},{"path":"mediation.html","id":"design-strategies","chapter":"36 Mediation","heading":"36.1.1.4.1 Design Strategies","text":"Randomization treatment variable. possible, also mediatorRandomization treatment variable. possible, also mediatorControl confounder (still measureable observables)Control confounder (still measureable observables)","code":""},{"path":"mediation.html","id":"statistical-strategies","chapter":"36 Mediation","heading":"36.1.1.4.2 Statistical Strategies","text":"Instrumental variable treatment\nSpecifically confounder affecting \\(M-Y\\) pair, front-door adjustment possible variable completely mediates effect mediator outcome unaffected confounder.\nInstrumental variable treatmentSpecifically confounder affecting \\(M-Y\\) pair, front-door adjustment possible variable completely mediates effect mediator outcome unaffected confounder.Weighting methods (e.g., inverse propensity) See Heiss R code\nNeed strong ignorability assumption (.e.., confounders included measured without error (Westfall Yarkoni 2016)). fixable, can examined robustness checks.\nWeighting methods (e.g., inverse propensity) See Heiss R codeNeed strong ignorability assumption (.e.., confounders included measured without error (Westfall Yarkoni 2016)). fixable, can examined robustness checks.","code":""},{"path":"mediation.html","id":"indirect-effect-tests","chapter":"36 Mediation","heading":"36.1.2 Indirect Effect Tests","text":"","code":""},{"path":"mediation.html","id":"sobel-test","chapter":"36 Mediation","heading":"36.1.2.1 Sobel Test","text":"developed Sobel (1982)developed Sobel (1982)also known delta methodalso known delta methodnot recommend assumes indirect effect \\(b\\) normal distribution ’s (D. P. MacKinnon, Warsi, Dwyer 1995).recommend assumes indirect effect \\(b\\) normal distribution ’s (D. P. MacKinnon, Warsi, Dwyer 1995).Mediation can occur even direct indirect effects oppose , termed “inconsistent mediation” (D. P. MacKinnon, Fairchild, Fritz 2007). mediator acts suppressor variable.Mediation can occur even direct indirect effects oppose , termed “inconsistent mediation” (D. P. MacKinnon, Fairchild, Fritz 2007). mediator acts suppressor variable.Standard Error\\[\n\\sqrt{\\hat{b}^2 s_{\\hat{}} + \\hat{}^2 s_{b}^2}\n\\]test indirect effect \\[\nz = \\frac{\\hat{ab}}{\\sqrt{\\hat{b}^2 s_{\\hat{}} + \\hat{}^2 s_{b}^2}}\n\\]DisadvantagesAssume \\(\\) \\(b\\) independent.Assume \\(\\) \\(b\\) independent.Assume \\(ab\\) normally distributed.Assume \\(ab\\) normally distributed.work well small sample sizes.work well small sample sizes.Power test low test conservative compared Bootstrapping.Power test low test conservative compared Bootstrapping.","code":""},{"path":"mediation.html","id":"joint-significance-test","chapter":"36 Mediation","heading":"36.1.2.2 Joint Significance Test","text":"Effective determining indirect effect nonzero (testing whether \\(\\) \\(b\\) statistically significant), assumes \\(\\perp b\\).Effective determining indirect effect nonzero (testing whether \\(\\) \\(b\\) statistically significant), assumes \\(\\perp b\\).’s recommended use tests similar performance Bootstrapping test (Hayes Scharkow 2013).’s recommended use tests similar performance Bootstrapping test (Hayes Scharkow 2013).test’s accuracy can affected heteroscedasticity (Fossum Montoya 2023) non-normality.test’s accuracy can affected heteroscedasticity (Fossum Montoya 2023) non-normality.Although helpful computing power test indirect effect, doesn’t provide confidence interval effect.Although helpful computing power test indirect effect, doesn’t provide confidence interval effect.","code":""},{"path":"mediation.html","id":"bootstrapping","chapter":"36 Mediation","heading":"36.1.2.3 Bootstrapping","text":"First used Bollen Stine (1990)First used Bollen Stine (1990)allows calculation confidence intervals, p-values, etc.allows calculation confidence intervals, p-values, etc.require \\(\\perp b\\) corrects bias bootstrapped distribution.require \\(\\perp b\\) corrects bias bootstrapped distribution.can handle non-normality (sampling distribution indirect effect), complex models, small samples.can handle non-normality (sampling distribution indirect effect), complex models, small samples.Concerns exist bias-corrected bootstrapping liberal (Fritz, Taylor, MacKinnon 2012). Hence, current recommendations favor percentile bootstrap without bias correction better Type error rates (Tibbe Montoya 2022).Concerns exist bias-corrected bootstrapping liberal (Fritz, Taylor, MacKinnon 2012). Hence, current recommendations favor percentile bootstrap without bias correction better Type error rates (Tibbe Montoya 2022).special case bootstrapping proposed don’t need access raw data generate resampling, need \\(, b, var(), var(b), cov(,b)\\) (can taken lots primary studies)special case bootstrapping proposed don’t need access raw data generate resampling, need \\(, b, var(), var(b), cov(,b)\\) (can taken lots primary studies)","code":"\nresult <-\n    causalverse::med_ind(\n        a = 0.5,\n        b = 0.7,\n        var_a = 0.04,\n        var_b = 0.05,\n        cov_ab = 0.01\n    )\nresult$plot"},{"path":"mediation.html","id":"with-instrument","chapter":"36 Mediation","heading":"36.1.2.3.1 With Instrument","text":"Alternatively, one can use robmed packagePower test use app","code":"\nlibrary(DiagrammeR)\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label = 'Treatment']\n    Y [label = 'Outcome']\n  edge [minlen = 2]\n    X->Y\n  { rank = same; X; Y }\n}\n\")\n\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label ='Treatment', shape = box]\n    Y [label ='Outcome', shape = box]\n    M [label ='Mediator', shape = box]\n    IV [label ='Instrument', shape = box]\n  edge [minlen = 2]\n    IV->X\n    X->M  \n    M->Y \n    X->Y \n  { rank = same; X; Y; M }\n}\n\")\nlibrary(mediation)\ndata(\"boundsdata\")\nlibrary(fixest)\n\n# Total Effect\nout1 <- feols(out ~ ttt, data = boundsdata)\n\n# Indirect Effect\nout2 <- feols(med ~ ttt, data = boundsdata)\n\n# Direct and Indirect Effect\nout3 <- feols(out ~ med + ttt, data = boundsdata)\n\n# Proportion Test\n# To what extent is the effect of the treatment mediated by the mediator?\ncoef(out2)['ttt'] * coef(out3)['med'] / coef(out1)['ttt'] * 100\n#>      ttt \n#> 68.63609\n\n\n# Sobel Test\nbda::mediation.test(boundsdata$med, boundsdata$ttt, boundsdata$out) |> \n    tibble::rownames_to_column() |> \n    causalverse::nice_tab(2)\n#>   rowname Sobel Aroian Goodman\n#> 1 z.value  4.05   4.03    4.07\n#> 2 p.value  0.00   0.00    0.00\n# Mediation Analysis using boot\nlibrary(boot)\nset.seed(1)\nmediation_fn <- function(data, i){\n    # sample the dataset\n    df <- data[i,]\n    \n    \n    a_path <- feols(med ~ ttt, data = df)\n    a <- coef(a_path)['ttt']\n    \n    b_path <-  feols(out ~ med + ttt, data = df)\n    b <- coef(b_path)['med']\n    \n    cp <- coef(b_path)['ttt']\n    \n    # indirect effect\n    ind_ef <- a*b\n    total_ef <- a*b + cp\n    return(c(ind_ef, total_ef))\n    \n}\n\nboot_med <- boot(boundsdata, mediation_fn, R = 100, parallel = \"multicore\", ncpus = 2)\nboot_med \n#> \n#> ORDINARY NONPARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> boot(data = boundsdata, statistic = mediation_fn, R = 100, parallel = \"multicore\", \n#>     ncpus = 2)\n#> \n#> \n#> Bootstrap Statistics :\n#>       original        bias    std. error\n#> t1* 0.04112035  0.0006346725 0.009539903\n#> t2* 0.05991068 -0.0004462572 0.029556611\n\nsummary(boot_med) |> \n    causalverse::nice_tab()\n#>     R original bootBias bootSE bootMed\n#> 1 100     0.04        0   0.01    0.04\n#> 2 100     0.06        0   0.03    0.06\n\n# confidence intervals (percentile is always recommended)\nboot.ci(boot_med, type = c(\"norm\", \"perc\"))\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#> Based on 100 bootstrap replicates\n#> \n#> CALL : \n#> boot.ci(boot.out = boot_med, type = c(\"norm\", \"perc\"))\n#> \n#> Intervals : \n#> Level      Normal             Percentile     \n#> 95%   ( 0.0218,  0.0592 )   ( 0.0249,  0.0623 )  \n#> Calculations and Intervals on Original Scale\n#> Some percentile intervals may be unstable\n\n# point estimates (Indirect, and Total Effects)\ncolMeans(boot_med$t)\n#> [1] 0.04175502 0.05946442\nlibrary(robmed)\nlibrary(pwr2ppl)\n\n# indirect path ab power\nmedjs(\n    # X on M (path a)\n    rx1m1 = .3,\n    # correlation between X and Y (path c')\n    rx1y  = .1,\n    # correlation between M and Y (path b)\n    rym1  = .3,\n    # sample size\n    n     = 100,\n    alpha = 0.05,\n    # number of mediators\n    mvars = 1,\n    # should use 10000\n    rep   = 1000\n)"},{"path":"mediation.html","id":"multiple-mediation","chapter":"36 Mediation","heading":"36.1.3 Multiple Mediation","text":"general package handle multiple cases manymomeSee vignette example","code":"\nlibrary(manymome)"},{"path":"mediation.html","id":"multiple-mediators","chapter":"36 Mediation","heading":"36.1.3.1 Multiple Mediators","text":"NotesNotesVignetteVignettePackagePackage","code":"\nlibrary(mma)"},{},{"path":"mediation.html","id":"multiple-treatments-1","chapter":"36 Mediation","heading":"36.1.3.2 Multiple Treatments","text":"(Hayes Preacher 2014)Code Process","code":""},{"path":"mediation.html","id":"causal-inference-approach","chapter":"36 Mediation","heading":"36.2 Causal Inference Approach","text":"","code":""},{"path":"mediation.html","id":"example-1-mediation-traditional","chapter":"36 Mediation","heading":"36.2.1 Example 1","text":"Virginia’s libraryTotal Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Total Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565Using mediation package suggested Imai, Keele, Yamamoto (2010). details package can found here2 types Inference package:Model-based inference:\nAssumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\n\nModel-based inference:Assumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\nAssumptions:Treatment randomized (use matching methods achieve ).Treatment randomized (use matching methods achieve ).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Design-based inferenceDesign-based inferenceNotations: stay consistent package instruction\\(M_i(t)\\) = mediator\\(M_i(t)\\) = mediator\\(T_i\\) = treatment status \\((0,1)\\)\\(T_i\\) = treatment status \\((0,1)\\)\\(Y_i(t,m)\\) = outcome \\(t\\) = treatment, \\(m\\) = mediating variables.\\(Y_i(t,m)\\) = outcome \\(t\\) = treatment, \\(m\\) = mediating variables.\\(X_i\\) = vector observed pre-treatment confounders\\(X_i\\) = vector observed pre-treatment confoundersTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effects\nCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)\nDirect effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)\nsumming treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)\nTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effectsCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Causal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)sequential ignorability\\[\n\\{ Y_i (t', m) , M_i (t) \\} \\perp T_i |X_i = x\n\\]\\[\nY_i(t',m) \\perp M_i(t) | T_i = t, X_i = x\n\\]\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)First condition standard strong ignorability condition treatment assignment random conditional pre-treatment confounders.Second condition stronger mediators also random given observed treatment pre-treatment confounders. condition satisfied unobserved pre-treatment confounders, post-treatment confounders, multiple mediators correlated.understanding moment write note, way test sequential ignorability assumption. Hence, researchers can sensitivity analysis argue result.","code":"\nmyData <-\n    read.csv('http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv')\n\n# Step 1 (no longer necessary)\nmodel.0 <- lm(Y ~ X, myData)\nsummary(model.0)\n#> \n#> Call:\n#> lm(formula = Y ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.0262 -1.2340 -0.3282  1.5583  5.1622 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.8572     0.6932   4.122 7.88e-05 ***\n#> X             0.3961     0.1112   3.564 0.000567 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.929 on 98 degrees of freedom\n#> Multiple R-squared:  0.1147, Adjusted R-squared:  0.1057 \n#> F-statistic:  12.7 on 1 and 98 DF,  p-value: 0.0005671\n\n# Step 2\nmodel.M <- lm(M ~ X, myData)\nsummary(model.M)\n#> \n#> Call:\n#> lm(formula = M ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3046 -0.8656  0.1344  1.1344  4.6954 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.49952    0.58920   2.545   0.0125 *  \n#> X            0.56102    0.09448   5.938 4.39e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.639 on 98 degrees of freedom\n#> Multiple R-squared:  0.2646, Adjusted R-squared:  0.2571 \n#> F-statistic: 35.26 on 1 and 98 DF,  p-value: 4.391e-08\n\n# Step 3\nmodel.Y <- lm(Y ~ X + M, myData)\nsummary(model.Y)\n#> \n#> Call:\n#> lm(formula = Y ~ X + M, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.7631 -1.2393  0.0308  1.0832  4.0055 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.9043     0.6055   3.145   0.0022 ** \n#> X             0.0396     0.1096   0.361   0.7187    \n#> M             0.6355     0.1005   6.321 7.92e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.631 on 97 degrees of freedom\n#> Multiple R-squared:  0.373,  Adjusted R-squared:  0.3601 \n#> F-statistic: 28.85 on 2 and 97 DF,  p-value: 1.471e-10\n\n# Step 4 (boostrapping)\nlibrary(mediation)\nresults <- mediate(\n    model.M,\n    model.Y,\n    treat = 'X',\n    mediator = 'M',\n    boot = TRUE,\n    sims = 500\n)\nsummary(results)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the Percentile Method\n#> \n#>                Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME             0.3565       0.2119         0.51  <2e-16 ***\n#> ADE              0.0396      -0.1750         0.28   0.760    \n#> Total Effect     0.3961       0.1743         0.64   0.004 ** \n#> Prop. Mediated   0.9000       0.5042         1.94   0.004 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 100 \n#> \n#> \n#> Simulations: 500"},{"path":"mediation.html","id":"model-based-causal-mediation-analysis","chapter":"36 Mediation","heading":"36.3 Model-based causal mediation analysis","text":"resources:hereFit 2 modelsmediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)mediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)mediation can accommodate almost types model mediator model outcome model except Censored mediator model.update estimation ACME rely product difference coefficients (see 36.2.1 ,requires strict assumption: (1) linear regression models mediator outcome, (2) \\(T_i\\) \\(M_i\\) effects additive interactionNonparametric bootstrap versionIf theoretically understanding suggests treatment mediator interactionmediation can used conjunction imputation packages.can also handle mediated moderation non-binary treatment variables, multi-level dataSensitivity Analysis sequential ignorabilitytest unobserved pre-treatment covariatestest unobserved pre-treatment covariates\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).ACME confidence intervals contains 0 \\(\\rho \\(0.3,0.4)\\)Alternatively, using \\(R^2\\) interpretation, need specify direction confounder affects mediator outcome variables plot using sign.prod = \"positive\" (.e., direction) sign.prod = \"negative\" (.e., opposite direction).","code":"\nlibrary(mediation)\nset.seed(2014)\ndata(\"framing\", package = \"mediation\")\n\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\n\n# Quasi-Bayesian Monte Carlo \nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100 # should be 10000 in practice\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0791       0.0351         0.15  <2e-16 ***\n#> ACME (treated)             0.0804       0.0367         0.16  <2e-16 ***\n#> ADE (control)              0.0206      -0.0976         0.12    0.70    \n#> ADE (treated)              0.0218      -0.1053         0.12    0.70    \n#> Total Effect               0.1009      -0.0497         0.23    0.14    \n#> Prop. Mediated (control)   0.6946      -6.3109         3.68    0.14    \n#> Prop. Mediated (treated)   0.7118      -5.7936         3.50    0.14    \n#> ACME (average)             0.0798       0.0359         0.15  <2e-16 ***\n#> ADE (average)              0.0212      -0.1014         0.12    0.70    \n#> Prop. Mediated (average)   0.7032      -6.0523         3.59    0.14    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        boot = TRUE,\n        treat = \"treat\",\n        mediator = \"emo\",\n        sims = 100, # should be 10000 in practice\n        boot.ci.type = \"bca\" # bias-corrected and accelerated intervals\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the BCa Method\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0848       0.0424         0.14  <2e-16 ***\n#> ACME (treated)             0.0858       0.0410         0.14  <2e-16 ***\n#> ADE (control)              0.0117      -0.0726         0.13    0.58    \n#> ADE (treated)              0.0127      -0.0784         0.14    0.58    \n#> Total Effect               0.0975       0.0122         0.25    0.06 .  \n#> Prop. Mediated (control)   0.8698       1.7460       151.20    0.06 .  \n#> Prop. Mediated (treated)   0.8804       1.6879       138.91    0.06 .  \n#> ACME (average)             0.0853       0.0434         0.14  <2e-16 ***\n#> ADE (average)              0.0122      -0.0756         0.14    0.58    \n#> Prop. Mediated (average)   0.8751       1.7170       145.05    0.06 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo * treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                           Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.07417      0.02401         0.14  <2e-16 ***\n#> ACME (treated)             0.09496      0.02702         0.16  <2e-16 ***\n#> ADE (control)             -0.01353     -0.11855         0.11    0.76    \n#> ADE (treated)              0.00726     -0.11007         0.11    0.90    \n#> Total Effect               0.08143     -0.05646         0.19    0.26    \n#> Prop. Mediated (control)   0.64510    -14.31243         3.13    0.26    \n#> Prop. Mediated (treated)   0.98006    -17.83202         4.01    0.26    \n#> ACME (average)             0.08457      0.02738         0.15  <2e-16 ***\n#> ADE (average)             -0.00314     -0.11457         0.12    1.00    \n#> Prop. Mediated (average)   0.81258    -16.07223         3.55    0.26    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\n\ntest.TMint(med.out, conf.level = .95) # test treatment-mediator interaction effect \n#> \n#>  Test of ACME(1) - ACME(0) = 0\n#> \n#> data:  estimates from med.out\n#> ACME(1) - ACME(0) = 0.020796, p-value = 0.3\n#> alternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.01757310  0.07110837\nplot(med.out)\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsens.out <-\n    medsens(med.out,\n            rho.by = 0.1, # \\rho varies from -0.9 to 0.9 by 0.1\n            effect.type = \"indirect\", # sensitivity on ACME\n            # effect.type = \"direct\", # sensitivity on ADE\n            # effect.type = \"both\", # sensitivity on ACME and ADE\n            sims = 100)\nsummary(sens.out)\n#> \n#> Mediation Sensitivity Analysis: Average Mediation Effect\n#> \n#> Sensitivity Region: ACME for Control Group\n#> \n#>      Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n#> [1,] 0.3        0.0062      -0.0073       0.0188         0.09       0.0493\n#> [2,] 0.4       -0.0084      -0.0238       0.0017         0.16       0.0877\n#> \n#> Rho at which ACME for Control Group = 0: 0.3\n#> R^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\n#> R^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 \n#> \n#> \n#> Sensitivity Region: ACME for Treatment Group\n#> \n#>      Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n#> [1,] 0.3        0.0071      -0.0092       0.0213         0.09       0.0493\n#> [2,] 0.4       -0.0101      -0.0295       0.0023         0.16       0.0877\n#> \n#> Rho at which ACME for Treatment Group = 0: 0.3\n#> R^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09\n#> R^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493\nplot(sens.out, sens.par = \"rho\", main = \"Anxiety\", ylim = c(-0.2, 0.2))\nplot(sens.out, sens.par = \"R2\", r.type = \"total\", sign.prod = \"positive\")"},{"path":"directed-acyclic-graph.html","id":"directed-acyclic-graph","chapter":"37 Directed Acyclic Graph","heading":"37 Directed Acyclic Graph","text":"Native R:dagittydagittyggdagggdagdagRdagRr-causal: Center Causal Discovery. Also available Pythonr-causal: Center Causal Discovery. Also available PythonPublication-ready (R Latex): shinyDAGStandalone program: DAG program Sven Knuppel","code":""},{"path":"directed-acyclic-graph.html","id":"basic-notations","chapter":"37 Directed Acyclic Graph","heading":"37.1 Basic Notations","text":"Basic building blocks DAGMediators (chains): \\(X \\Z \\Y\\)\ncontrolling Z blocks (closes) causal impact \\(X \\Y\\)\nMediators (chains): \\(X \\Z \\Y\\)controlling Z blocks (closes) causal impact \\(X \\Y\\)Common causes (forks): \\(X \\leftarrow Z \\Y\\)\nZ (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).\nControlling \\(Z\\) close association.\n\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.\nCommon causes (forks): \\(X \\leftarrow Z \\Y\\)Z (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).Z (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).Controlling \\(Z\\) close association.Controlling \\(Z\\) close association.\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.Common effects (colliders): \\(X \\Z \\leftarrow Y\\)\ncontrolling \\(Z\\) induce association \\(X\\) \\(Y\\)\nControlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)\nCommon effects (colliders): \\(X \\Z \\leftarrow Y\\)controlling \\(Z\\) induce association \\(X\\) \\(Y\\)controlling \\(Z\\) induce association \\(X\\) \\(Y\\)Controlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)Controlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)Notes:descendant variable behavior similarly variable (e.g., descendant \\(Z\\) can behave like \\(Z\\) partially control \\(Z\\))descendant variable behavior similarly variable (e.g., descendant \\(Z\\) can behave like \\(Z\\) partially control \\(Z\\))Rule thumb multiple Controls: o Causal inference \\(X \\Y\\), must\nClose backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)\nclose causal path \\(X\\) \\(Y\\) (mediators).\nRule thumb multiple Controls: o Causal inference \\(X \\Y\\), mustClose backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)Close backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)close causal path \\(X\\) \\(Y\\) (mediators).close causal path \\(X\\) \\(Y\\) (mediators).","code":""},{"path":"report.html","id":"report","chapter":"38 Report","heading":"38 Report","text":"StructureExploratory analysis\nplots\npreliminary results\ninteresting structure/features data\noutliers\nExploratory analysisplotspreliminary resultsinteresting structure/features dataoutliersModel\nAssumptions\nmodel/ model best one?\nConsideration: interactions, collinearity, dependence\nModelAssumptionsWhy model/ model best one?Consideration: interactions, collinearity, dependenceModel Fit\nwell fit?\nmodel assumptions met?\nResidual analysis\n\nModel FitHow well fit?well fit?model assumptions met?\nResidual analysis\nmodel assumptions met?Residual analysisInference/ Prediction\ndifferent way support inference?\nInference/ PredictionAre different way support inference?Conclusion\nRecommendation\nLimitation analysis\ncorrect future\nConclusionRecommendationRecommendationLimitation analysisLimitation analysisHow correct futureHow correct futureThis chapter based jtools package. information can found .","code":""},{"path":"report.html","id":"one-summary-table","chapter":"38 Report","heading":"38.1 One summary table","text":"Packages reporting:Summary Statistics Table:qwraps2vtablegtsummaryapaTablesstargazerRegression TablegtsummarysjPlot,sjmisc, sjlabelledstargazer: recommended (Example)modelsummaryModel Equation","code":"\nlibrary(jtools)\ndata(movies)\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\nsumm(fit)\nsumm(\n    fit,\n    scale = TRUE,\n    vifs = TRUE,\n    part.corr = TRUE,\n    confint = TRUE,\n    pvals = FALSE\n) # notice that scale here is TRUE\n\n#obtain clsuter-robust SE\ndata(\"PetersenCL\", package = \"sandwich\")\nfit2 <- lm(y ~ x, data = PetersenCL)\nsumm(fit2, robust = \"HC3\", cluster = \"firm\") \n# install.packages(\"equatiomatic\") # not available for R 4.2\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\n# show the theoretical model\nequatiomatic::extract_eq(fit)\n# display the actual coefficients\nequatiomatic::extract_eq(fit, use_coefs = TRUE)"},{"path":"report.html","id":"model-comparison","chapter":"38 Report","heading":"38.2 Model Comparison","text":"Table 35.1:  Another package modelsummaryAnother package stargazerCorrelation Table","code":"\nfit <- lm(metascore ~ log(budget), data = movies)\nfit_b <- lm(metascore ~ log(budget) + log(us_gross), data = movies)\nfit_c <- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies)\ncoef_names <- c(\"Budget\" = \"log(budget)\", \"US Gross\" = \"log(us_gross)\",\n                \"Runtime (Hours)\" = \"runtime\", \"Constant\" = \"(Intercept)\")\nexport_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nlibrary(modelsummary)\nlm_mod <- lm(mpg ~ wt + hp + cyl, mtcars)\nmsummary(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nmodelplot(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nlibrary(\"stargazer\")\nstargazer(attitude)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM\n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lccccc} \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#> Statistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Max} \\\\ \n#> \\hline \\\\[-1.8ex] \n#> rating & 30 & 64.633 & 12.173 & 40 & 85 \\\\ \n#> complaints & 30 & 66.600 & 13.315 & 37 & 90 \\\\ \n#> privileges & 30 & 53.133 & 12.235 & 30 & 83 \\\\ \n#> learning & 30 & 56.367 & 11.737 & 34 & 75 \\\\ \n#> raises & 30 & 64.633 & 10.397 & 43 & 88 \\\\ \n#> critical & 30 & 74.767 & 9.895 & 49 & 92 \\\\ \n#> advance & 30 & 42.933 & 10.289 & 25 & 72 \\\\ \n#> \\hline \\\\[-1.8ex] \n#> \\end{tabular} \n#> \\end{table}\n## 2 OLS models\nlinear.1 <-\n    lm(rating ~ complaints + privileges + learning + raises + critical,\n       data = attitude)\nlinear.2 <-\n    lm(rating ~ complaints + privileges + learning, data = attitude)\n## create an indicator dependent variable, and run a probit model\nattitude$high.rating <- (attitude$rating > 70)\n\nprobit.model <-\n    glm(\n        high.rating ~ learning + critical + advance,\n        data = attitude,\n        family = binomial(link = \"probit\")\n    )\nstargazer(linear.1,\n          linear.2,\n          probit.model,\n          title = \"Results\",\n          align = TRUE)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM\n#> % Requires LaTeX packages: dcolumn \n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{Results} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#>  & \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ \n#> \\cline{2-4} \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{rating} & \\multicolumn{1}{c}{high.rating} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{\\textit{OLS}} & \\multicolumn{1}{c}{\\textit{probit}} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{1}{c}{(1)} & \\multicolumn{1}{c}{(2)} & \\multicolumn{1}{c}{(3)}\\\\ \n#> \\hline \\\\[-1.8ex] \n#>  complaints & 0.692^{***} & 0.682^{***} &  \\\\ \n#>   & (0.149) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  privileges & -0.104 & -0.103 &  \\\\ \n#>   & (0.135) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  learning & 0.249 & 0.238^{*} & 0.164^{***} \\\\ \n#>   & (0.160) & (0.139) & (0.053) \\\\ \n#>   & & & \\\\ \n#>  raises & -0.033 &  &  \\\\ \n#>   & (0.202) &  &  \\\\ \n#>   & & & \\\\ \n#>  critical & 0.015 &  & -0.001 \\\\ \n#>   & (0.147) &  & (0.044) \\\\ \n#>   & & & \\\\ \n#>  advance &  &  & -0.062 \\\\ \n#>   &  &  & (0.042) \\\\ \n#>   & & & \\\\ \n#>  Constant & 11.011 & 11.258 & -7.476^{**} \\\\ \n#>   & (11.704) & (7.318) & (3.570) \\\\ \n#>   & & & \\\\ \n#> \\hline \\\\[-1.8ex] \n#> Observations & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} \\\\ \n#> R$^{2}$ & \\multicolumn{1}{c}{0.715} & \\multicolumn{1}{c}{0.715} &  \\\\ \n#> Adjusted R$^{2}$ & \\multicolumn{1}{c}{0.656} & \\multicolumn{1}{c}{0.682} &  \\\\ \n#> Log Likelihood &  &  & \\multicolumn{1}{c}{-9.087} \\\\ \n#> Akaike Inf. Crit. &  &  & \\multicolumn{1}{c}{26.175} \\\\ \n#> Residual Std. Error & \\multicolumn{1}{c}{7.139 (df = 24)} & \\multicolumn{1}{c}{6.863 (df = 26)} &  \\\\ \n#> F Statistic & \\multicolumn{1}{c}{12.063$^{***}$ (df = 5; 24)} & \\multicolumn{1}{c}{21.743$^{***}$ (df = 3; 26)} &  \\\\ \n#> \\hline \n#> \\hline \\\\[-1.8ex] \n#> \\textit{Note:}  & \\multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n#> \\end{tabular} \n#> \\end{table}\n# Latex\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\n# ASCII text output\nstargazer(\n    linear.1,\n    linear.2,\n    type = \"text\",\n    title = \"Regression Results\",\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    ci = TRUE,\n    ci.level = 0.90,\n    single.row = TRUE\n)\n#> \n#> Regression Results\n#> ========================================================================\n#>                                        Dependent variable:              \n#>                          -----------------------------------------------\n#>                                          Overall Rating                 \n#>                                    (1)                     (2)          \n#> ------------------------------------------------------------------------\n#> Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)\n#> No Special Privileges    -0.104 (-0.325, 0.118)  -0.103 (-0.316, 0.109) \n#> Opportunity to Learn      0.249 (-0.013, 0.512)   0.238* (0.009, 0.467) \n#> Performance-Based Raises -0.033 (-0.366, 0.299)                         \n#> Too Critical              0.015 (-0.227, 0.258)                         \n#> Advancement              11.011 (-8.240, 30.262) 11.258 (-0.779, 23.296)\n#> ------------------------------------------------------------------------\n#> Observations                       30                      30           \n#> R2                                0.715                   0.715         \n#> Adjusted R2                       0.656                   0.682         \n#> ========================================================================\n#> Note:                                        *p<0.1; **p<0.05; ***p<0.01\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\ncorrelation.matrix <-\n    cor(attitude[, c(\"rating\", \"complaints\", \"privileges\")])\nstargazer(correlation.matrix, title = \"Correlation Matrix\")"},{"path":"report.html","id":"changes-in-an-estimate","chapter":"38 Report","heading":"38.3 Changes in an estimate","text":"","code":"\ncoef_names <- coef_names[1:3] # Dropping intercept for plots\nplot_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nplot_summs(\n    fit_c,\n    robust = \"HC3\",\n    coefs = coef_names,\n    plot.distributions = TRUE\n)"},{"path":"report.html","id":"standard-errors-3","chapter":"38 Report","heading":"38.4 Standard Errors","text":"sandwich vignetteHeterogeneityWhite’s estimatorAll heterogeneity SE methods derivatives .small sample bias adjustmentUses degrees freedom-based correctionWhen number clusters small, HC2 HC3 better (Cameron, Gelbach, Miller 2008)Better linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrixBetter linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrix","code":"\ndata(cars)\nmodel <- lm(speed ~ dist, data = cars)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = speed ~ dist, data = cars)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -7.5293 -2.1550  0.3615  2.4377  6.4179 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  8.28391    0.87438   9.474 1.44e-12 ***\n#> dist         0.16557    0.01749   9.464 1.49e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.156 on 48 degrees of freedom\n#> Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 \n#> F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\nlmtest::coeftest(model, vcov. = sandwich::vcovHC(model, type = \"HC1\"))\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept) 8.283906   0.891860  9.2883 2.682e-12 ***\n#> dist        0.165568   0.019402  8.5335 3.482e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"report.html","id":"coefficient-uncertainty-and-distribution","chapter":"38 Report","heading":"38.5 Coefficient Uncertainty and Distribution","text":"ggdist allows us visualize uncertainty frequentist Bayesian frameworks","code":"\nlibrary(ggdist)"},{"path":"report.html","id":"descriptive-tables","chapter":"38 Report","heading":"38.6 Descriptive Tables","text":"Export APA themeExport LatexHowever, codes play well notes. Hence, create custom code follows AMA guidelines","code":"\ndata(\"mtcars\")\n\nlibrary(flextable)\ntheme_apa(flextable(mtcars[1:5,1:5]))\nprint(xtable::xtable(mtcars, type = \"latex\"),\n      file = file.path(getwd(), \"output\", \"mtcars_xtable.tex\"))\n\n# American Economic Review style\nstargazer::stargazer(\n    mtcars,\n    title = \"Testing\",\n    style = \"aer\",\n    out = file.path(getwd(), \"output\", \"mtcars_stargazer.tex\")\n)\n\n# other styles include\n# Administrative Science Quarterly\n# Quarterly Journal of Economics\nama_tbl <- function(data, caption, label, note, output_path) {\n  library(tidyverse)\n  library(xtable)\n  # Function to determine column alignment\n  get_column_alignment <- function(data) {\n    # Start with the alignment for the header row\n    alignment <- c(\"l\", \"l\")\n    \n    # Check each column\n    for (col in seq_len(ncol(data))[-1]) {\n      if (is.numeric(data[[col]])) {\n        alignment <- c(alignment, \"r\")  # Right alignment for numbers\n      } else {\n        alignment <- c(alignment, \"c\")  # Center alignment for other data\n      }\n    }\n    \n    return(alignment)\n  }\n  \n  data %>%\n    # bold + left align first column \n    rename_with(~paste(\"\\\\multicolumn{1}{l}{\\\\textbf{\", ., \"}}\"), 1) %>% \n    # bold + center align all other columns\n    `colnames<-`(ifelse(colnames(.) != colnames(.)[1],\n                        paste(\"\\\\multicolumn{1}{c}{\\\\textbf{\", colnames(.), \"}}\"),\n                        colnames(.))) %>% \n    \n    xtable(caption = caption,\n           label = label,\n           align = get_column_alignment(data),\n           auto = TRUE) %>%\n    print(\n      include.rownames = FALSE,\n      caption.placement = \"top\",\n      \n      hline.after=c(-1, 0),\n      \n       # p{0.9\\linewidth} sets the width of the column to 90% of the line width, and the @{} removes any extra padding around the cell.\n      \n      add.to.row = list(pos = list(nrow(data)), # Add at the bottom of the table\n                        command = c(paste0(\"\\\\hline \\n \\\\multicolumn{\",ncol(data), \"}{l} {\", \"\\n \\\\begin{tabular}{@{}p{0.9\\\\linewidth}@{}} \\n\",\"Note: \", note, \"\\n \\\\end{tabular}  } \\n\"))), # Add your note here\n      \n      # make sure your heading is untouched (because you manually change it above)\n      sanitize.colnames.function = identity,\n      \n      # place a the top of the page\n      table.placement = \"h\",\n      \n      file = output_path\n    )\n}\nama_tbl(\n    mtcars,\n    caption     = \"This is caption\",\n    label       = \"tab:this_is_label\",\n    note        = \"this is note\",\n    output_path = file.path(getwd(), \"output\", \"mtcars_custom_ama.tex\")\n)"},{"path":"report.html","id":"visualizations-and-plots","chapter":"38 Report","heading":"38.7 Visualizations and Plots","text":"can customize plots based preferred journals. , creating custom setting American Marketing Association.American-Marketing-Association-ready theme plotsExampleOther pre-specified themes","code":"\nlibrary(ggplot2)\n\n# check available fonts\n# windowsFonts()\n\n# for Times New Roman\n# names(windowsFonts()[windowsFonts()==\"TT Times New Roman\"])\n# Making a theme\namatheme = theme_bw(base_size = 14, base_family = \"serif\") + # This is Time New Roman\n    \n    theme(\n        # remove major gridlines\n        panel.grid.major   = element_blank(),\n\n        # remove minor gridlines\n        panel.grid.minor   = element_blank(),\n\n        # remove panel border\n        panel.border       = element_blank(),\n\n        line               = element_line(),\n\n        # change font\n        text               = element_text(),\n\n        # if you want to remove legend title\n        # legend.title     = element_blank(),\n\n        legend.title       = element_text(size = rel(0.6), face = \"bold\"),\n\n        # change font size of legend\n        legend.text        = element_text(size = rel(0.6)),\n        \n        legend.background  = element_rect(color = \"black\"),\n        \n        # legend.margin    = margin(t = 5, l = 5, r = 5, b = 5),\n        # legend.key       = element_rect(color = NA, fill = NA),\n\n        # change font size of main title\n        plot.title         = element_text(\n            size           = rel(1.2),\n            face           = \"bold\",\n            hjust          = 0.5,\n            margin         = margin(b = 15)\n        ),\n        \n        plot.margin        = unit(c(1, 1, 1, 1), \"cm\"),\n\n        # add black line along axes\n        axis.line          = element_line(colour = \"black\", linewidth = .8),\n        \n        axis.ticks         = element_line(),\n        \n\n        # axis title\n        axis.title.x       = element_text(size = rel(1.2), face = \"bold\"),\n        axis.title.y       = element_text(size = rel(1.2), face = \"bold\"),\n\n        # axis text size\n        axis.text.y        = element_text(size = rel(1)),\n        axis.text.x        = element_text(size = rel(1))\n    )\nlibrary(tidyverse)\nlibrary(ggsci)\ndata(\"mtcars\")\nyourplot <- mtcars %>%\n    select(mpg, cyl, gear) %>%\n    ggplot(., aes(x = mpg, y = cyl, fill = gear)) + \n    geom_point() +\n    labs(title=\"Some Plot\") \n\nyourplot + \n    amatheme + \n    # choose different color theme\n    scale_color_npg() \n\nyourplot + \n    amatheme + \n    scale_color_continuous()\nlibrary(ggthemes)\n\n\n# Stata theme\nyourplot +\n    theme_stata()\n\n# The economist theme\nyourplot + \n    theme_economist()\n\nyourplot + \n    theme_economist_white()\n\n# Wall street journal theme\nyourplot + \n    theme_wsj()\n\n# APA theme\nyourplot +\n    jtools::theme_apa(\n        legend.font.size = 24,\n        x.font.size = 20,\n        y.font.size = 20\n    )"},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"39 Exploratory Data Analysis","heading":"39 Exploratory Data Analysis","text":"Data ReportFeature EngineeringMissing DataError IdentificationSummary statisticsNot code-y processQuick dirty way look dataCode generation wranglingShiny-app based Tableu styleCustomized daily/automatic report","code":"\n# load to get txhousing data\nlibrary(ggplot2)\n# install.packages(\"DataExplorer\")\nlibrary(DataExplorer)\n\n# creat a html file that contain all reports\ncreate_report(txhousing)\n\nintroduce() # see basic info\n\n\ndummify() # create binary columns from discrete variables\nsplit_columns() # split data into discrete and continuous parts\n\n\n\nplot_correlation() # heatmap for discrete var\nplot_intro() \n\nplot_missing() # plot missing value\nprofile_missing() # profile missing values\n\n\nplot_prcomp() # plot PCA\n# install.packages(\"dataReporter\")\nlibrary(dataReporter)\nmakeDataReport() # detailed report like DataExplorer\nlibrary(skimr)\nskim() # give only few quick summary stat, not as detailed as the other two packages\n# install.packages(\"rpivotTable\")\nlibrary(rpivotTable)\n# give set up just like Excel table \ndata %>% \n    rpivotTable::rpivotTable()\n# install.packages(\"esquisse\")\nlibrary(esquisse)\nesquisse::esquisser()\n# install.packages(\"chronicle\")\nlibrary(chronicle)\n# install.packages(\"dlookr\")\n# install.packages(\"descriptr\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"sensitivity-analysis-robustness-check","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40 Sensitivity Analysis/ Robustness Check","text":"","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"specification-curve","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1 Specification curve","text":"also known Specification robustness graph coefficient stability plotResourcesIn Stata speccurveIn Stata speccurve(Simonsohn, Simmons, Nelson 2020)(Simonsohn, Simmons, Nelson 2020)","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"starbility","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1.1 starbility","text":"RecommendInstallationExample package’s authorPlot different combinations controlsNote:\\(p < 0.01\\): red\\(p < 0.05\\): green\\(p < 0.1\\): blue\\(p > 0.1\\): blackMore Advanced StuffIn step 2, can modify use function (e.g., glm)getting specification (e.g., different CI)get customized plotTo get different model specification (e.g., probit vs. logit)","code":"\ndevtools::install_github('https://github.com/AakaashRao/starbility')\nlibrary(starbility)\nlibrary(tidyverse)\nlibrary(starbility)\nlibrary(lfe)\ndata(\"diamonds\")\nset.seed(43)\nindices = sample(1:nrow(diamonds),\n                 replace = F,\n                 size = round(nrow(diamonds) / 20))\ndiamonds = diamonds[indices, ]\n\n# If you want to make the diamond dimensions as base control\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z' # include all variables under 1 dimension\n)\n\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding fixed effects\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding instrumental variables \ninstruments = 'x+y+z'\n\n# clustering and weights \ndiamonds$sample_weights = runif(n = nrow(diamonds))\n\n\n# robust standard errors \nstarb_felm_custom = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = lfe::felm(spec, data=data) %>% broom::tidy()\n\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n  \n  # 99% confidence interval\n  z = qnorm(0.995) \n  # one-tailed test\n  return(c(coef, p/2, coef+z*se, coef-z*se))\n}\n\nplots = stability_plot(\n    data = diamonds,\n    lhs = 'price',\n    rhs = 'carat',\n    error_geom = 'ribbon', # make the plot more aesthetics\n    # error_geom = 'none', # if you don't want ribbon (i.e., error bar)\n    model = starb_felm_custom,\n    cluster = 'cut',\n    weights = 'sample_weights',\n    # iv = instruments,\n    perm = perm_controls,\n    base = base_controls,\n    # perm_fe = perm_fe_controls,\n    \n    # if you want to include fixed effects sequentially (not all combinations) \n    # (e.g., you want to test country or state fixed effect, not both )\n    # nonperm_fe = nonperm_fe_controls, \n    # fe_always = F,  # if you want to have a model without any Fixed Effects\n    \n    # sort \"asc\", \"desc\", or by fixed effects: \"asc-by-fe\" or \"desc-by-fe\"\n    sort = \"asc-by-fe\", \n    \n    # if you have less variables and want more aesthetics \n    # control_geom = 'circle',\n    # point_size = 2,\n    # control_spacing = 0.3,\n    \n    \n    # error_alpha = 0.2, # change alpha of the error geom\n    # point_size = 1.5, # change the size of the coefficient points\n    # control_text_size = 10, # change the size of the control labels\n    # coef_ylim = c(-5000, 35000), # change the endpoints of the y-axis\n    # trip_top = 3, # change the spacing between the two panels\n    \n    rel_height = 0.6\n)\nplots\n\n# add comments\n# replacement_coef_panel = plots[[1]] +\n#   scale_y_reverse() +\n#   theme(panel.grid.minor = element_blank()) +\n#   geom_vline(xintercept = 41,\n#              linetype = 'dashed',\n#              alpha = 0.4) +\n#   annotate(\n#     geom = 'label',\n#     x = 52,\n#     y = 30000,\n#     label = 'What a great\\nspecification!',\n#     alpha = 0.75\n#   )\n# \n# combine_plots(replacement_coef_panel,\n#               plots[[2]],\n#               rel_height = 0.6)\n# Step 1: Control Grid\n\ndiamonds$high_clarity = diamonds$clarity %in% c('VS1','VVS2','VVS1','IF')\n\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z'\n)\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nperm_fe_controls = c(\n  'Cut FE' = 'cut',\n  'Color FE' = 'color'\n)\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\ngrid1 = stability_plot(data = diamonds, \n                      lhs = 'price', \n                      rhs = 'carat', \n                      perm = perm_controls,\n                      base = base_controls, \n                      perm_fe = perm_fe_controls, \n                      nonperm_fe = nonperm_fe_controls, \n                      run_to=2)\n\nknitr::kable(grid1 %>% head(10))\n\n# Step 2: Get model expression\n\ngrid2 = stability_plot(grid = grid1,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=2,\n                      run_to=3)\n\n\nknitr::kable(grid2 %>% head(10))\n\n# Step 3: Estimate models\ngrid3 = stability_plot(grid = grid2,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=3,\n                      run_to=4)\n\nknitr::kable(grid3 %>% head(10))\n\n# Step 4: Get dataframe to draw\ndfs = stability_plot(grid = grid3,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=4,\n                      run_to=5)\n\ncoef_grid = dfs[[1]]\ncontrol_grid = dfs[[2]]\n\nknitr::kable(coef_grid %>% head(10))\n\n# Step 5: plot the sensitivity graph \npanels = stability_plot(data = diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      coef_grid = coef_grid,\n                      control_grid = control_grid,\n                      run_from=5,\n                      run_to=6)\n\nstability_plot(data = diamonds,\n               lhs='price', \n               rhs='carat', \n               coef_panel = panels[[1]],\n               control_panel = panels[[2]],\n               run_from = 6,\n               run_to = 7)\ndiamonds$above_med_price = as.numeric(diamonds$price > median(diamonds$price))\n\nbase_controls = c('Diamond dimensions' = 'x + y + z')\n\nperm_controls = c('Depth' = 'depth',\n                  'Table width' = 'table',\n                  'Clarity' = 'clarity')\nlhs_var = 'above_med_price'\nrhs_var = 'carat'\n\ngrid1 = stability_plot(\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_to = 2\n)\n\n# Create control part of formula\nbase_perm = c(base_controls, perm_controls)\ngrid1$expr = apply(grid1[, 1:length(base_perm)], 1,\n                   function(x)\n                     paste(base_perm[names(base_perm)[which(x == 1)]], \n                           collapse = '+'))\n\n# Complete formula with LHS and RHS variables\ngrid1$expr = paste(lhs_var, '~', rhs_var, '+', grid1$expr, sep = '')\n\nknitr::kable(grid1 %>% head(10))\n\n# customer function for the logit model\nstarb_logit = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n    broom::tidy()\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n\n  return(c(coef, p, coef+1.96*se, coef-1.96*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from=3)\nlibrary(margins)\nstarb_logit_enhanced = function(spec, data, rhs, ...) {\n  # Unpack ...\n  l = list(...)\n  get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n  \n  spec = as.formula(spec)\n  if (get_mfx) {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      margins() %>%\n      summary\n    row = which(model$factor==rhs)\n    coef = model[row, 'AME'] %>% as.numeric()\n    se   = model[row, 'SE'] %>% as.numeric()\n    p    = model[row, 'p'] %>% as.numeric()\n  } else {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      broom::tidy()\n    row = which(model$term==rhs)\n    coef = model[row, 'estimate'] %>% as.numeric()\n    se   = model[row, 'std.error'] %>% as.numeric()\n    p    = model[row, 'p.value'] %>% as.numeric()\n  }\n\n  z = qnorm(0.995)\n  return(c(coef, p, coef+z*se, coef-z*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3)\ndfs = stability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3,\n               run_to = 5)\n\ncoef_grid_logit = dfs[[1]]\ncontrol_grid_logit = dfs[[2]]\n\nmin_space = 0.5\n\ncoef_plot = ggplot2::ggplot(coef_grid_logit, aes(\n  x = model,\n  y = coef,\n  shape = p,\n  group = p\n)) +\n  geom_linerange(aes(ymin = error_low, ymax = error_high), alpha = 0.75) +\n  geom_point(size = 5, aes(col = p, fill = p), alpha = 1) +\n  viridis::scale_color_viridis(discrete = TRUE, option = \"D\") +\n  scale_shape_manual(values = c(15, 17, 18, 19)) +\n  theme_classic() +\n  geom_hline(yintercept = 0, linetype = 'dotted') +\n  ggtitle('A custom coefficient stability plot!') +\n  labs(subtitle = \"Error bars represent 99% confidence intervals\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks.x = element_blank()\n  ) +\n  coord_cartesian(xlim = c(1 - min_space, max(coef_grid_logit$model) + min_space),\n                  ylim = c(-0.1, 1.6)) +\n  guides(fill = F, shape = F, col = F)\n\n\ncontrol_plot = ggplot(control_grid_logit) +\n  geom_point(aes(x = model, y = y, fill=value), shape=23, size=4) +\n  scale_fill_manual(values=c('#FFFFFF', '#000000')) +\n  guides(fill=F) +\n  scale_y_continuous(breaks = unique(control_grid_logit$y), \n                     labels = unique(control_grid_logit$key),\n                     limits=c(min(control_grid_logit$y)-1, max(control_grid_logit$y)+1)) +\n  scale_x_continuous(breaks=c(1:max(control_grid_logit$model))) +\n  coord_cartesian(xlim=c(1-min_space, max(control_grid_logit$model)+min_space)) +\n  theme_classic() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title = element_blank(),\n        axis.text.y = element_text(size=10),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()) \n\ncowplot::plot_grid(coef_plot, control_plot, rel_heights=c(1,0.5), \n                   align='v', ncol=1, axis='b')\nstarb_probit = function(spec, data, rhs, ...) {\n    # Unpack ...\n    l = list(...)\n    get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n    \n    spec = as.formula(spec)\n    if (get_mfx) {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            margins() %>%\n            summary\n        row = which(model$factor == rhs)\n        coef = model[row, 'AME'] %>% as.numeric()\n        se   = model[row, 'SE'] %>% as.numeric()\n        p    = model[row, 'p'] %>% as.numeric()\n    } else {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            broom::tidy()\n        row = which(model$term == rhs)\n        coef = model[row, 'estimate'] %>% as.numeric()\n        se   = model[row, 'std.error'] %>% as.numeric()\n        p    = model[row, 'p.value'] %>% as.numeric()\n    }\n    \n    z = qnorm(0.995)\n    return(c(coef, p, coef + z * se, coef - z * se))\n}\n\nprobit_dfs = stability_plot(\n    grid = grid1,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    model = starb_probit,\n    get_mfx = T,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 3,\n    run_to = 5\n)\n\n# We'll put the probit DFs on the left, \n #so we need to adjust the model numbers accordingly\n# so the probit and logit DFs don't plot on top of one another!\ncoef_grid_probit = probit_dfs[[1]] %>% \n    mutate(model = model + max(coef_grid_logit$model))\n\ncontrol_grid_probit = probit_dfs[[2]] %>% \n    mutate(model = model + max(control_grid_logit$model))\n\ncoef_grid    = bind_rows(coef_grid_logit, coef_grid_probit)\ncontrol_grid = bind_rows(control_grid_logit, control_grid_probit)\n\npanels = stability_plot(\n    coef_grid = coef_grid,\n    control_grid = control_grid,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 5,\n    run_to = 6\n)\n\ncoef_plot = panels[[1]] + geom_vline(xintercept = 8.5,\n                                     linetype = 'dashed',\n                                     alpha = 0.8) +\n    annotate(\n        geom = 'label',\n        x = 4.25,\n        y = 1.8,\n        label = 'Logit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    annotate(\n        geom = 'label',\n        x = 12.75,\n        y = 1.8,\n        label = 'Probit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    coord_cartesian(ylim = c(-0.5, 1.9))\n\ncontrol_plot = panels[[2]] + geom_vline(xintercept = 8.5,\n                                        linetype = 'dashed',\n                                        alpha = 0.8)\n\ncowplot::plot_grid(\n    coef_plot,\n    control_plot,\n    rel_heights = c(1, 0.5),\n    align = 'v',\n    ncol = 1,\n    axis = 'b'\n)"},{"path":"sensitivity-analysis-robustness-check.html","id":"rdfanalysis","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1.2 rdfanalysis","text":"recommendInstallationExample package’s authorShiny app readers explore","code":"\ndevtools::install_github(\"joachim-gassen/rdfanalysis\")\nlibrary(rdfanalysis)\nload(url(\"https://joachim-gassen.github.io/data/rdf_ests.RData\"))\nplot_rdf_spec_curve(ests, \"est\", \"lb\", \"ub\") \ndesign <- define_design(steps = c(\"read_data\",\n                                  \"select_idvs\",\n                                  \"treat_extreme_obs\",\n                                  \"specify_model\",\n                                  \"est_model\"),\n                        rel_dir = \"vignettes/case_study_code\")\n\nshiny_rdf_spec_curve(ests, list(\"est\", \"lb\", \"ub\"),\n                     design, \"vignettes/case_study_code\",\n                     \"https://joachim-gassen.github.io/data/wb_new.csv\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"coefficient-stability","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.2 Coefficient stability","text":"(Oster 2019)Coefficient stability can evident omitted variable bias.Coefficient stability can evident omitted variable bias.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.Packagesmplot: graphical Model stability Variable Selectionmplot: graphical Model stability Variable Selectionrobomit: Robustness checks omitted variable bias (implementation ofrobomit: Robustness checks omitted variable bias (implementation ","code":"\nlibrary(robomit)\n\n# estimate beta \no_beta(\n  y     = \"mpg\",       # dependent variable\n  x     = \"wt\",        # independent treatment variable\n  con   = \"hp + qsec\", # related control variables\n  delta = 1,           # delta\n  R2max = 0.9,         # maximum R-square\n  type  = \"lm\",        # model type\n  data  = mtcars       # dataset\n) \n#> # A tibble: 10 × 2\n#>    Name                           Value\n#>    <chr>                          <dbl>\n#>  1 beta*                         -2.00 \n#>  2 (beta*-beta controlled)^2      5.56 \n#>  3 Alternative Solution 1        -7.01 \n#>  4 (beta[AS1]-beta controlled)^2  7.05 \n#>  5 Uncontrolled Coefficient      -5.34 \n#>  6 Controlled Coefficient        -4.36 \n#>  7 Uncontrolled R-square          0.753\n#>  8 Controlled R-square            0.835\n#>  9 Max R-square                   0.9  \n#> 10 delta                          1"},{"path":"sensitivity-analysis-robustness-check.html","id":"omitted-variable-bias-quantification","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.3 Omitted Variable Bias Quantification","text":"quantify bias needed change substantive conclusion causal inference study.","code":"\nlibrary(konfound)\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5\n)\n#> Robustness of Inference to Replacement (RIR):\n#> To invalidate an inference,  21.506 % of the estimate would have to be due to bias. \n#> This is based on a threshold of 3.925 for statistical significance (alpha = 0.05).\n#> \n#> To invalidate an inference,  215  observations would have to be replaced with cases\n#> for which the effect is 0 (RIR = 215).\n#> \n#> See Frank et al. (2013) for a description of the method.\n#> \n#> Citation: Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. (2013).\n#> What would it take to change an inference?\n#> Using Rubin's causal model to interpret the \n#>         robustness of causal inferences.\n#> Education, Evaluation and \n#>                        Policy Analysis, 35 437-460.\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"thresh_plot\"\n)\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"corr_plot\"\n)"},{"path":"replication-and-synthetic-data.html","id":"replication-and-synthetic-data","chapter":"41 Replication and Synthetic Data","heading":"41 Replication and Synthetic Data","text":"Access comprehensive data pivotal replication, especially realm social sciences. Yet, often data inaccessible, making replication challenge (G. King 1995). chapter dives nuances replication, exceptions norms, significance synthetic data.","code":""},{"path":"replication-and-synthetic-data.html","id":"the-replication-standard","chapter":"41 Replication and Synthetic Data","heading":"41.1 The Replication Standard","text":"Replicability research ensures:Credibility comprehension empirical studies.Continuity progression discipline.Enhanced readership academic citations.research replicable, “replication standard” vital: entails providing requisite information replication third parties. quantitative research can, extent, offer clear data, qualitative studies pose complexities due data depth.","code":""},{"path":"replication-and-synthetic-data.html","id":"solutions-for-empirical-replication","chapter":"41 Replication and Synthetic Data","heading":"41.1.1 Solutions for Empirical Replication","text":"Role Individual Authors:\nAuthors need vouch replication standard enhancing work’s credibility.\nArchives like Inter-University Consortium Political Social Research (ICPSR) serve depositories replication datasets.\nAuthors need vouch replication standard enhancing work’s credibility.Archives like Inter-University Consortium Political Social Research (ICPSR) serve depositories replication datasets.Creation Replication Data Set:\npublic data set, consisting original relevant complementary data, can serve replication purposes.\npublic data set, consisting original relevant complementary data, can serve replication purposes.Professional Data Archives:\nOrganizations like ICPSR provide solutions data storage accessibility problems.\nOrganizations like ICPSR provide solutions data storage accessibility problems.Educational Implications:\nReplication can excellent educational tool, many programs now emphasize importance.\nReplication can excellent educational tool, many programs now emphasize importance.","code":""},{"path":"replication-and-synthetic-data.html","id":"free-data-repositories","chapter":"41 Replication and Synthetic Data","heading":"41.1.2 Free Data Repositories","text":"Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.figshare: Allows researchers upload, share, cite datasets.figshare: Allows researchers upload, share, cite datasets.Dryad: Primarily datasets associated published articles biological medical sciences.Dryad: Primarily datasets associated published articles biological medical sciences.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.SocArXiv: open archive social sciences.SocArXiv: open archive social sciences.EarthArXiv: preprints archive earth science.EarthArXiv: preprints archive earth science.Protein Data Bank (PDB): 3D structures large biological molecules.Protein Data Bank (PDB): 3D structures large biological molecules.Gene Expression Omnibus (GEO): public functional genomics data repository.Gene Expression Omnibus (GEO): public functional genomics data repository.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.","code":""},{"path":"replication-and-synthetic-data.html","id":"exceptions-to-replication","chapter":"41 Replication and Synthetic Data","heading":"41.1.3 Exceptions to Replication","text":"exceptions replication standard :Confidentiality: Sometimes data, even fragmented, sensitive share.Proprietary Data: Data sets owned entities might restrict dissemination, usually, parts data can still shared.Rights First Publication: Embargos might set, essential data used study accessible.","code":""},{"path":"replication-and-synthetic-data.html","id":"synthetic-data-an-overview","chapter":"41 Replication and Synthetic Data","heading":"41.2 Synthetic Data: An Overview","text":"Synthetic data, modeling real data ensuring anonymity, becoming pivotal research. promising, complexities approached caution.","code":""},{"path":"replication-and-synthetic-data.html","id":"benefits","chapter":"41 Replication and Synthetic Data","heading":"41.2.1 Benefits","text":"Privacy preservation.Data fairness augmentation.Acceleration research.","code":""},{"path":"replication-and-synthetic-data.html","id":"concerns","chapter":"41 Replication and Synthetic Data","heading":"41.2.2 Concerns","text":"Misconceptions inherent privacy.Challenges data outliers.Models relying solely synthetic data can pose risks.","code":""},{"path":"replication-and-synthetic-data.html","id":"further-insights-on-synthetic-data","chapter":"41 Replication and Synthetic Data","heading":"41.2.3 Further Insights on Synthetic Data","text":"Synthetic data bridges model-centric data-centric perspectives, making essential tool modern research. Analogously, ’s like viewing Mona Lisa’s replica, real painting stored securely.Future projects, utilizing R’s diamonds dataset synthetic data generation, hold promise demonstrating vast potentials technology.deeper dive synthetic data applications, refer (Jordon et al. 2022).","code":""},{"path":"replication-and-synthetic-data.html","id":"application-14","chapter":"41 Replication and Synthetic Data","heading":"41.3 Application","text":"easiest way create synthetic data use synthpop package. Alternatively, can manuallyOpen data can assessed utility two distinct ways:General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.General utilitySpecific utilityYou basically want lack--fit test non-significant.","code":"\nlibrary(synthpop)\nlibrary(tidyverse)\nlibrary(performance)\n\n# library(effectsize)\n# library(see)\n# library(patchwork)\n# library(knitr)\n# library(kableExtra)\n\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\n\nsynthpop::codebook.syn(iris)\n#> $tab\n#>       variable   class nmiss perctmiss ndistinct\n#> 1 Sepal.Length numeric     0         0        35\n#> 2  Sepal.Width numeric     0         0        23\n#> 3 Petal.Length numeric     0         0        43\n#> 4  Petal.Width numeric     0         0        22\n#> 5      Species  factor     0         0         3\n#>                             details\n#> 1                  Range: 4.3 - 7.9\n#> 2                    Range: 2 - 4.4\n#> 3                    Range: 1 - 6.9\n#> 4                  Range: 0.1 - 2.5\n#> 5 'setosa' 'versicolor' 'virginica'\n#> \n#> $labs\n#> NULL\n\nsyn_df <- syn(iris, seed = 3)\n#> \n#> Synthesis\n#> -----------\n#>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n\n# check for replciated uniques\nreplicated.uniques(syn_df, iris)\n#> $replications\n#>   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n#>  [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n#>  [25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n#>  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n#>  [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n#>  [97] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n#> [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#> [133] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [145] FALSE FALSE FALSE FALSE FALSE FALSE\n#> \n#> $no.uniques\n#> [1] 148\n#> \n#> $no.replications\n#> [1] 17\n#> \n#> $per.replications\n#> [1] 11.33333\n\n\n# remove replicated uniques and adds a FAKE_DATA label \n# (in case a person can see his or own data in \n# the replicated data by chance)\n\nsyn_df_sdc <- sdc(syn_df, iris, \n                  label = \"FAKE_DATA\",\n                  rm.replicated.uniques = T)\n#> no. of replicated uniques: 17\niris |> \n    GGally::ggpairs()\n\nsyn_df$syn |> \n    GGally::ggpairs()\nlm_ori <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = iris)\n# performance::check_model(lm_ori)\nsummary(lm_ori)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.96159 -0.23489  0.00077  0.21453  0.78557 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.24914    0.24797    9.07 7.04e-16 ***\n#> Sepal.Width   0.59552    0.06933    8.59 1.16e-14 ***\n#> Petal.Length  0.47192    0.01712   27.57  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3333 on 147 degrees of freedom\n#> Multiple R-squared:  0.8402, Adjusted R-squared:  0.838 \n#> F-statistic: 386.4 on 2 and 147 DF,  p-value: < 2.2e-16\n\nlm_syn <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df$syn)\n# performance::check_model(lm_syn)\nsummary(lm_syn)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = syn_df$syn)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.79165 -0.22790 -0.01448  0.15893  1.13360 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.96449    0.24538  12.081  < 2e-16 ***\n#> Sepal.Width   0.39214    0.06816   5.754  4.9e-08 ***\n#> Petal.Length  0.45267    0.01743  25.974  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3658 on 147 degrees of freedom\n#> Multiple R-squared:  0.8246, Adjusted R-squared:  0.8222 \n#> F-statistic: 345.6 on 2 and 147 DF,  p-value: < 2.2e-16\ncompare(syn_df, iris)\n# just like regular lm, but for synthetic data\nlm_syn <- lm.synds(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df)\ncompare(lm_syn, iris)\n#> \n#> Call used to fit models to the data:\n#> lm.synds(formula = Sepal.Length ~ Sepal.Width + Petal.Length, \n#>     data = syn_df)\n#> \n#> Differences between results based on synthetic and observed data:\n#>              Synthetic  Observed        Diff Std. coef diff CI overlap\n#> (Intercept)  2.9644900 2.2491402  0.71534988       2.884829  0.2640608\n#> Sepal.Width  0.3921429 0.5955247 -0.20338187      -2.933611  0.2516161\n#> Petal.Length 0.4526695 0.4719200 -0.01925058      -1.124602  0.7131064\n#> \n#> Measures for one synthesis and 3 coefficients\n#> Mean confidence interval overlap:  0.4095944\n#> Mean absolute std. coef diff:  2.314347\n#> \n#> Mahalanobis distance ratio for lack-of-fit (target 1.0): 3.08\n#> Lack-of-fit test: 9.23442; p-value 0.0263 for test that synthesis model is\n#> compatible with a chi-squared test with 3 degrees of freedom.\n#> \n#> Confidence interval plot:\n\n# summary(lm_syn)"},{"path":"appendix.html","id":"appendix","chapter":"A Appendix","heading":"A Appendix","text":"","code":""},{"path":"appendix.html","id":"git","chapter":"A Appendix","heading":"A.1 Git","text":"Cheat SheetCheat Sheet different languagesLearn GitInteractive Cheat SheetUltimate Guide Git GitHub R userSetting Git: git config --global option configure user name, email, editor, etc.Setting Git: git config --global option configure user name, email, editor, etc.Creating repository: git init initialize repo. Git stores repo data .git directory.Creating repository: git init initialize repo. Git stores repo data .git directory.Tracking changes:\ngit status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\n\ngit add put files staging area\ngit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\n\nTracking changes:git status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\ngit status shows status repoFile stored project’s working directory (users see)File stored project’s working directory (users see)staging area (next commit built)staging area (next commit built)local repo commits permanently recordedlocal repo commits permanently recordedgit add put files staging areagit add put files staging areagit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\ngit commit saves staged content new commit local repo.git commit -m \"message\" give messages purpose commit.History\ngit diff shows differences commits\ngit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\n\nHistorygit diff shows differences commitsgit diff shows differences commitsgit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\ngit checkout recovers old version fieldsgit checkout HEAD go last commitgit checkout HEAD go last commitgit checkout <unique ID commit> go commitgit checkout <unique ID commit> go commitIgnoring\n.gitignore file tells Git files ignore\ncat . gitignore *.dat results/ ignore files ending “dat” folder “results”.\nIgnoring.gitignore file tells Git files ignore.gitignore file tells Git files ignorecat . gitignore *.dat results/ ignore files ending “dat” folder “results”.cat . gitignore *.dat results/ ignore files ending “dat” folder “results”.Remotes GitHub\nlocal git repo can connected one remote repos.\nUse HTTPS protocol connect remote repos\ngit push copies changes local repo remote repo\ngit pull copies changes remote repo local repo\nRemotes GitHubA local git repo can connected one remote repos.local git repo can connected one remote repos.Use HTTPS protocol connect remote reposUse HTTPS protocol connect remote reposgit push copies changes local repo remote repogit push copies changes local repo remote repogit pull copies changes remote repo local repogit pull copies changes remote repo local repoCollaborating\ngit clone copies remote repo create local repo remote called origin automatically set \nCollaboratinggit clone copies remote repo create local repo remote called origin automatically set upBranching\ngit check - b <new-branch-name\ngit checkout master switch master branch.\nBranchinggit check - b <new-branch-namegit check - b <new-branch-namegit checkout master switch master branch.git checkout master switch master branch.Conflicts\noccur 2 people change lines file\nversion control system allow overwrite ’s changes blindly, highlights conflicts can resolved.\nConflictsoccur 2 people change lines fileoccur 2 people change lines filethe version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.Licensing\nPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .\nCreative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.\nLicensingPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .People incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Citation:\nAdd CITATION file repo explain want others cite work.\nCitation:Add CITATION file repo explain want others cite work.Hosting\nRules regarding intellectual property storage sensitive info apply matter code data hosted.\nHostingRules regarding intellectual property storage sensitive info apply matter code data hosted.","code":""},{"path":"appendix.html","id":"short-cut","chapter":"A Appendix","heading":"A.2 Short-cut","text":"shortcuts probably remember working R. Even though might take bit time learn use second nature, save lot time.\nJust like learning another language, speak practice , comfortable speaking .Sometimes can’t stage folder ’s large. case, use Terminal pane Rstudio type git add -stage changes commit push like usual.","code":""},{"path":"appendix.html","id":"function-short-cut","chapter":"A Appendix","heading":"A.3 Function short-cut","text":"apply one function data create new variable: mutate(mod=map(data,function))\ninstead using 1:length(object): (seq_along(object))\napply multiple function: map_dbl\napply multiple function multiple variables:map2autoplot(data) plot times series datamod_tidy = linear(reg) %>% set_engine('lm') %>% fit(price ~ ., data=data) fit lm model. also fit models (stan, spark, glmnet, keras)Sometimes, data-masking able recognize whether ’re calling environment data variables. bypass , use .data$variable .env$variable. example data %>% mutate(x=.env$variable/.data$variableProblems data-masking:Unexpected masking data-var: Use .data .env disambiguate\nData-var cant get :\nTunnel data-var {{}} + Subset .data [[]]\nUnexpected masking data-var: Use .data .env disambiguateData-var cant get :Tunnel data-var {{}} + Subset .data [[]]Passing Data-variables argumentsTrouble selection:","code":"\nlibrary(\"dplyr\")\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{{var}}\":=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings\n}\n\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{var}\":=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions\n}\nlibrary(\"purrr\")\nname <- c(\"mass\",\"height\")\nstarwars %>% select(name) # Data-var. Here you are referring to variable named \"name\"\n\nstarwars %>% select(all_of((name))) # use all_of() to disambiguate when \n\naverages <- function(data,vars){ # take character vectors with all_of()\n    data %>%\n        select(all_of(vars)) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)\n\n\n# Another way\naverages <- function(data,vars){ # Tunnel selectiosn with {{}}\n    data %>%\n        select({{vars}}) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)"},{"path":"appendix.html","id":"citation","chapter":"A Appendix","heading":"A.4 Citation","text":"include citation [@Farjam_2015]cite packages used sessionpackage=ls(sessionInfo()$loadedOnly) (package){print(toBibtex(citation()))}","code":"\npackage=ls(sessionInfo()$loadedOnly) \nfor (i in package){\n    print(toBibtex(citation(i)))\n    }"},{"path":"appendix.html","id":"install-all-necessary-packageslibaries-on-your-local-machine","chapter":"A Appendix","heading":"A.5 Install all necessary packages/libaries on your local machine","text":"Get list packages need install book (local device)installed.csv file new local machine, can just install list packages","code":"\ninstalled <- as.data.frame(installed.packages())\n\nhead(installed)\n#>         Package                            LibPath Version Priority\n#> abind     abind C:/Program Files/R/R-4.2.3/library   1.4-5     <NA>\n#> ade4       ade4 C:/Program Files/R/R-4.2.3/library  1.7-22     <NA>\n#> admisc   admisc C:/Program Files/R/R-4.2.3/library    0.33     <NA>\n#> AER         AER C:/Program Files/R/R-4.2.3/library  1.2-10     <NA>\n#> afex       afex C:/Program Files/R/R-4.2.3/library   1.3-0     <NA>\n#> agridat agridat C:/Program Files/R/R-4.2.3/library    1.21     <NA>\n#>                                                                                        Depends\n#> abind                                                                             R (>= 1.5.0)\n#> ade4                                                                               R (>= 2.10)\n#> admisc                                                                            R (>= 3.5.0)\n#> AER     R (>= 3.0.0), car (>= 2.0-19), lmtest, sandwich (>= 2.4-0),\\nsurvival (>= 2.37-5), zoo\n#> afex                                                             R (>= 3.5.0), lme4 (>= 1.1-8)\n#> agridat                                                                                   <NA>\n#>                                                                                 Imports\n#> abind                                                                    methods, utils\n#> ade4                graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\\nRcpp\n#> admisc                                                                          methods\n#> AER                                                           stats, Formula (>= 0.2-0)\n#> afex    pbkrtest (>= 0.4-1), lmerTest (>= 3.0-0), car, reshape2,\\nstats, methods, utils\n#> agridat                                                                            <NA>\n#>                   LinkingTo\n#> abind                  <NA>\n#> ade4    Rcpp, RcppArmadillo\n#> admisc                 <NA>\n#> AER                    <NA>\n#> afex                   <NA>\n#> agridat                <NA>\n#>                                                                                                                                                                                                                                                                                                                                                                                                Suggests\n#> abind                                                                                                                                                                                                                                                                                                                                                                                              <NA>\n#> ade4                                                                                                                                                                                                                                                  ade4TkGUI, adegraphics, adephylo, ape, CircStats, deldir,\\nlattice, spdep, splancs, waveslim, progress, foreach, parallel,\\ndoParallel, iterators\n#> admisc                                                                                                                                                                                                                                                                                                                                                                                     QCA (>= 3.7)\n#> AER                                                                                                                                  boot, dynlm, effects, fGarch, forecast, foreign, ineq,\\nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\\nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\\nscatterplot3d, strucchange, systemfit (>= 1.1-20), truncreg,\\ntseries, urca, vars\n#> afex    emmeans (>= 1.4), coin, xtable, parallel, plyr, optimx,\\nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\\nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\\npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\\ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\\nstatmod, performance (>= 0.7.2), see (>= 0.6.4), ez,\\nggResidpanel, grid, vdiffr\n#> agridat                    AER, agricolae, betareg, broom, car, coin, corrgram, desplot,\\ndplyr, effects, equivalence, emmeans, FrF2, gam, gge, ggplot2,\\ngnm, gstat, HH, knitr, lattice, latticeExtra, lme4, lucid,\\nmapproj, maps, MASS, MCMCglmm, metafor, mgcv, NADA, nlme,\\nnullabor, ordinal, pbkrtest, pls, pscl, reshape2, rgdal,\\nrmarkdown, qicharts, qtl, sp, SpATS, survival, vcd, testthat\n#>         Enhances       License License_is_FOSS License_restricts_use OS_type\n#> abind       <NA>   LGPL (>= 2)            <NA>                  <NA>    <NA>\n#> ade4        <NA>    GPL (>= 2)            <NA>                  <NA>    <NA>\n#> admisc      <NA>    GPL (>= 3)            <NA>                  <NA>    <NA>\n#> AER         <NA> GPL-2 | GPL-3            <NA>                  <NA>    <NA>\n#> afex        <NA>    GPL (>= 2)            <NA>                  <NA>    <NA>\n#> agridat     <NA>  CC BY-SA 4.0            <NA>                  <NA>    <NA>\n#>         MD5sum NeedsCompilation Built\n#> abind     <NA>               no 4.2.0\n#> ade4      <NA>              yes 4.2.3\n#> admisc    <NA>              yes 4.2.3\n#> AER       <NA>               no 4.2.3\n#> afex      <NA>               no 4.2.3\n#> agridat   <NA>               no 4.2.3\n\nwrite.csv(installed, file.path(getwd(),'installed.csv'))\n# import the list of packages\ninstalled <- read.csv('installed.csv')\n\n# get the list of packages that you have on your device\nbaseR <- as.data.frame(installed.packages())\n\n# install only those that you don't have\ninstall.packages(setdiff(installed, baseR))"},{"path":"bookdown-cheat-sheet.html","id":"bookdown-cheat-sheet","chapter":"B Bookdown cheat sheet","heading":"B Bookdown cheat sheet","text":"","code":"\n# to see non-scientific notation a result\nformat(12e-17, scientific = FALSE)\n#> [1] \"0.00000000000000012\""},{"path":"bookdown-cheat-sheet.html","id":"operation","chapter":"B Bookdown cheat sheet","heading":"B.1 Operation","text":"R commands derivatives defined function Taking derivatives R involves using expression, D, eval functions. wrap function want take derivative expression(), use D, eval follows.simple exampleEvaluateThe first argument passed eval expression want evaluatethe second list containing values quantities defined elsewhere.","code":"\n#define a function\nf=expression(sqrt(x))\n\n#take the first derivative\ndf.dx=D(f,'x')\ndf.dx\n#> 0.5 * x^-0.5\n\n#take the second derivative\nd2f.dx2=D(D(f,'x'),'x')\nd2f.dx2\n#> 0.5 * (-0.5 * x^-1.5)\n#evaluate the function at a given x\neval(f,list(x=3))\n#> [1] 1.732051\n\n#evaluate the first derivative at a given x\neval(df.dx,list(x=3))\n#> [1] 0.2886751\n\n#evaluate the second derivative at a given x\neval(d2f.dx2,list(x=3))\n#> [1] -0.04811252"},{"path":"bookdown-cheat-sheet.html","id":"math-expression-syntax","chapter":"B Bookdown cheat sheet","heading":"B.2 Math Expression/ Syntax","text":"Full listAligning equations\\[\n\\begin{aligned}\n& = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\n\\]Cross-reference equationto refer sentence (B.1) (\\@ref(eq:test))Limit P(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\\[\nP(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\n\\]Matrices\\[\n\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n\\]\\[\n\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n\\]Aligning EquationsAligning Equations Comments\\[\n\\begin{aligned}\n    3+x &=4 & &\\text{(Solve } x \\text{.)} \\\\\n    x &=4-3 && \\text{(Subtract 3 sides.)} \\\\\n    x &=1   && \\text{(Yielding solution.)}\n\\end{aligned}\n\\]","code":"\\begin{aligned}\na & = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\\begin{equation} \na = b\n(\\#eq:test)\n\\end{equation}$$\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n$$$$\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n$$\\begin{aligned}\n    3+x &=4 && \\text{(Solve for} x \\text{.)}\\\\\n    x &=4-3 && \\text{(Subtract 3 from both sides.)}\\\\\n    x &=1   && \\text{(Yielding the solution.)}\n\\end{aligned}"},{"path":"bookdown-cheat-sheet.html","id":"statistics-notation","chapter":"B Bookdown cheat sheet","heading":"B.2.1 Statistics Notation","text":"\\[\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n\\]\\[\n\\begin{cases}\n\\frac{1}{b-} & \\text{} x\\[,b]\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\]","code":"$$\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n$$\\begin{cases}\n\\frac{1}{b-a}&\\text{for $x\\in[a,b]$}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}"},{"path":"bookdown-cheat-sheet.html","id":"table","chapter":"B Bookdown cheat sheet","heading":"B.3 Table","text":"built-wrapperbright colorcures scurvytasty\\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\)","code":"+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| *Bananas*     | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - **tasty**        |\n+---------------+---------------+--------------------+(\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
