[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"intended audience includes little experience statistics, econometrics, data science, well individuals budding interest fields eager deepen knowledge. primary domain interest marketing, principles methods discussed book universally applicable discipline employs scientific methods data analysis.hope book provides valuable starting point aspiring statisticians, econometricians, data scientists, empowering navigate fascinating world causal inference data analysis confidence.","code":""},{"path":"index.html","id":"how-to-cite-this-book","chapter":"Preface","heading":"How to cite this book","text":"1. APA (7th edition):Nguyen, M. (2020). Guide Data Analysis. Bookdown.https://bookdown.org/mike/data_analysis/2. MLA (8th edition):Nguyen, Mike. Guide Data Analysis. Bookdown, 2020. https://bookdown.org/mike/data_analysis/3. Chicago (17th edition):Nguyen, Mike. 2020. Guide Data Analysis. Bookdown. https://bookdown.org/mike/data_analysis/4. Harvard:Nguyen, M. (2020) Guide Data Analysis. Bookdown. Available : https://bookdown.org/mike/data_analysis/","code":"@book{nguyen2020guide,\n  title={A Guide on Data Analysis},\n  author={Nguyen, Mike},\n  year={2020},\n  publisher={Bookdown},\n  url={https://bookdown.org/mike/data_analysis/}\n}"},{"path":"index.html","id":"more-books","chapter":"Preface","heading":"More books","text":"books author can found :Advanced Data Analysis: second book data analysis series, covers machine learning models (focus prediction)Marketing ResearchCommunication Theory","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Since turn century, witnessed remarkable advancements innovations, particularly statistics, information technology, computer science, rapidly emerging field data science. However, one challenge developments overuse buzzwords like big data, machine learning, deep learning. terms powerful context, can sometimes obscure foundational principles underlying application.Every substantive field often specialized metric subfield, :Econometrics economicsPsychometrics psychologyChemometrics chemistrySabermetrics sports analyticsBiostatistics public health medicineTo layperson, disciplines often grouped broader terms like:Data ScienceApplied StatisticsComputational Social ScienceAs exciting explore new tools techniques, must admit retaining concepts can challenging. , effective way internalize apply ideas document data analysis process start finish.mind, let’s dive explore fascinating world data analysis together.","code":""},{"path":"introduction.html","id":"general-recommendations","chapter":"1 Introduction","heading":"1.1 General Recommendations","text":"journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.journey mastering data analysis fueled practice repetition. lines code write, functions familiarize , experiment, enjoyable rewarding process becomes.Readers can approach book several ways:\nFocused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.\nSequential Learning: follow traditional path data analysis, start Linear Regression section.\nExperimental Approach: interested designing experiments testing hypotheses, explore Analysis Variance (ANOVA) section.\nReaders can approach book several ways:Focused Learning: interested specific methods tools, can jump directly relevant section navigating table contents.Sequential Learning: follow traditional path data analysis, start Linear Regression section.Experimental Approach: interested designing experiments testing hypotheses, explore Analysis Variance (ANOVA) section.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.primarily interested applications less concerned theoretical foundations, focus summary application sections chapter.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.concept unclear, consider researching topic online. book serves guide, external resources like tutorials articles can provide additional insights.customize code examples provided book, use R’s built-help functions. instance:\nlearn specific function, type help(function_name) ?function_name R console.\nexample, find details hist function, type ?hist help(hist) console.\ncustomize code examples provided book, use R’s built-help functions. instance:learn specific function, type help(function_name) ?function_name R console.example, find details hist function, type ?hist help(hist) console.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.Additionally, searching online powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages achieve similar results. instance, need create histogram R, simple search like “histogram R” provide multiple approaches examples.adopting strategies, can tailor learning experience maximize value book.Tools statisticsProbability TheoryMathematical AnalysisComputer ScienceNumerical AnalysisDatabase ManagementCode ReplicationThis book built R version 4.2.3 (2023-03-15 ucrt) following packages:","code":"#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.3 (2023-03-15 ucrt)\n#>  os       Windows 10 x64 (build 22631)\n#>  system   x86_64, mingw32\n#>  ui       RTerm\n#>  language (EN)\n#>  collate  English_United States.utf8\n#>  ctype    English_United States.utf8\n#>  tz       America/Los_Angeles\n#>  date     2024-02-08\n#>  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package     * version date (UTC) lib source\n#>  bookdown      0.35    2023-08-09 [1] CRAN (R 4.2.3)\n#>  bslib         0.6.1   2023-11-28 [1] CRAN (R 4.2.3)\n#>  cachem        1.0.8   2023-05-01 [1] CRAN (R 4.2.3)\n#>  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.3)\n#>  codetools     0.2-19  2023-02-01 [1] CRAN (R 4.2.3)\n#>  colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.2.3)\n#>  desc          1.4.3   2023-12-10 [1] CRAN (R 4.2.3)\n#>  devtools      2.4.5   2022-10-11 [1] CRAN (R 4.2.3)\n#>  digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.3)\n#>  dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.2.3)\n#>  ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.3)\n#>  evaluate      0.23    2023-11-01 [1] CRAN (R 4.2.3)\n#>  fansi         1.0.4   2023-01-22 [1] CRAN (R 4.2.3)\n#>  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.3)\n#>  forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.2.3)\n#>  fs            1.6.3   2023-07-20 [1] CRAN (R 4.2.3)\n#>  generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.3)\n#>  ggplot2     * 3.4.4   2023-10-12 [1] CRAN (R 4.2.3)\n#>  glue          1.6.2   2022-02-24 [1] CRAN (R 4.2.3)\n#>  gtable        0.3.4   2023-08-21 [1] CRAN (R 4.2.3)\n#>  highr         0.10    2022-12-22 [1] CRAN (R 4.2.3)\n#>  hms           1.1.3   2023-03-21 [1] CRAN (R 4.2.3)\n#>  htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.2.3)\n#>  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.3)\n#>  httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.2.3)\n#>  jpeg        * 0.1-10  2022-11-29 [1] CRAN (R 4.2.2)\n#>  jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.2.3)\n#>  jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.2.3)\n#>  knitr         1.45    2023-10-30 [1] CRAN (R 4.2.3)\n#>  later         1.3.1   2023-05-02 [1] CRAN (R 4.2.3)\n#>  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.2.3)\n#>  lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.2.3)\n#>  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.3)\n#>  memoise       2.0.1   2021-11-26 [1] CRAN (R 4.2.3)\n#>  mime          0.12    2021-09-28 [1] CRAN (R 4.2.0)\n#>  miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.2.3)\n#>  munsell       0.5.0   2018-06-12 [1] CRAN (R 4.2.3)\n#>  pillar        1.9.0   2023-03-22 [1] CRAN (R 4.2.3)\n#>  pkgbuild      1.4.3   2023-12-10 [1] CRAN (R 4.2.3)\n#>  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.3)\n#>  pkgload       1.3.3   2023-09-22 [1] CRAN (R 4.2.3)\n#>  profvis       0.3.8   2023-05-02 [1] CRAN (R 4.2.3)\n#>  promises      1.2.1   2023-08-10 [1] CRAN (R 4.2.3)\n#>  purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.2.3)\n#>  R6            2.5.1   2021-08-19 [1] CRAN (R 4.2.3)\n#>  Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.2.3)\n#>  readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.2.3)\n#>  remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.2.3)\n#>  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.3)\n#>  rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.2.3)\n#>  rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.3)\n#>  sass          0.4.8   2023-12-06 [1] CRAN (R 4.2.3)\n#>  scales      * 1.3.0   2023-11-28 [1] CRAN (R 4.2.3)\n#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.3)\n#>  shiny         1.7.5   2023-08-12 [1] CRAN (R 4.2.3)\n#>  stringi       1.7.12  2023-01-11 [1] CRAN (R 4.2.2)\n#>  stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.2.3)\n#>  tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.2.3)\n#>  tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.2.3)\n#>  tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.2.3)\n#>  tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.2.3)\n#>  timechange    0.2.0   2023-01-11 [1] CRAN (R 4.2.3)\n#>  tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.2.3)\n#>  urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.2.3)\n#>  usethis       2.2.2   2023-07-06 [1] CRAN (R 4.2.3)\n#>  utf8          1.2.3   2023-01-31 [1] CRAN (R 4.2.3)\n#>  vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.2.3)\n#>  withr         2.5.2   2023-10-30 [1] CRAN (R 4.2.3)\n#>  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.3)\n#>  xtable        1.8-4   2019-04-21 [1] CRAN (R 4.2.3)\n#>  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.3)\n#> \n#>  [1] C:/Program Files/R/R-4.2.3/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"},{"path":"prerequisites.html","id":"prerequisites","chapter":"2 Prerequisites","heading":"2 Prerequisites","text":"chapter serves concise review fundamental concepts Matrix Theory Probability Theory.confident understanding topics, can proceed directly Descriptive Statistics section begin exploring applied data analysis.","code":""},{"path":"prerequisites.html","id":"matrix-theory","chapter":"2 Prerequisites","heading":"2.1 Matrix Theory","text":"Matrix \\(\\) represents original matrix. ’s 2x2 matrix elements \\(a_{ij}\\), \\(\\) represents row \\(j\\) represents column.\\[\n=\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\] \\('\\) transpose \\(\\). transpose matrix flips rows columns.\\[\n' =\n\\begin{bmatrix}\na_{11} & a_{21} \\\\\na_{12} & a_{22}\n\\end{bmatrix}\n\\]Fundamental properties rules matrices, essential understanding operations linear algebra:\\[\n\\begin{aligned}\n\\mathbf{(ABC)'}   & = \\mathbf{C'B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(B+C)}   & = \\mathbf{AB + AC} \\quad &\\text{(Distributive property)} \\\\\n\\mathbf{AB}       & \\neq \\mathbf{BA} \\quad &\\text{(Multiplication commutative)} \\\\\n\\mathbf{(')'}    & = \\mathbf{} \\quad &\\text{(Double transpose original matrix)} \\\\\n\\mathbf{(+B)'}   & = \\mathbf{' + B'} \\quad &\\text{(Transpose sum sum transposes)} \\\\\n\\mathbf{(AB)'}    & = \\mathbf{B''} \\quad &\\text{(Transpose reverses order product)} \\\\\n\\mathbf{(AB)^{-1}} & = \\mathbf{B^{-1}^{-1}} \\quad &\\text{(Inverse reverses order product)} \\\\\n\\mathbf{+B}      & = \\mathbf{B + } \\quad &\\text{(Addition commutative)} \\\\\n\\mathbf{AA^{-1}}  & = \\mathbf{} \\quad &\\text{(Matrix times inverse identity)}\n\\end{aligned}\n\\] properties critical solving systems equations, optimizing models, performing data transformations.matrix \\(\\mathbf{}\\) inverse, called invertible. \\(\\mathbf{}\\) inverse, referred singular.product two matrices \\(\\mathbf{}\\) \\(\\mathbf{B}\\) computed :\\[\n\\begin{aligned}\n\\mathbf{} &=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \\sum_{=1}^{3}a_{1i}b_{i2} & \\sum_{=1}^{3}a_{1i}b_{i3} \\\\\n\\sum_{=1}^{3}a_{2i}b_{i1} & \\sum_{=1}^{3}a_{2i}b_{i2} & \\sum_{=1}^{3}a_{2i}b_{i3}\n\\end{bmatrix}\n\\end{aligned}\n\\]Quadratic FormLet \\(\\mathbf{}\\) \\(3 \\times 1\\) vector. quadratic form involving matrix \\(\\mathbf{B}\\) given :\\[\n\\mathbf{'Ba} = \\sum_{=1}^{3}\\sum_{j=1}^{3}a_i b_{ij} a_{j}\n\\]Length VectorThe length (2-norm) vector \\(\\mathbf{}\\), denoted \\(||\\mathbf{}||\\), defined square root inner product vector :\\[\n||\\mathbf{}|| = \\sqrt{\\mathbf{'}}\n\\]","code":""},{"path":"prerequisites.html","id":"rank-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.1 Rank of a Matrix","text":"rank matrix refers :dimension space spanned columns (rows).number linearly independent columns rows.\\(n \\times k\\) matrix \\(\\mathbf{}\\) \\(k \\times k\\) matrix \\(\\mathbf{B}\\), following properties hold:\\(\\text{rank}(\\mathbf{}) \\leq \\min(n, k)\\)\\(\\text{rank}(\\mathbf{}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{'}) = \\text{rank}(\\mathbf{AA'})\\)\\(\\text{rank}(\\mathbf{AB}) = \\min(\\text{rank}(\\mathbf{}), \\text{rank}(\\mathbf{B}))\\)\\(\\mathbf{B}\\) invertible (non-singular) \\(\\text{rank}(\\mathbf{B}) = k\\).","code":""},{"path":"prerequisites.html","id":"inverse-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.2 Inverse of a Matrix","text":"scalar algebra, \\(= 0\\), \\(1/\\) exist.matrix algebra, matrix invertible non-singular, meaning non-zero determinant inverse exists. square matrix \\(\\mathbf{}\\) invertible exists another square matrix \\(\\mathbf{B}\\) :\\[\n\\mathbf{AB} = \\mathbf{} \\quad \\text{(Identity Matrix)}.\n\\]case, \\(\\mathbf{}^{-1} = \\mathbf{B}\\).\\(2 \\times 2\\) matrix:\\[\n\\mathbf{} =\n\\begin{bmatrix}\n& b \\\\\nc & d\n\\end{bmatrix}\n\\]inverse :\\[\n\\mathbf{}^{-1} =\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & \n\\end{bmatrix}\n\\]inverse exists \\(ad - bc \\neq 0\\), \\(ad - bc\\) determinant \\(\\mathbf{}\\).partitioned block matrix:\\[\n\\begin{bmatrix}\n& B \\\\\nC & D\n\\end{bmatrix}^{-1}\n=\n\\begin{bmatrix}\n\\mathbf{(-BD^{-1}C)^{-1}} & \\mathbf{-(-BD^{-1}C)^{-1}BD^{-1}} \\\\\n\\mathbf{-D^{-1}C(-BD^{-1}C)^{-1}} & \\mathbf{D^{-1}+D^{-1}C(-BD^{-1}C)^{-1}BD^{-1}}\n\\end{bmatrix}\n\\]formula assumes \\(\\mathbf{D}\\) \\(\\mathbf{- BD^{-1}C}\\) invertible.Properties Inverse Non-Singular Matrices\\(\\mathbf{(^{-1})^{-1}} = \\mathbf{}\\)non-zero scalar \\(b\\), \\(\\mathbf{(bA)^{-1} = b^{-1}^{-1}}\\)matrix \\(\\mathbf{B}\\), \\(\\mathbf{(BA)^{-1} = B^{-1}^{-1}}\\) (\\(\\mathbf{B}\\) non-singular).\\(\\mathbf{(^{-1})' = (')^{-1}}\\) (transpose inverse equals inverse transpose).Never notate \\(\\mathbf{1/}\\); use \\(\\mathbf{^{-1}}\\) instead.Notes: - determinant matrix determines whether invertible. square matrices, determinant \\(0\\) means matrix singular inverse.\n- Always verify conditions invertibility, particularly dealing partitioned block matrices.","code":""},{"path":"prerequisites.html","id":"definiteness-of-a-matrix","chapter":"2 Prerequisites","heading":"2.1.3 Definiteness of a Matrix","text":"symmetric square \\(k \\times k\\) matrix \\(\\mathbf{}\\) classified based following conditions:Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Positive Semi-Definite (PSD): \\(\\mathbf{}\\) PSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\geq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Negative Semi-Definite (NSD): \\(\\mathbf{}\\) NSD , non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[\n\\mathbf{x'Ax \\leq 0}.\n\\]Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.Indefinite: \\(\\mathbf{}\\) indefinite neither PSD NSD.identity matrix always positive definite (PD).ExampleLet \\(\\mathbf{x} = (x_1, x_2)'\\), consider \\(2 \\times 2\\) identity matrix \\(\\mathbf{}\\):\\[\n\\begin{aligned}\n\\mathbf{x'Ix}\n&= (x_1, x_2)\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\n(x_1, x_2)\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix} \\\\\n&=\nx_1^2 + x_2^2 \\geq 0.\n\\end{aligned}\n\\]Thus, \\(\\mathbf{}\\) PD \\(\\mathbf{x'Ix} > 0\\) non-zero \\(\\mathbf{x}\\).Properties DefinitenessAny variance-covariance matrix PSD.matrix \\(\\mathbf{}\\) PSD exists matrix \\(\\mathbf{B}\\) : \\[\n\\mathbf{= B'B}.\n\\]\\(\\mathbf{}\\) PSD, \\(\\mathbf{B'AB}\\) also PSD conformable matrix \\(\\mathbf{B}\\).\\(\\mathbf{}\\) \\(\\mathbf{C}\\) non-singular, \\(\\mathbf{- C}\\) PSD \\(\\mathbf{C^{-1} - ^{-1}}\\) PSD.\\(\\mathbf{}\\) PD (ND), \\(\\mathbf{^{-1}}\\) also PD (ND).NotesAn indefinite matrix \\(\\mathbf{}\\) neither PSD NSD. concept direct counterpart scalar algebra.square matrix PSD invertible, PD.Examples DefinitenessInvertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Invertible / Indefinite: \\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & 10\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Non-Invertible / Indefinite: \\[\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Invertible / PSD: \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]Non-Invertible / PSD: \\[\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]","code":""},{"path":"prerequisites.html","id":"matrix-calculus","chapter":"2 Prerequisites","heading":"2.1.4 Matrix Calculus","text":"Consider scalar function \\(y = f(x_1, x_2, \\dots, x_k) = f(x)\\), \\(x\\) \\(1 \\times k\\) row vector.","code":""},{"path":"prerequisites.html","id":"gradient-first-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.1 Gradient (First-Order Derivative)","text":"gradient, first-order derivative \\(f(x)\\) respect vector \\(x\\), given :\\[\n\\frac{\\partial f(x)}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f(x)}{\\partial x_1} \\\\\n\\frac{\\partial f(x)}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f(x)}{\\partial x_k}\n\\end{bmatrix}.\n\\]","code":""},{"path":"prerequisites.html","id":"hessian-second-order-derivative","chapter":"2 Prerequisites","heading":"2.1.4.2 Hessian (Second-Order Derivative)","text":"Hessian, second-order derivative \\(f(x)\\) respect \\(x\\), symmetric matrix defined :\\[\n\\frac{\\partial^2 f(x)}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n\\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_k} \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_2} & \\cdots & \\frac{\\partial^2 f(x)}{\\partial x_k^2}\n\\end{bmatrix}.\n\\]","code":""},{"path":"prerequisites.html","id":"derivative-of-a-scalar-function-with-respect-to-a-matrix","chapter":"2 Prerequisites","heading":"2.1.4.3 Derivative of a Scalar Function with Respect to a Matrix","text":"Let \\(f(\\mathbf{X})\\) scalar function, \\(\\mathbf{X}\\) \\(n \\times p\\) matrix. derivative :\\[\n\\frac{\\partial f(\\mathbf{X})}{\\partial \\mathbf{X}} =\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{11}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{1p}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\mathbf{X})}{\\partial x_{n1}} & \\cdots & \\frac{\\partial f(\\mathbf{X})}{\\partial x_{np}}\n\\end{bmatrix}.\n\\]","code":""},{"path":"prerequisites.html","id":"common-matrix-derivatives","chapter":"2 Prerequisites","heading":"2.1.4.4 Common Matrix Derivatives","text":"\\(\\mathbf{}\\) vector \\(\\mathbf{}\\) matrix independent \\(\\mathbf{y}\\):\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\n\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\n\\(\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\\)\\(\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\)\\(\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = (\\mathbf{} + \\mathbf{'})\\mathbf{y}\\)\\(\\mathbf{X}\\) symmetric:\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\n\\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, & = j \\\\ X_{ij}, & \\neq j \\end{cases}\\) \\(X_{ij}\\) \\((,j)\\)-th cofactor \\(\\mathbf{X}\\).\\(\\mathbf{X}\\) symmetric \\(\\mathbf{}\\) matrix independent \\(\\mathbf{X}\\):\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\n\\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{'} - \\text{diag}(\\mathbf{})\\).\\(\\mathbf{X}\\) symmetric, let \\(\\mathbf{J}_{ij}\\) matrix 1 \\((,j)\\)-th position 0 elsewhere:\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)\n\\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, & = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, & \\neq j \\end{cases}.\\)","code":""},{"path":"prerequisites.html","id":"optimization-in-scalar-and-vector-spaces","chapter":"2 Prerequisites","heading":"2.1.5 Optimization in Scalar and Vector Spaces","text":"Optimization process finding minimum maximum function. conditions optimization differ depending whether function involves scalar vector. comparison scalar vector optimization:Second-Order ConditionFor convex functions, implies minimum.Key ConceptsFirst-Order Condition:\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\n\nfirst-order derivative function must equal zero critical point. holds scalar vector functions:\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.\nvector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.\nscalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points.vector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) gradient vector, condition satisfied elements gradient zero.Second-Order Condition:\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\n\nsecond-order derivative determines whether critical point minimum, maximum, saddle point:\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.\nvector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\n\nscalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) implies local minimum, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) implies local maximum.vector functions, Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) must :\nPositive Definite: minimum (convex function).\nNegative Definite: maximum (concave function).\nIndefinite: saddle point (neither minimum maximum).\nPositive Definite: minimum (convex function).Negative Definite: maximum (concave function).Indefinite: saddle point (neither minimum maximum).Convex Concave Functions:\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\n\nConvexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.\nfunction \\(f(x)\\) :\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.\nConcave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.\nConvex \\(\\frac{\\partial^2 f(x)}{\\partial x^2} > 0\\) Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) positive definite.Concave \\(\\frac{\\partial^2 f(x)}{\\partial x^2} < 0\\) Hessian negative definite.Convexity ensures global optimization minimization problems, concavity ensures global optimization maximization problems.Hessian Matrix:\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\n\nvector optimization, Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x'}\\) plays crucial role determining nature critical points:\nPositive definite Hessian: eigenvalues positive.\nNegative definite Hessian: eigenvalues negative.\nIndefinite Hessian: Eigenvalues mixed signs.\nPositive definite Hessian: eigenvalues positive.Negative definite Hessian: eigenvalues negative.Indefinite Hessian: Eigenvalues mixed signs.","code":""},{"path":"prerequisites.html","id":"probability-theory","chapter":"2 Prerequisites","heading":"2.2 Probability Theory","text":"","code":""},{"path":"prerequisites.html","id":"axioms-and-theorems-of-probability","chapter":"2 Prerequisites","heading":"2.2.1 Axioms and Theorems of Probability","text":"Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)Let \\(S\\) denote sample space experiment. : \\[\nP[S] = 1\n\\] (probability sample space always 1.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)event \\(\\): \\[\nP[] \\geq 0\n\\] (Probabilities always non-negative.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)Let \\(A_1, A_2, A_3, \\dots\\) finite infinite collection mutually exclusive events. : \\[\nP[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots\n\\] (probability union mutually exclusive events sum probabilities.)probability empty set : \\[\nP[\\emptyset] = 0\n\\]probability empty set : \\[\nP[\\emptyset] = 0\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]complement rule: \\[\nP['] = 1 - P[]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]probability union two events: \\[\nP[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\n\\]","code":""},{"path":"prerequisites.html","id":"conditional-probability","chapter":"2 Prerequisites","heading":"2.2.1.1 Conditional Probability","text":"conditional probability \\(\\) given \\(B\\) defined :\\[\nP[|B] = \\frac{P[\\cap B]}{P[B]}, \\quad \\text{provided } P[B] \\neq 0.\n\\]","code":""},{"path":"prerequisites.html","id":"independent-events","chapter":"2 Prerequisites","heading":"2.2.1.2 Independent Events","text":"Two events \\(\\) \\(B\\) independent :\\(P[\\cap B] = P[]P[B]\\)\\(P[|B] = P[]\\)\\(P[B|] = P[B]\\)collection events \\(A_1, A_2, \\dots, A_n\\) independent every subcollection independent.","code":""},{"path":"prerequisites.html","id":"multiplication-rule","chapter":"2 Prerequisites","heading":"2.2.1.3 Multiplication Rule","text":"probability intersection two events can calculated : \\[\nP[\\cap B] = P[|B]P[B] = P[B|]P[].\n\\]","code":""},{"path":"prerequisites.html","id":"bayes-theorem","chapter":"2 Prerequisites","heading":"2.2.1.4 Bayes’ Theorem","text":"Let \\(A_1, A_2, \\dots, A_n\\) collection mutually exclusive events whose union \\(S\\), let \\(B\\) event \\(P[B] \\neq 0\\). , event \\(A_j\\) (\\(j = 1, 2, \\dots, n\\)): \\[\nP[A_j|B] = \\frac{P[B|A_j]P[A_j]}{\\sum_{=1}^n P[B|A_i]P[A_i]}.\n\\]","code":""},{"path":"prerequisites.html","id":"jensens-inequality","chapter":"2 Prerequisites","heading":"2.2.1.5 Jensen’s Inequality","text":"\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) convex, : \\[\nE[g(X)] \\geq g(E[X])\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]\\(g(x)\\) concave, : \\[\nE[g(X)] \\leq g(E[X]).\n\\]","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectations","chapter":"2 Prerequisites","heading":"2.2.1.6 Law of Iterated Expectations","text":"law iterated expectations states: \\[\nE[Y] = E[E[Y|X]].\n\\]","code":""},{"path":"prerequisites.html","id":"correlation-and-independence","chapter":"2 Prerequisites","heading":"2.2.1.7 Correlation and Independence","text":"strength relationship random variables can ranked strongest weakest :Independence:\n\\(f(x, y) = f_X(x)f_Y(y)\\)\n\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\n\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)\n\\(f(x, y) = f_X(x)f_Y(y)\\)\\(f_{Y|X}(y|x) = f_Y(y)\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\)Mean Independence (implied independence):\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\n\\(E[Xg(Y)] = E[X]E[g(Y)]\\)\n\\(Y\\) mean independent \\(X\\) : \\[\nE[Y|X] = E[Y].\n\\]\\(E[Xg(Y)] = E[X]E[g(Y)]\\)Uncorrelatedness (implied independence mean independence):\n\\(\\text{Cov}(X, Y) = 0\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\\(E[XY] = E[X]E[Y]\\)\n\\(\\text{Cov}(X, Y) = 0\\)\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\\(E[XY] = E[X]E[Y]\\)","code":""},{"path":"prerequisites.html","id":"central-limit-theorem","chapter":"2 Prerequisites","heading":"2.2.2 Central Limit Theorem","text":"Central Limit Theorem states sufficiently large sample size (\\(n \\geq 25\\)), sampling distribution sample mean proportion approaches normal distribution, regardless population’s original distribution.Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) distribution \\(X\\) mean \\(\\mu\\) variance \\(\\sigma^2\\). , large \\(n\\):sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample mean \\(\\bar{X}\\) approximately normal: \\[\n\\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]sample proportion \\(\\hat{p}\\) approximately normal: \\[\n\\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal: \\[\n\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]difference sample means \\(\\bar{X}_1 - \\bar{X}_2\\) approximately normal: \\[\n\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}.\n\\]following random variables approximately standard normal:\n\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\n\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\n\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\nfollowing random variables approximately standard normal:\\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\)\\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)","code":""},{"path":"prerequisites.html","id":"limiting-distribution-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.2.2.1 Limiting Distribution of the Sample Mean","text":"\\(\\{X_i\\}_{=1}^{n}\\) iid random sample distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), sample mean \\(\\bar{X}\\) scaled \\(\\sqrt{n}\\) following limiting distribution:\\[\n\\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2).\n\\]Standardizing sample mean gives: \\[\n\\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\xrightarrow{d} N(0, 1).\n\\]Notes:CLT holds random samples distribution (continuous, discrete, unknown).extends multivariate case: random sample random vector converges multivariate normal distribution.","code":""},{"path":"prerequisites.html","id":"asymptotic-variance-and-limiting-variance","chapter":"2 Prerequisites","heading":"2.2.2.2 Asymptotic Variance and Limiting Variance","text":"Asymptotic Variance (Avar): \\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\sigma^2.\n\\]Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.Refers variance limiting distribution estimator sample size (\\(n\\)) approaches infinity.characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).characterizes variability scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) asymptotic distribution (e.g., normal distribution).Limiting Variance (\\(\\lim_{n \\\\infty} Var\\))\\[\n\\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2\n\\]Represents value actual variance \\(\\sqrt{n}(\\bar{x} - \\mu)\\) converges \\(n \\\\infty\\).well-behaved estimator,\\[\nAvar(\\sqrt{n}(\\bar{X} - \\mu)) = \\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2.\n\\]However, asymptotic variance necessarily equal limiting value variance asymptotic variance derived limiting distribution, limiting variance convergence result sequence variances.\\[\nAvar(.) \\neq lim_{n \\\\infty} Var(.)\n\\]asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.asymptotic variance \\(Avar\\) limiting variance \\(\\lim_{n \\\\infty} Var\\) numerically equal \\(\\sigma^2\\), conceptual definitions differ.\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\n\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\n\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\n\\(Avar(\\cdot) \\neq \\lim_{n \\\\infty} Var(\\cdot)\\). emphasizes numerical result may match, derivation meaning differ:\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(Avar\\) depends asymptotic (large-sample) distribution estimator.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.\\(\\lim_{n \\\\infty} Var(\\cdot)\\) involves sequence variances \\(n\\) grows.Cases two match:Sample Quantiles: Consider sample quantile order \\(p\\), \\(0 < p < 1\\). regularity conditions, asymptotic distribution sample quantile normal, variance depends \\(p\\) density distribution \\(p\\)-th quantile. However, variance sample quantile necessarily converge limit sample size grows.Bootstrap Methods: using bootstrapping techniques estimate distribution statistic, bootstrap distribution might converge different limiting distribution original statistic. cases, variance bootstrap distribution (bootstrap variance) might differ limiting variance original statistic.Statistics Randomly Varying Asymptotic Behavior: cases, asymptotic behavior statistic can vary randomly depending sample path. statistics, asymptotic variance might provide consistent estimate limiting variance.M-estimators Varying Asymptotic Behavior: M-estimators can sometimes different asymptotic behaviors depending tail behavior underlying distribution. heavy-tailed distributions, variance estimator might stabilize even sample size grows large, making asymptotic variance different variance limiting distribution.","code":""},{"path":"prerequisites.html","id":"random-variable","chapter":"2 Prerequisites","heading":"2.2.3 Random Variable","text":"Random variables can categorized either discrete continuous, distinct properties functions defining type.Expected Value Properties\\(E[c] = c\\) constant \\(c\\).\\(E[cX] = cE[X]\\) constant \\(c\\).\\(E[X + Y] = E[X] + E[Y]\\).\\(E[XY] = E[X]E[Y]\\) (\\(X\\) \\(Y\\) independent).Variance Properties\\(\\text{Var}(c) = 0\\) constant \\(c\\).\\(\\text{Var}(cX) = c^2 \\text{Var}(X)\\) constant \\(c\\).\\(\\text{Var}(X) \\geq 0\\).\\(\\text{Var}(X) = E[X^2] - (E[X])^2\\).\\(\\text{Var}(X + c) = \\text{Var}(X)\\).\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (\\(X\\) \\(Y\\) independent).standard deviation \\(\\sigma\\) given : \\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\text{Var}(X)}.\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-random-variables","chapter":"2 Prerequisites","heading":"2.2.3.1 Multivariate Random Variables","text":"Suppose \\(y_1, \\dots, y_p\\) random variables means \\(\\mu_1, \\dots, \\mu_p\\). :\\[\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_p\n\\end{bmatrix}, \\quad E[\\mathbf{y}] = \\begin{bmatrix}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix} = \\boldsymbol{\\mu}.\n\\]covariance \\(y_i\\) \\(y_j\\) \\(\\sigma_{ij} = \\text{Cov}(y_i, y_j)\\). variance-covariance (dispersion) matrix :\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij})= \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n\\end{bmatrix}.\n\\]\\(\\mathbf{\\Sigma}\\) symmetric \\((p+1)p/2\\) unique parameters.Alternatively, let \\(u_{p \\times 1}\\) \\(v_{v \\times 1}\\) random vectors means \\(\\mathbf{\\mu_u}\\) \\(\\mathbf{\\mu_v}\\). \\[ \\mathbf{\\Sigma_{uv}} = cov(\\mathbf{u,v}) = E[\\mathbf{(u-\\mu_u)(v-\\mu_v)'}] \\]\\(\\Sigma_{uv} \\neq \\Sigma_{vu}\\) (\\(\\Sigma_{uv} = \\Sigma_{vu}'\\))Properties Covariance MatricesSymmetry: \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\).Eigen-Decomposition (spectral decomposition,symmetric decomposition): \\(\\mathbf{\\Sigma = \\Phi \\Lambda \\Phi}\\), \\(\\mathbf{\\Phi}\\) matrix eigenvectors \\(\\mathbf{\\Phi \\Phi' = }\\) (orthonormal), \\(\\mathbf{\\Lambda}\\) diagonal matrix eigenvalues \\((\\lambda_1,...,\\lambda_p)\\) diagonal.Non-Negative Definiteness: \\(\\mathbf{\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\). Equivalently, eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge ... \\ge \\lambda_p \\ge 0\\)Generalized Variance: \\(|\\mathbf{\\Sigma}| = \\lambda_1 \\dots \\lambda_p \\geq 0\\).Trace: \\(\\text{tr}(\\mathbf{\\Sigma}) = \\lambda_1 + \\dots + \\lambda_p = \\sigma_{11} + \\dots+ \\sigma_{pp} = \\sum \\sigma_{ii}\\) = sum variances (total variance).Note: \\(\\mathbf{\\Sigma}\\) required positive definite. implies eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma}= \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\)","code":""},{"path":"prerequisites.html","id":"correlation-matrices","chapter":"2 Prerequisites","heading":"2.2.3.2 Correlation Matrices","text":"correlation coefficient \\(\\rho_{ij}\\) correlation matrix \\(\\mathbf{R}\\) defined :\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}, \\quad \\mathbf{R} = \\begin{bmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\dots & 1\n\\end{bmatrix}.\n\\]\\(\\rho_{ii} = 1 \\forall \\)","code":""},{"path":"prerequisites.html","id":"linear-transformations","chapter":"2 Prerequisites","heading":"2.2.3.3 Linear Transformations","text":"Let \\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants, \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants. :\\(E[\\mathbf{Ay + c}] = \\mathbf{\\mu_y + c}\\).\\(\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{\\Sigma_y '}\\).\\(\\text{Cov}(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B'}\\).","code":""},{"path":"prerequisites.html","id":"moment-generating-function-mgf","chapter":"2 Prerequisites","heading":"2.2.4 Moment Generating Function (MGF)","text":"","code":""},{"path":"prerequisites.html","id":"properties-of-the-moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4.1 Properties of the Moment Generating Function","text":"\\(\\frac{d^k(m_X(t))}{dt^k} \\bigg|_{t=0} = E[X^k]\\) (\\(k\\)-th derivative \\(t=0\\) gives \\(k\\)-th moment \\(X\\)).\\(\\mu = E[X] = m_X'(0)\\) (first derivative \\(t=0\\) gives mean).\\(E[X^2] = m_X''(0)\\) (second derivative \\(t=0\\) gives second moment).","code":""},{"path":"prerequisites.html","id":"theorems-involving-mgfs","chapter":"2 Prerequisites","heading":"2.2.4.2 Theorems Involving MGFs","text":"Let \\(X_1, X_2, \\dots, X_n, Y\\) random variables MGFs \\(m_{X_1}(t), m_{X_2}(t), \\dots, m_{X_n}(t), m_Y(t)\\):\\(m_{X_1}(t) = m_{X_2}(t)\\) \\(t\\) open interval 0, \\(X_1\\) \\(X_2\\) distribution.\\(Y = \\alpha + \\beta X_1\\), : \\[\nm_Y(t) = e^{\\alpha t}m_{X_1}(\\beta t).\n\\]\\(X_1, X_2, \\dots, X_n\\) independent \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), \\(\\alpha_0, \\alpha_1, \\dots, \\alpha_n\\) constants, : \\[\nm_Y(t) = e^{\\alpha_0 t} m_{X_1}(\\alpha_1 t) m_{X_2}(\\alpha_2 t) \\dots m_{X_n}(\\alpha_n t).\n\\]Suppose \\(X_1, X_2, \\dots, X_n\\) independent normal random variables means \\(\\mu_1, \\mu_2, \\dots, \\mu_n\\) variances \\(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2\\). \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), :\n\\(Y\\) normally distributed.\nMean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).\nVariance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).\n\\(Y\\) normally distributed.Mean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\).Variance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\).","code":""},{"path":"prerequisites.html","id":"moments","chapter":"2 Prerequisites","heading":"2.2.5 Moments","text":"Skewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\)Kurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\)","code":""},{"path":"prerequisites.html","id":"conditional-moments","chapter":"2 Prerequisites","heading":"2.2.5.1 Conditional Moments","text":"random variable \\(Y\\) given \\(X=x\\):Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Expected Value: \\[\nE[Y|X=x] =\n\\begin{cases}\n\\sum_y y f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y y f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]Variance: \\[\n\\text{Var}(Y|X=x) =\n\\begin{cases}\n\\sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \\text{discrete RV}, \\\\\n\\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy & \\text{continuous RV}.\n\\end{cases}\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-moments","chapter":"2 Prerequisites","heading":"2.2.5.2 Multivariate Moments","text":"Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}.\n\\]Expected Value: \\[\nE\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[X] \\\\\nE[Y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}.\n\\]Variance-Covariance Matrix: \\[\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}.\n\\]Variance-Covariance Matrix: \\[\n\\text{Var}\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n\\text{Cov}(X, Y) & \\text{Var}(Y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nE[(X-\\mu_X)^2] & E[(X-\\mu_X)(Y-\\mu_Y)] \\\\\nE[(X-\\mu_X)(Y-\\mu_Y)] & E[(Y-\\mu_Y)^2]\n\\end{bmatrix}.\n\\]","code":""},{"path":"prerequisites.html","id":"properties-of-moments","chapter":"2 Prerequisites","heading":"2.2.5.3 Properties of Moments","text":"\\(E[aX + + c] = aE[X] + [Y] + c\\)\\(\\text{Var}(aX + + c) = ^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\\)\\(\\text{Cov}(aX + , cX + dY) = ac \\text{Var}(X) + bd \\text{Var}(Y) + (ad + bc) \\text{Cov}(X, Y)\\)Correlation: \\(\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)","code":""},{"path":"prerequisites.html","id":"distributions","chapter":"2 Prerequisites","heading":"2.2.6 Distributions","text":"","code":""},{"path":"prerequisites.html","id":"conditional-distributions","chapter":"2 Prerequisites","heading":"2.2.6.1 Conditional Distributions","text":"\\[\nf_{X|Y}(x|y) = \\frac{f(x, y)}{f_Y(y)}\n\\]\\(X\\) \\(Y\\) independent: \\[\nf_{X|Y}(x|y) = f_X(x).\n\\]","code":""},{"path":"prerequisites.html","id":"discrete-distributions","chapter":"2 Prerequisites","heading":"2.2.6.2 Discrete Distributions","text":"","code":""},{"path":"prerequisites.html","id":"bernoulli-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.1 Bernoulli Distribution","text":"random variable \\(X\\) follows Bernoulli distribution, denoted \\(X \\sim \\text{Bernoulli}(p)\\), represents single trial :Success probability \\(p\\)Success probability \\(p\\)Failure probability \\(q = 1-p\\).Failure probability \\(q = 1-p\\).Density Function\\[\nf(x) = p^x (1-p)^{1-x}, \\quad x \\\\{0, 1\\}\n\\]CDF: Use table manual computation.PDFMean\\[\n\\mu = E[X] = p\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = p(1-p)\n\\]","code":"\nhist(\n    mc2d::rbern(1000, prob = 0.5),\n    main = \"Histogram of Bernoulli Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"binomial-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.2 Binomial Distribution","text":"\\(X \\sim B(n, p)\\) number successes \\(n\\) independent Bernoulli trials, :\\(n\\) number trials\\(n\\) number trials\\(p\\) success probability.\\(p\\) success probability.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.trials identical independent, probability success (\\(p\\)) probability failure (\\(q = 1 - p\\)) remains trials.Density Function\\[\nf(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n\n\\]PDFMGF\\[\nm_X(t) = (1 - p + p e^t)^n\n\\]Mean\\[\n\\mu = np\n\\]Variance\\[\n\\sigma^2 = np(1-p)\n\\]","code":"\nhist(\n    rbinom(1000, size = 100, prob = 0.5),\n    main = \"Histogram of Binomial Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"poisson-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.3 Poisson Distribution","text":"\\(X \\sim \\text{Poisson}(\\lambda)\\) models number occurrences event fixed interval, average rate \\(\\lambda\\).Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.random variable \\(X\\) number occurrences event within interval \\(s\\) units.random variable \\(X\\) number occurrences event within interval \\(s\\) units.parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k = \\lambda s\\).Density Function\\[\nf(x) = \\frac{e^{-k} k^x}{x!}, \\quad x = 0, 1, 2, \\dots\n\\]CDFPDFMGF\\[\nm_X(t) = e^{k (e^t - 1)}\n\\]Mean\\[\n\\mu = E(X) = k\n\\]Variance\\[\n\\sigma^2 = Var(X) = k\n\\]","code":"\nhist(rpois(1000, lambda = 5),\n     main = \"Histogram of Poisson Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"geometric-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.4 Geometric Distribution","text":"\\(X \\sim \\text{G}(p)\\) models number trials needed obtain first success, :\\(p\\): probability success\\(p\\): probability success\\(q = 1-p\\): probability failure.\\(q = 1-p\\): probability failure.experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (.e., Bernoulli trial).trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.trials identical independent sense outcome one trial effect outcome (..e, lack memory - momerylessness). probability success (\\(p\\)) probability failure (\\(q = 1- p\\)) remains trial trial.Density Function\\[\nf(x) = p(1-p)^{x-1}, \\quad x = 1, 2, \\dots\n\\]CDF\\[\nF(x) = 1 - (1-p)^x\n\\]PDFMGF\\[\nm_X(t) = \\frac{p e^t}{1 - (1-p)e^t}, \\quad t < -\\ln(1-p)\n\\]Mean\\[\n\\mu = \\frac{1}{p}\n\\]Variance\\[\n\\sigma^2 = \\frac{1-p}{p^2}\n\\]","code":"\nhist(rgeom(1000, prob = 0.5),\n     main = \"Histogram of Geometric Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"hypergeometric-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.5 Hypergeometric Distribution","text":"\\(X \\sim \\text{H}(N, r, n)\\) models number successes sample size \\(n\\) drawn without replacement population size \\(N\\), :\\(r\\) objects trait interest\\(r\\) objects trait interest\\(N-r\\) trait.\\(N-r\\) trait.Density Function\\[\nf(x) = \\frac{\\binom{r}{x} \\binom{N-r}{n-x}}{\\binom{N}{n}}, \\quad \\max(0, n-(N-r)) \\leq x \\leq \\min(n, r)\n\\]PDFMean\\[\n\\mu = E[X] = \\frac{n r}{N}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = n \\frac{r}{N} \\frac{N-r}{N} \\frac{N-n}{N-1}\n\\]Note: large \\(N\\) (\\(\\frac{n}{N} \\leq 0.05\\)), hypergeometric distribution can approximated binomial distribution \\(p = \\frac{r}{N}\\).","code":"\nhist(\n    rhyper(1000, m = 50, n = 20, k = 30),\n    main = \"Histogram of Hypergeometric Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"continuous-distributions","chapter":"2 Prerequisites","heading":"2.2.6.3 Continuous Distributions","text":"","code":""},{"path":"prerequisites.html","id":"uniform-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.1 Uniform Distribution","text":"Defined interval \\((, b)\\), probabilities “equally likely” subintervals equal length.Density Function: \\[\nf(x) = \\frac{1}{b-}, \\quad < x < b\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x < \\\\\n\\frac{x-}{b-} & \\le x \\le b \\\\\n1 & \\text{} x > b\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) =\n\\begin{cases}\n\\frac{e^{tb} - e^{ta}}{t(b-)} & \\text{} t \\neq 0 \\\\\n1 & \\text{} t = 0\n\\end{cases}\n\\]Mean\\[\n\\mu = E[X] = \\frac{+ b}{2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{(b-)^2}{12}\n\\]","code":"\nhist(\n    runif(1000, min = 0, max = 1),\n    main = \"Histogram of Uniform Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"gamma-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.2 Gamma Distribution","text":"gamma distribution used define exponential \\(\\chi^2\\) distributions.gamma function defined : \\[\n\\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz, \\quad \\alpha > 0\n\\]Properties Gamma Function:\\(\\Gamma(1) = 1\\)\\(\\Gamma(1) = 1\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(\\alpha > 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)\\(n\\) integer \\(n > 1\\), \\(\\Gamma(n) = (n-1)!\\)Density Function:\\[\nf(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n\\]CDF (\\(\\alpha = n\\), \\(x>0\\) positive integer):\\[\nF(x, n, \\beta) = 1 - \\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!}\n\\]PDF:MGF\\[\nm_X(t) = (1 - \\beta t)^{-\\alpha}, \\quad t < \\frac{1}{\\beta}\n\\]Mean\\[\n\\mu = E[X] = \\alpha \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\alpha \\beta^2\n\\]","code":"\nhist(\n    rgamma(n = 1000, shape = 5, rate = 1),\n    main = \"Histogram of Gamma Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"normal-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.3 Normal Distribution","text":"normal distribution, denoted \\(N(\\mu, \\sigma^2)\\), symmetric bell-shaped parameters \\(\\mu\\) (mean) \\(\\sigma^2\\) (variance). also known Gaussian distribution.Density Function:\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\quad -\\infty < x < \\infty, \\; \\sigma > 0\n\\]CDF: Use table numerical methods.PDFMGF\\[\nm_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}\n\\]Mean\\[\n\\mu = E[X]\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X)\n\\]Standard Normal Random Variable:normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(Z\\) mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\) called standard normal random variable.normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]normal random variable \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z\\): \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]Normal Approximation Binomial Distribution:Let \\(X\\) binomial parameters \\(n\\) \\(p\\). large \\(n\\):\\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p \\le 0.5\\) \\(np > 5\\), orIf \\(p > 0.5\\) \\(n(1-p) > 5\\),\\(p > 0.5\\) \\(n(1-p) > 5\\),\\(X\\) approximately normally distributed mean \\(\\mu = np\\) standard deviation \\(\\sigma = \\sqrt{np(1-p)}\\).using normal approximation, add subtract 0.5 needed continuity correction.Discrete Approximate Normal (Corrected):Normal Probability RuleIf X normally distributed parameters \\(\\mu\\) \\(\\sigma\\), \\(P(-\\sigma < X - \\mu < \\sigma) \\approx .68\\)\\(P(-2\\sigma < X - \\mu < 2\\sigma) \\approx .95\\)\\(P(-3\\sigma < X - \\mu < 3\\sigma) \\approx .997\\)","code":"\nhist(\n    rnorm(1000, mean = 0, sd = 1),\n    main = \"Histogram of Normal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"logistic-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.4 Logistic Distribution","text":"logistic distribution continuous probability distribution commonly used logistic regression types statistical modeling. resembles normal distribution heavier tails, allowing extreme values. - logistic distribution symmetric around \\(\\mu\\). - heavier tails make useful modeling outcomes occasional extreme values.Density Function\\[\nf(x; \\mu, s) = \\frac{e^{-(x-\\mu)/s}}{s \\left(1 + e^{-(x-\\mu)/s}\\right)^2}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(s > 0\\) scale parameter.CDF\\[\nF(x; \\mu, s) = \\frac{1}{1 + e^{-(x-\\mu)/s}}, \\quad -\\infty < x < \\infty\n\\]PDFMGFThe MGF logistic distribution exist expected value diverges \\(t\\).Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{\\pi^2 s^2}{3}\n\\]","code":"\nhist(\n    rlogis(1000, location = 0, scale = 1),\n    main = \"Histogram of Logistic Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"laplace-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.5 Laplace Distribution","text":"Laplace distribution, also known double exponential distribution, continuous probability distribution often used economics, finance, engineering. characterized peak mean heavier tails compared normal distribution.Laplace distribution symmetric around \\(\\mu\\).heavier tails normal distribution, making suitable modeling data extreme outliers.Density Function\\[\nf(x; \\mu, b) = \\frac{1}{2b} e^{-|x-\\mu|/b}, \\quad -\\infty < x < \\infty\n\\]\\(\\mu\\) location parameter (mean) \\(b > 0\\) scale parameter.CDF\\[\nF(x; \\mu, b) =\n\\begin{cases}\n    \\frac{1}{2} e^{(x-\\mu)/b} & \\text{} x < \\mu \\\\\n    1 - \\frac{1}{2} e^{-(x-\\mu)/b} & \\text{} x \\ge \\mu\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = \\frac{e^{\\mu t}}{1 - b^2 t^2}, \\quad |t| < \\frac{1}{b}\n\\]Mean\\[\n\\mu = E[X] = \\mu\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2b^2\n\\]","code":"\nhist(\n    VGAM::rlaplace(1000, location = 0, scale = 1),\n    main = \"Histogram of Laplace Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"log-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.6 Log-normal Distribution","text":"log-normal distribution denoted \\(\\text{Lognormal}(\\mu, \\sigma^2)\\).PDF","code":"\nhist(rlnorm(n = 1000, meanlog = 0, sdlog = 1), main=\"Histogram of Log-normal Distribution\", xlab=\"Value\", ylab=\"Frequency\")"},{"path":"prerequisites.html","id":"lognormal-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.7 Lognormal Distribution","text":"lognormal distribution continuous probability distribution random variable whose logarithm normally distributed. often used model variables positively skewed, income biological measurements.lognormal distribution positively skewed.useful modeling data take negative values often used finance environmental studies.Density Function\\[\nf(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-(\\ln(x) - \\mu)^2 / (2\\sigma^2)}, \\quad x > 0\n\\]\\(\\mu\\) mean underlying normal distribution \\(\\sigma > 0\\) standard deviation.CDFThe cumulative distribution function lognormal distribution given :\\[\nF(x; \\mu, \\sigma) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{\\ln(x) - \\mu}{\\sigma \\sqrt{2}} \\right) \\right], \\quad x > 0\n\\]PDFMGFThe moment generating function (MGF) lognormal distribution exist simple closed form.Mean\\[\nE[X] = e^{\\mu + \\sigma^2 / 2}\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\left( e^{\\sigma^2} - 1 \\right) e^{2\\mu + \\sigma^2}\n\\]","code":"\nhist(\n    rlnorm(1000, meanlog = 0, sdlog = 1),\n    main = \"Histogram of Lognormal Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"exponential-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.8 Exponential Distribution","text":"exponential distribution, denoted \\(\\text{Exp}(\\lambda)\\), special case gamma distribution \\(\\alpha = 1\\).commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.commonly used model time independent events occur constant rate. often applied reliability analysis queuing theory.exponential distribution memoryless, meaning probability event occurring future independent past.exponential distribution memoryless, meaning probability event occurring future independent past.commonly used model waiting times, time next customer arrives time radioactive particle decays.commonly used model waiting times, time next customer arrives time radioactive particle decays.Density Function\\[\nf(x) = \\frac{1}{\\beta} e^{-x/\\beta}, \\quad x, \\beta > 0\n\\]CDF\\[\nF(x) =\n\\begin{cases}\n0 & \\text{} x \\le 0 \\\\\n1 - e^{-x/\\beta} & \\text{} x > 0\n\\end{cases}\n\\]PDFMGF\\[\nm_X(t) = (1-\\beta t)^{-1}, \\quad t < 1/\\beta\n\\]Mean\\[\n\\mu = E[X] = \\beta\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = \\beta^2\n\\]","code":"\nhist(rexp(n = 1000, rate = 1),\n     main = \"Histogram of Exponential Distribution\",\n     xlab = \"Value\",\n     ylab = \"Frequency\")"},{"path":"prerequisites.html","id":"chi-squared-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.9 Chi-Squared Distribution","text":"chi-squared distribution continuous probability distribution commonly used statistical inference, particularly hypothesis testing construction confidence intervals variance. also used goodness--fit tests.chi-squared distribution defined positive values.often used model distribution sum squares \\(k\\) independent standard normal random variables.Density Function\\[\nf(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x \\ge 0\n\\]\\(k\\) degrees freedom \\(\\Gamma\\) gamma function.CDFThe cumulative distribution function chi-squared distribution given :\\[\nF(x; k) = \\frac{\\gamma(k/2, x/2)}{\\Gamma(k/2)}, \\quad x \\ge 0\n\\]\\(\\gamma\\) lower incomplete gamma function.PDFMGF\\[\nm_X(t) = (1 - 2t)^{-k/2}, \\quad t < \\frac{1}{2}\n\\]Mean\\[\nE[X] = k\n\\]Variance\\[\n\\sigma^2 = \\text{Var}(X) = 2k\n\\]","code":"\nhist(\n    rchisq(1000, df = 5),\n    main = \"Histogram of Chi-Squared Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"students-t-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.10 Student’s T Distribution","text":"Student’s t-distribution, denoted \\(T(v)\\), defined : \\[\nT = \\frac{Z}{\\sqrt{\\chi^2_v / v}},\n\\] \\(Z\\) standard normal random variable \\(\\chi^2_v\\) follows chi-squared distribution \\(v\\) degrees freedom.Student’s T distribution continuous probability distribution used statistical inference, particularly estimating population parameters sample size small /population variance unknown. similar normal distribution heavier tails, makes robust small sample sizes.Student’s T distribution symmetric around 0.heavier tails normal distribution, making useful dealing outliers small sample sizes.Density Function\\[\nf(x;u) = \\frac{\\Gamma((u + 1)/2)}{\\sqrt{u \\pi} \\Gamma(u/2)} \\left( 1 + \\frac{x^2}{u}\n\\right)^{-(u + 1)/2}\n\\]\\(u\\) degrees freedom \\(\\Gamma(x)\\) Gamma function.CDFThe cumulative distribution function Student’s T distribution complex typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) Student’s T distribution exist simple closed form.MeanFor \\(u > 1\\):\\[\nE[X] = 0\n\\]VarianceFor \\(u > 2\\):\\[\n\\sigma^2 =  \\text{Var}(X) = \\frac{\nu}{u - 2}\n\\]","code":"\nhist(\n    rt(1000, df = 5),\n    main = \"Histogram of Student's T Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"f-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.11 F Distribution","text":"F-distribution, denoted \\(F(d_1, d_2)\\), strictly positive used compare variances.Definition: \\[\nF = \\frac{\\chi^2_{d_1} / d_1}{\\chi^2_{d_2} / d_2},\n\\] \\(\\chi^2_{d_1}\\) \\(\\chi^2_{d_2}\\) independent chi-squared random variables degrees freedom \\(d_1\\) \\(d_2\\), respectively.distribution asymmetric never negative.F distribution arises frequently null distribution test statistic, especially context comparing variances, analysis variance (ANOVA).Density Function\\[\nf(x; d_1, d_2) = \\frac{\\sqrt{\\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\\left( \\frac{d_1}{2}, \\frac{d_2}{2} \\right)}, \\quad x > 0\n\\]\\(d_1\\) \\(d_2\\) degrees freedom \\(B\\) beta function.CDFThe cumulative distribution function F distribution typically evaluated using numerical methods.PDFMGFThe moment generating function (MGF) F distribution exist simple closed form.MeanFor \\(d_2 > 2\\):\\[\nE[X] = \\frac{d_2}{d_2 - 2}\n\\]VarianceFor \\(d_2 > 4\\):\\[\n\\sigma^2 = \\text{Var}(X) = \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}\n\\]","code":"\nhist(\n    rf(1000, df1 = 5, df2 = 2),\n    main = \"Histogram of F Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"cauchy-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.12 Cauchy Distribution","text":"Cauchy distribution continuous probability distribution often used physics heavier tails normal distribution. notable finite mean variance.Cauchy distribution finite mean variance.Central Limit Theorem Weak Law Large Numbers apply Cauchy distribution.Density Function\\[\nf(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\left[ 1 + \\left( \\frac{x - x_0}{\\gamma}\n\\right)^2 \\right]}\n\\]\\(x_0\\) location parameter \\(\\gamma > 0\\) scale parameter.CDFThe cumulative distribution function Cauchy distribution given :\\[\nF(x; x_0, \\gamma) = \\frac{1}{\\pi} \\arctan \\left( \\frac{x - x_0}{\\gamma}  \\right) + \\frac{1}{2}\n\\]PDFMGFThe MGF Cauchy distribution exist.MeanThe mean Cauchy distribution undefined.VarianceThe variance Cauchy distribution undefined.","code":"\nhist(\n    rcauchy(1000, location = 0, scale = 1),\n    main = \"Histogram of Cauchy Distribution\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"multivariate-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.6.3.13 Multivariate Normal Distribution","text":"Let \\(y\\) \\(p\\)-dimensional multivariate normal (MVN) random variable mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\). density function \\(y\\) given :\\[ f(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mu)' \\Sigma^{-1} (\\mathbf{y}-\\mu)\\right) \\]\\(|\\mathbf{\\Sigma}|\\) represents determinant variance-covariance matrix \\(\\Sigma\\), \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\).Properties:Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{y} \\sim N_r(\\mathbf{\\mu}, \\mathbf{\\Sigma '})\\). Note \\(r \\le p\\), rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma '}\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma^{-1} = G G'}\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G'\\mu}, \\mathbf{})\\) \\(\\mathbf{G'(y - \\mu)} \\sim N_p(\\mathbf{0}, \\mathbf{})\\).fixed linear combination \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c'y}\\), follows \\(\\mathbf{c'y} \\sim N_1(\\mathbf{c'\\mu}, \\mathbf{c'\\Sigma c})\\).Large Sample PropertiesSuppose \\(y_1, \\dots, y_n\\) random sample population mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\):\\[ \\mathbf{Y} \\sim MVN(\\mathbf{\\mu}, \\mathbf{\\Sigma}) \\]:\\(\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{=1}^n \\mathbf{y}_i\\) consistent estimator \\(\\mathbf{\\mu}\\).\\(\\mathbf{S} = \\frac{1}{n-1} \\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})'\\) consistent estimator \\(\\mathbf{\\Sigma}\\).Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\sim N_p(\\mathbf{0}, \\mathbf{\\Sigma})\\) \\(n\\) large relative \\(p\\) (e.g., \\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma/n})\\).Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S^{-1}} (\\bar{\\mathbf{y}} - \\mu) \\sim \\chi^2_{(p)}\\) \\(n\\) large relative \\(p\\).Density Function\\[\nf(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} | \\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\n\\right)\n\\]\\(\\boldsymbol{\\mu}\\) mean vector, \\(\\boldsymbol{\\Sigma}\\) covariance matrix, \\(\\mathbf{x} \\\\mathbb{R}^k\\) \\(k\\) number variables.CDFThe cumulative distribution function multivariate normal distribution simple closed form typically evaluated using numerical methods.PDFMGF\\[\nm_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left(\\boldsymbol{\\mu}^T \\mathbf{t} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t}\n\\right)\n\\]Mean\\[\nE[\\mathbf{X}] = \\boldsymbol{\\mu}\n\\]Variance\\[\n\\text{Var}(\\mathbf{X}) = \\boldsymbol{\\Sigma}\n\\]","code":"\nk <- 2\nn <- 1000\nmu <- c(0, 0)\nsigma <- matrix(c(1, 0.5, 0.5, 1), nrow = k)\nlibrary(MASS)\nhist(\n    mvrnorm(n, mu = mu, Sigma = sigma)[,1],\n    main = \"Histogram of MVN Distribution (1st Var)\",\n    xlab = \"Value\",\n    ylab = \"Frequency\"\n)"},{"path":"prerequisites.html","id":"general-math","chapter":"2 Prerequisites","heading":"2.3 General Math","text":"","code":""},{"path":"prerequisites.html","id":"number-sets","chapter":"2 Prerequisites","heading":"2.3.1 Number Sets","text":"","code":""},{"path":"prerequisites.html","id":"summation-notation-and-series","chapter":"2 Prerequisites","heading":"2.3.2 Summation Notation and Series","text":"","code":""},{"path":"prerequisites.html","id":"chebyshevs-inequality","chapter":"2 Prerequisites","heading":"2.3.2.1 Chebyshev’s Inequality","text":"Let \\(X\\) random variable mean \\(\\mu\\) standard deviation \\(\\sigma\\). positive number \\(k\\), Chebyshev’s Inequality states:\\[\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\n\\]provides probabilistic bound deviation \\(X\\) mean require \\(X\\) follow normal distribution.","code":""},{"path":"prerequisites.html","id":"geometric-sum","chapter":"2 Prerequisites","heading":"2.3.2.2 Geometric Sum","text":"geometric series form \\(\\sum_{k=0}^{n-1} ar^k\\), sum given :\\[\n\\sum_{k=0}^{n-1} ar^k = \\frac{1-r^n}{1-r} \\quad \\text{} r \\neq 1\n\\]","code":""},{"path":"prerequisites.html","id":"infinite-geometric-series","chapter":"2 Prerequisites","heading":"2.3.2.3 Infinite Geometric Series","text":"\\(|r| < 1\\), geometric series converges :\\[\n\\sum_{k=0}^\\infty ar^k = \\frac{}{1-r}\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-theorem","chapter":"2 Prerequisites","heading":"2.3.2.4 Binomial Theorem","text":"binomial expansion \\((x + y)^n\\) :\\[\n(x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} y^k \\quad \\text{} n \\geq 0\n\\]","code":""},{"path":"prerequisites.html","id":"binomial-series","chapter":"2 Prerequisites","heading":"2.3.2.5 Binomial Series","text":"non-integer exponents \\(\\alpha\\):\\[\n\\sum_{k=0}^\\infty \\binom{\\alpha}{k} x^k = (1 + x)^\\alpha \\quad \\text{} |x| < 1\n\\]","code":""},{"path":"prerequisites.html","id":"telescoping-sum","chapter":"2 Prerequisites","heading":"2.3.2.6 Telescoping Sum","text":"telescoping sum simplifies intermediate terms cancel, leaving:\\[\n\\sum_{\\leq k < b} \\Delta F(k) = F(b) - F() \\quad \\text{} , b \\\\mathbb{Z}, \\leq b\n\\]","code":""},{"path":"prerequisites.html","id":"vandermonde-convolution","chapter":"2 Prerequisites","heading":"2.3.2.7 Vandermonde Convolution","text":"Vandermonde convolution identity :\\[\n\\sum_{k=0}^n \\binom{r}{k} \\binom{s}{n-k} = \\binom{r+s}{n} \\quad \\text{} n \\\\mathbb{Z}\n\\]","code":""},{"path":"prerequisites.html","id":"exponential-series","chapter":"2 Prerequisites","heading":"2.3.2.8 Exponential Series","text":"exponential function \\(e^x\\) can represented :\\[\n\\sum_{k=0}^\\infty \\frac{x^k}{k!} = e^x \\quad \\text{} x \\\\mathbb{C}\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-series","chapter":"2 Prerequisites","heading":"2.3.2.9 Taylor Series","text":"Taylor series expansion function \\(f(x)\\) \\(x=\\) :\\[\n\\sum_{k=0}^\\infty \\frac{f^{(k)}()}{k!} (x-)^k = f(x)\n\\]\\(= 0\\), becomes Maclaurin series.","code":""},{"path":"prerequisites.html","id":"maclaurin-series-for-ez","chapter":"2 Prerequisites","heading":"2.3.2.10 Maclaurin Series for \\(e^z\\)","text":"special case Taylor series, Maclaurin expansion \\(e^z\\) :\\[\ne^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots\n\\]","code":""},{"path":"prerequisites.html","id":"eulers-summation-formula","chapter":"2 Prerequisites","heading":"2.3.2.11 Euler’s Summation Formula","text":"Euler’s summation formula connects sums integrals:\\[\n\\sum_{\\leq k < b} f(k) = \\int_a^b f(x) \\, dx + \\sum_{k=1}^m \\frac{B_k}{k!} \\left[f^{(k-1)}(x)\\right]_a^b\n+ (-1)^{m+1} \\int_a^b \\frac{B_m(x-\\lfloor x \\rfloor)}{m!} f^{(m)}(x) \\, dx\n\\], \\(B_k\\) Bernoulli numbers.\\(m=1\\) (Trapezoidal Rule):\\[\n\\sum_{\\leq k < b} f(k) \\approx \\int_a^b f(x) \\, dx - \\frac{1}{2}(f(b) - f())\n\\]","code":""},{"path":"prerequisites.html","id":"taylor-expansion","chapter":"2 Prerequisites","heading":"2.3.3 Taylor Expansion","text":"differentiable function, \\(G(x)\\), can written infinite sum derivatives. specifically, \\(G(x)\\) infinitely differentiable evaluated \\(\\), Taylor expansion :\\[\nG(x) = G() + \\frac{G'()}{1!} (x-) + \\frac{G''()}{2!}(x-)^2 + \\frac{G'''()}{3!}(x-)^3 + \\dots\n\\]expansion valid within radius convergence.","code":""},{"path":"prerequisites.html","id":"law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4 Law of Large Numbers","text":"Let \\(X_1, X_2, \\ldots\\) infinite sequence independent identically distributed (..d.) random variables finite mean \\(\\mu\\) variance \\(\\sigma^2\\). Law Large Numbers (LLN) states sample average:\\[\n\\bar{X}_n = \\frac{1}{n} \\sum_{=1}^n X_i\n\\]converges expected value \\(\\mu\\) \\(n \\rightarrow \\infty\\). can expressed :\\[\n\\bar{X}_n \\rightarrow \\mu \\quad \\text{($n \\rightarrow \\infty$)}.\n\\]","code":""},{"path":"prerequisites.html","id":"variance-of-the-sample-mean","chapter":"2 Prerequisites","heading":"2.3.4.1 Variance of the Sample Mean","text":"variance sample mean decreases sample size increases:\\[\nVar(\\bar{X}_n) = Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) = \\frac{\\sigma^2}{n}.\n\\]\\[\n\\begin{aligned}\nVar(\\bar{X}_n) &= Var(\\frac{1}{n}(X_1 + ... + X_n)) =Var\\left(\\frac{1}{n} \\sum_{=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2}Var(X_1 + ... + X_n) \\\\\n&=\\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}\n\\end{aligned}\n\\]Note: connection Law Large Numbers Normal Distribution lies Central Limit Theorem. CLT states , regardless original distribution dataset, distribution sample means tend follow normal distribution sample size becomes larger.difference [Weak Law] [Strong Law] regards mode convergence.","code":""},{"path":"prerequisites.html","id":"weak-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.2 Weak Law of Large Numbers","text":"Weak Law Large Numbers states sample average converges probability expected value:\\[\n\\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Formally, \\(\\epsilon > 0\\):\\[\n\\lim_{n \\\\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0.\n\\]Additionally, sample mean ..d. random sample (\\(\\{ X_i \\}_{=1}^n\\)) population finite mean variance consistent estimator population mean \\(\\mu\\):\\[\nplim(\\bar{X}_n) = plim\\left(\\frac{1}{n}\\sum_{=1}^{n} X_i\\right) = \\mu.\n\\]","code":""},{"path":"prerequisites.html","id":"strong-law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.4.3 Strong Law of Large Numbers","text":"Strong Law Large Numbers states sample average converges almost surely expected value:\\[\n\\bar{X}_n \\xrightarrow{.s.} \\mu \\quad \\text{} n \\rightarrow \\infty.\n\\]Equivalently, can expressed :\\[\nP\\left(\\lim_{n \\\\infty} \\bar{X}_n = \\mu\\right) = 1.\n\\]","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectation","chapter":"2 Prerequisites","heading":"2.3.5 Law of Iterated Expectation","text":"Law Iterated Expectation states random variables \\(X\\) \\(Y\\):\\[\nE(X) = E(E(X|Y)).\n\\]means expected value \\(X\\) can obtained first calculating conditional expectation \\(E(X|Y)\\) taking expectation quantity distribution \\(Y\\).","code":""},{"path":"prerequisites.html","id":"convergence","chapter":"2 Prerequisites","heading":"2.3.6 Convergence","text":"","code":""},{"path":"prerequisites.html","id":"convergence-in-probability","chapter":"2 Prerequisites","heading":"2.3.6.1 Convergence in Probability","text":"\\(n \\rightarrow \\infty\\), estimator (random variable) \\(\\theta_n\\) said converge probability constant \\(c\\) :\\[\n\\lim_{n \\\\infty} P(|\\theta_n - c| \\geq \\epsilon) = 0 \\quad \\text{} \\epsilon > 0.\n\\]denoted :\\[\nplim(\\theta_n) = c \\quad \\text{equivalently, } \\theta_n \\xrightarrow{p} c.\n\\]Properties Convergence Probability:Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\n\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]Slutsky’s Theorem: continuous function \\(g(\\cdot)\\), \\(plim(\\theta_n) = \\theta\\), :\\[\nplim(g(\\theta_n)) = g(\\theta)\n\\]\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\n\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\n\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\n\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).\n\\(\\gamma_n \\xrightarrow{p} \\gamma\\), :\\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\),\\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\),\\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (\\(\\gamma \\neq 0\\)).properties extend random vectors matrices.properties extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"convergence-in-distribution","chapter":"2 Prerequisites","heading":"2.3.6.2 Convergence in Distribution","text":"\\(n \\rightarrow \\infty\\), distribution random variable \\(X_n\\) may converge another (“fixed”) distribution. Formally, \\(X_n\\) CDF \\(F_n(x)\\) converges distribution \\(X\\) CDF \\(F(x)\\) :\\[\n\\lim_{n \\\\infty} |F_n(x) - F(x)| = 0\n\\]points continuity \\(F(x)\\). denoted :\\[\nX_n \\xrightarrow{d} X \\quad \\text{equivalently, } F(x) \\text{ limiting distribution } X_n.\n\\]Asymptotic Properties:\\(E(X)\\): Limiting mean (asymptotic mean).\\(Var(X)\\): Limiting variance (asymptotic variance).Note: Limiting expectations variances necessarily match expectations variances \\(X_n\\):\\[\n\\begin{aligned}\nE(X) &\\neq \\lim_{n \\\\infty} E(X_n), \\\\\nAvar(X_n) &\\neq \\lim_{n \\\\infty} Var(X_n).\n\\end{aligned}\n\\]Properties Convergence Distribution:Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\n\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]Continuous Mapping Theorem: continuous function \\(g(\\cdot)\\), \\(X_n \\xrightarrow{d} X\\), :\\[\ng(X_n) \\xrightarrow{d} g(X).\n\\]\\(Y_n \\xrightarrow{d} c\\) (constant), :\n\\(X_n + Y_n \\xrightarrow{d} X + c\\),\n\\(Y_n X_n \\xrightarrow{d} c X\\),\n\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).\n\\(Y_n \\xrightarrow{d} c\\) (constant), :\\(X_n + Y_n \\xrightarrow{d} X + c\\),\\(Y_n X_n \\xrightarrow{d} c X\\),\\(X_n / Y_n \\xrightarrow{d} X / c\\) (\\(c \\neq 0\\)).properties also extend random vectors matrices.properties also extend random vectors matrices.","code":""},{"path":"prerequisites.html","id":"summary-properties-of-convergence","chapter":"2 Prerequisites","heading":"2.3.6.3 Summary: Properties of Convergence","text":"Relationship Convergence Types:Convergence Probability stronger Convergence Distribution. Therefore:Convergence Distribution guarantee Convergence Probability.","code":""},{"path":"prerequisites.html","id":"sufficient-statistics-and-likelihood","chapter":"2 Prerequisites","heading":"2.3.7 Sufficient Statistics and Likelihood","text":"","code":""},{"path":"prerequisites.html","id":"likelihood","chapter":"2 Prerequisites","heading":"2.3.7.1 Likelihood","text":"likelihood describes degree observed data supports particular value parameter \\(\\theta\\).exact value likelihood meaningful; relative comparisons matter.Likelihood informative comparing parameter values, helping identify values \\(\\theta\\) plausible given data.single observation \\(Y=y\\), likelihood function :\\[\nL(\\theta_0; y) = P(Y = y | \\theta = \\theta_0) = f_Y(y; \\theta_0)\n\\]","code":""},{"path":"prerequisites.html","id":"likelihood-ratio","chapter":"2 Prerequisites","heading":"2.3.7.2 Likelihood Ratio","text":"likelihood ratio compares relative likelihood two parameter values \\(\\theta_0\\) \\(\\theta_1\\) given data:\\[\n\\frac{L(\\theta_0; y)}{L(\\theta_1; y)}\n\\]likelihood ratio greater 1 implies \\(\\theta_0\\) likely \\(\\theta_1\\), given observed data.","code":""},{"path":"prerequisites.html","id":"likelihood-function","chapter":"2 Prerequisites","heading":"2.3.7.3 Likelihood Function","text":"given sample, likelihood possible values \\(\\theta\\) forms likelihood function:\\[\nL(\\theta) = L(\\theta; y) = f_Y(y; \\theta).\n\\]sample size \\(n\\), assuming independence among observations:\\[\nL(\\theta) = \\prod_{=1}^{n} f_i(y_i; \\theta).\n\\]Taking natural logarithm likelihood gives log-likelihood function:\\[\nl(\\theta) = \\sum_{=1}^{n} \\log f_i(y_i; \\theta).\n\\]log-likelihood function particularly useful optimization problems, logarithms convert products sums, simplifying computation.","code":""},{"path":"prerequisites.html","id":"sufficient-statistics","chapter":"2 Prerequisites","heading":"2.3.7.4 Sufficient Statistics","text":"statistic \\(T(y)\\) sufficient parameter \\(\\theta\\) summarizes information data \\(\\theta\\). Formally, Factorization Theorem, \\(T(y)\\) sufficient \\(\\theta\\) :\\[\nL(\\theta; y) = c(y) L^*(\\theta; T(y)),\n\\]:\\(c(y)\\) function data independent \\(\\theta\\).\\(L^*(\\theta; T(y))\\) function depends \\(\\theta\\) \\(T(y)\\).words, likelihood function can rewritten terms \\(T(y)\\) alone, without loss information \\(\\theta\\).Example:sample ..d. observations \\(Y_1, Y_2, \\dots, Y_n\\) normal distribution \\(N(\\mu, \\sigma^2)\\):sample mean \\(\\bar{Y}\\) sufficient \\(\\mu\\).sample mean \\(\\bar{Y}\\) sufficient \\(\\mu\\).sufficient statistic conveys information \\(\\mu\\) contained data.sufficient statistic conveys information \\(\\mu\\) contained data.","code":""},{"path":"prerequisites.html","id":"nuisance-parameters","chapter":"2 Prerequisites","heading":"2.3.7.5 Nuisance Parameters","text":"Parameters direct interest analysis necessary model data called nuisance parameters.Profile Likelihood: handle nuisance parameters, replace maximum likelihood estimates (MLEs) likelihood function, creating profile likelihood parameter interest.","code":""},{"path":"prerequisites.html","id":"parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.8 Parameter Transformations","text":"Transformations parameters often used improve interpretability statistical properties models.","code":""},{"path":"prerequisites.html","id":"log-odds-transformation","chapter":"2 Prerequisites","heading":"2.3.8.1 Log-Odds Transformation","text":"log-odds transformation commonly used logistic regression binary classification problems. transforms probabilities (bounded 0 1) real line:\\[\n\\text{Log odds} = g(\\theta) = \\ln\\left(\\frac{\\theta}{1-\\theta}\\right),\n\\]\\(\\theta\\) represents probability (e.g., success probability Bernoulli trial).","code":""},{"path":"prerequisites.html","id":"general-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.8.2 General Parameter Transformations","text":"parameter \\(\\theta\\) transformation \\(g(\\cdot)\\):\\(\\theta \\(, b)\\), \\(g(\\theta)\\) may map \\(\\theta\\) different range (e.g., \\(\\mathbb{R}\\)).Useful transformations include:\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).\nExponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).\nSquare root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).\nLogarithmic: \\(g(\\theta) = \\ln(\\theta)\\) \\(\\theta > 0\\).Exponential: \\(g(\\theta) = e^{\\theta}\\) unconstrained \\(\\theta\\).Square root: \\(g(\\theta) = \\sqrt{\\theta}\\) \\(\\theta \\geq 0\\).Jacobian Adjustment Transformations: transforming parameter Bayesian inference, Jacobian transformation must included ensure proper posterior scaling.","code":""},{"path":"prerequisites.html","id":"applications-of-parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.8.3 Applications of Parameter Transformations","text":"Improving Interpretability:\nProbabilities can transformed odds log-odds logistic models.\nRates can transformed logarithmically multiplicative effects.\nProbabilities can transformed odds log-odds logistic models.Rates can transformed logarithmically multiplicative effects.Statistical Modeling:\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).\nRegularization simplification complex relationships.\nVariance-stabilizing transformations (e.g., log Poisson data arcsine proportions).Regularization simplification complex relationships.Optimization:\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.\nTransforming constrained parameters (e.g., probabilities positive scales) unconstrained scales simplifies optimization algorithms.","code":""},{"path":"prerequisites.html","id":"data-importexport","chapter":"2 Prerequisites","heading":"2.4 Data Import/Export","text":"Extended Manual RTable Rio VignetteR limitations:default, R use 1 core CPUBy default, R use 1 core CPUR puts data memory (limit around 2-4 GB), SAS uses data files demandR puts data memory (limit around 2-4 GB), SAS uses data files demandCategorization\nMedium-size file: within RAM limit, around 1-2 GB\nLarge file: 2-10 GB, might workaround solution\nlarge file > 10 GB, use distributed parallel computing\nCategorizationMedium-size file: within RAM limit, around 1-2 GBMedium-size file: within RAM limit, around 1-2 GBLarge file: 2-10 GB, might workaround solutionLarge file: 2-10 GB, might workaround solutionVery large file > 10 GB, use distributed parallel computingVery large file > 10 GB, use distributed parallel computingSolutions:buy RAMbuy RAMHPC packages\nExplicit Parallelism\nImplicit Parallelism\nLarge Memory\nMap/Reduce\nHPC packagesExplicit ParallelismExplicit ParallelismImplicit ParallelismImplicit ParallelismLarge MemoryLarge MemoryMap/ReduceMap/Reducespecify number rows columns, typically including command nrow =specify number rows columns, typically including command nrow =Use packages store data differently\nbigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class type\nmultiple class types, use ff package\nUse packages store data differentlybigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typebigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typeFor multiple class types, use ff packageFor multiple class types, use ff packageVery Large datasets use\nRHaddop package\nHadoopStreaming\nRhipe\nLarge datasets useRHaddop packageHadoopStreamingRhipe","code":""},{"path":"prerequisites.html","id":"medium-size","chapter":"2 Prerequisites","heading":"2.4.1 Medium size","text":"import multiple files directoryTo export single data fileTo export multiple data filesTo convert data file types","code":"\nlibrary(\"rio\")\nstr(import_list(dir()), which = 1)\nexport(data, \"data.csv\")\nexport(data,\"data.dta\")\nexport(data,\"data.txt\")\nexport(data,\"data_cyl.rds\")\nexport(data,\"data.rdata\")\nexport(data,\"data.R\")\nexport(data,\"data.csv.zip\")\nexport(data,\"list.json\")\nexport(list(mtcars = mtcars, iris = iris), \"data_file_type\") \n# where data_file_type should substituted with the extension listed above\n# convert Stata to SPSS\nconvert(\"data.dta\", \"data.sav\")"},{"path":"prerequisites.html","id":"large-size","chapter":"2 Prerequisites","heading":"2.4.2 Large size","text":"","code":""},{"path":"prerequisites.html","id":"cloud-computing-using-aws-for-big-data","chapter":"2 Prerequisites","heading":"2.4.2.1 Cloud Computing: Using AWS for Big Data","text":"Amazon Web Service (AWS): Compute resources can rented approximately $1/hr. Use AWS process large datasets without overwhelming local machine.","code":""},{"path":"prerequisites.html","id":"importing-large-files-as-chunks","chapter":"2 Prerequisites","heading":"2.4.2.2 Importing Large Files as Chunks","text":"","code":""},{"path":"prerequisites.html","id":"using-base-r","chapter":"2 Prerequisites","heading":"2.4.2.2.1 Using Base R","text":"","code":"\nfile_in <- file(\"in.csv\", \"r\")  # Open a connection to the file\nchunk_size <- 100000            # Define chunk size\nx <- readLines(file_in, n = chunk_size)  # Read data in chunks\nclose(file_in)                  # Close the file connection"},{"path":"prerequisites.html","id":"using-the-data.table-package","chapter":"2 Prerequisites","heading":"2.4.2.2.2 Using the data.table Package","text":"","code":"\nlibrary(data.table)\nmydata <- fread(\"in.csv\", header = TRUE)  # Fast and memory-efficient"},{"path":"prerequisites.html","id":"using-the-ff-package","chapter":"2 Prerequisites","heading":"2.4.2.2.3 Using the ff Package","text":"","code":"\nlibrary(ff)\nx <- read.csv.ffdf(\n  file = \"file.csv\",\n  nrow = 10,          # Total rows\n  header = TRUE,      # Include headers\n  VERBOSE = TRUE,     # Display progress\n  first.rows = 10000, # Initial chunk\n  next.rows = 50000,  # Subsequent chunks\n  colClasses = NA\n)"},{"path":"prerequisites.html","id":"using-the-bigmemory-package","chapter":"2 Prerequisites","heading":"2.4.2.2.4 Using the bigmemory Package","text":"","code":"\nlibrary(bigmemory)\nmy_data <- read.big.matrix('in.csv', header = TRUE)"},{"path":"prerequisites.html","id":"using-the-sqldf-package","chapter":"2 Prerequisites","heading":"2.4.2.2.5 Using the sqldf Package","text":"","code":"\nlibrary(sqldf)\nmy_data <- read.csv.sql('in.csv')\n\n# Example: Filtering during import\niris2 <- read.csv.sql(\"iris.csv\", \n    sql = \"SELECT * FROM file WHERE Species = 'setosa'\")"},{"path":"prerequisites.html","id":"using-the-rmysql-package","chapter":"2 Prerequisites","heading":"2.4.2.2.6 Using the RMySQL Package","text":"RQLite packageDownload SQLite, pick “bundle command-line tools managing SQLite database files” Window 10Unzip file, open sqlite3.exe.Type prompt\nsqlite> .cd 'C:\\Users\\data' specify path desired directory\nsqlite> .open database_name.db open database\nimport CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\n\nsqlite> .exit ’re done, exit sqlite program\nsqlite> .cd 'C:\\Users\\data' specify path desired directorysqlite> .open database_name.db open databaseTo import CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\nsqlite> .mode csv specify SQLite next file .csv filesqlite> .import file_name.csv datbase_name import csv file databasesqlite> .exit ’re done, exit sqlite program","code":"\nlibrary(RMySQL)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(\"RSQLite\")\nsetwd(\"\")\ncon <- dbConnect(RSQLite::SQLite(), \"data_base.db\")\ntbl <- tbl(con, \"data_table\")\ntbl %>% \n    filter() %>%\n    select() %>%\n    collect() # to actually pull the data into the workspace\ndbDisconnect(con)"},{"path":"prerequisites.html","id":"using-the-arrow-package","chapter":"2 Prerequisites","heading":"2.4.2.2.7 Using the arrow Package","text":"","code":"\nlibrary(arrow)\ndata <- read_csv_arrow(\"file.csv\")"},{"path":"prerequisites.html","id":"using-the-vroom-package","chapter":"2 Prerequisites","heading":"2.4.2.2.8 Using the vroom Package","text":"","code":"\nlibrary(vroom)\n\n# Import a compressed CSV file\ncompressed <- vroom_example(\"mtcars.csv.zip\")\ndata <- vroom(compressed)"},{"path":"prerequisites.html","id":"using-the-data.table-package-1","chapter":"2 Prerequisites","heading":"2.4.2.2.9 Using the data.table Package","text":"","code":"\ns = fread(\"sample.csv\")"},{"path":"prerequisites.html","id":"comparisons-regarding-storage-space","chapter":"2 Prerequisites","heading":"2.4.2.2.10 Comparisons Regarding Storage Space","text":"work large datasets, can compress csv.gz format. However, typically, R requires loading entire dataset exporting , can impractical data 10 GB. cases, processing data sequentially becomes necessary. Although read.csv slower compared readr::read_csv, can handle connections allows sequential looping, making useful large files.Currently, readr::read_csv support skip argument efficiently large data. Even specify skip, function reads preceding lines . instance, run read_csv(file, n_max = 100, skip = 0) followed read_csv(file, n_max = 200, skip = 100), first 100 rows re-read. contrast, read.csv can continue left without re-reading previous rows.encounter error :“Error (function (con, , n = 1L, size = NA_integer_, signed = TRUE): can read binary connection”,can modify connection mode \"r\" \"rb\" (read binary). Although file function designed detect appropriate format automatically, workaround can help resolve issue behave expected.","code":"\ntest = ff::read.csv.ffdf(file = \"\")\nobject.size(test) # Highest memory usage\n\ntest1 = data.table::fread(file = \"\")\nobject.size(test1) # Lowest memory usage\n\ntest2 = readr::read_csv(file = \"\")\nobject.size(test2) # Second lowest memory usage\n\ntest3 = vroom::vroom(file = \"\")\nobject.size(test3) # Similar to read_csv"},{"path":"prerequisites.html","id":"sequential-processing-for-large-data","chapter":"2 Prerequisites","heading":"2.4.2.3 Sequential Processing for Large Data","text":"","code":"\n# Open file for sequential reading\nfile_conn <- file(\"file.csv\", open = \"r\")\nwhile (TRUE) {\n  # Read a chunk of data\n  data_chunk <- read.csv(file_conn, nrows = 1000)\n  if (nrow(data_chunk) == 0) break  # Stop if no more rows\n  # Process the chunk here\n}\nclose(file_conn)  # Close connection"},{"path":"prerequisites.html","id":"data-manipulation","chapter":"2 Prerequisites","heading":"2.5 Data Manipulation","text":"verbs data manipulationselect: selecting (selecting) columns based names (eg: select columns Q1 Q25)slice: selecting (selecting) rows based position (eg: select rows 1:10)mutate: add derive new columns (variables) based existing columns (eg: create new column expresses measurement cm based existing measure inches)rename: rename variables change column names (eg: change “GraduationRate100” “grad100”)filter: selecting rows based condition (eg: rows gender = Male)arrange: ordering rows based variable(s) numeric alphabetical order (eg: sort descending order Income)sample: take random samples data (eg: sample 80% data create “training” set)summarize: condense aggregate multiple values single summary values (eg: calculate median income age group)group_by: convert tbl grouped tbl operations performed “group”; allows us summarize data apply verbs data groups (eg, gender treatment)pipe: %>%\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudio\npipe takes output function “pipes” first argument next function.\nnew pipe |> identical old one, except certain special cases.\nUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioUse Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioThe pipe takes output function “pipes” first argument next function.pipe takes output function “pipes” first argument next function.new pipe |> identical old one, except certain special cases.new pipe |> identical old one, except certain special cases.:= (Walrus operator): similar = , cases want use glue package (.e., dynamic changes variable name left-hand side)Writing function RTunneling{{ (called curly-curly) allows tunnel data-variables arg-variables (.e., function arguments)","code":"\n# Load required packages\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# -----------------------------\n# Data Structures in R\n# -----------------------------\n\n# Create vectors\nx <- c(1, 4, 23, 4, 45)\nn <- c(1, 3, 5)\ng <- c(\"M\", \"M\", \"F\")\n\n# Create a data frame\ndf <- data.frame(n, g)\ndf  # View the data frame\n#>   n g\n#> 1 1 M\n#> 2 3 M\n#> 3 5 F\nstr(df)  # Check its structure\n#> 'data.frame':    3 obs. of  2 variables:\n#>  $ n: num  1 3 5\n#>  $ g: chr  \"M\" \"M\" \"F\"\n\n# Using tibble for cleaner outputs\ndf <- tibble(n, g)\ndf  # View the tibble\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\nstr(df)\n#> tibble [3 × 2] (S3: tbl_df/tbl/data.frame)\n#>  $ n: num [1:3] 1 3 5\n#>  $ g: chr [1:3] \"M\" \"M\" \"F\"\n\n# Create a list\nlst <- list(x, n, g, df)\nlst  # Display the list\n#> [[1]]\n#> [1]  1  4 23  4 45\n#> \n#> [[2]]\n#> [1] 1 3 5\n#> \n#> [[3]]\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> [[4]]\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Name list elements\nlst2 <- list(num = x, size = n, sex = g, data = df)\nlst2  # Named list elements are easier to reference\n#> $num\n#> [1]  1  4 23  4 45\n#> \n#> $size\n#> [1] 1 3 5\n#> \n#> $sex\n#> [1] \"M\" \"M\" \"F\"\n#> \n#> $data\n#> # A tibble: 3 × 2\n#>       n g    \n#>   <dbl> <chr>\n#> 1     1 M    \n#> 2     3 M    \n#> 3     5 F\n\n# Another list example with numeric vectors\nlst3 <- list(\n  x = c(1, 3, 5, 7),\n  y = c(2, 2, 2, 4, 5, 5, 5, 6),\n  z = c(22, 3, 3, 3, 5, 10)\n)\nlst3\n#> $x\n#> [1] 1 3 5 7\n#> \n#> $y\n#> [1] 2 2 2 4 5 5 5 6\n#> \n#> $z\n#> [1] 22  3  3  3  5 10\n\n# Find means of list elements\n# One at a time\nmean(lst3$x)\n#> [1] 4\nmean(lst3$y)\n#> [1] 3.875\nmean(lst3$z)\n#> [1] 7.666667\n\n# Using lapply to calculate means\nlapply(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Simplified output with sapply\nsapply(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# Tidyverse alternative: map() function\nmap(lst3, mean)\n#> $x\n#> [1] 4\n#> \n#> $y\n#> [1] 3.875\n#> \n#> $z\n#> [1] 7.666667\n\n# Tidyverse with numeric output: map_dbl()\nmap_dbl(lst3, mean)\n#>        x        y        z \n#> 4.000000 3.875000 7.666667\n\n# -----------------------------\n# Binding Data Frames\n# -----------------------------\n\n# Create tibbles for demonstration\ndat01 <- tibble(x = 1:5, y = 5:1)\ndat02 <- tibble(x = 10:16, y = x / 2)\ndat03 <- tibble(z = runif(5))  # 5 random numbers from (0, 1)\n\n# Row binding\nbind_rows(dat01, dat02, dat01)\n#> # A tibble: 17 × 2\n#>        x     y\n#>    <int> <dbl>\n#>  1     1   5  \n#>  2     2   4  \n#>  3     3   3  \n#>  4     4   2  \n#>  5     5   1  \n#>  6    10   5  \n#>  7    11   5.5\n#>  8    12   6  \n#>  9    13   6.5\n#> 10    14   7  \n#> 11    15   7.5\n#> 12    16   8  \n#> 13     1   5  \n#> 14     2   4  \n#> 15     3   3  \n#> 16     4   2  \n#> 17     5   1\n\n# Add a new identifier column with .id\nbind_rows(dat01, dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 1         1   5  \n#>  2 1         2   4  \n#>  3 1         3   3  \n#>  4 1         4   2  \n#>  5 1         5   1  \n#>  6 2        10   5  \n#>  7 2        11   5.5\n#>  8 2        12   6  \n#>  9 2        13   6.5\n#> 10 2        14   7  \n#> 11 2        15   7.5\n#> 12 2        16   8\n\n# Use named inputs for better identification\nbind_rows(\"dat01\" = dat01, \"dat02\" = dat02, .id = \"id\")\n#> # A tibble: 12 × 3\n#>    id        x     y\n#>    <chr> <int> <dbl>\n#>  1 dat01     1   5  \n#>  2 dat01     2   4  \n#>  3 dat01     3   3  \n#>  4 dat01     4   2  \n#>  5 dat01     5   1  \n#>  6 dat02    10   5  \n#>  7 dat02    11   5.5\n#>  8 dat02    12   6  \n#>  9 dat02    13   6.5\n#> 10 dat02    14   7  \n#> 11 dat02    15   7.5\n#> 12 dat02    16   8\n\n# Bind a list of data frames\nlist01 <- list(\"dat01\" = dat01, \"dat02\" = dat02)\nbind_rows(list01, .id = \"source\")\n#> # A tibble: 12 × 3\n#>    source     x     y\n#>    <chr>  <int> <dbl>\n#>  1 dat01      1   5  \n#>  2 dat01      2   4  \n#>  3 dat01      3   3  \n#>  4 dat01      4   2  \n#>  5 dat01      5   1  \n#>  6 dat02     10   5  \n#>  7 dat02     11   5.5\n#>  8 dat02     12   6  \n#>  9 dat02     13   6.5\n#> 10 dat02     14   7  \n#> 11 dat02     15   7.5\n#> 12 dat02     16   8\n\n# Column binding\nbind_cols(dat01, dat03)\n#> # A tibble: 5 × 3\n#>       x     y     z\n#>   <int> <int> <dbl>\n#> 1     1     5 0.265\n#> 2     2     4 0.410\n#> 3     3     3 0.780\n#> 4     4     2 0.926\n#> 5     5     1 0.501\n\n# -----------------------------\n# String Manipulation\n# -----------------------------\n\nnames <- c(\"Ford, MS\", \"Jones, PhD\", \"Martin, Phd\", \"Huck, MA, MLS\")\n\n# Remove everything after the first comma\nstr_remove(names, pattern = \", [[:print:]]+\")\n#> [1] \"Ford\"   \"Jones\"  \"Martin\" \"Huck\"\n\n# Explanation: [[:print:]]+ matches one or more printable characters\n\n# -----------------------------\n# Reshaping Data\n# -----------------------------\n\n# Wide format data\nwide <- data.frame(\n  name = c(\"Clay\", \"Garrett\", \"Addison\"),\n  test1 = c(78, 93, 90),\n  test2 = c(87, 91, 97),\n  test3 = c(88, 99, 91)\n)\n\n# Long format data\nlong <- data.frame(\n  name = rep(c(\"Clay\", \"Garrett\", \"Addison\"), each = 3),\n  test = rep(1:3, 3),\n  score = c(78, 87, 88, 93, 91, 99, 90, 97, 91)\n)\n\n# Summary statistics\naggregate(score ~ name, data = long, mean)  # Mean score per student\n#>      name    score\n#> 1 Addison 92.66667\n#> 2    Clay 84.33333\n#> 3 Garrett 94.33333\naggregate(score ~ test, data = long, mean)  # Mean score per test\n#>   test    score\n#> 1    1 87.00000\n#> 2    2 91.66667\n#> 3    3 92.66667\n\n# Line plot of scores over tests\nggplot(long,\n       aes(\n           x = factor(test),\n           y = score,\n           color = name,\n           group = name\n       )) +\n    geom_point() +\n    geom_line() +\n    xlab(\"Test\") +\n    ggtitle(\"Test Scores by Student\")\n\n# Reshape wide to long\npivot_longer(wide, test1:test3, names_to = \"test\", values_to = \"score\")\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    test1    78\n#> 2 Clay    test2    87\n#> 3 Clay    test3    88\n#> 4 Garrett test1    93\n#> 5 Garrett test2    91\n#> 6 Garrett test3    99\n#> 7 Addison test1    90\n#> 8 Addison test2    97\n#> 9 Addison test3    91\n\n# Use names_prefix to clean column names\npivot_longer(\n    wide,\n    -name,\n    names_to = \"test\",\n    values_to = \"score\",\n    names_prefix = \"test\"\n)\n#> # A tibble: 9 × 3\n#>   name    test  score\n#>   <chr>   <chr> <dbl>\n#> 1 Clay    1        78\n#> 2 Clay    2        87\n#> 3 Clay    3        88\n#> 4 Garrett 1        93\n#> 5 Garrett 2        91\n#> 6 Garrett 3        99\n#> 7 Addison 1        90\n#> 8 Addison 2        97\n#> 9 Addison 3        91\n\n# Reshape long to wide with explicit id_cols argument\npivot_wider(\n  long,\n  id_cols = name, \n  names_from = test,\n  values_from = score\n)\n#> # A tibble: 3 × 4\n#>   name      `1`   `2`   `3`\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\n\n# Add a prefix to the resulting columns\npivot_wider(\n  long,\n  id_cols = name,  \n  names_from = test,\n  values_from = score,\n  names_prefix = \"test\"\n)\n#> # A tibble: 3 × 4\n#>   name    test1 test2 test3\n#>   <chr>   <dbl> <dbl> <dbl>\n#> 1 Clay       78    87    88\n#> 2 Garrett    93    91    99\n#> 3 Addison    90    97    91\nlibrary(tidyverse)\n# -----------------------------\n# Writing Functions with {{ }}\n# -----------------------------\n\n# Define a custom function using {{ }}\nget_mean <- function(data, group_var, var_to_mean) {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(mean = mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the function\ndata(\"mtcars\")\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl  mean\n#>   <dbl> <dbl>\n#> 1     4  26.7\n#> 2     6  19.7\n#> 3     8  15.1\n\n# Dynamically name the resulting variable\nget_mean <- function(data, group_var, var_to_mean, prefix = \"mean_of\") {\n  data %>%\n    group_by({{group_var}}) %>%\n    summarize(\"{prefix}_{{var_to_mean}}\" := mean({{var_to_mean}}, na.rm = TRUE))\n}\n\n# Apply the modified function\nmtcars %>%\n  get_mean(group_var = cyl, var_to_mean = mpg)\n#> # A tibble: 3 × 2\n#>     cyl mean_of_mpg\n#>   <dbl>       <dbl>\n#> 1     4        26.7\n#> 2     6        19.7\n#> 3     8        15.1"},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"3 Descriptive Statistics","heading":"3 Descriptive Statistics","text":"area interest want research, problem want solve, relationship want investigate, theoretical empirical processes help .Estimand defined “quantity scientific interest can calculated population change value depending data collection design used measure (.e., vary sample size survey design, number non-respondents, follow-efforts).” (Rubin 1996)Estimands include:population meansPopulation variancescorrelationsfactor loadingregression coefficients","code":""},{"path":"descriptive-statistics.html","id":"numerical-measures","chapter":"3 Descriptive Statistics","heading":"3.1 Numerical Measures","text":"differences population sample\\(m_2=\\sum_{=1}^{n}(y_1-\\overline{y})^2/n\\)\\(m_3=\\sum_{=1}^{n}(y_1-\\overline{y})^3/n\\)Note:Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) \\(y_{(1)}<y_{(2)}<...<y_{(n)}\\)Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) \\(y_{(1)}<y_{(2)}<...<y_{(n)}\\)Coefficient variation: standard deviation mean. metric stable, dimensionless statistic comparison.Coefficient variation: standard deviation mean. metric stable, dimensionless statistic comparison.Symmetric: mean = median, skewness = 0Symmetric: mean = median, skewness = 0Skewed right: mean > median, skewness > 0Skewed right: mean > median, skewness > 0Skewed left: mean < median, skewness < 0Skewed left: mean < median, skewness < 0Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\)Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\)normal distributions, \\(\\mu_3=0\\), \\(g_1=0\\)normal distributions, \\(\\mu_3=0\\), \\(g_1=0\\)\\(\\hat{g_1}\\) distributed approximately \\(N(0,6/n)\\) sample normal population. (valid \\(n > 150\\))\nlarge samples, inference skewness can based normal tables 95% confidence interval \\(g_1\\) \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\)\nsmall samples, special tables Snedecor Cochran 1989, Table 19() Monte Carlo test\n\\(\\hat{g_1}\\) distributed approximately \\(N(0,6/n)\\) sample normal population. (valid \\(n > 150\\))large samples, inference skewness can based normal tables 95% confidence interval \\(g_1\\) \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\)small samples, special tables Snedecor Cochran 1989, Table 19() Monte Carlo testFor normal distribution, \\(g_2^*=3\\). Kurtosis often redefined : \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) 4th central moment estimated \\(m_4=\\sum_{=1}^{n}(y_i-\\overline{y})^4/n\\)\nasymptotic sampling distribution \\(\\hat{g_2}\\) approximately \\(N(0,24/n)\\) (\\(n > 1000\\))\nlarge sample kurtosis uses standard normal tables\nsmall sample uses tables Snedecor Cochran, 1989, Table 19(ii) Geary 1936\nnormal distribution, \\(g_2^*=3\\). Kurtosis often redefined : \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) 4th central moment estimated \\(m_4=\\sum_{=1}^{n}(y_i-\\overline{y})^4/n\\)asymptotic sampling distribution \\(\\hat{g_2}\\) approximately \\(N(0,24/n)\\) (\\(n > 1000\\))large sample kurtosis uses standard normal tablessmall sample uses tables Snedecor Cochran, 1989, Table 19(ii) Geary 1936","code":"\ndata = rnorm(100)\nlibrary(e1071)\nskewness(data)\n#> [1] -0.2046225\nkurtosis(data)\n#> [1] -0.6313715"},{"path":"descriptive-statistics.html","id":"graphical-measures","chapter":"3 Descriptive Statistics","heading":"3.2 Graphical Measures","text":"","code":""},{"path":"descriptive-statistics.html","id":"shape","chapter":"3 Descriptive Statistics","heading":"3.2.1 Shape","text":"’s good habit label graph, others can easily follow.Others advanced plots","code":"\ndata = rnorm(100)\n\n# Histogram\nhist(data,labels = T,col=\"grey\",breaks = 12) \n\n# Interactive histogram  \npacman::p_load(\"highcharter\")\nhchart(data) \n\n# Box-and-Whisker plot\nboxplot(count ~ spray, data = InsectSprays,col = \"lightgray\",main=\"boxplot\")\n\n# Notched Boxplot\nboxplot(len~supp*dose, data=ToothGrowth, notch=TRUE,\n  col=(c(\"gold\",\"darkgreen\")),\n  main=\"Tooth Growth\", xlab=\"Suppliment and Dose\")\n# If notches differ -> medians differ\n\n# Stem-and-Leaf Plots\nstem(data)\n\n\n# Bagplot - A 2D Boxplot Extension\npacman::p_load(aplpack)\nattach(mtcars)\nbagplot(wt,mpg, xlab=\"Car Weight\", ylab=\"Miles Per Gallon\",\n  main=\"Bagplot Example\")\n# boxplot.matrix()  #library(\"sfsmisc\")\n# boxplot.n()       #library(\"gplots\")\n# vioplot()         #library(\"vioplot\")"},{"path":"descriptive-statistics.html","id":"scatterplot","chapter":"3 Descriptive Statistics","heading":"3.2.2 Scatterplot","text":"","code":"\n# pairs(mtcars)"},{"path":"descriptive-statistics.html","id":"normality-assessment","chapter":"3 Descriptive Statistics","heading":"3.3 Normality Assessment","text":"Since Normal (Gaussian) distribution many applications, typically want/ wish data variable normal. Hence, assess normality based Numerical Measures also Graphical Measures","code":""},{"path":"descriptive-statistics.html","id":"graphical-assessment","chapter":"3 Descriptive Statistics","heading":"3.3.1 Graphical Assessment","text":"straight line represents theoretical line normally distributed data. dots represent real empirical data checking. dots fall straight line, can confident data follow normal distribution. data wiggle deviate line, concerned normality assumption.","code":"\npacman::p_load(\"car\")\nqqnorm(precip, ylab = \"Precipitation [in/yr] for 70 US cities\")\nqqline(precip)"},{"path":"descriptive-statistics.html","id":"summary-statistics","chapter":"3 Descriptive Statistics","heading":"3.3.2 Summary Statistics","text":"Sometimes ’s hard tell whether data follow normal distribution just looking graph. Hence, often conduct statistical test aid decision. Common tests areMethods based normal probability plot\nCorrelation Coefficient Normal Probability Plots\nShapiro-Wilk Test\nMethods based normal probability plotCorrelation Coefficient Normal Probability PlotsShapiro-Wilk TestMethods based empirical cumulative distribution function\nAnderson-Darling Test\nKolmogorov-Smirnov Test\nCramer-von Mises Test\nJarque–Bera Test\nMethods based empirical cumulative distribution functionAnderson-Darling TestKolmogorov-Smirnov TestCramer-von Mises TestJarque–Bera Test","code":""},{"path":"descriptive-statistics.html","id":"methods-based-on-normal-probability-plot","chapter":"3 Descriptive Statistics","heading":"3.3.2.1 Methods based on normal probability plot","text":"","code":""},{"path":"descriptive-statistics.html","id":"correlation-coefficient-with-normal-probability-plots","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.1 Correlation Coefficient with Normal Probability Plots","text":"(Looney Gulledge Jr 1985) (Samuel S. Shapiro Francia 1972) correlation coefficient \\(y_{()}\\) \\(m_i^*\\) given normal probability plot:\\[W^*=\\frac{\\sum_{=1}^{n}(y_{()}-\\bar{y})(m_i^*-0)}{(\\sum_{=1}^{n}(y_{()}-\\bar{y})^2\\sum_{=1}^{n}(m_i^*-0)^2)^.5}\\]\\(\\bar{m^*}=0\\)Pearson product moment formula correlation:\\[\\hat{p}=\\frac{\\sum_{-1}^{n}(y_i-\\bar{y})(x_i-\\bar{x})}{(\\sum_{=1}^{n}(y_{}-\\bar{y})^2\\sum_{=1}^{n}(x_i-\\bar{x})^2)^.5}\\]correlation 1, plot exactly linear normality assumed.closer correlation zero, confident reject normalityInference W* needs based special tables (Looney Gulledge Jr 1985)","code":"\nlibrary(\"EnvStats\")\ngofTest(data,test=\"ppcc\")$p.value #Probability Plot Correlation Coefficient \n#> [1] 0.383608"},{"path":"descriptive-statistics.html","id":"shapiro-wilk-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.2 Shapiro-Wilk Test","text":"(Samuel Sanford Shapiro Wilk 1965)\\[W=(\\frac{\\sum_{=1}^{n}a_i(y_{()}-\\bar{y})(m_i^*-0)}{(\\sum_{=1}^{n}a_i^2(y_{()}-\\bar{y})^2\\sum_{=1}^{n}(m_i^*-0)^2)^.5})^2\\]\\(a_1,..,a_n\\) weights computed covariance matrix order statistics.Researchers typically use test assess normality. (n < 2000) normality, W close 1, just like \\(W^*\\). Notice difference W W* “weights”.","code":"\ngofTest(data,test=\"sw\")$p.value #Shapiro-Wilk is the default.\n#> [1] 0.3132036"},{"path":"descriptive-statistics.html","id":"methods-based-on-empirical-cumulative-distribution-function","chapter":"3 Descriptive Statistics","heading":"3.3.2.2 Methods based on empirical cumulative distribution function","text":"formula empirical cumulative distribution function (CDF) :\\(F_n(t)\\) = estimate probability observation \\(\\le\\) t = (number observation \\(\\le\\) t)/nThis method requires large sample sizes. However, can apply distributions normal (Gaussian) one.","code":"\n# Empirical CDF hand-code\nplot.ecdf(data,verticals = T, do.points=F)"},{"path":"descriptive-statistics.html","id":"anderson-darling-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.1 Anderson-Darling Test","text":"Anderson-Darling statistic (T. W. Anderson Darling 1952):\\[^2=\\int_{-\\infty}^{\\infty}(F_n(t)=F(t))^2\\frac{dF(t)}{F(t)(1-F(t))}\\]weight average squared deviations (weights small large values t )normal distribution,\\(^2 = - (\\sum_{=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-}))/n-n\\)\\(p_i=\\Phi(\\frac{y_{()}-\\bar{y}}{s})\\), probability standard normal variable less \\(\\frac{y_{()}-\\bar{y}}{s}\\)Reject normal assumption \\(^2\\) largeReject normal assumption \\(^2\\) largeEvaluate null hypothesis observations randomly selected normal population based critical value provided (Marsaglia Marsaglia 2004) (Stephens 1974)Evaluate null hypothesis observations randomly selected normal population based critical value provided (Marsaglia Marsaglia 2004) (Stephens 1974)test can applied distributions:\nExponential\nLogistic\nGumbel\nExtreme-value\nWeibull: log(Weibull) = Gumbel\nGamma\nLogistic\nCauchy\nvon Mises\nLog-normal (two-parameter)\ntest can applied distributions:ExponentialLogisticGumbelExtreme-valueWeibull: log(Weibull) = GumbelGammaLogisticCauchyvon MisesLog-normal (two-parameter)Consult (Stephens 1974) detailed transformation critical values.","code":"\ngofTest(data,test=\"ad\")$p.value #Anderson-Darling\n#> [1] 0.1961768"},{"path":"descriptive-statistics.html","id":"kolmogorov-smirnov-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.2 Kolmogorov-Smirnov Test","text":"Based largest absolute difference empirical expected cumulative distributionAnother deviation K-S test Kuiper’s test","code":"\ngofTest(data,test=\"ks\")$p.value #Komogorov-Smirnov \n#> [1] 0.6694967"},{"path":"descriptive-statistics.html","id":"cramer-von-mises-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.3 Cramer-von Mises Test","text":"Based average squared discrepancy empirical distribution given theoretical distribution. discrepancy weighted equally (unlike Anderson-Darling test weights end points heavily)","code":"\ngofTest(data,test=\"cvm\")$p.value #Cramer-von Mises\n#> [1] 0.1820569"},{"path":"descriptive-statistics.html","id":"jarquebera-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.4 Jarque–Bera Test","text":"(Bera Jarque 1981)Based skewness kurtosis test normality.\\(JB = \\frac{n}{6}(S^2+(K-3)^2/4)\\) \\(S\\) sample skewness \\(K\\) sample kurtosis\\(S=\\frac{\\hat{\\mu_3}}{\\hat{\\sigma}^3}=\\frac{\\sum_{=1}^{n}(x_i-\\bar{x})^3/n}{(\\sum_{=1}^{n}(x_i-\\bar{x})^2/n)^\\frac{3}{2}}\\)\\(K=\\frac{\\hat{\\mu_4}}{\\hat{\\sigma}^4}=\\frac{\\sum_{=1}^{n}(x_i-\\bar{x})^4/n}{(\\sum_{=1}^{n}(x_i-\\bar{x})^2/n)^2}\\)recall \\(\\hat{\\sigma^2}\\) estimate second central moment (variance) \\(\\hat{\\mu_3}\\) \\(\\hat{\\mu_4}\\) estimates third fourth central moments.data comes normal distribution, JB statistic asymptotically chi-squared distribution two degrees freedom.null hypothesis joint hypothesis skewness zero excess kurtosis zero.","code":""},{"path":"descriptive-statistics.html","id":"bivariate-statistics","chapter":"3 Descriptive Statistics","heading":"3.4 Bivariate Statistics","text":"Correlation betweenTwo Continuous variablesTwo Discrete variablesCategorical ContinuousPhi coefficientCramer’s VTschuprow’s TFreeman’s ThetaEpsilon-squaredGoodman Kruskal’s GammaSomers’ DKendall’s Tau-bYule’s Q YTetrachoric CorrelationPolychoric CorrelationPoint-Biserial CorrelationLogistic RegressionPearson CorrelationSpearman CorrelationQuestions keep mind:relationship linear non-linear?variable continuous, normal homoskadastic?big dataset?","code":""},{"path":"descriptive-statistics.html","id":"two-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.1 Two Continuous","text":"","code":"\nn = 100 # (sample size)\n\ndata = data.frame(A = sample(1:20, replace = TRUE, size = n),\n                  B = sample(1:30, replace = TRUE, size = n))"},{"path":"descriptive-statistics.html","id":"pearson-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.1 Pearson Correlation","text":"Good linear relationship","code":"\nlibrary(Hmisc)\nrcorr(data$A, data$B, type=\"pearson\") \n#>      x    y\n#> x 1.00 0.17\n#> y 0.17 1.00\n#> \n#> n= 100 \n#> \n#> \n#> P\n#>   x      y     \n#> x        0.0878\n#> y 0.0878"},{"path":"descriptive-statistics.html","id":"spearman-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.1.2 Spearman Correlation","text":"","code":"\nlibrary(Hmisc)\nrcorr(data$A, data$B, type=\"spearman\") \n#>      x    y\n#> x 1.00 0.18\n#> y 0.18 1.00\n#> \n#> n= 100 \n#> \n#> \n#> P\n#>   x    y   \n#> x      0.08\n#> y 0.08"},{"path":"descriptive-statistics.html","id":"categorical-and-continuous","chapter":"3 Descriptive Statistics","heading":"3.4.2 Categorical and Continuous","text":"","code":""},{"path":"descriptive-statistics.html","id":"point-biserial-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.2.1 Point-Biserial Correlation","text":"Similar Pearson correlation coefficient, point-biserial correlation coefficient -1 1 :-1 means perfectly negative correlation two variables-1 means perfectly negative correlation two variables0 means correlation two variables0 means correlation two variables1 means perfectly positive correlation two variables1 means perfectly positive correlation two variablesAlternatively","code":"\nx <- c(0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0)\ny <- c(12, 14, 17, 17, 11, 22, 23, 11, 19, 8, 12)\n\n#calculate point-biserial correlation\ncor.test(x, y)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  x and y\n#> t = 0.67064, df = 9, p-value = 0.5193\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.4391885  0.7233704\n#> sample estimates:\n#>       cor \n#> 0.2181635\nltm::biserial.cor(y,x, use = c(\"all.obs\"), level = 2)\n#> [1] 0.2181635"},{"path":"descriptive-statistics.html","id":"logistic-regression","chapter":"3 Descriptive Statistics","heading":"3.4.2.2 Logistic Regression","text":"See 3.4.2.2","code":""},{"path":"descriptive-statistics.html","id":"two-discrete","chapter":"3 Descriptive Statistics","heading":"3.4.3 Two Discrete","text":"","code":""},{"path":"descriptive-statistics.html","id":"distance-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.1 Distance Metrics","text":"consider distance correlation metric isn’t unit independent (.e., scale distance, metrics change), ’s still useful proxy. Distance metrics likely used similarity measure.Euclidean DistanceEuclidean DistanceManhattan DistanceManhattan DistanceChessboard DistanceChessboard DistanceMinkowski DistanceMinkowski DistanceCanberra DistanceCanberra DistanceHamming DistanceHamming DistanceCosine DistanceCosine DistanceSum Absolute DistanceSum Absolute DistanceSum Squared DistanceSum Squared DistanceMean-Absolute ErrorMean-Absolute Error","code":""},{"path":"descriptive-statistics.html","id":"statistical-metrics","chapter":"3 Descriptive Statistics","heading":"3.4.3.2 Statistical Metrics","text":"","code":""},{"path":"descriptive-statistics.html","id":"chi-squared-test","chapter":"3 Descriptive Statistics","heading":"3.4.3.2.1 Chi-squared test","text":"","code":""},{},{},{},{"path":"descriptive-statistics.html","id":"ordinal-association-rank-correlation","chapter":"3 Descriptive Statistics","heading":"3.4.3.3 Ordinal Association (Rank correlation)","text":"Good non-linear relationship","code":""},{"path":"descriptive-statistics.html","id":"ordinal-and-nominal","chapter":"3 Descriptive Statistics","heading":"3.4.3.3.1 Ordinal and Nominal","text":"","code":"\nn = 100 # (sample size)\nset.seed(1)\ndt = table(data.frame(\n    A = sample(1:4, replace = TRUE, size = n), # ordinal\n    B = sample(1:3, replace = TRUE, size = n)  # nominal\n)) \ndt\n#>    B\n#> A    1  2  3\n#>   1  7 11  9\n#>   2 11  6 14\n#>   3  7 11  4\n#>   4  6  4 10"},{},{},{"path":"descriptive-statistics.html","id":"two-ordinal","chapter":"3 Descriptive Statistics","heading":"3.4.3.3.2 Two Ordinal","text":"","code":"\nn = 100 # (sample size)\nset.seed(1)\ndt = table(data.frame(\n    A = sample(1:4, replace = TRUE, size = n), # ordinal\n    B = sample(1:3, replace = TRUE, size = n)  # ordinal\n)) \ndt\n#>    B\n#> A    1  2  3\n#>   1  7 11  9\n#>   2 11  6 14\n#>   3  7 11  4\n#>   4  6  4 10"},{},{},{},{},{},{},{"path":"descriptive-statistics.html","id":"summary","chapter":"3 Descriptive Statistics","heading":"3.5 Summary","text":"Get correlation table continuous variables onlyAlternatively, can also theDifferent comparison different correlation different types variables (.e., continuous vs. categorical) can problematic. Moreover, problem detecting non-linear vs. linear relationship/correlation another one. Hence, solution using mutual information information theory (.e., knowing one variable can reduce uncertainty ).implement mutual information, following approximations\\[\n\\downarrow \\text{prediction error} \\approx \\downarrow \\text{uncertainty} \\approx \\downarrow \\text{association strength}\n\\]specifically, following X2Y metric, following steps:Predict \\(y\\) without \\(x\\) (.e., baseline model)\nAverage \\(y\\) \\(y\\) continuous\nfrequent value \\(y\\) categorical\nPredict \\(y\\) without \\(x\\) (.e., baseline model)Average \\(y\\) \\(y\\) continuousAverage \\(y\\) \\(y\\) continuousMost frequent value \\(y\\) categoricalMost frequent value \\(y\\) categoricalPredict \\(y\\) \\(x\\) (e.g., linear, random forest, etc.)Predict \\(y\\) \\(x\\) (e.g., linear, random forest, etc.)Calculate prediction error difference 1 2Calculate prediction error difference 1 2To comprehensive table handlecontinuous vs. continuouscontinuous vs. continuouscategorical vs. continuouscategorical vs. continuouscontinuous vs. categoricalcontinuous vs. categoricalcategorical vs. categoricalcategorical vs. categoricalthe suggested model Classification Regression Trees (CART). can certainly use models well.downfall method might sufferSymmetry: \\((x,y) \\neq (y,x)\\)Comparability : Different pair comparison might use different metrics (e.g., misclassification error vs. MAE)","code":"\nlibrary(tidyverse)\n\ndata(\"mtcars\")\ndf = mtcars %>%\n    dplyr::select(cyl, vs, carb)\n\n\ndf_factor = df %>%\n    dplyr::mutate(\n        cyl = factor(cyl),\n        vs = factor(vs),\n        carb = factor(carb)\n    )\n# summary(df)\nstr(df)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#>  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#>  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nstr(df_factor)\n#> 'data.frame':    32 obs. of  3 variables:\n#>  $ cyl : Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n#>  $ vs  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 1 2 1 2 2 2 ...\n#>  $ carb: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 4 1 1 2 1 4 2 2 4 ...\ncor(df)\n#>             cyl         vs       carb\n#> cyl   1.0000000 -0.8108118  0.5269883\n#> vs   -0.8108118  1.0000000 -0.5696071\n#> carb  0.5269883 -0.5696071  1.0000000\n\n# only complete obs\n# cor(df, use = \"complete.obs\")\nHmisc::rcorr(as.matrix(df), type = \"pearson\")\n#>        cyl    vs  carb\n#> cyl   1.00 -0.81  0.53\n#> vs   -0.81  1.00 -0.57\n#> carb  0.53 -0.57  1.00\n#> \n#> n= 32 \n#> \n#> \n#> P\n#>      cyl    vs     carb  \n#> cyl         0.0000 0.0019\n#> vs   0.0000        0.0007\n#> carb 0.0019 0.0007\nmodelsummary::datasummary_correlation(df)\nggcorrplot::ggcorrplot(cor(df))\nlibrary(ppsr)\n\niris <- iris %>% \n  select(1:3)\n\n# ppsr::score_df(iris) # if you want a dataframe\nppsr::score_matrix(iris,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>              Sepal.Length Sepal.Width Petal.Length\n#> Sepal.Length   1.00000000  0.04632352    0.5491398\n#> Sepal.Width    0.06790301  1.00000000    0.2376991\n#> Petal.Length   0.61608360  0.24263851    1.0000000\n\n# if you want a similar correlation matrix\nppsr::score_matrix(df,\n                   do_parallel = TRUE,\n                   n_cores = parallel::detectCores() / 2)\n#>             cyl        vs      carb\n#> cyl  1.00000000 0.3982789 0.2092533\n#> vs   0.02514286 1.0000000 0.2000000\n#> carb 0.30798148 0.2537309 1.0000000"},{"path":"descriptive-statistics.html","id":"visualization","chapter":"3 Descriptive Statistics","heading":"3.5.1 Visualization","text":"Alternatively,general form,heat map correlation timeMore elaboration ggplot2","code":"\ncorrplot::corrplot(cor(df))\nPerformanceAnalytics::chart.Correlation(df, histogram = T, pch = 19)\nheatmap(as.matrix(df))\nppsr::visualize_pps(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_correlations(\n    df = iris\n)\nppsr::visualize_both(\n    df = iris,\n    do_parallel = TRUE,\n    n_cores = parallel::detectCores() / 2\n)\nppsr::visualize_pps(\n    df = iris,\n    color_value_high = 'red',\n    color_value_low = 'yellow',\n    color_text = 'black'\n) +\n    ggplot2::theme_classic() +\n    ggplot2::theme(plot.background = \n                       ggplot2::element_rect(fill = \"lightgrey\")) +\n    ggplot2::theme(title = ggplot2::element_text(size = 15)) +\n    ggplot2::labs(\n        title = 'Correlation aand Heatmap',\n        subtitle = 'Subtitle',\n        caption = 'Caption',\n        x = 'More info'\n    )"},{"path":"basic-statistical-inference.html","id":"basic-statistical-inference","chapter":"4 Basic Statistical Inference","heading":"4 Basic Statistical Inference","text":"One Sample InferenceTwo Sample InferenceCategorical Data AnalysisDivergence Metrics Test Comparing DistributionsMake inferences (interpretation) true parameter value \\(\\beta\\) based estimator/estimateTest whether underlying assumptions (true population parameters, random variables, model specification) hold true.Testing notConfirm 100% hypothesis trueConfirm 100% hypothesis falseTell interpret estimate value (Economic vs. Practical vs. Statistical Significance)Hypothesis: Translate objective better understanding results terms specifying value (sets values) population parameters /lie.Null hypothesis (\\(H_0\\)): statement population parameter take true need data provide substantial evidence .\nCan either single value (ex: \\(H_0: \\beta=0\\)) set values (ex: \\(H_0: \\beta_1 \\ge 0\\))\ngenerally value like population parameter (subjective)\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\n\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect\n\n“Test Significance” refers two-sided test: \\(H_0: \\beta_j=0\\)\nCan either single value (ex: \\(H_0: \\beta=0\\)) set values (ex: \\(H_0: \\beta_1 \\ge 0\\))generally value like population parameter (subjective)\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\n\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect“Test Significance” refers two-sided test: \\(H_0: \\beta_j=0\\)Alternative hypothesis (\\(H_a\\) \\(H_1\\)) (Research Hypothesis): possible values population parameter may null hypothesis hold.Type ErrorError made \\(H_0\\) rejected , fact, \\(H_0\\) true.\nprobability committing Type error \\(\\alpha\\) (known level significance test)Type error (\\(\\alpha\\)): probability rejecting \\(H_0\\) true.Legal analogy: U.S. law, defendant presumed “innocent proven guilty”.\nnull hypothesis person innocent, Type error probability conclude person guilty innocent.Type II ErrorType II error level (\\(\\beta\\)): probability fail reject null hypothesis false.legal analogy, probability fail find person guilty guilty.Error made \\(H_0\\) rejected , fact, \\(H_1\\) true\nprobability committing Type II error \\(\\beta\\) (known power test)Random sample size n: collection n independent random variables taken distribution X, distribution X.Sample mean\\[\n\\bar{X}= (\\sum_{=1}^{n}X_i)/n\n\\]Sample Median\\(\\tilde{x}\\) = middle observation sample observation order smallest largest (vice versa).n odd, \\(\\tilde{x}\\) middle observation,\nn even, \\(\\tilde{x}\\) average two middle observations.Sample variance \\[\nS^2 = \\frac{\\sum_{=1}^{n}(X_i = \\bar{X})^2}{n-1}= \\frac{n\\sum_{=1}^{n}X_i^2 -(\\sum_{=1}^{n}X_i)^2}{n(n-1)}\n\\]Sample standard deviation \\[\nS = \\sqrt{S^2}\n\\]Sample proportions \\[\n\\hat{p} = \\frac{X}{n} = \\frac{\\text{number sample trait}}{\\text{sample size}}\n\\]\\[\n\\widehat{p_1-p_2} = \\hat{p_1}-\\hat{p_2} = \\frac{X_1}{n_1} - \\frac{X_2}{n_2} = \\frac{n_2X_1 = n_1X_2}{n_1n_2}\n\\]EstimatorsPoint Estimator\\(\\hat{\\theta}\\) statistic used approximate population parameter \\(\\theta\\)Point estimate\nnumerical value assumed \\(\\hat{\\theta}\\) evaluated given sampleUnbiased estimator\n\\(E(\\hat{\\theta}) = \\theta\\), \\(\\hat{\\theta}\\) unbiased estimator \\(\\theta\\)\\(\\bar{X}\\) unbiased estimator \\(\\mu\\)\\(S^2\\) unbiased estimator \\(\\sigma^2\\)\\(\\hat{p}\\) unbiased estimator p\\(\\widehat{p_1-p_2}\\) unbiased estimator \\(p_1- p_2\\)\\(\\bar{X_1} - \\bar{X_2}\\) unbiased estimator \\(\\mu_1 - \\mu_2\\)Note: \\(S\\) biased estimator \\(\\sigma\\)Distribution sample meanIf \\(\\bar{X}\\) sample mean based random sample size \\(n\\) drawn normal distribution \\(X\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\), \\(\\bar{X}\\) normally distributed, mean \\(\\mu_{\\bar{X}} = \\mu\\) variance \\(\\sigma_{\\bar{X}}^2 = Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\). standard error mean : \\(\\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\)","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.1 One Sample Inference","text":"\\(Y_i \\sim ..d. N(\\mu, \\sigma^2)\\)..d. standards “independent identically distributed”Hence, following model:\\(Y_i=\\mu +\\epsilon_i\\) \\(\\epsilon_i \\sim^{iid} N(0,\\sigma^2)\\)\\(E(Y_i)=\\mu\\)\\(Var(Y_i)=\\sigma^2\\)\\(\\bar{y} \\sim N(\\mu,\\sigma^2/n)\\)","code":""},{"path":"basic-statistical-inference.html","id":"the-mean","chapter":"4 Basic Statistical Inference","heading":"4.1.1 The Mean","text":"\\(\\sigma^2\\) estimated \\(s^2\\), \\[\n\\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\sim t_{n-1}\n\\], \\(100(1-\\alpha) \\%\\) confidence interval \\(\\mu\\) obtained :\\[\n1 - \\alpha = P(-t_{\\alpha/2;n-1} \\le \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\le t_{\\alpha/2;n-1}) \\\\\n= P(\\bar{y} - (t_{\\alpha/2;n-1})s/\\sqrt{n} \\le \\mu \\le \\bar{y} + (t_{\\alpha/2;n-1})s/\\sqrt{n})\n\\]interval \\[\n\\bar{y} \\pm (t_{\\alpha/2;n-1})s/\\sqrt{n}\n\\]\\(s/\\sqrt{n}\\) standard error \\(\\bar{y}\\)experiment repeated many times, \\(100(1-\\alpha) \\%\\) intervals contain \\(\\mu\\)","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-mu_1-mu_2-independent-samples","chapter":"4 Basic Statistical Inference","heading":"4.1.1.1 For Difference of Means (\\(\\mu_1-\\mu_2\\)), Independent Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y","chapter":"4 Basic Statistical Inference","heading":"4.1.1.2 For Difference of Means (\\(\\mu_1 - \\mu_2\\)), Paired Samples (D = X-Y)","text":"\\(100(1-\\alpha)%\\) Confidence Interval\\[\n\\bar{D} \\pm t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}}\n\\]Hypothesis Testing Test Statistic\\[\nt = \\frac{\\bar{D}-D_0}{s_d / \\sqrt{n}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"difference-of-two-proportions","chapter":"4 Basic Statistical Inference","heading":"4.1.1.3 Difference of Two Proportions","text":"Mean\\[\n\\hat{p_1}-\\hat{p_2}\n\\]Variance \\[\n\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}\n\\]\\(100(1-\\alpha)%\\) Confidence Interval\\[\n\\hat{p_1}-\\hat{p_2} + z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]Sample Sizes, Confidence \\(\\alpha\\), Error d\n(Prior Estimate fo \\(\\hat{p_1},\\hat{p_2}\\))\\[\nn \\approx \\frac{z_{\\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2}\n\\](Prior Estimates \\(\\hat{p}\\))\\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Hypothesis Testing - Test StatisticsNull Value \\((p_1 - p_2) \\neq 0\\)\\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2})-(p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]Null Value \\((p_1 - p_2)_0 = 0\\)\\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}}\n\\]\\[\n\\hat{p}= \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1 \\hat{p_1} + n_2 \\hat{p_2}}{n_1 + n_2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"single-variance","chapter":"4 Basic Statistical Inference","heading":"4.1.2 Single Variance","text":"\\[\n1 - \\alpha = P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2) \\\\\n= P(\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2}^2})\n\\]\\(100(1-\\alpha) \\%\\) confidence interval \\(\\sigma^2\\) :\\[\n(\\frac{(n-1)s^2}{\\chi_{\\alpha/2;n-1}^2},\\frac{(n-1)s^2}{\\chi_{1-\\alpha/2;n-1}^2})\n\\] Confidence limits \\(\\sigma^2\\) obtained computing positive square roots limitsEquivalently,\\(100(1-\\alpha)%\\) Confidence Interval\\[\nL_1 = \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\\\\nL_1 = \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\n\\] Hypothesis Testing Test Statistic\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"single-proportion-p","chapter":"4 Basic Statistical Inference","heading":"4.1.3 Single Proportion (p)","text":"","code":""},{"path":"basic-statistical-inference.html","id":"power","chapter":"4 Basic Statistical Inference","heading":"4.1.4 Power","text":"Formally, power (test mean) given :\\[\n\\pi(\\mu) = 1 - \\beta = P(\\text{test rejects } H_0|\\mu)\n\\] evaluate power, one needs know distribution test statistic null hypothesis false.1-sided z-test \\(H_0: \\mu \\le \\mu_0 \\\\ H_A: \\mu >0\\)power :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P(\\bar{y} > \\mu_0 + z_{\\alpha} \\sigma/\\sqrt{n}|\\mu) \\\\\n&= P(Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} > z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma/ \\sqrt{n}}|\\mu) \\\\\n&= 1 - \\Phi(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) \\\\\n&= \\Phi(-z_{\\alpha}+\\frac{(\\mu -\\mu_0)\\sqrt{n}}{\\sigma})\n\\end{aligned}\n\\]\\(1-\\Phi(x) = \\Phi(-x)\\) since normal pdf symmetricPower correlated difference \\(\\mu - \\mu_0\\), sample size n, variance \\(\\sigma^2\\), \\(\\alpha\\)-level test (\\(z_{\\alpha}\\))\nEquivalently, power can increased making \\(\\alpha\\) large, \\(\\sigma^2\\) smaller, n larger.2-sided z-test :\\[\n\\pi(\\mu) = \\Phi(-z_{\\alpha/2} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) + \\Phi(-z_{\\alpha/2}+\\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma})\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-size","chapter":"4 Basic Statistical Inference","heading":"4.1.5 Sample Size","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.1.5.1 1-sided Z-test","text":"Example: show mean response \\(\\mu\\) treatment higher mean response \\(\\mu_0\\) without treatment (show treatment effect \\(\\delta = \\mu -\\mu_0\\) large)power increasing function \\(\\mu - \\mu_0\\), necessary find n makes power equal \\(1- \\beta\\) \\(\\mu = \\mu_0 + \\delta\\)Hence, \\[\n\\pi(\\mu_0 + \\delta) = \\Phi(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}) = 1 - \\beta\n\\]Since \\(\\Phi (z_{\\beta})= 1-\\beta\\), \\[\n-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta}\n\\]n \\[\nn = (\\frac{(z_{\\alpha}+z_{\\beta})\\sigma}{\\delta})^2\n\\], need larger samples, whenthe sample variability large (\\(\\sigma\\) large)\\(\\alpha\\) small (\\(z_{\\alpha}\\) large)power \\(1-\\beta\\) large (\\(z_{\\beta}\\) large)magnitude effect smaller (\\(\\delta\\) small)Since don’t know \\(\\delta\\) \\(\\sigma\\). can base \\(\\sigma\\) previous studies, pilot studies. , obtain estimate \\(\\sigma\\) anticipating range observation (without outliers). divide range 4 use resulting number approximate estimate \\(\\sigma\\). normal (distribution) data, reasonable.","code":""},{"path":"basic-statistical-inference.html","id":"sided-z-test-1","chapter":"4 Basic Statistical Inference","heading":"4.1.5.2 2-sided Z-test","text":"want know min n, required guarantee \\(1-\\beta\\) power treatment effect \\(\\delta = |\\mu - \\mu_0|\\) least greater 0. Since power function 2-sided increasing symmetric \\(|\\mu - \\mu_0|\\), need find n makes power equal \\(1-\\beta\\) \\(\\mu = \\mu_0 + \\delta\\)\\[\nn = (\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta})^2\n\\]also use confidence interval approach. require \\(\\alpha\\)-level two-sided CI \\(\\mu\\) \\[\n\\bar{y} \\pm D\n\\] \\(D = z_{\\alpha/2}\\sigma/\\sqrt{n}\\) gives\\[\nn = (\\frac{z_{\\alpha/2}\\sigma}{D})^2\n\\] (round nearest integer)\\[\nH_0: \\mu \\ge 30 \\\\\nH_a: \\mu < 30\n\\]","code":"\ndata = rnorm(100)\nt.test(data, conf.level=0.95)\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = 0.42865, df = 99, p-value = 0.6691\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.1728394  0.2680940\n#> sample estimates:\n#>  mean of x \n#> 0.04762729\nt.test(data, mu=30,alternative=\"less\")\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = -269.57, df = 99, p-value < 2.2e-16\n#> alternative hypothesis: true mean is less than 30\n#> 95 percent confidence interval:\n#>       -Inf 0.2321136\n#> sample estimates:\n#>  mean of x \n#> 0.04762729"},{"path":"basic-statistical-inference.html","id":"note","chapter":"4 Basic Statistical Inference","heading":"4.1.6 Note","text":"t-tests, sample power easy z-test.\\[\n\\pi(\\mu) = P(\\frac{\\bar{y}-\\mu_0}{s/\\sqrt{n}}> t_{n-1;\\alpha}|\\mu)\n\\]\\(\\mu > \\mu_0\\) (.e., \\(\\mu - \\mu_0 = \\delta\\)), random variable \\((\\bar{y} - \\mu_0)/(s/\\sqrt{n})\\) [Student’s t distribution][Student T], rather distributed non-central t-distribution non-centrality parameter \\(\\delta \\sqrt{n}/\\sigma\\) d.f. \\(n-1\\)power increasing function non-centrality parameter (note, \\(\\delta = 0\\) distribution usual Student’s t-distribution).evaluate power, one must consider numerical procedure use special chartsApproximate Sample Size Adjustment t-test. use adjustment z-test determination sample size.Let \\(v = n-1\\), n sample size derived based z-test power. 2-sided t-test sample size (approximate) given:\\[\nn^* = \\frac{(t_{v;\\alpha/2}+t_{v;\\beta})^2 \\sigma^2}{\\delta^2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-non-parametric-methods","chapter":"4 Basic Statistical Inference","heading":"4.1.7 One-sample Non-parametric Methods","text":"","code":"\nlecture.data = c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83,-0.43,-0.34, 3.34, 2.33)"},{"path":"basic-statistical-inference.html","id":"sign-test","chapter":"4 Basic Statistical Inference","heading":"4.1.7.1 Sign Test","text":"want test \\(H_0: \\mu_{(0.5)} = 0; H_a: \\mu_{(0.5)} >0\\) \\(\\mu_{(0.5)}\\) population median. canCount number observation (\\(y_i\\)’s) exceed 0. Denote number \\(s_+\\), called number plus signs. Let \\(s_- = n - s_+\\), number minus signs.Reject \\(H_0\\) \\(s_+\\) large equivalently, \\(s_-\\) small.determine large \\(s_+\\) must reject \\(H_0\\) given significance level, need know distribution corresponding random variable \\(S_+\\) null hypothesis, binomial p = 1/2,w hen null true.work null distribution using binomial formula, \\(\\alpha\\)-level test rejects \\(H_0\\) \\(s_+ \\ge b_{n,\\alpha}\\), \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical point \\(Bin(n,1/2)\\) distribution. \\(S_+\\) \\(S_-\\) distribution (\\(S = S_+ = S_-\\)).\\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{= s_+}^{n} {{n}\\choose{}} (\\frac{1}{2})^n\n\\] equivalently,\\[\nP(S \\le s_-) = \\sum_{=0}^{s_-}{{n}\\choose{}} (\\frac{1}{2})^2\n\\] large sample sizes, use normal approximation binomial, case reject \\(H_0\\) \\[\ns_+ \\ge n/2 + 1/2 + z_{\\alpha}\\sqrt{n/4}\n\\]2-sided test, use tests statistic \\(s_{max} = max(s_+,s_-)\\) \\(s_{min} = min(s_+, s_-)\\). \\(\\alpha\\)-level test rejects \\(H_0\\) p-value \\(\\le \\alpha\\), p-value computed :\\[\np-value = 2 \\sum_{=s_{max}}^{n} {{n}\\choose{}} (\\frac{1}{2})^n = s \\sum_{=0}^{s_{min}} {{n}\\choose{}} (\\frac{1}{2})^n\n\\] Equivalently, rejecting \\(H_0\\) \\(s_{max} \\ge b_{n,\\alpha/2}\\)large sample normal approximation can used, \\[\nz = \\frac{s_{max}- n/2 -1/2}{\\sqrt{n/4}}\n\\] reject \\(H_0\\) \\(\\alpha\\) \\(z \\ge z_{\\alpha/2}\\)However, treatment 0 problematic test.Solution 1: randomly assign 0 positive negative (2 researchers might get different results).Solution 2: count 0 contribution 1/2 toward \\(s_+\\) \\(s_-\\) (apply binomial distribution)Solution 3: ignore 0 (reduces power test due decreased sample size).","code":"\nbinom.test(sum(lecture.data > 0), length(lecture.data)) \n#> \n#>  Exact binomial test\n#> \n#> data:  sum(lecture.data > 0) and length(lecture.data)\n#> number of successes = 8, number of trials = 10, p-value = 0.1094\n#> alternative hypothesis: true probability of success is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.4439045 0.9747893\n#> sample estimates:\n#> probability of success \n#>                    0.8\n# alternative = \"greater\" or alternative = \"less\""},{"path":"basic-statistical-inference.html","id":"wilcoxon-signed-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.1.7.2 Wilcoxon Signed Rank Test","text":"Since Sign Test consider magnitude observation 0, Wilcoxon Signed Rank Test improves taking account ordered magnitudes observation, impose requirement symmetric test (Sign Test )\\[\nH_0: \\mu_{0.5} = 0 \\\\\nH_a: \\mu_{0.5} > 0\n\\] (assume ties observations)signed rank test procedure:rank order observation \\(y_i\\) terms absolute values. Let \\(r_i\\) rank \\(y_i\\) ordering. Since assume ties, ranks \\(r_i\\) uniquely determined permutation integers \\(1,2,…,n\\).Calculate \\(w_+\\), sum ranks positive values, \\(w_-\\), sum ranks negative values. Note \\(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\\)Reject \\(H_0\\) \\(w_+\\) large (\\(w_-\\) small)know large small regard \\(w_+\\) \\(w_-\\), need distribution \\(W_+\\) \\(W_-\\) null true.Since null distributions identical symmetric, p-value \\(P(W \\ge w_+) = P(W \\le w_-)\\)\\(\\alpha\\)-level test rejects null p-value \\(\\le \\alpha\\), \\(w_+ \\ge w_{n,\\alpha}\\), \\(w_{n,\\alpha}\\) upper \\(\\alpha\\) critical point null distribution W.distribution W special table. large n, distribution W approximately normal.\\[\nz = \\frac{w_+ - n(n+1) /4 -1/2}{\\sqrt{n(n+1)(2n+1)/24}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) \\[\nw_+ \\ge n(n+1)/4 +1/2 + z_{\\alpha}\\sqrt{n(n+1)(2n+1)/24} \\approx w_{n,\\alpha}\n\\]2-sided test, use \\(w_{max}=max(w_+,w_-)\\) \\(w_{min}=min(w_+,w_-)\\), p-value given :\\[\np-value = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\] Sign Test,ignore 0. cases \\(|y_i|\\)’s may tied rank, simply assign tied ranks average rank (“midrank”).Example, \\(y_1 = -1\\), \\(y_3 = 3\\) \\(y_3 = -3\\), \\(y_4 =5\\), \\(r_1 = 1\\), \\(r_2 = r_3=(2+3)/2 = 2.5\\), \\(r_4 = 4\\)","code":"\nwilcox.test(lecture.data) \n#> \n#>  Wilcoxon signed rank exact test\n#> \n#> data:  lecture.data\n#> V = 52, p-value = 0.009766\n#> alternative hypothesis: true location is not equal to 0\n# does not use normal approximation\n# (using the underlying W distribution)\n\nwilcox.test(lecture.data,exact=F) \n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  lecture.data\n#> V = 52, p-value = 0.01443\n#> alternative hypothesis: true location is not equal to 0\n# uses normal approximation"},{"path":"basic-statistical-inference.html","id":"two-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.2 Two Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"means","chapter":"4 Basic Statistical Inference","heading":"4.2.1 Means","text":"Suppose 2 sets observations,\\(y_1,..., y_{n_y}\\)\\(x_1,...,x_{n_x}\\)random samples two independent populations means \\(\\mu_y\\) \\(\\mu_x\\) variances \\(\\sigma^2_y\\),\\(\\sigma^2_x\\). goal compare \\(\\mu_x\\) \\(\\mu_y\\) \\(\\sigma^2_y = \\sigma^2_x\\)","code":""},{"path":"basic-statistical-inference.html","id":"large-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.2.1.1 Large Sample Tests","text":"Assume \\(n_y\\) \\(n_x\\) large (\\(\\ge 30\\)). ,\\[\nE(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x \\\\\nVar(\\bar{y} - \\bar{x}) = \\sigma^2_y /n_y + \\sigma^2_x/n_x\n\\],\\[\nZ = \\frac{\\bar{y}-\\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\sigma^2_y /n_y + \\sigma^2_x/n_x}} \\sim N(0,1)\n\\] (according Central Limit Theorem). large samples, can replace variances unbiased estimators (\\(s^2_y,s^2_x\\)), get large sample distribution.approximate \\(100(1-\\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) given :\\[\n\\bar{y} - \\bar{x} \\pm z_{\\alpha/2}\\sqrt{s^2_y/n_y + s^2_x/n_x}\n\\]\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\\\\nH_A: \\mu_y - \\mu_x \\neq \\delta_0\n\\]\\(\\alpha\\)-level statistic:\\[\nz = \\frac{\\bar{y}-\\bar{x} - \\delta_0}{\\sqrt{s^2_y /n_y + s^2_x/n_x}}\n\\]reject \\(H_0\\) \\(|z| > z_{\\alpha/2}\\)\\(\\delta = )\\), means testing whether two means equal.","code":""},{"path":"basic-statistical-inference.html","id":"small-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2 Small Sample Tests","text":"two samples normal distribution, iid \\(N(\\mu_y,\\sigma^2_y)\\) iid \\(N(\\mu_x,\\sigma^2_x)\\) two samples independent, can inference based [t-distribution][Student T]2 casesEqual VarianceUnequal Variance","code":""},{"path":"basic-statistical-inference.html","id":"equal-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2.1 Equal variance","text":"Assumptionsiid: \\(var(\\bar{y}) = \\sigma^2_y / n_y ; var(\\bar{x}) = \\sigma^2_x / n_x\\)Independence samples: observation one sample can influence observation sample, \\[\n\\begin{aligned}\nvar(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var{\\bar{x}} - 2cov(\\bar{y},\\bar{x}) \\\\\n&= var(\\bar{y}) + var{\\bar{x}} \\\\\n&= \\sigma^2_y / n_y + \\sigma^2_x / n_x\n\\end{aligned}\n\\]Normality: Justifies use [t-distribution][Student T]Let \\(\\sigma^2 = \\sigma^2_y = \\sigma^2_x\\). , \\(s^2_y\\) \\(s^2_x\\) unbiased estimators \\(\\sigma^2\\). can pool .pooled variance estimate \\[\ns^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)}\n\\] \\(n_y + n_x -2\\) df.test statistic\\[\nT = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{s\\sqrt{1/n_y + 1/n_x}} \\sim t_{n_y + n_x -2}\n\\]\\(100(1 - \\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) \\[\n\\bar{y} - \\bar{x} \\pm (t_{n_y + n_x -2})s\\sqrt{1/n_y + 1/n_x}\n\\]Hypothesis testing:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\\\\nH_1: \\mu_y - \\mu_x \\neq \\delta_0\n\\]reject \\(H_0\\) \\(|t| > t_{n_y + n_x -2;\\alpha/2}\\)","code":""},{"path":"basic-statistical-inference.html","id":"unequal-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2.2 Unequal Variance","text":"AssumptionsTwo samples independentScatter plots\nCorrelation coefficient (normal)\nScatter plotsCorrelation coefficient (normal)Independence observation sampleTest serial correlation\nTest serial correlationFor sample, homogeneity varianceScatter plots\nFormal tests\nScatter plotsFormal testsNormalityEquality variances (homogeneity variance samples)F-test\nBarlett test\n[Modified Levene Test]\nF-testBarlett test[Modified Levene Test]compare 2 normal \\(\\sigma^2_y \\neq \\sigma^2_x\\), use test statistic:\\[\nT = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{\\sqrt{s^2_y/n_y + s^2_x/n_x}}\n\\] case, T follow [t-distribution][Student T] (distribution depends ratio unknown variances \\(\\sigma^2_y,\\sigma^2_x\\)). case small sizes, can can approximate tests using Welch-Satterthwaite method (Satterthwaite 1946). assume T can approximated [t-distribution][Student T], adjust degrees freedom.Let \\(w_y = s^2_y /n_y\\) \\(w_x = s^2_x /n_x\\) (w’s square respective standard errors)\n, degrees freedom \\[\nv = \\frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)}\n\\]Since v usually fractional, truncate nearest integer.\\(100 (1-\\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) \\[\n\\bar{y} - \\bar{x} \\pm t_{v,\\alpha/2} \\sqrt{s^2_y/n_y + s^2_x /n_x}\n\\]Reject \\(H_0\\) \\(|t| > t_{v,\\alpha/2}\\), \\[\nt = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"variances","chapter":"4 Basic Statistical Inference","heading":"4.2.2 Variances","text":"\\[\nF_{ndf,ddf}= \\frac{s^2_1}{s^2_2}\n\\]\\(s^2_1>s^2_2, ndf = n_1-1,ddf = n_2-1\\)","code":""},{"path":"basic-statistical-inference.html","id":"f-test","chapter":"4 Basic Statistical Inference","heading":"4.2.2.1 F-test","text":"Test\\[\nH_0: \\sigma^2_y = \\sigma^2_x \\\\\nH_a: \\sigma^2_y \\neq \\sigma^2_x\n\\]Consider test statistic,\\[\nF= \\frac{s^2_y}{s^2_x}\n\\]Reject \\(H_0\\) \\(F>f_{n_y -1,n_x -1,\\alpha/2}\\) \\(F<f_{n_y -1,n_x -1,1-\\alpha/2}\\)\\(F>f_{n_y -1,n_x -1,\\alpha/2}\\) \\(F<f_{n_y -1,n_x -1,1-\\alpha/2}\\) upper lower \\(\\alpha/2\\) critical points [F-distribution][F-Distribution], \\(n_y-1\\) \\(n_x-1\\) degrees freedom.NoteThis test depends heavily assumption Normality.particular, give many significant results observations come long-tailed distributions (.e., positive kurtosis).find support normality, can use nonparametric tests Modified Levene Test (Brown-Forsythe Test)","code":"\ndata(iris)\nirisVe=iris$Petal.Width[iris$Species==\"versicolor\"] \nirisVi=iris$Petal.Width[iris$Species==\"virginica\"]\n\nvar.test(irisVe,irisVi)\n#> \n#>  F test to compare two variances\n#> \n#> data:  irisVe and irisVi\n#> F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\n#> alternative hypothesis: true ratio of variances is not equal to 1\n#> 95 percent confidence interval:\n#>  0.2941935 0.9135614\n#> sample estimates:\n#> ratio of variances \n#>          0.5184243"},{"path":"basic-statistical-inference.html","id":"modified-levene-test-brown-forsythe-test","chapter":"4 Basic Statistical Inference","heading":"4.2.2.2 Modified Levene Test (Brown-Forsythe Test)","text":"considers averages absolute deviations rather squared deviations. Hence, less sensitive long-tailed distributions.test still good normal dataFor sample, consider absolute deviation observation form median:\\[\nd_{y,} = |y_i - y_{.5}| \\\\\nd_{x,} = |x_i - x_{.5}|\n\\] ,\\[\nt_L^* = \\frac{\\bar{d}_y-\\bar{d}_x}{s \\sqrt{1/n_y + 1/n_x}}\n\\]pooled variance \\(s^2\\) given :\\[\ns^2 = \\frac{\\sum_i^{n_y}(d_{y,}-\\bar{d}_y)^2 + \\sum_j^{n_x}(d_{x,}-\\bar{d}_x)^2}{n_y + n_x -2}\n\\]error terms constant variance \\(n_y\\) \\(n_x\\) extremely small, \\(t_L^* \\sim t_{n_x + n_y -2}\\)reject null hypothesis \\(|t_L^*| > t_{n_y + n_x -2;\\alpha/2}\\)just two-sample t-test applied absolute deviations.","code":"\ndVe=abs(irisVe-median(irisVe)) \ndVi=abs(irisVi-median(irisVi)) \nt.test(dVe,dVi,var.equal=T)\n#> \n#>  Two Sample t-test\n#> \n#> data:  dVe and dVi\n#> t = -2.5584, df = 98, p-value = 0.01205\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.12784786 -0.01615214\n#> sample estimates:\n#> mean of x mean of y \n#>     0.154     0.226\n\n# small samples t-test  \nt.test(irisVe,irisVi,var.equal=F)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  irisVe and irisVi\n#> t = -14.625, df = 89.043, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.7951002 -0.6048998\n#> sample estimates:\n#> mean of x mean of y \n#>     1.326     2.026"},{"path":"basic-statistical-inference.html","id":"power-1","chapter":"4 Basic Statistical Inference","heading":"4.2.3 Power","text":"Consider \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\)\nassumption equal variances, take size samples groups (\\(n_y = n_x = n\\))1-sided testing,\\[\nH_0: \\mu_y - \\mu_x \\le 0 \\\\\nH_a: \\mu_y - \\mu_x > 0\n\\]\\(\\alpha\\)-level z-test rejects \\(H_0\\) \\[\nz = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{2/n}} > z_{\\alpha}\n\\]\\[\n\\pi(\\mu_y - \\mu_x) = \\Phi(-z_{\\alpha} + \\frac{\\mu_y -\\mu_x}{\\sigma}\\sqrt{n/2})\n\\]need sample size n give least \\(1-\\beta\\) power \\(\\mu_y - \\mu_x = \\delta\\), \\(\\delta\\) smallest difference want see.Power given :\\[\n\\Phi(-z_{\\alpha} + \\frac{\\delta}{\\sigma}\\sqrt{n/2}) = 1 - \\beta\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-size-1","chapter":"4 Basic Statistical Inference","heading":"4.2.4 Sample Size","text":", sample size \\[\nn = 2(\\frac{\\sigma (z_{\\alpha} + z_{\\beta}}{\\delta})^2\n\\]2-sided test, replace \\(z_{\\alpha}\\) \\(z_{\\alpha/2}\\).\none-sample case, perform exact 2-sample t-test sample size calculation, must use non-central [t-distribution][Student T].correction gives approximate t-test sample size can obtained using z-test n value formula:\\[\nn^* = 2(\\frac{\\sigma (t_{2n-2;\\alpha} + t_{2n-2;\\beta})}{\\delta})^2\n\\]use \\(\\alpha/2\\) two-sided test","code":""},{"path":"basic-statistical-inference.html","id":"matched-pair-designs","chapter":"4 Basic Statistical Inference","heading":"4.2.5 Matched Pair Designs","text":"two treatmentswe assume \\(y_i \\sim^{iid} N(\\mu_y, \\sigma^2_y)\\) \\(x_i \\sim^{iid} N(\\mu_x,\\sigma^2_x)\\), since \\(y_i\\) \\(x_i\\) measured subject, correlated.Let\\[\n\\mu_D = E(y_i - x_i) = \\mu_y -\\mu_x \\\\\n\\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i)\n\\]matching induces positive correlation, variance difference measurements reduced compared independent case. point Matched Pair Designs. Although covariance can negative, giving larger variance difference independent sample case, usually covariance positive. means \\(y_i\\) \\(x_i\\) large many subjects, others, measurement small. (still assume various subjects respond independently , necessary iid assumption within groups).Let \\(d_i = y_i - x_i\\), \\(\\bar{d} = \\bar{y}-\\bar{x}\\) sample mean \\(d_i\\)\\(s_d^2=\\frac{1}{n-1}\\sum_{=1}^n (d_i - \\bar{d})^2\\) sample variance differenceOnce data converted differences, back One Sample Inference can use tests CIs.","code":""},{"path":"basic-statistical-inference.html","id":"nonparametric-tests-for-two-samples","chapter":"4 Basic Statistical Inference","heading":"4.2.6 Nonparametric Tests for Two Samples","text":"Matched Pair Designs, can use One-sample Non-parametric Methods.Assume Y X random variables CDF \\(F_y\\) \\(F_x\\). , Y stochastically larger X real number u, \\(P(Y > u) \\ge P(X > u)\\).Equivalently, \\(P(Y \\le u) \\le P(X \\le u)\\), \\(F_Y(u) \\le F_X(u)\\), thing \\(F_Y < F_X\\)two distributions identical, except one shifted relative , distribution can indexed location parameter, say \\(\\theta_y\\) \\(\\theta_x\\). case, \\(Y>X\\) \\(\\theta_y > \\theta_x\\)Consider hypotheses,\\[\nH_0: F_Y = F_X \\\\\nH_a: F_Y < F_X\n\\] alternative upper one-sided alternative.can also consider lower one-sided alternative\\[\nH_a: F_Y > F_X \\text{ } \\\\\nH_a: F_Y < F_X \\text{ } F_Y > F_X\n\\]case, don’t use \\(H_a: F_Y \\neq F_X\\) allows arbitrary differences distributions, without requiring one stochastically larger .distributions differ terms location parameters, can focus hypothesis tests parameters (e.g., \\(H_0: \\theta_y = \\theta_x\\) vs. \\(\\theta_y > \\theta_x\\))2 equivalent nonparametric tests consider hypothesis mentioned aboveWilcoxon rank testMann-Whitney U test","code":""},{"path":"basic-statistical-inference.html","id":"wilcoxon-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.2.6.1 Wilcoxon rank test","text":"Combine \\(n= n_y + n_x\\) observations rank ascending order.Sum ranks \\(y\\)’s \\(x\\)’s separately. Let \\(w_y\\) \\(w_x\\) sums. (\\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\\))Reject \\(H_0\\) \\(w_y\\) large (equivalently, \\(w_x\\) small)\\(H_0\\), arrangement \\(y\\)’s \\(x\\)’s equally likely occur, \\((n_y + n_x)!/(n_y! n_x!)\\) possible arrangements.Technically, arrangement can compute values \\(w_y\\) \\(w_x\\), thus generate distribution statistic null hypothesis.lead computationally intensive.","code":"\nwilcox.test(\n    irisVe,\n    irisVi,\n    alternative = \"two.sided\",\n    conf.level = 0.95,\n    exact = F,\n    correct = T\n)\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  irisVe and irisVi\n#> W = 49, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"mann-whitney-u-test","chapter":"4 Basic Statistical Inference","heading":"4.2.6.2 Mann-Whitney U test","text":"Mann-Whitney test computed follows:Compare \\(y_i\\) \\(x_i\\).\nLet \\(u_y\\) number pairs \\(y_i > x_i\\) Let \\(u_x\\) number pairs \\(y_i < x_i\\). (assume ties). \\(n_y n_x\\) comparisons \\(u_y + u_x = n_y n_x\\).Reject \\(H_0\\) \\(u_y\\) large (\\(u_x\\) small)Mann-Whitney U test Wilcoxon rank test related:\\[\nu_y = w_y - n_y(n_y+1) /2 \\\\\nu_x = w_x - n_x(n_x +1)/2\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(u_y \\ge u_{n_y,n_x,\\alpha}\\), \\(u_{n_y,n_x,\\alpha}\\) upper \\(\\alpha\\) critical point null distribution random variable, U.p-value defined \\(P(Y \\ge u_y) = P(U \\le u_x)\\). One advantage Mann-Whitney U test can use either \\(u_y\\) \\(u_x\\) carry test.large \\(n_y\\) \\(n_x\\), null distribution U can well approximated normal distribution mean \\(E(U) = n_y n_x /2\\) variance \\(var(U) = n_y n_x (n+1)/12\\). large sample z-test can based statistic:\\[\nz = \\frac{u_y - n_y n_x /2 -1/2}{\\sqrt{n_y n_x (n+1)/12}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) \\(z \\ge z_{\\alpha}\\) \\(u_y \\ge u_{n_y,n_x,\\alpha}\\) \\[\nu_{n_y, n_x, \\alpha} \\approx n_y n_x /2 + 1/2 + z_{\\alpha}\\sqrt{n_y n_x (n+1)/12}\n\\]2-sided test, use test statistic \\(u_{max} = max(u_y,u_x)\\) \\(u_{min} = min(u_y, u_x)\\) p-value given \\[\np-value = 2P(U \\ge u_{max}) = 2P(U \\le u_{min})\n\\] Since assume ties (\\(y_i = x_j\\)), count 1/2 towards \\(u_y\\) \\(u_x\\). Even though sampling distribution , large sample approximation still reasonable,","code":""},{"path":"basic-statistical-inference.html","id":"categorical-data-analysis","chapter":"4 Basic Statistical Inference","heading":"4.3 Categorical Data Analysis","text":"Categorical Data Analysis categorical outcomesNominal variables: logical ordering (e.g., sex)Ordinal variables: logical order, relative distances values clear (e.g., small, medium, large)distribution one variable changes level (values) variable change. row percentages different column.","code":""},{"path":"basic-statistical-inference.html","id":"inferences-for-small-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.1 Inferences for Small Samples","text":"approximate tests based asymptotic normality \\(\\hat{p}_1 - \\hat{p}_2\\) apply small samples.Using Fisher’s Exact Test evaluate \\(H_0: p_1 = p_2\\)Assume \\(X_1\\) \\(X_2\\) independent BinomialLet \\(x_1\\) \\(x_2\\) corresponding observed values.Let \\(n= n_1 + n_2\\) total sample size\\(m = x_1 + x_2\\) observed number successes.assuming m (total successes) fixed, conditioning value, one can show conditional distribution number successes sample 1 [Hypergeometric]want test \\(H_0: p_1 = p_2\\) \\(H_a: p_1 \\neq p_2\\), \\[\nZ^2 = (\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}})^2 \\sim \\chi_{1,\\alpha}^2\n\\]\\(\\chi_{1,\\alpha}^2\\) upper \\(\\alpha\\) percentage point central [Chi-squared] one d.f.extends contingency table setting: whether observed frequencies equal expected null hypothesis association.","code":""},{"path":"basic-statistical-inference.html","id":"test-of-association","chapter":"4 Basic Statistical Inference","heading":"4.3.2 Test of Association","text":"Pearson Chi-square test statistic \\[\n\\chi^2 = \\sum_{\\text{categories}} \\frac{(observed - epxected)^2}{expected}\n\\]Comparison proportions several independent surveys experiments\\(H_0: p_1 = p_2 = \\dots = p_k\\) vs. alternative null true (least one pair equal).estimate common value probability success single trial assuming \\(H_0\\) true:\\[\n\\hat{p} = \\frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k}\n\\]use table expected counts \\(H_0\\) true:\\[\n\\chi^2 = \\sum_{\\text{cells table}} \\frac{(observed - expected)^2}{expected}\n\\]\\(k-1\\) degrees freedom","code":""},{"path":"basic-statistical-inference.html","id":"two-way-count-data","chapter":"4 Basic Statistical Inference","heading":"4.3.2.1 Two-way Count Data","text":"Design 1\ntotal sample size fixed \\(n\\) = constant (e.g., survey job satisfaction income); row column totals random variablesDesign 2\nFix sample size group (row) (e.g., Drug treatments success failure); fixed number participants treatment; independent random samples two row populations.different sampling designs imply two different probability models.","code":""},{"path":"basic-statistical-inference.html","id":"total-sample-size-fixed","chapter":"4 Basic Statistical Inference","heading":"4.3.2.2 Total Sample Size Fixed","text":"Design 1random sample size n drawn single population, sample units cross-classified \\(r\\) row categories \\(c\\) columnThis results \\(r \\times c\\) table observed counts\\(n_{ij} = 1,...,r;j=1,...,c\\)Let \\(p_{ij}\\) probability classification cell \\((,j)\\) \\(\\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1\\). Let \\(N_{ij}\\) random variable corresponding \\(n_{ij}\\)\njoint distribution \\(N_{ij}\\) multinomial unknown parameters \\(p_{ij}\\)Denote row variable \\(X\\) column variable Y, \\(p_{ij} = P(X=,Y = j)\\) \\(p_{.} = P(X = )\\) \\(p_{.j} = P(Y = j)\\) marginal probabilities.\nnull hypothesis X Y statistically independent (.e., association) just:\\[\nH_0: p_{ij} = P(X =,Y=j) = P(X =) P(Y =j) = p_{.}p_{.j} \\\\\nH_a: p_{ij} \\neq p_{.}p_{.j}\n\\] \\(,j\\).","code":""},{"path":"basic-statistical-inference.html","id":"row-total-fixed","chapter":"4 Basic Statistical Inference","heading":"4.3.2.3 Row Total Fixed","text":"Design 2Random samples sizes \\(n_1,...,n_r\\) drawn independently \\(r \\ge 2\\) row populations. case, 2-way table row totals \\(n_{.} = n_i\\) \\(= 1,...,r\\).counts row modeled independent multinomial distributions.\\(X\\) fixed, \\(Y\\) observed., \\(p_{ij}\\) represent conditional probabilities \\(p_{ij} = P(Y=j|X=)\\)null hypothesis probability response j , regardless row population (.e., association):\\[\n\\begin{cases}\nH_0: p_{ij} = P(Y = j | X = ) = p_j & \\text{} ,j =1,2,...,c \\\\\n\\text{} H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) & \\text{ } \\\\\nH_a: (p_{i1},p_{i2},...,p_{ic}) & \\text{ } \n\\end{cases}\n\\]Although hypotheses tested different two sampling designs, \\(\\chi^2\\) test identicalWe estimated expected frequencies:\\[\n\\hat{e}_{ij} = \\frac{n_{.}n_{.j}}{n}\n\\]Chi-square statistic \\[\n\\chi^2 = \\sum_{=1}^{r} \\sum_{j=1}^{c} \\frac{(n_{ij}-\\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi_{(r-1)(c-1)}\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(\\chi^2 > \\chi^2_{(r-1)(c-1),\\alpha}\\)","code":""},{"path":"basic-statistical-inference.html","id":"pearson-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.3.2.4 Pearson Chi-square Test","text":"Determine whether association existsSometimes, \\(H_0\\) represents model whose validity tested. Contrast conventional formulation \\(H_0\\) hypothesis disproved. goal case disprove model, see whether data consistent model deviation can attributed chance.tests measure strength association.tests depend reflect sample size - double sample size copying observation, double \\(\\chi^2\\) statistic even thought strength association change.Pearson Chi-square Test appropriate 20% cells expected cell frequency less 5 (large-sample p-values appropriate).sample size small exact p-values can calculated (prohibitive large samples); calculation exact p-values assumes column totals row totals fixed.\\[\nH_0: p_J = 0.5 \\\\\nH_a: p_J < 0.5\n\\]\\[\nH_0: p_J = p_S \\\\\nH_a: p_j \\neq p_S\n\\]","code":"\njuly.x=480 \njuly.n=1000 \nsept.x=704 \nsept.n=1600\nprop.test(\n    x = july.x,\n    n = july.n,\n    p = 0.5,\n    alternative = \"less\",\n    correct = F\n)\n#> \n#>  1-sample proportions test without continuity correction\n#> \n#> data:  july.x out of july.n, null probability 0.5\n#> X-squared = 1.6, df = 1, p-value = 0.103\n#> alternative hypothesis: true p is less than 0.5\n#> 95 percent confidence interval:\n#>  0.0000000 0.5060055\n#> sample estimates:\n#>    p \n#> 0.48\nprop.test(\n    x = c(july.x, sept.x),\n    n = c(july.n, sept.n),\n    correct = F\n)\n#> \n#>  2-sample test for equality of proportions without continuity correction\n#> \n#> data:  c(july.x, sept.x) out of c(july.n, sept.n)\n#> X-squared = 3.9701, df = 1, p-value = 0.04632\n#> alternative hypothesis: two.sided\n#> 95 percent confidence interval:\n#>  0.0006247187 0.0793752813\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.48   0.44"},{"path":"basic-statistical-inference.html","id":"ordinal-association","chapter":"4 Basic Statistical Inference","heading":"4.3.3 Ordinal Association","text":"ordinal association implies one variable increases, tends increase decrease (depending nature association).tests variables two levels, levels must logical ordering.","code":""},{"path":"basic-statistical-inference.html","id":"mantel-haenszel-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.3.3.1 Mantel-Haenszel Chi-square Test","text":"Mantel-Haenszel Chi-square Test powerful testing ordinal associations, test strength association.test presented case one series \\(2 \\times 2\\) tables examine effects different conditions (\\(K\\) tables, \\(2 \\times 2 \\times K\\) table)stratum \\(k\\), given marginal totals \\((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\\), sampling model cell counts [Hypergeometric] (knowing \\(n_{11k}\\) determines \\((n_{12k},n_{21k},n_{22k})\\), given marginal totals)Assuming conditional independence, [Hypergeometric] mean variance \\(n_{11k}\\) \\[\nm_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}} \\\\\nvar(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)}\n\\]test conditional independence, Mantel Haenszel proposed\\[\nM^2 = \\frac{(|\\sum_{k} n_{11k} - \\sum_k m_{11k}| -.5)^2}{\\sum_{k}var(n_{11k})} \\sim \\chi^2_{1}\n\\] method can extended general \\(\\times J \\times K\\) tables.\\((2 \\times 2 \\times 3)\\) table","code":"\nBron = array(\n    c(20, 9, 382, 214, 10, 7, 172, 120, 12, 6, 327, 183),\n    dim = c(2, 2, 3),\n    dimnames = list(\n        Particulate = c(\"High\", \"Low\"),\n        Bronchitis = c(\"Yes\", \"No\"),\n        Age = c(\"15-24\", \"25-39\", \"40+\")\n    )\n)\nmargin.table(Bron, c(1, 2))\n#>            Bronchitis\n#> Particulate Yes  No\n#>        High  42 881\n#>        Low   22 517\n# assess whether the relationship between \n# Bronchitis by Particulate Level varies by Age\nlibrary(samplesizeCMH)\nmarginal_table = margin.table(Bron, c(1, 2))\nodds.ratio(marginal_table)\n#> [1] 1.120318\n\n#  whether these odds vary by age. \n# The conditional odds can be calculated using the original table.\napply(Bron, 3, odds.ratio)\n#>     15-24     25-39       40+ \n#> 1.2449098 0.9966777 1.1192661\n\n# Mantel-Haenszel Test\nmantelhaen.test(Bron, correct = T)\n#> \n#>  Mantel-Haenszel chi-squared test with continuity correction\n#> \n#> data:  Bron\n#> Mantel-Haenszel X-squared = 0.11442, df = 1, p-value = 0.7352\n#> alternative hypothesis: true common odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  0.6693022 1.9265813\n#> sample estimates:\n#> common odds ratio \n#>          1.135546"},{"path":"basic-statistical-inference.html","id":"mcnemars-test","chapter":"4 Basic Statistical Inference","heading":"4.3.3.1.1 McNemar’s Test","text":"special case Mantel-Haenszel Chi-square Test","code":"\nvote = cbind(c(682, 22), c(86, 810))\nmcnemar.test(vote, correct = T)\n#> \n#>  McNemar's Chi-squared test with continuity correction\n#> \n#> data:  vote\n#> McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09"},{"path":"basic-statistical-inference.html","id":"spearman-rank-correlation","chapter":"4 Basic Statistical Inference","heading":"4.3.3.2 Spearman Rank Correlation","text":"test strength association two ordinally scaled variables, can use Spearman Rank Correlation statisticLet \\(X\\) \\(Y\\) two random variables measured ordinal scale. Consider \\(n\\) pairs observations (\\(x_i,y_i\\)), \\(= 1,\\dots,n\\)Spearman Rank Correlation coefficient (denoted \\(r_S\\) calculated using Pearson correlation formula, based ranks \\(x_i\\) \\(y_i\\)).Spearman Rank Correlation calculatedAssign ranks \\(x_i\\)’s \\(y_i\\)’s separately. Let \\(u_i = rank(x_i)\\) \\(v_i = rank(y_i)\\)Calculate \\(r_S\\) using formula Pearson correlation coefficient, applied ranks:\\[\nr_S = \\frac{\\sum_{=1}^{n}(u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{(\\sum_{= 1}^{n}(u_i - \\bar{u})^2)(\\sum_{=1}^{n}(v_i - \\bar{v})^2)}}\n\\]\\(r_S\\) ranges -1 +1 , \\(r_S = -1\\) perfect negative monotone association\\(r_S = +1\\) perfect positive monotone association X Y.test\\(H_0:\\) \\(X\\) \\(Y\\) independent\\(H_0:\\) \\(X\\) \\(Y\\) independent\\(H_a\\): \\(X\\) \\(Y\\) positively associated\\(H_a\\): \\(X\\) \\(Y\\) positively associatedFor large \\(n\\) (e.g., \\(n \\ge 10\\)),\\[\nr_S \\sim N(0,1/(n-1))\n\\],\\[\nZ = r_s \\sqrt{n-1} \\sim N(0,1)\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"divergence-metrics-and-test-for-comparing-distributions","chapter":"4 Basic Statistical Inference","heading":"4.4 Divergence Metrics and Test for Comparing Distributions","text":"Similarity among distributions using divergence statistics, different fromDeviation statistics: difference realization variable value (e.g., mean). Statistics deviation distributions consist standard deviation, average absolute deviation, median absolute deviation , maximum absolute deviation.Deviation statistics: difference realization variable value (e.g., mean). Statistics deviation distributions consist standard deviation, average absolute deviation, median absolute deviation , maximum absolute deviation.Deviance statistics: goodness--fit statistic statistical models (comparable sum squares residuals OLS cases use ML estimation). Usually used generalized linear models.Deviance statistics: goodness--fit statistic statistical models (comparable sum squares residuals OLS cases use ML estimation). Usually used generalized linear models.Divergence statistics statistical distance (different metrics)Divergences require symmetryDivergences require symmetryDivergences generalize squared distance (instead linear distance). Hence, fail triangle inequityDivergences generalize squared distance (instead linear distance). Hence, fail triangle inequityCan used forDetecting data drift machine learningDetecting data drift machine learningFeature selectionsFeature selectionsVariational Auto EncoderVariational Auto EncoderDetect similarity policies (.e., distributions) reinforcement learningDetect similarity policies (.e., distributions) reinforcement learningTo see consistency two measured variables two constructs.see consistency two measured variables two constructs.TechniquesKullback-Leibler DivergenceKullback-Leibler DivergenceJensen-Shannon DivergenceJensen-Shannon DivergenceKolmogorov-Smirnov TestKolmogorov-Smirnov TestPackagesentropyentropyphilentropyphilentropy","code":""},{"path":"basic-statistical-inference.html","id":"kullback-leibler-divergence","chapter":"4 Basic Statistical Inference","heading":"4.4.1 Kullback-Leibler Divergence","text":"Also known relative entropyAlso known relative entropyNot metric (satisfy triangle inequality)metric (satisfy triangle inequality)Can generalized multivariate caseCan generalized multivariate caseMeasure similarity two discrete probability distributions\n\\(P\\) = true data distribution\n\\(Q\\) = predicted data distribution\nMeasure similarity two discrete probability distributions\\(P\\) = true data distribution\\(P\\) = true data distribution\\(Q\\) = predicted data distribution\\(Q\\) = predicted data distributionIt quantifies info loss moving \\(P\\) \\(Q\\) (.e., information loss \\(P\\) approximated \\(Q\\))quantifies info loss moving \\(P\\) \\(Q\\) (.e., information loss \\(P\\) approximated \\(Q\\))Discrete\\[\nD_{KL}(P ||Q) = \\sum_i P_i \\log(\\frac{P_i}{Q_i})\n\\]Continuous\\[\nD_{KL}(P||Q) = \\int P(x) \\log(\\frac{P(x)}{Q(x)}) dx\n\\]\\(K \\[0, \\infty)\\) similar diverge\\(K \\[0, \\infty)\\) similar divergeNon-symmetric two distributions: \\(D_{KL}(P|Q) \\neq D_{KL}(Q|P)\\)Non-symmetric two distributions: \\(D_{KL}(P|Q) \\neq D_{KL}(Q|P)\\)","code":"\nlibrary(philentropy)\n# philentropy::dist.diversity(rbind(X = 1:10 / sum(1:10), \n#                                   Y = 1:20 / sum(1:20)),\n#                             p = 2,\n#                             unit = \"log2\")\n\n\n# continuous\nKL(rbind(X = 1:10 / sum(1:10), Y = 1:10 / sum(1:10)), unit = \"log2\")\n#> kullback-leibler \n#>                0\n\n# discrete\nKL(rbind(X = 1:10, Y = 1:10), est.prob = \"empirical\")\n#> kullback-leibler \n#>                0"},{"path":"basic-statistical-inference.html","id":"jensen-shannon-divergence","chapter":"4 Basic Statistical Inference","heading":"4.4.2 Jensen-Shannon Divergence","text":"Also known info radius total divergence average\\[\nD_{JS} (P ||Q) = \\frac{1}{2}( D_{KL}(P||M)+ D_{KL}(Q||M))\n\\]\\(M = \\frac{1}{2} (P + Q)\\) mixed distribution\\(M = \\frac{1}{2} (P + Q)\\) mixed distribution\\(D_{JS} \\[0,1]\\) \\(\\log_2\\) \\(D_{JS} \\[0,\\ln(2)]\\) \\(\\log_e\\)\\(D_{JS} \\[0,1]\\) \\(\\log_2\\) \\(D_{JS} \\[0,\\ln(2)]\\) \\(\\log_e\\)","code":"\nlibrary(philentropy)\n# continous\nJSD(rbind(X = 1:10, Y = 1:20), unit = \"log2\")\n#> jensen-shannon \n#>       20.03201\n\n# discrete\nJSD(rbind(X = 1:10, Y = 1:20), est.prob = \"empirical\")\n#> jensen-shannon \n#>     0.06004756"},{"path":"basic-statistical-inference.html","id":"wasserstein-distance","chapter":"4 Basic Statistical Inference","heading":"4.4.3 Wasserstein Distance","text":"measure distance two empirical CDFs\\[\nW = \\int_{x \\R}|E(x) - F(X)|^p\n\\]also test statistics","code":"\nset.seed(1)\ntransport::wasserstein1d(rnorm(100), rnorm(100, mean = 1))\n#> [1] 0.8533046\n\nset.seed(1)\n# Wasserstein metric \ntwosamples::wass_stat(rnorm(100), rnorm(100, mean = 1))\n#> [1] 0.8533046\n\nset.seed(1)\n# permutation-based tw sample test using Wasserstein metric\ntwosamples::wass_test(rnorm(100), rnorm(100, mean = 1))\n#> Test Stat   P-Value \n#> 0.8533046 0.0002500"},{"path":"basic-statistical-inference.html","id":"kolmogorov-smirnov-test-1","chapter":"4 Basic Statistical Inference","heading":"4.4.4 Kolmogorov-Smirnov Test","text":"Can used continuous distribution\\(H_0\\): Empirical distribution follows specified distribution\\(H_1\\): Empirical distribution follow specified distributionUsing non-parametric\\[\nD= \\max|P(X) - Q(X)|\n\\]\\(D \\[0,1]\\) densities evenly distributed evenly distributedTo use test discrete date, use bootstrap version KS test (bypass continuity requirement)","code":"\nlibrary(entropy)\nlibrary(tidyverse)\n\nlst = list(sample_1 = c(1:20), sample_2 = c(2:30), sample_3 = c(3:30))\n\nexpand.grid(1:length(lst), 1:length(lst)) %>%\n    rowwise() %>%\n    mutate(KL = KL.empirical(lst[[Var1]], lst[[Var2]]))\n#> # A tibble: 9 × 3\n#> # Rowwise: \n#>    Var1  Var2     KL\n#>   <int> <int>  <dbl>\n#> 1     1     1 0     \n#> 2     2     1 0.150 \n#> 3     3     1 0.183 \n#> 4     1     2 0.704 \n#> 5     2     2 0     \n#> 6     3     2 0.0679\n#> 7     1     3 0.622 \n#> 8     2     3 0.0870\n#> 9     3     3 0\nMatching::ks.boot(Tr = c(0:10), Co = c(0:10))\n#> $ks.boot.pvalue\n#> [1] 1\n#> \n#> $ks\n#> \n#>  Exact two-sample Kolmogorov-Smirnov test\n#> \n#> data:  Tr and Co\n#> D = 0, p-value = 1\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $nboots\n#> [1] 1000\n#> \n#> attr(,\"class\")\n#> [1] \"ks.boot\""},{"path":"linear-regression.html","id":"linear-regression","chapter":"5 Linear Regression","heading":"5 Linear Regression","text":"Estimating parameters -> parametric (finite parameters)Estimating functions -> non-parametricEstimator Desirable PropertiesUnbiasedUnbiasedConsistencyConsistency\\(plim\\hat{\\beta_n}=\\beta\\)based law large numbers, can derive consistencyMore observations means precise, closer true value.EfficiencyMinimum variance comparison another estimator.\nOLS BLUE (best linear unbiased estimator) means OLS efficient among class linear unbiased estimator Gauss-Markov Theorem\ncorrect distributional assumptions, Maximum Likelihood asymptotically efficient among consistent estimators.\nMinimum variance comparison another estimator.OLS BLUE (best linear unbiased estimator) means OLS efficient among class linear unbiased estimator Gauss-Markov TheoremIf correct distributional assumptions, Maximum Likelihood asymptotically efficient among consistent estimators.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1 Ordinary Least Squares","text":"fundamental model statistics econometric OLS linear regression. OLS = Maximum likelihood error term assumed normally distributed.Regression still great underlying CEF (conditional expectation function) linear. regression following properties:\\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = + \\sum_{k=1}^K b_k X_{ki}\\) (.e., CEF \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) linear, regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) CEFFor \\(E[Y_i | X_{1i} , \\dots, X_{Ki}]\\) nonlinear function conditioning variables, regression \\(Y_i\\) \\(X_{1i}, \\dots, X_{Ki}\\) give best linear approximation nonlinear CEF (.e., minimize expected squared deviation fitted values linear model CEF).","code":""},{"path":"linear-regression.html","id":"simple-regression-basic-model","chapter":"5 Linear Regression","heading":"5.1.1 Simple Regression (Basic Model)","text":"\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\\(Y_i\\): response (dependent) variable -th observation\\(\\beta_0,\\beta_1\\): regression parameters intercept slope.\\(X_i\\): known constant (independent predictor variable) -th observation\\(\\epsilon_i\\): random error term\\[\n\\begin{aligned}\nE(\\epsilon_i) &= 0 \\\\\nvar(\\epsilon_i) &= \\sigma^2 \\\\\ncov(\\epsilon_i,\\epsilon_j) &= 0  \\text{ } \\neq j\n\\end{aligned}\n\\]\\(Y_i\\) random since \\(\\epsilon_i\\) :\\[\n\\begin{aligned}\nE(Y_i) &= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= E(\\beta_0) + E(\\beta_1 X_i) + E(\\epsilon) \\\\\n&= \\beta_0 + \\beta_1 X_i\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nvar(Y_i) &= var(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= var(\\epsilon_i) \\\\\n&= \\sigma^2\n\\end{aligned}\n\\]Since \\(cov(\\epsilon_i, \\epsilon_j) = 0\\) (uncorrelated), outcome one trail effect outcome . Hence, \\(Y_i, Y_j\\) uncorrelated well (conditioned \\(X\\)’s)NoteLeast Squares require distributional assumptionRelationship bivariate regression covarianceCovariance 2 variables:\\[\nC(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])]\n\\]following properties\\(C(X_i, X_i) = \\sigma^2_X\\)either \\(E(X_i) = 0 | E(Y_i) = 0\\), \\(Cov(X_i, Y_i) = E[X_i Y_i]\\)Given \\(W_i = + b X_i\\) \\(Z_i = c + d Y_i\\), \\(Cov(W_i, Z_i) = bdC(X_i, Y_i)\\)bivariate regression, slope \\[\n\\beta = \\frac{Cov(Y_i, X_i)}{Var(X_i)}\n\\]extend multivariate case\\[\n\\beta_k = \\frac{C(Y_i, \\tilde{X}_{ki})}{Var(\\tilde{X}_{ki})}\n\\]\\(\\tilde{X}_{ki}\\) residual regression \\(X_{ki}\\) \\(K-1\\) covariates included modelAnd intercept\\[\n\\alpha = E[Y_i] - \\beta E(X_i)\n\\]","code":""},{"path":"linear-regression.html","id":"estimation","chapter":"5 Linear Regression","heading":"5.1.1.1 Estimation","text":"Deviation \\(Y_i\\) expected value:\\[\nY_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]Consider sum square deviations:\\[\nQ = \\sum_{=1}^{n} (Y_i - \\beta_0 -\\beta_1 X_i)^2\n\\]\\[\n\\begin{aligned}\nb_1 &= \\frac{\\sum_{=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\nb_0 &= \\frac{1}{n}(\\sum_{=1}^{n}Y_i - b_1\\sum_{=1}^{n}X_i) = \\bar{Y} - b_1 \\bar{X}\n\\end{aligned}\n\\]","code":""},{"path":"linear-regression.html","id":"properties-of-least-least-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2 Properties of Least Least Estimators","text":"\\[\n\\begin{aligned}\nE(b_1) &= \\beta_1 \\\\\nE(b_0) &= E(\\bar{Y}) - \\bar{X}\\beta_1 \\\\\nE(\\bar{Y}) &= \\beta_0 + \\beta_1 \\bar{X} \\\\\nE(b_0) &= \\beta_0 \\\\\nvar(b_1) &= \\frac{\\sigma^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\nvar(b_0) &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2})\n\\end{aligned}\n\\]\\(var(b_1) \\0\\) measurements taken \\(X_i\\) values (unless \\(X_i\\) mean value)\\(var(b_0) \\0\\) \\(n\\) increases \\(X_i\\) values judiciously selected.Mean Square Error\\[\nMSE = \\frac{SSE}{n-2} = \\frac{\\sum_{=1}^{n}e_i^2}{n-2} = \\frac{\\sum(Y_i - \\hat{Y_i})^2}{n-2}\n\\]Unbiased estimator MSE:\\[\nE(MSE) = \\sigma^2\n\\]\\[\n\\begin{aligned}\ns^2(b_1) &= \\widehat{var(b_1)} = \\frac{MSE}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\ns^2(b_0) &= \\widehat{var(b_0)} = MSE(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(s^2(b_1)) &= var(b_1) \\\\\nE(s^2(b_0)) &= var(b_0)\n\\end{aligned}\n\\]","code":""},{"path":"linear-regression.html","id":"residuals","chapter":"5 Linear Regression","heading":"5.1.1.3 Residuals","text":"\\[\ne_i = Y_i - \\hat{Y} = Y_i - (b_0 + b_1 X_i)\n\\]\\(e_i\\) estimate \\(\\epsilon_i = Y_i - E(Y_i)\\)\\(\\epsilon_i\\) always unknown since don’t know true \\(\\beta_0, \\beta_1\\)\\[\n\\begin{aligned}\n\\sum_{=1}^{n} e_i &= 0 \\\\\n\\sum_{=1}^{n} X_i e_i &= 0\n\\end{aligned}\n\\]Residual properties\\(E[e_i] =0\\)\\(E[X_i e_i] = 0\\) \\(E[\\hat{Y}_i e_i ] = 0\\)","code":""},{"path":"linear-regression.html","id":"inference","chapter":"5 Linear Regression","heading":"5.1.1.4 Inference","text":"Normality AssumptionLeast Squares estimation require assumptions normality.However, inference parameters, need distributional assumptions.Inference \\(\\beta_0,\\beta_1\\) \\(Y_h\\) extremely sensitive moderate departures normality, especially sample size large.Inference \\(Y_{pred}\\) sensitive normality assumptions.Normal Error Regression Model\\[\nY_i \\sim N(\\beta_0+\\beta_1X_i, \\sigma^2)\n\\]","code":""},{"path":"linear-regression.html","id":"beta_1","chapter":"5 Linear Regression","heading":"5.1.1.4.1 \\(\\beta_1\\)","text":"normal error model,\\[\nb_1 \\sim N(\\beta_1,\\frac{\\sigma^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]linear combination independent normal random variable normally distributedHence,\\[\n\\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2}\n\\]\\((1-\\alpha) 100 \\%\\) confidence interval \\(\\beta_1\\) \\[\nb_1 \\pm t_{t-\\alpha/2 ; n-2}s(b_1)\n\\]","code":""},{"path":"linear-regression.html","id":"beta_0","chapter":"5 Linear Regression","heading":"5.1.1.4.2 \\(\\beta_0\\)","text":"normal error model, sampling distribution \\(b_0\\) \\[\nb_0 \\sim N(\\beta_0,\\sigma^2(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2}))\n\\]Hence,\\[\n\\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2}\n\\] \\((1-\\alpha)100 \\%\\) confidence interval \\(\\beta_0\\) \\[\nb_0 \\pm t_{1-\\alpha/2;n-2}s(b_0)\n\\]","code":""},{"path":"linear-regression.html","id":"mean-response","chapter":"5 Linear Regression","heading":"5.1.1.4.3 Mean Response","text":"Let \\(X_h\\) denote level X wish estimate mean responseWe denote mean response \\(X = X_h\\) \\(E(Y_h)\\)point estimator \\(E(Y_h)\\) \\(\\hat{Y}_h\\):\\[\n\\hat{Y}_h = b_0 + b_1 X_h\n\\] Note\\[\n\\begin{aligned}\nE(\\bar{Y}_h) &= E(b_0 + b_1X_h) \\\\\n&= \\beta_0 + \\beta_1 X_h \\\\\n&= E(Y_h)\n\\end{aligned}\n\\] (unbiased estimator)\\[\n\\begin{aligned}\nvar(\\hat{Y}_h) &= var(b_0 + b_1 X_h) \\\\\n&= var(\\hat{Y} + b_1 (X_h - \\bar{X})) \\\\\n&= var(\\bar{Y}) + (X_h - \\bar{X})^2var(b_1) + 2(X_h - \\bar{X})cov(\\bar{Y},b_1) \\\\\n&= \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\\\\n&= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2})\n\\end{aligned}\n\\]Since \\(cov(\\bar{Y},b_1) = 0\\) due iid assumption \\(\\epsilon_i\\)estimate variance \\[\ns^2(\\hat{Y}_h) = MSE (\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]sampling distribution mean response \\[\n\\begin{aligned}\n\\hat{Y}_h &\\sim N(E(Y_h),var(\\hat{Y_h})) \\\\\n\\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} &\\sim t_{n-2}\n\\end{aligned}\n\\]\\(100(1-\\alpha) \\%\\) CI \\(E(Y_h)\\) \\[\n\\hat{Y}_h \\pm t_{1-\\alpha/2;n-2}s(\\hat{Y}_h)\n\\]","code":""},{"path":"linear-regression.html","id":"prediction-of-a-new-observation","chapter":"5 Linear Regression","heading":"5.1.1.4.4 Prediction of a new observation","text":"Regarding Mean Response, interested estimating mean distribution Y given certain X.Now, want predict individual outcome distribution Y given X. call \\(Y_{pred}\\)Estimation mean response versus prediction new observation:point estimates cases: \\(\\hat{Y}_{pred} = \\hat{Y}_h\\)point estimates cases: \\(\\hat{Y}_{pred} = \\hat{Y}_h\\)variance prediction different; hence, prediction intervals different confidence intervals. prediction variance must consider:\nVariation mean distribution \\(Y\\)\nVariation within distribution \\(Y\\)\nvariance prediction different; hence, prediction intervals different confidence intervals. prediction variance must consider:Variation mean distribution \\(Y\\)Variation within distribution \\(Y\\)want predict: mean response + error\\[\n\\beta_0 + \\beta_1 X_h + \\epsilon\n\\]Since \\(E(\\epsilon) = 0\\), use least squares predictor:\\[\n\\hat{Y}_h = b_0 + b_1 X_h\n\\]variance predictor \\[\n\\begin{aligned}\nvar(b_0 + b_1 X_h + \\epsilon) &= var(b_0 + b_1 X_h) + var(\\epsilon) \\\\\n&= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2}) + \\sigma^2 \\\\\n&= \\sigma^2(1+\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\end{aligned}\n\\]estimate variance given \\[\ns^2(pred)= MSE (1+ \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]\\[\n\\frac{Y_{pred}-\\hat{Y}_h}{s(pred)} \\sim t_{n-2}\n\\]\\(100(1-\\alpha) \\%\\) prediction interval \\[\n\\bar{Y}_h \\pm t_{1-\\alpha/2; n-2}s(pred)\n\\]prediction interval sensitive distributional assumption errors, \\(\\epsilon\\)","code":""},{"path":"linear-regression.html","id":"confidence-band","chapter":"5 Linear Regression","heading":"5.1.1.4.5 Confidence Band","text":"want know confidence interval entire regression line, can draw conclusions mean response fo entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) rather given response \\(Y\\)Working-Hotelling Confidence BandFor given \\(X_h\\), band \\[\n\\hat{Y}_h \\pm W s(\\hat{Y}_h)\n\\] \\(W^2 = 2F_{1-\\alpha;2,n-2}\\), just 2 times F-stat 2 \\(n-2\\) degrees freedomthe interval width change \\(X_h\\) (since \\(s(\\hat{Y}_h)\\) changes)boundary values confidence band always define hyperbole containing regression linewill smallest \\(X = \\bar{X}\\)","code":""},{"path":"linear-regression.html","id":"anova","chapter":"5 Linear Regression","heading":"5.1.1.5 ANOVA","text":"Partitioning Total Sum Squares: Consider corrected Total sum squares:\\[\nSSTO = \\sum_{=1}^{n} (Y_i -\\bar{Y})^2\n\\]Measures overall dispersion response variable\nuse term corrected correct mean, uncorrected total sum squares given \\(\\sum Y_i^2\\)use \\(\\hat{Y}_i = b_0 + b_1 X_i\\) estimate conditional mean Y \\(X_i\\)\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\\n&= \\sum_{=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n(\\hat{Y}_i - \\bar{Y})^2 + 2\\sum_{=1}^n(Y_i - \\hat{Y}_i)(\\hat{Y}_i-\\bar{Y}) \\\\\n&= \\sum_{=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n(\\bar{Y}_i -\\bar{Y})^2 \\\\\nSTTO &= SSE + SSR \\\\\n\\end{aligned}\n\\]SSR regression sum squares, measures conditional mean varies central value.cross-product term decomposition 0:\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) &= \\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\\\\n&= b_1 \\sum_{=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2\\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= b_1 \\frac{\\sum_{=1}^{n}(Y_i -\\bar{Y})(X_i - \\bar{X})}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\sum_{=1}^{n}(X_i - \\bar{X})^2 - b_1^2\\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= b_1^2 \\sum_{=1}^{n}(X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= 0\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nSSTO &= SSR + SSE \\\\\n(n-1 d.f) &= (1 d.f.) + (n-2 d.f.)\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(MSE) &= \\sigma^2 \\\\\nE(MSR) &= \\sigma^2 + \\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2\n\\end{aligned}\n\\]\\(\\beta_1 = 0\\), two expected values sameif \\(\\beta_1 \\neq 0\\) E(MSR) larger E(MSE)means ratio two quantities, can infer something \\(\\beta_1\\)Distribution theory tells us \\(\\epsilon_i \\sim iid N(0,\\sigma^2)\\) assuming \\(H_0: \\beta_1 = 0\\) true,\\[\n\\begin{aligned}\n\\frac{MSE}{\\sigma^2} &\\sim \\chi_{n-2}^2 \\\\\n\\frac{MSR}{\\sigma^2} &\\sim \\chi_{1}^2 \\text{ } \\beta_1=0\n\\end{aligned}\n\\]two chi-square random variables independent.Since ratio 2 independent chi-square random variable follows F distribution, consider:\\[\nF = \\frac{MSR}{MSE} \\sim F_{1,n-2}\n\\]\\(\\beta_1 =0\\). Thus, reject \\(H_0: \\beta_1 = 0\\) (\\(E(Y_i)\\) = constant) \\(\\alpha\\) \\[\nF > F_{1 - \\alpha;1,n-2}\n\\]null hypothesis can tested approach.Coefficient Determination\\[\nR^2 = \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO}\n\\]\\(0 \\le R^2 \\le 1\\)Interpretation: proportionate reduction total variation \\(Y\\) fitting linear model \\(X\\).really correct say \\(R^2\\) “variation \\(Y\\) explained \\(X\\)”.\\(R^2\\) related correlation coefficient \\(Y\\) \\(X\\):\\[\nR^2 = (r)^2\n\\]\\(r= corr(x,y)\\) estimate Pearson correlation coefficient. Also, note\\[\n\\begin{aligned}\nb_1 &= (\\frac{\\sum_{=1}^{n}(Y_i - \\bar{Y})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2})^{1/2} \\\\\nr &= \\frac{s_y}{s_x} r\n\\end{aligned}\n\\]Lack Fit\\(Y_{11},Y_{21}, \\dots ,Y_{n_1,1}\\): \\(n_1\\) repeat obs \\(X_1\\)\\(Y_{1c},Y_{2c}, \\dots ,Y_{n_c,c}\\): \\(n_c\\) repeat obs \\(X_c\\), \\(c\\) distinct \\(X\\) values.Let \\(\\bar{Y}_j\\) mean replicates \\(X_j\\)Partition Error Sum Squares:\\[\n\\begin{aligned}\n\\sum_{} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j + \\hat{Y}_{ij})^2 \\\\\n&=  \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{} \\sum_{j} (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\\n&= \\sum_{} \\sum_{j}(Y_{ij} - \\bar{Y}_j)^2 + \\sum_j n_j (\\bar{Y}_j- \\hat{Y}_{ij})^2 \\\\\nSSE &= SSPE + SSLF \\\\\n\\end{aligned}\n\\]SSPE: “pure error sum squares” \\(n-c\\) degrees freedom since need estimate \\(c\\) meansSSLF: “lack fit sum squares” \\(c - 2\\) degrees freedom (number unique \\(X\\) values - number parameters used specify conditional mean regression model)\\[\n\\begin{aligned}\nMSPE &= \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c} \\\\\nMSLF &= \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}\n\\end{aligned}\n\\]F-test Lack--Fit tests\\[\n\\begin{aligned}\nH_0: Y_{ij} &= \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\epsilon_{ij} \\sim iid N(0,\\sigma^2) \\\\\nH_a: Y_{ij} &= \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1,...) + \\epsilon_{ij}^*,\\epsilon_{ij}^* \\sim iid N(0, \\sigma^2)\n\\end{aligned}\n\\]\\(E(MSPE) = \\sigma^2\\) either \\(H_0\\), \\(H_a\\)\\(E(MSLF) = \\sigma^2 + \\frac{\\sum n_j(f(X_i,...))^2}{n-2}\\) general \\(E(MSLF) = \\sigma^2\\) \\(H_0\\) trueWe reject \\(H_0\\) (.e., model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) adequate) \\[\nF = \\frac{MSLF}{MSPE} > F_{1-\\alpha;c-2,n-c}\n\\]Failing reject \\(H_0\\) imply \\(H_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}\\) exactly true, suggests model may provide reasonable approximation true model.Repeat observations effect \\(R^2\\):impossible \\(R^2\\) attain 1 repeat obs. exist (SSE can’t 0)maximum \\(R^2\\) attainable situation:\\[\nR^2_{max} = \\frac{SSTo - SSPE}{SSTO}\n\\]levels X need repeat observations.Typically, \\(H_0\\) appropriate, one still uses MSE estimate \\(\\sigma^2\\) rather MSPE, Since MSE degrees freedom, sometimes people pool estimates.Joint Inference\nconfidence coefficient \\(\\beta_0\\) \\(\\beta_1\\) considered simultaneously \\(\\le \\alpha\\)Let\\(\\bar{}_1\\) event first interval covers \\(\\beta_0\\)\\(\\bar{}_2\\) event second interval covers \\(\\beta_1\\)\\[\n\\begin{aligned}\nP(\\bar{}_1) &= 1 - \\alpha \\\\\nP(\\bar{}_2) &= 1 - \\alpha\n\\end{aligned}\n\\]probability \\(\\bar{}_1\\) \\(\\bar{}_2\\)\\[\n\\begin{aligned}\nP(\\bar{}_1 \\cap \\bar{}_2) &= 1 - P(\\bar{}_1 \\cup \\bar{}_2) \\\\\n&= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2) \\\\\n&\\ge 1 - P(A_1) - P(A_2) \\\\\n&= 1 - 2\\alpha\n\\end{aligned}\n\\]\\(\\beta_0\\) \\(\\beta_1\\) separate 95% confidence intervals, joint (family) confidence coefficient least \\(1 - 2(0.05) = 0.9\\). called Bonferroni InequalityWe use procedure obtained \\(1-\\alpha/2\\) confidence intervals two regression parameters separately, joint (Bonferroni) family confidence coefficient least \\(1- \\alpha\\)\\(1-\\alpha\\) joint Bonferroni confidence interval \\(\\beta_0\\) \\(\\beta_1\\) given calculating:\\[\n\\begin{aligned}\nb_0 &\\pm B s(b_0) \\\\\nb_1 &\\pm B s(b_1)\n\\end{aligned}\n\\]\\(B= t_{1-\\alpha/4;n-2}\\)Interpretation: repeated samples taken joint \\((1-\\alpha)\\) intervals \\(\\beta_0\\) \\(\\beta_1\\) obtained, \\((1-\\alpha)100\\)% joint intervals contain true pair \\((\\beta_0, \\beta_1)\\). , \\(\\alpha \\times 100\\)% samples, one intervals contain true value.Bonferroni interval conservative. lower bound joint intervals tend correct \\((1-\\alpha)100\\)% time (lower power). People usually consider larger \\(\\alpha\\) Bonferroni joint tests (e.g, \\(\\alpha=0.1\\))Bonferroni procedure extends testing 2 parameters. Say interested testing \\(\\beta_0,\\beta_1,..., \\beta_{g-1}\\) (g parameters test). , joint Bonferroni interval obtained calculating \\((1-\\alpha/g)\\) 100% level interval separately.example, \\(\\alpha = 0.05\\) \\(g=10\\), individual test done \\(1- \\frac{.05}{10}\\) level. 2-sided intervals, corresponds using \\(t_{1-\\frac{0.05}{2(10)};n-p}\\) CI formula. procedure works best g relatively small, otherwise intervals individual parameter wide test way conservative.\\(b_0,b_1\\) usually correlated (negatively \\(\\bar{X} >0\\) positively \\(\\bar{X}<0\\))multiple comparison procedures available.","code":""},{"path":"linear-regression.html","id":"assumptions","chapter":"5 Linear Regression","heading":"5.1.1.6 Assumptions","text":"Linearity regression functionError terms constant varianceError terms independentNo outliersError terms normally distributedNo Omitted variables","code":""},{"path":"linear-regression.html","id":"diagnostics","chapter":"5 Linear Regression","heading":"5.1.1.7 Diagnostics","text":"Constant Variance\nPlot residuals vs. X\nConstant VariancePlot residuals vs. XOutliers\nplot residuals vs. X\nbox plots\nstem-leaf plots\nscatter plots\nOutliersplot residuals vs. Xplot residuals vs. Xbox plotsbox plotsstem-leaf plotsstem-leaf plotsscatter plotsscatter plotsWe use standardize residuals unit variance. standardized residuals called studentized residuals:\\[\nr_i = \\frac{e_i -\\bar{e}}{s(e_i)} = \\frac{e_i}{s(e_i)}\n\\]simplified standardization procedure gives semi-studentized residuals:\\[\ne_i^* = \\frac{e_i - \\bar{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\n\\]Non-independent Error Termsplot residuals vs. timeResiduals \\(e_i\\) independent random variables involve fitted values \\(\\hat{Y}_i\\), based fitted regression function.sample size large, dependency among \\(e_i\\) relatively unimportant.detect non-independence, helps plot residual \\(\\)-th response vs. \\((-1)\\)-thNon-normality Error Termsto detect non-normality (distribution plots residuals, box plots residuals, stem-leaf plots residuals, normal probability plots residuals)Need relatively large sample sizes.types departure affect distribution residuals (wrong regression function, non-constant error variance,…)","code":""},{"path":"linear-regression.html","id":"objective-tests-of-model-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.7.1 Objective Tests of Model Assumptions","text":"Normality\nUse Methods based empirical cumulative distribution function test residuals.\nNormalityUse Methods based empirical cumulative distribution function test residuals.Constancy error variance\nBrown-Forsythe Test (Modified Levene Test)\nBreusch-Pagan Test (Cook-Weisberg Test)\nConstancy error varianceBrown-Forsythe Test (Modified Levene Test)Breusch-Pagan Test (Cook-Weisberg Test)","code":""},{"path":"linear-regression.html","id":"remedial-measures","chapter":"5 Linear Regression","heading":"5.1.1.8 Remedial Measures","text":"simple linear regression appropriate, one can:complicated modelstransformations \\(X\\) /\\(Y\\) (may “optimal” results)Remedial measures based deviations:Non-linearity:\nTransformations\ncomplicated models\nNon-linearity:Transformationsmore complicated modelsNon-constant error variance:\nWeighted Least Squares\nTransformations\nNon-constant error variance:Weighted Least SquaresTransformationsCorrelated errors:\nserially correlated error models (times series)\nCorrelated errors:serially correlated error models (times series)Non-normalityNon-normalityAdditional variables: multiple regression.Additional variables: multiple regression.Outliers:\nRobust estimation.\nOutliers:Robust estimation.","code":""},{"path":"linear-regression.html","id":"transformations","chapter":"5 Linear Regression","heading":"5.1.1.8.1 Transformations","text":"use transformations one variables performing regression analysis.\nproperties least-squares estimates apply transformed regression, original variable.transform Y variable perform regression get:\\[\ng(Y_i) = b_0 + b_1 X_i\n\\]Transform back:\\[\n\\hat{Y}_i = g^{-1}(b_0 + b_1 X_i)\n\\]\\(\\hat{Y}_i\\) biased. can correct bias.Box-Cox Family Transformations\\[\nY'= Y^{\\lambda}\n\\]\\(\\lambda\\) parameter determined data.pick \\(\\lambda\\), can estimation :trial errormaximum likelihoodnumerical searchVariance Stabilizing TransformationsA general method finding variance stabilizing transformation, standard deviation function mean, delta method - application Taylor series expansion.\\[\n\\sigma = \\sqrt{var(Y)} = f(\\mu)\n\\]\\(\\mu = E(Y)\\) \\(f(\\mu)\\) smooth function mean.Consider transformation \\(h(Y)\\). Expand function Taylor series \\(\\mu\\). ,\\[\nh(Y) = h(\\mu) + h'(\\mu)(Y-\\mu) + \\text{small terms}\n\\]want select function h(.) variance h(Y) nearly constant values \\(\\mu= E(Y)\\):\\[\n\\begin{aligned}\nconst &= var(h(Y)) \\\\\n&= var(h(\\mu) + h'(\\mu)(Y-\\mu)) \\\\\n&= (h'(\\mu))^2 var(Y-\\mu) \\\\\n&= (h'(\\mu))^2 var(Y) \\\\\n&= (h'(\\mu))^2(f(\\mu))^2 \\\\\n\\end{aligned}\n\\]must ,\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)}\n\\],\\[\nh(\\mu) = \\int\\frac{1}{f(\\mu)}d\\mu\n\\]Example: Poisson distribution: \\(\\sigma^2 = var(Y) = E(Y) = \\mu\\),\\[\n\\begin{aligned}\n\\sigma = f(\\mu) &= \\sqrt{mu} \\\\\nh'(\\mu) &\\propto \\frac{1}{\\mu} = \\mu^{-.5}\n\\end{aligned}\n\\], variance stabilizing transformation :\\[\nh(\\mu) = \\int \\mu^{-.5} d\\mu = \\frac{1}{2} \\sqrt{\\mu}\n\\]hence, \\(\\sqrt{Y}\\) used variance stabilizing transformation.don’t know \\(f(\\mu)\\)Trial error. Look residuals plotsAsk researchers previous studies find published results similar experiments determine transformation used.multiple observations \\(Y_{ij}\\) X values, compute \\(\\bar{Y}_i\\) \\(s_i\\) plot \n\\(s_i \\propto \\bar{Y}_i^{\\lambda}\\) consider \\(s_i = \\bar{Y}_i^{\\lambda}\\) \\(ln(s_i) = ln() + \\lambda ln(\\bar{Y}_i)\\). regression natural log \\(s_i\\) natural log \\(\\bar{Y}_i\\) gives \\(\\hat{}\\) \\(\\hat{\\lambda}\\) suggests form \\(f(\\mu)\\) don’t multiple obs, might still able “group” observations get \\(\\bar{Y}_i\\) \\(s_i\\).","code":""},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"5 Linear Regression","heading":"5.1.2 Multiple Linear Regression","text":"Geometry Least Squares\\[\n\\begin{aligned}\n\\mathbf{\\hat{y}} &= \\mathbf{Xb} \\\\\n&= \\mathbf{X(X'X)^{-1}X'y} \\\\\n&= \\mathbf{Hy}\n\\end{aligned}\n\\]sometimes \\(\\mathbf{H}\\) denoted \\(\\mathbf{P}\\).\\(\\mathbf{H}\\) projection operator.\\[\n\\mathbf{\\hat{y}= Hy}\n\\]projection y onto linear space spanned columns \\(\\mathbf{X}\\) (model space). dimension model space rank \\(\\mathbf{X}\\).Facts:\\(\\mathbf{H}\\) symmetric (.e., \\(\\mathbf{H} = \\mathbf{H}'\\))\\(\\mathbf{HH} = \\mathbf{H}\\)\\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'}\n\\end{aligned}\n\\]\\(\\mathbf{H}\\) \\(n \\times n\\) matrix \\(rank(\\mathbf{H}) = rank(\\mathbf{X})\\)\\(\\mathbf{(-H) = - X(X'X)^{-1}X'}\\) also projection operator. projects onto \\(n - k\\) dimensional space orthogonal \\(k\\) dimensional space spanned columns \\(\\mathbf{X}\\)\\(\\mathbf{H(-H)=(-H)H = 0}\\)Partition uncorrected total sum squares:\\[\n\\begin{aligned}\n\\mathbf{y'y} &= \\mathbf{\\hat{y}'\\hat{y} + e'e} \\\\\n&= \\mathbf{(Hy)'(Hy) + ((-H)y)'((-H)y)} \\\\\n&= \\mathbf{y'H'Hy + y'(-H)'(-H)y} \\\\\n&= \\mathbf{y'Hy + y'(-H)y} \\\\\n\\end{aligned}\n\\]partition corrected total sum squares:\\[\n\\mathbf{y'(-H_1)y = y'(H-H_1)y + y'(-H)y}\n\\]\\(H_1 = \\frac{1}{n} J = 1'(1'1)1\\)Equivalently, can express\\[\n\\mathbf{Y = X\\hat{\\beta} + (Y - X\\hat{\\beta})}\n\\]\\(\\mathbf{\\hat{Y} = X \\hat{\\beta}}\\) = sum vector fitted values\\(\\mathbf{e = ( Y - X \\hat{\\beta})}\\) = residual\\(\\mathbf{Y}\\) \\(n \\times 1\\) vector n-dimensional space \\(R^n\\)\\(\\mathbf{X}\\) \\(n \\times p\\) full rank matrix. columns generate \\(p\\)-dimensional subspace \\(R^n\\). Hence, estimator \\(\\mathbf{X \\hat{\\beta}}\\) also subspace.choose least squares estimator minimize distance \\(\\mathbf{Y}\\) \\(\\mathbf{X \\hat{\\beta}}\\), orthogonal projection \\(\\mathbf{Y}\\) onto \\(\\mathbf{X\\beta}\\).\\[\n\\begin{aligned}\n||\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}||^2 &= \\mathbf{||Y - X\\hat{\\beta}||}^2 + \\mathbf{||X \\hat{\\beta}||}^2 \\\\\n&= \\mathbf{(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta}) +(X \\hat{\\beta})'(X \\hat{\\beta})} \\\\\n&= \\mathbf{(Y - X\\hat{\\beta})'Y - (Y - X\\hat{\\beta})'X\\hat{\\beta} + \\hat{\\beta}' X'X\\hat{\\beta}} \\\\\n&= \\mathbf{(Y-X\\hat{\\beta})'Y + \\hat{\\beta}'X'X(XX')^{-1}X'Y} \\\\\n&= \\mathbf{Y'Y - \\hat{\\beta}'X'Y + \\hat{\\beta}'X'Y}\n\\end{aligned}\n\\]norm \\((p \\times 1)\\) vector \\(\\mathbf{}\\) defined :\\[\n\\mathbf{|||| = \\sqrt{'}} = \\sqrt{\\sum_{=1}^p ^2_i}\n\\]Coefficient Multiple Determination\\[\nR^2 = \\frac{SSR}{SSTO}= 1- \\frac{SSE}{SSTO}\n\\]Adjusted Coefficient Multiple Determination\\[\nR^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)SSE}{(n-p)SSTO}\n\\]Sequential Partial Sums Squares:regression model coefficients \\(\\beta = (\\beta_0, \\beta_1,...,\\beta_{p-1})'\\), denote uncorrected corrected SS \\[\n\\begin{aligned}\nSSM &= SS(\\beta_0, \\beta_1,...,\\beta_{p-1}) \\\\\nSSM_m &= SS(\\beta_0, \\beta_1,...,\\beta_{p-1}|\\beta_0)\n\\end{aligned}\n\\]2 decompositions \\(SSM_m\\):Sequential SS: (unique -depends order, also referred Type SS, default anova() R)\\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + ...+ SS(\\beta_{p-1}| \\beta_0,...,\\beta_{p-2})\n\\]Partial SS: (use practice - contribution given others)\\[\nSSM_m = SS(\\beta_1 | \\beta_0,\\beta_2,...,\\beta_{p-1}) + ... + SS(\\beta_{p-1}| \\beta_0, \\beta_1,...,\\beta_{p-2})\n\\]","code":""},{"path":"linear-regression.html","id":"ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.3 OLS Assumptions","text":"A1 LinearityA2 Full rankA3 Exogeneity Independent VariablesA4 HomoskedasticityA5 Data Generation (random Sampling)A6 Normal Distribution","code":""},{"path":"linear-regression.html","id":"a1-linearity","chapter":"5 Linear Regression","heading":"5.1.3.1 A1 Linearity","text":"restrictive\\(x\\) can nonlinear transformation including interactions, natural log, quadraticWith A3 (Exogeneity Independent), linearity can restrictive","code":""},{"path":"linear-regression.html","id":"log-model","chapter":"5 Linear Regression","heading":"5.1.3.1.1 Log Model","text":"","code":""},{"path":"linear-regression.html","id":"higher-orders","chapter":"5 Linear Regression","heading":"5.1.3.1.2 Higher Orders","text":"\\(y=\\beta_0 + x_1\\beta_1 + x_1^2\\beta_2 + \\epsilon\\)\\[\n\\frac{\\partial y}{\\partial x_1}=\\beta_1 + 2x_1\\beta_2\n\\]effect \\(x_1\\) y depends level \\(x_1\\)partial effect average = \\(\\beta_1+2E(x_1)\\beta_2\\)Average Partial Effect = \\(E(\\beta_1 + 2x_1\\beta_2)\\)","code":""},{"path":"linear-regression.html","id":"interactions","chapter":"5 Linear Regression","heading":"5.1.3.1.3 Interactions","text":"\\(y=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3 + \\epsilon\\)\\(\\beta_1\\) average effect y unit change \\(x_1\\) \\(x_2=0\\)\\(\\beta_1 + x_2\\beta_3\\) partial effect \\(x_1\\) y depends level \\(x_2\\)","code":""},{"path":"linear-regression.html","id":"a2-full-rank","chapter":"5 Linear Regression","heading":"5.1.3.2 A2 Full rank","text":"also known identification conditioncolumns \\(\\mathbf{x}\\) written linear function columnswhich ensures parameter unique exists population regression equation","code":""},{"path":"linear-regression.html","id":"a3-exogeneity-of-independent-variables","chapter":"5 Linear Regression","heading":"5.1.3.3 A3 Exogeneity of Independent Variables","text":"strict exogeneityalso known mean independence check back Correlation Independenceby Law Iterated Expectations \\(E(\\epsilon)=0\\), can satisfied always including intercept.independent variables carry information prediction \\(\\epsilon\\)A3 implies \\(E(y|x)=x\\beta\\), means conditional mean function must linear function \\(x\\) A1 Linearity","code":""},{"path":"linear-regression.html","id":"a3a","chapter":"5 Linear Regression","heading":"5.1.3.3.1 A3a","text":"Weaker Exogeneity AssumptionExogeneity Independent variablesA3a: \\(E(\\mathbf{x_i'}\\epsilon_i)=0\\)\\(x_i\\) uncorrelated \\(\\epsilon_i\\) Correlation Independence\\(x_i\\) uncorrelated \\(\\epsilon_i\\) Correlation IndependenceWeaker mean independence A3\nA3 implies A3a, reverse\ncausality interpretations\ntest difference\nWeaker mean independence A3A3 implies A3a, reverseNo causality interpretationsCannot test difference","code":""},{"path":"linear-regression.html","id":"a4-homoskedasticity","chapter":"5 Linear Regression","heading":"5.1.3.4 A4 Homoskedasticity","text":"Variation disturbance independent variables","code":""},{"path":"linear-regression.html","id":"a5-data-generation-random-sampling","chapter":"5 Linear Regression","heading":"5.1.3.5 A5 Data Generation (random Sampling)","text":"random samplerandom sample mean samples independent identically distributed (iid) joint distribution \\((y,\\mathbf{x})\\)random sample mean samples independent identically distributed (iid) joint distribution \\((y,\\mathbf{x})\\)A3 A4, \nStrict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables carry information prediction \\(\\epsilon\\)\nNon-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) error term uncorrelated across draws conditional independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\)\nA3 A4, haveStrict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables carry information prediction \\(\\epsilon\\)Non-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) error term uncorrelated across draws conditional independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\)times series spatial settings, A5 less likely hold.times series spatial settings, A5 less likely hold.","code":""},{"path":"linear-regression.html","id":"a5a","chapter":"5 Linear Regression","heading":"5.1.3.5.1 A5a","text":"stochastic process \\(\\{x_t\\}_{t=1}^T\\) stationary every collection fo time indices \\(\\{t_1,t_2,...,t_m\\}\\), joint distribution \\[\nx_{t_1},x_{t_2},...,x_{t_m}\n\\]joint distribution \\[\nx_{t_1+h},x_{t_2+h},...,x_{t_m+h}\n\\]\\(h \\ge 1\\)joint distribution first ten observation next ten, etc.Independent draws automatically satisfies thisA stochastic process \\(\\{x_t\\}_{t=1}^T\\) weakly stationary \\(x_t\\) \\(x_{t+h}\\) “almost independent” h increases without bounds.two observation far apart “almost independent”Common Weakly Dependent ProcessesMoving Average process order 1 (MA(1))MA(1) means one period lag.\\[\n\\begin{aligned}\ny_t &= u_t + \\alpha_1 u_{t-1} \\\\\nE(y_t) &= E(u_t) + \\alpha_1E(u_{t-1}) = 0 \\\\\nVar(y_t) &= var(u_t) + \\alpha_1 var(u_{t-1}) \\\\\n&= \\sigma^2 + \\alpha_1^2 \\sigma^2 \\\\\n&= \\sigma^2(1+\\alpha_1^2)\n\\end{aligned}\n\\]\\(u_t\\) drawn iid t variance \\(\\sigma^2\\)increase absolute value \\(\\alpha_1\\) increases varianceWhen MA(1) process can inverted (\\(|\\alpha|<1\\) \\[\nu_t = y_t - \\alpha_1u_{t-1}\n\\]called autoregressive representation (express current observation term past observation).can expand 1 lag, MA(q) process\\[\ny_t = u_t + \\alpha_1 u_{t-1} + ... + \\alpha_q u_{t-q}\n\\]\\(u_t \\sim WN(0,\\sigma^2)\\)Covariance stationary: irrespective value parameters.Invertibility \\(\\alpha < 1\\)conditional mean MA(q) depends q lags (long-term memory).MA(q), autocorrelations beyond q 0.\\[\n\\begin{aligned}\nCov(y_t,y_{t-1}) &= Cov(u_t + \\alpha_1 u_{t-1},u_{t-1}+\\alpha_1u_{t-2}) \\\\\n&= \\alpha_1var(u_{t-1}) \\\\\n&= \\alpha_1\\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nCov(y_t,y_{t-2}) &= Cov(u_t + \\alpha_1 u_{t-1},u_{t-2}+\\alpha_{1}u_{t-3}) \\\\\n&= 0\n\\end{aligned}\n\\]MA models linear relationship dependent variable current past values stochastic term.Auto regressive process order 1 (AR(1))\\[\ny_t = \\rho y_{t-1}+ u_t, |\\rho|<1\n\\]\\(u_t\\) drawn iid t variance \\(\\sigma^2\\)\\[\n\\begin{aligned}\nCov(y_t,y_{t-1}) &= Cov(\\rho y_{t-1} + u-t,y_{t-1}) \\\\\n&= \\rho Var(y_{t-1}) \\\\\n&= \\rho \\frac{\\sigma^2}{1-\\rho^2}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nCov(y_t,y_{t-h}) &= \\rho^h \\frac{\\sigma^2}{1-\\rho^2}\n\\end{aligned}\n\\]Stationarity: continuum t, distribution t \\[\n\\begin{aligned}\nE(y_t) &= E(y_{t-1}) = ...= E(y_0) \\\\\ny_1 &= \\rho y_0 + u_1\n\\end{aligned}\n\\]initial observation \\(y_0=0\\)Assume \\(E(y_t)=0\\)\\[\n\\begin{aligned}\ny_t &= \\rho^t y_{t-t} + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\\\\n&= \\rho^t y_0 + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t\n\\end{aligned}\n\\]Hence, \\(y_t\\) weighted \\(u_t\\) time observations . y correlated previous observations well future observations.\\[\n\\begin{aligned}\nVar(y_t) &= Var(\\rho y_{t-1} + u_t) \\\\\n&= \\rho^2 Var(y_{t-1}) + Var(u_t) + 2\\rho Cov(y_{t-1}u_t) \\\\\n&= \\rho^2 Var(y_{t-1}) + \\sigma^2\n\\end{aligned}\n\\]Hence,\\[\nVar(y_t) = \\frac{\\sigma^2}{1-\\rho^2}\n\\]Variance constantly time, \\(\\rho \\neq 1\\) \\(-1\\).stationarity requires \\(\\rho \\neq 1\\) -1. weakly dependent process \\(|\\rho|<1\\)estimate AR(1) process, use Yule-Walker Equation\\[\n\\begin{aligned}\ny_t &= \\epsilon_t + \\phi y_{t-1} \\\\\ny_t y_{t-\\tau} &= \\epsilon_t y_{t-\\tau} + \\phi y_{t-1}y_{t-\\tau}\n\\end{aligned}\n\\]\\(\\tau \\ge 1\\), \\[\n\\gamma \\tau = \\phi \\gamma (\\tau -1)\n\\]\\[\n\\rho_t = \\phi^t\n\\]generalize \\(p\\)-th order autoregressive process, AR(p):\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\n\\]AR(p) process covariance stationary, decay autocorrelations.combine MA(q) AR(p), ARMA(p,q) process, can see seasonality. example, ARMA(1,1)\\[\ny_t = \\phi y_{t-1} + \\epsilon_t + \\alpha \\epsilon_{t-1}\n\\]Random Walk process\\[\ny_t = y_0 + \\sum_{s=1}^{t}u_t\n\\]stationary : \\(y_0 = 0\\) \\(E(y_t)= 0\\), \\(Var(y_t)=t\\sigma^2\\). along spectrum, variance largernot weakly dependent: \\(Cov(\\sum_{s=1}^{t}u_s,\\sum_{s=1}^{t-h}u_s) = (t-h)\\sigma^2\\). covariance (fixed) diminishing h increasesAssumption A5a: \\(\\{y_t,x_{t1},..,x_{tk-1} \\}\\)\\(t=1,...,T\\) stationary weakly dependent processes.Alternative [Weak Law], Central Limit Theorem\n\\(z_t\\) weakly dependent stationary process finite first absolute moment \\(E(z_t) = \\mu\\), \\[\nT^{-1}\\sum_{t=1}^{T}z_t \\^p \\mu\n\\]additional regulatory conditions hold (Greene 1990), \\[\n\\sqrt{T}(\\bar{z}-\\mu) \\^d N(0,B)\n\\]\\(B= Var(z_t) + 2\\sum_{h=1}^{\\infty}Cov(z_t,z_{t-h})\\)","code":""},{"path":"linear-regression.html","id":"a6-normal-distribution","chapter":"5 Linear Regression","heading":"5.1.3.6 A6 Normal Distribution","text":"error term normally distributedFrom A1-A3, identification (also known Orthogonality Condition) population parameter \\(\\beta\\)\\[\n\\begin{aligned}\ny &= {x}\\beta + \\epsilon && \\text{A1} \\\\\nx'y &= x'x\\beta + x'\\epsilon && \\text{} \\\\\nE(x'y) &= E(x'x)\\beta + E(x'\\epsilon)  && \\text{} \\\\\nE(x'y) &= E(x'x)\\beta && \\text{A3} \\\\\n[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\\beta && \\text{A2} \\\\\n[E(x'x)]^{-1}E(x'y) &= \\beta\n\\end{aligned}\n\\]\\(\\beta\\) row vector parameters produces best predictor y choose min \\(\\gamma\\) :\\[\n\\underset{\\gamma}{\\operatorname{argmin}}E((y-x\\gamma)^2)\n\\]First Order Condition\\[\n\\begin{aligned}\n\\frac{\\partial((y-x\\gamma)^2)}{\\partial\\gamma}&=0 \\\\\n-2E(x'(y-x\\gamma))&=0 \\\\\nE(x'y)-E(x'x\\gamma) &=0 \\\\\nE(x'y) &= E(x'x)\\gamma \\\\\n(E(x'x))^{-1}E(x'y) &= \\gamma\n\\end{aligned}\n\\]Second Order Condition\\[\n\\begin{aligned}\n\\frac{\\partial^2E((y-x\\gamma)^2)}{\\partial \\gamma'^2}&=0 \\\\\nE(\\frac{\\partial(y-x\\partial)^2)}{\\partial\\gamma\\partial\\gamma'}) &= 2E(x'x)\n\\end{aligned}\n\\]A3 holds, \\(2E(x'x)\\) PSD \\(\\rightarrow\\) minimum","code":""},{"path":"linear-regression.html","id":"theorems","chapter":"5 Linear Regression","heading":"5.1.4 Theorems","text":"","code":""},{"path":"linear-regression.html","id":"frisch-waugh-lovell-theorem","chapter":"5 Linear Regression","heading":"5.1.4.1 Frisch-Waugh-Lovell Theorem","text":"\\[\n\\mathbf{y=X\\beta + \\epsilon=X_1\\beta_1+X_2\\beta_2 +\\epsilon}\n\\]Equivalently,\\[\n\\left(\n\\begin{array}\n{cc}\nX_1'X_1 & X_1'X_2 \\\\\nX_2'X_1 & X_2'X_2\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{c}\n\\hat{\\beta_1} \\\\\n\\hat{\\beta_2}\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{c}\nX_1'y \\\\\nX_2'y\n\\end{array}\n\\right)\n\\]Hence,\\[\n\\mathbf{\\hat{\\beta_1}=(X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\\hat{\\beta_2}}\n\\]Betas multiple regression betas individual simple regressionDifferent set X affect coefficient estimates.\\(X_1'X_2 = 0\\) \\(\\hat{\\beta_2}=0\\), 1 2 hold.","code":""},{"path":"linear-regression.html","id":"gauss-markov-theorem","chapter":"5 Linear Regression","heading":"5.1.4.2 Gauss-Markov Theorem","text":"linear regression model\\[\n\\mathbf{y=X\\beta + \\epsilon}\n\\]A1, A2, A3, A4, OLS estimator defined \\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y}\n\\]minimum variance linear (\\(y\\)) unbiased estimator \\(\\beta\\)Let \\(\\tilde{\\beta}=\\mathbf{Cy}\\), another linear estimator \\(\\mathbf{C}\\) \\(k \\times n\\) function \\(\\mathbf{X}\\)), unbiased,\\[\n\\begin{aligned}\nE(\\tilde{\\beta}|\\mathbf{X}) &= E(\\mathbf{Cy|X}) \\\\\n&= E(\\mathbf{CX\\beta + C\\epsilon|X}) \\\\\n&= \\mathbf{CX\\beta}\n\\end{aligned}\n\\]equals true parameter \\(\\beta\\) \\(\\mathbf{CX=}\\)\nEquivalently, \\(\\tilde{\\beta} = \\beta + \\mathbf{C}\\epsilon\\) variance estimator \\(Var(\\tilde{\\beta}|\\mathbf{X}) = \\sigma^2\\mathbf{CC'}\\)show minimum variance,\\[\n\\begin{aligned}\n&=\\sigma^2\\mathbf{(C-(X'X)^{-1}X')(C-(X'X)^{-1}X')'} \\\\\n&= \\sigma^2\\mathbf{(CC' - CX(X'X)^{-1})-(X'X)^{-1}X'C + (X'X)^{-1}X'X(X'X)^{-1})} \\\\\n&= \\sigma^2 (\\mathbf{CC' - (X'X)^{-1}-(X'X)^{-1} + (X'X)^{-1}}) \\\\\n&= \\sigma^2\\mathbf{CC'} - \\sigma^2(\\mathbf{X'X})^{-1} \\\\\n&= Var(\\tilde{\\beta}|\\mathbf{X}) - Var(\\hat{\\beta}|\\mathbf{X})\n\\end{aligned}\n\\]Hierarchy OLS Assumptions","code":""},{"path":"linear-regression.html","id":"variable-selection","chapter":"5 Linear Regression","heading":"5.1.5 Variable Selection","text":"depends onObjectives goalsPreviously acquired expertiseAvailability dataAvailability computer softwareLet \\(P - 1\\) number possible \\(X\\) variables","code":""},{"path":"linear-regression.html","id":"mallowss-c_p-statistic","chapter":"5 Linear Regression","heading":"5.1.5.1 Mallows’s \\(C_p\\) Statistic","text":"(Mallows, 1973, Technometrics, 15, 661-675)measure predictive ability fitted modelLet \\(\\hat{Y}_{ip}\\) predicted value \\(Y_i\\) using model \\(p\\) parameters.total standardized mean square error prediction :\\[\n\\begin{aligned}\n\\Gamma_p &= \\frac{\\sum_{=1}^n E(\\hat{Y}_{ip}-E(Y_i))^2}{\\sigma^2} \\\\\n&= \\frac{\\sum_{=1}^n [E(\\hat{Y}_{ip})-E(Y_i)]^2+\\sum_{=1}^n var(\\hat{Y}_{ip})}{\\sigma^2}\n\\end{aligned}\n\\]first term numerator (bias)^2 term 2nd term prediction variance term.bias term decreases variables added model.assume full model \\((p=P)\\) true model, \\(E(\\hat{Y}_{ip}) - E(Y_i) = 0\\) bias 0.Prediction variance increase variables added model \\(\\sum var(\\hat{Y}_{ip}) = p \\sigma^2\\)thus, tradeoff bias variance terns achieved minimizing \\(\\Gamma_p\\).Since \\(\\Gamma_p\\) unknown (due \\(\\beta\\)). use estimate: \\(C_p = \\frac{SSE_p}{\\hat{\\sigma^2}}- (n-2p)\\) unbiased estimate \\(\\Gamma_p\\)variables added model, \\(SSE_p\\) decreases 2p increases. \\(\\hat{\\sigma^2}=MSE(X_1,..,X_{P-1})\\) MSE possible X variables model.bias \\(E(C_p) \\approx p\\). Thus, good models \\(C_p\\) close p.Prediction: consider models \\(C_p \\le p\\)Parameter estimation: consider models \\(C_p \\le 2p -(P-1)\\). Fewer variables eliminated model avoid excess bias estimates.","code":""},{"path":"linear-regression.html","id":"akaike-information-criterion-aic","chapter":"5 Linear Regression","heading":"5.1.5.2 Akaike Information Criterion (AIC)","text":"\\[\nAUC = n ln(\\frac{SSE_p}{n}) + 2p\n\\]increasing \\(p\\) (number parameters) leads first-term decreases, second-term increases.want model small values AIC. AIC increases parameter added model, parameter needed.AIC represents tradeoff precision fit number parameters used.","code":""},{"path":"linear-regression.html","id":"bayes-or-schwarz-information-criterion","chapter":"5 Linear Regression","heading":"5.1.5.3 Bayes (or Schwarz) Information Criterion","text":"\\[\nBIC = n \\ln(\\frac{SSE_p}{n})+ (\\ln n)p\n\\]coefficient front p tends penalize heavily models larger number parameters (compared AIC).","code":""},{"path":"linear-regression.html","id":"prediction-error-sum-of-squares-press","chapter":"5 Linear Regression","heading":"5.1.5.4 Prediction Error Sum of Squares (PRESS)","text":"\\[\nPRESS_p = \\sum_{=1}^{n} (Y_i - \\hat{Y}_{()})^2\n\\]\\(\\hat{Y}_{()}\\) prediction -th response -th observation used, obtained model p parameters.evaluates predictive ability postulated model omitting one observation time.want small \\(PRESS_p\\) valuesIt can computationally intensive large p.","code":""},{"path":"linear-regression.html","id":"best-subsets-algorithm","chapter":"5 Linear Regression","heading":"5.1.5.5 Best Subsets Algorithm","text":"“leap bounds” algorithm (Furnival Wilson 2000) combines comparison SSE different subset models control sequence subset regression computed.Guarantees finding best m subset regressions within subset size less computational burden possible subsets.","code":"\nlibrary(\"leaps\")\nregsubsets()"},{"path":"linear-regression.html","id":"stepwise-selection-procedures","chapter":"5 Linear Regression","heading":"5.1.5.6 Stepwise Selection Procedures","text":"forward stepwise procedure:finds plausible subset sequentially.step, variable added deleted.criterion adding deleting based SSE, \\(R^2\\), t, F-statistic.Note:Instead using exact F-values, computer packages usually specify equivalent “significance” level. example, SLE “significance” level enter, SLS “significance” level stay. SLE SLS guides rather true tests significance.choice SLE SLS represents balancing opposing tendencies. Use large SLE values tends result many predictor variables; models small SLE tend -specified resulting \\(\\sigma^2\\) badly overestimated.choice SLE, can choose 0.05 0.5.SLE > SLS cycling pattern may occur. Although computer packages can detect can stop happens. quick fix: SLS = SLE /2 (Bendel Afifi 1977).SLE < SLS procedure conservative may lead variables low contribution retained.Order variable entry matter.Automated Selection Procedures:Forward selection: idea forward stepwise except doesn’t test variables dropped enter. (good forward stepwise).Backward Elimination: begin variables identifies one smallest F-value dropped.","code":""},{"path":"linear-regression.html","id":"diagnostics-1","chapter":"5 Linear Regression","heading":"5.1.6 Diagnostics","text":"","code":""},{"path":"linear-regression.html","id":"normality-of-errors","chapter":"5 Linear Regression","heading":"5.1.6.1 Normality of errors","text":"use Methods based normal probability plot Methods based empirical cumulative distribution functionor plots ","code":"\ny = 1:100\nx = rnorm(100)\nqqplot(x,y)"},{"path":"linear-regression.html","id":"influential-observationsoutliers","chapter":"5 Linear Regression","heading":"5.1.6.2 Influential observations/outliers","text":"","code":""},{"path":"linear-regression.html","id":"hat-matrix","chapter":"5 Linear Regression","heading":"5.1.6.2.1 Hat matrix","text":"\\[\n\\mathbf{H = X(X'X)^{-1}}\n\\]\\(\\mathbf{\\hat{Y}= HY, e = (-H)Y}\\) \\(var(\\mathbf{e}) = \\sigma^2 (\\mathbf{-H})\\)\\(\\sigma^2(e_i) = \\sigma^2 (1-h_{ii})\\), \\(h_{ii}\\) \\(\\)-th element main diagonal \\(\\mathbf{H}\\) (must 0 1).\\(\\sum_{=1}^{n} h_{ii} = p\\)\\(cov(e_i,e_j) = -h_{ii}\\sigma^2\\) \\(\\neq j\\)Estimate: \\(s^2(e_i) = MSE (1-h_{ii})\\)Estimate: \\(\\hat{cov}(e_i,e_j) = -h_{ij}(MSE)\\); model assumption correct, covariance small large data sets.\\(\\mathbf{x}_i = [1 X_{,1} ... X_{,p-1}]'\\) (vector X-values given response), \\(h_{ii} = \\mathbf{x_i'(X'X)^{-1}x_i}\\) (depends relative positions design points \\(X_{,1},...,X_{,p-1}\\))","code":""},{"path":"linear-regression.html","id":"studentized-residuals","chapter":"5 Linear Regression","heading":"5.1.6.2.2 Studentized Residuals","text":"\\[\n\\begin{aligned}\nr_i &= \\frac{e_i}{s(e_i)} \\\\\nr_i &\\sim N(0,1)\n\\end{aligned}\n\\]\\(s(e_i) = \\sqrt{MSE(1-h_{ii})}\\). \\(r_i\\) called studentized residual standardized residual.can use semi-studentized residual , \\(e_i^*= e_i \\sqrt{MSE}\\). doesn’t take account different variances \\(e_i\\).want see model without particular value. delete \\(\\)-th case, fit regression remaining \\(n-1\\) cases, get estimated responses \\(\\)-th case, \\(\\hat{Y}_{()}\\), find difference, called deleted residual:\\[\n\\begin{aligned}\nd_i &= Y_i - \\hat{Y}_{()} \\\\\n&= \\frac{e_i}{1-h_{ii}}\n\\end{aligned}\n\\]don’t need recompute regression model caseAs \\(h_{ii}\\) increases, \\(d_i\\) increases.\\[\ns^2(d_i)= \\frac{MSE_{()}}{1-h_{ii}}\n\\]\\(MSE_{()}\\) mean square error -th case omitted.Let\\[\nt_i = \\frac{d_i}{s(d_i)} = \\frac{e_i}{\\sqrt{MSE_{()}(1-h_{ii})}}\n\\]studentized deleted residual, follows t-distribution \\(n-p-1\\) df.\\[\n(n-p)MSE = (n-p-1)MSE_{()}+ \\frac{e^2_{}}{1-h_{ii}}\n\\]hence, need fit regressions case \\[\nt_i = e_i (\\frac{n-p-1}{SSE(1-h_{ii})-e^2_i})^{1/2}\n\\]outlying \\(Y\\)-observations cases whose studentized deleted residuals large absolute value. many residuals consider, Bonferroni critical value can can (\\(t_{1-\\alpha/2n;n-p-1}\\))Outlying X ObservationsRecall, \\(0 \\le h_{ii} \\le 1\\) \\(\\sum_{=1}^{n}h_{ii}=p\\) (total number parameters)large \\(h_{ii}\\) indicates \\(\\)-th case distant center \\(X\\) observations (leverage \\(\\)-th case). , large value suggests observation exercises substantial leverage determining fitted value \\(\\hat{Y}_i\\)\\(\\mathbf{\\hat{Y}=HY}\\), linear combination Y-values; \\(h_{ii}\\) weight observation \\(Y_i\\); \\(h_{ii}\\) measures role X values determining important \\(Y_i\\) affecting \\(\\hat{Y}_i\\).Large \\(h_{ii}\\) implies \\(var(e_i)\\) small, larger \\(h_{ii}\\) implies \\(\\hat{Y}_i\\) close \\(Y_i\\)small data sets: \\(h_{ii} > .5\\) suggests “large”.large data sets: \\(h_{ii} > \\frac{2p}{n}\\) “large.Using hat matrix identify extrapolation:Let \\(\\mathbf{x_{new}}\\) vector containing X values inference mean response new observation made.Let \\(\\mathbf{X}\\) data design matrix used fit data. , \\(h_{new,new} = \\mathbf{x_{new}(X'X)^{-1}x_{new}}\\) within range leverage values (\\(h_{ii}\\)) cases data set, extrapolation involved; otherwise; extrapolation indicated.Identifying Influential Cases:influential mean exclusion observation causes major changes int fitted regression. (outliers influential)Influence Single Fitted Values: DFFITSInfluence Fitted Values: Cook’s DInfluence Regression Coefficients: DFBETAS","code":""},{"path":"linear-regression.html","id":"dffits","chapter":"5 Linear Regression","heading":"5.1.6.2.3 DFFITS","text":"Influence Single Fitted Values: DFFITS\\[\n\\begin{aligned}\n(DFFITS)_i &= \\frac{\\hat{Y}_i - \\hat{Y}_{()}}{\\sqrt{MSE_{()}h_{ii}}} \\\\\n&= t_i (\\frac{h_{ii}}{1-h_{ii}})^{1/2}\n\\end{aligned}\n\\]standardized difference -th fitted value observations -th case removed.standardized difference -th fitted value observations -th case removed.studentized deleted residual multiplied factor function fo -th leverage value.studentized deleted residual multiplied factor function fo -th leverage value.influence :\nsmall medium data sets: \\(|DFFITS|>1\\)\nlarge data sets: \\(|DFFITS|> 2 \\sqrt{p/n}\\)\ninfluence :small medium data sets: \\(|DFFITS|>1\\)large data sets: \\(|DFFITS|> 2 \\sqrt{p/n}\\)","code":""},{"path":"linear-regression.html","id":"cooks-d","chapter":"5 Linear Regression","heading":"5.1.6.2.4 Cook’s D","text":"Influence Fitted Values: Cook’s D\\[\n\\begin{aligned}\nD_i &= \\frac{\\sum_{j=1}^{n}(\\hat{Y}_j - \\hat{Y}_{j()})^2}{p(MSE)} \\\\\n&= \\frac{e^2_i}{p(MSE)}(\\frac{h_{ii}}{(1-h_{ii})^2})\n\\end{aligned}\n\\]gives influence -th case fitted values.\\(e_i\\) increases \\(h_{ii}\\) increases, \\(D_i\\) increases.\\(D_i\\) percentile \\(F_{(p,n-p)}\\) distribution. percentile greater \\(.5(50\\%)\\) \\(\\)-th case major influence. practice, \\(D_i >4/n\\), \\(\\)-th case major influence.","code":""},{"path":"linear-regression.html","id":"dfbetas","chapter":"5 Linear Regression","heading":"5.1.6.2.5 DFBETAS","text":"Influence Regression Coefficients: DFBETAS\\[\n(DFBETAS)_{k()} = \\frac{b_k - b_{k()}}{\\sqrt{MSE_{()}c_{kk}}}\n\\]\\(k = 0,...,p-1\\) \\(c_{kk}\\) k-th diagonal element \\(\\mathbf{X'X}^{-1}\\)Influence \\(\\)-th case regression coefficient \\(b_k\\) \\((k=0,\\dots,p-1)\\) difference estimated regression coefficients based \\(n\\) cases regression coefficients obtained \\(\\)-th case omitted (\\(b_{k()}\\))small data sets: \\(|DFBETA|>1\\)large data sets: \\(|DFBETA| > 2\\sqrt{n}\\)Sign DFBETA inculcates whether inclusion case leads increase decrease estimates regression coefficient.","code":""},{"path":"linear-regression.html","id":"collinearity","chapter":"5 Linear Regression","heading":"5.1.6.3 Collinearity","text":"Multicollinearity refers correlation among explanatory variables.large changes estimated regression coefficient predictor variable added deleted, observation altered deleted.non insignificant results individual tests regression coefficients important predictor variables.estimated regression coefficients algebraic sign opposite expected theoretical consideration prior experience.large coefficients simple correlation pairs predictor variables correlation matrix.wide confidence intervals regression coefficients representing important predictor variables.\\(X\\) variables highly correlated inverse \\((X'X)^{-1}\\) exist computationally unstable.Correlated Predictor Variables: X variables “perfectly” correlated, system undetermined infinite number models fit data. , \\(X'X\\) singular, \\((X'X)^{-1}\\) doesn’t exist. ,parameters interpreted (\\(\\mathbf{b = (X'X)^{-1}X'y}\\))sampling variability infinite (\\(\\mathbf{s^2(b) = MSE (X'X)^{-1}}\\))","code":""},{"path":"linear-regression.html","id":"vifs","chapter":"5 Linear Regression","heading":"5.1.6.3.1 VIFs","text":"Let \\(R^2_k\\) coefficient multiple determination \\(X_k\\) regressed \\(p - 2\\) \\(X\\) variables model. ,\\[\nVIF_k = \\frac{1}{1-R^2_k}\n\\]large values indicate near collinearity causing variance \\(b_k\\) inflated, \\(var(b_k) \\propto \\sigma^2 (VIF_k)\\)Typically, rule thumb \\(VIF > 4\\) mean see case, \\(VIF_k > 10\\) indicates serious problem collinearity problem result poor parameters estimates.mean VIF’s provide estimate ratio true multicollinearity model \\(X\\) variables uncorrelatedserious multicollinearity \\(avg(VIF) >>1\\)","code":""},{"path":"linear-regression.html","id":"condition-number","chapter":"5 Linear Regression","heading":"5.1.6.3.2 Condition Number","text":"Condition Numberspectral decomposition\\[\n\\mathbf{X'X}= \\sum_{=1}^{p} \\lambda_i \\mathbf{u_i u_i'}\n\\]\\(\\lambda_i\\) eigenvalue \\(\\mathbf{u}_i\\) eigenvector. \\(\\lambda_1 > ...>\\lambda_p\\) eigenvecotrs orthogonal:\\[\n\\begin{cases}\n\\mathbf{u_i'u_j} =\n0&\\text{$\\neq j$}\\\\\n1&\\text{$=j$}\\\\\n\\end{cases}\n\\]condition number \\[\nk = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_{min}}}\n\\]values \\(k>30\\) cause concernvalues \\(30<k<100\\) imply moderate dependencies.values \\(k>100\\) imply strong collinearityCondition index\\[\n\\delta_i = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_i}}\n\\]\\(= 1,...,p\\)can find proportion total variance associated k-th regression coefficient -th eigen mode:\\[\n\\frac{u_{ik}^2/\\lambda_i}{\\sum_j (u^2_{jk}/\\lambda_j)}\n\\]variance proportions can helpful identifying serious collinearitythe condition index must largethe variance proportions must large (>,5) least two regression coefficients.","code":""},{"path":"linear-regression.html","id":"constancy-of-error-variance","chapter":"5 Linear Regression","heading":"5.1.6.4 Constancy of Error Variance","text":"","code":""},{"path":"linear-regression.html","id":"brown-forsythe-test-modified-levene-test","chapter":"5 Linear Regression","heading":"5.1.6.4.1 Brown-Forsythe Test (Modified Levene Test)","text":"depend normalityApplicable error variance increases decreases \\(X\\)relatively large sample size needed (can ignore dependency residuals)Split residuals 2 groups (\\(e_{i1}, = 1, ..., n_1; e_{i2}, j=1,...,n_2\\))Let \\(d_{i1}= |e_{i1}-\\tilde{e}_{1}|\\) \\(\\tilde{e}_{1}\\) median group 1.Let \\(d_{j2}=|e_{j2}-\\tilde{e}_{2}|\\)., 2-sample t-test:\\[\nt_L = \\frac{\\bar{d}_1 - \\bar{d}_2}{s\\sqrt{1/n_1+1/n_2}}\n\\] \\[\ns^2 = \\frac{\\sum_i(d_{i1}-\\bar{d}_1)^2+\\sum_j(d_{j2}-\\bar{d}_2)^2}{n-2}\n\\] \\(|t_L|>t_{1-\\alpha/2;n-2}\\) conclude error variance constant.","code":""},{"path":"linear-regression.html","id":"breusch-pagan-test-cook-weisberg-test","chapter":"5 Linear Regression","heading":"5.1.6.4.2 Breusch-Pagan Test (Cook-Weisberg Test)","text":"Assume error terms independent normally distributed, \\[\n\\sigma^2_i = \\gamma_0 + \\gamma_1 X_i\n\\]Constant error variance corresponds \\(\\gamma_1 = 0\\), .e., test\\(H_0: \\gamma_1 =0\\)\\(H_1: \\gamma_1 \\neq 0\\)regressing squared residuals X usual manner. Obtain regression sum squares : \\(SSR^*\\) (SSR regression \\(e^2_i\\) \\(X_i\\)). , define\\[\nX^2_{BP} = \\frac{SSR^*/2}{(SSE/n)^2}\n\\]SSE error sum squares regression Y X.\\(H_0: \\gamma_1 = 0\\) holds n reasonably large, \\(X^2_{BP}\\) follows approximately \\(\\chi^2\\) distribution 1 d.f. reject \\(H_0\\) (Homogeneous variance) \\(X^2_{BP} > \\chi^2_{1-\\alpha;1}\\)","code":""},{"path":"linear-regression.html","id":"independence","chapter":"5 Linear Regression","heading":"5.1.6.5 Independence","text":"","code":""},{"path":"linear-regression.html","id":"plots","chapter":"5 Linear Regression","heading":"5.1.6.5.1 Plots","text":"","code":""},{"path":"linear-regression.html","id":"durbin-watson","chapter":"5 Linear Regression","heading":"5.1.6.5.2 Durbin-Watson","text":"","code":""},{"path":"linear-regression.html","id":"time-series","chapter":"5 Linear Regression","heading":"5.1.6.5.3 Time-series","text":"","code":""},{"path":"linear-regression.html","id":"spatial-statistics","chapter":"5 Linear Regression","heading":"5.1.6.5.4 Spatial Statistics","text":"","code":""},{"path":"linear-regression.html","id":"model-validation","chapter":"5 Linear Regression","heading":"5.1.7 Model Validation","text":"split data 2 groups: training (model building) sample validation (prediction) sample.split data 2 groups: training (model building) sample validation (prediction) sample.model MSE tend underestimate inherent variability making future predictions. consider actual predictive ability, consider mean squared prediction error (MSPE):\\[\nMSPE = \\frac{\\sum_{=1}^{n} (Y_i- \\hat{Y}_i)^2}{n^*}\n\\]\n\\(Y_i\\) known value response variable \\(\\)-th validation case.\n\\(\\hat{Y}_i\\) predicted value based model fit training data set.\n\\(n^*\\) number cases validation set.\nmodel MSE tend underestimate inherent variability making future predictions. consider actual predictive ability, consider mean squared prediction error (MSPE):\\[\nMSPE = \\frac{\\sum_{=1}^{n} (Y_i- \\hat{Y}_i)^2}{n^*}\n\\]\\(Y_i\\) known value response variable \\(\\)-th validation case.\\(\\hat{Y}_i\\) predicted value based model fit training data set.\\(n^*\\) number cases validation set.want MSPE close MSE (MSE biased); look ratio MSPE / MSE (closer 1, better).want MSPE close MSE (MSE biased); look ratio MSPE / MSE (closer 1, better).","code":""},{"path":"linear-regression.html","id":"finite-sample-properties","chapter":"5 Linear Regression","heading":"5.1.8 Finite Sample Properties","text":"\\(n\\) fixed\\(n\\) fixedBias average, close estimate true value\n\\(Bias = E(\\hat{\\beta}) -\\beta\\) \\(\\beta\\) true parameter value \\(\\hat{\\beta}\\) estimator \\(\\beta\\)\nestimator unbiased \n\\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)\nmeans estimator produce estimates , average, equal value trying estimate\n\nBias average, close estimate true value\\(Bias = E(\\hat{\\beta}) -\\beta\\) \\(\\beta\\) true parameter value \\(\\hat{\\beta}\\) estimator \\(\\beta\\)\\(Bias = E(\\hat{\\beta}) -\\beta\\) \\(\\beta\\) true parameter value \\(\\hat{\\beta}\\) estimator \\(\\beta\\)estimator unbiased \n\\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)\nmeans estimator produce estimates , average, equal value trying estimate\nestimator unbiased \\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)means estimator produce estimates , average, equal value trying estimateDistribution estimator: estimator function random variables (data)Distribution estimator: estimator function random variables (data)Standard Deviation: spread estimator.Standard Deviation: spread estimator.OLSUnder A1 A2 A3, OLS unbiased\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= E(\\mathbf{(X'X)^{-1}X'y}) && \\text{A2}\\\\\n     &= E(\\mathbf{(X'X)^{-1}X'(X\\beta + \\epsilon)}) && \\text{A1}\\\\\n     &= E(\\mathbf{(X'X)^{-1}X'X\\beta + (X'X)^{-1}X'\\epsilon})  && \\text{} \\\\\n     &= E(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n     &= \\beta + E(\\mathbf{(X'X^{-1}\\epsilon)}) \\\\\n     &= \\beta + E(E((\\mathbf{X'X)^{-1}X'\\epsilon|X})) &&\\text{LIE} \\\\\n     &= \\beta + E((\\mathbf{X'X)^{-1}X'}E\\mathbf{(\\epsilon|X})) \\\\\n     &= \\beta + E((\\mathbf{X'X)^{-1}X'}0)) && \\text{A3} \\\\\n     &= \\beta\n\\end{aligned}\n\\]LIE stands Law Iterated ExpectationIf A3 hold, OLS biasedFrom Frisch-Waugh-Lovell Theorem, omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) \\(\\mathbf{X_1'X_2} \\neq 0\\), omitted variable cause OLS estimator biased.A1 A2 A3 A4, conditional variance OLS estimator follows]\\[\n\\begin{aligned}\nVar(\\hat{\\beta}|\\mathbf{X}) &= Var(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon|X}) && \\text{A1-A2}\\\\\n    &= Var((\\mathbf{X'X)^{-1}X'\\epsilon|X)} \\\\\n    &= \\mathbf{X'X^{-1}X'} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X'X)^{-1}} \\\\\n    &= \\mathbf{X'X^{-1}X'} \\sigma^2I \\mathbf{X(X'X)^{-1}} && \\text{A4} \\\\\n    &= \\sigma^2\\mathbf{X'X^{-1}X'} \\mathbf{X(X'X)^{-1}} \\\\\n    &= \\sigma^2\\mathbf{(X'X)^{-1}}\n\\end{aligned}\n\\]Sources variation\\(\\sigma^2=Var(\\epsilon_i|\\mathbf{X})\\)\namount unexplained variation \\(\\epsilon_i\\) large relative explained \\(\\mathbf{x_i \\beta}\\) variation\n\\(\\sigma^2=Var(\\epsilon_i|\\mathbf{X})\\)amount unexplained variation \\(\\epsilon_i\\) large relative explained \\(\\mathbf{x_i \\beta}\\) variation“Small” \\(Var(x_{i1}), Var(x_{i1}),..\\)\nlot variation \\(\\mathbf{X}\\) (information)\nsmall sample size\n“Small” \\(Var(x_{i1}), Var(x_{i1}),..\\)lot variation \\(\\mathbf{X}\\) (information)small sample size“Strong” correlation explanatory variables\n\\(x_{i1}\\) highly correlated linear combination 1, \\(x_{i2}\\), \\(x_{i3}\\), …\ninclude many irrelevant variables contribute .\n\\(x_1\\) perfectly determined regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 violated.\n\\(x_1\\) highly correlated linear combination variables, Multicollinearity\n“Strong” correlation explanatory variables\\(x_{i1}\\) highly correlated linear combination 1, \\(x_{i2}\\), \\(x_{i3}\\), …include many irrelevant variables contribute .\\(x_1\\) perfectly determined regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 violated.\\(x_1\\) highly correlated linear combination variables, Multicollinearity","code":""},{"path":"linear-regression.html","id":"check-for-multicollinearity","chapter":"5 Linear Regression","heading":"5.1.8.1 Check for Multicollinearity","text":"Variance Inflation Factor (VIF) Rule thumb \\(VIF \\ge 10\\) large\\[\nVIF = \\frac{1}{1-R_1^2}\n\\]Multicollinearity VIF:\nHigh VIFs indicator variables normal problematic.\nVIF generally useful detecting multicollinearity concerns models fixed effects.\nMulticollinearity VIF:High VIFs indicator variables normal problematic.High VIFs indicator variables normal problematic.VIF generally useful detecting multicollinearity concerns models fixed effects.VIF generally useful detecting multicollinearity concerns models fixed effects.Overemphasis Multicollinearity:\nMulticollinearity inflates standard errors widens confidence intervals bias results.\nkey variables narrow confidence intervals, multicollinearity issue.\nOveremphasis Multicollinearity:Multicollinearity inflates standard errors widens confidence intervals bias results.Multicollinearity inflates standard errors widens confidence intervals bias results.key variables narrow confidence intervals, multicollinearity issue.key variables narrow confidence intervals, multicollinearity issue.Goldberger’s Insight (Goldberger 1991):\nMulticollinearity akin small sample size (“micronumerosity”).\nLarge standard errors expected highly correlated independent variables.\nGoldberger’s Insight (Goldberger 1991):Multicollinearity akin small sample size (“micronumerosity”).Multicollinearity akin small sample size (“micronumerosity”).Large standard errors expected highly correlated independent variables.Large standard errors expected highly correlated independent variables.Practical Implications:\nEvaluate whether confidence intervals key variables sufficiently narrow.\n, study inconclusive, larger dataset redesigned study needed.\nPractical Implications:Evaluate whether confidence intervals key variables sufficiently narrow.Evaluate whether confidence intervals key variables sufficiently narrow., study inconclusive, larger dataset redesigned study needed., study inconclusive, larger dataset redesigned study needed.","code":""},{"path":"linear-regression.html","id":"standard-errors","chapter":"5 Linear Regression","heading":"5.1.8.2 Standard Errors","text":"\\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X'X)^{-1}}\\) variance estimate \\(\\hat{\\beta}\\)Standard Errors estimators/estimates standard deviation (square root variance) estimator \\(\\hat{\\beta}\\)A1-A5, can estimate \\(\\sigma^2=Var(\\epsilon^2|\\mathbf{X})\\) standard errors \\[\n\\begin{aligned}\ns^2 &= \\frac{1}{n-k}\\sum_{=1}^{n}e_i^2 \\\\\n&= \\frac{1}{n-k}SSR\n\\end{aligned}\n\\]degrees freedom adjustment: \\(e_i \\neq \\epsilon_i\\) estimated using k estimates \\(\\beta\\), lose degrees freedom variance estimate.\\(s=\\sqrt{s^2}\\) biased estimator standard deviation (Jensen’s Inequality)Standard Errors \\(\\hat{\\beta}\\)\\[\n\\begin{aligned}\nSE(\\hat{\\beta}_{j-1})&=s\\sqrt{[(\\mathbf{X'X})^{-1}]_{jj}} \\\\\n&= \\frac{s}{\\sqrt{SST_{j-1}(1-R_{j-1}^2)}}\n\\end{aligned}\n\\]\\(SST_{j-1}\\) \\(R_{j-1}^2\\) following regression\\(x_{j-1}\\) 1, \\(x_1\\),… \\(x_{j-2}\\),\\(x_j\\),\\(x_{j+1}\\), …, \\(x_{k-1}\\)Summary Finite Sample PropertiesUnder A1-A3: OLS unbiasedUnder A1-A4: variance OLS estimator \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X'X)^{-1}}\\)A1-A4, A6: OLS estimator \\(\\hat{\\beta} \\sim N(\\beta,\\sigma^2\\mathbf{(X'X)^{-1}})\\)A1-A4, Gauss-Markov Theorem holds \\(\\rightarrow\\) OLS BLUEUnder A1-A5, standard errors unbiased estimator standard deviation \\(\\hat{\\beta}\\)","code":""},{"path":"linear-regression.html","id":"large-sample-properties","chapter":"5 Linear Regression","heading":"5.1.9 Large Sample Properties","text":"Let \\(n \\rightarrow \\infty\\)perspective allows us evaluate “quality” estimators finite sample properties informative, impossible computeConsistency, asymptotic distribution, asymptotic varianceMotivationFinite Sample Properties need strong assumption A1 A3 A4 A6Other estimation GLS, MLE need analyzed using Large Sample PropertiesLet \\(\\mu(\\mathbf{X})=E(y|\\mathbf{X})\\) Conditional Expectation Function\\(\\mu(\\mathbf{X})\\) minimum mean squared predictor (possible functions)\\[\nminE((y-f(\\mathbf{X}))^2)\n\\]A1 A3,\\[\n\\mu(\\mathbf{X})=\\mathbf{X}\\beta\n\\]linear projection\\[\nL(y|1,\\mathbf{X})=\\gamma_0 + \\mathbf{X}Var(X)^{-1}Cov(X,Y)\n\\]\\(\\mathbf{X}Var(X)^{-1}Cov(X,Y)=\\gamma\\)minimum mean squared linear approximation conditional mean function\\[\n(\\gamma_0,\\gamma) = arg min E((E(y|\\mathbf{X})-(+\\mathbf{Xb})^2)\n\\]OLS always consistent linear projection, necessarily unbiased.Linear projection causal interpretationLinear projection depend assumption A1 A3Evaluating estimator using large sample properties:Consistency: measure centralityLimiting Distribution: shape scaled estimator sample size increasesAsymptotic variance: spread estimator regards limiting distribution.estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) \\(\\hat{\\theta}_n \\^p \\theta\\)n increases, estimator converges population parameter value.Unbiased imply consistency consistency imply unbiased.Based [Weak Law] Large Numbers\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'X)^{-1}X'y} \\\\\n&= \\mathbf{(\\sum_{=1}^{n}x_i'x_i)^{-1} \\sum_{=1}^{n}x_i'y_i} \\\\\n&= (n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nplim(\\hat{\\beta}) &= plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\\\\n&= plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\\\\n&= (plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\text{ due A2, A5} \\\\\n&= E(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'y_i})\n\\end{aligned}\n\\]\\[\nE(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'y_i}) = \\beta + E(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'\\epsilon_i})\n\\]A1, A2, A3a, A5 OLS consistent, guarantee unbiased.A1, A2, A3a, A5, \\(\\mathbf{x_i'x_i}\\) finite first second moments (CLT), \\(Var(\\mathbf{x_i'}\\epsilon_i)=\\mathbf{B}\\)\\((n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i})^{-1} \\^p (E(\\mathbf{x'_ix_i}))^{-1}\\)\\(\\sqrt{n}(n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'}\\epsilon_i) \\^d N(0,\\mathbf{B})\\)\\[\n\\sqrt{n}(\\hat{\\beta}-\\beta) = (n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i})^{-1}\\sqrt{n}(n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i}) \\^{d} N(0,\\Sigma)\n\\]\\(\\Sigma=(E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1}\\)holds A3aholds A3aDo need A4 A6 apply CLT\nA4 hold, \\(\\mathbf{B}=Var(\\mathbf{x_i'}\\epsilon_i)=\\sigma^2E(x_i'x_i)\\) means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}\\), use standard errors\nneed A4 A6 apply CLTIf A4 hold, \\(\\mathbf{B}=Var(\\mathbf{x_i'}\\epsilon_i)=\\sigma^2E(x_i'x_i)\\) means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}\\), use standard errorsHeteroskedasticity can fromLimited dependent variableDependent variables large/skewed rangesSolving Asymptotic Variance\\[\n\\begin{aligned}\n\\Sigma &= (E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i-0)(\\mathbf{x_i'}\\epsilon_i-0)](E(\\mathbf{x_i'x_i}))^{-1} & \\text{A3a} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}E[E(\\mathbf{\\epsilon_i^2|x_i)x_i'x_i]}(E(\\mathbf{x_i'x_i}))^{-1} & \\text{LIE} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1} & \\text{A4} \\\\\n&= \\sigma^2(E(\\mathbf{x_i'x_i}))\n\\end{aligned}\n\\]A1, A2, A3a, A4, A5:\\[\n\\sqrt{n}(\\hat{\\beta}-\\beta) \\^d N(0,\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1})\n\\]Asymptotic variance approximation variance scaled random variable \\(\\sqrt{n}(\\hat{\\beta}-\\beta)\\) n large.use \\(Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n\\) approximation finite sample variance large n:\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta)) &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta})\n\\end{aligned}\n\\]\\(Avar(.)\\) behave way \\(Var(.)\\)\\[\n\\begin{aligned}\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &\\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\\n&\\neq Avar(\\hat{\\beta})\n\\end{aligned}\n\\]Finite Sample Properties, calculate standard errors estimate conditional standard deviation:\\[\nSE_{fs}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Var}}(\\hat{\\beta}_{j-1}|\\mathbf{X}) = \\sqrt{s^2[\\mathbf{(X'X)}^{-1}]_{jj}}\n\\]Large Sample Properties, calculate standard errors estimate square root asymptotic variance\\[\nSE_{ls}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Avar}(\\sqrt{n}\\hat{\\beta}_{j-1})/n} = \\sqrt{s^2[\\mathbf{(X'X)}^{-1}]_{jj}}\n\\]Hence, standard error estimator finite sample large sample.estimator, conceptually estimating two different things.Valid weaker assumptions: assumptions needed produce consistent estimator finite sample conditional variance (A1-A5) stronger needed produce consistent estimator asymptotic variance (A1,A2,A3a,A4,A5)Suppose \\(y_1,...,y_n\\) random sample population mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\)\\(\\bar{y} = \\frac{1}{n}\\sum_{=1}^{n} y_i\\) consistent estimator \\(\\mu\\)\\(S = \\frac{1}{n-1}\\sum_{=1}^{n} (y_i -\\bar{y})(y_i-\\bar{y})'\\) consistent estimator \\(\\Sigma\\).Multivariate Central limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{y}-\\mu) \\sim N_p(0,\\Sigma)\\), \\(n\\) large relative p (e.g., \\(n \\ge 25p\\)). Equivalently, \\(\\bar{y} \\sim N_p(\\mu,\\Sigma/n)\\).Wald’s Theorem: \\(n(\\bar{y} - \\mu)'S^{-1}(\\bar{y}-\\mu) \\sim \\chi^2_{(p)}\\) \\(n\\) large relative \\(p\\).","code":""},{"path":"linear-regression.html","id":"feasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2 Feasible Generalized Least Squares","text":"Motivation efficient estimatorGauss-Markov Theorem holds A1-A4Gauss-Markov Theorem holds A1-A4A4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\)\nHeteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\)\nSerial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\)\nA4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\)Heteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\)Serial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\)Without A4, can know unbiased estimator efficient?Without A4, can know unbiased estimator efficient?Original (unweighted) model:\\[\n\\mathbf{y=X\\beta+ \\epsilon}\n\\]Suppose A1-A3 hold, A4 hold,\\[\n\\mathbf{Var(\\epsilon|X)=\\Omega \\neq \\sigma^2 I_n}\n\\]try use OLS estimate transformed (weighted) model\\[\n\\mathbf{wy=wX\\beta + w\\epsilon}\n\\]need choose \\(\\mathbf{w}\\) \\[\n\\mathbf{w'w = \\Omega^{-1}}\n\\]\\(\\mathbf{w}\\) (full-rank matrix) Cholesky decomposition \\(\\mathbf{\\Omega^{-1}}\\) (full-rank matrix)words, \\(\\mathbf{w}\\) squared root \\(\\Omega\\) (squared root version matrix)\\[\n\\begin{aligned}\n\\Omega &= var(\\epsilon | X) \\\\\n\\Omega^{-1} &= var(\\epsilon | X)^{-1}\n\\end{aligned}\n\\], transformed equation (IGLS) following properties.\\[\n\\begin{aligned}\n\\mathbf{\\hat{\\beta}_{IGLS}} &= \\mathbf{(X'w'wX)^{-1}X'w'wy} \\\\\n& = \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y} \\\\\n& = \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon}\n\\end{aligned}\n\\]Since A1-A3 hold unweighted model\\[\n\\begin{aligned}\n\\mathbf{E(\\hat{\\beta}_{IGLS}|X)} & = E(\\mathbf{\\beta + (X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)}|X)\\\\\n& = \\mathbf{\\beta + E(X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)|X)} \\\\\n& = \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}E(\\epsilon|X)}  && \\text{since A3}: E(\\epsilon|X)=0 \\\\\n& = \\mathbf{\\beta}\n\\end{aligned}\n\\]\\(\\rightarrow\\) IGLS estimator unbiased\\[\n\\begin{aligned}\n\\mathbf{Var(w\\epsilon|X)} &= \\mathbf{wVar(\\epsilon|X)w'} \\\\\n& = \\mathbf{w\\Omega w'} \\\\\n& = \\mathbf{w(w'w)^{-1}w'} && \\text{since w full-rank matrix}\\\\\n& = \\mathbf{ww^{-1}(w')^{-1}w'} \\\\\n& = \\mathbf{I_n}\n\\end{aligned}\n\\]\\(\\rightarrow\\) A4 holds transformed (weighted) equationThen, variance estimator \\[\n\\begin{aligned}\nVar(\\hat{\\beta}_{IGLS}|\\mathbf{X}) & = \\mathbf{Var(\\beta + (X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|X)} \\\\\n&= \\mathbf{Var((X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|X)} \\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1} Var(\\epsilon|X)   \\Omega^{-1}X(X'\\Omega ^{-1}X)^{-1}} && \\text{A4 holds}\\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X'\\Omega ^{-1}X)^{-1}} \\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}}\n\\end{aligned}\n\\]Let \\(= \\mathbf{(X'X)^{-1}X'-(X'\\Omega ^{-1} X)X' \\Omega^{-1}}\\) \\[\nVar(\\hat{\\beta}_{OLS}|X)- Var(\\hat{\\beta}_{IGLS}|X) = \\Omega '\n\\] \\(\\Omega\\) Positive Semi Definite, \\(\\Omega '\\) also PSD, IGLS efficientThe name Infeasible comes fact impossible compute estimator.\\[\n\\mathbf{w} =\n\\left(\n\\begin{array}{ccccc}\nw_{11} & 0 & 0 & ... & 0 \\\\\nw_{21} & w_{22} & 0 & ... & 0 \\\\\nw_{31} & w_{32} & w_{33} & ... & ... \\\\\nw_{n1} & w_{n2} & w_{n3} & ... & w_{nn} \\\\\n\\end{array}\n\\right)\n\\]\\(n(n+1)/2\\) number elements n observations \\(\\rightarrow\\) infeasible estimate. (number equation > data)Hence, need make assumption \\(\\Omega\\) make feasible estimate \\(\\mathbf{w}\\):Heteroskedasticity : multiplicative exponential modelAR(1)Cluster","code":""},{"path":"linear-regression.html","id":"heteroskedasticity","chapter":"5 Linear Regression","heading":"5.2.1 Heteroskedasticity","text":"model,\\[\n\\begin{aligned}\ny_i &= x_i\\beta + \\epsilon_i \\\\\n(1/\\sigma_i)y_i &= (1/\\sigma_i)x_i\\beta + (1/\\sigma_i)\\epsilon_i\n\\end{aligned}\n\\], (5.7)\\[\n\\begin{aligned}\nVar((1/\\sigma_i)\\epsilon_i|X) &= (1/\\sigma_i^2) Var(\\epsilon_i|X) \\\\\n&= (1/\\sigma_i^2)\\sigma_i^2 \\\\\n&= 1\n\\end{aligned}\n\\]weight matrix \\(\\mathbf{w}\\) matrix equation\\[\n\\mathbf{wy=wX\\beta + w\\epsilon}\n\\]\\[\n\\mathbf{w}=\n\\left(\n\\begin{array}{ccccc}\n1/\\sigma_1 & 0 & 0 & ... & 0 \\\\\n0 & 1/\\sigma_2 & 0 & ... & 0 \\\\\n0 & 0 & 1/\\sigma_3 & ... & . \\\\\n. & . & . & . & 0 \\\\\n0 & 0 & . & . & 1/\\sigma_n\n\\end{array}\n\\right)\n\\]Infeasible Weighted Least SquaresAssume know \\(\\sigma_i^2\\) (Infeasible)IWLS estimator obtained least squared estimated following weighted equation\\[\n(1/\\sigma_i)y_i = (1/\\sigma_i)\\mathbf{x}_i\\beta + (1/\\sigma_i)\\epsilon_i\n\\]Usual standard errors weighted equation valid \\(Var(\\epsilon | \\mathbf{X}) = \\sigma_i^2\\)\\(Var(\\epsilon | \\mathbf{X}) \\neq \\sigma_i^2\\) heteroskedastic robust standard errors valid.Problem: know \\(\\sigma_i^2=Var(\\epsilon_i|\\mathbf{x_i})=E(\\epsilon_i^2|\\mathbf{x}_i)\\)One observation \\(\\epsilon_i\\) estimate sample variance estimate \\(\\sigma_i^2\\)\nModel \\(\\epsilon_i^2\\) reasonable (strictly positive) function \\(x_i\\) independent error \\(v_i\\) (strictly positive)\nOne observation \\(\\epsilon_i\\) estimate sample variance estimate \\(\\sigma_i^2\\)Model \\(\\epsilon_i^2\\) reasonable (strictly positive) function \\(x_i\\) independent error \\(v_i\\) (strictly positive)\\[\n\\epsilon_i^2=v_i exp(\\mathbf{x_i\\gamma})\n\\]can apply log transformation recover linear parameters model,\\[\nln(\\epsilon_i^2) = \\mathbf{x_i\\gamma} + ln(v_i)\n\\]\\(ln(v_i)\\) independent \\(\\mathbf{x}_i\\)observe \\(\\epsilon_i\\) * OLS residual (\\(e_i\\)) approximate","code":""},{"path":"linear-regression.html","id":"serial-correlation","chapter":"5 Linear Regression","heading":"5.2.2 Serial Correlation","text":"\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\n\\]covariance stationary,\\[\nCov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{+h}|\\mathbf{x_i,x_{+h}})=\\gamma_h\n\\]variance covariance matrix \\[\nVar(\\epsilon|\\mathbf{X}) = \\Omega =\n\\left(\n\\begin{array}{ccccc}\n\\sigma^2 & \\gamma_1 & \\gamma_2 & ... & \\gamma_{n-1} \\\\\n\\gamma_1 & \\sigma^2 & \\gamma_1 & ... & \\gamma_{n-2} \\\\\n\\gamma_2 & \\gamma_1 & \\sigma^2 & ... & ... \\\\\n. & . & . & . & \\gamma_1 \\\\\n\\gamma_{n-1} & \\gamma_{n-2} & . & \\gamma_1 & \\sigma^2\n\\end{array}\n\\right)\n\\]n parameters estimate - need sort fo structure reduce number parameters estimate.Time Series\nEffect inflation deficit Treasury Bill interest rates\nTime SeriesEffect inflation deficit Treasury Bill interest ratesCross-sectional\nClustering\nCross-sectionalClustering","code":""},{"path":"linear-regression.html","id":"ar1","chapter":"5 Linear Regression","heading":"5.2.2.1 AR(1)","text":"\\[\n\\begin{aligned}\ny_t &= \\beta_0 + x_t\\beta_1 + \\epsilon_t \\\\\n\\epsilon_t &= \\rho \\epsilon_{t-1} + u_t\n\\end{aligned}\n\\]variance covariance matrix \\[\nVar(\\epsilon | \\mathbf{X})= \\frac{\\sigma^2_u}{1-\\rho}\n\\left(\n\\begin{array}{ccccc}\n1 & \\rho & \\rho^2 & ... & \\rho^{n-1} \\\\\n\\rho & 1 & \\rho & ... & \\rho^{n-2} \\\\\n\\rho^2 & \\rho & 1 & . & . \\\\\n. & . & . & . & \\rho \\\\\n\\rho^{n-1} & \\rho^{n-2} & . & \\rho & 1 \\\\\n\\end{array}\n\\right)\n\\]Hence, 1 parameter estimate: \\(\\rho\\)A1, A2, A3a, A5a, OLS consistent asymptotically normalUse Newey West Standard Errors valid inference.Apply Infeasible Cochrane Orcutt (knew \\(\\rho\\))\\[\nu_t = \\epsilon_t - \\rho \\epsilon_{t-1}\n\\]satisfies A3, A4, A5 ’d like transform equation one \\(u_t\\) error.\\[\n\\begin{aligned}\ny_t - \\rho y_{t-1} &= (\\beta_0 + x\\beta_1 + \\epsilon_t) - \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}) \\\\\n& = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t\n\\end{aligned}\n\\]","code":""},{"path":"linear-regression.html","id":"infeasible-cochrane-orcutt","chapter":"5 Linear Regression","heading":"5.2.2.1.1 Infeasible Cochrane Orcutt","text":"Assume know \\(\\rho\\) (Infeasible)ICO estimator obtained least squared estimated following weighted first difference equation\\[\ny_t -\\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t\n\\]Usual standard errors weighted first difference equation valid errors truly follow AR(1) processIf serial correlation generated complex dynamic process Newey-West HAC standard errors validProblem know \\(\\rho\\)\\(\\rho\\) correlation \\(\\epsilon_t\\) \\(\\epsilon_{t-1}\\): estimate using OLS residuals (\\(e_i\\)) proxy\\[\n\\hat{\\rho} = \\frac{\\sum_{t=1}^{T}e_te_{t-1}}{\\sum_{t=1}^{T}e_t^2}\n\\]can obtained OLS regression \\[\ne_t = \\rho e_{t-1} + u_t\n\\]suppress intercept.losing observationWe losing observationBy taking first difference dropping first observationBy taking first difference dropping first observation\\[\ny_1 = \\beta_0 + x_1 \\beta_1 + \\epsilon_1\n\\]Feasiable Prais Winsten Transformation applies Infeasible Cochrane Orcutt includes weighted version first observation\\[\n(\\sqrt{1-\\rho^2})y_1 = \\beta_0 + (\\sqrt{1-\\rho^2})x_1 \\beta_1 + (\\sqrt{1-\\rho^2}) \\epsilon_1\n\\]","code":""},{"path":"linear-regression.html","id":"cluster","chapter":"5 Linear Regression","heading":"5.2.2.2 Cluster","text":"\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}\n\\]\\[\nCov(\\epsilon_{gi}, \\epsilon_{hj})\n\\begin{cases}\n= 0 & \\text{$g \\neq h$ pair (,j)} \\\\\n\\neq 0 & \\text{(,j) pair}\\\\\n\\end{cases}\n\\]Intra-group Correlation\nindividual single group may correlated independent across groups.A4 violated. usual standard errors OLS valid.Use cluster robust standard errors OLS.Suppose 3 groups different n\\[\nVar(\\epsilon| \\mathbf{X})= \\Omega =\n\\left(\n\\begin{array}{cccccc}\n\\sigma^2 & \\delta_{12}^1 & \\delta_{13}^1 & 0 & 0 & 0 \\\\\n\\delta_{12}^1 & \\sigma^2 & \\delta_{23}^1 & 0 & 0 & 0 \\\\\n\\delta_{13}^1 & \\delta_{23}^1 & \\sigma^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2 & \\delta_{12}^2 & 0 \\\\\n0 & 0 & 0 & \\delta_{12}^2 & \\sigma^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{array}\n\\right)\n\\]\\(Cov(\\epsilon_{gi}, \\epsilon_{gj}) = \\delta_{ij}^g\\) \\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) jInfeasible Generalized Least Squares (Cluster)Assume \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, plug \\(\\Omega\\) solve inverse \\(\\Omega^{-1}\\) (infeasible)Infeasible Generalized Least Squares Estimator \\[\n\\hat{\\beta}_{IGLS} = \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y}\n\\]Problem * know \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) + Can make assumptions data generating process causing clustering behavior. - give structure \\(Cov(\\epsilon_{gi},\\epsilon_{gj})= \\delta_{ij}^g\\) makes feasible estimate - assumptions wrong use cluster robust standard errors.Solution Assume group level random effects specification error\\[\n\\begin{aligned}\ny_{gi} &= \\mathbf{g}_i \\beta + c_g + u_{gi} \\\\\nVar(c_g|\\mathbf{x}_i) &= \\sigma^2_c \\\\\nVar(u_{gi}|\\mathbf{x}_i) &= \\sigma^2_u\n\\end{aligned}\n\\]\\(c_g\\) \\(u_{gi}\\) independent , mean independent \\(\\mathbf{x}_i\\)\\(c_g\\) captures common group shocks (independent across groups)\\(u_{gi}\\) captures individual shocks (independent across individuals groups)error variance \\[\nVar(\\epsilon| \\mathbf{X})= \\Omega =\n\\left(\n\\begin{array}{cccccc}\n\\sigma^2_c + \\sigma^2_u & \\sigma^2_c & \\sigma^2_c & 0 & 0 & 0 \\\\\n\\sigma^2_c & \\sigma^2 + \\sigma^2_u & \\sigma^2_c & 0 & 0 & 0 \\\\\n\\sigma^2_c & \\sigma^2_c  & \\sigma^2+ \\sigma^2_u & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2+ \\sigma^2_u & \\sigma^2_c & 0 \\\\\n0 & 0 & 0 & \\sigma^2_c & \\sigma^2+ \\sigma^2_u & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2+ \\sigma^2_u\n\\end{array}\n\\right)\n\\]Use Feasible group level Random Effects","code":""},{"path":"linear-regression.html","id":"weighted-least-squares","chapter":"5 Linear Regression","heading":"5.3 Weighted Least Squares","text":"Estimate following equation using OLS\\[\ny_i = \\mathbf{x}_i \\beta + \\epsilon_i\n\\]obtain residuals \\(e_i=y_i -\\mathbf{x}_i \\hat{\\beta}\\)Transform residual estimate following OLS,\\[\nln(e_i^2)= \\mathbf{x}_i\\gamma + ln(v_i)\n\\]obtain predicted values \\(g_i=\\mathbf{x}_i \\hat{\\gamma}\\)weights untransformed predicted outcome,\\[\n\\hat{\\sigma}_i =\\sqrt{exp(g_i)}\n\\]FWLS (Feasible WLS) estimator obtained least squared estimated following weighted equation\\[\n(1/\\hat{\\sigma}_i)y_i = (1/\\hat{\\sigma}_i) \\mathbf{x}_i\\beta + (1/\\hat{\\sigma}_i)\\epsilon_i\n\\]Properties FWLSThe infeasible WLS estimator unbiased A1-A3 unweighted equation.infeasible WLS estimator unbiased A1-A3 unweighted equation.FWLS estimator unbiased estimator.FWLS estimator unbiased estimator.FWLS estimator consistent A1, A2, (unweighted equation), A5, \\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma^2_i)=0\\)\nA3a sufficient equation\nA3 sufficient equation.\nFWLS estimator consistent A1, A2, (unweighted equation), A5, \\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma^2_i)=0\\)A3a sufficient equationA3 sufficient equation.FWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity.\nerrors truly multiplicative exponential heteroskedasticity, usual standard errors valid\nbelieve may mis-specification multiplicative exponential model, report heteroskedastic robust standard errors.\nFWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity.errors truly multiplicative exponential heteroskedasticity, usual standard errors validIf believe may mis-specification multiplicative exponential model, report heteroskedastic robust standard errors.","code":""},{"path":"linear-regression.html","id":"generalized-least-squares","chapter":"5 Linear Regression","heading":"5.4 Generalized Least Squares","text":"Consider\\[\n\\mathbf{y = X\\beta + \\epsilon}\n\\],\\[\nvar(\\epsilon) = \\mathbf{G} =\n\\left(\n\\begin{array}\n{cccc}\ng_{11} & g_{12} & ... & g_{1n} \\\\\ng_{21} & g_{22} & ... & g_{2n} \\\\\n. & . & . & . \\\\\ng_{n1} & . & . & g_{nn}\\\\\n\\end{array}\n\\right)\n\\]variances heterogeneous, errors correlated.\\[\n\\mathbf{\\hat{b}_G = (X'G^{-1}X)^{-1}X'G^{-1}Y}\n\\]know G, can estimate \\(\\mathbf{b}\\) just like OLS. However, know G. Hence, model structure G.","code":""},{"path":"linear-regression.html","id":"feasiable-prais-winsten","chapter":"5 Linear Regression","heading":"5.5 Feasible Prais Winsten","text":"Weighting Matrix\\[\n\\mathbf{w} =\n\\left(\n\\begin{array}{ccccc}\n\\sqrt{1- \\hat{\\rho}^2} & 0 & 0 &... & 0 \\\\\n-\\hat{\\rho} & 1 & 0 & ... & 0 \\\\\n0 &  -\\hat{\\rho} & 1 & & . \\\\\n. & . & . & . & 0 \\\\\n0 & . & 0 & -\\hat{\\rho} & 1\n\\end{array}\n\\right)\n\\]Estimate following equation using OLS\\[\ny_t = \\mathbf{x}_t \\beta + \\epsilon_t\n\\]obtain residuals \\(e_t = y_t - \\mathbf{x}_t \\hat{\\beta}\\)Estimate correlation coefficient AR(1) process estimating following OLS (without intercept)\\[\ne_t = \\rho e_{t-1} + u_t\n\\]Transform outcome independent variables \\(\\mathbf{wy}\\) \\(\\mathbf{wX}\\) respectively (weight matrix stated).FPW estimator obtained least squared estimated following weighted equation\\[\n\\mathbf{wy = wX\\beta + w\\epsilon}\n\\]Properties FeasiblePrais Winsten EstimatorThe Infeasible PW estimator A1-A3 unweighted equationThe FPW estimator biasedThe FPW consistent A1 A2 A5 \\[\nE((\\mathbf{x_t - \\rho x_{t-1}})')(\\epsilon_t - \\rho \\epsilon_{t-1})=0\n\\]A3a sufficient equationA3a sufficient equationA3 sufficient equationA3 sufficient equationThe FPW estimator asymptotically efficient OLS errors truly generated AR(1) process\nerrors truly generated AR(1) process usual standard errors valid\nconcerned may complex dependence structure heteroskedasticity, use Newey West Standard Errors\nFPW estimator asymptotically efficient OLS errors truly generated AR(1) processIf errors truly generated AR(1) process usual standard errors validIf concerned may complex dependence structure heteroskedasticity, use Newey West Standard Errors","code":""},{"path":"linear-regression.html","id":"feasible-group-level-random-effects","chapter":"5 Linear Regression","heading":"5.6 Feasible group level Random Effects","text":"Estimate following equation using OLS\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}\n\\]obtain residuals \\(e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}\\) 2. Estimate variance using usual $s^2 estimator\\[\ns^2 = \\frac{1}{n-k}\\sum_{=1}^{n}e_i^2\n\\]estimator \\(\\sigma^2_c + \\sigma^2_u\\) estimate within group correlation,\\[\n\\hat{\\sigma}^2_c = \\frac{1}{G} \\sum_{g=1}^{G} (\\frac{1}{\\sum_{=1}^{n_g-1}}\\sum_{\\neq j}\\sum_{j}^{n_g}e_{gi}e_{gj})\n\\]plug estimates obtain \\(\\hat{\\Omega}\\)feasible group level RE estimator obtained \\[\n\\hat{\\beta}= \\mathbf{(X'\\hat{\\Omega}^{-1}X)^{-1}X'\\hat{\\Omega}^{-1}y}\n\\]Properties Feasible group level Random Effects EstimatorThe infeasible group RE estimator linear estimator unbiased A1-A3 unweighted equation\nA3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) generally called random effects assumption\ninfeasible group RE estimator linear estimator unbiased A1-A3 unweighted equationA3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) generally called random effects assumptionThe Feasible group level Random Effects biasedThe Feasible group level Random Effects biasedThe Feasible group level Random Effects consistent A1-A3a, A5a unweighted equation.\nA3a requires \\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_{g})+ (\\mathbf{x}_i'u_{gi})=0\\)\nFeasible group level Random Effects consistent A1-A3a, A5a unweighted equation.A3a requires \\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_{g})+ (\\mathbf{x}_i'u_{gi})=0\\)Feasible group level Random Effects estimator asymptotically efficient OLS errors follow random effects specification\nerrors follow random effects specification usual standard errors consistent\nmight complex dependence structure heteroskedasticity, need cluster robust standard errors.\nFeasible group level Random Effects estimator asymptotically efficient OLS errors follow random effects specificationIf errors follow random effects specification usual standard errors consistentIf might complex dependence structure heteroskedasticity, need cluster robust standard errors.","code":""},{"path":"linear-regression.html","id":"ridge-regression","chapter":"5 Linear Regression","heading":"5.7 Ridge Regression","text":"Collinearity problem, use Ridge regression.main problem multicollinearity \\(\\mathbf{X'X}\\) “ill-conditioned”. idea ridge regression: adding constant diagonal \\(\\mathbf{X'X}\\) improves conditioning\\[\n\\mathbf{X'X} + c\\mathbf{} (c>0)\n\\]choice c hard. estimator\\[\n\\mathbf{b}^R = (\\mathbf{X'X}+c\\mathbf{})^{-1}\\mathbf{X'y}\n\\]biased.smaller variance OLS estimator; c increases, bias increases variance decreases.Always exists value c ridge regression estimator smaller total MSE OLSThe optimal c varies application data set.find “optimal” \\(c\\) use “ridge trace”.plot values \\(p - 1\\) parameter estimates different values c, simultaneously.Typically, c increases toward 1 coefficients decreases 0.values VIF tend decrease rapidly c gets bigger 0. VIF values begin change slowly \\(c \\1\\).can examine ridge trace VIF values chooses smallest value c regression coefficients first become stable ridge trace VIF values become sufficiently small (subjective).Typically, procedure applied standardized regression model.","code":""},{"path":"linear-regression.html","id":"principal-component-regression","chapter":"5 Linear Regression","heading":"5.8 Principal Component Regression","text":"also addresses problem multicollinearity","code":""},{"path":"linear-regression.html","id":"robust-regression","chapter":"5 Linear Regression","heading":"5.9 Robust Regression","text":"address problem influential cases.Can used known functional form fitted, errors normal due outlying cases.","code":""},{"path":"linear-regression.html","id":"least-absolute-residuals-lar-regression","chapter":"5 Linear Regression","heading":"5.9.1 Least Absolute Residuals (LAR) Regression","text":"also known minimum \\(L_1\\)-norm regression.\\[\nL_1 = \\sum_{=1}^{n}|Y_i - (\\beta_0 + \\beta_1 X_{i1} + .. + \\beta_{p-1}X_{,p-1})\n\\]sensitive outliers inadequacies model specification.","code":""},{"path":"linear-regression.html","id":"least-median-of-squares-lms-regression","chapter":"5 Linear Regression","heading":"5.9.2 Least Median of Squares (LMS) Regression","text":"\\[\nmedian\\{[Y_i - (\\beta_0 - \\beta_1X_{i1} + ... + \\beta_{p-1}X_{,p-1})]^2 \\}\n\\]","code":""},{"path":"linear-regression.html","id":"iteratively-reweighted-least-squares-irls-robust-regression","chapter":"5 Linear Regression","heading":"5.9.3 Iteratively Reweighted Least Squares (IRLS) Robust Regression","text":"uses Weighted Least Squares lessen influence outliers.weights \\(w_i\\) inversely proportional far outlying case (e.g., based residual)weights revised iteratively robust fitProcess:Step 1: Choose weight function weighting cases.\nStep 2: obtain starting weights cases.\nStep 3: Use starting weights WLS obtain residuals fitted regression function.\nStep 4: use residuals Step 3 obtain revised weights.\nStep 5: continue convergence.Note:don’t know form regression function, consider using nonparametric regression (e.g., locally weighted regression, regression trees, projection pursuit, neural networks, smoothing splines, loess, wavelets).use detect outliers confirm OLS.","code":""},{"path":"linear-regression.html","id":"maximum-likelihood-regression","chapter":"5 Linear Regression","heading":"5.10 Maximum Likelihood","text":"Premise: find values parameters maximize probability observing data words, try maximize value theta likelihood function\\[\nL(\\theta)=\\prod_{=1}^{n}f(y_i|\\theta)\n\\]\\(f(y|\\theta)\\) probability density observing single value \\(Y\\) given value \\(\\theta\\) \\(f(y|\\theta)\\) can specify various type distributions. can review back section Distributions. example, \\(y\\) dichotomous variable, \\[\nL(\\theta)=\\prod_{=1}^{n}\\theta^{y_i}(1-\\theta)^{1-y_i}\n\\]\\(\\hat{\\theta}\\) Maximum Likelihood estimate \\(L(\\hat{\\theta}) > L(\\theta_0)\\) values \\(\\theta_0\\) parameter space.","code":""},{"path":"linear-regression.html","id":"motivation-for-mle","chapter":"5 Linear Regression","heading":"5.10.1 Motivation for MLE","text":"Suppose know conditional distribution y given x:\\[\nf_{Y|X}(y,x;\\theta)\n\\]\\(\\theta\\) unknown parameter distribution. Sometimes concerned unconditional distribution \\(f_{Y}(y;\\theta)\\)given sample iid data, can calculate joint distribution entire sample,\\[\nf_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\\theta)}= \\prod_{=1}^{n}f_{Y|X}(y_i,x_i;\\theta)\n\\]joint distribution evaluated sample likelihood (probability) observed particular sample (depends \\(\\theta\\))Idea MLE: Given sample, choose estimates parameters gives highest likelihood (probability) observing particular sample\\[\nmax_{\\theta} \\prod_{=1}^{n}f_{Y|X}(y_i,x_i; \\theta)\n\\]Equivalently,\\[\nmax_{\\theta} \\prod_{=1}^{n} ln(f_{Y|X}(y_i,x_i; \\theta))\n\\]Solving Maximum Likelihood EstimatorSolve First Order Condition\\[\n\\frac{\\partial}{\\partial \\theta}\\sum_{=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) = 0\n\\]\\(\\hat{\\theta}_{MLE}\\) defined.Evaluate Second Order Condition\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) < 0\n\\]condition ensures can solve maximumExamples:\nUnconditional Poisson Distribution: Number products ordered Amazon within hour, number website visits day political campaign.Exponential Distribution: Length time earthquake occurs, length time car battery lasts.\\[\n\\begin{aligned}\nf_{Y|X}(y,x;\\theta) &= exp(-y/x\\theta)/x\\theta \\\\\nf_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\\theta)} &= \\prod_{=1}^{n}exp(-y_i/x_i \\theta)/x_i \\theta\n\\end{aligned}\n\\]","code":""},{"path":"linear-regression.html","id":"assumption","chapter":"5 Linear Regression","heading":"5.10.2 Assumption","text":"High Level Regulatory Assumptions sufficient condition used show large sample properties\nHence, MLE, need either assume verify regulatory assumption holds.\nHigh Level Regulatory Assumptions sufficient condition used show large sample propertiesHence, MLE, need either assume verify regulatory assumption holds.observations independent density function.observations independent density function.multivariate normal assumption, maximum likelihood yields consistent estimates means covariance matrix multivariate distribution finite fourth moments (Little 1988)multivariate normal assumption, maximum likelihood yields consistent estimates means covariance matrix multivariate distribution finite fourth moments (Little 1988)find MLE, usually differentiate log-likelihood function set equal 0.\\[\n\\frac{d}{d\\theta}l(\\theta) = 0\n\\]score equationOur confidence MLE quantified “pointedness” log-likelihood\\[\nI_O(\\theta)= \\frac{d^2}{d\\theta^2}l(\\theta) = 0\n\\]called observed informationwhile\\[\n(\\theta)=E[I_O(\\theta;Y)]\n\\]expected information. (also known Fisher Information). base variance estimator.\\[\nV(\\hat{\\Theta}) \\approx (\\theta)^{-1}\n\\]Consistency MLE\nSuppose \\(y_i\\) \\(x_i\\) iid drawn true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta_0)\\). following regulatory assumptions hold,R1: \\(\\theta \\neq \\theta_0\\) \\(f_{Y|X}(y_i,x_i;\\theta) \\neq f_{Y|X}(y_i,x_i;\\theta_0)\\)R1: \\(\\theta \\neq \\theta_0\\) \\(f_{Y|X}(y_i,x_i;\\theta) \\neq f_{Y|X}(y_i,x_i;\\theta_0)\\)R2: set \\(\\Theta\\) contains true parameters \\(\\theta_0\\) compactR2: set \\(\\Theta\\) contains true parameters \\(\\theta_0\\) compactR3: log-likelihood \\(ln(f_{Y|X}(y_i,x_i;\\theta_0))\\) continuous \\(\\theta\\) probability 1R3: log-likelihood \\(ln(f_{Y|X}(y_i,x_i;\\theta_0))\\) continuous \\(\\theta\\) probability 1R4: \\(E(sup_{\\theta \\\\Theta}|ln(f_{Y|X}(y_i,x_i;\\theta_0))|)\\)R4: \\(E(sup_{\\theta \\\\Theta}|ln(f_{Y|X}(y_i,x_i;\\theta_0))|)\\)MLE estimator consistent,\\[\n\\hat{\\theta}_{MLE} \\^p \\theta_0\n\\]Asymptotic Normality MLESuppose \\(y_1\\) \\(x_i\\) iid drawn true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta)\\). R1-R4 following holdR5: \\(\\theta_0\\) interior set \\(\\Theta\\)R5: \\(\\theta_0\\) interior set \\(\\Theta\\)R6: \\(f_{Y|X}(y_i,x_i;\\theta)\\) twice continuously differentiable \\(\\theta\\) \\(f_{Y|X}(y_i,x_i;\\theta) >0\\) neighborhood \\(N \\\\Theta\\) around \\(\\theta_0\\)R6: \\(f_{Y|X}(y_i,x_i;\\theta)\\) twice continuously differentiable \\(\\theta\\) \\(f_{Y|X}(y_i,x_i;\\theta) >0\\) neighborhood \\(N \\\\Theta\\) around \\(\\theta_0\\)R7: \\(\\int sup_{\\theta \\N}||\\partial f_{Y|X}(y_i,x_i;\\theta)\\partial\\theta||d(y,x) <\\infty\\), \\(\\int sup_{\\theta \\N} || \\partial^2 f_{Y|X}(y_i,x_i;\\theta)/\\partial \\theta \\partial \\theta' || d(y,x) < \\infty\\) \\(E(sup_{\\theta \\N} || \\partial^2ln(f_{Y|X}(y_i,x_i;\\theta)) / \\partial \\theta \\partial \\theta' ||) < \\infty\\)R7: \\(\\int sup_{\\theta \\N}||\\partial f_{Y|X}(y_i,x_i;\\theta)\\partial\\theta||d(y,x) <\\infty\\), \\(\\int sup_{\\theta \\N} || \\partial^2 f_{Y|X}(y_i,x_i;\\theta)/\\partial \\theta \\partial \\theta' || d(y,x) < \\infty\\) \\(E(sup_{\\theta \\N} || \\partial^2ln(f_{Y|X}(y_i,x_i;\\theta)) / \\partial \\theta \\partial \\theta' ||) < \\infty\\)R8: information matrix \\((\\theta_0) = Var(\\partial f_{Y|X}(y,x_i; \\theta_0)/\\partial \\theta)\\) exists non-singularR8: information matrix \\((\\theta_0) = Var(\\partial f_{Y|X}(y,x_i; \\theta_0)/\\partial \\theta)\\) exists non-singularthen MLE estimator asymptotically normal,\\[\n\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta_0) \\^d N(0,(\\theta_0)^{-1})\n\\]","code":""},{"path":"linear-regression.html","id":"properties","chapter":"5 Linear Regression","heading":"5.10.3 Properties","text":"Consistent: estimates approximately unbiased large samplesConsistent: estimates approximately unbiased large samplesAsymptotically efficient: approximately smaller standard errors compared estimatorAsymptotically efficient: approximately smaller standard errors compared estimatorAsymptotically normal: repeated sampling, estimates approximately normal distribution. Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based n independent observations. \\(\\hat{\\theta}_n \\sim N(\\theta,H^{-1})\\).\nH called Fisher information matrix. contains expected values second partial derivatives log-likelihood function. (.j)th element H \\(-E(\\frac{\\partial^2l(\\theta)}{\\partial \\theta_i \\partial \\theta_j})\\)\ncan estimate H finding form determined , evaluating \\(\\theta = \\hat{\\theta}_n\\)\nAsymptotically normal: repeated sampling, estimates approximately normal distribution. Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based n independent observations. \\(\\hat{\\theta}_n \\sim N(\\theta,H^{-1})\\).H called Fisher information matrix. contains expected values second partial derivatives log-likelihood function. (.j)th element H \\(-E(\\frac{\\partial^2l(\\theta)}{\\partial \\theta_i \\partial \\theta_j})\\)can estimate H finding form determined , evaluating \\(\\theta = \\hat{\\theta}_n\\)Invariance: MLE \\(g(\\theta) = g(\\hat{\\theta})\\) function g(.)Invariance: MLE \\(g(\\theta) = g(\\hat{\\theta})\\) function g(.)\\[\n\\hat{\\Theta} \\approx^d (\\theta,(\\hat{\\theta)^{-1}}))\n\\]Explicit vs Implicit MLEIf solve score equation get expression MLE, ’s called explicitIf closed form MLE, need algorithms derive expression, ’s called implicitLarge Sample Property MLEImplicit theorems assumption know conditional distribution,\\[\nf_{Y|X}(y_i,x_i;\\theta_0)\n\\]just now know exact parameter value.Distributional mis-specification result inconsistent parameter estimates.Quasi-MLE: Particular settings/ assumption allow certain types distributional mis-specification (Ex: long distribution part particular class satisfies particular assumption, estimating wrong distribution lead inconsistent parameter estimates).non-parametric/ Semi-parametric estimation: little distributional assumption made. (hard implement, derive properties, interpret)asymptotic variance MLE achieves Cramer-Rao Lower Bound C. R. Rao (1992)Cramer-Rao Lower Bound lower brand asymptotic variance consistent asymptotically normally distributed estimator.estimator achieves lower bound efficient estimator.maximum Likelihood estimator (assuming distribution correctly specified R1-R8 hold) efficient consistent asymptotically normal estimator. * efficient among consistent estimators (limited unbiased linear estimators).NoteML better choice binary, strictly positive, count, inherent heteroskedasticity linear model.ML better choice binary, strictly positive, count, inherent heteroskedasticity linear model.ML assume know conditional distribution outcome, derive estimator using information.\nAdds assumption know distribution (similar A6 Normal Distribution linear model)\nproduce efficient estimator.\nML assume know conditional distribution outcome, derive estimator using information.Adds assumption know distribution (similar A6 Normal Distribution linear model)produce efficient estimator.","code":""},{"path":"linear-regression.html","id":"compare-to-ols","chapter":"5 Linear Regression","heading":"5.10.4 Compare to OLS","text":"MLE cure OLS problems:joint inference MLE, typically use log-likelihood calculation, instead F-scoreFunctional form affects estimation MLE OLS.Perfect Collinearity/Multicollinearity: highly correlated likely yield large standard errors.Endogeneity (Omitted variables bias, Simultaneous equations): Like OLS, MLE also biased problem","code":""},{"path":"linear-regression.html","id":"application","chapter":"5 Linear Regression","heading":"5.10.5 Application","text":"applications MLECorner Solution\nEx: hours worked, donations charity\nEstimate Tobit\nCorner SolutionEx: hours worked, donations charityEstimate TobitNon-negative count\nEx: Numbers arrest, Number cigarettes smoked day\nEstimate Poisson regression\nNon-negative countEx: Numbers arrest, Number cigarettes smoked dayEstimate Poisson regressionMultinomial Choice\nEx: Demand cars, votes primary election\nEstimate mutinomial probit logit\nMultinomial ChoiceEx: Demand cars, votes primary electionEstimate mutinomial probit logitOrdinal Choice\nEx: Levels Happiness, Levels Income\nOrdered Probit\nOrdinal ChoiceEx: Levels Happiness, Levels IncomeOrdered ProbitModel binary Response\nbinary variable [Bernoulli] distribution:\\[\nf_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)}\n\\]\\(p\\) probability success. conditional distribution :\\[\nf_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}\n\\]choose \\(p(x_i)\\) reasonable function \\(x_i\\) unknown parameters \\(\\theta\\)can use latent variable model probability functions\\[\n\\begin{aligned}\ny_i &= 1\\{y_i^* > 0 \\}  \\\\\ny_i^* &= x_i \\beta-\\epsilon_i\n\\end{aligned}\n\\]\\(y_i^*\\) latent variable (unobserved) well-defined terms units/magnitudes\\(\\epsilon_i\\) mean 0 unobserved random variable.can rewrite model without latent variable,\\[\ny_i = 1\\{x_i beta > \\epsilon_i \\}\n\\]probability function,\\[\n\\begin{aligned}\np(x_i) &= P(y_i = 1|x_i) \\\\\n&= P(x_i \\beta > \\epsilon_i | x_i) \\\\\n&= F_{\\epsilon|X}(x_i \\beta | x_i)\n\\end{aligned}\n\\]need choose conditional distribution \\(\\epsilon_i\\). Hence, can make additional strong independence assumption\\(\\epsilon_i\\) independent \\(x_i\\)probability function simply,\\[\np(x_i) = F_\\epsilon(x_i \\beta)\n\\]probability function also conditional expectation function,\\[\nE(y_i | x_i) = P(y_i = 1|x_i) = F_\\epsilon (x_i \\beta)\n\\]allow conditional expectation function non-linear.Common distributional assumptionProbit: Assume \\(\\epsilon_i\\) standard normally distributed, \\(F_\\epsilon(.) = \\Phi(.)\\) standard normal CDF.Logit: Assume \\(\\epsilon_i\\) standard logistically distributed, \\(F_\\epsilon(.) = \\Lambda(.)\\) standard normal CDF.Step deriveChoose distribution (normal logistic) plug following log likelihood,\\[\nln(f_{Y|X} (y_i , x_i; \\beta)) = y_i ln(F_\\epsilon(x_i \\beta)) + (1-y_i)ln(1-F_\\epsilon(x_i \\beta))\n\\]Solve MLE finding Maximum \\[\n\\hat{\\beta}_{MLE} = argmax \\sum_{=1}^{n}ln(f_{Y|X}(y_i,x_i; \\beta))\n\\]Properties Probit Logit EstimatorsProbit Logit consistent asymptotically normal \nA2 Full rank holds: \\(E(x_i' x_i)\\) exists non-singular\nA5 Data Generation (random Sampling) (A5a) holds: {y_i,x_i} iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold: Normal/Logistic independent \\(x_i\\)\nProbit Logit consistent asymptotically normal ifA2 Full rank holds: \\(E(x_i' x_i)\\) exists non-singularA5 Data Generation (random Sampling) (A5a) holds: {y_i,x_i} iid (stationary weakly dependent).Distributional assumptions \\(\\epsilon_i\\) hold: Normal/Logistic independent \\(x_i\\)assumptions, Probit Logit also asymptotically efficient asymptotic variance,assumptions, Probit Logit also asymptotically efficient asymptotic variance,\\[\n(\\beta_0)^{-1} = [E(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i\\beta_0)(1-F_\\epsilon(x_i\\beta_0))}x_i' x_i)]^{-1}\n\\]\\(F_\\epsilon(x_i\\beta_0)\\) probability density function (derivative CDF)","code":""},{"path":"linear-regression.html","id":"interpretation","chapter":"5 Linear Regression","heading":"5.10.5.1 Interpretation","text":"\\(\\beta\\) average response latent variable associated change \\(x_i\\)Magnitudes meaningDirection meaningThe partial effect Non-linear binary response model\\[\n\\begin{aligned}\nE(y_i |x_i) &= F_\\epsilon (x_i \\beta) \\\\\nPE(x_{ij}) &= \\frac{\\partial E(y_i |x_i)}{\\partial x_{ij}} = f_\\epsilon (x_i \\beta)\\beta_j\n\\end{aligned}\n\\]partial effect coefficient parameter \\(\\beta_j\\) multiplied scaling factor \\(f_\\epsilon (x_i \\beta)\\)scaling factor depends \\(x_i\\) partial effect changes depending \\(x_i\\) isSingle value partial effectPartial Effect Average (PEA) partial effect average individual\\[\nf_{\\epsilon}(\\bar{x}\\hat{\\beta})\\hat{\\beta}_j\n\\]Average Partial Effect (APE) average partial effect individual.\\[\n\\frac{1}{n}\\sum_{=1}^{n}f_\\epsilon(x_i \\hat{\\beta})\\hat{\\beta}_j\n\\]linear model, \\(APE = PEA\\).non-linear model (e.g., binary response), \\(APE \\neq PEA\\)","code":""},{"path":"non-linear-regression.html","id":"non-linear-regression","chapter":"6 Non-linear Regression","heading":"6 Non-linear Regression","text":"Definition: models derivatives mean function respect parameters depend one parameters.approximate data, can approximate functionby high-order polynomialby linear model (e.g., Taylor expansion around \\(X\\)’s)collection locally linear models basis functionbut easy interpret, enough data, can’t interpret globally.intrinsically nonlinear models:\\[\nY_i = f(\\mathbf{x_i;\\theta}) + \\epsilon_i\n\\]\\(f(\\mathbf{x_i;\\theta})\\) nonlinear function relating \\(E(Y_i)\\) independent variables \\(x_i\\)\\(\\mathbf{x}_i\\) \\(k \\times 1\\) vector independent variables (fixed).\\(\\mathbf{\\theta}\\) \\(p \\times 1\\) vector parameters.\\(\\epsilon_i\\)s iid variables mean 0 variance \\(\\sigma^2\\). (sometimes ’s normal).","code":""},{"path":"non-linear-regression.html","id":"inference-1","chapter":"6 Non-linear Regression","heading":"6.1 Inference","text":"Since \\(Y_i = f(\\mathbf{x}_i,\\theta) + \\epsilon_i\\), \\(\\epsilon_i \\sim iid(0,\\sigma^2)\\), can obtain \\(\\hat{\\theta}\\) minimizing \\(\\sum_{=1}^{n}(Y_i - f(x_i,\\theta))^2\\) estimate \\(s^2 = \\hat{\\sigma}^2_{\\epsilon}=\\frac{\\sum_{=1}^{n}(Y_i - f(x_i,\\theta))^2}{n-p}\\)","code":""},{"path":"non-linear-regression.html","id":"linear-function-of-the-parameters","chapter":"6 Non-linear Regression","heading":"6.1.1 Linear Function of the Parameters","text":"assume \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), \\[\n\\hat{\\theta} \\sim (\\mathbf{\\theta},\\sigma^2[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1})\n\\]= asymptotic normalityAsymptotic means enough data make inference (sample size increases, becomes accurate (true value)).Since want inference linear combinations parameters contrasts.\\(\\mathbf{\\theta} = (\\theta_0,\\theta_1,\\theta_2)'\\) want look \\(\\theta_1 - \\theta_2\\); can define vector \\(\\mathbf{} = (0,1,-1)'\\), consider inference \\(\\mathbf{'\\theta}\\)Rules expectation variance fixed vector \\(\\mathbf{}\\) random vector \\(\\mathbf{Z}\\);\\[\n\\begin{aligned}\nE(\\mathbf{'Z}) &= \\mathbf{'}E(\\mathbf{Z}) \\\\\nvar(\\mathbf{'Z}) &= \\mathbf{'}var(\\mathbf{Z}) \\mathbf{}\n\\end{aligned}\n\\],\\[\n\\mathbf{'\\hat{\\theta}} \\sim (\\mathbf{'\\theta},\\sigma^2\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})\n\\]\\(\\mathbf{'\\hat{\\theta}}\\) asymptotically independent \\(s^2\\) (order 1/n) \\[\n\\frac{\\mathbf{'\\hat{\\theta}-'\\theta}}{s(\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})^{1/2}} \\sim t_{n-p}\n\\]construct \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mathbf{'\\theta}\\)\\[\n\\mathbf{'\\theta} \\pm t_{(1-\\alpha/2,n-p)}s(\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})^{1/2}\n\\]Suppose \\(\\mathbf{'} = (0,...,j,...,0)\\). , confidence interval \\(j\\)-th element \\(\\mathbf{\\theta}\\) \\[\n\\hat{\\theta}_j \\pm t_{(1-\\alpha/2,n-p)}s\\sqrt{\\hat{c}^{j}}\n\\]\\(\\hat{c}^{j}\\) \\(j\\)-th diagonal element \\([\\mathbf{F(\\hat{\\theta})'F(\\hat{\\theta})}]^{-1}\\)example, can get starting values using linearized version function \\(\\log y = \\log + b x\\). , can fit linear regression use estimates starting valueswith nls, can fit nonlinear model via least squares","code":"\n#set a seed value\nset.seed(23)\n\n#Generate x as 100 integers using seq function\nx <- seq(0, 100, 1)\n\n#Generate y as a*e^(bx)+c\ny <- runif(1, 0, 20) * exp(runif(1, 0.005, 0.075) * x) + runif(101, 0, 5)\n\n# visualize\nplot(x, y)\n\n#define our data frame\ndatf = data.frame(x, y)\n\n#define our model function\nmod = function(a, b, x)\n    a * exp(b * x)\n#get starting values by linearizing\nlin_mod = lm(log(y) ~ x, data = datf)\n\n#convert the a parameter back from the log scale; b is ok\nastrt = exp(as.numeric(lin_mod$coef[1]))\nbstrt = as.numeric(lin_mod$coef[2])\nprint(c(astrt, bstrt))\n#> [1] 14.07964761  0.01855635\nnlin_mod = nls(y ~ mod(a, b, x),\n               start = list(a = astrt, b = bstrt),\n               data = datf)\n\n#look at model fit summary\nsummary(nlin_mod)\n#> \n#> Formula: y ~ mod(a, b, x)\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> a 13.603909   0.165390   82.25   <2e-16 ***\n#> b  0.019110   0.000153  124.90   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.542 on 99 degrees of freedom\n#> \n#> Number of iterations to convergence: 3 \n#> Achieved convergence tolerance: 7.006e-07\n\n#add prediction to plot\nplot(x, y)\nlines(x, predict(nlin_mod), col = \"red\")"},{"path":"non-linear-regression.html","id":"nonlinear","chapter":"6 Non-linear Regression","heading":"6.1.2 Nonlinear","text":"Suppose \\(h(\\theta)\\) nonlinear function parameters. can use Taylor series \\(\\theta\\)\\[\nh(\\hat{\\theta}) \\approx h(\\theta) + \\mathbf{h}'[\\hat{\\theta}-\\theta]\n\\]\\(\\mathbf{h} = (\\frac{\\partial h}{\\partial \\theta_1},...,\\frac{\\partial h}{\\partial \\theta_p})'\\)\\[\n\\begin{aligned}\nE( \\hat{\\theta}) &\\approx \\theta \\\\\nvar(\\hat{\\theta}) &\\approx  \\sigma^2[\\mathbf{F(\\theta)'F(\\theta)}]^{-1} \\\\\nE(h(\\hat{\\theta})) &\\approx h(\\theta) \\\\\nvar(h(\\hat{\\theta})) &\\approx \\sigma^2 \\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h}\n\\end{aligned}\n\\]Thus,\\[\nh(\\hat{\\theta}) \\sim (h(\\theta),\\sigma^2\\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h})\n\\]approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(h(\\theta)\\) \\[\nh(\\hat{\\theta}) \\pm t_{(1-\\alpha/2;n-p)}s(\\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h})^{1/2}\n\\]\\(\\mathbf{h}\\) \\(\\mathbf{F}(\\theta)\\) evaluated \\(\\hat{\\theta}\\)Regarding prediction interval Y \\(x=x_0\\)\\[\n\\begin{aligned}\nY_0 &= f(x_0;\\theta) + \\epsilon_0, \\epsilon_0 \\sim N(0,\\sigma^2) \\\\\n\\hat{Y}_0 &= f(x_0,\\hat{\\theta})\n\\end{aligned}\n\\]\\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0,\\theta) + \\mathbf{f}_0(\\mathbf{\\theta})'[\\hat{\\theta}-\\theta]\n\\]\\[\nf_0(\\theta)= (\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_1},..,\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_p})'\n\\](note: \\(f_0(\\theta)\\) different \\(f(\\theta)\\)).\\[\n\\begin{aligned}\nY_0 - \\hat{Y}_0 &\\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\\n&= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(Y_0 - \\hat{Y}_0) &\\approx E(\\epsilon_0)E(\\hat{\\theta}-\\theta) = 0 \\\\\nvar(Y_0 - \\hat{Y}_0) &\\approx var(\\epsilon_0 - \\mathbf{(f_0(\\theta)'[\\hat{\\theta}-\\theta])}) \\\\\n&= \\sigma^2 + \\sigma^2 \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)} \\\\\n&= \\sigma^2 (1 + \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)})\n\\end{aligned}\n\\]Hence, combining\\[\nY_0 - \\hat{Y}_0 \\sim (0,\\sigma^2 (1 + \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)}))\n\\]Note:Confidence intervals mean response \\(Y_i\\) (different prediction intervals) can obtained similarly.","code":""},{"path":"non-linear-regression.html","id":"non-linear-least-squares","chapter":"6 Non-linear Regression","heading":"6.2 Non-linear Least Squares","text":"LS estimate \\(\\theta\\), \\(\\hat{\\theta}\\) set parameters minimizes residual sum squares:\\[\nS(\\hat{\\theta}) = SSE(\\hat{\\theta}) = \\sum_{=1}^{n}\\{Y_i - f(\\mathbf{x_i};\\hat{\\theta})\\}^2\n\\]obtain solution, can consider partial derivatives \\(S(\\theta)\\) respect \\(\\theta_j\\) set 0, gives system p equations. normal equation \\[\n\\frac{\\partial S(\\theta)}{\\partial \\theta_j} = -2\\sum_{=1}^{n}\\{Y_i -f(\\mathbf{x}_i;\\theta)\\}[\\frac{\\partial(\\mathbf{x}_i;\\theta)}{\\partial \\theta_j}] = 0\n\\]can’t obtain solution directly/analytically equation.Numerical SolutionsGrid search\n“grid” possible parameter values see one minimize residual sum squares.\nfiner grid = greater accuracy\ninefficient, hard p large.\nGrid searchA “grid” possible parameter values see one minimize residual sum squares.finer grid = greater accuracycould inefficient, hard p large.Gauss-Newton Algorithm\ninitial estimate \\(\\theta\\) denoted \\(\\hat{\\theta}^{(0)}\\)\nuse Taylor expansions \\(f(\\mathbf{x}_i;\\theta)\\) function \\(\\theta\\) point \\(\\hat{\\theta}^{(0)}\\)\nGauss-Newton Algorithmwe initial estimate \\(\\theta\\) denoted \\(\\hat{\\theta}^{(0)}\\)use Taylor expansions \\(f(\\mathbf{x}_i;\\theta)\\) function \\(\\theta\\) point \\(\\hat{\\theta}^{(0)}\\)\\[\n\\begin{aligned}\nY_i &= f(x_i;\\theta) + \\epsilon_i \\\\\n&= f(x_i;\\theta) + \\sum_{j=1}^{p}\\{\\frac{\\partial f(x_i;\\theta)}{\\partial \\theta_j}\\}_{\\theta = \\hat{\\theta}^{(0)}} (\\theta_j - \\hat{\\theta}^{(0)}) + \\text{remainder} + \\epsilon_i\n\\end{aligned}\n\\]Equivalently,matrix notation,\\[\n\\mathbf{Y} =\n\\left[\n\\begin{array}\n{c}\nY_1 \\\\\n. \\\\\nY_n\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{f}(\\hat{\\theta}^{(0)}) =\n\\left[\n\\begin{array}\n{c}\nf(\\mathbf{x_1,\\hat{\\theta}}^{(0)}) \\\\\n. \\\\\nf(\\mathbf{x_n,\\hat{\\theta}}^{(0)})\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{\\epsilon} =\n\\left[\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n. \\\\\n\\epsilon_n\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{F}(\\hat{\\theta}^{(0)}) =\n\\left[\n\\begin{array}\n{ccc}\n\\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_1} & ... & \\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_p}\\\\\n. & . & . \\\\\n\\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_1} & ... & \\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_p}\n\\end{array} \\right]_{\\theta = \\hat{\\theta}^{(0)}}\n\\]Hence,\\[\n\\mathbf{Y} = \\mathbf{f}(\\hat{\\theta}^{(0)}) + \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon + \\text{remainder}\n\\]assume remainder small error term assumed iid mean 0 variance \\(\\sigma^2\\).can rewrite equation \\[\n\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(0)}) \\approx \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon\n\\]form linear model. solve \\((\\theta - \\hat{\\theta}^{(0)})\\) let equal \\(\\hat{\\delta}^{(1)}\\)\nnew estimate given adding Gauss increment adjustment initial estimate \\(\\hat{\\theta}^{(1)} = \\hat{\\theta}^{(0)} + \\hat{\\delta}^{(1)}\\)\ncan repeat process.Gauss-Newton Algorithm Steps:initial estimate \\(\\hat{\\theta}^{(0)}\\), set j = 0Taylor series expansion calculate \\(\\mathbf{f}(\\hat{\\theta}^{(j)})\\) \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\)Use OLS get \\(\\hat{\\delta}^{(j+1)}\\)get new estimate \\(\\hat{\\theta}^{(j+1)}\\), return step 2continue “convergence”final parameter estimate \\(\\hat{\\theta}\\), can estimate \\(\\sigma^2\\) \\(\\epsilon \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{})\\) \\[\n\\hat{\\sigma}^2= \\frac{1}{n-p}(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta}))'(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta}))\n\\]Criteria convergenceMinor change objective function (SSE = residual sum squares)\\[\n\\frac{|SSE(\\hat{\\theta}^{(j+1)})-SSE(\\hat{\\theta}^{(j)})|}{SSE(\\hat{\\theta}^{(j)})} < \\gamma_1\n\\]Minor change parameter estimates\\[\n|\\hat{\\theta}^{(j+1)}-\\hat{\\theta}^{(j)}| < \\gamma_2\n\\]“residual projection” criterion (Bates Watts 1981)","code":""},{"path":"non-linear-regression.html","id":"alternative-of-gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1 Alternative of Gauss-Newton Algorithm","text":"","code":""},{"path":"non-linear-regression.html","id":"gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1.1 Gauss-Newton Algorithm","text":"Normal equations:\\[\n\\frac{\\partial SSE(\\theta)}{\\partial \\theta} = 2\\mathbf{F}(\\theta)'[\\mathbf{Y}-\\mathbf{f}(\\theta)]\n\\]\\[\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)} &= \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)} \\\\\n&= \\hat{\\theta}^{(j)} + [\\mathbf{F}((\\hat{\\theta})^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\mathbf{F}(\\hat{\\theta})^{(j)} \\\\\n&= \\hat{\\theta}^{(j)} - \\frac{1}{2}[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\end{aligned}\n\\]\\(\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vector (points direction SSE increases rapidly). path known steepest ascent.\\([\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\) indicates far move\\(-1/2\\): indicator direction steepest descent.","code":""},{"path":"non-linear-regression.html","id":"modified-gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1.2 Modified Gauss-Newton Algorithm","text":"avoid overstepping (local min), can use modified Gauss-Newton Algorithm. define new proposal \\(\\theta\\)\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}, 0 < \\alpha_j < 1\n\\]\\(\\alpha_j\\) (called “learning rate”): used modify step length.also \\(\\alpha \\times 1/2\\), typically assumed absorbed learning rate.way choose \\(\\alpha_j\\), can use step halving\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\frac{1}{2^k}\\hat{\\delta}^{(j+1)}\n\\]\\(k\\) smallest non-negative integer \\[\nSSE(\\hat{\\theta}^{(j)}+\\frac{1}{2^k}\\hat{\\delta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\n\\] means try \\(\\hat{\\delta}^{(j+1)}\\), \\(\\hat{\\delta}^{(j+1)}/2\\), \\(\\hat{\\delta}^{(j+1)}/4\\), etc.general form convergence algorithm \\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_j \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]\\(\\mathbf{}_j\\) positive definite matrix\\(\\alpha_j\\) learning rate\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\)gradient based objective function Q (function \\(\\theta\\)), typically SSE nonlinear regression applications (e.g., cross-entropy classification).Refer back Modified Gauss-Newton Algorithm, can see form\\[\n\\hat{\\theta}^{(j+1)} =\\hat{\\theta}^{(j)} - \\alpha_j[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]Q = SSE, \\([\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} = \\mathbf{}\\)","code":""},{"path":"non-linear-regression.html","id":"steepest-descent","chapter":"6 Non-linear Regression","heading":"6.2.1.3 Steepest Descent","text":"(also known just “gradient descent”)\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_{p \\times p}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]slow converge, moves rapidly initially.use starting values","code":""},{"path":"non-linear-regression.html","id":"levenberg--marquardt","chapter":"6 Non-linear Regression","heading":"6.2.1.4 Levenberg -Marquardt","text":"\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})+ \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]compromise Gauss-Newton Algorithm Steepest Descent.best \\(\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})\\) nearly singular (\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) isn’t full rank)similar ridge regressionIf \\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), \\(\\tau= \\tau/10\\) next iteration. Otherwise, \\(\\tau = 10 \\tau\\)","code":""},{"path":"non-linear-regression.html","id":"newton-raphson","chapter":"6 Non-linear Regression","heading":"6.2.1.5 Newton-Raphson","text":"\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\frac{\\partial^2Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}]^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]Hessian matrix can rewritten :\\[\n\\frac{ \\partial^2Q(\\hat{ \\theta}^{(j)})}{ \\partial \\theta \\partial \\theta'} = 2 \\mathbf{F}((\\hat{ \\theta})^{(j)})' \\mathbf{F} ( \\hat{\\theta}^{(j)}) - 2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\n\\]contains term Gauss-Newton Algorithm, combined one containing second partial derivatives f(). (methods require second derivatives objective function known “second-order methods”.)\nHowever, last term \\(\\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\\) can sometimes non-singular.","code":""},{"path":"non-linear-regression.html","id":"quasi-newton","chapter":"6 Non-linear Regression","heading":"6.2.1.6 Quasi-Newton","text":"update \\(\\theta\\) according \\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{H}_j^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]\\(H_j\\) symmetric positive definite approximation Hessian, gets closer \\(j \\\\infty\\).\\(\\mathbf{H}_j\\) computed iterativelyAmong first-order methods (first derivatives required), method performs best.","code":""},{"path":"non-linear-regression.html","id":"derivative-free-methods","chapter":"6 Non-linear Regression","heading":"6.2.1.7 Derivative Free Methods","text":"secant Method: like Gauss-Newton Algorithm, calculates derivatives numerically past iterations.Simplex MethodsGenetic AlgorithmDifferential Evolution AlgorithmsParticle Swarm OptimizationAnt Colony Optimization","code":""},{"path":"non-linear-regression.html","id":"practical-considerations","chapter":"6 Non-linear Regression","heading":"6.2.2 Practical Considerations","text":"converge, algorithm need good initial estimates.Starting values:\nPrior theoretical info\ngrid search graph \\(SSE(\\theta)\\)\nalso use OLS get starting values.\nModel interpretation: idea regarding form objective function, can try guess initial value.\nExpected Value Parameterization\nStarting values:Prior theoretical infoA grid search graph \\(SSE(\\theta)\\)also use OLS get starting values.Model interpretation: idea regarding form objective function, can try guess initial value.Expected Value ParameterizationConstrained Parameters: (constraints parameters like \\(\\theta_i>,< \\theta_i <b\\))\nfit model first see converged parameter estimates satisfy constraints.\ndon’t satisfy, try re-parameterizing\nConstrained Parameters: (constraints parameters like \\(\\theta_i>,< \\theta_i <b\\))fit model first see converged parameter estimates satisfy constraints.don’t satisfy, try re-parameterizing","code":""},{"path":"non-linear-regression.html","id":"failure-to-converge","chapter":"6 Non-linear Regression","heading":"6.2.2.1 Failure to converge","text":"\\(SSE(\\theta)\\) may “flat” neighborhood minimum.can try different “better” starting values.Might suggest model complex data, might consider simpler model.","code":""},{"path":"non-linear-regression.html","id":"convergence-to-a-local-minimum","chapter":"6 Non-linear Regression","heading":"6.2.2.2 Convergence to a Local Minimum","text":"Linear least squares property \\(SSE(\\theta) = \\mathbf{(Y-X\\beta)'(Y-X\\beta)}\\), quadratic unique minimum (maximum).Nonlinear east squares need unique minimumUsing different starting values can helpIf dimension \\(\\theta\\) low, graph \\(SSE(\\theta)\\) function \\(\\theta_i\\)Different algorithm can help (e.g., genetic algorithm, particle swarm)converge, algorithms need good initial estimates.Starting values:\nprior theoretical info\ngrid search graph\nOLS estimates starting values\nModel interpretation\nExpected Value Parameterization\nStarting values:prior theoretical infoA grid search graphOLS estimates starting valuesModel interpretationExpected Value ParameterizationConstrained Parameters:\ntry model without constraints first.\nresulted parameter estimates satisfy constraint, try re-parameterizing\nConstrained Parameters:try model without constraints first.resulted parameter estimates satisfy constraint, try re-parameterizingFor prediction intervalBased forms function, can also programmed starting values nls function (e.e.g, logistic growth, asymptotic regression, etc).example, logistic growth model:\\[\nP = \\frac{K}{1+ exp(P_0+ rt)} + \\epsilon\n\\]whereP = population time tK = carrying capacityr = population growth ratebut R slight different parameterization:\\[\nP = \\frac{asym}{1 + exp(\\frac{xmid - t}{scal})}\n\\]\\(asym\\) = carrying capacity\\(xmid\\) = x value inflection point curve\\(scal\\) = scaling parameter.Hence, \\(K = asym\\)\\(r = -1/scal\\)\\(P_0 = -rxmid\\)parameterizationIf can also define self-starting function models uncommon (built nls)Example based (Schabenberger Pierce 2001)suggested model (known plateau model) \\[\nE(Y_{ij}) = (\\beta_{0j} + \\beta_{1j}N_{ij})I_{N_{ij}\\le \\alpha_j} + (\\beta_{0j} + \\beta_{1j}\\alpha_j)I_{N_{ij} > \\alpha_j}\n\\]whereN observationi particular observationj = 1,2 corresponding depths (30,60)define selfStart function. defined model linear first part plateau (remain constant) can use first half predictors (sorted increasing value) get initial estimate slope intercept model, last predictor value (alpha) can starting value plateau parameter.combine model custom function calculate starting values.Instead modeling depths model separately model together - common slope, intercept, plateau.Examine residual values combined model.can test whether parameters two soil depth fits significantly different? know combined model appropriate, consider parameterization let parameters 60cm model equal parameters 30cm model plus increment:\\[\n\\begin{aligned}\n\\beta_{02} &= \\beta_{01} + d_0 \\\\\n\\beta_{12} &= \\beta_{11} + d_1 \\\\\n\\alpha_{2} &= \\alpha_{1} + d_a\n\\end{aligned}\n\\]can implement following function:Starting values easy now fit model individually., increment parameters, \\(d_1\\),\\(d_2\\),\\(d_a\\) significantly different 0, suggesting two models .","code":"\n# Grid search\n# choose grid of a and b values\naseq = seq(10,18,.2)\nbseq = seq(.001,.075,.001)\n\nna = length(aseq)\nnb = length(bseq)\nSSout = matrix(0,na*nb,3) #matrix to save output\ncnt = 0\nfor (k in 1:na){\n   for (j in 1:nb){\n      cnt = cnt+1\n      ypred = mod(aseq[k],bseq[j],x) #evaluate model w/ these parms\n      ss = sum((y-ypred)^2)  #this is our SSE objective function\n      #save values of a, b, and SSE\n      SSout[cnt,1]=aseq[k]\n      SSout[cnt,2]=bseq[j]\n      SSout[cnt,3]=ss\n   }\n}\n#find minimum SSE and associated a,b values\nmn_indx = which.min(SSout[,3])\nastrt = SSout[mn_indx,1]\nbstrt = SSout[mn_indx,2]\n#now, run nls function with these starting values\nnlin_modG=nls(y~mod(a,b,x),start=list(a=astrt,b=bstrt)) \n\nnlin_modG\n#> Nonlinear regression model\n#>   model: y ~ mod(a, b, x)\n#>    data: parent.frame()\n#>        a        b \n#> 13.60391  0.01911 \n#>  residual sum-of-squares: 235.5\n#> \n#> Number of iterations to convergence: 3 \n#> Achieved convergence tolerance: 2.293e-07\n# Note, the package `nls_multstart` will allow you \n# to do a grid search without programming your own loop\nplotFit(\n  nlin_modG,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = datf\n)  \napropos(\"^SS\")\n#>  [1] \"ss\"          \"SSasymp\"     \"SSasympOff\"  \"SSasympOrig\" \"SSbiexp\"    \n#>  [6] \"SSD\"         \"SSfol\"       \"SSfpl\"       \"SSgompertz\"  \"SSlogis\"    \n#> [11] \"SSmicmen\"    \"SSout\"       \"SSweibull\"\n# simulated data\ntime <- c(1, 2, 3, 5, 10, 15, 20, 25, 30, 35)\npopulation <-\n    c(2.8, 4.2, 3.5, 6.3, 15.7, 21.3, 23.7, 25.1, 25.8, 25.9)\nplot(time, population, las = 1, pch = 16)\n\n# model fitting\nlogisticModelSS <- nls(population ~ SSlogis(time, Asym, xmid, scal))\nsummary(logisticModelSS)\n#> \n#> Formula: population ~ SSlogis(time, Asym, xmid, scal)\n#> \n#> Parameters:\n#>      Estimate Std. Error t value Pr(>|t|)    \n#> Asym  25.5029     0.3666   69.56 3.34e-11 ***\n#> xmid   8.7347     0.3007   29.05 1.48e-08 ***\n#> scal   3.6353     0.2186   16.63 6.96e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 1.908e-06\ncoef(logisticModelSS)\n#>      Asym      xmid      scal \n#> 25.502890  8.734698  3.635333\n#convert to other parameterization\nKs = as.numeric(coef(logisticModelSS)[1])\nrs = -1 / as.numeric(coef(logisticModelSS)[3])\nPos = -rs * as.numeric(coef(logisticModelSS)[2])\n#let's refit with these parameters\nlogisticModel <-\n    nls(population ~ K / (1 + exp(Po + r * time)),\n        start = list(Po = Pos, r = rs, K = Ks))\nsummary(logisticModel)\n#> \n#> Formula: population ~ K/(1 + exp(Po + r * time))\n#> \n#> Parameters:\n#>    Estimate Std. Error t value Pr(>|t|)    \n#> Po  2.40272    0.12702   18.92 2.87e-07 ***\n#> r  -0.27508    0.01654  -16.63 6.96e-07 ***\n#> K  25.50289    0.36665   69.56 3.34e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6528 on 7 degrees of freedom\n#> \n#> Number of iterations to convergence: 0 \n#> Achieved convergence tolerance: 1.924e-06\n#note: initial values =  solution (highly unusual, but ok)\nplot(time, population, las = 1, pch = 16)\nlines(time, predict(logisticModel), col = \"red\")\n#Load data\ndat <- read.table(\"images/dat.txt\", header = T)\n# plot\ndat.plot <-\n    ggplot(dat) + geom_point(aes(\n        x = no3,\n        y = ryp,\n        color = as.factor(depth)\n    )) +\n    labs(color = 'Depth (cm)') + xlab('Soil NO3') + \n    ylab('relative yield percent')\n\ndat.plot\n#First define model as a function\nnonlinModel <- function(predictor, b0, b1, alpha) {\n    ifelse(predictor <= alpha,\n           #if observation less than cutoff simple linear model\n           b0 + b1 * predictor, \n           b0 + b1 * alpha) #otherwise flat line\n}\nnonlinModelInit <- function(mCall, LHS, data) {\n    # sort data by increasing predictor value -\n    # done so we can just use the low level \n    # no3 conc to fit a simple model\n    xy <- sortedXyData(mCall[['predictor']], LHS, data)\n    n <- nrow(xy)\n    #For the first half of the data a simple linear model is fit\n    lmFit <- lm(xy[1:(n / 2), 'y'] ~ xy[1:(n / 2), 'x'])\n    b0 <- coef(lmFit)[1]\n    b1 <- coef(lmFit)[2]\n    # for the cut off to the flat part select the last \n    # x value used in creating linear model\n    alpha <- xy[(n / 2), 'x']\n    value <- c(b0, b1, alpha)\n    names(value) <- mCall[c('b0', 'b1', 'alpha')]\n    value\n}\nSS_nonlinModel <- selfStart(nonlinModel,nonlinModelInit,c('b0','b1','alpha'))\n# Above code defined model and selfStart now just need to call it for each of the depths\nsep30_nls <-\n    nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n        data = dat[dat$depth == 30,])\n\nsep60_nls <-\n    nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha),\n        data = dat[dat$depth == 60,])\n\npar(mfrow = c(1, 2))\nplotFit(\n    sep30_nls,\n    interval = \"both\",\n    pch = 19,\n    shade = TRUE,\n    col.conf = \"skyblue4\",\n    col.pred = \"lightskyblue2\",\n    data = dat[dat$depth == 30,],\n    main = 'Results 30 cm depth',\n    ylab = 'relative yield percent',\n    xlab = 'Soil NO3 concentration',\n    xlim = c(0, 120)\n)\nplotFit(\n    sep60_nls,\n    interval = \"both\",\n    pch = 19,\n    shade = TRUE,\n    col.conf = \"lightpink4\",\n    col.pred = \"lightpink2\",\n    data = dat[dat$depth == 60,],\n    main = 'Results 60 cm depth',\n    ylab = 'relative yield percent',\n    xlab = 'Soil NO3 concentration',\n    xlim = c(0, 120)\n)\nsummary(sep30_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0     15.1943     2.9781   5.102 6.89e-07 ***\n#> b1      3.5760     0.1853  19.297  < 2e-16 ***\n#> alpha  23.1324     0.5098  45.373  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.258 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 6 \n#> Achieved convergence tolerance: 3.608e-09\nsummary(sep60_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      5.4519     2.9785    1.83   0.0684 .  \n#> b1      5.6820     0.2529   22.46   <2e-16 ***\n#> alpha  16.2863     0.2818   57.80   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.427 on 237 degrees of freedom\n#> \n#> Number of iterations to convergence: 5 \n#> Achieved convergence tolerance: 8.571e-09\nred_nls <-\n  nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), \n      data = dat)\n\nsummary(red_nls)\n#> \n#> Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n#> \n#> Parameters:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> b0      8.7901     2.7688   3.175   0.0016 ** \n#> b1      4.8995     0.2207  22.203   <2e-16 ***\n#> alpha  18.0333     0.3242  55.630   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.13 on 477 degrees of freedom\n#> \n#> Number of iterations to convergence: 7 \n#> Achieved convergence tolerance: 7.126e-09\n\npar(mfrow = c(1, 1))\nplotFit(\n  red_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightblue4\",\n  col.pred = \"lightblue2\",\n  data = dat,\n  main = 'Results combined',\n  ylab = 'relative yield percent',\n  xlab = 'Soil NO3 concentration'\n)\nlibrary(nlstools)\n# using nlstools nlsResiduals function to get some quick residual plots\n# can also use test.nlsResiduals(resid)\n# https://www.rdocumentation.org/packages/nlstools/versions/1.0-2\nresid <- nlsResiduals(red_nls)\nplot(resid)\nnonlinModelF <- function(predictor,soildep,b01,b11,a1,d0,d1,da){\n   b02 = b01 + d0 #make 60cm parms = 30cm parms + increment\n   b12 = b11 + d1\n   a2 = a1 + da\n   \n   y1 = ifelse(predictor<=a1, \n        #if observation less than cutoff simple linear model\n         b01+b11*predictor, \n         b01+b11*a1) # otherwise flat line\n   y2 = ifelse(predictor<=a2, \n               b02+b12*predictor, \n               b02+b12*a2) \n   y =  y1*(soildep == 30) + y2*(soildep == 60)  #combine models\n   return(y)\n}\nSoil_full = nls(\n    ryp ~ nonlinModelF(\n        predictor = no3,\n        soildep = depth,\n        b01,\n        b11,\n        a1,\n        d0,\n        d1,\n        da\n    ),\n    data = dat,\n    start = list(\n        b01 = 15.2,\n        b11 = 3.58,\n        a1 = 23.13,\n        d0 = -9.74,\n        d1 = 2.11,\n        da = -6.85\n    )\n)\n\nsummary(Soil_full)\n#> \n#> Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, \n#>     a1, d0, d1, da)\n#> \n#> Parameters:\n#>     Estimate Std. Error t value Pr(>|t|)    \n#> b01  15.1943     2.8322   5.365 1.27e-07 ***\n#> b11   3.5760     0.1762  20.291  < 2e-16 ***\n#> a1   23.1324     0.4848  47.711  < 2e-16 ***\n#> d0   -9.7424     4.2357  -2.300   0.0219 *  \n#> d1    2.1060     0.3203   6.575 1.29e-10 ***\n#> da   -6.8461     0.5691 -12.030  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.854 on 474 degrees of freedom\n#> \n#> Number of iterations to convergence: 1 \n#> Achieved convergence tolerance: 3.742e-06"},{"path":"non-linear-regression.html","id":"modelestimation-adequacy","chapter":"6 Non-linear Regression","heading":"6.2.3 Model/Estimation Adequacy","text":"(Bates Watts 1980) assess nonlinearity terms 2 components curvature:Intrinsic nonlinearity: degree bending twisting \\(f(\\theta)\\); estimation approach assumes true function relatively flat (planar) neighborhood fo \\(\\hat{\\theta}\\), true \\(f()\\) lot “bending” neighborhood \\(\\hat{\\theta}\\) (independent parameterization)\nbad, distribution residuals seriously distorted\nslow converge\ndifficult identify ( use function rms.curve)\nSolution:\nuse higher order Taylor expansions estimation\nBayesian method\n\nIntrinsic nonlinearity: degree bending twisting \\(f(\\theta)\\); estimation approach assumes true function relatively flat (planar) neighborhood fo \\(\\hat{\\theta}\\), true \\(f()\\) lot “bending” neighborhood \\(\\hat{\\theta}\\) (independent parameterization)bad, distribution residuals seriously distortedIf bad, distribution residuals seriously distortedslow convergeslow convergedifficult identify ( use function rms.curve)difficult identify ( use function rms.curve)Solution:\nuse higher order Taylor expansions estimation\nBayesian method\nSolution:use higher order Taylor expansions estimationBayesian methodParameter effects nonlinearity: degree curvature (nonlinearity) affected choice \\(\\theta\\) (data dependent; dependent parameterization)\nleads problems inference \\(\\hat{\\theta}\\)\nrms.curve MASS can identify\nbootstrap-based inference can also used\nSolution: try reparaemterize.\nParameter effects nonlinearity: degree curvature (nonlinearity) affected choice \\(\\theta\\) (data dependent; dependent parameterization)leads problems inference \\(\\hat{\\theta}\\)rms.curve MASS can identifybootstrap-based inference can also usedSolution: try reparaemterize.linear model, Linear Regression, goodness fit measure \\(R^2\\):\\[\n\\begin{aligned}\nR^2 &= \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO} \\\\\n&= \\frac{\\sum_{=1}^n (\\hat{Y}_i- \\bar{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2} = 1- \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\end{aligned}\n\\]valid nonlinear case error sum squares model sum squares add total corrected sum squares\\[\nSSR + SSE \\neq SST\n\\]can use pseudo-\\(R^2\\):\\[\nR^2_{pseudo} = 1 - \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\]can’t interpret proportion variability explained model. use relative comparison different models.Residual Plots: standardize, similar OLS. useful intrinsic curvature small:studentized residuals\\[\nr_i = \\frac{e_i}{s\\sqrt{1-\\hat{c}_i}}\n\\]\\(\\hat{c}_i\\)-th diagonal \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\)problems ofCollinearity: condition number \\(\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\\) less 30. Follow (Magel Hertsgaard 1987); reparameterize possibleCollinearity: condition number \\(\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\\) less 30. Follow (Magel Hertsgaard 1987); reparameterize possibleLeverage: Like OLS, consider \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\) (also known “tangent plant hat matrix”) (St Laurent Cook 1992)Leverage: Like OLS, consider \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\) (also known “tangent plant hat matrix”) (St Laurent Cook 1992)Heterogeneous Errors: weighted Non-linear Least SquaresHeterogeneous Errors: weighted Non-linear Least SquaresCorrelated Errors:\nGeneralized Nonlinear Least Squares\nNonlinear Mixed Models\nBayesian methods\nCorrelated Errors:Generalized Nonlinear Least SquaresNonlinear Mixed ModelsBayesian methods","code":"\n#check parameter effects and intrinsic curvature\n\nmodD = deriv3(~ a*exp(b*x), c(\"a\",\"b\"),function(a,b,x) NULL)\n\nnlin_modD = nls(y ~ modD(a, b, x),\n                start = list(a = astrt, b = bstrt),\n                data = datf)\n\nrms.curv(nlin_modD)\n#> Parameter effects: c^theta x sqrt(F) = 0.0626 \n#>         Intrinsic: c^iota  x sqrt(F) = 0.0062"},{"path":"non-linear-regression.html","id":"application-1","chapter":"6 Non-linear Regression","heading":"6.2.4 Application","text":"\\[\ny_i = \\frac{\\theta_0 + \\theta_1 x_i}{1 + \\theta_2 \\exp(0.4 x_i)} + \\epsilon_i\n\\]\\(= 1,..,n\\)Get starting valuesWe notice \\(Y_{max} = \\theta_0 + \\theta_1 x_i\\) can find x_i datahence, \\(x = 0.0094\\) \\(y = 2.6722\\) first equation \\[\n2.6722 = \\theta_0 + 0.0094 \\theta_1\n\\]\\[\n\\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722\n\\]Secondly, notice can obtain “average” y \\[\n1+ \\theta_2 exp(0.4 x) = 2\n\\]can find average numbers x ywe second equation\\[\n1 + \\theta_2 exp(0.4*11.0648) = 2\n\\]\\[\n0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1\n\\]Thirdly, can plug value x closest 1 find value yhence \\[\n1.457 = \\frac{\\theta_0 + \\theta_1*0.9895}{1 + \\theta_2 exp(0.4*0.9895)}\n\\]\\[\n1.457 + 2.164479 *\\theta_2 = \\theta_0 + \\theta_1*0.9895\n\\]\\[\n\\theta_0 + \\theta_1*0.9895 -  2.164479 *\\theta_2 = 1.457\n\\]3 equations, can solve get starting value \\(\\theta_0,\\theta_1, \\theta_2\\)\\[\n\\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722\n\\]\\[\n0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1\n\\]\\[\n\\theta_0 + \\theta_1*0.9895 -  2.164479 *\\theta_2 = 1.457\n\\]Construct manually Gauss-Newton AlgorithmAfter 8 iterations, function converged. objective function value convergence isand parameters \\(\\theta\\)s areand asymptotic variance covariance matrix isIssue encounter problem sensitive starting values. tried value 1 \\(\\theta\\)s, vastly different parameter estimates. , try use model interpretation try find reasonable starting values.Check predefined function nls","code":"\nplot(my_data)\nmax(my_data$y)\n#> [1] 2.6722\nmy_data$x[which.max(my_data$y)]\n#> [1] 0.0094\n# find mean y\nmean(my_data$y) \n#> [1] -0.0747864\n\n# find y closest to its mean\nmy_data$y[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] -0.0773\n\n\n# find x closest to the mean y\nmy_data$x[which.min(abs(my_data$y - (mean(my_data$y))))] \n#> [1] 11.0648\n# find value of x closet to 1\nmy_data$x[which.min(abs(my_data$x - 1))] \n#> [1] 0.9895\n\n# find index of x closest to 1\nmatch(my_data$x[which.min(abs(my_data$x - 1))], my_data$x) \n#> [1] 14\n\n# find y value\nmy_data$y[match(my_data$x[which.min(abs(my_data$x - 1))], my_data$x)]\n#> [1] 1.4577\nlibrary(matlib)\nA = matrix(\n    c(0, 0.0094, 0, 0, 0, 83.58967, 1, 0.9895,-2.164479),\n    nrow = 3,\n    ncol = 3,\n    byrow = T\n)\nb = c(2.6722, 1, 1.457)\nshowEqn(A, b)\n#> 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 \n#> 0*x1      + 0*x2 + 83.58967*x3  =       1 \n#> 1*x1 + 0.9895*x2 - 2.164479*x3  =   1.457\nSolve(A, b, fractions = F)\n#> x1      =  -279.80879739 \n#>   x2    =   284.27659574 \n#>     x3  =      0.0119632\n#starting value\ntheta_0_strt = -279.80879739\ntheta_1_strt =  284.27659574\ntheta_2_strt = 0.0119632\n\n#model\nmod_4 = function(theta_0, theta_1, theta_2, x) {\n    (theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x))\n}\n\n#define a function\nf_4 = expression((theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x)))\n\n#take the first derivative\ndf_4.d_theta_0 = D(f_4, 'theta_0')\n\ndf_4.d_theta_1 = D(f_4, 'theta_1')\n\ndf_4.d_theta_2 = D(f_4, 'theta_2')\n\n# save the result of all iterations\ntheta_vec = matrix(c(theta_0_strt, theta_1_strt, theta_2_strt))\ndelta = matrix(NA, nrow = 3, ncol = 1)\n\nf_theta = as.matrix(eval(\n    f_4,\n    list(\n        x = my_data$x,\n        theta_0 = theta_vec[1, 1],\n        theta_1 = theta_vec[2, 1],\n        theta_2 = theta_vec[3, 1]\n    )\n))\n\ni = 1\n\nrepeat {\n    F_theta_0 = as.matrix(cbind(\n        eval(\n            df_4.d_theta_0,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        ),\n        eval(\n            df_4.d_theta_1,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        ),\n        eval(\n            df_4.d_theta_2,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        )\n    ))\n    delta[, i] = (solve(t(F_theta_0) %*% F_theta_0)) %*% t(F_theta_0) %*% (my_data$y - f_theta[, i])\n    theta_vec = cbind(theta_vec, matrix(NA, nrow = 3, ncol = 1))\n    theta_vec[, i + 1] = theta_vec[, i] + delta[, i]\n    i = i + 1\n    \n    f_theta = cbind(f_theta, as.matrix(eval(\n        f_4,\n        list(\n            x = my_data$x,\n            theta_0 = theta_vec[1, i],\n            theta_1 = theta_vec[2, i],\n            theta_2 = theta_vec[3, i]\n        )\n    )))\n    delta = cbind(delta, matrix(NA, nrow = 3, ncol = 1))\n    \n    #convergence criteria based on SSE\n    if (abs(sum((my_data$y - f_theta[, i]) ^ 2) - sum((my_data$y - f_theta[, i - 1]) ^ 2)) / (sum((my_data$y - f_theta[, i - 1]) ^ 2)) < 0.001) {\n        break\n    }\n}\ndelta\n#>               [,1]        [,2]        [,3]       [,4]       [,5]       [,6]\n#> [1,]  2.811840e+02 -0.03929013  0.43160654  0.6904856  0.6746748  0.4056460\n#> [2,] -2.846545e+02  0.03198446 -0.16403964 -0.2895487 -0.2933345 -0.1734087\n#> [3,] -1.804567e-05  0.01530258  0.05137285  0.1183271  0.1613129  0.1160404\n#>             [,7] [,8]\n#> [1,]  0.09517681   NA\n#> [2,] -0.03928239   NA\n#> [3,]  0.03004911   NA\ntheta_vec\n#>              [,1]        [,2]        [,3]        [,4]       [,5]       [,6]\n#> [1,] -279.8087974  1.37521388  1.33592375  1.76753029  2.4580158  3.1326907\n#> [2,]  284.2765957 -0.37788712 -0.34590266 -0.50994230 -0.7994910 -1.0928255\n#> [3,]    0.0119632  0.01194515  0.02724773  0.07862059  0.1969477  0.3582607\n#>            [,7]       [,8]\n#> [1,]  3.5383367  3.6335135\n#> [2,] -1.2662342 -1.3055166\n#> [3,]  0.4743011  0.5043502\n\nhead(f_theta)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n#> [1,] -273.8482 1.355410 1.297194 1.633802 2.046023 2.296554 2.389041 2.404144\n#> [2,] -209.0859 1.268192 1.216738 1.514575 1.863098 2.059505 2.126009 2.135969\n#> [3,] -190.3323 1.242916 1.193433 1.480136 1.810629 1.992095 2.051603 2.060202\n#> [4,] -177.1891 1.225196 1.177099 1.456024 1.774000 1.945197 1.999945 2.007625\n#> [5,] -148.5872 1.186618 1.141549 1.403631 1.694715 1.844154 1.888953 1.894730\n#> [6,] -119.9585 1.147980 1.105961 1.351301 1.615968 1.744450 1.779859 1.783866\n\n# estimate sigma^2\n\nsigma2 = 1 / (nrow(my_data) - 3) * (t(my_data$y - (f_theta[, ncol(f_theta)]))) %*%\n    (my_data$y - (f_theta[, ncol(f_theta)])) # p = 3\nsigma2\n#>           [,1]\n#> [1,] 0.0801686\nsum((my_data$y - f_theta[,i])^2)\n#> [1] 19.80165\ntheta_vec[,ncol(theta_vec)]\n#> [1]  3.6335135 -1.3055166  0.5043502\nas.numeric(sigma2)*as.matrix(solve(crossprod(F_theta_0)))\n#>             [,1]        [,2]        [,3]\n#> [1,]  0.11552571 -0.04817428  0.02685848\n#> [2,] -0.04817428  0.02100861 -0.01158212\n#> [3,]  0.02685848 -0.01158212  0.00703916\nnlin_4 = nls(\n    y ~ mod_4(theta_0, theta_1, theta_2, x),\n    start = list(\n        theta_0 = -279.80879739 ,\n        theta_1 = 284.27659574 ,\n        theta_2 = 0.0119632\n    ),\n    data = my_data\n)\nnlin_4\n#> Nonlinear regression model\n#>   model: y ~ mod_4(theta_0, theta_1, theta_2, x)\n#>    data: my_data\n#> theta_0 theta_1 theta_2 \n#>  3.6359 -1.3064  0.5053 \n#>  residual sum-of-squares: 19.8\n#> \n#> Number of iterations to convergence: 9 \n#> Achieved convergence tolerance: 2.294e-07"},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"7 Generalized Linear Models","heading":"7 Generalized Linear Models","text":"Even though call generalized linear model, still paradigm non-linear regression, form regression model non-linear. name generalized linear model derived fact \\(\\mathbf{x'_i \\beta}\\) (linear form) model.","code":""},{"path":"generalized-linear-models.html","id":"logistic-regression-1","chapter":"7 Generalized Linear Models","heading":"7.1 Logistic Regression","text":"\\[\np_i = f(\\mathbf{x}_i ; \\beta) = \\frac{exp(\\mathbf{x_i'\\beta})}{1 + exp(\\mathbf{x_i'\\beta})}\n\\]Equivalently,\\[\nlogit(p_i) = log(\\frac{p_i}{1+p_i}) = \\mathbf{x_i'\\beta}\n\\]\\(\\frac{p_i}{1+p_i}\\)odds.form, model specified function mean response linear. Hence, Generalized Linear ModelsThe likelihood function\\[\nL(p_i) = \\prod_{=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}\n\\]\\(p_i = \\frac{\\mathbf{x'_i \\beta}}{1+\\mathbf{x'_i \\beta}}\\) \\(1-p_i = (1+ exp(\\mathbf{x'_i \\beta}))^{-1}\\)Hence, objective function \\[\nQ(\\beta) = log(L(\\beta)) = \\sum_{=1}^n Y_i \\mathbf{x'_i \\beta} - \\sum_{=1}^n  log(1+ exp(\\mathbf{x'_i \\beta}))\n\\]maximize function numerically using optimization method , allows us find numerical MLE \\(\\hat{\\beta}\\). can use standard asymptotic properties MLEs make inference.Property MLEs parameters asymptotically unbiased sample variance-covariance matrix given inverse Fisher information matrix\\[\n\\hat{\\beta} \\dot{\\sim} (\\beta,[\\mathbf{}(\\beta)]^{-1})\n\\]Fisher Information matrix, \\(\\mathbf{}(\\beta)\\) \\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= E[\\frac{\\partial \\log(L(\\beta))}{\\partial (\\beta)}\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta'}] \\\\\n&= E[(\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_i} \\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]regularity conditions, equivalent negative expected value Hessian Matrix\\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= -E[\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta \\partial \\beta'}] \\\\\n&= -E[(\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta_i \\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]Example:\\[\nx_i' \\beta = \\beta_0 + \\beta_1 x_i\n\\]\\[\n\\begin{aligned}\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_0} &= \\sum_{=1}^n \\frac{\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_1} &= \\sum_{=1}^n \\frac{x_i^2\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{x_i\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_i^2p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta_0 \\partial \\beta_1} &= \\sum_{=1}^n \\frac{x_i\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - x_i[\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_ip_i (1-p_i)\n\\end{aligned}\n\\]Hence,\\[\n\\mathbf{} (\\beta) =\n\\left[\n\\begin{array}\n{cc}\n\\sum_i p_i(1-p_i) & \\sum_i x_i p_i(1-p_i) \\\\\n\\sum_i x_i p_i(1-p_i) & \\sum_i x_i^2 p_i(1-p_i)\n\\end{array}\n\\right]\n\\]InferenceLikelihood Ratio TestsTo formulate test, let \\(\\beta = [\\beta_1', \\beta_2']'\\). interested testing hypothesis \\(\\beta_1\\), leave \\(\\beta_2\\) unspecified (called nuisance parameters). \\(\\beta_1\\) \\(\\beta_2\\) can either vector scalar, \\(\\beta_2\\) can null.Example: \\(H_0: \\beta_1 = \\beta_{1,0}\\) (\\(\\beta_{1,0}\\) specified) \\(\\hat{\\beta}_{2,0}\\) MLE \\(\\beta_2\\) restriction \\(\\beta_1 = \\beta_{1,0}\\). likelihood ratio test statistic \\[\n-2\\log\\Lambda = -2[\\log(L(\\beta_{1,0},\\hat{\\beta}_{2,0})) - \\log(L(\\hat{\\beta}_1,\\hat{\\beta}_2))]\n\\]wherethe first term value fo likelihood fitted restricted modelthe second term likelihood value fitted unrestricted modelUnder null,\\[\n-2 \\log \\Lambda \\sim \\chi^2_{\\upsilon}\n\\]\\(\\upsilon\\) dimension \\(\\beta_1\\)reject null \\(-2\\log \\Lambda > \\chi_{\\upsilon,1-\\alpha}^2\\)Wald StatisticsBased \\[\n\\hat{\\beta} \\sim (\\beta, [\\mathbf{}(\\beta)^{-1}])\n\\]\\[\nH_0: \\mathbf{L}\\hat{\\beta} = 0\n\\]\\(\\mathbf{L}\\) \\(q \\times p\\) matrix \\(q\\) linearly independent rows. \\[\nW = (\\mathbf{L\\hat{\\beta}})'(\\mathbf{L[(\\hat{\\beta})]^{-1}L'})^{-1}(\\mathbf{L\\hat{\\beta}})\n\\]null hypothesisConfidence interval\\[\n\\hat{\\beta}_i \\pm 1.96 \\hat{s}_{ii}^2\n\\]\\(\\hat{s}_{ii}^2\\) -th diagonal \\(\\mathbf{[(\\hat{\\beta})]}^{-1}\\)havelarge sample size, likelihood ratio Wald tests similar results.small sample size, likelihood ratio test better.Logistic Regression: Interpretation \\(\\beta\\)single regressor, model \\[\nlogit\\{\\hat{p}_{x_i}\\} \\equiv logit (\\hat{p}_i) = \\log(\\frac{\\hat{p}_i}{1 - \\hat{p}_i}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\\(x= x_i + 1\\)\\[\nlogit\\{\\hat{p}_{x_i +1}\\} = \\hat{\\beta}_0 + \\hat{\\beta}(x_i + 1) = logit\\{\\hat{p}_{x_i}\\} + \\hat{\\beta}_1\n\\],\\[\n\\begin{aligned}\nlogit\\{\\hat{p}_{x_i +1}\\} - logit\\{\\hat{p}_{x_i}\\} &= log\\{odds[\\hat{p}_{x_i +1}]\\} - log\\{odds[\\hat{p}_{x_i}]\\} \\\\\n&= log(\\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}) = \\hat{\\beta}_1\n\\end{aligned}\n\\]\\[\nexp(\\hat{\\beta}_1) = \\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}\n\\]estimated odds ratiothe estimated odds ratio, difference c units regressor x, \\(exp(c\\hat{\\beta}_1)\\). multiple covariates, \\(exp(\\hat{\\beta}_k)\\) estimated odds ratio variable \\(x_k\\), assuming variables held constant.Inference Mean ResponseLet \\(x_h = (1, x_{h1}, ...,x_{h,p-1})'\\). \\[\n\\hat{p}_h = \\frac{exp(\\mathbf{x'_h \\hat{\\beta}})}{1 + exp(\\mathbf{x'_h \\hat{\\beta}})}\n\\]\\(s^2(\\hat{p}_h) = \\mathbf{x'_h[(\\hat{\\beta})]^{-1}x_h}\\)new observation, can cutoff point decide whether y = 0 1.","code":""},{"path":"generalized-linear-models.html","id":"application-2","chapter":"7 Generalized Linear Models","heading":"7.1.1 Application","text":"Logistic Regression\\(x \\sim Unif(-0.5,2.5)\\). \\(\\eta = 0.5 + 0.75 x\\)Passing \\(\\eta\\)’s inverse-logit function, get\\[\np = \\frac{\\exp(\\eta)}{1+ \\exp(\\eta)}\n\\]\\(p \\[0,1]\\), generate \\(y \\sim Bernoulli(p)\\)Model FitBased odds ratio, \\(x = 0\\) , odds success 1.59\\(x = 1\\), odds success increase factor 2.19 (.e., 119.29% increase).Deviance Tests\\(H_0\\): variables related response (.e., model just intercept)\\(H_1\\): least one variable related responseSince see p-value 0, reject null variables related responseDeviance residualsHowever, plot informative. Hence, can can see residuals plots grouped bins based prediction values.can also see predicted value residuals.can also look binned plot logistic prediction versus true categoryFormal deviance testHosmer-Lemeshow testNull hypothesis: observed events match expected evens\\[\nX^2_{HL} = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{p}_j)^2}{m_j \\hat{p}_j(1-\\hat{p}_j)}\n\\]wherewithin j-th bin, \\(y_j\\) number successes\\(m_j\\) = number observations\\(\\hat{p}_j\\) = predicted probabilityUnder null hypothesis, \\(X^2_{HLL} \\sim \\chi^2_{J-1}\\)Since \\(p\\)-value = 0.99, reject null hypothesis (.e., model fitting well).","code":"\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(pscl)\nlibrary(ggplot2)\nlibrary(faraway)\nlibrary(nnet)\nlibrary(agridat)\nlibrary(nlstools)\nset.seed(23) #set seed for reproducibility\nx <- runif(1000, min = -0.5, max = 2.5)\neta1 <- 0.5 + 0.75 * x\np <- exp(eta1) / (1 + exp(eta1))\ny <- rbinom(1000, 1, p)\nBinData <- data.frame(X = x, Y = y)\nLogistic_Model <- glm(formula = Y ~ X,\n                      family = binomial, # family = specifies the response distribution\n                      data = BinData)\nsummary(Logistic_Model)\n#> \n#> Call:\n#> glm(formula = Y ~ X, family = binomial, data = BinData)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2317   0.4153   0.5574   0.7922   1.1469  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***\n#> X            0.78527    0.09296   8.447  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1106.7  on 999  degrees of freedom\n#> Residual deviance: 1027.4  on 998  degrees of freedom\n#> AIC: 1031.4\n#> \n#> Number of Fisher Scoring iterations: 4\nnlstools::confint2(Logistic_Model)\n#>                 2.5 %    97.5 %\n#> (Intercept) 0.2618709 0.6622204\n#> X           0.6028433 0.9676934\nOddsRatio <- coef(Logistic_Model) %>% exp\nOddsRatio \n#> (Intercept)           X \n#>    1.587318    2.192995\nTest_Dev <- Logistic_Model$null.deviance - Logistic_Model$deviance\np_val_dev <- 1 - pchisq(q = Test_Dev, df = 1)\nLogistic_Resids <- residuals(Logistic_Model, type = \"deviance\")\nplot(\n    y = Logistic_Resids,\n    x = BinData$X,\n    xlab = 'X',\n    ylab = 'Deviance Resids'\n)\nplot_bin <- function(Y,\n                     X,\n                     bins = 100,\n                     return.DF = FALSE) {\n    Y_Name <- deparse(substitute(Y))\n    X_Name <- deparse(substitute(X))\n    Binned_Plot <- data.frame(Plot_Y = Y, Plot_X = X)\n    Binned_Plot$bin <-\n        cut(Binned_Plot$Plot_X, breaks = bins) %>% as.numeric\n    Binned_Plot_summary <- Binned_Plot %>%\n        group_by(bin) %>%\n        summarise(\n            Y_ave = mean(Plot_Y),\n            X_ave = mean(Plot_X),\n            Count = n()\n        ) %>% as.data.frame\n    plot(\n        y = Binned_Plot_summary$Y_ave,\n        x = Binned_Plot_summary$X_ave,\n        ylab = Y_Name,\n        xlab = X_Name\n    )\n    if (return.DF)\n        return(Binned_Plot_summary)\n}\n\n\nplot_bin(Y = Logistic_Resids,\n         X = BinData$X,\n         bins = 100)\nLogistic_Predictions <- predict(Logistic_Model, type = \"response\")\nplot_bin(Y = Logistic_Resids, X = Logistic_Predictions, bins = 100)\nNumBins <- 10\nBinned_Data <- plot_bin(\n    Y = BinData$Y,\n    X = Logistic_Predictions,\n    bins = NumBins,\n    return.DF = TRUE\n)\nBinned_Data\n#>    bin     Y_ave     X_ave Count\n#> 1    1 0.5833333 0.5382095    72\n#> 2    2 0.5200000 0.5795887    75\n#> 3    3 0.6567164 0.6156540    67\n#> 4    4 0.7014925 0.6579674    67\n#> 5    5 0.6373626 0.6984765    91\n#> 6    6 0.7500000 0.7373341    72\n#> 7    7 0.7096774 0.7786747    93\n#> 8    8 0.8503937 0.8203819   127\n#> 9    9 0.8947368 0.8601232   133\n#> 10  10 0.8916256 0.9004734   203\nabline(0, 1, lty = 2, col = 'blue')\nHL_BinVals <-\n    (Binned_Data$Count * Binned_Data$Y_ave - Binned_Data$Count * Binned_Data$X_ave) ^ 2 /   Binned_Data$Count * Binned_Data$X_ave * (1 - Binned_Data$X_ave)\nHLpval <- pchisq(q = sum(HL_BinVals),\n                 df = NumBins,\n                 lower.tail = FALSE)\nHLpval\n#> [1] 0.9999989"},{"path":"generalized-linear-models.html","id":"probit-regression","chapter":"7 Generalized Linear Models","heading":"7.2 Probit Regression","text":"\\[\nE(Y_i) = p_i = \\Phi(\\mathbf{x_i'\\theta})\n\\]\\(\\Phi()\\) CDF \\(N(0,1)\\) random variable.models (e..g, t–distribution; log-log; complimentary log-log)let \\(Y_i = 1\\) success, \\(Y_i =0\\) success.assume \\(Y \\sim Ber\\) \\(p_i = P(Y_i =1)\\), success probability.assume \\(Y \\sim Ber\\) \\(p_i = P(Y_i =1)\\), success probability.consider logistic regression response function \\(logit(p_i) = x'_i \\beta\\)consider logistic regression response function \\(logit(p_i) = x'_i \\beta\\)Confusion matrixSensitivity: ability identify positive results\\[\n\\text{Sensitivity} = \\frac{TP}{TP + FN}\n\\]Specificity: ability identify negative results\\[\n\\text{Specificity} = \\frac{TN}{TN + FP}\n\\]False positive rate: Type error (1- specificity)\\[\n\\text{ False Positive Rate} = \\frac{FP}{TN+ FP}\n\\]False Negative Rate: Type II error (1-sensitivity)\\[\n\\text{False Negative Rate} = \\frac{FN}{TP + FN}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.3 Binomial Regression","text":"BinomialHere, cancer case = successes, control case = failures.","code":"\ndata(\"esoph\")\nhead(esoph, n = 3)\n#>   agegp     alcgp    tobgp ncases ncontrols\n#> 1 25-34 0-39g/day 0-9g/day      0        40\n#> 2 25-34 0-39g/day    10-19      0        10\n#> 3 25-34 0-39g/day    20-29      0         6\nplot(\n  esoph$ncases / (esoph$ncases + esoph$ncontrols) ~ esoph$alcgp,\n  ylab = \"Proportion\",\n  xlab = 'Alcohol consumption',\n  main = 'Esophageal Cancer data'\n)\nclass(esoph$agegp) <- \"factor\"\nclass(esoph$alcgp) <- \"factor\"\nclass(esoph$tobgp) <- \"factor\"\n#  only the alcohol consumption as a predictor\nmodel <- glm(cbind(ncases, ncontrols) ~ alcgp, data = esoph, family = binomial)\nsummary(model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -4.0759  -1.2037  -0.0183   1.0928   3.7336  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.5885     0.1925 -13.444  < 2e-16 ***\n#> alcgp40-79    1.2712     0.2323   5.472 4.46e-08 ***\n#> alcgp80-119   2.0545     0.2611   7.868 3.59e-15 ***\n#> alcgp120+     3.3042     0.3237  10.209  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 221.46  on 84  degrees of freedom\n#> AIC: 344.51\n#> \n#> Number of Fisher Scoring iterations: 5\n#Coefficient Odds\ncoefficients(model) %>% exp\n#> (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ \n#>  0.07512953  3.56527094  7.80261593 27.22570533\ndeviance(model)/df.residual(model)\n#> [1] 2.63638\nmodel$aic\n#> [1] 344.5109\n# alcohol consumption and age as predictors\nbetter_model <-\n    glm(cbind(ncases, ncontrols) ~ agegp + alcgp,\n        data = esoph,\n        family = binomial)\nsummary(better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2395  -0.7186  -0.2324   0.7930   3.3538  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -6.1472     1.0419  -5.900 3.63e-09 ***\n#> agegp35-44    1.6311     1.0800   1.510 0.130973    \n#> agegp45-54    3.4258     1.0389   3.297 0.000976 ***\n#> agegp55-64    3.9435     1.0346   3.811 0.000138 ***\n#> agegp65-74    4.3568     1.0413   4.184 2.87e-05 ***\n#> agegp75+      4.4242     1.0914   4.054 5.04e-05 ***\n#> alcgp40-79    1.4343     0.2448   5.859 4.64e-09 ***\n#> alcgp80-119   2.0071     0.2776   7.230 4.84e-13 ***\n#> alcgp120+     3.6800     0.3763   9.778  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 105.88  on 79  degrees of freedom\n#> AIC: 238.94\n#> \n#> Number of Fisher Scoring iterations: 6\nbetter_model$aic #smaller AIC is better\n#> [1] 238.9361\ncoefficients(better_model) %>% exp\n#>  (Intercept)   agegp35-44   agegp45-54   agegp55-64   agegp65-74     agegp75+ \n#>  0.002139482  5.109601844 30.748594216 51.596634690 78.005283850 83.448437749 \n#>   alcgp40-79  alcgp80-119    alcgp120+ \n#>  4.196747169  7.441782227 39.646885126\npchisq(\n    q = model$deviance - better_model$deviance,\n    df = model$df.residual - better_model$df.residual,\n    lower = FALSE\n)\n#> [1] 2.713923e-23\n# specify link function as probit\nProb_better_model <- glm(\n    cbind(ncases, ncontrols) ~ agegp + alcgp,\n    data = esoph,\n    family = binomial(link = probit)\n)\nsummary(Prob_better_model)\n#> \n#> Call:\n#> glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), \n#>     data = esoph)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1325  -0.6877  -0.1661   0.7654   3.3258  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -3.3741     0.4922  -6.855 7.13e-12 ***\n#> agegp35-44    0.8562     0.5081   1.685 0.092003 .  \n#> agegp45-54    1.7829     0.4904   3.636 0.000277 ***\n#> agegp55-64    2.1034     0.4876   4.314 1.61e-05 ***\n#> agegp65-74    2.3374     0.4930   4.741 2.13e-06 ***\n#> agegp75+      2.3694     0.5275   4.491 7.08e-06 ***\n#> alcgp40-79    0.8080     0.1330   6.076 1.23e-09 ***\n#> alcgp80-119   1.1399     0.1558   7.318 2.52e-13 ***\n#> alcgp120+     2.1204     0.2060  10.295  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 367.95  on 87  degrees of freedom\n#> Residual deviance: 104.48  on 79  degrees of freedom\n#> AIC: 237.53\n#> \n#> Number of Fisher Scoring iterations: 6"},{"path":"generalized-linear-models.html","id":"poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.4 Poisson Regression","text":"Poisson distribution\\[\n\\begin{aligned}\nf(Y_i) &= \\frac{\\mu_i^{Y_i}exp(-\\mu_i)}{Y_i!}, Y_i = 0,1,.. \\\\\nE(Y_i) &= \\mu_i  \\\\\nvar(Y_i) &= \\mu_i\n\\end{aligned}\n\\]natural distribution counts. can see variance function mean. let \\(\\mu_i = f(\\mathbf{x_i; \\theta})\\), similar Logistic Regression since can choose \\(f()\\) \\(\\mu_i = \\mathbf{x_i'\\theta}, \\mu_i = \\exp(\\mathbf{x_i'\\theta}), \\mu_i = \\log(\\mathbf{x_i'\\theta})\\)","code":""},{"path":"generalized-linear-models.html","id":"application-3","chapter":"7 Generalized Linear Models","heading":"7.4.1 Application","text":"Count Data Poisson regressionResidual 1634 909 df isn’t great.see Pearson \\(\\chi^2\\)interaction terms, improvementsConsider \\(\\hat{\\phi} = \\frac{\\text{deviance}}{df}\\)evidence -dispersion. Likely cause missing variables. remedies either include variables consider random effects.quick fix force Poisson Regression include value \\(\\phi\\), model called “Quasi-Poisson”.directly rerun model asQuasi-Poisson recommended, Negative Binomial Regression extra parameter account -dispersion .","code":"\ndata(bioChemists, package = \"pscl\")\nbioChemists <- bioChemists %>%\n    rename(\n        Num_Article = art, #articles in last 3 years of PhD\n        Sex = fem, #coded 1 if female\n        Married = mar, #coded 1 if married\n        Num_Kid5 = kid5, #number of childeren under age 6\n        PhD_Quality = phd, #prestige of PhD program\n        Num_MentArticle = ment #articles by mentor in last 3 years\n    )\nhist(bioChemists$Num_Article, breaks = 25, main = 'Number of Articles')\nPoisson_Mod <- glm(Num_Article ~ ., family=poisson, bioChemists)\nsummary(Poisson_Mod)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.304617   0.102981   2.958   0.0031 ** \n#> SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***\n#> MarriedMarried   0.155243   0.061374   2.529   0.0114 *  \n#> Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***\n#> PhD_Quality      0.012823   0.026397   0.486   0.6271    \n#> Num_MentArticle  0.025543   0.002006  12.733  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5\nPredicted_Means <- predict(Poisson_Mod,type = \"response\")\nX2 <- sum((bioChemists$Num_Article - Predicted_Means)^2/Predicted_Means)\nX2\n#> [1] 1662.547\npchisq(X2,Poisson_Mod$df.residual, lower.tail = FALSE)\n#> [1] 7.849882e-47\nPoisson_Mod_All2way <- glm(Num_Article ~ .^2, family=poisson, bioChemists)\nPoisson_Mod_All3way <- glm(Num_Article ~ .^3, family=poisson, bioChemists)\nPoisson_Mod$deviance / Poisson_Mod$df.residual\n#> [1] 1.797988\nphi_hat = Poisson_Mod$deviance/Poisson_Mod$df.residual\nsummary(Poisson_Mod,dispersion = phi_hat)\n#> \n#> Call:\n#> glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.30462    0.13809   2.206  0.02739 *  \n#> SexWomen        -0.22459    0.07323  -3.067  0.00216 ** \n#> MarriedMarried   0.15524    0.08230   1.886  0.05924 .  \n#> Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***\n#> PhD_Quality      0.01282    0.03540   0.362  0.71715    \n#> Num_MentArticle  0.02554    0.00269   9.496  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1.797988)\n#> \n#>     Null deviance: 1817.4  on 914  degrees of freedom\n#> Residual deviance: 1634.4  on 909  degrees of freedom\n#> AIC: 3314.1\n#> \n#> Number of Fisher Scoring iterations: 5\nquasiPoisson_Mod <- glm(Num_Article ~ ., family=quasipoisson, bioChemists)"},{"path":"generalized-linear-models.html","id":"negative-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.5 Negative Binomial Regression","text":"can see dispersion 2.264 SE = 0.271, significantly different 1, indicating -dispersion. Check -Dispersion detail","code":"\nlibrary(MASS)\nNegBinom_Mod <- MASS::glm.nb(Num_Article ~ .,bioChemists)\nsummary(NegBinom_Mod)\n#> \n#> Call:\n#> MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n#>     link = log)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1678  -1.3617  -0.2806   0.4476   3.4524  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n#> SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n#> MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n#> Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n#> PhD_Quality      0.015271   0.035873   0.426 0.670326    \n#> Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n#> \n#>     Null deviance: 1109.0  on 914  degrees of freedom\n#> Residual deviance: 1004.3  on 909  degrees of freedom\n#> AIC: 3135.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.264 \n#>           Std. Err.:  0.271 \n#> \n#>  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"multinomial","chapter":"7 Generalized Linear Models","heading":"7.6 Multinomial","text":"two categories groups want model relative covariates (e.g., observations \\(= 1,…,n\\) groups/ covariates \\(j = 1,2,…,J\\)), multinomial candidate modelLet\\(p_{ij}\\) probability -th observation belongs j-th group\\(Y_{ij}\\) number observations individual group j; individual observations \\(Y_{i1},Y_{i2},…Y_{iJ}\\)assume probability observing response given multinomial distribution terms probabilities \\(p_{ij}\\), \\(\\sum_{j = 1}^J p_{ij} = 1\\) . interpretation, baseline category \\(p_{i1} = 1 - \\sum_{j = 2}^J p_{ij}\\)link mean response (probability) \\(p_{ij}\\) linear function covariates\\[\n\\eta_{ij} = \\mathbf{x'_i \\beta_j} = \\log \\frac{p_{ij}}{p_{i1}}, j = 2,..,J\n\\]compare \\(p_{ij}\\) baseline \\(p_{i1}\\), suggesting\\[\np_{ij} = \\frac{\\exp(\\eta_{ij})}{1 + \\sum_{=2}^J \\exp(\\eta_{ij})}\n\\]known multinomial logistic model.Note:Softmax coding multinomial logistic regression: rather selecting baseline class, treat \\(K\\) class symmetrically - equally important (baseline).\\[\nP(Y = k | X = x) = \\frac{exp(\\beta_{k1} + \\dots + \\beta_{k_p x_p})}{\\sum_{l = 1}^K exp(\\beta_{l0} + \\dots + \\beta_{l_p x_p})}\n\\]log odds ratio k-th k’-th classes \\[\n\\log (\\frac{P(Y=k|X=x)}{P(Y = k' | X=x)}) = (\\beta_{k0} - \\beta_{k'0}) + \\dots + (\\beta_{kp} - \\beta_{k'p}) x_p\n\\]try understand political strengthvisualize political strength variableFit multinomial logistic model:model political strength function age educationAlternatively, stepwise model selection based AICcompare best model full model based devianceWe see significant differencePlot fitted modelIf categories ordered (.e., ordinal data), must use another approach (still multinomial, use cumulative probabilities).Another example\\[\nY \\sim Gamma\n\\]Gamma non-negative opposed Normal. canonical Gamma link function inverse (reciprocal) link\\[\n\\begin{aligned}\n\\eta_{ij} &= \\beta_{0j} + \\beta_{1j}x_{ij} + \\beta_2x_{ij}^2 \\\\\nY_{ij} &= \\eta_{ij}^{-1}\n\\end{aligned}\n\\]linear predictor quadratic model fit j-th blocks. different model (fitted) one common slopes: glm(y ~ x + (x^2),…)predict new value \\(x\\)","code":"\nlibrary(faraway)\nlibrary(dplyr)\ndata(nes96, package=\"faraway\")\nhead(nes96,3)\n#>   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote\n#> 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole\n#> 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton\n#> 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton\ntable(nes96$PID)\n#> \n#>  strDem weakDem  indDem  indind  indRep weakRep  strRep \n#>     200     180     108      37      94     150     175\nnes96$Political_Strength <- NA\nnes96$Political_Strength[nes96$PID %in% c(\"strDem\", \"strRep\")] <-\n    \"Strong\"\nnes96$Political_Strength[nes96$PID %in% c(\"weakDem\", \"weakRep\")] <-\n    \"Weak\"\nnes96$Political_Strength[nes96$PID %in% c(\"indDem\", \"indind\", \"indRep\")] <-\n    \"Neutral\"\nnes96 %>% group_by(Political_Strength) %>% summarise(Count = n())\n#> # A tibble: 3 × 2\n#>   Political_Strength Count\n#>   <chr>              <int>\n#> 1 Neutral              239\n#> 2 Strong               375\n#> 3 Weak                 330\nlibrary(ggplot2)\nPlot_DF <- nes96 %>%\n    mutate(Age_Grp = cut_number(age, 4)) %>%\n    group_by(Age_Grp, Political_Strength) %>%\n    summarise(count = n()) %>%\n    group_by(Age_Grp) %>%\n    mutate(etotal = sum(count), proportion = count / etotal)\n\nAge_Plot <- ggplot(\n    Plot_DF,\n    aes(\n        x        = Age_Grp,\n        y        = proportion,\n        group    = Political_Strength,\n        linetype = Political_Strength,\n        color    = Political_Strength\n    )\n) +\n    geom_line(size = 2)\nAge_Plot\nlibrary(nnet)\nMultinomial_Model <-\n    multinom(Political_Strength ~ age + educ, nes96, trace = F)\nsummary(Multinomial_Model)\n#> Call:\n#> multinom(formula = Political_Strength ~ age + educ, data = nes96, \n#>     trace = F)\n#> \n#> Coefficients:\n#>        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4\n#> Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307\n#> Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634\n#>            educ^5     educ^6\n#> Strong -0.1664377 -0.1359449\n#> Weak   -0.1489030 -0.2173144\n#> \n#> Std. Errors:\n#>        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4\n#> Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776\n#> Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149\n#>           educ^5    educ^6\n#> Strong 0.2515012 0.2166774\n#> Weak   0.2643747 0.2199186\n#> \n#> Residual Deviance: 2024.596 \n#> AIC: 2056.596\nMultinomial_Step <- step(Multinomial_Model,trace = 0)\n#> trying - age \n#> trying - educ \n#> trying - age\nMultinomial_Step\n#> Call:\n#> multinom(formula = Political_Strength ~ age, data = nes96, trace = F)\n#> \n#> Coefficients:\n#>        (Intercept)          age\n#> Strong -0.01988977  0.009832916\n#> Weak    0.59497046 -0.005954348\n#> \n#> Residual Deviance: 2030.756 \n#> AIC: 2038.756\npchisq(q = deviance(Multinomial_Step) - deviance(Multinomial_Model),\ndf = Multinomial_Model$edf-Multinomial_Step$edf,lower=F)\n#> [1] 0.9078172\nPlotData <- data.frame(age = seq(from = 19, to = 91))\nPreds <-\n  PlotData %>% bind_cols(data.frame(predict(\n    object = Multinomial_Step,\n    PlotData, type = \"probs\"\n  )))\n\nplot(\n  x       = Preds$age,\n  y       = Preds$Neutral,\n  type    = \"l\",\n  ylim    = c(0.2, 0.6),\n  col     = \"black\",\n  ylab    = \"Proportion\",\n  xlab    = \"Age\"\n)\n\nlines(x   = Preds$age,\n      y   = Preds$Weak,\n      col = \"blue\")\nlines(x   = Preds$age,\n      y   = Preds$Strong,\n      col = \"red\")\n\nlegend(\n  'topleft',\n  legend  = c('Neutral', 'Weak', 'Strong'),\n  col     = c('black', 'blue', 'red'),\n  lty     = 1\n)\npredict(Multinomial_Step,data.frame(age = 34)) # predicted result (categoriy of political strength) of 34 year old\n#> [1] Weak\n#> Levels: Neutral Strong Weak\npredict(Multinomial_Step,data.frame(age = c(34,35)),type=\"probs\") # predicted result of the probabilities of each level of political strength for a 34 and 35\n#>     Neutral    Strong      Weak\n#> 1 0.2597275 0.3556910 0.3845815\n#> 2 0.2594080 0.3587639 0.3818281\nlibrary(agridat)\ndat <- agridat::streibig.competition\n# See Schaberger and Pierce, pages 370+\n# Consider only the mono-species barley data (no competition from Sinapis)\ngammaDat <- subset(dat, sseeds < 1)\ngammaDat <-\n    transform(gammaDat,\n              x = bseeds,\n              y = bdwt,\n              block = factor(block))\n# Inverse yield looks like it will be a good fit for Gamma's inverse link\nggplot(gammaDat, aes(x = x, y = 1 / y)) + \n    geom_point(aes(color = block, shape = block)) +\n    xlab('Seeding Rate') + \n    ylab('Inverse yield') + \n    ggtitle('Streibig Competion - Barley only')\n# linear predictor is quadratic, with separate intercept and slope per block\nm1 <-\n    glm(y ~ block + block * x + block * I(x ^ 2),\n        data = gammaDat,\n        family = Gamma(link = \"inverse\"))\nsummary(m1)\n#> \n#> Call:\n#> glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = \"inverse\"), \n#>     data = gammaDat)\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -1.21708  -0.44148   0.02479   0.17999   0.80745  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***\n#> blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    \n#> blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    \n#> x              -2.075e-03  1.099e-03  -1.888 0.072884 .  \n#> I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    \n#> blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    \n#> blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    \n#> blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    \n#> blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Gamma family taken to be 0.3232083)\n#> \n#>     Null deviance: 13.1677  on 29  degrees of freedom\n#> Residual deviance:  7.8605  on 21  degrees of freedom\n#> AIC: 225.32\n#> \n#> Number of Fisher Scoring iterations: 5\nnewdf <-\n    expand.grid(x = seq(0, 120, length = 50), block = factor(c('B1', 'B2', 'B3')))\n\nnewdf$pred <- predict(m1, new = newdf, type = 'response')\n\nggplot(gammaDat, aes(x = x, y = y)) + \n    geom_point(aes(color = block, shape = block)) +\n    xlab('Seeding Rate') + ylab('Inverse yield') + \n    ggtitle('Streibig Competion - Barley only Predictions') +\n    geom_line(data = newdf, aes(\n        x = x,\n        y = pred,\n        color = block,\n        linetype = block\n    ))"},{"path":"generalized-linear-models.html","id":"generalization","chapter":"7 Generalized Linear Models","heading":"7.7 Generalization","text":"can see Poisson regression looks similar logistic regression. Hence, can generalize class modeling. Thanks Nelder Wedderburn (1972), generalized linear models (GLMs). Estimation generalize models.Exponential Family\ntheory GLMs developed data distribution given y exponential family.\nform data distribution useful GLMs \\[\nf(y;\\theta, \\phi) = \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y, \\phi))\n\\]\\(\\theta\\) called natural parameter\\(\\phi\\) called dispersion parameterNote:family includes [Gamma], [Normal], [Poisson], . parameterization exponential family, check linkExampleif \\(Y \\sim N(\\mu, \\sigma^2)\\)\\[\n\\begin{aligned}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{(2\\pi \\sigma^2)^{1/2}}\\exp(-\\frac{1}{2\\sigma^2}(y- \\mu)^2) \\\\\n&= \\exp(-\\frac{1}{2\\sigma^2}(y^2 - 2y \\mu +\\mu^2)- \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{y \\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y , \\phi))\n\\end{aligned}\n\\]\\(\\theta = \\mu\\)\\(b(\\theta) = \\frac{\\mu^2}{2}\\)\\((\\phi) = \\sigma^2 = \\phi\\)\\(c(y , \\phi) = - \\frac{1}{2}(\\frac{y^2}{\\phi}+\\log(2\\pi \\sigma^2))\\)Properties GLM exponential families\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime”, transpose)\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime”, transpose)\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\n\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\n\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\\((), b(), c()\\) identifiable, derive expected value variance Y.\\((), b(), c()\\) identifiable, derive expected value variance Y.ExampleNormal distribution\\[\n\\begin{aligned}\nb'(\\theta) &= \\frac{\\partial b(\\mu^2/2)}{\\partial \\mu} = \\mu \\\\\nV(\\mu) &= \\frac{\\partial^2 (\\mu^2/2)}{\\partial \\mu^2} = 1 \\\\\n\\var(Y) &= (\\phi) = \\sigma^2\n\\end{aligned}\n\\]Poisson distribution\\[\n\\begin{aligned}\nf(y, \\theta, \\phi) &= \\frac{\\mu^y \\exp(-\\mu)}{y!} \\\\\n&= \\exp(y\\log(\\mu) - \\mu - \\log(y!)) \\\\\n&= \\exp(y\\theta - \\exp(\\theta) - \\log(y!))\n\\end{aligned}\n\\]\\(\\theta = \\log(\\mu)\\)\\((\\phi) = 1\\)\\(b(\\theta) = \\exp(\\theta)\\)\\(c(y, \\phi) = \\log(y!)\\)Hence,\\[\n\\begin{aligned}\nE(Y) = \\frac{\\partial b(\\theta)}{\\partial \\theta} = \\exp(\\theta) &= \\mu \\\\\nvar(Y) = \\frac{\\partial^2 b(\\theta)}{\\partial \\theta^2} &= \\mu\n\\end{aligned}\n\\]Since \\(\\mu = E(Y) = b'(\\theta)\\)GLM, take monotone function (typically nonlinear) \\(\\mu\\) linear set covariates\\[\ng(\\mu) = g(b'(\\theta)) = \\mathbf{x'\\beta}\n\\]Equivalently,\\[\n\\mu = g^{-1}(\\mathbf{x'\\beta})\n\\]\\(g(.)\\) link function since links mean response (\\(\\mu = E(Y)\\)) linear expression covariatesSome people use \\(\\eta = \\mathbf{x'\\beta}\\) \\(\\eta\\) = “linear predictor”GLM composed 2 componentsThe random component:distribution chosen model response variables \\(Y_1,...,Y_n\\)distribution chosen model response variables \\(Y_1,...,Y_n\\)specified choice fo \\((), b(), c()\\) exponential formis specified choice fo \\((), b(), c()\\) exponential formNotation:\nAssume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densities\nAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .\nNotation:Assume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densitiesAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .systematic componentis portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)consists 2 parts:\nlink function, \\(g(.)\\)\nlinear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)\nconsists 2 parts:link function, \\(g(.)\\)linear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)Notation:\nassume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)\nparameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)\nNotation:assume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)parameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)Canonical LinkTo choose \\(g(.)\\), can use canonical link function (Remember: Canonical link just special case link function)link function \\(g(.)\\) \\(g(\\mu_i) = \\eta_i = \\theta_i\\), natural parameter, \\(g(.)\\) canonical link.\\(b(\\theta)\\) = cumulant moment generating function\\(g(\\mu)\\) link function, relates linear predictor mean required monotone increasing, continuously differentiable invertible.Equivalently, can think canonical link function \\[\n\\gamma^{-1} \\circ g^{-1} = \n\\] identity. Hence,\\[\n\\theta = \\eta\n\\]inverse link\\(g^{-1}(.)\\) also known mean function, take linear predictor output (ranging \\(-\\infty\\) \\(\\infty\\)) transform different scale.Exponential: converts \\(\\mathbf{\\beta X}\\) curve restricted 0 \\(\\infty\\) (can see useful case want convert linear predictor non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\)Exponential: converts \\(\\mathbf{\\beta X}\\) curve restricted 0 \\(\\infty\\) (can see useful case want convert linear predictor non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\)Inverse Logit (also known logistic): converts \\(\\mathbf{\\beta X}\\) curve restricted 0 1, useful case want convert linear predictor probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\)\n\\(y\\) = linear predictor value\n\\(\\theta\\) = transformed value\nInverse Logit (also known logistic): converts \\(\\mathbf{\\beta X}\\) curve restricted 0 1, useful case want convert linear predictor probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\)\\(y\\) = linear predictor value\\(\\theta\\) = transformed valueThe identity link \\[\n\\begin{aligned}\n\\eta_i &= g(\\mu_i) = \\mu_i \\\\\n\\mu_i &= g^{-1}(\\eta_i) = \\eta_i\n\\end{aligned}\n\\]Table 15.1 Generalized Linear Models 15.1 Structure Generalized Linear ModelsMore example link functions inverses can found page 380ExampleNormal random componentMean Response: \\(\\mu_i = \\theta_i\\)Mean Response: \\(\\mu_i = \\theta_i\\)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Binomial random componentMean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Mean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Poisson random componentMean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Mean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Gamma random component:Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Inverse Gaussian randomCanonical Link: \\(g(\\mu_i) = \\frac{1}{\\mu_i^2}\\)","code":""},{"path":"generalized-linear-models.html","id":"estimation-1","chapter":"7 Generalized Linear Models","heading":"7.7.1 Estimation","text":"MLE parameters systematic component (\\(\\beta\\))Unification derivation computation (thanks exponential forms)unification estimation dispersion parameter (\\(\\phi\\))","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-beta","chapter":"7 Generalized Linear Models","heading":"7.7.1.1 Estimation of \\(\\beta\\)","text":"\\[\n\\begin{aligned}\nf(y_i ; \\theta_i, \\phi) &= \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi)) \\\\\nE(Y_i) &= \\mu_i = b'(\\theta) \\\\\nvar(Y_i) &= b''(\\theta)(\\phi) = V(\\mu_i)(\\phi) \\\\\ng(\\mu_i) &= \\mathbf{x}_i'\\beta = \\eta_i\n\\end{aligned}\n\\]log-likelihood single observation \\(l_i (\\beta,\\phi)\\). log-likelihood n observations \\[\n\\begin{aligned}\nl(\\beta,\\phi) &= \\sum_{=1}^n l_i (\\beta,\\phi) \\\\\n&= \\sum_{=1}^n (\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\end{aligned}\n\\]Using MLE find \\(\\beta\\), use chain rule get derivatives\\[\n\\begin{aligned}\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} &=  \\frac{\\partial l_i (\\beta, \\phi)}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i}\\times \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\\\\n&= \\sum_{=1}^{n}(\\frac{ y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij})\n\\end{aligned}\n\\]let\\[\nw_i \\equiv ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1}\n\\],\\[\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} = \\sum_{=1}^n (\\frac{y_i \\mu_i}{(\\phi)} \\times w_i \\times \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\times x_{ij})\n\\]can also get second derivatives using chain rule.Example:\\[Newton-Raphson\\] algorithm, need\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k})\n\\]\\((j,k)\\)-th element Fisher information matrix \\(\\mathbf{}(\\beta)\\)Hence,\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)}x_{ij}x_{ik}\n\\](j,k)th elementIf Bernoulli model logit link function (canonical link)\\[\n\\begin{aligned}\nb(\\theta) &= \\log(1 + \\exp(\\theta)) = \\log(1 + \\exp(\\mathbf{x'\\beta})) \\\\\n(\\phi) &= 1  \\\\\nc(y_i, \\phi) &= 0 \\\\\nE(Y) = b'(\\theta) &= \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\mu = p \\\\\n\\eta = g(\\mu) &= \\log(\\frac{\\mu}{1-\\mu}) = \\theta = \\log(\\frac{p}{1-p}) = \\mathbf{x'\\beta}\n\\end{aligned}\n\\]\\(Y_i\\), = 1,.., log-likelihood \\[\nl_i (\\beta, \\phi) = \\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) = y_i \\mathbf{x}'_i \\beta - \\log(1+ \\exp(\\mathbf{x'\\beta}))\n\\]Additionally,\\[\n\\begin{aligned}\nV(\\mu_i) &= \\mu_i(1-\\mu_i)= p_i (1-p_i) \\\\\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} &= p_i(1-p_i)\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\frac{\\partial l(\\beta, \\phi)}{\\partial \\beta_j} &= \\sum_{=1}^n[\\frac{y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)}\\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij}] \\\\\n&= \\sum_{=1}^n (y_i - p_i) \\times \\frac{1}{p_i(1-p_i)} \\times p_i(1-p_i) \\times x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - p_i) x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - \\frac{\\exp(\\mathbf{x'_i\\beta})}{1+ \\exp(\\mathbf{x'_i\\beta})})x_{ij}\n\\end{aligned}\n\\]\\[\nw_i = ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1} = p_i (1-p_i)\n\\]\\[\n\\mathbf{}_{jk}(\\mathbf{\\beta}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)} x_{ij}x_{ik} = \\sum_{=1}^n p_i (1-p_i)x_{ij}x_{ik}\n\\]Fisher-scoring algorithm MLE \\(\\mathbf{\\beta}\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m+1)}\n=\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m)} +\n\\mathbf{}^{-1}(\\mathbf{\\beta})\n\\left(\n\\begin{array}\n{c}\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_1} \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_2} \\\\\n. \\\\\n. \\\\\n. \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_p} \\\\\n\\end{array}\n\\right)|_{\\beta = \\beta^{(m)}}\n\\]Similar \\[Newton-Raphson\\] expect matrix second derivatives expected value second derivative matrix.matrix notation,\\[\n\\begin{aligned}\n\\frac{\\partial l }{\\partial \\beta} &= \\frac{1}{(\\phi)}\\mathbf{X'W\\Delta(y - \\mu)} \\\\\n&= \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}(y - \\mu)} \\\\\n\\end{aligned}\n\\]\\[\n\\mathbf{}(\\beta) = \\frac{1}{(\\phi)}\\mathbf{X'WX} = \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}F}\n\\]\\(\\mathbf{X}\\) \\(n \\times p\\) matrix covariates\\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\)\\(\\mathbf{\\Delta}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\)\\(\\mathbf{F} = \\mathbf{\\frac{\\partial \\mu}{\\partial \\beta}}\\) \\(n \\times p\\) matrix \\(\\)-th row \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = (\\frac{\\partial \\mu_i}{\\partial \\eta_i})\\mathbf{x}'_i\\)\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(V(\\mu_i)\\)Setting derivative log-likelihood equal 0, ML estimating equations \\[\n\\mathbf{F'V^{-1}y= F'V^{-1}\\mu}\n\\]components equation expect y depends parameters \\(\\beta\\)Special CasesIf one canonical link, estimating equations reduce \\[\n\\mathbf{X'y= X'\\mu}\n\\]one identity link, \\[\n\\mathbf{X'V^{-1}y = X'V^{-1}X\\hat{\\beta}}\n\\]gives generalized least squares estimatorGenerally, can rewrite Fisher-scoring algorithm \\[\n\\beta^{(m+1)} = \\beta^{(m)} + \\mathbf{(\\hat{F}'\\hat{V}^{-1}\\hat{F})^{-1}\\hat{F}'\\hat{V}^{-1}(y- \\hat{\\mu})}\n\\]Since \\(\\hat{F},\\hat{V}, \\hat{\\mu}\\) depend \\(\\beta\\), evaluate \\(\\beta^{(m)}\\)starting values \\(\\beta^{(0)}\\), can iterate convergence.Notes:\\((\\phi)\\) constant form \\(m_i \\phi\\) known \\(m_i\\), \\(\\phi\\) cancels.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-phi","chapter":"7 Generalized Linear Models","heading":"7.7.1.2 Estimation of \\(\\phi\\)","text":"2 approaches:MLE\\[\n\\frac{\\partial l_i}{\\partial \\phi} = \\frac{(\\theta_i y_i - b(\\theta_i)'(\\phi))}{^2(\\phi)} + \\frac{\\partial c(y_i,\\phi)}{\\partial \\phi}\n\\]MLE \\(\\phi\\) solves\\[\n\\frac{^2(\\phi)}{'(\\phi)}\\sum_{=1}^n \\frac{\\partial c(y_i, \\phi)}{\\partial \\phi} = \\sum_{=1}^n(\\theta_i y_i - b(\\theta_i))\n\\]Situation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleSituation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleEven canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Even canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Moment Estimation (“Bias Corrected \\(\\chi^2\\)”)\nMLE conventional approach estimation \\(\\phi\\) GLMS.\nexponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n\\begin{aligned}\n(\\phi) &= \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\end{aligned}\n\\] \\(p\\) dimension \\(\\beta\\)\nGLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]\nmethod estimator \\((\\phi)=\\phi\\) \nMoment Estimation (“Bias Corrected \\(\\chi^2\\)”)MLE conventional approach estimation \\(\\phi\\) GLMS.exponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n\\begin{aligned}\n(\\phi) &= \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  &= \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\end{aligned}\n\\] \\(p\\) dimension \\(\\beta\\)GLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\n\\begin{aligned}\ng(\\mu) &= \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu &= g^{-1}(\\eta)= b'(\\eta)\n\\end{aligned}\n\\]method estimator \\((\\phi)=\\phi\\) \\[\n\\hat{\\phi} = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"inference-2","chapter":"7 Generalized Linear Models","heading":"7.7.2 Inference","text":"\\[\n\\hat{var}(\\beta) = (\\phi)(\\mathbf{\\hat{F}'\\hat{V}\\hat{F}})^{-1}\n\\]\\(\\mathbf{V}\\) \\(n \\times n\\) diagonal matrix diagonal elements given \\(V(\\mu_i)\\)\\(\\mathbf{F}\\) \\(n \\times p\\) matrix given \\(\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\\)\\(\\mathbf{V,F}\\) dependent mean \\(\\mu\\), thus \\(\\beta\\). Hence, estimates (\\(\\mathbf{\\hat{V},\\hat{F}}\\)) depend \\(\\hat{\\beta}\\).\\[\nH_0: \\mathbf{L\\beta = d}\n\\]\\(\\mathbf{L}\\) q x p matrix Wald test\\[\nW = \\mathbf{(L \\hat{\\beta}-d)'((\\phi)L(\\hat{F}'\\hat{V}^{-1}\\hat{F})L')^{-1}(L \\hat{\\beta}-d)}\n\\]follows \\(\\chi_q^2\\) distribution (asymptotically), \\(q\\) rank \\(\\mathbf{L}\\)simple case \\(H_0: \\beta_j = 0\\) gives \\(W = \\frac{\\hat{\\beta}^2_j}{\\hat{var}(\\hat{\\beta}_j)} \\sim \\chi^2_1\\) asymptoticallyLikelihood ratio test\\[\n\\Lambda = 2 (l(\\hat{\\beta}_f)-l(\\hat{\\beta}_r)) \\sim \\chi^2_q\n\\]\\(q\\) number constraints used fit reduced model \\(\\hat{\\beta}_r\\), \\(\\hat{\\beta}_r\\) fit full model.Wald test easier implement, likelihood ratio test better (especially small samples).","code":""},{"path":"generalized-linear-models.html","id":"deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3 Deviance","text":"Deviance necessary goodness fit, inference alternative estimation dispersion parameter. define consider Deviance likelihood ratio perspective.Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model”, saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model”, saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\](McCullagh 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] (McCullagh 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] \\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = devianceNote:random component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \n\\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\nrandom component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution \\(\\)-th observation.\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution \\(\\)-th observation.\\(D\\) used model selection\\(D\\) used model selection\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(d_i\\) used form deviance residuals\\(d_i\\) used form deviance residualsNormalWe \\[\n\\begin{aligned}\n\\theta &= \\mu \\\\\n\\phi &= \\sigma^2 \\\\\nb(\\theta) &= \\frac{1}{2} \\theta^2 \\\\\n(\\phi) &= \\phi\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\tilde{\\theta}_i &= y_i \\\\\n\\hat{\\theta}_i &= \\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i)\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nD &= 2 \\sum_{1=1}^n Y^2_i - y_i \\hat{\\mu}_i - \\frac{1}{2}y^2_i + \\frac{1}{2} \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n y_i^2 - 2y_i \\hat{\\mu}_i + \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n (y_i - \\hat{\\mu}_i)^2\n\\end{aligned}\n\\]residual sum squaresPoisson\\[\n\\begin{aligned}\nf(y) &= \\exp\\{y\\log(\\mu) - \\mu - \\log(y!)\\} \\\\\n\\theta &= \\log(\\mu) \\\\\nb(\\theta) &= \\exp(\\theta) \\\\\n(\\phi) &= 1 \\\\\n\\tilde{\\theta}_i &= \\log(y_i) \\\\\n\\hat{\\theta}_i &= \\log(\\hat{\\mu}_i) \\\\\n\\hat{\\mu}_i &= g^{-1}(\\hat{\\eta}_i)\n\\end{aligned}\n\\],\\[\n\\begin{aligned}\nD &= 2 \\sum_{= 1}^n y_i \\log(y_i) - y_i \\log(\\hat{\\mu}_i) - y_i + \\hat{\\mu}_i \\\\\n&= 2 \\sum_{= 1}^n y_i \\log(\\frac{y_i}{\\hat{\\mu}_i}) - (y_i - \\hat{\\mu}_i)\n\\end{aligned}\n\\]\\[\nd_i = 2\\{y_i \\log(\\frac{y_i}{\\hat{\\mu}})- (y_i - \\hat{\\mu}_i)\\}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"analysis-of-deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3.1 Analysis of Deviance","text":"difference deviance reduced full model, q difference number free parameters, asymptotic \\(\\chi^2_q\\). likelihood ratio test\\[\nD^*(\\mathbf{y;\\hat{\\mu}_r}) - D^*(\\mathbf{y;\\hat{\\mu}_f}) = 2\\{l(\\mathbf{y;\\hat{\\mu}_f})-l(\\mathbf{y;\\hat{\\mu}_r})\\}\n\\]comparison models Analysis Deviance. GLM uses analysis model selection.estimation \\(\\phi\\) \\[\n\\hat{\\phi} = \\frac{D(\\mathbf{y, \\hat{\\mu}})}{n - p}\n\\]\\(p\\) = number parameters fit.Excessive use \\(\\chi^2\\) test problematic since asymptotic (McCullagh 2019)","code":""},{"path":"generalized-linear-models.html","id":"deviance-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.2 Deviance Residuals","text":"\\(D = \\sum_{=1}^{n}d_i\\). , define deviance residuals\\[\nr_{D_i} = \\text{sign}(y_i -\\hat{\\mu}_i)\\sqrt{d_i}\n\\]Standardized version deviance residuals \\[\nr_{s,} = \\frac{y_i -\\hat{\\mu}}{\\hat{\\sigma}(1-h_{ii})^{1/2}}\n\\]Let \\(\\mathbf{H^{GLM} = W^{1/2}X(X'WX)^{-1}X'W^{-1/2}}\\), \\(\\mathbf{W}\\) \\(n \\times n\\) diagonal matrix \\((,)\\)-th element given \\(w_i\\) (see Estimation \\(\\beta\\)). Standardized deviance residuals equivalently\\[\nr_{s, D_i} = \\frac{r_{D_i}}{\\{\\hat{\\phi}(1-h_{ii}^{glm}\\}^{1/2}}\n\\]\\(h_{ii}^{glm}\\) \\(\\)-th diagonal \\(\\mathbf{H}^{GLM}\\)","code":""},{"path":"generalized-linear-models.html","id":"pearson-chi-square-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.3 Pearson Chi-square Residuals","text":"Another \\(\\chi^2\\) statistic Pearson \\(\\chi^2\\) statistics: (assume \\(m_i = 1\\))\\[\nX^2 = \\sum_{=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]\\(\\hat{\\mu}_i\\) fitted mean response fo model interest.Scaled Pearson \\(\\chi^2\\) statistic given \\(\\frac{X^2}{\\phi} \\sim \\chi^2_{n-p}\\) p number parameters estimated. Hence, Pearson \\(\\chi^2\\) residuals \\[\nX^2_i = \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]following assumptions:Independent samplesNo -dispersion: \\(\\phi = 1\\), \\(\\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p}\\) \\(\\frac{X^2}{n-p}\\) value substantially larger 1 indicates improperly specified model overdispersionMultiple groupsthen \\(\\frac{X^2}{\\phi}\\) \\(D^*(\\mathbf{y; \\hat{\\mu}})\\) follow \\(\\chi^2_{n-p}\\)","code":""},{"path":"generalized-linear-models.html","id":"diagnostic-plots","chapter":"7 Generalized Linear Models","heading":"7.7.4 Diagnostic Plots","text":"Standardized residual Plots:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))\nStandardized residual Plots:plot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:plot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))see:\nTrend, means might wrong link function, choice scale\nSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)\nsee:Trend, means might wrong link function, choice scaleSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.","code":""},{"path":"generalized-linear-models.html","id":"goodness-of-fit","chapter":"7 Generalized Linear Models","heading":"7.7.5 Goodness of Fit","text":"assess goodness fit, can useDeviancePearson Chi-square ResidualsIn nested model, use likelihood-based information measures:\\[\n\\begin{aligned}\nAIC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p \\\\\nAICC &= -2l(\\mathbf{\\hat{\\mu}}) + 2p(\\frac{n}{n-p-1}) \\\\\nBIC &= 2l(\\hat{\\mu}) + p \\log(n)\n\\end{aligned}\n\\]\\(l(\\hat{\\mu})\\) log-likelihood evaluated parameter estimates\\(p\\) number parameters\\(n\\) number observations.Note: use data model (.e., link function, random underlying random distribution). can different number parameters.Even though statisticians try come measures similar \\(R^2\\), practice, appropriate. example, compare log-likelihood fitted model model just intercept:\\[\nR^2_p = 1 - \\frac{l(\\hat{\\mu})}{l(\\hat{\\mu}_0)}\n\\]certain specific random components binary response model, rescaled generalized \\(R^2\\)\\[\n\\bar{R}^2 = \\frac{R^2_*}{\\max(R^2_*)} = \\frac{1-\\exp\\{-\\frac{2}{n}(l(\\hat{\\mu}) - l(\\hat{\\mu}_0) \\}}{1 - \\exp\\{\\frac{2}{n}l(\\hat{\\mu}_0)\\}}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"over-dispersion","chapter":"7 Generalized Linear Models","heading":"7.7.6 Over-Dispersion","text":"cases \\(\\phi = 1\\). Recall \\(b''(\\theta)= V(\\mu)\\) check Estimation \\(\\phi\\).find\\(\\phi >1\\): -dispersion (.e., much variation independent binomial Poisson distribution).\\(\\phi<1\\): -dispersion (.e., little variation independent binomial Poisson distribution).either -dispersion, means might unspecified random component, couldSelect different random component distribution can accommodate -dispersion (e.g., negative binomial, Conway-Maxwell Poisson)use Nonlinear Generalized Linear Mixed Models handle random effects generalized linear models.","code":""},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"dependent-data","chapter":"8 Linear Mixed Models","heading":"8.1 Dependent Data","text":"Forms dependent data:Multivariate measurements different individuals: (e.g., person’s blood pressure, fat, etc correlated)Clustered measurements: (e.g., blood pressure measurements people family can correlated).Repeated measurements: (e.g., measurement cholesterol time can correlated) “data collected repeatedly experimental material treatments applied initially, data repeated measure.” (Schabenberger Pierce 2001)Longitudinal data: (e.g., individual’s cholesterol tracked time correlated): “data collected repeatedly time observational study termed longitudinal.” (Schabenberger Pierce 2001)Spatial data: (e.g., measurement individuals living neighborhood correlated)Hence, like account correlations.Linear Mixed Model (LMM), also known Mixed Linear Model 2 components:Fixed effect (e.g, gender, age, diet, time)Fixed effect (e.g, gender, age, diet, time)Random effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsRandom effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsReview Two-Way Mixed Effects ANOVAWe choose model random subject-specific effect instead including dummy subject covariates model :reduction number parameters estimatewhen inference, make sense can infer population (.e., random effect).LLM MotivationIn repeated measurements analysis \\(Y_{ij}\\) response \\(\\)-th individual measured \\(j\\)-th time,\\(=1,…,N\\) ; \\(j = 1,…,n_i\\)\\[\n\\mathbf{Y}_i =\n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n. \\\\\n.\\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right)\n\\]measurements subject \\(\\).Stage 1: (Regression Model) response changes time \\(\\)-th subject\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]\\(Z_i\\) \\(n_i \\times q\\) matrix known covariates\\(\\beta_i\\) unknown \\(q \\times 1\\) vector subjective -specific coefficients (regression coefficients different subject)\\(\\epsilon_i\\) random errors (typically \\(\\sim N(0, \\sigma^2 )\\))notice two many \\(\\beta\\) estimate . Hence, motivation second stageStage 2: (Parameter Model)\\[\n\\mathbf{\\beta_i = K_i \\beta + b_i}\n\\]\\(K_i\\) \\(q \\times p\\) matrix known covariates\\(\\beta\\) \\(p \\times 1\\) vector unknown parameter\\(\\mathbf{b}_i\\) independent \\(N(0,D)\\) random variablesThis model explain observed variability subjects respect subject-specific regression coefficients, \\(\\beta_i\\). model different coefficient (\\(\\beta_i\\)) respect \\(\\beta\\).Example:Stage 1:\\[\nY_{ij} = \\beta_{1i} + \\beta_{2i}t_{ij} + \\epsilon_{ij}\n\\]\\(j = 1,..,n_i\\)matrix notation,\\[\n\\mathbf{Y_i} =\n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right); \\mathbf{Z}_i =\n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n. & . \\\\\n1 & t_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\beta_i =\n\\left(\n\\begin{array}\n{c}\n\\beta_{1i} \\\\\n\\beta_{2i}\n\\end{array}\n\\right); \\epsilon_i =\n\\left(\n\\begin{array}\n{c}\n\\epsilon_{i1} \\\\\n. \\\\\n\\epsilon_{in_i}\n\\end{array}\n\\right)\n\\]Thus,\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]Stage 2:\\[\n\\begin{aligned}\n\\beta_{1i} &= \\beta_0 + b_{1i} \\\\\n\\beta_{2i} &= \\beta_1 L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i}\n\\end{aligned}\n\\]\\(L_i, H_i, C_i\\) indicator variables defined 1 subject falls different categories.Subject specific intercepts depend upon treatment, \\(\\beta_0\\) (average response start treatment), \\(\\beta_1 , \\beta_2, \\beta_3\\) (average time effects three treatment groups).\\[\n\\begin{aligned}\n\\mathbf{K}_i &= \\left(\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i\n\\end{array}\n\\right) \\\\\n\\beta &= (\\beta_0 , \\beta_1, \\beta_2, \\beta_3)' \\\\\n\\mathbf{b}_i &=\n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i} \\\\\n\\end{array}\n\\right) \\\\\n\\beta_i &= \\mathbf{K_i \\beta + b_i}\n\\end{aligned}\n\\]get \\(\\hat{\\beta}\\), can fit model sequentially:Estimate \\(\\hat{\\beta_i}\\) first stageEstimate \\(\\hat{\\beta}\\) second stage replacing \\(\\beta_i\\) \\(\\hat{\\beta}_i\\)However, problems arise method:information lost summarizing vector \\(\\mathbf{Y}_i\\) solely \\(\\hat{\\beta}_i\\)need account variability replacing \\(\\beta_i\\) estimatedifferent subjects might different number observations.address problems, can use Linear Mixed Model (Laird Ware 1982)Substituting stage 2 stage 1:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]Let \\(\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i\\) \\(n_i \\times p\\) matrix . , LMM \\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]\\(= 1,..,N\\)\\(\\beta\\) fixed effects, common subjects\\(\\mathbf{b}_i\\) subject specific random effects. \\(\\mathbf{b}_i \\sim N_q (\\mathbf{0,D})\\)\\(\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) independent\\(\\mathbf{Z}_{(n_i \\times q})\\) \\(\\mathbf{X}_{(n_i \\times p})\\) matrices known covariates.Equivalently, hierarchical form, call conditional hierarchical formulation linear mixed model\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i) \\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D})\n\\end{aligned}\n\\]\\(= 1,..,N\\). denote respective functions \\(f(\\mathbf{Y}_i |\\mathbf{b}_i)\\) \\(f(\\mathbf{b}_i)\\)general,\\[\n\\begin{aligned}\nf(,B) &= f(|B)f(B) \\\\\nf() &= \\int f(,B)dB = \\int f(|B) f(B) dB\n\\end{aligned}\n\\]LMM, marginal density \\(\\mathbf{Y}_i\\) \\[\nf(\\mathbf{Y}_i) = \\int f(\\mathbf{Y}_i | \\mathbf{b}_i) f(\\mathbf{b}_i) d\\mathbf{b}_i\n\\]can shown\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X_i \\beta, Z_i DZ'_i + \\Sigma_i})\n\\]marginal formulation linear mixed modelNotes:longer \\(Z_i b_i\\) mean, add error variance (marginal dependence Y). kinda averaging common effect. Technically, shouldn’t call averaging error b (adding variance covariance matrix), called adding random effectContinue example\\[\nY_{ij} = (\\beta_0 + b_{1i}) + (\\beta_1L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i})t_{ij} + \\epsilon_{ij}\n\\]treatment group\\[\nY_{ik}=\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]Intercepts slopes subject specificDifferent treatment groups different slops, intercept.hierarchical model form\\[\n\\begin{aligned}\n\\mathbf{Y}_i | \\mathbf{b}_i &\\sim N(\\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i)\\\\\n\\mathbf{b}_i &\\sim N(\\mathbf{0,D})\n\\end{aligned}\n\\]X form \\[\n\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)'\n\\]\\[\n\\begin{aligned}\n\\mathbf{X}_i &= \\mathbf{Z}_i \\mathbf{K}_i \\\\\n&=\n\\left[\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2} \\\\\n. & . \\\\\n1 & t_{in_i}\n\\end{array}\n\\right]\n\\times\n\\left[\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i \\\\\n\\end{array}\n\\right] \\\\\n&=\n\\left[\n\\begin{array}\n{cccc}\n1 & t_{i1}L_i & t_{i1}H_i & T_{i1}C_i \\\\\n1 & t_{i2}L_i & t_{i2}H_i & T_{i2}C_i \\\\\n. &. &. &. \\\\\n1 & t_{in_i}L_i & t_{in_i}H_i & T_{in_i}C_i \\\\\n\\end{array}\n\\right]\\end{aligned}\n\\]\\[\n\\mathbf{b}_i =\n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i}\n\\end{array}\n\\right)\n\\]\\[\nD =\n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12}\\\\\nd_{12} & d_{22}\n\\end{array}\n\\right)\n\\]Assuming \\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}\\), called conditional independence, meaning response subject independent conditional \\(\\mathbf{b}_i\\) \\(\\beta\\)marginal model form\\[\nY_{ij} = \\beta_0 + \\beta_1 L_i t_{ij} + \\beta_2 H_i t_{ij} + \\beta_3 C_i t_{ij} + \\eta_{ij}\n\\]\\(\\eta_i \\sim N(\\mathbf{0},\\mathbf{Z}_i\\mathbf{DZ}_i'+ \\mathbf{\\Sigma}_i)\\)Equivalently,\\[\n\\mathbf{Y_i \\sim N(X_i \\beta, Z_i DZ_i' + \\Sigma_i})\n\\]case \\(n_i = 2\\)\\[\n\\begin{aligned}\n\\mathbf{Z_iDZ_i'} &=\n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2}\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12} \\\\\nd_{12} & d_{22}\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\n1 & 1 \\\\\nt_{i1} & t_{i2}\n\\end{array}\n\\right) \\\\\n&=\n\\left(\n\\begin{array}\n{cc}\nd_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\\\\nd_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2  \n\\end{array}\n\\right)\n\\end{aligned}\n\\]\\[\nvar(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \\sigma^2\n\\]top correlation errors, marginal implies variance function response quadratic time, positive curvature \\(d_{22}\\)","code":""},{"path":"linear-mixed-models.html","id":"random-intercepts-model","chapter":"8 Linear Mixed Models","heading":"8.1.1 Random-Intercepts Model","text":"remove random slopes,assumption variability subject-specific slopes can attributed treatment differencesthe model random-intercepts model. subject specific intercepts, slopes within treatment group.\\[\n\\begin{aligned}\n\\mathbf{Y}_i | b_i &\\sim N(\\mathbf{X}_i \\beta + 1 b_i , \\Sigma_i) \\\\\nb_i &\\sim N(0,d_{11})\n\\end{aligned}\n\\]marginal model (\\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}\\))\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, 11'd_{11} + \\sigma^2 \\mathbf{})\n\\]marginal covariance matrix \\[\n\\begin{aligned}\ncov(\\mathbf{Y}_i)  &= 11'd_{11} + \\sigma^2I \\\\\n&=\n\\left(\n\\begin{array}\n{cccc}\nd_{11}+ \\sigma^2 & d_{11} & ... & d_{11} \\\\\nd_{11} & d_{11} + \\sigma^2 & d_{11} & ... \\\\\n. & . & . & . \\\\\nd_{11} & ... & ... & d_{11} + \\sigma^2\n\\end{array}\n\\right)\n\\end{aligned}\n\\]associated correlation matrix \\[\ncorr(\\mathbf{Y}_i) =\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & \\rho & ... \\\\\n. & . & . & . \\\\\n\\rho & ... & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho \\equiv \\frac{d_{11}}{d_{11} + \\sigma^2}\\)Thu, haveconstant variance timeequal, positive correlation two measurements subjecta covariance structure called compound symmetry, \\(\\rho\\) called intra-class correlationthat \\(\\rho\\) large, inter-subject variability (\\(d_{11}\\)) large relative intra-subject variability (\\(\\sigma^2\\))","code":""},{"path":"linear-mixed-models.html","id":"covariance-models","chapter":"8 Linear Mixed Models","heading":"8.1.2 Covariance Models","text":"conditional independence assumption, (\\(\\mathbf{\\Sigma_i= \\sigma^2 I_{n_i}}\\)). Consider, \\(\\epsilon_i = \\epsilon_{(1)} + \\epsilon_{(2)}\\), \\(\\epsilon_{(1)}\\) “serial correlation” component. , part individual’s profile response time-varying stochastic processes.\\(\\epsilon_{(2)}\\) measurement error component, independent \\(\\epsilon_{(1)}\\)\\[\n\\mathbf{Y_i = X_i \\beta + Z_i b_i + \\epsilon_{(1)} + \\epsilon_{(2)}}\n\\]\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independent\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independentTo model structure \\(n_i \\times n_i\\) correlation (covariance ) matrix \\(\\mathbf{H}_i\\). Let (j,k)th element \\(\\mathbf{H}_i\\) \\(h_{ijk}= g(t_{ij}t_{ik})\\). function times \\(t_{ij}\\) \\(t_{ik}\\) , assumed function “distance’ times.\\[\nh_{ijk} = g(|t_{ij}-t_{ik}|)\n\\]decreasing function \\(g(.)\\) \\(g(0)=1\\) (correlation matrices).Examples type function:Exponential function: \\(g(|t_{ij}-t_{ik}|) = \\exp(-\\phi|t_{ij} - t_{ik}|)\\)Gaussian function: \\(g(|t_{ij} - t_{ik}|) = \\exp(-\\phi(t_{ij} - t_{ik})^2)\\)Similar structures also used \\(\\mathbf{D}\\) matrix (\\(\\mathbf{b}\\))Example: Autoregressive Covariance StructureA first order Autoregressive Model (AR(1)) form\\[\n\\alpha_t = \\phi \\alpha_{t-1} + \\eta_t\n\\]\\(\\eta_t \\sim iid N (0,\\sigma^2_\\eta)\\), covariance two observations \\[\ncov(\\alpha_t, \\alpha_{t+h}) = \\frac{\\sigma^2_\\eta \\phi^{|h|}}{1- \\phi^2}\n\\]\\(h = 0, \\pm 1, \\pm 2, ...; |\\phi|<1\\)Hence,\\[\ncorr(\\alpha_t, \\alpha_{t+h}) = \\phi^{|h|}\n\\]let \\(\\alpha_T = (\\alpha_1,...\\alpha_T)'\\), \\[\ncorr(\\alpha_T) =\n\\left[\n\\begin{array}\n{ccccc}\n1 & \\phi^1 & \\phi^2 & ... & \\phi^2 \\\\\n\\phi^1 & 1 & \\phi^1 & ... & \\phi^{T-1} \\\\\n\\phi^2 & \\phi^1 & 1 & ... & \\phi^{T-2} \\\\\n. & . & . & . &. \\\\\n\\phi^T & \\phi^{T-1} & \\phi^{T-2} & ... & 1\n\\end{array}\n\\right]\n\\]Notes:correlation decreases time lag increasesThis matrix structure known Toeplitz structureMore complicated covariance structures possible, critical component spatial random effects models time series models.Often, don’t need random effects \\(\\mathbf{b}\\) \\(\\epsilon_{(1)}\\)Time Series section","code":""},{"path":"linear-mixed-models.html","id":"estimation-2","chapter":"8 Linear Mixed Models","heading":"8.2 Estimation","text":"\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\beta, \\mathbf{b}_i, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) must obtain estimation data\\(\\mathbf{\\beta}, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) unknown, fixed, parameters, must estimated data\\(\\mathbf{b}_i\\) random variable. Thus, can’t estimate values, can predict . (.e., can’t estimate random thing).\\(\\hat{\\beta}\\) estimator \\(\\beta\\)\\(\\hat{\\mathbf{b}}_i\\) predictor \\(\\mathbf{b}_i\\),population average estimate \\(\\mathbf{Y}_i\\) \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta}\\)subject-specific prediction \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{b}_i\\)According (Henderson 1975), estimating equations known mixed model equations:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z +B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]\\[\n\\begin{aligned}\n\\mathbf{Y}\n&=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n. \\\\\n\\mathbf{y}_N\n\\end{array}\n\\right] ;\n\\mathbf{X}\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X}_1 \\\\\n. \\\\\n\\mathbf{X}_N\n\\end{array}\n\\right];\n\\mathbf{b} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b}_1 \\\\\n. \\\\\n\\mathbf{b}_N\n\\end{array}\n\\right] ;\n\\epsilon =\n\\left[\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n. \\\\\n\\epsilon_N\n\\end{array}\n\\right]\n\\\\\ncov(\\epsilon) &= \\mathbf{\\Sigma},\n\\mathbf{Z} =\n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{Z}_1 & 0 &  ... & 0 \\\\\n0 & \\mathbf{Z}_2 & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{Z}_n\n\\end{array}\n\\right],\n\\mathbf{B} =\n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{D} & 0 & ... & 0 \\\\\n0 & \\mathbf{D} & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{D}\n\\end{array}\n\\right]\n\\end{aligned}\n\\]model form\\[\n\\begin{aligned}\n\\mathbf{Y} &= \\mathbf{X \\beta + Z b + \\epsilon} \\\\\n\\mathbf{Y} &\\sim N(\\mathbf{X \\beta, ZBZ' + \\Sigma})\n\\end{aligned}\n\\]\\(\\mathbf{V = ZBZ' + \\Sigma}\\), solutions estimating equations can \\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'V^{-1}X)^{-1}X'V^{-1}Y} \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{BZ'V^{-1}(Y-X\\hat{\\beta}})\n\\end{aligned}\n\\]estimate \\(\\hat{\\beta}\\) generalized least squares estimate.predictor, \\(\\hat{\\mathbf{b}}\\) best linear unbiased predictor (BLUP), \\(\\mathbf{b}\\)\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= \\beta \\\\\nvar(\\hat{\\beta}) &= (\\mathbf{X'V^{-1}X})^{-1} \\\\\nE(\\hat{\\mathbf{b}}) &= 0\n\\end{aligned}\n\\]\\[\nvar(\\mathbf{\\hat{b}-b}) = \\mathbf{B-BZ'V^{-1}ZB + BZ'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}B}\n\\]variance variance prediction error (mean squared prediction error, MSPE), meaningful \\(var(\\hat{\\mathbf{b}})\\), since MSPE accounts variance bias prediction.derive mixed model equations, consider\\[\n\\mathbf{\\epsilon = Y - X\\beta - Zb}\n\\]Let \\(T = \\sum_{=1}^N n_i\\) total number observations (.e., length \\(\\mathbf{Y},\\epsilon\\)) \\(Nq\\) length \\(\\mathbf{b}\\). joint distribution \\(\\mathbf{b, \\epsilon}\\) \\[\nf(\\mathbf{b,\\epsilon})= \\frac{1}{(2\\pi)^{(T+ Nq)/2}}\n\\left|\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right| ^{-1/2}\n\\exp\n\\left(\n-\\frac{1}{2}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]\n\\right)\n\\]Maximization \\(f(\\mathbf{b},\\epsilon)\\) respect \\(\\mathbf{b}\\) \\(\\beta\\) requires minimization \\[\n\\begin{aligned}\nQ &=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right] \\\\\n&= \\mathbf{b'B^{-1}b+(Y-X \\beta-Zb)'\\Sigma^{-1}(Y-X \\beta-Zb)}\n\\end{aligned}\n\\]Setting derivatives Q respect \\(\\mathbf{b}\\) \\(\\mathbf{\\beta}\\) zero leads system equations:\\[\n\\begin{aligned}\n\\mathbf{X'\\Sigma^{-1}X\\beta + X'\\Sigma^{-1}Zb} &= \\mathbf{X'\\Sigma^{-1}Y}\\\\\n\\mathbf{(Z'\\Sigma^{-1}Z + B^{-1})b + Z'\\Sigma^{-1}X\\beta} &= \\mathbf{Z'\\Sigma^{-1}Y}\n\\end{aligned}\n\\]Rearranging\\[\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\beta \\\\\n\\mathbf{b}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Thus, solution mixed model equations give:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right] ^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Equivalently,Bayes’ theorem\\[\nf(\\mathbf{b}| \\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b})}{\\int f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b}) d\\mathbf{b}}\n\\]\\(f(\\mathbf{Y}|\\mathbf{b})\\) “likelihood”\\(f(\\mathbf{b})\\) priorthe denominator “normalizing constant”\\(f(\\mathbf{b}|\\mathbf{Y})\\) posterior distributionIn case\\[\n\\begin{aligned}\n\\mathbf{Y} | \\mathbf{b} &\\sim N(\\mathbf{X\\beta+Zb,\\Sigma}) \\\\\n\\mathbf{b} &\\sim N(\\mathbf{0,B})\n\\end{aligned}\n\\]posterior distribution form\\[\n\\mathbf{b}|\\mathbf{Y} \\sim N(\\mathbf{BZ'V^{-1}(Y-X\\beta),(Z'\\Sigma^{-1}Z + B^{-1})^{-1}})\n\\]Hence, best predictor (based squared error loss)\\[\nE(\\mathbf{b}|\\mathbf{Y}) = \\mathbf{BZ'V^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"estimating-mathbfv","chapter":"8 Linear Mixed Models","heading":"8.2.1 Estimating \\(\\mathbf{V}\\)","text":"\\(\\tilde{\\mathbf{V}}\\) (estimate \\(\\mathbf{V}\\)), can estimate:\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(X'\\tilde{V}^{-1}X)^{-1}X'\\tilde{V}^{-1}Y} \\\\\n\\hat{\\mathbf{b}} &= \\mathbf{BZ'\\tilde{V}^{-1}(Y-X\\hat{\\beta})}\n\\end{aligned}\n\\]\\({\\mathbf{b}}\\) EBLUP (estimated BLUP) empirical Bayes estimateNote:\\(\\hat{var}(\\hat{\\beta})\\) consistent estimator \\(var(\\hat{\\beta})\\) \\(\\tilde{\\mathbf{V}}\\) consistent estimator \\(\\mathbf{V}\\)However, \\(\\hat{var}(\\hat{\\beta})\\) biased since variability arises estimating \\(\\mathbf{V}\\) accounted estimate.Hence, \\(\\hat{var}(\\hat{\\beta})\\) underestimates true variabilityWays estimate \\(\\mathbf{V}\\)Maximum Likelihood Estimation (MLE)Restricted Maximum Likelihood (REML)Estimated Generalized Least SquaresBayesian Hierarchical Models (BHM)","code":""},{"path":"linear-mixed-models.html","id":"maximum-likelihood-estimation-mle","chapter":"8 Linear Mixed Models","heading":"8.2.1.1 Maximum Likelihood Estimation (MLE)","text":"Grouping unknown parameters \\(\\Sigma\\) \\(B\\) parameter vector \\(\\theta\\). MLE, \\(\\hat{\\theta}\\) \\(\\hat{\\beta}\\) maximize likelihood \\(\\mathbf{y} \\sim N(\\mathbf{X\\beta, V(\\theta))}\\). Synonymously, \\(-2\\log L(\\mathbf{y;\\theta,\\beta})\\):\\[\n-2l(\\mathbf{\\beta,\\theta,y}) = \\log |\\mathbf{V(\\theta)}| + \\mathbf{(y-X\\beta)'V(\\theta)^{-1}(y-X\\beta)} + N \\log(2\\pi)\n\\]Step 1: Replace \\(\\beta\\) maximum likelihood (\\(\\theta\\) known \\(\\hat{\\beta}= (\\mathbf{X'V(\\theta)^{-1}X)^{-1}X'V(\\theta)^{-1}y}\\)Step 2: Minimize equation respect \\(\\theta\\) get estimator \\(\\hat{\\theta}_{MLE}\\)Step 3: Substitute \\(\\hat{\\theta}_{MLE}\\) back get \\(\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\theta_{MLE})^{-1}X)^{-1}X'V(\\theta_{MLE})^{-1}y}\\)Step 4: Get \\(\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B(\\hat{\\theta}_{MLE})Z'V(\\hat{\\theta}_{MLE})^{-1}(y-X\\hat{\\beta}_{MLE})}\\)Note:\\(\\hat{\\theta}\\) typically negatively biased due unaccounted fixed effects estimated, try account .","code":""},{"path":"linear-mixed-models.html","id":"restricted-maximum-likelihood-reml","chapter":"8 Linear Mixed Models","heading":"8.2.1.2 Restricted Maximum Likelihood (REML)","text":"REML accounts number estimated mean parameters adjusting objective function. Specifically, likelihood linear combination elements \\(\\mathbf{y}\\) accounted .\\(\\mathbf{K'y}\\), \\(\\mathbf{K}\\) \\(N \\times (N - p)\\) full-rank contrast matrix, columns orthogonal \\(\\mathbf{X}\\) matrix (\\(\\mathbf{K'X} = 0\\)). ,\\[\n\\mathbf{K'y} \\sim N(0,\\mathbf{K'V(\\theta)K})\n\\]\\(\\beta\\) longer distributionWe can proceed maximize likelihood contrasts get \\(\\hat{\\theta}_{REML}\\), depend choice \\(\\mathbf{K}\\). \\(\\hat{\\beta}\\) based \\(\\hat{\\theta}\\)Comparison REML MLEBoth methods based upon likelihood principle, desired properties estimates:\nconsistency\nasymptotic normality\nefficiency\nmethods based upon likelihood principle, desired properties estimates:consistencyconsistencyasymptotic normalityasymptotic normalityefficiencyefficiencyML estimation provides estimates fixed effects, REML can’tML estimation provides estimates fixed effects, REML can’tIn balanced models, REML identical ANOVAIn balanced models, REML identical ANOVAREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeChanging \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)Changing \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)REML less sensitive outliers MLEREML less sensitive outliers MLEMLE better REML regarding model comparisons (e.g., AIC BIC)MLE better REML regarding model comparisons (e.g., AIC BIC)","code":""},{"path":"linear-mixed-models.html","id":"estimated-generalized-least-squares","chapter":"8 Linear Mixed Models","heading":"8.2.1.3 Estimated Generalized Least Squares","text":"MLE REML rely upon Gaussian assumption. overcome issue, EGLS uses first second moments.\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\epsilon_i \\sim (\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i \\sim (\\mathbf{0,D})\\)\\(cov(\\epsilon_i, \\mathbf{b}_i) = 0\\)EGLS estimator \\[\n\\begin{aligned}\n\\hat{\\beta}_{GLS} &= \\{\\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}X_i}  \\}^{-1} \\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}Y_i} \\\\\n&=\\{\\mathbf{X'V(\\theta)^{-1}X} \\}^{-1} \\mathbf{X'V(\\theta)^{-1}Y}\n\\end{aligned}\n\\]depends first two moments\\(E(\\mathbf{Y}_i) = \\mathbf{X}_i \\beta\\)\\(var(\\mathbf{Y}_i)= \\mathbf{V}_i\\)EGLS use \\(\\hat{\\mathbf{V}}\\) \\(\\mathbf{V(\\theta)}\\)\\[\n\\hat{\\beta}_{EGLS} = \\{ \\mathbf{X'\\hat{V}^{-1}X} \\}^{-1} \\mathbf{X'\\hat{V}^{-1}Y}\n\\]Hence, fixed effects estimators MLE, REML, EGLS form, except estimate \\(\\mathbf{V}\\)case non-iterative approach, EGLS can appealing \\(\\mathbf{V}\\) can estimated without much computational burden.","code":""},{"path":"linear-mixed-models.html","id":"bayesian-hierarchical-models-bhm","chapter":"8 Linear Mixed Models","heading":"8.2.1.4 Bayesian Hierarchical Models (BHM)","text":"Joint distribution cane decomposed hierarchically terms product conditional distributions marginal distribution\\[\nf(,B,C) = f(|B,C) f(B|C)f(C)\n\\]Applying estimate \\(\\mathbf{V}\\)\\[\n\\begin{aligned}\nf(\\mathbf{Y, \\beta, b, \\theta}) &= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta,\\beta})f(\\mathbf{\\beta|\\theta})f(\\mathbf{\\theta}) & \\text{based probability decomposition} \\\\\n&= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta}) & \\text{based simplifying modeling assumptions}\n\\end{aligned}\n\\]elaborate second equality, assume conditional independence (e.g., given \\(\\theta\\), additional info \\(\\mathbf{b}\\) given knowing \\(\\beta\\)), can simply first equalityUsing Bayes’ rule\\[\nf(\\mathbf{\\beta, b, \\theta|Y}) \\propto f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta})\n\\]\\[\n\\begin{aligned}\n\\mathbf{Y| \\beta, b, \\theta} &\\sim \\mathbf{N(X\\beta+ Zb, \\Sigma(\\theta))} \\\\\n\\mathbf{b | \\theta} &\\sim \\mathbf{N(0, B(\\theta))}\n\\end{aligned}\n\\]also prior distributions \\(f(\\beta), f(\\theta)\\)normalizing constant, can obtain posterior distribution. Typically, can’t get analytical solution right away. Hence, can use Markov Chain Monte Carlo (MCMC) obtain samples posterior distribution.Bayesian Methods:account uncertainty parameters estimates accommodate propagation uncertainty modelcan adjust prior information (.e., priori) parametersCan extend beyond Gaussian distributionsbut hard implement algorithms might problem converging","code":""},{"path":"linear-mixed-models.html","id":"inference-3","chapter":"8 Linear Mixed Models","heading":"8.3 Inference","text":"","code":""},{"path":"linear-mixed-models.html","id":"parameters-beta","chapter":"8 Linear Mixed Models","heading":"8.3.1 Parameters \\(\\beta\\)","text":"","code":""},{"path":"linear-mixed-models.html","id":"wald-test-GLMM","chapter":"8 Linear Mixed Models","heading":"8.3.1.1 Wald test","text":"\\[\n\\begin{aligned}\n\\mathbf{\\hat{\\beta}(\\theta)} &= \\mathbf{\\{X'V^{-1}(\\theta) X\\}^{-1}X'V^{-1}(\\theta) Y} \\\\\nvar(\\hat{\\beta}(\\theta)) &= \\mathbf{\\{X'V^{-1}(\\theta) X\\}^{-1}}\n\\end{aligned}\n\\]can use \\(\\hat{\\theta}\\) place \\(\\theta\\) approximate Wald test\\[\nH_0: \\mathbf{\\beta =d}\n\\]\\[\nW = \\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}\n\\]\\(W \\sim \\chi^2_{rank()}\\) \\(H_0\\) true. However, take account variability using \\(\\hat{\\theta}\\) place \\(\\theta\\), hence standard errors underestimated","code":""},{"path":"linear-mixed-models.html","id":"f-test-1","chapter":"8 Linear Mixed Models","heading":"8.3.1.2 F-test","text":"Alternatively, can use modified F-test, suppose \\(var(\\mathbf{Y}) = \\sigma^2 \\mathbf{V}(\\theta)\\), \\[\nF^* = \\frac{\\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}}{\\hat{\\sigma}^2 \\text{rank}()}\n\\]\\(F^* \\sim f_{rank(), den(df)}\\) null hypothesis. den(df) needs approximated data either:Satterthwaite methodKenward-Roger approximationUnder balanced cases, Wald F tests similar. small sample sizes, can differ p-values. can reduced t-test single \\(\\beta\\)","code":""},{"path":"linear-mixed-models.html","id":"likelihood-ratio-test","chapter":"8 Linear Mixed Models","heading":"8.3.1.3 Likelihood Ratio Test","text":"\\[\nH_0: \\beta \\\\Theta_{\\beta,0}\n\\]\\(\\Theta_{\\beta, 0}\\) subspace parameter space, \\(\\Theta_{\\beta}\\) fixed effects \\(\\beta\\) . \\[\n-2\\log \\lambda_N = -2\\log\\{\\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}}\\}\n\\]\\(\\hat{L}_{ML,0}\\) , \\(\\hat{L}_{ML}\\) maximized likelihood obtained maximizing \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)\\(-2 \\log \\lambda_N \\dot{\\sim} \\chi^2_{df}\\) df difference dimension (.e., number parameters) \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)method applicable REML. REML can still used test covariance parameters nested models.","code":""},{"path":"linear-mixed-models.html","id":"variance-components","chapter":"8 Linear Mixed Models","heading":"8.3.2 Variance Components","text":"ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesFor ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesWald test variance components analogous fixed effects case (see 8.3.1.1 )\nHowever, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))\nTypically works better covariance parameter, variance parameters.\nWald test variance components analogous fixed effects case (see 8.3.1.1 )However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))Typically works better covariance parameter, variance parameters.Typically works better covariance parameter, variance parameters.likelihood ratio tests can also used ML REML estimates. However, problem parametersThe likelihood ratio tests can also used ML REML estimates. However, problem parameters","code":""},{"path":"linear-mixed-models.html","id":"information-criteria","chapter":"8 Linear Mixed Models","heading":"8.4 Information Criteria","text":"account likelihood number parameters assess model comparison.","code":""},{"path":"linear-mixed-models.html","id":"akaikes-information-criteria-aic","chapter":"8 Linear Mixed Models","heading":"8.4.1 Akaike’s Information Criteria (AIC)","text":"Derived estimator expected Kullback discrepancy true model fitted candidate model\\[\nAIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + 2q\n\\]\\(l(\\hat{\\theta}, \\hat{\\beta})\\) log-likelihoodq = effective number parameters; total fixed associated random effects (variance/covariance; estimated boundary constraint)Note:comparing models differ random effects, method advised due inability get correct number effective parameters).prefer smaller AIC values.program uses \\(l-q\\) prefer larger AIC values (rarely).can used mixed model section, (e.g., selection covariance structure), sample size must large adequate comparison based criterionCan large negative bias (e.g., sample size small number parameters large) due penalty term can’t approximate bias adjustment adequately","code":""},{"path":"linear-mixed-models.html","id":"corrected-aic-aicc","chapter":"8 Linear Mixed Models","heading":"8.4.2 Corrected AIC (AICC)","text":"developed (Hurvich Tsai 1989)correct small-sample adjustmentdepends candidate model classOnly fixed covariance structure, AICC justified, general covariance structure","code":""},{"path":"linear-mixed-models.html","id":"bayesian-information-criteria-bic","chapter":"8 Linear Mixed Models","heading":"8.4.3 Bayesian Information Criteria (BIC)","text":"\\[\nBIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + q \\log n\n\\]n = number observations.prefer smaller BIC valueBIC AIC used REML MLE mean structure. Otherwise, general, prefer MLEWith example presented beginning Linear Mixed Models,\\[\nY_{ik}=\n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]\\(= 1,..,N\\)\\(j = 1,..,n_i\\) (measures time \\(t_{ij}\\))Note:subject-specific intercepts,\\[\n\\begin{aligned}\n\\mathbf{Y}_i |b_i &\\sim N(\\mathbf{X}_i \\beta + 1 b_i, \\sigma^2 \\mathbf{}) \\\\\nb_i &\\sim N(0,d_{11})\n\\end{aligned}\n\\], want estimate \\(\\beta, \\sigma^2, d_{11}\\) predict \\(b_i\\)","code":""},{"path":"linear-mixed-models.html","id":"split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5 Split-Plot Designs","text":"Typically used case two factors one needs much larger units .Example:: 3 levels (large units)B: 2 levels (small units)B levels randomized 4 blocks.differs Randomized Block Designs. block, one 6 (3x2) treatment combinations. Randomized Block Designs assign block randomly, split-plot randomize step.Moreover, needs applied large units, factor applied block B can applied multiple times.Hence, modelIf factor interest\\[\nY_{ij} = \\mu + \\rho_i + \\alpha_j + e_{ij}\n\\]\\(\\) = replication (block subject)\\(j\\) = level Factor \\(\\mu\\) = overall mean\\(\\rho_i\\) = variation due \\(\\)-th block\\(e_{ij} \\sim N(0, \\sigma^2_e)\\) = whole plot errorIf B factor interest\\[\nY_{ijk} = \\mu + \\phi_{ij} + \\beta_k + \\epsilon_{ijk}\n\\]\\(\\phi_{ij}\\) = variation due \\(ij\\)-th main plot\\(\\beta_k\\) = Factor B effect\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = subplot error\\(\\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\\)Together, split-plot model\\[\nY_{ijk} = \\mu + \\rho_i + \\alpha_j + e_{ij} + \\beta_k + (\\alpha \\beta)_{jk} + \\epsilon_{ijk}\n\\]\\(\\) = replicate (blocks subjects)\\(j\\) = level factor \\(k\\) = level factor B\\(\\mu\\) = overall mean\\(\\rho_i\\) = effect block\\(\\alpha_j\\) = main effect factor (fixed)\\(e_{ij} = (\\rho \\alpha)_{ij}\\) = block factor interaction (whole plot error, random)\\(\\beta_k\\) = main effect factor B (fixed)\\((\\alpha \\beta)_{jk}\\) = interaction factors B (fixed)\\(\\epsilon_{ijk}\\) = subplot error (random)can approach sub-plot analysis based onthe ANOVA perspective\nWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\n\nSub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\n\nANOVA perspectiveWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\nWhole plot comparisonsCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Sub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\nSub-plot comparisons:Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))mixed model perspectivethe mixed model perspective\\[\n\\mathbf{Y = X \\beta + Zb + \\epsilon}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"application-4","chapter":"8 Linear Mixed Models","heading":"8.5.1 Application","text":"","code":""},{"path":"linear-mixed-models.html","id":"example-1","chapter":"8 Linear Mixed Models","heading":"8.5.1.1 Example 1","text":"\\[\ny_{ijk} = \\mu + i_i + v_j + (iv)_{ij} + f_k + \\epsilon_{ijk}\n\\]\\(y_{ijk}\\) = observed yield\\(\\mu\\) = overall average yield\\(i_i\\) = irrigation effect\\(v_j\\) = variety effect\\((iv)_{ij}\\) = irrigation variety interaction\\(f_k\\) = random field (block) effect\\(\\epsilon_{ijk}\\) = residualbecause variety-field combination observed , can’t random interaction effects variety fieldSince p-value interaction term insignificant, consider fitting without .Since \\(p\\)-value \\(\\chi^2\\) test insignificant, can’t reject additive model already sufficient. Looking AIC BIC, can also see prefer additive modelRandom Effect ExaminationexactRLRT test\\(H_0\\): Var(random effect) (.e., \\(\\sigma^2\\))= 0\\(H_a\\): Var(random effect) (.e., \\(\\sigma^2\\)) > 0Since p-value significant, reject \\(H_0\\)","code":"\nlibrary(ggplot2)\ndata(irrigation, package = \"faraway\")\nsummary(irrigation)\n#>      field   irrigation variety     yield      \n#>  f1     :2   i1:4       v1:8    Min.   :34.80  \n#>  f2     :2   i2:4       v2:8    1st Qu.:37.60  \n#>  f3     :2   i3:4               Median :40.15  \n#>  f4     :2   i4:4               Mean   :40.23  \n#>  f5     :2                      3rd Qu.:42.73  \n#>  f6     :2                      Max.   :47.60  \n#>  (Other):4\nhead(irrigation, 4)\n#>   field irrigation variety yield\n#> 1    f1         i1      v1  35.4\n#> 2    f1         i1      v2  37.9\n#> 3    f2         i2      v1  36.7\n#> 4    f2         i2      v2  38.2\nggplot(irrigation,\n       aes(\n         x     = field,\n         y     = yield,\n         shape = irrigation,\n         color = variety\n       )) +\n  geom_point(size = 3)\nsp_model <-\n    lmerTest::lmer(yield ~ irrigation * variety \n                   + (1 |field), irrigation)\nsummary(sp_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ irrigation * variety + (1 | field)\n#>    Data: irrigation\n#> \n#> REML criterion at convergence: 45.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -0.7448 -0.5509  0.0000  0.5509  0.7448 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  field    (Intercept) 16.200   4.025   \n#>  Residual              2.107   1.452   \n#> Number of obs: 16, groups:  field, 8\n#> \n#> Fixed effects:\n#>                        Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***\n#> irrigationi2              1.200      4.279  4.487   0.280 0.791591    \n#> irrigationi3              0.700      4.279  4.487   0.164 0.877156    \n#> irrigationi4              3.500      4.279  4.487   0.818 0.454584    \n#> varietyv2                 0.600      1.452  4.000   0.413 0.700582    \n#> irrigationi2:varietyv2   -0.400      2.053  4.000  -0.195 0.855020    \n#> irrigationi3:varietyv2   -0.200      2.053  4.000  -0.097 0.927082    \n#> irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2\n#> irrigation2 -0.707                                          \n#> irrigation3 -0.707  0.500                                   \n#> irrigation4 -0.707  0.500  0.500                            \n#> varietyv2   -0.240  0.170  0.170  0.170                     \n#> irrgtn2:vr2  0.170 -0.240 -0.120 -0.120 -0.707              \n#> irrgtn3:vr2  0.170 -0.120 -0.240 -0.120 -0.707  0.500       \n#> irrgtn4:vr2  0.170 -0.120 -0.120 -0.240 -0.707  0.500  0.500\n\nanova(sp_model, ddf = c(\"Kenward-Roger\"))\n#> Type III Analysis of Variance Table with Kenward-Roger's method\n#>                    Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> irrigation         2.4545 0.81818     3     4  0.3882 0.7685\n#> variety            2.2500 2.25000     1     4  1.0676 0.3599\n#> irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612\nlibrary(lme4)\nsp_model_additive <- lmer(yield ~ irrigation + variety \n                          + (1 | field), irrigation)\nanova(sp_model_additive,sp_model,ddf = \"Kenward-Roger\")\n#> Data: irrigation\n#> Models:\n#> sp_model_additive: yield ~ irrigation + variety + (1 | field)\n#> sp_model: yield ~ irrigation * variety + (1 | field)\n#>                   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n#> sp_model_additive    7 83.959 89.368 -34.980   69.959                     \n#> sp_model            10 88.609 96.335 -34.305   68.609 1.3503  3     0.7172\nsp_model <- lme4::lmer(yield ~ irrigation * variety \n                       + (1 | field), irrigation)\nlibrary(RLRsim)\nexactRLRT(sp_model)\n#> \n#>  simulated finite sample distribution of RLRT.\n#>  \n#>  (p-value based on 10000 simulated values)\n#> \n#> data:  \n#> RLRT = 6.1118, p-value = 0.0087"},{"path":"linear-mixed-models.html","id":"repeated-measures-in-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.6 Repeated Measures in Mixed Models","text":"\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\delta_{(k)}+ \\epsilon_{ijk}\n\\]\\(\\)-th group (fixed)\\(j\\)-th (repeated measure) time effect (fixed)\\(k\\)-th subject\\(\\delta_{(k)} \\sim N(0,\\sigma^2_\\delta)\\) (k-th subject \\(\\)-th group) \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent error) random effects (\\(= 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\\))hence, variance-covariance matrix repeated observations k-th subject -th group, \\(\\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})'\\), \\[\n\\begin{aligned}\n\\mathbf{\\Sigma}_{subject} &=\n\\left(\n\\begin{array}\n{cccc}\n\\sigma^2_\\delta + \\sigma^2 & \\sigma^2_\\delta & ... & \\sigma^2_\\delta \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta +\\sigma^2 & ... & \\sigma^2_\\delta \\\\\n. & . & . & . \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta & ... & \\sigma^2_\\delta + \\sigma^2 \\\\\n\\end{array}\n\\right) \\\\\n&= (\\sigma^2_\\delta + \\sigma^2)\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & ... & \\rho \\\\\n. & . & . & . \\\\\n\\rho & \\rho & ... & 1 \\\\\n\\end{array}\n\\right)\n& \\text{product scalar correlation matrix}\n\\end{aligned}\n\\]\\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\), compound symmetry structure discussed Random-Intercepts ModelBut repeated measurements subject time, AR(1) structure might appropriateMixed model repeated measure\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) combines random error whole subplots.general,\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\epsilon \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) \\(\\mathbf{\\Sigma}\\) block diagonal random error covariance subjectThe variance covariance matrix AR(1) structure \\[\n\\mathbf{\\Sigma}_{subject} =\n\\sigma^2\n\\left(\n\\begin{array}\n{ccccc}\n1  & \\rho & \\rho^2 & ... & \\rho^{n_B-1} \\\\\n\\rho & 1 & \\rho & ... & \\rho^{n_B-2} \\\\\n. & . & . & . & . \\\\\n\\rho^{n_B-1} & \\rho^{n_B-2} & \\rho^{n_B-3} & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]Hence, mixed model repeated measure can written \\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) = random error whole subplotsGenerally,\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\epsilon \\sim N(0, \\mathbf{\\sigma^2 \\Sigma})\\) \\(\\Sigma\\) = block diagonal random error covariance subject.","code":""},{"path":"linear-mixed-models.html","id":"unbalanced-or-unequally-spaced-data","chapter":"8 Linear Mixed Models","heading":"8.7 Unbalanced or Unequally Spaced Data","text":"Consider model\\[\nY_{ikt} = \\beta_0 + \\beta_{0i} + \\beta_{1}t + \\beta_{1i}t + \\beta_{2} t^2 + \\beta_{2i} t^2 + \\epsilon_{ikt}\n\\]\\(= 1,2\\) (groups)\\(k = 1,…, n_i\\) ( individuals)\\(t = (t_1,t_2,t_3,t_4)\\) (times)\\(\\beta_{2i}\\) = common quadratic term\\(\\beta_{1i}\\) = common linear time trends\\(\\beta_{0i}\\) = common interceptsThen, assume variance-covariance matrix repeated measurements collected particular subject time form\\[\n\\mathbf{\\Sigma}_{ik} = \\sigma^2\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho^{t_2-t_1} & \\rho^{t_3-t_1} & \\rho^{t_4-t_1} \\\\\n\\rho^{t_2-t_1} & 1 & \\rho^{t_3-t_2} & \\rho^{t_4-t_2} \\\\\n\\rho^{t_3-t_1} & \\rho^{t_3-t_2} & 1 & \\rho^{t_4-t_3} \\\\\n\\rho^{t_4-t_1} & \\rho^{t_4-t_2} & \\rho^{t_4-t_3} & 1\n\\end{array}\n\\right)\n\\]called “power” covariance modelWe can consider \\(\\beta_{2i} , \\beta_{1i}, \\beta_{0i}\\) accordingly see whether terms needed final model","code":""},{"path":"linear-mixed-models.html","id":"application-5","chapter":"8 Linear Mixed Models","heading":"8.8 Application","text":"R Packages mixed modelsnlme\nnested structure\nflexible complex design\nuser-friendly\nnlmehas nested structurehas nested structureflexible complex designflexible complex designnot user-friendlynot user-friendlylme4\ncomputationally efficient\nuser-friendly\ncan handle non-normal response\ndetailed application, check Fitting Linear Mixed-Effects Models Using lme4\nlme4computationally efficientcomputationally efficientuser-friendlyuser-friendlycan handle non-normal responsecan handle non-normal responsefor detailed application, check Fitting Linear Mixed-Effects Models Using lme4for detailed application, check Fitting Linear Mixed-Effects Models Using lme4Others\nBayesian setting: MCMCglmm, brms\ngenetics: ASReml\nOthersBayesian setting: MCMCglmm, brmsBayesian setting: MCMCglmm, brmsFor genetics: ASRemlFor genetics: ASReml","code":""},{"path":"linear-mixed-models.html","id":"example-1-pulps","chapter":"8 Linear Mixed Models","heading":"8.8.1 Example 1 (Pulps)","text":"Model:\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\\(= 1,..,\\) groups random effect \\(\\alpha_i\\)\\(j = 1,...,n\\) individuals group\\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) random effects\\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) random effectsImply compound symmetry model intraclass correlation coefficient : \\(\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\epsilon}\\)factor \\(\\) explain much variation, low correlation within levels: \\(\\sigma^2_\\alpha \\0\\) \\(\\rho \\0\\)factor \\(\\) explain much variation, high correlation within levels \\(\\sigma^2_\\alpha \\\\infty\\) hence, \\(\\rho \\1\\)lmer applicationTo Satterthwaite approximation denominator df, use lmerTestIn example, can see confidence interval computed confint lmer package close confint lmerTest model.MCMglmm applicationunder Bayesian frameworkthis method offers confidence interval slightly positive lmer lmerTest","code":"\ndata(pulp, package = \"faraway\")\nplot(\n    y    = pulp$bright,\n    x    = pulp$operator,\n    xlab = \"Operator\",\n    ylab = \"Brightness\"\n)\npulp %>% dplyr::group_by(operator) %>% \n    dplyr::summarise(average = mean(bright))\n#> # A tibble: 4 × 2\n#>   operator average\n#>   <fct>      <dbl>\n#> 1 a           60.2\n#> 2 b           60.1\n#> 3 c           60.6\n#> 4 d           60.7\nlibrary(lme4)\nmixed_model <-\n    lmer(\n        # pipe (i..e, | ) denotes random-effect terms\n        formula = bright ~ 1 + (1 |operator), \n         data = pulp)\nsummary(mixed_model)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: bright ~ 1 + (1 | operator)\n#>    Data: pulp\n#> \n#> REML criterion at convergence: 18.6\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4666 -0.7595 -0.1244  0.6281  1.6012 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  operator (Intercept) 0.06808  0.2609  \n#>  Residual             0.10625  0.3260  \n#> Number of obs: 20, groups:  operator, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept)  60.4000     0.1494   404.2\ncoef(mixed_model)\n#> $operator\n#>   (Intercept)\n#> a    60.27806\n#> b    60.14088\n#> c    60.56767\n#> d    60.61340\n#> \n#> attr(,\"class\")\n#> [1] \"coef.mer\"\nfixef(mixed_model)   # fixed effects\n#> (Intercept) \n#>        60.4\nconfint(mixed_model) # confidence interval\n#>                 2.5 %     97.5 %\n#> .sig01       0.000000  0.6178987\n#> .sigma       0.238912  0.4821845\n#> (Intercept) 60.071299 60.7287012\nranef(mixed_model)   # random effects\n#> $operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n#> \n#> with conditional variances for \"operator\"\nVarCorr(mixed_model) # random effects standard deviation\n#>  Groups   Name        Std.Dev.\n#>  operator (Intercept) 0.26093 \n#>  Residual             0.32596\nre_dat = as.data.frame(VarCorr(mixed_model))\n\n# rho based on the above formula\nrho = re_dat[1, 'vcov'] / (re_dat[1, 'vcov'] + re_dat[2, 'vcov'])\nrho\n#> [1] 0.3905354\nlibrary(lmerTest)\nsummary(lmerTest::lmer(bright ~ 1 + (1 | operator), pulp))$coefficients\n#>             Estimate Std. Error df  t value     Pr(>|t|)\n#> (Intercept)     60.4  0.1494434  3 404.1664 3.340265e-08\nconfint(mixed_model)[3, ]\n#>   2.5 %  97.5 % \n#> 60.0713 60.7287\nlibrary(MCMCglmm)\nmixed_model_bayes <-\n    MCMCglmm(\n        bright ~ 1,\n        random =  ~ operator,\n        data = pulp,\n        verbose = FALSE\n    )\nsummary(mixed_model_bayes)$solutions\n#>             post.mean l-95% CI u-95% CI eff.samp pMCMC\n#> (Intercept)  60.40449  60.2055 60.66595     1000 0.001"},{"path":"linear-mixed-models.html","id":"prediction","chapter":"8 Linear Mixed Models","heading":"8.8.1.1 Prediction","text":"use bootMer() get bootstrap-based confidence intervals predictions.Another example using GLMM context blockingPenicillin dataExamine treatment effectSince p-value greater 0.05, can’t reject null hypothesis treatment effect.Since p-value greater 0.05, consistent previous observation, conclude can’t reject null hypothesis treatment effect.","code":"\n# random effects prediction (BLUPs)\nranef(mixed_model)$operator\n#>   (Intercept)\n#> a  -0.1219403\n#> b  -0.2591231\n#> c   0.1676679\n#> d   0.2133955\n\n# prediction for each categories\nfixef(mixed_model) + ranef(mixed_model)$operator \n#>   (Intercept)\n#> a    60.27806\n#> b    60.14088\n#> c    60.56767\n#> d    60.61340\n\n# equivalent to the above method\npredict(mixed_model, newdata = data.frame(operator = c('a', 'b', 'c', 'd'))) \n#>        1        2        3        4 \n#> 60.27806 60.14088 60.56767 60.61340\ndata(penicillin, package = \"faraway\")\nsummary(penicillin)\n#>  treat    blend       yield   \n#>  A:5   Blend1:4   Min.   :77  \n#>  B:5   Blend2:4   1st Qu.:81  \n#>  C:5   Blend3:4   Median :87  \n#>  D:5   Blend4:4   Mean   :86  \n#>        Blend5:4   3rd Qu.:89  \n#>                   Max.   :97\nlibrary(ggplot2)\nggplot(penicillin, aes(\n    y     = yield,\n    x     = treat,\n    shape = blend,\n    color = blend\n)) + \n    # treatment = fixed effect\n    # blend = random effects\n    geom_point(size = 3) +\n    xlab(\"Treatment\")\n\nlibrary(lmerTest) # for p-values\nmixed_model <- lmerTest::lmer(yield ~ treat + (1 | blend),\n                              data = penicillin)\nsummary(mixed_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: yield ~ treat + (1 | blend)\n#>    Data: penicillin\n#> \n#> REML criterion at convergence: 103.8\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4152 -0.5017 -0.1644  0.6830  1.2836 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  blend    (Intercept) 11.79    3.434   \n#>  Residual             18.83    4.340   \n#> Number of obs: 20, groups:  blend, 5\n#> \n#> Fixed effects:\n#>             Estimate Std. Error     df t value Pr(>|t|)    \n#> (Intercept)   84.000      2.475 11.075  33.941 1.51e-12 ***\n#> treatB         1.000      2.745 12.000   0.364   0.7219    \n#> treatC         5.000      2.745 12.000   1.822   0.0935 .  \n#> treatD         2.000      2.745 12.000   0.729   0.4802    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>        (Intr) treatB treatC\n#> treatB -0.555              \n#> treatC -0.555  0.500       \n#> treatD -0.555  0.500  0.500\n\n#The BLUPs for the each blend\nranef(mixed_model)$blend\n#>        (Intercept)\n#> Blend1   4.2878788\n#> Blend2  -2.1439394\n#> Blend3  -0.7146465\n#> Blend4   1.4292929\n#> Blend5  -2.8585859\nanova(mixed_model) # p-value based on lmerTest\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>       Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> treat     70  23.333     3    12  1.2389 0.3387\nlibrary(pbkrtest)\n# REML is not appropriate for testing fixed effects, it should be ML\nfull_model <-\n    lmer(yield ~ treat + (1 | blend), \n         penicillin, \n         REML = FALSE) \nnull_model <- lmer(yield ~ 1 + (1 | blend), \n                   penicillin, \n                   REML = FALSE)\n\n# use  Kenward-Roger approximation for df\nKRmodcomp(full_model, null_model) \n#> large : yield ~ treat + (1 | blend)\n#> small : yield ~ 1 + (1 | blend)\n#>          stat     ndf     ddf F.scaling p.value\n#> Ftest  1.2389  3.0000 12.0000         1  0.3387"},{"path":"linear-mixed-models.html","id":"example-2-rats","chapter":"8 Linear Mixed Models","heading":"8.8.2 Example 2 (Rats)","text":"interested whether treatment effect induces changes time.Since p-value significant, can confident concluding treatment effect","code":"\nrats <- read.csv(\n    \"images/rats.dat\",\n    header = F,\n    sep = ' ',\n    col.names = c('Treatment', 'rat', 'age', 'y')\n)\n\n# log transformed age\nrats$t <- log(1 + (rats$age - 45) / 10) \nrat_model <-\n    # treatment = fixed effect, rat = random effects\n    lmerTest::lmer(y ~ t:Treatment + (1 | rat), \n                   data = rats) \nsummary(rat_model)\n#> Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#> lmerModLmerTest]\n#> Formula: y ~ t:Treatment + (1 | rat)\n#>    Data: rats\n#> \n#> REML criterion at convergence: 932.4\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.25574 -0.65898 -0.01163  0.58356  2.88309 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  rat      (Intercept) 3.565    1.888   \n#>  Residual             1.445    1.202   \n#> Number of obs: 252, groups:  rat, 50\n#> \n#> Fixed effects:\n#>                Estimate Std. Error       df t value Pr(>|t|)    \n#> (Intercept)     68.6074     0.3312  89.0275  207.13   <2e-16 ***\n#> t:Treatmentcon   7.3138     0.2808 247.2762   26.05   <2e-16 ***\n#> t:Treatmenthig   6.8711     0.2276 247.7097   30.19   <2e-16 ***\n#> t:Treatmentlow   7.5069     0.2252 247.5196   33.34   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) t:Trtmntc t:Trtmnth\n#> t:Tretmntcn -0.327                    \n#> t:Tretmnthg -0.340  0.111             \n#> t:Tretmntlw -0.351  0.115     0.119\nanova(rat_model)\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>             Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \n#> t:Treatment 3181.9  1060.6     3 223.21  734.11 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"example-3-agridat","chapter":"8 Linear Mixed Models","heading":"8.8.3 Example 3 (Agridat)","text":"Remove outliersPlot age specieslme function nlme packagelmer function lme4 packageNotes:|| double pipes= uncorrelated random effects|| double pipes= uncorrelated random effectsTo remove intercept term:\n(0+ti|tree)\n(ti-1|tree)\nremove intercept term:(0+ti|tree)(0+ti|tree)(ti-1|tree)(ti-1|tree)include structured covariance terms, can use following way","code":"\nlibrary(agridat)\nlibrary(latticeExtra)\ndat <- harris.wateruse\n# Compare to Schabenberger & Pierce, fig 7.23\nuseOuterStrips(\n    xyplot(\n        water ~ day | species * age,\n        dat,\n        as.table = TRUE,\n        group = tree,\n        type = c('p', 'smooth'),\n        main = \"harris.wateruse 2 species, 2 ages (10 trees each)\"\n    )\n)\ndat <- subset(dat, day!=268)\nxyplot(\n    water ~ day | tree,\n    dat,\n    subset   = age == \"A2\" & species == \"S2\",\n    as.table = TRUE,\n    type     = c('p', 'smooth'),\n    ylab     = \"Water use profiles of individual trees\",\n    main     = \"harris.wateruse (Age 2, Species 2)\"\n)\n# Rescale day for nicer output, \n# and convergence issues, add quadratic term\ndat <- transform(dat, ti = day / 100)\ndat <- transform(dat, ti2 = ti * ti)\n# Start with a subgroup: age 2, species 2\nd22 <- droplevels(subset(dat, age == \"A2\" & species == \"S2\"))\nlibrary(nlme)\n\n## We use pdDiag() to get uncorrelated random effects\nm1n <- lme(\n    water ~ 1 + ti + ti2,\n    #intercept, time and time-squared = fixed effects\n    data = d22,\n    na.action = na.omit,\n    random = list(tree = pdDiag(~ 1 + ti + ti2)) \n    # random intercept, time \n    # and time squared per tree = random effects\n)\nranef(m1n)\n#>     (Intercept)            ti           ti2\n#> T04   0.1985796  1.609864e-09  4.990101e-10\n#> T05   0.3492827  2.487690e-10 -4.845287e-11\n#> T19  -0.1978989 -7.681202e-10 -1.961453e-10\n#> T23   0.4519003 -3.270426e-10 -2.413583e-10\n#> T38  -0.6457494 -1.608770e-09 -3.298010e-10\n#> T40   0.3739432  3.264705e-10 -2.543109e-11\n#> T49   0.8620648  9.021831e-10 -5.402247e-12\n#> T53  -0.5655049 -8.279040e-10 -4.579291e-11\n#> T67  -0.4394623 -3.485113e-10  2.147434e-11\n#> T71  -0.3871552  7.930610e-10  3.718993e-10\nfixef(m1n)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nsummary(m1n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC     BIC    logLik\n#>   276.5142 300.761 -131.2571\n#> \n#> Random effects:\n#>  Formula: ~1 + ti + ti2 | tree\n#>  Structure: Diagonal\n#>         (Intercept)           ti          ti2  Residual\n#> StdDev:   0.5187869 1.438333e-05 3.864019e-06 0.3836614\n#> \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -10.798799 0.8814666 227 -12.25094       0\n#> ti           12.346704 0.7827112 227  15.77428       0\n#> ti2          -2.838503 0.1720614 227 -16.49704       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.979       \n#> ti2  0.970 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.07588246 -0.58531056  0.01210209  0.65402695  3.88777402 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10\nm1lmer <-\n    lmer(water ~ 1 + ti + ti2 + (ti + ti2 ||\n                                     tree),\n         data = d22,\n         na.action = na.omit)\nranef(m1lmer)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\nfixef(m1lmer)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm1l <-\n    lmer(water ~ 1 + ti + ti2 \n         + (1 | tree) + (0 + ti | tree) \n         + (0 + ti2 | tree), data = d22)\nranef(m1l)\n#> $tree\n#>     (Intercept) ti ti2\n#> T04   0.1985796  0   0\n#> T05   0.3492827  0   0\n#> T19  -0.1978989  0   0\n#> T23   0.4519003  0   0\n#> T38  -0.6457494  0   0\n#> T40   0.3739432  0   0\n#> T49   0.8620648  0   0\n#> T53  -0.5655049  0   0\n#> T67  -0.4394623  0   0\n#> T71  -0.3871552  0   0\n#> \n#> with conditional variances for \"tree\"\nfixef(m1l)\n#> (Intercept)          ti         ti2 \n#>  -10.798799   12.346704   -2.838503\nm2n <- lme(\n    water ~ 1 + ti + ti2,\n    data = d22,\n    random = ~ 1 | tree,\n    cor = corExp(form =  ~ day | tree),\n    na.action = na.omit\n)\nranef(m2n)\n#>     (Intercept)\n#> T04   0.1929971\n#> T05   0.3424631\n#> T19  -0.1988495\n#> T23   0.4538660\n#> T38  -0.6413664\n#> T40   0.3769378\n#> T49   0.8410043\n#> T53  -0.5528236\n#> T67  -0.4452930\n#> T71  -0.3689358\nfixef(m2n)\n#> (Intercept)          ti         ti2 \n#>  -11.223310   12.712094   -2.913682\nsummary(m2n)\n#> Linear mixed-effects model fit by REML\n#>   Data: d22 \n#>        AIC      BIC   logLik\n#>   263.3081 284.0911 -125.654\n#> \n#> Random effects:\n#>  Formula: ~1 | tree\n#>         (Intercept)  Residual\n#> StdDev:   0.5154042 0.3925777\n#> \n#> Correlation Structure: Exponential spatial correlation\n#>  Formula: ~day | tree \n#>  Parameter estimate(s):\n#>    range \n#> 3.794624 \n#> Fixed effects:  water ~ 1 + ti + ti2 \n#>                  Value Std.Error  DF   t-value p-value\n#> (Intercept) -11.223310 1.0988725 227 -10.21348       0\n#> ti           12.712094 0.9794235 227  12.97916       0\n#> ti2          -2.913682 0.2148551 227 -13.56115       0\n#>  Correlation: \n#>     (Intr) ti    \n#> ti  -0.985       \n#> ti2  0.976 -0.997\n#> \n#> Standardized Within-Group Residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -3.04861039 -0.55703950  0.00278101  0.62558762  3.80676991 \n#> \n#> Number of Observations: 239\n#> Number of Groups: 10"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9 Nonlinear and Generalized Linear Mixed Models","text":"NLMMs extend nonlinear model include fixed effects random effectsGLMMs extend generalized linear model include fixed effects random effects.nonlinear mixed model form \\[\nY_{ij} = f(\\mathbf{x_{ij} , \\theta, \\alpha_i}) + \\epsilon_{ij}\n\\]j-th response cluster (subject) (\\(= 1,...,n\\)), \\(j = 1,...,n_i\\)\\(\\mathbf{\\theta}\\) fixed effects\\(\\mathbf{\\alpha}_i\\) random effects cluster \\(\\mathbf{x}_{ij}\\) regressors design variables\\(f(.)\\) nonlinear mean response functionA GLMM can written :assume\\[\ny_i |\\alpha_i \\sim \\text{indep } f(y_i | \\alpha)\n\\]\\(f(y_i | \\mathbf{\\alpha})\\) exponential family distribution,\\[\nf(y_i | \\alpha) = \\exp [\\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} - c(y_i, \\phi)]\n\\]conditional mean \\(y_i\\) related \\(\\theta_i\\)\\[\n\\mu_i = \\frac{\\partial b(\\theta_i)}{\\partial \\theta_i}\n\\]transformation mean give us desired linear model model fixed random effects.\\[\n\\begin{aligned}\nE(y_i |\\alpha) &= \\mu_i \\\\\ng(\\mu_i) &= \\mathbf{x_i' \\beta + z'_i \\alpha}\n\\end{aligned}\n\\]\\(g()\\) known link function \\(\\mu_i\\) conditional mean. can see similarity GLMWe also specify random effects distribution\\[\n\\alpha \\sim f(\\alpha)\n\\]similar specification mixed models.Moreover, law large number applies fixed effects know normal distribution. , can specify \\(\\alpha\\) subjectively.Hence, can show NLMM special case GLMM\\[\n\\begin{aligned}\n\\mathbf{Y}_i &= \\mathbf{f}(\\mathbf{x}_i, \\mathbf{\\theta, \\alpha}_i) + \\mathbf{\\epsilon}_i \\\\\n\\mathbf{Y}_i &= \\mathbf{g}^{-1} (\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}_i) + \\mathbf{\\epsilon}_i\n\\end{aligned}\n\\]inverse link function corresponds nonlinear transformation fixed random effects.Note:can’t derive analytical formulation marginal distribution nonlinear combination normal variables normally distributed, even case additive error (\\(e_i\\)) random effects (\\(\\alpha_i\\)) normal.Consequences random effectsThe marginal mean \\(y_i\\) \\[\nE(y_i) = E_\\alpha(E(y_i | \\alpha)) = E_\\alpha (\\mu_i) = E(g^{-1}(\\mathbf{x_i' \\beta + z_i' \\alpha}))\n\\]\\(g^{-1}()\\) nonlinear, simplified version can go .special cases log link (\\(g(\\mu) = \\log \\mu\\) \\(g^{-1}() = \\exp()\\)) \\[\nE(y_i) = E(\\exp(\\mathbf{x_i' \\beta + z_i' \\alpha})) = \\exp(\\mathbf{x'_i \\beta})E(\\exp(\\mathbf{z}_i'\\alpha))\n\\]moment generating function \\(\\alpha\\) evaluated \\(\\mathbf{z}_i\\)Marginal variance \\(y_i\\)\\[\n\\begin{aligned}\nvar(y_i) &= var_\\alpha (E(y_i | \\alpha)) + E_\\alpha (var(y_i | \\alpha)) \\\\\n&= var(\\mu_i) + E((\\phi) V(\\mu_i)) \\\\\n&= var(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})) + E((\\phi)V(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})))\n\\end{aligned}\n\\]Without specific assumption \\(g()\\) /conditional distribution \\(\\mathbf{y}\\), simplified version.Marginal covariance \\(\\mathbf{y}\\)linear mixed model, random effects introduce dependence among observations share random effect common\\[\n\\begin{aligned}\ncov(y_i, y_j) &= cov_{\\alpha}(E(y_i | \\mathbf{\\alpha}),E(y_j | \\mathbf{\\alpha})) + E_{\\alpha}(cov(y_i, y_j | \\mathbf{\\alpha})) \\\\\n&= cov(\\mu_i, \\mu_j) + E(0) \\\\\n&= cov(g^{-1}(\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}), g^{-1}(\\mathbf{x}'_j \\beta + \\mathbf{z}_j' \\mathbf{\\alpha}))\n\\end{aligned}\n\\]Important: conditioning induce covariabilityExample:Repeated measurements subjects.Let \\(y_{ij}\\) j-th count taken \\(\\)-th subject., model \\(y_{ij} | \\mathbf{\\alpha} \\sim \\text{indep } Pois(\\mu_{ij})\\). \\[\n\\log(\\mu_{ij}) = \\mathbf{x}_{ij}' \\beta + \\alpha_i\n\\]\\(\\alpha_i \\sim iid N(0,\\sigma^2_{\\alpha})\\)log-link random patient effect.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-3","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1 Estimation","text":"linear mixed models, marginal likelihood \\(\\mathbf{y}\\) integration random effects hierarchical formulation\\[\nf(\\mathbf{y}) = \\int f(\\mathbf{y}| \\alpha) f(\\alpha) d \\alpha\n\\]linear mixed models, assumed 2 component distributions Gaussian linear relationships, implied marginal distribution also linear Gaussian allows us solve integral analytically.hand, GLMMs, distribution \\(f(\\mathbf{y} | \\alpha)\\) Gaussian general, NLMMs, functional form mean response random (fixed) effects nonlinear. cases, can’t perform integral analytically, means solve itnumerically /ornumerically /orlinearize inverse link function.linearize inverse link function.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-numerical-integration","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.1 Estimation by Numerical Integration","text":"marginal likelihood \\[\nL(\\beta; \\mathbf{y}) = \\int f(\\mathbf{y} | \\alpha) f(\\alpha) d \\alpha\n\\]Estimation fo fixed effects requires \\(\\frac{\\partial l}{\\partial \\beta}\\), \\(l\\) log-likelihoodOne way obtain marginal inference numerically integrate random effects throughnumerical quadraturenumerical quadratureLaplace approximationLaplace approximationMonte Carlo methodsMonte Carlo methodsWhen dimension \\(\\mathbf{\\alpha}\\) relatively low, easy. dimension \\(\\alpha\\) high, additional approximation required.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-linearization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2 Estimation by Linearization","text":"Idea: Linearized version response (known working response, pseudo-response) called \\(\\tilde{y}_i\\) conditional mean \\[\nE(\\tilde{y}_i | \\alpha) = \\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\alpha\n\\]also estimate \\(var(\\tilde{y}_i | \\alpha)\\). , apply Linear Mixed Models estimation usual.difference linearization done (.e., expand \\(f(\\mathbf{x, \\theta, \\alpha})\\) inverse link function","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"penalized-quasi-likelihood","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.1 Penalized quasi-likelihood","text":"(PQL)popular method\\[\n\\tilde{y}_i^{(k)} = \\hat{\\eta}_i^{(k-1)} + ( y_i - \\hat{\\mu}_i^{(k-1)})\\frac{d \\eta}{d \\mu}| \\hat{\\eta}_i^{(k-1)}\n\\]\\(\\eta_i = g(\\mu_i)\\) linear predictor\\(\\eta_i = g(\\mu_i)\\) linear predictor\\(k\\) = iteration optimization algorithm\\(k\\) = iteration optimization algorithmThe algorithm updates \\(\\tilde{y}_i\\) linear mixed model fit using \\(E(\\tilde{y}_i | \\alpha)\\) \\(var(\\tilde{y}_i | \\alpha)\\)Comments:Easy implementEasy implementInference asymptotically correct due linearizatonInference asymptotically correct due linearizatonBiased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Biased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Hypothesis testing confidence intervals also problems.Hypothesis testing confidence intervals also problems.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"generalized-estimating-equations","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.2 Generalized Estimating Equations","text":"(GEE)Let marginal generalized linear model mean y function predictors, means linearize mean response function assume dependent error structureExample\nBinary data:\\[\nlogit (E(\\mathbf{y})) = \\mathbf{X} \\beta\n\\]assume “working covariance matrix”, \\(\\mathbf{V}\\) elements \\(\\mathbf{y}\\), maximum likelihood equations estimating \\(\\beta\\) \\[\n\\mathbf{X'V^{-1}y} = \\mathbf{X'V^{-1}} E(\\mathbf{y})\n\\]\\(\\mathbf{V}\\) correct, unbiased estimating equationsWe typically define \\(\\mathbf{V} = \\mathbf{}\\). Solutions unbiased estimating equation give consistent estimators.practice, assume covariance structure, logistic regression, calculate large sample varianceLet \\(y_{ij} , j = 1,..,n_i, = 1,..,K\\) j-th measurement \\(\\)-th subject.\\[\n\\mathbf{y}_i =\n\\left(\n\\begin{array}\n{c}\ny_{i1} \\\\\n. \\\\\ny_{in_i}\n\\end{array}\n\\right)\n\\]mean\\[\n\\mathbf{\\mu}_i =\n\\left(\n\\begin{array}\n{c}\n\\mu_{i1} \\\\\n. \\\\\n\\mu_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{x}_{ij} =\n\\left(\n\\begin{array}\n{c}\nX_{ij1} \\\\\n. \\\\\nX_{ijp}\n\\end{array}\n\\right)\n\\]Let \\(\\mathbf{V}_i = cov(\\mathbf{y}_i)\\), based (Liang Zeger 1986) GEE estimates \\(\\beta\\) can obtained solving equation:\\[\nS(\\beta) = \\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}_i'}{\\partial \\beta} \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{\\mu}_i) = 0\n\\]Let \\(\\mathbf{R}_i (\\mathbf{c})\\) \\(n_i \\times n_i\\) “working” correlation matrix specified parameters \\(\\mathbf{c}\\). , \\(\\mathbf{V}_i = (\\phi) \\mathbf{B}_i^{1/2}\\mathbf{R}(\\mathbf{c}) \\mathbf{B}_i^{1/2}\\), \\(\\mathbf{B}_i\\) \\(n_i \\times n_i\\) diagonal matrix \\(V(\\mu_{ij})\\) j-th diagonalIf \\(\\mathbf{R}(\\mathbf{c})\\) true correlation matrix \\(\\mathbf{y}_i\\), \\(\\mathbf{V}_i\\) true covariance matrixThe working correlation matrix must estimated iteratively fitting algorithm:Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Compute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Update \\(\\beta\\) according \n\\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Update \\(\\beta\\) according \\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Iterate algorithm convergesIterate algorithm convergesNote: Inference based likelihoods appropriate likelihood estimator","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-bayesian-hierarchical-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.3 Estimation by Bayesian Hierarchical Models","text":"Bayesian Estimation\\[\nf(\\mathbf{\\alpha}, \\mathbf{\\beta} | \\mathbf{y}) \\propto f(\\mathbf{y} | \\mathbf{\\alpha}, \\mathbf{\\beta}) f(\\mathbf{\\alpha})f(\\mathbf{\\beta})\n\\]Numerical techniques (e.g., MCMC) can used find posterior distribution. method best terms make simplifying approximation fully accounting uncertainty estimation prediction, complex, time-consuming, computationally intensive.Implementation Issues:valid joint distribution can constructed given conditional model random parametersNo valid joint distribution can constructed given conditional model random parametersThe mean/ variance relationship random effects lead constraints marginal covariance modelThe mean/ variance relationship random effects lead constraints marginal covariance modelDifficult fit computationallyDifficult fit computationally2 types estimation approaches:Approximate objective function (marginal likelihood) integral approximation\nLaplace methods\nQuadrature methods\nMonte Carlo integration\nApproximate objective function (marginal likelihood) integral approximationLaplace methodsLaplace methodsQuadrature methodsQuadrature methodsMonte Carlo integrationMonte Carlo integrationApproximate model (based Taylor series linearization)Approximate model (based Taylor series linearization)Packages RGLMM: MASS:glmmPQL lme4::glmer glmmTMBGLMM: MASS:glmmPQL lme4::glmer glmmTMBNLMM: nlme::nlme; lme4::nlmer brms::brmNLMM: nlme::nlme; lme4::nlmer brms::brmBayesian: MCMCglmm ; brms:brmBayesian: MCMCglmm ; brms:brmExample: Non-Gaussian Repeated measurementsWhen data Gaussian, Linear Mixed ModelsWhen data Gaussian, Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed Models","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"application-6","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2 Application","text":"","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-cbpp-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.1 Binomial (CBPP Data)","text":"PQLPro:Linearizes response pseudo-response mean response (like LMM)Linearizes response pseudo-response mean response (like LMM)computationally efficientcomputationally efficientCons:biased binary, Poisson data small countsbiased binary, Poisson data small countsrandom effects interpreted link scalerandom effects interpreted link scalecan’t interpret AIC/BIC valuecan’t interpret AIC/BIC valueis herd specific outcome odds varies.can interpret fixed effect coefficients just like GLM. use logit link function , can say log odds probability case period 2 -1.016 less period 1 (baseline).Numerical IntegrationPro:accurateCon:computationally expensivecomputationally expensivewon’t work complex models.won’t work complex models.small data set, difference two approaches minimalIn numerical integration, can set nAGQ > 1 switch method likelihood evaluation, might increase accuracyBayesian approach GLMMsassume fixed effects parameters distributionassume fixed effects parameters distributioncan handle models intractable result traditional methodscan handle models intractable result traditional methodscomputationally expensivecomputationally expensiveMCMCglmm fits residual variance component (useful dispersion issues)interpret Bayesian “credible intervals” similarly confidence intervalsMake sure make post-hoc diagnosesThere trend, well-mixedFor herd variable, lot 0, suggests problem. fix instability herd effect sampling, can eithermodify prior distribution herd variationmodify prior distribution herd variationincreases number iterationincreases number iterationTo change shape priors, MCMCglmm use:V controls location distribution (default = 1)V controls location distribution (default = 1)nu controls concentration around V (default = 0)nu controls concentration around V (default = 0)","code":"\ndata(cbpp,package = \"lme4\")\nhead(cbpp)\n#>   herd incidence size period\n#> 1    1         2   14      1\n#> 2    1         3   12      2\n#> 3    1         4    9      3\n#> 4    1         0    5      4\n#> 5    2         3   22      1\n#> 6    2         1   18      2\nlibrary(MASS)\npql_cbpp <-\n    glmmPQL(\n        cbind(incidence, size - incidence) ~ period,\n        random  = ~ 1 | herd,\n        data    = cbpp,\n        family  = binomial(link = \"logit\"),\n        verbose = F\n    )\nsummary(pql_cbpp)\n#> Linear mixed-effects model fit by maximum likelihood\n#>   Data: cbpp \n#>   AIC BIC logLik\n#>    NA  NA     NA\n#> \n#> Random effects:\n#>  Formula: ~1 | herd\n#>         (Intercept) Residual\n#> StdDev:   0.5563535 1.184527\n#> \n#> Variance function:\n#>  Structure: fixed weights\n#>  Formula: ~invwt \n#> Fixed effects:  cbind(incidence, size - incidence) ~ period \n#>                 Value Std.Error DF   t-value p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372  0.0000\n#> period2     -1.016126 0.3684079 38 -2.758156  0.0089\n#> period3     -1.149984 0.3937029 38 -2.920944  0.0058\n#> period4     -1.605217 0.5178388 38 -3.099839  0.0036\n#>  Correlation: \n#>         (Intr) perid2 perid3\n#> period2 -0.399              \n#> period3 -0.373  0.260       \n#> period4 -0.282  0.196  0.182\n#> \n#> Standardized Within-Group Residuals:\n#>        Min         Q1        Med         Q3        Max \n#> -2.0591168 -0.6493095 -0.2747620  0.5170492  2.6187632 \n#> \n#> Number of Observations: 56\n#> Number of Groups: 15\nexp(0.556)\n#> [1] 1.743684\nsummary(pql_cbpp)$tTable\n#>                 Value Std.Error DF   t-value      p-value\n#> (Intercept) -1.327364 0.2390194 38 -5.553372 2.333216e-06\n#> period2     -1.016126 0.3684079 38 -2.758156 8.888179e-03\n#> period3     -1.149984 0.3937029 38 -2.920944 5.843007e-03\n#> period4     -1.605217 0.5178388 38 -3.099839 3.637000e-03\nlibrary(lme4)\nnumint_cbpp <-\n    glmer(\n        cbind(incidence, size - incidence) ~ \n            period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\")\n    )\nsummary(numint_cbpp)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(incidence, size - incidence) ~ period + (1 | herd)\n#>    Data: cbpp\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>    194.1    204.2    -92.0    184.1       51 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.3816 -0.7889 -0.2026  0.5142  2.8791 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  herd   (Intercept) 0.4123   0.6421  \n#> Number of obs: 56, groups:  herd, 15\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3983     0.2312  -6.048 1.47e-09 ***\n#> period2      -0.9919     0.3032  -3.272 0.001068 ** \n#> period3      -1.1282     0.3228  -3.495 0.000474 ***\n#> period4      -1.5797     0.4220  -3.743 0.000182 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr) perid2 perid3\n#> period2 -0.363              \n#> period3 -0.340  0.280       \n#> period4 -0.260  0.213  0.198\nlibrary(rbenchmark)\nbenchmark(\n    \"MASS\" = {\n        pql_cbpp <-\n            glmmPQL(\n                cbind(incidence, size - incidence) ~ period,\n                random = ~ 1 | herd,\n                data = cbpp,\n                family = binomial(link = \"logit\"),\n                verbose = F\n            )\n    },\n    \"lme4\" = {\n        glmer(\n            cbind(incidence, size - incidence) ~ period + (1 | herd),\n            data = cbpp,\n            family = binomial(link = \"logit\")\n        )\n    },\n    replications = 50,\n    columns = c(\"test\", \"replications\", \"elapsed\", \"relative\"),\n    order = \"relative\"\n)\n#>   test replications elapsed relative\n#> 1 MASS           50    3.79    1.000\n#> 2 lme4           50    7.79    2.055\nlibrary(lme4)\nnumint_cbpp_GH <-\n    glmer(\n        cbind(incidence, size - incidence) ~ period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\"),\n        nAGQ = 20\n    )\nsummary(numint_cbpp_GH)$coefficients[, 1] - \n    summary(numint_cbpp)$coefficients[, 1]\n#>   (Intercept)       period2       period3       period4 \n#> -0.0008808634  0.0005160912  0.0004066218  0.0002644629\nlibrary(MCMCglmm)\nBayes_cbpp <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random  = ~ herd,\n        data    = cbpp,\n        family  = \"multinomial2\",\n        verbose = FALSE\n    )\nsummary(Bayes_cbpp)\n#> \n#>  Iterations = 3001:12991\n#>  Thinning interval  = 10\n#>  Sample size  = 1000 \n#> \n#>  DIC: 537.9598 \n#> \n#>  G-structure:  ~herd\n#> \n#>      post.mean l-95% CI u-95% CI eff.samp\n#> herd   0.03246 1.03e-16   0.2073    105.7\n#> \n#>  R-structure:  ~units\n#> \n#>       post.mean l-95% CI u-95% CI eff.samp\n#> units     1.095   0.3017    2.281    306.9\n#> \n#>  Location effects: cbind(incidence, size - incidence) ~ period \n#> \n#>             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n#> (Intercept)   -1.5247  -2.1732  -0.9248    717.5 <0.001 ***\n#> period2       -1.2812  -2.3489  -0.3661    821.7  0.012 *  \n#> period3       -1.4152  -2.3443  -0.3088    691.5  0.004 ** \n#> period4       -1.9335  -3.2407  -0.8315    554.9 <0.001 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# explains less variability\napply(Bayes_cbpp$VCV,2,sd)\n#>      herd     units \n#> 0.1031743 0.5423514\nsummary(Bayes_cbpp)$solutions\n#>             post.mean  l-95% CI   u-95% CI eff.samp pMCMC\n#> (Intercept) -1.524731 -2.173223 -0.9247605 717.5157 0.001\n#> period2     -1.281212 -2.348887 -0.3660568 821.6596 0.012\n#> period3     -1.415170 -2.344293 -0.3087640 691.5463 0.004\n#> period4     -1.933501 -3.240745 -0.8314840 554.9365 0.001\nlibrary(lattice)\nxyplot(as.mcmc(Bayes_cbpp$Sol), layout = c(2, 2))\nxyplot(as.mcmc(Bayes_cbpp$VCV),layout=c(2,1))\nlibrary(MCMCglmm)\nBayes_cbpp2 <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random = ~ herd,\n        data   = cbpp,\n        family = \"multinomial2\",\n        nitt   = 20000,\n        burnin = 10000,\n        prior  = list(G = list(list(\n            V  = 1, nu = .1\n        ))),\n        verbose = FALSE\n    )\nxyplot(as.mcmc(Bayes_cbpp2$VCV), layout = c(2, 1))"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"count-owl-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.2 Count (Owl Data)","text":"typical Poisson model, \\(\\lambda\\) (Poisson mean), model \\(\\log(\\lambda) = \\mathbf{x'\\beta}\\) response rate (e.g., counts per BroodSize), model \\(\\log(\\lambda / b) = \\mathbf{x'\\beta}\\) , equivalently \\(\\log(\\lambda) = \\log(b) + \\mathbf{x'\\beta}\\) \\(b\\) BroodSize. Hence, “offset” mean log variable.nest explains relatively large proportion variability (standard deviation larger coefficients)nest explains relatively large proportion variability (standard deviation larger coefficients)model fit isn’t great (deviance 5202 594 df)model fit isn’t great (deviance 5202 594 df)improvement using negative binomial considering -dispersionTo account many 0s data, can use zero-inflated Poisson (ZIP) model.glmmTMB can handle ZIP GLMMs since adds automatic differentiation existing estimation strategies.can see ZIP GLMM arrival time covariate zero best.arrival time positive effect observing nonzero number callsarrival time positive effect observing nonzero number callsinteractions non significant, food treatment significant (fewer calls eating)interactions non significant, food treatment significant (fewer calls eating)nest variability large magnitude (without , parameter estimates change)nest variability large magnitude (without , parameter estimates change)","code":"\nlibrary(glmmTMB)\nlibrary(dplyr)\ndata(Owls, package = \"glmmTMB\")\nOwls <- Owls %>% \n    rename(Ncalls = SiblingNegotiation)\nowls_glmer <-\n    glmer(\n        Ncalls ~ offset(log(BroodSize)) \n        + FoodTreatment * SexParent +\n            (1 | Nest),\n        family = poisson,\n        data = Owls\n    )\nsummary(owls_glmer)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: poisson  ( log )\n#> Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +  \n#>     (1 | Nest)\n#>    Data: Owls\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>   5212.8   5234.8  -2601.4   5202.8      594 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.5529 -1.7971 -0.6842  1.2689 11.4312 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.2063   0.4542  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Fixed effects:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.65585    0.09567   6.855 7.12e-12 ***\n#> FoodTreatmentSatiated               -0.65612    0.05606 -11.705  < 2e-16 ***\n#> SexParentMale                       -0.03705    0.04501  -0.823   0.4104    \n#> FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) FdTrtS SxPrnM\n#> FdTrtmntStt -0.225              \n#> SexParentMl -0.292  0.491       \n#> FdTrtmS:SPM  0.170 -0.768 -0.605\n# Negative binomial model\nowls_glmerNB <-\n    glmer.nb(Ncalls ~ offset(log(BroodSize)) \n             + FoodTreatment * SexParent\n             + (1 | Nest), data = Owls)\n\nc(Deviance = round(summary(owls_glmerNB)$AICtab[\"deviance\"], 3),\n  df = summary(owls_glmerNB)$AICtab[\"df.resid\"])\n#> Deviance.deviance       df.df.resid \n#>          3483.616           593.000\nhist(Owls$Ncalls,breaks=30)\nlibrary(glmmTMB)\nowls_glmm <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 0,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\nowls_glmm_zi <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 1,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\n# Scale Arrival time to use as a covariate for zero-inflation parameter\nOwls$ArrivalTime <- scale(Owls$ArrivalTime)\nowls_glmm_zi_cov <- glmmTMB(\n    Ncalls ~ FoodTreatment * SexParent +\n        offset(log(BroodSize)) +\n        (1 | Nest),\n    ziformula =  ~ ArrivalTime,\n    family = nbinom2(link = \"log\"),\n    data = Owls\n)\nas.matrix(anova(owls_glmm, owls_glmm_zi))\n#>              Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm     6 3495.610 3521.981 -1741.805 3483.610       NA     NA\n#> owls_glmm_zi  7 3431.646 3462.413 -1708.823 3417.646 65.96373      1\n#>                Pr(>Chisq)\n#> owls_glmm              NA\n#> owls_glmm_zi 4.592983e-16\nas.matrix(anova(owls_glmm_zi, owls_glmm_zi_cov))\n#>                  Df      AIC      BIC    logLik deviance    Chisq Chi Df\n#> owls_glmm_zi      7 3431.646 3462.413 -1708.823 3417.646       NA     NA\n#> owls_glmm_zi_cov  8 3422.532 3457.694 -1703.266 3406.532 11.11411      1\n#>                    Pr(>Chisq)\n#> owls_glmm_zi               NA\n#> owls_glmm_zi_cov 0.0008567362\nsummary(owls_glmm_zi_cov)\n#>  Family: nbinom2  ( log )\n#> Formula:          \n#> Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)\n#> Zero inflation:          ~ArrivalTime\n#> Data: Owls\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>   3422.5   3457.7  -1703.3   3406.5      591 \n#> \n#> Random effects:\n#> \n#> Conditional model:\n#>  Groups Name        Variance Std.Dev.\n#>  Nest   (Intercept) 0.07487  0.2736  \n#> Number of obs: 599, groups:  Nest, 27\n#> \n#> Dispersion parameter for nbinom2 family (): 2.22 \n#> \n#> Conditional model:\n#>                                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                          0.84778    0.09961   8.511  < 2e-16 ***\n#> FoodTreatmentSatiated               -0.39529    0.13742  -2.877  0.00402 ** \n#> SexParentMale                       -0.07025    0.10435  -0.673  0.50079    \n#> FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Zero-inflation model:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -1.3018     0.1261  -10.32  < 2e-16 ***\n#> ArrivalTime   0.3545     0.1074    3.30 0.000966 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.3 Binomial","text":"Fixed effects (\\(\\beta\\)) = genotypeFixed effects (\\(\\beta\\)) = genotypeRandom effects (\\(\\alpha\\)) = blockRandom effects (\\(\\alpha\\)) = blockEquivalently, can use MCMCglmm , Bayesian approach","code":"\nlibrary(agridat)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(spaMM)\ndata(gotway.hessianfly)\ndat <- gotway.hessianfly\ndat$prop <- dat$y / dat$n\n\nggplot(dat, aes(x = lat, y = long, fill = prop)) +\n    geom_tile() +\n    scale_fill_gradient(low = 'white', high = 'black') +\n    geom_text(aes(label = gen, color = block)) +\n    ggtitle('Gotway Hessian Fly')\nflymodel <-\n    glmer(\n        cbind(y, n - y) ~ gen + (1 | block),\n        data   = dat,\n        family = binomial,\n        nAGQ   = 5\n    )\nsummary(flymodel)\n#> Generalized linear mixed model fit by maximum likelihood (Adaptive\n#>   Gauss-Hermite Quadrature, nAGQ = 5) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: cbind(y, n - y) ~ gen + (1 | block)\n#>    Data: dat\n#> \n#>      AIC      BIC   logLik deviance df.resid \n#>    162.2    198.9    -64.1    128.2       47 \n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.38644 -1.01188  0.09631  1.03468  2.75479 \n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  block  (Intercept) 0.001022 0.03196 \n#> Number of obs: 64, groups:  block, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   1.5035     0.3914   3.841 0.000122 ***\n#> genG02       -0.1939     0.5302  -0.366 0.714644    \n#> genG03       -0.5408     0.5103  -1.060 0.289260    \n#> genG04       -1.4342     0.4714  -3.043 0.002346 ** \n#> genG05       -0.2037     0.5429  -0.375 0.707486    \n#> genG06       -0.9783     0.5046  -1.939 0.052533 .  \n#> genG07       -0.6041     0.5111  -1.182 0.237235    \n#> genG08       -1.6774     0.4907  -3.418 0.000630 ***\n#> genG09       -1.3984     0.4725  -2.960 0.003078 ** \n#> genG10       -0.6817     0.5333  -1.278 0.201181    \n#> genG11       -1.4630     0.4843  -3.021 0.002522 ** \n#> genG12       -1.4591     0.4918  -2.967 0.003010 ** \n#> genG13       -3.5528     0.6600  -5.383 7.31e-08 ***\n#> genG14       -2.5073     0.5264  -4.763 1.90e-06 ***\n#> genG15       -2.0872     0.4851  -4.302 1.69e-05 ***\n#> genG16       -2.9697     0.5383  -5.517 3.46e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(coda)\nBayes_flymodel <- MCMCglmm(\n    cbind(y, n - y) ~ gen ,\n    random  = ~ block,\n    data    = dat,\n    family  = \"multinomial2\",\n    verbose = FALSE\n)\nplot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1])\nautocorr.plot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1])"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"example-from-schabenberger_2001-section-8.4.1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.4 Example from (Schabenberger and Pierce 2001) section 8.4.1","text":"cumulative volume relates complementary diameter (subplots created based total tree height)proposed non-linear model:\\[\nV_{id_j} = (\\beta_0 + (\\beta_1 + b_{1i})\\frac{D^2_i H_i}{1000})(\\exp[-(\\beta_2 + b_{2i})t_{ij} \\exp(\\beta_3 t_{ij})]) + e_{ij}\n\\]\\(b_{1i}, b_{2i}\\) random effects\\(b_{1i}, b_{2i}\\) random effects\\(e_{ij}\\) random errors\\(e_{ij}\\) random errorsLittle different book different implementation nonlinear mixed models.red line = predicted observations based common fixed effectsteal line = tree-specific predictions random effects","code":"\ndat2 <- read.table(\"images/YellowPoplarData_r.txt\")\nnames(dat2) <- c('tn', 'k', 'dbh', 'totht',\n                 'dob', 'ht', 'maxd', 'cumv')\ndat2$t <- dat2$dob / dat2$dbh\ndat2$r <- 1 - dat2$dob / dat2$totht\nlibrary(ggplot2)\nlibrary(dplyr)\ndat2 <- dat2 %>% group_by(tn) %>% mutate(\n    z = case_when(\n        totht < 74 & totht >= 0 ~ 'a: 0-74ft',\n        totht < 88 & totht >= 74 ~ 'b: 74-88',\n        totht < 95 & totht >= 88 ~ 'c: 88-95',\n        totht < 99 & totht >= 95 ~ 'd: 95-99',\n        totht < 104 & totht >= 99 ~ 'e: 99-104',\n        totht < 109 & totht >= 104 ~ 'f: 104-109',\n        totht < 115 & totht >= 109 ~ 'g: 109-115',\n        totht < 120 & totht >= 115 ~ 'h: 115-120',\n        totht < 140 & totht >= 120 ~ 'i: 120-150',\n    )\n)\nggplot(dat2, aes(x = r, y = cumv)) + \n    geom_point(size = 0.5) + \n    facet_wrap(vars(z))\nlibrary(nlme)\ntmp <-\n    nlme(\n        cumv ~ (b0 + (b1 + u1) *\n                    (dbh * dbh * totht / 1000)) *\n            (exp(-(b2 + u2) * (t / 1000) * exp(b3 * t))), \n        data = dat2,\n        fixed = b0 + b1 + b2 + b3 ~ 1,\n        # 1 on the right hand side of the formula indicates \n        # a single fixed effects for the corresponding parameters\n        random = list(pdDiag(u1 + u2 ~ 1)),\n        #uncorrelated random effects\n        groups = ~ tn,\n        #group on trees so each tree w/ have u1 and u2\n        start = list(fixed = c(\n            b0 = 0.25,\n            b1 = 2.3,\n            b2 = 2.87,\n            b3 = 6.7\n        ))\n    )\nsummary(tmp)\n#> Nonlinear mixed-effects model fit by maximum likelihood\n#>   Model: cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht/1000)) * (exp(-(b2 +      u2) * (t/1000) * exp(b3 * t))) \n#>   Data: dat2 \n#>        AIC      BIC    logLik\n#>   31103.73 31151.33 -15544.86\n#> \n#> Random effects:\n#>  Formula: list(u1 ~ 1, u2 ~ 1)\n#>  Level: tn\n#>  Structure: Diagonal\n#>                u1       u2 Residual\n#> StdDev: 0.1508094 0.447829 2.226361\n#> \n#> Fixed effects:  b0 + b1 + b2 + b3 ~ 1 \n#>       Value  Std.Error   DF  t-value p-value\n#> b0 0.249386 0.12894687 6297   1.9340  0.0532\n#> b1 2.288832 0.01266804 6297 180.6776  0.0000\n#> b2 2.500497 0.05606685 6297  44.5985  0.0000\n#> b3 6.848871 0.02140677 6297 319.9395  0.0000\n#>  Correlation: \n#>    b0     b1     b2    \n#> b1 -0.639              \n#> b2  0.054  0.056       \n#> b3 -0.011 -0.066 -0.850\n#> \n#> Standardized Within-Group Residuals:\n#>           Min            Q1           Med            Q3           Max \n#> -6.694575e+00 -3.081861e-01 -8.904304e-05  3.469469e-01  7.855665e+00 \n#> \n#> Number of Observations: 6636\n#> Number of Groups: 336\nnlme::intervals(tmp)\n#> Approximate 95% confidence intervals\n#> \n#>  Fixed effects:\n#>           lower      est.     upper\n#> b0 -0.003318061 0.2493855 0.5020892\n#> b1  2.264006036 2.2888322 2.3136584\n#> b2  2.390620340 2.5004973 2.6103743\n#> b3  6.806919342 6.8488713 6.8908232\n#> \n#>  Random Effects:\n#>   Level: tn \n#>            lower      est.     upper\n#> sd(u1) 0.1376084 0.1508094 0.1652768\n#> sd(u2) 0.4056209 0.4478290 0.4944291\n#> \n#>  Within-group standard error:\n#>    lower     est.    upper \n#> 2.187258 2.226361 2.266162\nlibrary(cowplot)\nnlmmfn <- function(fixed,rand,dbh,totht,t){\n  b0 <- fixed[1]\n  b1 <- fixed[2]\n  b2 <- fixed[3]\n  b3 <- fixed[4]\n  u1 <- rand[1]\n  u2 <- rand[2]\n  #just made so we can predict w/o random effects\n  return((b0+(b1+u1)*(dbh*dbh*totht/1000))*(exp(-(b2+u2)*(t/1000)*exp(b3*t))))\n}\n\n\n\n#Tree 1\npred1 <- data.frame(seq(1, 24, length.out = 100))\nnames(pred1) <- 'dob'\npred1$tn <- 1\npred1$dbh <- unique(dat2[dat2$tn == 1, ]$dbh)\npred1$t <- pred1$dob / pred1$dbh\npred1$totht <- unique(dat2[dat2$tn == 1, ]$totht)\npred1$r <- 1 - pred1$dob / pred1$totht\n\n\npred1$test <- predict(tmp, pred1)\npred1$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred1$dbh,\n        pred1$totht,\n        pred1$t\n    )\n\np1 <-\n    ggplot(pred1) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 1, ], aes(x = r, y = cumv)) + \n    ggtitle('Tree 1') + theme(legend.position = \"none\")\n\n\n#Tree 151\npred151        <- data.frame(seq(1, 21, length.out = 100))\nnames(pred151) <- 'dob'\npred151$tn     <- 151\npred151$dbh    <- unique(dat2[dat2$tn == 151, ]$dbh)\npred151$t      <- pred151$dob / pred151$dbh\npred151$totht  <- unique(dat2[dat2$tn == 151, ]$totht)\npred151$r      <- 1 - pred151$dob / pred151$totht\n\n\npred151$test <- predict(tmp, pred151)\npred151$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred151$dbh,\n        pred151$totht,\n        pred151$t\n    )\n\np2 <-\n    ggplot(pred151) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 151,], aes(x = r, y = cumv)) + \n    ggtitle('Tree 151') + \n    theme(legend.position = \"none\")\n\n\n#Tree 279\npred279        <- data.frame(seq(1, 9, length.out = 100))\nnames(pred279) <- 'dob'\npred279$tn     <- 279\npred279$dbh    <- unique(dat2[dat2$tn == 279, ]$dbh)\npred279$t      <- pred279$dob / pred279$dbh\npred279$totht  <- unique(dat2[dat2$tn == 279, ]$totht)\npred279$r      <- 1 - pred279$dob / pred279$totht\n\n\npred279$test <- predict(tmp, pred279)\npred279$testno <-\n    nlmmfn(\n        fixed = tmp$coefficients$fixed,\n        rand = c(0, 0),\n        pred279$dbh,\n        pred279$totht,\n        pred279$t\n    )\n\np3 <-\n    ggplot(pred279) + \n    geom_line(aes(x = r, y = test, color = 'with random')) +\n    geom_line(aes(x = r, y = testno, color = 'No random')) + \n    labs(colour = \"\") + \n    geom_point(data = dat2[dat2$tn == 279, ], aes(x = r, y = cumv)) + \n    ggtitle('Tree 279') + \n    theme(legend.position = \"none\")\n\nplot_grid(p1, p2, p3)"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"summary-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.3 Summary","text":"","code":""},{"path":"model-specification.html","id":"model-specification","chapter":"10 Model Specification","heading":"10 Model Specification","text":"Test whether underlying assumptions hold trueNested Model (A1/A3)Non-Nested Model (A1/A3)Heteroskedasticity (A4)","code":""},{"path":"model-specification.html","id":"nested-model","chapter":"10 Model Specification","heading":"10.1 Nested Model","text":"\\[\n\\begin{aligned}\ny &= \\beta_0 + x_1\\beta_1 + x_2\\beta-2 + x_3\\beta_3 + \\epsilon & \\text{unrestricted model} \\\\\ny &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{restricted model}\n\\end{aligned}\n\\]Unrestricted model always longer restricted model\nrestricted model “nested” within unrestricted model\ndetermine variables included exclude, use Wald TestAdjusted \\(R^2\\)\\(R^2\\) always increase variables includedAdjusted \\(R^2\\) tries correct penalizing inclusion unnecessary variables.\\[\n\\begin{aligned}\n{R}^2 &= 1 - \\frac{SSR/n}{SST/n} \\\\\n{R}^2_{adj} &= 1 - \\frac{SSR/(n-k)}{SST/(n-1)} \\\\\n&= 1 - \\frac{(n-1)(1-R^2)}{(n-k)}\n\\end{aligned}\n\\]\\({R}^2_{adj}\\) increases t-statistic additional variable greater 1 absolute value.\\({R}^2_{adj}\\) valid models heteroskedasticitythere fore used determining variables included model (t F-tests appropriate)","code":""},{"path":"model-specification.html","id":"chow-test","chapter":"10 Model Specification","heading":"10.1.1 Chow test","text":"run two different regressions two groups?","code":""},{"path":"model-specification.html","id":"non-nested-model","chapter":"10 Model Specification","heading":"10.2 Non-Nested Model","text":"compare models different non-nested specifications","code":""},{"path":"model-specification.html","id":"davidson-mackinnon-test","chapter":"10 Model Specification","heading":"10.2.1 Davidson-Mackinnon test","text":"","code":""},{"path":"model-specification.html","id":"independent-variable","chapter":"10 Model Specification","heading":"10.2.1.1 Independent Variable","text":"independent variables logged? (decide non-nested alternatives)\\[\n\\begin{aligned}\ny =  \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(level eq)} \\\\\ny =  \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(log eq)}\n\\end{aligned}\n\\]Obtain predict outcome estimating model log equation \\(\\check{y}\\) estimate following auxiliary equation,\\[\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]evaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)Obtain predict outcome estimating model level equation \\(\\hat{y}\\), estimate following auxiliary equation,\\[\ny = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]evaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)reject null (1) step fail reject null second step, log equation preferred.fail reject null (1) step reject null (2) step , level equation preferred.reject steps, statistical evidence neither model used re-evaluate functional form model.fail reject steps, sufficient evidence prefer one model . can compare \\(R^2_{adj}\\) choose two models.\\[\n\\begin{aligned}\ny &= \\beta_0 + ln(x)\\beta_1 + \\epsilon \\\\\ny &= \\beta_0 + x(\\beta_1) + x^2\\beta_2 + \\epsilon\n\\end{aligned}\n\\]Compare better fits dataCompare standard \\(R^2\\) unfair second model less parsimonious (parameters estimate)\\(R_{adj}^2\\) penalize second model less parsimonious + valid heteroskedasticity (A4 holds)compare Davidson-Mackinnon test","code":""},{"path":"model-specification.html","id":"dependent-variable","chapter":"10 Model Specification","heading":"10.2.1.2 Dependent Variable","text":"\\[\n\\begin{aligned}\ny &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{level eq} \\\\\nln(y) &= \\beta_0 + x_1\\beta_1 + \\epsilon & \\text{log eq} \\\\\n\\end{aligned}\n\\]level model, regardless big y , x constant effect (.e., one unit change \\(x_1\\) results \\(\\beta_1\\) unit change y)log model, larger y , effect x stronger (.e., one unit change \\(x_1\\) increase y 1 \\(1+\\beta_1\\) 100 100+100x\\(\\beta_1\\))compare \\(R^2\\) \\(R^2_{adj}\\) outcomes complement different, scaling different (SST different)need “un-transform” \\(ln(y)\\) back scale y compare,Estimate model log equation obtain predicted outcome \\(\\hat{ln(y)}\\)“Un-transform” predicted outcome\\[\n\\hat{m} = exp(\\hat{ln(y)})\n\\]Estimate following model (without intercept)\\[\ny = \\alpha\\hat{m} + error\n\\]obtain predicted outcome \\(\\hat{y}\\)take square correlation \\(\\hat{y}\\) y scaled version \\(R^2\\) log model can now compare usual \\(R^2\\) level model.","code":""},{"path":"model-specification.html","id":"heteroskedasticity-1","chapter":"10 Model Specification","heading":"10.3 Heteroskedasticity","text":"Using roust standard errors always validUsing roust standard errors always validIf significant evidence heteroskedasticity implying A4 hold\nGauss-Markov Theorem longer holds, OLS BLUE.\nconsider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)\nsignificant evidence heteroskedasticity implying A4 holdGauss-Markov Theorem longer holds, OLS BLUE.consider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)","code":""},{"path":"model-specification.html","id":"breusch-pagan-test","chapter":"10 Model Specification","heading":"10.3.1 Breusch-Pagan test","text":"A4 implies\\[\nE(\\epsilon_i^2|\\mathbf{x_i})=\\sigma^2\n\\]\\[\n\\epsilon_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]determining whether \\(\\mathbf{x}_i\\) predictive valueif \\(\\mathbf{x}_i\\) predictive value, variance changes levels \\(\\mathbf{x}_i\\) evidence heteroskedasticityif \\(\\mathbf{x}_i\\) predictive value, variance constant levels \\(\\mathbf{x}_i\\)Breusch-Pagan test heteroskedasticity compute F-test total significance following model\\[\ne_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]low p-value means reject null homoskedasticityHowever, Breusch-Pagan test detect heteroskedasticity non-linear form","code":""},{"path":"model-specification.html","id":"white-test","chapter":"10 Model Specification","heading":"10.3.2 White test","text":"test heteroskedasticity allow non-linear relationship computing F-test total significance following model (assume three independent random variables)\\[\n\\begin{aligned}\ne_i^2 &= \\gamma_0 + x_i \\gamma_1 + x_{i2}\\gamma_2 + x_{i3}\\gamma_3 \\\\\n&+ x_{i1}^2\\gamma_4 + x_{i2}^2\\gamma_5 + x_{i3}^2\\gamma_6 \\\\\n&+ (x_{i1} \\times x_{i2})\\gamma_7 + (x_{i1} \\times x_{i3})\\gamma_8 + (x_{i2} \\times x_{i3})\\gamma_9 + error\n\\end{aligned}\n\\]low p-value means reject null homoskedasticityEquivalently, can compute LM \\(LM = nR^2_{e^2}\\) \\(R^2_{e^2}\\) come regression squared residual outcomeThe LM statistic [\\(\\chi_k^2\\)][Chi-squared] distribution","code":""},{"path":"imputation-missing-data.html","id":"imputation-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11 Imputation (Missing Data)","text":"Imputation statistical procedure replace missing data reasonable valuesUnit imputation = single data pointUnit imputation = single data pointItem imputation = single feature valueItem imputation = single feature valueImputation usually seen illegitimate child statistical analysis. Several reasons contribute negative views :Peopled hardly imputation correctly (introduce bias estimates)Imputation can applied small range problems correctlyIf missing data \\(y\\) (dependent variable), probably able imputation appropriately. However, certain type missing data (e.g., non-random missing data) \\(x\\)’s variable (independent variables), can still salvage collected data points imputation.also need talk want imputation first place. purpose inference/ explanation (valid statistical inference optimal point prediction), imputation offer much help (Rubin 1996). However, purpose prediction, want standard error reduced including information (non-missing data) variables data point. imputation tool ’re looking .software packages, use listwise deletion casewise deletion complete case analysis (analysis observations information). recently statistician can propose methods bit better listwise deletion maximum likelihood multiple imputation.“Judging quality missing data procedures ability recreate individual missing values (according hit rate, mean square error, etc) lead choosing procedures result valid inference”, (Rubin 1996)Missing data can make challenging big datasets.","code":""},{"path":"imputation-missing-data.html","id":"assumptions-1","chapter":"11 Imputation (Missing Data)","heading":"11.1 Assumptions","text":"","code":""},{"path":"imputation-missing-data.html","id":"missing-completely-at-random-mcar","chapter":"11 Imputation (Missing Data)","heading":"11.1.1 Missing Completely at Random (MCAR)","text":"Missing Completely Random, MCAR, means relationship missingness data values, observed missing. missing data points random subset data. nothing systematic going makes data likely missing others.probability missing data variable unrelated value values variables data set.Note: “missingness” Y can correlated “missingness” X can compare value variables observations missing data, observations without missing data. reject t-test mean difference, can say evidence data MCAR. say data MCAR fail reject t-test.propensity data point missing completely random.’s relationship whether data point missing values data set, missing observed.missing data just random subset data.Methods include:Universal singular value thresholding (Chatterjee 2015) (can recover mean, whole true distribution).Universal singular value thresholding (Chatterjee 2015) (can recover mean, whole true distribution).Softimputet: (Hastie et al. 2015) (doesn’t work well “Limited” -missing random).Softimputet: (Hastie et al. 2015) (doesn’t work well “Limited” -missing random).Synthetic nearest neighbor (Agarwal et al. 2023) (still work okay missing random). Available GitHub: syntheticNNSynthetic nearest neighbor (Agarwal et al. 2023) (still work okay missing random). Available GitHub: syntheticNN","code":""},{"path":"imputation-missing-data.html","id":"missing-at-random-mar","chapter":"11 Imputation (Missing Data)","heading":"11.1.2 Missing at Random (MAR)","text":"Missing Random, MAR, means systematic relationship propensity missing values observed data, missing data. Whether observation missing nothing missing values, values individual’s observed variables. , example, men likely tell weight women, weight MAR.MAR weaker MCAR\\[\nP(Y_{missing}|Y,X)= P(Y_{missing}|X)\n\\]probability Y missing given Y X equal probability Y missing given X. However, impossible provide evidence MAR condition.propensity data point missing related missing data, related observed data. another word, systematic relationship propensity missing values observed data, missing data.\nexample, men likely tell weight women, weight MAR\npropensity data point missing related missing data, related observed data. another word, systematic relationship propensity missing values observed data, missing data.example, men likely tell weight women, weight MARMAR requires cause missing data unrelated missing values may related observed values variables.MAR requires cause missing data unrelated missing values may related observed values variables.MAR means missing values related observed values variables. example CD missing data, missing income data may unrelated actual income values related education. Perhaps people education less likely reveal income less educationMAR means missing values related observed values variables. example CD missing data, missing income data may unrelated actual income values related education. Perhaps people education less likely reveal income less education","code":""},{"path":"imputation-missing-data.html","id":"ignorable","chapter":"11 Imputation (Missing Data)","heading":"11.1.3 Ignorable","text":"missing data mechanism ignorable whenThe data MARthe parameters function missing data process unrelated parameters (interest) need estimated.case, actually don’t need model missing data mechanisms unless like improve accuracy, case still need rigorous approach improve efficiency parameters.","code":""},{"path":"imputation-missing-data.html","id":"nonignorable","chapter":"11 Imputation (Missing Data)","heading":"11.1.4 Nonignorable","text":"Missing Random, MNAR, means relationship propensity value missing values.Example: people lowest education missing education sickest people likely drop study.MNAR called Nonignorable missing data mechanism modeled deal missing data. include model data missing likely values .Hence, case nonignorable, data MAR. , parameters interest biased model missing data mechanism. One widely used approach nonignorable missing data (James J. Heckman 1976)Another name: Missing Random (MNAR): relationship propensity value missing values\nexample, people low education less likely report \nAnother name: Missing Random (MNAR): relationship propensity value missing valuesFor example, people low education less likely report itWe need model data missing likely values .need model data missing likely values .missing data mechanism related missing valuesthe missing data mechanism related missing valuesIt commonly occurs people want reveal something personal unpopular themselvesIt commonly occurs people want reveal something personal unpopular themselvesComplete case analysis can give highly biased results NI missing data. proportionally low moderate income individuals left sample high income people missing, estimate mean income lower actual population mean.Complete case analysis can give highly biased results NI missing data. proportionally low moderate income individuals left sample high income people missing, estimate mean income lower actual population mean.One can use instrument can predict nonresponse process outcome variable, unrelated outcome population correct missingness (still use complete cases) (B. Sun et al. 2018; E. J. Tchetgen Tchetgen Wirth 2017)","code":""},{"path":"imputation-missing-data.html","id":"solutions-to-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.2 Solutions to Missing data","text":"","code":""},{"path":"imputation-missing-data.html","id":"listwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.2.1 Listwise Deletion","text":"Also known complete case deletion retain cases complete data features.Advantages:Can applied statistical test (SEM, multi-level regression, etc.)Can applied statistical test (SEM, multi-level regression, etc.)case MCAR, parameters estimates standard errors unbiased.case MCAR, parameters estimates standard errors unbiased.case MAR among independent variables (depend values dependent variables), listwise deletion parameter estimates can still unbiased. (Little 1992) example, model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) probability missing data \\(X1\\) independent \\(Y\\), dependent value \\(X1\\) \\(X2\\), model estimates still unbiased.\nmissing data mechanism depends values independent variables stratified sampling. stratified sampling bias estimates\ncase logistic regression, probability missing data variable depends value dependent variable, independent value independent variables, listwise deletion yield biased intercept estimate, consistent estimates slope standard errors (Vach Vach 1994). However, logistic regression still fail probability missing data dependent value dependent independent variables.\nregression analysis, listwise deletion robust maximum likelihood multiple imputation MAR assumption violated.\ncase MAR among independent variables (depend values dependent variables), listwise deletion parameter estimates can still unbiased. (Little 1992) example, model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) probability missing data \\(X1\\) independent \\(Y\\), dependent value \\(X1\\) \\(X2\\), model estimates still unbiased.missing data mechanism depends values independent variables stratified sampling. stratified sampling bias estimatesIn case logistic regression, probability missing data variable depends value dependent variable, independent value independent variables, listwise deletion yield biased intercept estimate, consistent estimates slope standard errors (Vach Vach 1994). However, logistic regression still fail probability missing data dependent value dependent independent variables.regression analysis, listwise deletion robust maximum likelihood multiple imputation MAR assumption violated.Disadvantages:yield larger standard errors sophisticated methods discussed later.data MCAR, MAR, listwise deletion can yield biased estimates.cases regression analysis, sophisticated methods can yield better estimates compared listwise deletion.","code":""},{"path":"imputation-missing-data.html","id":"pairwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.2.2 Pairwise Deletion","text":"method used case linear models linear regression, factor analysis, SEM. premise method based coefficient estimates calculated based means, standard deviations, correlation matrix. Compared listwise deletion, still utilized many correlation variables possible compute correlation matrix.Advantages:true missing data mechanism MCAR, pair wise deletion yield consistent estimates, unbiased large samplesIf true missing data mechanism MCAR, pair wise deletion yield consistent estimates, unbiased large samplesCompared listwise deletion: (Glasser 1964)\ncorrelation among variables low, pairwise deletion efficient estimates listwise\ncorrelations among variables high, listwise deletion efficient pairwise.\nCompared listwise deletion: (Glasser 1964)correlation among variables low, pairwise deletion efficient estimates listwiseIf correlations among variables high, listwise deletion efficient pairwise.Disadvantages:data mechanism MAR, pairwise deletion yield biased estimates.small sample, sometimes covariance matrix might positive definite, means coefficients estimates calculated.Note: need read carefully software specify sample size alter standard errors.","code":""},{"path":"imputation-missing-data.html","id":"dummy-variable-adjustment","chapter":"11 Imputation (Missing Data)","heading":"11.2.3 Dummy Variable Adjustment","text":"Also known Missing Indicator Method Proxy VariableAdd another variable database indicate whether value missing.Create 2 variables\\[\nD=\n\\begin{cases}\n1 & \\text{data X missing} \\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\]\\[\nX^* =\n\\begin{cases}\nX & \\text{data available} \\\\\nc & \\text{data missing}\\\\\n\\end{cases}\n\\]Note: typical choice \\(c\\) usually mean \\(X\\)Interpretation:Coefficient \\(D\\) difference expected value \\(Y\\) group data group without data \\(X\\).Coefficient \\(X^*\\) effect group data \\(Y\\)Disadvantages:method yields biased estimates coefficient even case MCAR (Jones 1996)","code":""},{"path":"imputation-missing-data.html","id":"imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4 Imputation","text":"","code":""},{"path":"imputation-missing-data.html","id":"mean-mode-median-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.1 Mean, Mode, Median Imputation","text":"Bad:\nMean imputation preserve relationships among variables\nMean imputation leads Underestimate Standard Errors → ’re making Type errors without realizing .\nBiased estimates variances covariances (Haitovsky 1968)\nhigh-dimensions, mean substitution account dependence structure among features.\nBad:Mean imputation preserve relationships among variablesMean imputation leads Underestimate Standard Errors → ’re making Type errors without realizing .Biased estimates variances covariances (Haitovsky 1968)high-dimensions, mean substitution account dependence structure among features.","code":""},{"path":"imputation-missing-data.html","id":"maximum-likelihood","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2 Maximum Likelihood","text":"missing data MAR monotonic (case panel studies), ML can adequately estimating coefficients.Monotonic means missing data X1, observation also missing data variables come .ML can generally handle linear models, log-linear model, beyond , ML still lacks theory software implement.","code":""},{"path":"imputation-missing-data.html","id":"expectation-maximization-algorithm-em-algorithm","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2.1 Expectation-Maximization Algorithm (EM Algorithm)","text":"iterative process:variables used impute value (Expectation).Check whether value likely (Maximization)., re-imputes likely value.start regression estimates based either listwise deletion pairwise deletion. regressing missing variables available variables, obtain regression model. Plug missing data back original model, modified variances covariances example, missing data \\(X_{ij}\\) regress available data \\(X_{(j)}\\), plug expected value \\(X_{ij}\\) back \\(X_{ij}^2\\) turn \\(X_{ij}^2 + s_{j(j)}^2\\) \\(s_{j(j)}^2\\) stands residual variance regressing \\(X_{ij}\\) \\(X_{(j)}\\) new estimated model, rerun process estimates converge.Advantages:Easy usePreserves relationship variables (important use Factor Analysis Linear Regression later ), best case Factor Analysis, doesn’t require standard error individuals item.Disadvantages:Standard errors coefficients incorrect (biased usually downward - underestimate)Models overidentification, estimates efficient","code":""},{"path":"imputation-missing-data.html","id":"direct-ml-raw-maximum-likelihood","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2.2 Direct ML (raw maximum likelihood)","text":"AdvantagesEfficient estimates correct standard errors.Disadvantages:Hard implements","code":""},{"path":"imputation-missing-data.html","id":"multiple-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3 Multiple Imputation","text":"MI designed use “Bayesian model-based approach create procedures, frequentist (randomization-based approach) evaluate procedures”. (Rubin 1996)MI estimates properties ML data MARConsistentAsymptotically efficientAsymptotically normalMI can applied type model, unlike Maximum Likelihood limited small set models.drawback MI produce slightly different estimates every time run . avoid problem, can set seed analysis ensure reproducibility.","code":""},{"path":"imputation-missing-data.html","id":"single-random-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3.1 Single Random Imputation","text":"Random draws form residual distribution imputed variable add random numbers imputed values.example, missing data \\(X\\), ’s MCAR, thenRegress \\(X\\) \\(Y\\) (Listwise Deletion method) get residual distribution.Regress \\(X\\) \\(Y\\) (Listwise Deletion method) get residual distribution.every missing value X, substitute \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) \n\\(u_i\\) random draw standard normal distribution\n\\(x_i\\) predicted value regression X Y\n\\(\\rho\\) standard deviation residual distribution X regressed Y.\nevery missing value X, substitute \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) \\(u_i\\) random draw standard normal distribution\\(x_i\\) predicted value regression X Y\\(\\rho\\) standard deviation residual distribution X regressed Y.However, model run imputed data still thinks data collected, imputed, leads standard error estimates low test statistics high.address problem, need repeat imputation process leads us repeated imputation multiple random imputation.","code":""},{"path":"imputation-missing-data.html","id":"repeated-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3.2 Repeated Imputation","text":"“Repeated imputations draws posterior predictive distribution missing values specific model , particular Bayesian model data missing mechanism”.(Rubin 1996)Repeated imputation, also known , multiple random imputation, allows us multiple “completed” data sets. variability across imputations adjust standard errors upward.estimate standard error \\(\\bar{r}\\) (mean correlation estimates X Y) \\[\nSE(\\bar{r})=\\sqrt{\\frac{1}{M}\\sum_{k}s_k^2+ (1+\\frac{1}{M})(\\frac{1}{M-1})\\sum_{k}(r_k-\\bar{r})^2}\n\\] M number replications, \\(r_k\\) correlation replication k, \\(s_k\\) estimated standard error replication k.However, method still considers parameter predicting \\(\\tilde{x}\\) still fixed, means assume using true parameters predict \\(\\tilde{x}\\). overcome challenge, need introduce variability model \\(\\tilde{x}\\) treating parameters random variables use Bayesian posterior distribution parameters predict parameters.However, sample large proportion missing data small, extra Bayesian step might necessary. sample small proportion missing data large, extra Bayesian step necessary.Two algorithms get random draws regression parameters posterior distribution:Data AugmentationSampling importance/resampling (SIR)Authors argued SIR superiority due computer time (G. King et al. 2001)","code":""},{},{"path":"imputation-missing-data.html","id":"nonparametric-semiparametric-methods","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4 Nonparametric/ Semiparametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"hot-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.1 Hot Deck Imputation","text":"Used U.S. Census Bureau public datasetsapproximate Bayesian bootstrapA randomly chosen value individual sample similar values variables. words, find sample subjects similar variables, randomly choose one values missing variable.\\(n_1\\) cases complete data \\(Y\\) \\(n_0\\) cases missing data \\(Y\\)Step 1: \\(n_1\\), take random sample (replacement) \\(n_1\\) casesStep 2: retrieved sample take random sample (replacement) \\(n_0\\) casesStep 3: Assign \\(n_0\\) cases step 2 \\(n_0\\) missing data cases.Step 4: Repeat process every variable.Step 5: multiple imputation, repeat four steps multiple times.Note:skip step 1, reduce variability estimating standard errors.skip step 1, reduce variability estimating standard errors.Good:\nConstrained possible values.\nSince value picked random, adds variability, might come handy calculating standard errors.\nGood:Constrained possible values.Since value picked random, adds variability, might come handy calculating standard errors.Challenge: can define “similar” .Challenge: can define “similar” .","code":""},{"path":"imputation-missing-data.html","id":"cold-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.2 Cold Deck Imputation","text":"Contrary Hot Deck, Cold Deck choose value systematically observation similar values variables, remove random variation want.Donor samples “cold-deck” imputation come different data set.","code":""},{"path":"imputation-missing-data.html","id":"predictive-mean-matching","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.3 Predictive Mean Matching","text":"Steps:Regress \\(Y\\) \\(X\\) (matrix covariates) \\(n_1\\) (.e., non-missing cases) get coefficients \\(b\\) (\\(k \\times 1\\) vector) residual variance estimates \\(s^2\\)Draw randomly posterior predictive distribution residual variance (assuming noninformative prior) calculating \\(\\frac{(n_1-k)s^2}{\\chi^2}\\), \\(\\chi^2\\) random draw \\(\\chi^2_{n_1-k}\\) let \\(s^2_{[1]}\\) -th random drawRandomly draw posterior distribution coefficients \\(b\\), drawing \\(MVN(b, s^2_{[1]}(X'X)^{-1})\\), X \\(n_1 \\times k\\) matrix X values. \\(b_{1}\\)Using step 1, can calculate standardized residuals \\(n_1\\) cases: \\(e_i = \\frac{y_i - bx_i}{\\sqrt{s^2(1-k/n_1)}}\\)Randomly draw sample (replacement) \\(n_0\\) \\(n_1\\) residuals step 4With \\(n_0\\) cases, can calculate imputed values \\(Y\\): \\(y_i = b_{[1]}x_i + s_{[1]}e_i\\) \\(e_i\\) taken step 5, \\(b_{[1]}\\) taken step 3, \\(s_{[1]}\\) taken step 2.Repeat steps 2 6 except step 4.Notes:can used multiple variables variable imputed using variables predictor.can also used heteroskedasticity imputed values.Example Statistics GlobeExample UCLA Statistical Consultingrr = number observations pairs values observedrm = number observations variables missing valuesmr = number observations first variable’s value (e.g. row variable) observed second (column) variable missingmm = number observations second variable’s value (e.g. col variable) observed first (row) variable missing","code":"\nset.seed(918273) # Seed\nN  <- 3000                                    # Sample size\ny  <- round(runif(N,-10, 10))                 # Target variable Y\nx1 <- y + round(runif(N, 0, 50))              # Auxiliary variable 1\nx2 <- round(y + 0.25 * x1 + rnorm(N,-3, 15))  # Auxiliary variable 2\nx3 <- round(0.1 * x1 + rpois(N, 2))           # Auxiliary variable 3\n# (categorical variable)\nx4 <- as.factor(round(0.02 * y + runif(N)))   # Auxiliary variable 4 \n\n# Insert 20% missing data in Y\ny[rbinom(N, 1, 0.2) == 1] <- NA               \n\ndata <- data.frame(y, x1, x2, x3, x4)         # Store data in dataset\nhead(data) # First 6 rows of our data\n#>    y x1  x2 x3 x4\n#> 1  8 38  -3  6  1\n#> 2  1 50  -9  5  0\n#> 3  5 43  20  5  1\n#> 4 NA  9  13  3  0\n#> 5 -4 40 -10  6  0\n#> 6 NA 29  -6  5  1\n\nlibrary(\"mice\") # Load mice package\n\n##### Impute data via predictive mean matching (single imputation)#####\n\nimp_single <- mice(data, m = 1, method = \"pmm\") # Impute missing values\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_imp_single <- complete(imp_single)         # Store imputed data\n# head(data_imp_single)\n\n# Since single imputation underestiamtes stnadard errors, \n# we use multiple imputaiton\n\n##### Predictive mean matching (multiple imputation) #####\n\n# Impute missing values multiple times\nimp_multi <- mice(data, m = 5, method = \"pmm\")  \n#> \n#>  iter imp variable\n#>   1   1  y\n#>   1   2  y\n#>   1   3  y\n#>   1   4  y\n#>   1   5  y\n#>   2   1  y\n#>   2   2  y\n#>   2   3  y\n#>   2   4  y\n#>   2   5  y\n#>   3   1  y\n#>   3   2  y\n#>   3   3  y\n#>   3   4  y\n#>   3   5  y\n#>   4   1  y\n#>   4   2  y\n#>   4   3  y\n#>   4   4  y\n#>   4   5  y\n#>   5   1  y\n#>   5   2  y\n#>   5   3  y\n#>   5   4  y\n#>   5   5  y\ndata_imp_multi_all <-\n    # Store multiply imputed data\n    complete(imp_multi,       \n             \"repeated\",\n             include = TRUE)\n\ndata_imp_multi <-\n    # Combine imputed Y and X1-X4 (for convenience)\n    data.frame(data_imp_multi_all[, 1:6], data[, 2:5])\n\nhead(data_imp_multi)\n#>   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4\n#> 1   8   8   8   8   8   8 38  -3  6  1\n#> 2   1   1   1   1   1   1 50  -9  5  0\n#> 3   5   5   5   5   5   5 43  20  5  1\n#> 4  NA   1  -2  -4   9  -8  9  13  3  0\n#> 5  -4  -4  -4  -4  -4  -4 40 -10  6  0\n#> 6  NA   4   7   7   6   0 29  -6  5  1\nlibrary(mice)\nlibrary(VIM)\nlibrary(lattice)\nlibrary(ggplot2)\n## set observations to NA\nanscombe <- within(anscombe, {\n    y1[1:3] <- NA\n    y4[3:5] <- NA\n})\n## view\nhead(anscombe)\n#>   x1 x2 x3 x4   y1   y2    y3   y4\n#> 1 10 10 10  8   NA 9.14  7.46 6.58\n#> 2  8  8  8  8   NA 8.14  6.77 5.76\n#> 3 13 13 13  8   NA 8.74 12.74   NA\n#> 4  9  9  9  8 8.81 8.77  7.11   NA\n#> 5 11 11 11  8 8.33 9.26  7.81   NA\n#> 6 14 14 14  8 9.96 8.10  8.84 7.04\n\n## check missing data patterns\nmd.pattern(anscombe)#>   x1 x2 x3 x4 y2 y3 y1 y4  \n#> 6  1  1  1  1  1  1  1  1 0\n#> 2  1  1  1  1  1  1  1  0 1\n#> 2  1  1  1  1  1  1  0  1 1\n#> 1  1  1  1  1  1  1  0  0 2\n#>    0  0  0  0  0  0  3  3 6\n\n## Number of observations per patterns for all pairs of variables\np <- md.pairs(anscombe)\np \n#> $rr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1 11 11 11 11  8 11 11  8\n#> x2 11 11 11 11  8 11 11  8\n#> x3 11 11 11 11  8 11 11  8\n#> x4 11 11 11 11  8 11 11  8\n#> y1  8  8  8  8  8  8  8  6\n#> y2 11 11 11 11  8 11 11  8\n#> y3 11 11 11 11  8 11 11  8\n#> y4  8  8  8  8  6  8  8  8\n#> \n#> $rm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  3  0  0  3\n#> x2  0  0  0  0  3  0  0  3\n#> x3  0  0  0  0  3  0  0  3\n#> x4  0  0  0  0  3  0  0  3\n#> y1  0  0  0  0  0  0  0  2\n#> y2  0  0  0  0  3  0  0  3\n#> y3  0  0  0  0  3  0  0  3\n#> y4  0  0  0  0  2  0  0  0\n#> \n#> $mr\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  3  3  3  3  0  3  3  2\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  3  3  3  3  2  3  3  0\n#> \n#> $mm\n#>    x1 x2 x3 x4 y1 y2 y3 y4\n#> x1  0  0  0  0  0  0  0  0\n#> x2  0  0  0  0  0  0  0  0\n#> x3  0  0  0  0  0  0  0  0\n#> x4  0  0  0  0  0  0  0  0\n#> y1  0  0  0  0  3  0  0  1\n#> y2  0  0  0  0  0  0  0  0\n#> y3  0  0  0  0  0  0  0  0\n#> y4  0  0  0  0  1  0  0  3\n## Margin plot of y1 and y4\nmarginplot(anscombe[c(5, 8)], col = c(\"blue\", \"red\", \"orange\"))\n\n## 5 imputations for all missing values\nimp1 <- mice(anscombe, m = 5)\n#> \n#>  iter imp variable\n#>   1   1  y1  y4\n#>   1   2  y1  y4\n#>   1   3  y1  y4\n#>   1   4  y1  y4\n#>   1   5  y1  y4\n#>   2   1  y1  y4\n#>   2   2  y1  y4\n#>   2   3  y1  y4\n#>   2   4  y1  y4\n#>   2   5  y1  y4\n#>   3   1  y1  y4\n#>   3   2  y1  y4\n#>   3   3  y1  y4\n#>   3   4  y1  y4\n#>   3   5  y1  y4\n#>   4   1  y1  y4\n#>   4   2  y1  y4\n#>   4   3  y1  y4\n#>   4   4  y1  y4\n#>   4   5  y1  y4\n#>   5   1  y1  y4\n#>   5   2  y1  y4\n#>   5   3  y1  y4\n#>   5   4  y1  y4\n#>   5   5  y1  y4\n\n## linear regression for each imputed data set - 5 regression are run\nfitm <- with(imp1, lm(y1 ~ y4 + x1))\nsummary(fitm)\n#> # A tibble: 15 × 6\n#>    term        estimate std.error statistic p.value  nobs\n#>    <chr>          <dbl>     <dbl>     <dbl>   <dbl> <int>\n#>  1 (Intercept)    8.60      2.67      3.23  0.0121     11\n#>  2 y4            -0.533     0.251    -2.12  0.0667     11\n#>  3 x1             0.334     0.155     2.16  0.0628     11\n#>  4 (Intercept)    4.19      2.93      1.43  0.190      11\n#>  5 y4            -0.213     0.273    -0.782 0.457      11\n#>  6 x1             0.510     0.167     3.05  0.0159     11\n#>  7 (Intercept)    6.51      2.35      2.77  0.0244     11\n#>  8 y4            -0.347     0.215    -1.62  0.145      11\n#>  9 x1             0.395     0.132     3.00  0.0169     11\n#> 10 (Intercept)    5.48      3.02      1.81  0.107      11\n#> 11 y4            -0.316     0.282    -1.12  0.295      11\n#> 12 x1             0.486     0.173     2.81  0.0230     11\n#> 13 (Intercept)    7.12      1.81      3.92  0.00439    11\n#> 14 y4            -0.436     0.173    -2.53  0.0355     11\n#> 15 x1             0.425     0.102     4.18  0.00308    11\n\n## pool coefficients and standard errors across all 5 regression models\npool(fitm)\n#> Class: mipo    m = 5 \n#>          term m   estimate       ubar           b           t dfcom       df\n#> 1 (Intercept) 5  6.3808015 6.72703243 2.785088109 10.06913816     8 3.902859\n#> 2          y4 5 -0.3690455 0.05860053 0.014674911  0.07621042     8 4.716160\n#> 3          x1 5  0.4301588 0.02191260 0.004980516  0.02788922     8 4.856052\n#>         riv    lambda       fmi\n#> 1 0.4968172 0.3319158 0.5254832\n#> 2 0.3005074 0.2310693 0.4303733\n#> 3 0.2727480 0.2142985 0.4143230\n\n## output parameter estimates\nsummary(pool(fitm))\n#>          term   estimate std.error statistic       df    p.value\n#> 1 (Intercept)  6.3808015 3.1731905  2.010847 3.902859 0.11643863\n#> 2          y4 -0.3690455 0.2760624 -1.336819 4.716160 0.24213491\n#> 3          x1  0.4301588 0.1670007  2.575791 4.856052 0.05107581"},{"path":"imputation-missing-data.html","id":"stochastic-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.4 Stochastic Imputation","text":"Regression imputation + random residual = Stochastic ImputationMost multiple imputation based form stochastic regression imputation.Good:advantage Regression Imputationand also random componentsBad:might lead implausible values (e.g. negative values)can’t handle heteroskadastic dataNote\nMultiple Imputation usually based form stochastic regression imputation.Single stochastic regression imputationSingle predictive mean matchingStochastic regression imputation contains negative valuesEvidence heteroskadastic dataSingle stochastic regression imputationSingle predictive mean matchingComparison predictive mean matching stochastic regression imputation","code":"\n# Income data\n \nset.seed(91919)                              # Set seed\nN <- 1000                                    # Sample size\n \nincome <- round(rnorm(N, 0, 500))            # Create some synthetic income data\nincome[income < 0] <- income[income < 0] * (- 1)\n \nx1 <- income + rnorm(N, 1000, 1500)          # Auxiliary variables\nx2 <- income + rnorm(N, - 5000, 2000)\n \nincome[rbinom(N, 1, 0.1) == 1] <- NA         # Create 10% missingness in income\n \ndata_inc_miss <- data.frame(income, x1, x2)\nimp_inc_sri  <- mice(data_inc_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_sri <- complete(imp_inc_sri)\nimp_inc_pmm  <- mice(data_inc_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  income\n#>   2   1  income\n#>   3   1  income\n#>   4   1  income\n#>   5   1  income\ndata_inc_pmm <- complete(imp_inc_pmm)\ndata_inc_sri$income[data_inc_sri$income < 0]\n#> [1]  -66.055957  -96.980053  -28.921432   -4.175686  -54.480798  -27.207102\n#> [7] -143.603500  -80.960488\n# No values below 0\ndata_inc_pmm$income[data_inc_pmm$income < 0] \n#> numeric(0)\n# Heteroscedastic data\n \nset.seed(654654)                             # Set seed\nN <- 1:5000                                  # Sample size\n \na <- 0\nb <- 1\nsigma2 <- N^2\neps <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n \ny <- a + b * N + eps                         # Heteroscedastic variable\nx <- 30 * N + rnorm(N[length(N)], 1000, 200) # Correlated variable\n \ny[rbinom(N[length(N)], 1, 0.3) == 1] <- NA   # 30% missing\n \ndata_het_miss <- data.frame(y, x)\nimp_het_sri  <- mice(data_het_miss, method = \"norm.nob\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_sri <- complete(imp_het_sri)\nimp_het_pmm  <- mice(data_het_miss, method = \"pmm\", m = 1)\n#> \n#>  iter imp variable\n#>   1   1  y\n#>   2   1  y\n#>   3   1  y\n#>   4   1  y\n#>   5   1  y\ndata_het_pmm <- complete(imp_het_pmm)\npar(mfrow = c(1, 2))                              # Both plots in one graphic\n\n# Plot of observed values\nplot(x[!is.na(data_het_sri$y)],\n     data_het_sri$y[!is.na(data_het_sri$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n# Plot of missing values\npoints(x[is.na(y)], data_het_sri$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Stochastic Regression Imputation\",        \n      line = 0.5)\n\n# Regression line\nabline(lm(y ~ x, data_het_sri),                   \n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\n# Plot of observed values\nplot(x[!is.na(data_het_pmm$y)],\n     data_het_pmm$y[!is.na(data_het_pmm$y)],\n     main = \"\",\n     xlab = \"X\",\n     ylab = \"Y\")\n\n\n# Plot of missing values\npoints(x[is.na(y)], data_het_pmm$y[is.na(y)],\n       col = \"red\")\n\n# Title of plot\ntitle(\"Predictive Mean Matching\",\n      line = 0.5)\nabline(lm(y ~ x, data_het_pmm),\n       col = \"#1b98e0\", lwd = 2.5)\n\n# Legend\nlegend(\n  \"topleft\",\n  c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n  pch = c(1, 1, NA),\n  lty = c(NA, NA, 1),\n  col = c(\"black\", \"red\", \"#1b98e0\")\n)\n\nmtext(\n  \"Imputation of Heteroscedastic Data\",\n  # Main title of plot\n  side = 3,\n  line = -1.5,\n  outer = TRUE,\n  cex = 2\n)"},{"path":"imputation-missing-data.html","id":"regression-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.5 Regression Imputation","text":"Also known conditional mean imputation Missing value based (regress) variables.Good:\nMaintain relationship variables (.e., preserve dependence structure among features, unlike 11.2.4.1).\ndata MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)\nCan improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).\n\nGood:Maintain relationship variables (.e., preserve dependence structure among features, unlike 11.2.4.1).Maintain relationship variables (.e., preserve dependence structure among features, unlike 11.2.4.1).data MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)\nCan improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).\ndata MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)Can improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).Bad:\nvariability left. treated data collected.\nUnderestimate standard errors overestimate test statistics\nBad:variability left. treated data collected.Underestimate standard errors overestimate test statistics","code":""},{"path":"imputation-missing-data.html","id":"interpolation-and-extrapolation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.6 Interpolation and Extrapolation","text":"estimated value observations individual. usually works longitudinal data.","code":""},{"path":"imputation-missing-data.html","id":"k-nearest-neighbor-knn-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.7 K-nearest neighbor (KNN) imputation","text":"methods model-based imputation (regression).\nexample neighbor-based imputation (K-nearest neighbor).every observation needs imputed, algorithm identifies ‘k’ closest observations based types distance (e.g., Euclidean) computes weighted average (weighted based distance) ‘k’ obs.discrete variable, uses frequent value among k nearest neighbors.Distance metrics: Hamming distance.continuous variable, uses mean mode.Distance metrics:\nEuclidean\nMahalanobis\nManhattan\nDistance metrics:EuclideanMahalanobisManhattan","code":""},{"path":"imputation-missing-data.html","id":"bayesian-ridge-regression-implementation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.8 Bayesian Ridge regression implementation","text":"","code":""},{"path":"imputation-missing-data.html","id":"matrix-completion","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.9 Matrix Completion","text":"Impute items missing random accounting dependence features using principal components, known matrix completion (James et al. 2013, Sec 12.3)Consider \\(n \\times p\\) feature matrix, \\(\\mathbf{X}\\), element \\(x_{ij}\\), missing.Similar 22.2, can approximate matrix \\(\\mathbf{X}\\) terms leading PCs.consider \\(M\\) principal components optimize\\[\n\\underset{\\mathbf{} \\\\mathbb{R}^{n \\times M}, \\mathbf{B} \\\\mathbb{R}^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\}\n\\]\\(\\mathcal{O}\\) set observed pairs indices \\((,j)\\), subset possible \\(n \\times p\\) pairsOnce minimization solved,One can impute missing observation, \\(x_{ij}\\), \\(\\hat{x}_{ij} = \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\\) \\(\\hat{}_{im}, \\hat{b}_{jm}\\) \\((,m)\\) \\((j.m)\\) elements, respectively, matrices \\(\\hat{\\mathbf{}}\\) \\(\\hat{\\mathbf{B}}\\) minimization, andOne can impute missing observation, \\(x_{ij}\\), \\(\\hat{x}_{ij} = \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\\) \\(\\hat{}_{im}, \\hat{b}_{jm}\\) \\((,m)\\) \\((j.m)\\) elements, respectively, matrices \\(\\hat{\\mathbf{}}\\) \\(\\hat{\\mathbf{B}}\\) minimization, andOne can approximately recover \\(M\\) principal component scores loadings, data completeOne can approximately recover \\(M\\) principal component scores loadings, data completeThe challenge solve minimization problem: eigen-decomposition non longer applies (22.2Hence, use iterative algorithm (James et al. 2013 Alg 12.1)Create complete data matrix \\(\\tilde{\\mathbf{X}}\\) dimension \\(n \\times p\\) \\((,j)\\) element equals\\[\n\\tilde{x}_{ij} =\n\\begin{cases}\nx_{ij} & \\text{} (,j) \\\\mathcal{O} \\\\\n\\bar{x}_{j} & \\text{} (,j) \\notin \\mathcal{O}\n\\end{cases}\n\\]\\(\\bar{x}_j\\) average observed values \\(j\\)th variable incomplete data matrix \\(\\mathbf{X}\\)\\(\\mathcal{O}\\) indexes observations observed \\(\\mathbf{X}\\)Repeat 3 steps objectives meta. Solve\\[\n\\underset{\\mathbf{} \\R^{n \\times M}, \\mathbf{B} \\R^{p \\times M}}{\\operatorname{min}} \\{ \\sum_{(,j) \\\\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\}\n\\]computing principal components \\(\\tilde{\\mathbf{X}}\\)b. element \\((,j) \\notin \\mathcal{O}\\), set \\(\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{}_{im}\\hat{b}_{jm}\\)c. Compute objective\\[\n\\sum_{(,j \\\\mathcal{O})} (x_{ij} - \\sum_{m=1}^M \\hat{}_{im} \\hat{b}_{jm})^2\n\\]Return estimated missing entries \\(\\tilde{x}_{ij}, (,j) \\notin \\mathcal{O}\\)","code":""},{"path":"imputation-missing-data.html","id":"other-methods","chapter":"11 Imputation (Missing Data)","heading":"11.2.5 Other methods","text":"panel data, clustered data, use pan package Schafer (1997)","code":""},{"path":"imputation-missing-data.html","id":"criteria-for-choosing-an-effective-approach","chapter":"11 Imputation (Missing Data)","heading":"11.3 Criteria for Choosing an Effective Approach","text":"Criteria ideal technique treating missing data:Unbiased parameter estimatesAdequate powerAccurate standard errors (p-values, confidence intervals)Multiple Imputation Full Information Maximum Likelihood ideal candidate. Single imputation generally lead underestimation standard errors.","code":""},{"path":"imputation-missing-data.html","id":"another-perspective","chapter":"11 Imputation (Missing Data)","heading":"11.4 Another Perspective","text":"Model bias can arisen various factors including:Imputation methodMissing data mechanism (MCAR vs. MAR)Proportion missing dataInformation available data setSince imputed observations estimates, values corresponding random error. put estimate data point, software doesn’t know . overlooks extra source error, resulting -small standard errors -small p-values. multiple imputation comes multiple estimates.multiple imputation random component, multiple estimates slightly different. re-introduces variation software can incorporate order give model accurate estimates standard error. Multiple imputation huge breakthrough statistics 20 years ago. solves lot problems missing data (though, unfortunately ) done well, leads unbiased parameter estimates accurate standard errors. rate missing data , small (2-3%) doesn’t matter technique use.Remember three goals multiple imputation, missing data technique:Unbiased parameter estimates final analysis (regression coefficients, group means, odds ratios, etc.)accurate standard errors parameter estimates, therefore, accurate p-values analysisadequate power find meaningful parameter values significant.Hence,Don’t round imputations dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions imputing categorical variables dummy code , impute , round imputed values 0 1. Recent research, however, found rounding imputed values actually leads biased parameter estimates analysis model. actually get better results leaving imputed values impossible values, even though ’s counter-intuitive.Don’t round imputations dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions imputing categorical variables dummy code , impute , round imputed values 0 1. Recent research, however, found rounding imputed values actually leads biased parameter estimates analysis model. actually get better results leaving imputed values impossible values, even though ’s counter-intuitive.Don’t transform skewed variables. Likewise, transform variable meet normality assumptions imputing, changing distribution variable relationship variable others use impute. can lead imputing outliers, creating bias just imputing skewed variable.Don’t transform skewed variables. Likewise, transform variable meet normality assumptions imputing, changing distribution variable relationship variable others use impute. can lead imputing outliers, creating bias just imputing skewed variable.Use imputations. advice years 5-10 imputations adequate. true unbiasedness, can get inconsistent results run multiple imputation . (Bodner 2008) recommends many imputations percentage missing data. Since running imputations isn’t work data analyst, ’s reason .Use imputations. advice years 5-10 imputations adequate. true unbiasedness, can get inconsistent results run multiple imputation . (Bodner 2008) recommends many imputations percentage missing data. Since running imputations isn’t work data analyst, ’s reason .Create multiplicative terms imputing. analysis model contains multiplicative term, like interaction term quadratic, create multiplicative terms first, impute. Imputing first, creating multiplicative terms actually biases regression parameters multiplicative term (Von Hippel 2009)Create multiplicative terms imputing. analysis model contains multiplicative term, like interaction term quadratic, create multiplicative terms first, impute. Imputing first, creating multiplicative terms actually biases regression parameters multiplicative term (Von Hippel 2009)","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-the-mechanism","chapter":"11 Imputation (Missing Data)","heading":"11.5 Diagnosing the Mechanism","text":"","code":""},{"path":"imputation-missing-data.html","id":"mar-vs.-mnar","chapter":"11 Imputation (Missing Data)","heading":"11.5.1 MAR vs. MNAR","text":"true way distinguish MNAR MAR measure missing data.’s common practice among professional surveyors , example, follow-paper survey phone calls group non-respondents ask key survey items. allows compare respondents non-respondents.responses key items differ much, ’s good evidence data MNAR.However missing data situations, can’t get hold missing data. can’t test directly, can examine patterns data get idea ’s likely mechanism.first thing diagnosing randomness missing data use substantive scientific knowledge data field. sensitive issue, less likely people tell . ’re going tell much cocaine usage phone usage.Likewise, many fields common research situations non-ignorable data common. Educate field’s literature.","code":""},{"path":"imputation-missing-data.html","id":"mcar-vs.-mar","chapter":"11 Imputation (Missing Data)","heading":"11.5.2 MCAR vs. MAR","text":"useful test MCAR, Little’s test.second technique create dummy variables whether variable missing.1 = missing 0 = observedYou can run t-tests chi-square tests variable variables data set see missingness variable related values variables.example, women really less likely tell weight men, chi-square test tell percentage missing data weight variable higher women men.","code":""},{"path":"imputation-missing-data.html","id":"application-7","chapter":"11 Imputation (Missing Data)","heading":"11.6 Application","text":"Read Missing Book Nicholas Tierney & Allison HorstHow many imputation:Usually 5. (unless extremely high portion missing, case probably need check data )According Rubin, relative efficiency estimate based \\(m\\) imputations infinity imputation approximately\\[\n(1+\\frac{\\lambda}{m})^{-1}\n\\]\\(\\lambda\\) rate missing dataExample 50% missing data means estimate based 5 imputation standard deviation 5% wider compared estimate based infinity imputation\n(\\(\\sqrt{1+0.5/5}=1.049\\))","code":"\nlibrary(visdat)\nlibrary(naniar)\nlibrary(ggplot2)\nvis_miss()\n\n\n\nggplot(data, aes(x, y)) +\n  geom_miss_point() +\n  facet_wrap( ~ group)\n\ngg_miss_var(data, facet = group)\n\ngg_miss_upset(data)\n\ngg_miss_fct(x = variable1, fct = variable2)\nlibrary(missForest)\n\n#load data\ndata <- iris\n\n#Generate 10% missing values at Random\nset.seed(1)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n#remove categorical variables\niris.mis.cat <- iris.mis\niris.mis <- subset(iris.mis, select = -c(Species))"},{"path":"imputation-missing-data.html","id":"imputation-with-mean-median-mode","chapter":"11 Imputation (Missing Data)","heading":"11.6.1 Imputation with mean / median / mode","text":"check accuracy","code":"\n# whole data set\ne1071::impute(iris.mis, what = \"mean\")        # replace with mean\ne1071::impute(iris.mis, what = \"median\")      # replace with median\n\n# by variables\nHmisc::impute(iris.mis$Sepal.Length, mean)    # mean\nHmisc::impute(iris.mis$Sepal.Length, median)  # median\nHmisc::impute(iris.mis$Sepal.Length, 0)       # replace specific number\n# library(DMwR2)\n# actuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\n# predicteds <- rep(mean(iris$Sepal.Width, na.rm=T), length(actuals))\n# regr.eval(actuals, predicteds)"},{"path":"imputation-missing-data.html","id":"knn","chapter":"11 Imputation (Missing Data)","heading":"11.6.2 KNN","text":"Compared MAPE (mean absolute percentage error) mean imputation, see almost always see improvements.","code":"\n# library(DMwR2)\n# # iris.mis[,!names(iris.mis) %in% c(\"Sepal.Length\")]\n# # data should be this line. But since knn cant work with 3 or less variables, \n# # we need to use at least 4 variables.\n# \n# # knn is not appropriate for categorical variables\n# knnOutput <-\n#   knnImputation(data = iris.mis.cat,\n#                 #k = 10,\n#                 meth = \"median\" # could use \"median\" or \"weighAvg\")  \n#                 # should exclude the dependent variable: Sepal.Length\n#                 anyNA(knnOutput)\n# library(DMwR2)\n# actuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\n# predicteds <- knnOutput[is.na(iris.mis$Sepal.Width), \"Sepal.Width\"]\n# regr.eval(actuals, predicteds)"},{"path":"imputation-missing-data.html","id":"rpart","chapter":"11 Imputation (Missing Data)","heading":"11.6.3 rpart","text":"categorical (factor) variables, rpart can handle","code":"\nlibrary(rpart)\nclass_mod <-\n  rpart(\n    Species ~ . - Sepal.Length,\n    data = iris.mis.cat[!is.na(iris.mis.cat$Species),],\n    method = \"class\",\n    na.action = na.omit\n  )  # since Species is a factor, and exclude dependent variable \"Sepal.Length\"\n\nanova_mod <-\n  rpart(\n    Sepal.Width ~ . - Sepal.Length,\n    data = iris.mis[!is.na(iris.mis$Sepal.Width),],\n    method = \"anova\",\n    na.action = na.omit\n  )  # since Sepal.Width is numeric.\n\n\nspecies_pred <-\n  predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species),])\n\n\nwidth_pred <-\n  predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width),])"},{"path":"imputation-missing-data.html","id":"mice-multivariate-imputation-via-chained-equations","chapter":"11 Imputation (Missing Data)","heading":"11.6.4 MICE (Multivariate Imputation via Chained Equations)","text":"Assumption: data MARIt imputes data per variable specifying imputation model variableExampleWe \\(X_1, X_2,..,X_k\\). \\(X_1\\) missing data, regressed rest variables. procedure applies \\(X_2\\) missing data. , predicted values used place missing values.default,Continuous variables use linear regression.Categorical Variables use logistic regression.Methods MICE:PMM (Predictive Mean Matching) – numeric variableslogreg(Logistic Regression) – Binary Variables( 2 levels)polyreg (Bayesian polytomous regression) – Factor Variables (>= 2 levels)Proportional odds model (ordered, >= 2 levels)Impute DataCheck imputed datasetRegression model using imputed datasets","code":"\n# load package\nlibrary(mice)\nlibrary(VIM)\n\n# check missing values\nmd.pattern(iris.mis)#>     Sepal.Width Sepal.Length Petal.Length Petal.Width   \n#> 100           1            1            1           1  0\n#> 15            1            1            1           0  1\n#> 8             1            1            0           1  1\n#> 2             1            1            0           0  2\n#> 11            1            0            1           1  1\n#> 1             1            0            1           0  2\n#> 1             1            0            0           1  2\n#> 1             1            0            0           0  3\n#> 7             0            1            1           1  1\n#> 3             0            1            0           1  2\n#> 1             0            0            1           1  2\n#>              11           15           15          19 60\n\n#plot the missing values\naggr(\n  iris.mis,\n  col = mdc(1:2),\n  numbers = TRUE,\n  sortVars = TRUE,\n  labels = names(iris.mis),\n  cex.axis = .7,\n  gap = 3,\n  ylab = c(\"Proportion of missingness\", \"Missingness Pattern\")\n)#> \n#>  Variables sorted by number of missings: \n#>      Variable      Count\n#>   Petal.Width 0.12666667\n#>  Sepal.Length 0.10000000\n#>  Petal.Length 0.10000000\n#>   Sepal.Width 0.07333333\n\n\nmice_plot <- aggr(\n  iris.mis,\n  col = c('navyblue', 'yellow'),\n  numbers = TRUE,\n  sortVars = TRUE,\n  labels = names(iris.mis),\n  cex.axis = .7,\n  gap = 3,\n  ylab = c(\"Missing data\", \"Pattern\")\n)#> \n#>  Variables sorted by number of missings: \n#>      Variable      Count\n#>   Petal.Width 0.12666667\n#>  Sepal.Length 0.10000000\n#>  Petal.Length 0.10000000\n#>   Sepal.Width 0.07333333\nimputed_Data <-\n    mice(\n        iris.mis,\n        m = 5, # number of imputed datasets\n        maxit = 50, # number of iterations taken to impute missing values\n        method = 'pmm', # method used in imputation. \n        # Here, we used predictive mean matching\n        \n        \n        # other methods can be \n        # \"pmm\": Predictive mean matching\n        # \"midastouch\" : weighted predictive mean matching\n        # \"sample\": Random sample from observed values\n        # \"cart\": classification and regression trees\n        # \"rf\": random forest imputations.\n        # \"2lonly.pmm\": Level-2 class predictive mean matching\n        # Other methods based on whether variables are \n        # (1) numeric, (2) binary, (3) ordered, (4), unordered\n        seed = 500\n    )\nsummary(imputed_Data)\n#> Class: mids\n#> Number of multiple imputations:  5 \n#> Imputation methods:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>        \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\" \n#> PredictorMatrix:\n#>              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> Sepal.Length            0           1            1           1\n#> Sepal.Width             1           0            1           1\n#> Petal.Length            1           1            0           1\n#> Petal.Width             1           1            1           0\n\n#make a density plot\ndensityplot(imputed_Data)\n#the red (imputed values) should be similar to the blue (observed)\n# 1st dataset \ncompleteData <- complete(imputed_Data,1)\n\n# 2nd dataset\ncomplete(imputed_Data,2)\n# regression model\nfit <-\n  with(data = imputed_Data,\n       exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width))\n\n#combine results of all 5 models\ncombine <- pool(fit)\nsummary(combine)\n#>           term   estimate  std.error statistic       df      p.value\n#> 1  (Intercept)  1.8963130 0.32453912  5.843095 131.0856 3.838556e-08\n#> 2 Sepal.Length  0.2974293 0.06679204  4.453066 130.2103 1.802241e-05\n#> 3  Petal.Width -0.4811603 0.07376809 -6.522608 108.8253 2.243032e-09"},{"path":"imputation-missing-data.html","id":"amelia","chapter":"11 Imputation (Missing Data)","heading":"11.6.5 Amelia","text":"Use bootstrap based EMB algorithm (faster robust impute many variables including cross sectional, time series data etc)Use parallel imputation feature using multicore CPUs.AssumptionsAll variables follow Multivariate Normal Distribution (MVN). Hence, package works best data MVN, transformation normality.Missing data Missing Random (MAR)Steps:m bootstrap samples applies EMB algorithm sample. m different estimates mean variances.first set estimates used impute first set missing values using regression, second set estimates used second set .However, Amelia different MICEMICE imputes data variable variable basis whereas MVN uses joint modeling approach based multivariate normal distribution.MICE can handle different types variables variables MVN need normally distributed transformed approximate normality.MICE can manage imputation variables defined subset data whereas MVN .idvars – keep ID variables variables don’t want imputenoms – keep nominal variables ","code":"\nlibrary(Amelia)\ndata(\"iris\")\n#seed 10% missing values\niris.mis <- prodNA(iris, noNA = 0.1)\n#specify columns and run amelia\namelia_fit <-\n  amelia(iris.mis,\n         m = 5,\n         parallel = \"multicore\",\n         noms = \"Species\")\n#> -- Imputation 1 --\n#> \n#>   1  2  3  4  5  6  7  8\n#> \n#> -- Imputation 2 --\n#> \n#>   1  2  3  4  5  6  7  8\n#> \n#> -- Imputation 3 --\n#> \n#>   1  2  3  4  5\n#> \n#> -- Imputation 4 --\n#> \n#>   1  2  3  4  5  6  7\n#> \n#> -- Imputation 5 --\n#> \n#>   1  2  3  4  5  6  7\n\n# access imputed outputs\n# amelia_fit$imputations[[1]]"},{"path":"imputation-missing-data.html","id":"missforest","chapter":"11 Imputation (Missing Data)","heading":"11.6.6 missForest","text":"implementation random forest algorithm (non parametric imputation method applicable various variable types). Hence, assumption function form f. Instead, tries estimate f can close data points possible.builds random forest model variable. uses model predict missing values variable help observed values.yields bag imputation error estimate. Moreover, provides high level control imputation process.Since bagging works well categorical variable , don’t need remove .check imputation errorNRMSE normalized mean squared error. used represent error derived imputing continuous values.PFC (proportion falsely classified) used represent error derived imputing categorical values.means categorical variables imputed 5% error continuous variables imputed 14% error.can improved tuning values mtry ntree parameter.mtry refers number variables randomly sampled split.ntree refers number trees grow forest.","code":"\nlibrary(missForest)\n#impute missing values, using all parameters as default values\niris.imp <- missForest(iris.mis)\n# check imputed values\n# iris.imp$ximp\niris.imp$OOBerror\n#>      NRMSE        PFC \n#> 0.13631893 0.04477612\n\n#comparing actual data accuracy\niris.err <- mixError(iris.imp$ximp, iris.mis, iris)\niris.err\n#>     NRMSE       PFC \n#> 0.1501524 0.0625000"},{"path":"imputation-missing-data.html","id":"hmisc","chapter":"11 Imputation (Missing Data)","heading":"11.6.7 Hmisc","text":"impute() function imputes missing value using user defined statistical method (mean, max, mean). ’s default median.aregImpute() allows mean imputation using additive regression, bootstrapping, predictive mean matching.bootstrapping, different bootstrap resamples used multiple imputations. , flexible additive model (non parametric regression method) fitted samples taken replacements original data missing values (acts dependent variable) predicted using non-missing values (independent variable).uses predictive mean matching (default) impute missing values. Predictive mean matching works well continuous categorical (binary & multi-level) without need computing residuals maximum likelihood fit.NoteFor predicting categorical variables, Fisher’s optimum scoring method used.Hmisc automatically recognizes variables types uses bootstrap sample predictive mean matching impute missing values.missForest can outperform Hmisc observed variables sufficient information.Assumptionlinearity variables predicted.","code":"library(Hmisc)\n# impute with mean value\niris.mis$imputed_age <- with(iris.mis, impute(Sepal.Length, mean))\n\n# impute with random value\niris.mis$imputed_age2 <-\n  with(iris.mis, impute(Sepal.Length, 'random'))\n\n# could also use min, max, median to impute missing value\n\n# using argImpute\nimpute_arg <-\n  # argImpute() automatically identifies the variable type \n  # and treats them accordingly.\n  aregImpute(\n    ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width +\n      Species,\n    data = iris.mis,\n    n.impute = 5\n  ) \n#> Iteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \n\nimpute_arg # R-squares are for predicted missing values.\n#> \n#> Multiple Imputation using Bootstrap and PMM\n#> \n#> aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + \n#>     Petal.Width + Species, data = iris.mis, n.impute = 5)\n#> \n#> n: 150   p: 5    Imputations: 5      nk: 3 \n#> \n#> Number of NAs:\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>           11           11           13           24           16 \n#> \n#>              type d.f.\n#> Sepal.Length    s    2\n#> Sepal.Width     s    2\n#> Petal.Length    s    2\n#> Petal.Width     s    2\n#> Species         c    2\n#> \n#> Transformation of Target Variables Forced to be Linear\n#> \n#> R-squares for Predicting Non-Missing Values for Each Variable\n#> Using Last Imputations of Predictors\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>        0.907        0.660        0.978        0.963        0.993\n\n# check imputed variable Sepal.Length\nimpute_arg$imputed$Sepal.Length\n#>     [,1] [,2] [,3] [,4] [,5]\n#> 19   5.2  5.2  5.2  5.8  5.7\n#> 21   5.1  5.0  5.1  5.7  5.4\n#> 31   4.8  5.0  5.2  5.0  4.8\n#> 35   4.6  4.9  4.9  4.9  4.8\n#> 49   5.0  5.1  5.1  5.1  5.1\n#> 62   6.2  5.7  6.0  6.4  5.6\n#> 65   5.5  5.5  5.2  5.8  5.5\n#> 67   6.5  5.8  5.8  6.3  6.5\n#> 82   5.2  5.1  5.7  5.8  5.5\n#> 113  6.4  6.5  7.4  7.2  6.3\n#> 122  6.2  5.8  5.5  5.8  6.7"},{"path":"imputation-missing-data.html","id":"mi","chapter":"11 Imputation (Missing Data)","heading":"11.6.8 mi","text":"allows graphical diagnostics imputation models convergence imputation process.uses Bayesian version regression models handle issue separation.automatically detects irregularities data (e.g., high collinearity among variables).adds noise imputation process solve problem additive constraints.","code":"\nlibrary(mi)\n# default values of parameters\n# 1. rand.imp.method as “bootstrap”\n# 2. n.imp (number of multiple imputations) as 3\n# 3. n.iter ( number of iterations) as 30\nmi_data <- mi(iris.mis, seed = 335)\nsummary(mi_data)"},{"path":"data.html","id":"data","chapter":"12 Data","heading":"12 Data","text":"multiple ways categorize data. example,Qualitative vs. Quantitative:","code":""},{"path":"data.html","id":"cross-sectional","chapter":"12 Data","heading":"12.1 Cross-Sectional","text":"","code":""},{"path":"data.html","id":"time-series-1","chapter":"12 Data","heading":"12.2 Time Series","text":"\\[\ny_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + ... + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t\n\\]ExamplesStatic Model\n\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)\nStatic Model\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)Finite Distributed Lag model\n\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)\nLong Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)\nFinite Distributed Lag model\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)Long Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)Dynamic Model\n\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)\nDynamic Model\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)Finite Sample Properties Time Series:A1-A3: OLS unbiasedA1-A4: usual standard errors consistent Gauss-Markov Theorem holds (OLS BLUE)A1-A6, A6: Finite Sample Wald Test (t-test F-test) validA3 might hold time series settingSpurious Time Trend - solvableStrict vs Contemporaneous Exogeneity - solvableIn time series data, many processes:Autoregressive model order p: AR(p)Moving average model order q: MA(q)Autoregressive model order p moving average model order q: ARMA(p,q)Autoregressive conditional heteroskedasticity model order p: ARCH(p)Generalized Autoregressive conditional heteroskedasticity orders p q; GARCH(p.q)","code":""},{"path":"data.html","id":"deterministic-time-trend","chapter":"12 Data","heading":"12.2.1 Deterministic Time trend","text":"dependent independent variables trending timeSpurious Time Series Regression\\[\ny_t = \\alpha_0 + t\\alpha_1 + v_t\n\\]x takes form\\[\nx_t = \\lambda_0 + t\\lambda_1 + u_t\n\\]\\(\\alpha_1 \\neq 0\\) \\(\\lambda_1 \\neq 0\\)\\(v_t\\) \\(u_t\\) independentthere relationship \\(y_t\\) \\(x_t\\)estimate regression,\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]true \\(\\beta_1=0\\)Inconsistent: \\(plim(\\hat{\\beta}_1)=\\frac{\\alpha_1}{\\lambda_1}\\)Invalid Inference: \\(|t| \\^d \\infty\\) \\(H_0: \\beta_1=0\\), always reject null \\(n \\\\infty\\)Uninformative \\(R^2\\): \\(plim(R^2) = 1\\) able perfectly predict \\(n \\\\infty\\)can rewrite equation \\[\n\\begin{aligned}\ny_t &=\\beta_0 + \\beta_1x_t+\\epsilon_t \\\\\n\\epsilon_t &= \\alpha_1t + v_t\n\\end{aligned}\n\\]\\(\\beta_0 = \\alpha_0\\) \\(\\beta_1=0\\). Since \\(x_t\\) deterministic function time, \\(\\epsilon_t\\) correlated \\(x_t\\) usual omitted variable bias.\nEven \\(y_t\\) \\(x_t\\) related (\\(\\beta_1 \\neq 0\\)) trending time, still get spurious results simple regression \\(y_t\\) \\(x_t\\)Solutions Spurious TrendInclude time trend \\(t\\) additional control\nconsistent parameter estimates valid inference\nInclude time trend \\(t\\) additional controlconsistent parameter estimates valid inferenceDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))\nDetrending partialing Frisch-Waugh-Lovell Theorem\nallow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\n\nDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))Detrending partialing Frisch-Waugh-Lovell Theorem\nallow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\nDetrending partialing Frisch-Waugh-Lovell TheoremCould allow non-linear time trends including \\(t\\) \\(t^2\\), \\(\\exp(t)\\)Allow seasonality including indicators relevant “seasons” (quarters, months, weeks).A3 hold :Feedback Effect\n\\(\\epsilon_t\\) influences next period’s independent variables\nFeedback Effect\\(\\epsilon_t\\) influences next period’s independent variablesDynamic Specification\ninclude last time period outcome explanatory variable\nDynamic Specificationinclude last time period outcome explanatory variableDynamically Complete\nfinite distrusted lag model, number lags needs absolutely correct.\nDynamically CompleteFor finite distrusted lag model, number lags needs absolutely correct.","code":""},{"path":"data.html","id":"feedback-effect","chapter":"12 Data","heading":"12.2.2 Feedback Effect","text":"\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]A3\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, \\(y_t\\) likely influence \\(x_{t+1},..,x_T\\)A3 violated require error uncorrelated time observation independent regressors (strict exogeneity)","code":""},{"path":"data.html","id":"dynamic-specification","chapter":"12 Data","heading":"12.2.3 Dynamic Specification","text":"\\[\ny_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T)\n\\]equal 0, \\(y_t\\) \\(\\epsilon_t\\) inherently correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Dynamic Specification allowed A3","code":""},{"path":"data.html","id":"dynamically-complete","chapter":"12 Data","heading":"12.2.4 Dynamically Complete","text":"\\[\ny_t = \\beta_0 + x_t\\delta_0 + x_{t-1}\\delta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, include enough lags, \\(x_{t-2}\\) \\(\\epsilon_t\\) correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Can corrected including lags (stop? )Without A3OLS biasedGauss-Markov TheoremFinite Sample Properties invalidthen, canFocus Large Sample PropertiesCan use A3a instead A3A3a time series become\\[\nA3a: E(\\mathbf{x}_t'\\epsilon_t)= 0\n\\]regressors time period need independent error time period (Contemporaneous Exogeneity)\\(\\epsilon_t\\) can correlated \\(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\\)can dynamic specification \\(y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\\)Deriving Large Sample Properties Time SeriesAssumptions A1, A2, A3aAssumptions A1, A2, A3a[Weak Law] Central Limit Theorem depend A5\n\\(x_t\\) \\(\\epsilon_t\\) dependent t\nwithout [Weak Law] Central Limit Theorem depend A5, Large Sample Properties OLS\nInstead A5, consider A5a\n[Weak Law] Central Limit Theorem depend A5\\(x_t\\) \\(\\epsilon_t\\) dependent twithout [Weak Law] Central Limit Theorem depend A5, Large Sample Properties OLSInstead A5, consider A5aDerivation Asymptotic Variance depends A4\ntime series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)\nDerivation Asymptotic Variance depends A4time series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)A1, A2, A3a, A5a, OLS estimator consistent, asymptotically normal","code":""},{"path":"data.html","id":"highly-persistent-data","chapter":"12 Data","heading":"12.2.5 Highly Persistent Data","text":"\\(y_t, \\mathbf{x}_t\\) weakly dependent stationary process\\(y_t\\) \\(y_{t-h}\\) almost independent large h\\(y_t\\) \\(y_{t-h}\\) almost independent large hA5a hold OLS consistent limiting distribution.A5a hold OLS consistent limiting distribution.Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\)Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\)Solution First difference stationary process\\[\ny_t - y_{t-1} = u_t\n\\]\\(u_t\\) weakly dependent process (also called integrated order 0) \\(y_t\\) said difference-stationary process (integrated order 1)regression, \\(\\{y_t, \\mathbf{x}_t \\}\\) random walks (integrated order 1), can consistently estimate first difference equation\\[\n\\begin{aligned}\ny_t - y_{t-1} &= (\\mathbf{x}_t - \\mathbf{x}_{t-1}\\beta + \\epsilon_t - \\epsilon_{t-1}) \\\\\n\\Delta y_t &= \\Delta \\mathbf{x}\\beta + \\Delta u_t\n\\end{aligned}\n\\]Unit Root Test\\[\ny_t = \\alpha + \\alpha y_{t-1} + u_t\n\\]tests \\(\\rho=1\\) (integrated order 1)null \\(H_0: \\rho = 1\\), OLS consistent asymptotically normal.alternative \\(H_a: \\rho < 1\\), OLS consistent asymptotically normal.usual t-test valid, need use transformed equation produce valid test.Dickey-Fuller Test \\[\n\\Delta y_t= \\alpha + \\theta y_{t-1} + v_t\n\\] \\(\\theta = \\rho -1\\)\\(H_0: \\theta = 0\\) \\(H_a: \\theta < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) .Dickey Fuller derived non-normal asymptotic distribution. reject null \\(y_t\\) random walk.Concerns standard Dickey Fuller Test\n1. considers fairly simplistic dynamic relationship\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta_{t-1} + ..+ \\gamma_p \\Delta_{t-p} +v_t\n\\]one additional lag, null \\(\\Delta_{y_t}\\) AR(1) process alternative \\(y_t\\) AR(2) process.Solution: include lags \\(\\Delta_{y_t}\\) controls.allow time trend \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t\n\\]allows \\(y_t\\) quadratic relationship \\(t\\)Solution: include time trend (changes critical values).Adjusted Dickey-Fuller Test \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + v_t\n\\] \\(\\theta = 1 - \\rho\\)\\(H_0: \\theta_1 = 0\\) \\(H_a: \\theta_1 < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) notCritical values different time trend, reject null \\(y_t\\) random walk.","code":""},{"path":"data.html","id":"newey-west-standard-errors","chapter":"12 Data","heading":"12.2.5.0.1 Newey West Standard Errors","text":"A4 hold, can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent)\\[\n\\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x'_tx_t} + \\sum_{h=1}^{g}(1-\\frac{h}{g+1})T^{-1}\\sum_{t=h+1}^{T} e_t e_{t-h}(\\mathbf{x_t'x_{t-h}+ x_{t-h}'x_t})\n\\]estimates covariances distance g partestimates covariances distance g partdownweights insure \\(\\hat{B}\\) PSDdownweights insure \\(\\hat{B}\\) PSDHow choose g:\nyearly data: \\(g = 1\\) 2 likely account correlation\nquarterly monthly data: g larger ($g = 4$ 8 quarterly \\(g = 12\\) 14 monthly)\ncan also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)\nchoose g:yearly data: \\(g = 1\\) 2 likely account correlationFor quarterly monthly data: g larger ($g = 4$ 8 quarterly \\(g = 12\\) 14 monthly)can also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)Testing Serial CorrelationRun OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Reject null serial correlation coefficient significant 5% level.\nTest using heteroskedastic robust standard errors\ncan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)\nReject null serial correlation coefficient significant 5% level.Test using heteroskedastic robust standard errorscan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)","code":""},{"path":"data.html","id":"repeated-cross-sections","chapter":"12 Data","heading":"12.3 Repeated Cross Sections","text":"time point (day, month, year, etc.), set data sampled. set data can different among different time points.example, can sample different groups students time survey.Allowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i\n\\]Dummy variables one time periodallows different intercept time periodallows outcome change average time periodAllowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows effects change based time period (structural break)Interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"pooled-cross-section","chapter":"12 Data","heading":"12.3.1 Pooled Cross Section","text":"\\[\ny_i=\\mathbf{x_i\\beta +x_i \\times y1\\gamma_1 + ...+ x_i \\times yT\\gamma_T + \\delta_1y_1+...+ \\delta_Ty_T + \\epsilon_i}\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows different slopes time periodallows effect change based time period (structural break)\ninteracting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.\nallows effect change based time period (structural break)interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"panel-data","chapter":"12 Data","heading":"12.4 Panel Data","text":"Detail notes R can found hereFollows individual T time periods.Panel data structure like n samples time series dataCharacteristicsInformation across individuals time (cross-sectional time-series)Information across individuals time (cross-sectional time-series)N individuals T time periodsN individuals T time periodsData can either\nBalanced: individuals observed time periods\nUnbalanced: individuals observed time periods.\nData can eitherBalanced: individuals observed time periodsUnbalanced: individuals observed time periods.Assume correlation (clustering) time given individual, independence individuals.Assume correlation (clustering) time given individual, independence individuals.TypesShort panel: many individuals time periods.Long panel: many time periods individualsBoth: many time periods many individualsTime Trends Time EffectsNonlinearSeasonalityDiscontinuous shocksRegressorsTime-invariant regressors \\(x_{}=x_i\\) t (e.g., gender, race, education) zero within variationIndividual-invariant regressors \\(x_{}=x_{t}\\) (e.g., time trend, economy trends) zero variationVariation dependent variable regressorsOverall variation: variation time individuals.variation: variation individualsWithin variation: variation within individuals (time).Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\)Since n observation time period t, can control time effect separately including time dummies (time effects)\\[\ny_{}=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + \\epsilon_{}\n\\]Note: use many time dummies time series data time series data, n 1. Hence, variation, sometimes enough data compared variables estimate coefficients.Unobserved Effects Model Similar group clustering, assume random effect captures differences across individuals constant time.\\[\ny_it=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + c_i + u_{}\n\\]\\(c_i + u_{} = \\epsilon_{}\\)\\(c_i\\) unobserved individual heterogeneity (effect)\\(u_{}\\) idiosyncratic shock\\(\\epsilon_{}\\) unobserved error term.","code":""},{"path":"data.html","id":"pooled-ols-estimator","chapter":"12 Data","heading":"12.4.1 Pooled OLS Estimator","text":"\\(c_i\\) uncorrelated \\(x_{}\\)\\[\nE(\\mathbf{x_{}'}(c_i+u_{})) = 0\n\\]A3a still holds. Pooled OLS consistent.A4 hold, OLS still consistent, efficient, need cluster robust SE.Sufficient A3a hold, needExogeneity \\(u_{}\\) A3a (contemporaneous exogeneity): \\(E(\\mathbf{x_{}'}u_{})=0\\) time varying errorRandom Effect Assumption (time constant error): \\(E(\\mathbf{x_{}'}c_{})=0\\)Pooled OLS give consistent coefficient estimates A1, A2, A3a (\\(u_{}\\) RE assumption), A5 (randomly sampling across ).","code":""},{"path":"data.html","id":"individual-specific-effects-model","chapter":"12 Data","heading":"12.4.2 Individual-specific effects model","text":"believe unobserved heterogeneity across individual (e.g., unobserved ability individual affects \\(y\\)), individual-specific effects correlated regressors, Fixed Effects Estimator. correlated Random Effects Estimator.","code":""},{"path":"data.html","id":"random-effects-estimator","chapter":"12 Data","heading":"12.4.2.1 Random Effects Estimator","text":"Random Effects estimator Feasible GLS estimator assumes \\(u_{}\\) serially uncorrelated homoskedasticUnder A1, A2, A3a (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.\nA4 holds \\(u_{}\\), RE efficient estimator\nA4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.\nA1, A2, A3a (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.A4 holds \\(u_{}\\), RE efficient estimatorIf A4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.","code":""},{"path":"data.html","id":"fixed-effects-estimator","chapter":"12 Data","heading":"12.4.2.2 Fixed Effects Estimator","text":"also known Within Estimator uses within variation (time)RE assumption hold (\\(E(\\mathbf{x_{}'}c_i) \\neq 0\\)), A3a hold (\\(E(\\mathbf{x_{}'}\\epsilon_i) \\neq 0\\)).Hence, OLS RE inconsistent/biased (omitted variable bias)However, FE can fix bias due time-invariant factors (observables unobservables) correlated treatment (time-variant factors correlated treatment).traditional FE technique flawed lagged dependent variables included model. (Nickell 1981) (Narayanan Nair 2013)measurement error independent, FE exacerbate errors---variables bias.","code":""},{"path":"data.html","id":"demean-approach","chapter":"12 Data","heading":"12.4.2.2.1 Demean Approach","text":"deal violation \\(c_i\\), \\[\ny_{}= \\mathbf{x_{} \\beta} + c_i + u_{}\n\\]\\[\n\\bar{y_i}=\\bar{\\mathbf{x_i}} \\beta + c_i + \\bar{u_i}\n\\]second equation time averaged equationusing within transformation, \\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})}\\beta + u_{} - \\bar{u_i}\n\\]\\(c_i\\) time constant.Fixed Effects estimator uses POLS transformed equation\\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})} \\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + u_{} - \\bar{u_i}\n\\]need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.Variables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.\ninterested effects time-invariant variables, consider OLS estimator\nVariables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.interested effects time-invariant variables, consider OLS estimatorIt’s recommended still use cluster robust standard errors.’s recommended still use cluster robust standard errors.","code":""},{"path":"data.html","id":"dummy-approach","chapter":"12 Data","heading":"12.4.2.2.2 Dummy Approach","text":"Equivalent within transformation (.e., mathematically equivalent Demean Approach), can fixed effect estimator dummy regression\\[\ny_{} = x_{}\\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + c_1\\gamma_1 + ... + c_{n-1}\\gamma_{n-1} + u_{}\n\\]\\[\nc_i\n=\n\\begin{cases}\n1 &\\text{observation } \\\\\n0 &\\text{otherwise} \\\\\n\\end{cases}\n\\]standard error incorrectly calculated.FE within transformation controlling difference across individual allowed correlated observables.","code":""},{"path":"data.html","id":"first-difference-approach","chapter":"12 Data","heading":"12.4.2.2.3 First-difference Approach","text":"Economists typically use approach\\[\ny_{} - y_{(t-1)} = (\\mathbf{x}_{} - \\mathbf{x}_{(t-1)}) \\beta +  + (u_{} - u_{(t-1)})\n\\]","code":""},{"path":"data.html","id":"fixed-effects-summary","chapter":"12 Data","heading":"12.4.2.2.4 Fixed Effects Summary","text":"three approaches almost equivalent.\nDemean Approach mathematically equivalent Dummy Approach\n1 period, 3 .\nthree approaches almost equivalent.Demean Approach mathematically equivalent Dummy ApproachDemean Approach mathematically equivalent Dummy ApproachIf 1 period, 3 .1 period, 3 .Since fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.\nHence, small number changes standard error \\(\\beta\\) explode\nSince fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.Hence, small number changes standard error \\(\\beta\\) explodeStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers.\nTreatment effect typically non-directional.\ncan give parameter direction needed.\nStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers.Treatment effect typically non-directional.Treatment effect typically non-directional.can give parameter direction needed.can give parameter direction needed.Issues:\nfundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.\nfixed effects focus bias reduction, might larger variance (typically, fixed effects less df)\nIssues:fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.can estimate FE different units (just individuals).can estimate FE different units (just individuals).FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)Recall\\[\nY_{} = \\beta_0 + X_{}\\beta_1 + \\alpha_i + u_{}\n\\]\\(\\epsilon_{} = \\alpha_i + u_{}\\)\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{SSR_{OLS}}{NT - K}\n\\]\\[\n\\hat{\\sigma}^2_u = \\frac{SSR_{FE}}{NT - (N+K)} = \\frac{SSR_{FE}}{N(T-1)-K}\n\\]’s ambiguous whether variance error changes SSR can increase denominator decreases.FE can unbiased, consistent (.e., converging true effect)","code":""},{"path":"data.html","id":"fe-examples","chapter":"12 Data","heading":"12.4.2.2.5 FE Examples","text":"","code":""},{"path":"data.html","id":"blau1999","chapter":"12 Data","heading":"12.4.2.2.6 Blau (1999)","text":"Intergenerational mobilityIntergenerational mobilityIf transfer resources low income family, can generate upward mobility (increase ability)?transfer resources low income family, can generate upward mobility (increase ability)?Mechanisms intergenerational mobilityGenetic (policy can’t affect) (.e., ability endowment)Environmental indirectEnvironmental direct\\[\n\\frac{\\% \\Delta \\text{Human capital}}{\\% \\Delta \\text{income}}\n\\]Financial transferIncome measures:Total household incomeWage incomeNon-wage incomeAnnual versus permanent incomeCore control variables:Bad controls jointly determined dependent variableControl mother = choice motherUncontrolled mothers:mother racemother racelocation birthlocation birtheducation parentseducation parentshousehold structure age 14household structure age 14\\[\nY_{ijt} = X_{jt} \\beta_i + I_{jt} \\alpha_i + \\epsilon_{ijt}\n\\]\\(\\) = test\\(\\) = test\\(j\\) = individual (child)\\(j\\) = individual (child)\\(t\\) = time\\(t\\) = timeGrandmother’s modelSince child nested within mother mother nested within grandmother, fixed effect child included fixed effect mother, included fixed-effect grandmother\\[\nY_{ijgmt} = X_{} \\beta_{} + I_{jt} \\alpha_i + \\gamma_g + u_{ijgmt}\n\\]\\(\\) = test, \\(j\\) = kid, \\(m\\) = mother, \\(g\\) = grandmother\\(\\) = test, \\(j\\) = kid, \\(m\\) = mother, \\(g\\) = grandmotherwhere \\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)\\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)Grandma fixed-effectPros:control genetics + fixed characteristics mother raisedcontrol genetics + fixed characteristics mother raisedcan estimate effect parameter incomecan estimate effect parameter incomeCon:Might sufficient controlCommon cluster fixed-effect level (common correlated component)Fixed effect exaggerates attenuation biasError rate survey can help fix (plug number , uncertainty associated number).","code":""},{"path":"data.html","id":"babcock2010","chapter":"12 Data","heading":"12.4.2.2.7 Babcock (2010)","text":"\\[\nT_{ijct} = \\alpha_0 + S_{jct} \\alpha_1 + X_{ijct} \\alpha_2 + u_{ijct}\n\\]\\(S_{jct}\\) average class expectation\\(S_{jct}\\) average class expectation\\(X_{ijct}\\alpha_2\\) individual characteristics\\(X_{ijct}\\alpha_2\\) individual characteristics\\(\\) student\\(\\) student\\(j\\) instructor\\(j\\) instructor\\(c\\) course\\(c\\) course\\(t\\) time\\(t\\) time\\[\nT_{ijct} = \\beta_0+ S_{jct} \\beta_1+ X_{ijct} \\beta_2 +\\mu_{jc} + \\epsilon_{ijct}\n\\]\\(\\mu_{jc}\\) instructor course fixed effect (unique id), different \\((\\theta_j + \\delta_c)\\)Decrease course shopping conditioned available information (\\(\\mu_{jc}\\)) (class grade instructor’s info).Grade expectation change even though class materials stay sameIdentification strategy isUnder (fixed) time-varying factor bias coefficient (simultaneity)\\[\nY_{ijt} = X_{} \\beta_1 + \\text{Teacher Experience}_{jt} \\beta_2 + \\text{Teacher education}_{jt} \\beta_3 + \\text{Teacher score}_{}\\beta_4 + \\dots + \\epsilon_{ijt}\n\\]Drop teacher characteristics, include teacher dummy effect\\[\nY_{ijt} = X_{} \\alpha + \\Gamma_{} \\theta_j + u_{ijt}\n\\]\\(\\alpha\\) within teacher (conditional teacher fixed effect) \\(j = 1 \\(J-1)\\)Nuisance sense don’t interpretation \\(\\alpha\\)least can say \\(\\theta_j\\) teacher effect conditional student test score.\\[\nY_{ijt} = X_{} \\gamma + \\epsilon_{ijt}\n\\]\\(\\gamma\\) within (unconditional) \\(e_{ijt}\\) prediction error\\[\ne_{ijt} = T_{} \\delta_j + \\tilde{e}_{ijt}\n\\]\\(\\delta_j\\) mean group\\[\nY_{ijkt} = Y_{ijkt-1} + X_{} \\beta + T_{} \\tau_j + (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(Y_{ijkt-1}\\) = lag control\\(Y_{ijkt-1}\\) = lag control\\(\\tau_j\\) = teacher fixed time\\(\\tau_j\\) = teacher fixed time\\(W_i\\) student fixed effect\\(W_i\\) student fixed effect\\(P_k\\) school fixed effect\\(P_k\\) school fixed effect\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)worry selection class schoolBias \\(\\tau\\) (1 teacher) \\[\n\\frac{1}{N_j} \\sum_{= 1}^N (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(N_j\\) = number student class teacher \\(j\\)can \\(P_k + \\frac{1}{N_j} \\sum_{= 1}^{N_j} (W_i + \\epsilon_{ijkt})\\)Shocks small class can bias \\(\\tau\\)\\[\n\\frac{1}{N_j} \\sum_{= 1}^{N_j} \\epsilon_{ijkt} \\neq 0\n\\]inflate teacher fixed effectEven create random teacher fixed effect put model, still contains bias mentioned can still \\(\\tau\\) (know way affect - whether positive negative).teachers switch schools, can estimate teacher school fixed effect (mobility web thin vs. thick)Mobility web refers web switchers (.e., one status another).\\[\nY_{ijkt} = Y_{ijk(t-1)} \\alpha + X_{}\\beta + T_{} \\tau + P_k + \\epsilon_{ijkt}\n\\]demean (fixed-effect), \\(\\tau\\) (teacher fixed effect) go awayIf want examine teacher fixed effect, include teacher fixed effectControl school, article argues selection biasFor \\(\\frac{1}{N_j} \\sum_{=1}^{N_j} \\epsilon_{ijkt}\\) (teacher-level average residuals), \\(var(\\tau)\\) change \\(N_j\\) (Figure 2 paper). words, quality teachers function number studentsIf \\(var(\\tau) =0\\) means teacher quality matterSpin-Measurement Error: Sampling error estimation error\\[\n\\hat{\\tau}_j = \\tau_j + \\lambda_j\n\\]\\[\nvar(\\hat{\\tau}) = var(\\tau + \\lambda)\n\\]Assume \\(cov(\\tau_j, \\lambda_j)=0\\) (reasonable) words, randomness getting children correlation teacher quality.Hence,\\[\n\\begin{aligned}\nvar(\\hat{\\tau}) &= var(\\tau) + var(\\lambda) \\\\\nvar(\\tau) &= var(\\hat{\\tau}) - var(\\lambda) \\\\\n\\end{aligned}\n\\]\\(var(\\hat{\\tau})\\) need estimate \\(var(\\lambda)\\)\\[\nvar(\\lambda) = \\frac{1}{J} \\sum_{j=1}^J \\hat{\\sigma}^2_j\n\\] \\(\\hat{\\sigma}^2_j\\) squared standard error teacher \\(j\\) (function \\(n\\))Hence,\\[\n\\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{reliability} = \\text{true variance signal}\n\\] also known much noise \\(\\hat{\\tau}\\) \\[\n1 - \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{noise}\n\\]Even cases true relationship \\(\\tau\\) function \\(N_j\\), recovery method \\(\\lambda\\) still affectedTo examine assumption\\[\n\\hat{\\tau}_j = \\beta_0 + X_j \\beta_1 + \\epsilon_j\n\\]Regressing teacher fixed-effect teacher characteristics give us \\(R^2\\) close 0, teacher characteristics predict sampling error (\\(\\hat{\\tau}\\) contain sampling error)","code":""},{"path":"data.html","id":"tests-for-assumptions","chapter":"12 Data","heading":"12.4.3 Tests for Assumptions","text":"typically don’t test heteroskedasticity use robust covariance matrix estimation anyway.Dataset","code":"\nlibrary(\"plm\")\ndata(\"EmplUK\", package=\"plm\")\ndata(\"Produc\", package=\"plm\")\ndata(\"Grunfeld\", package=\"plm\")\ndata(\"Wages\", package=\"plm\")"},{"path":"data.html","id":"poolability","chapter":"12 Data","heading":"12.4.3.1 Poolability","text":"also known F test stability (Chow test) coefficients\\(H_0\\): individuals coefficients (.e., equal coefficients individuals).\\(H_a\\) Different individuals different coefficients.Notes:within (.e., fixed) model, different intercepts individual assumedUnder random model, intercept assumedHence, reject null hypothesis coefficients stable. , use random model.","code":"\nlibrary(plm)\nplm::pooltest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  F statistic\n#> \n#> data:  inv ~ value + capital\n#> F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10\n#> alternative hypothesis: unstability"},{"path":"data.html","id":"individual-and-time-effects","chapter":"12 Data","heading":"12.4.3.2 Individual and time effects","text":"use Lagrange multiplier test test presence individual time (.e., individual time).Types:honda: (Honda 1985) Defaultbp: (Breusch Pagan 1980) unbalanced panelskw: (M. L. King Wu 1997) unbalanced panels, two-way effectsghm: (Gourieroux, Holly, Monfort 1982): two-way effects","code":"\npFtest(inv~value+capital, data=Grunfeld, effect=\"twoways\")\n#> \n#>  F test for twoways effects\n#> \n#> data:  inv ~ value + capital\n#> F = 17.403, df1 = 28, df2 = 169, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"individual\")\n#> \n#>  F test for individual effects\n#> \n#> data:  inv ~ value + capital\n#> F = 49.177, df1 = 9, df2 = 188, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"time\")\n#> \n#>  F test for time effects\n#> \n#> data:  inv ~ value + capital\n#> F = 0.23451, df1 = 19, df2 = 178, p-value = 0.9997\n#> alternative hypothesis: significant effects"},{"path":"data.html","id":"cross-sectional-dependencecontemporaneous-correlation","chapter":"12 Data","heading":"12.4.3.3 Cross-sectional dependence/contemporaneous correlation","text":"Null hypothesis: residuals across entities correlated.","code":""},{"path":"data.html","id":"global-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.1 Global cross-sectional dependence","text":"","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  inv ~ value + capital\n#> z = 4.6612, p-value = 3.144e-06\n#> alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"local-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.2 Local cross-sectional dependence","text":"use command, supply matrix w argument.","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")\n#> \n#>  Pesaran CD test for cross-sectional dependence in panels\n#> \n#> data:  inv ~ value + capital\n#> z = 4.6612, p-value = 3.144e-06\n#> alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"serial-correlation-1","chapter":"12 Data","heading":"12.4.3.4 Serial Correlation","text":"Null hypothesis: serial correlationNull hypothesis: serial correlationusually seen macro panels long time series (large N T), seen micro panels (small T large N)usually seen macro panels long time series (large N T), seen micro panels (small T large N)Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Can \nmarginal test: 1 two dependence (can biased towards rejection)\njoint test: dependencies (don’t know one causing problem)\nconditional test: assume correctly specify one dependence structure, test whether departure present.\nCan bemarginal test: 1 two dependence (can biased towards rejection)marginal test: 1 two dependence (can biased towards rejection)joint test: dependencies (don’t know one causing problem)joint test: dependencies (don’t know one causing problem)conditional test: assume correctly specify one dependence structure, test whether departure present.conditional test: assume correctly specify one dependence structure, test whether departure present.","code":""},{"path":"data.html","id":"unobserved-effect-test","chapter":"12 Data","heading":"12.4.3.4.1 Unobserved effect test","text":"semi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.\nnull, covariance matrix residuals = diagonal (-diagonal = 0)\nsemi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.null, covariance matrix residuals = diagonal (-diagonal = 0)robust unobserved effects constant within every group, kind serial correlation.robust unobserved effects constant within every group, kind serial correlation., reject null hypothesis unobserved effects residuals. Hence, exclude using pooled OLS.","code":"\npwtest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc)\n#> \n#>  Wooldridge's test for unobserved individual effects\n#> \n#> data:  formula\n#> z = 3.9383, p-value = 8.207e-05\n#> alternative hypothesis: unobserved effect"},{"path":"data.html","id":"locally-robust-tests-for-random-effects-and-serial-correlation","chapter":"12 Data","heading":"12.4.3.4.2 Locally robust tests for random effects and serial correlation","text":"joint LM test random effects serial correlation assuming normality homoskedasticity idiosyncratic errors [Baltagi Li (1991)](Baltagi Li 1995), reject null hypothesis presence serial correlation, random effects. still know whether serial correlation, random effects bothTo know departure null assumption, can use Bera, Sosa-Escudero, Yoon (2001)’s test first-order serial correlation random effects (normality homoskedasticity assumption error).BSY serial correlationBSY random effectsSince BSY locally robust, “know” serial correlation, test based LM test superior:hand, know random effects, test serial correlation, use (Breusch 1978)-(Godfrey 1978)’s testIf “know” random effects, use (Baltagi Li 1995)’s. test serial correlation AR(1) MA(1) processes.\\(H_0\\): Uncorrelated errors.Note:one-sided power positive serial correlation.applicable balanced panels.General serial correlation testsapplicable random effects model, OLS, FE (large T, also known long panel).can also test higher-order serial correlationin case short panels (small T large n), can use","code":"\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n         data = Produc,\n         test = \"j\")\n#> \n#>  Baltagi and Li AR-RE joint test\n#> \n#> data:  formula\n#> chisq = 4187.6, df = 2, p-value < 2.2e-16\n#> alternative hypothesis: AR(1) errors or random effects\npbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n         data = Produc)\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test\n#> \n#> data:  formula\n#> chisq = 52.636, df = 1, p-value = 4.015e-13\n#> alternative hypothesis: AR(1) errors sub random effects\npbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, \n         data=Produc, \n         test=\"re\")\n#> \n#>  Bera, Sosa-Escudero and Yoon locally robust test (one-sided)\n#> \n#> data:  formula\n#> z = 57.914, p-value < 2.2e-16\n#> alternative hypothesis: random effects sub AR(1) errors\nplmtest(inv ~ value + capital, data = Grunfeld, \n        type = \"honda\")\n#> \n#>  Lagrange Multiplier Test - (Honda)\n#> \n#> data:  inv ~ value + capital\n#> normal = 28.252, p-value < 2.2e-16\n#> alternative hypothesis: significant effects\nlmtest::bgtest()\npbltest(\n    log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n    data = Produc,\n    alternative = \"onesided\"\n)\n#> \n#>  Baltagi and Li one-sided LM test\n#> \n#> data:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\n#> z = 21.69, p-value < 2.2e-16\n#> alternative hypothesis: AR(1)/MA(1) errors in RE panel model\nplm::pbgtest(plm::plm(inv ~ value + capital,\n                      data = Grunfeld,\n                      model = \"within\"),\n             order = 2)\n#> \n#>  Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n#> \n#> data:  inv ~ value + capital\n#> chisq = 42.587, df = 2, p-value = 5.655e-10\n#> alternative hypothesis: serial correlation in idiosyncratic errors\npwartest(log(emp) ~ log(wage) + log(capital), data=EmplUK)\n#> \n#>  Wooldridge's test for serial correlation in FE panels\n#> \n#> data:  plm.model\n#> F = 312.3, df1 = 1, df2 = 889, p-value < 2.2e-16\n#> alternative hypothesis: serial correlation"},{"path":"data.html","id":"unit-rootsstationarity","chapter":"12 Data","heading":"12.4.3.5 Unit roots/stationarity","text":"Dickey-Fuller test stochastic trends.Null hypothesis: series non-stationary (unit root)want test less critical value (p<.5) evidence unit roots.","code":""},{"path":"data.html","id":"heteroskedasticity-2","chapter":"12 Data","heading":"12.4.3.6 Heteroskedasticity","text":"Breusch-Pagan testBreusch-Pagan testNull hypothesis: data homoskedasticNull hypothesis: data homoskedasticIf evidence heteroskedasticity, robust covariance matrix advised.evidence heteroskedasticity, robust covariance matrix advised.control heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)\n“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.\n“white2” - “white1” restricted common variance within groups. Recommended random effects.\n“arellano” - heteroskedasticity serial correlation. Recommended fixed effects\ncontrol heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.“white2” - “white1” restricted common variance within groups. Recommended random effects.“arellano” - heteroskedasticity serial correlation. Recommended fixed effects","code":""},{"path":"data.html","id":"model-selection","chapter":"12 Data","heading":"12.4.4 Model Selection","text":"","code":""},{"path":"data.html","id":"pols-vs.-re","chapter":"12 Data","heading":"12.4.4.1 POLS vs. RE","text":"continuum RE (used FGLS assumption ) POLS check back section FGLSBreusch-Pagan LM testTest random effect model based OLS residualNull hypothesis: variances across entities zero. another word, panel effect.test significant, RE preferable compared POLS","code":""},{"path":"data.html","id":"fe-vs.-re","chapter":"12 Data","heading":"12.4.4.2 FE vs. RE","text":"RE require strict exogeneity consistency (feedback effect residual covariates)Hausman TestFor Hausman test run, need assume thatstrict exogeneity holdA4 hold \\(u_{}\\),Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})'(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) \\(n(X)\\) number parameters time-varying regressors.low p-value means reject null hypothesis prefer FEA high p-value means reject null hypothesis consider RE estimator.Violation EstimatorBasic EstimatorInstrumental variable EstimatorVariable Coefficients estimatorGeneralized Method Moments estimatorGeneral FGLS estimatorMeans groups estimatorCCEMGEstimator limited dependent variables","code":"\ngw <- plm(inv ~ value + capital, data = Grunfeld, model = \"within\")\ngr <- plm(inv ~ value + capital, data = Grunfeld, model = \"random\")\nphtest(gw, gr)\n#> \n#>  Hausman Test\n#> \n#> data:  inv ~ value + capital\n#> chisq = 2.3304, df = 2, p-value = 0.3119\n#> alternative hypothesis: one model is inconsistent"},{"path":"data.html","id":"summary-2","chapter":"12 Data","heading":"12.4.5 Summary","text":"three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,POLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 hold, use cluster robust SE POLS efficient\nPOLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 hold, use cluster robust SE POLS efficientRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 (\\(u_{}\\)) holds usual SE valid RE efficient\nA4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)\nRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 (\\(u_{}\\)) holds usual SE valid RE efficientIf A4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)FE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)\nestimate effects time constant variables\nA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE needed\nFE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)estimate effects time constant variablesA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE neededNote: A5 individual (time dimension) implies A5a entire data set.Based table provided Ani Katchova","code":""},{"path":"data.html","id":"application-8","chapter":"12 Data","heading":"12.4.6 Application","text":"","code":""},{"path":"data.html","id":"plm-package","chapter":"12 Data","heading":"12.4.6.1 plm package","text":"Recommended application plm can found Yves CroissantAdvancedOther methods estimate random model:\"swar\": default (Swamy Arora 1972)\"walhus\": (Wallace Hussain 1969)\"amemiya\": (Amemiya 1971)\"nerlove\"” (Nerlove 1971)effects:Individual effects: defaultTime effects: \"time\"Individual time effects: \"twoways\"Note: random two-ways effect model random.method = \"nerlove\"call estimation variance error componentsCheck unbalancedness. Closer 1 indicates balanced data (Ahrens Pincus 1981)Instrumental variable\"bvk\": default (Balestra Varadharajan-Krishnakumar 1987)\"baltagi\": (Baltagi 1981)\"\" (Amemiya MaCurdy 1986)\"bms\": (Breusch, Mizon, Schmidt 1989)","code":"\n#install.packages(\"plm\")\nlibrary(\"plm\")\n\nlibrary(foreign)\nPanel <- read.dta(\"http://dss.princeton.edu/training/Panel101.dta\")\n\nattach(Panel)\nY <- cbind(y)\nX <- cbind(x1, x2, x3)\n\n# Set data as panel data\npdata <- pdata.frame(Panel, index = c(\"country\", \"year\"))\n\n# Pooled OLS estimator\npooling <- plm(Y ~ X, data = pdata, model = \"pooling\")\nsummary(pooling)\n\n# Between estimator\nbetween <- plm(Y ~ X, data = pdata, model = \"between\")\nsummary(between)\n\n# First differences estimator\nfirstdiff <- plm(Y ~ X, data = pdata, model = \"fd\")\nsummary(firstdiff)\n\n# Fixed effects or within estimator\nfixed <- plm(Y ~ X, data = pdata, model = \"within\")\nsummary(fixed)\n\n# Random effects estimator\nrandom <- plm(Y ~ X, data = pdata, model = \"random\")\nsummary(random)\n\n# LM test for random effects versus OLS\n# Accept Null, then OLS, Reject Null then RE\nplmtest(pooling, effect = \"individual\", type = c(\"bp\")) \n# other type: \"honda\", \"kw\",\" \"ghm\"; other effect : \"time\" \"twoways\"\n\n\n# B-P/LM and Pesaran CD (cross-sectional dependence) test\n# Breusch and Pagan's original LM statistic\npcdtest(fixed, test = c(\"lm\")) \n# Pesaran's CD statistic\npcdtest(fixed, test = c(\"cd\")) \n\n# Serial Correlation\npbgtest(fixed)\n\n# stationary\nlibrary(\"tseries\")\nadf.test(pdata$y, k = 2)\n\n# LM test for fixed effects versus OLS\npFtest(fixed, pooling)\n\n# Hausman test for fixed versus random effects model\nphtest(random, fixed)\n\n# Breusch-Pagan heteroskedasticity\nlibrary(lmtest)\nbptest(y ~ x1 + factor(country), data = pdata)\n\n# If there is presence of heteroskedasticity\n## For RE model\ncoeftest(random) #orginal coef\n\n# Heteroskedasticity consistent coefficients\ncoeftest(random, vcovHC) \n\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(\n        vcovHC(random, type = x)\n    )))) #show HC SE of the coef\n# HC0 - heteroskedasticity consistent. The default.\n# HC1,HC2, HC3 – Recommended for small samples. \n# HC3 gives less weight to influential observations.\n# HC4 - small samples with influential observations\n# HAC - heteroskedasticity and autocorrelation consistent\n\n## For FE model\ncoeftest(fixed) # Original coefficients\ncoeftest(fixed, vcovHC) # Heteroskedasticity consistent coefficients\n\n# Heteroskedasticity consistent coefficients (Arellano)\ncoeftest(fixed, vcovHC(fixed, method = \"arellano\")) \n\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x)\n    sqrt(diag(\n        vcovHC(fixed, type = x)\n    )))) #show HC SE of the coef\namemiya <-\n    plm(\n        Y ~ X,\n        data = pdata,\n        model = \"random\",\n        random.method = \"amemiya\",\n        effect = \"twoways\"\n    )\nercomp(Y ~ X,\n       data = pdata,\n       method = \"amemiya\",\n       effect = \"twoways\")\npunbalancedness(random)\ninstr <-\n    plm(\n        Y ~ X | X_ins,\n        data = pdata,\n        random.method = \"ht\",\n        model = \"random\",\n        inst.method = \"baltagi\"\n    )"},{"path":"data.html","id":"other-estimators","chapter":"12 Data","heading":"12.4.6.1.1 Other Estimators","text":"","code":""},{},{},{},{"path":"data.html","id":"fixest-package","chapter":"12 Data","heading":"12.4.6.2 fixest package","text":"Available functionsfeols: linear modelsfeols: linear modelsfeglm: generalized linear modelsfeglm: generalized linear modelsfemlm: maximum likelihood estimationfemlm: maximum likelihood estimationfeNmlm: non-linear RHS parametersfeNmlm: non-linear RHS parametersfepois: Poisson fixed-effectfepois: Poisson fixed-effectfenegbin: negative binomial fixed-effectfenegbin: negative binomial fixed-effectNotescan work fixest objectExamples package’s authorsFor multiple estimation","code":"\nlibrary(fixest)\ndata(airquality)\n\n# Setting a dictionary\nsetFixest_dict(\n    c(\n        Ozone   = \"Ozone (ppb)\",\n        Solar.R = \"Solar Radiation (Langleys)\",\n        Wind    = \"Wind Speed (mph)\",\n        Temp    = \"Temperature\"\n    )\n)\n\n\n# On multiple estimations: see the dedicated vignette\nest = feols(\n    Ozone ~ Solar.R + sw0(Wind + Temp) | csw(Month, Day),\n    data = airquality,\n    cluster = ~ Day\n)\n\netable(est)\n#>                                         est.1              est.2\n#> Dependent Var.:                   Ozone (ppb)        Ozone (ppb)\n#>                                                                 \n#> Solar Radiation (Langleys) 0.1148*** (0.0234)   0.0522* (0.0202)\n#> Wind Speed (mph)                              -3.109*** (0.7986)\n#> Temperature                                    1.875*** (0.3671)\n#> Fixed-Effects:             ------------------ ------------------\n#> Month                                     Yes                Yes\n#> Day                                        No                 No\n#> __________________________ __________________ __________________\n#> S.E.: Clustered                       by: Day            by: Day\n#> Observations                              111                111\n#> R2                                    0.31974            0.63686\n#> Within R2                             0.12245            0.53154\n#> \n#>                                        est.3              est.4\n#> Dependent Var.:                  Ozone (ppb)        Ozone (ppb)\n#>                                                                \n#> Solar Radiation (Langleys) 0.1078** (0.0329)   0.0509* (0.0236)\n#> Wind Speed (mph)                             -3.289*** (0.7777)\n#> Temperature                                   2.052*** (0.2415)\n#> Fixed-Effects:             ----------------- ------------------\n#> Month                                    Yes                Yes\n#> Day                                      Yes                Yes\n#> __________________________ _________________ __________________\n#> S.E.: Clustered                      by: Day            by: Day\n#> Observations                             111                111\n#> R2                                   0.58018            0.81604\n#> Within R2                            0.12074            0.61471\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# in latex\netable(est, tex = T)\n#> \\begingroup\n#> \\centering\n#> \\begin{tabular}{lcccc}\n#>    \\tabularnewline \\midrule \\midrule\n#>    Dependent Variable: & \\multicolumn{4}{c}{Ozone (ppb)}\\\\\n#>    Model:                     & (1)            & (2)            & (3)            & (4)\\\\  \n#>    \\midrule\n#>    \\emph{Variables}\\\\\n#>    Solar Radiation (Langleys) & 0.1148$^{***}$ & 0.0522$^{**}$  & 0.1078$^{***}$ & 0.0509$^{**}$\\\\   \n#>                               & (0.0234)       & (0.0202)       & (0.0329)       & (0.0236)\\\\   \n#>    Wind Speed (mph)           &                & -3.109$^{***}$ &                & -3.289$^{***}$\\\\   \n#>                               &                & (0.7986)       &                & (0.7777)\\\\   \n#>    Temperature                &                & 1.875$^{***}$  &                & 2.052$^{***}$\\\\   \n#>                               &                & (0.3671)       &                & (0.2415)\\\\   \n#>    \\midrule\n#>    \\emph{Fixed-effects}\\\\\n#>    Month                      & Yes            & Yes            & Yes            & Yes\\\\  \n#>    Day                        &                &                & Yes            & Yes\\\\  \n#>    \\midrule\n#>    \\emph{Fit statistics}\\\\\n#>    Observations               & 111            & 111            & 111            & 111\\\\  \n#>    R$^2$                      & 0.31974        & 0.63686        & 0.58018        & 0.81604\\\\  \n#>    Within R$^2$               & 0.12245        & 0.53154        & 0.12074        & 0.61471\\\\  \n#>    \\midrule \\midrule\n#>    \\multicolumn{5}{l}{\\emph{Clustered (Day) standard-errors in parentheses}}\\\\\n#>    \\multicolumn{5}{l}{\\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\\\\n#> \\end{tabular}\n#> \\par\\endgroup\n\n\n# get the fixed-effects coefficients for 1 model\nfixedEffects = fixef(est[[1]])\nsummary(fixedEffects)\n#> Fixed_effects coefficients\n#> Number of fixed-effects for variable Month is 5.\n#>  Mean = 19.6 Variance = 272\n#> \n#> COEFFICIENTS:\n#>   Month:     5     6     7     8     9\n#>          3.219 8.288 34.26 40.12 12.13\n\n# see the fixed effects for one dimension\nfixedEffects$Month\n#>         5         6         7         8         9 \n#>  3.218876  8.287899 34.260812 40.122257 12.130971\n\nplot(fixedEffects)\n# set up\nlibrary(fixest)\n\n# let R know the base dataset (the biggest/ultimate \n# dataset that includes everything in your analysis)\nbase = iris\n\n# rename variables\nnames(base) = c(\"y1\", \"y2\", \"x1\", \"x2\", \"species\")\n\nres_multi = feols(\n    c(y1, y2) ~ x1 + csw(x2, x2 ^ 2) |\n        sw0(species),\n    data = base,\n    fsplit = ~ species,\n    lean = TRUE,\n    vcov = \"hc1\" # can also clustered at the fixed effect level\n)\n# it's recommended to use vcov at \n# estimation stage, not summary stage\n\nsummary(res_multi, \"compact\")\n#>         sample   fixef lhs               rhs     (Intercept)                x1\n#> 1  Full sample 1        y1 x1 + x2           4.19*** (0.104)  0.542*** (0.076)\n#> 2  Full sample 1        y1 x1 + x2 + I(x2^2) 4.27*** (0.101)  0.719*** (0.082)\n#> 3  Full sample 1        y2 x1 + x2           3.59*** (0.103) -0.257*** (0.066)\n#> 4  Full sample 1        y2 x1 + x2 + I(x2^2) 3.68*** (0.097)    -0.030 (0.078)\n#> 5  Full sample species  y1 x1 + x2                            0.906*** (0.076)\n#> 6  Full sample species  y1 x1 + x2 + I(x2^2)                  0.900*** (0.077)\n#> 7  Full sample species  y2 x1 + x2                              0.155* (0.073)\n#> 8  Full sample species  y2 x1 + x2 + I(x2^2)                    0.148. (0.075)\n#> 9  setosa      1        y1 x1 + x2           4.25*** (0.474)     0.399 (0.325)\n#> 10 setosa      1        y1 x1 + x2 + I(x2^2) 4.00*** (0.504)     0.405 (0.325)\n#> 11 setosa      1        y2 x1 + x2           2.89*** (0.416)     0.247 (0.305)\n#> 12 setosa      1        y2 x1 + x2 + I(x2^2) 2.82*** (0.423)     0.248 (0.304)\n#> 13 setosa      species  y1 x1 + x2                               0.399 (0.325)\n#> 14 setosa      species  y1 x1 + x2 + I(x2^2)                     0.405 (0.325)\n#> 15 setosa      species  y2 x1 + x2                               0.247 (0.305)\n#> 16 setosa      species  y2 x1 + x2 + I(x2^2)                     0.248 (0.304)\n#> 17 versicolor  1        y1 x1 + x2           2.38*** (0.423)  0.934*** (0.166)\n#> 18 versicolor  1        y1 x1 + x2 + I(x2^2)   0.323 (1.44)   0.901*** (0.164)\n#> 19 versicolor  1        y2 x1 + x2           1.25*** (0.275)     0.067 (0.095)\n#> 20 versicolor  1        y2 x1 + x2 + I(x2^2)   0.097 (1.01)      0.048 (0.099)\n#> 21 versicolor  species  y1 x1 + x2                            0.934*** (0.166)\n#> 22 versicolor  species  y1 x1 + x2 + I(x2^2)                  0.901*** (0.164)\n#> 23 versicolor  species  y2 x1 + x2                               0.067 (0.095)\n#> 24 versicolor  species  y2 x1 + x2 + I(x2^2)                     0.048 (0.099)\n#> 25 virginica   1        y1 x1 + x2             1.05. (0.539)  0.995*** (0.090)\n#> 26 virginica   1        y1 x1 + x2 + I(x2^2)   -2.39 (2.04)   0.994*** (0.088)\n#> 27 virginica   1        y2 x1 + x2             1.06. (0.572)     0.149 (0.107)\n#> 28 virginica   1        y2 x1 + x2 + I(x2^2)    1.10 (1.76)      0.149 (0.108)\n#> 29 virginica   species  y1 x1 + x2                            0.995*** (0.090)\n#> 30 virginica   species  y1 x1 + x2 + I(x2^2)                  0.994*** (0.088)\n#> 31 virginica   species  y2 x1 + x2                               0.149 (0.107)\n#> 32 virginica   species  y2 x1 + x2 + I(x2^2)                     0.149 (0.108)\n#>                  x2          I(x2^2)\n#> 1   -0.320. (0.170)                 \n#> 2  -1.52*** (0.307) 0.348*** (0.075)\n#> 3    0.364* (0.142)                 \n#> 4  -1.18*** (0.313) 0.446*** (0.074)\n#> 5    -0.006 (0.163)                 \n#> 6     0.290 (0.408)   -0.088 (0.117)\n#> 7  0.623*** (0.114)                 \n#> 8    0.951* (0.472)   -0.097 (0.125)\n#> 9    0.712. (0.418)                 \n#> 10    2.51. (1.47)     -2.91 (2.10) \n#> 11    0.702 (0.560)                 \n#> 12     1.27 (2.39)    -0.911 (3.28) \n#> 13   0.712. (0.418)                 \n#> 14    2.51. (1.47)     -2.91 (2.10) \n#> 15    0.702 (0.560)                 \n#> 16     1.27 (2.39)    -0.911 (3.28) \n#> 17   -0.320 (0.364)                 \n#> 18     3.01 (2.31)     -1.24 (0.841)\n#> 19 0.929*** (0.244)                 \n#> 20    2.80. (1.65)    -0.695 (0.583)\n#> 21   -0.320 (0.364)                 \n#> 22     3.01 (2.31)     -1.24 (0.841)\n#> 23 0.929*** (0.244)                 \n#> 24    2.80. (1.65)    -0.695 (0.583)\n#> 25    0.007 (0.205)                 \n#> 26    3.50. (2.09)    -0.870 (0.519)\n#> 27 0.535*** (0.122)                 \n#> 28    0.503 (1.56)     0.008 (0.388)\n#> 29    0.007 (0.205)                 \n#> 30    3.50. (2.09)    -0.870 (0.519)\n#> 31 0.535*** (0.122)                 \n#> 32    0.503 (1.56)     0.008 (0.388)\n\n# call the first 3 estimated models only\netable(res_multi[1:3],\n       \n       # customize the headers\n       headers = c(\"mod1\", \"mod2\", \"mod3\")) \n#>                   res_multi[1:3].1   res_multi[1:3].2    res_multi[1:3].3\n#>                               mod1               mod2                mod3\n#> Dependent Var.:                 y1                 y1                  y2\n#>                                                                          \n#> Constant         4.191*** (0.1037)  4.266*** (0.1007)   3.587*** (0.1031)\n#> x1              0.5418*** (0.0761) 0.7189*** (0.0815) -0.2571*** (0.0664)\n#> x2               -0.3196. (0.1700) -1.522*** (0.3072)    0.3640* (0.1419)\n#> x2 square                          0.3479*** (0.0748)                    \n#> _______________ __________________ __________________ ___________________\n#> S.E. type       Heteroskedas.-rob. Heteroskedas.-rob. Heteroskedast.-rob.\n#> Observations                   150                150                 150\n#> R2                         0.76626            0.79456             0.21310\n#> Adj. R2                    0.76308            0.79034             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"multiple-estimation-left-hand-side","chapter":"12 Data","heading":"12.4.6.2.1 Multiple estimation (Left-hand side)","text":"multiple interested dependent variablesTo input list dependent variable","code":"\netable(feols(c(y1, y2) ~ x1 + x2, base))\n#>                 feols(c(y1, y2)..1 feols(c(y1, y2) ..2\n#> Dependent Var.:                 y1                  y2\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> x1              0.5418*** (0.0693) -0.2571*** (0.0669)\n#> x2               -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndepvars <- c(\"y1\", \"y2\")\n\nres <- lapply(depvars, function(var) {\n    res <- feols(xpd(..lhs ~ x1 + x2, ..lhs = var), data = base)\n    # summary(res)\n})\netable(res)\n#>                            model 1             model 2\n#> Dependent Var.:                 y1                  y2\n#>                                                       \n#> Constant         4.191*** (0.0970)   3.587*** (0.0937)\n#> x1              0.5418*** (0.0693) -0.2571*** (0.0669)\n#> x2               -0.3196* (0.1605)    0.3640* (0.1550)\n#> _______________ __________________ ___________________\n#> S.E. type                      IID                 IID\n#> Observations                   150                 150\n#> R2                         0.76626             0.21310\n#> Adj. R2                    0.76308             0.20240\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"multiple-estimation-right-hand-side","chapter":"12 Data","heading":"12.4.6.2.2 Multiple estimation (Right-hand side)","text":"Options write functionssw (stepwise): sequentially analyze elements\ny ~ sw(x1, x2) estimated y ~ x1 y ~ x2\nsw (stepwise): sequentially analyze elementsy ~ sw(x1, x2) estimated y ~ x1 y ~ x2sw0 (stepwise 0): similar sw also estimate model without elements set first\ny ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2\nsw0 (stepwise 0): similar sw also estimate model without elements set firsty ~ sw(x1, x2) estimated y ~ 1 y ~ x1 y ~ x2csw (cumulative stepwise): sequentially add element set formula\ny ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2\ncsw (cumulative stepwise): sequentially add element set formulay ~ csw(x1, x2) estimated y ~ x1 y ~ x1 + x2csw0 (cumulative stepwise 0): similar csw also estimate model without elements set first\ny ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2\ncsw0 (cumulative stepwise 0): similar csw also estimate model without elements set firsty ~ csw(x1, x2) estimated y~ 1 y ~ x1 y ~ x1 + x2mvsw (multiverse stepwise): possible combination elements set (get large quick).\nmvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)\nmvsw (multiverse stepwise): possible combination elements set (get large quick).mvsw(x1, x2, x3) sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3)","code":""},{"path":"data.html","id":"split-sample-estimation","chapter":"12 Data","heading":"12.4.6.2.3 Split sample estimation","text":"","code":"\netable(feols(y1 ~ x1 + x2, fsplit = ~ species, data = base))\n#>                  feols(y1 ~ x1 +..1 feols(y1 ~ x1 ..2 feols(y1 ~ x1 +..3\n#> Sample (species)        Full sample            setosa         versicolor\n#> Dependent Var.:                  y1                y1                 y1\n#>                                                                         \n#> Constant          4.191*** (0.0970) 4.248*** (0.4114)  2.381*** (0.4493)\n#> x1               0.5418*** (0.0693)   0.3990 (0.2958) 0.9342*** (0.1693)\n#> x2                -0.3196* (0.1605)   0.7121 (0.4874)   -0.3200 (0.4024)\n#> ________________ __________________ _________________ __________________\n#> S.E. type                       IID               IID                IID\n#> Observations                    150                50                 50\n#> R2                          0.76626           0.11173            0.57432\n#> Adj. R2                     0.76308           0.07393            0.55620\n#> \n#>                  feols(y1 ~ x1 +..4\n#> Sample (species)          virginica\n#> Dependent Var.:                  y1\n#>                                    \n#> Constant            1.052* (0.5139)\n#> x1               0.9946*** (0.0893)\n#> x2                  0.0071 (0.1795)\n#> ________________ __________________\n#> S.E. type                       IID\n#> Observations                     50\n#> R2                          0.74689\n#> Adj. R2                     0.73612\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"data.html","id":"standard-errors-1","chapter":"12 Data","heading":"12.4.6.2.4 Standard Errors","text":"iid: errors homoskedastic independent identically distributediid: errors homoskedastic independent identically distributedhetero: errors heteroskedastic using White correctionhetero: errors heteroskedastic using White correctioncluster: errors correlated within cluster groupscluster: errors correlated within cluster groupsnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.\nvcov = newey_west ~ id + period id subject id period time period panel.\nspecify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.\nnewey_west: (Newey West 1986) use time series panel data. Errors heteroskedastic serially correlated.vcov = newey_west ~ id + period id subject id period time period panel.vcov = newey_west ~ id + period id subject id period time period panel.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.specify lag period consider vcov = newey_west(2) ~ id + period ’re considering 2 lag periods.driscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.\nvcov = discoll_kraay ~ period\ndriscoll_kraay (Driscoll Kraay 1998) use panel data. Errors cross-sectionally serially correlated.vcov = discoll_kraay ~ periodconley: (Conley 1999) cross-section data. Errors spatially correlated\nvcov = conley ~ latitude + longitude\nspecify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.\nconley: (Conley 1999) cross-section data. Errors spatially correlatedvcov = conley ~ latitude + longitudevcov = conley ~ latitude + longitudeto specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.specify distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), use conley() helper function.hc: sandwich package\nvcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))\nhc: sandwich packagevcov = function(x) sandwich::vcovHC(x, type = \"HC1\"))let R know SE estimation want use, insert vcov = vcov_type ~ variables","code":""},{"path":"data.html","id":"small-sample-correction","chapter":"12 Data","heading":"12.4.6.2.5 Small sample correction","text":"specify R needs use small sample correction addssc = ssc(adj = T, cluster.adj = T)","code":""},{"path":"variable-transformation.html","id":"variable-transformation","chapter":"13 Variable Transformation","heading":"13 Variable Transformation","text":"trafo vignette","code":""},{"path":"variable-transformation.html","id":"continuous-variables","chapter":"13 Variable Transformation","heading":"13.1 Continuous Variables","text":"Purposes:change scale variablesTo change scale variablesTo transform skewed data distribution normal distributionTo transform skewed data distribution normal distribution","code":""},{"path":"variable-transformation.html","id":"standardization","chapter":"13 Variable Transformation","heading":"13.1.1 Standardization","text":"\\[\nx_i' = \\frac{x_i - \\bar{x}}{s}\n\\]large numbers","code":""},{"path":"variable-transformation.html","id":"min-max-scaling","chapter":"13 Variable Transformation","heading":"13.1.2 Min-max scaling","text":"\\[\nx_i' = \\frac{x_i - x_{max}}{x_{max} - x_{min}}\n\\]dependent min max values, makes sensitive outliers.best use values fixed interval.","code":""},{"path":"variable-transformation.html","id":"square-rootcube-root","chapter":"13 Variable Transformation","heading":"13.1.3 Square Root/Cube Root","text":"variables positive skewness residuals positive heteroskasticity.variables positive skewness residuals positive heteroskasticity.Frequency counts variableFrequency counts variableData many 0 extremely small values.Data many 0 extremely small values.","code":""},{"path":"variable-transformation.html","id":"logarithmic","chapter":"13 Variable Transformation","heading":"13.1.4 Logarithmic","text":"Variables positively skewed distributionFor general case \\(\\log(x_i + c)\\), choosing constant rather tricky.choice constant critically important, especially want inference. can dramatically change model fit (see (Ekwaru Veugelers 2018) independent variable case).J. Chen Roth (2023) show causal inference problem, \\(\\log\\) transformation values meaningful 0 problematic. solutions approach (e.g., , IV).However, assuming 0s ofCensoringCensoringNo measurement errors (stemming measurement tools)measurement errors (stemming measurement tools)can proceed choosing c (’s okay 0’s represent really small values).","code":"\ndata(cars)\ncars$speed %>% head()\n#> [1] 4 4 7 7 8 9\n\nlog(cars$speed) %>% head()\n#> [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225\n\n# log(x+1)\nlog1p(cars$speed) %>% head()\n#> [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585"},{"path":"variable-transformation.html","id":"exponential","chapter":"13 Variable Transformation","heading":"13.1.5 Exponential","text":"Negatively skewed dataNegatively skewed dataUnderlying logarithmic trend (e.g., survival, decay)Underlying logarithmic trend (e.g., survival, decay)","code":""},{"path":"variable-transformation.html","id":"power-2","chapter":"13 Variable Transformation","heading":"13.1.6 Power","text":"Variables negatively skewed distribution","code":""},{"path":"variable-transformation.html","id":"inversereciprocal","chapter":"13 Variable Transformation","heading":"13.1.7 Inverse/Reciprocal","text":"Variables platykurtic distributionVariables platykurtic distributionData positively skewedData positively skewedRatio dataRatio data","code":"\ndata(cars)\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\nplot(cars$dist)\nplot(1/(cars$dist))"},{"path":"variable-transformation.html","id":"hyperbolic-arcsine","chapter":"13 Variable Transformation","heading":"13.1.8 Hyperbolic arcsine","text":"Variables positively skewed distribution","code":""},{"path":"variable-transformation.html","id":"ordered-quantile-norm","chapter":"13 Variable Transformation","heading":"13.1.9 Ordered Quantile Norm","text":"(Bartlett 1947)\\[\nx_i' = \\Phi^{-1} (\\frac{rank(x_i) - 1/2}{length(x)})\n\\]","code":"\nord_dist <- bestNormalize::orderNorm(cars$dist)\nord_dist\n#> orderNorm Transformation with 50 nonmissing obs and ties\n#>  - 35 unique values \n#>  - Original quantiles:\n#>   0%  25%  50%  75% 100% \n#>    2   26   36   56  120\nord_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"arcsinh","chapter":"13 Variable Transformation","heading":"13.1.10 Arcsinh","text":"Proportion variable (0-1)\\[\narcsinh(Y) = \\log(\\sqrt{1 + Y^2} + Y)\n\\]simple regression model, \\(Y = \\beta X\\)\\(Y\\) \\(X\\) transformed, coefficient estimate represents elasticity, indicating percentage change \\(Y\\) 1% change \\(X\\).\\(Y\\) transformed \\(X\\) raw form, coefficient estimate represents percentage change \\(Y\\) one-unit change \\(X\\).","code":"\ncars$dist %>% hist()\n# cars$dist %>% MASS::truehist()\n\nas_dist <- bestNormalize::arcsinh_x(cars$dist)\nas_dist\n#> Standardized asinh(x) Transformation with 50 nonmissing obs.:\n#>  Relevant statistics:\n#>  - mean (before standardization) = 4.230843 \n#>  - sd (before standardization) = 0.7710887\nas_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"lambert-w-x-f-transformation","chapter":"13 Variable Transformation","heading":"13.1.11 Lambert W x F Transformation","text":"LambertW packageUsing moments normalize data.Using moments normalize data.Usually need compare Box-Cox Transformation Yeo-Johnson TransformationUsually need compare Box-Cox Transformation Yeo-Johnson TransformationCan handle skewness, heavy-tailed.Can handle skewness, heavy-tailed.","code":"\ndata(cars)\nhead(cars$dist)\n#> [1]  2 10  4 22 16 10\ncars$dist %>% hist()\n\n\nl_dist <- LambertW::Gaussianize(cars$dist)\n# small fix\nl_dist %>% hist()"},{"path":"variable-transformation.html","id":"inverse-hyperbolic-sine-ihs-transformation","chapter":"13 Variable Transformation","heading":"13.1.12 Inverse Hyperbolic Sine (IHS) transformation","text":"Proposed (N. L. Johnson 1949)Proposed (N. L. Johnson 1949)Can applied real numbers.Can applied real numbers.\\[\n\\begin{aligned}\nf(x,\\theta) &= \\frac{\\sinh^{-1} (\\theta x)}{\\theta} \\\\\n&= \\frac{\\log(\\theta x + (\\theta^2 x^2 + 1)^{1/2})}{\\theta}\n\\end{aligned}\n\\]","code":""},{"path":"variable-transformation.html","id":"box-cox-transformation","chapter":"13 Variable Transformation","heading":"13.1.13 Box-Cox Transformation","text":"\\[\ny^\\lambda = \\beta x+ \\epsilon\n\\]fix non-linearity error termswork well (-3,3) (.e., small transformation).independent variables\\[\nx_i'^\\lambda =\n\\begin{cases}\n\\frac{x_i^\\lambda-1}{\\lambda} & \\text{} \\lambda \\neq 0\\\\\n\\log(x_i) & \\text{} \\lambda = 0\n\\end{cases}\n\\]two-parameter version \\[\nx_i' (\\lambda_1, \\lambda_2) =\n\\begin{cases}\n\\frac{(x_i + \\lambda_2)^{\\lambda_1}-1}{} & \\text{} \\lambda_1 \\neq 0 \\\\\n\\log(x_i + \\lambda_2) & \\text{} \\lambda_1 = 0\n\\end{cases}\n\\]advances(Manly 1976)(Manly 1976)(Bickel Doksum 1981; Box Cox 1981)(Bickel Doksum 1981; Box Cox 1981)","code":"\nlibrary(MASS)\ndata(cars)\nmod <- lm(cars$speed ~ cars$dist, data = cars)\n# check residuals\nplot(mod)\n\nbc <- boxcox(mod, lambda = seq(-3, 3))\n\n# best lambda\nbc$x[which(bc$y == max(bc$y))]\n#> [1] 1.242424\n\n# model with best lambda\nmod_lambda = lm(cars$speed ^ (bc$x[which(bc$y == max(bc$y))]) ~ cars$dist, \n                data = cars)\nplot(mod_lambda)\n\n# 2-parameter version\ntwo_bc = geoR::boxcoxfit(cars$speed)\ntwo_bc\n#> Fitted parameters:\n#>    lambda      beta   sigmasq \n#>  1.028798 15.253008 31.935297 \n#> \n#> Convergence code returned by optim: 0\nplot(two_bc)\n\n\n# bestNormalize\nbc_dist <- bestNormalize::boxcox(cars$dist)\nbc_dist\n#> Standardized Box Cox Transformation with 50 nonmissing obs.:\n#>  Estimated statistics:\n#>  - lambda = 0.4950628 \n#>  - mean (before standardization) = 10.35636 \n#>  - sd (before standardization) = 3.978036\nbc_dist$x.t %>% hist()"},{"path":"variable-transformation.html","id":"yeo-johnson-transformation","chapter":"13 Variable Transformation","heading":"13.1.14 Yeo-Johnson Transformation","text":"Similar Box-Cox Transformation (\\(\\lambda = 1\\)), allows negative value\\[\nx_i'^\\lambda =\n\\begin{cases}\n\\frac{(x_i+1)^\\lambda -1}{\\lambda} & \\text{} \\lambda \\neq0, x_i \\ge 0 \\\\\n\\log(x_i + 1) & \\text{} \\lambda = 0, x_i \\ge 0 \\\\\n\\frac{-[(-x_i+1)^{2-\\lambda}-1]}{2 - \\lambda} & \\text{} \\lambda \\neq 2, x_i <0 \\\\\n-\\log(-x_i + 1) & \\text{} \\lambda = 2, x_i <0\n\\end{cases}\n\\]","code":"\ndata(cars)\nyj_speed <- bestNormalize::yeojohnson(cars$speed)\nyj_speed$x.t %>% hist()"},{"path":"variable-transformation.html","id":"rankgauss","chapter":"13 Variable Transformation","heading":"13.1.15 RankGauss","text":"Turn values ranks, ranks values normal distribution.","code":""},{"path":"variable-transformation.html","id":"summary-3","chapter":"13 Variable Transformation","heading":"13.1.16 Summary","text":"Automatically choose best method normalize data (code bestNormalize)","code":"\nbestdist <- bestNormalize::bestNormalize(cars$dist)\nbestdist$x.t %>% hist()\n\nboxplot(log10(bestdist$oos_preds), yaxt = \"n\")\n# axis(2, at = log10(c(.1, .5, 1, 2, 5, 10)), \n#      labels = c(.1, .5, 1, 2, 5, 10))"},{"path":"variable-transformation.html","id":"categorical-variables","chapter":"13 Variable Transformation","heading":"13.2 Categorical Variables","text":"PurposesTo transform continuous variable (machine learning models) (e.g., encoding/ embedding text mining)Approaches:One-hot encodingOne-hot encodingLabel encodingLabel encodingFeature hashingFeature hashingBinary encodingBinary encodingBase N encodingBase N encodingFrequency encodingFrequency encodingTarget encodingTarget encodingOrdinal encodingOrdinal encodingHelmert encodingHelmert encodingMean encodingMean encodingWeight evidence encodingWeight evidence encodingProbability ratio encodingProbability ratio encodingBackward difference encodingBackward difference encodingLeave one encodingLeave one encodingJames-Stein encodingJames-Stein encodingM-estimator encodingM-estimator encodingThermometer encodingThermometer encoding","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"14 Hypothesis Testing","heading":"14 Hypothesis Testing","text":"Error types:Type Error (False Positive):\nReality: nope\nDiagnosis/Analysis: yes\nType Error (False Positive):Reality: nopeDiagnosis/Analysis: yesType II Error (False Negative):\nReality: yes\nDiagnosis/Analysis: nope\nType II Error (False Negative):Reality: yesDiagnosis/Analysis: nopePower: probability rejecting null hypothesis actually falseNote:Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Sometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\n\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\n\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\nSometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]Assuming null hypothesis true, (asymptotic) distribution estimatorAssuming null hypothesis true, (asymptotic) distribution estimatorTwo-sidedTwo-sided\\[\n\\begin{aligned}\n&H_0: \\beta_j = 0 \\\\\n&H_1: \\beta_j \\neq 0\n\\end{aligned}\n\\]null, OLS estimator following distribution\\[\nA1-A3a, A5: \\sqrt{n} \\hat{\\beta_j}  \\sim  N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]one-sided test, null set values, now choose worst case single value hardest prove derive distribution nullOne-sided\\[\n\\begin{aligned}\n&H_0: \\beta_j\\ge 0 \\\\\n&H_1: \\beta_j < 0\n\\end{aligned}\n\\]hardest null value prove \\(H_0: \\beta_j=0\\). specific null, OLS estimator following asymptotic distribution\\[\nA1-A3a, A5: \\sqrt{n}\\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]","code":""},{"path":"hypothesis-testing.html","id":"types-of-hypothesis-testing","chapter":"14 Hypothesis Testing","heading":"14.1 Types of hypothesis testing","text":"\\(H_0 : \\theta = \\theta_0\\)\\(H_1 : \\theta \\neq \\theta_0\\)far away / extreme \\(\\theta\\) can null hypothesis trueAssume likelihood function q \\(L(q) = q^{30}(1-q)^{70}\\) Likelihood functionLog-Likelihood functionFigure (Fox 1997)typically, likelihood ratio test (Lagrange Multiplier (Score)) performs better small moderate sample sizes, Wald test requires one maximization (full model).","code":"\nq = seq(0, 1, length = 100)\nL = function(q) {\n    q ^ 30 * (1 - q) ^ 70\n}\n\nplot(q,\n     L(q),\n     ylab = \"L(q)\",\n     xlab = \"q\",\n     type = \"l\")\nq = seq(0, 1, length = 100)\nl = function(q) {\n    30 * log(q) + 70 * log(1 - q)\n}\nplot(q,\n     l(q) - l(0.3),\n     ylab = \"l(q) - l(qhat)\",\n     xlab = \"q\",\n     type = \"l\")\nabline(v = 0.2)"},{"path":"hypothesis-testing.html","id":"wald-test","chapter":"14 Hypothesis Testing","heading":"14.2 Wald test","text":"\\[\n\\begin{aligned}\nW &= (\\hat{\\theta}-\\theta_0)'[cov(\\hat{\\theta})]^{-1}(\\hat{\\theta}-\\theta_0) \\\\\nW &\\sim \\chi_q^2\n\\end{aligned}\n\\]\\(cov(\\hat{\\theta})\\) given inverse Fisher Information matrix evaluated \\(\\hat{\\theta}\\) q rank \\(cov(\\hat{\\theta})\\), number non-redundant parameters \\(\\theta\\)Alternatively,\\[\nt_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{(\\theta_0)^{-1}} \\sim \\chi^2_{(v)}\n\\]v degree freedom.Equivalently,\\[\ns_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{(\\hat{\\theta})^{-1}}} \\sim Z\n\\]far away distribution sample estimate hypothesized population parameter.null value, probability obtained realization “extreme” “worse” estimate actually obtained?Significance Level (\\(\\alpha\\)) Confidence Level (\\(1-\\alpha\\))significance level benchmark probability low reject nullThe confidence level probability sets bounds far away realization estimator reject null.Test StatisticsStandardized (transform) estimator null value test statistic always distributionTest Statistic OLS estimator single hypothesis\\[\nT = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]Equivalently,\\[\nT = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]test statistic another random variable function data null hypothesis.T denotes random variable test statistict denotes single realization test statisticEvaluating Test Statistic: determine whether reject fail reject null hypothesis given significance / confidence levelThree equivalent waysCritical ValueCritical ValueP-valueP-valueConfidence IntervalConfidence IntervalCritical ValueCritical ValueFor given significance level, determine critical value \\((c)\\)One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\nP(T<c|H_0)=\\alpha\n\\]Reject null \\(t<c\\)One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\nP(T>c|H_0)=\\alpha\n\\]Reject null \\(t>c\\)Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\nP(|T|>c|H_0)=\\alpha\n\\]Reject null \\(|t|>c\\)p-valueCalculate probability test statistic worse realization haveOne-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\n\\text{p-value} = P(T<t|H_0)\n\\]One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\n\\text{p-value} = P(T>t|H_0)\n\\]Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\n\\text{p-value} = P(|T|<t|H_0)\n\\]reject null p-value \\(< \\alpha\\)Confidence IntervalUsing critical value associated null hypothesis significance level, create interval\\[\nCI(\\hat{\\beta}_j)_{\\alpha} = [\\hat{\\beta}_j-(c \\times SE(\\hat{\\beta}_j)),\\hat{\\beta}_j+(c \\times SE(\\hat{\\beta}_j))]\n\\]null set lies outside interval reject null.testing whether true population value close estimate, testing given field true population value parameter, like observed estimate.Can interpreted believe \\((1-\\alpha)\\times 100 \\%\\) probability confidence interval captures true parameter value.stronger assumption (A1-A6), consider Finite Sample Properties\\[\nT = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k)\n\\]distributional derivation strongly dependent A4 A5T student t-distribution numerator normal denominator \\(\\chi^2\\).Critical value p-values calculated student t-distribution rather standard normal distribution.\\(n \\\\infty\\), \\(T(n-k)\\) asymptotically standard normal.Rule thumbif \\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k<120\\)\n(A1-A6) hold t-test exact finite distribution test\n(A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))\n\\(n-k<120\\)(A1-A6) hold t-test exact finite distribution testif (A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))","code":""},{"path":"hypothesis-testing.html","id":"multiple-hypothesis","chapter":"14 Hypothesis Testing","heading":"14.2.1 Multiple Hypothesis","text":"test multiple parameters time\n\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\n\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)\ntest multiple parameters time\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).test statistic based restriction written matrix form.test statistic based restriction written matrix form.\\[\ny=\\beta_0+x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon\n\\]Null hypothesis \\(H_0: \\beta_1 = 0\\) & \\(\\beta_2=0\\) can rewritten \\(H_0: \\mathbf{R}\\beta -\\mathbf{q}=0\\) \\(\\mathbf{R}\\) \\(m \\times k\\) matrix m number restrictions \\(k\\) number parameters. \\(\\mathbf{q}\\) \\(k \\times 1\\) vector\\(\\mathbf{R}\\) “picks ” relevant parameters \\(\\mathbf{q}\\) null value parameter\\[\n\\mathbf{R}=\n\\left(\n\\begin{array}{cccc}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right),\n\\mathbf{q} =\n\\left(\n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right)\n\\]Test Statistic OLS estimator multiple hypothesis\\[\nF = \\frac{(\\mathbf{R\\hat{\\beta}-q})\\hat{\\Sigma}^{-1}(\\mathbf{R\\hat{\\beta}-q})}{m} \\sim^F(m,n-k)\n\\]\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrix\nA4 holds, homoskedastic heteroskedastic versions produce valid estimator\nA4 hold, heteroskedastic version produces valid estimators.\n\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrixif A4 holds, homoskedastic heteroskedastic versions produce valid estimatorIf A4 hold, heteroskedastic version produces valid estimators.\\(m = 1\\), single restriction, \\(F\\)-statistic \\(t\\)-statistic squared.\\(m = 1\\), single restriction, \\(F\\)-statistic \\(t\\)-statistic squared.\\(F\\) distribution strictly positive, check [F-Distribution] details.\\(F\\) distribution strictly positive, check [F-Distribution] details.","code":""},{"path":"hypothesis-testing.html","id":"linear-combination","chapter":"14 Hypothesis Testing","heading":"14.2.2 Linear Combination","text":"Testing multiple parameters time\\[\n\\begin{aligned}\nH_0&: \\beta_1 -\\beta_2 = 0 \\\\\nH_0&: \\beta_1 - \\beta_2 > 0 \\\\\nH_0&: \\beta_1 - 2\\times\\beta_2 =0\n\\end{aligned}\n\\]single restriction function parameters.Null hypothesis:\\[\nH_0: \\beta_1 -\\beta_2 = 0\n\\]can rewritten \\[\nH_0: \\mathbf{R}\\beta -\\mathbf{q}=0\n\\]\\(\\mathbf{R}\\)=(0 1 -1 0 0) \\(\\mathbf{q}=0\\)","code":""},{"path":"hypothesis-testing.html","id":"estimate-difference-in-coefficients","chapter":"14 Hypothesis Testing","heading":"14.2.3 Estimate Difference in Coefficients","text":"package estimate difference two coefficients CI, simple function created Katherine Zee can used calculate difference. modifications might needed don’t use standard lm model R.","code":"\ndifftest_lm <- function(x1, x2, model) {\n    diffest <-\n        summary(model)$coef[x1, \"Estimate\"] - summary(model)$coef[x2, \"Estimate\"]\n    \n    vardiff <- (summary(model)$coef[x1, \"Std. Error\"] ^ 2 +\n                    summary(model)$coef[x2, \"Std. Error\"] ^ 2) - (2 * (vcov(model)[x1, x2]))\n    # variance of x1 + variance of x2 - 2*covariance of x1 and x2\n    diffse <- sqrt(vardiff)\n    tdiff <- (diffest) / (diffse)\n    ptdiff <- 2 * (1 - pt(abs(tdiff), model$df, lower.tail = T))\n    upr <-\n        # will usually be very close to 1.96\n        diffest + qt(.975, df = model$df) * diffse \n    lwr <- diffest + qt(.025, df = model$df) * diffse\n    df <- model$df\n    return(list(\n        est = round(diffest, digits = 2),\n        t = round(tdiff, digits = 2),\n        p = round(ptdiff, digits = 4),\n        lwr = round(lwr, digits = 2),\n        upr = round(upr, digits = 2),\n        df = df\n    ))\n}"},{"path":"hypothesis-testing.html","id":"application-9","chapter":"14 Hypothesis Testing","heading":"14.2.4 Application","text":"","code":"\nlibrary(\"car\")\n\n# Multiple hypothesis\nmod.davis <- lm(weight ~ repwt, data=Davis)\nlinearHypothesis(mod.davis, c(\"(Intercept) = 0\", \"repwt = 1\"),white.adjust = TRUE)\n#> Linear hypothesis test\n#> \n#> Hypothesis:\n#> (Intercept) = 0\n#> repwt = 1\n#> \n#> Model 1: restricted model\n#> Model 2: weight ~ repwt\n#> \n#> Note: Coefficient covariance matrix supplied.\n#> \n#>   Res.Df Df      F  Pr(>F)  \n#> 1    183                    \n#> 2    181  2 3.3896 0.03588 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Linear Combination\nmod.duncan <- lm(prestige ~ income + education, data=Duncan)\nlinearHypothesis(mod.duncan, \"1*income - 1*education = 0\")\n#> Linear hypothesis test\n#> \n#> Hypothesis:\n#> income - education = 0\n#> \n#> Model 1: restricted model\n#> Model 2: prestige ~ income + education\n#> \n#>   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n#> 1     43 7518.9                           \n#> 2     42 7506.7  1    12.195 0.0682 0.7952"},{"path":"hypothesis-testing.html","id":"nonlinear-1","chapter":"14 Hypothesis Testing","heading":"14.2.5 Nonlinear","text":"Suppose q nonlinear functions parameters\\[\n\\mathbf{h}(\\theta) = \\{ h_1 (\\theta), ..., h_q (\\theta)\\}'\n\\],n, Jacobian matrix (\\(\\mathbf{H}(\\theta)\\)), rank q \\[\n\\mathbf{H}_{q \\times p}(\\theta) =\n\\left(\n\\begin{array}\n{ccc}\n\\frac{\\partial h_1(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_1(\\theta)}{\\partial \\theta_p} \\\\\n. & . & . \\\\\n\\frac{\\partial h_q(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_q(\\theta)}{\\partial \\theta_p}\n\\end{array}\n\\right)\n\\]null hypothesis \\(H_0: \\mathbf{h} (\\theta) = 0\\) can tested 2-sided alternative Wald statistic\\[\nW = \\frac{\\mathbf{h(\\hat{\\theta})'\\{H(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}H(\\hat{\\theta})'\\}^{-1}h(\\hat{\\theta})}}{s^2q} \\sim F_{q,n-p}\n\\]","code":""},{"path":"hypothesis-testing.html","id":"the-likelihood-ratio-test","chapter":"14 Hypothesis Testing","heading":"14.3 The likelihood ratio test","text":"\\[\nt_{LR} = 2[l(\\hat{\\theta})-l(\\theta_0)] \\sim \\chi^2_v\n\\]v degree freedom.Compare height log-likelihood sample estimate relation height log-likelihood hypothesized population parameterAlternatively,test considers ratio two maximizations,\\[\n\\begin{aligned}\nL_r &= \\text{maximized value likelihood $H_0$ (reduced model)} \\\\\nL_f &= \\text{maximized value likelihood $H_0 \\cup H_a$ (full model)}\n\\end{aligned}\n\\], likelihood ratio :\\[\n\\Lambda = \\frac{L_r}{L_f}\n\\]can’t exceed 1 (since \\(L_f\\) always least large \\(L-r\\) \\(L_r\\) result maximization restricted set parameter values).likelihood ratio statistic :\\[\n\\begin{aligned}\n-2ln(\\Lambda) &= -2ln(L_r/L_f) = -2(l_r - l_f) \\\\\n\\lim_{n \\\\infty}(-2ln(\\Lambda)) &\\sim \\chi^2_v\n\\end{aligned}\n\\]\\(v\\) number parameters full model minus number parameters reduced model.\\(L_r\\) much smaller \\(L_f\\) (likelihood ratio exceeds \\(\\chi_{\\alpha,v}^2\\)), reject reduced model accept full model \\(\\alpha \\times 100 \\%\\) significance level","code":""},{"path":"hypothesis-testing.html","id":"lagrange-multiplier-score","chapter":"14 Hypothesis Testing","heading":"14.4 Lagrange Multiplier (Score)","text":"\\[\nt_S= \\frac{S(\\theta_0)^2}{(\\theta_0)} \\sim \\chi^2_v\n\\]\\(v\\) degree freedom.Compare slope log-likelihood sample estimate relation slope log-likelihood hypothesized population parameter","code":""},{"path":"hypothesis-testing.html","id":"two-one-sided-tests-tost-equivalence-testing","chapter":"14 Hypothesis Testing","heading":"14.5 Two One-Sided Tests (TOST) Equivalence Testing","text":"good way test whether population effect size within range practical interest (e.g., effect size 0).","code":"\nlibrary(TOSTER)"},{"path":"marginal-effects.html","id":"marginal-effects","chapter":"15 Marginal Effects","heading":"15 Marginal Effects","text":"cases without polynomials interactions, can easy interpret marginal effect.example,\\[\nY = \\beta_1 X_1 + \\beta_2 X_2\n\\]\\(\\beta\\) marginal effects.Numerical derivation easier analytical derivation.need choose values variables calculate marginal effect \\(X\\)Analytical derivation\\[\nf'(x) \\equiv \\lim_{h \\0} \\frac{f(x+h) - f(x)}{h}\n\\]E.g., \\(f(x) = X^2\\)\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h} \\\\\n&= \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\\n&= \\frac{2xh + h^2}{h} \\\\\n&= 2x + h \\\\\n&= 2x\n\\end{aligned}\n\\]numerically approach, “just” need find small \\(h\\) plug function. However, also need large enough \\(h\\) numerically accurate computation (Gould, Pitblado, Poi 2010, chap. 1)Numerically approachOne-sided derivative\\[\n\\begin{aligned}\nf'(x) &= \\lim_{h \\0} \\frac{(x+h)^2 - x^2}{h}  \\\\\n& \\approx \\frac{f(x+h) -f(x)}{h}\n\\end{aligned}\n\\]Alternatively, two-sided derivative\\[\nf'_2(x) \\approx \\frac{f(x+h) - f(x- h)}{2h}\n\\]Marginal effects fordiscrete variables (also known incremental effects) change \\(E[Y|X]\\) one unit change \\(X\\)discrete variables (also known incremental effects) change \\(E[Y|X]\\) one unit change \\(X\\)continuous variables change \\(E[Y|X]\\) small changes \\(X\\) (unit changes), ’s derivative, limit \\(h \\0\\)continuous variables change \\(E[Y|X]\\) small changes \\(X\\) (unit changes), ’s derivative, limit \\(h \\0\\)","code":""},{"path":"marginal-effects.html","id":"delta-method","chapter":"15 Marginal Effects","heading":"15.1 Delta Method","text":"approximate mean variance function random variables using first-order Taylor approximationA semi-parametric methodAlternative approaches:\nAnalytically derive probability function margin\nSimulation/Bootstrapping\nAnalytically derive probability function marginAnalytically derive probability function marginSimulation/BootstrappingSimulation/BootstrappingResources:\nAdvanced: modmarg\nIntermediate: UCLA stat\nSimple: Another one\nAdvanced: modmargAdvanced: modmargIntermediate: UCLA statIntermediate: UCLA statSimple: Another oneSimple: Another oneLet \\(G(\\beta)\\) function parameters \\(\\beta\\), \\[\nvar(G(\\beta)) \\approx \\nabla G(\\beta) cov (\\beta) \\nabla G(\\beta)'\n\\]\\(\\nabla G(\\beta)\\) = gradient partial derivatives \\(G(\\beta)\\) (also known Jacobian)","code":""},{"path":"marginal-effects.html","id":"average-marginal-effect-algorithm","chapter":"15 Marginal Effects","heading":"15.2 Average Marginal Effect Algorithm","text":"one-sided derivative \\(\\frac{\\partial p(\\mathbf{X},\\beta)}{\\partial X}\\) probability scaleEstimate modelFor observation \\(\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed values\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\n\nCalculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.\nCalculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{new} = X + h\\))\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) discrete, \\(h = 1\\)\\(X\\) discrete, \\(h = 1\\)Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X\\) variables’ observed vales.Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\)Average numerical derivative \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\)Two-sided derivativesEstimate modelFor observation \\(\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed values\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\n\nCalculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.\nCalculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.\nCalculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)\nCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesCalculate \\(\\hat{Y}_{i0}\\) prediction probability scale using observed valuesIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\n\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\n\\(X\\) discrete, \\(h = 1\\)\nIncrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{1} = X + h\\)) decrease \\(X\\) (variable interest) “small” amount \\(h\\) (\\(X_{2} = X - h\\))\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) \\(\\bar{X}\\) mean value \\(X\\)\\(X\\) discrete, \\(h = 1\\)\\(X\\) discrete, \\(h = 1\\)Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i1}\\) prediction probability scale using new \\(X_1\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.Calculate \\(\\hat{Y}_{i2}\\) prediction probability scale using new \\(X_2\\) variables’ observed vales.Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)Calculate difference two predictions fraction \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\)Average numerical derivative \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\)","code":"\nlibrary(margins)\nlibrary(tidyverse)\n\ndata(\"mtcars\")\nmod <- lm(mpg ~ cyl * disp * hp, data = mtcars)\nmargins::margins(mod) %>% summary()\n#>  factor     AME     SE       z      p    lower   upper\n#>     cyl -4.0592 3.7614 -1.0792 0.2805 -11.4313  3.3130\n#>    disp -0.0350 0.0132 -2.6473 0.0081  -0.0610 -0.0091\n#>      hp -0.0284 0.0185 -1.5348 0.1248  -0.0647  0.0079\n\n# function for variable\nget_mae <- function(mod, var = \"disp\") {\n    data = mod$model\n    \n    pred <- predict(mod)\n    \n    if (class(mod$model[[{\n        {\n            var\n        }\n    }]]) == \"numeric\") {\n        h = (abs(mean(mod$model[[var]])) + 0.01) * 0.01\n    } else {\n        h = 1\n    }\n    \n    data[[{\n        {\n            var\n        }\n    }]] <- data[[{\n{\nvar\n}\n}]] - h\n\n    pred_new <- predict(mod, newdata = data)\n\n    mean(pred - pred_new) / h\n}\n\nget_mae(mod, var = \"disp\")\n#> [1] -0.03504546"},{"path":"marginal-effects.html","id":"packages","chapter":"15 Marginal Effects","heading":"15.3 Packages","text":"","code":""},{"path":"marginal-effects.html","id":"marginaleffects","chapter":"15 Marginal Effects","heading":"15.3.1 MarginalEffects","text":"MarginalEffects package successor margins emtrends (faster, efficient, adaptable). Hence, advocated used.limitation readily function correct multiple comparisons. Hence, one can use p.adjust function overcome disadvantage.Definitions package:Marginal effects slopes derivatives (.e., effect changes variable outcome)\nmargins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”\nMarginal effects slopes derivatives (.e., effect changes variable outcome)margins package defines marginal effects “partial derivatives regression equation respect variable model unit data.”Marginal means averages integrals (.e., marginalizing across rows prediction grid)Marginal means averages integrals (.e., marginalizing across rows prediction grid)customize plot using plot_cme (ggplot class), can check post author MarginalEffects packageCausal inference parametric g-formulaBecause plug-g estimator equivalent average contrast marginaleffects package.get predicted values","code":"\nlibrary(marginaleffects)\nlibrary(tidyverse)\ndata(mtcars)\n\nmod <- lm(mpg ~ hp * wt * am, data = mtcars)\npredictions(mod) %>% head()\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      22.5      0.884 25.4   <0.001 471.7  20.8   24.2\n#>      20.8      1.194 17.4   <0.001 223.3  18.5   23.1\n#>      25.3      0.709 35.7   <0.001 922.7  23.9   26.7\n#>      20.3      0.704 28.8   <0.001 601.5  18.9   21.6\n#>      17.0      0.712 23.9   <0.001 416.2  15.6   18.4\n#>      19.7      0.875 22.5   <0.001 368.8  17.9   21.4\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am\n# for specific reference values\npredictions(mod, newdata = datagrid(am = 0, wt = c(2, 4)))\n#> \n#>  am wt Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp\n#>   0  2     22.0       2.04 10.8   <0.001  87.4  18.0   26.0 147\n#>   0  4     16.6       1.08 15.3   <0.001 173.8  14.5   18.7 147\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt\nplot_cap(mod, condition = c(\"hp\",\"wt\"))\n# Average Margianl Effects\nmfx <- marginaleffects(mod, variables = c(\"hp\", \"wt\"))\nsummary(mfx)\n#> \n#>  Term    Contrast Estimate Std. Error     z Pr(>|z|)   2.5 % 97.5 %\n#>    hp mean(dY/dX)  -0.0381     0.0128 -2.98  0.00291 -0.0631 -0.013\n#>    wt mean(dY/dX)  -3.9391     1.0858 -3.63  < 0.001 -6.0672 -1.811\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n# Group-Average Marginal Effects\nmarginaleffects::marginaleffects(mod, by = \"hp\", variables = \"am\")\n#> \n#>  Term          Contrast  hp Estimate Std. Error      z Pr(>|z|)   S  2.5 %\n#>    am mean(1) - mean(0)  52    3.976       5.20  0.764    0.445 1.2  -6.22\n#>    am mean(1) - mean(0)  62   -2.774       2.51 -1.107    0.268 1.9  -7.68\n#>    am mean(1) - mean(0)  65    2.999       4.13  0.725    0.468 1.1  -5.10\n#>    am mean(1) - mean(0)  66    2.025       3.48  0.582    0.561 0.8  -4.80\n#>    am mean(1) - mean(0)  91    1.858       2.76  0.674    0.500 1.0  -3.54\n#>    am mean(1) - mean(0)  93    1.201       2.35  0.511    0.609 0.7  -3.40\n#>    am mean(1) - mean(0)  95   -1.832       1.97 -0.931    0.352 1.5  -5.69\n#>    am mean(1) - mean(0)  97    0.708       2.04  0.347    0.728 0.5  -3.28\n#>    am mean(1) - mean(0) 105   -2.682       2.37 -1.132    0.258 2.0  -7.32\n#>    am mean(1) - mean(0) 109   -0.237       1.59 -0.149    0.881 0.2  -3.35\n#>    am mean(1) - mean(0) 110   -0.640       1.57 -0.407    0.684 0.5  -3.73\n#>    am mean(1) - mean(0) 113    4.081       3.94  1.037    0.300 1.7  -3.63\n#>    am mean(1) - mean(0) 123   -2.098       2.10 -0.998    0.318 1.7  -6.22\n#>    am mean(1) - mean(0) 150   -1.429       1.90 -0.753    0.452 1.1  -5.15\n#>    am mean(1) - mean(0) 175   -0.416       1.56 -0.266    0.790 0.3  -3.48\n#>    am mean(1) - mean(0) 180   -1.381       2.47 -0.560    0.576 0.8  -6.22\n#>    am mean(1) - mean(0) 205   -2.873       6.24 -0.460    0.645 0.6 -15.11\n#>    am mean(1) - mean(0) 215   -2.534       6.95 -0.364    0.716 0.5 -16.16\n#>    am mean(1) - mean(0) 230   -1.477       7.07 -0.209    0.835 0.3 -15.34\n#>    am mean(1) - mean(0) 245    1.115       2.28  0.488    0.625 0.7  -3.36\n#>    am mean(1) - mean(0) 264    2.106       2.29  0.920    0.358 1.5  -2.38\n#>    am mean(1) - mean(0) 335    4.027       3.24  1.243    0.214 2.2  -2.32\n#>  97.5 %\n#>   14.18\n#>    2.14\n#>   11.10\n#>    8.85\n#>    7.26\n#>    5.80\n#>    2.02\n#>    4.70\n#>    1.96\n#>    2.87\n#>    2.45\n#>   11.79\n#>    2.02\n#>    2.29\n#>    2.64\n#>    3.46\n#>    9.36\n#>   11.09\n#>   12.39\n#>    5.59\n#>    6.59\n#>   10.38\n#> \n#> Columns: term, contrast, hp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted\n\n# Marginal effects at representative values\nmarginaleffects::marginaleffects(mod, \n                                 newdata = datagrid(am = 0, \n                                                    wt = c(2, 4)))\n#> \n#>  Term Contrast am wt Estimate Std. Error      z Pr(>|z|)   S   2.5 %   97.5 %\n#>    am    1 - 0  0  2   2.5465     2.7860  0.914   0.3607 1.5 -2.9139  8.00694\n#>    am    1 - 0  0  4  -2.9661     3.0381 -0.976   0.3289 1.6 -8.9207  2.98852\n#>    hp    dY/dX  0  2  -0.0598     0.0283 -2.115   0.0344 4.9 -0.1153 -0.00439\n#>    hp    dY/dX  0  4  -0.0309     0.0187 -1.654   0.0981 3.3 -0.0676  0.00572\n#>    wt    dY/dX  0  2  -2.6762     1.4194 -1.885   0.0594 4.1 -5.4582  0.10587\n#>    wt    dY/dX  0  4  -2.6762     1.4199 -1.885   0.0595 4.1 -5.4591  0.10676\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, predicted_lo, predicted_hi, predicted, mpg, hp\n\n# Marginal Effects at the Mean\nmarginaleffects::marginaleffects(mod, newdata = \"mean\")\n#> \n#>  Term Contrast Estimate Std. Error      z Pr(>|z|)    S  2.5 %  97.5 %\n#>    am    1 - 0  -0.8086    1.52383 -0.531  0.59568  0.7 -3.795  2.1781\n#>    hp    dY/dX  -0.0323    0.00956 -3.375  < 0.001 10.4 -0.051 -0.0135\n#>    wt    dY/dX  -3.7959    1.21310 -3.129  0.00175  9.2 -6.174 -1.4183\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am\n# counterfactual\ncomparisons(mod, variables = list(am = 0:1)) %>% summary()\n#> \n#>  Term          Contrast Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %\n#>    am mean(1) - mean(0)  -0.0481       1.85 -0.026    0.979 -3.68   3.58\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"marginal-effects.html","id":"margins","chapter":"15 Marginal Effects","heading":"15.3.2 margins","text":"Marginal effects partial derivative regression equation respect variable model unit dataAverage Partial Effects: contribution variable outcome scale, conditional variables involved link function transformation linear predictorAverage Partial Effects: contribution variable outcome scale, conditional variables involved link function transformation linear predictorAverage Marginal Effects: marginal contribution variable scale linear predictor.Average Marginal Effects: marginal contribution variable scale linear predictor.Average marginal effects mean unit-specific partial derivatives sampleAverage marginal effects mean unit-specific partial derivatives samplemargins package gives marginal effects models (replication margins command Stata).prediction package gives unit-specific sample average predictions models (similar predictive margins Stata).cases interaction polynomial terms, coefficient estimates interpreted marginal effects X Y. Hence, want know average marginal effects variable thenMarginal effects mean (MEM):Marginal effects mean values sampleFor discrete variables, ’s called average discrete change (ADC)Average Marginal Effect (AME)average marginal effects value sampleMarginal Effects representative values (MER)","code":"\nlibrary(margins)\n\n# examples by the package's authors\nmod <- lm(mpg ~ cyl * hp + wt, data = mtcars)\nsummary(mod)\n#> \n#> Call:\n#> lm(formula = mpg ~ cyl * hp + wt, data = mtcars)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.3440 -1.4144 -0.6166  1.2160  4.2815 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 52.017520   4.916935  10.579 4.18e-11 ***\n#> cyl         -2.742125   0.800228  -3.427  0.00197 ** \n#> hp          -0.163594   0.052122  -3.139  0.00408 ** \n#> wt          -3.119815   0.661322  -4.718 6.51e-05 ***\n#> cyl:hp       0.018954   0.006645   2.852  0.00823 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.242 on 27 degrees of freedom\n#> Multiple R-squared:  0.8795, Adjusted R-squared:  0.8616 \n#> F-statistic: 49.25 on 4 and 27 DF,  p-value: 5.065e-12\nsummary(margins(mod))\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236\n\n# equivalently \nmargins_summary(mod)\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236\n\nplot(margins(mod))\nmargins(mod, at = list(hp = 150))\n#>  at(hp)    cyl       hp    wt\n#>     150 0.1009 -0.04632 -3.12\n\nmargins(mod, at = list(hp = 150)) %>% summary()\n#>  factor       hp     AME     SE       z      p   lower   upper\n#>     cyl 150.0000  0.1009 0.6128  0.1647 0.8692 -1.1001  1.3019\n#>      hp 150.0000 -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179\n#>      wt 150.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236"},{"path":"marginal-effects.html","id":"mfx","chapter":"15 Marginal Effects","heading":"15.3.3 mfx","text":"Works well Generalized Linear Models/glm packageFor technical details, see package vignetteThis package can give marginal effect variable glm model, average marginal effect might look .","code":"\nlibrary(mfx)\ndata(\"mtcars\")\npoissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> Call:\n#> poissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars)\n#> \n#> Marginal Effects:\n#>                    dF/dx   Std. Err.       z  P>|z|\n#> mpg           1.4722e-03  8.7531e-03  0.1682 0.8664\n#> cyl           6.6420e-03  3.9263e-02  0.1692 0.8657\n#> disp          1.5899e-04  9.4555e-04  0.1681 0.8665\n#> mpg:cyl      -3.4698e-04  2.0564e-03 -0.1687 0.8660\n#> mpg:disp     -7.6794e-06  4.5545e-05 -0.1686 0.8661\n#> cyl:disp     -3.3837e-05  1.9919e-04 -0.1699 0.8651\n#> mpg:cyl:disp  1.6812e-06  9.8919e-06  0.1700 0.8650"},{"path":"prediction-and-estimation.html","id":"prediction-and-estimation","chapter":"16 Prediction and Estimation","heading":"16 Prediction and Estimation","text":"Prediction Estimation (Causal Inference) serve distinct roles understanding modeling data.","code":""},{"path":"prediction-and-estimation.html","id":"prediction-1","chapter":"16 Prediction and Estimation","heading":"16.1 Prediction","text":"Definition: Prediction, denoted \\(\\hat{y}\\), creating algorithm predicting outcome variable \\(y\\) predictors \\(x\\).Definition: Prediction, denoted \\(\\hat{y}\\), creating algorithm predicting outcome variable \\(y\\) predictors \\(x\\).Goal: primary goal loss minimization, aiming model accuracy unseen data:\n\\[\n\\hat{f} \\approx \\min E_{(y,x)} L(f(x), y)\n\\]Goal: primary goal loss minimization, aiming model accuracy unseen data:\\[\n\\hat{f} \\approx \\min E_{(y,x)} L(f(x), y)\n\\]Applications Economics:\nMeasure variables.\nEmbed prediction tasks within parameter estimation treatment effects.\nControl observed confounders.\nApplications Economics:Measure variables.Embed prediction tasks within parameter estimation treatment effects.Control observed confounders.","code":""},{"path":"prediction-and-estimation.html","id":"parameter-estimation","chapter":"16 Prediction and Estimation","heading":"16.2 Parameter Estimation","text":"Definition: Parameter estimation, represented \\(\\hat{\\beta}\\), focuses estimating relationship \\(y\\) \\(x\\).Definition: Parameter estimation, represented \\(\\hat{\\beta}\\), focuses estimating relationship \\(y\\) \\(x\\).Goal: aim consistency, ensuring models perform well training data:\n\\[\nE[\\hat{f}] = f\n\\]Goal: aim consistency, ensuring models perform well training data:\\[\nE[\\hat{f}] = f\n\\]Challenges:\nHigh-dimensional spaces can lead covariance among variables multicollinearity.\nleads bias-variance tradeoff (Hastie et al. 2009).\nChallenges:High-dimensional spaces can lead covariance among variables multicollinearity.leads bias-variance tradeoff (Hastie et al. 2009).","code":""},{"path":"prediction-and-estimation.html","id":"causation-versus-prediction","chapter":"16 Prediction and Estimation","heading":"16.3 Causation versus Prediction","text":"Understanding relationship causation prediction crucial statistical modeling.Let \\(Y\\) outcome variable dependent \\(X\\), aim manipulate \\(X\\) maximize payoff function \\(\\pi(X, Y)\\) (Kleinberg et al. 2015). decision \\(X\\) hinges :\\[\n\\begin{aligned}\n\\frac{d\\pi(X, Y)}{d X} &= \\frac{\\partial \\pi}{\\partial X} (Y) + \\frac{\\partial \\pi}{\\partial Y} \\frac{\\partial Y}{\\partial X} \\\\\n&= \\frac{\\partial \\pi}{\\partial X} \\text{(Prediction)} + \\frac{\\partial \\pi}{\\partial Y} \\text{(Causation)}\n\\end{aligned}\n\\]Empirical work essential estimating derivatives equation:\\(\\frac{\\partial Y}{\\partial X}\\) required causal inference determine \\(X\\)’s effect \\(Y\\),\\(\\frac{\\partial Y}{\\partial X}\\) required causal inference determine \\(X\\)’s effect \\(Y\\),\\(\\frac{\\partial \\pi}{\\partial X}\\) required prediction \\(Y\\).\\(\\frac{\\partial \\pi}{\\partial X}\\) required prediction \\(Y\\).","code":""},{"path":"moderation.html","id":"moderation","chapter":"17 Moderation","heading":"17 Moderation","text":"Spotlight Analysis: Compare mean dependent two groups (treatment control) every value (Simple Slopes Analysis)Floodlight Analysis: spotlight analysis whole range moderator (Johnson-Neyman intervals)Resources:BANOVAL : floodlight analysis Bayesian ANOVA modelsBANOVAL : floodlight analysis Bayesian ANOVA modelscSEM : doFloodlightAnalysis SEM modelcSEM : doFloodlightAnalysis SEM model(Spiller et al. 2013)(Spiller et al. 2013)Terminology:Main effects (slopes): coefficients involve interaction termsMain effects (slopes): coefficients involve interaction termsSimple slope: continuous independent variable interact moderating variable, slope particular level moderating variableSimple slope: continuous independent variable interact moderating variable, slope particular level moderating variableSimple effect: categorical independent variable interacts moderating variable, effect particular level moderating variable.Simple effect: categorical independent variable interacts moderating variable, effect particular level moderating variable.Example:\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 M + \\beta_3 X \\times M\n\\]\\(\\beta_0\\) = intercept\\(\\beta_0\\) = intercept\\(\\beta_1\\) = simple effect (slope) \\(X\\) (independent variable)\\(\\beta_1\\) = simple effect (slope) \\(X\\) (independent variable)\\(\\beta_2\\) = simple effect (slope) \\(M\\) (moderating variable)\\(\\beta_2\\) = simple effect (slope) \\(M\\) (moderating variable)\\(\\beta_3\\) = interaction \\(X\\) \\(M\\)\\(\\beta_3\\) = interaction \\(X\\) \\(M\\)Three types interactions:Continuous continuousContinuous categoricalCategorical categoricalWhen interpreting three-way interactions, one can use slope difference test (Dawson Richter 2006)","code":""},{"path":"moderation.html","id":"emmeans-package","chapter":"17 Moderation","heading":"17.1 emmeans package","text":"Data set UCLA seminar gender prog categorical","code":"\ninstall.packages(\"emmeans\")\nlibrary(emmeans)\ndat <- readRDS(\"data/exercise.rds\") %>%\n    mutate(prog = factor(prog, labels = c(\"jog\", \"swim\", \"read\"))) %>%\n    mutate(gender = factor(gender, labels = c(\"male\", \"female\")))"},{"path":"moderation.html","id":"continuous-by-continuous","chapter":"17 Moderation","heading":"17.1.1 Continuous by continuous","text":"Simple slopes continuous continuous modelSpotlight analysis (Aiken West 2005): usually pick 3 values moderating variable:Mean Moderating Variable + \\(\\sigma \\times\\) (Moderating variable)Mean Moderating Variable + \\(\\sigma \\times\\) (Moderating variable)Mean Moderating VariableMean Moderating VariableMean Moderating Variable - \\(\\sigma \\times\\) (Moderating variable)Mean Moderating Variable - \\(\\sigma \\times\\) (Moderating variable)3 p-values interaction term.publication, use","code":"\ncontcont <- lm(loss~hours*effort,data=dat)\nsummary(contcont)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * effort, data = dat)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -29.52 -10.60  -1.78  11.13  34.51 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   7.79864   11.60362   0.672   0.5017  \n#> hours        -9.37568    5.66392  -1.655   0.0982 .\n#> effort       -0.08028    0.38465  -0.209   0.8347  \n#> hours:effort  0.39335    0.18750   2.098   0.0362 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 13.56 on 896 degrees of freedom\n#> Multiple R-squared:  0.07818,    Adjusted R-squared:  0.07509 \n#> F-statistic: 25.33 on 3 and 896 DF,  p-value: 9.826e-16\neffar <- round(mean(dat$effort) + sd(dat$effort), 1)\neffr  <- round(mean(dat$effort), 1)\neffbr <- round(mean(dat$effort) - sd(dat$effort), 1)\n# specify list of points\nmylist <- list(effort = c(effbr, effr, effar))\n\n# get the estimates\nemtrends(contcont, ~ effort, var = \"hours\", at = mylist)\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.352 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.308 896    1.745     6.88\n#> \n#> Confidence level used: 0.95\n\n# plot\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\nemmip(contcont, effort ~ hours, at = mylist, CIs = TRUE)\n\n# statistical test for slope difference\nemtrends(\n    contcont,\n    pairwise ~ effort,\n    var = \"hours\",\n    at = mylist,\n    adjust = \"none\"\n)\n#> $emtrends\n#>  effort hours.trend    SE  df lower.CL upper.CL\n#>    24.5       0.261 1.352 896   -2.392     2.91\n#>    29.7       2.307 0.915 896    0.511     4.10\n#>    34.8       4.313 1.308 896    1.745     6.88\n#> \n#> Results are averaged over the levels of: hours \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast                estimate    SE  df t.ratio p.value\n#>  effort24.5 - effort29.7    -2.05 0.975 896  -2.098  0.0362\n#>  effort24.5 - effort34.8    -4.05 1.931 896  -2.098  0.0362\n#>  effort29.7 - effort34.8    -2.01 0.956 896  -2.098  0.0362\n#> \n#> Results are averaged over the levels of: hours\nlibrary(ggplot2)\n\n# data\nmylist <- list(hours = seq(0, 4, by = 0.4),\n               effort = c(effbr, effr, effar))\ncontcontdat <-\n    emmip(contcont,\n          effort ~ hours,\n          at = mylist,\n          CIs = TRUE,\n          plotit = FALSE)\ncontcontdat$feffort <- factor(contcontdat$effort)\nlevels(contcontdat$feffort) <- c(\"low\", \"med\", \"high\")\n\n# plot\np  <-\n    ggplot(data = contcontdat, \n           aes(x = hours, y = yvar, color = feffort)) +  \n    geom_line()\np1 <-\n    p + \n    geom_ribbon(aes(ymax = UCL, ymin = LCL, fill = feffort), \n                    alpha = 0.4)\np1  + labs(x = \"Hours\",\n           y = \"Weight Loss\",\n           color = \"Effort\",\n           fill = \"Effort\")"},{"path":"moderation.html","id":"continuous-by-categorical","chapter":"17 Moderation","heading":"17.1.2 Continuous by categorical","text":"Get simple slopes level categorical moderator","code":"\n# use Female as basline\ndat$gender <- relevel(dat$gender, ref = \"female\")\n\ncontcat <- lm(loss ~ hours * gender, data = dat)\nsummary(contcat)\n#> \n#> Call:\n#> lm(formula = loss ~ hours * gender, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -27.118 -11.350  -1.963  10.001  42.376 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)         3.335      2.731   1.221    0.222  \n#> hours               3.315      1.332   2.489    0.013 *\n#> gendermale          3.571      3.915   0.912    0.362  \n#> hours:gendermale   -1.724      1.898  -0.908    0.364  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 14.06 on 896 degrees of freedom\n#> Multiple R-squared:  0.008433,   Adjusted R-squared:  0.005113 \n#> F-statistic:  2.54 on 3 and 896 DF,  p-value: 0.05523\nemtrends(contcat, ~ gender, var = \"hours\")\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95\n\n# test difference in slopes\nemtrends(contcat, pairwise ~ gender, var = \"hours\")\n#> $emtrends\n#>  gender hours.trend   SE  df lower.CL upper.CL\n#>  female        3.32 1.33 896    0.702     5.93\n#>  male          1.59 1.35 896   -1.063     4.25\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast      estimate  SE  df t.ratio p.value\n#>  female - male     1.72 1.9 896   0.908  0.3639\n# which is the same as the interaction term\n# plot\n(mylist <- list(\n    hours = seq(0, 4, by = 0.4),\n    gender = c(\"female\", \"male\")\n))\n#> $hours\n#>  [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0\n#> \n#> $gender\n#> [1] \"female\" \"male\"\nemmip(contcat, gender ~ hours, at = mylist, CIs = TRUE)"},{"path":"moderation.html","id":"categorical-by-categorical","chapter":"17 Moderation","heading":"17.1.3 Categorical by categorical","text":"Simple effectsPlotBar graph","code":"\n# relevel baseline\ndat$prog   <- relevel(dat$prog, ref = \"read\")\ndat$gender <- relevel(dat$gender, ref = \"female\")\ncatcat <- lm(loss ~ gender * prog, data = dat)\nsummary(catcat)\n#> \n#> Call:\n#> lm(formula = loss ~ gender * prog, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -19.1723  -4.1894  -0.0994   3.7506  27.6939 \n#> \n#> Coefficients:\n#>                     Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          -3.6201     0.5322  -6.802 1.89e-11 ***\n#> gendermale           -0.3355     0.7527  -0.446    0.656    \n#> progjog               7.9088     0.7527  10.507  < 2e-16 ***\n#> progswim             32.7378     0.7527  43.494  < 2e-16 ***\n#> gendermale:progjog    7.8188     1.0645   7.345 4.63e-13 ***\n#> gendermale:progswim  -6.2599     1.0645  -5.881 5.77e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.519 on 894 degrees of freedom\n#> Multiple R-squared:  0.7875, Adjusted R-squared:  0.7863 \n#> F-statistic: 662.5 on 5 and 894 DF,  p-value: < 2.2e-16\nemcatcat <- emmeans(catcat, ~ gender*prog)\n\n# differences in predicted values\ncontrast(emcatcat, \n         \"revpairwise\", \n         by = \"prog\", \n         adjust = \"bonferroni\")\n#> prog = read:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -0.335 0.753 894  -0.446  0.6559\n#> \n#> prog = jog:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female    7.483 0.753 894   9.942  <.0001\n#> \n#> prog = swim:\n#>  contrast      estimate    SE  df t.ratio p.value\n#>  male - female   -6.595 0.753 894  -8.762  <.0001\nemmip(catcat, prog ~ gender,CIs=TRUE)\ncatcatdat <- emmip(catcat,\n                   gender ~ prog,\n                   CIs = TRUE,\n                   plotit = FALSE)\np <-\n    ggplot(data = catcatdat,\n           aes(x = prog, y = yvar, fill = gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")\n\np1 <-\n    p + geom_errorbar(\n        position = position_dodge(.9),\n        width = .25,\n        aes(ymax = UCL, ymin = LCL),\n        alpha = 0.3\n    )\np1  + labs(x = \"Program\", y = \"Weight Loss\", fill = \"Gender\")"},{"path":"moderation.html","id":"probmod-package","chapter":"17 Moderation","heading":"17.2 probmod package","text":"recommend: package serious problem subscript.","code":"\ninstall.packages(\"probemod\")\nlibrary(probemod)\n\nmyModel <-\n    lm(loss ~ hours * gender, data = dat %>% \n           select(loss, hours, gender))\njnresults <- jn(myModel,\n                dv = 'loss',\n                iv = 'hours',\n                mod = 'gender')\n\n\npickapoint(\n    myModel,\n    dv = 'loss',\n    iv = 'hours',\n    mod = 'gender',\n    alpha = .01\n)\n\nplot(jnresults)"},{"path":"moderation.html","id":"interactions-package","chapter":"17 Moderation","heading":"17.3 interactions package","text":"Recommend","code":"\ninstall.packages(\"interactions\")"},{"path":"moderation.html","id":"continuous-interaction","chapter":"17 Moderation","heading":"17.3.1 Continuous interaction","text":"(least one two variables continuous)continuous moderator, three values chosen :-1 SD mean-1 SD meanThe meanThe mean-1 SD mean-1 SD meanTo include weights regression inn plotPartial Effect PlotCheck linearity assumption modelPlot lines based subsample (red line), whole sample (black line)","code":"\nlibrary(interactions)\nlibrary(jtools) # for summ()\nstates <- as.data.frame(state.x77)\nfiti <- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)\nsumm(fiti)\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              \n              # if you don't want the plot to mean-center\n              # centered = \"none\", \n              \n              # exclude the mean value of the moderator\n              # modx.values = \"plus-minus\", \n              \n              # split moderator's distribution into 3 groups\n              # modx.values = \"terciles\" \n              \n              plot.points = T, # overlay data\n              \n              \n              # different shape for differennt levels of the moderator\n              point.shape = T, \n              \n              # if two data points are on top one another, \n              # this moves them apart by little\n              jitter = 0.1, \n              \n              # other appearance option\n              x.label = \"X label\", \n              y.label = \"Y label\",\n              main.title = \"Title\",\n              legend.main = \"Legend Title\",\n              colors = \"blue\",\n              \n              # include confidence band\n              interval = TRUE, \n              int.width = 0.9, \n              robust = TRUE # use robust SE\n              ) \nfiti <- lm(Income ~ Illiteracy * Murder,\n           data = states,\n           weights = Population)\n\ninteract_plot(fiti,\n              pred = Illiteracy,\n              modx = Murder,\n              plot.points = TRUE)\nlibrary(ggplot2)\ndata(cars)\nfitc <- lm(cty ~ year + cyl * displ + class + fl + drv, \n           data = mpg)\nsumm(fitc)\n\ninteract_plot(\n    fitc,\n    pred = displ,\n    modx = cyl,\n    # the observed data is based on displ, cyl, and model error\n    partial.residuals = TRUE, \n    modx.values = c(4, 5, 6, 8)\n)\nx_2 <- runif(n = 200, min = -3, max = 3)\nw   <- rbinom(n = 200, size = 1, prob = 0.5)\nerr <- rnorm(n = 200, mean = 0, sd = 4)\ny_2 <- 2.5 - x_2 ^ 2 - 5 * w + 2 * w * (x_2 ^ 2) + err\n\ndata_2 <- as.data.frame(cbind(x_2, y_2, w))\n\nmodel_2 <- lm(y_2 ~ x_2 * w, data = data_2)\nsumm(model_2)\ninteract_plot(\n    model_2,\n    pred = x_2,\n    modx = w,\n    linearity.check = TRUE,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"simple-slopes-analysis","chapter":"17 Moderation","heading":"17.3.1.1 Simple Slopes Analysis","text":"continuous continuous variable interaction (still work binary)continuous continuous variable interaction (still work binary)conditional slope variable interest (.e., slope \\(X\\) hold \\(M\\) constant value)conditional slope variable interest (.e., slope \\(X\\) hold \\(M\\) constant value)Using sim_slopes willmean-center variables except variable interestmean-center variables except variable interestFor moderator \nContinuous, pick mean, plus/minus 1 SD\nCategorical, use factor\nmoderator isContinuous, pick mean, plus/minus 1 SDContinuous, pick mean, plus/minus 1 SDCategorical, use factorCategorical, use factorsim_slopes requiresA regression model interaction term)regression model interaction term)Variable interest (pred =)Variable interest (pred =)Moderator: (modx =)Moderator: (modx =)Table 17.1:  ","code":"\nsim_slopes(fiti,\n           pred = Illiteracy,\n           modx = Murder,\n           johnson_neyman = FALSE)\n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -71.59   268.65    -0.27   0.79\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -437.12   175.82    -2.49   0.02\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -802.66   145.72    -5.51   0.00\n\n# plot the coefficients\nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\nplot(ss)\n\n# table \nss <- sim_slopes(fiti,\n                 pred = Illiteracy,\n                 modx = Murder,\n                 modx.values = c(0, 5, 10))\nlibrary(huxtable)\nas_huxtable(ss)"},{"path":"moderation.html","id":"johnson-neyman-intervals","chapter":"17 Moderation","heading":"17.3.1.2 Johnson-Neyman intervals","text":"know values moderator slope variable interest statistically significant, can use Johnson-Neyman interval (P. O. Johnson Neyman 1936)Even though kind know alpha level implementing Johnson-Neyman interval correct (Bauer Curran 2005), recently correction type II errors (Esarey Sumner 2018).Since Johnson-Neyman inflates type error (comparisons across values moderator)plotting, can use johnson_neymanNote:y-axis conditional slope variable interest","code":"\nsim_slopes(\n    fiti,\n    pred = Illiteracy,\n    modx = Murder,\n    johnson_neyman = TRUE,\n    control.fdr = TRUE,\n    # correction for type I and II\n    \n    # include conditional intecepts\n    # cond.int = TRUE, \n    \n    robust = \"HC3\",\n    # rubust SE\n    \n    # don't mean-centered non-focal variables\n    # centered = \"none\",\n    jnalpha = 0.05\n)\n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When Murder is OUTSIDE the interval [-11.70, 8.75], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.33 \n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of Illiteracy when Murder =  5.420973 (- 1 SD): \n#> \n#>     Est.     S.E.   t val.      p\n#> -------- -------- -------- ------\n#>   -71.59   256.60    -0.28   0.78\n#> \n#> Slope of Illiteracy when Murder =  8.685043 (Mean): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -437.12   191.07    -2.29   0.03\n#> \n#> Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): \n#> \n#>      Est.     S.E.   t val.      p\n#> --------- -------- -------- ------\n#>   -802.66   178.75    -4.49   0.00\njohnson_neyman(fiti,\n               pred = Illiteracy,\n               modx = Murder,\n               \n               # correction for type I and II\n               control.fdr = TRUE, \n               alpha = .05)\n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When Murder is OUTSIDE the interval [-22.57, 8.52], the slope of Illiteracy\n#> is p < .05.\n#> \n#> Note: The range of observed values of Murder is [1.40, 15.10]\n#> \n#> Interval calculated using false discovery rate adjusted t = 2.33"},{"path":"moderation.html","id":"way-interaction","chapter":"17 Moderation","heading":"17.3.1.3 3-way interaction","text":"Johnson-Neyman 3-way interactionReportTable 17.2:  ","code":"\n# fita3 <-\n#     lm(rating ~ privileges * critical * learning, \n#        data = attitude)\n# \n# probe_interaction(\n#     fita3,\n#     pred = critical,\n#     modx = learning,\n#     mod2 = privileges,\n#     alpha = .1\n# )\n\n\nmtcars$cyl <- factor(mtcars$cyl,\n                     labels = c(\"4 cylinder\", \"6 cylinder\", \"8 cylinder\"))\nfitc3 <- lm(mpg ~ hp * wt * cyl, data = mtcars)\ninteract_plot(fitc3,\n              pred = hp,\n              modx = wt,\n              mod2 = cyl) +\n    theme_apa(legend.pos = \"bottomright\")\nlibrary(survey)\ndata(api)\n\ndstrat <- svydesign(\n    id = ~ 1,\n    strata = ~ stype,\n    weights = ~ pw,\n    data = apistrat,\n    fpc = ~ fpc\n)\n\nregmodel3 <-\n    survey::svyglm(api00 ~ avg.ed * growth * enroll, design = dstrat)\n\nsim_slopes(\n    regmodel3,\n    pred = growth,\n    modx = avg.ed,\n    mod2 = enroll,\n    jnplot = TRUE\n)\n#> ███████████████ While enroll (2nd moderator) =  153.0518 (- 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   1.25   0.32     3.86   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.39   0.22     1.75   0.08\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.48   0.35    -1.37   0.17\n#> \n#> ████████████████ While enroll (2nd moderator) =  595.2821 (Mean) ███████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p\n#> < .05.\n#> \n#> Note: The range of observed values of avg.ed is [1.38, 4.44]\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.72   0.22     3.29   0.00\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.34   0.16     2.16   0.03\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>    Est.   S.E.   t val.      p\n#> ------- ------ -------- ------\n#>   -0.04   0.24    -0.16   0.87\n#> \n#> ███████████████ While enroll (2nd moderator) = 1037.5125 (+ 1 SD) ██████████████ \n#> \n#> JOHNSON-NEYMAN INTERVAL \n#> \n#> The Johnson-Neyman interval could not be found. Is the p value for your\n#> interaction term below the specified alpha?\n#> \n#> SIMPLE SLOPES ANALYSIS \n#> \n#> Slope of growth when avg.ed = 2.085002 (- 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.18   0.31     0.58   0.56\n#> \n#> Slope of growth when avg.ed = 2.787381 (Mean): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.29   0.20     1.49   0.14\n#> \n#> Slope of growth when avg.ed = 3.489761 (+ 1 SD): \n#> \n#>   Est.   S.E.   t val.      p\n#> ------ ------ -------- ------\n#>   0.40   0.27     1.49   0.14\nss3 <-\n    sim_slopes(regmodel3,\n               pred = growth,\n               modx = avg.ed,\n               mod2 = enroll)\nplot(ss3)\nas_huxtable(ss3)"},{"path":"moderation.html","id":"categorical-interaction","chapter":"17 Moderation","heading":"17.3.2 Categorical interaction","text":"","code":"\nlibrary(ggplot2)\nmpg2 <- mpg %>% \n    mutate(cyl = factor(cyl))\n\nmpg2[\"auto\"] <- \"auto\"\nmpg2$auto[mpg2$trans %in% c(\"manual(m5)\", \"manual(m6)\")] <- \"manual\"\nmpg2$auto <- factor(mpg2$auto)\nmpg2[\"fwd\"] <- \"2wd\"\nmpg2$fwd[mpg2$drv == \"4\"] <- \"4wd\"\nmpg2$fwd <- factor(mpg2$fwd)\n## Drop the two cars with 5 cylinders (rest are 4, 6, or 8)\nmpg2 <- mpg2[mpg2$cyl != \"5\", ]\n## Fit the model\nfit3 <- lm(cty ~ cyl * fwd * auto, data = mpg2)\n\nlibrary(jtools) # for summ()\nsumm(fit3)\ncat_plot(fit3,\n         pred = cyl,\n         modx = fwd,\n         plot.points = T)\n#line plots\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"line\",\n    point.shape = TRUE,\n    # colors = \"Set2\", # choose color\n    vary.lty = TRUE\n)\n\n\n# bar plot\ncat_plot(\n    fit3,\n    pred = cyl,\n    modx = fwd,\n    geom = \"bar\",\n    interval = T,\n    plot.points = TRUE\n)"},{"path":"moderation.html","id":"interactionr-package","chapter":"17 Moderation","heading":"17.4 interactionR package","text":"publication purposesFollowing\n(Knol VanderWeele 2012) presentation\n(Hosmer Lemeshow 1992) confidence intervals based delta method\n(Zou 2008) variance recovery “mover” method\n(Assmann et al. 1996) bootstrapping\n(Knol VanderWeele 2012) presentation(Knol VanderWeele 2012) presentation(Hosmer Lemeshow 1992) confidence intervals based delta method(Hosmer Lemeshow 1992) confidence intervals based delta method(Zou 2008) variance recovery “mover” method(Zou 2008) variance recovery “mover” method(Assmann et al. 1996) bootstrapping(Assmann et al. 1996) bootstrapping","code":"\ninstall.packages(\"interactionR\")"},{"path":"moderation.html","id":"sjplot-package","chapter":"17 Moderation","heading":"17.5 sjPlot package","text":"publication purposes (recommend, advanced)publication purposes (recommend, advanced)linklink","code":""},{"path":"causal-inference.html","id":"causal-inference","chapter":"18 Causal Inference","heading":"18 Causal Inference","text":"mambo jumbo learned far, want now talk concept causality. usually say correlation causation. , causation?\nOne favorite books explained concept beautifully (Pearl Mackenzie 2018). just going quickly summarize gist understanding. hope can give initial grasp concept later can continue read develop deeper understanding.’s important deep understanding regarding method research. However, one needs aware limitation. mentioned various sections throughout book, see need ask experts number baseline visit literature gain insight past research., dive conceptual side statistical analysis whole, regardless particular approach.probably heard scientists say correlation doesn’t mean causation. ridiculous spurious correlations give firm grip previous phrase means. pioneer tried use regression infer causation social science Yule (1899) (fatal attempt found relief policy increases poverty). make causal inference statistics, equation (function form) must stable intervention (.e., variables manipulated). Statistics used causality-free enterprise past.development path analysis Sewall Wright 1920s discipline started pay attention causation. , remained dormant Causal Revolution (quoted Judea Pearl’s words). revolution introduced calculus causation includes (1) causal diagrams), (2) symbolic languageThe world using \\(P(Y|X)\\) (statistics use derive ), want compare difference \\(P(Y|(X))\\): treatment group\\(P(Y|(X))\\): treatment group\\(P(Y|(-X))\\): control group\\(P(Y|(-X))\\): control groupHence, can see clear difference \\(P(Y|X) \\neq P(Y|(X))\\)conclusion want make data counterfactuals: happened X?teach robot make inference, need inference engineLevels cognitive ability causal learner:SeeingDoingImaginingLadder causation (associated levels cognitive ability well):Association: conditional probability, correlation, regressionInterventionCounterfactualsAssociation\\(P(y|x)\\)?seeing X change belief Y?Intervention\\(P(y|(x),z)\\)DoingInterveningWhat ?X?Counterfactuals\\(P(y_x|x',y')\\)?\nX caused Y?acted differentlyTable (Pearl 2019, 57)define causation probability aloneIf say X causes Y X raises probability Y.” surface, might sound intuitively right. translate probability notation: \\(P(Y|X) >P(Y)\\) , can’t wrong. Just seeing X (1st level), doesn’t mean probability Y increases.either (1) X causes Y, (2) Z affects X Y. Hence, people might use control variables, translate: \\(P(Y|X, Z=z) > P(Y|Z=z)\\), can confident probabilistic observation. However, question can choose \\(Z\\)invention -operator, now can represent X causes Y \\[\nP(Y|(X)) > P(Y)\n\\]help causal diagram, now can answer questions 2nd level (Intervention)Note: people econometrics might still use “Granger causality” “vector autoregression” use probability language represent causality (’s ).7 tools Structural Causal Model framework (Pearl 2019):Encoding Causal Assumptions - transparency testability (graphical representation)Encoding Causal Assumptions - transparency testability (graphical representation)-calculus control confounding: “back-door”-calculus control confounding: “back-door”algorithmization CounterfactualsThe algorithmization CounterfactualsMediation Analysis Assessment Direct Indirect EffectsMediation Analysis Assessment Direct Indirect EffectsAdaptability, External validity Sample Selection Bias: still researched “domain adaptation”, “transfer learning”Adaptability, External validity Sample Selection Bias: still researched “domain adaptation”, “transfer learning”Recovering missing dataRecovering missing dataCausal Discovery:\nd-separation\nFunctional decomposition (Hoyer et al. 2008)\nSpontaneous local changes (Pearl 2014)\nCausal Discovery:d-separationd-separationFunctional decomposition (Hoyer et al. 2008)Functional decomposition (Hoyer et al. 2008)Spontaneous local changes (Pearl 2014)Spontaneous local changes (Pearl 2014)List packages causal inference RSimpson’s Paradox:statistical association seen entire population reversed sub-population.Structural Causal Model accompanies graphical causal model create efficient language represent causalityStructural Causal Model solution curse dimensionality (.e., large numbers variable \\(p\\), small dataset \\(n\\)) thanks product decomposition. allows us solve problems without knowing function, parameters, distributions error terms.Suppose causal chain \\(X \\Y \\Z\\):\\[\nP(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)\n\\]Criticisms quasi-experimental versus experimental designs:Quasi-experimental methods don’t approximate well experimental results. example,\nLaLonde (1986) shows Matching Methods, Difference--differences, Tobit-2 (Heckman-type) can’t approximate experimental estimates.\nQuasi-experimental methods don’t approximate well experimental results. example,LaLonde (1986) shows Matching Methods, Difference--differences, Tobit-2 (Heckman-type) can’t approximate experimental estimates.Tools hierarchical orderExperimental Design: Randomized Control Trials (Gold standard): Tier 1Experimental Design: Randomized Control Trials (Gold standard): Tier 1Quasi-experimental\nRegression Discontinuity\nSynthetic Difference--Differences\nDifference--Differences\nSynthetic Control\nEvent Studies\nFixed Effects Estimator 12.4.2.2\nEndogenous Treatment: mostly Instrumental Variables\nMatching Methods\nInterrupted Time Series\nEndogenous Sample Selection 33.2: mostly Heckman’s correction\nQuasi-experimentalRegression DiscontinuityRegression DiscontinuitySynthetic Difference--DifferencesSynthetic Difference--DifferencesDifference--DifferencesDifference--DifferencesSynthetic ControlSynthetic ControlEvent StudiesEvent StudiesFixed Effects Estimator 12.4.2.2Fixed Effects Estimator 12.4.2.2Endogenous Treatment: mostly Instrumental VariablesEndogenous Treatment: mostly Instrumental VariablesMatching MethodsMatching MethodsInterrupted Time SeriesInterrupted Time SeriesEndogenous Sample Selection 33.2: mostly Heckman’s correctionEndogenous Sample Selection 33.2: mostly Heckman’s correctionInternal vs. External ValidityInternal Validity: Economists applied scientists largely care .Internal Validity: Economists applied scientists largely care .External Validity: Localness might affect external validity.External Validity: Localness might affect external validity.many economic policies, difference treatment intention treat.example, might effective vaccine (.e., intention treat), mean everybody take (.e., treatment).four types subjects deal :Non-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .\nAlways takers\nNever takers\nNon-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .Always takersAlways takersNever takersNever takersSwitchers\nCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\n\nDefiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\n\nSwitchersCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\nCompliers: defined respect intervention.care compliers introduce intervention, something. don’t interventions, won’t .care compliers introduce intervention, something. don’t interventions, won’t .Tools used identify causal impact intervention compliersTools used identify causal impact intervention compliersIf compliers dataset, intention treatment = treatment effect.compliers dataset, intention treatment = treatment effect.Defiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\nDefiers: go opposite direction treatment.typically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.Directional Bias due selection treatment comes 2 general opposite sourcesMitigation-based: select treatment combat problemPreference-based: select treatment units like kind treatment.","code":""},{"path":"causal-inference.html","id":"treatment-effect-types","chapter":"18 Causal Inference","heading":"18.1 Treatment effect types","text":"section based Paul Testa’s noteTerminology:Quantities causal interest (.e., treatment effect types)Quantities causal interest (.e., treatment effect types)Estimands: parameters interestEstimands: parameters interestEstimators: procedures calculate hesitates parameters interestEstimators: procedures calculate hesitates parameters interestSources bias (according prof. Luke Keele)\\[\n\\begin{aligned}\n&\\text{Estimator - True Causal Effect} \\\\\n&= \\text{Hidden bias + Misspecification bias + Statistical Noise} \\\\\n&= \\text{Due design + Due modeling + Due finite sample}\n\\end{aligned}\n\\]","code":""},{"path":"causal-inference.html","id":"average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.1 Average Treatment Effects","text":"Average treatment effect (ATE) difference means treated control groupsRandomization Experimental Design can provide unbiased estimate ATE.Let \\(Y_i(1)\\) denote outcome individual \\(\\) treatment \\(Y_i(0)\\) denote outcome individual \\(\\) controlThen, treatment effect individual \\(\\) difference outcome treatment control\\[\n\\tau_i = Y_i(1) - Y_i(0)\n\\]Without time machine dimension portal, can observe one two event: either individual \\(\\) experiences treatment doesn’t., ATE quantity interest can come handy since can observe across individuals\\[\nATE = \\frac{1}{N} \\sum_{=1}^N \\tau_i = \\frac{\\sum_1^N Y_i(1)}{N} - \\frac{\\sum_i^N Y_i(0)}{N}\n\\]random assignment (.e., treatment assignment independent potential outcome observables unobservables), observed means difference two groups unbiased estimator average treatment effect\\[\nE(Y_i (1) |D = 1) = E(Y_i(1)|D=0) = E(Y_i(1)) \\\\\nE(Y_i(0) |D = 1) = E(Y_i(0)|D = 0 ) = E(Y_i(0))\n\\]\\[\nATE = E(Y_i(1)) - E(Y_i(0))\n\\]Alternatively, can write potential outcomes model regression form\\[\nY_i = Y_i(0)  + [Y_i (1) - Y_i(0)] D_i\n\\]Let \\(\\beta_{0i} = Y_i (0) ; \\beta_{1i} = Y_i(1) - Y_i(0)\\), \\[\nY_i = \\beta_{0i} + \\beta_{1i} D_i\n\\]\\(\\beta_{0i}\\) = outcome unit receive treatment\\(\\beta_{0i}\\) = outcome unit receive treatment\\(\\beta_{1i}\\) = treatment effect (.e., random coefficients unit \\(\\))\\(\\beta_{1i}\\) = treatment effect (.e., random coefficients unit \\(\\))understand endogeneity (.e., nonrandom treatment assignment), can examine standard linear model\\[\n\\begin{aligned}\nY_i &= \\beta_{0i} + \\beta_{1i} D_i \\\\\n&= ( \\bar{\\beta}_{0} + \\epsilon_{0i} ) + (\\bar{\\beta}_{1} + \\epsilon_{1i} )D_i \\\\\n&=  \\bar{\\beta}_{0} + \\epsilon_{0i} + \\bar{\\beta}_{1} D_i + \\epsilon_{1i} D_i\n\\end{aligned}\n\\]random assignment, \\(E(\\epsilon_{0i}) = E(\\epsilon_{1i}) = 0\\)selection bias: \\(D_i \\perp e_{0i}\\)selection bias: \\(D_i \\perp e_{0i}\\)Treatment effect independent treatment assignment: \\(D_i \\perp e_{1i}\\)Treatment effect independent treatment assignment: \\(D_i \\perp e_{1i}\\)otherwise, residuals can correlate \\(D_i\\)estimation,\\(\\hat{\\beta}_1^{OLS}\\) identical difference means (.e., \\(Y_i(1) - Y_i(0)\\))\\(\\hat{\\beta}_1^{OLS}\\) identical difference means (.e., \\(Y_i(1) - Y_i(0)\\))case heteroskedasticity (.e., \\(\\epsilon_{0i} + D_i \\epsilon_{1i} \\neq 0\\) ), residual’s variance depends \\(X\\) heterogeneous treatment effects (.e., \\(\\epsilon_{1i} \\neq 0\\))\nRobust SE still give consistent estimate \\(\\hat{\\beta}_1\\) case\nAlternatively, one can use two-sample t-test difference means unequal variances.\ncase heteroskedasticity (.e., \\(\\epsilon_{0i} + D_i \\epsilon_{1i} \\neq 0\\) ), residual’s variance depends \\(X\\) heterogeneous treatment effects (.e., \\(\\epsilon_{1i} \\neq 0\\))Robust SE still give consistent estimate \\(\\hat{\\beta}_1\\) caseRobust SE still give consistent estimate \\(\\hat{\\beta}_1\\) caseAlternatively, one can use two-sample t-test difference means unequal variances.Alternatively, one can use two-sample t-test difference means unequal variances.","code":""},{"path":"causal-inference.html","id":"conditional-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.2 Conditional Average Treatment Effects","text":"Treatment effects can different different groups people. words, treatment effects can vary across subgroups.examine heterogeneity across groups (e.g., men vs. women), can estimate conditional average treatment effects (CATE) subgroup\\[\nCATE = E(Y_i(1) - Y_i(0) |D_i, X_i))\n\\]","code":""},{"path":"causal-inference.html","id":"intent-to-treat-effects","chapter":"18 Causal Inference","heading":"18.1.3 Intent-to-treat Effects","text":"encounter non-compliance (either people suppose receive treatment don’t receive , people suppose control group receive treatment), treatment receipt independent potential outcomes confounders.case, difference observed means treatment control groups Average Treatment Effects, Intent--treat Effects (ITT). words, ITT treatment effect receive treatment","code":""},{"path":"causal-inference.html","id":"local-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.4 Local Average Treatment Effects","text":"Instead estimating treatment effects receive treatment (.e., Intent--treat Effects), want estimate treatment effect actually comply treatment. local average treatment effects (LATE) complier average causal effects (CACE). assume don’t use CATE denote complier average treatment effect reserved conditional average treatment effects.Using random treatment assignment instrument, can recover effect treatment compliers.percent compliers increases, Intent--treat Effects Local Average Treatment Effects convergeAs percent compliers increases, Intent--treat Effects Local Average Treatment Effects convergeRule thumb: SE(LATE) = SE(ITT)/(share compliers)Rule thumb: SE(LATE) = SE(ITT)/(share compliers)LATE estimate always greater ITT estimateLATE estimate always greater ITT estimateLATE can also estimated using pure placebo group (Gerber et al. 2010).LATE can also estimated using pure placebo group (Gerber et al. 2010).Partial compliance hard study, IV/2SLS estimator biased, use Bayesian (Long, Little, Lin 2010; Jin Rubin 2009, 2008).Partial compliance hard study, IV/2SLS estimator biased, use Bayesian (Long, Little, Lin 2010; Jin Rubin 2009, 2008).","code":""},{"path":"causal-inference.html","id":"one-sided-noncompliance","chapter":"18 Causal Inference","heading":"18.1.4.1 One-sided noncompliance","text":"One-sided noncompliance sample, compliers never-takersOne-sided noncompliance sample, compliers never-takersWith exclusion restriction (.e., excludability), never-takers results treatment control group (.e., never treated)exclusion restriction (.e., excludability), never-takers results treatment control group (.e., never treated)random assignment, can number never-takers treatment control groupsWith random assignment, can number never-takers treatment control groupsHence,Hence,\\[\nLATE = \\frac{ITT}{\\text{share compliers}}\n\\]","code":""},{"path":"causal-inference.html","id":"two-sided-noncompliance","chapter":"18 Causal Inference","heading":"18.1.4.2 Two-sided noncompliance","text":"Two-sided noncompliance sample, compliers, never-takers, always-takersTwo-sided noncompliance sample, compliers, never-takers, always-takersTo estimate LATE, beyond excludability like One-sided noncompliance case, need assume defiers (.e., monotonicity assumption) (excusable practical studies)estimate LATE, beyond excludability like One-sided noncompliance case, need assume defiers (.e., monotonicity assumption) (excusable practical studies)\\[\nLATE = \\frac{ITT}{\\text{share compliers}}\n\\]","code":""},{"path":"causal-inference.html","id":"population-vs.-sample-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.5 Population vs. Sample Average Treatment Effects","text":"See (Imai, King, Stuart 2008) sample average treatment effect (SATE) diverges population average treatment effect (PATE).stay consistent, section uses notations (Imai, King, Stuart 2008)’s paper.finite population \\(N\\), observe \\(n\\) observations (\\(N>>n\\)), half control half treatment group.unknown data generating process, \\[\nI_i =\n\\begin{cases}\n1 \\text{ unit sample} \\\\\n0 \\text{ otherwise}\n\\end{cases}\n\\]\\[\nT_i =\n\\begin{cases}\n1 \\text{ unit treatment group} \\\\\n0 \\text{ unit control group}\n\\end{cases}\n\\]\\[\n\\text{potential outcome} =\n\\begin{cases}\nY_i(1) \\text{ } T_i = 1 \\\\\nY_i(0) \\text{ } T_i = 0\n\\end{cases}\n\\]Observed outcome \\[\nY_i | I_i = 1= T_i Y_i(1) + (1-T_i)Y_i(0)\n\\]Since can never observed outcome individual, treatment effect always unobserved unit \\(\\)\\[\nTE_i = Y_i(1) - Y_i(0)\n\\]Sample average treatment effect \\[\nSATE = \\frac{1}{n}\\sum_{\\\\{I_i = 1\\}} TE_i\n\\]Population average treatment effect \\[\nPATE = \\frac{1}{N}\\sum_{=1}^N TE_i\n\\]Let \\(X_i\\) observables \\(U_i\\) unobservables unit \\(\\)baseline estimator SATE PATE \\[\n\\begin{aligned}\nD &= \\frac{1}{n/2} \\sum_{\\(I_i = 1, T_i = 1)} Y_i - \\frac{1}{n/2} \\sum_{\\(I_i = 1 , T_i = 0)} Y_i \\\\\n&= \\text{observed sample mean treatment group} \\\\\n&- \\text{observed sample mean control group}\n\\end{aligned}\n\\]Let \\(\\Delta\\) estimation error (deviation truth), additive model\\[\nY_i(t) = g_t(X_i) + h_t(U_i)\n\\]decomposition estimation error \\[\n\\begin{aligned}\nPATE - D = \\Delta &= \\Delta_S + \\Delta_T \\\\\n&= (PATE - SATE) + (SATE - D)\\\\\n&= \\text{sample selection}+ \\text{treatment imbalance} \\\\\n&= (\\Delta_{S_X} + \\Delta_{S_U}) + (\\Delta_{T_X} + \\Delta_{T_U}) \\\\\n&= \\text{(selection observed + selection unobserved)} \\\\\n&+ (\\text{treatment imbalance observed + unobserved})\n\\end{aligned}\n\\]","code":""},{"path":"causal-inference.html","id":"estimation-error-from-sample-selection","chapter":"18 Causal Inference","heading":"18.1.5.1 Estimation Error from Sample Selection","text":"Also known sample selection error\\[\n\\Delta_S = PATE - SATE = \\frac{N - n}{N}(NATE - SATE)\n\\]NATE non-sample average treatment effect (.e., average treatment effect population sample:\\[\nNATE = \\sum_{\\(I_i = 0)} \\frac{TE_i}{N-n}\n\\]equation, zero sample selection error (.e., \\(\\Delta_S = 0\\)), can eitherGet \\(N = n\\) redefining sample population interestGet \\(N = n\\) redefining sample population interest\\(NATE = SATE\\) (e.g., \\(TE_i\\) constant \\(\\) selected sample, population select)\\(NATE = SATE\\) (e.g., \\(TE_i\\) constant \\(\\) selected sample, population select)NoteWhen heterogeneous treatment effects, random sampling can warrant sample selection bias, sample selection error.heterogeneous treatment effects, random sampling can warrant sample selection bias, sample selection error.Since can rarely know true underlying distributions observables (\\(X\\)) unobservables (\\(U\\)), verify whether empirical distributions observables unobservables sample identical population (reduce \\(\\Delta_S\\)). special case,\nSay census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))\nSay willing assume \\(TE_i\\) constant \n\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\n\\(U_i\\), \\(\\Delta_{U}=0\\)\n\nSince can rarely know true underlying distributions observables (\\(X\\)) unobservables (\\(U\\)), verify whether empirical distributions observables unobservables sample identical population (reduce \\(\\Delta_S\\)). special case,Say census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))Say census population, can adjust observables \\(X\\) reduce \\(\\Delta_{S_X}\\), still adjust unobservables (\\(U\\))Say willing assume \\(TE_i\\) constant \n\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\n\\(U_i\\), \\(\\Delta_{U}=0\\)\nSay willing assume \\(TE_i\\) constant \\(X_i\\), \\(\\Delta_{S_X} = 0\\)\\(X_i\\), \\(\\Delta_{S_X} = 0\\)\\(U_i\\), \\(\\Delta_{U}=0\\)\\(U_i\\), \\(\\Delta_{U}=0\\)","code":""},{"path":"causal-inference.html","id":"estimation-error-from-treatment-imbalance","chapter":"18 Causal Inference","heading":"18.1.5.2 Estimation Error from Treatment Imbalance","text":"Also known treatment imbalance error\\[\n\\Delta_T = SATE - D\n\\]\\(\\Delta_T \\0\\) treatment control groups balanced (.e., identical empirical distributions) observables (\\(X\\)) unobservables (\\(U\\))However, reality, can readjust observables, unobservables.","code":""},{"path":"causal-inference.html","id":"average-treatment-effects-on-the-treated-and-control","chapter":"18 Causal Inference","heading":"18.1.6 Average Treatment Effects on the Treated and Control","text":"Average Effect treatment Treated (ATT) \\[\n\\begin{aligned}\nATT &= E(Y_i(1) - Y_i(0)|D_i = 1) \\\\\n&= E(Y_i(1)|D_i = 1) - E(Y_i(0) |D_i = 1)\n\\end{aligned}\n\\]Average Effect treatment Control (ATC) (.e., effect weren’t treated) \\[\n\\begin{aligned}\nATC &= E(Y_i(1) - Y_i (0) |D_i =0) \\\\\n&= E(Y_i(1)|D_i = 0) - E(Y_i(0)|D_i = 0)\n\\end{aligned}\n\\]random assignment full compliance,\\[\nATE = ATT = ATC\n\\]Sample average treatment effect treated \\[\nSATT = \\frac{1}{n} \\sum_i TE_i\n\\]\\(TE_i\\) treatment effect unit \\(\\)\\(TE_i\\) treatment effect unit \\(\\)\\(n\\) number treated units sample\\(n\\) number treated units sample\\(\\) belongs subset (.e., sample) population interest treated.\\(\\) belongs subset (.e., sample) population interest treated.Population average treatment effect treated \\[\nPATT = \\frac{1}{N} \\sum_i TE_i\n\\]\\(TE_i\\) treatment effect unit \\(\\)\\(TE_i\\) treatment effect unit \\(\\)\\(N\\) number treated units population\\(N\\) number treated units population\\(\\) belongs population interest treated.\\(\\) belongs population interest treated.","code":""},{"path":"causal-inference.html","id":"quantile-average-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.7 Quantile Average Treatment Effects","text":"Instead middle point estimate (ATE), can also understand changes distribution outcome variable due treatment.Using quantile regression assumptions (Abadie, Angrist, Imbens 2002; Chernozhukov Hansen 2005), can consistent estimate quantile treatment effects (QTE), can make inference regarding given quantile.","code":""},{"path":"causal-inference.html","id":"mediation-effects","chapter":"18 Causal Inference","heading":"18.1.8 Mediation Effects","text":"additional assumptions (.e., sequential ignorability (Imai, Keele, Tingley 2010; Bullock Ha 2011)), can examine mechanism treatment outcome.causal framework,indirect effect treatment via mediator called average causal mediation effect (ACME)indirect effect treatment via mediator called average causal mediation effect (ACME)direct effect treatment outcome average direct effect (ADE)direct effect treatment outcome average direct effect (ADE)Mediation Section 36","code":""},{"path":"causal-inference.html","id":"log-odds-treatment-effects","chapter":"18 Causal Inference","heading":"18.1.9 Log-odds Treatment Effects","text":"binary outcome variable, might interested log-odds success. See (Freedman 2008) estimate consistent causal effect.Alternatively, attributable effects (Rosenbaum 2002) can also appropriate binary outcome.","code":""},{"path":"experimental-design.html","id":"experimental-design","chapter":"19 Experimental Design","heading":"19 Experimental Design","text":"Randomized Control Trials (RCT) Experiments always likely continue future holy grail causal inference, \nunbiased estimates\nelimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))\nunbiased estimatesunbiased estimateselimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))elimination confounding factors average (covariate imbalance always possible. Hence, want Rerandomization achieve platinum standard set (Tukey 1993))RCT means two group treatment (experimental) gorp control group. Hence, introduce treatment (exogenous variable) treatment group, expected difference outcomes two group due treatment.Subjects population randomly assigned either treatment control group. random assignment give us confidence changes outcome variable due treatment, source (variable).can easier hard science RCT can introduce treatment, control environments. ’s hard social scientists subjects usually human, treatment can hard introduce, environments uncontrollable. Hence, social scientists develop different tools (Quasi-experimental) recover causal inference recreate treatment control group environment.RCT, can easily establish internal validityEven though random assignment thing ceteris paribus (.e., holding everything else constant), effect (.e., random manipulation, things equal can observed, average, across treatment control groups).Selection ProblemAssume havebinary treatment \\(D_i =(0,1)\\)binary treatment \\(D_i =(0,1)\\)outcome interest \\(Y_i\\) individual \\(\\)\n\\(Y_{0i}\\) treated\n\\(Y_{1i}\\) treated\noutcome interest \\(Y_i\\) individual \\(\\)\\(Y_{0i}\\) treated\\(Y_{0i}\\) treated\\(Y_{1i}\\) treated\\(Y_{1i}\\) treated\\[\n\\text{Potential Outcome} =\n\\begin{cases}\nY_{1i} \\text{ } D_i = 1 \\\\\nY_{0i} \\text{ } D_i = 0\n\\end{cases}\n\\], observe outcome variable \\[\nY_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i\n\\]’s likely \\(Y_{1i}\\) \\(Y_{0i}\\) distributions (.e., different treatment effect different people). Since can’t see outcomes individual (unless time machine), can make inference regarding average outcome treated .\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n&= (E[Y_{1i}-Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n\\text{Observed difference treatment} &= \\text{Average treatment effect treated} + \\text{Selection bias}\n\\end{aligned}\n\\]average treatment effect average person treated person (another parallel universe) treatedThe average treatment effect average person treated person (another parallel universe) treatedThe selection bias difference treated weren’t treatedThe selection bias difference treated weren’t treatedWith random assignment treatment (\\(D_i\\)) Experimental Design, can \\(D_i\\) independent potential outcomes\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i|D_i = 0] &= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\\\\n&= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] && D_i \\perp Y_i \\\\\n&= E[Y_{1i} - Y_{0i}|D_i = 1] \\\\\n&= E[Y_{1i} - Y_{0i}]\n\\end{aligned}\n\\]Another representation regressionSuppose know effect \\[\nY_{1i} - Y_{0i} = \\rho\n\\]observed outcome variable (individual) can rewritten \\[\n\\begin{aligned}\nY_i &= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\\\\n&= \\alpha + \\rho D_i + \\eta_i\n\\end{aligned}\n\\]\\(\\eta_i\\) = random variation \\(Y_{0i}\\)Hence, conditional expectation individual outcome treatment status \\[\n\\begin{aligned}\nE[Y_i |D_i = 1] &= \\alpha + \\rho &+ E[\\eta_i |D_i = 1] \\\\\nE[Y_i |D_i = 0] &= \\alpha &+ E[\\eta_i |D_i = 0]\n\\end{aligned}\n\\]Thus,\\[\nE[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \\rho + E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\n\\]\\(E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\\) selection bias - correlation regression error term (\\(\\eta_i\\)), regressor (\\(D_i\\))regression, \\[\nE[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0]\n\\]difference outcomes weren’t treated get treated weren’t treated stay untreatedSay control variables (\\(X_i\\)), uncorrelated treatment (\\(D_i\\)), can include model, won’t (principle) affect estimate treatment effect (\\(\\rho\\)) added benefit reducing residual variance, subsequently reduces standard error estimates.\\[\nY_i = \\alpha + \\rho D_i + X_i'\\gamma + \\eta_i\n\\]Examples:Bertrand Mullainathan (2004) randomly assign race job application study effect race callbacks.","code":""},{"path":"experimental-design.html","id":"notes","chapter":"19 Experimental Design","heading":"19.1 Notes","text":"outcomes 0s, can’t use log-like transformation, ’s sensitive outcome unit (J. Chen Roth 2023). info issue, check [Zero-valued Outcomes]. use:Percentage changes Average: using Poisson QMLE, can interpret coefficients effect treatment treated group relative mean control group.Percentage changes Average: using Poisson QMLE, can interpret coefficients effect treatment treated group relative mean control group.Extensive vs. Intensive Margins: Distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1).\nget bounds intensive-margin, use Lee (2009) (assuming treatment monotonic effect outcome)\nExtensive vs. Intensive Margins: Distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1).get bounds intensive-margin, use Lee (2009) (assuming treatment monotonic effect outcome)Percentage changes AverageTo calculate proportional effectHence, conclude treatment effect 1215% higher treated group compared control group.Extensive vs. Intensive MarginsHere, can estimate intensive-margin treatment effect (.e., treatment effect “always-takers”).Since case, bounds contains 0, can’t say much intensive margin always-takers.aim examine sensitivity always-takers, consider scenarios average outcome compliers \\(100 \\times c\\%\\) lower higher always-takers.assume \\(E(Y(1)|Complier) = (1-c)E(Y(1)|Always-taker)\\)assume \\(c = 0.1\\) (.e., treatment, compliers outcome equal 10% outcome always-takers), intensive-margin effect always-takers 6.6 unit outcome.assume \\(c = 0.1\\) (.e., treatment, compliers outcome equal 10% outcome always-takers), intensive-margin effect always-takers 6.6 unit outcome.assume \\(c = 0.5\\) (.e., treatment, compliers outcome equal 50% outcome always-takers), intensive-margin effect always-takers 2.54 unit outcome.assume \\(c = 0.5\\) (.e., treatment, compliers outcome equal 50% outcome always-takers), intensive-margin effect always-takers 2.54 unit outcome.","code":"\nset.seed(123) # For reproducibility\nlibrary(tidyverse)\n\nn <- 1000 # Number of observations\np_treatment <- 0.5 # Probability of being treated\n\n# Step 1: Generate the treatment variable D\nD <- rbinom(n, 1, p_treatment)\n\n# Step 2: Generate potential outcomes\n# Untreated potential outcome (mostly zeroes)\nY0 <- rnorm(n, mean = 0, sd = 1) * (runif(n) < 0.3)\n\n# Treated potential outcome (shifting both the probability of being positive - extensive margin and its magnitude - intensive margin)\nY1 <- Y0 + rnorm(n, mean = 2, sd = 1) * (runif(n) < 0.7)\n\n# Step 3: Combine effects based on treatment\nY_observed <- (1 - D) * Y0 + D * Y1\n\n# Add explicit zeroes to model situations with no effect\nY_observed[Y_observed < 0] <- 0\n\n\ndata <-\n    data.frame(\n        ID = 1:n,\n        Treatment = D,\n        Outcome = Y_observed,\n        X = rnorm(n)\n    ) |>\n    # whether outcome is positive\n    dplyr::mutate(positive = Outcome > 0)\n\n# Viewing the first few rows of the dataset\nhead(data)\n#>   ID Treatment   Outcome          X positive\n#> 1  1         0 0.0000000  1.4783345    FALSE\n#> 2  2         1 2.2369379 -1.4067867     TRUE\n#> 3  3         0 0.0000000 -1.8839721    FALSE\n#> 4  4         1 3.2192276 -0.2773662     TRUE\n#> 5  5         1 0.6649693  0.4304278     TRUE\n#> 6  6         0 0.0000000 -0.1287867    FALSE\n\nhist(data$Outcome)\nlibrary(fixest)\nres_pois <-\n    fepois(\n        fml = Outcome ~ Treatment + X,\n        data = data, \n        vcov = \"hetero\"\n    )\netable(res_pois)\n#>                           res_pois\n#> Dependent Var.:            Outcome\n#>                                   \n#> Constant        -2.223*** (0.1440)\n#> Treatment        2.579*** (0.1494)\n#> X                  0.0235 (0.0406)\n#> _______________ __________________\n#> S.E. type       Heteroskedas.-rob.\n#> Observations                 1,000\n#> Squared Cor.               0.33857\n#> Pseudo R2                  0.26145\n#> BIC                        1,927.9\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# proportional effect\nexp(coefficients(res_pois)[\"Treatment\"]) - 1\n#> Treatment \n#>  12.17757\n\n# SE\nexp(coefficients(res_pois)[\"Treatment\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment\", \"Treatment\"])\n#> Treatment \n#>  1.968684\nres <- causalverse::lee_bounds(\n    df = data,\n    d = \"Treatment\",\n    m = \"positive\",\n    y = \"Outcome\",\n    numdraws = 10\n) |> \n    causalverse::nice_tab(2)\nprint(res)\n#>          term estimate std.error\n#> 1 Lower bound    -0.22      0.09\n#> 2 Upper bound     2.77      0.14\nset.seed(1)\nc_values = c(.1, .5, .7)\n\ncombined_res <- bind_rows(lapply(c_values, function(c) {\n    res <- causalverse::lee_bounds(\n        df = data,\n        d = \"Treatment\",\n        m = \"positive\",\n        y = \"Outcome\",\n        numdraws = 10,\n        c_at_ratio = c\n    )\n    \n    res$c_value <- as.character(c)\n    return(res)\n}))\n\ncombined_res |> \n    dplyr::select(c_value, everything()) |> \n    causalverse::nice_tab()\n#>   c_value           term estimate std.error\n#> 1     0.1 Point estimate     6.60      0.71\n#> 2     0.5 Point estimate     2.54      0.13\n#> 3     0.7 Point estimate     1.82      0.08"},{"path":"experimental-design.html","id":"semi-random-experiment","chapter":"19 Experimental Design","heading":"19.2 Semi-random Experiment","text":"Chicago Open Enrollment Program (Cullen, Jacob, Levitt 2005)Students can apply “choice” schoolsStudents can apply “choice” schoolsMany schools oversubscribed (Demand > Supply)Many schools oversubscribed (Demand > Supply)Resolve scarcity via random lotteriesResolve scarcity via random lotteriesNon-random enrollment, random lottery mean aboveNon-random enrollment, random lottery mean aboveLet\\[\n\\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1)\n\\]\\[\n\\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1)\n\\]Hence, can clearly see \\(\\delta_j \\neq \\theta_j\\) can enroll, ensure win. Thus, intention treat different treatment effect.Non-random enrollment, random lottery means can estimate \\(\\theta_j\\)recover true treatment effect, can use\\[\n\\delta_j = \\frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)}\n\\]\\(\\delta_j\\) = treatment effect\\(\\delta_j\\) = treatment effect\\(W\\) = Whether students win lottery\\(W\\) = Whether students win lottery\\(\\) = Whether student apply lottery\\(\\) = Whether student apply lotteryi = applicationi = applicationj = schoolj = schoolSay have10 win10 loseIntent treatment = Average effect give option choose\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=1; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\\\\n&= 0.25\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=0; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\\\\n&= 0.05\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\text{Intent treatment} &= 0.25 - 0.05 = 0.2 \\\\\n\\text{Treatment effect} &= 1\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nP(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &= \\frac{1+2}{10} = 0.3 \\\\\nP(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &= \\frac{1}{10} = 0.1\n\\end{aligned}\n\\]\\[\n\\text{Treatment effect} = \\frac{0.2}{0.3-0.1} = 1\n\\]knowing recover treatment effect, turn attention main model\\[\nY_{ia} = \\delta W_{ia} + \\lambda L_{ia} + e_{ia}\n\\]\\(W\\) = whether student wins lottery\\(W\\) = whether student wins lottery\\(L\\) = whether student enrolls lottery\\(L\\) = whether student enrolls lottery\\(\\delta\\) = intent treat\\(\\delta\\) = intent treatHence,Conditional lottery, \\(\\delta\\) validConditional lottery, \\(\\delta\\) validBut without lottery, \\(\\delta\\) randomBut without lottery, \\(\\delta\\) randomWinning losing identified within lotteryWinning losing identified within lotteryEach lottery multiple entries. Thus, can within estimatorEach lottery multiple entries. Thus, can within estimatorWe can also include control variables (\\(X_i \\theta\\))\\[\nY_{ia} = \\delta_1 W_{ia} + \\lambda_1 L_{ia} + X_i \\theta + u_{ia}\n\\]\\[\n\\begin{aligned}\nE(\\delta) &= E(\\delta_1) \\\\\nE(\\lambda) &\\neq E(\\lambda_1) && \\text{choosing lottery random}\n\\end{aligned}\n\\]Including \\((X_i \\theta)\\) just shifts around control variables (.e., reweighting lottery), affect treatment effect \\(E(\\delta)\\)","code":""},{"path":"experimental-design.html","id":"rerandomization","chapter":"19 Experimental Design","heading":"19.3 Rerandomization","text":"Since randomization balances baseline covariates average, imbalance variables due random chance can still happen.Since randomization balances baseline covariates average, imbalance variables due random chance can still happen.case “bad” randomization (.e., imbalance important baseline covariates), (Morgan Rubin 2012) introduce idea rerandomization.case “bad” randomization (.e., imbalance important baseline covariates), (Morgan Rubin 2012) introduce idea rerandomization.Rerandomization checking balance randomization process (experiment), eliminate bad allocation (.e., unacceptable balance).Rerandomization checking balance randomization process (experiment), eliminate bad allocation (.e., unacceptable balance).greater number variables, greater likelihood least one covariate imbalanced across treatment groups.\nExample: 10 covariates, probability significant difference \\(\\alpha = .05\\) least one covariate \\(1 - (1-.05)^{10} = 0.4 = 40\\%\\)\ngreater number variables, greater likelihood least one covariate imbalanced across treatment groups.Example: 10 covariates, probability significant difference \\(\\alpha = .05\\) least one covariate \\(1 - (1-.05)^{10} = 0.4 = 40\\%\\)Rerandomization increase treatment effect estimate precision covariates correlated outcome.\nImprovement precision treatment effect estimate depends (1) improvement covariate balance (2) correlation covariates outcome.\nRerandomization increase treatment effect estimate precision covariates correlated outcome.Improvement precision treatment effect estimate depends (1) improvement covariate balance (2) correlation covariates outcome.also need take account rerandomization analysis making inference.also need take account rerandomization analysis making inference.Rerandomization equivalent increasing sample size.Rerandomization equivalent increasing sample size.Alternatives include\nStratified randomization (Johansson Schultzberg 2022)\nMatched randomization (Greevy et al. 2004; Kapelner Krieger 2014)\nMinimization (Pocock Simon 1975)\nAlternatives includeStratified randomization (Johansson Schultzberg 2022)Stratified randomization (Johansson Schultzberg 2022)Matched randomization (Greevy et al. 2004; Kapelner Krieger 2014)Matched randomization (Greevy et al. 2004; Kapelner Krieger 2014)Minimization (Pocock Simon 1975)Minimization (Pocock Simon 1975)Rerandomization CriterionAcceptable randomization based function covariate matrix \\(\\mathbf{X}\\) vector treatment assignments \\(\\mathbf{W}\\)\\[\nW_i =\n\\begin{cases}\n1 \\text{ treated} \\\\\n0 \\text{ control}\n\\end{cases}\n\\]Mahalanobis Distance, \\(M\\), can used criteria acceptable balanceLet \\(M\\) multivariate distance groups means\\[\n\\begin{aligned}\nM &= (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' cov(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C) \\\\\n&= (\\frac{1}{n_T}+ \\frac{1}{n_C})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)' cov(\\mathbf{X})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)\n\\end{aligned}\n\\]large sample size “pure” randomization \\(M \\sim \\chi^2_k\\) \\(k\\) number covariates balancedThen let \\(p_a\\) probability accepting randomization. Choosing appropriate \\(p_a\\) tradeoff balance time.rule thumb re-randomize \\(M > \\)","code":""},{"path":"experimental-design.html","id":"two-stage-randomized-experiments-with-interference-and-noncompliance","chapter":"19 Experimental Design","heading":"19.4 Two-Stage Randomized Experiments with Interference and Noncompliance","text":"(Imai, Jiang, Malani 2021)","code":""},{"path":"sampling.html","id":"sampling","chapter":"20 Sampling","heading":"20 Sampling","text":"","code":""},{"path":"sampling.html","id":"simple-sampling","chapter":"20 Sampling","heading":"20.1 Simple Sampling","text":"Simple (random) SamplingIdentify missing points sample collected data","code":"\nlibrary(dplyr)\niris_df <- iris\nset.seed(1)\nsample_n(iris_df, 10)\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#> 1           5.8         2.7          4.1         1.0 versicolor\n#> 2           6.4         2.8          5.6         2.1  virginica\n#> 3           4.4         3.2          1.3         0.2     setosa\n#> 4           4.3         3.0          1.1         0.1     setosa\n#> 5           7.0         3.2          4.7         1.4 versicolor\n#> 6           5.4         3.0          4.5         1.5 versicolor\n#> 7           5.4         3.4          1.7         0.2     setosa\n#> 8           7.6         3.0          6.6         2.1  virginica\n#> 9           6.1         2.8          4.7         1.2 versicolor\n#> 10          4.6         3.4          1.4         0.3     setosa\nlibrary(sampling)\n# set unique id number for each row\niris_df$id = 1:nrow(iris_df)\n\n# Simple random sampling with replacement\nsrswor(10, length(iris_df$id))\n#>   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n#>  [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n#> [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#> [149] 0 0\n\n# Simple random sampling without replacement (sequential method)\nsrswor1(10, length(iris_df$id))\n#>   [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#> [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n#> [149] 0 0\n\n# Simple random sampling with replacement\nsrswr(10, length(iris_df$id))\n#>   [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0\n#>  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#>  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n#> [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n#> [149] 0 0\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~1)\nlibrary(sampler)\nrsamp(albania,\n      n = 260,\n      over = 0.1, # desired oversampling proportion\n      rep = F)\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nrmissing(sampdf = alsample,\n         colldf = alreceived,\n         col_name = qvKod)"},{"path":"sampling.html","id":"stratified-sampling","chapter":"20 Sampling","heading":"20.2 Stratified Sampling","text":"stratum subset population least one common characteristic.Steps:Identify relevant stratums representation population.Randomly sample select sufficient number subjects stratum.Stratified sampling reduces sampling error.Identify number missing points strata sample collected data","code":"\nlibrary(dplyr)\n# by number of rows\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_n(5)\nsample_iris\n#> # A tibble: 15 × 5\n#> # Groups:   Species [3]\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n#>  1          4.4         3            1.3         0.2 setosa    \n#>  2          5.2         3.5          1.5         0.2 setosa    \n#>  3          5.1         3.8          1.5         0.3 setosa    \n#>  4          5.2         3.4          1.4         0.2 setosa    \n#>  5          4.5         2.3          1.3         0.3 setosa    \n#>  6          5.5         2.5          4           1.3 versicolor\n#>  7          7           3.2          4.7         1.4 versicolor\n#>  8          6.7         3            5           1.7 versicolor\n#>  9          6.1         2.9          4.7         1.4 versicolor\n#> 10          5.5         2.4          3.8         1.1 versicolor\n#> 11          6.4         2.7          5.3         1.9 virginica \n#> 12          6.4         2.8          5.6         2.1 virginica \n#> 13          6.4         3.2          5.3         2.3 virginica \n#> 14          6.8         3.2          5.9         2.3 virginica \n#> 15          7.2         3.6          6.1         2.5 virginica\n\n# by fraction\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_frac(size = .15)\nsample_iris\n#> # A tibble: 24 × 5\n#> # Groups:   Species [3]\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n#>  1          5.5         4.2          1.4         0.2 setosa    \n#>  2          5           3            1.6         0.2 setosa    \n#>  3          5.2         4.1          1.5         0.1 setosa    \n#>  4          4.6         3.1          1.5         0.2 setosa    \n#>  5          5.1         3.7          1.5         0.4 setosa    \n#>  6          4.8         3.4          1.9         0.2 setosa    \n#>  7          5.1         3.3          1.7         0.5 setosa    \n#>  8          5.5         3.5          1.3         0.2 setosa    \n#>  9          5           2.3          3.3         1   versicolor\n#> 10          5.6         2.9          3.6         1.3 versicolor\n#> # ℹ 14 more rows\nlibrary(sampler)\n# Stratified sample using proportional allocation without replacement\nssamp(df=albania, n=360, strata=qarku, over=0.1)\n#> # A tibble: 395 × 45\n#>    qarku  Q_ID bashkia   BAS_ID zaz   njesiaAdministrative COM_ID qvKod zgjedhes\n#>    <fct> <int> <fct>      <int> <fct> <fct>                 <int> <fct>    <int>\n#>  1 Berat     1 Berat         11 ZAZ … \"Berat \"               1101 \"\\\"3…      558\n#>  2 Berat     1 Berat         11 ZAZ … \"Berat \"               1101 \"\\\"3…      815\n#>  3 Berat     1 Berat         11 ZAZ … \"Sinje\"                1108 \"\\\"3…      419\n#>  4 Berat     1 Kucove        13 ZAZ … \"Lumas\"                1104 \"\\\"3…      237\n#>  5 Berat     1 Kucove        13 ZAZ … \"Kucove\"               1201 \"\\\"3…      562\n#>  6 Berat     1 Skrapar       17 ZAZ … \"Corovode\"             1303 \"\\\"3…      829\n#>  7 Berat     1 Berat         11 ZAZ … \"Roshnik\"              1107 \"\\\"3…      410\n#>  8 Berat     1 Ura Vajg…     19 ZAZ … \"Ura Vajgurore\"        1110 \"\\\"3…      708\n#>  9 Berat     1 Kucove        13 ZAZ … \"Perondi\"              1203 \"\\\"3…      835\n#> 10 Berat     1 Kucove        13 ZAZ … \"Kucove\"               1201 \"\\\"3…      907\n#> # ℹ 385 more rows\n#> # ℹ 36 more variables: meshkuj <int>, femra <int>, totalSeats <int>,\n#> #   vendndodhja <fct>, ambienti <fct>, totalVoters <int>, femVoters <int>,\n#> #   maleVoters <int>, unusedBallots <int>, damagedBallots <int>,\n#> #   ballotsCast <int>, invalidVotes <int>, validVotes <int>, lsi <int>,\n#> #   ps <int>, pkd <int>, sfida <int>, pr <int>, pd <int>, pbdksh <int>,\n#> #   adk <int>, psd <int>, ad <int>, frd <int>, pds <int>, pdiu <int>, …\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nsmissing(\n    sampdf = alsample,\n    colldf = alreceived,\n    strata = qarku,\n    col_name = qvKod\n)"},{"path":"sampling.html","id":"unequal-probability-sampling","chapter":"20 Sampling","heading":"20.3 Unequal Probability Sampling","text":"","code":"\nUPbrewer()\nUPmaxentropy()\nUPmidzuno()\nUPmidzunopi2()\nUPmultinomial()\nUPpivotal()\nUPrandompivotal()\nUPpoisson()\nUPsampford()\nUPsystematic()\nUPrandomsystematic()\nUPsystematicpi2()\nUPtille()\nUPtillepi2()"},{"path":"sampling.html","id":"balanced-sampling","chapter":"20 Sampling","heading":"20.4 Balanced Sampling","text":"Purpose: get means population sample auxiliary variablesPurpose: get means population sample auxiliary variablesBalanced sampling different purposive selectionBalanced sampling different purposive selectionBalancing equations\\[\n\\sum_{k \\S} \\frac{\\mathbf{x}_k}{\\pi_k} = \\sum_{k \\U} \\mathbf{x}_k\n\\]\\(\\mathbf{x}_k\\) vector auxiliary variables","code":""},{"path":"sampling.html","id":"cube","chapter":"20 Sampling","heading":"20.4.1 Cube","text":"flight phaseflight phaselanding phaselanding phase","code":"\nsamplecube()\nfastflightcube()\nlandingcube()"},{"path":"sampling.html","id":"stratification","chapter":"20 Sampling","heading":"20.4.2 Stratification","text":"Try replicate population based original multivariate histogram","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        strata = ~stype,\n                        id = ~1)\nbalancedstratification()"},{"path":"sampling.html","id":"cluster-1","chapter":"20 Sampling","heading":"20.4.3 Cluster","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus1,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~dnum)\nbalancedcluster()"},{"path":"sampling.html","id":"two-stage","chapter":"20 Sampling","heading":"20.4.4 Two-stage","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus2, \n                        fpc = ~fpc1 + fpc2, \n                        id = ~ dnum + snum)\nbalancedtwostage()"},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21 Analysis of Variance (ANOVA)","text":"ANOVA using underlying mechanism linear regression. However, angle ANOVA chooses look slightly different traditional linear regression. can useful case qualitative variables designed experiments.Experimental DesignFactor: explanatory predictor variable studied investigationTreatment (Factor Level): “value” factor applied experimental unitExperimental Unit: person, animal, piece material, etc. subjected treatment(s) provides responseSingle Factor Experiment: one explanatory variable consideredMultifactor Experiment: one explanatory variableClassification Factor: factor control experimenter (observational data)Experimental Factor: assigned experimenterBasics experimental design:Choices statistician make:\nset treatments\nset experimental units\ntreatment assignment (selection bias)\nmeasurement (measurement bias, blind experiments)\nChoices statistician make:set treatmentsset experimental unitstreatment assignment (selection bias)measurement (measurement bias, blind experiments)Advancements experimental design:\nFactorial Experiments:\nconsider multiple factors time (interaction)\nReplication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\n\nRandomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\n\nLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\n\nAdvancements experimental design:Factorial Experiments:\nconsider multiple factors time (interaction)Factorial Experiments:\nconsider multiple factors time (interaction)Replication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\nReplication: repetition experimentassess mean squared errorcontrol precision experiment (power)Randomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\nRandomizationBefore R.. Fisher (1900s), treatments assigned systematically subjectivelyrandomization: assign treatments experimental units random, averages systematic effects control investigatorLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\nLocal control: Blocking StratificationReduce experimental errors increase power placing restrictions randomization treatments experimental units.Randomization may also eliminate correlations due time space.","code":""},{"path":"analysis-of-variance-anova.html","id":"completely-randomized-design-crd","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1 Completely Randomized Design (CRD)","text":"Treatment factor \\(\\ge2\\) treatments levels. Experimental units randomly assigned treatment. number experimental units group can beequal (balanced): nunequal (unbalanced): \\(n_i\\) -th group (= 1,…,).total sample size \\(N=\\sum_{=1}^{}n_i\\)Possible assignments units treatments \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\)probability 1/k selected. experimental unit measured response \\(Y_{ij}\\), j denotes unit denotes treatment.Treatmentwhere \\(\\bar{Y_{.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\)\\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\)grand mean \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{}\\sum_{j}Y_{ij}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-fixed-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1 Single Factor Fixed Effects Model","text":"also known Single Factor (One-Way) ANOVA ANOVA Type model.Partitioning VarianceThe total variability \\(Y_{ij}\\) observation can measured deviation \\(Y_{ij}\\) around overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\)can rewritten :\\[\n\\begin{aligned}\nY_{ij} - \\bar{Y_{..}}&=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{.}} - \\bar{Y_{.}} \\\\\n&= (\\bar{Y_{.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{.}})\n\\end{aligned}\n\\]wherethe first term treatment differences (.e., deviation treatment mean overall mean)second term within treatment differences (.e., deviation observation around treatment mean)\\[\n\\begin{aligned}\n\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &=  \\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})^2+\\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\nSSTO &= SSTR + SSE \\\\\ntotal~SS &= treatment~SS + error~SS \\\\\n(N-1)~d.f. &= (-1)~d.f. + (N - ) ~ d.f.\n\\end{aligned}\n\\]lose d.f. total corrected SSTO estimation mean (\\(\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\))\n, SSTR \\(\\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})=0\\)Accordingly, \\(MSTR= \\frac{SST}{-1}\\) \\(MSR=\\frac{SSE}{N-}\\)ANOVA TableLinear Model Explanation ANOVA","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.1 Cell means model","text":"\\[\nY_{ij}=\\mu_i+\\epsilon\\_{ij}\n\\]\\(Y_{ij}\\) response variable \\(j\\)-th subject \\(\\)-th treatment\\(Y_{ij}\\) response variable \\(j\\)-th subject \\(\\)-th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)observations varianceAll observations varianceExample:\\(= 3\\) (3 treatments) \\(n_1=n_2=n_3=2\\)\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\]\\(X_{k,ij}=1\\) \\(k\\)-th treatment used\\(X_{k,ij}=0\\) OtherwiseNote: intercept term.\\[\\begin{equation}\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\nn_1 & 0 & 0\\\\\n0 & n_2 & 0\\\\\n0 & 0 & n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_1\\\\\nY_2\\\\\nY_3\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\\begin{array}{c}\n\\bar{Y_1}\\\\\n\\bar{Y_2}\\\\\n\\bar{Y_3}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\tag{21.1}\n\\end{equation}\\]BLUE (best linear unbiased estimator) \\(\\beta=[\\mu_1 \\mu_2\\mu_3]'\\)\\[\nE(\\mathbf{b})=\\beta\n\\]\\[\nvar(\\mathbf{b})=\\sigma^2(\\mathbf{X'X})^{-1}=\\sigma^2\n\\left[\\begin{array}{ccc}\n1/n_1 & 0 & 0\\\\\n0 & 1/n_2 & 0\\\\\n0 & 0 & 1/n_3\\\\\n\\end{array}\\right]\n\\]\\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X'X})^{-1})\\)\\[\n\\begin{aligned}\nMSE &= \\frac{1}{N-} \\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\n    &= \\frac{1}{N-} \\sum_{}[(n_i-1)\\frac{\\sum_{}(Y_{ij}-\\bar{Y_{.}})^2}{n_i-1}] \\\\\n    &= \\frac{1}{N-} \\sum_{}(n_i-1)s_1^2\n\\end{aligned}\n\\]\\(E(s_i^2)=\\sigma^2\\)\\(E(MSE)=\\frac{1}{N-}\\sum_{}(n_i-1)\\sigma^2=\\sigma^2\\)Hence, MSE unbiased estimator \\(\\sigma^2\\), regardless whether treatment means equal .\\(E(MSTR)=\\sigma^2+\\frac{\\sum_{}n_i(\\mu_i-\\mu_.)^2}{-1}\\)\n\\(\\mu_.=\\frac{\\sum_{=1}^{}n_i\\mu_i}{\\sum_{=1}^{}n_i}\\)\ntreatment means equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\).can use \\(F\\)-test equality treatment means:\\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\]\\[H_a: ~al l~ \\mu_i ~ ~ equal \\]\\(F=\\frac{MSTR}{MSE}\\)\nlarge values F support \\(H_a\\) (since MSTR tend exceed MSE \\(H_a\\) holds)\nF near 1 support \\(H_0\\) (upper tail test)Equivalently, \\(H_0\\) true, \\(F \\sim f_{(-1,N-)}\\)\\(F \\leq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)\\(F \\geq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)Note: \\(= 2\\) (2 treatments), \\(F\\)-test = two sample \\(t\\)-test","code":""},{"path":"analysis-of-variance-anova.html","id":"treatment-effects-factor-effects","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2 Treatment Effects (Factor Effects)","text":"Besides Cell means model, another way formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] \\(Y_{ij}\\) \\(j\\)-th response \\(\\)-th treatment\\(\\tau_i\\) \\(\\)-th treatment effect\\(\\mu\\) constant component, common observations\\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\)example, \\(= 3\\), \\(n_1=n_2=n_3=2\\)\\[\\begin{equation}\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{cccc}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_1 \\\\\n\\tau_2 \\\\\n\\tau_3\\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\tag{21.2}\n\\end{equation}\\]However,\\[\n\\mathbf{X'X} =\n\\left(\n\\begin{array}\n{cccc}\n\\sum_{}n_i & n_1 & n_2 & n_3 \\\\\nn_1 & n_1 & 0 & 0 \\\\\nn_2 & 0 & n_2 & 0 \\\\\nn_3 & 0 & 0 & n_3 \\\\\n\\end{array}\n\\right)\n\\]singular thus exist, \\(\\mathbf{b}\\) insolvable (infinite solutions)Hence, impose restrictions parameters model matrix \\(\\mathbf{X}\\) full rank.Whatever restriction use, still :\\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ ~ -th ~ treatment\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-sum-of-tau","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2.1 Restriction on sum of tau","text":"\\(\\sum_{=1}^{}\\tau_i=0\\)implies\\[\n\\mu= \\mu +\\frac{1}{}\\sum_{=1}^{}(\\mu+\\tau_i)\n\\]average treatment mean (grand mean) (overall mean)\\[\n\\begin{aligned}\n\\tau_i  &=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\\n        &= \\text{treatment  mean} - \\text{grand~mean} \\\\\n        &= \\text{treatment  effect}\n\\end{aligned}\n\\]\\[\n\\tau_a=-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, mean -th treatment \\[\n\\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, model need “” parameters:\\[\n\\mu,\\tau_1,\\tau_2,..,\\tau_{-1}\n\\]Equation (21.2) becomes\\[\\begin{equation}\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & -1 & -1 \\\\\n1 & -1 & -1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_1 \\\\\n\\tau_2 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\end{equation}\\]\\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]'\\)Equation (21.1) \\(\\sum_{}\\tau_i=0\\) becomes\\[\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_1} \\\\\n\\hat{\\tau_2} \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_1-n_3 & n_2-n_3\\\\\nn_1-n_3 & n_1+n_3 & n_3\\\\\nn_2-n_3 & n_3 & n_2-n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{1.}-Y_{3.}\\\\\nY_{2.}-Y_{3.}\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\\begin{array}{c}\n\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{1.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{2.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\end{array}\\right]\\\\\n& =\n\\left[\\begin{array}{c}\n\\hat{\\mu}\\\\\n\\hat{\\tau_1}\\\\\n\\hat{\\tau_2}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\]\\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{}\\bar{Y_{.}}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-first-tau","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.2.2 Restriction on first tau","text":"R, lm() uses restriction \\(\\tau_1=0\\)previous example, \\(n_1=n_2=n_3=2\\), \\(\\tau_1=0\\).treatment means can written :\\[\n\\begin{aligned}\n\\mu_1 &= \\mu + \\tau_1 = \\mu + 0 = \\mu  \\\\\n\\mu_2 &= \\mu + \\tau_2 \\\\\n\\mu_3 &= \\mu + \\tau_3\n\\end{aligned}\n\\]Hence, \\(\\mu\\) mean response first treatmentIn matrix form,\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &=\n\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{aligned}\n\\]\\(\\beta = [\\mu,\\tau_2,\\tau_3]'\\)\\[\n\\begin{aligned}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_2} \\\\\n\\hat{\\tau_3} \\\\\n\\end{array}\\right] &=\n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& =\n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_2 & n_3\\\\\nn_2 & n_2 & 0\\\\\nn_3 & 0 & n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{2.}\\\\\nY_{3.}\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\n\\begin{array}{c}\n\\bar{Y_{1.}} \\\\\n\\bar{Y_{2.}} - \\bar{Y_{1.}} \\\\\n\\bar{Y_{3.}} - \\bar{Y_{1.}}\\\\\n\\end{array}\\right]\n\\end{aligned}\n\\]\\[\nE(\\mathbf{b})= \\beta =\n\\left[\\begin{array}{c}\n{\\mu}\\\\\n{\\tau_2}\\\\\n{\\tau_3}\\\\\n\\end{array}\\right]\n=\n\\left[\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2-\\mu_1\\\\\n\\mu_3-\\mu_1\\\\\n\\end{array}\\right]\n\\]\\[\n\\begin{aligned}\nvar(\\mathbf{b}) &= \\sigma^2(\\mathbf{X'X})^{-1} \\\\\nvar(\\hat{\\mu}) &= var(\\bar{Y_{1.}})=\\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_2}) &= var(\\bar{Y_{2.}}-\\bar{Y_{1.}}) = \\sigma^2/n_2 + \\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_3}) &= var(\\bar{Y_{3.}}-\\bar{Y_{1.}}) = \\sigma^2/n_3 + \\sigma^2/n_1\n\\end{aligned}\n\\]Note three parameterization, ANOVA table sameModel 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\)Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\sum_{} \\tau_i=0\\)Model 3: \\(Y_{ij}= \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\tau_1=0\\)models calculation \\(\\hat{Y}\\) \\[\n\\mathbf{\\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}\n\\]ANOVA TableError(within treatments)\\(\\mathbf{P_1} = \\frac{1}{n}\\mathbf{J}\\)\\(F\\)-statistic \\((-1,N-)\\) degrees freedom, gives value three parameterization, hypothesis test written bit different:\\[\n\\begin{aligned}\n&H_0 : \\mu_1 = \\mu_2 = ... = \\mu_a \\\\\n&H_0 : \\mu + \\tau_1 = \\mu + \\tau_2 = ... = \\mu + \\tau_a \\\\\n&H_0 : \\tau_1 = \\tau_2 = ...= \\tau_a\n\\end{aligned}\n\\]\\(F\\)-test serves preliminary analysis, see difference different factors. -depth analysis, consider different testing treatment effects.","code":""},{"path":"analysis-of-variance-anova.html","id":"testing-of-treatment-effects","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3 Testing of Treatment Effects","text":"Single Treatment Mean \\(\\mu_i\\)Differences Treatment MeansA Contrast Among Treatment MeansA Linear Combination Treatment Means","code":""},{"path":"analysis-of-variance-anova.html","id":"single-treatment-mean","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.1 Single Treatment Mean","text":"\\(\\hat{\\mu_i}=\\bar{Y_{.}}\\) \\(E(\\bar{Y_{.}})=\\mu_i\\)\\(var(\\bar{Y_{}})=\\sigma^2/n_i\\) estimated \\(s^2(\\bar{Y_{.}})=MSE / n_i\\)Since \\(\\frac{\\bar{Y_{.}}-\\mu_i}{s(\\bar{Y_{.}})} \\sim t_{N-}\\) confidence interval \\(\\mu_i\\) \\(\\bar{Y_{.}} \\pm t_{1-\\alpha/2;N-}s(\\bar{Y_{.}})\\),\ncan t-test means difference constant \\(c\\)\\[\n\\begin{aligned}\n&H_0: \\mu_i = c \\\\\n&H_1: \\mu_i \\neq c\n\\end{aligned}\n\\]\\[\nT =\\frac{\\bar{Y_{.}}-c}{s(\\bar{Y_{.}})}\n\\]follows \\(t_{N-}\\) \\(H_0\\) true.\n\\(|T| > t_{1-\\alpha/2;N-}\\), can reject \\(H_0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"differences-between-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.2 Differences Between Treatment Means","text":"Let \\(D=\\mu_i - \\mu_i'\\), also known pairwise comparison\\(D\\) can estimated \\(\\hat{D}=\\bar{Y_{}}-\\bar{Y_{}}'\\) unbiased (\\(E(\\hat{D})=\\mu_i-\\mu_i'\\))Since \\(\\bar{Y_{}}\\) \\(\\bar{Y_{}}'\\) independent, \\[\nvar(\\hat{D})=var(\\bar{Y_{}}) + var(\\bar{Y_{'}}) = \\sigma^2(1/n_i + 1/n_i')\n\\]can estimated \\[\ns^2(\\hat{D}) = MSE(1/n_i + 1/n_i')\n\\]single treatment inference,\\[\n\\frac{\\hat{D}-D}{s(\\hat{D})} \\sim t_{N-}\n\\]hence,\\[\n\\hat{D} \\pm t_{(1-\\alpha/2;N-)}s(\\hat{D})\n\\]Hypothesis tests:\\[\n\\begin{aligned}\n&H_0: \\mu_i = \\mu_i' \\\\\n&H_a: \\mu_i \\neq \\mu_i'\n\\end{aligned}\n\\]can tested following statistic\\[\nT = \\frac{\\hat{D}}{s(\\hat{D})} \\sim t_{1-\\alpha/2;N-}\n\\]reject \\(H_0\\) \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"contrast-among-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.3 Contrast Among Treatment Means","text":"generalize comparison two means, contrastsA contrast linear combination treatment means:\\[\nL = \\sum_{=1}^{}c_i \\mu_i\n\\]\\(c_i\\) non-random constant sum 0:\\[\n\\sum_{=1}^{} c_i = 0\n\\]unbiased estimator contrast L \\[\n\\hat{L} = \\sum_{=1}^{}c_i \\bar{Y}_{.}\n\\]\\(E(\\hat{L}) = L\\). Since \\(\\bar{Y}_{.}\\), = 1,…, independent.\\[\n\\begin{aligned}\nvar(\\hat{L}) &= var(\\sum_{=1}^c_i \\bar{Y}_{.}) = \\sum_{=1}^var(c_i \\bar{Y}_i)  \\\\\n&= \\sum_{=1}^c_i^2 var(\\bar{Y}_i) = \\sum_{=1}^c_i^2 \\sigma^2 /n_i \\\\\n&= \\sigma^2 \\sum_{=1}^{} c_i^2 /n_i\n\\end{aligned}\n\\]Estimation variance:\\[\ns^2(\\hat{L}) = MSE \\sum_{=1}^{} \\frac{c_i^2}{n_i}\n\\]\\(\\hat{L}\\) normally distributed (since linear combination independent normal random variables)., since \\(SSE/\\sigma^2\\) \\(\\chi_{N-}^2\\)\\[\n\\frac{\\hat{L}-L}{s(\\hat{L})} \\sim t_{N-}\n\\]\\(1-\\alpha\\) confidence limits given \\[\n\\hat{L} \\pm t_{1-\\alpha/2; N-}s(\\hat{L})\n\\]Hypothesis testing\\[\n\\begin{aligned}\n&H_0: L = 0 \\\\\n&H_a: L \\neq 0\n\\end{aligned}\n\\]\\[\nT = \\frac{\\hat{L}}{s(\\hat{L})}\n\\]reject \\(H_0\\) \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"linear-combination-of-treatment-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.4 Linear Combination of Treatment Means","text":"just like contrast \\(L = \\sum_{=1}^c_i \\mu_i\\) restrictions \\(c_i\\) coefficients.Tests single treatment mean, two treatment means, contrasts can considered form perspective.\\[\n\\begin{aligned}\n&H_0: \\sum c_i \\mu_i = c \\\\\n&H_a: \\sum c_i \\mu_i \\neq c\n\\end{aligned}\n\\]test statistics ( \\(t\\)-stat) can considered equivalently \\(F\\)-tests; \\(F = (T)^2\\) \\(F \\sim F_{1,N-}\\). Since numerator degrees freedom always 1 cases, refer single-degree--freedom tests.Multiple ContrastsTo test simultaneously \\(k \\ge 2\\) contrasts, let \\(T_1,...,T_k\\) t-stat. joint distribution random variables multivariate t-distribution (tests dependent since re based data).Limitations comparing multiple contrasts:confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 \\(t\\)-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 \\(t\\)-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.\noften, results experiment suggest important (.e.,..g, potential significant) relationships.\nprocess studying effects suggests data called data snooping\nconfidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.often, results experiment suggest important (.e.,..g, potential significant) relationships.process studying effects suggests data called data snoopingMultiple Comparison Procedures:TukeyScheffeBonferroni","code":""},{},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"multiple-comparisons-with-a-control","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.5 Multiple comparisons with a control","text":"","code":""},{},{"path":"analysis-of-variance-anova.html","id":"summary-4","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.1.3.6 Summary","text":"choosing multiple contrast method:Pairwise\nEqual groups sizes: Tukey\nUnequal groups sizes: Tukey, Scheffe\nPairwiseEqual groups sizes: TukeyUnequal groups sizes: Tukey, ScheffeNot pairwise\ncontrol: Dunnett\ngeneral: Bonferroni, Scheffe\npairwisewith control: Dunnettgeneral: Bonferroni, Scheffe","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-random-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2 Single Factor Random Effects Model","text":"Also known ANOVA Type II models.Treatments chosen larger population. extend inference treatments population restrict inference treatments happened selected study.","code":""},{"path":"analysis-of-variance-anova.html","id":"random-cell-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1 Random Cell Means","text":"\\[\nY_{ij} = \\mu_i + \\epsilon_{ij}\n\\]\\(\\mu_i \\sim N(\\mu, \\sigma^2_{\\mu})\\) independent\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent\\(\\mu_i\\) \\(\\epsilon_{ij}\\) mutually independent \\(=1,...,; j = 1,...,n\\)treatment sample sizes equal\\[\n\\begin{aligned}\nE(Y_{ij}) &= E(\\mu_i) = \\mu \\\\\nvar(Y_{ij}) &= var(\\mu_i) + var(\\epsilon_i) = \\sigma^2_{\\mu} + \\sigma^2\n\\end{aligned}\n\\]Since \\(Y_{ij}\\) independent\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\\\\n&= E(\\mu_i^2 + \\mu_i \\epsilon_{ij'} + \\mu_i \\epsilon_{ij} + \\epsilon_{ij}\\epsilon_{ij'}) - \\mu^2 \\\\\n&= \\sigma^2_{\\mu} + \\mu^2 - \\mu^2 & \\text{} j \\neq j' \\\\\n&= \\sigma^2_{\\mu} & \\text{} j \\neq j'\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{'j'}) &= E(\\mu_i \\mu_{'} + \\mu_i \\epsilon_{'j'}+ \\mu_{'}\\epsilon_{ij}+ \\epsilon_{ij}\\epsilon_{'j'}) - \\mu^2 \\\\\n&= \\mu^2 - \\mu^2 & \\text{} \\neq ' \\\\\n&= 0 \\\\\n\\end{aligned}\n\\]Hence,observations varianceany two observations treatment covariance \\(\\sigma^2_{\\mu}\\)correlation two responses treatment:\\[\n\\begin{aligned}\n\\rho(Y_{ij},Y_{ij'}) &= \\frac{\\sigma^2_{\\mu}}{\\sigma^2_{\\mu}+ \\sigma^2} && \\text{$j \\neq j'$}\n\\end{aligned}\n\\]InferenceIntraclass Correlation Coefficient\\[\n\\frac{\\sigma^2_{\\mu}}{\\sigma^2 + \\sigma^2_{\\mu}}\n\\]measures proportion total variability \\(Y_{ij}\\) accounted variance \\(\\mu_i\\)\\[\n\\begin{aligned}\n&H_0: \\sigma_{\\mu}^2 = 0 \\\\\n&H_a: \\sigma_{\\mu}^2 \\neq 0\n\\end{aligned}\n\\]\\(H_0\\) implies \\(\\mu_i = \\mu\\) , can tested F-test ANOVA.understandings Single Factor Fixed Effects Model Single Factor Random Effects Model different, ANOVA one factor model. difference expected mean squaresIf \\(\\sigma^2_\\mu\\), MSE MSTR expectation (\\(\\sigma^2\\)). Otherwise, \\(E(MSTR) >E(MSE)\\). Large values statistic\\[\nF = \\frac{MSTR}{MSE}\n\\]suggest reject \\(H_0\\).Since \\(F \\sim F_{(-1,(n-1))}\\) \\(H_0\\) holds. \\(F > f_{(1-\\alpha;-1,(n-1))}\\) reject \\(H_0\\).sample sizes equal, \\(F\\)-test can still used, df \\(-1\\) \\(N-\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-mu","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.1 Estimation of \\(\\mu\\)","text":"unbiased estimator \\(E(Y_{ij})=\\mu\\) grand mean: \\(\\hat{\\mu} = \\hat{Y}_{..}\\)variance estimator \\[\n\\begin{aligned}\nvar(\\bar{Y}_{..}) &= var(\\sum_i \\bar{Y}_{.}/) \\\\\n&= \\frac{1}{^2}\\sum_ivar(\\bar{Y}_{.}) \\\\\n&= \\frac{1}{^2}\\sum_i(\\sigma^2_\\mu+\\sigma^2/n) \\\\\n&= \\frac{1}{^2}(\\sigma^2_{\\mu}+\\sigma^2/n) \\\\\n&= \\frac{n\\sigma^2_{\\mu}+ \\sigma^2}{}\n\\end{aligned}\n\\]unbiased estimator variance \\(s^2(\\bar{Y})=\\frac{MSTR}{}\\). Thus \\(\\frac{\\bar{Y}_{..}-\\mu}{s(\\bar{Y}_{..})} \\sim t_{-1}\\)\\(1-\\alpha\\) confidence interval \\(\\bar{Y}_{..} \\pm t_{(1-\\alpha/2;-1)}s(\\bar{Y}_{..})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_musigma2_musigma2","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.2 Estimation of \\(\\sigma^2_\\mu/(\\sigma^2_{\\mu}+\\sigma^2)\\)","text":"random fixed effects model, MSTR MSE independent. sample sizes equal (\\(n_i = n\\) ),\\[\n\\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\sim f_{(-1,(n-1))}\n\\]\\[\nP(f_{(\\alpha/2;-1,(n-1))}\\le \\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\le f_{(1-\\alpha/2;-1,(n-1))}) = 1-\\alpha\n\\]\\[\n\\begin{aligned}\nL &= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(1-\\alpha/2;-1,(n-1))}})-1) \\\\\nU &= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(\\alpha/2;-1,(n-1))}})-1)\n\\end{aligned}\n\\]lower upper \\((L^*,U^*)\\) confidence limits \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\)\\[\n\\begin{aligned}\nL^* &= \\frac{L}{1+L} \\\\\nU^* &= \\frac{U}{1+U}\n\\end{aligned}\n\\]lower limit \\(\\frac{\\sigma^2_\\mu}{\\sigma^2}\\) negative, customary set \\(L = 0\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.3 Estimation of \\(\\sigma^2\\)","text":"\\((n-1)MSE/\\sigma^2 \\sim \\chi^2_{(n-1)}\\), \\((1-\\alpha)\\) confidence interval \\(\\sigma^2\\):\\[\n\\frac{(n-1)MSE}{\\chi^2_{1-\\alpha/2;(n-1)}} \\le \\sigma^2 \\le \\frac{(n-1)MSE}{\\chi^2_{\\alpha/2;(n-1)}}\n\\]can also used case sample sizes equal - df N-.","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_mu","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.1.4 Estimation of \\(\\sigma^2_\\mu\\)","text":"\\(E(MSE) = \\sigma^2\\) \\(E(MSTR) = \\sigma^2 + n\\sigma^2_\\mu\\). Hence,\\[\n\\sigma^2_{\\mu} = \\frac{E(MSTR)- E(MSE)}{n}\n\\]unbiased estimator \\(\\sigma^2_\\mu\\) given \\[\ns^2_\\mu =\\frac{MSTR-MSE}{n}\n\\]\\(s^2_\\mu < 0\\), set \\(s^2_\\mu = 0\\)sample sizes equal,\\[\ns^2_\\mu = \\frac{MSTR - MSE}{n'}\n\\]\\(n' = \\frac{1}{-1}(\\sum_i n_i- \\frac{\\sum_i n^2_i}{\\sum_i n_i})\\)exact confidence intervals \\(\\sigma^2_\\mu\\), can approximate intervals.Satterthewaite Procedure can used construct approximate confidence intervals linear combination expected mean squares\nlinear combination:\\[\n\\sigma^2_\\mu = \\frac{1}{n} E(MSTR) + (-\\frac{1}{n}) E(MSE)\n\\]\\[\nS = d_1 E(MS_1) + ..+ d_h E(MS_h)\n\\]\\(d_i\\) coefficients.unbiased estimator S \\[\n\\hat{S} = d_1 MS_1 + ...+ d_h  MS_h\n\\]Let \\(df_i\\) degrees freedom associated mean square \\(MS_i\\). Satterthwaite approximation:\\[\n\\frac{(df)\\hat{S}}{S} \\sim \\chi^2_{df}\n\\]\\[\ndf = \\frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}\n\\]approximate \\(1-\\alpha\\) confidence interval S:\\[\n\\frac{(df)\\hat{S}}{\\chi^2_{1-\\alpha/2;df}} \\le S \\le \\frac{(df)\\hat{S}}{\\chi^2_{\\alpha/2;df}}\n\\]single factor random effects model\\[\n\\frac{(df)s^2_\\mu}{\\chi^2_{1-\\alpha/2;df}} \\le \\sigma^2_\\mu \\le \\frac{(df)s^2_\\mu}{\\chi^2_{\\alpha/2;df}}\n\\]\\[\ndf = \\frac{(sn^2_\\mu)^2}{\\frac{(MSTR)^2}{-1}+ \\frac{(MSE)^2}{(n-1)}}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"random-treatment-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.2.2 Random Treatment Effects Model","text":"\\[\n\\tau_i = \\mu_i - E(\\mu_i) = \\mu_i - \\mu\n\\]\\(\\mu_i = \\mu + \\tau_i\\) \\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]\\(\\mu\\) = constant, common observations\\(\\tau_i \\sim N(0,\\sigma^2_\\tau)\\) independent (random variables)\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent.\\(\\tau_{}, \\epsilon_{ij}\\) independent (=1,…,; j =1,..,n)model concerned balanced single factor ANOVA.Diagnostics MeasuresNon-constant error variance (plots, Levene test, Hartley test).Non-independence errors (plots, Durban-Watson test).Outliers (plots, regression methods).Non-normality error terms (plots, Shapiro-Wilk, Anderson-Darling).Omitted Variable Bias (plots)RemedialWeighted Least SquaresTransformationsNon-parametric Procedures.NoteFixed effect ANOVA relatively robust \nnon-normality\nunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.\nFixed effect ANOVA relatively robust tonon-normalityunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.Lack independence can seriously affect fixed random effect ANVOA.Lack independence can seriously affect fixed random effect ANVOA.","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-fixed-effect-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3 Two Factor Fixed Effect ANOVA","text":"multi-factor experiment ismore efficientprovides infogives validity findings.","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1 Balanced","text":"Assumption:treatment sample sizes equalAll treatment means equal importanceAssume:Factor \\(\\) levels Factor \\(B\\) b levels. \\(\\times b\\) factor levels considered.number treatments level n. \\(N = abn\\) observations study.","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1.1 Cell Means Model","text":"\\[\nY_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{ij}\\) fixed parameters (cell means)\\(= 1,...,\\) = levels Factor \\(j = 1,...,b\\) = levels Factor B.\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(= 1,...,\\), \\(j = 1,..,b\\) \\(k = 1,..,n\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{ij} \\\\\nvar(Y_{ijk}) &= var(\\epsilon_{ijk}) = \\sigma^2\n\\end{aligned}\n\\]Hence,\\[\nY_{ijk} \\sim \\text{indep } N(\\mu_{ij},\\sigma^2)\n\\]model \\[\n\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\n\\]Thus,\\[\n\\begin{aligned}\nE(\\mathbf{Y}) &= \\mathbf{X}\\beta \\\\\nvar(\\mathbf{Y}) &= \\sigma^2 \\mathbf{}\n\\end{aligned}\n\\]Interaction\\[\n(\\alpha \\beta)_{ij} = \\mu_{ij} - (\\mu_{..}+ \\alpha_i + \\beta_j)\n\\]\\(\\mu_{..} = \\sum_i \\sum_j \\mu_{ij}/ab\\) grand mean\\(\\alpha_i = \\mu_{.}-\\mu_{..}\\) main effect factor \\(\\) \\(\\)-th level\\(\\beta_j = \\mu_{.j} - \\mu_{..}\\) main effect factor \\(B\\) \\(j\\)-th level\\((\\alpha \\beta)_{ij}\\) interaction effect factor \\(\\) \\(\\)-th level factor \\(B\\) \\(j\\)-th level.\\((\\alpha \\beta)_{ij} = \\mu_{ij} - \\mu_{.}-\\mu_{.j}+ \\mu_{..}\\)Examine interactions:Examine whether \\(\\mu_{ij}\\) can expressed sums \\(\\mu_{..} + \\alpha_i + \\beta_j\\)Examine whether difference mean responses two levels factor \\(B\\) levels factor \\(\\).Examine whether difference mean response two levels factor \\(\\) levels factor \\(B\\)Examine whether treatment mean curves different factor levels treatment plot parallel.\\(j = 1,...,b\\)\\[\n\\begin{aligned}\n\\sum_i(\\alpha \\beta)_{ij} &= \\sum_i (\\mu_{ij} - \\mu_{..} - \\alpha_i - \\beta_j) \\\\\n&= \\sum_i \\mu_{ij} - \\mu_{..} - \\sum_i \\alpha_i - \\beta_j \\\\\n&= \\mu_{.j} - \\mu_{..}- \\sum_i (\\mu_{.} - \\mu_{..}) - (\\mu_{.j}-\\mu_{..}) \\\\\n&= \\mu_{.j} - \\mu_{..} - \\mu_{..}+ \\mu_{..} - (\\mu_{.j} - \\mu_{..}) \\\\\n&= 0\n\\end{aligned}\n\\]Similarly, \\(\\sum_j (\\alpha \\beta) = 0, = 1,...,\\) \\(\\sum_i \\sum_j (\\alpha \\beta)_{ij} =0\\), \\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"factor-effects-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.1.2 Factor Effects Model","text":"\\[\n\\begin{aligned}\n\\mu_{ij} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\end{aligned}\n\\]\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i=0\\)\\(\\beta_j\\) constants subject restriction \\(\\sum_j \\beta_j = 0\\)\\((\\alpha \\beta)_{ij}\\) constants subject restriction \\(\\sum_i(\\alpha \\beta)_{ij} = 0\\) \\(j=1,...,b\\) \\(\\sum_j(\\alpha \\beta)_{ij} = 0\\) \\(= 1,...,\\)\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(k = 1,..,n\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\\\\nvar(Y_{ijk}) &= \\sigma^2 \\\\\nY_{ijk} &\\sim N (\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}, \\sigma^2)\n\\end{aligned}\n\\]\\(1++b+ab\\) parameters. \\(ab\\) parameters Cell Means Model. Factor Effects Model, restrictions limit number parameters can estimated:\\[\n\\begin{aligned}\n1 &\\text{ } \\mu_{..} \\\\\n(-1) &\\text{ } \\alpha_i \\\\\n(b-1) &\\text{ } \\beta_j \\\\\n(-1)(b-1) &\\text{ } (\\alpha \\beta)_{ij}\n\\end{aligned}\n\\]Hence, \\[\n1 + - 1 + b - 1 + ab - - b + 1 = ab\n\\]parameters model.can several restrictions considering model form \\(\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\\)One way:\\[\n\\begin{aligned}\n\\alpha_a  &= \\alpha_1 - \\alpha_2 - ... - \\alpha_{-1} \\\\\n\\beta_b &= -\\beta_1 - \\beta_2 - ... - \\beta_{b-1} \\\\\n(\\alpha \\beta)_{ib} &= -(\\alpha \\beta)_{i1} -(\\alpha \\beta)_{i2} -...-(\\alpha \\beta)_{,b-1} ; = 1,..,\\\\\n(\\alpha \\beta)_{aj}& = -(\\alpha \\beta)_{1j}-(\\alpha \\beta)_{2j} - ... -(\\alpha \\beta)_{-1,j}; j = 1,..,b\n\\end{aligned}\n\\]can fit model least squares maximum likelihoodCell Means Model\nminimize\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk}-\\mu_{ij})^2\n\\]estimators\\[\n\\begin{aligned}\n\\hat{\\mu}_{ij} &= \\bar{Y}_{ij} \\\\\n\\hat{Y}_{ijk} &= \\bar{Y}_{ij} \\\\\ne_{ijk} = Y_{ijk} - \\hat{Y}_{ijk} &= Y_{ijk} - \\bar{Y}_{ij}\n\\end{aligned}\n\\]Factor Effects Model\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{..}-\\alpha_i = \\beta_j - (\\alpha \\beta)_{ij})^2\n\\]subject restrictions\\[\n\\begin{aligned}\n\\sum_i \\alpha_i &= 0 \\\\\n\\sum_j \\beta_j &= 0 \\\\\n\\sum_i (\\alpha \\beta)_{ij} &= 0 \\\\\n\\sum_j (\\alpha \\beta)_{ij} &= 0\n\\end{aligned}\n\\]estimators\\[\n\\begin{aligned}\n\\hat{\\mu}_{..} &= \\bar{Y}_{...} \\\\\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\beta}_j &= \\bar{Y}_{.j.}-\\bar{Y}_{...} \\\\\n(\\hat{\\alpha \\beta})_{ij} &= \\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.}+ \\bar{Y}_{...}\n\\end{aligned}\n\\]fitted values\\[\n\\hat{Y}_{ijk} = \\bar{Y}_{...}+ (\\bar{Y}_{..}- \\bar{Y}_{...})+ (\\bar{Y}_{.j.}- \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{..}-\\bar{Y}_{.j.}+\\bar{Y}_{...}) = \\bar{Y}_{ij.}\n\\]\\[\n\\begin{aligned}\ne_{ijk} &= Y_{ijk} - \\bar{Y}_{ij.} \\\\\ne_{ijk} &\\sim \\text{ indep } (0,\\sigma^2)\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ns^2_{\\hat{\\mu}..} &= \\frac{MSE}{nab} \\\\\ns^2_{\\hat{\\alpha}_i} &= MSE(\\frac{1}{nb} - \\frac{1}{nab}) \\\\\ns^2_{\\hat{\\beta}_j} &= MSE(\\frac{1}{na} - \\frac{1}{nab}) \\\\\ns^2_{(\\hat{\\alpha\\beta})_{ij}} &= MSE (\\frac{1}{n} - \\frac{1}{na}- \\frac{1}{nb} + \\frac{1}{nab})\n\\end{aligned}\n\\]","code":""},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"unbalanced","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.3.2 Unbalanced","text":"unequal numbers replications treatment combinations:Observational studiesDropouts designed studiesLarger sample sizes inexpensive treatmentsSample sizes match population makeup.Assume factor combination least 1 observation (empty cells)Consider model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]sample sizes : \\(n_{ij}\\):\\[\n\\begin{aligned}\nn_{.} &= \\sum_j n_{ij} \\\\\nn_{.j} &= \\sum_i n_{ij} \\\\\nn_T &= \\sum_i \\sum_j n_{ij}\n\\end{aligned}\n\\]Problem \\[\nSSTO \\neq SSA + SSB + SSAB + SSE\n\\](design non-orthogonal)\\(= 1,...,-1,\\)\\[\nu_i = \\begin{cases} +1 & \\text{obs -th level Factor 1} \\\\ -1 & \\text{obs -th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\ \\end{cases}\n\\]\\(j=1,...,b-1\\)\\[\nv_i =\n\\begin{cases} +1 & \\text{obs j-th level Factor 1} \\\\ -1 & \\text{obs b-th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\\n\\end{cases}\n\\]can use indicator variables predictor variables \\(\\mu_{..}, \\alpha_i ,\\beta_j, (\\alpha \\beta)_{ij}\\) unknown parameters.\\[\nY = \\mu_{..} + \\sum_{=1}^{-1} \\alpha_i u_i + \\sum_{j=1}^{b-1} \\beta_j v_j + \\sum_{=1}^{-1} \\sum_{j=1}^{b-1}(\\alpha \\beta)_{ij} u_i v_j + \\epsilon\n\\]test hypotheses, use extra sum squares idea.interaction effects\\[\n\\begin{aligned}\n&H_0: (\\alpha \\beta)_{ij} = 0 \\\\\n&H_a: \\text{}(\\alpha \\beta)_{ij} =0\n\\end{aligned}\n\\]test\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\\n&H_a: \\text{} \\beta_j = 0\n\\end{aligned}\n\\]Analysis Factor Means(e.g., contrasts) analogous balanced case, modifications formulas means standard errors account unequal sample sizes., can fit cell means model consider regression perspectiveIf empty cells (.e., factor combinations observation), equivalent regression approach can’t used. can still partial analyses","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-random-effects-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.4 Two-Way Random Effects ANOVA","text":"\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i \\sim N(0,\\sigma^2_{\\alpha}), = 1,..,\\) (independent)\\(\\beta_j \\sim N(0,\\sigma^2_{\\beta}), j = 1,..,b\\) (independent)\\((\\alpha \\beta)_{ij} \\sim N(0,\\sigma^2_{\\alpha \\beta}),=1,...,,j=1,..,b\\) (independent)\\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent)\\(\\alpha_i, \\beta_j, (\\alpha \\beta)_{ij}\\) pairwise independentTheoretical means, variances, covariances \\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} \\\\\nvar(Y_{ijk}) &= \\sigma^2_Y= \\sigma^2_\\alpha + \\sigma^2_\\beta +  \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\end{aligned}\n\\]\\(Y_{ijk} \\sim N(\\mu_{..},\\sigma^2_\\alpha + \\sigma^2_\\beta + \\sigma^2_{\\alpha \\beta} + \\sigma^2)\\)\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{ij'k'}) &= \\sigma^2_{\\alpha}, j \\neq j' \\\\\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_{\\beta}, \\neq '\\\\\ncov(Y_{ijk},Y_{ijk'}) &= \\sigma^2_\\alpha + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= , \\neq ', j \\neq j'\n\\end{aligned}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-mixed-effects-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5 Two-Way Mixed Effects ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5.1 Balanced","text":"One fixed factor, random treatment levels, mixed effects model mixed modelRestricted mixed model 2-way ANOVA:\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i\\): fixed effects constraints subject restriction \\(\\sum \\alpha_i = 0\\)\\(\\beta_j \\sim indep N(0,\\sigma^2_\\beta)\\)\\((\\alpha \\beta)_{ij} \\sim N(0,\\frac{-1}{}\\sigma^2_{\\alpha \\beta})\\) subject restriction \\(\\sum_i (\\alpha \\beta)_{ij} = 0\\) j, variance written proportion convenience; makes expected mean squares simpler (assumed \\(var((\\alpha \\beta)_{ij}= \\sigma^2_{\\alpha \\beta}\\))\\(cov((\\alpha \\beta)_{ij},(\\alpha \\beta)_{'j'}) = - \\frac{1}{} \\sigma^2_{\\alpha \\beta}, \\neq '\\)\\(\\epsilon_{ijk}\\sim indepN(0,\\sigma^2)\\)\\(\\beta_j, (\\alpha \\beta)_{ij}, \\epsilon_{ijk}\\) pairwise independentTwo-way mixed models written “unrestricted” form, restrictions interaction effects \\((\\alpha \\beta)_{ij}\\), pairwise independent.Let \\(\\beta^*, (\\alpha \\beta)^*_{ij}\\) unrestricted random effects, \\((\\bar{\\alpha \\beta})_{ij}^*\\) means averaged fixed factor level random factor B.\\[\n\\begin{aligned}\n\\beta_j &= \\beta_j^* + (\\bar{\\alpha \\beta})_{ij}^* \\\\\n(\\alpha \\beta)_{ij} &= (\\alpha \\beta)_{ij}^* - (\\bar{\\alpha \\beta})_{ij}^*\n\\end{aligned}\n\\]consider restricted model general. consider restricted form.\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) &= \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\end{aligned}\n\\]Responses random factor \\((B)\\) level correlated\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{ijk'}) &= E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\\\\n&= \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} , k \\neq k'\n\\end{aligned}\n\\]Similarly,\\[\n\\begin{aligned}\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_\\beta - \\frac{1}{} \\sigma^2_{\\alpha\\ \\beta}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= 0,  j \\neq j'\n\\end{aligned}\n\\]Hence, can see way don’t dependence \\(Y\\) don’t share random effect.advantage restricted mixed model 2 observations random factor b level can positively negatively correlated. unrestricted model, can positively correlated.Fixed ANOVA(, B Fixed)Random ANOVA(,B random)Mixed ANVOA(fixed, B random)fixed, random, mixed models (balanced), ANOVA table sums squares calculations identical. (also true df mean squares). difference expected mean squares, thus test statistics.Random ANOVA, test\\[\n\\begin{aligned}\n&H_0: \\sigma^2 = 0 \\\\\n&H_a: \\sigma^2 > 0\n\\end{aligned}\n\\]considering \\(F= \\frac{MSA}{MSAB} \\sim F_{-1;(-1)(b-1)}\\)test statistic used mixed models, case testing null hypothesis \\(\\alpha_i = 0\\)test statistic different null hypothesis fixed effects model.Fixed ANOVA(&B fixed)Random ANOVA(&B random)Mixed ANOVA(fixed, B random)Estimation Variance ComponentsIn random mixed effects models, interested estimating variance components\nVariance component \\(\\sigma^2_\\beta\\) mixed ANOVA.\\[\nE(\\sigma^2_\\beta) = \\frac{E(MSB)-E(MSE)}{na} = \\frac{\\sigma^2 + na \\sigma^2_\\beta - \\sigma^2}{na} = \\sigma^2_\\beta\n\\]can estimated \\[\n\\hat{\\sigma}^2_\\beta = \\frac{MSB - MSE}{na}\n\\]Confidence intervals variance components can constructed (approximately) using Satterthwaite procedure MLS procedure (like 1-way random effects)Estimation Fixed Effects Mixed Models\\[\n\\begin{aligned}\n\\hat{\\alpha}_i &= \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\mu}_{.} &= \\bar{Y}_{...} + (\\bar{Y}_{..}- \\bar{Y}_{...}) = \\bar{Y}_{..}  \\\\\n\\sigma^2(\\hat{\\alpha}_i) &= \\frac{\\sigma^2 + n \\sigma^2_{\\alpha \\beta}}{bn} = \\frac{E(MSAB)}{bn} \\\\\ns^2(\\hat{\\alpha}_i) &= \\frac{MSAB}{bn}\n\\end{aligned}\n\\]Contrasts Fixed Effects\\[\n\\begin{aligned}\nL &= \\sum c_i \\alpha_i \\\\\n\\sum c_i &= 0 \\\\\n\\hat{L} &= \\sum c_i \\hat{\\alpha}_i \\\\\n\\sigma^2(\\hat{L}) &= \\sum c^2_i \\sigma^2 (\\hat{\\alpha}_i) \\\\\ns^2(\\hat{L}) &= \\frac{MSAB}{bn} \\sum c^2_i\n\\end{aligned}\n\\]Confidence intervals tests can constructed usual","code":""},{"path":"analysis-of-variance-anova.html","id":"unbalanced-1","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.1.5.2 Unbalanced","text":"mixed model = 2, b = 4\\[\n\\begin{aligned}\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\\\\nvar(\\beta_j)&= \\sigma^2_\\beta \\\\\nvar((\\alpha \\beta)_{ij})&= \\frac{2-1}{2}\\sigma^2_{\\alpha \\beta} = \\frac{\\sigma^2_{\\alpha \\beta}}{2} \\\\\nvar(\\epsilon_{ijk}) &= \\sigma^2 \\\\\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) &= \\sigma^2_{\\beta} + \\frac{\\sigma^2_{\\alpha \\beta}}{2} + \\sigma^2 \\\\\ncov(Y_{ijk},Y_{ijk'}) &= \\sigma^2 + \\frac{\\sigma^2_{\\alpha \\beta}}{2}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'jk'}) &= \\sigma^2_{\\beta} - \\frac{\\sigma^2_{\\alpha \\beta}}{2}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) &= 0, j \\neq j'\n\\end{aligned}\n\\]assume\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\beta, M)\n\\]\\(M\\) block diagonaldensity function\\[\nf(\\mathbf{Y}) = \\frac{1}{(2\\pi)^{N/2}|M|^{1/2}}exp(-\\frac{1}{2}\\mathbf{(Y - X \\beta)' M^{-1}(Y-X\\beta)})\n\\]knew variance components, use GLS:\\[\n\\hat{\\beta}_{GLS} = \\mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}\n\\]usually don’t know variance components \\(\\sigma^2, \\sigma^2_\\beta, \\sigma^2_{\\alpha \\beta}\\) make \\(M\\)\nAnother way get estimates Maximum likelihood estimationwe try maximize log\\[\n\\ln L = - \\frac{N}{2} \\ln (2\\pi) - \\frac{1}{2}\\ln|M| - \\frac{1}{2} \\mathbf{(Y-X \\beta)'\\Sigma^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"nonparametric-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2 Nonparametric ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"kruskal-wallis","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2.1 Kruskal-Wallis","text":"Generalization independent samples Wilcoxon Rank sum test 2 independent samples (like F-test one-way ANOVA generalization several independent samples two sample t-test)Consider one-way case:\\(\\ge2\\) treatments\\(n_i\\) sample size \\(\\)-th treatment\\(Y_{ij}\\) \\(j\\)-th observation \\(\\)-th treatment.make assumption normalityWe assume observations \\(\\)-th treatment random sample continuous CDF \\(F_i\\), = 1,..,n, mutually independent.\\[\n\\begin{aligned}\n&H_0: F_1 = F_2 = ... = F_a \\\\\n&H_a: F_i < F_j \\text{ } \\neq j\n\\end{aligned}\n\\]distribution location-scale family, \\(H_0: \\theta_1 = \\theta_2 = ... = \\theta_a\\))ProcedureRank \\(N = \\sum_{=1}^n_i\\) observations ascending order. Let \\(r_{ij} = rank(Y_{ij})\\), note \\(\\sum_i \\sum_j r_{ij} = 1 + 2 .. + N = \\frac{N(N+1)}{2}\\)Calculate rank sums averages:\\[\nr_{.} = \\sum_{j=1}^{n_i} r_{ij}\n\\] \\[\n\\bar{r}_{.} = \\frac{r_{.}}{n_i}, = 1,..,\n\\]Calculate test statistic ranks: \\[\n\\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}}\n\\] \\(SSTR = \\sum n_i (\\bar{r}_{.}- \\bar{r}_{..})^2\\) \\(SSTO = \\sum \\sum (\\bar{r}_{ij}- \\bar{r}_{..})^2\\)large \\(n_i\\) (\\(\\ge 5\\) observations) Kruskal-Wallis statistic approximated \\(\\chi^2_{-1}\\) distribution treatment means equal. Hence, reject \\(H_0\\) \\(\\chi^2_{KW} > \\chi^2_{(1-\\alpha;-1)}\\).sample sizes small, one can exhaustively work possible distinct ways assigning N ranks observations treatments calculate value KW statistic case (\\(\\frac{N!}{n_1!..n_a!}\\) possible combinations). \\(H_0\\) assignments equally likely.","code":""},{"path":"analysis-of-variance-anova.html","id":"friedman-test","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.2.2 Friedman Test","text":"responses \\(Y_{ij} = 1,..,n, j = 1,..,r\\) randomized complete block design normally distributed (constant variance), nonparametric test helpful.distribution-free rank-based test comparing treatments setting Friedman test. Let \\(F_{ij}\\) CDF random \\(Y_{ij}\\), corresponding observed value \\(y_{ij}\\)null hypothesis, \\(F_{ij}\\) identical treatments j separately block .\\[\n\\begin{aligned}\n&H_0: F_{i1} = F_{i2} = ... = F_{ir}  \\text{ } \\\\\n&H_a: F_{ij} < F_{ij'} \\text{ } j \\neq j' \\text{ } \n\\end{aligned}\n\\]location parameter distributions, treatment effects can tested:\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = ... = \\tau_r \\\\\n&H_a: \\tau_j > \\tau_{j'} \\text{ } j \\neq j'\n\\end{aligned}\n\\]ProcedureRank observations r treatments separately within block (ascending order; ties, tied observation given mean ranks involved). Let ranks called \\(r_{ij}\\)Calculate Friedman test statistic\\[\n\\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}}\n\\] \\[\n\\begin{aligned}\nSSTR &= n \\sum (\\bar{r}_{.j}-\\bar{r}_{..})^2 \\\\\nSSE &= \\sum \\sum (r_{ij} - \\bar{r}_{.j})^2 \\\\\n\\bar{r}_{.j} &= \\frac{\\sum_i r_{ij}}{n}\\\\\n\\bar{r}_{..} &= \\frac{r+1}{2}\n\\end{aligned}\n\\]ties, can rewritten \\[\n\\chi^2_{F} = [\\frac{12}{nr(n+1)}\\sum_j r_{.j}^2] - 3n(r+1)\n\\]large number blocks, \\(\\chi^2_F\\) approximately \\(\\chi^2_{r-1}\\) \\(H_0\\). Hence, reject \\(H_0\\) \\(\\chi^2_F > \\chi^2_{(1-\\alpha;r-1)}\\)\nexact null distribution \\(\\chi^2_F\\) can derived since r! possible ways assigning ranks 1,2,…,r r observations within block. n blocks thus \\((r!)^n\\) possible assignments ranks, equally likely \\(H_0\\) true.","code":""},{"path":"analysis-of-variance-anova.html","id":"sample-size-planning-for-anova","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3 Sample Size Planning for ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1 Balanced Designs","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-studies","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.1 Single Factor Studies","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"fixed-cell-means","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.1.1 Fixed cell means","text":"\\[\nP(F>f_{(1-\\alpha;-1,N-)}|\\phi) = 1 - \\beta\n\\]\\(\\phi\\) non-centrality parameter (measures unequal treatment means \\(\\mu_i\\) )\\[\n\\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n}{}\\sum_i (\\mu_i - \\mu_.)^2} , (n_i \\equiv n)\n\\]\\[\n\\mu_. = \\frac{\\sum \\mu_i}{}\n\\]decide power probabilities use non-central F distribution.use power table directly effects fixed design balanced using minimum range factor level means desired differences\\[\n\\Delta = \\max(\\mu_i) - \\min(\\mu_i)\n\\]Hence, need\\(\\alpha\\) level\\(\\Delta\\)\\(\\sigma\\)\\(\\beta\\)Notes:\\(\\Delta/\\sigma\\) small greatly affects sample size, \\(\\Delta/\\sigma\\) large.Reducing \\(\\alpha\\) \\(\\beta\\) increases required sample sizes.Error estimating \\(\\sigma\\) can make large difference.","code":""},{"path":"analysis-of-variance-anova.html","id":"multi-factor-studies","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.1.2 Multi-factor Studies","text":"noncentral \\(F\\) tables can used hereFor two-factor fixed effect modelTest interactions:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\alpha \\beta_{ij})^2}{(-1)(b-1)+1}} = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\mu_{ij}- \\mu_{.} - \\mu_{.j} + \\mu_{..})^2}{(-1)(b-1)+1}} \\\\\n\\upsilon_1 &= (-1)(b-1) \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Test Factor \\(\\) main effects:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum \\alpha_i^2}{}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nb \\sum (\\mu_{.}- \\mu_{..})^2}{}} \\\\\n\\upsilon_1 &= -1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Test Factor \\(B\\) main effects:\\[\n\\begin{aligned}\n\\phi &= \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum \\beta_j^2}{b}} = \\frac{1}{\\sigma}\\sqrt{\\frac{na \\sum (\\mu_{.j}- \\mu_{..})^2}{b}} \\\\\n\\upsilon_1 &= b-1 \\\\\n\\upsilon_2 &= ab(n-1)\n\\end{aligned}\n\\]Procedure:Specify minimum range Factor \\(\\) meansObtain sample sizes \\(r = \\). resulting sample size \\(bn\\), \\(n\\) can obtained.Repeat first 2 steps Factor \\(B\\) minimum range.Choose greater number sample size \\(\\) \\(B\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-experiments","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.3.2 Randomized Block Experiments","text":"Analogous completely randomized designs . power F-test treatment effects randomized block design uses non-centrality parameter completely randomized design:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{r} \\sum (\\mu_i - \\mu_.)^2}\n\\]However, power level different randomized block design becauseerror variance \\(\\sigma^2\\) differentdf(MSE) different.","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.4 Randomized Block Designs","text":"improve precision treatment comparisons, can reduce variability among experimental units. can group experimental units blocks block contains relatively homogeneous units.Within block, random assignment treatments units (separate random assignment block)number units per block multiple number factor combinations.Commonly, use treatment block.Benefits BlockingReduction variability estimators treatment means\nImproved power t-tests F-tests\nNarrower confidence intervals\nSmaller MSE\nReduction variability estimators treatment meansImproved power t-tests F-testsNarrower confidence intervalsSmaller MSECompare treatments different conditions (related different blocks).Compare treatments different conditions (related different blocks).Loss Blocking (little lose)don’t blocking well, waste df negligible block effects used estimate \\(\\sigma^2\\)Hence, df \\(t\\)-tests denominator df \\(F\\)-tests reduced without reducing MSE small loss power tests.Consider\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]\\(= 1, 2, \\dots, n\\)\\(j = 1, 2, \\dots, r\\)\\(\\mu_{..}\\): overall mean response, averaging across blocks treatments\\(\\rho_i\\): block effect, average difference response -th block (\\(\\sum \\rho_i =0\\))\\(\\tau_j\\) treatment effect, average across blocks (\\(\\sum \\tau_j = 0\\))\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random experimental error., assume block treatment effects additive. difference average response pair treatments within block\\[\n(\\mu_{..} +  \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j') = \\tau_j - \\tau_j'\n\\]\\(=1,..,n\\) blocks\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\bar{Y}_{..} \\\\\n\\hat{\\rho}_i &= \\bar{Y}_{.} - \\bar{Y}_{..} \\\\\n\\hat{\\tau}_j &= \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\hat{Y}_{ij} &= \\bar{Y}_{..} + (\\bar{Y}_{.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j}- \\bar{Y}_{..}) = \\bar{Y}_{.} + \\bar{Y}_{.j} - \\bar{Y}_{..} \\\\\ne_{ij} &= Y_{ij} - \\hat{Y}_{ij} = Y_{ij}- \\bar{Y}_{.} - \\bar{Y}_{.j} + \\bar{Y}_{..}\n\\end{aligned}\n\\]ANOVA tableFixed TreatmentsE(MS)Random TreatmentsE(MS)F-tests\\[\n\\begin{aligned}\nH_0: \\tau_1 = \\tau_2 = ... = \\tau_r = 0 && \\text{Fixed Treatment Effects} \\\\\nH_a: \\text{} \\tau_j = 0 \\\\\n\\\\\nH_0: \\sigma^2_{\\tau} = 0 && \\text{Random Treatment Effects} \\\\\nH_a: \\sigma^2_{\\tau} \\neq 0\n\\end{aligned}\n\\]cases \\(F = \\frac{MSTR}{MSE}\\), reject \\(H_0\\) \\(F > f_{(1-\\alpha; r-1,(n-1)(r-1))}\\)don’t use F-test compare blocks, becauseWe priori blocs differentRandomization done “within” block.estimate efficiency gained blocking (relative completely randomized design).\\[\n\\begin{aligned}\n\\hat{\\sigma}^2_{CR} &= \\frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\\\\n\\hat{\\sigma}^2_{RB} &= MSE \\\\\n\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}} &= \\text{1} \\\\\n\\end{aligned}\n\\]completely randomized experiment \\[\n(\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}}-1)\\%%\n\\]observations randomized block design get MSEIf batches randomly selected random effects. , experiment repeated, new sample batches selected,d yielding new values \\(\\rho_1, \\rho_2,...,\\rho_i\\) .\\[\n\\rho_1, \\rho_2,...,\\rho_j \\sim N(0,\\sigma^2_\\rho)\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) fixed\\(\\rho_i\\): random iid \\(N(0,\\sigma^2_p)\\)\\(\\tau_j\\) fixed (random) \\(\\sum \\tau_j = 0\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\)Fixed Treatment\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) &= \\sigma^2_{\\rho} + \\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= \\sigma^2 , j \\neq j' \\text{ treatments within block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) &= 0 , \\neq ' , j \\neq j'\n\\end{aligned}\n\\]Correlation 2 observations block\\[\n\\frac{\\sigma^2_{\\rho}}{\\sigma^2 + \\sigma^2_{\\rho}}\n\\]expected MS additive fixed treatment effect, random block effect isInteractions Blocks\nwithout replications within block treatment, can’t consider interaction block treatment block effect fixed. Hence, random block effect, \\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + (\\rho \\tau)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) constant\\(\\rho_i \\sim idd N(0,\\sigma^2_{\\rho})\\) random\\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\))\\((\\rho \\tau)_{ij} \\sim N(0,\\frac{r-1}{r}\\sigma^2_{\\rho \\tau})\\) \\(\\sum_j (\\rho \\tau)_{ij}=0\\) \\(cov((\\rho \\tau)_{ij},(\\rho \\tau)_{ij'})= -\\frac{1}{r} \\sigma^2_{\\rho \\tau}\\) \\(j \\neq j'\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\) randomNote: special case mixed 2-factor model 1 observation per “cell”\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) &= \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, j \\neq j' \\text{ obs block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) &= 0, \\neq ', j \\neq j' \\text{ obs different blocks independent}\n\\end{aligned}\n\\]sum squares degrees freedom interaction model additive model. difference exists expected mean squaresNo exact test possible block effects interaction present (important blocks used primarily reduce experimental error variability)\\(E(MSE) = \\sigma^2 + \\sigma^2_{\\rho \\tau}\\) error term variance interaction variance \\(\\sigma^2_{\\rho \\tau}\\). can’t estimate components separately model. two confounded.1 observation per treatment block combination, one can consider interaction fixed block effects, called generalized randomized block designs (multifactor analysis).","code":""},{"path":"analysis-of-variance-anova.html","id":"tukey-test-of-additivity","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.4.1 Tukey Test of Additivity","text":"(Tukey’s 1 df test additivity)formal test interaction effects blocks treatments randomized block design. can also considered testing additivity 2-way analyses one observation per cell.consider less restricted interaction term\\[\n(\\rho \\tau)_{ij} = D\\rho_i \\tau_j \\text{(D: Constant)}\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + D\\rho_i \\tau_j + \\epsilon_{ij}\n\\]least square estimate MLE D\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j \\rho_i \\tau_j Y_{ij}}{\\sum_i \\rho_i^2 \\sum_j \\tau^2_j}\n\\]replacing parameters estimates\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j (\\bar{Y}_{.}- \\bar{Y}_{..})(\\bar{Y}_{.j}- \\bar{Y}_{..})Y_{ij}}{\\sum_i (\\bar{Y}_{.}- \\bar{Y}_{..})^2 \\sum_j(\\bar{Y}_{.j}- \\bar{Y}_{..})^2}\n\\]Thus, interaction sum squares\\[\nSSint = \\sum_i \\sum_j \\hat{D}^2(\\bar{Y}_{.}- \\bar{Y}_{..})^2(\\bar{Y}_{.j}- \\bar{Y}_{..})^2\n\\]ANOVA decomposition\\[\nSSTO = SSBL + SSTR + SSint + SSRem\n\\]\\(SSRem\\): remainder sum squares\\[\nSSRem = SSTO - SSBL - SSTR - SSint\n\\]\\(D = 0\\) (.e., interactions type \\(D \\rho_i \\tau_j\\)). \\(SSint\\) \\(SSRem\\) independent \\(\\chi^2_{1,rn-r-n}\\).\\(D = 0\\),\\[\nF = \\frac{SSint/1}{SSRem/(rn-r-n)} \\sim f_{(1-\\alpha;rn-r-n)}\n\\]\\[\n\\begin{aligned}\n&H_0: D = 0 \\text{ interaction present} \\\\\n&H_a: D \\neq 0 \\text{ interaction form $D \\rho_i \\tau_j$ present}\n\\end{aligned}\n\\]reject \\(H_0\\) \\(F > f_{(1-\\alpha;1,nr-r-n)}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"nested-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.5 Nested Designs","text":"Let \\(\\mu_{ij}\\) mean response factor -th level factor B j-th level.\nfactors crossed, \\(j\\)-th level B levels .\nfactor B nested within , j-th level B level 1 nothing common j-th level B level 2.Factors can’t manipulated designated classification factors, opposed experimental factors (.e., assign experimental units).","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-nested-designs","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.5.1 Two-Factor Nested Designs","text":"Consider B nested within .factors fixedAll treatment means equally important.Mean responses\\[\n\\mu_{.} = \\sum_j \\mu_{ij}/b\n\\]Main effect factor \\[\n\\alpha_i = \\mu_{.} - \\mu_{..}\n\\]\\(\\mu_{..} = \\frac{\\mu_{ij}}{ab} = \\frac{\\sum_i \\mu_{.}}{}\\) \\(\\sum_i \\alpha_i = 0\\)Individual effects \\(B\\) denoted \\(\\beta_{j()}\\) \\(j()\\) indicates \\(j\\)-th level factor \\(B\\) nested within -h level factor \\[\n\\begin{aligned}\n\\beta_{j()} &= \\mu_{ij} - \\mu_{.} \\\\\n&= \\mu_{ij} - \\alpha_i - \\mu_{..} \\\\\n\\sum_j \\beta_{j()}&=0 , = 1,...,\n\\end{aligned}\n\\]\\(\\beta_{j()}\\) specific effect \\(j\\)-th level factor \\(B\\) nested within \\(\\)-th level factor \\(\\). Hence,\\[\n\\mu_{ij} \\equiv \\mu_{..} + \\alpha_i + \\beta_{j()} \\equiv \\mu_{..} + (\\mu_{.} - \\mu_{..}) + (\\mu_{ij} - \\mu_{.})\n\\]Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk}\n\\]\\(Y_{ijk}\\) response \\(k\\)-th treatment factor \\(\\) \\(\\)-th level factor \\(B\\) \\(j\\)-th level \\((= 1,..,; j = 1,..,b; k = 1,..n)\\)\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i = 0\\)\\(\\beta_{j()}\\) constants subject restriction \\(\\sum_j \\beta_{j()} = 0\\) \\(\\)\\(\\epsilon_{ijk} \\sim iid N(0,\\sigma^2)\\)\\[\n\\begin{aligned}\nE(Y_{ijk}) &= \\mu_{..} + \\alpha_i + \\beta_{j()} \\\\\nvar(Y_{ijk}) &= \\sigma^2\n\\end{aligned}\n\\]interaction term nested modelANOVA Two-Factor Nested DesignsLeast Squares MLE estimatesresidual \\(e_{ijk} = Y_{ijk} - \\bar{Y}_{ijk}\\)\\[\n\\begin{aligned}\nSSTO &= SSA + SSB() + SSE \\\\\n\\sum_i \\sum_j \\sum_k (Y_{ijk}- \\bar{Y}_{...})^2 &= bn \\sum_i (\\bar{Y}_{..}- \\bar{Y}_{...})^2 + n \\sum_i \\sum_j (\\bar{Y}_{ij.}- \\bar{Y}_{..})^2  \\\\\n&+ \\sum_i \\sum_j \\sum_k (Y_{ijk} -\\bar{Y}_{ij.})^2\n\\end{aligned}\n\\]ANOVA TableTests Factor Effects\\[\n\\begin{aligned}\n&H_0: \\text{ } \\alpha_i =0 \\\\\n&H_a: \\text{ } \\alpha_i = 0\n\\end{aligned}\n\\]\\(F = \\frac{MSA}{MSE} \\sim f_{(1-\\alpha;-1,(n-1)ab)}\\) reject \\(F > f\\)\\[\n\\begin{aligned}\n&H_0: \\text{ } \\beta_{j()} =0 \\\\\n&H_a: \\text{ } \\beta_{j()} = 0\n\\end{aligned}\n\\]\\(F = \\frac{MSB()}{MSE} \\sim f_{(1-\\alpha;(b-1),(n-1)ab)}\\) reject \\(F>f\\)Testing Factor Effect Contrasts\\(L = \\sum c_i \\mu_i\\) \\(\\sum c_i =0\\)\\[\n\\begin{aligned}\n\\hat{L} &= \\sum c_i \\bar{Y}_{..} \\\\\n\\hat{L} &\\pm t_{(1-\\alpha/2;df)}s(\\hat{L})\n\\end{aligned}\n\\]\\(s^2(\\hat{L}) = \\sum c_i^2 s^2(\\bar{Y}_{..})\\), \\(s^2(\\bar{Y}_{..}) = \\frac{MSE}{bn}, df = ab(n-1)\\)Testing Treatment Means\\(L = \\sum c_i \\mu_{.j}\\) estimated \\(\\hat{L} = \\sum c_i \\bar{Y}_{ij}\\) confidence limits:\\[\n\\hat{L} \\pm t_{(1-\\alpha/2;(n-1)ab)}s(\\hat{L})\n\\]\\[\ns^2(\\hat{L}) = \\frac{MSE}{n}\\sum c^2_i\n\\]Unbalanced Nested Two-Factor DesignsIf different number levels factor \\(B\\) different levels factor \\(\\), design called unbalancedThe model\\[\n\\begin{aligned}\nY_{ijk} &= \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk} \\\\\n\\sum_{=1}^2 \\alpha_i &=0 \\\\\n\\sum_{j=1}^3 \\beta_{j(1)} &= 0 \\\\\n\\sum_{j=1}^2 \\beta_{j(2)}&=0\n\\end{aligned}\n\\]\\(= 1,2;j =1,..,b_i;k=1,..,n_{ij}\\)\\(= 1,2;j =1,..,b_i;k=1,..,n_{ij}\\)\\(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\)\\(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\)\\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) parameters.\\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) parameters.constraints: \\(\\alpha_2 = - \\alpha_1, \\beta_{3(1)}= - \\beta_{1(1)}-\\beta_{2(1)}, \\beta_{2(2)}=-\\beta_{1(2)}\\)4 indicator variables\\[\\begin{equation}\nX_1 =\n\\begin{cases}\n1&\\text{obs school 1}\\\\\n-1&\\text{obs school 2}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_2 =\n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_3 =\n\\begin{cases}\n1&\\text{obs instructor 2 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_4 =\n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 2 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]Regression Full Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_1 X_{ijk1} + \\beta_{1(1)}X_{ijk2} + \\beta_{2(1)}X_{ijk3} + \\beta_{1(2)}X_{ijk4} + \\epsilon_{ijk}\n\\]Random Factor EffectsIf\\[\n\\begin{aligned}\n\\alpha_1 &\\sim iid N(0,\\sigma^2_\\alpha) \\\\\n\\beta_{j()} &\\sim iid N(0,\\sigma^2_\\beta)\n\\end{aligned}\n\\]Expected Mean SquaresA fixed, B randomExpected Mean SquaresA random, B randomTest StatisticsAnother way increase precision treatment comparisons reducing variability use regression models adjust differences among experimental units (also known analysis covariance).","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-covariance-model","chapter":"21 Analysis of Variance (ANOVA)","heading":"21.6 Single Factor Covariance Model","text":"\\[\nY_{ij} = \\mu_{.} + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) + \\epsilon_{ij}\n\\]\\(= 1,...,r;j=1,..,n_i\\)\\(\\mu_.\\) overall mean\\(\\tau_i\\): fixed treatment effects (\\(\\sum \\tau_i =0\\))\\(\\gamma\\): fixed regression coefficient effect X Y\\(X_{ij}\\) covariate (random)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random errorsIf just use \\(\\gamma X_{ij}\\) regression term (rather \\(\\gamma(X_{ij}-\\bar{X}_{..})\\)), \\(\\mu_.\\) longer overall mean; thus need centered mean.\\[\n\\begin{aligned}\nE(Y_{ij}) &= \\mu_. + \\tau_i + \\gamma(X_{ij}-\\bar{X}_{..}) \\\\\nvar(Y_{ij}) &= \\sigma^2\n\\end{aligned}\n\\]\\(Y_{ij} \\sim N(\\mu_{ij},\\sigma^2)\\),\\[\n\\begin{aligned}\n\\mu_{ij} &= \\mu_. + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) \\\\\n\\sum \\tau_i &=0\n\\end{aligned}\n\\]Thus, mean response (\\(\\mu_{ij}\\)) regression line intercept \\(\\mu_. + \\tau_i\\) slope \\(\\gamma\\) treatment $$.Assumption:treatment regression lines slopewhen treatment interact covariate \\(X\\) (non-parallel slopes), covariance analysis appropriate. case use separate regression lines.complicated regression features (e.g., quadratic, cubic) additional covariates e.g.,\\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma_1(X_{ij1}-\\bar{X}_{..2}) + \\gamma_2(X_{ij2}-\\bar{X}_{..2}) + \\epsilon_{ij}\n\\]Regression FormulationWe can use indicator variables treatments\\[\nl_1 =\n\\begin{cases}\n1 & \\text{case treatment 1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n\\]\\[\n.\n\\]\\[\n.\n\\]\\[\nl_{r-1} =\n\\begin{cases}\n1 & \\text{case treatment r-1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n\\]Let \\(x_{ij} = X_{ij}- \\bar{X}_{..}\\). regression model \\[\nY_{ij} = \\mu_. + \\tau_1l_{ij,1} + .. + \\tau_{r-1}l_{ij,r-1} + \\gamma x_{ij}+\\epsilon_{ij}\n\\]\\(I_{ij,1}\\) indicator variable \\(l_1\\) j-th case treatment . treatment effect \\(\\tau_1,..\\tau_{r-1}\\) just regression coefficients indicator variables.use diagnostic tools case.InferenceTreatment effects\\[\n\\begin{aligned}\n&H_0: \\tau_1 = \\tau_2 = ...= 0 \\\\\n&H_a: \\text{} \\tau_i =0\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n&\\text{Full Model}: Y_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} +\\epsilon_{ij}  \\\\\n&\\text{Reduced Model}: Y_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij}\n\\end{aligned}\n\\]\\[\nF = \\frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \\frac{SSE(F)}{N-(r+1)} \\sim F_{(r-1,N-(r+1))}\n\\]interested comparisons treatment effects.\nexample, r - 3. estimate \\(\\tau_1,\\tau_2, \\tau_3 = -\\tau_1 - \\tau_2\\)Testing Parallel SlopesExample:r = 3\\[\nY_{ij} = \\mu_{.} + \\tau_1 I_{ij,1} + \\tau_2 I_{ij,2} + \\gamma X_{ij} + \\beta_1 I_{ij,1}X_{ij} + \\beta_2 I_{ij,2}X_{ij} + \\epsilon_{ij}\n\\]\\(\\beta_1,\\beta_2\\): interaction coefficients.\\[\n\\begin{aligned}\n&H_0: \\beta_1 = \\beta_2 = 0 \\\\\n&H_a: \\text{least one} \\beta \\neq 0\n\\end{aligned}\n\\]can’t reject \\(H_0\\) using F-test evidence slopes parallelAdjusted MeansThe means response adjusting covariate effect\\[\nY_{.}(adj) = \\bar{Y}_{.} - \\hat{\\gamma}(\\bar{X}_{.} - \\bar{X}_{..})\n\\]","code":""},{"path":"multivariate-methods.html","id":"multivariate-methods","chapter":"22 Multivariate Methods","heading":"22 Multivariate Methods","text":"\\(y_1,...,y_p\\) possibly correlated random variables means \\(\\mu_1,...,\\mu_p\\)\\[\n\\mathbf{y} =\n\\left(\n\\begin{array}\n{c}\ny_1 \\\\\n. \\\\\ny_p \\\\\n\\end{array}\n\\right)\n\\]\\[\nE(\\mathbf{y}) =\n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n. \\\\\n\\mu_p \\\\\n\\end{array}\n\\right)\n\\]Let \\(\\sigma_{ij} = cov(y_i, y_j)\\) \\(,j = 1,…,p\\)\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij}) =\n\\left(\n\\begin{array}\n{cccc}\n\\sigma_{11} & \\sigma_{22} & ... &  \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p} \\\\\n. & . & . & . \\\\\n\\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n\\end{array}\n\\right)\n\\]\\(\\mathbf{\\Sigma}\\) (symmetric) variance-covariance dispersion matrixLet \\(\\mathbf{u}_{p \\times 1}\\) \\(\\mathbf{v}_{q \\times 1}\\) random vectors means \\(\\mu_u\\) \\(\\mu_v\\) . \\[\n\\mathbf{\\Sigma}_{uv} = cov(\\mathbf{u,v}) = E[(\\mathbf{u} - \\mu_u)(\\mathbf{v} - \\mu_v)']\n\\]\\(\\mathbf{\\Sigma}_{uv} \\neq \\mathbf{\\Sigma}_{vu}\\) \\(\\mathbf{\\Sigma}_{uv} = \\mathbf{\\Sigma}_{vu}'\\)Properties Covariance MatricesSymmetric \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\)Non-negative definite \\(\\mathbf{'\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\), equivalent eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0\\)\\(|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 ... \\lambda_p \\ge 0\\) (generalized variance) (bigger number , variation \\(trace(\\mathbf{\\Sigma}) = tr(\\mathbf{\\Sigma}) = \\lambda_1 + ... + \\lambda_p = \\sigma_{11} + ... + \\sigma_{pp} =\\) sum variance (total variance)Note:\\(\\mathbf{\\Sigma}\\) typically required positive definite, means eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\) \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma} = \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma \\Sigma}^{-1}\\)Correlation Matrices\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n\\]\\[\n\\mathbf{R} =\n\\left(\n\\begin{array}\n{cccc}\n\\rho_{11} & \\rho_{12} & ... & \\rho_{1p} \\\\\n\\rho_{21} & \\rho_{22} & ... & \\rho_{2p} \\\\\n. & . & . &. \\\\\n\\rho_{p1} & \\rho_{p2} & ... & \\rho_{pp} \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho_{ij}\\) correlation, \\(\\rho_{ii} = 1\\) iAlternatively,\\[\n\\mathbf{R} = [diag(\\mathbf{\\Sigma})]^{-1/2}\\mathbf{\\Sigma}[diag(\\mathbf{\\Sigma})]^{-1/2}\n\\]\\(diag(\\mathbf{\\Sigma})\\) matrix \\(\\sigma_{ii}\\)’s diagonal 0’s elsewhereand \\(\\mathbf{}^{1/2}\\) (square root symmetric matrix) symmetric matrix \\(\\mathbf{} = \\mathbf{}^{1/2}\\mathbf{}^{1/2}\\)EqualitiesLet\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constantsThen\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)Multivariate Normal DistributionLet \\(\\mathbf{y}\\) multivariate normal (MVN) random variable mean \\(\\mu\\) variance \\(\\mathbf{\\Sigma}\\). density \\(\\mathbf{y}\\) \\[\nf(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2} \\mathbf{(y-\\mu)'\\Sigma^{-1}(y-\\mu)} )\n\\]\\(\\mathbf{y} \\sim N_p(\\mu, \\mathbf{\\Sigma})\\)","code":""},{"path":"multivariate-methods.html","id":"properties-of-mvn","chapter":"22 Multivariate Methods","heading":"22.0.1 Properties of MVN","text":"Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)Define partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \n\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\n\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\n\\(p_1 + p_2 = p\\)\n\\(p_1,p_2 \\ge 1\\) \nDefine partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(p_1 + p_2 = p\\)\\(p_1 + p_2 = p\\)\\(p_1,p_2 \\ge 1\\) \\(p_1,p_2 \\ge 1\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n\\mathbf{y}_2 \\\\\n\\end{array}\n\\right)\n\\sim\nN\n\\left(\n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n\\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\\n\\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22}\\\\\n\\end{array}\n\\right)\n\\right)\n\\]marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)conditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\nanalogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\nconditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)formula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)Multiple Regression\\[\n\\left(\n\\begin{array}\n{c}\nY \\\\\n\\mathbf{x}\n\\end{array}\n\\right)\n\\sim\nN_{p+1}\n\\left(\n\\left[\n\\begin{array}\n{c}\n\\mu_y \\\\\n\\mathbf{\\mu}_x\n\\end{array}\n\\right]\n,\n\\left[\n\\begin{array}\n{cc}\n\\sigma^2_Y & \\mathbf{\\Sigma}_{yx} \\\\\n\\mathbf{\\Sigma}_{yx} & \\mathbf{\\Sigma}_{xx}\n\\end{array}\n\\right]\n\\right)\n\\]conditional distribution Y given x follows univariate normal distribution \\[\n\\begin{aligned}\nE(Y| \\mathbf{x}) &= \\mu_y + \\mathbf{\\Sigma}_{yx} \\Sigma_{xx}^{-1} (\\mathbf{x}- \\mu_x) \\\\\n&= \\mu_y - \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mu_x + \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mathbf{x} \\\\\n&= \\beta_0 + \\mathbf{\\beta'x}\n\\end{aligned}\n\\]\\(\\beta = (\\beta_1,...,\\beta_p)' = \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{yx}'\\) (e.g., analogous \\(\\mathbf{(x'x)^{-1}x'y}\\) consider \\(Y_i\\) \\(\\mathbf{x}_i\\), \\(= 1,..,n\\) use empirical covariance formula: \\(var(Y|\\mathbf{x}) = \\sigma^2_Y - \\mathbf{\\Sigma_{yx}\\Sigma^{-1}_{xx} \\Sigma'_{yx}}\\))Samples Multivariate Normal PopulationsA random sample size n, \\(\\mathbf{y}_1,.., \\mathbf{y}_n\\) \\(N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). ThenSince \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)Since \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\n\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\n\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).Large Sample Properties\\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) random sample population mean \\(\\mathbf{\\mu}\\) variance-covariance matrix \\(\\mathbf{\\Sigma}\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Maximum Likelihood Estimation MVNSuppose iid \\(\\mathbf{y}_1 ,... \\mathbf{y}_n \\sim N_p (\\mu, \\mathbf{\\Sigma})\\), likelihood function data \\[\n\\begin{aligned}\nL(\\mu, \\mathbf{\\Sigma}) &= \\prod_{j=1}^n (\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2}(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)) \\\\\n&= \\frac{1}{(2\\pi)^{np/2}|\\mathbf{\\Sigma}|^{n/2}} \\exp(-\\frac{1}{2} \\sum_{j=1}^n(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)\n\\end{aligned}\n\\], MLEs \\[\n\\hat{\\mu} = \\bar{\\mathbf{y}}\n\\]\\[\n\\hat{\\mathbf{\\Sigma}} = \\frac{n-1}{n} \\mathbf{S}\n\\]using derivatives log likelihood function respect \\(\\mu\\) \\(\\mathbf{\\Sigma}\\)Properties MLEsInvariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Invariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Consistency: MLEs consistent estimators, usually biasedConsistency: MLEs consistent estimators, usually biasedEfficiency: MLEs efficient estimators (estimator smaller variance large samples)Efficiency: MLEs efficient estimators (estimator smaller variance large samples)Asymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\n\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\ncan estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)\nAsymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)Likelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio test\nstatistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)\nlarge n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)\nLikelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio testThe statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)Test Multivariate NormalityCheck univariate normality trait (X) separately\nCan check \\[Normality Assessment\\]\ngood thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.\nHowever, marginal normality traits imply joint MVN\nEasily rule multivariate normality, easy prove \nCheck univariate normality trait (X) separatelyCan check \\[Normality Assessment\\]Can check \\[Normality Assessment\\]good thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.good thing univariate trait normal, joint distribution normal (see [m]). joint multivariate distribution normal, marginal distribution normal.However, marginal normality traits imply joint MVNHowever, marginal normality traits imply joint MVNEasily rule multivariate normality, easy prove itEasily rule multivariate normality, easy prove itMardia’s tests multivariate normality\nMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\n\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\nMultivariate kurtosis defined \n\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\nMVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)\nsample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\n\n(Mardia 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n\nMardia’s tests multivariate normalityMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]Multivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)Multivariate kurtosis defined asMultivariate kurtosis defined \\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)sample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\nsample size n, can estimate\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance(Mardia 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n(Mardia 1970) shows large n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)Alternatively, Doornik-Hansen test multivariate normality (Doornik Hansen 2008)Alternatively, Doornik-Hansen test multivariate normality (Doornik Hansen 2008)Chi-square Q-Q plot\nLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\n\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\nplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degree\nrequires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.\nChi-square Q-Q plotLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)plot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeit requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.data normal, can\nignore \nuse nonparametric methods\nuse models based upon approximate distribution (e.g., GLMM)\ntry performing transformation\ndata normal, canignore itignore ituse nonparametric methodsuse nonparametric methodsuse models based upon approximate distribution (e.g., GLMM)use models based upon approximate distribution (e.g., GLMM)try performing transformationtry performing transformation","code":"\nlibrary(heplots)\nlibrary(ICSNP)\nlibrary(MVN)\nlibrary(tidyverse)\n\ntrees = read.table(\"images/trees.dat\")\nnames(trees) <- c(\"Nitrogen\",\"Phosphorous\",\"Potassium\",\"Ash\",\"Height\")\nstr(trees)\n#> 'data.frame':    26 obs. of  5 variables:\n#>  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...\n#>  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...\n#>  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...\n#>  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...\n#>  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...\n\nsummary(trees)\n#>     Nitrogen      Phosphorous       Potassium           Ash        \n#>  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  \n#>  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  \n#>  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  \n#>  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  \n#>  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  \n#>  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  \n#>      Height     \n#>  Min.   : 65.0  \n#>  1st Qu.:122.5  \n#>  Median :181.0  \n#>  Mean   :196.6  \n#>  3rd Qu.:276.0  \n#>  Max.   :373.0\ncor(trees, method = \"pearson\") # correlation matrix\n#>              Nitrogen Phosphorous Potassium       Ash    Height\n#> Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641\n#> Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656\n#> Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683\n#> Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771\n#> Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000\n\n# qq-plot \ngg <- trees %>%\n    pivot_longer(everything(), names_to = \"Var\", values_to = \"Value\") %>%\n    ggplot(aes(sample = Value)) +\n    geom_qq() +\n    geom_qq_line() +\n    facet_wrap(\"Var\", scales = \"free\")\ngg\n\n# Univariate normality\nsw_tests <- apply(trees, MARGIN = 2, FUN = shapiro.test)\nsw_tests\n#> $Nitrogen\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.96829, p-value = 0.5794\n#> \n#> \n#> $Phosphorous\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.93644, p-value = 0.1104\n#> \n#> \n#> $Potassium\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.95709, p-value = 0.3375\n#> \n#> \n#> $Ash\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.92071, p-value = 0.04671\n#> \n#> \n#> $Height\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  newX[, i]\n#> W = 0.94107, p-value = 0.1424\n# Kolmogorov-Smirnov test \nks_tests <- map(trees, ~ ks.test(scale(.x),\"pnorm\"))\nks_tests\n#> $Nitrogen\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.12182, p-value = 0.8351\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Phosphorous\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.17627, p-value = 0.3944\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Potassium\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.10542, p-value = 0.9348\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Ash\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.14503, p-value = 0.6449\n#> alternative hypothesis: two-sided\n#> \n#> \n#> $Height\n#> \n#>  Asymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  scale(.x)\n#> D = 0.1107, p-value = 0.9076\n#> alternative hypothesis: two-sided\n\n# Mardia's test, need large sample size for power\nmardia_test <-\n    mvn(\n        trees,\n        mvnTest = \"mardia\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\n\nmardia_test$multivariateNormality\n#>              Test         Statistic            p value Result\n#> 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES\n#> 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281    YES\n#> 3             MVN              <NA>               <NA>    YES\n\n# Doornik-Hansen's test \ndh_test <-\n    mvn(\n        trees,\n        mvnTest = \"dh\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\ndh_test$multivariateNormality\n#>             Test        E df      p value MVN\n#> 1 Doornik-Hansen 161.9446 10 1.285352e-29  NO\n\n# Henze-Zirkler's test \nhz_test <-\n    mvn(\n        trees,\n        mvnTest = \"hz\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nhz_test$multivariateNormality\n#>            Test        HZ   p value MVN\n#> 1 Henze-Zirkler 0.7591525 0.6398905 YES\n# The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05.\n\n# Royston's test\n# can only apply for 3 < obs < 5000 (because of Shapiro-Wilk's test)\nroyston_test <-\n    mvn(\n        trees,\n        mvnTest = \"royston\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nroyston_test$multivariateNormality\n#>      Test        H    p value MVN\n#> 1 Royston 9.064631 0.08199215 YES\n\n\n# E-statistic\nestat_test <-\n    mvn(\n        trees,\n        mvnTest = \"energy\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nestat_test$multivariateNormality\n#>          Test Statistic p value MVN\n#> 1 E-statistic  1.091101   0.532 YES"},{"path":"multivariate-methods.html","id":"mean-vector-inference","chapter":"22 Multivariate Methods","heading":"22.0.2 Mean Vector Inference","text":"univariate normal distribution, test \\(H_0: \\mu =\\mu_0\\) using\\[\nT = \\frac{\\bar{y}- \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}\n\\]null hypothesis. reject null \\(|T|\\) large relative \\(t_{(1-\\alpha/2,n-1)}\\) means seeing value large observed rare null trueEquivalently,\\[\nT^2 = \\frac{(\\bar{y}- \\mu_0)^2}{s^2/n} = n(\\bar{y}- \\mu_0)(s^2)^{-1}(\\bar{y}- \\mu_0) \\sim f_{(1,n-1)}\n\\]","code":""},{"path":"multivariate-methods.html","id":"natural-multivariate-generalization","chapter":"22 Multivariate Methods","heading":"22.0.2.1 Natural Multivariate Generalization","text":"\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0 \\\\\n&H_a: \\mathbf{\\mu} \\neq \\mathbf{\\mu}_0\n\\end{aligned}\n\\]Define Hotelling’s \\(T^2\\) \\[\nT^2 = n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)\n\\]can viewed generalized distance \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{\\mu}_0\\)assumption normality,\\[\nF = \\frac{n-p}{(n-1)p} T^2 \\sim f_{(p,n-p)}\n\\]reject null hypothesis \\(F > f_{(1-\\alpha, p, n-p)}\\)\\(T^2\\) test invariant changes measurement units.\n\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\n\\(T^2\\) test invariant changes measurement units.\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)","code":""},{"path":"multivariate-methods.html","id":"confidence-intervals","chapter":"22 Multivariate Methods","heading":"22.0.2.2 Confidence Intervals","text":"","code":""},{"path":"multivariate-methods.html","id":"confidence-region","chapter":"22 Multivariate Methods","heading":"22.0.2.2.1 Confidence Region","text":"“exact” \\(100(1-\\alpha)\\%\\) confidence region \\(\\mathbf{\\mu}\\) set vectors, \\(\\mathbf{v}\\), “close enough” observed mean vector, \\(\\bar{\\mathbf{y}}\\) satisfy\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\le \\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)}\n\\]\\(\\mathbf{v}\\) just mean vectors rejected \\(T^2\\) test \\(\\mathbf{\\bar{y}}\\) observed.case 2 parameters, confidence region “hyper-ellipsoid”.region, consists \\(\\mathbf{\\mu}_0\\) vectors \\(T^2\\) test reject \\(H_0\\) significance level \\(\\alpha\\)Even though confidence region better assesses joint knowledge concerning plausible values \\(\\mathbf{\\mu}\\) , people typically include confidence statement individual component means. ’d like separate confidence statements hold simultaneously specified high probability. Simultaneous confidence intervals: intervals statement incorrect","code":""},{},{},{"path":"multivariate-methods.html","id":"general-hypothesis-testing","chapter":"22 Multivariate Methods","heading":"22.0.3 General Hypothesis Testing","text":"","code":""},{"path":"multivariate-methods.html","id":"one-sample-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.1 One-sample Tests","text":"\\[\nH_0: \\mathbf{C \\mu= 0}\n\\]\\(\\mathbf{C}\\) \\(c \\times p\\) matrix rank c \\(c \\le p\\)can test hypothesis using following statistic\\[\nF = \\frac{n - c}{(n-1)c} T^2\n\\]\\(T^2 = n(\\mathbf{C\\bar{y}})' (\\mathbf{CSC'})^{-1} (\\mathbf{C\\bar{y}})\\)Example:\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_p\n\\]Equivalently,\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0 \\\\\n&\\vdots \\\\\n\\mu_{p-1} - \\mu_p &= 0\n\\end{aligned}\n\\]total \\(p-1\\) tests. Hence, \\(\\mathbf{C}\\) \\(p - 1 \\times p\\) matrix\\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & 1 & -1\n\\end{array}\n\\right)\n\\]number rows = \\(c = p -1\\)Equivalently, can also compare means first mean. , test \\(\\mu_1 - \\mu_2 = 0, \\mu_1 - \\mu_3 = 0,..., \\mu_1 - \\mu_p = 0\\), \\((p-1) \\times p\\) matrix \\(\\mathbf{C}\\) \\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n-1 & 1 & 0 & \\ldots & 0 \\\\\n-1 & 0 & 1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n-1 & 0 & \\ldots & 0 & 1\n\\end{array}\n\\right)\n\\]value \\(T^2\\) invariant equivalent choices \\(\\mathbf{C}\\)often used repeated measures designs, subject receives treatment successive periods time (treatments administered unit).Example:Let \\(y_{ij}\\) response subject time j \\(= 1,..,n, j = 1,...,T\\). case, \\(\\mathbf{y}_i = (y_{i1}, ..., y_{})', = 1,...,n\\) random sample \\(N_T (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(n=8\\) subjects, \\(T = 6\\). interested \\(\\mu_1, .., \\mu_6\\)\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_6\n\\]Equivalently,\\[\n\\begin{aligned}\n\\mu_1 - \\mu_2 &= 0 \\\\\n\\mu_2 - \\mu_3 &= 0 \\\\\n&... \\\\\n\\mu_5  - \\mu_6 &= 0\n\\end{aligned}\n\\]can test orthogonal polynomials 4 equally spaced time points. test example null hypothesis quadratic cubic effects jointly equal 0, define \\(\\mathbf{C}\\)\\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{cccc}\n1 & -1 & -1 & 1 \\\\\n-1 & 3 & -3 & 1\n\\end{array}\n\\right)\n\\]","code":""},{"path":"multivariate-methods.html","id":"two-sample-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.2 Two-Sample Tests","text":"Consider analogous two sample multivariate tests.Example: data two independent random samples, one sample two populations\\[\n\\begin{aligned}\n\\mathbf{y}_{1i} &\\sim N_p (\\mathbf{\\mu_1, \\Sigma}) \\\\\n\\mathbf{y}_{2j} &\\sim N_p (\\mathbf{\\mu_2, \\Sigma})\n\\end{aligned}\n\\]assumenormalitynormalityequal variance-covariance matricesequal variance-covariance matricesindependent random samplesindependent random samplesWe can summarize data using sufficient statistics \\(\\mathbf{\\bar{y}}_1, \\mathbf{S}_1, \\mathbf{\\bar{y}}_2, \\mathbf{S}_2\\) respective sample sizes, \\(n_1,n_2\\)Since assume \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\), compute pooled estimate variance-covariance matrix \\(n_1 + n_2 - 2\\) df\\[\n\\mathbf{S} = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2-1) \\mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}\n\\]\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 \\\\\n&H_a: \\mathbf{\\mu}_1 \\neq \\mathbf{\\mu}_2\n\\end{aligned}\n\\]least one element mean vectors differentWe use\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)\nNote: assume two populations independent, covariance\n\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)Note: assume two populations independent, covariance\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)Reject \\(H_0\\) \\[\n\\begin{aligned}\nT^2 &= (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} (\\frac{1}{n_1} + \\frac{1}{n_2})\\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n&= \\frac{n_1 n_2}{n_1 +n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} \\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n& \\ge \\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \\alpha,n_1 + n_2 - p -1)}\n\\end{aligned}\n\\]equivalently, \\[\nF = \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\ge f_{(1- \\alpha, p , n_1 + n_2 -p -1)}\n\\]\\(100(1-\\alpha) \\%\\) confidence region \\(\\mu_1 - \\mu_2\\) consists vector \\(\\delta\\) satisfy\\[\n\\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta})' \\mathbf{S}^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta}) \\le \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\\alpha, p , n_1 + n_2 - p -1)}\n\\]simultaneous confidence intervals linear combinations \\(\\mu_1 - \\mu_2\\) form\\[\n\\mathbf{'}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\pm \\sqrt{\\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\\alpha, p, n_1 + n_2 -p -1)} \\times \\sqrt{\\mathbf{'Sa}(\\frac{1}{n_1} + \\frac{1}{n_2})}\n\\]Bonferroni intervals, k combinations\\[\n(\\bar{y}_{1i} - \\bar{y}_{2i}) \\pm t_{(1-\\alpha/2k, n_1 + n_2 - 2)}\\sqrt{(\\frac{1}{n_1}  + \\frac{1}{n_2})s_{ii}}\n\\]","code":""},{"path":"multivariate-methods.html","id":"model-assumptions","chapter":"22 Multivariate Methods","heading":"22.0.3.3 Model Assumptions","text":"model assumption metUnequal Covariance Matrices\n\\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample test\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflated\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction power\nUnequal Covariance MatricesIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerSample Normal\nType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributions\nOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.\nIntuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.\nSolutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\n\nSample NormalType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.One sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Solutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\nSolutions:Transform make data normalTransform make data normalLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)","code":""},{"path":"multivariate-methods.html","id":"equal-covariance-matrices-tests","chapter":"22 Multivariate Methods","heading":"22.0.3.3.1 Equal Covariance Matrices Tests","text":"independent random samples k populations \\(p\\)-dimensional vectors. compute sample covariance matrix , \\(\\mathbf{S}_i\\), \\(= 1,...,k\\)\\[\n\\begin{aligned}\n&H_0: \\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\ldots = \\mathbf{\\Sigma}_k = \\mathbf{\\Sigma} \\\\\n&H_a: \\text{least 2 different}\n\\end{aligned}\n\\]Assume \\(H_0\\) true, use pooled estimate common covariance matrix, \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{S} = \\frac{\\sum_{=1}^k (n_i -1)\\mathbf{S}_i}{\\sum_{=1}^k (n_i - 1)}\n\\]\\(\\sum_{=1}^k (n_i -1)\\)","code":""},{},{"path":"multivariate-methods.html","id":"two-sample-repeated-measurements","chapter":"22 Multivariate Methods","heading":"22.0.3.4 Two-Sample Repeated Measurements","text":"Define \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TDefine \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TAssume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)Assume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)test statistic formThe test statistic form\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\mathbf{C}'(\\mathbf{CSC}')^{-1}\\mathbf{C} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\n\\]\\(\\mathbf{S}\\) pooled covariance estimate. ,\\[\nF = \\frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \\sim f_{(c, n_1 + n_2 - c-1)}\n\\]\\(H_0\\) trueIf null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) rejected. weaker hypothesis profiles two groups parallel.\\[\n\\begin{aligned}\n\\mu_{11} - \\mu_{21} &= \\mu_{12} - \\mu_{22} \\\\\n&\\vdots \\\\\n\\mu_{1t-1} - \\mu_{2t-1} &= \\mu_{1t} - \\mu_{2t}\n\\end{aligned}\n\\]null hypothesis matrix term \\(H_0: \\mathbf{C}(\\mu_1 - \\mu_2) = \\mathbf{0}_c\\) , \\(c = t - 1\\) \\[\n\\mathbf{C} =\n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & -1\n\\end{array}\n\\right)_{(t-1) \\times t}\n\\]can’t reject null hypothesized vector meansreject null two labs’ measurements equalreject null. Hence, difference means bivariate normal distributions","code":"\n# One-sample Hotelling's T^2 test\n#  Create data frame\nplants <- data.frame(\n    y1 = c(2.11, 2.36, 2.13, 2.78, 2.17),\n    y2 = c(10.1, 35.0, 2.0, 6.0, 2.0),\n    y3 = c(3.4, 4.1, 1.9, 3.8, 1.7)\n)\n\n# Center the data with \n# the hypothesized means and make a matrix\nplants_ctr <- plants %>%\n    transmute(y1_ctr = y1 - 2.85,\n              y2_ctr = y2 - 15.0,\n              y3_ctr = y3 - 6.0) %>%\n    as.matrix()\n\n# Use anova.mlm to calculate Wilks' lambda\nonesamp_fit <- anova(lm(plants_ctr ~ 1), test = \"Wilks\")\nonesamp_fit\n#> Analysis of Variance Table\n#> \n#>             Df    Wilks approx F num Df den Df  Pr(>F)  \n#> (Intercept)  1 0.054219   11.629      3      2 0.08022 .\n#> Residuals    4                                          \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Paired-Sample Hotelling's T^2 test\nlibrary(ICSNP)\n\n#  Create data frame\nwaste <- data.frame(\n    case = 1:11,\n    com_y1 = c(6, 6, 18, 8, 11, 34, 28, 71, 43, 33, 20),\n    com_y2 = c(27, 23, 64, 44, 30, 75, 26, 124, 54, 30, 14),\n    state_y1 = c(25, 28, 36, 35, 15, 44, 42, 54, 34, 29, 39),\n    state_y2 = c(15, 13, 22, 29, 31, 64, 30, 64, 56, 20, 21)\n)\n\n# Calculate the difference between commercial and state labs\nwaste_diff <- waste %>%\n    transmute(y1_diff = com_y1 - state_y1,\n              y2_diff = com_y2 - state_y2)\n# Run the test\npaired_fit <- HotellingsT2(waste_diff)\n# value T.2 in the output corresponds to \n# the approximate F-value in the output from anova.mlm\npaired_fit \n#> \n#>  Hotelling's one sample T2-test\n#> \n#> data:  waste_diff\n#> T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083\n#> alternative hypothesis: true location is not equal to c(0,0)\n# Independent-Sample Hotelling's T^2 test with Bartlett's test\n\n# Read in data\nsteel <- read.table(\"images/steel.dat\")\nnames(steel) <- c(\"Temp\", \"Yield\", \"Strength\")\nstr(steel)\n#> 'data.frame':    12 obs. of  3 variables:\n#>  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...\n#>  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...\n#>  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...\n\n# Plot the data\nggplot(steel, aes(x = Yield, y = Strength)) +\n    geom_text(aes(label = Temp), size = 5) +\n    geom_segment(aes(\n        x = 33,\n        y = 57.5,\n        xend = 42,\n        yend = 65\n    ), col = \"red\")\n\n\n# Bartlett's test for equality of covariance matrices\n# same thing as Box's M test in the multivariate setting\nbart_test <- boxM(steel[, -1], steel$Temp)\nbart_test # fail to reject the null of equal covariances \n#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  steel[, -1]\n#> Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n\n# anova.mlm\ntwosamp_fit <-\n    anova(lm(cbind(Yield, Strength) ~ factor(Temp), \n             data = steel), \n          test = \"Wilks\")\ntwosamp_fit\n#> Analysis of Variance Table\n#> \n#>              Df    Wilks approx F num Df den Df    Pr(>F)    \n#> (Intercept)   1 0.001177   3818.1      2      9 6.589e-14 ***\n#> factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** \n#> Residuals    10                                              \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ICSNP package\ntwosamp_fit2 <-\n    HotellingsT2(cbind(steel$Yield, steel$Strength) ~ \n                     factor(steel$Temp))\ntwosamp_fit2\n#> \n#>  Hotelling's two sample T2-test\n#> \n#> data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)\n#> T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\n#> alternative hypothesis: true location difference is not equal to c(0,0)"},{"path":"multivariate-methods.html","id":"manova","chapter":"22 Multivariate Methods","heading":"22.1 MANOVA","text":"Multivariate Analysis VarianceOne-way MANOVACompare treatment means h different populationsPopulation 1: \\(\\mathbf{y}_{11}, \\mathbf{y}_{12}, \\dots, \\mathbf{y}_{1n_1} \\sim idd N_p (\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\)\\(\\vdots\\)Population h: \\(\\mathbf{y}_{h1}, \\mathbf{y}_{h2}, \\dots, \\mathbf{y}_{hn_h} \\sim idd N_p (\\mathbf{\\mu}_h, \\mathbf{\\Sigma})\\)AssumptionsIndependent random samples \\(h\\) different populationsCommon covariance matricesEach population multivariate normalCalculate summary statistics \\(\\mathbf{\\bar{y}}_i, \\mathbf{S}\\) pooled estimate covariance matrix \\(\\mathbf{S}\\)Similar univariate one-way ANVOA, can use effects model formulation \\(\\mathbf{\\mu}_i = \\mathbf{\\mu} + \\mathbf{\\tau}_i\\), \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.one-way model: \\(\\mathbf{y}_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(= 1,..,h; j = 1,..., n_i\\) \\(\\epsilon_{ij} \\sim N_p(\\mathbf{0, \\Sigma})\\)However, model -parameterized (.e., infinite number ways define \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\tau}_i\\)’s add \\(\\mu_i\\). Thus can constrain \\[\n\\sum_{=1}^h n_i \\tau_i = 0\n\\]\\[\n\\mathbf{\\tau}_h = 0\n\\]observational equivalent effects model \\[\n\\begin{aligned}\n\\mathbf{y}_{ij} &= \\mathbf{\\bar{y}} + (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}}) + (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i) \\\\\n&= \\text{overall sample mean} + \\text{treatement effect} + \\text{residual} \\text{ (univariate ANOVA)}\n\\end{aligned}\n\\]manipulation\\[\n\\sum_{= 1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})' = \\sum_{= 1}^h n_i (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})' + \\sum_{=1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}}_i)'\n\\]LHS = Total corrected sums squares cross products (SSCP) matrixRHS =1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)2nd term = residual (within subject) SSCP matrix denoted (E;W)2nd term = residual (within subject) SSCP matrix denoted (E;W)Note:\\[\n\\mathbf{E} = (n_1 - 1)\\mathbf{S}_1  + ... + (n_h -1) \\mathbf{S}_h = (n-h) \\mathbf{S}\n\\]MANOVA tableMONOVA table\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h = \\mathbf{0}\n\\]consider relative “sizes” \\(\\mathbf{E}\\) \\(\\mathbf{H+E}\\)Wilk’s LambdaDefine Wilk’s Lambda\\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H+E}|}\n\\]Properties:Wilk’s Lambda equivalent F-statistic univariate caseWilk’s Lambda equivalent F-statistic univariate caseThe exact distribution \\(\\Lambda^*\\) can determined especial cases.exact distribution \\(\\Lambda^*\\) can determined especial cases.large sample sizes, reject \\(H_0\\) ifFor large sample sizes, reject \\(H_0\\) \\[\n-(\\sum_{=1}^h n_i - 1 - \\frac{p+h}{2}) \\log(\\Lambda^*) > \\chi^2_{(1-\\alpha, p(h-1))}\n\\]","code":""},{"path":"multivariate-methods.html","id":"testing-general-hypotheses","chapter":"22 Multivariate Methods","heading":"22.1.1 Testing General Hypotheses","text":"\\(h\\) different treatments\\(h\\) different treatmentswith -th treatmentwith -th treatmentapplied \\(n_i\\) subjects thatapplied \\(n_i\\) subjects thatare observed \\(p\\) repeated measures.observed \\(p\\) repeated measures.Consider \\(p\\) dimensional obs random sample \\(h\\) different treatment populations.\\[\n\\mathbf{y}_{ij} = \\mathbf{\\mu} + \\mathbf{\\tau}_i + \\mathbf{\\epsilon}_{ij}\n\\]\\(= 1,..,h\\) \\(j = 1,..,n_i\\)Equivalently,\\[\n\\mathbf{Y} = \\mathbf{XB} + \\mathbf{\\epsilon}\n\\]\\(n = \\sum_{= 1}^h n_i\\) restriction \\(\\mathbf{\\tau}_h = 0\\)\\[\n\\mathbf{Y}_{(n \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_{11}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{1n_1}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{hn_h}'\n\\end{array}\n\\right],\n\\mathbf{B}_{(h \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\mathbf{\\mu}' \\\\\n\\mathbf{\\tau}_1' \\\\\n\\vdots \\\\\n\\mathbf{\\tau}_{h-1}'\n\\end{array}\n\\right],\n\\mathbf{\\epsilon}_{(n \\times p)} =\n\\left[\n\\begin{array}\n{c}\n\\epsilon_{11}' \\\\\n\\vdots \\\\\n\\epsilon_{1n_1}' \\\\\n\\vdots \\\\\n\\epsilon_{hn_h}'\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{X}_{(n \\times h)} =\n\\left[\n\\begin{array}\n{ccccc}\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0\n\\end{array}\n\\right]\n\\]Estimation\\[\n\\mathbf{\\hat{B}} = (\\mathbf{X'X})^{-1} \\mathbf{X'Y}\n\\]Rows \\(\\mathbf{Y}\\) independent (.e., \\(var(\\mathbf{Y}) = \\mathbf{}_n \\otimes \\mathbf{\\Sigma}\\) , \\(np \\times np\\) matrix, \\(\\otimes\\) Kronecker product).\\[\n\\begin{aligned}\n&H_0: \\mathbf{LBM} = 0 \\\\\n&H_a: \\mathbf{LBM} \\neq 0\n\\end{aligned}\n\\]\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traits\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traitsThe general treatment corrected sums squares cross product \\[\n\\mathbf{H} = \\mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}\n\\]null hypothesis \\(H_0: \\mathbf{LBM} = \\mathbf{D}\\)\\[\n\\mathbf{H} = (\\mathbf{\\hat{LBM}} - \\mathbf{D})'[\\mathbf{X(X'X)^{-1}L}]^{-1}(\\mathbf{\\hat{LBM}} - \\mathbf{D})\n\\]general matrix residual sums squares cross product\\[\n\\mathbf{E} = \\mathbf{M'Y'[-X(X'X)^{-1}X']YM} = \\mathbf{M'[Y'Y - \\hat{B}'(X'X)^{-1}\\hat{B}]M}\n\\]can compute following statistic eigenvalues \\(\\mathbf{}^{-1}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)\\(H_0\\) true n large, \\(-(n-1- \\frac{p+h}{2})\\ln \\Lambda^* \\sim \\chi^2_{p(h-1)}\\). special values p h can give exact F-dist \\(H_0\\)reject null equal multivariate mean vectors three admmission groupsIf independent = time 3 levels -> univariate ANOVA (require sphericity assumption (.e., variances differences equal))level independent time separate variable -> MANOVA (require sphericity assumption)can’t reject null hypothesis sphericity, hence univariate ANOVA also appropriate.also see linear significant time effect, quadratic time effectreject null hypothesis difference means treatmentsthere significant difference means control bww9 drugthere significant difference means ax23 drug treatment rest treatments","code":"\n# One-way MANOVA\n\nlibrary(car)\nlibrary(emmeans)\nlibrary(profileR)\nlibrary(tidyverse)\n\n## Read in the data\ngpagmat <- read.table(\"images/gpagmat.dat\")\n\n## Change the variable names\nnames(gpagmat) <- c(\"y1\", \"y2\", \"admit\")\n\n## Check the structure\nstr(gpagmat)\n#> 'data.frame':    85 obs. of  3 variables:\n#>  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...\n#>  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...\n#>  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...\n\n\n## Plot the data\ngg <- ggplot(gpagmat, aes(x = y1, y = y2)) +\n    geom_text(aes(label = admit, col = as.character(admit))) +\n    scale_color_discrete(name = \"Admission\",\n                         labels = c(\"Admit\", \"Do not admit\", \"Borderline\")) +\n    scale_x_continuous(name = \"GPA\") +\n    scale_y_continuous(name = \"GMAT\")\n\n## Fit one-way MANOVA\noneway_fit <- manova(cbind(y1, y2) ~ admit, data = gpagmat)\nsummary(oneway_fit, test = \"Wilks\")\n#>           Df  Wilks approx F num Df den Df    Pr(>F)    \n#> admit      1 0.6126   25.927      2     82 1.881e-09 ***\n#> Residuals 83                                            \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Repeated Measures MANOVA\n\n\n## Create data frame\nstress <- data.frame(\n    subject = 1:8,\n    begin = c(3, 2, 5, 6, 1, 5, 1, 5),\n    middle = c(3, 4, 3, 7, 4, 7, 1, 2),\n    final = c(6, 7, 4, 7, 6, 7, 3, 5)\n)\n## MANOVA\nstress_mod <- lm(cbind(begin, middle, final) ~ 1, data = stress)\nidata <-\n    data.frame(time = factor(\n        c(\"begin\", \"middle\", \"final\"),\n        levels = c(\"begin\", \"middle\", \"final\")\n    ))\nrepeat_fit <-\n    Anova(\n        stress_mod,\n        idata = idata,\n        idesign = ~ time,\n        icontrasts = \"contr.poly\"\n    )\nsummary(repeat_fit) \n#> \n#> Type III Repeated Measures MANOVA Tests:\n#> \n#> ------------------------------------------\n#>  \n#> Term: (Intercept) \n#> \n#>  Response transformation matrix:\n#>        (Intercept)\n#> begin            1\n#> middle           1\n#> final            1\n#> \n#> Sum of squares and products for the hypothesis:\n#>             (Intercept)\n#> (Intercept)        1352\n#> \n#> Multivariate Tests: (Intercept)\n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.896552 60.66667      1      7 0.00010808 ***\n#> Wilks             1  0.103448 60.66667      1      7 0.00010808 ***\n#> Hotelling-Lawley  1  8.666667 60.66667      1      7 0.00010808 ***\n#> Roy               1  8.666667 60.66667      1      7 0.00010808 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> ------------------------------------------\n#>  \n#> Term: time \n#> \n#>  Response transformation matrix:\n#>               time.L     time.Q\n#> begin  -7.071068e-01  0.4082483\n#> middle -7.850462e-17 -0.8164966\n#> final   7.071068e-01  0.4082483\n#> \n#> Sum of squares and products for the hypothesis:\n#>           time.L   time.Q\n#> time.L 18.062500 6.747781\n#> time.Q  6.747781 2.520833\n#> \n#> Multivariate Tests: time\n#>                  Df test stat approx F num Df den Df   Pr(>F)  \n#> Pillai            1 0.7080717 7.276498      2      6 0.024879 *\n#> Wilks             1 0.2919283 7.276498      2      6 0.024879 *\n#> Hotelling-Lawley  1 2.4254992 7.276498      2      6 0.024879 *\n#> Roy               1 2.4254992 7.276498      2      6 0.024879 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n#> \n#>             Sum Sq num Df Error SS den Df F value    Pr(>F)    \n#> (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***\n#> time         20.58      2    24.75     14  5.8215 0.0144578 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> \n#> Mauchly Tests for Sphericity\n#> \n#>      Test statistic p-value\n#> time         0.7085 0.35565\n#> \n#> \n#> Greenhouse-Geisser and Huynh-Feldt Corrections\n#>  for Departure from Sphericity\n#> \n#>       GG eps Pr(>F[GG])  \n#> time 0.77429    0.02439 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>         HF eps Pr(>F[HF])\n#> time 0.9528433 0.01611634\n## Polynomial contrasts\n# What is the reference for the marginal means?\nref_grid(stress_mod, mult.name = \"time\")\n#> 'emmGrid' object with variables:\n#>     1 = 1\n#>     time = multivariate response levels: begin, middle, final\n\n# marginal means for the levels of time\ncontr_means <- emmeans(stress_mod, ~ time, mult.name = \"time\")\ncontrast(contr_means, method = \"poly\")\n#>  contrast  estimate    SE df t.ratio p.value\n#>  linear        2.12 0.766  7   2.773  0.0276\n#>  quadratic     1.38 0.944  7   1.457  0.1885\n# MANOVA\n\n\n## Read in Data\nheart <- read.table(\"images/heart.dat\")\nnames(heart) <- c(\"drug\", \"y1\", \"y2\", \"y3\", \"y4\")\n## Create a subject ID nested within drug\nheart <- heart %>%\n    group_by(drug) %>%\n    mutate(subject = row_number()) %>%\n    ungroup()\nstr(heart)\n#> tibble [24 × 6] (S3: tbl_df/tbl/data.frame)\n#>  $ drug   : chr [1:24] \"ax23\" \"ax23\" \"ax23\" \"ax23\" ...\n#>  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...\n#>  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...\n#>  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...\n#>  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...\n#>  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...\n\n## Create means summary for profile plot,\n# pivot longer for plotting with ggplot\nheart_means <- heart %>%\n    group_by(drug) %>%\n    summarize_at(vars(starts_with(\"y\")), mean) %>%\n    ungroup() %>%\n    pivot_longer(-drug, names_to = \"time\", values_to = \"mean\") %>%\n    mutate(time = as.numeric(as.factor(time)))\ngg_profile <- ggplot(heart_means, aes(x = time, y = mean)) +\n    geom_line(aes(col = drug)) +\n    geom_point(aes(col = drug)) +\n    ggtitle(\"Profile Plot\") +\n    scale_y_continuous(name = \"Response\") +\n    scale_x_discrete(name = \"Time\")\ngg_profile\n## Fit model\nheart_mod <- lm(cbind(y1, y2, y3, y4) ~ drug, data = heart)\nman_fit <- car::Anova(heart_mod)\nsummary(man_fit)\n#> \n#> Type II MANOVA Tests:\n#> \n#> Sum of squares and products for error:\n#>        y1      y2      y3     y4\n#> y1 641.00 601.750 535.250 426.00\n#> y2 601.75 823.875 615.500 534.25\n#> y3 535.25 615.500 655.875 555.25\n#> y4 426.00 534.250 555.250 674.50\n#> \n#> ------------------------------------------\n#>  \n#> Term: drug \n#> \n#> Sum of squares and products for the hypothesis:\n#>        y1       y2       y3    y4\n#> y1 567.00 335.2500  42.7500 387.0\n#> y2 335.25 569.0833 404.5417 367.5\n#> y3  42.75 404.5417 391.0833 171.0\n#> y4 387.00 367.5000 171.0000 316.0\n#> \n#> Multivariate Tests: drug\n#>                  Df test stat  approx F num Df den Df     Pr(>F)    \n#> Pillai            2  1.283456  8.508082      8     38 1.5010e-06 ***\n#> Wilks             2  0.079007 11.509581      8     36 6.3081e-08 ***\n#> Hotelling-Lawley  2  7.069384 15.022441      8     34 3.9048e-09 ***\n#> Roy               2  6.346509 30.145916      4     19 5.4493e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Contrasts\nheart$drug <- factor(heart$drug)\nL <- matrix(c(0, 2,\n              1, -1,-1, -1), nrow = 3, byrow = T)\ncolnames(L) <- c(\"bww9:ctrl\", \"ax23:rest\")\nrownames(L) <- unique(heart$drug)\ncontrasts(heart$drug) <- L\ncontrasts(heart$drug)\n#>      bww9:ctrl ax23:rest\n#> ax23         0         2\n#> bww9         1        -1\n#> ctrl        -1        -1\n\n# do not set contrast L if you do further analysis (e.g., Anova, lm)\n# do M matrix instead\n\nM <- matrix(c(1, -1, 0, 0,\n              0, 1, -1, 0,\n              0, 0, 1, -1), nrow = 4)\n## update model to test contrasts\nheart_mod2 <- update(heart_mod)\ncoef(heart_mod2)\n#>                  y1         y2        y3    y4\n#> (Intercept)   75.00 78.9583333 77.041667 74.75\n#> drugbww9:ctrl  4.50  5.8125000  3.562500  4.25\n#> drugax23:rest -2.25  0.7708333  1.979167 -0.75\n\n# Hypothesis test for bww9 vs control after transformation M\n# same as linearHypothesis(heart_mod, hypothesis.matrix = c(0,1,-1), P = M)\nbww9vctrl <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 1, 0),\n                     P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\n\nbww9vctrl <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(0, 1, -1),\n                     P = M)\nbww9vctrl\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>          [,1]   [,2]     [,3]\n#> [1,]  27.5625 -47.25  14.4375\n#> [2,] -47.2500  81.00 -24.7500\n#> [3,]  14.4375 -24.75   7.5625\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df Pr(>F)\n#> Pillai            1 0.2564306 2.184141      3     19 0.1233\n#> Wilks             1 0.7435694 2.184141      3     19 0.1233\n#> Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n#> Roy               1 0.3448644 2.184141      3     19 0.1233\n# Hypothesis test for ax23 vs rest after transformation M\naxx23vrest <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 0, 1),\n                     P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  438.0208  175.20833 -395.7292\n#> [2,]  175.2083   70.08333 -158.2917\n#> [3,] -395.7292 -158.29167  357.5208\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.855364 37.45483      3     19 3.5484e-08 ***\n#> Wilks             1  0.144636 37.45483      3     19 3.5484e-08 ***\n#> Hotelling-Lawley  1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> Roy               1  5.913921 37.45483      3     19 3.5484e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naxx23vrest <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(2, -1, 1),\n                     P = M)\naxx23vrest\n#> \n#>  Response transformation matrix:\n#>    [,1] [,2] [,3]\n#> y1    1    0    0\n#> y2   -1    1    0\n#> y3    0   -1    1\n#> y4    0    0   -1\n#> \n#> Sum of squares and products for the hypothesis:\n#>           [,1]       [,2]      [,3]\n#> [1,]  402.5208  127.41667 -390.9375\n#> [2,]  127.4167   40.33333 -123.7500\n#> [3,] -390.9375 -123.75000  379.6875\n#> \n#> Sum of squares and products for error:\n#>          [,1]     [,2]    [,3]\n#> [1,]  261.375 -141.875  28.000\n#> [2,] -141.875  248.750 -19.375\n#> [3,]   28.000  -19.375 219.875\n#> \n#> Multivariate Tests: \n#>                  Df test stat approx F num Df den Df     Pr(>F)    \n#> Pillai            1  0.842450 33.86563      3     19 7.9422e-08 ***\n#> Wilks             1  0.157550 33.86563      3     19 7.9422e-08 ***\n#> Hotelling-Lawley  1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> Roy               1  5.347205 33.86563      3     19 7.9422e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multivariate-methods.html","id":"profile-analysis","chapter":"22 Multivariate Methods","heading":"22.1.2 Profile Analysis","text":"Examine similarities treatment effects (subjects), useful longitudinal analysis. Null treatments average effect.\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_h\n\\]Equivalently,\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h\n\\]exact nature similarities differences treatments can examined analysis.Sequential steps profile analysis:profiles parallel? (.e., interaction treatment time)profiles coincidental? (.e., profiles identical?)profiles horizontal? (.e., differences time points?)reject null hypothesis profiles parallel, can testAre differences among groups within subset total time points?differences among groups within subset total time points?differences among time points particular group (groups)?differences among time points particular group (groups)?differences within subset total time points particular group (groups)?differences within subset total time points particular group (groups)?Example4 times (p = 4)4 times (p = 4)3 treatments (h=3)3 treatments (h=3)","code":""},{"path":"multivariate-methods.html","id":"parallel-profile","chapter":"22 Multivariate Methods","heading":"22.1.2.1 Parallel Profile","text":"profiles population identical expect mean shift?\\[\n\\begin{aligned}\nH_0: \\mu_{11} - \\mu_{21} - \\mu_{12} - \\mu_{22} = &\\dots = \\mu_{1t} - \\mu_{2t} \\\\\n\\mu_{11} - \\mu_{31} - \\mu_{12} - \\mu_{32} = &\\dots = \\mu_{1t} - \\mu_{3t} \\\\\n&\\dots\n\\end{aligned}\n\\]\\(h-1\\) equationsEquivalently,\\[\nH_0: \\mathbf{LBM = 0}\n\\]\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{ccc}\n1 & -1 & 0 \\\\\n1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n\\mu_{11} & \\dots & \\mu_{14} \\\\\n\\mu_{21} & \\dots & \\mu_{24} \\\\\n\\mu_{31} & \\dots & \\mu_{34}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n=\n\\mathbf{0}\n\\]cell means parameterization \\(\\mathbf{B}\\)multiplication first 2 matrices \\(\\mathbf{LB}\\) \\[\n\\left[\n\\begin{array}\n{cccc}\n\\mu_{11} - \\mu_{21} & \\mu_{12} - \\mu_{22} & \\mu_{13} - \\mu_{23} & \\mu_{14} - \\mu_{24}\\\\\n\\mu_{11} - \\mu_{31} & \\mu_{12} - \\mu_{32} & \\mu_{13} - \\mu_{33} & \\mu_{14} - \\mu_{34}\n\\end{array}\n\\right]\n\\]differences treatment means timeMultiplying \\(\\mathbf{M}\\), get comparison across time\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{21}) - (\\mu_{12} - \\mu_{22}) & (\\mu_{11} - \\mu_{21}) -(\\mu_{13} - \\mu_{23}) & (\\mu_{11} - \\mu_{21}) - (\\mu_{14} - \\mu_{24}) \\\\\n(\\mu_{11} - \\mu_{31}) - (\\mu_{12} - \\mu_{32}) & (\\mu_{11} - \\mu_{31}) - (\\mu_{13} - \\mu_{33}) & (\\mu_{11} - \\mu_{31}) -(\\mu_{14} - \\mu_{34})\n\\end{array}\n\\right]\n\\]Alternatively, can also use effects parameterization\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{cccc}\n0 & 1 & -1 & 0 \\\\\n0 & 1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\mu' \\\\\n\\tau'_1 \\\\\n\\tau_2' \\\\\n\\tau_3'\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \\mathbf{0}\n\\]parameterizations, \\(rank(\\mathbf{L}) = h-1\\) \\(rank(\\mathbf{M}) = p-1\\)also choose \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) forms\\[\n\\mathbf{L} = \\left[\n\\begin{array}\n{cccc}\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]still obtain result.","code":""},{"path":"multivariate-methods.html","id":"coincidental-profiles","chapter":"22 Multivariate Methods","heading":"22.1.2.2 Coincidental Profiles","text":"evidence profiles parallel (.e., fail reject parallel profile test), can ask whether identical?Given profiles parallel, sums components \\(\\mu_i\\) identical treatments, profiles identical.\\[\nH_0: \\mathbf{1'}_p \\mu_1 = \\mathbf{1'}_p \\mu_2 = \\dots = \\mathbf{1'}_p \\mu_h\n\\]Equivalently,\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]cell means parameterization\\[\n\\mathbf{L} =\n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} =\n\\left[\n\\begin{array}\n{cccc}\n1 & 1 & 1 & 1\n\\end{array}\n\\right]'\n\\]multiplication yields\\[\n\\left[\n\\begin{array}\n{c}\n(\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\\\\n(\\mu_{21} + \\mu_{22} + \\mu_{23} + \\mu_{24}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34})\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0\n\\end{array}\n\\right]\n\\]Different choices \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) can yield result","code":""},{"path":"multivariate-methods.html","id":"horizontal-profiles","chapter":"22 Multivariate Methods","heading":"22.1.2.3 Horizontal Profiles","text":"Given can’t reject null hypothesis \\(h\\) profiles , can ask whether elements common profile equal? (.e., horizontal)\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]\\[\n\\mathbf{L} =\n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]hence,\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{12}) & (\\mu_{12} - \\mu_{13}) & (\\mu_{13} + \\mu_{14})\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{ccc}\n0 & 0 & 0\n\\end{array}\n\\right]\n\\]Note:fail reject 3 hypotheses, fail reject null hypotheses difference treatments differences traits.","code":"\nprofile_fit <-\n    pbg(\n        data = as.matrix(heart[, 2:5]),\n        group = as.matrix(heart[, 1]),\n        original.names = TRUE,\n        profile.plot = FALSE\n    )\nsummary(profile_fit)\n#> Call:\n#> pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, \n#>     1]), original.names = TRUE, profile.plot = FALSE)\n#> \n#> Hypothesis Tests:\n#> $`Ho: Profiles are parallel`\n#>   Multivariate.Test Statistic  Approx.F num.df den.df      p.value\n#> 1             Wilks 0.1102861 12.737599      6     38 7.891497e-08\n#> 2            Pillai 1.0891707  7.972007      6     40 1.092397e-05\n#> 3  Hotelling-Lawley 6.2587852 18.776356      6     36 9.258571e-10\n#> 4               Roy 5.9550887 39.700592      3     20 1.302458e-08\n#> \n#> $`Ho: Profiles have equal levels`\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> group        2  328.7  164.35   5.918 0.00915 **\n#> Residuals   21  583.2   27.77                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> $`Ho: Profiles are flat`\n#>          F df1 df2      p-value\n#> 1 14.30928   3  19 4.096803e-05\n# reject null hypothesis of parallel profiles\n# reject the null hypothesis of coincidental profiles\n# reject the null hypothesis that the profiles are flat"},{"path":"multivariate-methods.html","id":"summary-5","chapter":"22 Multivariate Methods","heading":"22.1.3 Summary","text":"","code":""},{"path":"multivariate-methods.html","id":"principal-components","chapter":"22 Multivariate Methods","heading":"22.2 Principal Components","text":"Unsupervised learningfind important featuresreduce dimensions data set“decorrelate” multivariate vectors dependence.uses eigenvector/eigvenvalue decomposition covariance (correlation) matrices.According “spectral decomposition theorem”, \\(\\mathbf{\\Sigma}_{p \\times p}\\) s positive semi-definite, symmetric, real matrix, exists orthogonal matrix \\(\\mathbf{}\\) \\(\\mathbf{'\\Sigma } = \\Lambda\\) \\(\\Lambda\\) diagonal matrix containing eigenvalues \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{\\Lambda} =\n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{} =\n\\left(\n\\begin{array}\n{cccc}\n\\mathbf{}_1 & \\mathbf{}_2 & \\ldots & \\mathbf{}_p\n\\end{array}\n\\right)\n\\]-th column \\(\\mathbf{}\\) , \\(\\mathbf{}_i\\), -th \\(p \\times 1\\) eigenvector \\(\\mathbf{\\Sigma}\\) corresponds eigenvalue, \\(\\lambda_i\\) , \\(\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_p\\) . Alternatively, express matrix decomposition:\\[\n\\mathbf{\\Sigma} = \\mathbf{\\Lambda }'\n\\]\\[\n\\mathbf{\\Sigma} = \\mathbf{}\n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\mathbf{}'\n= \\sum_{=1}^p \\lambda_i \\mathbf{}_i \\mathbf{}_i'\n\\]outer product \\(\\mathbf{}_i \\mathbf{}_i'\\) \\(p \\times p\\) matrix rank 1.example,\\(\\mathbf{x} \\sim N_2(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\[\n\\mathbf{\\mu} =\n\\left(\n\\begin{array}\n{c}\n5 \\\\\n12\n\\end{array}\n\\right);\n\\mathbf{\\Sigma} =\n\\left(\n\\begin{array}\n{cc}\n4 & 1 \\\\\n1 & 2\n\\end{array}\n\\right)\n\\],\\[\n\\mathbf{} =\n\\left(\n\\begin{array}\n{cc}\n0.9239 & -0.3827 \\\\\n0.3827 & 0.9239 \\\\\n\\end{array}\n\\right)\n\\]Columns \\(\\mathbf{}\\) eigenvectors decompositionUnder matrix multiplication (\\(\\mathbf{'\\Sigma }\\) \\(\\mathbf{'}\\) ), -diagonal elements equal 0Multiplying data matrix (.e., projecting data onto orthogonal axes); distribution resulting data (.e., “scores”) \\[\nN_2 (\\mathbf{'\\mu,'\\Sigma }) = N_2 (\\mathbf{'\\mu, \\Lambda})\n\\]Equivalently,\\[\n\\mathbf{y} = \\mathbf{'x} \\sim N\n\\left[\n\\left(\n\\begin{array}\n{c}\n9.2119 \\\\\n9.1733\n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n4.4144 & 0 \\\\\n0 & 1.5859\n\\end{array}\n\\right)\n\\right]\n\\]dependence data structure, plotNotes:-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto “new principal component axesThe values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto “new principal component axesVariances \\(y_1\\) greater possible projectionVariances \\(y_1\\) greater possible projectionCovariance matrix decomposition projection onto orthogonal axes = PCA","code":"\nlibrary(MASS)\nmu = as.matrix(c(5, 12))\nSigma = matrix(c(4, 1, 1, 2), nrow = 2, byrow = T)\nsim <- mvrnorm(n = 1000, mu = mu, Sigma = Sigma)\nplot(sim[, 1], sim[, 2])\nA_matrix = matrix(c(0.9239, -0.3827, 0.3827, 0.9239),\n                  nrow = 2,\n                  byrow = T)\nt(A_matrix) %*% A_matrix\n#>          [,1]     [,2]\n#> [1,] 1.000051 0.000000\n#> [2,] 0.000000 1.000051\n\nsim1 <-\n    mvrnorm(\n        n = 1000,\n        mu = t(A_matrix) %*% mu,\n        Sigma = t(A_matrix) %*% Sigma %*% A_matrix\n    )\nplot(sim1[, 1], sim1[, 2])"},{"path":"multivariate-methods.html","id":"population-principal-components","chapter":"22 Multivariate Methods","heading":"22.2.1 Population Principal Components","text":"\\(p \\times 1\\) vectors \\(\\mathbf{x}_1, \\dots , \\mathbf{x}_n\\) iid \\(var(\\mathbf{x}_i) = \\mathbf{\\Sigma}\\)first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)continues \\(y_i\\) \\(y_p\\)continues \\(y_i\\) \\(y_p\\)\\(\\mathbf{}_i\\)’s make matrix \\(\\mathbf{}\\) symmetric decomposition \\(\\mathbf{'\\Sigma } = \\mathbf{\\Lambda}\\) , \\(var(y_1) = \\lambda_1, \\dots , var(y_p) = \\lambda_p\\) total variance \\(\\mathbf{x}\\) \\[\n\\begin{aligned}\nvar(x_1) + \\dots + var(x_p) &= tr(\\Sigma) = \\lambda_1 + \\dots + \\lambda_p \\\\\n&= var(y_1) + \\dots + var(y_p)\n\\end{aligned}\n\\]Data ReductionTo reduce dimension data p (original) k dimensions without much “loss information”, can use properties population principal componentsSuppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)Suppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).proportion total variance accounted j-th principal component isThe proportion total variance accounted j-th principal component \\[\n\\frac{var(y_j)}{\\sum_{=1}^p var(y_i)} = \\frac{\\lambda_j}{\\sum_{=1}^p \\lambda_i}\n\\]proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal componentAbove example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal component","code":""},{"path":"multivariate-methods.html","id":"sample-principal-components","chapter":"22 Multivariate Methods","heading":"22.2.2 Sample Principal Components","text":"Since \\(\\mathbf{\\Sigma}\\) unknown, use\\[\n\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})'\n\\]Let \\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots \\ge \\hat{\\lambda}_p \\ge 0\\) eigenvalues \\(\\mathbf{S}\\) \\(\\hat{\\mathbf{}}_1, \\hat{\\mathbf{}}_2, \\dots, \\hat{\\mathbf{}}_p\\) denote eigenvectors \\(\\mathbf{S}\\), -th sample principal component score (principal component score) \\[\n\\hat{y}_{ij} = \\sum_{k=1}^p \\hat{}_{ik}x_{kj} = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\n\\]Properties Sample Principal ComponentsThe estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)estimated correlation \\(\\)-th principal component score \\(l\\)-th attribute \\(\\mathbf{x}\\) isThe estimated correlation \\(\\)-th principal component score \\(l\\)-th attribute \\(\\mathbf{x}\\) \\[\nr_{x_l , \\hat{y}_i} = \\frac{\\hat{}_{il}\\sqrt{\\lambda_i}}{\\sqrt{s_{ll}}}\n\\]correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According R. . Johnson, Wichern, et al. (2002), pp.433-434, \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According R. . Johnson, Wichern, et al. (2002), pp.433-434, \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”use k principal components, must calculate scores data vector sample\\[\n\\mathbf{y}_j =\n\\left(\n\\begin{array}\n{c}\ny_{1j} \\\\\ny_{2j} \\\\\n\\vdots \\\\\ny_{kj}\n\\end{array}\n\\right) =\n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\mathbf{x}_j \\\\\n\\hat{\\mathbf{}}_2' \\mathbf{x}_j \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k' \\mathbf{x}_j\n\\end{array}\n\\right) =\n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\\\\n\\hat{\\mathbf{}}_2' \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k'\n\\end{array}\n\\right) \\mathbf{x}_j\n\\]Issues:Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.PC invariant changes scale (Exception: trait rescaled multiplying constant, feet inches).\nPCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)\nPCA correlation matrix just rescaling trait unit variance\nTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCA\ntransformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)\nPCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\n\nmany PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\n\nPC invariant changes scale (Exception: trait rescaled multiplying constant, feet inches).PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA correlation matrix just rescaling trait unit variancePCA correlation matrix just rescaling trait unit varianceTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCATransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCAAfter transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )use \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.scale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.many PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\nmany PCs use can guided byScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.Scree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.Kaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocCompare eigenvalue scree plot data scree plot data randomized.Compare eigenvalue scree plot data scree plot data randomized.","code":""},{"path":"multivariate-methods.html","id":"application-10","chapter":"22 Multivariate Methods","heading":"22.2.3 Application","text":"PCA covariance matrix usually preferred due fact PCA invariant changes scale. Hence, PCA correlation matrix preferredThis also addresses problem multicollinearityThe eigvenvectors may differ multiplication -1 different implementation, interpretation.Covid ExampleTo reduce collinearity problem dataset, can use principal components regressors.MSE PC-based model larger regular regression, models large degree collinearity can still perform well.pcr function pls can used fitting PC regression (select optimal number components model).","code":"\nlibrary(tidyverse)\n## Read in and check data\nstock <- read.table(\"images/stock.dat\")\nnames(stock) <- c(\"allied\", \"dupont\", \"carbide\", \"exxon\", \"texaco\")\nstr(stock)\n#> 'data.frame':    100 obs. of  5 variables:\n#>  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...\n#>  $ dupont : num  0 -0.04485 0.06077 0.02995 -0.00379 ...\n#>  $ carbide: num  0 -0.00303 0.08815 0.06681 -0.03979 ...\n#>  $ exxon  : num  0.0395 -0.0145 0.0862 0.0135 -0.0186 ...\n#>  $ texaco : num  0 0.0435 0.0781 0.0195 -0.0242 ...\n\n## Covariance matrix of data\ncov(stock)\n#>               allied       dupont      carbide        exxon       texaco\n#> allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715\n#> dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431\n#> carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767\n#> exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734\n#> texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370\n\n## Correlation matrix of data\ncor(stock)\n#>            allied    dupont   carbide     exxon    texaco\n#> allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781\n#> dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534\n#> carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266\n#> exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293\n#> texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000\n\n# cov(scale(stock)) # give the same result\n\n## PCA with covariance\ncov_pca <- prcomp(stock) \n# uses singular value decomposition for calculation and an N -1 divisor\n# alternatively, princomp can do PCA via spectral decomposition, \n# but it has worse numerical accuracy\n\n# eigen values\ncov_results <- data.frame(eigen_values = cov_pca$sdev ^ 2)\ncov_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion)) \n#>   eigen_values proportion cumulative\n#> 1 0.0035953867 0.60159252  0.6015925\n#> 2 0.0007921798 0.13255027  0.7341428\n#> 3 0.0007364426 0.12322412  0.8573669\n#> 4 0.0005086686 0.08511218  0.9424791\n#> 5 0.0003437707 0.05752091  1.0000000\n# first 2 PCs account for 73% variance in the data\n\n# eigen vectors\ncov_pca$rotation # prcomp calls rotation\n#>               PC1         PC2        PC3         PC4         PC5\n#> allied  0.5605914  0.73884565 -0.1260222  0.28373183 -0.20846832\n#> dupont  0.4698673 -0.09286987 -0.4675066 -0.68793190  0.28069055\n#> carbide 0.5473322 -0.65401929 -0.1140581  0.50045312 -0.09603973\n#> exxon   0.2908932 -0.11267353  0.6099196 -0.43808002 -0.58203935\n#> texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638\n# princomp calls loadings.\n\n# first PC = overall average\n# second PC compares Allied to Carbide\n\n## PCA with correlation\n#same as scale(stock) %>% prcomp\ncor_pca <- prcomp(stock, scale = T)\n\n\n\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\ncor_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion))\n#>   eigen_values proportion cumulative\n#> 1    2.8564869 0.57129738  0.5712974\n#> 2    0.8091185 0.16182370  0.7331211\n#> 3    0.5400440 0.10800880  0.8411299\n#> 4    0.4513468 0.09026936  0.9313992\n#> 5    0.3430038 0.06860076  1.0000000\n\n# first egiven values corresponds to less variance \n# than PCA based on the covariance matrix\n\n# eigen vectors\ncor_pca$rotation\n#>               PC1        PC2        PC3        PC4        PC5\n#> allied  0.4635405 -0.2408499  0.6133570 -0.3813727  0.4532876\n#> dupont  0.4570764 -0.5090997 -0.1778996 -0.2113068 -0.6749814\n#> carbide 0.4699804 -0.2605774 -0.3370355  0.6640985  0.3957247\n#> exxon   0.4216770  0.5252647 -0.5390181 -0.4728036  0.1794482\n#> texaco  0.4213291  0.5822416  0.4336029  0.3812273 -0.3874672\n# interpretation of PC2 is different from above: \n# it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco \nload('images/MOcovid.RData')\ncovidpca <- prcomp(ndat[,-1],scale = T,center = T)\n\ncovidpca$rotation[,1:2]\n#>                                                          PC1         PC2\n#> X..Population.in.Rural.Areas                      0.32865838  0.05090955\n#> Area..sq..miles.                                  0.12014444 -0.28579183\n#> Population.density..sq..miles.                   -0.29670124  0.28312922\n#> Literacy.rate                                    -0.12517700 -0.08999542\n#> Families                                         -0.25856941  0.16485752\n#> Area.of.farm.land..sq..miles.                     0.02101106 -0.31070363\n#> Number.of.farms                                  -0.03814582 -0.44809679\n#> Average.value.of.all.property.per.farm..dollars. -0.05410709  0.14404306\n#> Estimation.of.rurality..                         -0.19040210  0.12089501\n#> Male..                                            0.02182394 -0.09568768\n#> Number.of.Physcians.per.100.000                  -0.31451606  0.13598026\n#> average.age                                       0.29414708  0.35593459\n#> X0.4.age.proportion                              -0.11431336 -0.23574057\n#> X20.44.age.proportion                            -0.32802128 -0.22718550\n#> X65.and.over.age.proportion                       0.30585033  0.32201626\n#> prop..White..nonHisp                              0.35627561 -0.14142646\n#> prop..Hispanic                                   -0.16655381 -0.15105342\n#> prop..Black                                      -0.33333359  0.24405802\n\n\n# Variability of each principal component: pr.var\npr.var <- covidpca$sdev ^ 2\n# Variance explained by each principal component: pve\npve <- pr.var / sum(pr.var)\nplot(\n    pve,\n    xlab = \"Principal Component\",\n    ylab = \"Proportion of Variance Explained\",\n    ylim = c(0, 0.5),\n    type = \"b\"\n)\n\nplot(\n    cumsum(pve),\n    xlab = \"Principal Component\",\n    ylab = \"Cumulative Proportion of Variance Explained\",\n    ylim = c(0, 1),\n    type = \"b\"\n)\n\n# the first six principe account for around 80% of the variance. \n\n\n#using base lm function for PC regression\npcadat <- data.frame(covidpca$x[, 1:6])\npcadat$y <- ndat$Y\npcr.man <- lm(log(y) ~ ., pcadat)\nmean(pcr.man$residuals ^ 2)\n#> [1] 0.03453371\n\n#comparison to lm w/o prin comps\nlm.fit <- lm(log(Y) ~ ., data = ndat)\nmean(lm.fit$residuals ^ 2)\n#> [1] 0.02335128"},{"path":"multivariate-methods.html","id":"factor-analysis","chapter":"22 Multivariate Methods","heading":"22.3 Factor Analysis","text":"PurposeUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsSimilar PCA, factor analysis model basedSimilar PCA, factor analysis model basedMore details can found PSU stat UMN statLet \\(\\mathbf{y}\\) set \\(p\\) measured variables\\(E(\\mathbf{y}) = \\mathbf{\\mu}\\)\\(var(\\mathbf{y}) = \\mathbf{\\Sigma}\\)\\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&=\n\\left(\n\\begin{array}\n{c}\nl_{11}f_1 + l_{12}f_2 + \\dots + l_{tm}f_m \\\\\n\\vdots \\\\\nl_{p1}f_1 + l_{p2}f_2 + \\dots + l_{pm} f_m\n\\end{array}\n\\right)\n+\n\\left(\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n\\vdots \\\\\n\\epsilon_p\n\\end{array}\n\\right)\n\\end{aligned}\n\\]\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).want \\(m\\) (number factors) much smaller \\(p\\) (number measured attributes)Restrictions model\\(E(\\epsilon) = \\mathbf{0}\\)\\(E(\\epsilon) = \\mathbf{0}\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independent\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independentAdditional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)Additional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)\\[\n\\begin{aligned}\nvar(\\mathbf{y}) = \\mathbf{\\Sigma} &=  var(\\mathbf{Lf} + \\mathbf{\\epsilon}) \\\\\n&= var(\\mathbf{Lf}) + var(\\epsilon) \\\\\n&= \\mathbf{L} var(\\mathbf{f}) \\mathbf{L}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LIL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LL}' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Since \\(\\mathbf{\\Psi}\\) diagonal, -diagonal elements \\(\\mathbf{LL}'\\) \\(\\sigma_{ij}\\), co variances \\(\\mathbf{\\Sigma}\\), means \\(cov(y_i, y_j) = \\sum_{k=1}^m l_{ik}l_{jk}\\) covariance \\(\\mathbf{y}\\) completely determined m factors ( \\(m <<p\\))\\(var(y_i) = \\sum_{k=1}^m l_{ik}^2 + \\psi_i\\) \\(\\psi_i\\) specific variance summation term -th communality (.e., portion variance -th variable contributed \\(m\\) common factors (\\(h_i^2 = \\sum_{k=1}^m l_{ik}^2\\))factor model uniquely determined orthogonal transformation factors.Let \\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix \\(\\mathbf{TT}' = \\mathbf{T'T} = \\mathbf{}\\) \\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&= \\mathbf{LTT'f} + \\epsilon \\\\\n&= \\mathbf{L}^*(\\mathbf{T'f}) + \\epsilon & \\text{} \\mathbf{L}^* = \\mathbf{LT}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\mathbf{LL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LTT'L} + \\mathbf{\\Psi} \\\\\n&= (\\mathbf{L}^*)(\\mathbf{L}^*)' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Hence, orthogonal transformation factors equally good description correlations among observed traits.Let \\(\\mathbf{y} = \\mathbf{Cx}\\) , \\(\\mathbf{C}\\) diagonal matrix, \\(\\mathbf{L}_y = \\mathbf{CL}_x\\) \\(\\mathbf{\\Psi}_y = \\mathbf{C\\Psi}_x\\mathbf{C}\\)Hence, can see factor analysis also invariant changes scale","code":""},{"path":"multivariate-methods.html","id":"methods-of-estimation","chapter":"22 Multivariate Methods","heading":"22.3.1 Methods of Estimation","text":"estimate \\(\\mathbf{L}\\)Principal Component MethodPrincipal Factor Method22.3.1.3","code":""},{"path":"multivariate-methods.html","id":"principal-component-method","chapter":"22 Multivariate Methods","heading":"22.3.1.1 Principal Component Method","text":"Spectral decomposition\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\lambda_1 \\mathbf{}_1 \\mathbf{}_1' + \\dots + \\lambda_p \\mathbf{}_p \\mathbf{}_p' \\\\\n&= \\mathbf{\\Lambda }' \\\\\n&= \\sum_{k=1}^m \\lambda+k \\mathbf{}_k \\mathbf{}_k' + \\sum_{k= m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k' \\\\\n&= \\sum_{k=1}^m l_k l_k' + \\sum_{k=m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k'\n\\end{aligned}\n\\]\\(l_k = \\mathbf{}_k \\sqrt{\\lambda_k}\\) second term diagonal general.Assume\\[\n\\psi_i = \\sigma_{ii} - \\sum_{k=1}^m l_{ik}^2 = \\sigma_{ii} -  \\sum_{k=1}^m \\lambda_i a_{ik}^2\n\\]\\[\n\\mathbf{\\Sigma} \\approx \\mathbf{LL}' + \\mathbf{\\Psi}\n\\]estimate \\(\\mathbf{L}\\) \\(\\Psi\\) , use expected eigenvalues eigenvectors \\(\\mathbf{S}\\) \\(\\mathbf{R}\\)estimated factor loadings don’t change number actors increasesThe estimated factor loadings don’t change number actors increasesThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedWe select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)","code":""},{"path":"multivariate-methods.html","id":"principal-factor-method","chapter":"22 Multivariate Methods","heading":"22.3.1.2 Principal Factor Method","text":"Consider modeling correlation matrix, \\(\\mathbf{R} = \\mathbf{L} \\mathbf{L}' + \\mathbf{\\Psi}\\) . \\[\n\\mathbf{L} \\mathbf{L}' = \\mathbf{R} - \\mathbf{\\Psi} =\n\\left(\n\\begin{array}\n{cccc}\nh_1^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & h_2^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & h_p^2\n\\end{array}\n\\right)\n\\]\\(h_i^2 = 1- \\psi_i\\) (communality)Suppose initial estimates available communalities, \\((h_1^*)^2,(h_2^*)^2, \\dots , (h_p^*)^2\\), can regress trait others, use \\(r^2\\) \\(h^2\\)estimate \\(\\mathbf{R} - \\mathbf{\\Psi}\\) step k \\[\n(\\mathbf{R} - \\mathbf{\\Psi})_k =\n\\left(\n\\begin{array}\n{cccc}\n(h_1^*)^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & (h_2^*)^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & (h_p^*)^2\n\\end{array}\n\\right) =\n\\mathbf{L}_k^*(\\mathbf{L}_k^*)'\n\\]\\[\n\\mathbf{L}_k^* = (\\sqrt{\\hat{\\lambda}_1^*\\hat{\\mathbf{}}_1^* , \\dots \\hat{\\lambda}_m^*\\hat{\\mathbf{}}_m^*})\n\\]\\[\n\\hat{\\psi}_{,k}^* = 1 - \\sum_{j=1}^m \\hat{\\lambda}_i^* (\\hat{}_{ij}^*)^2\n\\]used spectral decomposition estimated matrix \\((\\mathbf{R}- \\mathbf{\\Psi})\\) calculate \\(\\hat{\\lambda}_i^* s\\) \\(\\mathbf{\\hat{}}_i^* s\\)updating values \\((\\hat{h}_i^*)^2 = 1 - \\hat{\\psi}_{,k}^*\\) use form new \\(\\mathbf{L}_{k+1}^*\\) via another spectral decomposition. Repeat processNotes:matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, can\nfix communality greater 1 1 continues.\ncontinue iterations regardless size communalities. However, results can outside fo parameter space.\n\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, canfix communality greater 1 1 continues.fix communality greater 1 1 continues.continue iterations regardless size communalities. However, results can outside fo parameter space.continue iterations regardless size communalities. However, results can outside fo parameter space.","code":""},{"path":"multivariate-methods.html","id":"maximum-likelihood-method-factor-analysis","chapter":"22 Multivariate Methods","heading":"22.3.1.3 Maximum Likelihood Method","text":"Since need likelihood function, make additional (critical) assumption \\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)restriction\\(\\mathbf{L}' \\mathbf{\\Psi}^{-1}\\mathbf{L} = \\mathbf{\\Delta}\\) \\(\\mathbf{\\Delta}\\) diagonal matrix. (since factor loading matrix unique, need restriction).Notes:Finding MLE can computationally expensiveFinding MLE can computationally expensivewe typically use methods exploratory data analysiswe typically use methods exploratory data analysisLikelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)Likelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)","code":""},{"path":"multivariate-methods.html","id":"factor-rotation","chapter":"22 Multivariate Methods","heading":"22.3.2 Factor Rotation","text":"\\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix property \\[\n\\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}} = \\hat{\\mathbf{L}}^*(\\hat{\\mathbf{L}}^*)' + \\hat{\\mathbf{\\Psi}}\n\\]\\(\\mathbf{L}^* = \\mathbf{LT}\\)means estimated specific variances communalities altered orthogonal transformation.Since infinite number choices \\(\\mathbf{T}\\), selection criterion necessaryFor example, can find orthogonal transformation maximizes objective function\\[\n\\sum_{j = 1}^m [\\frac{1}{p}\\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 - \\{\\frac{\\gamma}{p} \\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 \\}^2]\n\\]\\(\\frac{l_{ij}^{*2}}{h_i}\\) “scaled loadings”, gives variables small communalities influence.Different choices \\(\\gamma\\) objective function correspond different orthogonal rotation found literature;Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Quartimax \\(\\gamma = 0\\)Quartimax \\(\\gamma = 0\\)Equimax \\(\\gamma = m/2\\)Equimax \\(\\gamma = m/2\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Promax: non-orthogonal olique transformationsPromax: non-orthogonal olique transformationsHarris-Kaiser (HK): non-orthogonal oblique transformationsHarris-Kaiser (HK): non-orthogonal oblique transformations","code":""},{"path":"multivariate-methods.html","id":"estimation-of-factor-scores","chapter":"22 Multivariate Methods","heading":"22.3.3 Estimation of Factor Scores","text":"Recall\\[\n(\\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}_{p \\times m}\\mathbf{f}_j + \\epsilon_j\n\\]factor model correct \\[\nvar(\\epsilon_j) = \\mathbf{\\Psi} = diag (\\psi_1, \\dots , \\psi_p)\n\\]Thus consider using weighted least squares estimate \\(\\mathbf{f}_j\\) , vector factor scores j-th sampled unit \\[\n\\begin{aligned}\n\\hat{\\mathbf{f}} &= (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\mu}) \\\\\n& \\approx (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\end{aligned}\n\\]","code":""},{"path":"multivariate-methods.html","id":"the-regression-method","chapter":"22 Multivariate Methods","heading":"22.3.3.1 The Regression Method","text":"Alternatively, can use regression method estimate factor scoresConsider joint distribution \\((\\mathbf{y}_j - \\mathbf{\\mu})\\) \\(\\mathbf{f}_j\\) assuming multivariate normality, maximum likelihood approach. ,\\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_j - \\mathbf{\\mu} \\\\\n\\mathbf{f}_j\n\\end{array}\n\\right) \\sim\nN_{p + m}\n\\left(\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{LL}' + \\mathbf{\\Psi} & \\mathbf{L} \\\\\n\\mathbf{L}' & \\mathbf{}_{m\\times m}\n\\end{array}\n\\right]\n\\right)\n\\]\\(m\\) factor model correctHence,\\[\nE(\\mathbf{f}_j | \\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}(\\mathbf{y}_j - \\mathbf{\\mu})\n\\]notice \\(\\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}\\) \\(m \\times p\\) matrix regression coefficientsThen, use estimated conditional mean vector estimate factor scores\\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}})^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]Alternatively, reduce effect possible incorrect determination fo number factors \\(m\\) using \\(\\mathbf{S}\\) substitute \\(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}}\\) \\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'\\mathbf{S}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]\\(j = 1,\\dots,n\\)","code":""},{"path":"multivariate-methods.html","id":"model-diagnostic","chapter":"22 Multivariate Methods","heading":"22.3.4 Model Diagnostic","text":"PlotsPlotsCheck outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check multivariate normality assumptionCheck multivariate normality assumptionUse univariate tests normality check factor scoresUse univariate tests normality check factor scoresConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fitConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fit","code":""},{"path":"multivariate-methods.html","id":"application-11","chapter":"22 Multivariate Methods","heading":"22.3.5 Application","text":"psych package,h2 = communalitiesh2 = communalitiesu2 = uniquenessu2 = uniquenesscom = complexitycom = complexityThe output info null hypothesis common factors statement “degrees freedom null model ..”output info null hypothesis number factors sufficient statement “total number observations …”One factor enough, two sufficient, enough data 3 factors (df -2 NA p-value). Hence, use 2-factor model.","code":"\nlibrary(psych)\nlibrary(tidyverse)\n## Load the data from the psych package\ndata(Harman.5)\nHarman.5\n#>         population schooling employment professional housevalue\n#> Tract1        5700      12.8       2500          270      25000\n#> Tract2        1000      10.9        600           10      10000\n#> Tract3        3400       8.8       1000           10       9000\n#> Tract4        3800      13.6       1700          140      25000\n#> Tract5        4000      12.8       1600          140      25000\n#> Tract6        8200       8.3       2600           60      12000\n#> Tract7        1200      11.4        400           10      16000\n#> Tract8        9100      11.5       3300           60      14000\n#> Tract9        9900      12.5       3400          180      18000\n#> Tract10       9600      13.7       3600          390      25000\n#> Tract11       9600       9.6       3300           80      12000\n#> Tract12       9400      11.4       4000          100      13000\n\n# Correlation matrix\ncor_mat <- cor(Harman.5)\ncor_mat\n#>              population  schooling employment professional housevalue\n#> population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157\n#> schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009\n#> employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599\n#> professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425\n#> housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000\n\n## Principal Component Method with Correlation\ncor_pca <- prcomp(Harman.5, scale = T)\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\n\ncor_results <- cor_results %>%\n    mutate(\n        proportion = eigen_values / sum(eigen_values),\n        cumulative = cumsum(proportion),\n        number = row_number()\n    )\ncor_results\n#>   eigen_values  proportion cumulative number\n#> 1   2.87331359 0.574662719  0.5746627      1\n#> 2   1.79666009 0.359332019  0.9339947      2\n#> 3   0.21483689 0.042967377  0.9769621      3\n#> 4   0.09993405 0.019986811  0.9969489      4\n#> 5   0.01525537 0.003051075  1.0000000      5\n\n# Scree plot of Eigenvalues\nscree_gg <- ggplot(cor_results, aes(x = number, y = eigen_values)) +\n    geom_line(alpha = 0.5) +\n    geom_text(aes(label = number)) +\n    scale_x_continuous(name = \"Number\") +\n    scale_y_continuous(name = \"Eigenvalue\") +\n    theme_bw()\nscree_gg\n\nscreeplot(cor_pca, type = 'lines')\n\n## Keep 2 factors based on scree plot and eigenvalues\nfactor_pca <- principal(Harman.5, nfactors = 2, rotate = \"none\")\nfactor_pca\n#> Principal Components Analysis\n#> Call: principal(r = Harman.5, nfactors = 2, rotate = \"none\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               PC1   PC2   h2    u2 com\n#> population   0.58  0.81 0.99 0.012 1.8\n#> schooling    0.77 -0.54 0.89 0.115 1.8\n#> employment   0.67  0.73 0.98 0.021 2.0\n#> professional 0.93 -0.10 0.88 0.120 1.0\n#> housevalue   0.79 -0.56 0.94 0.062 1.8\n#> \n#>                        PC1  PC2\n#> SS loadings           2.87 1.80\n#> Proportion Var        0.57 0.36\n#> Cumulative Var        0.57 0.93\n#> Proportion Explained  0.62 0.38\n#> Cumulative Proportion 0.62 1.00\n#> \n#> Mean item complexity =  1.7\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.03 \n#>  with the empirical chi square  0.29  with prob <  0.59 \n#> \n#> Fit based upon off diagonal values = 1\n\n# factor 1 = overall socioeconomic health\n# factor 2 = contrast of the population and employment against school and house value\n\n\n## Ssquared multiple correlation (SMC) prior, no rotation\nfactor_pca_smc <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_pca_smc\n#> Factor Analysis using method =  pa\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               PA1   PA2   h2      u2 com\n#> population   0.62  0.78 1.00 -0.0027 1.9\n#> schooling    0.70 -0.53 0.77  0.2277 1.9\n#> employment   0.70  0.68 0.96  0.0413 2.0\n#> professional 0.88 -0.15 0.80  0.2017 1.1\n#> housevalue   0.78 -0.60 0.96  0.0361 1.9\n#> \n#>                        PA1  PA2\n#> SS loadings           2.76 1.74\n#> Proportion Var        0.55 0.35\n#> Cumulative Var        0.55 0.90\n#> Proportion Explained  0.61 0.39\n#> Cumulative Proportion 0.61 1.00\n#> \n#> Mean item complexity =  1.7\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.34 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.03 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.02  with prob <  0.88 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.596\n#> RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n#> BIC =  -0.04\n#> Fit based upon off diagonal values = 1\n\n## SMC prior, Promax rotation\nfactor_pca_smc_pro <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"Promax\",\n    SMC = TRUE\n)\nfactor_pca_smc_pro\n#> Factor Analysis using method =  pa\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"Promax\", SMC = TRUE, \n#>     fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                PA1   PA2   h2      u2 com\n#> population   -0.11  1.02 1.00 -0.0027 1.0\n#> schooling     0.90 -0.11 0.77  0.2277 1.0\n#> employment    0.02  0.97 0.96  0.0413 1.0\n#> professional  0.75  0.33 0.80  0.2017 1.4\n#> housevalue    1.01 -0.14 0.96  0.0361 1.0\n#> \n#>                        PA1  PA2\n#> SS loadings           2.38 2.11\n#> Proportion Var        0.48 0.42\n#> Cumulative Var        0.48 0.90\n#> Proportion Explained  0.53 0.47\n#> Cumulative Proportion 0.53 1.00\n#> \n#>  With factor correlations of \n#>      PA1  PA2\n#> PA1 1.00 0.25\n#> PA2 0.25 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.34 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.03 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.02  with prob <  0.88 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.596\n#> RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n#> BIC =  -0.04\n#> Fit based upon off diagonal values = 1\n\n## SMC prior, varimax rotation\nfactor_pca_smc_var <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"varimax\",\n    SMC = TRUE\n)\n## Make a data frame of the loadings for ggplot2\nfactors_df <-\n    bind_rows(\n        data.frame(\n            y = rownames(factor_pca_smc$loadings),\n            unclass(factor_pca_smc$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_pro$loadings),\n            unclass(factor_pca_smc_pro$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_var$loadings),\n            unclass(factor_pca_smc_var$loadings)\n        ),\n        .id = \"Rotation\"\n    )\nflag_gg <- ggplot(factors_df) +\n    geom_vline(aes(xintercept = 0)) +\n    geom_hline(aes(yintercept = 0)) +\n    geom_point(aes(\n        x = PA2,\n        y = PA1,\n        col = y,\n        shape = y\n    ), size = 2) +\n    scale_x_continuous(name = \"Factor 2\", limits = c(-1.1, 1.1)) +\n    scale_y_continuous(name = \"Factor1\", limits = c(-1.1, 1.1)) +\n    facet_wrap(\"Rotation\", labeller = labeller(Rotation = c(\n        \"1\" = \"Original\", \"2\" = \"Promax\", \"3\" = \"Varimax\"\n    ))) +\n    coord_fixed(ratio = 1) # make aspect ratio of each facet 1\n\nflag_gg\n\n# promax and varimax did a good job to assign trait to a particular factor\n\nfactor_mle_1 <- fa(\n    Harman.5,\n    nfactors = 1,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_1\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 1, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>               ML1    h2     u2 com\n#> population   0.97 0.950 0.0503   1\n#> schooling    0.14 0.021 0.9791   1\n#> employment   1.00 0.995 0.0049   1\n#> professional 0.51 0.261 0.7388   1\n#> housevalue   0.12 0.014 0.9864   1\n#> \n#>                 ML1\n#> SS loadings    2.24\n#> Proportion Var 0.45\n#> \n#> Mean item complexity =  1\n#> Test of the hypothesis that 1 factor is sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 5  and the objective function was  3.14 \n#> \n#> The root mean square of the residuals (RMSR) is  0.41 \n#> The df corrected root mean square of the residuals is  0.57 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  39.41  with prob <  2e-07 \n#> The total n.obs was  12  with Likelihood Chi Square =  24.56  with prob <  0.00017 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.022\n#> RMSEA index =  0.564  and the 90 % confidence intervals are  0.374 0.841\n#> BIC =  12.14\n#> Fit based upon off diagonal values = 0.5\n#> Measures of factor score adequacy             \n#>                                                    ML1\n#> Correlation of (regression) scores with factors   1.00\n#> Multiple R square of scores with factors          1.00\n#> Minimum correlation of possible factor scores     0.99\n\nfactor_mle_2 <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_2\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                ML2  ML1   h2    u2 com\n#> population   -0.03 1.00 1.00 0.005 1.0\n#> schooling     0.90 0.04 0.81 0.193 1.0\n#> employment    0.09 0.98 0.96 0.036 1.0\n#> professional  0.78 0.46 0.81 0.185 1.6\n#> housevalue    0.96 0.05 0.93 0.074 1.0\n#> \n#>                        ML2  ML1\n#> SS loadings           2.34 2.16\n#> Proportion Var        0.47 0.43\n#> Cumulative Var        0.47 0.90\n#> Proportion Explained  0.52 0.48\n#> Cumulative Proportion 0.52 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are 1  and the objective function was  0.31 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.05 \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0.05  with prob <  0.82 \n#> The total n.obs was  12  with Likelihood Chi Square =  2.22  with prob <  0.14 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.658\n#> RMSEA index =  0.307  and the 90 % confidence intervals are  0 0.945\n#> BIC =  -0.26\n#> Fit based upon off diagonal values = 1\n#> Measures of factor score adequacy             \n#>                                                    ML2  ML1\n#> Correlation of (regression) scores with factors   0.98 1.00\n#> Multiple R square of scores with factors          0.95 1.00\n#> Minimum correlation of possible factor scores     0.91 0.99\n\nfactor_mle_3 <- fa(\n    Harman.5,\n    nfactors = 3,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_3\n#> Factor Analysis using method =  ml\n#> Call: fa(r = Harman.5, nfactors = 3, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                ML2  ML1   ML3   h2     u2 com\n#> population   -0.12 0.98 -0.11 0.98 0.0162 1.1\n#> schooling     0.89 0.15  0.29 0.90 0.0991 1.3\n#> employment    0.00 1.00  0.04 0.99 0.0052 1.0\n#> professional  0.72 0.52 -0.10 0.80 0.1971 1.9\n#> housevalue    0.97 0.13 -0.09 0.97 0.0285 1.1\n#> \n#>                        ML2  ML1  ML3\n#> SS loadings           2.28 2.26 0.11\n#> Proportion Var        0.46 0.45 0.02\n#> Cumulative Var        0.46 0.91 0.93\n#> Proportion Explained  0.49 0.49 0.02\n#> Cumulative Proportion 0.49 0.98 1.00\n#> \n#> Mean item complexity =  1.2\n#> Test of the hypothesis that 3 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  6.38 with Chi Square =  54.25\n#> df of  the model are -2  and the objective function was  0 \n#> \n#> The root mean square of the residuals (RMSR) is  0 \n#> The df corrected root mean square of the residuals is  NA \n#> \n#> The harmonic n.obs is  12 with the empirical chi square  0  with prob <  NA \n#> The total n.obs was  12  with Likelihood Chi Square =  0  with prob <  NA \n#> \n#> Tucker Lewis Index of factoring reliability =  1.318\n#> Fit based upon off diagonal values = 1\n#> Measures of factor score adequacy             \n#>                                                    ML2  ML1  ML3\n#> Correlation of (regression) scores with factors   0.99 1.00 0.82\n#> Multiple R square of scores with factors          0.98 1.00 0.68\n#> Minimum correlation of possible factor scores     0.96 0.99 0.36"},{"path":"multivariate-methods.html","id":"discriminant-analysis","chapter":"22 Multivariate Methods","heading":"22.4 Discriminant Analysis","text":"Suppose two different populations observations come . Discriminant analysis seeks determine possible population observation comes making mistakes possibleThis alternative logistic approaches following advantages:\nclear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches suffer\nX normal classes sample size small, discriminant approaches can accurate\nalternative logistic approaches following advantages:clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferwhen clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferIf X normal classes sample size small, discriminant approaches can accurateIf X normal classes sample size small, discriminant approaches can accurateNotationSimilar MANOVA, let \\(\\mathbf{y}_{j1},\\mathbf{y}_{j2},\\dots, \\mathbf{y}_{in_j} \\sim iid f_j (\\mathbf{y})\\) \\(j = 1,\\dots, h\\)Let \\(f_j(\\mathbf{y})\\) density function population j . Note vector \\(\\mathbf{y}\\) contain measurements \\(p\\) traitsAssume observation one \\(h\\) possible populations.want form discriminant rule allocate observation \\(\\mathbf{y}\\) population j \\(\\mathbf{y}\\) fact population","code":""},{"path":"multivariate-methods.html","id":"known-populations","chapter":"22 Multivariate Methods","heading":"22.4.1 Known Populations","text":"maximum likelihood discriminant rule assigning observation \\(\\mathbf{y}\\) one \\(h\\) populations allocates \\(\\mathbf{y}\\) population gives largest likelihood \\(\\mathbf{y}\\)Consider likelihood single observation \\(\\mathbf{y}\\), form \\(f_j (\\mathbf{y})\\) j true population.Since \\(j\\) unknown, make likelihood large possible, choose value j causes \\(f_j (\\mathbf{y})\\) large possibleConsider simple univariate example. Suppose data one two binomial populations.first population \\(n= 10\\) trials success probability \\(p = .5\\)first population \\(n= 10\\) trials success probability \\(p = .5\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)population assign observation \\(y = 7\\)population assign observation \\(y = 7\\)Note:\n\\(f(y = 7|n = 10, p = .5) = .117\\)\n\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\nHence, choose second population\nNote:\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.Hence, choose second populationHence, choose second populationAnother exampleWe 2 populations, whereFirst population: \\(N(\\mu_1, \\sigma^2_1)\\)First population: \\(N(\\mu_1, \\sigma^2_1)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)likelihood single observation \\[\nf_j (y) = (2\\pi \\sigma^2_j)^{-1/2} \\exp\\{ -\\frac{1}{2}(\\frac{y - \\mu_j}{\\sigma_j})^2\\}\n\\]Consider likelihood ratio rule\\[\n\\begin{aligned}\n\\Lambda &= \\frac{\\text{likelihood y pop 1}}{\\text{likelihood y pop 2}} \\\\\n&= \\frac{f_1(y)}{f_2(y)} \\\\\n&= \\frac{\\sigma_2}{\\sigma_1} \\exp\\{-\\frac{1}{2}[(\\frac{y - \\mu_1}{\\sigma_1})^2- (\\frac{y - \\mu_2}{\\sigma_2})^2] \\}\n\\end{aligned}\n\\]Hence, classify intopop 1 \\(\\Lambda >1\\)pop 1 \\(\\Lambda >1\\)pop 2 \\(\\Lambda <1\\)pop 2 \\(\\Lambda <1\\)ties, flip coinfor ties, flip coinAnother way think:classify population 1 “standardized distance” y \\(\\mu_1\\) less “standardized distance” y \\(\\mu_2\\) referred quadratic discriminant rule.(Significant simplification occurs th special case \\(\\sigma_1 = \\sigma_2 = \\sigma^2\\))Thus, classify population 1 \\[\n(y - \\mu_2)^2 > (y - \\mu_1)^2\n\\]\\[\n|y- \\mu_2| > |y - \\mu_1|\n\\]\\[\n-2 \\log (\\Lambda) = -2y  \\frac{(\\mu_1 - \\mu_2)}{\\sigma^2} + \\frac{(\\mu_1^2 - \\mu_2^2)}{\\sigma^2} = \\beta y + \\alpha\n\\]Thus, classify population 1 less 0.Discriminant classification rule linear y case.","code":""},{"path":"multivariate-methods.html","id":"multivariate-expansion","chapter":"22 Multivariate Methods","heading":"22.4.1.1 Multivariate Expansion","text":"Suppose 2 populations\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\[\n\\begin{aligned}\n-2 \\log(\\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})}) &= \\log|\\mathbf{\\Sigma}_1| + (\\mathbf{x} - \\mathbf{\\mu}_1)' \\mathbf{\\Sigma}^{-1}_1 (\\mathbf{x} - \\mathbf{\\mu}_1) \\\\\n&- [\\log|\\mathbf{\\Sigma}_2|+ (\\mathbf{x} - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}_2 (\\mathbf{x} - \\mathbf{\\mu}_2) ]\n\\end{aligned}\n\\], classify population 1 less 0, otherwise, population 2. like univariate case non-equal variances, quadratic discriminant rule.covariance matrices equal: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}_1\\) classify population 1 \\[\n(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\ge 0\n\\]linear discriminant rule also referred Fisher’s linear discriminant functionBy assuming covariance matrices equal, assume shape orientation fo two populations must (can strong restriction)words, variable, can different mean variance.Note: LDA Bayes decision boundary linear. Hence, quadratic decision boundary might lead better classification. Moreover, assumption variance/covariance matrix across classes Gaussian densities imposes linear rule, allow predictors class follow MVN distribution class-specific mean vectors variance/covariance matrices, Quadratic Discriminant Analysis. , parameters estimate (gives flexibility LDA) cost variance (bias -variance tradeoff).\\(\\mathbf{\\mu}_1, \\mathbf{\\mu}_2, \\mathbf{\\Sigma}\\) known, probability misclassification can determined:\\[\n\\begin{aligned}\nP(2|1) &= P(\\text{calssify pop 2| x pop 1}) \\\\\n&= P((\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} \\mathbf{x} \\le \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)|\\mathbf{x} \\sim N(\\mu_1, \\mathbf{\\Sigma}) \\\\\n&= \\Phi(-\\frac{1}{2} \\delta)\n\\end{aligned}\n\\]\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\Phi\\) standard normal CDF\\(\\Phi\\) standard normal CDFSuppose \\(h\\) possible populations, distributed \\(N_p (\\mathbf{\\mu}_p, \\mathbf{\\Sigma})\\). , maximum likelihood (linear) discriminant rule allocates \\(\\mathbf{y}\\) population j j minimizes squared Mahalanobis distance\\[\n(\\mathbf{y} - \\mathbf{\\mu}_j)' \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{\\mu}_j)\n\\]","code":""},{"path":"multivariate-methods.html","id":"bayes-discriminant-rules","chapter":"22 Multivariate Methods","heading":"22.4.1.2 Bayes Discriminant Rules","text":"know population j prior probabilities \\(\\pi_j\\) (assume \\(\\pi_j >0\\)) can form Bayes discriminant rule.rule allocates observation \\(\\mathbf{y}\\) population \\(\\pi_j f_j (\\mathbf{y})\\) maximized.Note:Maximum likelihood discriminant rule special case Bayes discriminant rule, sets \\(\\pi_j = 1/h\\)Optimal Properties Bayes Discriminant Ruleslet \\(p_{ii}\\) probability correctly assigning observation population ilet \\(p_{ii}\\) probability correctly assigning observation population ithen one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .rule better alternative called admissibleA rule better alternative called admissibleBayes Discriminant Rules admissibleBayes Discriminant Rules admissibleIf utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)Bayes Discriminant Rules largest possible posterior probability correct allocation respect priorBayes Discriminant Rules largest possible posterior probability correct allocation respect priorThese properties show Bayes Discriminant rule best approach.properties show Bayes Discriminant rule best approach.Unequal CostWe want consider cost misallocation\nDefine \\(c_{ij}\\) cost associated allocation member population j population .\nwant consider cost misallocationDefine \\(c_{ij}\\) cost associated allocation member population j population .Assume \n\\(c_{ij} >0\\) \\(\\neq j\\)\n\\(c_{ij} = 0\\) \\(= j\\)\nAssume \\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} = 0\\) \\(= j\\)\\(c_{ij} = 0\\) \\(= j\\)determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)Example:Two binomial populations, size 10, probabilities \\(p_1 = .5\\) \\(p_2 = .7\\)probability first population .9However, suppose cost inappropriately allocating first population 1 cost incorrectly allocating second population 5.case, pick population 1 population 2In general, consider two regions, \\(R_1\\) \\(R_2\\) associated population 1 2:\\[\nR_1: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} \\ge \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\[\nR_2: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} < \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\(c_{12}\\) cost assigning member population 2 population 1.","code":""},{"path":"multivariate-methods.html","id":"discrimination-under-estimation","chapter":"22 Multivariate Methods","heading":"22.4.1.3 Discrimination Under Estimation","text":"Suppose know form distributions populations interests, still estimate parameters.Example:know distributions multivariate normal, estimate means variancesThe maximum likelihood discriminant rule allocates observation \\(\\mathbf{y}\\) population j j maximizes function\\[\nf_j (\\mathbf{y} |\\hat{\\theta})\n\\]\\(\\hat{\\theta}\\) maximum likelihood estimates unknown parametersFor instance, 2 multivariate normal populations distinct means, common variance covariance matrixMLEs \\(\\mathbf{\\mu}_1\\) \\(\\mathbf{\\mu}_2\\) \\(\\mathbf{\\bar{y}}_1\\) \\(\\mathbf{\\bar{y}}_2\\)common \\(\\mathbf{\\Sigma}\\) \\(\\mathbf{S}\\).Thus, estimated discriminant rule formed substituting sample values population values","code":""},{"path":"multivariate-methods.html","id":"native-bayes","chapter":"22 Multivariate Methods","heading":"22.4.1.4 Native Bayes","text":"challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .Naive Bayes makes one assumption: within k-th class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)Naive Bayes makes one assumption: within k-th class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)\n\\]\\(f_{kj}\\) density function j-th predictor among observation k-th class.assumption allows use joint distribution without need account dependence observations. However, (native) assumption can unrealistic, still works well cases number sample (n) large relative number features (p).assumption, \\[\nP(Y=k|X=x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1)\\times \\dots f_{lp}(x_p)}\n\\]need estimate one-dimensional density function \\(f_{kj}\\) either approaches:\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.\\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.","code":""},{"path":"multivariate-methods.html","id":"comparison-of-classification-methods","chapter":"22 Multivariate Methods","heading":"22.4.1.5 Comparison of Classification Methods","text":"Assuming K classes K baseline (James , Witten, Hastie, Tibshirani book)Comparing log odds relative K class","code":""},{"path":"multivariate-methods.html","id":"logistic-regression-2","chapter":"22 Multivariate Methods","heading":"22.4.1.5.1 Logistic Regression","text":"\\[\n\\log(\\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj}x_j\n\\]","code":""},{"path":"multivariate-methods.html","id":"lda","chapter":"22 Multivariate Methods","heading":"22.4.1.5.2 LDA","text":"\\[\n\\log(\\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \\sum_{j=1}^p b_{kj} x_j\n\\]\\(a_k\\) \\(b_{kj}\\) functions \\(\\pi_k, \\pi_K, \\mu_k , \\mu_K, \\mathbf{\\Sigma}\\)Similar logistic regression, LDA assumes log odds linear \\(x\\)Even though look like form, parameters logistic regression estimated MLE, LDA linear parameters specified prior normal distributionsWe expect LDA outperform logistic regression normality assumption (approximately) holds, logistic regression perform better ","code":""},{"path":"multivariate-methods.html","id":"qda","chapter":"22 Multivariate Methods","heading":"22.4.1.5.3 QDA","text":"\\[\n\\log(\\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \\sum_{j=1}^{p}b_{kj}x_{j} + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl}x_j x_l\n\\]\\(a_k, b_{kj}, c_{kjl}\\) functions \\(\\pi_k , \\pi_K, \\mu_k, \\mu_K ,\\mathbf{\\Sigma}_k, \\mathbf{\\Sigma}_K\\)","code":""},{"path":"multivariate-methods.html","id":"naive-bayes","chapter":"22 Multivariate Methods","heading":"22.4.1.5.4 Naive Bayes","text":"\\[\n\\log (\\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \\sum_{j=1}^p g_{kj} (x_j)\n\\]\\(a_k = \\log (\\pi_k / \\pi_K)\\) \\(g_{kj}(x_j) = \\log(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\\) form generalized additive model","code":""},{"path":"multivariate-methods.html","id":"summary-6","chapter":"22 Multivariate Methods","heading":"22.4.1.5.5 Summary","text":"LDA special case QDALDA special case QDALDA robust comes high dimensionsLDA robust comes high dimensionsAny classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).Compare non-parametric method (KNN)KNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN also limited high-dimensions due curse dimensionalityKNN also limited high-dimensions due curse dimensionalitySince QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methods KNN classification. QDA can fewer training observations KNN flexible.Since QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methods KNN classification. QDA can fewer training observations KNN flexible.simulation:like linear regression, can also introduce flexibility including transformed features \\(\\sqrt{X}, X^2, X^3\\)","code":""},{"path":"multivariate-methods.html","id":"probabilities-of-misclassification","chapter":"22 Multivariate Methods","heading":"22.4.2 Probabilities of Misclassification","text":"distribution exactly known, can determine misclassification probabilities exactly. however, need estimate population parameters, estimate probability misclassificationNaive method\nPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.\ntend optimistic number samples one populations small.\nNaive methodPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.Plugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.tend optimistic number samples one populations small.tend optimistic number samples one populations small.Resubstitution method\nUse proportion samples population allocated another population estimate misclassification probability\nalso optimistic number samples small\nResubstitution methodUse proportion samples population allocated another population estimate misclassification probabilityUse proportion samples population allocated another population estimate misclassification probabilityBut also optimistic number samples smallBut also optimistic number samples smallJack-knife estimates:\ntwo methods use observation estimate parameters also misclassification probabilities based upon discriminant rule\nAlternatively, determine discriminant rule based upon data except k-th observation j-th population\n, determine k-th observation misclassified rule\nperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassified\nrepeat process \\(\\neq j\\) populations\nmethod reliable others, also computationally intensive\nJack-knife estimates:two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleThe two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleAlternatively, determine discriminant rule based upon data except k-th observation j-th populationAlternatively, determine discriminant rule based upon data except k-th observation j-th populationthen, determine k-th observation misclassified rulethen, determine k-th observation misclassified ruleperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassifiedperform process \\(n_j\\) observation population j . estimate fo misclassification probability fraction \\(n_j\\) observations misclassifiedrepeat process \\(\\neq j\\) populationsrepeat process \\(\\neq j\\) populationsThis method reliable others, also computationally intensiveThis method reliable others, also computationally intensiveCross-ValidationCross-ValidationSummaryConsider group-specific densities \\(f_j (\\mathbf{x})\\) multivariate vector \\(\\mathbf{x}\\).Assume equal misclassifications costs, Bayes classification probability \\(\\mathbf{x}\\) belonging j-th population \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j f_j (\\mathbf{x})}{\\sum_{k=1}^h \\pi_k f_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)\\(h\\) possible groups.classify group probability membership largestAlternatively, can write terms generalized squared distance formation\\[\nD_j^2 (\\mathbf{x}) = d_j^2 (\\mathbf{x})+ g_1(j) + g_2 (j)\n\\]\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \n\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\n\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\n\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriateand\\[\ng_1(j) =\n\\begin{cases}\n\\ln |\\mathbf{S}_j| & \\text{within group covariances equal} \\\\\n0 & \\text{pooled covariance}\n\\end{cases}\n\\]\\[\ng_2(j) =\n\\begin{cases}\n-2 \\ln \\pi_j & \\text{prior probabilities equal} \\\\\n0 & \\text{prior probabilities equal}\n\\end{cases}\n\\], posterior probability belonging group j \\[\np(j| \\mathbf{x})  = \\frac{\\exp(-.5 D_j^2(\\mathbf{x}))}{\\sum_{k=1}^h \\exp(-.5 D^2_k (\\mathbf{x}))}\n\\]\\(j = 1,\\dots , h\\)\\(\\mathbf{x}\\) classified group j \\(p(j | \\mathbf{x})\\) largest \\(j = 1,\\dots,h\\) (, \\(D_j^2(\\mathbf{x})\\) smallest).","code":""},{"path":"multivariate-methods.html","id":"assessing-classification-performance","chapter":"22 Multivariate Methods","heading":"22.4.2.1 Assessing Classification Performance","text":"binary classification, confusion matrixand table 4.6 (James et al. 2013)ROC curve (receiver Operating Characteristics) graphical comparison sensitivity (true positive) specificity ( = 1 - false positive)y-axis = true positive ratex-axis = false positive rateas change threshold rate classifying observation 0 1AUC (area ROC) ideally equal 1, bad classifier AUC = 0.5 (pure chance)","code":""},{"path":"multivariate-methods.html","id":"unknown-populations-nonparametric-discrimination","chapter":"22 Multivariate Methods","heading":"22.4.3 Unknown Populations/ Nonparametric Discrimination","text":"multivariate data Gaussian, known distributional form , can use following methods","code":""},{"path":"multivariate-methods.html","id":"kernel-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.1 Kernel Methods","text":"approximate \\(f_j (\\mathbf{x})\\) kernel density estimate\\[\n\\hat{f}_j(\\mathbf{x}) = \\frac{1}{n_j} \\sum_{= 1}^{n_j} K_j (\\mathbf{x} - \\mathbf{x}_i)\n\\]\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.Thus, finding \\(\\hat{f}_j (\\mathbf{x})\\) \\(h\\) populations, posterior probability group membership \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j \\hat{f}_j (\\mathbf{x})}{\\sum_{k-1}^h \\pi_k \\hat{f}_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)different choices kernel function:UniformUniformNormalNormalEpanechnikovEpanechnikovBiweightBiweightTriweightTriweightWe kernels, pick “radius” (variance, width, window width, bandwidth) kernel, smoothing parameter (larger radius, smooth kernel estimate density).select smoothness parameter, can use following methodIf believe populations close multivariate normal, \\[\nR = (\\frac{4/(2p+1)}{n_j})^{1/(p+1}\n\\]since know sure, might choose several different values select one vies best sample cross-validation discrimination.Moreover, also decide whether use different kernel smoothness different populations, similar individual pooled covariances classical methodology.","code":""},{"path":"multivariate-methods.html","id":"nearest-neighbor-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.2 Nearest Neighbor Methods","text":"nearest neighbor (also known k-nearest neighbor) method performs classification new observation vector based group membership nearest neighbors. practice, find\\[\nd_{ij}^2 (\\mathbf{x}, \\mathbf{x}_i) = (\\mathbf{x}, \\mathbf{x}_i) V_j^{-1}(\\mathbf{x}, \\mathbf{x}_i)\n\\]distance vector \\(\\mathbf{x}\\) \\(\\)-th observation group \\(j\\)consider different choices \\(\\mathbf{V}_j\\)example,\\[\n\\begin{aligned}\n\\mathbf{V}_j &= \\mathbf{S}_p \\\\\n\\mathbf{V}_j &= \\mathbf{S}_j \\\\\n\\mathbf{V}_j &= \\mathbf{} \\\\\n\\mathbf{V}_j &= diag (\\mathbf{S}_p)\n\\end{aligned}\n\\]find \\(k\\) observations closest \\(\\mathbf{x}\\) (users pick \\(k\\)). classify common population, weighted prior.","code":""},{"path":"multivariate-methods.html","id":"modern-discriminant-methods","chapter":"22 Multivariate Methods","heading":"22.4.3.3 Modern Discriminant Methods","text":"Note:Logistic regression (without random effects) flexible model-based procedure classification two populations.extension logistic regression multi-group setting polychotomous logistic regression (, mulinomial regression).machine learning pattern recognition growing strong focus nonlinear discriminant analysis methods :radial basis function networksradial basis function networkssupport vector machinessupport vector machinesmultiplayer perceptrons (neural networks)multiplayer perceptrons (neural networks)general framework\\[\ng_j (\\mathbf{x}) = \\sum_{l = 1}^m w_{jl}\\phi_l (\\mathbf{x}; \\mathbf{\\theta}_l) + w_{j0}\n\\]\\(j = 1,\\dots, h\\)\\(j = 1,\\dots, h\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)assign \\(\\mathbf{x}\\) \\(j\\)-th population \\(g_j(\\mathbf{x})\\) maximum \\(j = 1,\\dots, h\\)Development usually focuses choice estimation basis functions, \\(\\phi_l\\) estimation weights \\(w_{jl}\\)details can found (Webb, Copsey, Cawley 2011)","code":""},{"path":"multivariate-methods.html","id":"application-12","chapter":"22 Multivariate Methods","heading":"22.4.4 Application","text":"","code":"\nlibrary(class)\nlibrary(klaR)\nlibrary(MASS)\nlibrary(tidyverse)\n\n## Read in the data\ncrops <- read.table(\"images/crops.txt\")\nnames(crops) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops)\n#> 'data.frame':    36 obs. of  5 variables:\n#>  $ crop: chr  \"Corn\" \"Corn\" \"Corn\" \"Corn\" ...\n#>  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...\n#>  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...\n#>  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...\n#>  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...\n\n\n## Read in test data\ncrops_test <- read.table(\"images/crops_test.txt\")\nnames(crops_test) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops_test)\n#> 'data.frame':    5 obs. of  5 variables:\n#>  $ crop: chr  \"Corn\" \"Soybeans\" \"Cotton\" \"Sugarbeets\" ...\n#>  $ y1  : int  16 21 29 54 32\n#>  $ y2  : int  27 25 24 23 32\n#>  $ y3  : int  31 23 26 21 62\n#>  $ y4  : int  33 24 28 54 16"},{"path":"multivariate-methods.html","id":"lda-1","chapter":"22 Multivariate Methods","heading":"22.4.4.1 LDA","text":"Default prior proportional sample size lda qda fit constant intercept termLDA didn’t well within sample --sample data.","code":"\n## Linear discriminant analysis\nlda_mod <- lda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\nlda_mod\n#> Call:\n#> lda(crop ~ y1 + y2 + y3 + y4, data = crops)\n#> \n#> Prior probabilities of groups:\n#>     Clover       Corn     Cotton   Soybeans Sugarbeets \n#>  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 \n#> \n#> Group means:\n#>                  y1       y2       y3       y4\n#> Clover     46.36364 32.63636 34.18182 36.63636\n#> Corn       15.28571 22.71429 27.42857 33.14286\n#> Cotton     34.50000 32.66667 35.00000 39.16667\n#> Soybeans   21.00000 27.00000 23.50000 29.66667\n#> Sugarbeets 31.00000 32.16667 20.00000 40.50000\n#> \n#> Coefficients of linear discriminants:\n#>              LD1          LD2         LD3          LD4\n#> y1 -6.147360e-02  0.009215431 -0.02987075 -0.014680566\n#> y2 -2.548964e-02  0.042838972  0.04631489  0.054842132\n#> y3  1.642126e-02 -0.079471595  0.01971222  0.008938745\n#> y4  5.143616e-05 -0.013917423  0.05381787 -0.025717667\n#> \n#> Proportion of trace:\n#>    LD1    LD2    LD3    LD4 \n#> 0.7364 0.1985 0.0576 0.0075\n\n## Look at accuracy on the training data\nlda_fitted <- predict(lda_mod,newdata = crops)\n# Contingency table\nlda_table <- table(truth = crops$crop, fitted = lda_fitted$class)\nlda_table\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          6    0      3        0          2\n#>   Corn            0    6      0        1          0\n#>   Cotton          3    0      1        2          0\n#>   Soybeans        0    1      1        3          1\n#>   Sugarbeets      1    1      0        2          2\n# accuracy of 0.5 is just random (not good)\n\n## Posterior probabilities of membership\ncrops_post <- cbind.data.frame(crops,\n                               crop_pred = lda_fitted$class,\n                               lda_fitted$posterior)\ncrops_post <- crops_post %>%\n    mutate(missed = crop != crop_pred)\nhead(crops_post)\n#>   crop y1 y2 y3 y4 crop_pred     Clover      Corn    Cotton  Soybeans\n#> 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845\n#> 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101\n#> 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105\n#> 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477\n#> 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696\n#> 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924\n#>   Sugarbeets missed\n#> 1 0.08971545  FALSE\n#> 2 0.07219340  FALSE\n#> 3 0.11573442  FALSE\n#> 4 0.09546233  FALSE\n#> 5 0.03980738  FALSE\n#> 6 0.10109590   TRUE\n# posterior shows that posterior of corn membership is much higher than the prior\n\n## LOOCV\n# leave-one-out cross validation for linear discriminant analysis\n# cannot run the predict function using the object with CV = TRUE \n# because it returns the within sample predictions\nlda_cv <- lda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nlda_table_cv <- table(truth = crops$crop, fitted = lda_cv$class)\nlda_table_cv\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          4    3      1        0          3\n#>   Corn            0    4      1        2          0\n#>   Cotton          3    0      0        2          1\n#>   Soybeans        0    1      1        3          1\n#>   Sugarbeets      2    1      0        2          1\n\n## Predict the test data\nlda_pred <- predict(lda_mod, newdata = crops_test)\n\n## Make a contingency table with truth and most likely class\ntable(truth=crops_test$crop, predict=lda_pred$class)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          0    0      1        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      0        1          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      1    0      0        0          0"},{"path":"multivariate-methods.html","id":"qda-1","chapter":"22 Multivariate Methods","heading":"22.4.4.2 QDA","text":"","code":"\n## Quadratic discriminant analysis\nqda_mod <- qda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\n\n## Look at accuracy on the training data\nqda_fitted <- predict(qda_mod, newdata = crops)\n# Contingency table\nqda_table <- table(truth = crops$crop, fitted = qda_fitted$class)\nqda_table\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          9    0      0        0          2\n#>   Corn            0    7      0        0          0\n#>   Cotton          0    0      6        0          0\n#>   Soybeans        0    0      0        6          0\n#>   Sugarbeets      0    0      1        1          4\n\n## LOOCV\nqda_cv <- qda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nqda_table_cv <- table(truth = crops$crop, fitted = qda_cv$class)\nqda_table_cv\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          9    0      0        0          2\n#>   Corn            3    2      0        0          2\n#>   Cotton          3    0      2        0          1\n#>   Soybeans        3    0      0        2          1\n#>   Sugarbeets      3    0      1        1          1\n\n## Predict the test data\nqda_pred <- predict(qda_mod, newdata = crops_test)\n## Make a contingency table with truth and most likely class\ntable(truth = crops_test$crop, predict = qda_pred$class)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      1        0          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1"},{"path":"multivariate-methods.html","id":"knn-1","chapter":"22 Multivariate Methods","heading":"22.4.4.3 KNN","text":"knn uses design matrices features.","code":"\n## Design matrices\nX_train <- crops %>%\n    dplyr::select(-crop)\nX_test <- crops_test %>%\n    dplyr::select(-crop)\nY_train <- crops$crop\nY_test <- crops_test$crop\n\n## Nearest neighbors with 2 neighbors\nknn_2 <- knn(X_train, X_train, Y_train, k = 2)\ntable(truth = Y_train, fitted = knn_2)\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          7    0      2        1          1\n#>   Corn            0    7      0        0          0\n#>   Cotton          0    0      4        0          2\n#>   Soybeans        0    0      0        4          2\n#>   Sugarbeets      1    0      2        0          3\n\n## Accuracy\nmean(Y_train==knn_2)\n#> [1] 0.6944444\n\n## Performance on test data\nknn_2_test <- knn(X_train, X_test, Y_train, k = 2)\ntable(truth = Y_test, predict = knn_2_test)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      0        0          1\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1\n\n## Accuracy\nmean(Y_test==knn_2_test)\n#> [1] 0.8\n\n## Nearest neighbors with 3 neighbors\nknn_3 <- knn(X_train, X_train, Y_train, k = 3)\ntable(truth = Y_train, fitted = knn_3)\n#>             fitted\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          8    0      1        1          1\n#>   Corn            0    4      1        2          0\n#>   Cotton          1    1      3        0          1\n#>   Soybeans        0    1      1        4          0\n#>   Sugarbeets      0    0      0        2          4\n\n## Accuracy\nmean(Y_train==knn_3)\n#> [1] 0.6388889\n\n## Performance on test data\nknn_3_test <- knn(X_train, X_test, Y_train, k = 3)\ntable(truth = Y_test, predict = knn_3_test)\n#>             predict\n#> truth        Clover Corn Cotton Soybeans Sugarbeets\n#>   Clover          1    0      0        0          0\n#>   Corn            0    1      0        0          0\n#>   Cotton          0    0      1        0          0\n#>   Soybeans        0    0      0        1          0\n#>   Sugarbeets      0    0      0        0          1\n\n## Accuracy\nmean(Y_test==knn_3_test)\n#> [1] 1"},{"path":"multivariate-methods.html","id":"stepwise","chapter":"22 Multivariate Methods","heading":"22.4.4.4 Stepwise","text":"Stepwise discriminant analysis using stepclass function klaR package.Iris Data","code":"\nstep <- stepclass(\n    crop ~ y1 + y2 + y3 + y4,\n    data = crops,\n    method = \"qda\",\n    improvement = 0.15\n)\n#> correctness rate: 0.45;  in: \"y1\";  variables (1): y1 \n#> \n#>  hr.elapsed min.elapsed sec.elapsed \n#>        0.00        0.00        0.16\n\nstep$process\n#>    step var varname result.pm\n#> 0 start   0      --      0.00\n#> 1    in   1      y1      0.45\n\nstep$performance.measure\n#> [1] \"correctness rate\"\n\nlibrary(dplyr)\ndata('iris')\nset.seed(1)\nsamp <-\n    sample.int(nrow(iris), size = floor(0.70 * nrow(iris)), replace = F)\n\ntrain.iris <- iris[samp,] %>% mutate_if(is.numeric,scale)\ntest.iris <- iris[-samp,] %>% mutate_if(is.numeric,scale)\n\nlibrary(ggplot2)\niris.model <- lda(Species ~ ., data = train.iris)\n#pred\npred.lda <- predict(iris.model, test.iris)\ntable(truth = test.iris$Species, prediction = pred.lda$class)\n#>             prediction\n#> truth        setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         17         0\n#>   virginica       0          0        13\n\nplot(iris.model)\n\niris.model.qda <- qda(Species~.,data=train.iris)\n#pred\npred.qda <- predict(iris.model.qda,test.iris)\ntable(truth=test.iris$Species,prediction=pred.qda$class)\n#>             prediction\n#> truth        setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         16         1\n#>   virginica       0          0        13"},{"path":"multivariate-methods.html","id":"pca-with-discriminant-analysis","chapter":"22 Multivariate Methods","heading":"22.4.4.5 PCA with Discriminant Analysis","text":"can use PCA dimension reduction discriminant analysis","code":"\nzeros <- as.matrix(read.table(\"images/mnist0_train_b.txt\"))\nnines <- as.matrix(read.table(\"images/mnist9_train_b.txt\"))\ntrain <- rbind(zeros[1:1000, ], nines[1:1000, ])\ntrain <- train / 255 #divide by 255 per notes (so ranges from 0 to 1)\ntrain <- t(train) #each column is an observation\nimage(matrix(train[, 1], nrow = 28), main = 'Example image, unrotated')\n\n\ntest <- rbind(zeros[2501:3000, ], nines[2501:3000, ])\ntest <- test / 255\ntest <- t(test)\ny.train <- c(rep(0, 1000), rep(9, 1000))\ny.test <- c(rep(0, 500), rep(9, 500))\n\n\nlibrary(MASS)\npc <- prcomp(t(train))\ntrain.large <- data.frame(cbind(y.train, pc$x[, 1:10]))\nlarge <- lda(y.train ~ ., data = train.large)\n#the test data set needs to be constucted w/ the same 10 princomps\ntest.large <- data.frame(cbind(y.test, predict(pc, t(test))[, 1:10]))\npred.lda <- predict(large, test.large)\ntable(truth = test.large$y.test, prediction = pred.lda$class)\n#>      prediction\n#> truth   0   9\n#>     0 491   9\n#>     9   5 495\n\nlarge.qda <- qda(y.train~.,data=train.large)\n#prediction\npred.qda <- predict(large.qda,test.large)\ntable(truth=test.large$y.test,prediction=pred.qda$class)\n#>      prediction\n#> truth   0   9\n#>     0 493   7\n#>     9   3 497"},{"path":"quasi-experimental.html","id":"quasi-experimental","chapter":"23 Quasi-experimental","heading":"23 Quasi-experimental","text":"cases, means pre- post-intervention data.Great resources causal inference include Causal Inference Mixtape Recent Advances Micro, especially like read history causal inference field well (codes Stata, R, Python).Libraries R:EconometricsEconometricsCausal InferenceCausal InferenceIdentification strategy quasi-experiment (ways prove formal statistical test, can provide plausible argument evidence)exogenous variation comes (argument institutional knowledge)Exclusion restriction: Evidence variation exogenous shock outcome due factors\nstable unit treatment value assumption (SUTVA) states treatment unit \\(\\) affect outcome unit \\(\\) (.e., spillover control groups)\nstable unit treatment value assumption (SUTVA) states treatment unit \\(\\) affect outcome unit \\(\\) (.e., spillover control groups)quasi-experimental methods involve tradeoff power support exogeneity assumption (.e., discard variation data exogenous).Consequently, don’t usually look \\(R^2\\) (Ebbes, Papies, Van Heerde 2011). can even misleading use \\(R^2\\) basis model comparison.Clustering based design, expectations correlation (Abadie et al. 2023). small sample, use wild bootstrap procedure (Cameron, Gelbach, Miller 2008) correct downward bias (see (Cai et al. 2022)additional assumptions).Typical robustness check: recommended (Goldfarb, Tucker, Wang 2022)Different controls: show models without controls. Typically, want see change estimate interest. See (Altonji, Elder, Taber 2005) formal assessment based Rosenbaum bounds (.e., changes estimate threat Omitted variables estimate). specific applications marketing, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012)Different controls: show models without controls. Typically, want see change estimate interest. See (Altonji, Elder, Taber 2005) formal assessment based Rosenbaum bounds (.e., changes estimate threat Omitted variables estimate). specific applications marketing, see (Manchanda, Packard, Pattabhiramaiah 2015) (Shin, Sudhir, Yoon 2012)Different functional formsDifferent functional formsDifferent window time (longitudinal setting)Different window time (longitudinal setting)Different dependent variables (related) different measures dependent variablesDifferent dependent variables (related) different measures dependent variablesDifferent control group size (matched vs. un-matched samples)Different control group size (matched vs. un-matched samples)Placebo tests: see placebo test setting .Placebo tests: see placebo test setting .Showing mechanism:Mediation analysisMediation analysisModeration analysis\nEstimate model separately (different groups)\nAssess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.\nModeration analysisEstimate model separately (different groups)Estimate model separately (different groups)Assess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.Assess whether three-way interaction source variation (e.g., , cross-sectional time series) group membership significant.External Validity:Assess representative sample isAssess representative sample isExplain limitation design.Explain limitation design.Use quasi-experimental results conjunction structural models: see (J. E. Anderson, Larch, Yotov 2015; Einav, Finkelstein, Levin 2010; Chung, Steenburgh, Sudhir 2014)Use quasi-experimental results conjunction structural models: see (J. E. Anderson, Larch, Yotov 2015; Einav, Finkelstein, Levin 2010; Chung, Steenburgh, Sudhir 2014)LimitationWhat identifying assumptions identification strategyWhat threats validity assumptions?address ? maybe future research can address .","code":""},{"path":"quasi-experimental.html","id":"natural-experiments","chapter":"23 Quasi-experimental","heading":"23.1 Natural Experiments","text":"Reusing natural experiments research, particularly employing identical methods determine treatment effect given setting, can pose problems hypothesis testing.Simulations show \\(N_{\\text{Outcome}} >> N_{\\text{True effect}}\\), 50% statistically significant findings may false positives (Heath et al. 2023, 2331).Solutions:Bonferroni correctionBonferroni correctionRomano Wolf (2005) Romano Wolf (2016) correction: recommendedRomano Wolf (2005) Romano Wolf (2016) correction: recommendedBenjamini Yekutieli (2001) correctionBenjamini Yekutieli (2001) correctionAlternatively, refer rules thumb Table AI (Heath et al. 2023, 2356).Alternatively, refer rules thumb Table AI (Heath et al. 2023, 2356).applying multiple testing corrections, can either use (give similar results anyway (Heath et al. 2023, 2335)):Chronological Sequencing: Outcomes ordered date first reported, multiple testing corrections applied sequence. method progressively raises statistical significance threshold outcomes reviewed time.Chronological Sequencing: Outcomes ordered date first reported, multiple testing corrections applied sequence. method progressively raises statistical significance threshold outcomes reviewed time.Best Foot Forward Policy: Outcomes ordered least likely rejected based experimental data. Used primarily clinical trials, approach gives priority intended treatment effects, subjected less stringent statistical requirements. New outcomes added sequence linked primary treatment effect.Best Foot Forward Policy: Outcomes ordered least likely rejected based experimental data. Used primarily clinical trials, approach gives priority intended treatment effects, subjected less stringent statistical requirements. New outcomes added sequence linked primary treatment effect.tests, one can use multtest::mt.rawp2adjp includes:BonferroniHolm (1979)Šidák (1967)Hochberg (1988)Benjamini Hochberg (1995)Benjamini Yekutieli (2001)Adaptive Benjamini Hochberg (2000)Two-stage Benjamini, Krieger, Yekutieli (2006)Permutation adjusted p-values simple multiple testing procedures","code":"# Romano-Wolf correction\nlibrary(fixest)\nlibrary(wildrwolf)\n\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\n\nfit1 <- feols(Sepal.Width ~ Sepal.Length , data = iris)\nfit2 <- feols(Petal.Length ~ Sepal.Length, data = iris)\nfit3 <- feols(Petal.Width ~ Sepal.Length, data = iris)\n\nres <- rwolf(\n  models = list(fit1, fit2, fit3), \n  param = \"Sepal.Length\",  \n  B = 500\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\nres\n#>   model   Estimate Std. Error   t value     Pr(>|t|) RW Pr(>|t|)\n#> 1     1 -0.0618848 0.04296699 -1.440287    0.1518983 0.139720559\n#> 2     2   1.858433 0.08585565  21.64602 1.038667e-47 0.001996008\n#> 3     3  0.7529176 0.04353017  17.29645 2.325498e-37 0.001996008\n# BiocManager::install(\"multtest\")\nlibrary(multtest)\n\nprocs <-\n    c(\"Bonferroni\",\n      \"Holm\",\n      \"Hochberg\",\n      \"SidakSS\",\n      \"SidakSD\",\n      \"BH\",\n      \"BY\",\n      \"ABH\",\n      \"TSBH\")\n\nmt.rawp2adjp(\n    # p-values\n    runif(10),\n    procs) |> causalverse::nice_tab()\n#>    adjp.rawp adjp.Bonferroni adjp.Holm adjp.Hochberg adjp.SidakSS adjp.SidakSD\n#> 1       0.12               1         1          0.75         0.72         0.72\n#> 2       0.22               1         1          0.75         0.92         0.89\n#> 3       0.24               1         1          0.75         0.94         0.89\n#> 4       0.29               1         1          0.75         0.97         0.91\n#> 5       0.36               1         1          0.75         0.99         0.93\n#> 6       0.38               1         1          0.75         0.99         0.93\n#> 7       0.44               1         1          0.75         1.00         0.93\n#> 8       0.59               1         1          0.75         1.00         0.93\n#> 9       0.65               1         1          0.75         1.00         0.93\n#> 10      0.75               1         1          0.75         1.00         0.93\n#>    adjp.BH adjp.BY adjp.ABH adjp.TSBH_0.05 index h0.ABH h0.TSBH\n#> 1     0.63       1     0.63           0.63     2     10      10\n#> 2     0.63       1     0.63           0.63     6     10      10\n#> 3     0.63       1     0.63           0.63     8     10      10\n#> 4     0.63       1     0.63           0.63     3     10      10\n#> 5     0.63       1     0.63           0.63    10     10      10\n#> 6     0.63       1     0.63           0.63     1     10      10\n#> 7     0.63       1     0.63           0.63     7     10      10\n#> 8     0.72       1     0.72           0.72     9     10      10\n#> 9     0.72       1     0.72           0.72     5     10      10\n#> 10    0.75       1     0.75           0.75     4     10      10"},{"path":"regression-discontinuity.html","id":"regression-discontinuity","chapter":"24 Regression Discontinuity","heading":"24 Regression Discontinuity","text":"regression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).\nRunning variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .\nregression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).Running variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .Review paper (G. Imbens Lemieux 2008; Lee Lemieux 2010)Review paper (G. Imbens Lemieux 2008; Lee Lemieux 2010)readings:\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf\nreadings:https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdfhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf(Thistlethwaite Campbell 1960): first paper use RD context merit awards future academic outcomes.(Thistlethwaite Campbell 1960): first paper use RD context merit awards future academic outcomes.RD localized experiment cutoff point\nHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”\nRD localized experiment cutoff pointHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018); Mathematica). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018); Mathematica). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Tradeoff efficiency biasTradeoff efficiency biasRegression discontinuity framework Instrumental Variable (structural IV) argued (J. D. Angrist Lavy 1999) special case Matching Methods (matching one point) argued (James J. Heckman, LaLonde, Smith 1999).Regression discontinuity framework Instrumental Variable (structural IV) argued (J. D. Angrist Lavy 1999) special case Matching Methods (matching one point) argued (James J. Heckman, LaLonde, Smith 1999).hard part find setting can apply, find one, ’s easy applyThe hard part find setting can apply, find one, ’s easy applyWe can also multiple cutoff lines. However, cutoff line, can one breakup pointWe can also multiple cutoff lines. However, cutoff line, can one breakup pointRD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.RD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better Interrupted Time SeriesAs running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better Interrupted Time SeriesMultiple alternative model specifications produce consistent results reliable (parametric - linear regression polynomials terms, non-parametric - local linear regression). according (Lee Lemieux 2010), one straightforward method ease linearity assumption incorporating polynomial functions forcing variable. choice polynomial terms can determined based data.\n. According (Gelman Imbens 2019), accounting global high-order polynomials presents three issues: (1) imprecise estimates due noise, (2) sensitivity polynomial’s degree, (3) inadequate coverage confidence intervals. address , researchers instead employ estimators rely local linear quadratic polynomials smooth functions.\nMultiple alternative model specifications produce consistent results reliable (parametric - linear regression polynomials terms, non-parametric - local linear regression). according (Lee Lemieux 2010), one straightforward method ease linearity assumption incorporating polynomial functions forcing variable. choice polynomial terms can determined based data.. According (Gelman Imbens 2019), accounting global high-order polynomials presents three issues: (1) imprecise estimates due noise, (2) sensitivity polynomial’s degree, (3) inadequate coverage confidence intervals. address , researchers instead employ estimators rely local linear quadratic polynomials smooth functions.RD viewed description data generating process, rather method approach (similar randomized experiment)RD viewed description data generating process, rather method approach (similar randomized experiment)RD close \nquasi-experimental methods sense ’s based discontinuity threshold\nrandomized experiments sense ’s local randomization.\nRD close toother quasi-experimental methods sense ’s based discontinuity thresholdother quasi-experimental methods sense ’s based discontinuity thresholdrandomized experiments sense ’s local randomization.randomized experiments sense ’s local randomization.several types Regression Discontinuity:Sharp RD: Change treatment probability cutoff point 1\nKink design: Instead discontinuity level running variable, discontinuity slope function (function/level can remain continuous) (Nielsen, Sørensen, Taber 2010). See (Böckerman, Kanninen, Suoniemi 2018) application, (Card et al. 2015) theory.\nSharp RD: Change treatment probability cutoff point 1Kink design: Instead discontinuity level running variable, discontinuity slope function (function/level can remain continuous) (Nielsen, Sørensen, Taber 2010). See (Böckerman, Kanninen, Suoniemi 2018) application, (Card et al. 2015) theory.Kink RDKink RDFuzzy RD: Change treatment probability less 1Fuzzy RD: Change treatment probability less 1Fuzzy Kink RDFuzzy Kink RDRDiT: running variable time.RDiT: running variable time.Others:Multiple cutoffMultiple cutoffMultiple ScoresMultiple ScoresGeographic RDGeographic RDDynamic TreatmentsDynamic TreatmentsContinuous TreatmentsContinuous TreatmentsConsider\\[\nD_i = 1_{X_i > c}\n\\]\\[\nD_i =\n\\begin{cases}\nD_i = 1 \\text{ } X_i > C \\\\\nD_i = 0 \\text{ } X_i < C\n\\end{cases}\n\\]\\(D_i\\) = treatment effect\\(D_i\\) = treatment effect\\(X_i\\) = score variable (continuous)\\(X_i\\) = score variable (continuous)\\(c\\) = cutoff point\\(c\\) = cutoff pointIdentification (Identifying assumptions) RD:Average Treatment Effect cutoff (Continuity-based)\\[\n\\begin{aligned}\n\\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\\\\n&= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\\\\n&= \\lim_{x \\c^+} E[Y_{1i}|X_i = c] - \\lim_{x \\c^=} E[Y_{0i}|X_i = c]\n\\end{aligned}\n\\]Average Treatment Effect neighborhood (Local Randomization-based):\\[\n\\begin{aligned}\n\\alpha_{LR} &= E[Y_{1i} - Y_{0i}|X_i \\W] \\\\\n&= \\frac{1}{N_1} \\sum_{X_i \\W, T_i = 1}Y_i - \\frac{1}{N_0}\\sum_{X_i \\W, T_i =0} Y_i\n\\end{aligned}\n\\]RDD estimates local average treatment effect (LATE), cutoff point individual population levels.Since researchers typically care internal validity, external validity, localness affects external validity.Assumptions:Independent assignmentIndependent assignmentContinuity conditional regression functions\n\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.\nContinuity conditional regression functions\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.RD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableRD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableOnly treatment(s) (e.g., joint distribution multiple treatments) cause discontinuity jump outcome variableOnly treatment(s) (e.g., joint distribution multiple treatments) cause discontinuity jump outcome variableAll factors smooth cutoff (.e., threshold) value. (can also test assumption seeing discontinuity factors). “jump”, bias causal estimateAll factors smooth cutoff (.e., threshold) value. (can also test assumption seeing discontinuity factors). “jump”, bias causal estimateThreats RDVariables (treatment) change discontinuously cutoff\ncan test jumps variables (including pre-treatment outcome)\nVariables (treatment) change discontinuously cutoffWe can test jumps variables (including pre-treatment outcome)Multiple discontinuities assignment variableMultiple discontinuities assignment variableManipulation assignment variable\ncutoff point, check continuity density assignment variable.\nManipulation assignment variableAt cutoff point, check continuity density assignment variable.","code":""},{"path":"regression-discontinuity.html","id":"estimation-and-inference","chapter":"24 Regression Discontinuity","heading":"24.1 Estimation and Inference","text":"","code":""},{"path":"regression-discontinuity.html","id":"local-randomization-based","chapter":"24 Regression Discontinuity","heading":"24.1.1 Local Randomization-based","text":"Additional Assumption: Local Randomization approach assumes inside chosen window \\(W = [c-w, c+w]\\) assigned treatment good random:Joint probability distribution scores units inside chosen window \\(W\\) knownPotential outcomes affected value scoreThis approach stronger Continuity-based assume regressions continuously \\(c\\) unaffected running variable within window \\(W\\)can choose window \\(W\\) (within random assignment plausible), sample size can typically small.choose window \\(W\\), can base eitherwhere pre-treatment covariate-balance observedindependent tests outcome scoredomain knowledgeTo make inference, can either use(Fisher) randomization inference(Fisher) randomization inference(Neyman) design-based(Neyman) design-based","code":""},{"path":"regression-discontinuity.html","id":"continuity-based","chapter":"24 Regression Discontinuity","heading":"24.1.2 Continuity-based","text":"also known local polynomial method\nname suggests, global polynomial regression recommended (lack robustness, -fitting Runge’s phenomenon)\nalso known local polynomial methodas name suggests, global polynomial regression recommended (lack robustness, -fitting Runge’s phenomenon)Step estimate local polynomial regressionChoose polynomial order weighting schemeChoose bandwidth optimal MSE coverage errorEstimate parameter interestExamine robust bias-correct inference","code":""},{"path":"regression-discontinuity.html","id":"specification-checks","chapter":"24 Regression Discontinuity","heading":"24.2 Specification Checks","text":"Balance ChecksSorting/Bunching/ManipulationPlacebo TestsSensitivity Bandwidth Choice","code":""},{"path":"regression-discontinuity.html","id":"balance-checks","chapter":"24 Regression Discontinuity","heading":"24.2.1 Balance Checks","text":"Also known checking Discontinuities Average CovariatesAlso known checking Discontinuities Average CovariatesNull Hypothesis: average effect covariates pseudo outcomes (.e., qualitatively affected treatment) 0.Null Hypothesis: average effect covariates pseudo outcomes (.e., qualitatively affected treatment) 0.hypothesis rejected, better good reason can cast serious doubt RD design.hypothesis rejected, better good reason can cast serious doubt RD design.","code":""},{"path":"regression-discontinuity.html","id":"sortingbunchingmanipulation","chapter":"24 Regression Discontinuity","heading":"24.2.2 Sorting/Bunching/Manipulation","text":"Also known checking Discontinuity Distribution Forcing VariableAlso known checking Discontinuity Distribution Forcing VariableAlso known clustering density testAlso known clustering density testFormal test McCrary sorting test (McCrary 2008) (Cattaneo, Idrobo, Titiunik 2019)Formal test McCrary sorting test (McCrary 2008) (Cattaneo, Idrobo, Titiunik 2019)Since human subjects can manipulate running variable just cutoff (assuming running variable manipulable), especially cutoff point known advance subjects, can result discontinuity distribution running variable cutoff (.e., see “bunching” behavior right cutoff)>\nPeople like sort treatment ’s desirable. density running variable 0 just threshold\nPeople like treatment ’s undesirable\nSince human subjects can manipulate running variable just cutoff (assuming running variable manipulable), especially cutoff point known advance subjects, can result discontinuity distribution running variable cutoff (.e., see “bunching” behavior right cutoff)>People like sort treatment ’s desirable. density running variable 0 just thresholdPeople like sort treatment ’s desirable. density running variable 0 just thresholdPeople like treatment ’s undesirablePeople like treatment ’s undesirable(McCrary 2008) proposes density test (.e., formal test manipulation assignment variable).\n\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\n\\(H_a\\): jump density function point\nEven though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.\n(McCrary 2008) proposes density test (.e., formal test manipulation assignment variable).\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\\(H_0\\): continuity density running variable (.e., covariate underlies assignment discontinuity point)\\(H_a\\): jump density function point\\(H_a\\): jump density function pointEven though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.Even though ’s requirement density running must continuous cutoff, discontinuity can suggest manipulations.(J. L. Zhang Rubin 2003; Lee 2009; Aronow, Baron, Pinson 2019) offers guide know warrant manipulation(J. L. Zhang Rubin 2003; Lee 2009; Aronow, Baron, Pinson 2019) offers guide know warrant manipulationUsually ’s better know research design inside can suspect manipulation attempts.\nsuspect direction manipulation. typically, ’s one-way manipulation. cases might ways, theoretically cancel .\nUsually ’s better know research design inside can suspect manipulation attempts.suspect direction manipulation. typically, ’s one-way manipulation. cases might ways, theoretically cancel .also observe partial manipulation reality (e.g., subjects can imperfectly manipulate). typically, treat like fuzzy RD, identification problems. complete manipulation lead serious identification issues.also observe partial manipulation reality (e.g., subjects can imperfectly manipulate). typically, treat like fuzzy RD, identification problems. complete manipulation lead serious identification issues.Remember: even cases fail reject null hypothesis density test, rule completely identification problem exists (just like hypotheses)Remember: even cases fail reject null hypothesis density test, rule completely identification problem exists (just like hypotheses)Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Review paper (Kleven 2016)Review paper (Kleven 2016)test can detect manipulation changes distribution running variable. can choose cutoff point 2-sided manipulation, test fail detect .test can detect manipulation changes distribution running variable. can choose cutoff point 2-sided manipulation, test fail detect .Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)can also use bunching method study individuals’ firm’s responsiveness changes policy.can also use bunching method study individuals’ firm’s responsiveness changes policy.RD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.\nBunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).\nfraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.\nRD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)\nRD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)running variable outcome variable simultaneously determined, can use modified RDD estimator consistent estimate. (Bajari et al. 2011)Assumptions:\nManipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1\nManipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)\nAssumptions:Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 33.1.3.1Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, Seegert 2021)Steps:Identify window running variable contains bunching behavior. can step empirically based Bosch, Dekker, Strohmaier (2020). Additionally robustness test needed (.e., varying manipulation window).Estimate manipulation-free counterfactualCalculating standard errors inference can follow (Chetty, Hendren, Katz 2016) bootstrap re-sampling residuals estimation counts individuals within bins (large data can render step unnecessary).pass bunching test, can move Placebo TestMcCrary (2008) testA jump density threshold (.e., discontinuity) hold can serve evidence sorting around cutoff pointCattaneo, Idrobo, Titiunik (2019) test","code":"\nlibrary(rdd)\n\n# you only need the runing variable and the cutoff point\n\n# Example by the package's authors\n#No discontinuity\nx<-runif(1000,-1,1)\nDCdensity(x,0)#> [1] 0.6126802\n\n#Discontinuity\nx<-runif(1000,-1,1)\nx<-x+2*(runif(1000,-1,1)>0&x<0)\nDCdensity(x,0)#> [1] 0.0008519227\nlibrary(rddensity)\n\n# Example by the package's authors\n# Continuous Density\nset.seed(1)\nx <- rnorm(2000, mean = -0.5)\nrdd <- rddensity(X = x, vce = \"jackknife\")\nsummary(rdd)\n#> \n#> Manipulation testing using local polynomial density estimation.\n#> \n#> Number of obs =       2000\n#> Model =               unrestricted\n#> Kernel =              triangular\n#> BW method =           estimated\n#> VCE method =          jackknife\n#> \n#> c = 0                 Left of c           Right of c          \n#> Number of obs         1376                624                 \n#> Eff. Number of obs    354                 345                 \n#> Order est. (p)        2                   2                   \n#> Order bias (q)        3                   3                   \n#> BW est. (h)           0.514               0.609               \n#> \n#> Method                T                   P > |T|             \n#> Robust                -0.6798             0.4966              \n#> \n#> \n#> P-values of binomial tests (H0: p=0.5).\n#> \n#> Window Length / 2          <c     >=c    P>|T|\n#> 0.036                      28      20    0.3123\n#> 0.072                      46      39    0.5154\n#> 0.107                      68      59    0.4779\n#> 0.143                      94      79    0.2871\n#> 0.179                     122     103    0.2301\n#> 0.215                     145     130    0.3986\n#> 0.250                     163     156    0.7370\n#> 0.286                     190     176    0.4969\n#> 0.322                     214     200    0.5229\n#> 0.358                     249     218    0.1650\n\n# you have to specify your own plot (read package manual)"},{"path":"regression-discontinuity.html","id":"placebo-tests","chapter":"24 Regression Discontinuity","heading":"24.2.3 Placebo Tests","text":"Also known Discontinuities Average Outcomes ValuesAlso known Discontinuities Average Outcomes ValuesWe see jumps values (either \\(X_i <c\\) \\(X_i \\ge c\\))\nUse bandwidth use cutoff, move along running variable: testing jump conditional mean outcome median running variable.\nsee jumps values (either \\(X_i <c\\) \\(X_i \\ge c\\))Use bandwidth use cutoff, move along running variable: testing jump conditional mean outcome median running variable.Also known falsification checksAlso known falsification checksBefore cutoff point, can run placebo test see whether X’s different).cutoff point, can run placebo test see whether X’s different).placebo test expect coefficients different 0.placebo test expect coefficients different 0.test can used \nTesting discontinuity predetermined variables:\nTesting discontinuities\nPlacebo outcomes: see changes outcomes shouldn’t changed.\nInclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.\ntest can used forTesting discontinuity predetermined variables:Testing discontinuity predetermined variables:Testing discontinuitiesTesting discontinuitiesPlacebo outcomes: see changes outcomes shouldn’t changed.Placebo outcomes: see changes outcomes shouldn’t changed.Inclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.Inclusion exclusion covariates: RDD parameter estimates sensitive inclusion exclusion covariates.analogous Experimental Design test whether observables similar treatment control groups (reject , don’t random assignment), test unobservables.analogous Experimental Design test whether observables similar treatment control groups (reject , don’t random assignment), test unobservables.Balance observable characteristics sides\\[\nZ_i = \\alpha_0 + \\alpha_1 f(x_i) + [(x_i \\ge c)] \\alpha_2 + [f(x_i) \\times (x_i \\ge c)]\\alpha_3 + u_i\n\\]\\(x_i\\) running variable\\(x_i\\) running variable\\(Z_i\\) characteristics people (e.g., age, etc)\\(Z_i\\) characteristics people (e.g., age, etc)Theoretically, \\(Z_i\\) affected treatment. Hence, \\(E(\\alpha_2) = 0\\)Moreover, multiple \\(Z_i\\), typically simulate joint distribution (avoid significant coefficient based chance).way don’t need generate joint distribution \\(Z_i\\)’s independent (unlikely reality).RD, shouldn’t Matching Methods. just like random assignment, need make balanced dataset cutoff. balancing, RD assumptions probably wrong first place.","code":""},{"path":"regression-discontinuity.html","id":"sensitivity-to-bandwidth-choice","chapter":"24 Regression Discontinuity","heading":"24.2.4 Sensitivity to Bandwidth Choice","text":"Methods bandwidth selection\nAd-hoc substantively driven\nData driven: cross validation\nConservative approach: (Calonico, Cattaneo, Farrell 2020)\nMethods bandwidth selectionAd-hoc substantively drivenAd-hoc substantively drivenData driven: cross validationData driven: cross validationConservative approach: (Calonico, Cattaneo, Farrell 2020)Conservative approach: (Calonico, Cattaneo, Farrell 2020)objective minimize mean squared error estimated actual treatment effects.objective minimize mean squared error estimated actual treatment effects., need see sensitive results dependent choice bandwidth., need see sensitive results dependent choice bandwidth.cases, best bandwidth testing covariates may best bandwidth treating , may close.cases, best bandwidth testing covariates may best bandwidth treating , may close.","code":"\n# find optimal bandwidth by Imbens-Kalyanaraman\nrdd::IKbandwidth(running_var,\n                 outcome_var,\n                 cutpoint = \"\",\n                 kernel = \"triangular\") # can also pick other kernels"},{"path":"regression-discontinuity.html","id":"manipulation-robust-regression-discontinuity-bounds","chapter":"24 Regression Discontinuity","heading":"24.2.5 Manipulation Robust Regression Discontinuity Bounds","text":"McCrary (2008) linked density jumps cutoffs RD studies potential manipulation.\njump detected, researchers proceed RD analysis; detected, halt using cutoff inference.\nstudies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.\nFalse negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.\nEven correct rejections manipulation may overlook data can still informative despite modest manipulation.\nGerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.\n\nMcCrary (2008) linked density jumps cutoffs RD studies potential manipulation.jump detected, researchers proceed RD analysis; detected, halt using cutoff inference.jump detected, researchers proceed RD analysis; detected, halt using cutoff inference.studies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.\nFalse negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.\nEven correct rejections manipulation may overlook data can still informative despite modest manipulation.\nGerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.\nstudies use “doughnut-hole” method, excluding near-cutoff observations extrapolating, contradicts RD principles.False negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.False negative due small sample size can lead biased estimates, units near cutoff may still differ unobserved ways.Even correct rejections manipulation may overlook data can still informative despite modest manipulation.Even correct rejections manipulation may overlook data can still informative despite modest manipulation.Gerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.Gerard, Rokkanen, Rothe (2020) introduces systematic approach handle potentially manipulated variables RD designs, addressing concerns.model introduces two types unobservable units RD designs:\nalways-assigned units, always one side cutoff,\npotentially-assigned units, fit traditional RD assumptions.\nstandard RD model subset broader model, assumes always-assigned units.\n\nmodel introduces two types unobservable units RD designs:always-assigned units, always one side cutoff,always-assigned units, always one side cutoff,potentially-assigned units, fit traditional RD assumptions.\nstandard RD model subset broader model, assumes always-assigned units.\npotentially-assigned units, fit traditional RD assumptions.standard RD model subset broader model, assumes always-assigned units.Identifying assumption: manipulation occurs one-sided selection.Identifying assumption: manipulation occurs one-sided selection.approach make binary decision manipulation RD designs assesses extent worst-case impact.approach make binary decision manipulation RD designs assesses extent worst-case impact.Two steps used:Determining proportion always-assigned units using discontinuity cutoffBounding treatment effects based extreme feasible outcomes units.sharp RD designs, bounds established trimming extreme outcomes near cutoff; fuzzy designs, process involves complex adjustments due additional model constraints.sharp RD designs, bounds established trimming extreme outcomes near cutoff; fuzzy designs, process involves complex adjustments due additional model constraints.Extensions study use covariate information economic behavior assumptions refine bounds identify covariate distributions among unit types cutoff.Extensions study use covariate information economic behavior assumptions refine bounds identify covariate distributions among unit types cutoff.SetupIndependent data points \\((X_i, Y_i, D_i)\\), \\(X_i\\) running variable, \\(Y_i\\) outcome, \\(D_i\\) indicates treatment status (1 treated, 0 otherwise). Treatment assigned based \\(X_i \\geq c\\).design sharp \\(D_i = (X_i \\geq c)\\) fuzzy otherwise.population divided :Potentially-assigned units (\\(M_i = 0\\)): Follow standard RD framework, potential outcomes \\(Y_i(d)\\) potential treatment states \\(D_i(x)\\).Potentially-assigned units (\\(M_i = 0\\)): Follow standard RD framework, potential outcomes \\(Y_i(d)\\) potential treatment states \\(D_i(x)\\).Always-assigned units (\\(M_i = 1\\)): units require potential outcomes states, always \\(X_i\\) values beyond cutoff.Always-assigned units (\\(M_i = 1\\)): units require potential outcomes states, always \\(X_i\\) values beyond cutoff.AssumptionsLocal Independence Continuity:\n\\(P(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0)\\)\ndefiers: \\(P(D^+ \\geq D^-|X = c, M = 0) = 1\\)\nContinuity potential outcomes states \\(c\\).\n\\(F_{X|M=0}(x)\\) differentiable \\(c\\), positive derivative.\n\\(P(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0)\\)defiers: \\(P(D^+ \\geq D^-|X = c, M = 0) = 1\\)Continuity potential outcomes states \\(c\\).\\(F_{X|M=0}(x)\\) differentiable \\(c\\), positive derivative.Smoothness Running Variable among Potentially-Assigned Units:\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).\nderivative \\(F_{X|M=0}(x)\\) continuous \\(c\\).Restrictions Always-Assigned Units:\n\\(P(X \\geq c|M = 1) = 1\\) \\(F_{X|M=1}(x)\\) right-differentiable (left-differentiable) \\(c\\).\n(local) one-sided manipulation assumption allows identification proportion always-assigned units among units close cutoff.\n\\(P(X \\geq c|M = 1) = 1\\) \\(F_{X|M=1}(x)\\) right-differentiable (left-differentiable) \\(c\\).(local) one-sided manipulation assumption allows identification proportion always-assigned units among units close cutoff.always-assigned unit exist, RD design fuzzy haveTreated untreated units among potentially-assigned (cutoff)Always-assigned units (cutoff).Causal Effects Interestcausal effects among potentially-assigned units:\\[\n\\Gamma = E[Y(1) - Y(0) | X = c, D^+ > D^-, M = 0]\n\\]parameter represents local average treatment effect (LATE) subgroup “compliers”—units receive treatment running variable \\(X_i\\) exceeds certain cutoff.parameter \\(\\Gamma\\) captures causal effect changes cutoff level treatment status among potentially-assigned compliers.Focuses actual observations cutoff, hypothetical true values.Provides direct observable estimate causal effects, without reliance hypothetical constructs.Exclude observations around cutoff use extrapolation trends outside excluded range infer causal effects cutoffAssumes hypothetical population existing counterfactual scenario without manipulation.Requires strong assumptions nature manipulation minimal impact extrapolation biases.Identification \\(\\tau\\) RD DesignsIdentification challenges arise due inability distinguish always-assigned potentially-assigned units, thus Γ point identified. establish sharp bounds ΓIdentification challenges arise due inability distinguish always-assigned potentially-assigned units, thus Γ point identified. establish sharp bounds ΓThese bounds supported stochastic dominance potential outcome CDFs observed distributions.bounds supported stochastic dominance potential outcome CDFs observed distributions.Unit Types Notation:\\(C_0\\): Potentially-assigned compliers.\\(A_0\\): Potentially-assigned always-takers.\\(N_0\\): Potentially-assigned never-takers.\\(T_1\\): Always-assigned treated units.\\(U_1\\): Always-assigned untreated units.measure \\(\\tau\\) , representing proportion always-assigned units near cutoff, point identified discontinuity observed running variable density \\(f_X\\) cutoffSharp RD:Units left cutoff potentially assigned units. distribution observed outcomes (\\(Y\\)) outcomes \\(Y(0)\\) potentially-assigned compliers (\\(C_0\\)) cutoff.Units left cutoff potentially assigned units. distribution observed outcomes (\\(Y\\)) outcomes \\(Y(0)\\) potentially-assigned compliers (\\(C_0\\)) cutoff.determine bounds treatment effect (\\(\\Gamma\\)), need assess distribution treated outcomes (\\(Y(1)\\)) potentially-assigned compliers cutoff.determine bounds treatment effect (\\(\\Gamma\\)), need assess distribution treated outcomes (\\(Y(1)\\)) potentially-assigned compliers cutoff.Information regarding treated outcomes (\\(Y(1)\\)) comes exclusively subpopulation treated units, includes potentially-assigned compliers (\\(C_0\\)) always assigned units (\\(T_1\\)).Information regarding treated outcomes (\\(Y(1)\\)) comes exclusively subpopulation treated units, includes potentially-assigned compliers (\\(C_0\\)) always assigned units (\\(T_1\\)).\\(\\tau\\) point identified, can estimate sharp bounds \\(\\Gamma\\).\\(\\tau\\) point identified, can estimate sharp bounds \\(\\Gamma\\).Fuzzy RD:Note: Table page 848 (Gerard, Rokkanen, Rothe 2020)Unit Types Combinations: five distinct unit types four combinations treatment assignments decisions relevant analysis. distinctions important affect potential outcomes analyzed bounded.Unit Types Combinations: five distinct unit types four combinations treatment assignments decisions relevant analysis. distinctions important affect potential outcomes analyzed bounded.Outcome Distributions: analysis involves estimating distribution potential outcomes (treated untreated) among potentially-assigned compliers cutoff.Outcome Distributions: analysis involves estimating distribution potential outcomes (treated untreated) among potentially-assigned compliers cutoff.Three-Step Process:\nPotential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.\nPotential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.\nBounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.\nThree-Step Process:Potential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.Potential Outcomes Treatment: Bounds distribution treated outcomes determined using data treated units.Potential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.Potential Outcomes Non-Treatment: Bounds distribution untreated outcomes derived using data untreated units.Bounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.Bounds Parameters Interest: Using bounds first two steps, sharp upper lower bounds local average treatment effect derived.Extreme Value Consideration: bounds treatment effects based “extreme” scenarios worst-case assumptions distribution potential outcomes, making sharp empirically relevant within data constraints.Extreme Value Consideration: bounds treatment effects based “extreme” scenarios worst-case assumptions distribution potential outcomes, making sharp empirically relevant within data constraints.Extensions:Quantile Treatment Effects: alternative average effects focusing different quantiles outcome distribution, less affected extreme values.Quantile Treatment Effects: alternative average effects focusing different quantiles outcome distribution, less affected extreme values.Applicability Discrete OutcomesApplicability Discrete OutcomesBehavioral Assumptions Impact: Assuming high likelihood treatment among always-assigned units can narrow bounds treatment effects refining analysis potential outcomes.Behavioral Assumptions Impact: Assuming high likelihood treatment among always-assigned units can narrow bounds treatment effects refining analysis potential outcomes.Utilization Covariates: Incorporating covariates measured prior treatment can refine bounds treatment effects help target policies identifying covariate distributions among different unit types.Utilization Covariates: Incorporating covariates measured prior treatment can refine bounds treatment effects help target policies identifying covariate distributions among different unit types.Notes:Quantile Treatment Effects (QTEs): QTE bounds less sensitive tails outcome distribution, making tighter ATE bounds.\nInference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.\nInference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.\nQuantile Treatment Effects (QTEs): QTE bounds less sensitive tails outcome distribution, making tighter ATE bounds.Inference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.Inference ATEs sensitive extent manipulation, confidence intervals widening significantly small degrees assumed manipulation.Inference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.Inference QTEs less affected manipulation, remaining meaningful even larger degrees manipulation.Alternative Inference Strategy manipulation believed unlikely. Try different hypothetical values \\(\\tau\\)Alternative Inference Strategy manipulation believed unlikely. Try different hypothetical values \\(\\tau\\)","code":"\ndevtools::install_github(\"francoisgerard/rdbounds/R\")\nlibrary(formattable)\nlibrary(data.table)\nlibrary(rdbounds)\nset.seed(123)\ndf <- rdbounds_sampledata(10000, covs = FALSE)\n#> [1] \"True tau: 0.117999815082062\"\n#> [1] \"True treatment effect on potentially-assigned: 2\"\n#> [1] \"True treatment effect on right side of cutoff: 2.35399944524618\"\nhead(df)\n#>            x        y treatment\n#> 1 -1.2532616 3.489563         0\n#> 2 -0.5146925 3.365232         0\n#> 3  3.4853777 6.193533         0\n#> 4  0.1576616 8.820440         1\n#> 5  0.2890962 4.791972         0\n#> 6  3.8350019 7.316907         0\n\nrdbounds_est <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        \n        # for median effect use \n        # type = \"qte\", \n        # percentiles = .5, \n        \n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209\"\n#> [1] \"2024-05-13 19:12:33 Estimating CDFs for point estimates\"\n#> [1] \"2024-05-13 19:12:33 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:35 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2024-05-13 19:12:35 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:38 Beginning parallelized output by bootstrap..\"\n#> [1] \"2024-05-13 19:12:42 Computing Confidence Intervals\"\n#> [1] \"2024-05-13 19:12:51 Time taken:0.3 minutes\"\nrdbounds_summary(rdbounds_est, title_prefix = \"Sample Data Results\")\n#> [1] \"Time taken: 0.3 minutes\"\n#> [1] \"Sample size: 10000\"\n#> [1] \"Local Average Treatment Effect:\"\n#> $tau_hat\n#> [1] 0.04209028\n#> \n#> $tau_hat_CI\n#> [1] 0.1671043 0.7765031\n#> \n#> $takeup_increase\n#> [1] 0.7521208\n#> \n#> $takeup_increase_CI\n#> [1] 0.7065353 0.7977063\n#> \n#> $TE_SRD_naive\n#> [1] 1.770963\n#> \n#> $TE_SRD_naive_CI\n#> [1] 1.541314 2.000612\n#> \n#> $TE_SRD_bounds\n#> [1] 1.569194 1.912681\n#> \n#> $TE_SRD_CI\n#> [1] -0.1188634  3.5319468\n#> \n#> $TE_SRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_SRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_FRD_naive\n#> [1] 2.356601\n#> \n#> $TE_FRD_naive_CI\n#> [1] 1.995430 2.717772\n#> \n#> $TE_FRD_bounds\n#> [1] 1.980883 2.362344\n#> \n#> $TE_FRD_CI\n#> [1] -0.6950823  4.6112538\n#> \n#> $TE_FRD_bounds_refinementA\n#> [1] 1.980883 2.357499\n#> \n#> $TE_FRD_refinementA_CI\n#> [1] -0.6950823  4.6112538\n#> \n#> $TE_FRD_bounds_refinementB\n#> [1] 1.980883 2.351411\n#> \n#> $TE_FRD_refinementB_CI\n#> [1] -0.6152215  4.2390830\n#> \n#> $TE_FRD_covs_bounds\n#> [1] NA NA\n#> \n#> $TE_FRD_covs_CI\n#> [1] NA NA\n#> \n#> $TE_SRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_FRD_CIs_manipulation\n#> [1] NA NA\n#> \n#> $TE_SRD_right_bounds\n#> [1] 1.376392 2.007746\n#> \n#> $TE_SRD_right_CI\n#> [1] -5.036752  5.889137\n#> \n#> $TE_FRD_right_bounds\n#> [1] 1.721121 2.511504\n#> \n#> $TE_FRD_right_CI\n#> [1] -6.663269  7.414185\nrdbounds_est_tau <-\n    rdbounds(\n        y = df$y,\n        x = df$x,\n        # covs = as.factor(df$cov),\n        treatment = df$treatment,\n        c = 0,\n        discrete_x = FALSE,\n        discrete_y = FALSE,\n        bwsx = c(.2, .5),\n        bwy = 1,\n        kernel = \"epanechnikov\",\n        orders = 1,\n        evaluation_ys = seq(from = 0, to = 15, by = 1),\n        refinement_A = TRUE,\n        refinement_B = TRUE,\n        right_effects = TRUE,\n        potential_taus = c(.025, .05, .1, .2),\n        yextremes = c(0, 15),\n        num_bootstraps = 5\n    )\n#> [1] \"The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209\"\n#> [1] \"2024-05-13 19:12:52 Estimating CDFs for point estimates\"\n#> [1] \"2024-05-13 19:12:52 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:53 Estimating CDFs with nudged tau (tau_star)\"\n#> [1] \"2024-05-13 19:12:53 .....Estimating CDFs for units just to the right of the cutoff\"\n#> [1] \"2024-05-13 19:12:56 Beginning parallelized output by bootstrap..\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.025\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.05\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.1\"\n#> [1] \"2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.2\"\n#> [1] \"2024-05-13 19:13:03 Beginning parallelized output by bootstrap x fixed tau..\"\n#> [1] \"2024-05-13 19:13:09 Computing Confidence Intervals\"\n#> [1] \"2024-05-13 19:13:19 Time taken:0.46 minutes\"\ncausalverse::plot_rd_aa_share(rdbounds_est_tau) # For SRD (default)\n# causalverse::plot_rd_aa_share(rdbounds_est_tau, rd_type = \"FRD\")  # For FRD"},{"path":"regression-discontinuity.html","id":"fuzzy-rd-design","chapter":"24 Regression Discontinuity","heading":"24.3 Fuzzy RD Design","text":"cutoff perfectly determine treatment, creates discontinuity likelihood receiving treatment, need another instrumentFor close cutoff, create instrument \\(D_i\\)\\[\nZ_i=\n\\begin{cases}\n1 & \\text{} X_i \\ge c \\\\\n0 & \\text{} X_c < c\n\\end{cases}\n\\], can estimate effect treatment compliers (.e., treatment \\(D_i\\) depends \\(Z_i\\))LATE parameter\\[\n\\lim_{c - \\epsilon \\le X \\le c + \\epsilon, \\epsilon \\0}( \\frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)})\n\\]equivalently, canonical parameter:\\[\n\\frac{lim_{x \\downarrow c}E(Y|X = x) - \\lim_{x \\uparrow c} E(Y|X = x)}{\\lim_{x \\downarrow c } E(D |X = x) - \\lim_{x \\uparrow c}E(D |X=x)}\n\\]Two equivalent ways estimateFirst\nSharp RDD \\(Y\\)\nSharp RDD \\(D\\)\nTake estimate step 1 divide step 2\nFirstSharp RDD \\(Y\\)Sharp RDD \\(Y\\)Sharp RDD \\(D\\)Sharp RDD \\(D\\)Take estimate step 1 divide step 2Take estimate step 1 divide step 2Second: Subset observations close \\(c\\) run instrumental variable \\(Z\\)Second: Subset observations close \\(c\\) run instrumental variable \\(Z\\)","code":""},{"path":"regression-discontinuity.html","id":"regression-kink-design","chapter":"24 Regression Discontinuity","heading":"24.4 Regression Kink Design","text":"slope treatment intensity changes cutoff (instead level treatment assignment), can regression kink designIf slope treatment intensity changes cutoff (instead level treatment assignment), can regression kink designExample: unemployment benefitsExample: unemployment benefitsSharp Kink RD parameter\\[\n\\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}b(x) - \\lim_{x \\uparrow c} \\frac{d}{dx}b(x)}\n\\]\\(b(x)\\) known function inducing “kink”Fuzzy Kink RD parameter\\[\n\\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}E[D_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[D_i |X_i = x]}\n\\]","code":""},{"path":"regression-discontinuity.html","id":"multi-cutoff","chapter":"24 Regression Discontinuity","heading":"24.5 Multi-cutoff","text":"\\[\n\\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c]\n\\]","code":""},{"path":"regression-discontinuity.html","id":"multi-score","chapter":"24 Regression Discontinuity","heading":"24.6 Multi-score","text":"Multi-score (multiple dimensions) (e.g., math English cutoff certain honor class):\\[\n\\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x]\n\\]","code":""},{"path":"regression-discontinuity.html","id":"steps-for-sharp-rd","chapter":"24 Regression Discontinuity","heading":"24.7 Steps for Sharp RD","text":"Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Run regression sides cutoff get treatment effectRun regression sides cutoff get treatment effectRobustness checks:\nAssess possible jumps variables around cutoff\nHypothesis testing bunching\nPlacebo tests\nVarying bandwidth\nRobustness checks:Assess possible jumps variables around cutoffAssess possible jumps variables around cutoffHypothesis testing bunchingHypothesis testing bunchingPlacebo testsPlacebo testsVarying bandwidthVarying bandwidth","code":""},{"path":"regression-discontinuity.html","id":"steps-for-fuzzy-rd","chapter":"24 Regression Discontinuity","heading":"24.8 Steps for Fuzzy RD","text":"Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph data computing average value outcome variable set bins (large enough see smooth graph, small enough make jump around cutoff clear).Graph probability treatmentGraph probability treatmentEstimate treatment effect using 2SLSEstimate treatment effect using 2SLSRobustness checks:\nAssess possible jumps variables around cutoff\nHypothesis testing bunching\nPlacebo tests\nVarying bandwidth\nRobustness checks:Assess possible jumps variables around cutoffAssess possible jumps variables around cutoffHypothesis testing bunchingHypothesis testing bunchingPlacebo testsPlacebo testsVarying bandwidthVarying bandwidth","code":""},{"path":"regression-discontinuity.html","id":"steps-for-rdit-regression-discontinuity-in-time","chapter":"24 Regression Discontinuity","heading":"24.9 Steps for RDiT (Regression Discontinuity in Time)","text":"Notes:Additional assumption: Time-varying confounders change smoothly across cutoff dateTypically used policy implementation date subjects, can also used cases implementation dates different subjects. second case, researchers typically use different RDiT specification time series.Sometimes date implementation randomly assigned chosen strategically. Hence, RDiT thought “discontinuity threshold” interpretation RD (“local randomization”). (C. Hausman Rapson 2018, 8)Normal RD uses variation \\(N\\) dimension, RDiT uses variation \\(T\\) dimensionChoose polynomials based BIC typically. can either global polynomial pre-period post-period polynomial time series (usually global one perform better)use augmented local linear outlined (C. Hausman Rapson 2018, 12), estimate model control first take residuals include model RDiT treatment (remember use bootstrapping method account first-stage variance second stage).Pros:can overcome cases cross-sectional variation treatment implementation (feasible)\npapers use RDiT (1) see differential treatment effects across individuals/ space (Auffhammer Kellogg 2011) (2) compare 2 estimates control group’s validity questionable (Gallego, Montero, Salas 2013).\ncan overcome cases cross-sectional variation treatment implementation (feasible)papers use RDiT (1) see differential treatment effects across individuals/ space (Auffhammer Kellogg 2011) (2) compare 2 estimates control group’s validity questionable (Gallego, Montero, Salas 2013).Better pre/post comparison can include flexible controlsBetter pre/post comparison can include flexible controlsBetter event studies can use long-time horizons (may relevant now since development long-time horizon event studies), can use higher-order polynomials time control variables.Better event studies can use long-time horizons (may relevant now since development long-time horizon event studies), can use higher-order polynomials time control variables.Cons:Taking observation threshold (time) can bias estimates unobservables time-series properties data generating process.Taking observation threshold (time) can bias estimates unobservables time-series properties data generating process.(McCrary 2008) test possible (see Sorting/Bunching/Manipulation) density running (time) uniform, can’t use test.(McCrary 2008) test possible (see Sorting/Bunching/Manipulation) density running (time) uniform, can’t use test.Time-varying unobservables may impact dependent variable discontinuouslyTime-varying unobservables may impact dependent variable discontinuouslyError terms likely include persistence (serially correlated errors)Error terms likely include persistence (serially correlated errors)Researchers model time-varying treatment RDiT\nsmall enough window, local linear specification fine, global polynomials can either big small (C. Hausman Rapson 2018)\nResearchers model time-varying treatment RDiTIn small enough window, local linear specification fine, global polynomials can either big small (C. Hausman Rapson 2018)BiasesTime-Varying treatment Effects\nincrease sample size either \ngranular data (greater frequency): increase power problem serial correlation\nincreasing time window: increases bias confounders\n\n2 additional assumption:\nModel correctly specified (confoudners global polynomial approximation)\nTreatment effect correctly specified (whether ’s smooth constant, varies)\n2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)\n\nusually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)\nTime-Varying treatment Effectsincrease sample size either \ngranular data (greater frequency): increase power problem serial correlation\nincreasing time window: increases bias confounders\nincrease sample size either bymore granular data (greater frequency): increase power problem serial correlationmore granular data (greater frequency): increase power problem serial correlationincreasing time window: increases bias confoundersincreasing time window: increases bias confounders2 additional assumption:\nModel correctly specified (confoudners global polynomial approximation)\nTreatment effect correctly specified (whether ’s smooth constant, varies)\n2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)\n2 additional assumption:Model correctly specified (confoudners global polynomial approximation)Model correctly specified (confoudners global polynomial approximation)Treatment effect correctly specified (whether ’s smooth constant, varies)Treatment effect correctly specified (whether ’s smooth constant, varies)2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)2 assumptions interact ( don’t want interact - .e., don’t want polynomial correlated unobserved variation treatment effect)usually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)usually difference short-run long-run treatment effects, ’s also possibly bias can stem -fitting problem polynomial specification. (C. Hausman Rapson 2018, 544)Autoregression (serial dependence)\nNeed use clustered standard errors account serial dependence residuals\ncase serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)\ncase serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)\nAutoregression (serial dependence)Need use clustered standard errors account serial dependence residualsNeed use clustered standard errors account serial dependence residualsIn case serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)case serial dependence \\(\\epsilon_{}\\), don’t solution, including lagged dependent variable misspecify model (probably find another research project)case serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)case serial dependence \\(y_{}\\), long window, becomes fuzzy try recover. can include lagged dependent variable (bias can still come time-varying treatment -fitting global polynomial)Sorting Anticipation Effects\nrun (McCrary 2008) density time running variable uniform\nCan still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)\nHence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behavior\nSorting Anticipation EffectsCannot run (McCrary 2008) density time running variable uniformCannot run (McCrary 2008) density time running variable uniformCan still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)Can still run tests check discontinuities covariates (want discontinuities) discontinuities outcome variable placebo thresholds ( don’t want discontinuities)Hence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behaviorHence, ’s hard argue causal effect total effect causal treatment unobserved sorting/anticipation/adaptation/avoidance effects. can argue behaviorRecommendations robustness check following (C. Hausman Rapson 2018, 549)Plot raw data residuals (removing confounders trend). varying polynomial local linear controls, inconsistent results can sign time-varying treatment effects.Using global polynomial, overfit, show polynomial different order alternative local linear bandwidths. results consistent, ’re okayPlacebo Tests: estimate another RD (1) another location subject (receive treatment) (2) use another date.Plot RD discontinuity continuous controlsDonut RD see avoiding selection close cutoff yield better results (Barreca et al. 2011)Test auto-regression (using pre-treatment data). evidence autoregression, include lagged dependent variableAugmented local linear (need use global polynomial avoid -fitting)\nUse full sample exclude effect important predictors\nEstimate conditioned second stage smaller sample bandwidth\nUse full sample exclude effect important predictorsUse full sample exclude effect important predictorsEstimate conditioned second stage smaller sample bandwidthEstimate conditioned second stage smaller sample bandwidthExamples (C. Hausman Rapson 2018, 534) inecon(Davis 2008): Air quality(Davis 2008): Air quality(Auffhammer Kellogg 2011): Air quality(Auffhammer Kellogg 2011): Air quality(H. Chen et al. 2018): Air quality(H. Chen et al. 2018): Air quality(De Paola, Scoppa, Falcone 2013): car accidents(De Paola, Scoppa, Falcone 2013): car accidents(Gallego, Montero, Salas 2013): air quality(Gallego, Montero, Salas 2013): air quality(Bento et al. 2014): Traffic(Bento et al. 2014): Traffic(M. L. Anderson 2014): Traffic(M. L. Anderson 2014): Traffic(Burger, Kaffine, Yu 2014): Car accidents(Burger, Kaffine, Yu 2014): Car accidents(Brodeur et al. 2021): Covid19 lock-downs well-(Brodeur et al. 2021): Covid19 lock-downs well-beingmarketingM. R. Busse et al. (2013): Vehicle pricesM. R. Busse et al. (2013): Vehicle prices(X. Chen et al. 2009): Customer Satisfaction(X. Chen et al. 2009): Customer Satisfaction(M. R. Busse, Simester, Zettelmeyer 2010): Vehicle prices(M. R. Busse, Simester, Zettelmeyer 2010): Vehicle prices(Davis Kahn 2010): vehicle prices(Davis Kahn 2010): vehicle prices","code":""},{"path":"regression-discontinuity.html","id":"evaluation-of-an-rd","chapter":"24 Regression Discontinuity","heading":"24.10 Evaluation of an RD","text":"Evidence (either formal tests graphs)\nTreatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .\nmanipulation assignment variable.\nEvidence (either formal tests graphs)Treatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .Treatment outcomes change discontinuously cutoff, variables pre-treatment outcomes .manipulation assignment variable.manipulation assignment variable.Results robust various functional forms forcing variableResults robust various functional forms forcing variableIs (unobserved) confound cause discontinuous change cutoff (.e., multiple forcing variables / bundling institutions)?(unobserved) confound cause discontinuous change cutoff (.e., multiple forcing variables / bundling institutions)?External Validity: likely result cutoff generalize?External Validity: likely result cutoff generalize?General Model\\[\nY_i = \\beta_0 + f(x_i) \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]\\(f(x_i)\\) functional form \\(x_i\\)Simple caseWhen \\(f(x_i) = x_i\\) (linear function)\\[\nY_i = \\beta_0 + x_i \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]RD gives \\(\\beta_2\\) (causal effect) \\(X\\) \\(Y\\) cutoff pointIn practice, everyone \\[\nY_i = \\alpha_0 + f(x) \\alpha _1 + [(x_i \\ge c)]\\alpha_2 + [f(x_i)\\times [(x_i \\ge c)]\\alpha_3 + u_i\n\\]estimate different slope different sides lineand estimate \\(\\alpha_3\\) different 0 return simple caseNotes:Sparse data can make \\(\\alpha_3\\) large differential effectSparse data can make \\(\\alpha_3\\) large differential effectPeople skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) good. However, still insist, non-parametric estimation can best bet.People skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) good. However, still insist, non-parametric estimation can best bet.Bandwidth \\(c\\) (window)Closer \\(c\\) can give lower bias, also efficiencyCloser \\(c\\) can give lower bias, also efficiencyWider \\(c\\) can increase bias, higher efficiency.Wider \\(c\\) can increase bias, higher efficiency.Optimal bandwidth controversial, usually appendix research article anyway.Optimal bandwidth controversial, usually appendix research article anyway.can either\ndrop observations outside bandwidth \nweight depends far close \\(c\\)\ncan eitherdrop observations outside bandwidth ordrop observations outside bandwidth orweight depends far close \\(c\\)weight depends far close \\(c\\)","code":""},{"path":"regression-discontinuity.html","id":"applications","chapter":"24 Regression Discontinuity","heading":"24.11 Applications","text":"Examples marketing:(Narayanan Kalyanam 2015)(Narayanan Kalyanam 2015)(Hartmann, Nair, Narayanan 2011): nonparametric estimation guide identifying causal marketing mix effects(Hartmann, Nair, Narayanan 2011): nonparametric estimation guide identifying causal marketing mix effectsPackages R (see (Thoemmes, Liao, Jin 2017) detailed comparisons): can handle sharp fuzzy RDrddrddrdrobust estimation, inference plotrdrobust estimation, inference plotrddensity discontinuity density tests (Sorting/Bunching/Manipulation) using local polynomials binomial testrddensity discontinuity density tests (Sorting/Bunching/Manipulation) using local polynomials binomial testrdlocrand covariate balance, binomial tests, window selectionrdlocrand covariate balance, binomial tests, window selectionrdmulti multiple cutoffs multiple scoresrdmulti multiple cutoffs multiple scoresrdpower power, sample selectionrdpower power, sample selectionrddtoolsrddtools(Calonico, Cattaneo, Farrell 2020)(G. Imbens Kalyanaraman 2012)(Calonico, Cattaneo, Titiunik 2014)Kernel functionsTriangularTriangularRectangularRectangularEpanechnikovGaussianIncludeResidualsMcCrary sortingEquality covariates distribution meanbased table 1 (Thoemmes, Liao, Jin 2017) (p. 347)","code":""},{"path":"regression-discontinuity.html","id":"example-1-1","chapter":"24 Regression Discontinuity","heading":"24.11.1 Example 1","text":"Example Leihua Ye\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i\n\\]\\[\nX_i =\n\\begin{cases}\n1, W_i \\ge c \\\\\n0, W_i < c\n\\end{cases}\n\\]","code":"\n#cutoff point = 3.5\nGPA <- runif(1000, 0, 4)\nfuture_success <- 10 + 2 * GPA + 10 * (GPA >= 3.5) + rnorm(1000)\n#install and load the package ‘rddtools’\n#install.packages(“rddtools”)\nlibrary(rddtools)\ndata <- rdd_data(future_success, GPA, cutpoint = 3.5)\n# plot the dataset\nplot(\n    data,\n    col =  \"red\",\n    cex = 0.1,\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)\n# estimate the sharp RDD model\nrdd_mod <- rdd_reg_lm(rdd_object = data, slope =  \"same\")\nsummary(rdd_mod)\n#> \n#> Call:\n#> lm(formula = y ~ ., data = dat_step1, weights = weights)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.90364 -0.70348  0.00278  0.66828  3.00603 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 16.90704    0.06637  254.75   <2e-16 ***\n#> D           10.09058    0.11063   91.21   <2e-16 ***\n#> x            1.97078    0.03281   60.06   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9908 on 997 degrees of freedom\n#> Multiple R-squared:  0.9654, Adjusted R-squared:  0.9654 \n#> F-statistic: 1.392e+04 on 2 and 997 DF,  p-value: < 2.2e-16\n# plot the RDD model along with binned observations\nplot(\n    rdd_mod,\n    cex = 0.1,\n    col =  \"red\",\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)"},{"path":"regression-discontinuity.html","id":"example-2","chapter":"24 Regression Discontinuity","heading":"24.11.2 Example 2","text":"Bowblis Smith (2021)Occupational licensing can either increase decrease market efficiency:information means efficiencyMore information means efficiencyIncreased entry barriers (.e., friction) increase efficiencyIncreased entry barriers (.e., friction) increase efficiencyComponents RDRunning variableCutoff: 120 beds aboveTreatment: treatment cutoff point.OLS\\[\nY_i = \\alpha_0 + X_i \\alpha_1 + LW_i \\alpha_2 + \\epsilon_i\n\\]\\(LW_i\\) Licensed/certified workers (fraction format center).\\(LW_i\\) Licensed/certified workers (fraction format center).\\(Y_i\\) = Quality service\\(Y_i\\) = Quality serviceBias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Preference-based: places higher quality staff want keep high quality staffs.Preference-based: places higher quality staff want keep high quality staffs.RD\\[\n\\begin{aligned}\nY_{ist} &= \\beta_0 + [(Bed \\ge121)_{ist}]\\beta_1 + f(Size_{ist}) \\beta_2\\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\beta_3 \\\\\n&+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\end{aligned}\n\\]\\(s\\) = state\\(s\\) = state\\(t\\) = year\\(t\\) = year\\(\\) = hospital\\(\\) = hospitalThis RD fuzzyIf right near threshold (bandwidth), states different sorting (.e., non-random), need fixed-effect state \\(s\\). RD assumption wrong anyway, won’t first placeIf right near threshold (bandwidth), states different sorting (.e., non-random), need fixed-effect state \\(s\\). RD assumption wrong anyway, won’t first placeTechnically, also run fixed-effect regression, ’s lower causal inference hierarchy. Hence, don’t .Technically, also run fixed-effect regression, ’s lower causal inference hierarchy. Hence, don’t .Moreover, RD framework, don’t include \\(t\\) treatment (FE include )Moreover, RD framework, don’t include \\(t\\) treatment (FE include )include \\(\\pi_i\\) hospital, don’t variation causal estimates (hardly hospital changes bed size panel)include \\(\\pi_i\\) hospital, don’t variation causal estimates (hardly hospital changes bed size panel)\\(\\beta_1\\) intent treat (treatment effect coincide intent treat)\\(\\beta_1\\) intent treat (treatment effect coincide intent treat)take fuzzy cases , introduce selection bias.take fuzzy cases , introduce selection bias.Note drop cases based behavioral choice (exclude non-compliers), can drop particular behaviors ((e.g., people like round numbers).Note drop cases based behavioral choice (exclude non-compliers), can drop particular behaviors ((e.g., people like round numbers).Thus, use Instrument variable 33.1.3.1Stage 1:\\[\n\\begin{aligned}\nQSW_{ist} &= \\alpha_0 + [(Bed \\ge121)_{ist}]\\alpha_1 + f(Size_{ist}) \\alpha_2\\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\alpha_3 \\\\\n&+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\end{aligned}\n\\](Note: different fixed effects error term - \\(\\delta, \\gamma_s, \\theta_t, \\epsilon_{ist}\\) first equation, ran Greek letters)Stage 2:\\[\n\\begin{aligned}\nY_{ist} &= \\gamma_0 + \\gamma_1 \\hat{QWS}_{ist} + f(Size_{ist}) \\delta_2 \\\\\n&+ [f(Size_{ist}) \\times (Bed \\ge 121)] \\delta_3 \\\\\n&+ X_{} \\lambda + \\eta_s + \\tau_t + u_{ist}\n\\end{aligned}\n\\]bigger jump (discontinuity), similar 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) \\(\\gamma_1\\) average treatment effect (exposing policy)bigger jump (discontinuity), similar 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) \\(\\gamma_1\\) average treatment effect (exposing policy)\\(\\beta_1\\) always closer 0 \\(\\gamma_1\\)\\(\\beta_1\\) always closer 0 \\(\\gamma_1\\)Figure 1 shows bunching every 5 units cutoff, 120 still .Figure 1 shows bunching every 5 units cutoff, 120 still .manipulable bunching, decrease 130If manipulable bunching, decrease 130Since limited number mass points (round numbers), clustered standard errors mass pointSince limited number mass points (round numbers), clustered standard errors mass point","code":""},{"path":"regression-discontinuity.html","id":"example-3","chapter":"24 Regression Discontinuity","heading":"24.11.3 Example 3","text":"Replication (Carpenter Dobkin 2009) Philipp Leppert, dataset ","code":""},{"path":"regression-discontinuity.html","id":"example-4","chapter":"24 Regression Discontinuity","heading":"24.11.4 Example 4","text":"detailed application, see (Thoemmes, Liao, Jin 2017) use rdd, rdrobust, rddtools","code":""},{"path":"synthetic-difference-in-differences.html","id":"synthetic-difference-in-differences","chapter":"25 Synthetic Difference-in-Differences","heading":"25 Synthetic Difference-in-Differences","text":"(Arkhangelsky et al. 2021)also known weighted double-differencing estimatorsSetting: Researchers use panel data study effects policy changes.\nPanel data: repeated observations across time various units.\nunits exposed policy different times others.\nSetting: Researchers use panel data study effects policy changes.Panel data: repeated observations across time various units.Panel data: repeated observations across time various units.units exposed policy different times others.units exposed policy different times others.Policy changes often aren’t random across units time.\nChallenge: Observed covariates might lead credible conclusions confounding (G. W. Imbens Rubin 2015)\nPolicy changes often aren’t random across units time.Challenge: Observed covariates might lead credible conclusions confounding (G. W. Imbens Rubin 2015)estimate effects, either\nDifference--differences () method widely used applied economics.\nSynthetic Control (SC) methods offer alternative approach comparative case studies.\nestimate effects, eitherDifference--differences () method widely used applied economics.Difference--differences () method widely used applied economics.Synthetic Control (SC) methods offer alternative approach comparative case studies.Synthetic Control (SC) methods offer alternative approach comparative case studies.Difference SC:\n: used many policy-exposed units; relies “parallel trends” assumption.\nSC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.\nDifference SC:: used many policy-exposed units; relies “parallel trends” assumption.: used many policy-exposed units; relies “parallel trends” assumption.SC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.SC: used policy-exposed units; compensates lack parallel trends reweighting units based pre-exposure trends.New proposition: Synthetic Difference Differences (SDID).\nCombines features SC.\nReweights matches pre-exposure trends (similar SC).\nInvariant additive unit-level shifts, valid large-panel inference (like ).\nNew proposition: Synthetic Difference Differences (SDID).Combines features SC.Combines features SC.Reweights matches pre-exposure trends (similar SC).Reweights matches pre-exposure trends (similar SC).Invariant additive unit-level shifts, valid large-panel inference (like ).Invariant additive unit-level shifts, valid large-panel inference (like ).Attractive features:\nSDID provides consistent asymptotically normal estimates.\nSDID performs par better traditional settings.\ncan handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.\n\nSimilarly, SDID good better SC traditional SC settings.\nUniformly random treatment assignment results unbiased outcomes methods, SDID precise.\nSDID reduces bias effectively non-uniformly random assignments.\nSDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).\nmuch similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)\nAttractive features:SDID provides consistent asymptotically normal estimates.SDID provides consistent asymptotically normal estimates.SDID performs par better traditional settings.\ncan handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.\nSDID performs par better traditional settings.can handle completely random treatment assignment, SDID can handle cases treatment assignment correlated time unit latent factors.Similarly, SDID good better SC traditional SC settings.Similarly, SDID good better SC traditional SC settings.Uniformly random treatment assignment results unbiased outcomes methods, SDID precise.Uniformly random treatment assignment results unbiased outcomes methods, SDID precise.SDID reduces bias effectively non-uniformly random assignments.SDID reduces bias effectively non-uniformly random assignments.SDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).SDID’s double robustness akin augmented inverse probability weighting estimator Scharfstein, Rotnitzky, Robins (1999).much similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)much similar augmented SC estimator (Ben-Michael, Feller, Rothstein 2021; Arkhangelsky et al. 2021, 4112)Ideal case use SDID estimator \\(N_{ctr} \\approx T_{pre}\\)\\(N_{ctr} \\approx T_{pre}\\)Small \\(T_{post}\\)Small \\(T_{post}\\)\\(N_{tr} <\\sqrt{N_{ctr}}\\)\\(N_{tr} <\\sqrt{N_{ctr}}\\)Applications marketing:Lambrecht, Tucker, Zhang (2024): TV ads online browsing sales.Lambrecht, Tucker, Zhang (2024): TV ads online browsing sales.Keller, Guyt, Grewal (2024): soda tax marketing effectiveness.Keller, Guyt, Grewal (2024): soda tax marketing effectiveness.","code":""},{"path":"synthetic-difference-in-differences.html","id":"understanding","chapter":"25 Synthetic Difference-in-Differences","heading":"25.1 Understanding","text":"Consider traditional time-series cross-sectional dataLet \\(Y_{}\\) denote outcome unit \\(\\) period \\(t\\)balanced panel \\(N\\) units \\(T\\) time periods\\(W_{} \\\\{0, 1\\}\\) binary treatment\\(W_{} \\\\{0, 1\\}\\) binary treatment\\(N_c\\) never-treated units (control)\\(N_c\\) never-treated units (control)\\(N_t\\) treated units time \\(T_{pre}\\)\\(N_t\\) treated units time \\(T_{pre}\\)Steps:Find unit weights \\(\\hat{w}^{sdid}\\) \\(\\sum_{= 1}^{N_c} \\hat{w}_i^{sdid} Y_{} \\approx N_t^{-1} \\sum_{= N_c + 1}^N Y_{} \\forall t = 1, \\dots, T_{pre}\\) (.e., pre-treatment trends outcome treated similar control units) (similar SC).Find time weights \\(\\hat{\\lambda}_t\\) balanced window (.e., posttreatment outcomes control units differ consistently weighted average pretreatment outcomes).Estimate average causal effect treatment\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_ t - W_{} \\tau)^2 \\hat{w}_i^{sdid} \\hat{\\lambda}_t^{sdid} \\}\n\\]Better estimator \\(\\tau^{}\\) consider time unit weights\\[\n(\\hat{\\tau}^{}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_ t - W_{} \\tau)^2 \\}\n\\]Better SC estimator \\(\\tau^{sc}\\) lacks unit fixed effete time weights\\[\n(\\hat{\\tau}^{sc}, \\hat{\\mu}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\beta} \\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\beta_ t - W_{} \\tau)^2 \\hat{w}_i^{sdid}  \\}\n\\]Alternatively, think parameter interest :\\[\n\\hat{\\tau} = \\hat{\\delta}_t - \\sum_{= 1}^{N_c} \\hat{w}_i \\hat{\\delta}_i\n\\]\\(\\hat{\\delta}_t = \\frac{1}{N_t} \\sum_{= N_c + 1}^N \\hat{\\delta}_i\\)SDID estimator uses weights:\nMakes two-way fixed effect regression “local.”\nEmphasizes units similar past treated units.\nPrioritizes periods resembling treated periods.\nSDID estimator uses weights:Makes two-way fixed effect regression “local.”Makes two-way fixed effect regression “local.”Emphasizes units similar past treated units.Emphasizes units similar past treated units.Prioritizes periods resembling treated periods.Prioritizes periods resembling treated periods.Benefits localization:\nRobustness: Using similar units periods boosts estimator’s robustness.\nImproved Precision: Weights can eliminate predictable outcome components.\nSEs SDID smaller SC \nCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .\n\nBenefits localization:Robustness: Using similar units periods boosts estimator’s robustness.Robustness: Using similar units periods boosts estimator’s robustness.Improved Precision: Weights can eliminate predictable outcome components.\nSEs SDID smaller SC \nCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .\nImproved Precision: Weights can eliminate predictable outcome components.SEs SDID smaller SC DIDThe SEs SDID smaller SC DIDCaveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .Caveat: ’s minor systematic heterogeneity outcomes, unequal weighting might reduce precision compared standard .Weight Design:\nUnit Weights: Makes average outcome treated units roughly parallel weighted average control units.\nTime Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.\nWeight Design:Unit Weights: Makes average outcome treated units roughly parallel weighted average control units.Unit Weights: Makes average outcome treated units roughly parallel weighted average control units.Time Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.Time Weights: Ensures posttreatment outcomes control units differ consistently weighted average pretreatment outcomes.Weights enhance ’s plausibility:\nRaw data often lacks parallel time trends treated/control units.\nSimilar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).\nSDID automates process, applying similar logic weight units time periods.\nWeights enhance ’s plausibility:Raw data often lacks parallel time trends treated/control units.Raw data often lacks parallel time trends treated/control units.Similar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).Similar techniques (e.g., adjusting covariates selecting specific time periods) used (Callaway Sant’Anna 2021).SDID automates process, applying similar logic weight units time periods.SDID automates process, applying similar logic weight units time periods.Time Weights SDID:\nRemoves bias boosts precision (.e., minimizes influence time periods vastly different posttreatment periods).\nTime Weights SDID:Removes bias boosts precision (.e., minimizes influence time periods vastly different posttreatment periods).Argument Unit Fixed Effects:\nFlexibility: Increases model flexibility thereby bolsters robustness.\nEnhanced Precision: Unit fixed effects explain significant portion outcome variation.\nArgument Unit Fixed Effects:Flexibility: Increases model flexibility thereby bolsters robustness.Flexibility: Increases model flexibility thereby bolsters robustness.Enhanced Precision: Unit fixed effects explain significant portion outcome variation.Enhanced Precision: Unit fixed effects explain significant portion outcome variation.SC Weighting & Unit Fixed Effects:\ncertain conditions, SC weighting can inherently account unit fixed effects.\nexample, weighted average outcome control units pretreatment treated units. (unlikely reality)\n\nuse unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)\nSC Weighting & Unit Fixed Effects:certain conditions, SC weighting can inherently account unit fixed effects.\nexample, weighted average outcome control units pretreatment treated units. (unlikely reality)\ncertain conditions, SC weighting can inherently account unit fixed effects.example, weighted average outcome control units pretreatment treated units. (unlikely reality)use unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)use unit fixed effect synthetic control regression (.e., synthetic control intercept) proposed Doudchenko Imbens (2016) Ferman Pinto (2021) (called DIFP)details applicationChoose unit weightsRegularization Parameter:\nEqual size typical one-period outcome change control units pre-period, multiplied scaling factor (Arkhangelsky et al. 2021, 4092).\nRegularization Parameter:Equal size typical one-period outcome change control units pre-period, multiplied scaling factor (Arkhangelsky et al. 2021, 4092).Relation SC Weights:\nSDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:\nInclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\n\nRegularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\n\n\nweights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.\nRelation SC Weights:SDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:\nInclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\n\nRegularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\n\nSDID weights similar used (Abadie, Diamond, Hainmueller 2010) except two distinctions:Inclusion Intercept Term:\nweights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.\nflexibility comes use unit fixed effects, can absorb consistent differences units.\nInclusion Intercept Term:weights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.weights SynthDiD necessarily make control pre-trends perfectly match treated trends, just make parallel.flexibility comes use unit fixed effects, can absorb consistent differences units.flexibility comes use unit fixed effects, can absorb consistent differences units.Regularization Penalty:\nAdopted Doudchenko Imbens (2016) .\nEnhances dispersion ensures uniqueness weights.\nRegularization Penalty:Adopted Doudchenko Imbens (2016) .Adopted Doudchenko Imbens (2016) .Enhances dispersion ensures uniqueness weights.Enhances dispersion ensures uniqueness weights.weights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.weights identical used (Abadie, Diamond, Hainmueller 2010) without intercept regularization penalty 1 treated unit.Choose time weightsAlso include intercept term, regularization (correlated observations within time periods unit plausible, across units within period).Note: account time-varying variables weights, one can use residuals regression observed outcome time-varying variables, instead observed outcomes (\\(Y_{}^{res} = Y_{} - X_{} \\hat{\\beta}\\), \\(\\hat{\\beta}\\) come \\(Y = \\beta X_{}\\)).SDID method can account systematic effects, often referred unit effects unit heterogeneity, influence treatment assignment (.e., treatment assignment correlated systematic effects). Consequently, provides unbiased estimates, especially valuable ’s suspicion treatment might influenced persistent, unit-specific attributes.Even cases completely random assignment, SDID, , SC unbiased, SynthDiD smallest SE.","code":""},{"path":"synthetic-difference-in-differences.html","id":"application-13","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2 Application","text":"SDID AlgorithmCompute regularization parameter \\(\\zeta\\)\\[\n\\zeta = (N_{t}T_{post})^{1/4} \\hat{\\sigma}\n\\]\\[\n\\hat{\\sigma}^2 = \\frac{1}{N_c(T_{pre}- 1)} \\sum_{= 1}^{N_c} \\sum_{t = 1}^{T_{re}-1}(\\Delta_{} - \\hat{\\Delta})^2\n\\]\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\Delta_{} = Y_{(t + 1)} - Y_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)\\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{= 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{}\\)Compute unit weights \\(\\hat{w}^{sdid}\\)\\[\n(\\hat{w}_0, \\hat{w}^{sidid}) = \\arg \\min_{w_0 \\R, w \\\\Omega}l_{unit}(w_0, w)\n\\]\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{= 1}^{N_c}w_i Y_{} - \\frac{1}{N_t}\\sum_{= N_c + 1}^NY_{})^2 + \\zeta^2 T_{pre}||w||_2^2\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)\\(\\Omega = \\{w \\R_+^N: \\sum_{= 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall = N_c + 1, \\dots, N \\}\\)Compute time weights \\(\\hat{\\lambda}^{sdid}\\)\\[\n(\\hat{\\lambda}_0 , \\hat{\\lambda}^{sdid}) = \\arg \\min_{\\lambda_0 \\R, \\lambda \\\\Lambda} l_{time}(\\lambda_0, \\lambda)\n\\]\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(l_{time} (\\lambda_0, \\lambda) = \\sum_{= 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{})^2\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)\\(\\Lambda = \\{ \\lambda \\R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\)Compute SDID estimator\\[\n(\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta}\\{ \\sum_{= 1}^N \\sum_{t = 1}^T (Y_{} - \\mu - \\alpha_i - \\beta_t - W_{} \\tau)^2 \\hat{w}_i^{sdid}\\hat{\\lambda}_t^{sdid}\n\\]SE EstimationUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUnder certain assumptions (errors, samples, interaction properties time unit fixed effects) detailed (Arkhangelsky et al. 2019, 4107), SDID asymptotically normal zero-centeredUsing asymptotic variance, conventional confidence intervals can applied SDID.Using asymptotic variance, conventional confidence intervals can applied SDID.\\[\n\\tau \\\\hat{\\tau}^{sdid} \\pm z_{\\alpha/2}\\sqrt{\\hat{V}_\\tau}\n\\]3 approaches variance estimation confidence intervals:\nClustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\n\nJackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\n\nPlacebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\n\n3 approaches variance estimation confidence intervals:Clustered Bootstrap (Efron 1992):\nIndependently resample units.\nAdvantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.\nDisadvantage: Computationally expensive.\nClustered Bootstrap (Efron 1992):Independently resample units.Independently resample units.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Advantages: Simple use; robust performance large panels due natural approach inference panel data observations unit might correlated.Disadvantage: Computationally expensive.Disadvantage: Computationally expensive.Jackknife (Miller 1974):\nApplied weighted SDID regression fixed weights.\nGenerally conservative precise treated control units sufficiently similar.\nrecommended methods, like SC estimator, due potential biases.\nAppropriate jackknifing without random weights.\nJackknife (Miller 1974):Applied weighted SDID regression fixed weights.Applied weighted SDID regression fixed weights.Generally conservative precise treated control units sufficiently similar.Generally conservative precise treated control units sufficiently similar.recommended methods, like SC estimator, due potential biases.recommended methods, like SC estimator, due potential biases.Appropriate jackknifing without random weights.Appropriate jackknifing without random weights.Placebo Variance Estimation:\nCan used cases one treated unit large panels.\nPlacebo evaluations swap treated unit untreated ones estimate noise.\nRelies homoskedasticity across units.\nDepends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.\nvalidity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).\nPlacebo Variance Estimation:Can used cases one treated unit large panels.Can used cases one treated unit large panels.Placebo evaluations swap treated unit untreated ones estimate noise.Placebo evaluations swap treated unit untreated ones estimate noise.Relies homoskedasticity across units.Relies homoskedasticity across units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.Depends homoskedasticity across units. hinges empirical distribution residuals placebo estimators control units.validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).validity placebo method hinges consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity feasible inference. Detailed analysis available Conley Taber (2011).algorithms Arkhangelsky et al. (2021), p. 4109:Bootstrap Variance EstimationFor \\(b\\) \\(1 \\B\\):\nSample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.\nsample lacks treated control units, resample.\nCalculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).\n\\(b\\) \\(1 \\B\\):Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.Sample \\(N\\) rows \\((\\mathbf{Y}, \\mathbf{W})\\) get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) replacement.sample lacks treated control units, resample.sample lacks treated control units, resample.Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)).Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)Jackknife Variance EstimationFor \\(\\) \\(1 \\N\\):\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)\nCalculate \\(\\hat{\\tau}^{(-)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, , t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{})^2 \\hat{w}_j \\hat{\\lambda}_t\\)Calculate: \\(\\hat{V}_{\\tau} = (N - 1) N^{-1} \\sum_{= 1}^N (\\hat{\\tau}^{(-)} - \\hat{\\tau})^2\\)Placebo Variance EstimationFor \\(b\\) \\(1 \\B\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatment\nConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controls\nCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)\nSample \\(N_t\\) \\(N_c\\) without replacement get “placebo” treatmentConstruct placebo treatment matrix \\(\\mathbf{W}_c^b\\) controlsCalculate \\(\\hat{\\tau}\\) based  \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\)Calculate \\(\\hat{V}_\\tau = \\frac{1}{B}\\sum_{b = 1}^B (\\hat{\\tau}^b - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\)","code":""},{"path":"synthetic-difference-in-differences.html","id":"block-treatment","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2.1 Block Treatment","text":"Code provided synthdid package","code":"\nlibrary(synthdid)\nlibrary(tidyverse)\n\n# Estimate the effect of California Proposition 99 on cigarette consumption\ndata('california_prop99')\n\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\ntau.hat = synthdid::synthdid_estimate(setup$Y, setup$N0, setup$T0)\n\n# se = sqrt(vcov(tau.hat, method = 'placebo'))\n\nplot(tau.hat) + causalverse::ama_theme()\nsetup = synthdid::panel.matrices(synthdid::california_prop99)\n\n# Run for specific estimators\nresults_selected = causalverse::panel_estimate(setup,\n                                               selected_estimators = c(\"synthdid\", \"did\", \"sc\"))\n\nresults_selected\n#> $synthdid\n#> $synthdid$estimate\n#> synthdid: -15.604 +- NA. Effective N0/N0 = 16.4/38~0.4. Effective T0/T0 = 2.8/19~0.1. N1,T1 = 1,12. \n#> \n#> $synthdid$std.error\n#> [1] 10.05324\n#> \n#> \n#> $did\n#> $did$estimate\n#> synthdid: -27.349 +- NA. Effective N0/N0 = 38.0/38~1.0. Effective T0/T0 = 19.0/19~1.0. N1,T1 = 1,12. \n#> \n#> $did$std.error\n#> [1] 15.81479\n#> \n#> \n#> $sc\n#> $sc$estimate\n#> synthdid: -19.620 +- NA. Effective N0/N0 = 3.8/38~0.1. Effective T0/T0 = Inf/19~Inf. N1,T1 = 1,12. \n#> \n#> $sc$std.error\n#> [1] 11.16422\n\n# to access more details in the estimate object\nsummary(results_selected$did$estimate)\n#> $estimate\n#> [1] -27.34911\n#> \n#> $se\n#>      [,1]\n#> [1,]   NA\n#> \n#> $controls\n#>                estimate 1\n#> Wyoming             0.026\n#> Wisconsin           0.026\n#> West Virginia       0.026\n#> Virginia            0.026\n#> Vermont             0.026\n#> Utah                0.026\n#> Texas               0.026\n#> Tennessee           0.026\n#> South Dakota        0.026\n#> South Carolina      0.026\n#> Rhode Island        0.026\n#> Pennsylvania        0.026\n#> Oklahoma            0.026\n#> Ohio                0.026\n#> North Dakota        0.026\n#> North Carolina      0.026\n#> New Mexico          0.026\n#> New Hampshire       0.026\n#> Nevada              0.026\n#> Nebraska            0.026\n#> Montana             0.026\n#> Missouri            0.026\n#> Mississippi         0.026\n#> Minnesota           0.026\n#> Maine               0.026\n#> Louisiana           0.026\n#> Kentucky            0.026\n#> Kansas              0.026\n#> Iowa                0.026\n#> Indiana             0.026\n#> Illinois            0.026\n#> Idaho               0.026\n#> Georgia             0.026\n#> Delaware            0.026\n#> Connecticut         0.026\n#> \n#> $periods\n#>      estimate 1\n#> 1988      0.053\n#> 1987      0.053\n#> 1986      0.053\n#> 1985      0.053\n#> 1984      0.053\n#> 1983      0.053\n#> 1982      0.053\n#> 1981      0.053\n#> 1980      0.053\n#> 1979      0.053\n#> 1978      0.053\n#> 1977      0.053\n#> 1976      0.053\n#> 1975      0.053\n#> 1974      0.053\n#> 1973      0.053\n#> 1972      0.053\n#> 1971      0.053\n#> \n#> $dimensions\n#>           N1           N0 N0.effective           T1           T0 T0.effective \n#>            1           38           38           12           19           19\n\ncausalverse::process_panel_estimate(results_selected)\n#>     Method Estimate    SE\n#> 1 SYNTHDID   -15.60 10.05\n#> 2      DID   -27.35 15.81\n#> 3       SC   -19.62 11.16"},{"path":"synthetic-difference-in-differences.html","id":"staggered-adoption","chapter":"25 Synthetic Difference-in-Differences","heading":"25.2.2 Staggered Adoption","text":"apply staggered adoption settings using SDID estimator (see examples Arkhangelsky et al. (2021), p. 4115 similar Ben-Michael, Feller, Rothstein (2022)), can:Apply SDID estimator repeatedly, every adoption date.Apply SDID estimator repeatedly, every adoption date.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Using Ben-Michael, Feller, Rothstein (2022) ’s method, form matrices adoption date. Apply SDID average based treated unit/time-period fractions.Create multiple samples splitting data time periods. sample consistent adoption date.Create multiple samples splitting data time periods. sample consistent adoption date.formal note special case, see Porreca (2022). compares outcomes using SynthDiD estimators:Two-Way Fixed Effects (TWFE),Two-Way Fixed Effects (TWFE),group time average treatment effect estimator Callaway Sant’Anna (2021),group time average treatment effect estimator Callaway Sant’Anna (2021),partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.partially pooled synthetic control method estimator Ben-Michael, Feller, Rothstein (2021), staggered treatment adoption context.findings reveal SynthDiD produces different estimate average treatment effect compared methods.\nSimulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.\nfindings reveal SynthDiD produces different estimate average treatment effect compared methods.Simulation results suggest differences due SynthDiD’s data generating process assumption (latent factor model) aligning closely actual data additive fixed effects model assumed traditional methods.explore heterogeneity treatment effect, can subgroup analysis (Berman Israeli 2022, 1092)Split data separate subsets subgroup.Compute synthetic effects subset.Use control group consisting non-adopters balanced panel cohort analysis.Switch treatment units subgroup analyzed.Perform synthdid analysis.Use data estimate synthetic control weights.Compute treatment effects using treated subgroup units treatment units.Plot different estimators","code":"\nlibrary(tidyverse)\ndf <- fixest::base_stagg |>\n   dplyr::mutate(treatvar = if_else(time_to_treatment >= 0, 1, 0)) |>\n   dplyr::mutate(treatvar = as.integer(if_else(year_treated > (5 + 2), 0, treatvar)))\n\n\nest <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\"\n)\n#> adoption_cohort: 5 \n#> Treated units: 5 Control units: 65 \n#> adoption_cohort: 6 \n#> Treated units: 5 Control units: 60 \n#> adoption_cohort: 7 \n#> Treated units: 5 Control units: 55\n\ndata.frame(\n    Period = names(est$TE_mean_w),\n    ATE    = est$TE_mean_w,\n    SE     = est$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2 -0.05 0.22\n#> 2      -1  0.05 0.22\n#> 3       0 -5.07 0.80\n#> 4       1 -4.68 0.51\n#> 5       2 -3.70 0.79\n#> 6 cumul.0 -5.07 0.80\n#> 7 cumul.1 -4.87 0.55\n#> 8 cumul.2 -4.48 0.53\n\n\ncausalverse::synthdid_plot_ate(est)\nest_sub <- causalverse::synthdid_est_ate(\n  data               = df,\n  adoption_cohorts   = 5:7,\n  lags               = 2,\n  leads              = 2,\n  time_var           = \"year\",\n  unit_id_var        = \"id\",\n  treated_period_var = \"year_treated\",\n  treat_stat_var     = \"treatvar\",\n  outcome_var        = \"y\",\n  # a vector of subgroup id (from unit id)\n  subgroup           =  c(\n    # some are treated\n    \"11\", \"30\", \"49\" ,\n    # some are control within this period\n    \"20\", \"25\", \"21\")\n)\n#> adoption_cohort: 5 \n#> Treated units: 3 Control units: 65 \n#> adoption_cohort: 6 \n#> Treated units: 0 Control units: 60 \n#> adoption_cohort: 7 \n#> Treated units: 0 Control units: 55\n\ndata.frame(\n    Period = names(est_sub$TE_mean_w),\n    ATE = est_sub$TE_mean_w,\n    SE = est_sub$SE_mean_w\n) |>\n    causalverse::nice_tab()\n#>    Period   ATE   SE\n#> 1      -2  0.32 0.44\n#> 2      -1 -0.32 0.44\n#> 3       0 -4.29 1.68\n#> 4       1 -4.00 1.52\n#> 5       2 -3.44 2.90\n#> 6 cumul.0 -4.29 1.68\n#> 7 cumul.1 -4.14 1.52\n#> 8 cumul.2 -3.91 1.82\n\ncausalverse::synthdid_plot_ate(est)\nlibrary(causalverse)\nmethods <- c(\"synthdid\", \"did\", \"sc\", \"sc_ridge\", \"difp\", \"difp_ridge\")\n\nestimates <- lapply(methods, function(method) {\n  synthdid_est_ate(\n    data               = df,\n    adoption_cohorts   = 5:7,\n    lags               = 2,\n    leads              = 2,\n    time_var           = \"year\",\n    unit_id_var        = \"id\",\n    treated_period_var = \"year_treated\",\n    treat_stat_var     = \"treatvar\",\n    outcome_var        = \"y\",\n    method = method\n  )\n})\n\nplots <- lapply(seq_along(estimates), function(i) {\n  causalverse::synthdid_plot_ate(estimates[[i]],\n                                 title = methods[i],\n                                 theme = causalverse::ama_theme(base_size = 6))\n})\n\ngridExtra::grid.arrange(grobs = plots, ncol = 2)"},{"path":"difference-in-differences.html","id":"difference-in-differences","chapter":"26 Difference-in-differences","heading":"26 Difference-in-differences","text":"List packagesExamples marketing(Liaukonyte, Teixeira, Wilbur 2015): TV ad online shopping(Yanwen Wang, Lewis, Schweidel 2018): political ad source message tone vote shares turnout using discontinuities level political ads borders(Datta, Knox, Bronnenberg 2018): streaming service total music consumption using timing users adoption music streaming service(Janakiraman, Lim, Rishika 2018): data breach announcement affect customer spending using timing data breach variation whether customer info breached event(Israeli 2018): digital monitoring enforcement violations using enforcement min ad price policies(Ramani Srinivasan 2019): firms respond foreign direct investment liberalization using India’s reform 1991.(Pattabhiramaiah, Sriram, Manchanda 2019): paywall affects readership(Akca Rao 2020): aggregators airlines business effect(Lim et al. 2020): nutritional labels nutritional quality brands category using variation timing adoption nutritional labels across categories(Guo, Sriram, Manchanda 2020): payment disclosure laws effect physician prescription behavior using Timing Massachusetts open payment law exogenous shock(S. , Hollenbeck, Proserpio 2022): using Amazon policy change examine causal impact fake reviews sales, average ratings.(Peukert et al. 2022): using European General data protection Regulation, examine impact policy change website usage.Examples econ:(Rosenzweig Wolpin 2000)(Rosenzweig Wolpin 2000)(J. D. Angrist Krueger 2001)(J. D. Angrist Krueger 2001)(Fuchs-Schündeln Hassan 2016): macro(Fuchs-Schündeln Hassan 2016): macroShow mechanism viaMediation analysis: see (Habel, Alavi, Linsenmayer 2021)Mediation analysis: see (Habel, Alavi, Linsenmayer 2021)Moderation analysis: see (Goldfarb Tucker 2011)Moderation analysis: see (Goldfarb Tucker 2011)Steps trust :Visualize treatment rollout (e.g., panelView).Visualize treatment rollout (e.g., panelView).Document number treated units cohort (e.g., control treated).Document number treated units cohort (e.g., control treated).Visualize trajectory average outcomes across cohorts (multiple periods).Visualize trajectory average outcomes across cohorts (multiple periods).Parallel Trends Conduct event-study analysis without covariates.Parallel Trends Conduct event-study analysis without covariates.case covariates, check overlap covariates treated control groups ensure control group validity (e.g., control relatively small treated group, might overlap, make extrapolation).case covariates, check overlap covariates treated control groups ensure control group validity (e.g., control relatively small treated group, might overlap, make extrapolation).Conduct sensitivity analysis parallel trend violations (e.g., honestDiD).Conduct sensitivity analysis parallel trend violations (e.g., honestDiD).","code":""},{"path":"difference-in-differences.html","id":"visualization-1","chapter":"26 Difference-in-differences","heading":"26.1 Visualization","text":"","code":"\nlibrary(panelView)\nlibrary(fixest)\nlibrary(tidyverse)\nbase_stagg <- fixest::base_stagg |>\n    # treatment status\n    dplyr::mutate(treat_stat = dplyr::if_else(time_to_treatment < 0, 0, 1)) |> \n    select(id, year, treat_stat, y)\n\nhead(base_stagg)\n#>   id year treat_stat           y\n#> 2 90    1          0  0.01722971\n#> 3 89    1          0 -4.58084528\n#> 4 88    1          0  2.73817174\n#> 5 87    1          0 -0.65103066\n#> 6 86    1          0 -5.33381664\n#> 7 85    1          0  0.49562631\n\npanelView::panelview(\n    y ~ treat_stat,\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# alternatively specification\npanelView::panelview(\n    Y = \"y\",\n    D = \"treat_stat\",\n    data = base_stagg,\n    index = c(\"id\", \"year\"),\n    xlab = \"Year\",\n    ylab = \"Unit\",\n    display.all = F,\n    gridOff = T,\n    by.timing = T\n)\n\n# Average outcomes for each cohort\npanelView::panelview(\n    data = base_stagg, \n    Y = \"y\",\n    D = \"treat_stat\",\n    index = c(\"id\", \"year\"),\n    by.timing = T,\n    display.all = F,\n    type = \"outcome\", \n    by.cohort = T\n)\n#> Number of unique treatment histories: 10"},{"path":"difference-in-differences.html","id":"simple-dif-n-dif","chapter":"26 Difference-in-differences","heading":"26.2 Simple Dif-n-dif","text":"tool developed intuitively study “natural experiment”, uses much broader.tool developed intuitively study “natural experiment”, uses much broader.Fixed Effects Estimator foundation DIDFixed Effects Estimator foundation DIDWhy dif--dif attractive? Identification strategy: Inter-temporal variation groups\nCross-sectional estimator helps avoid omitted (unobserved) common trends\nTime-series estimator helps overcome omitted (unobserved) cross-sectional differences\ndif--dif attractive? Identification strategy: Inter-temporal variation groupsCross-sectional estimator helps avoid omitted (unobserved) common trendsCross-sectional estimator helps avoid omitted (unobserved) common trendsTime-series estimator helps overcome omitted (unobserved) cross-sectional differencesTime-series estimator helps overcome omitted (unobserved) cross-sectional differencesConsider\\(D_i = 1\\) treatment group\\(D_i = 1\\) treatment group\\(D_i = 0\\) control group\\(D_i = 0\\) control group\\(T= 1\\) treatment\\(T= 1\\) treatment\\(T =0\\) treatment\\(T =0\\) treatmentmissing \\(E[Y_{0i}(1)|D=1]\\)Average Treatment Effect Treated (ATT)\\[\n\\begin{aligned}\nE[Y_1(1) - Y_0(1)|D=1] &= \\{E[Y(1)|D=1] - E[Y(1)|D=0] \\} \\\\\n&- \\{E[Y(0)|D=1] - E[Y(0)|D=0] \\}\n\\end{aligned}\n\\]elaboration:treatment group, isolate difference treated treated. untreated group affected different way, design estimate tell us nothing.Alternatively, can’t observe treatment variation control group, can’t say anything treatment effect group.ExtensionMore 2 groups (multiple treatments multiple controls), 2 period (pre post)\\[\nY_{igt} = \\alpha_g + \\gamma_t + \\beta I_{gt} + \\delta X_{igt} + \\epsilon_{igt}\n\\]\\(\\alpha_g\\) group-specific fixed effect\\(\\alpha_g\\) group-specific fixed effect\\(\\gamma_t\\) = time specific fixed effect\\(\\gamma_t\\) = time specific fixed effect\\(\\beta\\) = dif--dif effect\\(\\beta\\) = dif--dif effect\\(I_{gt}\\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity time)\\(I_{gt}\\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity time)specification “two-way fixed effects ” - TWFE (.e., 2 sets fixed effects: group + time).However, Staggered Dif-n-dif (.e., treatment applied different times different groups). TWFE really bad.Long-term EffectsTo examine dynamic treatment effects (rollout/staggered design), can create centered time variable,Last period right treatment periodRemember use period reference groupBy interacting factor variable, can examine dynamic effect treatment (.e., whether ’s fading intensifying)\\[\n\\begin{aligned}\nY &= \\alpha_0 + \\alpha_1 Group + \\alpha_2 Time  \\\\\n&+ \\beta_{-T_1} Treatment+  \\beta_{-(T_1 -1)} Treatment + \\dots +  \\beta_{-1} Treatment \\\\\n&+ \\beta_1 + \\dots + \\beta_{T_2} Treatment\n\\end{aligned}\n\\]\\(\\beta_0\\) used reference group (.e., drop model)\\(\\beta_0\\) used reference group (.e., drop model)\\(T_1\\) pre-treatment period\\(T_1\\) pre-treatment period\\(T_2\\) post-treatment period\\(T_2\\) post-treatment periodWith variables (.e., interaction terms), coefficients estimates can less precise (.e., higher SE).relationship, levels. Technically, can apply research design variables, also coefficients estimates regression models policy implemented.Goal:Pre-treatment coefficients non-significant \\(\\beta_{-T_1}, \\dots, \\beta_{-1} = 0\\) (similar Placebo Test)Post-treatment coefficients expected significant \\(\\beta_1, \\dots, \\beta_{T_2} \\neq0\\)\ncan now examine trend post-treatment coefficients (.e., increasing decreasing)\ncan now examine trend post-treatment coefficients (.e., increasing decreasing)","code":"\nlibrary(tidyverse)\nlibrary(fixest)\n\nod <- causaldata::organ_donations %>%\n    \n    # Treatment variable\n    dplyr::mutate(California = State == 'California') %>%\n    # centered time variable\n    dplyr::mutate(center_time = as.factor(Quarter_Num - 3))  \n# where 3 is the reference period precedes the treatment period\n\nclass(od$California)\n#> [1] \"logical\"\nclass(od$State)\n#> [1] \"character\"\n\ncali <- feols(Rate ~ i(center_time, California, ref = 0) |\n                  State + center_time,\n              data = od)\n\netable(cali)\n#>                                              cali\n#> Dependent Var.:                              Rate\n#>                                                  \n#> California x center_time = -2    -0.0029 (0.0051)\n#> California x center_time = -1   0.0063** (0.0023)\n#> California x center_time = 1  -0.0216*** (0.0050)\n#> California x center_time = 2  -0.0203*** (0.0045)\n#> California x center_time = 3    -0.0222* (0.0100)\n#> Fixed-Effects:                -------------------\n#> State                                         Yes\n#> center_time                                   Yes\n#> _____________________________ ___________________\n#> S.E.: Clustered                         by: State\n#> Observations                                  162\n#> R2                                        0.97934\n#> Within R2                                 0.00979\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\niplot(cali, pt.join = T)\ncoefplot(cali)"},{"path":"difference-in-differences.html","id":"notes-1","chapter":"26 Difference-in-differences","heading":"26.3 Notes","text":"Matching Methods\nMatch treatment control based pre-treatment observables\nModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effect\nWhereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.\ncareful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.\nMatching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.\nMatching MethodsMatch treatment control based pre-treatment observablesMatch treatment control based pre-treatment observablesModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effectModify SEs appropriately (James J. Heckman, Ichimura, Todd 1997). ’s might easier just use Doubly Robust (Sant’Anna Zhao 2020) just need either matching regression work order identify treatment effectWhereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.Whereas group fixed effects control group time-invariant effects, control selection bias (.e., certain groups likely treated others). Hence, backdoor open (.e., selection bias) (1) propensity treated (2) dynamics evolution outcome post-treatment, matching can potential close backdoor.careful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.careful matching time-varying covariates might encounter “regression mean” problem, pre-treatment periods can unusually bad good time (ordinary), post-treatment period outcome can just artifact regression mean (Daw Hatfield 2018). problem concern time-invariant variables.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.’s always good show results without controls \ncontrols fixed within group within time, absorbed fixed effects\ncontrols dynamic across group across, parallel trends assumption plausible.\n’s always good show results without controls becauseIf controls fixed within group within time, absorbed fixed effectsIf controls fixed within group within time, absorbed fixed effectsIf controls dynamic across group across, parallel trends assumption plausible.controls dynamic across group across, parallel trends assumption plausible.causal inference, \\(R^2\\) important.causal inference, \\(R^2\\) important.count data, one can use fixed-effects Poisson pseudo-maximum likelihood estimator (PPML) Puhani (2012) (applied papers, see Burtch, Carnahan, Greenwood (2018) management C. et al. (2021) marketing). also allows robust standard errors -dispersion (Wooldridge 1999).estimator outperforms log OLS data many 0s(Silva Tenreyro 2011), since log-OLS can produce biased estimates (O’Hara Kotze 2010) heteroskedascity (Silva Tenreyro 2006).estimator outperforms log OLS data many 0s(Silva Tenreyro 2011), since log-OLS can produce biased estimates (O’Hara Kotze 2010) heteroskedascity (Silva Tenreyro 2006).thinking negative binomial fixed effects, isn’t estimator right now (Allison Waterman 2002).thinking negative binomial fixed effects, isn’t estimator right now (Allison Waterman 2002).[Zero-valued Outcomes], distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). Alternatively, can either focus onProportional treatment effects: \\(\\theta_{ATT\\%} = \\frac{E(Y_{}(1) | D_i = 1, Post_t = 1) - E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) | D_i = 1 , Post_t = 1}\\) (.e., percentage change treated group’s average post-treatment outcome). Instead relying parallel trends assumption levels, also rely parallel trends assumption ratio (Wooldridge 2023).\ncan use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).\nexamine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).\nEven see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).\nProportional treatment effects: \\(\\theta_{ATT\\%} = \\frac{E(Y_{}(1) | D_i = 1, Post_t = 1) - E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) | D_i = 1 , Post_t = 1}\\) (.e., percentage change treated group’s average post-treatment outcome). Instead relying parallel trends assumption levels, also rely parallel trends assumption ratio (Wooldridge 2023).can use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).can use Poisson QMLE estimate treatment effect: \\(Y_{} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{}) \\epsilon_{}\\) \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\).examine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).examine parallel trends assumption ratio holds, can also estimate dynamic version Poisson QMLE: \\(Y_{} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) \\(r < 0\\).Even see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).Even see plot coefficients 0, still run sensitivity analysis (Rambachan Roth 2023) examine violation assumption (see Prior Parallel Trends Test).Log Effects Calibrated Extensive-margin value: due problem mean value interpretation proportional treatment effects outcomes heavy-tailed, might interested extensive margin effect. , can explicit model much weight put intensive vs. extensive margin (J. Chen Roth 2023, 39).Log Effects Calibrated Extensive-margin value: due problem mean value interpretation proportional treatment effects outcomes heavy-tailed, might interested extensive margin effect. , can explicit model much weight put intensive vs. extensive margin (J. Chen Roth 2023, 39).Proportional treatment effectsIn example, coefficient significant. However, say ’s significant, can interpret coefficient 3 percent increase posttreatment period due treatment.parallel trend “ratio” version Wooldridge (2023) :\\[\n\\frac{E(Y_{}(0) |D_i = 1, Post_t = 1)}{E(Y_{}(0) |D_i = 1, Post_t = 0)} = \\frac{E(Y_{}(0) |D_i = 0, Post_t = 1)}{E(Y_{}(0) |D_i =0, Post_t = 0)}\n\\]means without treatment, average percentage change mean outcome treated group identical control group.Log Effects Calibrated Extensive-margin valueIf want study treatment effect concave transformation outcome less influenced distribution’s tail, can perform analysis.Steps:Normalize outcomes 1 represents minimum non-zero positve value (.e., divide outcome minimum non-zero positive value).Estimate treatment effects new outcome\\[\nm(y) =\n\\begin{cases}\n\\log(y) & \\text{} y >0 \\\\\n-x & \\text{} y = 0\n\\end{cases}\n\\]choice \\(x\\) depends researcher interested :dynamic treatment effects different hypothesized extensive-margin value \\(x \\(0, .1, .5, 1, 3, 5)\\)first column zero-valued outcome equal \\(y_{min, y>0}\\) (.e., different minimum outcome zero outcome - \\(x = 0\\))particular example, extensive margin increases, see increase effect magnitude. second column assume extensive-margin change 0 \\(y_{min, y >0}\\) equivalent 10 (.e., \\(0.1 \\times 100\\)) log point change along intensive margin.","code":"\nset.seed(123) # For reproducibility\n\nn <- 500 # Number of observations per group (treated and control)\n# Generating IDs for a panel setup\nID <- rep(1:n, times = 2)\n\n# Defining groups and periods\nGroup <- rep(c(\"Control\", \"Treated\"), each = n)\nTime <- rep(c(\"Before\", \"After\"), times = n)\nTreatment <- ifelse(Group == \"Treated\", 1, 0)\nPost <- ifelse(Time == \"After\", 1, 0)\n\n# Step 1: Generate baseline outcomes with a zero-inflated model\nlambda <- 20 # Average rate of occurrence\nzero_inflation <- 0.5 # Proportion of zeros\nY_baseline <-\n    ifelse(runif(2 * n) < zero_inflation, 0, rpois(2 * n, lambda))\n\n# Step 2: Apply DiD treatment effect on the treated group in the post-treatment period\nTreatment_Effect <- Treatment * Post\nY_treatment <-\n    ifelse(Treatment_Effect == 1, rpois(n, lambda = 2), 0)\n\n# Incorporating a simple time trend, ensuring outcomes are non-negative\nTime_Trend <- ifelse(Time == \"After\", rpois(2 * n, lambda = 1), 0)\n\n# Step 3: Combine to get the observed outcomes\nY_observed <- Y_baseline + Y_treatment + Time_Trend\n\n# Ensure no negative outcomes after the time trend\nY_observed <- ifelse(Y_observed < 0, 0, Y_observed)\n\n# Create the final dataset\ndata <-\n    data.frame(\n        ID = ID,\n        Treatment = Treatment,\n        Period = Post,\n        Outcome = Y_observed\n    )\n\n# Viewing the first few rows of the dataset\nhead(data)\n#>   ID Treatment Period Outcome\n#> 1  1         0      0       0\n#> 2  2         0      1      25\n#> 3  3         0      0       0\n#> 4  4         0      1      20\n#> 5  5         0      0      19\n#> 6  6         0      1       0\nlibrary(fixest)\nres_pois <-\n    fepois(Outcome ~ Treatment + Period + Treatment * Period,\n           data = data,\n           vcov = \"hetero\")\netable(res_pois)\n#>                             res_pois\n#> Dependent Var.:              Outcome\n#>                                     \n#> Constant           2.249*** (0.0717)\n#> Treatment           0.1743. (0.0932)\n#> Period               0.0662 (0.0960)\n#> Treatment x Period   0.0314 (0.1249)\n#> __________________ _________________\n#> S.E. type          Heteroskeda.-rob.\n#> Observations                   1,000\n#> Squared Cor.                 0.01148\n#> Pseudo R2                    0.00746\n#> BIC                         15,636.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Average percentage change\nexp(coefficients(res_pois)[\"Treatment:Period\"]) - 1\n#> Treatment:Period \n#>       0.03191643\n\n# SE using delta method\nexp(coefficients(res_pois)[\"Treatment:Period\"]) *\n    sqrt(res_pois$cov.scaled[\"Treatment:Period\", \"Treatment:Period\"])\n#> Treatment:Period \n#>        0.1288596\nlibrary(fixest)\n\nbase_did_log0 <- base_did |> \n    mutate(y = if_else(y > 0, y, 0))\n\nres_pois_es <-\n    fepois(y ~ x1 + i(period, treat, 5) | id + period,\n           data = base_did_log0,\n           vcov = \"hetero\")\n\netable(res_pois_es)\n#>                            res_pois_es\n#> Dependent Var.:                      y\n#>                                       \n#> x1                  0.1895*** (0.0108)\n#> treat x period = 1    -0.2769 (0.3545)\n#> treat x period = 2    -0.2699 (0.3533)\n#> treat x period = 3     0.1737 (0.3520)\n#> treat x period = 4    -0.2381 (0.3249)\n#> treat x period = 6     0.3724 (0.3086)\n#> treat x period = 7    0.7739* (0.3117)\n#> treat x period = 8    0.5028. (0.2962)\n#> treat x period = 9   0.9746** (0.3092)\n#> treat x period = 10  1.310*** (0.3193)\n#> Fixed-Effects:      ------------------\n#> id                                 Yes\n#> period                             Yes\n#> ___________________ __________________\n#> S.E. type           Heteroskedas.-rob.\n#> Observations                     1,080\n#> Squared Cor.                   0.51131\n#> Pseudo R2                      0.34836\n#> BIC                            5,868.8\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\niplot(res_pois_es)\nlibrary(fixest)\nbase_did_log0_cali <- base_did_log0 |> \n    # get min \n    mutate(min_y = min(y[y > 0])) |> \n    \n    # normalized the outcome \n    mutate(y_norm = y / min_y)\n\nmy_regression <-\n    function(x) {\n        base_did_log0_cali <-\n            base_did_log0_cali %>% mutate(my = ifelse(y_norm == 0,-x,\n                                                      log(y_norm)))\n        my_reg <-\n            feols(\n                fml = my ~ x1 + i(period, treat, 5) | id + period,\n                data = base_did_log0_cali,\n                vcov = \"hetero\"\n            )\n        \n        return(my_reg)\n    }\n\nxvec <- c(0, .1, .5, 1, 3)\nreg_list <- purrr::map(.x = xvec, .f = my_regression)\n\n\niplot(reg_list, \n      pt.col =  1:length(xvec),\n      pt.pch = 1:length(xvec))\nlegend(\"topleft\", \n       col = 1:length(xvec),\n       pch = 1:length(xvec),\n       legend = as.character(xvec))\n\n\netable(\n    reg_list,\n    headers = list(\"Extensive-margin value (x)\" = as.character(xvec)),\n    digits = 2,\n    digits.stats = 2\n)\n#>                                   model 1        model 2        model 3\n#> Extensive-margin value (x)              0            0.1            0.5\n#> Dependent Var.:                        my             my             my\n#>                                                                        \n#> x1                         0.43*** (0.02) 0.44*** (0.02) 0.46*** (0.03)\n#> treat x period = 1           -0.92 (0.67)   -0.94 (0.69)    -1.0 (0.73)\n#> treat x period = 2           -0.41 (0.66)   -0.42 (0.67)   -0.43 (0.71)\n#> treat x period = 3           -0.34 (0.67)   -0.35 (0.68)   -0.38 (0.73)\n#> treat x period = 4            -1.0 (0.67)    -1.0 (0.68)    -1.1 (0.73)\n#> treat x period = 6            0.44 (0.66)    0.44 (0.67)    0.45 (0.72)\n#> treat x period = 7            1.1. (0.64)    1.1. (0.65)    1.2. (0.70)\n#> treat x period = 8            1.1. (0.64)    1.1. (0.65)     1.1 (0.69)\n#> treat x period = 9           1.7** (0.65)   1.7** (0.66)    1.8* (0.70)\n#> treat x period = 10         2.4*** (0.62)  2.4*** (0.63)  2.5*** (0.68)\n#> Fixed-Effects:             -------------- -------------- --------------\n#> id                                    Yes            Yes            Yes\n#> period                                Yes            Yes            Yes\n#> __________________________ ______________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080          1,080\n#> R2                                   0.43           0.43           0.43\n#> Within R2                            0.26           0.26           0.25\n#> \n#>                                   model 4        model 5\n#> Extensive-margin value (x)              1              3\n#> Dependent Var.:                        my             my\n#>                                                         \n#> x1                         0.49*** (0.03) 0.62*** (0.04)\n#> treat x period = 1            -1.1 (0.79)     -1.5 (1.0)\n#> treat x period = 2           -0.44 (0.77)   -0.51 (0.99)\n#> treat x period = 3           -0.43 (0.78)    -0.60 (1.0)\n#> treat x period = 4            -1.2 (0.78)     -1.5 (1.0)\n#> treat x period = 6            0.45 (0.77)     0.46 (1.0)\n#> treat x period = 7             1.2 (0.75)     1.3 (0.97)\n#> treat x period = 8             1.2 (0.74)     1.3 (0.96)\n#> treat x period = 9            1.8* (0.75)    2.1* (0.97)\n#> treat x period = 10         2.7*** (0.73)  3.2*** (0.94)\n#> Fixed-Effects:             -------------- --------------\n#> id                                    Yes            Yes\n#> period                                Yes            Yes\n#> __________________________ ______________ ______________\n#> S.E. type                  Heterosk.-rob. Heterosk.-rob.\n#> Observations                        1,080          1,080\n#> R2                                   0.42           0.41\n#> Within R2                            0.25           0.24\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"difference-in-differences.html","id":"standard-errors-2","chapter":"26 Difference-in-differences","heading":"26.4 Standard Errors","text":"Serial correlation big problem (Bertrand, Duflo, Mullainathan 2004)often uses long time seriesOutcomes often highly positively serially correlatedMinimal variation treatment variable time within group (e.g., state).overcome problem:Using parametric correction (standard AR correction) good.Using nonparametric (e.g., block bootstrap- keep obs group state together) good number groups large.Remove time series dimension (.e., aggregate data 2 periods: pre post). still works small number groups (See (Donald Lang 2007) notes small-sample aggregation).Empirical arbitrary variance-covariance matrix corrections work large samples.","code":""},{"path":"difference-in-differences.html","id":"examples","chapter":"26 Difference-in-differences","heading":"26.5 Examples","text":"Example Philipp Leppert replicating Card Krueger (1994)Example Anthony Schmidt","code":""},{"path":"difference-in-differences.html","id":"example-by-doleac2020unintended","chapter":"26 Difference-in-differences","heading":"26.5.1 Example by Doleac and Hansen (2020)","text":"purpose banning checking box ex-criminal banned thought gives access felonsThe purpose banning checking box ex-criminal banned thought gives access felonsEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based raceEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based race3 types ban boxPublic employer onlyPrivate employer government contractAll employersMain identification strategyIf county Metropolitan Statistical Area (MSA) adopts ban box, means whole MSA treated. state adopts “ban ban,” every county treatedUnder Simple Dif-n-dif\\[ Y_{} = \\beta_0 + \\beta_1 Post_t + \\beta_2 treat_i + \\beta_2 (Post_t \\times Treat_i) + \\epsilon_{} \\]common post time, use Staggered Dif-n-dif\\[ \\begin{aligned} E_{imrt} &= \\alpha + \\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\\\  &+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + \\delta_m\\times f(t) \\beta_7 + e_{imrt} \\end{aligned} \\]\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., Midwest) ; \\(r\\) = region; \\(t\\) = year\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., Midwest) ; \\(r\\) = region; \\(t\\) = year\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\delta_m\\) = dummy MSI\\(\\delta_m\\) = dummy MSI\\(D_{imt}\\) = control people\\(D_{imt}\\) = control people\\(\\lambda_{rt}\\) = region time fixed effect\\(\\lambda_{rt}\\) = region time fixed effect\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)put \\(\\lambda_r - \\lambda_t\\) (separately) broad fixed effect, \\(\\lambda_{rt}\\) give us deeper narrower fixed effect.running model, drop races. \\(\\beta_1, \\beta_2, \\beta_3\\) collinear interaction terms \\(BTB_{mt}\\)just want estimate model black men, modify \\[ E_{imrt} = \\alpha + BTB_{mt} \\beta_1 + \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt} \\]\\[ \\begin{aligned} E_{imrt} &= \\alpha + BTB_{m (t - 3t)} \\theta_1 + BTB_{m(t-2)} \\theta_2 + BTB_{mt} \\theta_4 \\\\ &+ BTB_{m(t+1)}\\theta_5 + BTB_{m(t+2)}\\theta_6 + BTB_{m(t+3t)}\\theta_7 \\\\ &+ [\\delta_m + D_{imt}\\beta_5 + \\lambda_r + (\\delta_m \\times (f(t))\\beta_7 + e_{imrt}] \\end{aligned} \\]leave \\(BTB_{m(t-1)}\\theta_3\\) category perfect collinearitySo year BTB (\\(\\theta_1, \\theta_2, \\theta_3\\)) similar (.e., pre-trend). Remember, run places BTB.\\(\\theta_2\\) statistically different \\(\\theta_3\\) (baseline), problem, also make sense pre-trend announcement.","code":""},{"path":"difference-in-differences.html","id":"example-from-princeton","chapter":"26 Difference-in-differences","heading":"26.5.2 Example from Princeton","text":"estimate estimatorThe coefficient differences--differences estimator. Treat negative effect","code":"\nlibrary(foreign)\nmydata = read.dta(\"http://dss.princeton.edu/training/Panel101.dta\") %>%\n    # create a dummy variable to indicate the time when the treatment started\n    dplyr::mutate(time = ifelse(year >= 1994, 1, 0)) %>%\n    # create a dummy variable to identify the treatment group\n    dplyr::mutate(treated = ifelse(country == \"E\" |\n                                country == \"F\" | country == \"G\" ,\n                            1,\n                            0)) %>%\n    # create an interaction between time and treated\n    dplyr::mutate(did = time * treated)\ndidreg = lm(y ~ treated + time + did, data = mydata)\nsummary(didreg)\n#> \n#> Call:\n#> lm(formula = y ~ treated + time + did, data = mydata)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -9.768e+09 -1.623e+09  1.167e+08  1.393e+09  6.807e+09 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  3.581e+08  7.382e+08   0.485   0.6292  \n#> treated      1.776e+09  1.128e+09   1.575   0.1200  \n#> time         2.289e+09  9.530e+08   2.402   0.0191 *\n#> did         -2.520e+09  1.456e+09  -1.731   0.0882 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.953e+09 on 66 degrees of freedom\n#> Multiple R-squared:  0.08273,    Adjusted R-squared:  0.04104 \n#> F-statistic: 1.984 on 3 and 66 DF,  p-value: 0.1249"},{"path":"difference-in-differences.html","id":"example-by-card1993minimum","chapter":"26 Difference-in-differences","heading":"26.5.3 Example by Card and Krueger (1993)","text":"found increase minimum wage increases employmentExperimental Setting:New Jersey (treatment) increased minimum wageNew Jersey (treatment) increased minimum wagePenn (control) increase minimum wagePenn (control) increase minimum wagewhereA - B = treatment effect + effect time (additive)- B = treatment effect + effect time (additive)C - D = effect timeC - D = effect time(- B) - (C - D) = dif-n-dif(- B) - (C - D) = dif-n-difThe identifying assumptions:Can’t switchersCan’t switchersPA control group\ngood counter factual\nNJ look like hadn’t treatment\nPA control groupis good counter factualis good counter factualis NJ look like hadn’t treatmentis NJ look like hadn’t treatment\\[\nY_{jt} = \\beta_0 + NJ_j \\beta_1 + POST_t \\beta_2 + (NJ_j \\times POST_t)\\beta_3+ X_{jt}\\beta_4 + \\epsilon_{jt}\n\\]\\(j\\) = restaurant\\(j\\) = restaurant\\(NJ\\) = dummy \\(1 = NJ\\), \\(0 = PA\\)\\(NJ\\) = dummy \\(1 = NJ\\), \\(0 = PA\\)\\(POST\\) = dummy \\(1 = post\\), \\(0 = pre\\)\\(POST\\) = dummy \\(1 = post\\), \\(0 = pre\\)Notes:don’t need \\(\\beta_4\\) model unbiased \\(\\beta_3\\), including give coefficients efficiencyWe don’t need \\(\\beta_4\\) model unbiased \\(\\beta_3\\), including give coefficients efficiencyIf use \\(\\Delta Y_{jt}\\) dependent variable, don’t need \\(POST_t \\beta_2\\) anymoreIf use \\(\\Delta Y_{jt}\\) dependent variable, don’t need \\(POST_t \\beta_2\\) anymoreAlternative model specification authors use NJ high wage restaurant control group (still choose close border)Alternative model specification authors use NJ high wage restaurant control group (still choose close border)reason can’t control everything (PA + NJ high wage) ’s hard interpret causal treatmentThe reason can’t control everything (PA + NJ high wage) ’s hard interpret causal treatmentDif-n-dif utilizes similarity pretrend dependent variables. However, neither necessary sufficient identifying assumption.\n’s sufficient can multiple treatments (technically, include control, treatment can’t interact)\n’s necessary trends can parallel treatment\nDif-n-dif utilizes similarity pretrend dependent variables. However, neither necessary sufficient identifying assumption.’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s necessary trends can parallel treatmentIt’s necessary trends can parallel treatmentHowever, can’t never certain; just try find evidence consistent theory dif-n-dif can work.However, can’t never certain; just try find evidence consistent theory dif-n-dif can work.Notice don’t need treatment levels dependent variable (e.g., wage average NJ PA), dif-n-dif needs pre-trend (.e., slope) two groups.Notice don’t need treatment levels dependent variable (e.g., wage average NJ PA), dif-n-dif needs pre-trend (.e., slope) two groups.","code":""},{"path":"difference-in-differences.html","id":"example-by-butcher2014effects","chapter":"26 Difference-in-differences","heading":"26.5.4 Example by Butcher, McEwan, and Weerapana (2014)","text":"Theory:Highest achieving students usually hard science. ?\nHard give students students benefit doubt hard science\nunpleasant easy get job. Degrees lower market value typically want make feel pleasant\nHighest achieving students usually hard science. ?Hard give students students benefit doubt hard scienceHard give students students benefit doubt hard scienceHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantUnder OLS\\[\nE_{ij} = \\beta_0 + X_i \\beta_1 + G_j \\beta_2 + \\epsilon_{ij}\n\\]\\(X_i\\) = student attributes\\(X_i\\) = student attributes\\(\\beta_2\\) = causal estimate (grade change)\\(\\beta_2\\) = causal estimate (grade change)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(G_j\\) = grade given major \\(j\\)\\(G_j\\) = grade given major \\(j\\)Examine \\(\\hat{\\beta}_2\\)Negative bias: Endogenous response department lower enrollment rate give better gradeNegative bias: Endogenous response department lower enrollment rate give better gradePositive bias: hard science already best students (.e., ability), don’t grades can even lowerPositive bias: hard science already best students (.e., ability), don’t grades can even lowerUnder dif-n-dif\\[\nY_{idt} = \\beta_0 + POST_t \\beta_1 + Treat_d \\beta_2 + (POST_t \\times Treat_d)\\beta_3 + X_{idt} + \\epsilon_{idt}\n\\]\\(Y_{idt}\\) = grade averageA general specification dif-n-dif \\[\nY_{idt} = \\alpha_0 + (POST_t \\times Treat_d) \\alpha_1 + \\theta_d + \\delta_t + X_{idt} + u_{idt}\n\\]\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)","code":""},{"path":"difference-in-differences.html","id":"one-difference","chapter":"26 Difference-in-differences","heading":"26.6 One Difference","text":"regression formula follows (Liaukonytė, Tuchman, Zhu 2023):\\[\ny_{ut} = \\beta \\text{Post}_t + \\gamma_u + \\gamma_w(t) + \\gamma_l + \\gamma_g(u)p(t) + \\epsilon_{ut}\n\\]\\(y_{ut}\\): Outcome interest unit u time t.\\(\\text{Post}_t\\): Dummy variable representing specific post-event period.\\(\\beta\\): Coefficient measuring average change outcome event relative pre-period.\\(\\gamma_u\\): Fixed effects unit.\\(\\gamma_w(t)\\): Time-specific fixed effects account periodic variations.\\(\\gamma_l\\): Dummy variable specific significant period (e.g., major event change).\\(\\gamma_g(u)p(t)\\): Group x period fixed effects flexible trends may vary across different categories (e.g., geographical regions) periods.\\(\\epsilon_{ut}\\): Error term.model can used analyze impact event outcome interest controlling various fixed effects time-specific variations, using units pre-treatment controls.","code":""},{"path":"difference-in-differences.html","id":"two-way-fixed-effects","chapter":"26 Difference-in-differences","heading":"26.7 Two-way Fixed-effects","text":"generalization dif-n-dif model two-way fixed-effects models multiple groups time effects. designed-based, non-parametric causal estimator (Imai Kim 2021)applying TWFE multiple groups multiple periods, supposedly causal coefficient weighted average two-group/two-period estimators data weights can negative. specifically, weights proportional group sizes treatment indicator’s variation pair, units middle panel highest weight.canonical/standard TWFE works whenEffects homogeneous across units across time periods (.e., dynamic changes effects treatment). See (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021) details. Similarly, relies assumption linear additive effects (Imai Kim 2021)\nargue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.\nNeed manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.\nTreatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).\nEffects homogeneous across units across time periods (.e., dynamic changes effects treatment). See (Goodman-Bacon 2021; Clément De Chaisemartin d’Haultfoeuille 2020; L. Sun Abraham 2021; Borusyak, Jaravel, Spiess 2021) details. Similarly, relies assumption linear additive effects (Imai Kim 2021)argue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.argue treatment heterogeneity problem (e.g., plot treatment timing decompose treatment coefficient using Goodman-Bacon Decomposition) know percentage observation never treated (never-treated group increases, bias TWFE decreases, 80% sample never-treated, bias negligible). problem worsen long-run effects.Need manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.Need manually drop two relative time periods everyone eventually treated (avoid multicollinearity). Programs might randomly chooses drop post-treatment period, create biases. choice usually -1, -2 periods.Treatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).Treatment heterogeneity can come (1) might take time treatment measurable changes outcomes (2) period treatment, effect can different (phase increasing effects).2 time periods.2 time periods.Within setting, TWFE works , using baseline (e.g., control units treatment status unchanged across time periods), comparison can beGood \nNewly treated units vs. control\nNewly treated units vs -yet treated\nGood forNewly treated units vs. controlNewly treated units vs. controlNewly treated units vs -yet treatedNewly treated units vs -yet treatedBad \nNewly treated vs. already treated (already treated serve potential outcome newly treated).\nStrict exogeneity (.e., time-varying confounders, feedback past outcome treatment) (Imai Kim 2019)\nSpecific functional forms (.e., treatment effect homogeneity carryover effects anticipation effects) (Imai Kim 2019)\nBad forNewly treated vs. already treated (already treated serve potential outcome newly treated).Strict exogeneity (.e., time-varying confounders, feedback past outcome treatment) (Imai Kim 2019)Specific functional forms (.e., treatment effect homogeneity carryover effects anticipation effects) (Imai Kim 2019)Note: Notation section consistent (2020)\\[\nY_{} = \\alpha_i + \\lambda_t + \\tau W_{} + \\beta X_{} + \\epsilon_{}\n\\]\\(Y_{}\\) outcome\\(Y_{}\\) outcome\\(\\alpha_i\\) unit FE\\(\\alpha_i\\) unit FE\\(\\lambda_t\\) time FE\\(\\lambda_t\\) time FE\\(\\tau\\) causal effect treatment\\(\\tau\\) causal effect treatment\\(W_{}\\) treatment indicator\\(W_{}\\) treatment indicator\\(X_{}\\) covariates\\(X_{}\\) covariatesWhen \\(T = 2\\), TWFE traditional modelUnder following assumption, \\(\\hat{\\tau}_{OLS}\\) unbiased:homogeneous treatment effectparallel trends assumptionslinear additive effects (Imai Kim 2021)Remedies TWFE’s shortcomings(Goodman-Bacon 2021): diagnostic robustness tests TWFE identify influential observations estimate (Goodman-Bacon Decomposition)(Goodman-Bacon 2021): diagnostic robustness tests TWFE identify influential observations estimate (Goodman-Bacon Decomposition)(Callaway Sant’Anna 2021): 2-step estimation bootstrap procedure can account autocorrelation clustering,\nparameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)\nComparing post-treatment outcomes fo groups treated period similar group never treated (using matching).\nTreatment status switch (treated, stay treated rest panel)\nPackage: \n(Callaway Sant’Anna 2021): 2-step estimation bootstrap procedure can account autocorrelation clustering,parameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)parameters interest group-time average treatment effects, group defined first treated (Multiple periods variation treatment timing)Comparing post-treatment outcomes fo groups treated period similar group never treated (using matching).Comparing post-treatment outcomes fo groups treated period similar group never treated (using matching).Treatment status switch (treated, stay treated rest panel)Treatment status switch (treated, stay treated rest panel)Package: didPackage: (L. Sun Abraham 2021): specialization (Callaway Sant’Anna 2021) event-study context.\ninclude lags leads design\ncohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)\npropose “interaction-weighted” estimator.\nPackage: fixest\n(L. Sun Abraham 2021): specialization (Callaway Sant’Anna 2021) event-study context.include lags leads designThey include lags leads designhave cohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)cohort-specific estimates (similar group-time estimates (Callaway Sant’Anna 2021)propose “interaction-weighted” estimator.propose “interaction-weighted” estimator.Package: fixestPackage: fixest(Imai Kim 2021)\nDifferent (Callaway Sant’Anna 2021) allow units switch treatment.\nBased matching methods, weighted TWFE\nPackage: wfe PanelMatch\n(Imai Kim 2021)Different (Callaway Sant’Anna 2021) allow units switch treatment.Different (Callaway Sant’Anna 2021) allow units switch treatment.Based matching methods, weighted TWFEBased matching methods, weighted TWFEPackage: wfe PanelMatchPackage: wfe PanelMatch(Gardner 2022): two-stage \ndid2s\n(Gardner 2022): two-stage DiDdid2sIn cases unaffected unit (.e., never-treated), using exposure-adjusted difference--differences estimators can recover average treatment effect (Clément De Chaisemartin d’Haultfoeuille 2020). However, want see treatment effect heterogeneity (cases true heterogeneous treatment effects vary exposure rate), exposure-adjusted still fails (L. Sun Shapiro 2022).cases unaffected unit (.e., never-treated), using exposure-adjusted difference--differences estimators can recover average treatment effect (Clément De Chaisemartin d’Haultfoeuille 2020). However, want see treatment effect heterogeneity (cases true heterogeneous treatment effects vary exposure rate), exposure-adjusted still fails (L. Sun Shapiro 2022).(2020): see (2020): see belowTo robust againsttime- unit-varying effectsWe can use reshaped inverse probability weighting (RIPW)- TWFE estimatorWith following assumptions:SUTVASUTVABinary treatment: \\(\\mathbf{W}_i = (W_{i1}, \\dots, W_{})\\) \\(\\mathbf{W}_i \\sim \\mathbf{\\pi}_i\\) generalized propensity score (.e., person treatment likelihood follow \\(\\pi\\) regardless period)Binary treatment: \\(\\mathbf{W}_i = (W_{i1}, \\dots, W_{})\\) \\(\\mathbf{W}_i \\sim \\mathbf{\\pi}_i\\) generalized propensity score (.e., person treatment likelihood follow \\(\\pi\\) regardless period), unit-time specific effect \\(\\tau_{} = Y_{}(1) - Y_{}(0)\\)Doubly Average Treatment Effect (DATE) \\[\n\\tau(\\xi) = \\sum_{T=1}^T \\xi_t \\left(\\frac{1}{n} \\sum_{= 1}^n \\tau_{} \\right)\n\\]\\(\\frac{1}{n} \\sum_{= 1}^n \\tau_{}\\) unweighted effect treatment across units (.e., time-specific ATE).\\(\\frac{1}{n} \\sum_{= 1}^n \\tau_{}\\) unweighted effect treatment across units (.e., time-specific ATE).\\(\\xi = (\\xi_1, \\dots, \\xi_t)\\) user-specific weights time period.\\(\\xi = (\\xi_1, \\dots, \\xi_t)\\) user-specific weights time period.estimand called DATE ’s weighted (averaged) across time units.estimand called DATE ’s weighted (averaged) across time units.special case DATE time unit-weights equal\\[\n\\tau_{eq} = \\frac{1}{nT} \\sum_{t=1}^T \\sum_{= 1}^n \\tau_{}\n\\]Borrowing idea inverse propensity-weighted least squares estimator cross-sectional case reweight objective function via treatment assignment mechanism:\\[\n\\hat{\\tau} \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n (Y_i -\\mu - W_i \\tau)^2 \\frac{1}{\\pi_i (W_i)}\n\\]wherethe first term least squares objectivethe first term least squares objectivethe second term propensity scorethe second term propensity scoreIn panel data case, IPW estimator \\[\n\\hat{\\tau}_{IPW} \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n \\sum_{t =1}^T (Y_{t}-\\alpha_i - \\lambda_t - W_{} \\tau)^2 \\frac{1}{\\pi_i (W_i)}\n\\], DATE users can specify structure time weight, use reshaped IPW estimator (2020)\\[\n\\hat{\\tau}_{RIPW} (\\Pi) \\triangleq \\arg \\min_{\\tau} \\sum_{= 1}^n \\sum_{t =1}^T (Y_{t}-\\alpha_i - \\lambda_t - W_{} \\tau)^2 \\frac{\\Pi(W_i)}{\\pi_i (W_i)}\n\\]’s function data-independent distribution \\(\\Pi\\) depends support treatment path \\(\\mathbb{S} = \\cup_i Supp(W_i)\\)generalization can transform toIPW-TWFE estimator \\(\\Pi \\sim Unif(\\mathbb{S})\\)IPW-TWFE estimator \\(\\Pi \\sim Unif(\\mathbb{S})\\)randomized experiment \\(\\Pi = \\pi_i\\)randomized experiment \\(\\Pi = \\pi_i\\)choose \\(\\Pi\\), don’t need data, just need possible assignments setting.practical problems (, staggered, transient), closed form solutionsFor practical problems (, staggered, transient), closed form solutionsFor generic solver, can use nonlinear programming (e..g, BFGS algorithm)generic solver, can use nonlinear programming (e..g, BFGS algorithm)argued (Imai Kim 2021) TWFE non-parametric approach, can subjected incorrect model assumption (.e., model dependence).Hence, advocate matching methods time-series cross-sectional data (Imai Kim 2021)Hence, advocate matching methods time-series cross-sectional data (Imai Kim 2021)Use wfe PanelMatch apply paper.Use wfe PanelMatch apply paper.package based (Somaini Wolak 2016)Standard errors estimation optionsAlternatively, can also manually plm package, careful SEs estimatedAs can see, differences stem SE estimation, coefficient estimate.","code":"\n# dataset\nlibrary(bacondecomp)\ndf <- bacondecomp::castle\n# devtools::install_github(\"paulosomaini/xtreg2way\")\n\nlibrary(xtreg2way)\n# output <- xtreg2way(y,\n#                     data.frame(x1, x2),\n#                     iid,\n#                     tid,\n#                     w,\n#                     noise = \"1\",\n#                     se = \"1\")\n\n# equilvalently\noutput <- xtreg2way(l_homicide ~ post,\n                    df,\n                    iid = df$state, # group id\n                    tid = df$year, # time id\n                    # w, # vector of weight\n                    se = \"1\")\noutput$betaHat\n#>                  [,1]\n#> l_homicide 0.08181162\noutput$aVarHat\n#>             [,1]\n#> [1,] 0.003396724\n\n# to save time, you can use your structure in the \n# last output for a new set of variables\n# output2 <- xtreg2way(y, x1, struc=output$struc)\nlibrary(multiwayvcov) # get vcov matrix \nlibrary(lmtest) # robust SEs estimation\n\n# manual\noutput3 <- lm(l_homicide ~ post + factor(state) + factor(year),\n              data = df)\n\n# get variance-covariance matrix\nvcov_tw <- multiwayvcov::cluster.vcov(output3,\n                        cbind(df$state, df$year),\n                        use_white = F,\n                        df_correction = F)\n\n# get coefficients\ncoeftest(output3, vcov_tw)[2,] \n#>   Estimate Std. Error    t value   Pr(>|t|) \n#> 0.08181162 0.05671410 1.44252696 0.14979397\n# using the plm package\nlibrary(plm)\n\noutput4 <- plm(l_homicide ~ post, \n               data = df, \n               index = c(\"state\", \"year\"), \n               model = \"within\", \n               effect = \"twoways\")\n\n# get coefficients\ncoeftest(output4, vcov = vcovHC, type = \"HC1\")\n#> \n#> t test of coefficients:\n#> \n#>      Estimate Std. Error t value Pr(>|t|)\n#> post 0.081812   0.057748  1.4167   0.1572"},{"path":"difference-in-differences.html","id":"multiple-periods-and-variation-in-treatment-timing","chapter":"26 Difference-in-differences","heading":"26.8 Multiple periods and variation in treatment timing","text":"extension framework settings havemore 2 time periodsmore 2 time periodsdifferent treatment timingdifferent treatment timingWhen treatment effects heterogeneous across time units, standard Two-way Fixed-effects inappropriate.Notation consistent package (Callaway Sant’Anna 2021)\\(Y_{}(0)\\) potential outcome unit \\(\\)\\(Y_{}(0)\\) potential outcome unit \\(\\)\\(Y_{}(g)\\) potential outcome unit \\(\\) time period \\(t\\) ’s treated period \\(g\\)\\(Y_{}(g)\\) potential outcome unit \\(\\) time period \\(t\\) ’s treated period \\(g\\)\\(Y_{}\\) observed outcome unit \\(\\) time period \\(t\\)\\(Y_{}\\) observed outcome unit \\(\\) time period \\(t\\)\\[\nY_{} =\n\\begin{cases}\nY_{} = Y_{}(0) & \\forall \\\\text{never-treated group} \\\\\nY_{} = 1\\{G_i > t\\} Y_{}(0) +  1\\{G_i \\le t \\}Y_{}(G_i) & \\forall \\\\text{groups}\n\\end{cases}\n\\]\\(G_i\\) time period \\(\\) treated\\(G_i\\) time period \\(\\) treated\\(C_i\\) dummy \\(\\) belongs never-treated group\\(C_i\\) dummy \\(\\) belongs never-treated group\\(D_{}\\) dummy whether \\(\\) treated period \\(t\\)\\(D_{}\\) dummy whether \\(\\) treated period \\(t\\)Assumptions:Staggered treatment adoption: treated, unit untreated (revert)Staggered treatment adoption: treated, unit untreated (revert)Parallel trends assumptions (conditional covariates):\nBased never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)\nWithout treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.\n\nBased -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)\n-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)\nAdditional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)\n\nParallel trends assumptions (conditional covariates):Based never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)\nWithout treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.\nBased never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\)Without treatment, average potential outcomes group \\(g\\) equals average potential outcomes never-treated group (.e., control group), means (1) enough data never-treated group (2) control group similar eventually treated group.Based -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)\n-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)\nAdditional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)\nBased -yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\)-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)-yet treated units time \\(s\\) ( \\(s \\ge t\\)) can used comparison groups calculate average treatment effects group first treated time \\(g\\)Additional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)Additional assumption: pre-treatment trends across groups (Marcus Sant’Anna 2021)Random samplingRandom samplingIrreversibility treatment (treated, untreated)Irreversibility treatment (treated, untreated)Overlap (treatment propensity \\(e \\[0,1]\\))Overlap (treatment propensity \\(e \\[0,1]\\))Group-Time ATEThis equivalent average treatment effect standard case (2 groups, 2 periods) multiple time periods.\\[\nATT(g,t) = E[Y_t(g) - Y_t(0) |G = g]\n\\]average treatment effect group \\(g\\) period \\(t\\)Identification: parallel trends assumption based \nNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)\n-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)\nIdentification: parallel trends assumption based onNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\)Identification: parallel trends assumption holds conditional covariates based \nNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)\n-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)\nplausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).\nIdentification: parallel trends assumption holds conditional covariates based onNever-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\)plausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).plausible suspected selection bias can corrected using covariates (.e., much similar matching methods plausible parallel trends).Possible parameters interest :Average treatment effect per group\\[\n\\theta_S(g) = \\frac{1}{\\tau - g + 1} \\sum_{t = 2}^\\tau \\mathbb{1} \\{ \\le t \\} ATT(g,t)\n\\]Average treatment effect across groups (treated) (similar average treatment effect treated canonical case)\\[\n\\theta_S^O := \\sum_{g=2}^\\tau \\theta_S(g) P(G=g)\n\\]Average treatment effect dynamics (.e., average treatment effect groups exposed treatment \\(e\\) time periods):\\[\n\\theta_D(e) := \\sum_{g=2}^\\tau \\mathbb{1} \\{g + e \\le \\tau \\}ATT(g,g + e) P(G = g|G + e \\le \\tau)\n\\]Average treatment effect period \\(t\\) groups treated period \\(t\\))\\[\n\\theta_C(t) = \\sum_{g=2}^\\tau \\mathbb{1}\\{g \\le t\\} ATT(g,t) P(G = g|g \\le t)\n\\]Average treatment effect calendar time\\[\n\\theta_C = \\frac{1}{\\tau-1}\\sum_{t=2}^\\tau \\theta_C(t)\n\\]","code":""},{"path":"difference-in-differences.html","id":"staggered-dif-n-dif","chapter":"26 Difference-in-differences","heading":"26.9 Staggered Dif-n-dif","text":"See Wing et al. (2024) checklist.Recommendations Baker, Larcker, Wang (2022)TWFE regressions suitable single treatment periods treatment effects homogeneous, provided ’s solid rationale effect homogeneity.TWFE regressions suitable single treatment periods treatment effects homogeneous, provided ’s solid rationale effect homogeneity.TWFE staggered , researchers evaluate bias risks, plot treatment timings check variations, use decompositions like Goodman-Bacon (2021) possible. decompositions aren’t feasible (e.g., unbalanced panel), percentage never-treated units can indicate bias severity. Expected treatment effect variability also discussed.TWFE staggered , researchers evaluate bias risks, plot treatment timings check variations, use decompositions like Goodman-Bacon (2021) possible. decompositions aren’t feasible (e.g., unbalanced panel), percentage never-treated units can indicate bias severity. Expected treatment effect variability also discussed.TWFE staggered event studies, avoid binning time periods without evidence uniform effects. Use full relative-time indicators, justify reference periods, wary multicollinearity causing bias.TWFE staggered event studies, avoid binning time periods without evidence uniform effects. Use full relative-time indicators, justify reference periods, wary multicollinearity causing bias.address treatment timing bias concerns, use alternative estimators like stacked regressions, L. Sun Abraham (2021), Callaway Sant’Anna (2021), separate regressions event “clean” controls.address treatment timing bias concerns, use alternative estimators like stacked regressions, L. Sun Abraham (2021), Callaway Sant’Anna (2021), separate regressions event “clean” controls.Justify selection comparison groups (-yet treated, last treated, never treated) ensure parallel-trends assumption holds, especially anticipating effects certain groups.Justify selection comparison groups (-yet treated, last treated, never treated) ensure parallel-trends assumption holds, especially anticipating effects certain groups.Notes:subjects treated different point time (variation treatment timing across units), use staggered (also known event study dynamic ).design treatment applied units exposed treatment time afterward, see (Athey Imbens 2022)example, basic design (Stevenson Wolfers 2006)\\[\n\\begin{aligned}\nY_{} &= \\sum_k \\beta_k Treatment_{}^k + \\sum_i \\eta_i  State_i \\\\\n&+ \\sum_t \\lambda_t Year_t + Controls_{} + \\epsilon_{}\n\\end{aligned}\n\\]\\(Treatment_{}^k\\) series dummy variables equal 1 state \\(\\) treated \\(k\\) years ago period \\(t\\)\\(Treatment_{}^k\\) series dummy variables equal 1 state \\(\\) treated \\(k\\) years ago period \\(t\\)SE usually clustered group level (occasionally time level).SE usually clustered group level (occasionally time level).avoid collinearity, period right treatment usually chosen drop.avoid collinearity, period right treatment usually chosen drop.general form TWFE (L. Sun Abraham 2021):First, define relative period bin indicator \\[\nD_{}^l = \\mathbf{1}(t - E_i = l)\n\\]’s indicator function unit \\(\\) \\(l\\) periods first treatment time \\(t\\)Static specification\\[\nY_{} = \\alpha_i + \\lambda_t + \\mu_g \\sum_{l \\ge0} D_{}^l + \\epsilon_{}\n\\]\\(\\alpha_i\\) unit FE\\(\\alpha_i\\) unit FE\\(\\lambda_t\\) time FE\\(\\lambda_t\\) time FE\\(\\mu_g\\) coefficient interest \\(g = [0,T)\\)\\(\\mu_g\\) coefficient interest \\(g = [0,T)\\)exclude periods first adoption.exclude periods first adoption.Dynamic specification\\[\nY_{} = \\alpha_i + \\lambda_t + \\sum_{\\substack{l = -K \\\\ l \\neq -1}}^{L} \\mu_l D_{}^l + \\epsilon_{}\n\\]exclude relative periods avoid multicollinearity problem (e.g., either period right treatment, treatment period).setting, try show treatment control groups statistically different (.e., coefficient estimates treatment different 0) show pre-treatment parallel trends.However, two-way fixed effects design criticized L. Sun Abraham (2021); Callaway Sant’Anna (2021); Goodman-Bacon (2021). researchers include leads lags treatment see long-term effects treatment, leads lags can biased effects periods, pre-trends can falsely arise due treatment effects heterogeneity.Applying new proposed method, finance accounting researchers find many cases, causal estimates turn null (Baker, Larcker, Wang 2022).Assumptions Staggered DIDRollout Exogeneity (.e., exogeneity treatment adoption): treatment randomly implemented time (.e., unrelated variables also affect dependent variables)\nEvidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)\nEvidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\n\nRollout Exogeneity (.e., exogeneity treatment adoption): treatment randomly implemented time (.e., unrelated variables also affect dependent variables)Evidence: Regress adoption pre-treatment variables. find evidence correlation, include linear trends interacted pre-treatment variables (Hoynes Schanzenbach 2009)Evidence: (Deshpande Li 2019, 223)\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .\nTreatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .\nTreatment random: Regress treatment status unit level pre-treatment observables. predictive treatment status, might argue ’s worry. best, want .Treatment timing random: Conditional treatment, regress timing treatment pre-treatment observables. least, want .confounding eventsNo confounding eventsExclusion restrictions\n-anticipation assumption: future treatment time affect current outcomes\nInvariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.\nExclusion restrictionsNo-anticipation assumption: future treatment time affect current outcomesNo-anticipation assumption: future treatment time affect current outcomesInvariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.Invariance--history assumption: time unit treatment affect outcome (.e., time exposed matter, just whether exposed ). presents causal effect early late adoption outcome.assumptions listed Multiple periods variation treatment timingAnd assumptions listed Multiple periods variation treatment timingAuxiliary assumptions:\nConstant treatment effects across units\nConstant treatment effect time\nRandom sampling\nEffect Additivity\nAuxiliary assumptions:Constant treatment effects across unitsConstant treatment effects across unitsConstant treatment effect timeConstant treatment effect timeRandom samplingRandom samplingEffect AdditivityEffect AdditivityRemedies staggered (Baker, Larcker, Wang 2022):treated cohort compared appropriate controls (-yet-treated, never-treated)\n(Goodman-Bacon 2021)\n(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)\n(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))\n\n(Clément De Chaisemartin d’Haultfoeuille 2020)\n(Borusyak, Jaravel, Spiess 2021)\ntreated cohort compared appropriate controls (-yet-treated, never-treated)(Goodman-Bacon 2021)(Goodman-Bacon 2021)(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)\n(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))\n(Callaway Sant’Anna 2021) consistent average ATT. complicated also flexible (L. Sun Abraham 2021)(L. Sun Abraham 2021) (special case (Callaway Sant’Anna 2021))(Clément De Chaisemartin d’Haultfoeuille 2020)(Clément De Chaisemartin d’Haultfoeuille 2020)(Borusyak, Jaravel, Spiess 2021)(Borusyak, Jaravel, Spiess 2021)Stacked (biased simple):\n(Gormley Matsa 2011)\n(Cengiz et al. 2019)\n(Deshpande Li 2019)\nStacked (biased simple):(Gormley Matsa 2011)(Gormley Matsa 2011)(Cengiz et al. 2019)(Cengiz et al. 2019)(Deshpande Li 2019)(Deshpande Li 2019)","code":""},{"path":"difference-in-differences.html","id":"stacked-did","chapter":"26 Difference-in-differences","heading":"26.9.1 Stacked DID","text":"Notations following slides\\[\nY_{} = \\beta_{FE} D_{} + A_i + B_t + \\epsilon_{}\n\\]\\(A_i\\) group fixed effects\\(A_i\\) group fixed effects\\(B_t\\) period fixed effects\\(B_t\\) period fixed effectsStepsChoose Event WindowEnumerate Sub-experimentsDefine Inclusion CriteriaStack DataSpecify Estimating EquationEvent WindowLet\\(\\kappa_a\\) length pre-event window\\(\\kappa_a\\) length pre-event window\\(\\kappa_b\\) length post-event window\\(\\kappa_b\\) length post-event windowBy setting common event window analysis, essentially exclude events meet criteria.Sub-experimentsLet \\(T_1\\) earliest period dataset\\(T_T\\) last period datasetThen, collection policy adoption periods event window \\[\n\\Omega_A = \\{ A_i |T_1 + \\kappa_a \\le A_i \\le T_T - \\kappa_b\\}\n\\]events existat least \\(\\kappa_a\\) periods earliest periodat least \\(\\kappa_a\\) periods earliest periodat least \\(\\kappa_b\\) periods last periodat least \\(\\kappa_b\\) periods last periodLet \\(d = 1, \\dots, D\\) index column sub-experiments \\(\\Omega_A\\)\\(\\omega_d\\) event date d-th sub-experiment (e.g., \\(\\omega_1\\) = adoption date 1st experiment)Inclusion CriteriaValid treated Units\nWithin sub-experiment \\(d\\), treated units adoption date\nmakes sure unit can serve treated unit 1 sub-experiment\nWithin sub-experiment \\(d\\), treated units adoption dateWithin sub-experiment \\(d\\), treated units adoption dateThis makes sure unit can serve treated unit 1 sub-experimentThis makes sure unit can serve treated unit 1 sub-experimentClean controls\nunits satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment d\nensures controls \nnever treated units\nunits treated far future\n\nunit can control unit multiple sub-experiments (need correct SE)\nunits satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment dOnly units satisfying \\(A_i >\\omega_d + \\kappa_b\\) included controls sub-experiment dThis ensures controls \nnever treated units\nunits treated far future\nensures controls onlynever treated unitsnever treated unitsunits treated far futureunits treated far futureBut unit can control unit multiple sub-experiments (need correct SE)unit can control unit multiple sub-experiments (need correct SE)Valid Time Periods\nobservations within sub-experiment d time periods within sub-experiment’s event window\nensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) included\nobservations within sub-experiment d time periods within sub-experiment’s event windowAll observations within sub-experiment d time periods within sub-experiment’s event windowThis ensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) includedThis ensures sub-experiment d, observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) includedStack DataEstimating Equation\\[\nY_{itd} = \\beta_0 + \\beta_1 T_{id} + \\beta_2 P_{td} + \\beta_3 (T_{id} \\times P_{td}) + \\epsilon_{itd}\n\\]\\(T_{id}\\) = 1 unit \\(\\) treated sub-experiment \\(d\\), 0 control\\(T_{id}\\) = 1 unit \\(\\) treated sub-experiment \\(d\\), 0 control\\(P_{td}\\) = 1 ’s period treatment sub-experiment \\(d\\)\\(P_{td}\\) = 1 ’s period treatment sub-experiment \\(d\\)Equivalently,\\[\nY_{itd} = \\beta_3 (T_{id} \\times P_{td}) + \\theta_{id} + \\gamma_{td} + \\epsilon_{itd}\n\\]\\(\\beta_3\\) averages time-varying effects single number (can’t see time-varying effects)Stacked Event StudyLet \\(YSE_{td} = t - \\omega_d\\) “time since event” variable sub-experiment \\(d\\), \\(YSE_{td} = -\\kappa_a, \\dots, 0, \\dots, \\kappa_b\\) every sub-experimentIn sub-experiment, can fit\\[\nY_{}^d = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j^d \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j^d (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_i^d + \\epsilon_{}^d\n\\]Different set event study coefficients sub-experiment\\[\nY_{itd} = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_{id} + \\epsilon_{itd}\n\\]ClusteringClustered unit x sub-experiment level (Cengiz et al. 2019)Clustered unit x sub-experiment level (Cengiz et al. 2019)Clustered unit level (Deshpande Li 2019)Clustered unit level (Deshpande Li 2019)","code":"\nlibrary(did)\nlibrary(tidyverse)\nlibrary(fixest)\n\ndata(base_stagg)\n\n# first make the stacked datasets\n# get the treatment cohorts\ncohorts <- base_stagg %>%\n    select(year_treated) %>%\n    # exclude never-treated group\n    filter(year_treated != 10000) %>%\n    unique() %>%\n    pull()\n\n# make formula to create the sub-datasets\ngetdata <- function(j, window) {\n    #keep what we need\n    base_stagg %>%\n        # keep treated units and all units not treated within -5 to 5\n        # keep treated units and all units not treated within -window to window\n        filter(year_treated == j | year_treated > j + window) %>%\n        # keep just year -window to window\n        filter(year >= j - window & year <= j + window) %>%\n        # create an indicator for the dataset\n        mutate(df = j)\n}\n\n# get data stacked\nstacked_data <- map_df(cohorts, ~ getdata(., window = 5)) %>%\n    mutate(rel_year = if_else(df == year_treated, time_to_treatment, NA_real_)) %>%\n    fastDummies::dummy_cols(\"rel_year\", ignore_na = TRUE) %>%\n    mutate(across(starts_with(\"rel_year_\"), ~ replace_na(., 0)))\n\n# get stacked value\nstacked <-\n    feols(\n        y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` +\n            `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 +\n            rel_year_4 + rel_year_5 |\n            id ^ df + year ^ df,\n        data = stacked_data\n    )$coefficients\n\nstacked_se = feols(\n    y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` +\n        `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 +\n        rel_year_4 + rel_year_5 |\n        id ^ df + year ^ df,\n    data = stacked_data\n)$se\n\n# add in 0 for omitted -1\nstacked <- c(stacked[1:4], 0, stacked[5:10])\nstacked_se <- c(stacked_se[1:4], 0, stacked_se[5:10])\n\n\ncs_out <- att_gt(\n    yname = \"y\",\n    data = base_stagg,\n    gname = \"year_treated\",\n    idname = \"id\",\n    # xformla = \"~x1\",\n    tname = \"year\"\n)\ncs <-\n    aggte(\n        cs_out,\n        type = \"dynamic\",\n        min_e = -5,\n        max_e = 5,\n        bstrap = FALSE,\n        cband = FALSE\n    )\n\n\n\nres_sa20 = feols(y ~ sunab(year_treated, year) |\n                     id + year, base_stagg)\nsa = tidy(res_sa20)[5:14, ] %>% pull(estimate)\nsa = c(sa[1:4], 0, sa[5:10])\n\nsa_se = tidy(res_sa20)[6:15, ] %>% pull(std.error)\nsa_se = c(sa_se[1:4], 0, sa_se[5:10])\n\ncompare_df_est = data.frame(\n    period = -5:5,\n    cs = cs$att.egt,\n    sa = sa,\n    stacked = stacked\n)\n\ncompare_df_se = data.frame(\n    period = -5:5,\n    cs = cs$se.egt,\n    sa = sa_se,\n    stacked = stacked_se\n)\n\ncompare_df_longer <- compare_df_est %>%\n    pivot_longer(!period, names_to = \"estimator\", values_to = \"est\") %>%\n    \n    full_join(compare_df_se %>% \n                  pivot_longer(!period, names_to = \"estimator\", values_to = \"se\")) %>%\n    \n    mutate(upper = est +  1.96 * se,\n           lower = est - 1.96 * se)\n\n\nggplot(compare_df_longer) +\n    geom_ribbon(aes(\n        x = period,\n        ymin = lower,\n        ymax = upper,\n        group = estimator\n    )) +\n    geom_line(aes(\n        x = period,\n        y = est,\n        group = estimator,\n        col = estimator\n    ),\n    linewidth = 1) + \n    causalverse::ama_theme()"},{"path":"difference-in-differences.html","id":"goodman-bacon-decomposition","chapter":"26 Difference-in-differences","heading":"26.9.2 Goodman-Bacon Decomposition","text":"Paper: (Goodman-Bacon 2021)excellent explanation slides author, seeTakeaways:pairwise (\\(\\tau\\)) gets weight change close middle study windowA pairwise (\\(\\tau\\)) gets weight change close middle study windowA pairwise (\\(\\tau\\)) gets weight includes observations.pairwise (\\(\\tau\\)) gets weight includes observations.Code bacondecomp vignetteTwo-way Fixed effect estimateHence, naive TWFE fixed effect equals weighted average Bacon decomposition (= 0.08).time-varying controls can identify variation within-treatment timing group, ”early vs. late” “late vs. early” estimates collapse just one estimate (.e., treated).","code":"\nlibrary(bacondecomp)\nlibrary(tidyverse)\ndata(\"castle\")\ncastle <- bacondecomp::castle %>% \n    dplyr::select(\"l_homicide\", \"post\", \"state\", \"year\")\nhead(castle)\n#>   l_homicide post   state year\n#> 1   2.027356    0 Alabama 2000\n#> 2   2.164867    0 Alabama 2001\n#> 3   1.936334    0 Alabama 2002\n#> 4   1.919567    0 Alabama 2003\n#> 5   1.749841    0 Alabama 2004\n#> 6   2.130440    0 Alabama 2005\n\n\ndf_bacon <- bacon(\n    l_homicide ~ post,\n    data = castle,\n    id_var = \"state\",\n    time_var = \"year\"\n)\n#>                       type  weight  avg_est\n#> 1 Earlier vs Later Treated 0.05976 -0.00554\n#> 2 Later vs Earlier Treated 0.03190  0.07032\n#> 3     Treated vs Untreated 0.90834  0.08796\n\n# weighted average of the decomposition\nsum(df_bacon$estimate * df_bacon$weight)\n#> [1] 0.08181162\nlibrary(broom)\nfit_tw <- lm(l_homicide ~ post + factor(state) + factor(year),\n             data = bacondecomp::castle)\nhead(tidy(fit_tw))\n#> # A tibble: 6 × 5\n#>   term                    estimate std.error statistic   p.value\n#>   <chr>                      <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)               1.95      0.0624    31.2   2.84e-118\n#> 2 post                      0.0818    0.0317     2.58  1.02e-  2\n#> 3 factor(state)Alaska      -0.373     0.0797    -4.68  3.77e-  6\n#> 4 factor(state)Arizona      0.0158    0.0797     0.198 8.43e-  1\n#> 5 factor(state)Arkansas    -0.118     0.0810    -1.46  1.44e-  1\n#> 6 factor(state)California  -0.108     0.0810    -1.34  1.82e-  1\nlibrary(ggplot2)\n\nggplot(df_bacon) +\n    aes(\n        x = weight,\n        y = estimate,\n        # shape = factor(type),\n        color = type\n    ) +\n    labs(x = \"Weight\", y = \"Estimate\", shape = \"Type\") +\n    geom_point() +\n    causalverse::ama_theme()"},{"path":"difference-in-differences.html","id":"did-with-in-and-out-treatment-condition","chapter":"26 Difference-in-differences","heading":"26.9.3 DID with in and out treatment condition","text":"","code":""},{"path":"difference-in-differences.html","id":"panel-match","chapter":"26 Difference-in-differences","heading":"26.9.3.1 Panel Match","text":"Imai Kim (2021)case generalizes staggered adoption setting, allowing units vary treatment time. \\(N\\) units across \\(T\\) time periods (potentially unbalanced panels), let \\(X_{}\\) represent treatment \\(Y_{}\\) outcome unit \\(\\) time \\(t\\). use two-way linear fixed effects model:\\[\nY_{} = \\alpha_i + \\gamma_t + \\beta X_{} + \\epsilon_{}\n\\]\\(= 1, \\dots, N\\) \\(t = 1, \\dots, T\\). , \\(\\alpha_i\\) \\(\\gamma_t\\) unit time fixed effects. capture time-invariant unit-specific unit-invariant time-specific unobserved confounders, respectively. can express \\(\\alpha_i = h(\\mathbf{U}_i)\\) \\(\\gamma_t = f(\\mathbf{V}_t)\\), \\(\\mathbf{U}_i\\) \\(\\mathbf{V}_t\\) confounders. model doesn’t assume specific form \\(h(.)\\) \\(f(.)\\), ’re additive separable given binary treatment.least squares estimate \\(\\beta\\) leverages covariance outcome treatment (Imai Kim 2021, 406). Specifically, uses within-unit within-time variations. Many researchers prefer two fixed effects (2FE) estimator adjusts types unobserved confounders without specific functional-form assumptions, wrong (Imai Kim 2019). need functional-form assumption (.e., linearity assumption) 2FE work (Imai Kim 2021, 406)Two-Way Matching Estimator:\ncan lead mismatches; units treatment status get matched estimating counterfactual outcomes.\nObservations need matched opposite treatment status correct causal effects estimation.\nMismatches can cause attenuation bias.\n2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.\nTwo-Way Matching Estimator:can lead mismatches; units treatment status get matched estimating counterfactual outcomes.can lead mismatches; units treatment status get matched estimating counterfactual outcomes.Observations need matched opposite treatment status correct causal effects estimation.Observations need matched opposite treatment status correct causal effects estimation.Mismatches can cause attenuation bias.Mismatches can cause attenuation bias.2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.2FE estimator adjusts bias using factor \\(K\\), represents net proportion proper matches observations opposite treatment status.Weighting 2FE:\nObservation \\((,t)\\) weighted based often acts control unit.\nweighted 2FE estimator still mismatches, fewer standard 2FE estimator.\nAdjustments made based observations neither belong unit time period matched observation.\nmeans challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.\nWeighting 2FE:Observation \\((,t)\\) weighted based often acts control unit.Observation \\((,t)\\) weighted based often acts control unit.weighted 2FE estimator still mismatches, fewer standard 2FE estimator.weighted 2FE estimator still mismatches, fewer standard 2FE estimator.Adjustments made based observations neither belong unit time period matched observation.Adjustments made based observations neither belong unit time period matched observation.means challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.means challenges adjusting unit-specific time-specific unobserved confounders two-way fixed effect framework.Equivalence & Assumptions:\nEquivalence 2FE estimator estimator dependent linearity assumption.\nmulti-period estimator described average two-time-period, two-group estimators applied changes control treatment.\nEquivalence & Assumptions:Equivalence 2FE estimator estimator dependent linearity assumption.Equivalence 2FE estimator estimator dependent linearity assumption.multi-period estimator described average two-time-period, two-group estimators applied changes control treatment.multi-period estimator described average two-time-period, two-group estimators applied changes control treatment.Comparison :\nsimple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.\ndoesn’t hold multi-period designs units change treatment status multiple times different intervals.\nContrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.\nmulti-period can equivalent weighted 2FE, control observations may negative regression weights.\nComparison :simple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.simple settings (two time periods, treatment given one group second period), standard nonparametric estimator equals 2FE estimator.doesn’t hold multi-period designs units change treatment status multiple times different intervals.doesn’t hold multi-period designs units change treatment status multiple times different intervals.Contrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.Contrary popular belief, unweighted 2FE estimator isn’t generally equivalent multi-period estimator.multi-period can equivalent weighted 2FE, control observations may negative regression weights.multi-period can equivalent weighted 2FE, control observations may negative regression weights.Conclusion:\nJustifying 2FE estimator estimator isn’t warranted without imposing linearity assumption.\nConclusion:Justifying 2FE estimator estimator isn’t warranted without imposing linearity assumption.Application (Imai, Kim, Wang 2021)Matching Methods:\nEnhance validity causal inference.\nReduce model dependence provide intuitive diagnostics (Ho et al. 2007)\nRarely utilized analyzing time series cross-sectional data.\nproposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specified\nBetter synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.\nMatching Methods:Enhance validity causal inference.Enhance validity causal inference.Reduce model dependence provide intuitive diagnostics (Ho et al. 2007)Reduce model dependence provide intuitive diagnostics (Ho et al. 2007)Rarely utilized analyzing time series cross-sectional data.Rarely utilized analyzing time series cross-sectional data.proposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specifiedThe proposed matching estimators robust standard two-way fixed effects estimator, can biased mis-specifiedBetter synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.Better synthetic controls (e.g., (Xu 2017)) needs less data achieve good performance adapt context unit switching treatment status multiple times.Notes:\nPotential carryover effects (treatment may long-term effect), leading post-treatment bias.\nNotes:Potential carryover effects (treatment may long-term effect), leading post-treatment bias.Proposed Approach:\nTreated observations matched control observations units time period treatment history specified number lags.\nStandard matching weighting techniques employed refine matched set.\nApply estimator adjust time trend.\ngoal treated matched control observations similar covariate values.\nProposed Approach:Treated observations matched control observations units time period treatment history specified number lags.Treated observations matched control observations units time period treatment history specified number lags.Standard matching weighting techniques employed refine matched set.Standard matching weighting techniques employed refine matched set.Apply estimator adjust time trend.Apply estimator adjust time trend.goal treated matched control observations similar covariate values.goal treated matched control observations similar covariate values.Assessment:\nquality matches evaluated covariate balancing.\nAssessment:quality matches evaluated covariate balancing.Estimation:\nshort-term long-term average treatment effects treated (ATT) estimated.\nEstimation:short-term long-term average treatment effects treated (ATT) estimated.Treatment Variation plotVisualize variation treatment across space timeVisualize variation treatment across space timeAids discerning whether treatment fluctuates adequately time units variation primarily clustered subset data.Aids discerning whether treatment fluctuates adequately time units variation primarily clustered subset data.Select \\(F\\) (.e., number leads - time periods treatment). Driven authors interested estimating:\\(F = 0\\) contemporaneous effect (short-term effect)\\(F = 0\\) contemporaneous effect (short-term effect)\\(F = n\\) treatment effect outcome two time periods treatment. (cumulative long-term effect)\\(F = n\\) treatment effect outcome two time periods treatment. (cumulative long-term effect)Select \\(L\\) (number lags adjust).Driven identification assumption.Driven identification assumption.Balances bias-variance tradeoff.Balances bias-variance tradeoff.Higher \\(L\\) values increase credibility reduce efficiency limiting potential matches.Higher \\(L\\) values increase credibility reduce efficiency limiting potential matches.Model assumption:spillover effect assumed.spillover effect assumed.Carryover effect allowed \\(L\\) periods.Carryover effect allowed \\(L\\) periods.Potential outcome unit depends neither others’ treatment status past treatment \\(L\\) periods.Potential outcome unit depends neither others’ treatment status past treatment \\(L\\) periods.defining causal quantity parameters \\(L\\) \\(F\\).Focus average treatment effect treatment status change.\\(\\delta(F,L)\\) average causal effect treatment change (ATT), \\(F\\) periods post-treatment, considering treatment history \\(L\\) periods.Causal quantity considers potential future treatment reversals, meaning treatment revert control outcome measurement.Also possible estimate average treatment effect treatment reversal reversed (ART).Choose \\(L,F\\) based specific needs.large \\(L\\) value:\nIncreases credibility limited carryover effect assumption.\nAllows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).\nMight reduce number matches lead less precise estimates.\nlarge \\(L\\) value:Increases credibility limited carryover effect assumption.Increases credibility limited carryover effect assumption.Allows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).Allows past treatments (\\(t−L\\)) influence outcome \\(Y_{,t+F}\\).Might reduce number matches lead less precise estimates.Might reduce number matches lead less precise estimates.Selecting appropriate number lags\nResearchers base choice substantive knowledge.\nSensitivity empirical results choice examined.\nSelecting appropriate number lagsResearchers base choice substantive knowledge.Researchers base choice substantive knowledge.Sensitivity empirical results choice examined.Sensitivity empirical results choice examined.choice \\(F\\) :\nSubstantively motivated.\nDecides whether interest lies short-term long-term causal effects.\nlarge \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.\nchoice \\(F\\) :Substantively motivated.Substantively motivated.Decides whether interest lies short-term long-term causal effects.Decides whether interest lies short-term long-term causal effects.large \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.large \\(F\\) value can complicate causal effect interpretation, especially many units switch treatment status \\(F\\) lead time period.Identification AssumptionParallel trend assumption conditioned treatment, outcome (excluding immediate lag), covariate histories.Parallel trend assumption conditioned treatment, outcome (excluding immediate lag), covariate histories.Doesn’t require strong unconfoundedness assumption.Doesn’t require strong unconfoundedness assumption.account unobserved time-varying confounders.account unobserved time-varying confounders.Essential examine outcome time trends.\nCheck ’re parallel treated matched control units using pre-treatment data\nEssential examine outcome time trends.Check ’re parallel treated matched control units using pre-treatment dataConstructing Matched Sets:\ntreated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).\nMatching based treatment history helps control carryover effects.\nPast treatments often act major confounders, method can correct .\nExact matching time period adjusts time-specific unobserved confounders.\nUnlike staggered adoption methods, units can change treatment status multiple times.\nMatched set allows treatment switching treatment\nConstructing Matched Sets:treated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).treated observation, create matched control units identical treatment history \\(t−L\\) \\(t−1\\).Matching based treatment history helps control carryover effects.Matching based treatment history helps control carryover effects.Past treatments often act major confounders, method can correct .Past treatments often act major confounders, method can correct .Exact matching time period adjusts time-specific unobserved confounders.Exact matching time period adjusts time-specific unobserved confounders.Unlike staggered adoption methods, units can change treatment status multiple times.Unlike staggered adoption methods, units can change treatment status multiple times.Matched set allows treatment switching treatmentMatched set allows treatment switching treatmentRefining Matched Sets:\nInitially, matched sets adjust treatment history.\nParallel trend assumption requires adjustments confounders like past outcomes covariates.\nMatching methods:\nMatch treated observation \\(J\\) control units.\nDistance measures like Mahalanobis distance propensity score can used.\nMatch based estimated propensity score, considering pretreatment covariates.\nRefined matched set selects similar control units based observed confounders.\n\nWeighting methods:\nAssign weight control unit matched set.\nWeights prioritize similar units.\nInverse propensity score weighting method can applied.\nWeighting generalized method matching.\n\nRefining Matched Sets:Initially, matched sets adjust treatment history.Initially, matched sets adjust treatment history.Parallel trend assumption requires adjustments confounders like past outcomes covariates.Parallel trend assumption requires adjustments confounders like past outcomes covariates.Matching methods:\nMatch treated observation \\(J\\) control units.\nDistance measures like Mahalanobis distance propensity score can used.\nMatch based estimated propensity score, considering pretreatment covariates.\nRefined matched set selects similar control units based observed confounders.\nMatching methods:Match treated observation \\(J\\) control units.Match treated observation \\(J\\) control units.Distance measures like Mahalanobis distance propensity score can used.Distance measures like Mahalanobis distance propensity score can used.Match based estimated propensity score, considering pretreatment covariates.Match based estimated propensity score, considering pretreatment covariates.Refined matched set selects similar control units based observed confounders.Refined matched set selects similar control units based observed confounders.Weighting methods:\nAssign weight control unit matched set.\nWeights prioritize similar units.\nInverse propensity score weighting method can applied.\nWeighting generalized method matching.\nWeighting methods:Assign weight control unit matched set.Assign weight control unit matched set.Weights prioritize similar units.Weights prioritize similar units.Inverse propensity score weighting method can applied.Inverse propensity score weighting method can applied.Weighting generalized method matching.Weighting generalized method matching.Difference--Differences Estimator:Using refined matched sets, ATT (Average Treatment Effect Treated) policy change estimated.Using refined matched sets, ATT (Average Treatment Effect Treated) policy change estimated.treated observation, estimate counterfactual outcome using weighted average control units refined set.treated observation, estimate counterfactual outcome using weighted average control units refined set.estimate ATT computed treated observation, averaged across observations.estimate ATT computed treated observation, averaged across observations.noncontemporaneous treatment effects \\(F > 0\\):\nATT doesn’t specify future treatment sequence.\nMatched control units might units receiving treatment time \\(t\\) \\(t + F\\).\ntreated units return control conditions times.\nnoncontemporaneous treatment effects \\(F > 0\\):ATT doesn’t specify future treatment sequence.ATT doesn’t specify future treatment sequence.Matched control units might units receiving treatment time \\(t\\) \\(t + F\\).Matched control units might units receiving treatment time \\(t\\) \\(t + F\\).treated units return control conditions times.treated units return control conditions times.Checking Covariate Balance:proposed methodology offers advantage checking covariate balance treated matched control observations.proposed methodology offers advantage checking covariate balance treated matched control observations.check helps see treated matched control observations comparable respect observed confounders.check helps see treated matched control observations comparable respect observed confounders.matched sets refined, covariate balance examination becomes straightforward.matched sets refined, covariate balance examination becomes straightforward.Examine mean difference covariate treated observation matched controls pretreatment time period.Examine mean difference covariate treated observation matched controls pretreatment time period.Standardize difference using standard deviation covariate across treated observations dataset.Standardize difference using standard deviation covariate across treated observations dataset.Aggregate covariate balance measure across treated observations covariate pretreatment time period.Aggregate covariate balance measure across treated observations covariate pretreatment time period.Examine balance lagged outcome variables multiple pretreatment periods time-varying covariates.\nhelps evaluate validity parallel trend assumption underlying proposed estimator.\nExamine balance lagged outcome variables multiple pretreatment periods time-varying covariates.helps evaluate validity parallel trend assumption underlying proposed estimator.Relations Linear Fixed Effects Regression Estimators:standard estimator equivalent linear two-way fixed effects regression estimator :\ntwo time periods exist.\nTreatment given units exclusively second period.\nstandard estimator equivalent linear two-way fixed effects regression estimator :two time periods exist.two time periods exist.Treatment given units exclusively second period.Treatment given units exclusively second period.equivalence doesn’t extend multiperiod designs, :\ntwo time periods considered.\nUnits might receive treatment multiple times.\nequivalence doesn’t extend multiperiod designs, :two time periods considered.two time periods considered.Units might receive treatment multiple times.Units might receive treatment multiple times.Despite , many researchers relate use two-way fixed effects estimator design.Despite , many researchers relate use two-way fixed effects estimator design.Standard Error Calculation:Approach:\nCondition weights implied matching process.\nweights denote often observation utilized matching (G. W. Imbens Rubin 2015)\nApproach:Condition weights implied matching process.Condition weights implied matching process.weights denote often observation utilized matching (G. W. Imbens Rubin 2015)weights denote often observation utilized matching (G. W. Imbens Rubin 2015)Context:\nAnalogous conditional variance seen regression models.\nResulting standard errors don’t factor uncertainties around matching procedure.\ncan viewed measure uncertainty conditional upon matching process (Ho et al. 2007).\nContext:Analogous conditional variance seen regression models.Analogous conditional variance seen regression models.Resulting standard errors don’t factor uncertainties around matching procedure.Resulting standard errors don’t factor uncertainties around matching procedure.can viewed measure uncertainty conditional upon matching process (Ho et al. 2007).can viewed measure uncertainty conditional upon matching process (Ho et al. 2007).Key Findings:Even conditions favoring OLS, proposed matching estimator displayed higher robustness omitted relevant lags linear regression model fixed effects.Even conditions favoring OLS, proposed matching estimator displayed higher robustness omitted relevant lags linear regression model fixed effects.robustness offered matching came cost - reduced statistical power.robustness offered matching came cost - reduced statistical power.emphasizes classic statistical tradeoff bias (matching advantage) variance (regression models might efficient).emphasizes classic statistical tradeoff bias (matching advantage) variance (regression models might efficient).Data RequirementsThe treatment variable binary:\n0 signifies “assignment” control.\n1 signifies assignment treatment.\ntreatment variable binary:0 signifies “assignment” control.0 signifies “assignment” control.1 signifies assignment treatment.1 signifies assignment treatment.Variables identifying units data must : Numeric integer.Variables identifying units data must : Numeric integer.Variables identifying time periods : Consecutive numeric/integer data.Variables identifying time periods : Consecutive numeric/integer data.Data format requirement: Must provided standard data.frame object.Data format requirement: Must provided standard data.frame object.Basic functions:Utilize treatment histories create matching sets treated control units.Utilize treatment histories create matching sets treated control units.Refine matched sets determining weights control unit set.\nUnits higher weights larger influence estimations.\nRefine matched sets determining weights control unit set.Units higher weights larger influence estimations.Matching Treatment History:Goal match units transitioning untreated treated status control units similar past treatment histories.Goal match units transitioning untreated treated status control units similar past treatment histories.Setting Quantity Interest (qoi =)\natt average treatment effect treated units\natc average treatment effect treatment control units\nart average effect treatment reversal units experience treatment reversal\nate average treatment effect\nSetting Quantity Interest (qoi =)att average treatment effect treated unitsatt average treatment effect treated unitsatc average treatment effect treatment control unitsatc average treatment effect treatment control unitsart average effect treatment reversal units experience treatment reversalart average effect treatment reversal units experience treatment reversalate average treatment effectate average treatment effectControl units treated unit identical treatment histories lag window (1988-1991)set limited first one, can still see exact past histories.Refining Matched Sets\nRefinement involves assigning weights control units.\nUsers must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\n\nRefining Matched SetsRefinement involves assigning weights control units.Refinement involves assigning weights control units.Users must:\nSpecify method calculating unit similarity/distance.\nChoose variables similarity/distance calculations.\nUsers must:Specify method calculating unit similarity/distance.Specify method calculating unit similarity/distance.Choose variables similarity/distance calculations.Choose variables similarity/distance calculations.Select Refinement Method\nUsers determine refinement method via refinement.method argument.\nOptions include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\n\nMethods “match” name Mahalanobis assign equal weights similar control units.\n“Weighting” methods give higher weights control units similar treated units.\nSelect Refinement MethodUsers determine refinement method via refinement.method argument.Users determine refinement method via refinement.method argument.Options include:\nmahalanobis\nps.match\nCBPS.match\nps.weight\nCBPS.weight\nps.msm.weight\nCBPS.msm.weight\nnone\nOptions include:mahalanobismahalanobisps.matchps.matchCBPS.matchCBPS.matchps.weightps.weightCBPS.weightCBPS.weightps.msm.weightps.msm.weightCBPS.msm.weightCBPS.msm.weightnonenoneMethods “match” name Mahalanobis assign equal weights similar control units.Methods “match” name Mahalanobis assign equal weights similar control units.“Weighting” methods give higher weights control units similar treated units.“Weighting” methods give higher weights control units similar treated units.Variable Selection\nUsers need define covariates used covs.formula argument, one-sided formula object.\nVariables right side formula used calculations.\n“Lagged” versions variables can included using format: (lag(name..var, 0:n)).\nVariable SelectionUsers need define covariates used covs.formula argument, one-sided formula object.Users need define covariates used covs.formula argument, one-sided formula object.Variables right side formula used calculations.Variables right side formula used calculations.“Lagged” versions variables can included using format: (lag(name..var, 0:n)).“Lagged” versions variables can included using format: (lag(name..var, 0:n)).Understanding PanelMatch matched.set objects\nPanelMatch function returns PanelMatch object.\ncrucial element within PanelMatch object matched.set object.\nWithin PanelMatch object, matched.set object names like att, art, atc.\nqoi = ate, two matched.set objects: att atc.\nUnderstanding PanelMatch matched.set objectsThe PanelMatch function returns PanelMatch object.PanelMatch function returns PanelMatch object.crucial element within PanelMatch object matched.set object.crucial element within PanelMatch object matched.set object.Within PanelMatch object, matched.set object names like att, art, atc.Within PanelMatch object, matched.set object names like att, art, atc.qoi = ate, two matched.set objects: att atc.qoi = ate, two matched.set objects: att atc.Matched.set Object Details\nmatched.set named list added attributes.\nAttributes include:\nLag\nNames treatment\nUnit time variables\n\nlist entry represents matched set treated control units.\nNaming follows structure: [id variable].[time variable].\nlist element vector control unit ids match treated unit mentioned element name.\nSince ’s matching method, weights given size.match similar control units based distance calculations.\nMatched.set Object Detailsmatched.set named list added attributes.matched.set named list added attributes.Attributes include:\nLag\nNames treatment\nUnit time variables\nAttributes include:LagLagNames treatmentNames treatmentUnit time variablesUnit time variablesEach list entry represents matched set treated control units.list entry represents matched set treated control units.Naming follows structure: [id variable].[time variable].Naming follows structure: [id variable].[time variable].list element vector control unit ids match treated unit mentioned element name.list element vector control unit ids match treated unit mentioned element name.Since ’s matching method, weights given size.match similar control units based distance calculations.Since ’s matching method, weights given size.match similar control units based distance calculations.Visualizing Matched Sets plot methodUsers can visualize distribution matched set sizes.Users can visualize distribution matched set sizes.red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).red line, default, indicates count matched sets treated units matching control units (.e., empty matched sets).Plot adjustments can made using graphics::plot.Plot adjustments can made using graphics::plot.Comparing Methods RefinementUsers encouraged :\nUse substantive knowledge experimentation evaluation.\nConsider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\n\nNote: Large numbers small matched sets can lead larger standard errors estimation stage.\nCovariates aren’t well balanced can lead undesirable comparisons treated control units.\nAspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\n\nUsers encouraged :Use substantive knowledge experimentation evaluation.Use substantive knowledge experimentation evaluation.Consider following configuring PanelMatch:\nnumber matched sets.\nnumber controls matched treated unit.\nAchieving covariate balance.\nConsider following configuring PanelMatch:number matched sets.number matched sets.number controls matched treated unit.number controls matched treated unit.Achieving covariate balance.Achieving covariate balance.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Note: Large numbers small matched sets can lead larger standard errors estimation stage.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Covariates aren’t well balanced can lead undesirable comparisons treated control units.Aspects consider include:\nRefinement method.\nVariables weight calculation.\nSize lag window.\nProcedures addressing missing data (refer match.missing listwise.delete arguments).\nMaximum size matched sets (matching methods).\nAspects consider include:Refinement method.Refinement method.Variables weight calculation.Variables weight calculation.Size lag window.Size lag window.Procedures addressing missing data (refer match.missing listwise.delete arguments).Procedures addressing missing data (refer match.missing listwise.delete arguments).Maximum size matched sets (matching methods).Maximum size matched sets (matching methods).Supportive Features:\nprint, plot, summary methods assist understanding matched sets sizes.\nget_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\n\nSupportive Features:print, plot, summary methods assist understanding matched sets sizes.print, plot, summary methods assist understanding matched sets sizes.get_covariate_balance helps evaluate covariate balance:\nLower values covariate balance calculations preferred.\nget_covariate_balance helps evaluate covariate balance:Lower values covariate balance calculations preferred.get_covariate_balance Function Options:Allows generation plots displaying covariate balance using plot = TRUE.Allows generation plots displaying covariate balance using plot = TRUE.Plots can customized using arguments typically used base R plot method.Plots can customized using arguments typically used base R plot method.Option set use.equal.weights = TRUE :\nObtaining balance unrefined sets.\nFacilitating understanding refinement’s impact.\nOption set use.equal.weights = TRUE :Obtaining balance unrefined sets.Obtaining balance unrefined sets.Facilitating understanding refinement’s impact.Facilitating understanding refinement’s impact.PanelEstimateStandard Error Calculation Methods\ndifferent methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\n\nqoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\n\nStandard Error Calculation MethodsThere different methods available:\nBootstrap (default method 1000 iterations).\nConditional: Assumes independence across units, time.\nUnconditional: Doesn’t make assumptions independence across units time.\ndifferent methods available:Bootstrap (default method 1000 iterations).Bootstrap (default method 1000 iterations).Conditional: Assumes independence across units, time.Conditional: Assumes independence across units, time.Unconditional: Doesn’t make assumptions independence across units time.Unconditional: Doesn’t make assumptions independence across units time.qoi values set att, art, atc (Imai, Kim, Wang 2021):\ncan use analytical methods calculating standard errors, include “conditional” “unconditional” methods.\nqoi values set att, art, atc (Imai, Kim, Wang 2021):can use analytical methods calculating standard errors, include “conditional” “unconditional” methods.Moderating VariablesTo write journal submission, can follow following report:study, closely aligned research (Acemoglu et al. 2019), two key effects democracy economic growth estimated: impact democratization authoritarian reversal. treatment variable, \\(X_{}\\), defined one country \\(\\) democratic year \\(t\\), zero otherwise.Average Treatment Effect Treated (ATT) democratization formulated follows:\\[\n\\begin{aligned}\n\\delta(F, L) &= \\mathbb{E} \\left\\{ Y_{, t + F} (X_{} = 1, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 0, X_{, t - 1} = 0, \\{X_{,t-l}\\}_{l=2}^L) | X_{} = 1, X_{, t - 1} = 0 \\right\\}\n\\end{aligned}\n\\]framework, treated observations countries transition authoritarian regime \\(X_{-1} = 0\\) democratic one \\(X_{} = 1\\). variable \\(F\\) represents number leads, denoting time periods following treatment, \\(L\\) signifies number lags, indicating time periods preceding treatment.ATT authoritarian reversal given :\\[\n\\begin{aligned}\n&\\mathbb{E} \\left[ Y_{, t + F} (X_{} = 0, X_{, t - 1} = 1, \\{ X_{, t - l}\\}_{l=2}^L ) \\right. \\\\\n&\\left. - Y_{, t + F} (X_{} = 1, X_{-1} = 1, \\{X_{, t - l} \\}_{l=2}^L ) | X_{} = 0, X_{, t - 1} = 1 \\right]\n\\end{aligned}\n\\]ATT calculated conditioning 4 years lags (\\(L = 4\\)) 4 years following policy change \\(F = 1, 2, 3, 4\\). Matched sets treated observation constructed based treatment history, number matched control units generally decreasing considering 4-year treatment history compared 1-year history.enhance quality matched sets, methods Mahalanobis distance matching, propensity score matching, propensity score weighting utilized. approaches enable us evaluate effectiveness refinement method. process matching, employ --five --ten matching investigate sensitive empirical results maximum number allowed matches. information refinement process, please see Web AppendixThe Mahalanobis distance expressed specific formula. aim pair treated unit maximum \\(J\\) control units, permitting replacement, denoted \\(| \\mathcal{M}_{} \\le J|\\). average Mahalanobis distance treated control unit time computed :\\[ S_{} (') = \\frac{1}{L} \\sum_{l = 1}^L \\sqrt{(\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})^T \\mathbf{\\Sigma}_{, t - l}^{-1} (\\mathbf{V}_{, t - l} - \\mathbf{V}_{', t -l})} \\]matched control unit \\(' \\\\mathcal{M}_{}\\), \\(\\mathbf{V}_{'}\\) represents time-varying covariates adjust , \\(\\mathbf{\\Sigma}_{'}\\) sample covariance matrix \\(\\mathbf{V}_{'}\\). Essentially, calculate standardized distance using time-varying covariates average across different time intervals.context propensity score matching, employ logistic regression model balanced covariates derive propensity score. Defined conditional likelihood treatment given pre-treatment covariates (Rosenbaum Rubin 1983), propensity score estimated first creating data subset comprised treated matched control units year. logistic regression model fitted follows:\\[ \\begin{aligned} & e_{} (\\{\\mathbf{U}_{, t - l} \\}^L_{l = 1}) \\\\ &= Pr(X_{} = 1| \\mathbf{U}_{, t -1}, \\ldots, \\mathbf{U}_{, t - L}) \\\\ &= \\frac{1}{1 = \\exp(- \\sum_{l = 1}^L \\beta_l^T \\mathbf{U}_{, t - l})} \\end{aligned} \\]\\(\\mathbf{U}_{'} = (X_{'}, \\mathbf{V}_{'}^T)^T\\). Given model, estimated propensity score treated matched control units computed. enables adjustment lagged covariates via matching calculated propensity score, resulting following distance measure:\\[ S_{} (') = | \\text{logit} \\{ \\hat{e}_{} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\} - \\text{logit} \\{ \\hat{e}_{'t}( \\{ \\mathbf{U}_{', t - l} \\}^L_{l = 1})\\} | \\], \\(\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t - l}\\}^L_{l = 1})\\) represents estimated propensity score matched control unit \\(' \\\\mathcal{M}_{}\\).distance measure \\(S_{} (')\\) determined control units original matched set, fine-tune set selecting \\(J\\) closest control units, meet researcher-defined caliper constraint \\(C\\). control units receive zero weight. results refined matched set treated unit \\((, t)\\):\\[ \\mathcal{M}_{}^* = \\{' : ' \\\\mathcal{M}_{}, S_{} (') < C, S_{} \\le S_{}^{(J)}\\} \\]\\(S_{}^{(J)}\\) \\(J\\)th smallest distance among control units original set \\(\\mathcal{M}_{}\\).refinement using weighting, weight assigned control unit \\('\\) matched set corresponding treated unit \\((, t)\\), greater weight accorded similar units. utilize inverse propensity score weighting, based propensity score model mentioned earlier:\\[ w_{}^{'} \\propto \\frac{\\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )}{1 - \\hat{e}_{'t} (\\{ \\mathbf{U}_{, t-l} \\}^L_{l = 1} )} \\]model, \\(\\sum_{' \\\\mathcal{M}_{}} w_{}^{'} = 1\\) \\(w_{}^{'} = 0\\) \\(' \\notin \\mathcal{M}_{}\\). model fitted complete sample treated matched control units.Checking Covariate Balance distinct advantage proposed methodology regression methods ability offers researchers inspect covariate balance treated matched control observations. facilitates evaluation whether treated matched control observations comparable regarding observed confounders. investigate mean difference covariate (e.g., \\(V_{'j}\\), representing \\(j\\)-th variable \\(\\mathbf{V}_{'}\\)) treated observation matched control observation pre-treatment time period (.e., \\(t' < t\\)), standardize difference. given pretreatment time period, adjust standard deviation covariate across treated observations dataset. Thus, mean difference quantified terms standard deviation units. Formally, treated observation \\((,t)\\) \\(D_{} = 1\\), define covariate balance variable \\(j\\) pretreatment time period \\(t - l\\) : \\[\\begin{equation}\nB_{}(j, l) = \\frac{V_{, t- l,j}- \\sum_{' \\\\mathcal{M}_{}}w_{}^{'}V_{', t-l,j}}{\\sqrt{\\frac{1}{N_1 - 1} \\sum_{'=1}^N \\sum_{t' = L+1}^{T-F}D_{'t'}(V_{', t'-l, j} - \\bar{V}_{t' - l, j})^2}}\n\\label{eq:covbalance}\n\\end{equation}\\] \\(N_1 = \\sum_{'= 1}^N \\sum_{t' = L+1}^{T-F} D_{'t'}\\) denotes total number treated observations \\(\\bar{V}_{t-l,j} = \\sum_{=1}^N D_{,t-l,j}/N\\). aggregate covariate balance measure across treated observations covariate pre-treatment time period: \\[\\begin{equation}\n\\bar{B}(j, l) = \\frac{1}{N_1} \\sum_{=1}^N \\sum_{t = L+ 1}^{T-F}D_{} B_{}(j,l)\n\\label{eq:aggbalance}\n\\end{equation}\\] Lastly, evaluate balance lagged outcome variables several pre-treatment periods time-varying covariates. examination aids assessing validity parallel trend assumption integral estimator justification.Figure ??, demonstrate enhancement covariate balance thank refinement matched sets. scatter plot contrasts absolute standardized mean difference, detailed Equation (??), (horizontal axis) (vertical axis) refinement. Points 45-degree line indicate improved standardized mean balance certain time-varying covariates post-refinement. majority variables benefit refinement process. Notably, propensity score weighting (bottom panel) shows significant improvement, whereas Mahalanobis matching (top panel) yields modest improvement.can either sequentaillyor parallelNote: Scatter plots display standardized mean difference covariate \\(j\\) lag year \\(l\\) defined Equation (??) (x-axis) (y-axis) matched set refinement. plot includes varying numbers possible matches matching method. Rows represent different matching/weighting methods, columns indicate adjustments various lag lengths.exportNote: graph displays standardized mean difference, outlined Equation (??), plotted vertical axis across pre-treatment duration four years represented horizontal axis. leftmost column illustrates balance prior refinement, subsequent three columns depict covariate balance post application distinct refinement techniques. individual line signifies balance specific variable pre-treatment phase.red line tradewb blue line lagged outcome variable.Figure ??, observe marked improvement covariate balance due implemented matching procedures pre-treatment period. analysis prioritizes methods adjust time-varying covariates span four years preceding treatment initiation. two rows delineate standardized mean balance treatment modalities, individual lines representing balance covariate.Across scenarios, refinement attributed matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances confounders. degree imbalance remains evident Mahalanobis distance propensity score matching techniques, standardized mean difference lagged outcome remains stable throughout pre-treatment phase. consistency lends credence validity proposed estimator.Estimation ResultsWe now detail estimated ATTs derived matching techniques. Figure offers visual representations impacts treatment initiation (upper panel) treatment reversal (lower panel) outcome variable duration 5 years post-transition, specifically, (F = 0, 1, …, 4). Across five methods (columns), becomes evident point estimates effects associated treatment initiation consistently approximate zero 5-year window. contrast, estimated outcomes treatment reversal notably negative maintain statistical significance refinement techniques initial year transition 1 4 years follow, provided treatment reversal permissible. effects notably pronounced, pointing estimated reduction roughly X% outcome variable.Collectively, findings indicate transition treated state absence doesn’t invariably lead heightened outcome. Instead, transition treated state back absence exerts considerable negative effect outcome variable short intermediate terms. Hence, positive effect treatment (use traditional ) actually driven negative effect treatment reversal.export","code":"\nlibrary(PanelMatch)\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    \n    hide.x.tick.label = TRUE, hide.y.tick.label = TRUE, \n    # dense.plot = TRUE,\n    data = dem\n)\nlibrary(PanelMatch)\n# All examples follow the package's vignette\n# Create the matched sets\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# visualize the treated unit and matched controls\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    data = dem,\n    matched.set = PM.results.none$att[1],\n    # highlight the particular set\n    show.set.only = TRUE\n)\nDisplayTreatment(\n    unit.id = \"wbcode2\",\n    time.id = \"year\",\n    legend.position = \"none\",\n    xlab = \"year\",\n    ylab = \"Country Code\",\n    treatment = \"dem\",\n    data = dem,\n    matched.set = PM.results.none$att[2],\n    # highlight the particular set\n    show.set.only = TRUE\n)\n# PanelMatch without any refinement\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# Extract the matched.set object\nmsets.none <- PM.results.none$att\n\n# PanelMatch with refinement\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\", # use Mahalanobis distance\n        data = dem,\n        match.missing = TRUE,\n        covs.formula = ~ tradewb,\n        size.match = 5,\n        qoi = \"att\" ,\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nmsets.maha <- PM.results.maha$att\n# these 2 should be identical because weights are not shown\nmsets.none |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\nmsets.maha |> head()\n#>   wbcode2 year matched.set.size\n#> 1       4 1992               74\n#> 2       4 1997                2\n#> 3       6 1973               63\n#> 4       6 1983               73\n#> 5       7 1991               81\n#> 6       7 1998                1\n# summary(msets.none)\n# summary(msets.maha)\nplot(msets.none)\nPM.results.none <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"none\",\n        data = dem,\n        match.missing = TRUE,\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPM.results.maha <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\",\n        data = dem,\n        match.missing = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# listwise deletion used for missing data\nPM.results.listwise <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"mahalanobis\",\n        data = dem,\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\n\n# propensity score based weighting method\nPM.results.ps.weight <-\n    PanelMatch(\n        lag = 4,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        refinement.method = \"ps.weight\",\n        data = dem,\n        match.missing = FALSE,\n        listwise.delete = TRUE,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match = 5,\n        qoi = \"att\",\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE\n    )\n\nget_covariate_balance(\n    PM.results.none$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb            y\n#> t_4 -0.07245466  0.291871990\n#> t_3 -0.20930129  0.208654876\n#> t_2 -0.24425207  0.107736647\n#> t_1 -0.10806125 -0.004950238\n\nget_covariate_balance(\n    PM.results.maha$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4  0.04558637 0.09701606\n#> t_3 -0.03312750 0.10844046\n#> t_2 -0.01396793 0.08890753\n#> t_1  0.10474894 0.06618865\n\n\nget_covariate_balance(\n    PM.results.listwise$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4  0.05634922 0.05223623\n#> t_3 -0.01104797 0.05217896\n#> t_2  0.01411473 0.03094133\n#> t_1  0.06850180 0.02092209\n\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = FALSE\n)\n#>         tradewb          y\n#> t_4 0.014362590 0.04035905\n#> t_3 0.005529734 0.04188731\n#> t_2 0.009410044 0.04195008\n#> t_1 0.027907540 0.03975173\n# Use equal weights\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    use.equal.weights = TRUE,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = TRUE,\n    # visualize by setting plot to TRUE\n    ylim = c(-1, 1)\n)\n\n# Compare covariate balance to refined sets\n# See large improvement in balance\nget_covariate_balance(\n    PM.results.ps.weight$att,\n    data = dem,\n    covariates = c(\"tradewb\", \"y\"),\n    plot = TRUE,\n    # visualize by setting plot to TRUE\n    ylim = c(-1, 1)\n)\n\n\nbalance_scatter(\n    matched_set_list = list(PM.results.maha$att,\n                            PM.results.ps.weight$att),\n    data = dem,\n    covariates = c(\"y\", \"tradewb\")\n)\nPE.results <- PanelEstimate(\n    sets              = PM.results.ps.weight,\n    data              = dem,\n    se.method         = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level  = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.6399349 1.0304938 1.3825265 1.7625951 2.1672629\n\n\n# use conditional method\nPE.results <- PanelEstimate(\n    sets             = PM.results.ps.weight,\n    data             = dem,\n    se.method        = \"conditional\",\n    confidence.level = .95\n)\n\n# point estimates\nPE.results[[\"estimates\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846\n\n# standard errors\nPE.results[[\"standard.error\"]]\n#>       t+0       t+1       t+2       t+3       t+4 \n#> 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143\n\nsummary(PE.results)\n#> Weighted Difference-in-Differences with Propensity Score\n#> Matches created with 4 lags\n#> \n#> Standard errors computed with conditional  method\n#> \n#> Estimate of Average Treatment Effect on the Treated (ATT) by Period:\n#> $summary\n#>      estimate std.error       2.5%    97.5%\n#> t+0 0.2609565 0.4844805 -0.6886078 1.210521\n#> t+1 0.9630847 0.8170604 -0.6383243 2.564494\n#> t+2 1.2851017 1.1171942 -0.9045586 3.474762\n#> t+3 1.7370930 1.4116879 -1.0297644 4.503950\n#> t+4 1.4871846 1.7172143 -1.8784937 4.852863\n#> \n#> $lag\n#> [1] 4\n#> \n#> $qoi\n#> [1] \"att\"\n\nplot(PE.results)\n# moderating variable\ndem$moderator <- 0\ndem$moderator <- ifelse(dem$wbcode2 > 100, 1, 2)\n\nPM.results <-\n    PanelMatch(\n        lag                          = 4,\n        time.id                      = \"year\",\n        unit.id                      = \"wbcode2\",\n        treatment                    = \"dem\",\n        refinement.method            = \"mahalanobis\",\n        data                         = dem,\n        match.missing                = TRUE,\n        covs.formula                 = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        size.match                   = 5,\n        qoi                          = \"att\",\n        outcome.var                  = \"y\",\n        lead                         = 0:4,\n        forbid.treatment.reversal    = FALSE,\n        use.diagonal.variance.matrix = TRUE\n    )\nPE.results <-\n    PanelEstimate(sets      = PM.results,\n                  data      = dem,\n                  moderator = \"moderator\")\n\n# Each element in the list corresponds to a level in the moderator\nplot(PE.results[[1]])\n\nplot(PE.results[[2]])\nlibrary(PanelMatch)\nlibrary(causalverse)\n\nrunPanelMatch <- function(method, lag, size.match=NULL, qoi=\"att\") {\n    \n    # Default parameters for PanelMatch\n    common.args <- list(\n        lag = lag,\n        time.id = \"year\",\n        unit.id = \"wbcode2\",\n        treatment = \"dem\",\n        data = dem,\n        covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi = qoi,\n        outcome.var = \"y\",\n        lead = 0:4,\n        forbid.treatment.reversal = FALSE,\n        size.match = size.match  # setting size.match here for all methods\n    )\n    \n    if(method == \"mahalanobis\") {\n        common.args$refinement.method <- \"mahalanobis\"\n        common.args$match.missing <- TRUE\n        common.args$use.diagonal.variance.matrix <- TRUE\n    } else if(method == \"ps.match\") {\n        common.args$refinement.method <- \"ps.match\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    } else if(method == \"ps.weight\") {\n        common.args$refinement.method <- \"ps.weight\"\n        common.args$match.missing <- FALSE\n        common.args$listwise.delete <- TRUE\n    }\n    \n    return(do.call(PanelMatch, common.args))\n}\n\nmethods <- c(\"mahalanobis\", \"ps.match\", \"ps.weight\")\nlags <- c(1, 4)\nsizes <- c(5, 10)\nres_pm <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm[[name]] <- runPanelMatch(method, lag, size)\n        }\n    }\n}\n\n# Now, you can access res_pm using res_pm[[\"mahalanobis.1lag.5m\"]] etc.\n\n# for treatment reversal\nres_pm_rev <- list()\n\nfor(method in methods) {\n    for(lag in lags) {\n        for(size in sizes) {\n            name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n            res_pm_rev[[name]] <- runPanelMatch(method, lag, size, qoi = \"art\")\n        }\n    }\n}\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(cores = 4)\n# Initialize an empty list to store results\nres_pm <- list()\n\n# Replace nested for-loops with foreach\nresults <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <- runPanelMatch(method, lag, size)\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results)) {\n  res_pm[[name]] <- results[[name]]\n}\n\n# Treatment reversal\n# Initialize an empty list to store results\nres_pm_rev <- list()\n\n# Replace nested for-loops with foreach\nresults_rev <-\n  foreach(\n    method = methods,\n    .combine = 'c',\n    .multicombine = TRUE,\n    .packages = c(\"PanelMatch\", \"causalverse\")\n  ) %dopar% {\n    tmp <- list()\n    for (lag in lags) {\n      for (size in sizes) {\n        name <- paste0(method, \".\", lag, \"lag.\", size, \"m\")\n        tmp[[name]] <-\n          runPanelMatch(method, lag, size, qoi = \"art\")\n      }\n    }\n    tmp\n  }\n\n# Collate results\nfor (name in names(results_rev)) {\n  res_pm_rev[[name]] <- results_rev[[name]]\n}\n\n\nstopImplicitCluster()\nlibrary(gridExtra)\n\n# Updated plotting function\ncreate_balance_plot <- function(method, lag, sizes, res_pm, dem) {\n    matched_set_lists <- lapply(sizes, function(size) {\n        res_pm[[paste0(method, \".\", lag, \"lag.\", size, \"m\")]]$att\n    })\n    \n    return(\n        balance_scatter_custom(\n            matched_set_list = matched_set_lists,\n            legend.title = \"Possible Matches\",\n            set.names = as.character(sizes),\n            legend.position = c(0.2, 0.8),\n            \n            # for compiled plot, you don't need x,y, or main labs\n            x.axis.label = \"\",\n            y.axis.label = \"\",\n            main = \"\",\n            data = dem,\n            dot.size = 5,\n            # show.legend = F,\n            them_use = causalverse::ama_theme(base_size = 32),\n            covariates = c(\"y\", \"tradewb\")\n        )\n    )\n}\n\nplots <- list()\n\nfor (method in methods) {\n    for (lag in lags) {\n        plots[[paste0(method, \".\", lag, \"lag\")]] <-\n            create_balance_plot(method, lag, sizes, res_pm, dem)\n    }\n}\n\n# # Arranging plots in a 3x2 grid\n# grid.arrange(plots[[\"mahalanobis.1lag\"]],\n#              plots[[\"mahalanobis.4lag\"]],\n#              plots[[\"ps.match.1lag\"]],\n#              plots[[\"ps.match.4lag\"]],\n#              plots[[\"ps.weight.1lag\"]],\n#              plots[[\"ps.weight.4lag\"]],\n#              ncol=2, nrow=3)\n\n\n# Standardized Mean Difference of Covariates\nlibrary(gridExtra)\nlibrary(grid)\n\n# Create column and row labels using textGrob\ncol_labels <- c(\"1-year Lag\", \"4-year Lag\")\nrow_labels <- c(\"Maha Matching\", \"PS Matching\", \"PS Weigthing\")\n\nmajor.axes.fontsize = 40\nminor.axes.fontsize = 30\n\npng(\n    file.path(getwd(), \"images\", \"did_balance_scatter.png\"),\n    width = 1200,\n    height = 1000\n)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(textGrob(\n        row_labels[1],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"mahalanobis.1lag\"]], plots[[\"mahalanobis.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[2],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.match.1lag\"]], plots[[\"ps.match.4lag\"]]),\n    \n    list(textGrob(\n        row_labels[3],\n        gp = gpar(fontsize = minor.axes.fontsize),\n        rot = 90\n    ), plots[[\"ps.weight.1lag\"]], plots[[\"ps.weight.4lag\"]])\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\ngrid.arrange(\n    grobs = grobs,\n    ncol = 3,\n    nrow = 4,\n    widths = c(0.15, 0.42, 0.42),\n    heights = c(0.15, 0.28, 0.28, 0.28)\n)\n\ngrid.text(\n    \"Before Refinement\",\n    x = 0.5,\n    y = 0.03,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"After Refinement\",\n    x = 0.03,\n    y = 0.5,\n    rot = 90,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ndev.off()\n#> png \n#>   2\n# Step 1: Define configurations\nconfigurations <- list(\n    list(refinement.method = \"none\", qoi = \"att\"),\n    list(refinement.method = \"none\", qoi = \"art\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"att\"),\n    list(refinement.method = \"mahalanobis\", qoi = \"art\"),\n    list(refinement.method = \"ps.match\", qoi = \"att\"),\n    list(refinement.method = \"ps.match\", qoi = \"art\"),\n    list(refinement.method = \"ps.weight\", qoi = \"att\"),\n    list(refinement.method = \"ps.weight\", qoi = \"art\")\n)\n\n# Step 2: Use lapply or loop to generate results\nresults <- lapply(configurations, function(config) {\n    PanelMatch(\n        lag                       = 4,\n        time.id                   = \"year\",\n        unit.id                   = \"wbcode2\",\n        treatment                 = \"dem\",\n        data                      = dem,\n        match.missing             = FALSE,\n        listwise.delete           = TRUE,\n        size.match                = 5,\n        outcome.var               = \"y\",\n        lead                      = 0:4,\n        forbid.treatment.reversal = FALSE,\n        refinement.method         = config$refinement.method,\n        covs.formula              = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)),\n        qoi                       = config$qoi\n    )\n})\n\n# Step 3: Get covariate balance and plot\nplots <- mapply(function(result, config) {\n    df <- get_covariate_balance(\n        if (config$qoi == \"att\")\n            result$att\n        else\n            result$art,\n        data = dem,\n        covariates = c(\"tradewb\", \"y\"),\n        plot = F\n    )\n    causalverse::plot_covariate_balance_pretrend(df, main = \"\", show_legend = F)\n}, results, configurations, SIMPLIFY = FALSE)\n\n# Set names for plots\nnames(plots) <- sapply(configurations, function(config) {\n    paste(config$qoi, config$refinement.method, sep = \".\")\n})\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <-\n    c(\"None\",\n      \"Mahalanobis\",\n      \"Propensity Score Matching\",\n      \"Propensity Score Weighting\")\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_covariate_balance.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n    list(\n        nullGrob(),\n        textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n        textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize))\n    ),\n    \n    list(\n        textGrob(\n            row_labels[1],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$att.none,\n        plots$att.mahalanobis,\n        plots$att.ps.match,\n        plots$att.ps.weight\n    ),\n    \n    list(\n        textGrob(\n            row_labels[2],\n            gp = gpar(fontsize = minor.axes.fontsize),\n            rot = 90\n        ),\n        plots$art.none,\n        plots$art.mahalanobis,\n        plots$art.ps.match,\n        plots$art.ps.weight\n    )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n    grobs   = grobs,\n    ncol    = 5,\n    nrow    = 3,\n    widths  = c(0.1, 0.225, 0.225, 0.225, 0.225),\n    heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n    \"Refinement Methods\",\n    x  = 0.5,\n    y  = 0.01,\n    gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n    \"Quantities of Interest\",\n    x   = 0.02,\n    y   = 0.5,\n    rot = 90,\n    gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_covariate_balance.png\"))\n# sequential\n# Step 1: Apply PanelEstimate function\n\n# Initialize an empty list to store results\nres_est <- vector(\"list\", length(res_pm))\n\n# Iterate over each element in res_pm\nfor (i in 1:length(res_pm)) {\n  res_est[[i]] <- PanelEstimate(\n    res_pm[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est list\n  names(res_est)[i] <- names(res_pm)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function\n\n# Initialize an empty list to store plot results\nres_est_plot <- vector(\"list\", length(res_est))\n\n# Iterate over each element in res_est\nfor (i in 1:length(res_est)) {\n    res_est_plot[[i]] <-\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n    # Transfer the name of the current element to the res_est_plot list\n    names(res_est_plot)[i] <- names(res_est)[i]\n}\n\n# check results\n# res_est_plot$mahalanobis.1lag.5m\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev\n\n# Initialize an empty list to store results\nres_est_rev <- vector(\"list\", length(res_pm_rev))\n\n# Iterate over each element in res_pm_rev\nfor (i in 1:length(res_pm_rev)) {\n  res_est_rev[[i]] <- PanelEstimate(\n    res_pm_rev[[i]],\n    data = dem,\n    se.method = \"bootstrap\",\n    number.iterations = 1000,\n    confidence.level = .95\n  )\n  # Transfer the name of the current element to the res_est_rev list\n  names(res_est_rev)[i] <- names(res_pm_rev)[i]\n}\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev\n\n# Initialize an empty list to store plot results\nres_est_plot_rev <- vector(\"list\", length(res_est_rev))\n\n# Iterate over each element in res_est_rev\nfor (i in 1:length(res_est_rev)) {\n    res_est_plot_rev[[i]] <-\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 14))\n  # Transfer the name of the current element to the res_est_plot_rev list\n  names(res_est_plot_rev)[i] <- names(res_est_rev)[i]\n}\n# parallel\nlibrary(doParallel)\nlibrary(foreach)\n\n# Detect the number of cores to use for parallel processing\nnum_cores <- 4\n\n# Register the parallel backend\ncl <- makeCluster(num_cores)\nregisterDoParallel(cl)\n\n# Step 1: Apply PanelEstimate function in parallel\nres_est <-\n    foreach(i = 1:length(res_pm), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm to res_est\nnames(res_est) <- names(res_pm)\n\n# Step 2: Apply plot_PanelEstimate function in parallel\nres_est_plot <-\n    foreach(\n        i = 1:length(res_est),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est to res_est_plot\nnames(res_est_plot) <- names(res_est)\n\n\n\n# Step 1: Apply PanelEstimate function for res_pm_rev in parallel\nres_est_rev <-\n    foreach(i = 1:length(res_pm_rev), .packages = \"PanelMatch\") %dopar% {\n        PanelEstimate(\n            res_pm_rev[[i]],\n            data = dem,\n            se.method = \"bootstrap\",\n            number.iterations = 1000,\n            confidence.level = .95\n        )\n    }\n\n# Transfer names from res_pm_rev to res_est_rev\nnames(res_est_rev) <- names(res_pm_rev)\n\n# Step 2: Apply plot_PanelEstimate function for res_est_rev in parallel\nres_est_plot_rev <-\n    foreach(\n        i = 1:length(res_est_rev),\n        .packages = c(\"PanelMatch\", \"causalverse\", \"ggplot2\")\n    ) %dopar% {\n        plot_PanelEstimate(res_est_rev[[i]],\n                           main = \"\",\n                           theme_use = causalverse::ama_theme(base_size = 10))\n    }\n\n# Transfer names from res_est_rev to res_est_plot_rev\nnames(res_est_plot_rev) <- names(res_est_rev)\n\n# Stop the cluster\nstopCluster(cl)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Column and row labels\ncol_labels <- c(\"Mahalanobis 5m\", \n                \"Mahalanobis 10m\", \n                \"PS Matching 5m\", \n                \"PS Matching 10m\", \n                \"PS Weighting 5m\")\n\nrow_labels <- c(\"ATT\", \"ART\")\n\n# Specify your desired fontsize for labels\nminor.axes.fontsize <- 16\nmajor.axes.fontsize <- 20\n\npng(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"), width=1200, height=1000)\n\n# Create a list-of-lists, where each inner list represents a row\ngrid_list <- list(\n  list(\n    nullGrob(),\n    textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize)),\n    textGrob(col_labels[5], gp = gpar(fontsize = minor.axes.fontsize))\n  ),\n  \n  list(\n    textGrob(row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot$mahalanobis.1lag.5m,\n    res_est_plot$mahalanobis.1lag.10m,\n    res_est_plot$ps.match.1lag.5m,\n    res_est_plot$ps.match.1lag.10m,\n    res_est_plot$ps.weight.1lag.5m\n  ),\n  \n  list(\n    textGrob(row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90),\n    res_est_plot_rev$mahalanobis.1lag.5m,\n    res_est_plot_rev$mahalanobis.1lag.10m,\n    res_est_plot_rev$ps.match.1lag.5m,\n    res_est_plot_rev$ps.match.1lag.10m,\n    res_est_plot_rev$ps.weight.1lag.5m\n  )\n)\n\n# \"Flatten\" the list-of-lists into a single list of grobs\ngrobs <- do.call(c, grid_list)\n\n# Arrange your plots with text labels\ngrid.arrange(\n  grobs   = grobs,\n  ncol    = 6,\n  nrow    = 3,\n  widths  = c(0.1, 0.18, 0.18, 0.18, 0.18, 0.18),\n  heights = c(0.1, 0.45, 0.45)\n)\n\n# Add main x and y axis titles\ngrid.text(\n  \"Methods\",\n  x  = 0.5,\n  y  = 0.02,\n  gp = gpar(fontsize = major.axes.fontsize)\n)\ngrid.text(\n  \"\",\n  x   = 0.02,\n  y   = 0.5,\n  rot = 90,\n  gp  = gpar(fontsize = major.axes.fontsize)\n)\n\ndev.off()\nlibrary(knitr)\ninclude_graphics(file.path(getwd(), \"images\", \"p_did_est_in_n_out.png\"))"},{"path":"difference-in-differences.html","id":"counterfactual-estimators","chapter":"26 Difference-in-differences","heading":"26.9.3.2 Counterfactual Estimators","text":"Also known imputation approach (Liu, Wang, Xu 2022)class estimator consider observation treatment missing data. Models built using data control units impute conterfactuals treated observations.’s called counterfactual estimators predict outcomes treated observations received treatment.Advantages:\nAvoids negative weights biases using treated observations modeling applying uniform weights.\nSupports various models, including may relax strict exogeneity assumptions.\nAvoids negative weights biases using treated observations modeling applying uniform weights.Supports various models, including may relax strict exogeneity assumptions.Methods including\nFixed-effects conterfactual estimator (FEct) (special case):\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\n\nInteractive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\n\nMatrix completion (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\n\n[Synthetic Controls] (case studies)\nFixed-effects conterfactual estimator (FEct) (special case):\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).\nBased Two-way Fixed-effects, assumes linear additive functional form unobservables based unit time FEs. FEct fixes improper weighting TWFE comparing within matched pair (pair treated observation predicted counterfactual weighted sum untreated observations).Interactive Fixed Effects conterfactual estimator (IFEct) Xu (2017):\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.\nGeneralized Synthetic Controls subset IFEct treatments don’t revert.\nsuspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses factor-augmented models relax strict exogeneity assumption effects unobservables can decomposed unit FE + time FE + unit x time FE.Generalized Synthetic Controls subset IFEct treatments don’t revert.Matrix completion (MC) (Athey et al. 2021):\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.\nlatent factors (unobservables) strong sparse, IFEct outperforms MC.\nGeneralization factor-augmented models. Different IFEct uses hard impute, MC uses soft impute regularize singular values decomposing residual matrix.latent factors (unobservables) strong sparse, IFEct outperforms MC.[Synthetic Controls] (case studies)Identifying Assumptions:Function Form: Additive separability observables, unobservables, idiosyncratic error term.\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).\nHence, models scale dependent (Athey Imbens 2006) (e.g., log-transform outcome can invadiate assumption).Strict Exogeneity: Conditional observables unobservables, potential outcomes independent treatment assignment (.e., baseline quasi-randomization)\n, unobservables = unit + time FEs, assumption parallel trends assumption\n, unobservables = unit + time FEs, assumption parallel trends assumptionLow-dimensional Decomposition (Feasibility Assumption): Unobservable effects can decomposed low-dimension.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.\ncase \\(U_{} = f_t \\times \\lambda_i\\) \\(f_t\\) = common time trend (time FE), \\(\\lambda_i\\) = unit heterogeneity (unit FE). \\(U_{} = f_t \\times \\lambda_i\\) , can satisfy assumption. assumption weaker , allows us control unobservables based data.Estimation Procedure:Using control observations, estimate functions observable unobservable variables (relying Assumptions 1 3).Predict counterfactual outcomes treated unit using obtained functions.Calculate difference treatment effect treated individual.averaging treated individuals, can obtain Average Treatment Effect Treated (ATT).Notes:Use jackknife number treated units small (Liu, Wang, Xu 2022, 166).","code":""},{"path":"difference-in-differences.html","id":"imputation-method","chapter":"26 Difference-in-differences","heading":"26.9.3.2.1 Imputation Method","text":"Liu, Wang, Xu (2022) can also account treatment reversals heterogeneous treatment effects.imputation estimators include[@gardner2022two @borusyak2021revisiting][@gardner2022two @borusyak2021revisiting]N. Brown, Butts, Westerlund (2023)N. Brown, Butts, Westerlund (2023)F-test \\(H_0\\): residual averages pre-treatment periods = 0To see treatment reversal effects","code":"\nlibrary(fect)\n\nPanelMatch::dem\n\nmodel.fect <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        parallel = TRUE,\n        seed = 1234,\n        # twfe\n        force = \"two-way\"\n    )\nprint(model.fect$est.avg)\n\nplot(model.fect)\n\nplot(model.fect, stats = \"F.p\")\nplot(model.fect, stats = \"F.p\", type = 'exit')"},{"path":"difference-in-differences.html","id":"placebo-test","chapter":"26 Difference-in-differences","heading":"26.9.3.2.2 Placebo Test","text":"selecting part data excluding observations within specified range improve model fitting, evaluate whether estimated Average Treatment Effect (ATT) within range significantly differs zero. approach helps us analyze periods treatment.test fails, either functional form strict exogeneity assumption problematic.","code":"\nout.fect.p <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        placeboTest = TRUE,\n        # using 3 periods\n        placebo.period = c(-2, 0)\n    )\nplot(out.fect.p, proportion = 0.1, stats = \"placebo.p\")"},{"path":"difference-in-differences.html","id":"no-carryover-effects-test","chapter":"26 Difference-in-differences","heading":"26.9.3.2.3 (No) Carryover Effects Test","text":"placebo test can adapted assess carryover effects masking several post-treatment periods instead pre-treatment ones. carryover effects present, average prediction error approximate zero. carryover test, set carryoverTest = TRUE. Specify post-treatment period range carryover.period exclude observations model fitting, evaluate estimated ATT significantly deviates zero.Even carryover effects, cases staggered adoption setting, researchers interested cumulative effects, aggregated treatment effects, ’s okay.evidence carryover effects.","code":"\nout.fect.c <-\n    fect(\n        Y = \"y\",\n        D = \"dem\",\n        X = \"tradewb\",\n        data = na.omit(PanelMatch::dem),\n        method = \"fe\",\n        index = c(\"wbcode2\", \"year\"),\n        se = TRUE,\n        carryoverTest = TRUE,\n        # how many periods of carryover\n        carryover.period = c(1, 3)\n    )\nplot(out.fect.c,  stats = \"carryover.p\")"},{"path":"difference-in-differences.html","id":"matrix-completion-1","chapter":"26 Difference-in-differences","heading":"26.9.3.3 Matrix Completion","text":"Applications marketing:Bronnenberg, Dubé, Sanders (2020)estimate average causal effects panel data units exposed treatment intermittently, two literatures pivotal:Unconfoundedness (G. W. Imbens Rubin 2015): Imputes missing potential control outcomes treated units using observed outcomes similar control units previous periods.Unconfoundedness (G. W. Imbens Rubin 2015): Imputes missing potential control outcomes treated units using observed outcomes similar control units previous periods.Synthetic Control (Abadie, Diamond, Hainmueller 2010): Imputes missing control outcomes treated units using weighted averages control units, matching lagged outcomes treated control units.Synthetic Control (Abadie, Diamond, Hainmueller 2010): Imputes missing control outcomes treated units using weighted averages control units, matching lagged outcomes treated control units.exploit missing potential outcomes different assumptions:Unconfoundedness assumes time patterns stable across units.Unconfoundedness assumes time patterns stable across units.Synthetic control assumes unit patterns stable time.Synthetic control assumes unit patterns stable time.regularization applied, approaches applicable similar settings (Athey et al. 2021).Matrix Completion method, nesting , based matrix factorization, focusing imputing missing matrix elements assuming:Complete matrix = low-rank matrix + noise.Missingness completely random.’s distinguished imposing factorization restrictions utilizing regularization define estimator, particularly effective nuclear norm regularizer complex missing patterns (Athey et al. 2021).Contributions Athey et al. (2021) matrix completion include:Recognizing structured missing patterns allowing time correlation, enabling staggered adoption.Modifying estimators unregularized unit time fixed effects.Performing well across various \\(T\\) \\(N\\) sizes, unlike unconfoundedness synthetic control, falter \\(T >> N\\) \\(N >> T\\), respectively.Identifying Assumptions:SUTVA: Potential outcomes indexed unit’s contemporaneous treatment.dynamic effects (’s okay staggered adoption, gives different interpretation estimand).Setup:\\(Y_{}(0)\\) \\(Y_{}(1)\\) represent potential outcomes \\(Y_{}\\).\\(W_{}\\) binary treatment indicator.Aim estimate average effect treated:\\[\n\\tau = \\frac{\\sum_{(,t): W_{} = 1}[Y_{}(1) - Y_{}(0)]}{\\sum_{,t}W_{}}\n\\]observe relevant values \\(Y_{}(1)\\)want impute missing entries \\(Y(0)\\) matrix treated units \\(W_{} = 1\\).Define \\(\\mathcal{M}\\) set pairs indices \\((,t)\\), \\(\\N\\) \\(t \\T\\), corresponding missing entries \\(W_{} = 1\\); \\(\\mathcal{O}\\) set pairs indices corresponding observed entries \\(Y(0)\\) \\(W_{} = 0\\).Data conceptualized two \\(N \\times T\\) matrices, one incomplete one complete:\\[\nY = \\begin{pmatrix}\nY_{11} & Y_{12} & ? & \\cdots & Y_{1T} \\\\\n? & ? & Y_{23} & \\cdots & ? \\\\\nY_{31} & ? & Y_{33} & \\cdots & ? \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nY_{N1} & ? & Y_{N3} & \\cdots & ?\n\\end{pmatrix},\n\\]\\[\nW = \\begin{pmatrix}\n0 & 0 & 1 & \\cdots & 0 \\\\\n1 & 1 & 0 & \\cdots & 1 \\\\\n0 & 1 & 0 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 1 & 0 & \\cdots & 1\n\\end{pmatrix},\n\\]\\[\nW_{} =\n\\begin{cases}\n1 & \\text{} (,t) \\\\mathcal{M}, \\\\\n0 & \\text{} (,t) \\\\mathcal{O},\n\\end{cases}\n\\]indicator event corresponding component \\(Y\\), \\(Y_{}\\), missing.Patterns missing data \\(\\mathbf{Y}\\):Block (treatment) structure 2 special cases\nSingle-treated-period block structure (G. W. Imbens Rubin 2015)\nSingle-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)\nBlock (treatment) structure 2 special casesSingle-treated-period block structure (G. W. Imbens Rubin 2015)Single-treated-period block structure (G. W. Imbens Rubin 2015)Single-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)Single-treated-unit block structure (Abadie, Diamond, Hainmueller 2010)Staggered AdoptionStaggered AdoptionShape matrix \\(\\mathbf{Y}\\):Thin (\\(N >> T\\))Thin (\\(N >> T\\))Fat (\\(T >> N\\))Fat (\\(T >> N\\))Square (\\(N \\approx T\\))Square (\\(N \\approx T\\))Combinations patterns missingness shape create different literatures:Horizontal Regression = Thin matrix + single-treated-period block (focusing cross-section correlation patterns)Horizontal Regression = Thin matrix + single-treated-period block (focusing cross-section correlation patterns)Vertical Regression = Fat matrix + single-treated-unit block (focusing time-series correlation patterns)Vertical Regression = Fat matrix + single-treated-unit block (focusing time-series correlation patterns)TWFE = Square matrixTWFE = Square matrixTo combine, can exploit stable patterns time, across units (e.g., TWFE, interactive FEs matrix completion).factor model\\[\n\\mathbf{Y = UV}^T + \\mathbf{\\epsilon}\n\\]\\(\\mathbf{U}\\) \\(N \\times R\\) \\(\\mathbf{V}\\) \\(T\\times R\\)interactive FE literature focuses fixed number factors \\(R\\) \\(\\mathbf{U, V}\\), matrix completion focuses impute \\(\\mathbf{Y}\\) using forms regularization (e.g., nuclear norm).can also estimate number factors \\(R\\) Moon Weidner (2015)use nuclear norm minimization estimator, must add penalty term regularize objective function. However, , need explicitly estimate time (\\(\\lambda_t\\)) unit (\\(\\mu_i\\)) fixed effects implicitly embedded missing data matrix reduce bias regularization term.Specifically,\\[\nY_{}  =L_{} + \\sum_{p = 1}^P \\sum_{q= 1}^Q X_{ip} H_{pq}Z_{qt} + \\mu_i + \\lambda_t + V_{} \\beta + \\epsilon_{}\n\\]\\(X_{ip}\\) matrix \\(p\\) variables unit \\(\\)\\(X_{ip}\\) matrix \\(p\\) variables unit \\(\\)\\(Z_{qt}\\) matrix \\(q\\) variables time \\(t\\)\\(Z_{qt}\\) matrix \\(q\\) variables time \\(t\\)\\(V_{}\\) matrix time-varying variables.\\(V_{}\\) matrix time-varying variables.Lasso-type \\(l_1\\) norm (\\(||H|| = \\sum_{p = 1}^p \\sum_{q = 1}^Q |H_{pq}|\\)) used shrink \\(H \\0\\)several options regularize \\(L\\):Frobenius (.e., Ridge): informative since imputes missing values 0.Nuclear Norm (.e., Lasso): computationally feasible (using SOFT-IMPUTE algorithm (Mazumder, Hastie, Tibshirani 2010)).Rank (.e., Subset selection): computationally feasibleThis method allows touse covariatesuse covariatesleverage data treated units (can used treatment effect constant pattern missing complex).leverage data treated units (can used treatment effect constant pattern missing complex).autocorrelated errorshave autocorrelated errorshave weighted loss function (.e., take account probability outcomes unit missing)weighted loss function (.e., take account probability outcomes unit missing)","code":""},{"path":"difference-in-differences.html","id":"gardner2022two-and-borusyak2021revisiting","chapter":"26 Difference-in-differences","heading":"26.9.4 Gardner (2022) and Borusyak, Jaravel, and Spiess (2021)","text":"Estimate time unit fixed effects separatelyEstimate time unit fixed effects separatelyKnown imputation method (Borusyak, Jaravel, Spiess 2021) two-stage (Gardner 2022)Known imputation method (Borusyak, Jaravel, Spiess 2021) two-stage (Gardner 2022)Borusyak, Jaravel, Spiess (2021) didimputationThis version currently working","code":"\n# remotes::install_github(\"kylebutts/did2s\")\nlibrary(did2s)\nlibrary(ggplot2)\nlibrary(fixest)\nlibrary(tidyverse)\ndata(base_stagg)\n\n\nest <- did2s(\n    data = base_stagg |> mutate(treat = if_else(time_to_treatment >= 0, 1, 0)),\n    yname = \"y\",\n    first_stage = ~ x1 | id + year,\n    second_stage = ~ i(time_to_treatment, ref = c(-1,-1000)),\n    treatment = \"treat\" ,\n    cluster_var = \"id\"\n)\n\nfixest::esttable(est)\n#>                                       est\n#> Dependent Var.:                         y\n#>                                          \n#> time_to_treatment = -9  0.3518** (0.1332)\n#> time_to_treatment = -8  -0.3130* (0.1213)\n#> time_to_treatment = -7    0.0894 (0.2367)\n#> time_to_treatment = -6    0.0312 (0.2176)\n#> time_to_treatment = -5   -0.2079 (0.1519)\n#> time_to_treatment = -4   -0.1152 (0.1438)\n#> time_to_treatment = -3   -0.0127 (0.1483)\n#> time_to_treatment = -2    0.1503 (0.1440)\n#> time_to_treatment = 0  -5.139*** (0.3680)\n#> time_to_treatment = 1  -3.480*** (0.3784)\n#> time_to_treatment = 2  -2.021*** (0.3055)\n#> time_to_treatment = 3   -0.6965. (0.3947)\n#> time_to_treatment = 4    1.070** (0.3501)\n#> time_to_treatment = 5   2.173*** (0.4456)\n#> time_to_treatment = 6   4.449*** (0.3680)\n#> time_to_treatment = 7   4.864*** (0.3698)\n#> time_to_treatment = 8   6.187*** (0.2702)\n#> ______________________ __________________\n#> S.E. type                          Custom\n#> Observations                          950\n#> R2                                0.62486\n#> Adj. R2                           0.61843\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfixest::iplot(\n    est,\n    main = \"Event study\",\n    xlab = \"Time to treatment\",\n    ref.line = -1\n)\n\ncoefplot(est)\nmult_est <- did2s::event_study(\n    data = fixest::base_stagg |>\n        dplyr::mutate(year_treated = dplyr::if_else(year_treated == 10000, 0, year_treated)),\n    gname = \"year_treated\",\n    idname = \"id\",\n    tname = \"year\",\n    yname = \"y\",\n    estimator = \"all\"\n)\n#> Error in purrr::map(., function(y) { : ℹ In index: 1.\n#> ℹ With name: y.\n#> Caused by error in `.subset2()`:\n#> ! no such index at level 1\ndid2s::plot_event_study(mult_est)\nlibrary(didimputation)\nlibrary(fixest)\ndata(\"base_stagg\")\n\ndid_imputation(\n    data = base_stagg,\n    yname = \"y\",\n    gname = \"year_treated\",\n    tname = \"year\",\n    idname = \"id\"\n)"},{"path":"difference-in-differences.html","id":"de2020two","chapter":"26 Difference-in-differences","heading":"26.9.5 Clément De Chaisemartin and d’Haultfoeuille (2020)","text":"use twowayfeweights GitHub (Clément De Chaisemartin d’Haultfoeuille 2020)Average instant treatment effect changes treatment\nrelaxes -carryover-effect assumption.\nAverage instant treatment effect changes treatmentThis relaxes -carryover-effect assumption.Drawbacks:\nobserve treatment effects manifest time.\nDrawbacks:observe treatment effects manifest time.still isn’t good package estimator.don’t recommend TwoWayFEWeights since gives aggregated average treatment effect post-treatment periods, period.","code":"\n# remotes::install_github(\"shuo-zhang-ucsb/did_multiplegt\") \nlibrary(DIDmultiplegt)\nlibrary(fixest)\nlibrary(tidyverse)\n\ndata(\"base_stagg\")\n\nres <-\n    did_multiplegt(\n        df = base_stagg |>\n            dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n        Y        = \"y\",\n        G        = \"year_treated\",\n        T        = \"year\",\n        D        = \"treatment\",\n        controls = \"x1\",\n        # brep     = 20, # getting SE will take forever\n        placebo  = 5,\n        dynamic  = 5, \n        average_effect = \"simple\"\n    )\n\nhead(res)\n#> $effect\n#> treatment \n#> -5.214207 \n#> \n#> $N_effect\n#> [1] 675\n#> \n#> $N_switchers_effect\n#> [1] 45\n#> \n#> $dynamic_1\n#> [1] -3.63556\n#> \n#> $N_dynamic_1\n#> [1] 580\n#> \n#> $N_switchers_effect_1\n#> [1] 40\nlibrary(TwoWayFEWeights)\n\nres <- twowayfeweights(\n    data = base_stagg |> dplyr::mutate(treatment = dplyr::if_else(time_to_treatment < 0, 0, 1)),\n    Y = \"y\",\n    G = \"year_treated\",\n    T = \"year\",\n    D = \"treatment\", \n    summary_measures = T\n)\n\nprint(res)\n#> Under the common trends assumption, beta estimates a weighted sum of 45 ATTs.\n#> 41 ATTs receive a positive weight, and 4 receive a negative weight.\n#> \n#> ────────────────────────────────────────── \n#> Treat. var: treatment    ATTs    Σ weights \n#> ────────────────────────────────────────── \n#> Positive weights           41       1.0238 \n#> Negative weights            4      -0.0238 \n#> ────────────────────────────────────────── \n#> Total                      45            1 \n#> ──────────────────────────────────────────\n#> \n#> Summary Measures:\n#>   TWFE Coefficient (β_fe): -3.4676\n#>   min σ(Δ) compatible with β_fe and Δ_TR = 0: 4.8357\n#>   min σ(Δ) compatible with β_fe and Δ_TR of a different sign: 36.1549\n#>   Reference: Corollary 1, de Chaisemartin, C and D'Haultfoeuille, X (2020a)\n#> \n#> The development of this package was funded by the European Union (ERC, REALLYCREDIBLE,GA N. 101043899)."},{"path":"difference-in-differences.html","id":"callaway2021difference","chapter":"26 Difference-in-differences","heading":"26.9.6 Callaway and Sant’Anna (2021)","text":"staggered packagestaggered packageGroup-time average treatment effectGroup-time average treatment effectFisher’s Randomization Test (.e., permutation test)\\(H_0\\): \\(TE = 0\\)","code":"\nlibrary(staggered) \nlibrary(fixest)\ndata(\"base_stagg\")\n\n# simple weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245\n\n# cohort weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"cohort\"\n)\n#>    estimate        se se_neyman\n#> 1 -2.724242 0.2701093 0.2701745\n\n# calendar weighted average\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"calendar\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.5861831 0.1768297 0.1770729\n\nres <- staggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"eventstudy\", \n    eventTime = -9:8\n)\nhead(res)\n#>      estimate        se se_neyman eventTime\n#> 1  0.20418779 0.1045821 0.1045821        -9\n#> 2 -0.06215104 0.1669703 0.1670886        -8\n#> 3  0.02744671 0.1413273 0.1420377        -7\n#> 4 -0.02131747 0.2203695 0.2206338        -6\n#> 5 -0.30690897 0.2015697 0.2036412        -5\n#> 6  0.05594029 0.1908101 0.1921745        -4\n\n\nggplot(\n    res |> mutate(\n        ymin_ptwise = estimate + 1.96 * se,\n        ymax_ptwise = estimate - 1.96 * se\n    ),\n    aes(x = eventTime, y = estimate)\n) +\n    geom_pointrange(aes(ymin = ymin_ptwise, ymax = ymax_ptwise)) +\n    geom_hline(yintercept = 0) +\n    xlab(\"Event Time\") +\n    ylab(\"Estimate\") +\n    causalverse::ama_theme()\n# Callaway and Sant'Anna estimator for the simple weighted average\nstaggered_cs(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7994889 0.4484987 0.4486122\n\n# Sun and Abraham estimator for the simple weighted average\nstaggered_sa(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\"\n)\n#>     estimate        se se_neyman\n#> 1 -0.7551901 0.4407818 0.4409525\nstaggered(\n    df = base_stagg,\n    i = \"id\",\n    t = \"year\",\n    g = \"year_treated\",\n    y = \"y\",\n    estimand = \"simple\",\n    compute_fisher = T,\n    num_fisher_permutations = 100\n)\n#>     estimate        se se_neyman fisher_pval fisher_pval_se_neyman\n#> 1 -0.7110941 0.2211943 0.2214245           0                     0\n#>   num_fisher_permutations\n#> 1                     100"},{"path":"difference-in-differences.html","id":"sun2021estimating","chapter":"26 Difference-in-differences","heading":"26.9.7 L. Sun and Abraham (2021)","text":"paper utilizes Cohort Average Treatment Effects Treated (CATT), measures cohort-specific average difference outcomes relative never treated, offering detailed analysis Goodman-Bacon (2021). scenarios lacking never-treated group, method designates last cohort treated control group.Parameter interest cohort-specific ATT \\(l\\) periods int ital treatment period \\(e\\)\\[\nCATT = E[Y_{, e + } - Y_{, e + }^\\infty|E_i = e]\n\\]paper uses interaction-weighted estimator panel data setting, original paper Gibbons, Suárez Serrato, Urbancic (2018) used idea cross-sectional setting.Callaway Sant’Anna (2021) explores group-time average treatment effects, employing cohorts yet treated controls, permits conditioning time-varying covariates.Callaway Sant’Anna (2021) explores group-time average treatment effects, employing cohorts yet treated controls, permits conditioning time-varying covariates.Athey Imbens (2022) examines treatment effect relation counterfactual outcome always-treated group, diverging conventional focus never-treated.Athey Imbens (2022) examines treatment effect relation counterfactual outcome always-treated group, diverging conventional focus never-treated.Borusyak, Jaravel, Spiess (2021) presumes uniform treatment effect across cohorts, effectively simplifying CATT ATT.Borusyak, Jaravel, Spiess (2021) presumes uniform treatment effect across cohorts, effectively simplifying CATT ATT.Identifying Assumptions dynamic TWFE:Parallel Trends: Baseline outcomes follow parallel trends across cohorts treatment.\ngives us CATT (including , included bins, excluded bins)\nParallel Trends: Baseline outcomes follow parallel trends across cohorts treatment.gives us CATT (including , included bins, excluded bins)Anticipatory Behavior: effect treatment pre-treatment periods, indicating outcomes influenced anticipation treatment.Anticipatory Behavior: effect treatment pre-treatment periods, indicating outcomes influenced anticipation treatment.Treatment Effect Homogeneity: treatment effect consistent across cohorts relative period. adoption cohort path treatment effects. words, trajectory treatment cohort similar. Compare designs:\nAthey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.\nBorusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.\nCallaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.\nClement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.\nGoodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.\nL. Sun Abraham (2021) allows treatment effect heterogeneity across units time.\nTreatment Effect Homogeneity: treatment effect consistent across cohorts relative period. adoption cohort path treatment effects. words, trajectory treatment cohort similar. Compare designs:Athey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.Athey Imbens (2022) assume heterogeneity treatment effects vary adoption cohorts, time.Borusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.Borusyak, Jaravel, Spiess (2021) assume heterogeneity treatment effects vary time, adoption cohorts.Callaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.Callaway Sant’Anna (2021) assume heterogeneity treatment effects vary time across cohorts.Clement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.Clement De Chaisemartin D’haultfœuille (2023) assume heterogeneity treatment effects vary across groups time.Goodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.Goodman-Bacon (2021) assume heterogeneity either “vary across units time” “vary time across units”.L. Sun Abraham (2021) allows treatment effect heterogeneity across units time.L. Sun Abraham (2021) allows treatment effect heterogeneity across units time.Sources Heterogeneous Treatment EffectsAdoption cohorts can differ based certain covariates. Similarly, composition units within adoption cohort different.Adoption cohorts can differ based certain covariates. Similarly, composition units within adoption cohort different.response treatment varies among cohorts units self-select initial treatment timing based anticipated treatment effects. However, self-selection still compatible parallel trends assumption. true units choose based evaluation baseline outcomes - , baseline outcomes similar (following parallel trends), might see selection treatment based evaluation baseline outcome.response treatment varies among cohorts units self-select initial treatment timing based anticipated treatment effects. However, self-selection still compatible parallel trends assumption. true units choose based evaluation baseline outcomes - , baseline outcomes similar (following parallel trends), might see selection treatment based evaluation baseline outcome.Treatment effects can vary across cohorts due calendar time-varying effects, changes economic conditions.Treatment effects can vary across cohorts due calendar time-varying effects, changes economic conditions.Notes:TWFE, actually drop 2 terms avoid multicollinearity:\nPeriod right treatment (one known paper)\nDrop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.\nTWFE, actually drop 2 terms avoid multicollinearity:Period right treatment (one known paper)Period right treatment (one known paper)Drop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.Drop bin trim distant lag period (one clarified paper). reason multicollinearity linear relationship TWFE relative period indicators.Contamination treatment effect estimates excluded periods type “normalization”. avoid , assume pre-treatment periods CATT.\nL. Sun Abraham (2021) estimation method gives reasonable weights CATT (..e, weights sum 1, non negative). estimate weighted average CATT weights shares cohorts experience least \\(l\\) periods treatment, normalized size total periods \\(g\\).\nContamination treatment effect estimates excluded periods type “normalization”. avoid , assume pre-treatment periods CATT.L. Sun Abraham (2021) estimation method gives reasonable weights CATT (..e, weights sum 1, non negative). estimate weighted average CATT weights shares cohorts experience least \\(l\\) periods treatment, normalized size total periods \\(g\\).Aggregation CATT similar Callaway Sant’Anna (2021)Aggregation CATT similar Callaway Sant’Anna (2021)Applicationcan use fixest r sunab functionUsing syntax fixest","code":"\nlibrary(fixest)\ndata(\"base_stagg\")\nres_sa20 = feols(y ~ x1 + sunab(year_treated, year) | id + year, base_stagg)\niplot(res_sa20)\n\nsummary(res_sa20, agg = \"att\")\n#> OLS estimation, Dep. Var.: y\n#> Observations: 950 \n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>      Estimate Std. Error  t value  Pr(>|t|)    \n#> x1   0.994678   0.018378 54.12293 < 2.2e-16 ***\n#> ATT -1.133749   0.205070 -5.52858 2.882e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n\nsummary(res_sa20, agg = c(\"att\" = \"year::[^-]\")) \n#> OLS estimation, Dep. Var.: y\n#> Observations: 950 \n#> Fixed-effects: id: 95,  year: 10\n#> Standard-errors: Clustered (id) \n#>                      Estimate Std. Error   t value   Pr(>|t|)    \n#> x1                   0.994678   0.018378 54.122928  < 2.2e-16 ***\n#> year::-9:cohort::10  0.351766   0.359073  0.979649 3.2977e-01    \n#> year::-8:cohort::9   0.033914   0.471437  0.071937 9.4281e-01    \n#> year::-8:cohort::10 -0.191932   0.352896 -0.543876 5.8781e-01    \n#> year::-7:cohort::8  -0.589387   0.736910 -0.799809 4.2584e-01    \n#> year::-7:cohort::9   0.872995   0.493427  1.769249 8.0096e-02 .  \n#> year::-7:cohort::10  0.019512   0.603411  0.032336 9.7427e-01    \n#> year::-6:cohort::7  -0.042147   0.865736 -0.048683 9.6127e-01    \n#> year::-6:cohort::8  -0.657571   0.573257 -1.147078 2.5426e-01    \n#> year::-6:cohort::9   0.877743   0.533331  1.645775 1.0315e-01    \n#> year::-6:cohort::10 -0.403635   0.347412 -1.161832 2.4825e-01    \n#> year::-5:cohort::6  -0.658034   0.913407 -0.720418 4.7306e-01    \n#> year::-5:cohort::7  -0.316974   0.697939 -0.454158 6.5076e-01    \n#> year::-5:cohort::8  -0.238213   0.469744 -0.507113 6.1326e-01    \n#> year::-5:cohort::9   0.301477   0.604201  0.498968 6.1897e-01    \n#> year::-5:cohort::10 -0.564801   0.463214 -1.219308 2.2578e-01    \n#> year::-4:cohort::5  -0.983453   0.634492 -1.549984 1.2451e-01    \n#> year::-4:cohort::6   0.360407   0.858316  0.419900 6.7552e-01    \n#> year::-4:cohort::7  -0.430610   0.661356 -0.651102 5.1657e-01    \n#> year::-4:cohort::8  -0.895195   0.374901 -2.387816 1.8949e-02 *  \n#> year::-4:cohort::9  -0.392478   0.439547 -0.892914 3.7418e-01    \n#> year::-4:cohort::10  0.519001   0.597880  0.868069 3.8757e-01    \n#> year::-3:cohort::4   0.591288   0.680169  0.869324 3.8688e-01    \n#> year::-3:cohort::5  -1.000650   0.971741 -1.029749 3.0577e-01    \n#> year::-3:cohort::6   0.072188   0.652641  0.110609 9.1216e-01    \n#> year::-3:cohort::7  -0.836820   0.804275 -1.040465 3.0079e-01    \n#> year::-3:cohort::8  -0.783148   0.701312 -1.116691 2.6697e-01    \n#> year::-3:cohort::9   0.811285   0.564470  1.437251 1.5397e-01    \n#> year::-3:cohort::10  0.527203   0.320051  1.647250 1.0285e-01    \n#> year::-2:cohort::3   0.036941   0.673771  0.054828 9.5639e-01    \n#> year::-2:cohort::4   0.832250   0.859544  0.968246 3.3541e-01    \n#> year::-2:cohort::5  -1.574086   0.525563 -2.995051 3.5076e-03 ** \n#> year::-2:cohort::6   0.311758   0.832095  0.374666 7.0875e-01    \n#> year::-2:cohort::7  -0.558631   0.871993 -0.640638 5.2332e-01    \n#> year::-2:cohort::8   0.429591   0.305270  1.407250 1.6265e-01    \n#> year::-2:cohort::9   1.201899   0.819186  1.467188 1.4566e-01    \n#> year::-2:cohort::10 -0.002429   0.682087 -0.003562 9.9717e-01    \n#> att                 -1.133749   0.205070 -5.528584 2.8820e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.921817     Adj. R2: 0.887984\n#>                  Within R2: 0.876406\n\n# alternatively\nsummary(res_sa20, agg = c(\"att\" = \"year::[012345678]\")) |> \n    etable(digits = 2)\n#>                         summary(res_..\n#> Dependent Var.:                      y\n#>                                       \n#> x1                      0.99*** (0.02)\n#> year = -9 x cohort = 10    0.35 (0.36)\n#> year = -8 x cohort = 9     0.03 (0.47)\n#> year = -8 x cohort = 10   -0.19 (0.35)\n#> year = -7 x cohort = 8    -0.59 (0.74)\n#> year = -7 x cohort = 9    0.87. (0.49)\n#> year = -7 x cohort = 10    0.02 (0.60)\n#> year = -6 x cohort = 7    -0.04 (0.87)\n#> year = -6 x cohort = 8    -0.66 (0.57)\n#> year = -6 x cohort = 9     0.88 (0.53)\n#> year = -6 x cohort = 10   -0.40 (0.35)\n#> year = -5 x cohort = 6    -0.66 (0.91)\n#> year = -5 x cohort = 7    -0.32 (0.70)\n#> year = -5 x cohort = 8    -0.24 (0.47)\n#> year = -5 x cohort = 9     0.30 (0.60)\n#> year = -5 x cohort = 10   -0.56 (0.46)\n#> year = -4 x cohort = 5    -0.98 (0.63)\n#> year = -4 x cohort = 6     0.36 (0.86)\n#> year = -4 x cohort = 7    -0.43 (0.66)\n#> year = -4 x cohort = 8   -0.90* (0.37)\n#> year = -4 x cohort = 9    -0.39 (0.44)\n#> year = -4 x cohort = 10    0.52 (0.60)\n#> year = -3 x cohort = 4     0.59 (0.68)\n#> year = -3 x cohort = 5     -1.0 (0.97)\n#> year = -3 x cohort = 6     0.07 (0.65)\n#> year = -3 x cohort = 7    -0.84 (0.80)\n#> year = -3 x cohort = 8    -0.78 (0.70)\n#> year = -3 x cohort = 9     0.81 (0.56)\n#> year = -3 x cohort = 10    0.53 (0.32)\n#> year = -2 x cohort = 3     0.04 (0.67)\n#> year = -2 x cohort = 4     0.83 (0.86)\n#> year = -2 x cohort = 5   -1.6** (0.53)\n#> year = -2 x cohort = 6     0.31 (0.83)\n#> year = -2 x cohort = 7    -0.56 (0.87)\n#> year = -2 x cohort = 8     0.43 (0.31)\n#> year = -2 x cohort = 9      1.2 (0.82)\n#> year = -2 x cohort = 10  -0.002 (0.68)\n#> att                     -1.1*** (0.21)\n#> Fixed-Effects:          --------------\n#> id                                 Yes\n#> year                               Yes\n#> _______________________ ______________\n#> S.E.: Clustered                 by: id\n#> Observations                       950\n#> R2                             0.90982\n#> Within R2                      0.87641\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# devtools::install_github(\"kylebutts/fwlplot\")\nlibrary(fwlplot)\nfwl_plot(y ~ x1, data = base_stagg)\n\nfwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100)\n\nfwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100, fsplit = ~ treated)"},{"path":"difference-in-differences.html","id":"wooldridge2022simple","chapter":"26 Difference-in-differences","heading":"26.9.8 Wooldridge (2022)","text":"use etwfe(Extended two-way Fixed Effects) (Wooldridge 2022)","code":""},{"path":"difference-in-differences.html","id":"doubly-robust-did","chapter":"26 Difference-in-differences","heading":"26.9.9 Doubly Robust DiD","text":"Also known locally efficient doubly robust (Sant’Anna Zhao 2020)Code example authorsThe package (method) rather limited application:Use OLS (handle glm)Use OLS (handle glm)Canonical (handle DDD).Canonical (handle DDD).","code":"\nlibrary(DRDID)\ndata(\"nsw_long\")\neval_lalonde_cps <-\n    subset(nsw_long, nsw_long$treated == 0 | nsw_long$sample == 2)\nhead(eval_lalonde_cps)\n#>   id year treated age educ black married nodegree dwincl      re74 hisp\n#> 1  1 1975      NA  42   16     0       1        0     NA     0.000    0\n#> 2  1 1978      NA  42   16     0       1        0     NA     0.000    0\n#> 3  2 1975      NA  20   13     0       0        0     NA  2366.794    0\n#> 4  2 1978      NA  20   13     0       0        0     NA  2366.794    0\n#> 5  3 1975      NA  37   12     0       1        0     NA 25862.322    0\n#> 6  3 1978      NA  37   12     0       1        0     NA 25862.322    0\n#>   early_ra sample experimental         re\n#> 1       NA      2            0     0.0000\n#> 2       NA      2            0   100.4854\n#> 3       NA      2            0  3317.4678\n#> 4       NA      2            0  4793.7451\n#> 5       NA      2            0 22781.8555\n#> 6       NA      2            0 25564.6699\n\n\n# locally efficient doubly robust DiD Estimators for the ATT\nout <-\n    drdid(\n        yname = \"re\",\n        tname = \"year\",\n        idname = \"id\",\n        dname = \"experimental\",\n        xformla = ~ age + educ + black + married + nodegree + hisp + re74,\n        data = eval_lalonde_cps,\n        panel = TRUE\n    )\nsummary(out)\n#>  Call:\n#> drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n#>     xformla = ~age + educ + black + married + nodegree + hisp + \n#>         re74, data = eval_lalonde_cps, panel = TRUE)\n#> ------------------------------------------------------------------\n#>  Further improved locally efficient DR DID estimator for the ATT:\n#>  \n#>    ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n#> -901.2703   393.6247   -2.2897     0.022    -1672.7747  -129.766 \n#> ------------------------------------------------------------------\n#>  Estimator based on panel data.\n#>  Outcome regression est. method: weighted least squares.\n#>  Propensity score est. method: inverse prob. tilting.\n#>  Analytical standard error.\n#> ------------------------------------------------------------------\n#>  See Sant'Anna and Zhao (2020) for details.\n\n\n\n# Improved locally efficient doubly robust DiD estimator \n# for the ATT, with panel data\n# drdid_imp_panel()\n\n# Locally efficient doubly robust DiD estimator for the ATT, \n# with panel data\n# drdid_panel()\n\n# Locally efficient doubly robust DiD estimator for the ATT, \n# with repeated cross-section data\n# drdid_rc()\n\n# Improved locally efficient doubly robust DiD estimator for the ATT, \n# with repeated cross-section data\n# drdid_imp_rc()"},{"path":"difference-in-differences.html","id":"augmentedforward-did","chapter":"26 Difference-in-differences","heading":"26.9.10 Augmented/Forward DID","text":"Methods Limited Pre-Treatment Periods:Augmented (K. T. Li Van den Bulte 2023)Forward (K. T. Li 2024)","code":""},{"path":"difference-in-differences.html","id":"multiple-treatments","chapter":"26 Difference-in-differences","heading":"26.10 Multiple Treatments","text":"2 treatments setting, always try model one regression see whether significantly different.Never use one treated groups control , run separate regression.check answer\\[\n\\begin{aligned}\nY_{} &= \\alpha + \\gamma_1 Treat1_{} + \\gamma_2 Treat2_{} + \\lambda Post_t  \\\\\n&+ \\delta_1(Treat1_i \\times Post_t) + \\delta_2(Treat2_i \\times Post_t) + \\epsilon_{}\n\\end{aligned}\n\\](Fricke 2017)(Clement De Chaisemartin D’haultfœuille 2023) video code","code":""},{"path":"difference-in-differences.html","id":"mediation-under-did","chapter":"26 Difference-in-differences","heading":"26.11 Mediation Under DiD","text":"Check post","code":""},{"path":"difference-in-differences.html","id":"assumptions-2","chapter":"26 Difference-in-differences","heading":"26.12 Assumptions","text":"Parallel Trends: Difference treatment control groups remain constant treatment.\nused cases \nobserve event\ntreatment control groups\n\ncases \ntreatment random\nconfounders.\n\nsupport use\nPlacebo test\nPrior Parallel Trends Test\n\nParallel Trends: Difference treatment control groups remain constant treatment.used cases \nobserve event\ntreatment control groups\nused cases whereyou observe eventyou observe eventyou treatment control groupsyou treatment control groupsnot cases \ntreatment random\nconfounders.\ncases wheretreatment randomtreatment randomconfounders.confounders.support use\nPlacebo test\nPrior Parallel Trends Test\nsupport usePlacebo testPlacebo testPrior Parallel Trends TestPrior Parallel Trends TestLinear additive effects (group/unit specific time-specific):\nadditively interact, use weighted 2FE estimator (Imai Kim 2021)\nTypically seen Staggered Dif-n-dif\nLinear additive effects (group/unit specific time-specific):additively interact, use weighted 2FE estimator (Imai Kim 2021)additively interact, use weighted 2FE estimator (Imai Kim 2021)Typically seen Staggered Dif-n-difTypically seen Staggered Dif-n-difNo anticipation: causal effect treatment implementation.anticipation: causal effect treatment implementation.Possible issuesEstimate dependent functional form:\nsize response depends (nonlinearly) size intervention, might want look difference group high intensity vs. low.\nEstimate dependent functional form:size response depends (nonlinearly) size intervention, might want look difference group high intensity vs. low.Selection (time–varying) unobservables\nCan use overall sensitivity coefficient estimates hidden bias using Rosenbaum Bounds\nSelection (time–varying) unobservablesCan use overall sensitivity coefficient estimates hidden bias using Rosenbaum BoundsLong-term effects\nParallel trends likely observed shorter period (window observation)\nLong-term effectsParallel trends likely observed shorter period (window observation)Heterogeneous effects\nDifferent intensity (e.g., doses) different groups.\nHeterogeneous effectsDifferent intensity (e.g., doses) different groups.Ashenfelter dip (Ashenfelter Card 1985) (job training program participant likely experience earning drop prior enrolling programs)\nParticipants systemically different nonparticipants treatment, leading question permanent transitory changes.\nfix transient endogeneity calculate long-run differences (exclude number periods symmetrically around adoption/ implementation date). see sustained impact, strong evidence causal impact policy. (Proserpio Zervas 2017b) (James J. Heckman Smith 1999) (Jepsen, Troske, Coomes 2014) (X. Li, Gan, Hu 2011)\nAshenfelter dip (Ashenfelter Card 1985) (job training program participant likely experience earning drop prior enrolling programs)Participants systemically different nonparticipants treatment, leading question permanent transitory changes.fix transient endogeneity calculate long-run differences (exclude number periods symmetrically around adoption/ implementation date). see sustained impact, strong evidence causal impact policy. (Proserpio Zervas 2017b) (James J. Heckman Smith 1999) (Jepsen, Troske, Coomes 2014) (X. Li, Gan, Hu 2011)Response event might immediate (can’t observed right away dependent variable)\nUsing lagged dependent variable \\(Y_{-1}\\) might appropriate (Blundell Bond 1998)\nResponse event might immediate (can’t observed right away dependent variable)Using lagged dependent variable \\(Y_{-1}\\) might appropriate (Blundell Bond 1998)factors affect difference trends two groups (.e., treatment control) bias estimation.factors affect difference trends two groups (.e., treatment control) bias estimation.Correlated observations within group timeCorrelated observations within group timeIncidental parameters problems (Lancaster 2000): ’s always better use individual time fixed effect.Incidental parameters problems (Lancaster 2000): ’s always better use individual time fixed effect.examining effects variation treatment timing, careful negative weights (per group) can negative heterogeneity treatment effects time. Example: [Athey Imbens (2022)](Borusyak, Jaravel, Spiess 2021)(Goodman-Bacon 2021). case use new estimands proposed @callaway2021difference(Clément De Chaisemartin d’Haultfoeuille 2020), package. expect lags leads, see (L. Sun Abraham 2021)examining effects variation treatment timing, careful negative weights (per group) can negative heterogeneity treatment effects time. Example: [Athey Imbens (2022)](Borusyak, Jaravel, Spiess 2021)(Goodman-Bacon 2021). case use new estimands proposed @callaway2021difference(Clément De Chaisemartin d’Haultfoeuille 2020), package. expect lags leads, see (L. Sun Abraham 2021)(Gibbons, Suárez Serrato, Urbancic 2018) caution suspect treatment effect treatment variance vary across groups(Gibbons, Suárez Serrato, Urbancic 2018) caution suspect treatment effect treatment variance vary across groups","code":""},{"path":"difference-in-differences.html","id":"prior-parallel-trends-test","chapter":"26 Difference-in-differences","heading":"26.12.1 Prior Parallel Trends Test","text":"Plot average outcomes time treatment control group treatment time.Statistical test difference trends (using data treatment period)\\[\nY = \\alpha_g + \\beta_1 T + \\beta_2 T\\times G + \\epsilon\n\\]\\(Y\\) = outcome variable\\(Y\\) = outcome variable\\(\\alpha_g\\) = group fixed effects\\(\\alpha_g\\) = group fixed effects\\(T\\) = time (e.g., specific year, month)\\(T\\) = time (e.g., specific year, month)\\(\\beta_2\\) = different time trends group\\(\\beta_2\\) = different time trends groupHence, \\(\\beta_2 =0\\) provides evidence differences trend two groups prior time treatment.can also use different functional forms (e..g, polynomial nonlinear).\\(\\beta_2 \\neq 0\\) statistically, possible reasons can :Statistical significance can driven large sampleStatistical significance can driven large sampleOr trends consistent, just one period deviation can throw trends. Hence, statistical statistical significance.trends consistent, just one period deviation can throw trends. Hence, statistical statistical significance.Technically, can still salvage research including time fixed effects, instead just --time fixed effect (actually, researchers mechanically anyway nowadays). However, side effect can time fixed effects can also absorb part treatment effect well, especially cases treatment effects vary time (.e., stronger weaker time) (Wolfers 2003).Debate:(Kahn-Lang Lang 2020) argue plausible treatment control groups similar trends, also levels. observe dissimilar levels prior treatment, okay think affect future trends?\nShow plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).\ncase don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.\n(Kahn-Lang Lang 2020) argue plausible treatment control groups similar trends, also levels. observe dissimilar levels prior treatment, okay think affect future trends?Show plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).Show plot dependent variable’s time series treated control groups also similar plot matched sample. (Ryan et al. 2019) show evidence matched well setting non-parallel trends (least health care setting).case don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.case don’t similar levels ex ante treatment control groups, functional form assumptions matter need justification choice.Pre-trend statistical tests: (Roth 2022) provides evidence test usually powered.\nSee PretrendsPower pretrends packages correcting .\nPre-trend statistical tests: (Roth 2022) provides evidence test usually powered.See PretrendsPower pretrends packages correcting .Parallel trends assumption specific transformation units outcome (Roth Sant’Anna 2023)\nSee falsification test (\\(H_0\\): parallel trends insensitive functional form).\nParallel trends assumption specific transformation units outcome (Roth Sant’Anna 2023)See falsification test (\\(H_0\\): parallel trends insensitive functional form).alarming since one periods significantly different 0, means parallel trends assumption plausible.cases might violations parallel trends assumption, check (Rambachan Roth 2023)Impose restrictions different post-treatment violations parallel trends can pre-trends.Impose restrictions different post-treatment violations parallel trends can pre-trends.Partial identification causal parameterPartial identification causal parameterSensitivity analysisSensitivity analysisAlternatively, Ban Kedagni (2022) propose method information set (.e., pre-treatment covariates), assumption selection bias post-treatment period (.e., lies within convex hull selection biases), can still identify set ATT, stricter assumption selection bias policymakers perspective, can also point estimate.Alternatively, can use pretrends package examine assumptions (Roth 2022)","code":"\nlibrary(tidyverse)\nlibrary(fixest)\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    filter(Quarter_Num <= 3) %>% \n    # Treatment variable\n    dplyr::mutate(California = State == 'California')\n\n# use my package\ncausalverse::plot_par_trends(\n    data = od,\n    metrics_and_names = list(\"Rate\" = \"Rate\"),\n    treatment_status_var = \"California\",\n    time_var = list(Quarter_Num = \"Time\"),\n    display_CI = F\n)\n#> [[1]]\n\n# do it manually\n# always good but plot the dependent out\nod |>\n    # group by treatment status and time\n    dplyr::group_by(California, Quarter) |>\n    dplyr::summarize_all(mean) |>\n    dplyr::ungroup() |>\n    # view()\n    \n    ggplot2::ggplot(aes(x = Quarter_Num, y = Rate, color = California)) +\n    ggplot2::geom_line() +\n    causalverse::ama_theme()\n\n\n# but it's also important to use statistical test\nprior_trend <- fixest::feols(Rate ~ i(Quarter_Num, California) | State + Quarter,\n               data = od)\n\nfixest::coefplot(prior_trend, grid = F)\nfixest::iplot(prior_trend, grid = F)\n# https://github.com/asheshrambachan/HonestDiD\n# remotes::install_github(\"asheshrambachan/HonestDiD\")\n# library(HonestDiD)"},{"path":"difference-in-differences.html","id":"placebo-test-1","chapter":"26 Difference-in-differences","heading":"26.12.2 Placebo Test","text":"Procedure:Sample data period treatment time.Consider different fake cutoff time, either\nTry whole sequence time\nGenerate random treatment period, use randomization inference account sampling distribution fake effect.\nTry whole sequence timeTry whole sequence timeGenerate random treatment period, use randomization inference account sampling distribution fake effect.Generate random treatment period, use randomization inference account sampling distribution fake effect.Estimate model post-time = 1 fake cutoffA significant coefficient means violate parallel trends! big problem.Alternatively,data multiple control groups, drop treated group, assign another control group “fake” treated group. even fails (.e., find significant effect) among control groups, can still fine. However, method used Synthetic ControlCode theeffectbook.netWe like “supposed” insignificant.","code":"\nlibrary(tidyverse)\nlibrary(fixest)\n\nod <- causaldata::organ_donations %>%\n    # Use only pre-treatment data\n    dplyr::filter(Quarter_Num <= 3) %>%\n    \n    # Create fake treatment variables\n    dplyr::mutate(\n        FakeTreat1 = State == 'California' &\n            Quarter %in% c('Q12011', 'Q22011'),\n        FakeTreat2 = State == 'California' &\n            Quarter == 'Q22011'\n    )\n\n\nclfe1 <- fixest::feols(Rate ~ FakeTreat1 | State + Quarter,\n               data = od)\nclfe2 <- fixest::feols(Rate ~ FakeTreat2 | State + Quarter,\n               data = od)\n\nfixest::etable(clfe1,clfe2)\n#>                           clfe1            clfe2\n#> Dependent Var.:            Rate             Rate\n#>                                                 \n#> FakeTreat1TRUE  0.0061 (0.0051)                 \n#> FakeTreat2TRUE                  -0.0017 (0.0028)\n#> Fixed-Effects:  --------------- ----------------\n#> State                       Yes              Yes\n#> Quarter                     Yes              Yes\n#> _______________ _______________ ________________\n#> S.E.: Clustered       by: State        by: State\n#> Observations                 81               81\n#> R2                      0.99377          0.99376\n#> Within R2               0.00192          0.00015\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"difference-in-differences.html","id":"assumption-violations","chapter":"26 Difference-in-differences","heading":"26.12.3 Assumption Violations","text":"Endogenous TimingIf timing units can influenced strategic decisions analysis, instrumental variable approach control function can used control endogeneity timing.Questionable CounterfactualsIn situations control units may serve reliable counterfactual treated units, matching methods propensity score matching generalized random forest can utilized. Additional methods can found Matching Methods.","code":""},{"path":"difference-in-differences.html","id":"robustness-checks","chapter":"26 Difference-in-differences","heading":"26.12.4 Robustness Checks","text":"Placebo (estimate \\(\\neq 0\\), parallel trend violated, original biased):\nGroup: Use fake treatment groups: population affect treatment\nTime: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).\nPlacebo (estimate \\(\\neq 0\\), parallel trend violated, original biased):Group: Use fake treatment groups: population affect treatmentGroup: Use fake treatment groups: population affect treatmentTime: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).Time: Redo analysis period treatment (expected treatment effect 0) (e.g., previous year period).Possible alternative control group: Expected results similarPossible alternative control group: Expected results similarTry different windows (away treatment point, factors can creep nullify effect).Try different windows (away treatment point, factors can creep nullify effect).Treatment Reversal (don’t see treatment event)Treatment Reversal (don’t see treatment event)Higher-order polynomial time trend (relax linearity assumption)Higher-order polynomial time trend (relax linearity assumption)Test whether dependent variables affected event indeed unaffected.\nUse control treatment period (\\(\\neq0\\), problem)\nTest whether dependent variables affected event indeed unaffected.Use control treatment period (\\(\\neq0\\), problem)triple-difference strategy involves examining interaction treatment variable probability affected program, group-level participation rate. identification assumption differential trends high low participation groups early versus late implementing countries.triple-difference strategy involves examining interaction treatment variable probability affected program, group-level participation rate. identification assumption differential trends high low participation groups early versus late implementing countries.","code":""},{"path":"changes-in-changes.html","id":"changes-in-changes","chapter":"27 Changes-in-Changes","heading":"27 Changes-in-Changes","text":"Introduction\nChanges--Changes (CiC) estimator, introduced Athey Imbens (2006), alternative Difference--Differences () strategy.\nUnlike traditional , estimates Average Treatment Effect Treated (ATT), CiC focuses Quantile Treatment Effect Treated (QTT).\nQTT captures difference potential outcome distributions treated units specific quantile.\nBeyond Averages: Policymakers often look beyond average program impacts, considering benefits distributed across different groups.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.\nTraditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.\nQTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.\n\nQTE vs. ATE: QTEs provide detailed insights distributional impacts, also allow recovery ATEs. However, ATEs usually identified weaker assumptions, making QTEs suitable exploring shape treatment effects rather just central tendency.\nIntroductionThe Changes--Changes (CiC) estimator, introduced Athey Imbens (2006), alternative Difference--Differences () strategy.Unlike traditional , estimates Average Treatment Effect Treated (ATT), CiC focuses Quantile Treatment Effect Treated (QTT).QTT captures difference potential outcome distributions treated units specific quantile.Beyond Averages: Policymakers often look beyond average program impacts, considering benefits distributed across different groups.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.\nTraditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.\nQTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.\nJob Training Example: Two programs negative average impact may treated differently: one benefiting high earners might rejected, one benefiting low earners approved.Traditional Methods’ Limitations: Methods like linear regression, assume uniform effects, fail capture important distributional differences.QTEs’ Advantage: QTE methods tailored analyzing treatment effects vary across different segments population.QTE vs. ATE: QTEs provide detailed insights distributional impacts, also allow recovery ATEs. However, ATEs usually identified weaker assumptions, making QTEs suitable exploring shape treatment effects rather just central tendency.Key Concepts\nQuantile Treatment Effect Treated (QTT): Difference quantiles treated units’ potential outcome distributions.\nRank Preservation: Assumes unit’s rank remains constant across potential outcome distributions—strong assumption.\nCounterfactual Distribution: Estimation focuses determining distribution treated units period 1.\nKey ConceptsQuantile Treatment Effect Treated (QTT): Difference quantiles treated units’ potential outcome distributions.Rank Preservation: Assumes unit’s rank remains constant across potential outcome distributions—strong assumption.Counterfactual Distribution: Estimation focuses determining distribution treated units period 1.Estimating QTT\nCiC uses four distributions 2x2 design:\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\n\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\n\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\n\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.\n\nQTT defined difference inverses \\(F_{Y(1),11}\\) counterfactual distribution \\(F_{Y(0),11}\\) quantile \\(q\\):\n\\[\n  \\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta)\n  \\]Estimating QTTCiC uses four distributions 2x2 design:\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\n\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\n\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\n\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.\n\\(F_{Y(0),00}\\): CDF \\(Y(0)\\) control units period 0.\\(F_{Y(0),10}\\): CDF \\(Y(0)\\) treatment units period 0.\\(F_{Y(0),01}\\): CDF \\(Y(0)\\) control units period 1.\\(F_{Y(1),11}\\): CDF \\(Y(1)\\) treatment units period 1.QTT defined difference inverses \\(F_{Y(1),11}\\) counterfactual distribution \\(F_{Y(0),11}\\) quantile \\(q\\):\\[\n  \\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta)\n  \\]Estimation Process\nCounterfactual CDF:\n\\[\n  \\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right)\n  \\]\nEquivalent Expression:\n\\[\n  \\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right)\n  \\]\nTreatment Effect Estimate:\n\\[\n  \\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta)\n  \\]\nEquivalently:\n\\(\\Delta^{CIC}_{\\theta}\\) difference two QTE estimates:\n\\[\n  \\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta',0}\n  \\]\n:\n\\(\\Delta^{QTT}_{\\theta,1}\\) = change time \\(y\\) quantile \\(\\theta\\) \\(D = 1\\) group.\n\\(\\Delta^{QTU}_{\\theta',0}\\) = change time \\(y\\) quantile \\(\\theta'\\) \\(D = 0\\) group, \\(q'\\) quantile \\(D = 0, T = 0\\) distribution corresponding value \\(y\\) associated quantile \\(\\theta\\) \\(D = 1, T = 0\\) distribution.\nEstimation ProcessCounterfactual CDF:\\[\n  \\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right)\n  \\]Equivalent Expression:\\[\n  \\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right)\n  \\]Treatment Effect Estimate:\\[\n  \\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta)\n  \\]Equivalently:\\(\\Delta^{CIC}_{\\theta}\\) difference two QTE estimates:\\[\n  \\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta',0}\n  \\]:\\(\\Delta^{QTT}_{\\theta,1}\\) = change time \\(y\\) quantile \\(\\theta\\) \\(D = 1\\) group.\\(\\Delta^{QTU}_{\\theta',0}\\) = change time \\(y\\) quantile \\(\\theta'\\) \\(D = 0\\) group, \\(q'\\) quantile \\(D = 0, T = 0\\) distribution corresponding value \\(y\\) associated quantile \\(\\theta\\) \\(D = 1, T = 0\\) distribution.Marketing Example\nSuppose company implements new online marketing strategy aimed improving customer retention rates.\nQTT: goal estimate effect strategy customer retention rates different quantiles (e.g., median retention rate).\nRank Preservation: Assumes customers’ rank retention distribution remains , regardless strategy—assumption strong carefully considered.\nCounterfactual: CiC helps estimate retention rates changed without new strategy comparing control group.\nMarketing ExampleSuppose company implements new online marketing strategy aimed improving customer retention rates.QTT: goal estimate effect strategy customer retention rates different quantiles (e.g., median retention rate).Rank Preservation: Assumes customers’ rank retention distribution remains , regardless strategy—assumption strong carefully considered.Counterfactual: CiC helps estimate retention rates changed without new strategy comparing control group.References\nAthey Imbens (2006)\nFrölich Melly (2013): IV-based\nCallaway Li (2019): panel data\nHuber, Schelker, Strittmatter (2022)\nReferencesAthey Imbens (2006)Frölich Melly (2013): IV-basedCallaway Li (2019): panel dataHuber, Schelker, Strittmatter (2022)Additional Resources\nCode examples available Stata.\nAdditional ResourcesCode examples available Stata.","code":""},{"path":"changes-in-changes.html","id":"application-14","chapter":"27 Changes-in-Changes","heading":"27.1 Application","text":"","code":""},{"path":"changes-in-changes.html","id":"ecic-package","chapter":"27 Changes-in-Changes","heading":"27.1.1 ECIC package","text":"","code":"\nlibrary(ecic)\ndata(dat, package = \"ecic\")\nmod =\n  ecic(\n    yvar  = lemp,         # dependent variable\n    gvar  = first.treat,  # group indicator\n    tvar  = year,         # time indicator\n    ivar  = countyreal,   # unit ID\n    dat   = dat,          # dataset\n    boot  = \"weighted\",   # bootstrap proceduce (\"no\", \"normal\", or \"weighted\")\n    nReps = 3            # number of bootstrap runs\n    )\nmod_res <- summary(mod)\nmod_res\n#>   perc    coefs          se\n#> 1  0.1 1.206140 0.021351711\n#> 2  0.2 1.316599 0.009225026\n#> 3  0.3 1.449963 0.001859468\n#> 4  0.4 1.583415 0.015296156\n#> 5  0.5 1.739932 0.011240454\n#> 6  0.6 1.915558 0.013060348\n#> 7  0.7 2.114966 0.014482208\n#> 8  0.8 2.363105 0.005173865\n#> 9  0.9 2.779202 0.020831180\n\necic_plot(mod_res)"},{"path":"changes-in-changes.html","id":"qte-package","chapter":"27 Changes-in-Changes","heading":"27.1.2 QTE package","text":"QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.QTE compares quantiles entire population treatment control, whereas QTET compares quantiles within treated group . difference means QTE reflects overall population-level impact, QTET focuses treated group’s specific impact.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.CIA enables identification QTE QTET, since QTET conditional treatment, might reflect different effects QTE, especially treatment effect heterogeneous across different subpopulations. example, QTE show generalized effect across individuals, QTET may reveal stronger weaker effects subgroup actually received treatment.-like modelsWith distributional difference--differences assumption Callaway Li (2019), extension parallel trends assumption, can estimate QTET.2 periods, distributional assumption can partially identify QTET bounds (Fan Yu 2012)restrictive assumption difference quantiles distribution potential outcomes treated untreated groups values quantiles, can mean modelOn top distributional assumption, need copula stability assumption (.e., , treatment, units highest outcomes improving , expect see improving current period .) models:","code":"\nlibrary(qte)\ndata(lalonde)\n\n# randomized setting\n# qte is identical to qtet\njt.rand <-\n    ci.qtet(\n        re78 ~ treat,\n        data = lalonde.exp,\n        iters = 10\n    )\nsummary(jt.rand)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05    0.00    0.00\n#> 0.1     0.00    0.00\n#> 0.15    0.00    0.00\n#> 0.2     0.00   18.33\n#> 0.25  338.65  377.74\n#> 0.3   846.40  470.45\n#> 0.35 1451.51  515.86\n#> 0.4  1177.72  869.19\n#> 0.45 1396.08  918.39\n#> 0.5  1123.55  925.74\n#> 0.55 1181.54  938.82\n#> 0.6  1466.51  951.64\n#> 0.65 2115.04  892.16\n#> 0.7  1795.12  842.66\n#> 0.75 2347.49  678.45\n#> 0.8  2278.12  971.21\n#> 0.85 2178.28  973.90\n#> 0.9  3239.60 1889.23\n#> 0.95 3979.62 2872.52\n#> \n#> Average Treatment Effect:    1794.34\n#>   Std. Error:        665.59\nggqte(jt.rand)\n# conditional independence assumption (CIA)\njt.cia <- ci.qte(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.cia)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1       0.00        0.00\n#> 0.15  -4433.18      710.76\n#> 0.2   -8219.15      419.90\n#> 0.25 -10435.74      793.20\n#> 0.3  -12232.03     1037.03\n#> 0.35 -12428.30     1425.39\n#> 0.4  -14195.24     1793.20\n#> 0.45 -14248.66     1907.98\n#> 0.5  -15538.67     2095.11\n#> 0.55 -16550.71     2329.67\n#> 0.6  -15595.02     2686.45\n#> 0.65 -15827.52     2745.62\n#> 0.7  -16090.32     3390.26\n#> 0.75 -16091.49     3376.67\n#> 0.8  -17864.76     3245.52\n#> 0.85 -16756.71     3533.91\n#> 0.9  -17914.99     2305.10\n#> 0.95 -23646.22     2003.55\n#> \n#> Average Treatment Effect:    -13435.40\n#>   Std. Error:        1259.01\nggqte(jt.cia)\n\njt.ciat <- ci.qtet(\n    re78 ~ treat,\n    xformla =  ~ age + education,\n    data = lalonde.psid,\n    iters = 10\n)\nsummary(jt.ciat)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05      0.00        0.00\n#> 0.1   -1018.15      614.29\n#> 0.15  -3251.00     1557.37\n#> 0.2   -7240.86     1433.54\n#> 0.25  -8379.94      475.33\n#> 0.3   -8758.82      345.53\n#> 0.35  -9897.44      606.54\n#> 0.4  -10239.57      747.91\n#> 0.45 -10751.39      736.39\n#> 0.5  -10570.14      899.75\n#> 0.55 -11348.96      898.80\n#> 0.6  -11550.84      687.20\n#> 0.65 -12203.56      780.92\n#> 0.7  -13277.72      979.47\n#> 0.75 -14011.74      993.28\n#> 0.8  -14373.95      706.69\n#> 0.85 -14499.18     1048.62\n#> 0.9  -15008.63     2201.11\n#> 0.95 -15954.05     2655.30\n#> \n#> Average Treatment Effect:    4266.19\n#>   Std. Error:        600.51\nggqte(jt.ciat)\n# distributional DiD assumption\njt.pqtet <- panel.qtet(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tmin2 = 1974,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.pqtet)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05  4779.21     1222.37\n#> 0.1   1987.35      776.82\n#> 0.15   842.95     3332.09\n#> 0.2  -7366.04     4852.87\n#> 0.25 -8449.96     3522.70\n#> 0.3  -7992.15     1201.51\n#> 0.35 -7429.21     1161.43\n#> 0.4  -6597.37     1288.64\n#> 0.45 -5519.45     1391.04\n#> 0.5  -4702.88     1129.80\n#> 0.55 -3904.52     1131.23\n#> 0.6  -2741.80     1157.60\n#> 0.65 -1507.31     1223.03\n#> 0.7   -771.12     1264.45\n#> 0.75   707.81     1280.34\n#> 0.8    580.00      793.09\n#> 0.85   821.75      969.38\n#> 0.9   -250.77     1662.49\n#> 0.95 -1874.54     2706.67\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        795.44\nggqte(jt.pqtet)\nres_bound <-\n    bounds(\n        re ~ treat,\n        t = 1978,\n        tmin1 = 1975,\n        data = lalonde.psid.panel,\n        idname = \"id\",\n        tname = \"year\"\n    )\nsummary(res_bound)\n#> \n#> Bounds on the Quantile Treatment Effect on the Treated:\n#>      \n#> tau  Lower Bound Upper Bound\n#>         tau  Lower Bound Upper Bound\n#>        0.05       -51.72           0\n#>         0.1     -1220.84           0\n#>        0.15      -1881.9           0\n#>         0.2     -2601.32           0\n#>        0.25     -2916.38      485.23\n#>         0.3     -3080.16      943.05\n#>        0.35     -3327.89     1505.98\n#>         0.4     -3240.59     2133.59\n#>        0.45     -2982.51     2616.84\n#>         0.5     -3108.01      2566.2\n#>        0.55     -3342.66     2672.82\n#>         0.6      -3491.4      3065.7\n#>        0.65     -3739.74     3349.74\n#>         0.7     -4647.82     2992.03\n#>        0.75     -4826.78     3219.32\n#>         0.8      -5801.7     2702.33\n#>        0.85     -6588.61     2499.41\n#>         0.9     -8953.84     2020.84\n#>        0.95    -14283.61      397.04\n#> \n#> Average Treatment Effect on the Treated: 2326.51\nplot(res_bound)\njt.mdid <- ddid2(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10\n)\nsummary(jt.mdid)\n#> \n#> Quantile Treatment Effect:\n#>      \n#> tau  QTE Std. Error\n#> 0.05 10616.61      744.99\n#> 0.1   5019.83      447.82\n#> 0.15  2388.12      334.57\n#> 0.2   1033.23      365.01\n#> 0.25   485.23      445.95\n#> 0.3    943.05      631.10\n#> 0.35   931.45      756.72\n#> 0.4    945.35      888.69\n#> 0.45  1205.88      903.88\n#> 0.5   1362.11      778.89\n#> 0.55  1279.05      871.73\n#> 0.6   1618.13      734.08\n#> 0.65  1834.30      674.83\n#> 0.7   1326.06      793.46\n#> 0.75  1586.35      714.42\n#> 0.8   1256.09      591.37\n#> 0.85   723.10      871.86\n#> 0.9    251.36     1703.13\n#> 0.95 -1509.92     2033.88\n#> \n#> Average Treatment Effect:    2326.51\n#>   Std. Error:        514.81\nplot(jt.mdid)\njt.qdid <- QDiD(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)\n\njt.cic <- CiC(\n    re ~ treat,\n    t = 1978,\n    tmin1 = 1975,\n    tname = \"year\",\n    idname = \"id\",\n    data = lalonde.psid.panel,\n    iters = 10,\n    panel = T\n)"},{"path":"synthetic-control.html","id":"synthetic-control","chapter":"28 Synthetic Control","heading":"28 Synthetic Control","text":"Examples marketing:(Tirunillai Tellis 2017): offline TV ad Online Chatter(Yanwen Wang, Wu, Zhu 2019): mobile hailing technology adoption drivers’ hourly earnings(Guo, Sriram, Manchanda 2020): payment disclosure laws effect physician prescription behavior using Timing Massachusetts open payment law exogenous shock(Adalja et al. 2023): mandatory GMO labels impact consumer demand (Using Vermont mandatory state)NotesThe SC method provides asymptotically normal estimators various linear panel data models, given sufficiently large pre-treatment periods, making natural alternative Difference--differences model (Arkhangelsky Hirshberg 2023).SC method provides asymptotically normal estimators various linear panel data models, given sufficiently large pre-treatment periods, making natural alternative Difference--differences model (Arkhangelsky Hirshberg 2023).SCM superior Matching Methods matches covariates (.e., pre-treatment variables), also outcomes.SCM superior Matching Methods matches covariates (.e., pre-treatment variables), also outcomes.review method, see (Abadie 2021)review method, see (Abadie 2021)SCMs can also used Bayesian framework (Bayesian Synthetic Control) impose restrictive priori (S. Kim, Lee, Gupta 2020)SCMs can also used Bayesian framework (Bayesian Synthetic Control) impose restrictive priori (S. Kim, Lee, Gupta 2020)Different Matching Methods SCMs match pre-treatment outcomes period Matching Methods match number covariates.Different Matching Methods SCMs match pre-treatment outcomes period Matching Methods match number covariates.data driven procedure construct comparable control groups (.e., black box).data driven procedure construct comparable control groups (.e., black box).causal inference control treatment group using Matching Methods, typically similar covariates control treated groups. However, don’t methods like Propensity Scores can perform rather poorly (.e., large bias).causal inference control treatment group using Matching Methods, typically similar covariates control treated groups. However, don’t methods like Propensity Scores can perform rather poorly (.e., large bias).Advantages Difference--differencesMaximization observable similarity control treatment (maybe also unobservables)Can also used cases untreated case similar matching dimensions treated casesObjective selection controls.Advantages linear regressionRegression weights estimator outside [0,1] (regression allows extrapolation), sparse (.e., can less 0).Regression weights estimator outside [0,1] (regression allows extrapolation), sparse (.e., can less 0).extrapolation SCMsNo extrapolation SCMsExplicitly state fit (.e., weight)Explicitly state fit (.e., weight)Can estimated without post-treatment outcomes control group (can’t p-hack)Can estimated without post-treatment outcomes control group (can’t p-hack)Advantages:selection criteria, researchers can understand relative importance candidatePost-intervention outcomes used synthetic. Hence, can’t retro-fit.Observable similarity control treatment cases maximizedDisadvantages:’s hard argue weights use create “synthetic control”SCM recommended whenSocial events evaluate large-scale program policyOnly one treated case several control candidates.AssumptionsDonor subject good match synthetic control (.e., gap dependent donor subject synthetic control 0 treatment)Donor subject good match synthetic control (.e., gap dependent donor subject synthetic control 0 treatment)treated subject undergoes treatment subjects donor pool.treated subject undergoes treatment subjects donor pool.changes subjects whole window.changes subjects whole window.counterfactual outcome treatment group can imputed linear combination control groups.counterfactual outcome treatment group can imputed linear combination control groups.Identification: exclusion restriction met conditional pre-treatment outcomes.Synth provides algorithm finds weighted combination comparison units weights chosen best resembles values predictors outcome variable affected units interventionSetting (notation followed professor Alberto Abadie)\\(J + 1\\) units periods \\(1, \\dots, T\\)\\(J + 1\\) units periods \\(1, \\dots, T\\)first unit treated one \\(T_0 + 1, \\dots, T\\)first unit treated one \\(T_0 + 1, \\dots, T\\)\\(J\\) units called donor pool\\(J\\) units called donor pool\\(Y_{}^\\) outcome unit \\(\\) ’s exposed treatment \\(T_0 + 1 , \\dots T\\)\\(Y_{}^\\) outcome unit \\(\\) ’s exposed treatment \\(T_0 + 1 , \\dots T\\)\\(Y_{}^N\\) outcome unit \\(\\) ’s exposed treatment\\(Y_{}^N\\) outcome unit \\(\\) ’s exposed treatmentWe try estimate effect treatment treated unit\\[\n\\tau_{1t} = Y_{1t}^- Y_{1t}^N\n\\]observe first treated unit already \\(Y_{1t}^= Y_{1t}\\)construct synthetic control unit, find appropriate weight donor donor pool finding \\(\\mathbf{W} = (w_2, \\dots, w_{J=1})'\\) \\(w_j \\ge 0\\) \\(j = 2, \\dots, J+1\\)\\(w_j \\ge 0\\) \\(j = 2, \\dots, J+1\\)\\(w_2 + \\dots + w_{J+1} = 1\\)\\(w_2 + \\dots + w_{J+1} = 1\\)“appropriate” vector \\(\\mathbf{W}\\) constrained \\[\n\\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}||\n\\]\\(\\mathbf{X}_1\\) \\(k \\times 1\\) vector pre-treatment characteristics treated unit\\(\\mathbf{X}_1\\) \\(k \\times 1\\) vector pre-treatment characteristics treated unit\\(\\mathbf{X}_0\\) \\(k \\times J\\) matrix pre-treatment characteristics untreated units\\(\\mathbf{X}_0\\) \\(k \\times J\\) matrix pre-treatment characteristics untreated unitsFor simplicity, researchers usually use\\[\n\\begin{aligned}\n&\\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}|| \\\\\n&= (\\sum_{h=1}^k v_h(X_{h1}- w_2 X-{h2} - \\dots - w_{J+1} X_{hJ +1})^{1/2}\n\\end{aligned}\n\\]\\(v_1, \\dots, v_k\\) vector positive constants represent predictive power \\(k\\) predictors \\(Y_{1t}^N\\) (.e., potential outcome treated without treatment) can chosen either explicitly researcher data-driven methodsFor penalized synthetic control (Abadie L’hour 2021), minimization problem becomes\\[\n\\min_{\\mathbf{W}} ||\\mathbf{X}_1 - \\sum_{j=2}^{J + 1}W_j \\mathbf{X}_j ||^2 + \\lambda \\sum_{j=2}^{J+1} W_j ||\\mathbf{X}_1 - \\mathbf{X}_j||^2\n\\]\\(W_j \\ge 0\\) \\(\\sum_{j=2}^{J+1} W_j = 1\\)\\(W_j \\ge 0\\) \\(\\sum_{j=2}^{J+1} W_j = 1\\)\\(\\lambda >0\\) balances -fitting treated minimize sum pairwise distances\n\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\n\\(\\lambda \\\\infty\\): nearest neighbor matching\n\\(\\lambda >0\\) balances -fitting treated minimize sum pairwise distances\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\\(\\lambda \\0\\): pure synthetic control (.e solution unpenalized estimator)\\(\\lambda \\\\infty\\): nearest neighbor matching\\(\\lambda \\\\infty\\): nearest neighbor matchingAdvantages:\\(\\lambda >0\\), unique sparse solutionFor \\(\\lambda >0\\), unique sparse solutionReduces interpolation bias averaging dissimilar unitsReduces interpolation bias averaging dissimilar unitsPenalized SC never uses dissimilar unitsPenalized SC never uses dissimilar unitsThen synthetic control estimator \\[\n\\hat{\\tau}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt}\n\\]\\(Y_{jt}\\) outcome unit \\(j\\) time \\(t\\)ConsiderationUnder factor model (Abadie, Diamond, Hainmueller 2010)\\[\nY_{}^N = \\mathbf{\\theta}_t \\mathbf{Z}_i + \\mathbf{\\lambda}_t \\mathbf{\\mu}_i + \\epsilon_{}\n\\]\\(Z_i\\) = observables\\(Z_i\\) = observables\\(\\mu_i\\) = unobservables\\(\\mu_i\\) = unobservables\\(\\epsilon_{}\\) = unit-level transitory shock (.e., random noise)\\(\\epsilon_{}\\) = unit-level transitory shock (.e., random noise)assumptions \\(\\mathbf{W}^*\\) \\[\n\\begin{aligned}\n\\sum_{j=2}^{J+1} w_j^* \\mathbf{Z}_j  &= \\mathbf{Z}_1 \\\\\n&\\dots \\\\\n\\sum_{j=2}^{J+1} w_j^* Y_{j1} &= Y_{11} \\\\\n\\sum_{j=2}^{J+1} w_j^* Y_{jT_0} &= Y_{1T_0}\n\\end{aligned}\n\\]Basically, assume synthetic control good counterfactual treated unit exposed treatment.,bias bound depends close fit, controlled ratio \\(\\epsilon_{}\\) (transitory shock) \\(T_0\\) (number pre-treatment periods). words, good fit \\(Y_{1t}\\) pre-treatment period (.e., \\(T_0\\) large small variance \\(\\epsilon_{}\\))bias bound depends close fit, controlled ratio \\(\\epsilon_{}\\) (transitory shock) \\(T_0\\) (number pre-treatment periods). words, good fit \\(Y_{1t}\\) pre-treatment period (.e., \\(T_0\\) large small variance \\(\\epsilon_{}\\))poor fit, use bias correction version synthetic control. See Ben-Michael, Feller, Rothstein (2020)poor fit, use bias correction version synthetic control. See Ben-Michael, Feller, Rothstein (2020)Overfitting can result small \\(T_0\\) (number pre-treatment periods), large \\(J\\) (number units donor pool), large \\(\\epsilon_{}\\) (noise)\nMitigation: put similar units (treated one) donor pool\nOverfitting can result small \\(T_0\\) (number pre-treatment periods), large \\(J\\) (number units donor pool), large \\(\\epsilon_{}\\) (noise)Mitigation: put similar units (treated one) donor poolTo make inference, create permutation distribution (iteratively reassigning treatment units donor pool estimate placebo effects iteration). say effect treatment magnitude value treatment effect treated unit extreme relative permutation distribution.’s recommended use one-sided inference. permutation distribution superior p-values alone (sampling-based inference hard SCMs either undefined sampling mechanism sample population).benchmark (permutation) distribution (e.g., uniform), see (Firpo Possebom 2018)","code":""},{"path":"synthetic-control.html","id":"applications-1","chapter":"28 Synthetic Control","heading":"28.1 Applications","text":"","code":""},{"path":"synthetic-control.html","id":"example-1-2","chapter":"28 Synthetic Control","heading":"28.1.1 Example 1","text":"Danilo Freiresimulate data 10 states 30 years. State receives treatment T = 20 year 15.Gaps plot:Alternatively, gsynth provides options estimate iterative fixed effects, handle multiple treated units tat time., use two=way fixed effects bootstrapped standard errors","code":"\n# install.packages(\"Synth\")\n# install.packages(\"gsynth\")\nlibrary(\"Synth\")\nlibrary(\"gsynth\")\nset.seed(1)\nyear         <- rep(1:30, 10)\nstate        <- rep(LETTERS[1:10], each = 30)\nX1           <- round(rnorm(300, mean = 2, sd = 1), 2)\nX2           <- round(rbinom(300, 1, 0.5) + rnorm(300), 2)\nY            <- round(1 + 2 * X1 + rnorm(300), 2)\ndf           <- as.data.frame(cbind(Y, X1, X2, state, year))\ndf$Y         <- as.numeric(as.character(df$Y))\ndf$X1        <- as.numeric(as.character(df$X1))\ndf$X2        <- as.numeric(as.character(df$X2))\ndf$year      <- as.numeric(as.character(df$year))\ndf$state.num <- rep(1:10, each = 30)\ndf$state     <- as.character(df$state)\ndf$`T`       <- ifelse(df$state == \"A\" & df$year >= 15, 1, 0)\ndf$Y         <- ifelse(df$state == \"A\" & df$year >= 15, \n                       df$Y + 20, df$Y)\nstr(df)\n#> 'data.frame':    300 obs. of  7 variables:\n#>  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...\n#>  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...\n#>  $ X2       : num  1.96 0.4 -0.75 -0.56 -0.45 1.06 0.51 -2.1 0 0.54 ...\n#>  $ state    : chr  \"A\" \"A\" \"A\" \"A\" ...\n#>  $ year     : num  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ T        : num  0 0 0 0 0 0 0 0 0 0 ...\ndataprep.out <-\n    dataprep(\n        df,\n        predictors            = c(\"X1\", \"X2\"),\n        dependent             = \"Y\",\n        unit.variable         = \"state.num\",\n        time.variable         = \"year\",\n        unit.names.variable   = \"state\",\n        treatment.identifier  = 1,\n        controls.identifier   = c(2:10),\n        time.predictors.prior = c(1:14),\n        time.optimize.ssr     = c(1:14),\n        time.plot             = c(1:30)\n    )\n\n\nsynth.out <- synth(dataprep.out)\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 9.831789 \n#> \n#> solution.v:\n#>  0.3888387 0.6111613 \n#> \n#> solution.w:\n#>  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853\nprint(synth.tables   <- synth.tab(\n        dataprep.res = dataprep.out,\n        synth.res    = synth.out)\n      )\n#> $tab.pred\n#>    Treated Synthetic Sample Mean\n#> X1   2.028     2.028       2.017\n#> X2   0.513     0.513       0.394\n#> \n#> $tab.v\n#>    v.weights\n#> X1 0.389    \n#> X2 0.611    \n#> \n#> $tab.w\n#>    w.weights unit.names unit.numbers\n#> 2      0.112          B            2\n#> 3      0.183          C            3\n#> 4      0.103          D            4\n#> 5      0.312          E            5\n#> 6      0.061          F            6\n#> 7      0.035          G            7\n#> 8      0.059          H            8\n#> 9      0.057          I            9\n#> 10     0.078          J           10\n#> \n#> $tab.loss\n#>            Loss W   Loss V\n#> [1,] 9.761708e-12 9.831789\npath.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Y\"),\n          Xlab         = c(\"Year\"),\n          Legend       = c(\"State A\",\"Synthetic State A\"),\n          Legend.position = c(\"topleft\")\n)\n\nabline(v   = 15,\n       lty = 2)\ngaps.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Gap\"),\n          Xlab         = c(\"Year\"),\n          Ylim         = c(-30, 30),\n          Main         = \"\"\n)\n\nabline(v   = 15,\n       lty = 2)gsynth.out <- gsynth(\n  Y ~ `T` + X1 + X2,\n  data = df,\n  index = c(\"state\", \"year\"),\n  force = \"two-way\",\n  CV = TRUE,\n  r = c(0, 5),\n  se = TRUE,\n  inference = \"parametric\",\n  nboots = 1000,\n  parallel = F # TRUE\n)\n#> Cross-validating ... \n#>  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502\n#>  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375\n#>  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*\n#>  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319\n#>  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301\n#>  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596\n#> \n#>  r* = 2\n#> \n#> \nSimulating errors .............\nBootstrapping ...\n#> ..........\nplot(gsynth.out)\nplot(gsynth.out, type = \"counterfactual\")\nplot(gsynth.out, type = \"counterfactual\", raw = \"all\") \n# shows estimations for the control cases"},{"path":"synthetic-control.html","id":"example-2-1","chapter":"28 Synthetic Control","heading":"28.1.2 Example 2","text":"Leihua Yetransform data used synth()\\(X_1\\) = control case treatment\\(X_1\\) = control case treatment\\(X_0\\) = control cases treatment\\(X_0\\) = control cases treatment\\(Z_1\\): treatment case treatment\\(Z_1\\): treatment case treatment\\(Z_0\\): treatment case treatment\\(Z_0\\): treatment case treatmentCalculate difference real basque region synthetic controlRelative importance unitDoubly Robust Difference--DifferencesExample DRDID packageEstimate Average Treatment Effect Treated using Improved Locally Efficient Doubly Robust estimator","code":"\n\nlibrary(Synth)\ndata(\"basque\")\ndim(basque) #774*17\n#> [1] 774  17\nhead(basque)\n#>   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry\n#> 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA\n#> 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA\n#> 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA\n#> 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA\n#> 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA\n#> 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA\n#>   sec.construction sec.services.venta sec.services.nonventa school.illit\n#> 1               NA                 NA                    NA           NA\n#> 2               NA                 NA                    NA           NA\n#> 3               NA                 NA                    NA           NA\n#> 4               NA                 NA                    NA           NA\n#> 5               NA                 NA                    NA           NA\n#> 6               NA                 NA                    NA           NA\n#>   school.prim school.med school.high school.post.high popdens invest\n#> 1          NA         NA          NA               NA      NA     NA\n#> 2          NA         NA          NA               NA      NA     NA\n#> 3          NA         NA          NA               NA      NA     NA\n#> 4          NA         NA          NA               NA      NA     NA\n#> 5          NA         NA          NA               NA      NA     NA\n#> 6          NA         NA          NA               NA      NA     NA\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op =  \"mean\",\n    # the operator\n    time.predictors.prior = 1964:1969,\n    #the entire time frame from the #beginning to the end\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969,  \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969,  \"mean\")\n    ),\n    dependent =  \"gdpcap\",\n    # dv\n    unit.variable =  \"regionno\",\n    #identifying unit numbers\n    unit.names.variable =  \"regionname\",\n    #identifying unit names\n    time.variable =  \"year\",\n    #time-periods\n    treatment.identifier = 17,\n    #the treated case\n    controls.identifier = c(2:16, 18),\n    #the control cases; all others #except number 17\n    time.optimize.ssr = 1960:1969,\n    #the time-period over which to optimize\n    time.plot = 1955:1997\n) #the entire time period before/after the treatment\nsynth.out = synth(data.prep.obj = dataprep.out, method = \"BFGS\")\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 0.008864606 \n#> \n#> solution.v:\n#>  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n#> \n#> solution.w:\n#>  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps = dataprep.out$Y1plot - (dataprep.out$Y0plot \n                                     %*% synth.out$solution.w)\ngaps[1:3,1]\n#>       1955       1956       1957 \n#> 0.15023473 0.09168035 0.03716475\nsynth.tables = synth.tab(dataprep.res = dataprep.out,\n                         synth.res = synth.out)\nnames(synth.tables)\n#> [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\nsynth.tables$tab.pred[1:13,]\n#>                                          Treated Synthetic Sample Mean\n#> school.illit                              39.888   256.337     170.786\n#> school.prim                             1031.742  2730.104    1127.186\n#> school.med                                90.359   223.340      76.260\n#> school.high                               25.728    63.437      24.235\n#> school.post.high                          13.480    36.153      13.478\n#> invest                                    24.647    21.583      21.424\n#> special.gdpcap.1960.1969                   5.285     5.271       3.581\n#> special.sec.agriculture.1961.1969          6.844     6.179      21.353\n#> special.sec.energy.1961.1969               4.106     2.760       5.310\n#> special.sec.industry.1961.1969            45.082    37.636      22.425\n#> special.sec.construction.1961.1969         6.150     6.952       7.276\n#> special.sec.services.venta.1961.1969      33.754    41.104      36.528\n#> special.sec.services.nonventa.1961.1969    4.072     5.371       7.111\nsynth.tables$tab.w[8:14, ]\n#>    w.weights            unit.names unit.numbers\n#> 9      0.000    Castilla-La Mancha            9\n#> 10     0.851              Cataluna           10\n#> 11     0.000  Comunidad Valenciana           11\n#> 12     0.000           Extremadura           12\n#> 13     0.000               Galicia           13\n#> 14     0.149 Madrid (Comunidad De)           14\n#> 15     0.000    Murcia (Region de)           15\n# plot the changes before and after the treatment \npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita gdp (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab =  \"gap in real per - capita GDP (1986 USD, thousand)\",\n    Xlab =  \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)\nlibrary(DRDID)\ndata(nsw_long)\n# Form the Lalonde sample with CPS comparison group\neval_lalonde_cps <- subset(nsw_long, nsw_long$treated == 0 | \n                               nsw_long$sample == 2)\nout <-\n    drdid(\n        yname = \"re\",\n        tname = \"year\",\n        idname = \"id\",\n        dname = \"experimental\",\n        xformla = ~ age + educ + black + married + nodegree + hisp + re74,\n        data = eval_lalonde_cps,\n        panel = TRUE\n    )\nsummary(out)\n#>  Call:\n#> drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n#>     xformla = ~age + educ + black + married + nodegree + hisp + \n#>         re74, data = eval_lalonde_cps, panel = TRUE)\n#> ------------------------------------------------------------------\n#>  Further improved locally efficient DR DID estimator for the ATT:\n#>  \n#>    ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n#> -901.2703   393.6247   -2.2897     0.022    -1672.7747  -129.766 \n#> ------------------------------------------------------------------\n#>  Estimator based on panel data.\n#>  Outcome regression est. method: weighted least squares.\n#>  Propensity score est. method: inverse prob. tilting.\n#>  Analytical standard error.\n#> ------------------------------------------------------------------\n#>  See Sant'Anna and Zhao (2020) for details."},{"path":"synthetic-control.html","id":"example-3-1","chapter":"28 Synthetic Control","heading":"28.1.3 Example 3","text":"Synth package’s authorssynth() requires\\(X_1\\) vector treatment predictors\\(X_1\\) vector treatment predictors\\(X_0\\) matrix variables control group\\(X_0\\) matrix variables control group\\(Z_1\\) vector outcome variable treatment group\\(Z_1\\) vector outcome variable treatment group\\(Z_0\\) matrix outcome variable control group\\(Z_0\\) matrix outcome variable control groupuse dataprep() prepare data format can used throughout Synth packagefind optimal weights identifies synthetic control treatment groupYou also run placebo tests","code":"\nlibrary(Synth)\ndata(\"basque\")\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op = \"mean\",\n    time.predictors.prior = 1964:1969,\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969 , \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969, \"mean\")\n    ),\n    dependent = \"gdpcap\",\n    unit.variable = \"regionno\",\n    unit.names.variable = \"regionname\",\n    time.variable = \"year\",\n    treatment.identifier = 17,\n    controls.identifier = c(2:16, 18),\n    time.optimize.ssr = 1960:1969,\n    time.plot = 1955:1997\n)\nsynth.out <- synth(data.prep.obj = dataprep.out, method = \"BFGS\")\n#> \n#> X1, X0, Z1, Z0 all come directly from dataprep object.\n#> \n#> \n#> **************** \n#>  searching for synthetic control unit  \n#>  \n#> \n#> **************** \n#> **************** \n#> **************** \n#> \n#> MSPE (LOSS V): 0.008864606 \n#> \n#> solution.v:\n#>  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n#> \n#> solution.w:\n#>  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps <- dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w)\ngaps[1:3, 1]\n#>       1955       1956       1957 \n#> 0.15023473 0.09168035 0.03716475\nsynth.tables <-\n    synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nnames(synth.tables) # you can pick tables to see \n#> [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"gap in real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)"},{"path":"synthetic-control.html","id":"example-4-1","chapter":"28 Synthetic Control","heading":"28.1.4 Example 4","text":"Michael Robbins Steven Davenport authors MicroSynth following improvements:Standardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsStandardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsOmnibus statistic (set omnibus.var ) multiple outcome variablesOmnibus statistic (set omnibus.var ) multiple outcome variablesincorporate multiple follow-periods end.postincorporate multiple follow-periods end.postNotes:predictors outcome used match units intervention\nOutcome variable time-variant\nPredictors time-invariant\npredictors outcome used match units interventionOutcome variable time-variantOutcome variable time-variantPredictors time-invariantPredictors time-invariant","code":"\n# right now the package is not availabe for R version 4.2\nlibrary(microsynth)\ndata(\"seattledmi\")\n\n\ncov.var <-\n    c(\n        \"TotalPop\",\n        \"BLACK\",\n        \"HISPANIC\",\n        \"Males_1521\",\n        \"HOUSEHOLDS\",\n        \"FAMILYHOUS\",\n        \"FEMALE_HOU\",\n        \"RENTER_HOU\",\n        \"VACANT_HOU\"\n    )\nmatch.out <- c(\"i_felony\", \"i_misdemea\", \"i_drugs\", \"any_crime\")\nsea1 <- microsynth(\n    seattledmi,\n    idvar       = \"ID\",\n    timevar     = \"time\",\n    intvar      = \"Intervention\",\n    start.pre   = 1,\n    end.pre     = 12,\n    end.post    = 16,\n    match.out   = match.out, # outcome variable will be matched on exactly\n    match.covar = cov.var, # specify covariates will be matched on exactly\n    result.var  = match.out, # used to report results\n    omnibus.var = match.out, # feature in the omnibus p-value\n    test        = \"lower\",\n    n.cores     = min(parallel::detectCores(), 2)\n)\nsea1\nsummary(sea1)\nplot_microsynth(sea1)\nsea2 <- microsynth(\n    seattledmi,\n    idvar = \"ID\",\n    timevar = \"time\",\n    intvar = \"Intervention\",\n    start.pre = 1,\n    end.pre = 12,\n    end.post = c(14, 16),\n    match.out = match.out,\n    match.covar = cov.var,\n    result.var = match.out,\n    omnibus.var = match.out,\n    test = \"lower\",\n    perm = 250,\n    jack = TRUE,\n    n.cores = min(parallel::detectCores(), 2)\n)"},{"path":"synthetic-control.html","id":"augmented-synthetic-control-method","chapter":"28 Synthetic Control","heading":"28.2 Augmented Synthetic Control Method","text":"package: augsynth (Ben-Michael, Feller, Rothstein 2021)","code":""},{"path":"synthetic-control.html","id":"synthetic-control-with-staggered-adoption","chapter":"28 Synthetic Control","heading":"28.3 Synthetic Control with Staggered Adoption","text":"references: https://ebenmichael.github.io/assets/research/jamboree.pdf (Ben-Michael, Feller, Rothstein 2022) package: augsynth","code":""},{"path":"synthetic-control.html","id":"bayesian-synthetic-control","chapter":"28 Synthetic Control","heading":"28.4 Bayesian Synthetic Control","text":"S. Kim, Lee, Gupta (2020)Pang, Liu, Xu (2022)","code":""},{"path":"synthetic-control.html","id":"generalized-synthetic-control","chapter":"28 Synthetic Control","heading":"28.5 Generalized Synthetic Control","text":"reference: (Xu 2017)Bootstrap procedure biased (K. T. Li Sonnier 2023). Hence, need follow K. T. Li Sonnier (2023) terms SEs estimation.","code":""},{"path":"synthetic-control.html","id":"other-advances","chapter":"28 Synthetic Control","heading":"28.6 Other Advances","text":"L. Sun, Ben-Michael, Feller (2023) Using Multiple Outcomes Improve SCMCommon Weights Across Outcomes: paper proposes using single set synthetic control weights across multiple outcomes, rather estimating separate weights outcome.Reduced Bias Low-Rank Factor Model: balancing vector index outcomes, approach yields lower bias bounds low-rank factor model, improvements number outcomes increases.Reduced Bias Low-Rank Factor Model: balancing vector index outcomes, approach yields lower bias bounds low-rank factor model, improvements number outcomes increases.Evidence: re-analysis Flint water crisis’s impact educational outcome.Evidence: re-analysis Flint water crisis’s impact educational outcome.","code":""},{"path":"event-studies.html","id":"event-studies","chapter":"29 Event Studies","heading":"29 Event Studies","text":"earliest paper used event study (Dolley 1933)(Campbell et al. 1998) introduced method, based efficient markets theory (Fama 1970)Review:(McWilliams Siegel 1997): management(McWilliams Siegel 1997): management(. Sorescu, Warren, Ertekin 2017): marketing(. Sorescu, Warren, Ertekin 2017): marketingPrevious marketing studies:Firm-initiated activities(Horsky Swyngedouw 1987): name change(Horsky Swyngedouw 1987): name change(Chaney, Devinney, Winer 1991) new product announcements(Chaney, Devinney, Winer 1991) new product announcements(Agrawal Kamakura 1995): celebrity endorsement(Agrawal Kamakura 1995): celebrity endorsement(Lane Jacobson 1995): brand extensions(Lane Jacobson 1995): brand extensions(Houston Johnson 2000): joint venture(Houston Johnson 2000): joint venture(Geyskens, Gielens, Dekimpe 2002): Internet channel (newspapers)(Geyskens, Gielens, Dekimpe 2002): Internet channel (newspapers)(Cornwell, Pruitt, Clark 2005): sponsorship announcements(Cornwell, Pruitt, Clark 2005): sponsorship announcements(Elberse 2007): casting announcements(Elberse 2007): casting announcements(. B. Sorescu, Chandy, Prabhu 2007): M&(. B. Sorescu, Chandy, Prabhu 2007): M&(Sood Tellis 2009): innovation payoff(Sood Tellis 2009): innovation payoff(Wiles Danielova 2009): product placements movies(Wiles Danielova 2009): product placements movies(Joshi Hanssens 2009): movie releases(Joshi Hanssens 2009): movie releases(Wiles et al. 2010): Regulatory Reports Deceptive Advertising(Wiles et al. 2010): Regulatory Reports Deceptive Advertising(Boyd, Chandy, Cunha Jr 2010): new CMO appointments(Boyd, Chandy, Cunha Jr 2010): new CMO appointments(Karniouchina, Uslay, Erenburg 2011): product placement(Karniouchina, Uslay, Erenburg 2011): product placement(Wiles, Morgan, Rego 2012): Brand Acquisition Disposal(Wiles, Morgan, Rego 2012): Brand Acquisition Disposal(Kalaignanam Bahadir 2013): corporate brand name change(Kalaignanam Bahadir 2013): corporate brand name change(Raassens, Wuyts, Geyskens 2012): new product development outsourcing(Raassens, Wuyts, Geyskens 2012): new product development outsourcing(Mazodier Rezaee 2013): sports announcements(Mazodier Rezaee 2013): sports announcements(Borah Tellis 2014): make, buy ally innovations(Borah Tellis 2014): make, buy ally innovations(Homburg, Vollmayr, Hahn 2014): channel expansions(Homburg, Vollmayr, Hahn 2014): channel expansions(Fang, Lee, Yang 2015): Co-development agreements(Fang, Lee, Yang 2015): Co-development agreements(Wu et al. 2015): horizontal collaboration new product development(Wu et al. 2015): horizontal collaboration new product development(Fama et al. 1969): stock split(Fama et al. 1969): stock splitNon-firm-initiated activities(. B. Sorescu, Chandy, Prabhu 2003): FDA approvals(. B. Sorescu, Chandy, Prabhu 2003): FDA approvals(Pandey, Shanahan, Hansen 2005): diversity elite list(Pandey, Shanahan, Hansen 2005): diversity elite list(Balasubramanian, Mathur, Thakur 2005): high-quality achievements(Balasubramanian, Mathur, Thakur 2005): high-quality achievements(Tellis Johnson 2007): quality reviews Walter Mossberg(Tellis Johnson 2007): quality reviews Walter Mossberg(Fornell et al. 2006): customer satisfaction(Fornell et al. 2006): customer satisfaction(Gielens et al. 2008): Walmart’s entry UK market(Gielens et al. 2008): Walmart’s entry UK market(Boyd Spekman 2008): indirect ties(Boyd Spekman 2008): indirect ties(R. S. Rao, Chandy, Prabhu 2008): FDA approvals(R. S. Rao, Chandy, Prabhu 2008): FDA approvals(Ittner, Larcker, Taylor 2009): customer satisfaction(Ittner, Larcker, Taylor 2009): customer satisfaction(Tipton, Bharadwaj, Robertson 2009): Deceptive advertising(Tipton, Bharadwaj, Robertson 2009): Deceptive advertising(Y. Chen, Ganesan, Liu 2009): product recalls(Y. Chen, Ganesan, Liu 2009): product recalls(Jacobson Mizik 2009): satisfaction score release(Jacobson Mizik 2009): satisfaction score release(Karniouchina, Moore, Cooney 2009): Mad money Jim Cramer(Karniouchina, Moore, Cooney 2009): Mad money Jim Cramer(Wiles et al. 2010): deceptive advertising(Wiles et al. 2010): deceptive advertising(Y. Chen, Liu, Zhang 2012): third-party movie reviews(Y. Chen, Liu, Zhang 2012): third-party movie reviews(Xiong Bharadwaj 2013): positive negative news(Xiong Bharadwaj 2013): positive negative news(Gao et al. 2015): product recall(Gao et al. 2015): product recall(Malhotra Kubowicz Malhotra 2011): data breach(Malhotra Kubowicz Malhotra 2011): data breach(Bhagat, Bizjak, Coles 1998): litigation(Bhagat, Bizjak, Coles 1998): litigationPotential avenues:Ad campaignsAd campaignsMarket entryMarket entryproduct failure/recallsproduct failure/recallsPatentsPatentsPros:Better accounting based measures (e.g., profits) managers can manipulate profits (Benston 1985)Better accounting based measures (e.g., profits) managers can manipulate profits (Benston 1985)Easy doEasy doFun fact:(Dubow Monteiro 2006) came way gauge ‘clean’ market . based measure much prices seemed move way suggested insider knowledge, release important regulatory announcements affect stock prices. price shifts might suggest insider trading occurring. Essentially, watching unusual price changes day announcement.Events can beInternal (e.g., stock repurchase)Internal (e.g., stock repurchase)External (e.g., macroeconomic variables)External (e.g., macroeconomic variables)Assumptions:Efficient market theoryShareholders important group among stakeholdersThe event sharply affects share priceExpected return calculated appropriatelySteps:Event Identification: (e.g., dividends, M&, stock buyback, laws regulation, privatization vs. nationalization, celebrity endorsements, name changes, brand extensions etc. see list events US international, see WRDS S&P Capital IQ Key Developments). Events must affect either cash flows discount rate firms (. Sorescu, Warren, Ertekin 2017, 191)\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\n\nSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date\n(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date\nLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).\n\nEvent window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)\nOne day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)\nTwo days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)\n10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)\n\nPost Event window: \\(T_2 \\T_3\\)\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\n\nSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date\n(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date\nLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).\nEstimation window: Normal return expected return (\\(T_0 \\T_1\\)) (sometimes include days capture leakages).Recommendation (Johnston 2007) use 250 days event (45-day estimation window event window).\n(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).\n(Gielens et al. 2008) 260 10 days 300 46 days \n(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.\nRecommendation (Johnston 2007) use 250 days event (45-day estimation window event window).(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).(Wiles, Morgan, Rego 2012) used 90-trading-day estimation window ending 6 days event (consistent finance literature).(Gielens et al. 2008) 260 10 days 300 46 days (Gielens et al. 2008) 260 10 days 300 46 days (Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.(Tirunillai Tellis 2012) estimation window 255 days ends 46 days event.Similarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event dateSimilarly, (McWilliams Siegel 1997) (Fornell et al. 2006) 255 days ending 46 days event date(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event date(. Sorescu, Warren, Ertekin 2017, 194) suggest 100 days event dateLeakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).Leakage: try cover broad news sources possible (LexisNexis, Factiva, RavenPack).Event window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)\nOne day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)\nTwo days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)\n10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)\nEvent window: contain event date (\\(T_1 \\T_2\\)) (argue event window can’t empirically)One day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)One day: (Balasubramanian, Mathur, Thakur 2005; Boyd, Chandy, Cunha Jr 2010; Fornell et al. 2006)Two days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)Two days: (Raassens, Wuyts, Geyskens 2012; Sood Tellis 2009)10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)10 days: (Cornwell, Pruitt, Clark 2005; Kalaignanam Bahadir 2013; . B. Sorescu, Chandy, Prabhu 2007)Post Event window: \\(T_2 \\T_3\\)Post Event window: \\(T_2 \\T_3\\)Normal vs. Abnormal returns\\[\n\\epsilon_{}^* = \\frac{P_{} - E(P_{})}{P_{-1}} = R_{} - E(R_{}|X_t)\n\\]\\(\\epsilon_{}^*\\) = abnormal return\\(\\epsilon_{}^*\\) = abnormal return\\(R_{}\\) = realized (actual) return\\(R_{}\\) = realized (actual) return\\(P\\) = dividend-adjusted price stock\\(P\\) = dividend-adjusted price stock\\(E(R_{}|X_t)\\) normal expected return\\(E(R_{}|X_t)\\) normal expected returnThere several model calculate expected returnA. Statistical Models: assumes jointly multivariate normal iid time (need distributional assumptions valid finite-sample estimation) rather robust (hence, recommended)Constant Mean Return ModelMarket ModelAdjusted Market Return ModelFactor ModelB. Economic Model (strong assumption regarding investor behavior)Capital Asset Pricing Model (CAPM)Arbitrage Pricing Theory (APT)","code":""},{"path":"event-studies.html","id":"other-issues","chapter":"29 Event Studies","heading":"29.1 Other Issues","text":"","code":""},{"path":"event-studies.html","id":"event-studies-in-marketing","chapter":"29 Event Studies","heading":"29.1.1 Event Studies in marketing","text":"(Skiera, Bayer, Schöler 2017) dependent variable marketing-related event studies?Based valuation theory, Shareholder value = value operating business + non-operating asset - debt (Schulze, Skiera, Wiesel 2012)\nMany marketing events affect operating business value, non-operating assets debt\nBased valuation theory, Shareholder value = value operating business + non-operating asset - debt (Schulze, Skiera, Wiesel 2012)Many marketing events affect operating business value, non-operating assets debtIgnoring differences firm-specific leverage effects dual effects:\ninflates impact observation pertaining firms large debt\ndeflates pertaining firms large non-operating asset.\nIgnoring differences firm-specific leverage effects dual effects:inflates impact observation pertaining firms large debtinflates impact observation pertaining firms large debtdeflates pertaining firms large non-operating asset.deflates pertaining firms large non-operating asset.’s recommended marketing papers report \\(CAR^{OB}\\) \\(CAR^{SHV}\\) argue whichever one appropriate.’s recommended marketing papers report \\(CAR^{OB}\\) \\(CAR^{SHV}\\) argue whichever one appropriate.paper, two previous event studies control financial structure: (Gielens et al. 2008) (Chaney, Devinney, Winer 1991)paper, two previous event studies control financial structure: (Gielens et al. 2008) (Chaney, Devinney, Winer 1991)Definitions:Cumulative abnormal percentage return shareholder value (\\(CAR^{SHV}\\))\nShareholder value refers firm’s market capitalization = share price x # shares.\nCumulative abnormal percentage return shareholder value (\\(CAR^{SHV}\\))Shareholder value refers firm’s market capitalization = share price x # shares.Cumulative abnormal percentage return value operating business (\\(CAR^{OB}\\))\n\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)\nLeverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).\nValue operating business = shareholder value - non-operating assets + debt\nLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm size\ndebt = long-term + short-term debt; long-term debt\nfirm size = book value equity; market cap; total assets; debt + equity\n\nCumulative abnormal percentage return value operating business (\\(CAR^{OB}\\))\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)\\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{}\\)Leverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).Leverage effect = Operating business value / Shareholder value (LE describes 1% change operating business translates percentage change shareholder value).Value operating business = shareholder value - non-operating assets + debtValue operating business = shareholder value - non-operating assets + debtLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm size\ndebt = long-term + short-term debt; long-term debt\nfirm size = book value equity; market cap; total assets; debt + equity\nLeverage effect \\(\\neq\\) leverage ratio, leverage ratio debt / firm sizedebt = long-term + short-term debt; long-term debtdebt = long-term + short-term debt; long-term debtfirm size = book value equity; market cap; total assets; debt + equityfirm size = book value equity; market cap; total assets; debt + equityOperating assets used firm core business operations (e..g, property, plant, equipment, natural resources, intangible asset)Operating assets used firm core business operations (e..g, property, plant, equipment, natural resources, intangible asset)Non–operating assets (redundant assets), play role firm’s operations, still generate form return (e.g., excess cash , marketable securities - commercial papers, market instruments)Non–operating assets (redundant assets), play role firm’s operations, still generate form return (e.g., excess cash , marketable securities - commercial papers, market instruments)Marketing events usually influence value firm’s operating assets (specifically intangible assets). , changes value operating business can impact shareholder value.Three rare instances marketing events can affect non-operating assets debt\n(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt\n(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt\n(Bhaduri 2002) production unique products.\nThree rare instances marketing events can affect non-operating assets debt(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt(G. C. Hall, Hutchinson, Michaelas 2004): excess pre-orderings can influence short-term debt(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt(Berger, Ofek, Yermack 1997) Firing CMO increase debt manager’s tenure negatively associated firm’s debt(Bhaduri 2002) production unique products.(Bhaduri 2002) production unique products.marketing-related event can either influencevalue components firm’s value (= firm’s operating business, non-operating assets debt)value components firm’s value (= firm’s operating business, non-operating assets debt)operating business.operating business.Replication leverage effect\\[\n\\begin{aligned}\n\\text{leverage effect} &= \\frac{\\text{operating business}}{\\text{shareholder value}} \\\\\n&= \\frac{\\text{(shareholder value - non-operating assets + debt)}}{\\text{shareholder value}} \\\\\n&= \\frac{prcc_f \\times csho - ivst + dd1 + dltt + pstk}{prcc_f \\times csho}\n\\end{aligned}\n\\]Compustat Data Itemshort-term investments(Non-operating assets)Since WRDS longer maintains S&P 500 list time writing, can’t replicate list used (Skiera, Bayer, Schöler 2017) paper.","code":"\nlibrary(tidyverse)\ndf_leverage_effect <- read.csv(\"data/leverage_effect.csv.gz\") %>% \n    \n    # get active firms only\n    filter(costat == \"A\") %>% \n    \n    # drop missing values\n    drop_na() %>% \n    \n    # create the leverage effect variable\n    mutate(le = (prcc_f * csho - ivst + dd1 + dltt + pstk)/ (prcc_f * csho)) %>% \n    \n    # get shareholder value\n    mutate(shv = prcc_f * csho) %>% \n    \n    # remove Infinity value for leverage effect (i.e., shareholder value = 0)\n    filter_all(all_vars(!is.infinite(.))) %>% \n    \n    # positive values only \n    filter_all(all_vars(. > 0)) %>% \n    \n    # get the within coefficient of variation\n    group_by(gvkey) %>% \n    mutate(within_var_mean_le = mean(le),\n           within_var_sd_le = sd(le)) %>% \n    ungroup()\n\n\n# get the mean and standard deviation\nmean(df_leverage_effect$le)\n#> [1] 150.1087\nmax(df_leverage_effect$le)\n#> [1] 183629.6\nhist(df_leverage_effect$le)\n\n# coefficient of variation \nsd(df_leverage_effect$le) / mean(df_leverage_effect$le) * 100\n#> [1] 2749.084\n\n# Within-firm variation (similar to fig 3a)\ndf_leverage_effect %>% \n    group_by(gvkey) %>% \n    slice(1) %>% \n    ungroup() %>% \n    dplyr::select(within_var_mean_le, within_var_sd_le) %>% \n    dplyr::mutate(cv = within_var_sd_le/ within_var_mean_le) %>% \n    dplyr::select(cv) %>% \n    pull() %>% \n    hist()"},{"path":"event-studies.html","id":"economic-significance","chapter":"29 Event Studies","heading":"29.1.2 Economic significance","text":"Total wealth gain (loss) event\\[\n\\Delta W_t = CAR_t \\times MKTVAL_0\n\\]\\(\\Delta W_t\\) = gain (loss)\\(\\Delta W_t\\) = gain (loss)\\(CAR_t\\) = cumulative residuals date \\(t\\)\\(CAR_t\\) = cumulative residuals date \\(t\\)\\(MKTVAL_0\\) market value firm event window\\(MKTVAL_0\\) market value firm event window","code":""},{"path":"event-studies.html","id":"statistical-power","chapter":"29 Event Studies","heading":"29.1.3 Statistical Power","text":"increases withmore firmsmore firmsless days event window (avoiding potential contamination confounds)less days event window (avoiding potential contamination confounds)","code":""},{"path":"event-studies.html","id":"testing","chapter":"29 Event Studies","heading":"29.2 Testing","text":"","code":""},{"path":"event-studies.html","id":"parametric-test","chapter":"29 Event Studies","heading":"29.2.1 Parametric Test","text":"(S. J. Brown Warner 1985) provide evidence even presence non-normality, parametric tests still perform well. Since proportion positive negative abnormal returns tends equal sample (least 5 securities). excess returns coverage normality sample size increases. Hence, parametric test advocated non-parametric one.Low power detect significance (Kothari Warner 1997)Power = f(sample, size, actual size abnormal returns, variance abnormal returns across firms)","code":""},{"path":"event-studies.html","id":"t-test","chapter":"29 Event Studies","heading":"29.2.1.1 T-test","text":"Applying CLT\\[\n\\begin{aligned}\nt_{CAR} &= \\frac{\\bar{CAR_{}}}{\\sigma (CAR_{})/\\sqrt{n}} \\\\\nt_{BHAR} &= \\frac{\\bar{BHAR_{}}}{\\sigma (BHAR_{})/\\sqrt{n}}\n\\end{aligned}\n\\]AssumeAbnormal returns normally distributedAbnormal returns normally distributedVar(abnormal returns) equal across firmsVar(abnormal returns) equal across firmsNo cross-correlation abnormal returns.cross-correlation abnormal returns.Hence, misspecified suspectedHeteroskedasticityHeteroskedasticityCross-sectional dependenceCross-sectional dependenceTechnically, abnormal returns follow non-normal distribution (design abnormal returns calculation, typically forces distribution normal)Technically, abnormal returns follow non-normal distribution (design abnormal returns calculation, typically forces distribution normal)address concerns, Patell Standardized Residual (PSR) can sometimes help.","code":""},{"path":"event-studies.html","id":"patell-standardized-residual-psr","chapter":"29 Event Studies","heading":"29.2.1.2 Patell Standardized Residual (PSR)","text":"(Patell 1976)Since market model uses observations outside event window, abnormal returns contain prediction errors top true residuals , standardized:\\[\nAR_{} = \\frac{\\hat{u}_{}}{s_i \\sqrt{C_{}}}\n\\]\\(\\hat{u}_{}\\) = estimated residual\\(\\hat{u}_{}\\) = estimated residual\\(s_i\\) = standard deviation estimate residuals (estimation period)\\(s_i\\) = standard deviation estimate residuals (estimation period)\\(C_{}\\) = correction account prediction’s increased variation outside estimation period (Strong 1992)\\(C_{}\\) = correction account prediction’s increased variation outside estimation period (Strong 1992)\\[\nC_{} = 1 + \\frac{1}{T} + \\frac{(R_{mt} - \\bar{R}_m)^2}{\\sum_t (R_{mt} - \\bar{R}_m)^2}\n\\]\\(T\\) = number observations (estimation period)\\(T\\) = number observations (estimation period)\\(R_{mt}\\) = average rate return stocks trading stock market time \\(t\\)\\(R_{mt}\\) = average rate return stocks trading stock market time \\(t\\)\\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^T R_{mt}\\)\\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^T R_{mt}\\)","code":""},{"path":"event-studies.html","id":"non-parametric-test","chapter":"29 Event Studies","heading":"29.2.2 Non-parametric Test","text":"assumptions return distributionNo assumptions return distributionSign Test (assumes symmetry returns)\nbinom.test()\nSign Test (assumes symmetry returns)binom.test()Wilcoxon Signed-Rank Test (allows non-symmetry returns)\nUse wilcox.test(sample)\nWilcoxon Signed-Rank Test (allows non-symmetry returns)Use wilcox.test(sample)Gen Sign TestGen Sign TestCorrado Rank TestCorrado Rank Test","code":""},{"path":"event-studies.html","id":"sample","chapter":"29 Event Studies","heading":"29.3 Sample","text":"Sample can relative small\n(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcements\nCan range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)\nSample can relative small(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcements(Wiles, Morgan, Rego 2012) 572 acquisition announcements, 308 disposal announcementsCan range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)Can range 71 (Markovitch Golder 2008) 3552 (Borah Tellis 2014)","code":""},{"path":"event-studies.html","id":"confounders","chapter":"29 Event Studies","heading":"29.3.1 Confounders","text":"Avoid confounding events: earnings announcements, key executive changes, unexpected stock buybacks, changes dividends within two-trading day window surrounding event, mergers acquisitions, spin-offers, stock splits, management changes, joint ventures, unexpected dividend, IPO, debt defaults, dividend cancellations (McWilliams Siegel 1997)According (Fornell et al. 2006), need control:one-day event period = day Wall Street Journal publish ACSI announcement.one-day event period = day Wall Street Journal publish ACSI announcement.5 days event rule news (PR Newswires, Dow Jones, Business Wires)\nM&, Spin-offs, stock splits\nCEO CFO changes,\nLayoffs, restructurings, earnings announcements, lawsuits\nCapital IQ - Key Developments: covers almost important events don’t search news.\n5 days event rule news (PR Newswires, Dow Jones, Business Wires)M&, Spin-offs, stock splitsM&, Spin-offs, stock splitsCEO CFO changes,CEO CFO changes,Layoffs, restructurings, earnings announcements, lawsuitsLayoffs, restructurings, earnings announcements, lawsuitsCapital IQ - Key Developments: covers almost important events don’t search news.Capital IQ - Key Developments: covers almost important events don’t search news.(. Sorescu, Warren, Ertekin 2017) examine confounding events short-term windows:RavenPack, 3982 US publicly traded firms, press releases (2000-2013)RavenPack, 3982 US publicly traded firms, press releases (2000-2013)3-day window around event dates3-day window around event datesThe difference sample full observations sample without confounded events negligible (non-significant).difference sample full observations sample without confounded events negligible (non-significant).Conclusion: excluding confounded observations may unnecessary short-term event studies.\nBiases can stem researchers pick choose events exclude\ntime progresses, events need exclude can infeasible.\nConclusion: excluding confounded observations may unnecessary short-term event studies.Biases can stem researchers pick choose events excludeBiases can stem researchers pick choose events excludeAs time progresses, events need exclude can infeasible.time progresses, events need exclude can infeasible.illustrate point, let’s quick simulation exerciseIn example, explore three types events:Focal eventsFocal eventsCorrelated events (.e., events correlated focal events; presence correlated events can follow presence focal event)Correlated events (.e., events correlated focal events; presence correlated events can follow presence focal event)Uncorrelated events (.e., events dates might randomly coincide focal events, correlated ).Uncorrelated events (.e., events dates might randomly coincide focal events, correlated ).ability control strength correlation focal correlated events study, well number unrelated events wish examine.Let’s examine implications including excluding correlated uncorrelated events estimates focal events.depicted plot, inclusion correlated events demonstrates minimal impact estimation focal events. Conversely, excluding correlated events can diminish statistical power. true cases pronounced correlation.However, consequences excluding unrelated events notably significant. becomes evident omitting around 40 unrelated events study, lose ability accurately identify true effects focal events. reality within research, often rely Key Developments database, excluding 150 events, practice can substantially impair capacity ascertain authentic impact focal events.little experiment really drives home point – better darn good reason exclude event study (make super convincing)!","code":"\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tidyverse)\n\n# Parameters\nn                  <- 100000         # Number of observations\nn_focal            <- round(n * 0.2) # Number of focal events\noverlap_correlated <- 0.5            # Overlapping percentage between focal and correlated events\n\n# Function to compute mean and confidence interval\nmean_ci <- function(x) {\n    m <- mean(x)\n    ci <- qt(0.975, length(x)-1) * sd(x) / sqrt(length(x)) # 95% confidence interval\n    list(mean = m, lower = m - ci, upper = m + ci)\n}\n\n# Simulate data\nset.seed(42)\ndata <- tibble(\n    date       = seq.Date(from = as.Date(\"2010-01-01\"), by = \"day\", length.out = n), # Date sequence\n    focal      = rep(0, n),\n    correlated = rep(0, n),\n    ab_ret     = rnorm(n)\n)\n\n\n# Define focal events\nfocal_idx <- sample(1:n, n_focal)\ndata$focal[focal_idx] <- 1\n\ntrue_effect <- 0.25\n\n# Adjust the ab_ret for the focal events to have a mean of true_effect\ndata$ab_ret[focal_idx] <- data$ab_ret[focal_idx] - mean(data$ab_ret[focal_idx]) + true_effect\n\n\n\n# Determine the number of correlated events that overlap with focal and those that don't\nn_correlated_overlap <- round(length(focal_idx) * overlap_correlated)\nn_correlated_non_overlap <- n_correlated_overlap\n\n# Sample the overlapping correlated events from the focal indices\ncorrelated_idx <- sample(focal_idx, size = n_correlated_overlap)\n\n# Get the remaining indices that are not part of focal\nremaining_idx <- setdiff(1:n, focal_idx)\n\n# Check to ensure that we're not attempting to sample more than the available remaining indices\nif (length(remaining_idx) < n_correlated_non_overlap) {\n    stop(\"Not enough remaining indices for non-overlapping correlated events\")\n}\n\n# Sample the non-overlapping correlated events from the remaining indices\ncorrelated_non_focal_idx <- sample(remaining_idx, size = n_correlated_non_overlap)\n\n# Combine the two to get all correlated indices\nall_correlated_idx <- c(correlated_idx, correlated_non_focal_idx)\n\n# Set the correlated events in the data\ndata$correlated[all_correlated_idx] <- 1\n\n\n# Inflate the effect for correlated events to have a mean of \ncorrelated_non_focal_idx <- setdiff(all_correlated_idx, focal_idx) # Fixing the selection of non-focal correlated events\ndata$ab_ret[correlated_non_focal_idx] <- data$ab_ret[correlated_non_focal_idx] - mean(data$ab_ret[correlated_non_focal_idx]) + 1\n\n\n# Define the numbers of uncorrelated events for each scenario\nnum_uncorrelated <- c(5, 10, 20, 30, 40)\n\n# Define uncorrelated events\nfor (num in num_uncorrelated) {\n    for (i in 1:num) {\n        data[paste0(\"uncorrelated_\", i)] <- 0\n        uncorrelated_idx <- sample(1:n, round(n * 0.1))\n        data[uncorrelated_idx, paste0(\"uncorrelated_\", i)] <- 1\n    }\n}\n\n\n# Define uncorrelated columns and scenarios\nunc_cols <- paste0(\"uncorrelated_\", 1:num_uncorrelated)\nresults <- tibble(\n    Scenario = c(\"Include Correlated\", \"Correlated Effects\", \"Exclude Correlated\", \"Exclude Correlated and All Uncorrelated\"),\n    MeanEffect = c(\n        mean_ci(data$ab_ret[data$focal == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$mean,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$mean\n    ),\n    LowerCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$lower,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$lower\n    ),\n    UpperCI = c(\n        mean_ci(data$ab_ret[data$focal == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0])$upper,\n        mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, paste0(\"uncorrelated_\", 1:num_uncorrelated)]) == 0])$upper\n    )\n)\n\n# Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated\nfor (num in num_uncorrelated) {\n    unc_cols <- paste0(\"uncorrelated_\", 1:num)\n    results <- results %>%\n        add_row(\n            Scenario = paste(\"Exclude\", num, \"Uncorrelated\"),\n            MeanEffect = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$mean,\n            LowerCI = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$lower,\n            UpperCI = mean_ci(data$ab_ret[data$focal == 1 & data$correlated == 0 & rowSums(data[, unc_cols]) == 0])$upper\n        )\n}\n\n\nggplot(results,\n       aes(\n           x = factor(Scenario, levels = Scenario),\n           y = MeanEffect,\n           ymin = LowerCI,\n           ymax = UpperCI\n       )) +\n    geom_pointrange() +\n    coord_flip() +\n    ylab(\"Mean Effect\") +\n    xlab(\"Scenario\") +\n    ggtitle(\"Mean Effect of Focal Events under Different Scenarios\") +\n    geom_hline(yintercept = true_effect,\n               linetype = \"dashed\",\n               color = \"red\") "},{"path":"event-studies.html","id":"biases","chapter":"29 Event Studies","heading":"29.4 Biases","text":"Different closing time obscure estimation abnormal returns, check (Campbell et al. 1998)Different closing time obscure estimation abnormal returns, check (Campbell et al. 1998)Upward bias aggregating CAR + transaction prices (bid ask)Upward bias aggregating CAR + transaction prices (bid ask)Cross-sectional dependence returns bias standard deviation estimates downward, inflates test statistics events share common dates (MacKinlay 1997). Hence, (Jaffe 1974) Calendar-time Portfolio Abnormal Returns (CTARs) used correct bias.Cross-sectional dependence returns bias standard deviation estimates downward, inflates test statistics events share common dates (MacKinlay 1997). Hence, (Jaffe 1974) Calendar-time Portfolio Abnormal Returns (CTARs) used correct bias.(Wiles, Morgan, Rego 2012): events confined relatively industries, cross-sectional dependence returns can bias SD estimate downward, inflating associated test statistics” (p. 47). control potential cross-sectional correlation abnormal returns, can use time-series standard deviation test statistic (S. J. Brown Warner 1980)(Wiles, Morgan, Rego 2012): events confined relatively industries, cross-sectional dependence returns can bias SD estimate downward, inflating associated test statistics” (p. 47). control potential cross-sectional correlation abnormal returns, can use time-series standard deviation test statistic (S. J. Brown Warner 1980)Sample selection bias (self-selection firms event treatment) similar omitted variable bias omitted variable private info leads firm take action.\nSee Endogenous Sample Selection methods correct bias.\nUse Heckman model (Acharya 1993)\nhard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)\nCan estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).\nExamples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)\n\nCounterfactual observations\nPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\n\nSwitching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)\n\nSample selection bias (self-selection firms event treatment) similar omitted variable bias omitted variable private info leads firm take action.See Endogenous Sample Selection methods correct bias.See Endogenous Sample Selection methods correct bias.Use Heckman model (Acharya 1993)\nhard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)\nCan estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).\nExamples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)\nUse Heckman model (Acharya 1993)hard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)hard find instrument meets exclusion requirements (strong, weak instruments can lead multicollinearity second equation)Can estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).Can estimate private information unknown investors (Mills ratio \\(\\lambda\\) ). Testing \\(\\lambda\\) significance see whether private info can explain outcomes (e.g., magnitude CARs announcement).Examples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)Examples: (Y. Chen, Ganesan, Liu 2009) (Wiles, Morgan, Rego 2012) (Fang, Lee, Yang 2015)Counterfactual observations\nPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\n\nSwitching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)\nCounterfactual observationsPropensity score matching:\nFinance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)\nMarketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)\nPropensity score matching:Finance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)Finance: Doan Iskandar-Datta (2021) (Masulis Nahata 2011)Marketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)Marketing: (Warren Sorescu 2017) (Borah Tellis 2014) (Cao Sorescu 2013)Switching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)Switching regression: comparison 2 specific outcomes (also account selection unobservables - using instruments) (Cao Sorescu 2013)","code":""},{"path":"event-studies.html","id":"long-run-event-studies","chapter":"29 Event Studies","heading":"29.5 Long-run event studies","text":"Usually make assumption distribution abnormal returns events mean 0 (. Sorescu, Warren, Ertekin 2017, 192). (. Sorescu, Warren, Ertekin 2017) provide evidence events examine results samples without confounding events differ.Usually make assumption distribution abnormal returns events mean 0 (. Sorescu, Warren, Ertekin 2017, 192). (. Sorescu, Warren, Ertekin 2017) provide evidence events examine results samples without confounding events differ.Long-horizon event studies face challenges due systematic errors time sensitivity model choice.Long-horizon event studies face challenges due systematic errors time sensitivity model choice.Two main approaches used measure long-term abnormal stock returns\nBuy Hold Abnormal Returns (BHAR)\nLong-term Cumulative Abnormal Returns (LCARs)\nCalendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecification\nTwo main approaches used measure long-term abnormal stock returnsBuy Hold Abnormal Returns (BHAR)Buy Hold Abnormal Returns (BHAR)Long-term Cumulative Abnormal Returns (LCARs)Long-term Cumulative Abnormal Returns (LCARs)Calendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecificationCalendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better less sensitive (asset pricing) model misspecificationTwo types:\nUnexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.\nComplex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)\nTwo types:Unexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.Unexpected changes firm specific variables (typically announced, may immediately visible investors, impact firm value straightforward): customer satisfaction scores effect firm value (Jacobson Mizik 2009) unexpected changes marketing expenditures (M. Kim McAlister 2011) determine mispricing.Complex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)Complex consequences (investors take time learn incorporate info): acquisition depends integration (. B. Sorescu, Chandy, Prabhu 2007)12 - 60 months event window: (Loughran Ritter 1995) (Brav Gompers 1997)12 - 60 months event window: (Loughran Ritter 1995) (Brav Gompers 1997)Example: (Dutta et al. 2018)Example: (Dutta et al. 2018)","code":"\nlibrary(crseEventStudy)\n\n# example by the package's author\ndata(demo_returns)\nSAR <-\n    sar(event = demo_returns$EON,\n        control = demo_returns$RWE,\n        logret = FALSE)\nmean(SAR)\n#> [1] 0.006870196"},{"path":"event-studies.html","id":"buy-and-hold-abnormal-returns-bhar","chapter":"29 Event Studies","heading":"29.5.1 Buy and Hold Abnormal Returns (BHAR)","text":"Classic references: (Loughran Ritter 1995) (Barber Lyon 1997) (Lyon, Barber, Tsai 1999)Use portfolio stocks close matches current firm period benchmark, see difference firm return portfolio.technical note measures returns buying stocks event-experiencing firms shorting stocks similar non-event firms within time.high cross-sectional correlations, BHARs’ t-stat can inflated, rank order affected (Markovitch Golder 2008; . B. Sorescu, Chandy, Prabhu 2007)construct portfolio, use similarsizebook--marketmomentumMatching Procedure (Barber Lyon 1997):year July June, common stocks CRSP database categorized ten groups (deciles) based market capitalization previous June.year July June, common stocks CRSP database categorized ten groups (deciles) based market capitalization previous June.Within deciles, firms sorted five groups (quintiles) based book--market ratios December previous year earlier, considering possible delays financial statement reporting.Within deciles, firms sorted five groups (quintiles) based book--market ratios December previous year earlier, considering possible delays financial statement reporting.Benchmark portfolios designed exclude firms specific events include firms can classified characteristic-based portfolios.Benchmark portfolios designed exclude firms specific events include firms can classified characteristic-based portfolios.Similarly, Wiles et al. (2010) uses following matching procedure:firms two-digit SIC code market values 50% 150% focal firms selectedFrom list, 10 firms comparable book--market ratios chosen serve matched portfolio (matched portfolio can less 10 firms).Calculations:\\[\nAR_{} = R_{} - E(R_{}|X_t)\n\\]Cumulative Abnormal Return (CAR):\\[\nCAR_{} = \\sum_{t=1}^T (R_{} - E(R_{}))\n\\]Buy--Hold Abnormal Return (BHAR)\\[\nBHAR_{t = 1}^T = \\Pi_{t=1}^T(1 + R_{}) - \\Pi_{t = 1}^T (1 + E(R_{}))\n\\]CAR arithmetic sum, BHAR geometric sum.short-term event studies, differences CAR BHAR often minimal. However, long-term studies, difference significantly skew results. (Barber Lyon 1997) shows BHAR usually slightly lower annual CAR, dramatically surpasses CAR annual BHAR exceeds 28%.calculate long-run return (\\(\\Pi_{t=1}^T (1 + E(R_{}))\\)) benchmark portfolio, can:annual rebalance: period, portfolio re-balanced compound mean stock returns portfolio given period:\\[\n\\Pi_{t = 1}^T (1 + E(R_{})) = \\Pi_{t}^T (1 + \\sum_{= s}^{n_t}w_{} R_{})\n\\]\\(n_t\\) number firms period \\(t\\), \\(w_{}\\) (1) \\(1/n_t\\) (2) value-weight firm \\(\\) period \\(t\\).avoid favoring recent events, cross-sectional event studies, researchers usually treat events equally studying impact stock market time. approach helps identify abnormal changes stock prices, especially dealing series unplanned events.Potential problems:Solution first: Form benchmark portfolios never change constituent firms (Mitchell Stafford 2000), problems:\nNewly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).\nRegularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).\nValue-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.\nSolution first: Form benchmark portfolios never change constituent firms (Mitchell Stafford 2000), problems:Newly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).Newly public companies often perform worse balanced market index (Ritter 1991), , time, might distort long-term return expectations due inclusion new companies (phenomenon called “new listing bias” identified Barber Lyon (1997)).Regularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).Regularly rebalancing equal-weight portfolio can lead overestimated long-term returns potentially skew buy--hold abnormal returns (BHARs) negatively due constant selling winning stocks buying underperformers (.e., “rebalancing bias” (Barber Lyon 1997)).Value-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.Value-weight portfolios, favor larger market cap stocks, can viewed active investment strategy keeps buying winning stocks selling underperformers. time, approach tends positively distort BHARs.Without annual rebalance: Compounding returns securities comprising portfolio, followed calculating average across securities\\[\n\\Pi_{t = s}^{T} (1 + E(R_{})) = \\sum_{=s}^{n_t} (w_{} \\Pi_{t=1}^T (1 + R_{}))\n\\]\\(t\\) investment period, \\(R_{}\\) return security \\(\\), \\(n_i\\) number securities, \\(w_{}\\) either \\(1/n_s\\) value-weight factor security \\(\\) initial period \\(s\\). portfolio’s profits come simple investment included stocks given equal importance, weighted according market value, specific past period (period s). means doesn’t consider stocks listed period, adjust portfolio month. However, one problem method value assigned stock, based market size, needs corrected. make sure recent stocks don’t end much influence.Fortunately, WRDS, give types BHAR (2x2) (equal-weighted vs. value-weighted annual rebalance without annual rebalance)“MINWIN” smallest number months company trades event included study.“MAXWIN” months study considers calculations.\nCompanies aren’t excluded less MAXWIN months, unless also fewer MINWIN months.\n“MAXWIN” months study considers calculations.Companies aren’t excluded less MAXWIN months, unless also fewer MINWIN months.term “MONTH” signifies chosen months (typically 12, 24, 36) used work BHAR.\nmonthly returns missing set period, matching portfolio returns fill gaps.\nterm “MONTH” signifies chosen months (typically 12, 24, 36) used work BHAR.monthly returns missing set period, matching portfolio returns fill gaps.","code":""},{"path":"event-studies.html","id":"long-term-cumulative-abnormal-returns-lcars","chapter":"29 Event Studies","heading":"29.5.2 Long-term Cumulative Abnormal Returns (LCARs)","text":"Formula LCARs \\((1,T)\\) postevent horizon (. B. Sorescu, Chandy, Prabhu 2007)\\[\nLCAR_{pT} = \\sum_{t = 1}^{t = T} (R_{} - R_{pt})\n\\]\\(R_{}\\) rate return stock \\(\\) month \\(t\\)\\(R_{pt}\\) rate return counterfactual portfolio month \\(t\\)","code":""},{"path":"event-studies.html","id":"calendar-time-portfolio-abnormal-returns-ctars","chapter":"29 Event Studies","heading":"29.5.3 Calendar-time Portfolio Abnormal Returns (CTARs)","text":"section follows strictly procedure (Wiles et al. 2010)portfolio every day calendar time (including securities experience event time).portfolio, securities returns equally weightedFor portfolios, average abnormal return calculated \\[\nAAR_{Pt} = \\frac{\\sum_{=1}^S AR_i}{S}\n\\]\\(S\\) number securities portfolio \\(P\\)\\(AR_i\\) abnormal return stock \\(\\) portfolioFor every portfolio \\(P\\), time series estimate \\(\\sigma(AAR_{Pt})\\) calculated preceding \\(k\\) days, assuming \\(AAR_{Pt}\\) independent time.portfolio’s average abnormal return standardized\\[\nSAAR_{Pt} = \\frac{AAR_{Pt}}{SD(AAR_{Pt})}\n\\]Average standardized residual across portfolio’s calendar time\\[\nASAAR = \\frac{1}{n}\\sum_{=1}^{255} SAAR_{Pt} \\times D_t\n\\]\\(D_t = 1\\) least one security portfolio \\(t\\)\\(D_t = 1\\) least one security portfolio \\(t\\)\\(D_t = 0\\) security portfolio \\(t\\)\\(D_t = 0\\) security portfolio \\(t\\)\\(n\\) number days portfolio least one security \\(n = \\sum_{= 1}^{255}D_t\\)\\(n\\) number days portfolio least one security \\(n = \\sum_{= 1}^{255}D_t\\)cumulative average standardized average abnormal returns \\[\nCASSAR_{S_1, S_2} = \\sum_{=S_1}^{S_2} ASAAR\n\\]ASAAR independent time, standard deviation estimate \\(\\sqrt{S_2 - S_1 + 1}\\), test statistics \\[\nt = \\frac{CASAAR_{S_1,S_2}}{\\sqrt{S_2 - S_1 + 1}}\n\\]LimitationsCannot examine individual stock difference, can see difference portfolio level.\nOne can construct multiple portfolios (based metrics interest) firms portfolio shares characteristics. , one can compare intercepts portfolio.\nexamine individual stock difference, can see difference portfolio level.One can construct multiple portfolios (based metrics interest) firms portfolio shares characteristics. , one can compare intercepts portfolio.Low power (Loughran Ritter 2000), type II error likely.Low power (Loughran Ritter 2000), type II error likely.","code":""},{"path":"event-studies.html","id":"aggregation","chapter":"29 Event Studies","heading":"29.6 Aggregation","text":"","code":""},{"path":"event-studies.html","id":"over-time","chapter":"29 Event Studies","heading":"29.6.1 Over Time","text":"calculate cumulative abnormal (CAR) event windows\\(H_0\\): Standardized cumulative abnormal return stock \\(\\) 0 (effect events stock performance)\\(H_1\\): SCAR 0 (effect events stock performance)","code":""},{"path":"event-studies.html","id":"across-firms-over-time","chapter":"29 Event Studies","heading":"29.6.2 Across Firms + Over Time","text":"Additional assumptions: Abnormal returns different socks uncorrelated (rather strong), ’s valid event windows different stocks overlap. windows different overlap, follow (Bernard 1987) Schipper Smith (1983)\\(H_0\\): mean abnormal returns across firms 0 (effect)\\(H_1\\): mean abnormal returns across firms different form 0 (effect)Parametric (empirically either one works fine) (assume abnormal returns normally distributed) :Aggregate CAR stocks (Use true abnormal variance greater stocks higher variance)Aggregate SCAR stocks (Use true abnormal return constant across stocks)Non-parametric (parametric assumptions):Sign test:\nAssume abnormal returns CAR independent across stocks\nAssume 50% positive abnormal returns 50% negative abnormal return\nnull positive abnormal return correlated event (want alternative negative relationship)\nskewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might better\nAssume abnormal returns CAR independent across stocksAssume abnormal returns CAR independent across stocksAssume 50% positive abnormal returns 50% negative abnormal returnAssume 50% positive abnormal returns 50% negative abnormal returnThe null positive abnormal return correlated event (want alternative negative relationship)null positive abnormal return correlated event (want alternative negative relationship)skewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might betterWith skewed distribution (likely daily stock data), size test trustworthy. Hence, rank test might betterRank test\nNull: abnormal return event window\nNull: abnormal return event window","code":""},{"path":"event-studies.html","id":"heterogeneity-in-the-event-effect","chapter":"29 Event Studies","heading":"29.7 Heterogeneity in the event effect","text":"\\[\ny = X \\theta + \\eta\n\\]\\(y\\) = CAR\\(y\\) = CAR\\(X\\) = Characteristics lead heterogeneity event effect (.e., abnormal returns) (e.g., firm event specific)\\(X\\) = Characteristics lead heterogeneity event effect (.e., abnormal returns) (e.g., firm event specific)\\(\\eta\\) = error term\\(\\eta\\) = error termNote:cases selection bias (firm characteristics investor anticipation event: larger firms might enjoy great positive effect event, investors endogenously anticipate effect overvalue stock), use White’s \\(t\\)-statistics lower bounds true significance estimates.technique employed even average CAR significantly different 0, especially CAR variance high (Boyd, Chandy, Cunha Jr 2010)","code":""},{"path":"event-studies.html","id":"common-variables-in-marketing","chapter":"29 Event Studies","heading":"29.7.1 Common variables in marketing","text":"(. Sorescu, Warren, Ertekin 2017) Table 4Firm size negatively correlated abnormal return finance (. Sorescu, Warren, Ertekin 2017), mixed results marketing.Firm size negatively correlated abnormal return finance (. Sorescu, Warren, Ertekin 2017), mixed results marketing.# event occurrences# event occurrencesR&D expenditureR&D expenditureAdvertising expenseAdvertising expenseMarketing investment (SG&)Marketing investment (SG&)Industry concentration (HHI, # competitors)Industry concentration (HHI, # competitors)Financial leverageFinancial leverageMarket shareMarket shareMarket size (total sales volume within firm’s SIC code)Market size (total sales volume within firm’s SIC code)marketing capabilitymarketing capabilityBook market valueBook market valueROAROAFree cash flowFree cash flowSales growthSales growthFirm ageFirm age","code":""},{"path":"event-studies.html","id":"expected-return-calculation","chapter":"29 Event Studies","heading":"29.8 Expected Return Calculation","text":"","code":""},{"path":"event-studies.html","id":"statistical-models","chapter":"29 Event Studies","heading":"29.8.1 Statistical Models","text":"based statistical assumptions behavior returns (e..g, multivariate normality)based statistical assumptions behavior returns (e..g, multivariate normality)need assume stable distributions (Owen Rabinovitch 1983)need assume stable distributions (Owen Rabinovitch 1983)","code":""},{"path":"event-studies.html","id":"constant-mean-return-model","chapter":"29 Event Studies","heading":"29.8.1.1 Constant Mean Return Model","text":"expected normal return mean real returns\\[\nRa_{} = R_{} - \\bar{R}_i\n\\]Assumption:returns revert mean (questionable)basic mean returns model generally delivers similar findings complex models since variance abnormal returns decreased considerably (S. J. Brown Warner 1985)","code":""},{"path":"event-studies.html","id":"market-model","chapter":"29 Event Studies","heading":"29.8.1.2 Market Model","text":"\\[\nR_{} = \\alpha_i + \\beta R_{mt} + \\epsilon_{}\n\\]\\(R_{}\\) = stock return \\(\\) period \\(t\\)\\(R_{}\\) = stock return \\(\\) period \\(t\\)\\(R_{mt}\\) = market return\\(R_{mt}\\) = market return\\(\\epsilon_{}\\) = zero mean (\\(E(e_{}) = 0\\)) error term variance \\(\\sigma^2\\)\\(\\epsilon_{}\\) = zero mean (\\(E(e_{}) = 0\\)) error term variance \\(\\sigma^2\\)Notes:People typically use S&P 500, CRSP value-weighed equal-weighted index market portfolio.People typically use S&P 500, CRSP value-weighed equal-weighted index market portfolio.\\(\\beta =0\\), Market Model Constant Mean Return ModelWhen \\(\\beta =0\\), Market Model Constant Mean Return Modelbetter fit market-model, less variance abnormal return, easy detect event’s effectbetter fit market-model, less variance abnormal return, easy detect event’s effectrecommend generalized method moments robust auto-correlation heteroskedasticityrecommend generalized method moments robust auto-correlation heteroskedasticity","code":""},{"path":"event-studies.html","id":"fama-french-model","chapter":"29 Event Studies","heading":"29.8.1.3 Fama-French Model","text":"Please note difference just taking return versus taking excess return dependent variable.correct way use excess return firm market (Fama French 2010, 1917).\\(\\alpha_i\\) “average return left unexplained benchmark model” (.e., abnormal return)","code":""},{"path":"event-studies.html","id":"ff3","chapter":"29 Event Studies","heading":"29.8.1.3.1 FF3","text":"(Fama French 1993)\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} = \\alpha_i &+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t\n\\end{aligned}\n\\]\\(r_{ft}\\) risk-free rate (e.g., 3-month Treasury bill)\\(r_{ft}\\) risk-free rate (e.g., 3-month Treasury bill)\\(R_{mt}\\) market-rate (e.g., S&P 500)\\(R_{mt}\\) market-rate (e.g., S&P 500)SML: returns small (size) portfolio minus returns big portfolioSML: returns small (size) portfolio minus returns big portfolioHML: returns high (B/M) portfolio minus returns low portfolio.HML: returns high (B/M) portfolio minus returns low portfolio.","code":""},{"path":"event-studies.html","id":"ff4","chapter":"29 Event Studies","heading":"29.8.1.3.2 FF4","text":"(. Sorescu, Warren, Ertekin 2017, 195) suggest use Market Model marketing short-term window Fama-French Model long-term window (statistical properties model examined daily setting).(Carhart 1997)\\[\n\\begin{aligned}\nE(R_{}|X_t) - r_{ft} = \\alpha_i &+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\\n&+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t\n\\end{aligned}\n\\]\\(UMD_t\\) momentum factor (difference high low prior return stock portfolios) day \\(t\\).","code":""},{"path":"event-studies.html","id":"economic-model","chapter":"29 Event Studies","heading":"29.8.2 Economic Model","text":"difference CAPM APT APT multiple factors (including factors beyond focal company)Economic models put limits statistical model come assumed behavior derived theory.","code":""},{"path":"event-studies.html","id":"capital-asset-pricing-model-capm","chapter":"29 Event Studies","heading":"29.8.2.1 Capital Asset Pricing Model (CAPM)","text":"\\[\nE(R_i) = R_f + \\beta_i (E(R_m) - R_f)\n\\]\\(E(R_i)\\) = expected firm return\\(E(R_i)\\) = expected firm return\\(R_f\\) = risk free rate\\(R_f\\) = risk free rate\\(E(R_m - R_f)\\) = market risk premium\\(E(R_m - R_f)\\) = market risk premium\\(\\beta_i\\) = firm sensitivity\\(\\beta_i\\) = firm sensitivity","code":""},{"path":"event-studies.html","id":"arbitrage-pricing-theory-apt","chapter":"29 Event Studies","heading":"29.8.2.2 Arbitrage Pricing Theory (APT)","text":"\\[\nR = R_f + \\Lambda f + \\epsilon\n\\]\\(\\epsilon \\sim N(0, \\Psi)\\)\\(\\epsilon \\sim N(0, \\Psi)\\)\\(\\Lambda\\) = factor loadings\\(\\Lambda\\) = factor loadings\\(f \\sim N(\\mu, \\Omega)\\) = general factor model\n\\(\\mu\\) = expected risk premium vector\n\\(\\Omega\\) = factor covariance matrix\n\\(f \\sim N(\\mu, \\Omega)\\) = general factor model\\(\\mu\\) = expected risk premium vector\\(\\mu\\) = expected risk premium vector\\(\\Omega\\) = factor covariance matrix\\(\\Omega\\) = factor covariance matrix","code":""},{"path":"event-studies.html","id":"application-15","chapter":"29 Event Studies","heading":"29.9 Application","text":"Packages:eventstudieseventstudiesererererEventStudyEventStudyAbnormalReturnsAbnormalReturnsEvent Study ToolsEvent Study Toolsestudy2estudy2PerformanceAnalyticsPerformanceAnalyticsIn practice, people usually sort portfolio sure whether FF model specified correctly.Steps:Sort returns CRSP 10 deciles based size.decile, sort returns 10 decides based BMGet average return 100 portfolios period (.e., expected returns stocks given decile - characteristics)stock event study: Compare return stock corresponding portfolio based size BM.Notes:Sorting produces outcomes often conservative (e.g., FF abnormal returns can greater used sorting).Sorting produces outcomes often conservative (e.g., FF abnormal returns can greater used sorting).results change B/M first size vice versa, results robust (extends just two characteristics - e.g., momentum).results change B/M first size vice versa, results robust (extends just two characteristics - e.g., momentum).Examples:Forestry:(Mei Sun 2008) M&financial performance (forest product)(Mei Sun 2008) M&financial performance (forest product)(C. Sun Liao 2011) litigation firm values(C. Sun Liao 2011) litigation firm valuesExample Ana Julia Akaishi Padula, Pedro Albuquerque (posted LAMFO)Example AbnormalReturns package","code":"\nlibrary(erer)\n\n# example by the package's author\ndata(daEsa)\nhh <- evReturn(\n    y = daEsa,       # dataset\n    firm = \"wpp\",    # firm name\n    y.date = \"date\", # date in y \n    index = \"sp500\", # index\n    est.win = 250,   # estimation window wedith in days\n    digits = 3, \n    event.date = 19990505, # firm event dates \n    event.win = 5          # one-side event window wdith in days (default = 3, where 3 before + 1 event date + 3 days after = 7 days)\n)\nhh; plot(hh)\n#> \n#> === Regression coefficients by firm =========\n#>   N firm event.date alpha.c alpha.e alpha.t alpha.p alpha.s beta.c beta.e\n#> 1 1  wpp   19990505  -0.135   0.170  -0.795   0.428          0.665  0.123\n#>   beta.t beta.p beta.s\n#> 1  5.419  0.000    ***\n#> \n#> === Abnormal returns by date ================\n#>    day Ait.wpp    HNt\n#> 1   -5   4.564  4.564\n#> 2   -4   0.534  5.098\n#> 3   -3  -1.707  3.391\n#> 4   -2   2.582  5.973\n#> 5   -1  -0.942  5.031\n#> 6    0  -3.247  1.784\n#> 7    1  -0.646  1.138\n#> 8    2  -2.071 -0.933\n#> 9    3   0.368 -0.565\n#> 10   4   4.141  3.576\n#> 11   5   0.861  4.437\n#> \n#> === Average abnormal returns across firms ===\n#>      name estimate error t.value p.value sig\n#> 1 CiT.wpp    4.437 8.888   0.499   0.618    \n#> 2     GNT    4.437 8.888   0.499   0.618"},{"path":"event-studies.html","id":"eventus","chapter":"29 Event Studies","heading":"29.9.1 Eventus","text":"2 types output:Using different estimation methods (e.g., market model calendar-time approach)\ninclude event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.\nUsing different estimation methods (e.g., market model calendar-time approach)Using different estimation methods (e.g., market model calendar-time approach)include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.include event-specific returns. Hence, regression later determine variables can affect abnormal stock returns.Cross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)\nSince stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.\nCross-sectional Analysis Eventus: Event-specific abnormal returns (using monthly data data) cross-sectional analysis (Cross-Sectional Analysis section)Since stock-specific abnormal returns, can regression CARs later. gives market-adjusted model. However, according (. Sorescu, Warren, Ertekin 2017), advocate use market-adjusted model short-term , reserve FF4 longer-term event studies using monthly daily.","code":""},{"path":"event-studies.html","id":"basic-event-study","chapter":"29 Event Studies","heading":"29.9.1.1 Basic Event Study","text":"Input text file contains firm identifier (e.g., PERMNO, CUSIP) event dateChoose market indices: equally weighted value weighted index (.e., weighted market capitalization). check Fama-French Carhart factors.Estimation options\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.\nUse “autodate” options: first trading event date used event falls weekend holiday\nEstimation period: ESTLEN = 100 convention estimation impacted outliers.Estimation period: ESTLEN = 100 convention estimation impacted outliers.Use “autodate” options: first trading event date used event falls weekend holidayUse “autodate” options: first trading event date used event falls weekend holidayAbnormal returns window: depends specific eventChoose test: either parametric (including Patell Standardized Residual (PSR)) non-parametric","code":""},{"path":"event-studies.html","id":"cross-sectional-analysis-of-eventus","chapter":"29 Event Studies","heading":"29.9.1.2 Cross-sectional Analysis of Eventus","text":"Similar Basic Event Study, now can event-specific abnormal returns.","code":""},{"path":"event-studies.html","id":"evenstudies","chapter":"29 Event Studies","heading":"29.9.2 Evenstudies","text":"package use Fama-French model, market models.example author package","code":"\nlibrary(eventstudies)\n# firm and date data\ndata(\"SplitDates\")\nhead(SplitDates)\n\n# stock price data \ndata(\"StockPriceReturns\")\nhead(StockPriceReturns)\nclass(StockPriceReturns)\n\nes <-\n    eventstudy(\n        firm.returns = StockPriceReturns,\n        event.list = SplitDates,\n        event.window = 5,\n        type = \"None\",\n        to.remap = TRUE,\n        remap = \"cumsum\",\n        inference = TRUE,\n        inference.strategy = \"bootstrap\"\n    )\n\nplot(es)"},{"path":"event-studies.html","id":"eventstudy","chapter":"29 Event Studies","heading":"29.9.3 EventStudy","text":"pay API key. (’s $10/month).Example authors packageData PrepReference market Germany DAXCreate files01_RequestFile.csv02_FirmData.csv03_MarketData.csvCalculating abnormal returns","code":"\nlibrary(EventStudy)\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(\"Quandl\")\nlibrary(\"quantmod\")\nQuandl.auth(\"LDqWhYXzVd2omw4zipN2\")\nTWTR <- Quandl(\"NSE/OIL\",type =\"xts\")\ncandleChart(TWTR)\naddSMA(col=\"red\") #Adding a Simple Moving Average\naddEMA() #Adding an Exponential Moving Average\n# Index Data\n# indexName <- c(\"DAX\")\n\nindexData <- tq_get(\"^GDAXI\", from = \"2014-05-01\", to = \"2015-12-31\") %>%\n    mutate(date = format(date, \"%d.%m.%Y\")) %>%\n    mutate(symbol = \"DAX\")\n\nhead(indexData)\n# get & set parameters for abnormal return Event Study\n# we use a garch model and csv as return\n# Attention: fitting a GARCH(1, 1) model is compute intensive\nesaParams <- EventStudy::ARCApplicationInput$new()\nesaParams$setResultFileType(\"csv\")\nesaParams$setBenchmarkModel(\"garch\")\n\n\ndataFiles <-\n    c(\n        \"request_file\" = file.path(getwd(), \"data\", \"EventStudy\", \"01_requestFile.csv\"),\n        \"firm_data\"    = file.path(getwd(), \"data\", \"EventStudy\", \"02_firmDataPrice.csv\"),\n        \"market_data\"  = file.path(getwd(), \"data\", \"EventStudy\", \"03_marketDataPrice.csv\")\n    )\n\n# check data files, you can do it also in our R6 class\nEventStudy::checkFiles(dataFiles)\narEventStudy <- estSetup$performEventStudy(estParams     = esaParams, \n                                      dataFiles     = dataFiles, \n                                      downloadFiles = T)\nlibrary(EventStudy)\n\napiUrl <- \"https://api.eventstudytools.com\"\nSys.setenv(EventStudyapiKey = \"\")\n\n# The URL is already set by default\noptions(EventStudy.URL = apiUrl)\noptions(EventStudy.KEY = Sys.getenv(\"EventStudyapiKey\"))\n\n# use EventStudy estAPIKey function\nestAPIKey(Sys.getenv(\"EventStudyapiKey\"))\n\n# initialize object\nestSetup <- EventStudyAPI$new()\nestSetup$authentication(apiKey = Sys.getenv(\"EventStudyapiKey\"))"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"30 Instrumental Variables","heading":"30 Instrumental Variables","text":"Similar RCT, try introduce randomization (random assignment treatment) treatment variable using variation instrument.Logic using instrument:Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation treatment (try exclude endogenous variation treatment)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)Use exogenous variation see variation outcome (try exclude endogenous variation outcome)See relationship treatment outcome terms residual variations exogenous omitted variables.See relationship treatment outcome terms residual variations exogenous omitted variables.Notes:Instruments can used remove attenuation bias errors--variables.Instruments can used remove attenuation bias errors--variables.careful F-test standard errors 2SLS hand (need correct ).careful F-test standard errors 2SLS hand (need correct ).Repeated use related IVs across different studies can collectively invalidate instruments, primarily violation exclusion restriction (Gallen 2020). One needs test invalid instruments (Hausman-like test).\nMellon (2023) shows widespread use weather instrument social sciences (289 studies linking weather 195 variables) demonstrates significant exclusion violations can overturn many IV results.\nRepeated use related IVs across different studies can collectively invalidate instruments, primarily violation exclusion restriction (Gallen 2020). One needs test invalid instruments (Hausman-like test).Mellon (2023) shows widespread use weather instrument social sciences (289 studies linking weather 195 variables) demonstrates significant exclusion violations can overturn many IV results.[Zero-valued Outcomes], can’t directly interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change. percentage change interpretation, can either :\nProportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,\nRegress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.\nGet estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\n\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\nSE can obtained non-parametric bootstrap.\nspecific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).\n\nLee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.\n[Zero-valued Outcomes], can’t directly interpret treatment coefficient log-transformed outcome regression percentage change (J. Chen Roth 2023). distinguish treatment effect intensive (outcome: 10 11) vs. extensive margins (outcome: 0 1), can’t readily interpret treatment coefficient log-transformed outcome regression percentage change. percentage change interpretation, can either :Proportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,\nRegress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.\nGet estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\n\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\nSE can obtained non-parametric bootstrap.\nspecific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).\nProportional LATE: estimate \\(\\theta_{ATE\\%}\\) compliers instrument. estimate proportional LATE,Regress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.Regress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS instrument \\(D_i\\), \\(\\beta\\) interpreted LATE levels control group’s mean compliers.Get estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)Get estimate control complier mean regressing 2SLS regression (Abadie, Angrist, Imbens 2002) final outcome \\(-(D_i - 1)Y_i\\) , refer new new estimated effect \\(D_i\\) \\(\\beta_{cc}\\)\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.\\(\\theta_{ATE \\%}\\) compliers induced instrument \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), can interpreted directly percentage change compliers induced instrument treatment compared control.SE can obtained non-parametric bootstrap.SE can obtained non-parametric bootstrap.specific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).specific case instrument binary, \\(\\theta\\) intensive margin compliers can directly obtained Poisson IV regression (ivpoisson Stata).Lee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.Lee (2009) bounds: can get bounds average treatment effect logs compliers positive outcome regardless treatment status (.e., intensive-margin effect). requires monotonicity assumption compliers still positive outcome regardless treatment status.Notes First-stage:Always use OLS regression first stage (regardless type endogenous variables - e.g., continuous discreet) (suggested (J. D. Angrist Pischke 2009). Estimates IV can still consistent regardless form endogenous variables (discreet vs. continuous).\nAlternatively, use “biprobit” model, applicable cases dependent endogenous variables binary.\nAlways use OLS regression first stage (regardless type endogenous variables - e.g., continuous discreet) (suggested (J. D. Angrist Pischke 2009). Estimates IV can still consistent regardless form endogenous variables (discreet vs. continuous).Alternatively, use “biprobit” model, applicable cases dependent endogenous variables binary.still want continue use logit probit models first stage binary variables, “forbidden regression” (also 1, 2) (.e., incorrect extension 2SLS nonlinear case).still want continue use logit probit models first stage binary variables, “forbidden regression” (also 1, 2) (.e., incorrect extension 2SLS nonlinear case).several ways understand problem:Identification strategy: identification strategy instrumental variables analysis relies fact instrumental variable affects outcome variable effect endogenous variable. However, endogenous variable binary, relationship instrumental variable endogenous variable continuous. means instrumental variable can affect endogenous variable discrete jumps, rather continuous change. result, identification causal effect endogenous variable outcome variable may possible probit logit regression first stage.Identification strategy: identification strategy instrumental variables analysis relies fact instrumental variable affects outcome variable effect endogenous variable. However, endogenous variable binary, relationship instrumental variable endogenous variable continuous. means instrumental variable can affect endogenous variable discrete jumps, rather continuous change. result, identification causal effect endogenous variable outcome variable may possible probit logit regression first stage.Model assumptions: models assume error term specific distribution (normal logistic), probability binary outcome function linear combination regressors.\nendogenous variable binary, however, distribution error term specified, continuous relationship endogenous variable outcome variable. means assumptions probit logit models may hold, resulting estimates may reliable interpretable.Model assumptions: models assume error term specific distribution (normal logistic), probability binary outcome function linear combination regressors.endogenous variable binary, however, distribution error term specified, continuous relationship endogenous variable outcome variable. means assumptions probit logit models may hold, resulting estimates may reliable interpretable.Issue weak instruments: instrument weak, variance inverse Mills ratio (used correct endogeneity instrumental variables analysis) can large. case binary endogenous variables, inverse Mills ratio consistently estimated using probit logit regression, can lead biased inconsistent estimates causal effect endogenous variable outcome variable.Issue weak instruments: instrument weak, variance inverse Mills ratio (used correct endogeneity instrumental variables analysis) can large. case binary endogenous variables, inverse Mills ratio consistently estimated using probit logit regression, can lead biased inconsistent estimates causal effect endogenous variable outcome variable.Problems weak instruments (Bound, Jaeger, Baker 1995):Weak instrumental variables can produce (finite-sample) biased inconsistent estimates causal effect endogenous variable outcome variable (even presence large sample size)Weak instrumental variables can produce (finite-sample) biased inconsistent estimates causal effect endogenous variable outcome variable (even presence large sample size)finite sample, instrumental variables (IV) estimates can biased direction ordinary least squares (OLS) estimates. Additionally, bias IV estimates approaches OLS estimates correlation (R2) instruments endogenous explanatory variable approaches zero. means correlation instruments endogenous variable weak, bias IV estimates can similar OLS estimates.finite sample, instrumental variables (IV) estimates can biased direction ordinary least squares (OLS) estimates. Additionally, bias IV estimates approaches OLS estimates correlation (R2) instruments endogenous explanatory variable approaches zero. means correlation instruments endogenous variable weak, bias IV estimates can similar OLS estimates.Weak instruments problematic enough variation fully capture variation endogenous variable, leading measurement error sources noise estimates.Weak instruments problematic enough variation fully capture variation endogenous variable, leading measurement error sources noise estimates.Using weak instruments can produce large standard errors low t-ratio. feedback (reverse causality) strong, bias IV even greater OLS (C. Nelson Startz 1988).Using weak instruments can produce large standard errors low t-ratio. feedback (reverse causality) strong, bias IV even greater OLS (C. Nelson Startz 1988).Using lagged dependent variables instruments current values depends serial correlations, typically low (C. Nelson Startz 1988).Using lagged dependent variables instruments current values depends serial correlations, typically low (C. Nelson Startz 1988).Using multiple covariates artificially increase first-stage \\(R^2\\) solve weak instrument problem (C. Nelson Startz 1988).Using multiple covariates artificially increase first-stage \\(R^2\\) solve weak instrument problem (C. Nelson Startz 1988).Solutions:\nuse multiple instruments\nuse instrumental variables higher correlation\nuse alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.\nSolutions:use multiple instrumentsuse multiple instrumentsuse instrumental variables higher correlationuse instrumental variables higher correlationuse alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.use alternative estimation methods limited information maximum likelihood (LIML) two-stage least squares (2SLS) heteroscedasticity-robust standard errors.Instrument Validity:Random assignment (Exogeneity Assumption).effect instrument outcome must endogenous variable (Relevance Assumption).","code":""},{"path":"instrumental-variables.html","id":"framework","chapter":"30 Instrumental Variables","heading":"30.1 Framework","text":"\\(D_i \\sim Bern\\) Dummy Treatment\\(D_i \\sim Bern\\) Dummy Treatment\\(Y_{0i}, Y_{1i}\\) potential outcomes\\(Y_{0i}, Y_{1i}\\) potential outcomes\\(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\\) observed outcome\\(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\\) observed outcome\\(Z_i \\perp Y_{0i}, Y_{1i}\\) Instrumental variables (also correlate \\(D_i\\))\\(Z_i \\perp Y_{0i}, Y_{1i}\\) Instrumental variables (also correlate \\(D_i\\))constant-effects linear (\\(Y_{1i} - Y_{0i}\\) everyone)\\[ \\begin{aligned} Y_{0i} &= \\alpha + \\eta_i \\\\ Y_{1i} - Y_{0i} &= \\rho \\\\ Y_i &= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\\\ &= \\alpha + \\eta_i  + D_i \\rho \\\\ &= \\alpha + \\rho D_i + \\eta_i \\end{aligned} \\]\\(\\eta_i\\) individual differences\\(\\eta_i\\) individual differences\\(\\rho\\) difference treated outcome untreated outcome. assume constant everyone\\(\\rho\\) difference treated outcome untreated outcome. assume constant everyoneHowever, problem OLS \\(D_i\\) correlated \\(\\eta_i\\) unitBut \\(Z_i\\) can come rescue, causal estimate can written \\[ \\begin{aligned} \\rho &= \\frac{Cov( Y_i, Z_i)}{Cov(D_i, Z_i)} \\\\ &= \\frac{Cov(Y_i, Z_i) / V(Z_i) }{Cov( D_i, Z_i) / V(Z_i)} = \\frac{Reduced form}{First-stage} \\\\ &= \\frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]} \\end{aligned} \\]heterogeneous treatment effect (\\(Y_{1i} - Y_{0i}\\) different everyone) LATE framework\\(Y_i(d,z)\\) denotes potential outcome unit \\(\\) treatment \\(D_i = d\\) instrument \\(Z_i = z\\)Observed treatment status\\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}) \\]\\(D_{1i}\\) treatment status unit \\(\\) \\(z_i = 1\\)\\(D_{1i}\\) treatment status unit \\(\\) \\(z_i = 1\\)\\(D_{0i}\\) treatment status unit \\(\\) \\(z_i = 0\\)\\(D_{0i}\\) treatment status unit \\(\\) \\(z_i = 0\\)\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\)\\(D_{1i} - D_{0i}\\) causal effect \\(Z_i\\) \\(D_i\\)AssumptionsIndependence: instrument randomly assigned (.e., independent potential outcomes potential treatments)\n\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)\nassumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)\n\\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &= E[D_{1i} - D_{0i}] \\end{aligned} \\]\nassumption also sufficient causal interpretation reduced form, see effect instrument outcome.\nIndependence: instrument randomly assigned (.e., independent potential outcomes potential treatments)\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)\\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\)assumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)assumption let first-stage equation average causal effect \\(Z_i\\) \\(D_i\\)\\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &= E[D_{1i} - D_{0i}] \\end{aligned} \\]assumption also sufficient causal interpretation reduced form, see effect instrument outcome.\\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \\]Exclusion (.e., existence instruments (G. W. Imbens Angrist 1994)\ntreatment \\(D_i\\) fully mediates effect \\(Z_i\\) \\(Y_i\\)\n\\[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\\\  Y_{0i} = Y_i (0,1) = Y_i (0, 0) \\]\nassumption, observed outcome \\(Y_i\\) can thought (assume \\(Y_{1i}, Y_{0i}\\) already satisfy independence assumption)\n\\[ \\begin{aligned} Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\ &= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \\end{aligned} \\]\nassumption let us go reduced-form causal effects treatment effects (J. D. Angrist Imbens 1995)\nExclusion (.e., existence instruments (G. W. Imbens Angrist 1994)treatment \\(D_i\\) fully mediates effect \\(Z_i\\) \\(Y_i\\)\\[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\\\  Y_{0i} = Y_i (0,1) = Y_i (0, 0) \\]assumption, observed outcome \\(Y_i\\) can thought (assume \\(Y_{1i}, Y_{0i}\\) already satisfy independence assumption)\\[ \\begin{aligned} Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\ &= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \\end{aligned} \\]assumption let us go reduced-form causal effects treatment effects (J. D. Angrist Imbens 1995)Monotonicity: \\(D_{1i} > D_{0i} \\forall \\)\nassumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)\nassumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)\nassumption used solve problem shifts participation status back non-participation status.\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.\nthird solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).\n\nMonotonicity: \\(D_{1i} > D_{0i} \\forall \\)assumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)assumption, \\(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\\)assumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)assumption lets us assume first stage, examine proportion population \\(D_i\\) driven \\(Z_i\\)assumption used solve problem shifts participation status back non-participation status.\nAlternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.\nthird solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).\nassumption used solve problem shifts participation status back non-participation status.Alternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.Alternatively, one can solve problem assuming constant (homogeneous) treatment effect (G. W. Imbens Angrist 1994), rather restrictive.third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).third solution assumption exists value instrument, probability participation conditional value 0 J. Angrist Imbens (1991).three assumptions, LATE theorem (J. D. Angrist Pischke 2009, 4.4.1)\\[ \\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} > D_{0i}] \\]LATE assumptions allow us go back types subjects Causal InferenceSwitchers:\nCompliers: \\(D_{1i} > D_{0i}\\)\nSwitchers:Compliers: \\(D_{1i} > D_{0i}\\)Non-switchers:\nAlways-takers: \\(D_{1i} = D_{0i} = 1\\)\nNever-takers: \\(D_{1i} = D_{0i} = 0\\)\nNon-switchers:Always-takers: \\(D_{1i} = D_{0i} = 1\\)Always-takers: \\(D_{1i} = D_{0i} = 1\\)Never-takers: \\(D_{1i} = D_{0i} = 0\\)Never-takers: \\(D_{1i} = D_{0i} = 0\\)Instrumental Variables can’t say anything non-switchers treatment status \\(D_i\\) effects (similar fixed effects models).groups , come back constant-effects world.Treatment effects treated weighted average always-takers compliers.special case IV randomized trials, compliance problem (compliance voluntary), treated always take treatment (.e., might selection bias).Intention--treat analysis valid, contaminated non-complianceIntention--treat analysis valid, contaminated non-complianceIV case (\\(Z_i\\) = random assignment treatment; \\(D_i\\) = whether unit actually received/took treatment) can solve problem.IV case (\\(Z_i\\) = random assignment treatment; \\(D_i\\) = whether unit actually received/took treatment) can solve problem.certain assumptions (.e., SUTVA, random assignment, exclusion restriction, defiers, monotinicity), analysis can give causal interpreation LATE ’s average causal effect compliers .\nWithout assumptions, ’s ratio intention--treat.\ncertain assumptions (.e., SUTVA, random assignment, exclusion restriction, defiers, monotinicity), analysis can give causal interpreation LATE ’s average causal effect compliers .Without assumptions, ’s ratio intention--treat.Without always-takers case, LATE = Treatment effects treatedWithout always-takers case, LATE = Treatment effects treatedSee proof Bloom (1984) examples Bloom et al. (1997) Sherman Berk (1984)\\[ \\frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \\frac{\\text{Intention--treat effect}}{\\text{Compliance rate}} \\\\ = E[Y_{1i} - Y_{0i} |D_i = 1] \\]","code":""},{"path":"instrumental-variables.html","id":"estimation-4","chapter":"30 Instrumental Variables","heading":"30.2 Estimation","text":"","code":""},{"path":"instrumental-variables.html","id":"sls-estimation","chapter":"30 Instrumental Variables","heading":"30.2.1 2SLS Estimation","text":"special case IV-GMMExamples authors fixest packageDefault statisticsF-test first-stage (weak instrument test)Wu-Hausman endogeneity testOver-identifying restriction (Sargan) J-testTo set default printingTo see results different stages","code":"\nlibrary(fixest)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\n# est_iv = feols(y ~ x1 | x_endo_1  ~ x_inst_1 , base)\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nest_iv\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\nfitstat(\n    est_iv,\n    type = c(\n        \"n\", \"ll\", \"aic\", \"bic\", \"rmse\", # ll means log-likelihood\n        \n        \"my\", # mean dependent var\n\n        \"g\", # degrees of freedom used to compute the t-test\n\n        \"r2\", \"ar2\", \"wr2\", \"awr2\", \"pr2\", \"apr2\", \"wpr2\", \"awpr2\",\n\n        \"theta\", # over-dispersion parameter in Negative Binomial models\n\n        \"f\", \"wf\", # F-tests of nullity of the coefficients\n\n        \"wald\", # Wald test of joint nullity of the coefficients\n\n        \"ivf\",\n        \n        \"ivf1\",\n\n        \"ivf2\",\n\n        \"ivfall\",\n        \n        \"ivwald\", \"ivwald1\", \"ivwald2\", \"ivwaldall\",\n\n        \"cd\"\n        \n        # \"kpr\"\n        \n        \n        ),\n    cluster = 'fe'\n)\n#>                 Observations: 150\n#>               Log-Likelihood: -75.0\n#>                          AIC: 157.9\n#>                          BIC: 170.0\n#>                         RMSE: 0.398842\n#>               Dep. Var. mean: 5.84333\n#>                            G: 3\n#>                           R2: 0.766452\n#>                      Adj. R2: 0.761653\n#>                    Within R2: NA\n#>                         awr2: NA\n#>                    Pseudo R2: 0.592684\n#>               Adj. Pseudo R2: 0.576383\n#>             Within Pseudo R2: NA\n#>                        awpr2: NA\n#>              Over-dispersion: NA\n#>                       F-test: stat =       1.80769, p = 0.375558, on 3 and 2 DoF.\n#>           F-test (projected): NA\n#>         Wald (joint nullity): stat = 539,363.2    , p < 2.2e-16 , on 3 and 146 DoF, VCOV: Clustered (fe).\n#> F-test (1st stage), x_endo_1: stat =     903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =       3.25828, p = 0.041268, on 2 and 146 DoF.\n#>           F-test (2nd stage): stat =     194.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#>             F-test (IV only): stat =     194.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> Wald (1st stage), x_endo_1  : stat =   1,482.6    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#> Wald (1st stage), x_endo_2  : stat =       2.22157, p = 0.112092, on 2 and 146 DoF, VCOV: Clustered (fe).\n#>             Wald (2nd stage): stat = 539,363.2    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#>               Wald (IV only): stat = 539,363.2    , p < 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe).\n#>                 Cragg-Donald: 3.11162\n# always add second-stage Wald test\nsetFixest_print(fitstat = ~ . + ivwald2)\nest_iv\n# first-stage\nsummary(est_iv, stage = 1)\n\n# second-stage\nsummary(est_iv, stage = 2)\n\n# both stages\netable(summary(est_iv, stage = 1:2), fitstat = ~ . + ivfall + ivwaldall.p)\netable(summary(est_iv, stage = 2:1), fitstat = ~ . + ivfall + ivwaldall.p)\n# .p means p-value, not statistic\n# `all` means IV only"},{"path":"instrumental-variables.html","id":"iv-gmm","chapter":"30 Instrumental Variables","heading":"30.2.2 IV-GMM","text":"general framework.2SLS Estimation special case IV-GMM estimator\\[\nY = X \\beta + u, u \\sim (0, \\Omega)\n\\]\\(X\\) matrix endogenous variables (\\(N\\times k\\))use matrix instruments \\(X\\) \\(N \\times l\\) dimensions (\\(l \\ge k\\)), can set \\(l\\) moments:\\[\ng_i (\\beta) = Z_i' u_i = Z_i' (Y_i - X_i \\beta)\n\\]\\(\\(1,N)\\)\\(l\\) moment equation sample moment, can estimated averaging \\(N\\)\\[\n\\bar{g}(\\beta) = \\frac{1}{N} \\sum_{= 1}^N Z_i (Y_i - X_i \\beta) = \\frac{1}{N} Z'u\n\\]GMM estimate \\(\\beta\\) \\(\\bar{g}(\\hat{\\beta}_{GMM}) = 0\\)\\(l = k\\) unique solution system equations (equivalent IV estimator)\\[\n\\hat{\\beta}_{IV} = (Z'X)^{-1}Z'Y\n\\]\\(l > k\\), set \\(k\\) instruments\\[\n\\hat{X} = Z(Z'Z)^{-1} Z' X = P_ZX\n\\]can use 2SLS estimator\\[\n\\begin{aligned}\n\\hat{\\beta}_{2SLS} &= (\\hat{X}'X)^{-1} \\hat{X}' Y \\\\\n&= (X'P_Z X)^{-1}X' P_Z Y\n\\end{aligned}\n\\]Differences 2SLS IV-GMM:2SLS method, instruments available actually needed estimation, address , matrix created includes necessary instruments, simplifies calculation.2SLS method, instruments available actually needed estimation, address , matrix created includes necessary instruments, simplifies calculation.IV-GMM method uses available instruments, applies weighting system prioritize instruments relevant. approach useful instruments necessary, can make calculation complex. IV-GMM method uses criterion function weight estimates improve accuracy.IV-GMM method uses available instruments, applies weighting system prioritize instruments relevant. approach useful instruments necessary, can make calculation complex. IV-GMM method uses criterion function weight estimates improve accuracy.short, always use IV-GMM overid problemsIn short, always use IV-GMM overid problemsGMM estimator minimizes\\[\nJ (\\hat{\\beta}_{GMM} ) = N \\bar{g}(\\hat{\\beta}_{GMM})' W \\bar{g} (\\hat{\\beta}_{GMM})\n\\]\\(W\\) symmetric weighting matrix \\(l \\times l\\)overid equation, solving set FOCs IV-GMM estimator, \\[\n\\hat{\\beta}_{GMM} = (X'ZWZ' X)^{-1} X'ZWZ'Y\n\\]identical \\(W\\) matrices. optimal \\(W = S^{-1}\\) (L. P. Hansen 1982) \\(S\\) covariance matrix moment conditions produce efficient estimator:\\[\nS = E[Z'uu'Z] = \\lim_{N \\\\infty} N^{-1}[Z' \\Omega Z]\n\\]consistent estimator \\(S\\) 2SLS residuals, feasible IV-GMM estimator can defined \\[\n\\hat{\\beta}_{FEGMM} = (X'Z \\hat{S}^{-1} Z' X)^{-1} X'Z \\hat{S}^{-1} Z'Y\n\\]cases \\(\\Omega\\) (.e., vcov error process \\(u\\)) satisfy classical assumptionsIID\\(S = \\sigma^2_u I_N\\)optimal weighting matrix proportional identity matrixThen, IV-GMM estimator standard IV (2SLS) estimator.IV-GMM, also additional test overid restrictions: GMM distance (also known Hayashi C statistic)account clustering, one can use code provided blog","code":""},{"path":"instrumental-variables.html","id":"inference-4","chapter":"30 Instrumental Variables","heading":"30.3 Inference","text":"just-identified instrument variable model, \\[\nY = \\beta X + u\n\\]\\(corr(u, Z) = 0\\) (relevant assumption) \\(corr(Z,X) \\neq 0\\) (exogenous assumption)t-ratio approach construct 95 CIs \\[\n\\hat{\\beta} \\pm 1.96 \\sqrt{\\hat{V}_N(\\hat{\\beta})}\n\\]wrong, long recognized understand “weak instruments” problem Dufour (1997)test null hypothesis \\(\\beta = \\beta_0\\) (Lee et al. 2022) \\[ \\frac{(\\hat{\\beta} - \\beta_0)^2}{\\hat{V}_N(\\hat{\\beta})} = \\hat{t}^2 = \\hat{t}^2_{AR} \\times \\frac{1}{1 - \\hat{\\rho} \\frac{\\hat{t}_{AR}}{\\hat{f}} + \\frac{\\hat{t}^2_{AR}}{\\hat{f}^2}} \\] \\(\\hat{t}_{AR}^2 \\sim \\chi^2(1)\\) (even weak instruments) (T. W. Anderson Rubin 1949)\\[\n\\hat{t}_{AR} = \\frac{\\hat{\\pi}(\\hat{\\beta} - \\beta_0)}{\\sqrt{\\hat{V}_N (\\hat{\\pi} (\\hat{\\beta} - \\beta_0))}} \\sim N(0,1)\n\\]\\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\sim N\\)\\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\sim N\\)\\(\\hat{\\pi}\\) = 1st-stage coefficient\\(\\hat{\\pi}\\) = 1st-stage coefficient\\(\\hat{\\rho} = COV(Zv, Zu)\\) = correlation 1st-stage residual estimate \\(u\\)\\(\\hat{\\rho} = COV(Zv, Zu)\\) = correlation 1st-stage residual estimate \\(u\\)Even large samples, \\(\\hat{t}^2 \\neq \\hat{t}^2_{AR}\\) right-hand term degenerate distribution. Thus, normal t critical values wouldn’t work.t-ratios match standard normal, matches proposed density Staiger Stock (1997) J. H. Stock Yogo (2005) .deviation \\(\\hat{t}^2 , \\hat{t}^2_{AR}\\) depends \\(\\pi\\) (.e., correlation instrument endogenous variable)\\(E(F)\\) (.e., strength first-stage)Magnitude \\(|\\rho|\\) (.e., degree endogeneity)Hence, can think several scenarios:Worst case: weak first stage (\\(\\pi = 0\\)) high degree endogeneity (\\(|\\rho |= 1\\)).interval \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) contain true parameter \\(\\beta\\).interval \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) contain true parameter \\(\\beta\\).5 percent significance test conditions incorrectly reject null hypothesis (\\(\\beta = \\beta_0\\)) 100% time.5 percent significance test conditions incorrectly reject null hypothesis (\\(\\beta = \\beta_0\\)) 100% time.Best case: endogeneity (\\(\\rho =0\\)) large \\(\\hat{f}\\) (strong first-stage)interval \\(\\hat{\\beta} \\pm 1.96 \\times SD\\) accurately contains \\(\\beta\\) least 95% time.Intermediate case: performance interval lies two extremes.Solutions: valid inference \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) using t-ratio (\\(\\hat{t}^2 \\approx \\hat{t}^2_{AR}\\)), can eitherAssume problem away\nAssume \\(E(F) > 142.6\\) (Lee et al. 2022) (much assumption since can observe first-stage F-stat empirically).\nAssume \\(|\\rho| < 0.565\\) Lee et al. (2022), defeats motivation use IV first place think strong endogeneity bias, ’s trying correct (circular argument).\nAssume \\(E(F) > 142.6\\) (Lee et al. 2022) (much assumption since can observe first-stage F-stat empirically).Assume \\(|\\rho| < 0.565\\) Lee et al. (2022), defeats motivation use IV first place think strong endogeneity bias, ’s trying correct (circular argument).Deal head \nAR approach (T. W. Anderson Rubin 1949)\ntF Procedure (Lee et al. 2022)\nAK approach (J. Angrist Kolesár 2023)\nAR approach (T. W. Anderson Rubin 1949)tF Procedure (Lee et al. 2022)AK approach (J. Angrist Kolesár 2023)Common Practices & Challenges:t-ratio test preferred many researchers pitfalls:\nKnown -reject (equivalently, -cover confidence intervals), especially weak instruments Dufour (1997).\nt-ratio test preferred many researchers pitfalls:Known -reject (equivalently, -cover confidence intervals), especially weak instruments Dufour (1997).address :\nfirst-stage F-statistic used indicator weak instruments.\nJ. H. Stock Yogo (2005) provided framework understand correct distortions.\naddress :first-stage F-statistic used indicator weak instruments.first-stage F-statistic used indicator weak instruments.J. H. Stock Yogo (2005) provided framework understand correct distortions.J. H. Stock Yogo (2005) provided framework understand correct distortions.Misinterpretations:Common errors application:\nUsing rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).\nMislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)\nCommon errors application:Using rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).Using rule--thumb F-stat threshold 10 instead referring J. H. Stock Yogo (2005).Mislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)Mislabeling intervals \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) 95% confidence intervals (passed \\(F>10\\) rule thumb). Staiger Stock (1997) clarified intervals actually represent 85% confidence using \\(F > 16.38\\) J. H. Stock Yogo (2005)Pretesting weak instruments might exacerbate -rejection t-ratio test mentioned (. R. Hall, Rudebusch, Wilcox 1996).Pretesting weak instruments might exacerbate -rejection t-ratio test mentioned (. R. Hall, Rudebusch, Wilcox 1996).Selective model specification (.e., dropping certain specification) based F-statistics also leads significant distortions (. Andrews, Stock, Sun 2019).Selective model specification (.e., dropping certain specification) based F-statistics also leads significant distortions (. Andrews, Stock, Sun 2019).","code":""},{"path":"instrumental-variables.html","id":"ar-approach","chapter":"30 Instrumental Variables","heading":"30.3.1 AR approach","text":"Validity Anderson-Rubin Test (notated AR) (T. W. Anderson Rubin 1949):Gives accurate results even non-normal homoskedastic errors (Staiger Stock 1997).Gives accurate results even non-normal homoskedastic errors (Staiger Stock 1997).Maintains validity across diverse error structures (J. H. Stock Wright 2000).Maintains validity across diverse error structures (J. H. Stock Wright 2000).Minimizes type II error among several alternative tests, cases :\nHomoskedastic errors M. J. Moreira (2009).\nGeneralized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).\nMinimizes type II error among several alternative tests, cases :Homoskedastic errors M. J. Moreira (2009).Homoskedastic errors M. J. Moreira (2009).Generalized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).Generalized heteroskedastic, clustered, autocorrelated errors (H. Moreira Moreira 2019).","code":"\nlibrary(ivDiag)\n\n# AR test (robust to weak instruments)\n# example by the package's authors\nivDiag::AR_test(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    # treatment\n    D = \"lm_pob_mesa\",\n    # instruments\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    CI = FALSE\n)\n#> $Fstat\n#>         F       df1       df2         p \n#>   50.5097    1.0000 4350.0000    0.0000\n\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$AR\n#> $Fstat\n#>         F       df1       df2         p \n#>   50.5097    1.0000 4350.0000    0.0000 \n#> \n#> $ci.print\n#> [1] \"[-1.2545, -0.7156]\"\n#> \n#> $ci\n#> [1] -1.2545169 -0.7155854\n#> \n#> $bounded\n#> [1] TRUE\nivDiag::plot_coef(g)"},{"path":"instrumental-variables.html","id":"tf-procedure","chapter":"30 Instrumental Variables","heading":"30.3.2 tF Procedure","text":"Lee et al. (2022) propose new method aligned better traditional econometric training AR, called tF procedure. incorporates 1st-stage F-stat 2SLS \\(t\\)-value. method applicable single instrumental variable (.e., just-identified model), includingRandomized trials imperfect compliance (G. W. Imbens Angrist 1994).Randomized trials imperfect compliance (G. W. Imbens Angrist 1994).Fuzzy Regression Discontinuity designs (Lee Lemieux 2010).Fuzzy Regression Discontinuity designs (Lee Lemieux 2010).Fuzzy regression kink designs (Card et al. 2015).Fuzzy regression kink designs (Card et al. 2015).See . Andrews, Stock, Sun (2019) comparison AR approach tF Procedure.tF Procedure:Adjusts t-ratio based first-stage F-statistic.Adjusts t-ratio based first-stage F-statistic.Rather fixed pretesting threshold, applies adjustment factor 2SLS standard errors.Rather fixed pretesting threshold, applies adjustment factor 2SLS standard errors.Adjustment factors provided 95% 99% confidence levels.Adjustment factors provided 95% 99% confidence levels.Advantages tF Procedure:Smooth Adjustment:\nGives usable finite confidence intervals smaller F statistic values.\n95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.\nSmooth Adjustment:Gives usable finite confidence intervals smaller F statistic values.Gives usable finite confidence intervals smaller F statistic values.95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.95% confidence applicable \\(F > 3.84\\), aligning AR’s bounded 95% confidence intervals.Clear Confidence Levels:\nlevels incorporate effects basing inference first-stage F.\nMirrors AR zero distortion procedures.\nClear Confidence Levels:levels incorporate effects basing inference first-stage F.levels incorporate effects basing inference first-stage F.Mirrors AR zero distortion procedures.Mirrors AR zero distortion procedures.Robustness:\nRobust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).\nadjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).\nRobustness:Robust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).Robust common error structures (e.g., heteroskedasticity clustering /autocorrelated errors).adjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).adjustments necessary long robust variance estimators consistently used (robust variance estimator used 1st-stage IV estimate).Comparison AR:\nSurprisingly, \\(F > 3.84\\), AR’s expected interval length infinite, tF’s finite (.e., better).\nComparison AR:Surprisingly, \\(F > 3.84\\), AR’s expected interval length infinite, tF’s finite (.e., better).Applicability:\ntF adjustment can re-evaluate published studies first-stage F-statistic available.\nOriginal data access needed.\nApplicability:tF adjustment can re-evaluate published studies first-stage F-statistic available.tF adjustment can re-evaluate published studies first-stage F-statistic available.Original data access needed.Original data access needed.Impacts Applied Research:Lee et al. (2022) examined recent single-instrument specification studies American Economic Review (AER).Lee et al. (2022) examined recent single-instrument specification studies American Economic Review (AER).Observations:\nleast 25% studied specifications, using tF increased confidence interval lengths :\n49% (5% significance level).\n136% (1% significance level).\n\nspecifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.\nConclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.\nObservations:least 25% studied specifications, using tF increased confidence interval lengths :\n49% (5% significance level).\n136% (1% significance level).\nleast 25% studied specifications, using tF increased confidence interval lengths :49% (5% significance level).49% (5% significance level).136% (1% significance level).136% (1% significance level).specifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.specifications \\(F > 10\\) \\(t > 1.96\\), 25% became statistically insignificant 5% level adjusted using tF.Conclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.Conclusion: tF adjustments greatly influence inferences research employing t-ratio inferences.Notation\\(Y = X \\beta + W \\gamma + u\\)\\(X = Z \\pi + W \\xi + \\nu\\)\\(W\\): Additional covariates, possibly including intercept term.\\(X\\): variable interest\\(Z\\): instrumentsKey Statistics:\\(t\\)-ratio instrumental variable estimator: \\(\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\\)\\(t\\)-ratio instrumental variable estimator: \\(\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\\)\\(t\\)-ratio first-stage coefficient: \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\\)\\(t\\)-ratio first-stage coefficient: \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\\)\\(\\hat{F} = \\hat{f}^2\\)\\(\\hat{F} = \\hat{f}^2\\)\\(\\hat{\\beta}\\): Instrumental variable estimator.\\(\\hat{V}_N (\\hat{\\beta})\\): Estimated variance \\(\\hat{\\beta}\\), possibly robust deal non-iid errors.\\(\\hat{t}\\): \\(t\\)-ratio null hypothesis.\\(\\hat{f}\\): \\(t\\)-ratio null hypothesis \\(\\pi=0\\).Traditional \\(t\\) Inference:large samples, \\(\\hat{t}^2 \\^d t^2\\)Standard normal critical values \\(\\pm 1.96\\) 5% significance level testing.Distortions Inference case IV:Use standard normal can lead distorted inferences even large samples.\nDespite large samples, t-distribution might normal.\nDespite large samples, t-distribution might normal.magnitude distortion can quantified.\nJ. H. Stock Yogo (2005) provides formula Wald test statistics using 2SLS.\n\\(t^2\\) formula allows quantification inference distortions.\njust-identified case one endogenous regressor \\(t^2 = f + t_{AR} + \\rho f t_{AR}\\) (J. H. Stock Yogo 2005)\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\n\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\n\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))\n\nJ. H. Stock Yogo (2005) provides formula Wald test statistics using 2SLS.\\(t^2\\) formula allows quantification inference distortions.just-identified case one endogenous regressor \\(t^2 = f + t_{AR} + \\rho f t_{AR}\\) (J. H. Stock Yogo 2005)\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\n\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\n\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))\n\\(\\hat{f} \\^d f\\) \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) \\(AV(\\hat{\\pi})\\) asymptotic variance \\(\\hat{\\pi}\\)\\(t_{AR}\\) standard normal \\(AR = t^2_{AR}\\)\\(\\rho\\) (degree endogeneity) correlation \\(Zu\\) \\(Z \\nu\\) (data homoskedastic, \\(\\rho\\) correlation \\(u\\) \\(\\nu\\))Implications \\(t^2\\) formula:Varies rejection rates depending \\(\\rho\\) value.\n\\(\\rho \\(0,0.5]\\) (low) t-ratio rejects probability nominal \\(0.05\\) rate\n\\(\\rho = 0.8\\) (high) rejection rate can \\(0.13\\)\n\\(\\rho \\(0,0.5]\\) (low) t-ratio rejects probability nominal \\(0.05\\) rate\\(\\rho = 0.8\\) (high) rejection rate can \\(0.13\\)short, incorrect test size relying solely \\(t^2\\) (based traditional econometric understanding)correct , one canEstimate usually 2SLS standard errorsMultiply SE adjustment factor based observed first-stage \\(\\hat{F}\\) statOne can go back traditional hypothesis using either t-ratio confidence intervalsLee et al. (2022) call adjusted SE “0.05 tF SE”.","code":"\nlibrary(ivDiag)\ng <- ivDiag::ivDiag(\n    data = rueda,\n    Y = \"e_vote_buying\",\n    D = \"lm_pob_mesa\",\n    Z = \"lz_pob_mesa_f\",\n    controls = c(\"lpopulation\", \"lpotencial\"),\n    cl = \"muni_code\",\n    cores = 4,\n    bootstrap = FALSE\n)\ng$tF\n#>         F        cF      Coef        SE         t    CI2.5%   CI97.5%   p-value \n#> 8598.3264    1.9600   -0.9835    0.1540   -6.3872   -1.2853   -0.6817    0.0000\n# example in fixest package\nlibrary(fixest)\nlibrary(tidyverse)\nbase = iris\nnames(base) = c(\"y\", \"x1\", \"x_endo_1\", \"x_inst_1\", \"fe\")\nset.seed(2)\nbase$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5)\nbase$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5)\n\nest_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base)\nest_iv\n#> TSLS estimation - Dep. Var.: y\n#>                   Endo.    : x_endo_1, x_endo_2\n#>                   Instr.   : x_inst_1, x_inst_2\n#> Second stage: Dep. Var.: y\n#> Observations: 150\n#> Standard-errors: IID \n#>              Estimate Std. Error  t value   Pr(>|t|)    \n#> (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***\n#> fit_x_endo_1 0.444982   0.022086 20.14744  < 2.2e-16 ***\n#> fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  \n#> x1           0.565095   0.084715  6.67051 4.9180e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> RMSE: 0.398842   Adj. R2: 0.761653\n#> F-test (1st stage), x_endo_1: stat = 903.2    , p < 2.2e-16 , on 2 and 146 DoF.\n#> F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.\n#>                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.\n\nres_est_iv <- est_iv$coeftable |> \n    rownames_to_column()\n\n\ncoef_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Estimate\"]\nse_of_interest <-\n    res_est_iv[res_est_iv$rowname == \"fit_x_endo_1\", \"Std. Error\"]\nfstat_1st <- fitstat(est_iv, type = \"ivf1\")[[1]]$stat\n\n# To get the correct SE based on 1st-stage F-stat (This result is similar without adjustment since F is large)\n# the results are the new CIS and p.value\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = fstat_1st) |> \n    causalverse::nice_tab(5)\n#>          F   cF    Coef      SE        t  CI2.5. CI97.5. p.value\n#> 1 903.1628 1.96 0.44498 0.02209 20.14744 0.40169 0.48827       0\n\n# We can try to see a different 1st-stage F-stat and how it changes the results\ntF(coef = coef_of_interest, se = se_of_interest, Fstat = 2) |> \n    causalverse::nice_tab(5)\n#>   F    cF    Coef      SE        t  CI2.5. CI97.5. p.value\n#> 1 2 18.66 0.44498 0.02209 20.14744 0.03285 0.85711 0.03432"},{"path":"instrumental-variables.html","id":"ak-approach","chapter":"30 Instrumental Variables","heading":"30.3.3 AK approach","text":"(J. Angrist Kolesár 2023)","code":""},{"path":"instrumental-variables.html","id":"testing-assumptions","chapter":"30 Instrumental Variables","heading":"30.4 Testing Assumptions","text":"\\[\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\\(X_1\\) exogenous variables\\(X_1\\) exogenous variables\\(X_2\\) endogenous variables\\(X_2\\) endogenous variables\\(Z\\) instrumental variables\\(Z\\) instrumental variablesIf \\(Z\\) satisfies relevance condition, means \\(Cov(Z, X_2) \\neq 0\\)important need able estimate \\(\\beta_2\\) \\[\n\\beta_2 = \\frac{Cov(Z,Y)}{Cov(Z, X_2)}\n\\]\\(Z\\) satisfies exogeneity condition, \\(E[Z\\epsilon]=0\\), can achieve \\(Z\\) direct effect \\(Y\\) except \\(X_2\\)\\(Z\\) direct effect \\(Y\\) except \\(X_2\\)presence omitted variable, \\(Z\\) uncorrelated variable.presence omitted variable, \\(Z\\) uncorrelated variable.just want know effect \\(Z\\) \\(Y\\) (reduced form) coefficient \\(Z\\) \\[\n\\rho = \\frac{Cov(Y, Z)}{Var(Z)}\n\\]effect \\(X_2\\) (exclusion restriction assumption).can also consistently estimate effect \\(Z\\) \\(X\\) (first stage) coefficient \\(X_2\\) \\[\n\\pi = \\frac{Cov(X_2, Z)}{Var(Z)}\n\\]IV estimate \\[\n\\beta_2 = \\frac{Cov(Y,Z)}{Cov(X_2, Z)} = \\frac{\\rho}{\\pi}\n\\]","code":""},{"path":"instrumental-variables.html","id":"relevance-assumption","chapter":"30 Instrumental Variables","heading":"30.4.1 Relevance Assumption","text":"Weak instruments: can explain little variation endogenous regressor\nCoefficient estimate endogenous variable inaccurate.\ncases weak instruments unavoidable, M. J. Moreira (2003) proposes conditional likelihood ratio test robust inference. test considered approximately optimal weak instrument scenarios (D. W. Andrews, Moreira, Stock 2008; D. W. Andrews Marmer 2008).\nWeak instruments: can explain little variation endogenous regressorCoefficient estimate endogenous variable inaccurate.cases weak instruments unavoidable, M. J. Moreira (2003) proposes conditional likelihood ratio test robust inference. test considered approximately optimal weak instrument scenarios (D. W. Andrews, Moreira, Stock 2008; D. W. Andrews Marmer 2008).Rule thumb:\nCompute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)\nuse linearHypothesis() see instrument coefficients.\nRule thumb:Compute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)Compute F-statistic first-stage, greater 10. discouraged now Lee et al. (2022)use linearHypothesis() see instrument coefficients.use linearHypothesis() see instrument coefficients.First-Stage F-TestIn context two-stage least squares (2SLS) setup estimating equation:\\[\nY = X \\beta + \\epsilon\n\\]\\(X\\) endogenous, typically estimate first-stage regression :\\[\nX = Z \\pi + u\n\\]𝑍Z instrument.first-stage F-test evaluates joint significance instruments first stage:\\[\nF = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/ (n - k - 1)}\n\\]:\\(SSR_r\\) sum squared residuals restricted model (instruments, just constant).\\(SSR_r\\) sum squared residuals restricted model (instruments, just constant).\\(SSR_{ur}\\) sum squared residuals unrestricted model (instruments).\\(SSR_{ur}\\) sum squared residuals unrestricted model (instruments).\\(q\\) number instruments excluded main equation.\\(q\\) number instruments excluded main equation.\\(n\\) number observations.\\(n\\) number observations.\\(k\\) number explanatory variables excluding instruments.\\(k\\) number explanatory variables excluding instruments.Cragg-Donald TestThe Cragg-Donald statistic essentially Wald statistic joint significance instruments first stage, ’s used specifically multiple endogenous regressors. ’s calculated :\\[\nCD = n \\times (R_{ur}^2 - R_r^2)\n\\]:\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(R_{ur}^2\\) \\(R_r^2\\) R-squared values unrestricted restricted models respectively.\\(n\\) number observations.\\(n\\) number observations.one endogenous variable, Cragg-Donald test results align closely Stock Yogo. Anderson canonical correlation test, likelihood ratio test, also works similar conditions, contrasting Cragg-Donald’s Wald statistic approach. valid one endogenous variable least one instrument.Stock-Yogo Weak IV TestThe Stock-Yogo test directly compute statistic like F-test Cragg-Donald, rather uses pre-computed critical values assess strength instruments. often uses eigenvalues derived concentration matrix:\\[\nS = \\frac{1}{n} (Z' X) (X'Z)\n\\]\\(Z\\) matrix instruments \\(X\\) matrix endogenous regressors.Stock Yogo provide critical values different scenarios (bias, size distortion) given number instruments endogenous regressors, based smallest eigenvalue \\(S\\). test compares eigenvalues critical values correspond thresholds permissible bias size distortion 2SLS estimator.Critical Values Test Conditions: critical values derived Stock Yogo depend level acceptable bias, number endogenous regressors, number instruments. example, 5% maximum acceptable bias, one endogenous variable, three instruments, critical value sufficient first stage F-statistic 13.91. Note framework requires least two overidentifying degree freedom.ComparisonAll mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors independently identically distributed. assumption violated, Kleinbergen-Paap test robust violations iid assumption can applied even single endogenous variable instrument, provided model properly identified (Baum Schaffer 2021).","code":""},{"path":"instrumental-variables.html","id":"cragg-donald","chapter":"30 Instrumental Variables","heading":"30.4.1.1 Cragg-Donald","text":"(Cragg Donald 1993)Similar first-stage F-statisticLarge CD statistic implies instruments strong, case . judge critical value, look Stock-Yogo","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\n\ncragg_donald(\n    # control variables\n    X = ~ 1, \n    # endogeneous variables\n    D = ~ x, \n    # instrument variables \n    Z = ~ z, \n    data = WeakInstrument\n)\n#> Cragg-Donald test for weak instruments:\n#> \n#>      Data:                        WeakInstrument \n#>      Controls:                    ~1 \n#>      Treatments:                  ~x \n#>      Instruments:                 ~z \n#> \n#>      Cragg-Donald Statistic:        4.566136 \n#>      Df:                                 198"},{"path":"instrumental-variables.html","id":"stock-yogo","chapter":"30 Instrumental Variables","heading":"30.4.1.2 Stock-Yogo","text":"J. H. Stock Yogo (2002) set critical values bias less 10% (default)\\(H_0:\\) Instruments weak\\(H_1:\\) Instruments weakThe CD statistic bigger set critical value considered strong instruments.","code":"\nlibrary(cragg)\nlibrary(AER) # for dataaset\ndata(\"WeakInstrument\")\nstock_yogo_test(\n    # control variables\n    X = ~ 1,\n    # endogeneous variables\n    D = ~ x,\n    # instrument variables\n    Z = ~ z,\n    size_bias = \"bias\",\n    data = WeakInstrument\n)"},{"path":"instrumental-variables.html","id":"exogeneity-assumption","chapter":"30 Instrumental Variables","heading":"30.4.2 Exogeneity Assumption","text":"local average treatment effect (LATE) defined :\\[\n\\text{LATE} = \\frac{\\text{reduced form}}{\\text{first stage}} = \\frac{\\rho}{\\phi}\n\\]implies reduced form (\\(\\rho\\)) product first stage (\\(\\phi\\)) LATE:\\[\n\\rho = \\phi \\times \\text{LATE}\n\\]Thus, first stage (\\(\\phi\\)) 0, reduced form (\\(\\rho\\)) also 0.statistically significant reduced form estimate without corresponding first stage indicates issue, suggesting alternative channel linking instruments outcomes direct effect IV outcome.Direct Effect: direct effect 0 first stage 0, reduced form 0.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.\nNote: Extremely rare cases multiple additional paths perfectly cancel can also produce result, testing possible paths impractical.Direct Effect: direct effect IV outcome, reduced form can significantly different 0, even first stage 0.\nviolates exogeneity assumption, IV affect outcome treatment variable.\nviolates exogeneity assumption, IV affect outcome treatment variable.test validity exogeneity assumption, can use sanity test:Identify groups effects instruments treatment variable small significantly different 0. reduced form estimate groups also 0. “-first-stage samples” provide evidence whether exogeneity assumption violated.","code":"\n# Load necessary libraries\nlibrary(shiny)\nlibrary(AER)  # for ivreg\nlibrary(ggplot2)  # for visualization\nlibrary(dplyr)  # for data manipulation\n\n# Function to simulate the dataset\nsimulate_iv_data <- function(n, beta, phi, direct_effect) {\n  Z <- rnorm(n)\n  epsilon_x <- rnorm(n)\n  epsilon_y <- rnorm(n)\n  X <- phi * Z + epsilon_x\n  Y <- beta * X + direct_effect * Z + epsilon_y\n  data <- data.frame(Y = Y, X = X, Z = Z)\n  return(data)\n}\n\n# Function to run the simulations and calculate the effects\nrun_simulation <- function(n, beta, phi, direct_effect) {\n  # Simulate the data\n  simulated_data <- simulate_iv_data(n, beta, phi, direct_effect)\n  \n  # Estimate first-stage effect (phi)\n  first_stage <- lm(X ~ Z, data = simulated_data)\n  phi <- coef(first_stage)[\"Z\"]\n  phi_ci <- confint(first_stage)[\"Z\", ]\n  \n  # Estimate reduced-form effect (rho)\n  reduced_form <- lm(Y ~ Z, data = simulated_data)\n  rho <- coef(reduced_form)[\"Z\"]\n  rho_ci <- confint(reduced_form)[\"Z\", ]\n  \n  # Estimate LATE using IV regression\n  iv_model <- ivreg(Y ~ X | Z, data = simulated_data)\n  iv_late <- coef(iv_model)[\"X\"]\n  iv_late_ci <- confint(iv_model)[\"X\", ]\n  \n  # Calculate LATE as the ratio of reduced-form and first-stage coefficients\n  calculated_late <- rho / phi\n  calculated_late_se <- sqrt(\n    (rho_ci[2] - rho)^2 / phi^2 + (rho * (phi_ci[2] - phi) / phi^2)^2\n  )\n  calculated_late_ci <- c(calculated_late - 1.96 * calculated_late_se, \n                          calculated_late + 1.96 * calculated_late_se)\n  \n  # Return a list of results\n  list(phi = phi, \n       phi_ci = phi_ci,\n       rho = rho, \n       rho_ci = rho_ci,\n       direct_effect = direct_effect,\n       direct_effect_ci = c(direct_effect, direct_effect),  # Placeholder for direct effect CI\n       iv_late = iv_late, \n       iv_late_ci = iv_late_ci,\n       calculated_late = calculated_late, \n       calculated_late_ci = calculated_late_ci,\n       true_effect = beta,\n       true_effect_ci = c(beta, beta))  # Placeholder for true effect CI\n}\n\n# Define UI for the sliders\nui <- fluidPage(\n  titlePanel(\"IV Model Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"beta\", \"True Effect of X on Y (beta):\", min = 0, max = 1.0, value = 0.5, step = 0.1),\n      sliderInput(\"phi\", \"First Stage Effect (phi):\", min = 0, max = 1.0, value = 0.7, step = 0.1),\n      sliderInput(\"direct_effect\", \"Direct Effect of Z on Y:\", min = -0.5, max = 0.5, value = 0, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"dotPlot\")\n    )\n  )\n)\n\n# Define server logic to run the simulation and generate the plot\nserver <- function(input, output) {\n  output$dotPlot <- renderPlot({\n    # Run simulation\n    results <- run_simulation(n = 1000, beta = input$beta, phi = input$phi, direct_effect = input$direct_effect)\n    \n    # Prepare data for plotting\n    plot_data <- data.frame(\n      Effect = c(\"First Stage (phi)\", \"Reduced Form (rho)\", \"Direct Effect\", \"LATE (Ratio)\", \"LATE (IV)\", \"True Effect\"),\n      Value = c(results$phi, results$rho, results$direct_effect, results$calculated_late, results$iv_late, results$true_effect),\n      CI_Lower = c(results$phi_ci[1], results$rho_ci[1], results$direct_effect_ci[1], results$calculated_late_ci[1], results$iv_late_ci[1], results$true_effect_ci[1]),\n      CI_Upper = c(results$phi_ci[2], results$rho_ci[2], results$direct_effect_ci[2], results$calculated_late_ci[2], results$iv_late_ci[2], results$true_effect_ci[2])\n    )\n    \n    # Create dot plot with confidence intervals\n    ggplot(plot_data, aes(x = Effect, y = Value)) +\n      geom_point(size = 3) +\n      geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +\n      labs(title = \"IV Model Effects\",\n           y = \"Coefficient Value\") +\n      coord_cartesian(ylim = c(-1, 1)) +  # Limits the y-axis to -1 to 1 but allows CI beyond\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"},{"path":"instrumental-variables.html","id":"overid-tests","chapter":"30 Instrumental Variables","heading":"30.4.2.1 Overid Tests","text":"Wald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenous\nPeople might prefer Wald test Hausman test.\nWald test Hausman test exogeneity \\(X\\) assuming \\(Z\\) exogenousPeople might prefer Wald test Hausman test.Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Sargan (2SLS) simpler version Hansen’s J test (IV-GMM)Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).Modified J test (.e., Regularized jacknife IV): can handle weak instruments small sample size (Carrasco Doukali 2022) (also proposed regularized F-test test relevance assumption robust heteroskedasticity).New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)New advances: endogeneity robust inference finite sample sensitivity analysis inference (Kiviet 2020)tests can provide evidence fo validity -identifying restrictions sufficient necessary validity moment conditions (.e., assumption tested). (Deaton 2010; Parente Silva 2012)-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.-identifying restriction can still valid even instruments correlated error terms, case, ’re estimating longer parameters interest.Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)Rejection -identifying restrictions can also result parameter heterogeneity (J. D. Angrist, Graddy, Imbens 2000)overid tests hold value/info?Overidentifying restrictions valid irrespective instruments’ validity\nWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)\nOveridentifying restrictions valid irrespective instruments’ validityWhenever instruments motivation scale, estimated parameter interests close (Parente Silva 2012, 316)Overidentifying restriction invalid instrument valid\neffect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)\nOveridentifying restriction invalid instrument validWhen effect parameter interest heterogeneous (e.g., two groups two different true effects), first instrument can correlated variable interest first group second interments can correlated variable interest second group (.e., instrument valid), use instrument, can still identify parameter interest. However, use , estimate mixture two groups. Hence, overidentifying restriction invalid (single parameters can make errors model orthogonal instruments). result may seem confusing first subset overidentifying restrictions valid, full set also valid. However, interpretation flawed residual’s orthogonality instruments depends chosen set instruments, therefore set restrictions tested using two sets instruments together union sets restrictions tested using set instruments separately (Parente Silva 2012, 316)tests (overidentifying restrictions) used check whether different instruments identify parameters interest, check validity(J. . Hausman 1983; Parente Silva 2012)","code":""},{"path":"instrumental-variables.html","id":"wald-test-1","chapter":"30 Instrumental Variables","heading":"30.4.2.1.1 Wald Test","text":"Assuming \\(Z\\) exogenous (valid instrument), want know whether \\(X_2\\) exogenous1st stage:\\[\nX_2 = \\hat{\\alpha} Z + \\hat{\\epsilon}\n\\]2nd stage:\\[\nY = \\delta_0 X_1 + \\delta_1 X_2 + \\delta_2 \\hat{\\epsilon} + u\n\\]\\(\\hat{\\epsilon}\\) residuals 1st stageThe Wald test exogeneity assumes\\[\nH_0: \\delta_2 = 0 \\\\\nH_1: \\delta_2 \\neq 0\n\\]one endogenous variable one instrument, \\(\\delta_2\\) vector residuals first-stage equations. null hypothesis jointly equal 0.reject hypothesis, means \\(X_2\\) endogenous. Hence, test, want reject null hypothesis.test sacrificially significant, might just don’t enough information reject null.valid instrument \\(Z\\), whether \\(X_2\\) endogenous exogenous, coefficient estimates \\(X_2\\) still consistent. \\(X_2\\) exogenous, 2SLS inefficient (.e., larger standard errors).Intuition:\\(\\hat{\\epsilon}\\) supposed endogenous part \\(X_2\\), regress \\(Y\\) \\(\\hat{\\epsilon}\\) observe coefficient different 0. means exogenous part \\(X_2\\) can explain well impact \\(Y\\), endogenous part.","code":""},{"path":"instrumental-variables.html","id":"hausmans-test","chapter":"30 Instrumental Variables","heading":"30.4.2.1.2 Hausman’s Test","text":"Similar Wald Test identical Wald Test homoskedasticity (.e., homogeneity variances). assumption, ’s used less often Wald Test","code":""},{"path":"instrumental-variables.html","id":"hansens-j","chapter":"30 Instrumental Variables","heading":"30.4.2.1.3 Hansen’s J","text":"(L. P. Hansen 1982)(L. P. Hansen 1982)J-test (-identifying restrictions test): test whether additional instruments exogenous\nCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\nAssume least one instrument within \\(Z\\) exogenous\nJ-test (-identifying restrictions test): test whether additional instruments exogenousCan applied cases instruments endogenous variables\n\\(dim(Z) > dim(X_2)\\)\n\\(dim(Z) > dim(X_2)\\)Assume least one instrument within \\(Z\\) exogenousProcedure IV-GMM:Obtain residuals 2SLS estimationRegress residuals instruments exogenous variables.Test joint hypothesis coefficients residuals across instruments 0 (.e., true instruments exogenous).\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).\nexogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.\nCompute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).Compute \\(J = mF\\) \\(m\\) number instruments, \\(F\\) equation \\(F\\) statistic (can use linearHypothesis() ).exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.exogeneity assumption true, \\(J \\sim \\chi^2_{m-k}\\) \\(k\\) number endogenous variables.reject hypothesis, can \nfirst sets instruments invalid\nsecond sets instruments invalid\nsets instruments invalid\nfirst sets instruments invalidThe first sets instruments invalidThe second sets instruments invalidThe second sets instruments invalidBoth sets instruments invalidBoth sets instruments invalidNote: test true residuals homoskedastic.heteroskedasticity-robust \\(J\\)-statistic, see (Carrasco Doukali 2022; H. Li et al. 2022)","code":""},{"path":"instrumental-variables.html","id":"sargan-test","chapter":"30 Instrumental Variables","heading":"30.4.2.1.4 Sargan Test","text":"(Sargan 1958)Similar Hansen’s J, assumes homoskedasticityHave careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).careful sample collected exogenously. , choice-based sampling design, sampling weights considered consistent estimates. However, even apply sampling weights, tests suitable iid assumption errors already violated. Hence, test invalid case (Pitt 2011).one heteroskedasticity design, Sargan test invalid (Pitt 2011})one heteroskedasticity design, Sargan test invalid (Pitt 2011})","code":""},{"path":"instrumental-variables.html","id":"negative-r2","chapter":"30 Instrumental Variables","heading":"30.5 Negative \\(R^2\\)","text":"’s okay negative \\(R^2\\) 2nd stage. care consistent coefficient estimates.\\(R^2\\) statistical meaning instrumental variable regression 2 3SLS\\[\nR^2 = \\frac{MSS}{TSS}\n\\]whereMSS = model sum squares (TSS- RSS)TSS = total sum squares (\\(\\sum(y - \\bar{y})^2\\))RSS = residual sum squares (\\(\\sum (y - Xb)^2\\))\\(TSS > RSS\\), negative RSS negative \\(R^2\\). Since predicted values endogenous variables different endogenous variables , error used calculate RSS can different error second stage, RSS second stage can less TSS. information, see .","code":""},{"path":"instrumental-variables.html","id":"treatment-intensity","chapter":"30 Instrumental Variables","heading":"30.6 Treatment Intensity","text":"Two-Stage Least Squares (TSLS) can used estimate average causal effect variable treatment intensity, “identifies weighted average per-unit treatment effects along length causal response function” (J. D. Angrist Imbens 1995, 431). exampleDrug dosageDrug dosageHours exam prep score (Powers Swinton 1984)Hours exam prep score (Powers Swinton 1984)Cigarette smoking birth weights (Permutt Hebel 1989)Cigarette smoking birth weights (Permutt Hebel 1989)Years educationYears educationClass size test score (J. D. Angrist Lavy 1999)Class size test score (J. D. Angrist Lavy 1999)Sibship size earning (Lavy, Angrist, Schlosser 2006)Sibship size earning (Lavy, Angrist, Schlosser 2006)Social Media AdoptionSocial Media AdoptionThe average causal effect refers conditional expectation difference outcomes treated happened counterfactual world.Notes:need linearity assumption relationships dependent variable, treatment intensities, instruments.ExampleIn original paper, J. D. Angrist Imbens (1995) take example schooling effect earnings quarters birth instrumental variable.additional year schooling, can increase earnings, additional year can heterogeneous (sense grade 9th grade 10th qualitatively different one can change different school).\\[\nY = \\gamma_0 + \\gamma_1 X_1 + \\rho S + \\epsilon\n\\]\\(S\\) years schooling (.e., endogenous regressor)\\(S\\) years schooling (.e., endogenous regressor)\\(\\rho\\) return year schooling\\(\\rho\\) return year schooling\\(X_1\\) matrix exogenous covariates\\(X_1\\) matrix exogenous covariatesSchooling can also related exogenous variable \\(X_1\\)\\[\nS = \\delta_0 + X_1 \\delta_1 + X_2 \\delta_2 + \\eta\n\\]\\(X_2\\) exogenous instrument\\(X_2\\) exogenous instrument\\(\\delta_2\\) coefficient instrument\\(\\delta_2\\) coefficient instrumentby using fitted value second, TSLS can give consistent estimate effect schooling earning\\[\nY = \\gamma_0 + X_1 \\gamma-1 + \\rho \\hat{S} + \\nu\n\\]give \\(\\rho\\) causal interpretation,first SUTVA (stable unit treatment value assumption), potential outcomes person different years schooling independent.\\(\\rho\\) probability limit equal weighted average \\(E[Y_j - Y_{j-1}] \\forall j\\)Even though first bullet point trivial, time don’t defend much research article, second bullet point harder one argue apply certain cases.","code":""},{"path":"instrumental-variables.html","id":"control-function","chapter":"30 Instrumental Variables","heading":"30.7 Control Function","text":"Also known two-stage residual inclusionResources:Binary outcome binary endogenous variable application (E. Tchetgen Tchetgen 2014)\nrare events: use logistic model 2nd stage\nnon-rare events: use risk ratio regression 2nd stage\nBinary outcome binary endogenous variable application (E. Tchetgen Tchetgen 2014)rare events: use logistic model 2nd stageIn rare events: use logistic model 2nd stageIn non-rare events: use risk ratio regression 2nd stageIn non-rare events: use risk ratio regression 2nd stageApplication marketing consumer choice model (Petrin Train 2010)Application marketing consumer choice model (Petrin Train 2010)NotesThis approach better suited models nonadditive errors (e.g., discrete choice models), binary endogenous model, binary response variable, etc.\\[\nY = g(X) + U \\\\\nX = \\pi(Z) + V \\\\\nE(U |Z,V) = E(U|V) \\\\\nE(V|Z) = 0\n\\]control function approach,\\[\nE(Y|Z,V) = g(X) + E(U|Z,V) \\\\\n= g(X) + E(U|V) \\\\\n= g(X) + h(V)\n\\]\\(h(V)\\) control function models endogeneityLinear parametersLinear Endogenous Variables:\ncontrol function function approach identical usual 2SLS estimator\ncontrol function function approach identical usual 2SLS estimatorNonlinear Endogenous Variables:\ncontrol function different 2SLS estimator\ncontrol function different 2SLS estimatorNonlinear parameters:\nCF function superior 2SLS estimator\nCF function superior 2SLS estimator","code":""},{"path":"instrumental-variables.html","id":"simulation","chapter":"30 Instrumental Variables","heading":"30.7.1 Simulation","text":"Linear parameter linear endogenous variableNonlinear endogenous variableNonlinear parameters","code":"\nlibrary(fixest)\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n# Set the seed for reproducibility\nset.seed(123)\nn = 10000\n# Generate the exogenous variable from a normal distribution\nexogenous <- rnorm(n, mean = 5, sd = 1)\n\n# Generate the omitted variable as a function of the exogenous variable\nomitted <- rnorm(n, mean = 2, sd = 1)\n\n# Generate the endogenous variable as a function of the omitted variable and the exogenous variable\nendogenous <- 5 * omitted + 2 * exogenous + rnorm(n, mean = 0, sd = 1)\n\n# nonlinear endogenous variable\nendogenous_nonlinear <- 5 * omitted^2 + 2 * exogenous + rnorm(100, mean = 0, sd = 1)\n\nunrelated <- rexp(n, rate = 1)\n\n# Generate the response variable as a function of the endogenous variable and the omitted variable\nresponse <- 4 +  3 * endogenous + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear <- 4 +  3 * endogenous_nonlinear + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\nresponse_nonlinear_para <- 4 +  3 * endogenous ^ 2 + 6 * omitted + rnorm(n, mean = 0, sd = 1)\n\n\n# Combine the variables into a data frame\nmy_data <-\n    data.frame(\n        exogenous,\n        omitted,\n        endogenous,\n        response,\n        unrelated,\n        response,\n        response_nonlinear,\n        response_nonlinear_para\n    )\n\n# View the first few rows of the data frame\n# head(my_data)\n\nwo_omitted <- feols(response ~ endogenous + sw0(unrelated), data = my_data)\nw_omitted  <- feols(response ~ endogenous + omitted + unrelated, data = my_data)\n\n\n# ivreg::ivreg(response ~ endogenous + unrelated | exogenous, data = my_data)\niv <- feols(response ~ 1 + sw0(unrelated) | endogenous ~ exogenous, data = my_data)\n\netable(\n    wo_omitted,\n    w_omitted,\n    iv, \n    digits = 2\n    # vcov = list(\"each\", \"iid\", \"hetero\")\n)\n#>                   wo_omitted.1   wo_omitted.2      w_omitted           iv.1\n#> Dependent Var.:       response       response       response       response\n#>                                                                            \n#> Constant        -3.9*** (0.10) -4.0*** (0.10)  4.0*** (0.05) 15.7*** (0.59)\n#> endogenous      4.0*** (0.005) 4.0*** (0.005) 3.0*** (0.004)  3.0*** (0.03)\n#> unrelated                         0.03 (0.03)  0.002 (0.010)               \n#> omitted                                        6.0*** (0.02)               \n#> _______________ ______________ ______________ ______________ ______________\n#> S.E. type                  IID            IID            IID            IID\n#> Observations            10,000         10,000         10,000         10,000\n#> R2                     0.98566        0.98567        0.99803        0.92608\n#> Adj. R2                0.98566        0.98566        0.99803        0.92607\n#> \n#>                           iv.2\n#> Dependent Var.:       response\n#>                               \n#> Constant        15.6*** (0.59)\n#> endogenous       3.0*** (0.03)\n#> unrelated         0.10. (0.06)\n#> omitted                       \n#> _______________ ______________\n#> S.E. type                  IID\n#> Observations            10,000\n#> R2                     0.92610\n#> Adj. R2                0.92608\n#> ---\n#> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# manual\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -77.683 -14.374  -0.107  14.289  78.274 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     15.6743     2.0819   7.529 5.57e-14 ***\n#> new_endogenous   3.0142     0.1039  29.025  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 21.26 on 9998 degrees of freedom\n#> Multiple R-squared:  0.07771,    Adjusted R-squared:  0.07762 \n#> F-statistic: 842.4 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response ~ endogenous + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response ~ endogenous + residual, data = new_data_cf)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.360 -1.016  0.003  1.023  5.201 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 15.674265   0.149350   105.0   <2e-16 ***\n#> endogenous   3.014202   0.007450   404.6   <2e-16 ***\n#> residual     1.140920   0.008027   142.1   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.525 on 9997 degrees of freedom\n#> Multiple R-squared:  0.9953, Adjusted R-squared:  0.9953 \n#> F-statistic: 1.048e+06 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous_nonlinear ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous_nonlinear = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ new_endogenous_nonlinear, data = new_data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -94.43 -52.10 -15.29  36.50 446.08 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               15.3390    11.8175   1.298    0.194    \n#> new_endogenous_nonlinear   3.0174     0.3376   8.938   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 69.51 on 9998 degrees of freedom\n#> Multiple R-squared:  0.007927,   Adjusted R-squared:  0.007828 \n#> F-statistic: 79.89 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear ~ endogenous_nonlinear + residual, \n#>     data = new_data_cf)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -17.5437  -0.8348   0.4614   1.4424   4.8154 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          15.33904    0.38459   39.88   <2e-16 ***\n#> endogenous_nonlinear  3.01737    0.01099  274.64   <2e-16 ***\n#> residual              0.24919    0.01104   22.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.262 on 9997 degrees of freedom\n#> Multiple R-squared:  0.9989, Adjusted R-squared:  0.9989 \n#> F-statistic: 4.753e+06 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))\n# 2SLS\nfirst_stage = lm(endogenous ~ exogenous, data = my_data)\n\nnew_data = cbind(my_data, new_endogenous = predict(first_stage, my_data))\nsecond_stage = lm(response_nonlinear_para ~ new_endogenous, data = new_data)\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ new_endogenous, data = new_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1536.5  -452.4   -80.7   368.4  3780.9 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    -1089.943     61.706  -17.66   <2e-16 ***\n#> new_endogenous   119.829      3.078   38.93   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 630.2 on 9998 degrees of freedom\n#> Multiple R-squared:  0.1316, Adjusted R-squared:  0.1316 \n#> F-statistic:  1516 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nnew_data_cf = cbind(my_data, residual = resid(first_stage))\nsecond_stage_cf = lm(response_nonlinear_para ~ endogenous_nonlinear + residual, data = new_data_cf)\nsummary(second_stage_cf)\n#> \n#> Call:\n#> lm(formula = response_nonlinear_para ~ endogenous_nonlinear + \n#>     residual, data = new_data_cf)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -961.00 -139.32  -16.02  135.57 1403.62 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)          678.1593     9.9177   68.38   <2e-16 ***\n#> endogenous_nonlinear  17.7884     0.2759   64.46   <2e-16 ***\n#> residual              52.5016     1.1552   45.45   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 231.9 on 9997 degrees of freedom\n#> Multiple R-squared:  0.8824, Adjusted R-squared:  0.8824 \n#> F-statistic: 3.751e+04 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nmodelsummary(list(second_stage, second_stage_cf))"},{"path":"instrumental-variables.html","id":"new-advances","chapter":"30 Instrumental Variables","heading":"30.8 New Advances","text":"Combine ML IV (Singh, Hosanagar, Gandhi 2020)","code":""},{"path":"matching-methods.html","id":"matching-methods","chapter":"31 Matching Methods","heading":"31 Matching Methods","text":"Matching process aims close back doors - potential sources bias - constructing comparison groups similar according set matching variables. helps ensure observed differences outcomes treatment comparison groups can confidently attributed treatment , rather factors may differ groups.Matching can use pre-treatment outcomes correct selection bias. real world data simulation, (Chabé-Ferret 2015) found matching generally underestimates average causal effect gets closer true effect number pre-treatment outcomes. selection bias symmetric around treatment date, still consistent implemented symmetrically (.e., number period treatment). cases selection bias asymmetric, MC simulations show Symmetric still performs better Matching.Matching useful, general solution causal problems (J. . Smith Todd 2005)Assumption: Observables can identify selection treatment control groupsIdentification: exclusion restriction can met conditional observablesMotivationEffect college quality earningsThey ultimately estimate treatment effect treated attending top (high ACT) versus bottom (low ACT) quartile collegeExampleAaronson, Barrow, Sander (2007)teachers qualifications (causally) affect student test scores?Step 1:\\[\nY_{ijt} = \\delta_0 + Y_{ij(t-1)} \\delta_1 + X_{} \\delta_2 + Z_{jt} \\delta_3 + \\epsilon_{ijt}\n\\]can always another variableAny observable sorting imperfectStep 2:\\[\nY_{ijst} = \\alpha_0 + Y_{ij(t-1)}\\alpha_1 + X_{} \\alpha_2 + Z_{jt} \\alpha_3 + \\gamma_s + u_{isjt}\n\\]\\(\\delta_3 >0\\)\\(\\delta_3 >0\\)\\(\\delta_3 > \\alpha_3\\)\\(\\delta_3 > \\alpha_3\\)\\(\\gamma_s\\) = school fixed effect\\(\\gamma_s\\) = school fixed effectSorting less within school. Hence, can introduce school fixed effectStep 3:Find schools look like putting students class randomly (good random) + run step 2\\[\n\\begin{aligned}\nY_{isjt} = Y_{isj(t-1)} \\lambda &+ X_{} \\alpha_1 +Z_{jt} \\alpha_{21} \\\\\n&+ (Z_{jt} \\times D_i)\\alpha_{22}+ \\gamma_5 + u_{isjt}\n\\end{aligned}\n\\]\\(D_{}\\) element \\(X_{}\\)\\(D_{}\\) element \\(X_{}\\)\\(Z_{}\\) = teacher experience\\(Z_{}\\) = teacher experience\\[\nD_{}=\n\\begin{cases}\n1 & \\text{ high poverty} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\\(H_0:\\) \\(\\alpha_{22} = 0\\) test effect heterogeneity whether effect teacher experience (\\(Z_{jt}\\)) differentFor low poverty \\(\\alpha_{21}\\)low poverty \\(\\alpha_{21}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)Matching selection observables works good observables.Sufficient identification assumption Selection observable/ back-door criterion (based Bernard Koch’s presentation)Strong conditional ignorability\n\\(Y(0),Y(1) \\perp T|X\\)\nhidden confounders\nStrong conditional ignorability\\(Y(0),Y(1) \\perp T|X\\)\\(Y(0),Y(1) \\perp T|X\\)hidden confoundersNo hidden confoundersOverlap\n\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\ntreatments non-zero probability observed\nOverlap\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)treatments non-zero probability observedAll treatments non-zero probability observedSUTVA/ Consistency\nTreatment outcomes different subjects independent\nSUTVA/ ConsistencyTreatment outcomes different subjects independentRelative OLSMatching makes common support explicit (changes default “ignore” “enforce”)Relaxes linear function form. Thus, less parametric.also helps high ratio controls treatments.detail summary (Stuart 2010)Matching defined “method aims equate (”balance”) distribution covariates treated control groups.” (Stuart 2010, 1)Equivalently, matching selection observables identifications strategy.think OLS estimate biased, matching estimate (almost surely) .Unconditionally, consider\\[\n\\begin{aligned}\nE(Y_i^T | T) - E(Y_i^C |C) &+ E(Y_i^C | T) - E(Y_i^C | T) \\\\\n= E(Y_i^T - Y_i^C | T) &+ [E(Y_i^C | T) - E(Y_i^C |C)] \\\\\n= E(Y_i^T - Y_i^C | T) &+ \\text{selection bias}\n\\end{aligned}\n\\]\\(E(Y_i^T - Y_i^C | T)\\) causal inference want know.Randomization eliminates selection bias.don’t randomization, \\(E(Y_i^C | T) \\neq E(Y_i^C |C)\\)Matching tries selection observables \\(E(Y_i^C | X, T) = E(Y_i^C|X, C)\\)Propensity Scores basically \\(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\\)Matching standard errors exceed OLS standard errorsThe treatment larger predictive power control use treatment pick control (control pick treatment).average treatment effect (ATE) \\[\n\\frac{1}{N_T} \\sum_{=1}^{N_T} (Y_i^T - \\frac{1}{N_{C_T}} \\sum_{=1}^{N_{C_T}} Y_i^C)\n\\]Since closed-form solution standard error average treatment effect, use bootstrapping get standard error.Professor Gary King advocates instead using word “matching”, use “pruning” (.e., deleting observations). preprocessing step prunes nonmatches make control variables less important analysis.Without MatchingImbalance data leads model dependence lead lot researcher discretion leads biasWith MatchingWe balance data essentially erase human discretionTable @ref(tab:Gary King - International Methods Colloquium talk 2015)Fully blocked superior onimbalanceimbalancemodel dependencemodel dependencepowerpowerefficiencyefficiencybiasbiasresearch costsresearch costsrobustnessrobustnessMatching used whenOutcomes available select subjects follow-upOutcomes available select subjects follow-upOutcomes available improve precision estimate (.e., reduce bias)Outcomes available improve precision estimate (.e., reduce bias)Hence, can observe one outcome unit (either treated control), can think problem missing data well. Thus, section closely related Imputation (Missing Data)observational studies, randomize treatment effect. Subjects select treatments, introduce selection bias (.e., systematic differences group differences confound effects response variable differences).Matching used toreduce model dependencereduce model dependencediagnose balance datasetdiagnose balance datasetAssumptions matching:treatment assignment independent potential outcomes given covariates\n\\(T \\perp (Y(0),Y(1))|X\\)\nknown ignorability, ignorable, hidden bias, unconfounded.\ntypically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\n\ntreatment assignment independent potential outcomes given covariates\\(T \\perp (Y(0),Y(1))|X\\)\\(T \\perp (Y(0),Y(1))|X\\)known ignorability, ignorable, hidden bias, unconfounded.known ignorability, ignorable, hidden bias, unconfounded.typically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\ntypically satisfy assumption unobserved covariates correlated observed covariates.unobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)positive probability receiving treatment X\n\\(0 < P(T=1|X)<1 \\forall X\\)\npositive probability receiving treatment X\\(0 < P(T=1|X)<1 \\forall X\\)Stable Unit Treatment value Assumption (SUTVA)\nOutcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\n\nStable Unit Treatment value Assumption (SUTVA)Outcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\nOutcomes affected treatment B.hard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.Generalization\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_c\\): control population -> \\(N_c\\): random sample control\\(P_c\\): control population -> \\(N_c\\): random sample control\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(T_j\\) = treatment assignment\\(T_j\\) = treatment assignment\\(Y_j\\) = observed outcome\\(Y_j\\) = observed outcomeAssume: \\(N_t < N_c\\)Assume: \\(N_t < N_c\\)Treatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \n\\(R_1(x) = E(Y(1)|X)\\)\n\\(R_0(x) = E(Y(0)|X)\\)\nTreatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \\(R_1(x) = E(Y(1)|X)\\)\\(R_1(x) = E(Y(1)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)Assume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)\nparallel trends assumed, average effect can estimated.\nAssume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)parallel trends assumed, average effect can estimated.Common estimands:\nAverage effect treatment treated (ATT): effects treatment group\nAverage treatment effect (ATE): effect treatment control\nCommon estimands:Average effect treatment treated (ATT): effects treatment groupAverage effect treatment treated (ATT): effects treatment groupAverage treatment effect (ATE): effect treatment controlAverage treatment effect (ATE): effect treatment controlSteps:Define “closeness”: decide distance measure used\nvariables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\n\ndistance measures: \nDefine “closeness”: decide distance measure usedWhich variables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\nvariables include:Ignorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\nIgnorability (unobserved differences treatment control)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)include variables affected treatment.include variables affected treatment.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.distance measures: belowWhich distance measures: belowMatching methods\nNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\n\nSubclassification, Full Matching Weighting\nNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\n\nAssessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\n\nMatching methodsNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\nNearest neighbor matchingSimple (greedy) matching: performs poorly competition controls.Simple (greedy) matching: performs poorly competition controls.Optimal matching: considers global distance measureOptimal matching: considers global distance measureRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).Ratio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).Subclassification, Full Matching Weighting\nNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\nSubclassification, Full Matching WeightingNearest neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.Subclassification: distribution multiple subclass (e.g., 5-10)Subclassification: distribution multiple subclass (e.g., 5-10)Full matching: optimal ly minimize average distances treated unit control unit within matched set.Full matching: optimal ly minimize average distances treated unit control unit within matched set.Weighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Kernel weighting (e.g., economics) averages multiple units control group.Kernel weighting (e.g., economics) averages multiple units control group.Assessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\nAssessing Common Supportcommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.Assessing quality matched samples (Diagnose)\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\n\nNumerical Diagnostics\nstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\n\nGraphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\n\nAssessing quality matched samples (Diagnose)Balance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.Numerical Diagnostics\nstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\nNumerical Diagnosticsstandardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.standardized difference means covariate (common), also known ”standardized bias”, “standardized difference means”.standardized difference means propensity score (< 0.25) (Rubin 2001)standardized difference means propensity score (< 0.25) (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)covariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.covariate, ratio fo variance residuals orthogonal propensity score treated control groups.Note: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.Graphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\nGraphical DiagnosticsQQ plotsQQ plotsEmpirical Distribution PlotEmpirical Distribution PlotEstimate treatment effect\nk:1\nNeed account weights use matching replacement.\n\nSubclassification Full Matching\nWeighting subclass estimates number treated units subclass ATT\nWeighting overall number individual subclass ATE.\n\nVariance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)\nEstimate treatment effectAfter k:1\nNeed account weights use matching replacement.\nk:1Need account weights use matching replacement.Subclassification Full Matching\nWeighting subclass estimates number treated units subclass ATT\nWeighting overall number individual subclass ATE.\nSubclassification Full MatchingWeighting subclass estimates number treated units subclass ATTWeighting subclass estimates number treated units subclass ATTWeighting overall number individual subclass ATE.Weighting overall number individual subclass ATE.Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Notes:missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)Violation ignorable treatment assignment (.e., unobservables affect treatment outcome). control \nmeasure pre-treatment measure outcome variable\nfind difference outcomes multiple control groups. significant difference, evidence violation.\nfind range correlations unobservables treatment assignment outcome nullify significant effect.\nViolation ignorable treatment assignment (.e., unobservables affect treatment outcome). control bymeasure pre-treatment measure outcome variablemeasure pre-treatment measure outcome variablefind difference outcomes multiple control groups. significant difference, evidence violation.find difference outcomes multiple control groups. significant difference, evidence violation.find range correlations unobservables treatment assignment outcome nullify significant effect.find range correlations unobservables treatment assignment outcome nullify significant effect.Choosing methods\nsmallest standardized difference mean across largest number covariates\nminimize standardized difference means particularly prognostic covariates\nfest number large standardized difference means (> 0.25)\n(Diamond Sekhon 2013) automates process\nChoosing methodssmallest standardized difference mean across largest number covariatessmallest standardized difference mean across largest number covariatesminimize standardized difference means particularly prognostic covariatesminimize standardized difference means particularly prognostic covariatesfest number large standardized difference means (> 0.25)fest number large standardized difference means (> 0.25)(Diamond Sekhon 2013) automates process(Diamond Sekhon 2013) automates processIn practice\nATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT instead\nATT, ask controls across full range treated group\npracticeIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATT, ask controls across full range treated groupIf ATT, ask controls across full range treated groupChoose matching method\nATE, use IPTW full matching\nATT, controls treated (least 3 times), k:1 nearest neighbor without replacement\nATT, controls , use subclassification, full matching, weighting odds\nChoose matching methodIf ATE, use IPTW full matchingIf ATE, use IPTW full matchingIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls , use subclassification, full matching, weighting oddsIf ATT, controls , use subclassification, full matching, weighting oddsDiagnostic\nbalance, use regression matched samples\nimbalance covariates, treat Mahalanobis\nimbalance many covariates, try k:1 matching replacement\nDiagnosticIf balance, use regression matched samplesIf balance, use regression matched samplesIf imbalance covariates, treat MahalanobisIf imbalance covariates, treat MahalanobisIf imbalance many covariates, try k:1 matching replacementIf imbalance many covariates, try k:1 matching replacementWays define distance \\(D_{ij}\\)Exact\\[\nD_{ij} =\n\\begin{cases}\n0, \\text{ } X_i = X_j, \\\\\n\\infty, \\text{ } X_i \\neq X_j\n\\end{cases}\n\\]advanced Coarsened Exact MatchingMahalanobis\\[\nD_{ij} = (X_i - X_j)'\\Sigma^{-1} (X_i - X_j)\n\\]\\(\\Sigma\\) = variance covariance matrix X thecontrol group ATT interestedcontrol group ATT interestedpolled treatment control groups ATE interestedpolled treatment control groups ATE interestedPropensity score:\\[\nD_{ij} = |e_i - e_j|\n\\]\\(e_k\\) = propensity score individual kAn advanced Prognosis score (B. B. Hansen 2008), know (.e., specify) relationship covariates outcome.Linear propensity score\\[\nD_{ij} = |logit(e_i) - logit(e_j)|\n\\]exact Mahalanobis good high dimensional non normally distributed X’s cases.can combine Mahalanobis matching propensity score calipers (Rubin Thomas 2000)advanced methods longitudinal settingsmarginal structural models (Robins, Hernan, Brumback 2000)marginal structural models (Robins, Hernan, Brumback 2000)balanced risk set matching (Y. P. Li, Propert, Rosenbaum 2001)balanced risk set matching (Y. P. Li, Propert, Rosenbaum 2001)matching methods based (ex-post)propensity scorepropensity scoredistance metricdistance metriccovariatescovariatesPackagescem Coarsened exact matchingcem Coarsened exact matchingMatching Multivariate propensity score matching balance optimizationMatching Multivariate propensity score matching balance optimizationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2017)MatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2017)optmatchoptimal matching variable ratio, optimal full matchingoptmatchoptimal matching variable ratio, optimal full matchingPSAgraphics Propensity score graphicsPSAgraphics Propensity score graphicsrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptionrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptiontwang weighting analysis non-equivalent groupstwang weighting analysis non-equivalent groupsCBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.CBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.PanelMatch based Imai, Kim, Wang (2018)PanelMatch based Imai, Kim, Wang (2018)Easier asses whether ’s workingEasier explainallows nice visualization evaluationHowever, problem omitted variables (.e., affect outcome whether observation treated) - unobserved confounders still present matching methods.Difference matching regression following Pischke’s lectureSuppose want estimate effect treatment treated\\[\n\\begin{aligned}\n\\delta_{TOT} &= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\\\\n&= E\\{E[Y_{1i} | X_i, D_i = 1] \\\\\n& - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\\} && \\text{law itereated expectations}\n\\end{aligned}\n\\]conditional independence\\[\nE[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1]\n\\]\\[\n\\begin{aligned}\n\\delta_{TOT} &= E \\{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\\} \\\\\n&= E\\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\\} \\\\\n&= E[\\delta_X |D_i = 1]\n\\end{aligned}\n\\]\\(\\delta_X\\) X-specific difference means covariate value \\(X_i\\)\\(X_i\\) discrete, matching estimand \\[\n\\delta_M = \\sum_x \\delta_x P(X_i = x |D_i = 1)\n\\]\\(P(X_i = x |D_i = 1)\\) probability mass function \\(X_i\\) given \\(D_i = 1\\)According Bayes rule,\\[\nP(X_i = x | D_i = 1) = \\frac{P(D_i = 1 | X_i = x) \\times P(X_i = x)}{P(D_i = 1)}\n\\]hence,\\[\n\\begin{aligned}\n\\delta_M &= \\frac{\\sum_x \\delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}\n\\end{aligned}\n\\]hand, suppose regression\\[\ny_i = \\sum_x d_{ix} \\beta_x + \\delta_R D_i + \\epsilon_i\n\\]\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\delta_R\\) = regression estimand \\(\\delta_R\\) = regression estimand \\[\n\\begin{aligned}\n\\delta_R &= \\frac{\\sum_x \\delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}\n\\end{aligned}\n\\]difference regression matching estimand weights use combine covariate specific treatment effect \\(\\delta_x\\)\\(P(D_i = 1|X_i = x)\\)fraction treated observations covariate cell (.e., mean \\(D_i\\))\\(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\\)variance \\(D_i\\) covariate cellThe goal matching produce covariate balance (.e., distributions covariates treatment control groups approximately similar successful randomized experiment).","code":""},{"path":"matching-methods.html","id":"selection-on-observables","chapter":"31 Matching Methods","heading":"31.1 Selection on Observables","text":"","code":""},{"path":"matching-methods.html","id":"matchit","chapter":"31 Matching Methods","heading":"31.1.1 MatchIt","text":"Procedure typically involves (proposed Noah Freifer using MatchIt)planningmatchingchecking (balance)estimating treatment effectexamine treat re78Planningselect type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select target populationselect target populationselect variables match/balance (Austin 2011) (T. J. VanderWeele 2019)select variables match/balance (Austin 2011) (T. J. VanderWeele 2019)Check Initial ImbalanceMatchingCheck balanceSometimes make trade-balance sample size.Try Full Match (.e., every treated matches one control, every control one treated).Checking balance againExact MatchingSubclassficationOptimal MatchingGenetic MatchingEstimating Treatment Effecttreat coefficient = estimated ATTWhen reporting, remember mentionthe matching specification (method, additional options)distance measure (e.g., propensity score)methods, rationale final chosen method.balance statistics matched dataset.number matched, unmatched, discardedestimation method treatment effect.","code":"\nlibrary(MatchIt)\ndata(\"lalonde\")\n# No matching; constructing a pre-match matchit object\nm.out0 <- matchit(\n    formula(treat ~ age + educ + race \n            + married + nodegree + re74 + re75, env = lalonde),\n    data = data.frame(lalonde),\n    method = NULL,\n    # assess balance before matching\n    distance = \"glm\" # logistic regression\n)\n\n# Checking balance prior to matching\nsummary(m.out0)\n# 1:1 NN PS matching w/o replacement\nm.out1 <- matchit(treat ~ age + educ,\n                  data = lalonde,\n                  method = \"nearest\",\n                  distance = \"glm\")\nm.out1\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# Checking balance after NN matching\nsummary(m.out1, un = FALSE)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ, data = lalonde, method = \"nearest\", \n#>     distance = \"glm\")\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3080        0.3077          0.0094     0.9963    0.0033\n#> age            25.8162       25.8649         -0.0068     1.0300    0.0050\n#> educ           10.3459       10.2865          0.0296     0.5886    0.0253\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0432          0.0146\n#> age        0.0162          0.0597\n#> educ       0.1189          0.8146\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           429     185\n#> Matched       185     185\n#> Unmatched     244       0\n#> Discarded       0       0\n\n# examine visually\nplot(m.out1, type = \"jitter\", interactive = FALSE)\n\nplot(\n    m.out1,\n    type = \"qq\",\n    interactive = FALSE,\n    which.xs = c(\"age\")\n)\n# Full matching on a probit PS\nm.out2 <- matchit(treat ~ age + educ, \n                  data = lalonde,\n                  method = \"full\", \n                  distance = \"glm\", \n                  link = \"probit\")\nm.out2\n#> A matchit object\n#>  - method: Optimal full matching\n#>  - distance: Propensity score\n#>              - estimated with probit regression\n#>  - number of obs.: 614 (original), 614 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# Checking balance after full matching\nsummary(m.out2, un = FALSE)\n#> \n#> Call:\n#> matchit(formula = treat ~ age + educ, data = lalonde, method = \"full\", \n#>     distance = \"glm\", link = \"probit\")\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.3082        0.3081          0.0023     0.9815    0.0028\n#> age            25.8162       25.8035          0.0018     0.9825    0.0062\n#> educ           10.3459       10.2315          0.0569     0.4390    0.0481\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0270          0.0382\n#> age        0.0249          0.1110\n#> educ       0.1300          0.9805\n#> \n#> Sample Sizes:\n#>               Control Treated\n#> All            429.       185\n#> Matched (ESS)  145.23     185\n#> Matched        429.       185\n#> Unmatched        0.         0\n#> Discarded        0.         0\n\nplot(summary(m.out2))\n# Full matching on a probit PS\nm.out3 <-\n    matchit(\n        treat ~ age + educ,\n        data = lalonde,\n        method = \"exact\"\n    )\nm.out3\n#> A matchit object\n#>  - method: Exact matching\n#>  - number of obs.: 614 (original), 332 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out4 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"subclass\"\n)\nm.out4\n#> A matchit object\n#>  - method: Subclassification (6 subclasses)\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 614 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n\n# Or you can use in conjunction with \"nearest\"\nm.out4 <- matchit(\n    treat ~ age + educ,\n    data = lalonde,\n    method = \"nearest\",\n    option = \"subclass\"\n)\nm.out4\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out5 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"optimal\",\n    ratio = 2\n)\nm.out5\n#> A matchit object\n#>  - method: 2:1 optimal pair matching\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 555 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\nm.out6 <- matchit(\n    treat ~ age + educ, \n    data = lalonde,\n    method = \"genetic\"\n)\nm.out6\n#> A matchit object\n#>  - method: 1:1 genetic matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 614 (original), 370 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, educ\n# get matched data\nm.data1 <- match.data(m.out1)\n\nhead(m.data1)\n#>      treat age educ   race married nodegree re74 re75       re78  distance\n#> NSW1     1  37   11  black       1        1    0    0  9930.0460 0.2536942\n#> NSW2     1  22    9 hispan       0        1    0    0  3595.8940 0.3245468\n#> NSW3     1  30   12  black       0        0    0    0 24909.4500 0.2881139\n#> NSW4     1  27   11  black       0        1    0    0  7506.1460 0.3016672\n#> NSW5     1  33    8  black       0        1    0    0   289.7899 0.2683025\n#> NSW6     1  22    9  black       0        1    0    0  4056.4940 0.3245468\n#>      weights subclass\n#> NSW1       1        1\n#> NSW2       1       98\n#> NSW3       1      109\n#> NSW4       1      120\n#> NSW5       1      131\n#> NSW6       1      142\nlibrary(\"lmtest\") #coeftest\nlibrary(\"sandwich\") #vcovCL\n\n# imbalance matched dataset\nfit1 <- lm(re78 ~ treat + age + educ ,\n           data = m.data1, \n           weights = weights)\n\ncoeftest(fit1, vcov. = vcovCL, cluster = ~subclass)\n#> \n#> t test of coefficients:\n#> \n#>              Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  -174.902   2445.013 -0.0715 0.943012   \n#> treat       -1139.085    780.399 -1.4596 0.145253   \n#> age           153.133     55.317  2.7683 0.005922 **\n#> educ          358.577    163.860  2.1883 0.029278 * \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# balance matched dataset \nm.data2 <- match.data(m.out2)\n\nfit2 <- lm(re78 ~ treat + age + educ , \n           data = m.data2, weights = weights)\n\ncoeftest(fit2, vcov. = vcovCL, cluster = ~subclass)\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept) 2151.952   3141.152  0.6851  0.49355  \n#> treat       -725.184    703.297 -1.0311  0.30289  \n#> age          120.260     53.933  2.2298  0.02612 *\n#> educ         175.693    241.694  0.7269  0.46755  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"matching-methods.html","id":"designmatch","chapter":"31 Matching Methods","heading":"31.1.2 designmatch","text":"package includesdistmatch optimal distance matchingdistmatch optimal distance matchingbmatch optimal bipartile matchingbmatch optimal bipartile matchingcardmatch optimal cardinality matchingcardmatch optimal cardinality matchingprofmatch optimal profile matchingprofmatch optimal profile matchingnmatch optimal nonbipartile matchingnmatch optimal nonbipartile matching","code":"\nlibrary(designmatch)"},{"path":"matching-methods.html","id":"matchingfrontier","chapter":"31 Matching Methods","heading":"31.1.3 MatchingFrontier","text":"mentioned MatchIt, make trade-(also known bias-variance trade-) balance sample size. automated procedure optimize trade-implemented MatchingFrontier (G. King, Lucas, Nielsen 2017), solves joint optimization problem.Following MatchingFrontier guide","code":"\n# library(devtools)\n# install_github('ChristopherLucas/MatchingFrontier')\nlibrary(MatchingFrontier)\ndata(\"lalonde\")\n# choose var to match on\nmatch.on <-\n    colnames(lalonde)[!(colnames(lalonde) %in% c('re78', 'treat'))]\nmatch.on\n\n# Mahanlanobis frontier (default)\nmahal.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = \"treat\",\n        match.on = match.on\n    )\nmahal.frontier\n\n# L1 frontier\nL1.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = 'treat',\n        match.on = match.on,\n        QOI = 'SATT',\n        metric = 'L1',\n        ratio = 'fixed'\n    )\nL1.frontier\n\n# estimate effects along the frontier\n\n# Set base form\nmy.form <-\n    as.formula(re78 ~ treat + age + black + education \n               + hispanic + married + nodegree + re74 + re75)\n\n# Estimate effects for the mahalanobis frontier\nmahal.estimates <-\n    estimateEffects(\n        mahal.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Estimate effects for the L1 frontier\nL1.estimates <-\n    estimateEffects(\n        L1.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )\n\n# Plot covariates means \n# plotPrunedMeans()\n\n\n# Plot estimates (deprecated)\n# plotEstimates(\n#     L1.estimates,\n#     ylim = c(-10000, 3000),\n#     cex.lab = 1.4,\n#     cex.axis = 1.4,\n#     panel.first = grid(NULL, NULL, lwd = 2,)\n# )\n\n# Plot estimates\nplotMeans(L1.frontier)\n\n\n# parallel plot\nparallelPlot(\n    L1.frontier,\n    N = 400,\n    variables = c('age', 're74', 're75', 'black'),\n    treated.col = 'blue',\n    control.col = 'gray'\n)\n\n# export matched dataset\n# take 400 units\nmatched.data <- generateDataset(L1.frontier, N = 400) "},{"path":"matching-methods.html","id":"propensity-scores","chapter":"31 Matching Methods","heading":"31.1.4 Propensity Scores","text":"Even though mention propensity scores matching method , longer recommended use method research publication (G. King Nielsen 2019) increasesimbalanceimbalanceinefficiencyinefficiencymodel dependence: small changes model specification lead big changes model resultsmodel dependence: small changes model specification lead big changes model resultsbiasbias(Abadie Imbens 2016)noteThe initial estimation propensity score influences large sample distribution estimators.initial estimation propensity score influences large sample distribution estimators.Adjustments made large sample variances estimators ATE ATT.\nadjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.\nATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.\nAdjustments made large sample variances estimators ATE ATT.adjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.adjustment ATE estimator either negative zero, indicating greater efficiency matching estimated propensity score versus true score large samples.ATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.ATET estimator, sign adjustment depends data generating process. Neglecting estimation error propensity score can lead inaccurate confidence intervals ATT estimator, making either large small.PSM tries accomplish complete randomization methods try achieve fully blocked. Hence, probably better use methods.Propensity “probability receiving treatment given observed covariates.” (Rosenbaum Rubin 1985)Equivalently, can understood probability treated.\\[\ne_i (X_i) = P(T_i = 1 | X_i)\n\\]Estimation usinglogistic regressionlogistic regressionNon parametric methods:\nboosted CART\ngeneralized boosted models (gbm)\nNon parametric methods:boosted CARTboosted CARTgeneralized boosted models (gbm)generalized boosted models (gbm)Steps Gary King’s slidesreduce k elements X scalarreduce k elements X scalar\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)match treated unit nearest control unitmatch treated unit nearest control unitcontrol units: reused; pruned unusedcontrol units: reused; pruned unusedprune matches distances > caliperprune matches distances > caliperIn best case scenario, randomly prune, increases imbalanceOther methods dominate try match exactly hence\\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)\\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)Notes:include/control irrelevant covariates leads PSM random, hence imbalanceDo include/control irrelevant covariates leads PSM random, hence imbalanceDo include (Bhattacharya Vogt 2007) instrumental variable predictor set propensity score matching estimator. generally, using variables control potential confounders, even predictive treatment, can result biased estimatesDo include (Bhattacharya Vogt 2007) instrumental variable predictor set propensity score matching estimator. generally, using variables control potential confounders, even predictive treatment, can result biased estimatesWhat left pruning important start throw .Diagnostics:balance covariatesbalance covariatesno need concern collinearityno need concern collinearitycan’t use c-stat stepwise model fit stat applycan’t use c-stat stepwise model fit stat apply","code":""},{"path":"matching-methods.html","id":"look-ahead-propensity-score-matching","chapter":"31 Matching Methods","heading":"31.1.4.1 Look Ahead Propensity Score Matching","text":"(Bapna, Ramaprasad, Umyarov 2018)","code":""},{"path":"matching-methods.html","id":"mahalanobis-distance","chapter":"31 Matching Methods","heading":"31.1.5 Mahalanobis Distance","text":"Approximates fully blocked experimentDistance \\((X_c,X_t)\\) = \\(\\sqrt{(X_c - X_t)'S^{-1}(X_c - X_t)}\\)\\(S^{-1}\\) standardize distanceIn application use Euclidean distance.Prune unused control units, prune matches distance > caliper","code":""},{"path":"matching-methods.html","id":"coarsened-exact-matching","chapter":"31 Matching Methods","heading":"31.1.6 Coarsened Exact Matching","text":"Steps Gray King’s slides International Methods Colloquium talk 2015Temporarily coarsen \\(X\\)Temporarily coarsen \\(X\\)Apply exact matching coarsened \\(X, C(X)\\)\nsort observation strata, unique values \\(C(X)\\)\nprune stratum 0 treated 0 control units\nApply exact matching coarsened \\(X, C(X)\\)sort observation strata, unique values \\(C(X)\\)sort observation strata, unique values \\(C(X)\\)prune stratum 0 treated 0 control unitsprune stratum 0 treated 0 control unitsPass original (uncoarsened) units except prunedPass original (uncoarsened) units except prunedProperties:Monotonic imbalance bounding (MIB) matching method\nmaximum imbalance treated control chosen ex ante\nMonotonic imbalance bounding (MIB) matching methodmaximum imbalance treated control chosen ex antemeets congruence principlemeets congruence principlerobust measurement errorrobust measurement errorcan implemented multiple imputationcan implemented multiple imputationworks well multi-category treatmentsworks well multi-category treatmentsAssumptions:Ignorability (.e., omitted variable bias)detail (Iacus, King, Porro 2012)Example package’s authorsautomated coarseningcoarsening explicit user choiceCan also use progressive coarsening method control number matches.Can also use progressive coarsening method control number matches.cem can also handle missingness.cem can also handle missingness.","code":"\nlibrary(cem)\ndata(LeLonde)\n\nLe <- data.frame(na.omit(LeLonde)) # remove missing data\n# treated and control groups\ntr <- which(Le$treated==1)\nct <- which(Le$treated==0)\nntr <- length(tr)\nnct <- length(ct)\n\n# unadjusted, biased difference in means\nmean(Le$re78[tr]) - mean(Le$re78[ct])\n#> [1] 759.0479\n\n# pre-treatment covariates\nvars <-\n    c(\n        \"age\",\n        \"education\",\n        \"black\",\n        \"married\",\n        \"nodegree\",\n        \"re74\",\n        \"re75\",\n        \"hispanic\",\n        \"u74\",\n        \"u75\",\n        \"q1\"\n    )\n\n# overall imbalance statistics\nimbalance(group=Le$treated, data=Le[vars]) # L1 = 0.902\n#> \n#> Multivariate Imbalance Measure: L1=0.902\n#> Percentage of local common support: LCS=5.8%\n#> \n#> Univariate Imbalance Measures:\n#> \n#>               statistic   type           L1 min 25%      50%       75%\n#> age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n#> education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n#> black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n#> married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n#> nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n#> re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n#> re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n#> hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n#> u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n#> u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n#> q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n#>                  max\n#> age          -6.0000\n#> education     1.0000\n#> black         0.0000\n#> married       0.0000\n#> nodegree      0.0000\n#> re74      -2139.0195\n#> re75        490.3945\n#> hispanic      0.0000\n#> u74           0.0000\n#> u75           0.0000\n#> q1                NA\n\n# drop other variables that are not pre - treatmentt matching variables\ntodrop <- c(\"treated\", \"re78\")\nimbalance(group=Le$treated, data=Le, drop=todrop)\n#> \n#> Multivariate Imbalance Measure: L1=0.902\n#> Percentage of local common support: LCS=5.8%\n#> \n#> Univariate Imbalance Measures:\n#> \n#>               statistic   type           L1 min 25%      50%       75%\n#> age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n#> education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n#> black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n#> married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n#> nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n#> re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n#> re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n#> hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n#> u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n#> u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n#> q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n#>                  max\n#> age          -6.0000\n#> education     1.0000\n#> black         0.0000\n#> married       0.0000\n#> nodegree      0.0000\n#> re74      -2139.0195\n#> re75        490.3945\n#> hispanic      0.0000\n#> u74           0.0000\n#> u75           0.0000\n#> q1                NA\nmat <-\n    cem(\n        treatment = \"treated\",\n        data = Le,\n        drop = \"re78\",\n        keep.all = TRUE\n    )\n#> \n#> Using 'treated'='1' as baseline group\nmat\n#>            G0  G1\n#> All       392 258\n#> Matched    95  84\n#> Unmatched 297 174\n\n# mat$w\n# categorial variables\nlevels(Le$q1) # grouping option\n#> [1] \"agree\"             \"disagree\"          \"neutral\"          \n#> [4] \"no opinion\"        \"strongly agree\"    \"strongly disagree\"\nq1.grp <-\n    list(\n        c(\"strongly agree\", \"agree\"),\n        c(\"neutral\", \"no opinion\"),\n        c(\"strongly disagree\", \"disagree\")\n    ) # if you want ordered categories\n\n# continuous variables\ntable(Le$education)\n#> \n#>   3   4   5   6   7   8   9  10  11  12  13  14  15 \n#>   1   5   4   6  12  55 106 146 173 113  19   9   1\neducut <- c(0, 6.5, 8.5, 12.5, 17)  # use cutpoints\n\nmat1 <-\n    cem(\n        treatment = \"treated\",\n        data = Le,\n        drop = \"re78\",\n        cutpoints = list(education = educut),\n        grouping = list(q1 = q1.grp)\n    )\n#> \n#> Using 'treated'='1' as baseline group\nmat1\n#>            G0  G1\n#> All       392 258\n#> Matched   158 115\n#> Unmatched 234 143"},{"path":"matching-methods.html","id":"genetic-matching","chapter":"31 Matching Methods","heading":"31.1.7 Genetic Matching","text":"GM uses iterative checking process propensity scores, combines propensity scores Mahalanobis distance.\nGenMatch (Diamond Sekhon 2013)\nGM uses iterative checking process propensity scores, combines propensity scores Mahalanobis distance.GenMatch (Diamond Sekhon 2013)GM arguably “superior” method nearest neighbor full matching imbalanced dataGM arguably “superior” method nearest neighbor full matching imbalanced dataUse genetic search algorithm find weights covariate optimal balance.Use genetic search algorithm find weights covariate optimal balance.Implementation\nuse replacement\nbalance can based \npaired \\(t\\)-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\n\nImplementationcould use replacementcould use replacementbalance can based \npaired \\(t\\)-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\nbalance can based onpaired \\(t\\)-tests (dichotomous variables)paired \\(t\\)-tests (dichotomous variables)Kolmogorov-Smirnov (multinomial continuous)Kolmogorov-Smirnov (multinomial continuous)PackagesMatching","code":"\nlibrary(Matching)\ndata(lalonde)\nattach(lalonde)\n\n#The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n\n#The covariates we want to obtain balance on\nBalanceMat <-\n    cbind(age,\n          educ,\n          black,\n          hisp,\n          married,\n          nodegr,\n          u74,\n          u75,\n          re75,\n          re74,\n          I(re74 * re75))\n\n#\n#Let's call GenMatch() to find the optimal weight to give each\n#covariate in 'X' so as we have achieved balance on the covariates in\n#'BalanceMat'. This is only an example so we want GenMatch to be quick\n#so the population size has been set to be only 16 via the 'pop.size'\n#option. This is *WAY* too small for actual problems.\n#For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf.\n#\ngenout <-\n    GenMatch(\n        Tr = treat,\n        X = X,\n        BalanceMatrix = BalanceMat,\n        estimand = \"ATE\",\n        M = 1,\n        pop.size = 16,\n        max.generations = 10,\n        wait.generations = 1\n    )\n\n#The outcome variable\nY=re78/1000\n\n#\n# Now that GenMatch() has found the optimal weights, let's estimate\n# our causal effect of interest using those weights\n#\nmout <-\n    Match(\n        Y = Y,\n        Tr = treat,\n        X = X,\n        estimand = \"ATE\",\n        Weight.matrix = genout\n    )\nsummary(mout)\n\n#                        \n#Let's determine if balance has actually been obtained on the variables of interest\n#                        \nmb <-\n    MatchBalance(\n        treat ~ age + educ + black + hisp + married + nodegr \n        + u74 + u75 + re75 + re74 + I(re74 * re75),\n        match.out = mout,\n        nboots = 500\n    )"},{"path":"matching-methods.html","id":"entropy-balancing","chapter":"31 Matching Methods","heading":"31.1.8 Entropy Balancing","text":"(Hainmueller 2012)Entropy balancing method achieving covariate balance observational studies binary treatments.Entropy balancing method achieving covariate balance observational studies binary treatments.uses maximum entropy reweighting scheme ensure treatment control groups balanced based sample moments.uses maximum entropy reweighting scheme ensure treatment control groups balanced based sample moments.method adjusts inequalities covariate distributions, reducing dependence model used estimating treatment effects.method adjusts inequalities covariate distributions, reducing dependence model used estimating treatment effects.Entropy balancing improves balance across included covariate moments removes need repetitive balance checking iterative model searching.Entropy balancing improves balance across included covariate moments removes need repetitive balance checking iterative model searching.","code":""},{"path":"matching-methods.html","id":"matching-for-high-dimensional-data","chapter":"31 Matching Methods","heading":"31.1.9 Matching for high-dimensional data","text":"One reduce number dimensions using methods :Lasso (Gordon et al. 2019)Lasso (Gordon et al. 2019)Penalized logistic regression (Eckles Bakshy 2021)Penalized logistic regression (Eckles Bakshy 2021)PCA (Principal Component Analysis)PCA (Principal Component Analysis)Locality Preserving Projections (LPP) (S. Li et al. 2016)Locality Preserving Projections (LPP) (S. Li et al. 2016)Random projectionRandom projectionAutoencoders (Ramachandra 2018)Autoencoders (Ramachandra 2018)Additionally, one jointly dimension reduction balancing distributions control treated groups (Yao et al. 2018).","code":""},{"path":"matching-methods.html","id":"matching-for-time-series-cross-section-data","chapter":"31 Matching Methods","heading":"31.1.10 Matching for time series-cross-section data","text":"Examples: (Scheve Stasavage 2012) (Acemoglu et al. 2019)Identification strategy:Within-unit -time variationWithin-unit -time variationwithin-time across-units variationwithin-time across-units variationSee treatment condition details method","code":""},{"path":"matching-methods.html","id":"matching-for-multiple-treatments","chapter":"31 Matching Methods","heading":"31.1.11 Matching for multiple treatments","text":"cases multiple treatment groups, want matching, ’s important baseline (control) group. details, see(McCaffrey et al. 2013)(McCaffrey et al. 2013)(Lopez Gutman 2017)(Lopez Gutman 2017)(Zhao et al. 2021): also continuous treatment(Zhao et al. 2021): also continuous treatmentIf insist using MatchIt package, see answer","code":""},{"path":"matching-methods.html","id":"matching-for-multi-level-treatments","chapter":"31 Matching Methods","heading":"31.1.12 Matching for multi-level treatments","text":"See (Yang et al. 2016)Package R shuyang1987/multilevelMatching Github","code":""},{"path":"matching-methods.html","id":"matching-for-repeated-treatments","chapter":"31 Matching Methods","heading":"31.1.13 Matching for repeated treatments","text":"https://cran.r-project.org/web/packages/twang/vignettes/iptw.pdfpackage R twang","code":""},{"path":"matching-methods.html","id":"selection-on-unobservables","chapter":"31 Matching Methods","heading":"31.2 Selection on Unobservables","text":"several ways one can deal selection unobservables:Rosenbaum BoundsRosenbaum BoundsEndogenous Sample Selection (.e., Heckman-style correction): examine \\(\\lambda\\) term see whether ’s significant (sign endogenous selection)Endogenous Sample Selection (.e., Heckman-style correction): examine \\(\\lambda\\) term see whether ’s significant (sign endogenous selection)Relative Correlation RestrictionsRelative Correlation RestrictionsCoefficient-stability BoundsCoefficient-stability Bounds","code":""},{"path":"matching-methods.html","id":"rosenbaum-bounds","chapter":"31 Matching Methods","heading":"31.2.1 Rosenbaum Bounds","text":"Examples marketing(Oestreicher-Singer Zalmanson 2013): range 1.5 1.8 important effect level community participation users willingness pay premium services.(Oestreicher-Singer Zalmanson 2013): range 1.5 1.8 important effect level community participation users willingness pay premium services.(M. Sun Zhu 2013): factor 1.5 essential understanding relationship launch ad revenue-sharing program popularity content.(M. Sun Zhu 2013): factor 1.5 essential understanding relationship launch ad revenue-sharing program popularity content.(Manchanda, Packard, Pattabhiramaiah 2015): factor 1.6 required social dollar effect nullified.(Manchanda, Packard, Pattabhiramaiah 2015): factor 1.6 required social dollar effect nullified.(Sudhir Talukdar 2015): factor 1.9 needed adoption impact labor productivity, 2.2 adoption affect floor productivity.(Sudhir Talukdar 2015): factor 1.9 needed adoption impact labor productivity, 2.2 adoption affect floor productivity.(Proserpio Zervas 2017b): factor 2 necessary firm’s use management responses influence online reputation.(Proserpio Zervas 2017b): factor 2 necessary firm’s use management responses influence online reputation.(S. Zhang et al. 2022): factor 1.55 critical acquisition verified images drive demand Airbnb properties.(S. Zhang et al. 2022): factor 1.55 critical acquisition verified images drive demand Airbnb properties.(Chae, Ha, Schweidel 2023): factor 27 (typo) significant paywall suspensions affect subsequent subscription decisions.(Chae, Ha, Schweidel 2023): factor 27 (typo) significant paywall suspensions affect subsequent subscription decisions.GeneralMatching Methods favored estimating treatment effects observational data, offering advantages regression methods \nreduces reliance functional form assumptions.\nAssumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.\nMatching Methods favored estimating treatment effects observational data, offering advantages regression methods becauseIt reduces reliance functional form assumptions.reduces reliance functional form assumptions.Assumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.Assumes selection-influencing covariates observable; estimates unbiased unobserved confounders missed.Concerns arise potentially relevant covariates unmeasured.\nRosenbaum Bounds assess overall sensitivity coefficient estimates hidden bias (Rosenbaum Rosenbaum 2002) without knowledge (e.g., direction) bias. unboservables cause hidden bias affect selection treatment factor \\(\\Gamma\\) predictive outcome, method also known worst case analyses (DiPrete Gangl 2004).\nConcerns arise potentially relevant covariates unmeasured.Rosenbaum Bounds assess overall sensitivity coefficient estimates hidden bias (Rosenbaum Rosenbaum 2002) without knowledge (e.g., direction) bias. unboservables cause hidden bias affect selection treatment factor \\(\\Gamma\\) predictive outcome, method also known worst case analyses (DiPrete Gangl 2004).Can’t provide precise bounds estimates treatment effects (see Relative Correlation Restrictions)Can’t provide precise bounds estimates treatment effects (see Relative Correlation Restrictions)Typically, show p-value H-L point estimate level gamma \\(\\Gamma\\)Typically, show p-value H-L point estimate level gamma \\(\\Gamma\\)random treatment assignment, can use non-parametric test (Wilcoxon signed rank test) see treatment effect.Without random treatment assignment (.e., observational data), use test. Selection Observables, can use test believe unmeasured confounders. Rosenbaum (2002) can come talk believability notion.layman’s terms, consider treatment assignment based method odds treatment unit control differ multiplier \\(\\Gamma\\)example, \\(\\Gamma = 1\\) means odds assignment identical, indicating random treatment assignment.Another example, \\(\\Gamma = 2\\), matched pair, one unit twice likely receive treatment (due unobservables).Since can’t know \\(\\Gamma\\) certainty, run sensitivity analysis see results change different values \\(\\Gamma\\)bias product unobservable influences treatment selection outcome factor \\(\\Gamma\\) (omitted variable bias)technical terms,Treatment Assignment Probability:\nConsider unit \\(j\\) probability \\(\\pi_j\\) receiving treatment, unit \\(\\) \\(\\pi_i\\).\nIdeally, matching, ’s hidden bias, ’d \\(\\pi_i = \\pi_j\\).\nHowever, observing \\(\\pi_i \\neq \\pi_j\\) raises questions potential biases affecting inference. evaluated using odds ratio.\nConsider unit \\(j\\) probability \\(\\pi_j\\) receiving treatment, unit \\(\\) \\(\\pi_i\\).Ideally, matching, ’s hidden bias, ’d \\(\\pi_i = \\pi_j\\).However, observing \\(\\pi_i \\neq \\pi_j\\) raises questions potential biases affecting inference. evaluated using odds ratio.Odds Ratio Hidden Bias:\nodds treatment unit \\(j\\) defined \\(\\frac{\\pi_j}{1 - \\pi_j}\\).\nodds ratio two matched units \\(\\) \\(j\\) constrained \\(\\frac{1}{\\Gamma} \\le \\frac{\\pi_i / (1- \\pi_i)}{\\pi_j/ (1- \\pi_j)} \\le \\Gamma\\).\n\\(\\Gamma = 1\\), implies absence hidden bias.\n\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.\n\nodds treatment unit \\(j\\) defined \\(\\frac{\\pi_j}{1 - \\pi_j}\\).odds ratio two matched units \\(\\) \\(j\\) constrained \\(\\frac{1}{\\Gamma} \\le \\frac{\\pi_i / (1- \\pi_i)}{\\pi_j/ (1- \\pi_j)} \\le \\Gamma\\).\n\\(\\Gamma = 1\\), implies absence hidden bias.\n\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.\n\\(\\Gamma = 1\\), implies absence hidden bias.\\(\\Gamma = 2\\), odds receiving treatment differ factor 2 two units.Sensitivity Analysis Using Gamma:\nvalue \\(\\Gamma\\) helps measure potential departure bias-free study.\nSensitivity analysis involves varying \\(\\Gamma\\) examine inferences might change presence hidden biases.\nvalue \\(\\Gamma\\) helps measure potential departure bias-free study.Sensitivity analysis involves varying \\(\\Gamma\\) examine inferences might change presence hidden biases.Incorporating Unobserved Covariates:\nConsider scenario unit \\(\\) observed covariates \\(x_i\\) unobserved covariate \\(u_i\\), affect outcome.\nlogistic regression model link odds assignment covariates: \\(\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\kappa x_i + \\gamma u_i\\), \\(\\gamma\\) represents impact unobserved covariate.\nConsider scenario unit \\(\\) observed covariates \\(x_i\\) unobserved covariate \\(u_i\\), affect outcome.logistic regression model link odds assignment covariates: \\(\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\kappa x_i + \\gamma u_i\\), \\(\\gamma\\) represents impact unobserved covariate.Steps Sensitivity Analysis (create table different levels \\(\\Gamma\\) assess magnitude biases can affect evidence treatment effect (estimate):\nSelect range values \\(\\Gamma\\) (e.g., \\(1 \\2\\)).\nAssess p-value magnitude treatment effect (Hodges Jr Lehmann 2011) (details, see (Hollander, Wolfe, Chicken 2013)) changes varying \\(\\Gamma\\) values.\nEmploy specific randomization tests based type outcome establish bounds inferences.\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)\n\nSelect range values \\(\\Gamma\\) (e.g., \\(1 \\2\\)).Assess p-value magnitude treatment effect (Hodges Jr Lehmann 2011) (details, see (Hollander, Wolfe, Chicken 2013)) changes varying \\(\\Gamma\\) values.Employ specific randomization tests based type outcome establish bounds inferences.\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)\nreport minimum value \\(\\Gamma\\) treatment treat nullified (.e., become insignificant). literature’s rules thumb \\(\\Gamma > 2\\), strong evidence treatment effect robust large biases (Proserpio Zervas 2017a)Notes:treatment assignment clustered (e.g., within school, within state) need adjust bounds clustered treatment assignment (B. B. Hansen, Rosenbaum, Small 2014) (similar clustered standard errors).Packagesrbounds (Keele 2010)rbounds (Keele 2010)sensitivitymv (Rosenbaum 2015)sensitivitymv (Rosenbaum 2015)Since typically assess estimate sensitivity unboservables matching, first matching.multiple control group matchingsensitivitymw faster sensitivitymw. sensitivitymw can match matched sets can differing numbers controls (Rosenbaum 2015).","code":"\nlibrary(MatchIt)\nlibrary(Matching)\ndata(\"lalonde\")\n\nmatched <- MatchIt::matchit(\n    treat ~ age + educ,\n    data = lalonde,\n    method = \"nearest\"\n)\nsummary(matched)\n#> \n#> Call:\n#> MatchIt::matchit(formula = treat ~ age + educ, data = lalonde, \n#>     method = \"nearest\")\n#> \n#> Summary of Balance for All Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.4203        0.4125          0.1689     1.2900    0.0431\n#> age            25.8162       25.0538          0.1066     1.0278    0.0254\n#> educ           10.3459       10.0885          0.1281     1.5513    0.0287\n#>          eCDF Max\n#> distance   0.1251\n#> age        0.0652\n#> educ       0.1265\n#> \n#> Summary of Balance for Matched Data:\n#>          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#> distance        0.4203        0.4179          0.0520     1.1691    0.0105\n#> age            25.8162       25.5081          0.0431     1.1518    0.0148\n#> educ           10.3459       10.2811          0.0323     1.5138    0.0224\n#>          eCDF Max Std. Pair Dist.\n#> distance   0.0595          0.0598\n#> age        0.0486          0.5628\n#> educ       0.0757          0.3602\n#> \n#> Sample Sizes:\n#>           Control Treated\n#> All           260     185\n#> Matched       185     185\n#> Unmatched      75       0\n#> Discarded       0       0\nmatched_data <- match.data(matched)\n\ntreatment_group <- subset(matched_data, treat == 1)\ncontrol_group <- subset(matched_data, treat == 0)\n\n\nlibrary(rbounds)\n\n# p-value sensitivity \npsens_res <-\n    psens(treatment_group$re78,\n          control_group$re78,\n          Gamma = 2,\n          GammaInc = .1)\n\npsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n#>  \n#> Unconfounded estimate ....  0.0058 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0      0.0058      0.0058\n#>    1.1      0.0011      0.0235\n#>    1.2      0.0002      0.0668\n#>    1.3      0.0000      0.1458\n#>    1.4      0.0000      0.2599\n#>    1.5      0.0000      0.3967\n#>    1.6      0.0000      0.5378\n#>    1.7      0.0000      0.6664\n#>    1.8      0.0000      0.7723\n#>    1.9      0.0000      0.8523\n#>    2.0      0.0000      0.9085\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \n\n# Hodges-Lehmann point estimate sensitivity\n# median difference between treatment and control\nhlsens_res <-\n    hlsens(treatment_group$re78,\n           control_group$re78,\n           Gamma = 2,\n           GammaInc = .1)\nhlsens_res\n#> \n#>  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n#>  \n#> Unconfounded estimate ....  1745.843 \n#> \n#>  Gamma Lower bound Upper bound\n#>    1.0 1745.800000      1745.8\n#>    1.1 1139.100000      1865.6\n#>    1.2  830.840000      2160.9\n#>    1.3  533.740000      2462.4\n#>    1.4  259.940000      2793.8\n#>    1.5   -0.056912      3059.3\n#>    1.6 -144.960000      3297.8\n#>    1.7 -380.560000      3535.7\n#>    1.8 -554.360000      3751.0\n#>    1.9 -716.360000      4012.1\n#>    2.0 -918.760000      4224.3\n#> \n#>  Note: Gamma is Odds of Differential Assignment To\n#>  Treatment Due to Unobserved Factors \n#> \nlibrary(Matching)\nlibrary(MatchIt)\n\nn_ratio <- 2\nmatched <- MatchIt::matchit(treat ~ age + educ ,\n                   method = \"nearest\", ratio = n_ratio)\nsummary(matched)\nmatched_data <- match.data(matched)\n\nmcontrol_res <- rbounds::mcontrol(\n    y          = matched_data$re78,\n    grp.id     = matched_data$subclass,\n    treat.id   = matched_data$treat,\n    group.size = n_ratio + 1,\n    Gamma      = 2.5,\n    GammaInc   = .1\n)\n\nmcontrol_res\nlibrary(sensitivitymv)\ndata(lead150)\nhead(lead150)\n#>      [,1] [,2] [,3] [,4] [,5] [,6]\n#> [1,] 1.40 1.23 2.24 0.96 1.90 1.14\n#> [2,] 0.63 0.99 0.87 1.90 0.67 1.40\n#> [3,] 1.98 0.82 0.66 0.58 1.00 1.30\n#> [4,] 1.45 0.53 1.43 1.70 0.85 1.50\n#> [5,] 1.60 1.70 0.63 1.05 1.08 0.92\n#> [6,] 1.13 0.31 0.71 1.10 0.86 1.14\nsenmv(lead150,gamma=2,trim=2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524\n\nlibrary(sensitivitymw)\nsenmw(lead150,gamma=2,trim=2)\n#> $pval\n#> [1] 0.02665519\n#> \n#> $deviate\n#> [1] 1.932398\n#> \n#> $statistic\n#> [1] 27.97564\n#> \n#> $expectation\n#> [1] 18.0064\n#> \n#> $variance\n#> [1] 26.61524"},{"path":"matching-methods.html","id":"relative-correlation-restrictions","chapter":"31 Matching Methods","heading":"31.2.2 Relative Correlation Restrictions","text":"Examples marketing(Manchanda, Packard, Pattabhiramaiah 2015): 3.23 social dollar effect nullified(Manchanda, Packard, Pattabhiramaiah 2015): 3.23 social dollar effect nullified(Chae, Ha, Schweidel 2023): 6.69 (.e., much stronger selection unobservables compared selection observables negate result) paywall suspensions affect subsequent subscription decisions(Chae, Ha, Schweidel 2023): 6.69 (.e., much stronger selection unobservables compared selection observables negate result) paywall suspensions affect subsequent subscription decisions(M. Sun Zhu 2013)(M. Sun Zhu 2013)GeneralProposed Altonji, Elder, Taber (2005)Proposed Altonji, Elder, Taber (2005)Generalized Krauth (2016)Generalized Krauth (2016)Estimate bounds treatment effects due unobserved selection.Estimate bounds treatment effects due unobserved selection.\\[\nY_i = X_i \\beta  + C_i \\gamma + \\epsilon_i\n\\]\\(\\beta\\) effect interest\\(\\beta\\) effect interest\\(C_i\\) control variable\\(C_i\\) control variableUsing OLS, \\(cor(X_i, \\epsilon_i) = 0\\)Using OLS, \\(cor(X_i, \\epsilon_i) = 0\\)RCR analysis, assume\\[\ncor(X_i, \\epsilon_i) = \\lambda cor(X_i, C_i \\gamma)\n\\]\\(\\lambda \\(\\lambda_l, \\lambda_h)\\)Choice \\(\\lambda\\)Strong assumption omitted variable bias (smallStrong assumption omitted variable bias (smallIf \\(\\lambda = 0\\), \\(cor(X_i, \\epsilon_i) = 0\\)\\(\\lambda = 0\\), \\(cor(X_i, \\epsilon_i) = 0\\)\\(\\lambda = 1\\), \\(cor(X_i, \\epsilon_i) = cor(X_i, C_i \\gamma)\\)\\(\\lambda = 1\\), \\(cor(X_i, \\epsilon_i) = cor(X_i, C_i \\gamma)\\)typically examine \\(\\lambda \\(0, 1)\\)typically examine \\(\\lambda \\(0, 1)\\)","code":"\n# remotes::install_github(\"bvkrauth/rcr/r/rcrbounds\")\nlibrary(rcrbounds)\n# rcrbounds::install_rcrpy()\ndata(\"ChickWeight\")\n\nrcr_res <-\n    rcrbounds::rcr(weight ~ Time |\n                       Diet, ChickWeight, rc_range = c(0, 10))\nrcr_res\n#> \n#> Call:\n#> rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, \n#>     rc_range = c(0, 10))\n#> \n#> Coefficients:\n#>     rcInf effectInf       rc0   effectL   effectH \n#> 34.676505 71.989336 34.741955  7.447713  8.750492\nsummary(rcr_res)\n#> \n#> Call:\n#> rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, \n#>     rc_range = c(0, 10))\n#> \n#> Coefficients:\n#>            Estimate  Std. Error    t value      Pr(>|t|)\n#> rcInf     34.676505  50.1295005  0.6917385  4.891016e-01\n#> effectInf 71.989336 112.5711682  0.6395007  5.224973e-01\n#> rc0       34.741955  58.7169195  0.5916856  5.540611e-01\n#> effectL    7.447713   2.4276246  3.0679014  2.155677e-03\n#> effectH    8.750492   0.2607671 33.5567355 7.180405e-247\n#> ---\n#> conservative confidence interval:\n#>          2.5  %  97.5  %\n#> effect 2.689656 9.261586\n\n# hypothesis test for the coefficient\nrcrbounds::effect_test(rcr_res, h0 = 0)\n#> [1] 0.001234233\nplot(rcr_res)"},{"path":"matching-methods.html","id":"coefficient-stability-bounds","chapter":"31 Matching Methods","heading":"31.2.3 Coefficient-stability Bounds","text":"Developed Oster (2019)Assess robustness omitted variable bias observing:\nChanges coefficient interest\nShifts model \\(R^2\\)\nChanges coefficient interestChanges coefficient interestShifts model \\(R^2\\)Shifts model \\(R^2\\)Refer Masten Poirier (2022) reverse sign problem.","code":""},{"path":"interrupted-time-series.html","id":"interrupted-time-series","chapter":"32 Interrupted Time Series","heading":"32 Interrupted Time Series","text":"Regression Discontinuity TimeRegression Discontinuity TimeControl \nSeasonable trends\nConcurrent events\nControl forSeasonable trendsSeasonable trendsConcurrent eventsConcurrent eventsPros (Penfold Zhang 2013)\ncontrol long-term trends\nPros (Penfold Zhang 2013)control long-term trendsCons\nMin 8 data points 8 intervention\nMultiple events hard distinguish\nConsMin 8 data points 8 interventionMin 8 data points 8 interventionMultiple events hard distinguishMultiple events hard distinguishNotes:subgroup analysis (heterogeneity effect size), see (Harper Bruckner 2017)interpret control variables, see (Bottomley, Scott, Isham 2019)Interrupted time series used whenlongitudinal data (outcome time - observations intervention)full population affected one specific point time (can stacked based intervention)framework, can 4 possible scenarios outcome interventionNo effectsNo effectsImmediate effectImmediate effectSustained (long-term) effect (smooth)Sustained (long-term) effect (smooth)immediate sustained effectBoth immediate sustained effect\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 P + \\epsilon\n\\]\\(Y\\) outcome variable\n\\(\\beta_0\\) baseline level outcome\n\\(Y\\) outcome variable\\(\\beta_0\\) baseline level outcome\\(T\\) time variable (e.g., days, weeks, etc.) passed start observation period\n\\(\\beta_1\\) slope line intervention\n\\(T\\) time variable (e.g., days, weeks, etc.) passed start observation period\\(\\beta_1\\) slope line intervention\\(D\\) treatment variable \\(1\\) intervention \\(0\\) intervention.\n\\(\\beta_2\\) immediate effect intervention\n\\(D\\) treatment variable \\(1\\) intervention \\(0\\) intervention.\\(\\beta_2\\) immediate effect intervention\\(P\\) time variable indicating time passed since intervention (intervention, value set 0) (examine sustained effect).\n\\(\\beta_3\\) sustained effect = difference slope line prior intervention slope line subsequent intervention\n\\(P\\) time variable indicating time passed since intervention (intervention, value set 0) (examine sustained effect).\\(\\beta_3\\) sustained effect = difference slope line prior intervention slope line subsequent interventionExampleCreate fictitious dataset know true data generating process\\[\nOutcome = 10 \\times time + 20 \\times treatment + 25 \\times timesincetreatment + noise\n\\]VisualizeInterpretationTime coefficient shows -intervention outcome trend. Positive significant, indicating rising trend. Every day adds 15 points.Time coefficient shows -intervention outcome trend. Positive significant, indicating rising trend. Every day adds 15 points.treatment coefficient shows immediate increase outcome. Immediate effect positive significant, increasing outcome 20 points.treatment coefficient shows immediate increase outcome. Immediate effect positive significant, increasing outcome 20 points.time since treatment coefficient reflects change trend subsequent intervention. sustained effect positive statistically significant, showing outcome increases 25 points per day intervention.time since treatment coefficient reflects change trend subsequent intervention. sustained effect positive statistically significant, showing outcome increases 25 points per day intervention.See Lee Rodgers, Beasley, Schuelke (2014) suggestionsPlot counterfactualPossible threats validity interrupted time series analysis (Baicker Svoronos 2019)Delayed effects (Rodgers, John, Coleman 2005) (may make assess time intervention - assess immediate dates).Delayed effects (Rodgers, John, Coleman 2005) (may make assess time intervention - assess immediate dates).confounding events Linden (2017)confounding events Linden (2017)Intervention introduced later withdrawn (Linden 2015)Intervention introduced later withdrawn (Linden 2015)Autocorrelation (every time series data): might cause underestimation standard errors (.e., overestimating statistical significance treatment effect)Autocorrelation (every time series data): might cause underestimation standard errors (.e., overestimating statistical significance treatment effect)Regression mean: short-term shock outcome, individuals can revert back initial states.Regression mean: short-term shock outcome, individuals can revert back initial states.Selection bias: certain individuals affected treatment (use Multiple Groups).Selection bias: certain individuals affected treatment (use Multiple Groups).","code":"\n# number of days\nn = 365\n\n\n# intervention at day\ninterven = 200\n\n# time index from 1 to 365\ntime = c(1:n)\n\n# treatment variable: before internvation = day 1 to 200, \n# after intervention = day 201 to 365\ntreatment = c(rep(0, interven), rep(1, n - interven))\n\n# time since treatment\ntimesincetreat = c(rep(0, interven), c(1:(n - interven)))\n\n# outcome\noutcome = 10 + 15 * time + 20 * treatment + \n    25 * timesincetreat + rnorm(n, mean = 0, sd = 1)\n\ndf = data.frame(outcome, time, treatment, timesincetreat)\n\nhead(df, 10)\n#>      outcome time treatment timesincetreat\n#> 1   25.79832    1         0              0\n#> 2   42.08680    2         0              0\n#> 3   55.55952    3         0              0\n#> 4   68.54228    4         0              0\n#> 5   82.75827    5         0              0\n#> 6  100.82867    6         0              0\n#> 7  114.41550    7         0              0\n#> 8  131.06942    8         0              0\n#> 9  145.22532    9         0              0\n#> 10 161.08298   10         0              0\nplot(df$time, df$outcome)\n\n# intervention date\nabline(v = interven, col = \"blue\")\n\n# regression line\nts <- lm(outcome ~ time + treatment + timesincetreat, data = df)\nlines(df$time, ts$fitted.values, col = \"red\")\nsummary(ts)\n#> \n#> Call:\n#> lm(formula = outcome ~ time + treatment + timesincetreat, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.58812 -0.67771  0.03995  0.63623  2.82507 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)     9.705206   0.135820    71.46   <2e-16 ***\n#> time           15.002674   0.001172 12802.61   <2e-16 ***\n#> treatment      19.852727   0.201416    98.57   <2e-16 ***\n#> timesincetreat 24.996424   0.001954 12791.27   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9568 on 361 degrees of freedom\n#> Multiple R-squared:      1,  Adjusted R-squared:      1 \n#> F-statistic: 1.042e+09 on 3 and 361 DF,  p-value: < 2.2e-16\n# treatment prediction\npred <- predict(ts, df)\n\n# counterfactual dataset\nnew_df <-\n    as.data.frame(cbind(\n        time = time,\n        # treatment = 0 means counterfactual\n        treatment = rep(0, n),\n        # time since treatment = 0 means counterfactual\n        timesincetreat = rep(0)\n    ))\n\n# counterfactual predictions\npred_cf <- predict(ts, new_df)\n\n# plot\nplot(\n    outcome,\n    col = gray(0.2, 0.2),\n    pch = 19,\n    xlim  = c(1,365),\n    ylim = c(0, 10000),\n    xlab = \"xlab\",\n    ylab = \"ylab\"\n)\n\n# regression line before treatment\nlines(rep(1:interven), pred[1:interven], col = \"blue\", lwd = 3)\n\n# regression line after treatment\nlines(rep((interven + 1):n), pred[(interven + 1):n], \n      col = \"blue\", lwd = 3)\n\n# regression line after treatment (counterfactual)\nlines(\n    rep(interven:n),\n    pred_cf[(interven):n],\n    col = \"yellow\",\n    lwd = 3,\n    lty = 5\n)\n\nabline(v = interven, col = \"red\", lty = 2)"},{"path":"interrupted-time-series.html","id":"autocorrelation","chapter":"32 Interrupted Time Series","heading":"32.1 Autocorrelation","text":"Assess autocorrelation residualThis best example since created dataset. residuals autocorrelation, see patterns (.e., points randomly distributed plot)formally test autocorrelation, can use Durbin-Watson testFrom p-value, know autocorrelation time seriesA solution problem use advanced time series analysis (e.g., ARIMA - coming book) adjust seasonality dependency.","code":"\n# simple regression on time \nsimple_ts <- lm(outcome ~ time, data = df)\n\nplot(resid(simple_ts))\n\n# alternatively\nacf(resid(simple_ts))\nlmtest::dwtest(df$outcome ~ df$time)\n#> \n#>  Durbin-Watson test\n#> \n#> data:  df$outcome ~ df$time\n#> DW = 0.00037607, p-value < 2.2e-16\n#> alternative hypothesis: true autocorrelation is greater than 0\nforecast::auto.arima(df$outcome, xreg = as.matrix(df[,-1]))\n#> Series: df$outcome \n#> Regression with ARIMA(3,0,2) errors \n#> \n#> Coefficients:\n#>          ar1      ar2     ar3      ma1     ma2  intercept     time  treatment\n#>       0.1904  -0.9672  0.0925  -0.1327  0.9557     9.7122  15.0026    19.8588\n#> s.e.  0.0693   0.0356  0.0543   0.0467  0.0338     0.1446   0.0012     0.2141\n#>       timesincetreat\n#>              24.9965\n#> s.e.          0.0021\n#> \n#> sigma^2 = 0.91:  log likelihood = -496.34\n#> AIC=1012.67   AICc=1013.3   BIC=1051.67"},{"path":"interrupted-time-series.html","id":"multiple-groups","chapter":"32 Interrupted Time Series","heading":"32.2 Multiple Groups","text":"suspect might confounding events selection bias, can add control group experience treatment (much similar Difference--differences)model becomes\\[\n\\begin{aligned}\nY = \\beta_0 &+ \\beta_1 time+ \\beta_2 treatment +\\beta_3 \\times timesincetreat \\\\\n&+\\beta_4 group + \\beta_5 group \\times time + \\beta_6 group \\times treatment \\\\\n&+ \\beta_7 group \\times timesincetreat\n\\end{aligned}\n\\]whereGroup = 1 observation treatment 0 controlGroup = 1 observation treatment 0 control\\(\\beta_4\\) = baseline difference treatment control group\\(\\beta_4\\) = baseline difference treatment control group\\(\\beta_5\\) = slope difference treatment control group treatment\\(\\beta_5\\) = slope difference treatment control group treatment\\(\\beta_6\\) = baseline difference treatment control group associated treatment.\\(\\beta_6\\) = baseline difference treatment control group associated treatment.\\(\\beta_7\\) = difference sustained effect treatment control group treatment.\\(\\beta_7\\) = difference sustained effect treatment control group treatment.","code":""},{"path":"endogeneity.html","id":"endogeneity","chapter":"33 Endogeneity","heading":"33 Endogeneity","text":"RefresherA general model framework\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{X} = n \\times k\\)\\(\\mathbf{X} = n \\times k\\)\\(\\beta = k \\times 1\\)\\(\\beta = k \\times 1\\)\\(\\epsilon = n \\times 1\\)\\(\\epsilon = n \\times 1\\), OLS estimates coefficients \\[\n\\begin{aligned}\n\\hat{\\beta}_{OLS} &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{Y}) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'(\\mathbf{X \\beta + \\epsilon})) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{X}) \\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon}) \\\\\n\\hat{\\beta}_{OLS} & \\\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\n\\end{aligned}\n\\]unbiased estimates, get rid second part \\((\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\\)2 conditions achieve unbiased estimates:\\(E(\\epsilon |X) = 0\\) (easy, putting intercept can solve issue)\\(Cov(\\mathbf{X}, \\epsilon) = 0\\) (hard part)care omitted variableUsually, problem stem Omitted Variables Bias, care omitted variable bias whenOmitted variables correlate variables care (\\(X\\)). OMV correlate \\(X\\), don’t care, random assignment makes correlation goes 0)Omitted variables correlates outcome/ dependent variableThere types endogeneity listed .Types endogeneity (See Hill et al. (2021) review management):Endogenous TreatmentOmitted Variables Bias\nMotivation\nAbility/talent\nSelf-selection\nOmitted Variables BiasMotivationAbility/talentSelf-selectionFeedback Effect (Simultaneity): also known bidirectionalityFeedback Effect (Simultaneity): also known bidirectionalityReverse Causality: Subtle difference Simultaneity: Technically, two variables affect sequentially, big enough time frame, (e.g., monthly, yearly), coefficient biased just like simultaneity.Reverse Causality: Subtle difference Simultaneity: Technically, two variables affect sequentially, big enough time frame, (e.g., monthly, yearly), coefficient biased just like simultaneity.Measurement ErrorMeasurement ErrorEndogenous Sample SelectionTo deal problem, toolbox (mentioned previous chapter 18)Using control variables regression “selection observables” identification strategy.words, believe omitted variable, can measure , including regression model solves problem. uninterested variables called control variables model.However, rarely case (problem don’t measurements). Hence, need elaborate methods:Endogenous TreatmentEndogenous TreatmentEndogenous Sample SelectionEndogenous Sample SelectionBefore get methods deal bias arises omitted variables, consider cases measurements variable, measurement error (bias).","code":""},{"path":"endogeneity.html","id":"endogenous-treatment","chapter":"33 Endogeneity","heading":"33.1 Endogenous Treatment","text":"","code":""},{"path":"endogeneity.html","id":"measurement-error","chapter":"33 Endogeneity","heading":"33.1.1 Measurement Error","text":"Data error can stem \nCoding errors\nReporting errors\nData error can stem fromCoding errorsCoding errorsReporting errorsReporting errorsTwo forms measurement error:Random (stochastic) (indeterminate error) (Classical Measurement Errors): noise measurement errors show consistent predictable way.Systematic (determinate error) (Non-classical Measurement Errors): measurement error consistent predictable across observations.\nInstrument errors (e.g., faulty scale) -> calibration adjustment\nMethod errors (e.g., sampling errors) -> better method development + study design\nHuman errors (e.g., judgement)\nInstrument errors (e.g., faulty scale) -> calibration adjustmentMethod errors (e.g., sampling errors) -> better method development + study designHuman errors (e.g., judgement)Usually systematic measurement error bigger issue introduces “bias” estimates, random error introduces noise estimatesNoise -> regression estimate 0Bias -> can pull estimate upward downward.","code":""},{"path":"endogeneity.html","id":"classical-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.1 Classical Measurement Errors","text":"","code":""},{"path":"endogeneity.html","id":"right-hand-side","chapter":"33 Endogeneity","heading":"33.1.1.1.1 Right-hand side","text":"Right-hand side measurement error: measurement covariates, endogeneity problem.Say know true model \\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\]don’t observe \\(X_i\\), observe\\[\n\\tilde{X}_i = X_i + e_i\n\\]known classical measurement errors assume \\(e_i\\) uncorrelated \\(X_i\\) (.e., \\(E(X_i e_i) = 0\\)), estimate observed variables, (substitute \\(X_i\\) \\(\\tilde{X}_i - e_i\\) ):\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 (\\tilde{X}_i - e_i)+ u_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + u_i - \\beta_1 e_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + v_i\n\\end{aligned}\n\\]words, measurement error \\(X_i\\) now part error term regression equation \\(v_i\\). Hence, endogeneity bias.Endogeneity arises \\[\n\\begin{aligned}\nE(\\tilde{X}_i v_i) &= E((X_i + e_i )(u_i - \\beta_1 e_i)) \\\\\n&= -\\beta_1 Var(e_i) \\neq 0\n\\end{aligned}\n\\]Since \\(\\tilde{X}_i\\) \\(e_i\\) positively correlated, leads toa negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea positive bias \\(\\beta_1\\) negativea positive bias \\(\\beta_1\\) negativeIn words, measurement errors cause attenuation bias, inter turn pushes coefficient towards 0As \\(Var(e_i)\\) increases \\(\\frac{Var(e_i)}{Var(\\tilde{X})} \\1\\) \\(e_i\\) random (noise) \\(\\beta_1 \\0\\) (random variable \\(\\tilde{X}\\) relation \\(Y_i\\))Technical note:size bias OLS-estimator \\[\n\\hat{\\beta}_{OLS} = \\frac{ cov(\\tilde{X}, Y)}{var(\\tilde{X})} = \\frac{cov(X + e, \\beta X + u)}{var(X + e)}\n\\]\\[\nplim \\hat{\\beta}_{OLS} = \\beta \\frac{\\sigma^2_X}{\\sigma^2_X + \\sigma^2_e} = \\beta \\lambda\n\\]\\(\\lambda\\) reliability signal--total variance ratio attenuation factorReliability affect extent measurement error attenuates \\(\\hat{\\beta}\\). attenuation bias \\[\n\\hat{\\beta}_{OLS} - \\beta = -(1-\\lambda)\\beta\n\\]Thus, \\(\\hat{\\beta}_{OLS} < \\beta\\) (unless \\(\\lambda = 1\\), case don’t even measurement error).Note:Data transformation worsen (magnify) measurement error\\[\ny= \\beta x + \\gamma x^2 + \\epsilon\n\\], attenuation factor \\(\\hat{\\gamma}\\) square attenuation factor \\(\\hat{\\beta}\\) (.e., \\(\\lambda_{\\hat{\\gamma}} = \\lambda_{\\hat{\\beta}}^2\\))Adding covariates increases attenuation biasTo fix classical measurement error problem, canFind estimates either \\(\\sigma^2_X, \\sigma^2_\\epsilon\\) \\(\\lambda\\) validation studies, survey data.Endogenous Treatment Use instrument \\(Z\\) correlated \\(X\\) uncorrelated \\(\\epsilon\\)Abandon project","code":""},{"path":"endogeneity.html","id":"left-hand-side","chapter":"33 Endogeneity","heading":"33.1.1.1.2 Left-hand side","text":"measurement outcome variable, econometricians causal scientists care still unbiased estimate coefficients (zero conditional mean assumption violated, hence don’t endogeneity). However, statisticians might care might inflate uncertainty coefficient estimates (.e., higher standard errors).\\[\n\\tilde{Y} = Y + v\n\\]model estimate \\[\n\\tilde{Y} = \\beta X + u + v\n\\]Since \\(v\\) uncorrelated \\(X\\), \\(\\hat{\\beta}\\) consistently estimated OLSIf measurement error \\(Y_i\\), pass \\(\\beta_1\\) go \\(u_i\\)","code":""},{"path":"endogeneity.html","id":"non-classical-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.2 Non-classical Measurement Errors","text":"Relaxing assumption \\(X\\) \\(\\epsilon\\) uncorrelatedRecall true model true estimate \\[\n\\hat{\\beta} = \\frac{cov(X + \\epsilon, \\beta X + u)}{var(X + \\epsilon)}\n\\]without assumption, \\[\n\\begin{aligned}\nplim \\hat{\\beta} &= \\frac{\\beta (\\sigma^2_X + \\sigma_{X \\epsilon})}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}} \\\\\n&= (1 - \\frac{\\sigma^2_{\\epsilon} + \\sigma_{X \\epsilon}}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}}) \\beta \\\\\n&= (1 - b_{\\epsilon \\tilde{X}}) \\beta\n\\end{aligned}\n\\]\\(b_{\\epsilon \\tilde{X}}\\) covariance \\(\\tilde{X}\\) \\(\\epsilon\\) (also regression coefficient regression \\(\\epsilon\\) \\(\\tilde{X}\\))Hence, Classical Measurement Errors just special case Non-classical Measurement Errors \\(b_{\\epsilon \\tilde{X}} = 1 - \\lambda\\)\\(\\sigma_{X \\epsilon} = 0\\) (Classical Measurement Errors), increasing covariance \\(b_{\\epsilon \\tilde{X}}\\) increases covariance increases attenuation factor half variance \\(\\tilde{X}\\) measurement error, decreases attenuation factor otherwise. also known mean reverting measurement error Bound, Brown, Mathiowetz (2001)general framework right-hand side left-hand side measurement error (Bound, Brown, Mathiowetz 2001):consider true model\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1}\\tilde{X} \\tilde{Y}} \\\\\n&= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (\\tilde{X} \\beta - U \\beta + v + \\epsilon )} \\\\\n&= \\mathbf{\\beta + (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (-U \\beta + v + \\epsilon)} \\\\\nplim \\hat{\\beta} &= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' ( -U\\beta + v) \\\\\n&= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' W\n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\end{aligned}\n\\]Since collect measurement errors matrix \\(W = [U|v]\\), \\[\n( -U\\beta + v) = W\n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\]Hence, general, biases coefficients \\(\\beta\\) regression coefficients regressing measurement errors mis-measured \\(\\tilde{X}\\)Notes:Instrumental Variable can help fix problemInstrumental Variable can help fix problemThere can also measurement error dummy variables can still use Instrumental Variable fix .can also measurement error dummy variables can still use Instrumental Variable fix .","code":""},{"path":"endogeneity.html","id":"solution-to-measurement-errors","chapter":"33 Endogeneity","heading":"33.1.1.3 Solution to Measurement Errors","text":"","code":""},{"path":"endogeneity.html","id":"correlation","chapter":"33 Endogeneity","heading":"33.1.1.3.1 Correlation","text":"\\[\n\\begin{aligned}\nP(\\rho | data) &= \\frac{P(data|\\rho)P(\\rho)}{P(data)} \\\\\n\\text{Posterior Probability} &\\propto \\text{Likelihood} \\times \\text{Prior Probability}\n\\end{aligned}\n\\] \\(\\rho\\) correlation coefficient\\(P(data|\\rho)\\) likelihood function evaluated \\(\\rho\\)\\(P(\\rho)\\) prior probability\\(P(data)\\) normalizing constantWith sample correlation coefficient \\(r\\):\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\n\\] posterior density approximation \\(\\rho\\) (Schisterman et al. 2003, 3)\\[\nP(\\rho| x, y)  \\propto P(\\rho) \\frac{(1- \\rho^2)^{(n-1)/2}}{(1- \\rho \\times r)^{n - (3/2)}}\n\\]\\(\\rho = \\tanh \\xi\\) \\(\\xi \\sim N(z, 1/n)\\)\\(r = \\tanh z\\)posterior density follow normal distribution whereMean\\[\n\\mu_{posterior} = \\sigma^2_{posterior} \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood})\n\\]variance\\[\n\\sigma^2_{posterior} = \\frac{1}{n_{prior} + n_{Likelihood}}\n\\]simplify integration process, choose prior \\[\nP(\\rho) \\propto (1 - \\rho^2)^c\n\\] \\(c\\) weight prior estimation (.e., \\(c = 0\\) prior info, hence \\(P(\\rho) \\propto 1\\))Example:Current study: \\(r_{xy} = 0.5, n = 200\\)Previous study: \\(r_{xy} = 0.2765, (n=50205)\\)Combining two, posterior following normal distribution variance \\[\n\\sigma^2_{posterior} =  \\frac{1}{n_{prior} + n_{Likelihood}} = \\frac{1}{200 + 50205} = 0.0000198393\n\\]Mean\\[\n\\begin{aligned}\n\\mu_{Posterior} &= \\sigma^2_{Posterior}  \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood}) \\\\\n&= 0.0000198393 \\times (50205 \\times \\tanh^{-1} 0.2765 + 200 \\times \\tanh^{-1}0.5 )\\\\\n&= 0.2849415\n\\end{aligned}\n\\]Hence, \\(Posterior \\sim N(0.691, 0.0009)\\), means correlation coefficient \\(\\tanh(0.691) = 0.598\\) 95% CI \\[\n\\mu_{posterior} \\pm 1.96 \\times \\sqrt{\\sigma^2_{Posterior}} = 0.2849415 \\pm 1.96 \\times (0.0000198393)^{1/2} = (0.2762115, 0.2936714)\n\\]Hence, interval posterior \\(\\rho\\) \\((0.2693952, 0.2855105)\\)future authors suspect haveLarge sampling variationMeasurement error either measures correlation, attenuates relationship two variablesApplying Bayesian correction can give better estimate correlation two.implement calculation R, see ","code":"\nn_new              <- 200\nr_new              <- 0.5\nalpha              <- 0.05\n\nupdate_correlation <- function(n_new, r_new, alpha) {\n    n_meta             <- 50205\n    r_meta             <- 0.2765\n    \n    # Variance\n    var_xi         <- 1 / (n_new + n_meta)\n    format(var_xi, scientific = FALSE)\n    \n    # mean\n    mu_xi          <- var_xi * (n_meta * atanh(r_meta) + n_new * (atanh(r_new)))\n    format(mu_xi, scientific  = FALSE)\n    \n    # confidence interval\n    upper_xi       <- mu_xi + qnorm(1 - alpha / 2) * sqrt(var_xi)\n    lower_xi       <- mu_xi - qnorm(1 - alpha / 2) * sqrt(var_xi)\n    \n    # rho\n    mean_rho       <- tanh(mu_xi)\n    upper_rho      <- tanh(upper_xi)\n    lower_rho      <- tanh(lower_xi)\n    \n    # return a list\n    return(\n        list(\n            \"mu_xi\" = mu_xi,\n            \"var_xi\" = var_xi,\n            \"upper_xi\" = upper_xi,\n            \"lower_xi\" = lower_xi,\n            \"mean_rho\" = mean_rho,\n            \"upper_rho\" = upper_rho,\n            \"lower_rho\" = lower_rho\n        )\n    )\n}\n\n\n\n\n# Old confidence interval\nr_new + qnorm(1 - alpha / 2) * sqrt(1/n_new)\n#> [1] 0.6385904\nr_new - qnorm(1 - alpha / 2) * sqrt(1/n_new)\n#> [1] 0.3614096\n\ntesting = update_correlation(n_new = n_new, r_new = r_new, alpha = alpha)\n\n# Updated rho\ntesting$mean_rho\n#> [1] 0.2774723\n\n# Updated confidence interval\ntesting$upper_rho\n#> [1] 0.2855105\ntesting$lower_rho\n#> [1] 0.2693952"},{"path":"endogeneity.html","id":"simultaneity","chapter":"33 Endogeneity","heading":"33.1.2 Simultaneity","text":"independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).Examples: quantity price demand supply, investment productivity, sales advertisementExamples: quantity price demand supply, investment productivity, sales advertisementGeneral Simultaneous (Structural) Equations\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 X_i + u_i \\\\\nX_i &= \\alpha_0 + \\alpha_1 Y_i + v_i\n\\end{aligned}\n\\]Hence, solutions \\[\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 v_i + u_i}{1 - \\alpha_1 \\beta_1} \\\\\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}\n\\end{aligned}\n\\]run one regression, biased estimators (simultaneity bias):\\[\n\\begin{aligned}\nCov(X_i, u_i) &= Cov(\\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}, u_i) \\\\\n&= \\frac{\\alpha_1}{1- \\alpha_1 \\beta_1} Var(u_i)\n\\end{aligned}\n\\]even general model\\[\n\\begin{cases}\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + u_i \\\\\nX_i = \\alpha_0 + \\alpha_1 Y_i + \\alpha_2 Z_i + v_i\n\\end{cases}\n\\]\\(X_i, Y_i\\) endogenous variables determined within system\\(X_i, Y_i\\) endogenous variables determined within system\\(T_i, Z_i\\) exogenous variables\\(T_i, Z_i\\) exogenous variablesThen, reduced form model \\[\n\\begin{cases}\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 \\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{u}_i \\\\\n&= B_0 + B_1 Z_i + B_2 T_i + \\tilde{u}_i\n\\end{aligned}\n\\\\\n\\begin{aligned}\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\alpha_1\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{v}_i \\\\\n&= A_0 + A_1 Z_i + A_2 T_i + \\tilde{v}_i\n\\end{aligned}\n\\end{cases}\n\\], now can get consistent estimates reduced form parametersAnd get original parameter estimates\\[\n\\begin{aligned}\n\\frac{B_1}{A_1} &= \\beta_1 \\\\\nB_2 (1 - \\frac{B_1 A_2}{A_1B_2}) &= \\beta_2 \\\\\n\\frac{A_2}{B_2} &= \\alpha_1 \\\\\nA_1 (1 - \\frac{B_1 A_2}{A_1 B_2}) &= \\alpha_2\n\\end{aligned}\n\\]Rules IdentificationOrder Condition (necessary sufficient)\\[\nK - k \\ge m - 1\n\\]\\(M\\) = number endogenous variables model\\(M\\) = number endogenous variables modelK = number exogenous variables int modelK = number exogenous variables int model\\(m\\) = number endogenous variables given\\(m\\) = number endogenous variables given\\(k\\) = number exogenous variables given equation\\(k\\) = number exogenous variables given equationThis actually general framework instrumental variables","code":""},{"path":"endogeneity.html","id":"endogenous-treatment-solutions","chapter":"33 Endogeneity","heading":"33.1.3 Endogenous Treatment Solutions","text":"Using OLS estimates reference point","code":"\nlibrary(AER)\nlibrary(REndo)\nset.seed(421)\ndata(\"CASchools\")\nschool <- CASchools\nschool$stratio <- with(CASchools, students / teachers)\nm1.ols <-\n    lm(read ~ stratio + english + lunch \n       + grades + income + calworks + county,\n       data = school)\nsummary(m1.ols)$coefficients[1:7,]\n#>                 Estimate Std. Error     t value      Pr(>|t|)\n#> (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e-218\n#> stratio      -0.30035544 0.25797023  -1.1643027  2.450536e-01\n#> english      -0.20550107 0.03765408  -5.4576041  8.871666e-08\n#> lunch        -0.38684059 0.03700982 -10.4523759  1.427370e-22\n#> gradesKK-08  -1.91291321 1.35865394  -1.4079474  1.599886e-01\n#> income        0.71615378 0.09832843   7.2832829  1.986712e-12\n#> calworks     -0.05273312 0.06154758  -0.8567863  3.921191e-01"},{"path":"endogeneity.html","id":"instrumental-variable","chapter":"33 Endogeneity","heading":"33.1.3.1 Instrumental Variable","text":"A3a requires \\(\\epsilon_i\\) uncorrelated \\(\\mathbf{x}_i\\)Assume A1 , A2, A5\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x_i'x_i})]^{-1}E(\\mathbf{x_i'}\\epsilon_i)\n\\]A3a weakest assumption needed OLS consistentA3 fails \\(x_{ik}\\) correlated \\(\\epsilon_i\\)Omitted Variables Bias: \\(\\epsilon_i\\) includes factors may influence dependent variable (linearly)Simultaneity Demand prices simultaneously determined.Endogenous Sample Selection iid sampleMeasurement ErrorNoteOmitted Variable: omitted variable variable, omitted model (\\(\\epsilon_i\\)) unobserved predictive power towards outcome.Omitted Variable Bias: bias (inconsistency looking large sample properties) OLS estimator omitted variable.cam positive negative selection bias (depends story )structural equation used emphasize interested understanding causal relationship\\[\ny_{i1} = \\beta_0 + \\mathbf{z}_i1 \\beta_1 + y_{i2}\\beta_2 +  \\epsilon_i\n\\]\\(y_{}\\) outcome variable (inherently correlated \\(\\epsilon_i\\))\\(y_{i2}\\) endogenous covariate (presumed correlated \\(\\epsilon_i\\))\\(\\beta_1\\) represents causal effect \\(y_{i2}\\) \\(y_{i1}\\)\\(\\mathbf{z}_{i1}\\) exogenous controls (uncorrelated \\(\\epsilon_i\\)) (\\(E(z_{1i}'\\epsilon_i) = 0\\))OLS inconsistent estimator causal effect \\(\\beta_2\\)endogeneity\\(E(y_{i2}'\\epsilon_i) = 0\\)exogenous variation \\(y_{i2}\\) identifies causal effectIf endogeneityAny wiggle \\(y_{i2}\\) shift simultaneously \\(\\epsilon_i\\)\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\n\\]\\(\\beta\\) causal effect\\([E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\\) endogenous effectHence \\(\\hat{\\beta}_{OLS}\\) can either positive negative true causal effect.Motivation Two Stage Least Squares (2SLS)\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]want understand movement \\(y_{i2}\\) effects movement \\(y_{i1}\\), whenever move \\(y_{i2}\\), \\(\\epsilon_i\\) also moves.Solution\nneed way move \\(y_{i2}\\) independently \\(\\epsilon_i\\), can analyze response \\(y_{i1}\\) causal effectFind instrumental variable(s) \\(z_{i2}\\)\nInstrument Relevance: ** \\(z_{i2}\\) moves \\(y_{i2}\\) also moves\nInstrument Exogeneity: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\nFind instrumental variable(s) \\(z_{i2}\\)Instrument Relevance: ** \\(z_{i2}\\) moves \\(y_{i2}\\) also movesInstrument Exogeneity: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)Finding Instrumental variable:Random Assignment: + Effect class size educational outcomes: instrument initial randomRelation’s Choice + Effect Education Fertility: instrument parent’s educational levelEligibility + Trade-IRA 401K retirement savings: instrument 401k eligibilityExampleReturn Collegeeducation correlated ability - endogenouseducation correlated ability - endogenousNear 4year instrument\nInstrument Relevance: near moves education also moves\nInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.\nNear 4year instrumentInstrument Relevance: near moves education also movesInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.potential instruments; near 2-year college. Parent’s Education. Owning Library CardOther potential instruments; near 2-year college. Parent’s Education. Owning Library Card\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]First Stage (Reduced Form) Equation:\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i\n\\]\\(\\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2}\\) exogenous variation \\(v_i\\) endogenous variationThis called reduced form equationNot interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)Instrumental variable \\(z_{i2}\\)Instrument Relevance: \\(\\pi_2 \\neq 0\\)Instrument Exogeneity: \\(E(\\mathbf{z_{i2}\\epsilon_i})=0\\)Moving exogenous part \\(y_i2\\) moving\\[\n\\tilde{y}_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1 + z_{i2}\\pi_2}\n\\]two Stage Least Squares (2SLS)\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ y_{i2}\\beta_2 + \\epsilon_i\n\\]\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]Equivalently,\\(\\tilde{y}_{i2} =\\pi_0 + \\mathbf{z_{i2}\\pi_2}\\)\\(u_i = v_i \\beta_2+ \\epsilon_i\\)(33.1) holds A1, A5A2 holds instrument relevant \\(\\pi_2 \\neq 0\\) + \\(y_{i1} = \\beta_0 + \\mathbf{z_{i1}\\beta_1 + (\\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2)}\\beta_2 + u_i\\)A3a holds instrument exogenous \\(E(\\mathbf{z}_{i2}\\epsilon_i)=0\\)\\[\n\\begin{aligned}\nE(\\tilde{y}_{i2}'u_i) &= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})(v_i\\beta_2 + \\epsilon_i)) \\\\\n&= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})( \\epsilon_i)) \\\\\n&= E(\\epsilon_i)\\pi_0 + E(\\epsilon_iz_{i1})\\pi_1 + E(\\epsilon_iz_{i2}) \\\\\n&=0\n\\end{aligned}\n\\]Hence, (33.1) consistentThe 2SLS Estimator\n1. Estimate first stage using OLS\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]obtained estimated value \\(\\hat{y}_{i2}\\)Estimate altered equation using OLS\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ \\hat{y}_{i2}\\beta_2 + \\epsilon_i\n\\]Properties 2SLS EstimatorUnder A1, A2, A3a (\\(z_{i1}\\)), A5 instrument satisfies following two conditions, + Instrument Relevance: \\(\\pi_2 \\neq 0\\) + Instrument Exogeneity: \\(E(\\mathbf{z}_{i2}'\\epsilon_i) = 0\\) 2SLS estimator consistentCan handle one endogenous variable one instrumental variable\\[\n\\begin{aligned}\ny_{i1} &= \\beta_0 + z_{i1}\\beta_1 + y_{i2}\\beta_2 + y_{i3}\\beta_3 + \\epsilon_i \\\\\ny_{i2} &= \\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2 + z_{i3}\\pi_3 + z_{i4}\\pi_4 + v_{i2} \\\\\ny_{i3} &= \\gamma_0 + z_{i1}\\gamma_1 + z_{i2}\\gamma_2 + z_{i3}\\gamma_3 + z_{i4}\\gamma_4 + v_{i3}\n\\end{aligned}\n\\]Standard errors produced second step correct\nknow \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variation\nproblem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.\nA4 hold, need report robust standard errors.\nStandard errors produced second step correctBecause know \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variationWe problem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.A4 hold, need report robust standard errors.2SLS less efficient OLS always larger standard errors.\nFirst, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)\nSecond, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)\n2SLS less efficient OLS always larger standard errors.First, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)Second, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)number instruments need least many number endogenous variables.number instruments need least many number endogenous variables.Note2SLS can combined FGLS make estimator efficient: first-stage, second-stage, instead using OLS, can use FLGS weight matrix \\(\\hat{w}\\)Generalized Method Moments can efficient 2SLS.second-stage 2SLS, can also use MLE, making assumption distribution outcome variable, endogenous variable, relationship (joint distribution).","code":"    + **IV estimator**: one endogenous variable with a single instrument \n    + **2SLS estimator**: one endogenous variable with multiple instruments \n    + **GMM estimator**: multiple endogenous variables with multiple instruments\n    "},{"path":"endogeneity.html","id":"testing-assumptions-1","chapter":"33 Endogeneity","heading":"33.1.3.1.1 Testing Assumptions","text":"Endogeneity Test: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Endogeneity Test: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Exogeneity (always test can might informative)Exogeneity (always test can might informative)Relevancy (need avoid “weak instruments”)Relevancy (need avoid “weak instruments”)","code":""},{},{},{},{"path":"endogeneity.html","id":"checklist","chapter":"33 Endogeneity","heading":"33.1.3.1.2 Checklist","text":"Regress dependent variable instrument (reduced form). Since OLS, unbiased estimate, coefficient estimate significant (make sure sign makes sense)Report F-stat excluded instruments. F-stat < 10 means weak instrument (J. H. Stock, Wright, Yogo 2002).Present \\(R^2\\) including instrument (Rossi 2014)models multiple instrument, present firs-t second-stage result instrument separately. Overid test conducted (e.g., Sargan-Hansen J)Hausman test OLS 2SLS (don’t confuse test evidence endogeneity irrelevant - invalid IV, test useless)Compare 2SLS limited information ML. different, evidence weak instruments.","code":""},{"path":"endogeneity.html","id":"good-instruments","chapter":"33 Endogeneity","heading":"33.1.3.2 Good Instruments","text":"Exogeneity Relevancy necessary sufficient IV produce consistent estimates.Without theory possible explanation, can always create new variable correlated \\(X\\) uncorrelated \\(\\epsilon\\)example, want estimate effect price quantity (Reiss 2011, 960)\\[\n\\begin{aligned}\nQ &= \\beta_1 P + \\beta_2 X + \\epsilon \\\\\nP &= \\pi_1 X + \\eta\n\\end{aligned}\n\\]\\(\\epsilon\\) \\(\\eta\\) jointly determined, \\(X \\perp \\epsilon, \\eta\\)Without theory, can just create new variable \\(Z = X + u\\) \\(E(u) = 0; u \\perp X, \\epsilon, \\eta\\), \\(Z\\) satisfied conditions:Relevancy: \\(X\\) correlates \\(P\\) \\(\\rightarrow\\) \\(Z\\) correlates \\(P\\)Relevancy: \\(X\\) correlates \\(P\\) \\(\\rightarrow\\) \\(Z\\) correlates \\(P\\)Exogeneity: \\(u \\perp \\epsilon\\) (random noise)Exogeneity: \\(u \\perp \\epsilon\\) (random noise)obviously, ’s valid instrument (intuitively). theoretically, relevance exogeneity sufficient identify \\(\\beta\\) unsatisfied rank condition identification.Moreover, functional form instrument also plays role choosing good instrument. Hence, always need check robustness instrument.IV methods even valid instruments can still poor sampling properties (finite sample bias, large sampling errors) (Rossi 2014)weak instrument, ’s important report appropriately. problem exacerbated multiple instruments (Larcker Rusticus 2010).","code":""},{"path":"endogeneity.html","id":"lagged-dependent-variable","chapter":"33 Endogeneity","heading":"33.1.3.2.1 Lagged dependent variable","text":"time series data sets, can use lagged dependent variable instrument influenced current shocks. example, Chetty, Friedman, Rockoff (2014) used lagged dependent variable econ.","code":""},{"path":"endogeneity.html","id":"lagged-explanatory-variable","chapter":"33 Endogeneity","heading":"33.1.3.2.2 Lagged explanatory variable","text":"Common practice applied economics: Replace suspected simultaneously determined explanatory variable lagged value Bellemare, Masaki, Pepinsky (2017).\npractice avoid simultaneity bias.\nEstimates using method still inconsistent.\nHypothesis testing becomes invalid approach.\nLagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.\nCommon practice applied economics: Replace suspected simultaneously determined explanatory variable lagged value Bellemare, Masaki, Pepinsky (2017).practice avoid simultaneity bias.practice avoid simultaneity bias.Estimates using method still inconsistent.Estimates using method still inconsistent.Hypothesis testing becomes invalid approach.Hypothesis testing becomes invalid approach.Lagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.Lagging variables changes endogeneity bias operates, adding “dynamics among unobservables” assumption “selection observables” assumption.Key conditions appropriate use (Bellemare, Masaki, Pepinsky 2017):\nunobserved confounding:\ndynamics among unobservables.\nlagged variable \\(X\\) stationary autoregressive process.\n\nunobserved confounding:\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).\nReverse causality contemporaneous, one-period lag effect.\nReverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).\n\nKey conditions appropriate use (Bellemare, Masaki, Pepinsky 2017):unobserved confounding:\ndynamics among unobservables.\nlagged variable \\(X\\) stationary autoregressive process.\ndynamics among unobservables.lagged variable \\(X\\) stationary autoregressive process.unobserved confounding:\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).\nReverse causality contemporaneous, one-period lag effect.\nReverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).\nreverse causality; causal effect operates one-period lag (\\(X_{t-1} \\Y\\), \\(X_t \\\\Y_t\\)).Reverse causality contemporaneous, one-period lag effect.Reverse causality contemporaneous; dynamics \\(Y\\), dynamics exist \\(X\\) (\\(X_{t-1} \\X\\)).Alternative approach: Use lagged values endogenous variable IV estimation. However, IV estimation effective (Reed 2015):\nLagged values belong estimating equation.\nLagged values sufficiently correlated simultaneously determined explanatory variable.\nLagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).\nAlternative approach: Use lagged values endogenous variable IV estimation. However, IV estimation effective (Reed 2015):Lagged values belong estimating equation.Lagged values belong estimating equation.Lagged values sufficiently correlated simultaneously determined explanatory variable.Lagged values sufficiently correlated simultaneously determined explanatory variable.Lagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).Lagged IVs help mitigate endogeneity violate independence assumption. However, lagged IVs violate independence assumption exclusion restriction, may aggravate endogeneity (Yu Wang Bellemare 2019).","code":""},{"path":"endogeneity.html","id":"internal-instrumental-variable","chapter":"33 Endogeneity","heading":"33.1.3.3 Internal instrumental variable","text":"(also known instrument free methods). section based Raluca Gui’s guide(also known instrument free methods). section based Raluca Gui’s guidealternative external instrumental variable approachesalternative external instrumental variable approachesAll approaches assume continuous dependent variableAll approaches assume continuous dependent variable","code":""},{"path":"endogeneity.html","id":"non-hierarchical-data-cross-classified","chapter":"33 Endogeneity","heading":"33.1.3.3.1 Non-hierarchical Data (Cross-classified)","text":"\\[\nY_t = \\beta_0 + \\beta_1 P_t + \\beta_2 X_t + \\epsilon_t\n\\]\\(t = 1, .., T\\) (indexes either time cross-sectional units)\\(Y_t\\) \\(k \\times 1\\) response variable\\(X_t\\) \\(k \\times n\\) exogenous regressor\\(P_t\\) \\(k \\times 1\\) continuous endogenous regressor\\(\\epsilon_t\\) structural error term \\(\\mu_\\epsilon =0\\) \\(E(\\epsilon^2) = \\sigma^2\\)\\(\\beta\\) model parametersThe endogeneity problem arises correlation \\(P_t\\) \\(\\epsilon_t\\):\\[\nP_t = \\gamma Z_t + v_t\n\\]\\(Z_t\\) \\(l \\times 1\\) vector internal instrumental variables\\(ν_t\\) random error \\(\\mu_{v_t}, E(v^2) = \\sigma^2_v, E(\\epsilon v) = \\sigma_{\\epsilon v}\\)\\(Z_t\\) assumed stochastic distribution \\(G\\)\\(ν_t\\) assumed density \\(h(·)\\)","code":""},{},{},{},{},{"path":"endogeneity.html","id":"hierarchical-data","chapter":"33 Endogeneity","heading":"33.1.3.3.2 Hierarchical Data","text":"Multiple independent assumptions involving various random components different levels mean moderate correlation predictors random component error term can result significant bias coefficients variance components. (J.-S. Kim Frees 2007) proposed generalized method moments uses , within variations exogenous variables, assumes within variation variables endogenous.Assumptionsthe errors level \\(\\sim iid N\\)slope variables exogenousthe level-1 \\(\\epsilon \\perp X, P\\). case, additional, external instruments necessaryHierarchical Model\\[\n\\begin{aligned}\nY_{cst} &= Z_{cst}^1 \\beta_{cs}^1 + X_{cst}^1 \\beta_1 + \\epsilon_{cst}^1 \\\\\n\\beta^1_{cs} &= Z_{cs}^2 \\beta_{c}^2 + X_{cst}^2 \\beta_2 + \\epsilon_{cst}^2 \\\\\n\\beta^2_{c} &= X^3_c \\beta_3 + \\epsilon_c^3\n\\end{aligned}\n\\]Bias stem :errors higher two levels (\\(\\epsilon_c^3,\\epsilon_{cst}^2\\)) correlated regressorsonly third level errors (\\(\\epsilon_c^3\\)) correlated regressors(J.-S. Kim Frees 2007) proposedWhen variables assumed exogenous, proposed estimator equals random effects estimatorWhen variables assumed endogenous, equals fixed effects estimatoralso use omitted variable test (based Hausman-test (J. . Hausman 1978) panel data), allows comparison robust estimator estimator efficient null hypothesis omitted variables comparison two robust estimators different levels.Another example using simulated datalevel-1 regressors: \\(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\\), \\(X_{15}\\) correlated level-2 error (.e., endogenous).level-2 regressors: \\(X_{21}, X_{22}, X_{23}, X_{24}\\)level-3 regressors: \\(X_{31}, X_{32}, X_{33}\\)estimate three-level model X15 assumed endogenous. three-level hierarchy, multilevelIV() returns five estimators, robust omitted variables (FE_L2), efficient (REF) (.e. lowest mean squared error).random effects estimator (REF) efficient assuming omitted variablesThe fixed effects estimator (FE) unbiased asymptotically normal even presence omitted variables.efficiency, random effects estimator preferable think omitted. variablesThe robust estimator preferable think omitted variables.True \\(\\beta_{X_{15}} =-1\\). can see estimators bias \\(X_{15}\\) correlated level-two error, FE_L2 GMM_L2 robustTo select appropriate estimator, use omitted variable test.three-level setting, can different estimator comparisons:Fixed effects vs. random effects estimators: Test omitted level-two level-three omitted effects, simultaneously, one compares FE_L2 REF. know omitted variables exist.Fixed effects vs. GMM estimators: existence omitted effects established sure level, test level-2 omitted effects comparing FE_L2 vs GMM_L3. reject null, omitted variables level-2 accomplished testing FE_L2 vs. GMM_L2, since latter consistent omitted effects level-2.Fixed effects vs. fixed effects estimators: can test omitted level-2 effects, allowing omitted level-3 effects comparing FE_L2 vs. FE_L3 since FE_L2 robust level-2 level-3 omitted effects FE_L3 robust level-3 omitted variables.Summary, use omitted variable test comparing REF vs. FE_L2 first.null hypothesis rejected, omitted variables either level-2 level-3If null hypothesis rejected, omitted variables either level-2 level-3Next, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:\nFE_L2 vs. FE_L3\nFE_L2 vs. GMM_L2\nNext, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:FE_L2 vs. FE_L3FE_L2 vs. GMM_L2If omitted variables level-2 found, test omitted level-3 effects comparing either\nFE_L3 vs. GMM_L3\nGMM_L2 vs. GMM_L3\nomitted variables level-2 found, test omitted level-3 effects comparing eitherFE_L3 vs. GMM_L3GMM_L2 vs. GMM_L3Since null hypothesis rejected (p = 0.000139), bias random effects estimator.test level-2 omitted effects (regardless level-3 omitted effects), compare FE_L2 versus FE_L3The null hypothesis omitted level-2 effects rejected (\\(p = 3.92e − 05\\)). Hence, omitted effects level-two. use FE_L2 consistent underlying data generated (level-2 error correlated \\(X_15\\), leads biased FE_L3 coefficients.omitted variable test FE_L2 GMM_L2 reject null hypothesis omitted level-2 effects (p-value 0).assume endogenous variable exogenous, RE GMM estimators biased wrong set internal instrumental variables. increase confidence, compare omitted variable tests variable considered endogenous vs. exogenous get sense whether variable truly endogenous.","code":"\n# function 'cholmod_factor_ldetA' not provided by package 'Matrix'\nset.seed(113)\nschool$gr08 <- school$grades == \"KK-06\"\nm7.multilevel <-\n    multilevelIV(read ~ stratio + english + lunch + income + gr08 +\n                     calworks + (1 | county) | endo(stratio),\n                 data = school)\nsummary(m7.multilevel)$coefficients[1:7,]\n# function 'cholmod_factor_ldetA' not provided by package 'Matrix'’\ndata(dataMultilevelIV)\nset.seed(114)\nformula1 <-\n    y ~ X11 + X12 + X13 + X14 + X15 + X21 + X22 + X23 + X24 +\n    X31 + X32 + X33 + (1 | CID) + (1 | SID) | endo(X15)\nm8.multilevel <-\n    multilevelIV(formula = formula1, data = dataMultilevelIV)\ncoef(m8.multilevel)\n\nsummary(m8.multilevel, \"REF\")\nsummary(m8.multilevel, \"REF\")\n# compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE_L2 (the most robust estimator), equivalently we are testing simultaneously for level-2 and level-3 omitted effects. \nsummary(m8.multilevel,\"FE_L2\")"},{"path":"endogeneity.html","id":"proxy-variables","chapter":"33 Endogeneity","heading":"33.1.3.4 Proxy Variables","text":"Can place omitted variableCan place omitted variablewill able estimate effect omitted variablewill able estimate effect omitted variablewill able reduce endogeneity caused bye omitted variablewill able reduce endogeneity caused bye omitted variablebut can Measurement Error. Hence, extremely careful using proxies.can Measurement Error. Hence, extremely careful using proxies.Criteria proxy variable:proxy correlated omitted variable.omitted variable regression solve problem endogeneityThe variation omitted variable unexplained proxy uncorrelated independent variables, including proxy.IQ test can proxy ability regression wage explained education.third requirement\\[\nability = \\gamma_0 + \\gamma_1 IQ + \\epsilon\n\\]\\(\\epsilon\\) uncorrelated education IQ test.","code":""},{"path":"endogeneity.html","id":"endogenous-sample-selection","chapter":"33 Endogeneity","heading":"33.2 Endogenous Sample Selection","text":"Also known sample selection self-selection problem incidental truncation.Also known sample selection self-selection problem incidental truncation.omitted variable people selected sampleThe omitted variable people selected sampleSome disciplines consider nonresponse bias selection bias sample selection.unobservable factors affect sample independent unobservable factors affect outcome, sample selection endogenous. Hence, sample selection ignorable estimator ignores sample selection still consistent.unobservable factors affect included sample correlated unobservable factors affect outcome, sample selection endogenous ignorable, estimators ignore endogenous sample selection consistent (don’t know part observable outcome related causal relationship part due different people selected treatment control groups).Assumptions: - unobservables affect treatment selection outcome jointly distributed bivariate normal.Notes:don’t strong exclusion restriction, identification driven assumed non linearity functional form (inverse Mills ratio). E.g., estimate depend bivariate normal distribution error structure:\nstrong exclusion restriction covariate correction equation, variation variable can help identify control selection\nweak exclusion restriction, variable exists steps, ’s assumed error structure identifies control selection (J. Heckman Navarro-Lozano 2004).\ndon’t strong exclusion restriction, identification driven assumed non linearity functional form (inverse Mills ratio). E.g., estimate depend bivariate normal distribution error structure:strong exclusion restriction covariate correction equation, variation variable can help identify control selectionWith weak exclusion restriction, variable exists steps, ’s assumed error structure identifies control selection (J. Heckman Navarro-Lozano 2004).management, Wolfolds Siegel (2019) found papers valid exclusion conditions, without , simulations show results using Heckman method less reliable obtained OLS.management, Wolfolds Siegel (2019) found papers valid exclusion conditions, without , simulations show results using Heckman method less reliable obtained OLS.differences Heckman Sample Selection vs. Heckman-type correctionThere differences Heckman Sample Selection vs. Heckman-type correctionTo deal [Sample Selection], canRandomization: participants randomly selected treatment control.Instruments determine treatment status (.e., treatment vs. control) outcome (\\(Y\\))Functional form selection outcome processes: originated (James J. Heckman 1976), later generalize (Amemiya 1984)main model\\[\n\\mathbf{y^* = xb + \\epsilon}\n\\]However, pattern missingness (.e., censored) related unobserved (latent) process:\\[\n\\mathbf{z^* = w \\gamma + u}\n\\]\\[\nz_i =\n\\begin{cases}\n1& \\text{} z_i^*>0 \\\\\n0&\\text{} z_i^*\\le0\\\\\n\\end{cases}\n\\]Equivalently, \\(z_i = 1\\) (\\(y_i\\) observed) \\[\nu_i \\ge -w_i \\gamma\n\\]Hence, probability observed \\(y_i\\) \\[\n\\begin{aligned}\nP(u_i \\ge -w_i \\gamma) &= 1 - \\Phi(-w_i \\gamma) \\\\\n&= \\Phi(w_i \\gamma) & \\text{symmetry standard normal distribution}\n\\end{aligned}\n\\]assumethe error term selection \\(\\mathbf{u \\sim N(0,)}\\)\\(Var(u_i) = 1\\) identification purposesVisually, \\(P(u_i \\ge -w_i \\gamma)\\) shaded area.Hence observed model, seeand joint distribution selection model (\\(u_i\\)), observed equation (\\(\\epsilon_i\\)) \\[\n\\left[\n\\begin{array}\n{c}\nu \\\\\n\\epsilon \\\\\n\\end{array}\n\\right]\n\\sim^{iid}N\n\\left(\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right],\n\\left[\n\\begin{array}\n{cc}\n1 & \\rho \\\\\n\\rho & \\sigma^2_{\\epsilon} \\\\\n\\end{array}\n\\right]\n\\right)\n\\]relation observed selection models:\\[\n\\begin{aligned}\nE(y_i | y_i \\text{ observed}) &= E(y_i| z^*>0) \\\\\n&= E(y_i| -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + E(\\epsilon_i | u_i > -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + \\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\n\\end{aligned}\n\\]\\(\\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\\) Inverse Mills Ratio. \\(\\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)} \\ge 0\\)property IMR: derivative : \\(IMR'(x) = -x IMR(x) - IMR(x)^2\\)Great visualization special cases correlation patterns among data errors professor Rob HickNote:(Bareinboim Pearl 2014) excellent summary cases can still causal inference case selection bias. ’ll try summarize idea :Let \\(X\\) action, \\(Y\\) outcome, S binary indicator entry data pool (\\(S = 1 =\\) sample, \\(S = 0 =\\) sample) Q conditional distribution \\(Q = P(y|x)\\).Usually want understand , \\(S\\), \\(P(y, x|S = 1)\\). Hence, ’d like recover \\(P(y|x)\\) \\(P(y, x|S = 1)\\)X Y affect S, can’t unbiasedly estimate \\(P(y|x)\\)case Omitted variable bias (\\(U\\)) sample selection bias (\\(S\\)), unblocked extraneous “flow” information X \\(Y\\), causes spurious correlation \\(X\\) \\(Y\\). Traditionally, recover \\(Q\\) parametric assumption ofThe data generating process (e.g., Heckman 2-step)Type data-generating model (e..g, treatment-dependent outcome-dependent)Selection’s probability \\(P(S = 1|P a_s)\\) non-parametrically based causal graphical models, authors proposed robust way model misspecification regardless type data-generating model, require selection’s probability. Hence, can recover Q\nWithout external data\nexternal data\nCausal effects Selection-backdoor criterion\nWithout external dataWith external dataCausal effects Selection-backdoor criterion","code":"\nx = seq(-3, 3, length = 200)\ny = dnorm(x, mean = 0, sd = 1)\nplot(x,\n     y,\n     type = \"l\",\n     main = bquote(\"Probabibility distribution of\" ~ u[i]))\nx = seq(0.3, 3, length = 100)\ny = dnorm(x, mean = 0, sd = 1)\npolygon(c(0.3, x, 3), c(0, y, 0), col = \"gray\")\ntext(1, 0.1, bquote(1 - Phi ~ (-w[i] ~ gamma)))\narrows(-0.5, 0.1, 0.3, 0, length = .15)\ntext(-0.5, 0.12, bquote(-w[i] ~ gamma))\nlegend(\n    \"topright\",\n    \"Gray = Prob of Observed\",\n    pch = 1,\n    title = \"legend\",\n    inset = .02\n)"},{"path":"endogeneity.html","id":"tobit-2","chapter":"33 Endogeneity","heading":"33.2.1 Tobit-2","text":"also known Heckman’s standard sample selection model\nAssumption: joint normality errorsData taken Mroz (1984).want estimate log(wage) married women, education, experience, experience squared, dummy variable living big city. can observe wage women working, means lot married women 1975 labor force unaccounted . Hence, OLS estimate wage equation bias due sample selection. Since data non-participants (.e., working pay), can correct selection process.Tobit-2 estimates consistent","code":""},{"path":"endogeneity.html","id":"example-1-3","chapter":"33 Endogeneity","heading":"33.2.1.1 Example 1","text":"2-stage Heckman’s model:probit equation estimates selection process (labor force?)results 1st stage used construct variable captures selection effect wage equation. correction variable called inverse Mills ratio.Use variables affect selection process selection equation. Technically, selection equation equation interest set regressors. recommended use variables (least one) selection equation affect selection process, wage process (.e., instruments). , variable kids fulfill role: women kids may likely stay home, working moms kids wages change.Alternatively,ManualSimilarly,Rho estimate correlation errors selection wage equations. lower panel, estimated coefficient inverse Mills ratio given Heckman model. fact statistically different zero consistent idea selection bias serious problem case.estimated coefficient inverse Mills ratio Heckman model statistically different zero, selection bias serious problem.","code":"\nlibrary(sampleSelection)\nlibrary(dplyr)\n# 1975 data on married women’s pay and labor-force participation \n# from the Panel Study of Income Dynamics (PSID)\ndata(\"Mroz87\") \nhead(Mroz87)\n#>   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage\n#> 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288\n#> 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416\n#> 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807\n#> 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417\n#> 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000\n#> 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106\n#>   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll\n#> 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE\n#> 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE\n#> 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE\n#> 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE\n#> 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE\n#> 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE\nMroz87 = Mroz87 %>%\n    mutate(kids = kids5 + kids618)\n\nlibrary(nnet)\nlibrary(ggplot2)\nlibrary(reshape2)\n# OLS: log wage regression on LF participants only\nols1 = lm(log(wage) ~ educ + exper + I(exper ^ 2) + city, \n          data = subset(Mroz87, lfp == 1))\n# Heckman's Two-step estimation with LFP selection equation\nheck1 = heckit(\n    selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ,\n    # the selection process, l\n    # fp = 1 if the woman is participating in the labor force\n    outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city,\n    data = Mroz87\n)\n\nsummary(heck1$probit)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                  Estimate  Std. error t value   Pr(> t)    \n#> XS(Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> XSage          0.18608901  0.06517476  2.8552  0.004301 ** \n#> XSI(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> XSkids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> XShuswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> XSeduc         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\nsummary(heck1$lm)\n#> \n#> Call:\n#> lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> XO(Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> XOeduc         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> XOexper        0.0419205  0.0136176   3.078  0.00222 ** \n#> XOI(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> XOcity         0.0510492  0.0692414   0.737  0.46137    \n#> imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.7734, Adjusted R-squared:  0.7702 \n#> F-statistic:   240 on 6 and 422 DF,  p-value: < 2.2e-16\n# ML estimation of selection model\nml1 = selection(\n    selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ,\n    outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city,\n    data = Mroz87\n) \nsummary(ml1)\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 3 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -914.0777 \n#> 753 observations (325 censored and 428 observed)\n#> 13 free parameters (df = 740)\n#> Probit selection equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.1484037  1.4109302  -2.940 0.003382 ** \n#> age          0.1842132  0.0658041   2.799 0.005253 ** \n#> I(age^2)    -0.0023925  0.0007664  -3.122 0.001868 ** \n#> kids        -0.1488158  0.0384888  -3.866 0.000120 ***\n#> huswage     -0.0434253  0.0123229  -3.524 0.000451 ***\n#> educ         0.1255639  0.0229229   5.478 5.91e-08 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.5814781  0.3052031  -1.905  0.05714 .  \n#> educ         0.1078481  0.0172998   6.234 7.63e-10 ***\n#> exper        0.0415752  0.0133269   3.120  0.00188 ** \n#> I(exper^2)  -0.0008125  0.0003974  -2.044  0.04129 *  \n#> city         0.0522990  0.0682652   0.766  0.44385    \n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.66326    0.02309  28.729   <2e-16 ***\n#> rho    0.05048    0.23169   0.218    0.828    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\n# summary(ml1$twoStep)\nmyprob <- probit(lfp ~ age + I( age^2 ) + kids + huswage + educ, \n                 # x = TRUE, \n                 # iterlim = 30, \n                 data = Mroz87)\nsummary(myprob)\n#> --------------------------------------------\n#> Probit binary choice model/Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -482.8212 \n#> Model: Y == '1' in contrary to '0'\n#> 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)\n#> Estimates:\n#>                Estimate  Std. error t value   Pr(> t)    \n#> (Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** \n#> age          0.18608901  0.06517476  2.8552  0.004301 ** \n#> I(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** \n#> kids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***\n#> huswage     -0.04303635  0.01220791 -3.5253  0.000423 ***\n#> educ         0.12502818  0.02277645  5.4894 4.034e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> Significance test:\n#> chi2(5) = 64.10407 (p=1.719042e-12)\n#> --------------------------------------------\n\nimr <- invMillsRatio(myprob)\nMroz87$IMR1 <- imr$IMR1\n\nmanually_est <- lm(log(wage) ~ educ + exper + I( exper^2 ) + city + IMR1,\n                   data = Mroz87, \n                   subset = (lfp == 1))\n\nsummary(manually_est)\n#> \n#> Call:\n#> lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, \n#>     data = Mroz87, subset = (lfp == 1))\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.09494 -0.30953  0.05341  0.36530  2.34770 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143381  0.3768796  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> IMR1         0.0551177  0.2111916   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6674 on 422 degrees of freedom\n#> Multiple R-squared:  0.1582, Adjusted R-squared:  0.1482 \n#> F-statistic: 15.86 on 5 and 422 DF,  p-value: 2.505e-14\nprobit_selection <-\n    glm(lfp ~ age + I( age^2 ) + kids + huswage + educ,\n        data = Mroz87,\n        family = binomial(link = 'probit'))\n\n# library(fixest)\n# probit_selection <-\n#     fixest::feglm(lfp ~ age + I( age^2 ) + kids + huswage + educ,\n#         data = Mroz87,\n#         family = binomial(link = 'probit'))\n\nprobit_lp <- -predict(probit_selection)\ninv_mills <- dnorm(probit_lp) / (1 - pnorm(probit_lp))\nMroz87$inv_mills <- inv_mills\n\n\nprobit_outcome <-\n    glm(\n        log(wage) ~ educ + exper + I(exper ^ 2) + city + inv_mills,\n        data = Mroz87,\n        subset = (lfp == 1)\n    )\nsummary(probit_outcome)\n#> \n#> Call:\n#> glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + \n#>     inv_mills, data = Mroz87, subset = (lfp == 1))\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -3.09494  -0.30953   0.05341   0.36530   2.34770  \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.6143383  0.3768798  -1.630  0.10383    \n#> educ         0.1092363  0.0197062   5.543 5.24e-08 ***\n#> exper        0.0419205  0.0136176   3.078  0.00222 ** \n#> I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  \n#> city         0.0510492  0.0692414   0.737  0.46137    \n#> inv_mills    0.0551179  0.2111918   0.261  0.79423    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.4454809)\n#> \n#>     Null deviance: 223.33  on 427  degrees of freedom\n#> Residual deviance: 187.99  on 422  degrees of freedom\n#> AIC: 876.49\n#> \n#> Number of Fisher Scoring iterations: 2\nlibrary(\"stargazer\")\nlibrary(\"Mediana\")\nlibrary(\"plm\")\n# function to calculate corrected SEs for regression \ncse = function(reg) {\n  rob = sqrt(diag(vcovHC(reg, type = \"HC1\")))\n  return(rob)\n}\n\n# stargazer table\nstargazer(\n    # ols1,\n    heck1,\n    ml1,\n    # manually_est,\n    \n    se = list(cse(ols1), NULL, NULL),\n    title = \"Married women's wage regressions\",\n    type = \"text\",\n    df = FALSE,\n    digits = 4,\n    selection.equation = T\n)\n#> \n#> Married women's wage regressions\n#> ===================================================\n#>                           Dependent variable:      \n#>                     -------------------------------\n#>                                   lfp              \n#>                         Heckman        selection   \n#>                        selection                   \n#>                           (1)             (2)      \n#> ---------------------------------------------------\n#> age                    0.1861***       0.1842***   \n#>                                        (0.0658)    \n#>                                                    \n#> I(age2)                 -0.0024       -0.0024***   \n#>                                        (0.0008)    \n#>                                                    \n#> kids                  -0.1496***      -0.1488***   \n#>                                        (0.0385)    \n#>                                                    \n#> huswage                 -0.0430       -0.0434***   \n#>                                        (0.0123)    \n#>                                                    \n#> educ                    0.1250         0.1256***   \n#>                        (0.0130)        (0.0229)    \n#>                                                    \n#> Constant              -4.1815***      -4.1484***   \n#>                        (0.2032)        (1.4109)    \n#>                                                    \n#> ---------------------------------------------------\n#> Observations              753             753      \n#> R2                      0.1582                     \n#> Adjusted R2             0.1482                     \n#> Log Likelihood                         -914.0777   \n#> rho                     0.0830      0.0505 (0.2317)\n#> Inverse Mills Ratio 0.0551 (0.2099)                \n#> ===================================================\n#> Note:                   *p<0.1; **p<0.05; ***p<0.01\n\n\nstargazer(\n    ols1,\n    # heck1,\n    # ml1,\n    manually_est,\n    \n    se = list(cse(ols1), NULL, NULL),\n    title = \"Married women's wage regressions\",\n    type = \"text\",\n    df = FALSE,\n    digits = 4,\n    selection.equation = T\n)\n#> \n#> Married women's wage regressions\n#> ================================================\n#>                         Dependent variable:     \n#>                     ----------------------------\n#>                              log(wage)          \n#>                          (1)            (2)     \n#> ------------------------------------------------\n#> educ                  0.1057***      0.1092***  \n#>                        (0.0130)      (0.0197)   \n#>                                                 \n#> exper                 0.0411***      0.0419***  \n#>                        (0.0154)      (0.0136)   \n#>                                                 \n#> I(exper2)              -0.0008*      -0.0008**  \n#>                        (0.0004)      (0.0004)   \n#>                                                 \n#> city                    0.0542        0.0510    \n#>                        (0.0653)      (0.0692)   \n#>                                                 \n#> IMR1                                  0.0551    \n#>                                      (0.2112)   \n#>                                                 \n#> Constant              -0.5308***      -0.6143   \n#>                        (0.2032)      (0.3769)   \n#>                                                 \n#> ------------------------------------------------\n#> Observations             428            428     \n#> R2                      0.1581        0.1582    \n#> Adjusted R2             0.1501        0.1482    \n#> Residual Std. Error     0.6667        0.6674    \n#> F Statistic           19.8561***    15.8635***  \n#> ================================================\n#> Note:                *p<0.1; **p<0.05; ***p<0.01"},{"path":"endogeneity.html","id":"example-2-2","chapter":"33 Endogeneity","heading":"33.2.1.2 Example 2","text":"code R package sampleSelectionwithout exclusion restriction, generate yo using xs instead xo.can see estimates still unbiased standard errors substantially larger. exclusion restriction (.e., independent information selection process) certain identifying power desire. Hence, ’s better different set variable selection process interested equation. Without exclusion restriction, solely rely functional form identification.","code":"\nset.seed(0)\nlibrary(\"sampleSelection\")\nlibrary(\"mvtnorm\")\n# bivariate normal disturbances\neps <-\n    rmvnorm(500, c(0, 0), matrix(c(1, -0.7, -0.7, 1), 2, 2)) \n\n# uniformly distributed explanatory variable \n# (vectors of explanatory variables for the selection)\nxs <- runif(500)\n\n# probit data generating process\nys <- xs + eps[, 1] > 0 \n\n# vectors of explanatory variables for outcome equation\nxo <- runif(500) \nyoX <- xo + eps[, 2] # latent outcome\nyo <- yoX * (ys > 0) # observable outcome\n# true intercepts = 0 and our true slopes = 1\n# xs and xo are independent. \n# Hence, exclusion restriction is fulfilled\nsummary(selection(ys ~ xs, yo ~ xo))\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 5 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -712.3163 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  \n#> xs            1.3377     0.2014   6.642 8.18e-11 ***\n#> Outcome equation:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.0002265  0.1294178  -0.002    0.999    \n#> xo           0.7299070  0.1635925   4.462 1.01e-05 ***\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma   0.9190     0.0574  16.009  < 2e-16 ***\n#> rho    -0.5392     0.1521  -3.544 0.000431 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nyoX <- xs + eps[,2]\nyo <- yoX*(ys > 0)\nsummary(selection(ys ~ xs, yo ~ xs))\n#> --------------------------------------------\n#> Tobit 2 model (sample selection model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 14 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -712.8298 \n#> 500 observations (172 censored and 328 observed)\n#> 6 free parameters (df = 494)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  \n#> xs            1.2907     0.2085   6.191 1.25e-09 ***\n#> Outcome equation:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  -0.5499     0.5644  -0.974  0.33038   \n#> xs            1.3987     0.4482   3.120  0.00191 **\n#>    Error terms:\n#>       Estimate Std. Error t value Pr(>|t|)    \n#> sigma  0.85091    0.05352  15.899   <2e-16 ***\n#> rho   -0.13226    0.72684  -0.182    0.856    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"endogeneity.html","id":"tobit-5","chapter":"33 Endogeneity","heading":"33.2.2 Tobit-5","text":"Also known switching regression model\nCondition: least one variable X selection process included observed process. Used separate models participants, non-participants.exclusion restriction fulfilled \\(x\\)’s independent.estimates close true values.Example functional form misspecificationAlthough still exclusion restriction (xo1 xo2 independent), now problems intercepts (.e., statistically significantly different true values zero), convergence problems.don’t exclusion restriction, larger variance xsUsually converge. Even , results may seriously biased.NoteThe log-likelihood function models might globally concave. Hence, might converge, converge local maximum. combat , can useDifferent starting valueDifferent maximization methods.refer Non-linear Least Squares suggestions.","code":"\nset.seed(0)\nvc <- diag(3)\nvc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)\nvc[upper.tri(vc)] <- vc[lower.tri(vc)]\n\n# 3 disturbance vectors by a 3-dimensional normal distribution\neps <- rmvnorm(500, c(0,0,0), vc) \nxs <- runif(500) # uniformly distributed on [0, 1]\nys <- xs + eps[,1] > 0\nxo1 <- runif(500) # uniformly distributed on [0, 1]\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(500) # uniformly distributed on [0, 1]\nyo2 <- xo2 + eps[,3]\n# one selection equation and a list of two outcome equations\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2))) \n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 11 iterations\n#> Return code 1: gradient close to zero (gradtol)\n#> Log-Likelihood: -895.8201 \n#> 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)\n#> 10 free parameters (df = 490)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.1550     0.1051  -1.474    0.141    \n#> xs            1.1408     0.1785   6.390 3.86e-10 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02708    0.16395   0.165    0.869    \n#> xo1          0.83959    0.14968   5.609  3.4e-08 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.1583     0.1885   0.840    0.401    \n#> xo2           0.8375     0.1707   4.908 1.26e-06 ***\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.93191    0.09211  10.118   <2e-16 ***\n#> sigma2  0.90697    0.04434  20.455   <2e-16 ***\n#> rho1    0.88988    0.05353  16.623   <2e-16 ***\n#> rho2    0.17695    0.33139   0.534    0.594    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nset.seed(5)\neps <- rmvnorm(1000, rep(0, 3), vc)\neps <- eps^2 - 1 # subtract 1 in order to get the mean zero disturbances\n\n# interval [−1, 0] to get an asymmetric distribution over observed choices\nxs <- runif(1000, -1, 0) \nys <- xs + eps[,1] > 0\nxo1 <- runif(1000)\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(1000)\nyo2 <- xo2 + eps[,3]\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim=20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 4 iterations\n#> Return code 3: Last step could not find a value above the current.\n#> Boundary of parameter space?  \n#> Consider switching to a more robust optimisation method temporarily.\n#> Log-Likelihood: -1665.936 \n#> 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.53698    0.05808  -9.245  < 2e-16 ***\n#> xs           0.31268    0.09395   3.328 0.000906 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.70679    0.03573  -19.78   <2e-16 ***\n#> xo1          0.91603    0.05626   16.28   <2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   0.1446        NaN     NaN      NaN  \n#> xo2           1.1196     0.5014   2.233   0.0258 *\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.67770    0.01760   38.50   <2e-16 ***\n#> sigma2  2.31432    0.07615   30.39   <2e-16 ***\n#> rho1   -0.97137        NaN     NaN      NaN    \n#> rho2    0.17039        NaN     NaN      NaN    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------\nset.seed(6)\nxs <- runif(1000, -1, 1)\nys <- xs + eps[,1] > 0\nyo1 <- xs + eps[,2]\nyo2 <- xs + eps[,3]\nsummary(tmp <- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=20))\n#> --------------------------------------------\n#> Tobit 5 model (switching regression model)\n#> Maximum Likelihood estimation\n#> Newton-Raphson maximisation, 16 iterations\n#> Return code 8: successive function values within relative tolerance limit (reltol)\n#> Log-Likelihood: -1936.431 \n#> 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE)\n#> 10 free parameters (df = 990)\n#> Probit selection equation:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.3528     0.0424  -8.321 2.86e-16 ***\n#> xs            0.8354     0.0756  11.050  < 2e-16 ***\n#> Outcome equation 1:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.55448    0.06339  -8.748   <2e-16 ***\n#> xs           0.81764    0.06048  13.519   <2e-16 ***\n#> Outcome equation 2:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   0.6457     0.4994   1.293    0.196\n#> xs            0.3520     0.3197   1.101    0.271\n#>    Error terms:\n#>        Estimate Std. Error t value Pr(>|t|)    \n#> sigma1  0.59187    0.01853  31.935   <2e-16 ***\n#> sigma2  1.97257    0.07228  27.289   <2e-16 ***\n#> rho1    0.15568    0.15914   0.978    0.328    \n#> rho2   -0.01541    0.23370  -0.066    0.947    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> --------------------------------------------"},{"path":"endogeneity.html","id":"pattern-mixture-models","chapter":"33 Endogeneity","heading":"33.2.2.0.1 Pattern-Mixture Models","text":"compared Heckman’s model assumes value missing data predetermined, pattern-mixture models assume missingness affect distribution variable interest (e.g., Y)read , can check NCSU, stefvanbuuren.","code":""},{"path":"other-biases.html","id":"other-biases","chapter":"34 Other Biases","heading":"34 Other Biases","text":"econometrics, main objective often uncover causal relationships. However, coefficient estimates can affected various biases. ’s list common biases can affect coefficient estimates:’ve covered far (see Linear Regression Endogeneity):Omitted Variable Bias (OVB):\nArises variable affects dependent variable correlated independent variable left regression.\nOmitted Variable Bias (OVB):Arises variable affects dependent variable correlated independent variable left regression.Endogeneity Bias:\nOccurs error term correlated independent variable. can due :\nSimultaneity: dependent variable simultaneously affects independent variable.\nOmitted variables.\nMeasurement error independent variable.\n\nEndogeneity Bias:Occurs error term correlated independent variable. can due :\nSimultaneity: dependent variable simultaneously affects independent variable.\nOmitted variables.\nMeasurement error independent variable.\nOccurs error term correlated independent variable. can due :Simultaneity: dependent variable simultaneously affects independent variable.Simultaneity: dependent variable simultaneously affects independent variable.Omitted variables.Omitted variables.Measurement error independent variable.Measurement error independent variable.Measurement Error:\nBias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.\nMeasurement Error:Bias introduced variables model measured error. error independent variable classical (mean zero uncorrelated true value), typically biases coefficient towards zero.Sample Selection Bias:\nArises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.\nSample Selection Bias:Arises sample randomly selected selection related dependent variable. classic example Heckman correction labor market studies participants self-select workforce.Simultaneity Bias (Reverse Causality):\nHappens dependent variable causes changes independent variable, leading two-way causation.\nSimultaneity Bias (Reverse Causality):Happens dependent variable causes changes independent variable, leading two-way causation.Multicollinearity:\nbias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.\nMulticollinearity:bias strictest sense, presence high multicollinearity (independent variables highly correlated), coefficient estimates can become unstable standard errors large. makes hard determine individual effect predictors dependent variable.Specification Errors:\nArise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.\nSpecification Errors:Arise functional form model incorrectly specified, e.g., omitting interaction terms polynomial terms needed.Autocorrelation (Serial Correlation):\nOccurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.\nAutocorrelation (Serial Correlation):Occurs time-series data error terms correlated time. doesn’t cause bias coefficient estimates OLS, can make standard errors biased, leading incorrect inference.Heteroskedasticity:\nOccurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.\nHeteroskedasticity:Occurs variance error term constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias OLS estimates can bias standard errors.section, mention biases may encounter conducting researchIntroduced data aggregated, analysis conducted aggregate level rather individual level.[Survivorship Bias] (much related Sample Selection):Arises sample includes “survivors” “passed” certain threshold. Common finance funds firms “survive” analyzed.bias econometric estimation per se, relevant context empirical studies. refers tendency journals publish significant positive results, leading overrepresentation results literature.","code":""},{"path":"other-biases.html","id":"aggregation-bias","chapter":"34 Other Biases","heading":"34.1 Aggregation Bias","text":"Aggregation bias, also known ecological fallacy, refers error introduced data aggregated analysis conducted aggregate level, rather individual level. can especially problematic econometrics, analysts often concerned understanding individual behavior.relationship variables different aggregate level individual level, aggregation bias can result. bias arises inferences individual behaviors made based aggregate data.Example: Suppose data individuals’ incomes personal consumption. individual level, ’s possible income rises, consumption also rises. However, aggregate data , say, neighborhood level, neighborhoods diverse income levels might similar average consumption due unobserved factors.Step 1: Create individual level dataThis show significant positive relationship income consumption.Step 2: Aggregate data ‘neighborhood’ levelIf aggregation bias present, coefficient income aggregate regression might different coefficient individual regression, even individual relationship significant strong.plots, can see relationship individual level, neighborhood colored differently first plot. second plot shows aggregate data, point now represents whole neighborhood.Direction Bias: direction aggregation bias isn’t predetermined. depends underlying relationship data distribution. cases, aggregation might attenuate (reduce) relationship, cases, might exaggerate .Relation Biases: Aggregation bias closely related several biases econometrics:Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.Specification bias: don’t properly account hierarchical structure data (like individuals nested within neighborhoods), model might mis-specified, leading biased estimates.Measurement Error: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.Measurement Error: Aggregation can introduce amplify measurement errors. instance, aggregate noisy measures, aggregate might accurately represent underlying signal.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.Omitted Variable Bias (see Endogeneity): aggregate data, lose information. loss information results omitting important predictors correlated independent dependent variables, can introduce omitted variable bias.","code":"\nset.seed(123)\n\n# Generate data for 1000 individuals\nn <- 1000\n\nincome <- rnorm(n, mean = 50, sd = 10)\nconsumption <- 0.5 * income + rnorm(n, mean = 0, sd = 5)\n\n# Individual level regression\nindividual_lm <- lm(consumption ~ income)\nsummary(individual_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -15.1394  -3.4572   0.0213   3.5436  16.4557 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1.99596    0.82085  -2.432   0.0152 *  \n#> income       0.54402    0.01605  33.888   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.032 on 998 degrees of freedom\n#> Multiple R-squared:  0.535,  Adjusted R-squared:  0.5346 \n#> F-statistic:  1148 on 1 and 998 DF,  p-value: < 2.2e-16\n# Assume 100 neighborhoods with 10 individuals each\nn_neighborhoods <- 100\n\ndf <- data.frame(income, consumption)\ndf$neighborhood <- rep(1:n_neighborhoods, each = n / n_neighborhoods)\n\naggregate_data <- aggregate(. ~ neighborhood, data = df, FUN = mean)\n\n# Aggregate level regression\naggregate_lm <- lm(consumption ~ income, data = aggregate_data)\nsummary(aggregate_lm)\n#> \n#> Call:\n#> lm(formula = consumption ~ income, data = aggregate_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.4517 -0.9322 -0.0826  1.0556  3.5728 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -4.94338    2.60699  -1.896   0.0609 .  \n#> income       0.60278    0.05188  11.618   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.54 on 98 degrees of freedom\n#> Multiple R-squared:  0.5794, Adjusted R-squared:  0.5751 \n#> F-statistic:   135 on 1 and 98 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Individual scatterplot\np1 <- ggplot(df, aes(x = income, y = consumption)) +\n    geom_point(aes(color = neighborhood), alpha = 0.6) +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Individual Level Data\") +\n    causalverse::ama_theme()\n\n# Aggregate scatterplot\np2 <- ggplot(aggregate_data, aes(x = income, y = consumption)) +\n    geom_point(color = \"red\") +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                color = \"black\") +\n    labs(title = \"Aggregate Level Data\") +\n    causalverse::ama_theme()\n\n# print(p1)\n# print(p2)\n\ngridExtra::grid.arrange(grobs = list(p1, p2), ncol = 2)"},{"path":"other-biases.html","id":"simpsons-paradox","chapter":"34 Other Biases","heading":"34.1.1 Simpson’s Paradox","text":"Simpson’s Paradox, also known Yule-Simpson effect, phenomenon probability statistics trend appears different groups data disappears reverses groups combined. ’s striking example aggregated data can sometimes provide misleading representation actual situation.Illustration Simpson’s Paradox:Consider hypothetical scenario involving two hospitals: Hospital Hospital B. want analyze success rates treatments hospitals. break data severity cases (.e., minor cases vs. major cases):Hospital :\nMinor cases: 95% success rate\nMajor cases: 80% success rate\nHospital :Minor cases: 95% success rateMinor cases: 95% success rateMajor cases: 80% success rateMajor cases: 80% success rateHospital B:\nMinor cases: 90% success rate\nMajor cases: 85% success rate\nHospital B:Minor cases: 90% success rateMinor cases: 90% success rateMajor cases: 85% success rateMajor cases: 85% success rateFrom breakdown, Hospital appears better treating minor major cases since higher success rate categories.However, let’s consider overall success rates without considering case severity:Hospital : 83% overall success rateHospital : 83% overall success rateHospital B: 86% overall success rateHospital B: 86% overall success rateSuddenly, Hospital B seems better overall. surprising reversal happens two hospitals might handle different proportions minor major cases. example, Hospital treats many major cases (lower success rates) Hospital B, can drag overall success rate.Causes:Simpson’s Paradox can arise due :lurking confounding variable wasn’t initially considered (example, severity medical cases).lurking confounding variable wasn’t initially considered (example, severity medical cases).Different group sizes, one group might much larger , influencing aggregate results.Different group sizes, one group might much larger , influencing aggregate results.Implications:Simpson’s Paradox highlights dangers interpreting aggregated data without considering potential underlying sub-group structures. underscores importance disaggregating data aware context ’s analyzed.Relation Aggregation BiasIn extreme case, aggregation bias can reverse coefficient sign relationship interest (.e., Simpson’s Paradox).Example: Suppose studying effect new study technique student grades. two groups students: used new technique (treatment = 1) (treatment = 0). want see using new study technique related higher grades.Let’s assume grades influenced starting ability students. Perhaps sample, many high-ability students didn’t use new technique (felt didn’t need ), many low-ability students .’s setup:High-ability students tend high grades regardless technique.High-ability students tend high grades regardless technique.new technique positive effect grades, masked fact many low-ability students use .new technique positive effect grades, masked fact many low-ability students use .simulation:overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .overall_lm might show new study technique associated lower grades (negative coefficient), many high-ability students (naturally high grades) use .high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.high_ability_lm likely show high-ability students used technique slightly lower grades high-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.low_ability_lm likely show low-ability students used technique much higher grades low-ability students didn’t.classic example Simpson’s Paradox: within ability group, technique appears beneficial, data aggregated, effect seems negative distribution technique across ability groups.","code":"\nset.seed(123)\n\n# Generate data for 1000 students\nn <- 1000\n\n# 500 students are of high ability, 500 of low ability\nability <- c(rep(\"high\", 500), rep(\"low\", 500))\n\n# High ability students are less likely to use the technique\ntreatment <-\n  ifelse(ability == \"high\", rbinom(500, 1, 0.2), rbinom(500, 1, 0.8))\n\n# Grades are influenced by ability and treatment (new technique),\n# but the treatment has opposite effects based on ability.\ngrades <-\n  ifelse(\n    ability == \"high\",\n    rnorm(500, mean = 85, sd = 5) + treatment * -3,\n    rnorm(500, mean = 60, sd = 5) + treatment * 5\n  )\n\ndf <- data.frame(ability, treatment, grades)\n\n# Regression without considering ability\noverall_lm <- lm(grades ~ factor(treatment), data = df)\nsummary(overall_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -33.490  -4.729   0.986   6.368  25.607 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         80.0133     0.4373   183.0   <2e-16 ***\n#> factor(treatment)1 -11.7461     0.6248   -18.8   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.877 on 998 degrees of freedom\n#> Multiple R-squared:  0.2615, Adjusted R-squared:  0.2608 \n#> F-statistic: 353.5 on 1 and 998 DF,  p-value: < 2.2e-16\n\n# Regression within ability groups\nhigh_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"high\",])\nlow_ability_lm <-\n  lm(grades ~ factor(treatment), data = df[df$ability == \"low\",])\nsummary(high_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"high\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -14.2156  -3.4813   0.1186   3.4952  13.2919 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         85.1667     0.2504 340.088  < 2e-16 ***\n#> factor(treatment)1  -3.9489     0.5776  -6.837 2.37e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.046 on 498 degrees of freedom\n#> Multiple R-squared:  0.08581,    Adjusted R-squared:  0.08398 \n#> F-statistic: 46.75 on 1 and 498 DF,  p-value: 2.373e-11\nsummary(low_ability_lm)\n#> \n#> Call:\n#> lm(formula = grades ~ factor(treatment), data = df[df$ability == \n#>     \"low\", ])\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.3717  -3.5413   0.1097   3.3531  17.0568 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         59.8950     0.4871 122.956   <2e-16 ***\n#> factor(treatment)1   5.2979     0.5474   9.679   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.968 on 498 degrees of freedom\n#> Multiple R-squared:  0.1583, Adjusted R-squared:  0.1566 \n#> F-statistic: 93.68 on 1 and 498 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\n\n# Scatterplot for overall data\np1 <-\n  ggplot(df, aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Overall Effect of Study Technique on Grades\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for high-ability students\np2 <-\n  ggplot(df[df$ability == \"high\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (High Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# Scatterplot for low-ability students\np3 <-\n  ggplot(df[df$ability == \"low\", ], aes(\n    x = factor(treatment),\n    y = grades,\n    color = ability\n  )) +\n  geom_jitter(width = 0.2, height = 0) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  labs(title = \"Effect of Study Technique on Grades (Low Ability)\",\n       x = \"Treatment (0 = No Technique, 1 = New Technique)\",\n       y = \"Grades\")\n\n# print(p1)\n# print(p2)\n# print(p3)\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), ncol = 1)"},{"path":"other-biases.html","id":"contamination-bias","chapter":"34 Other Biases","heading":"34.2 Contamination Bias","text":"Goldsmith-Pinkham, Hull, Kolesár (2022) show regressions multiple treatments flexible controls often fail estimate convex averages heterogeneous treatment effects, resulting contamination non-convex averages treatments’ effects.3 estimation methods avoid bias find significant contamination bias observational studies, experimental studies showing less due lower variability propensity scores.","code":""},{"path":"other-biases.html","id":"survivorship-bias","chapter":"34 Other Biases","heading":"34.3 Survivorship Bias","text":"Survivorship bias refers logical error concentrating entities made past selection process overlooking didn’t, typically lack visibility. can skew results lead overly optimistic conclusions.Example: analyze success companies based ones still business today, ’d miss insights failed. give distorted view makes successful company, wouldn’t account attributes didn’t succeed.Relation Biases:Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Sample Selection Bias: Survivorship bias specific form sample selection bias. survivorship bias focuses entities “survive”, sample selection bias broadly deals non-random sample.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Confirmation Bias: Survivorship bias can reinforce confirmation bias. looking “winners”, might confirm existing beliefs leads success, ignoring evidence contrary didn’t survive.Using histogram visualize distribution earnings, highlighting “survivors”.plot, “True Avg” might lower “Survivor Avg”, indicating looking survivors, overestimate average earnings.Remedies:Awareness: Recognizing potential survivorship bias first step.Awareness: Recognizing potential survivorship bias first step.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Inclusive Data Collection: Wherever possible, try include data entities didn’t “survive” sample.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.Statistical Techniques: cases missing data inherent, methods like Heckman’s two-step procedure can used correct sample selection bias.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.External Data Sources: Sometimes, complementary datasets can provide insights missing “non-survivors”.Sensitivity Analysis: Test sensitive results assumptions non-survivors.Sensitivity Analysis: Test sensitive results assumptions non-survivors.","code":"\nset.seed(42)\n\n# Generating data for 100 companies\nn <- 100\n\n# Randomly generate earnings; assume true average earnings is 50\nearnings <- rnorm(n, mean = 50, sd = 10)\n\n# Threshold for bankruptcy\nthreshold <- 40\n\n# Only companies with earnings above the threshold \"survive\"\nsurvivor_earnings <- earnings[earnings > threshold]\n\n# Average earnings for all companies vs. survivors\ntrue_avg <- mean(earnings)\nsurvivor_avg <- mean(survivor_earnings)\n\ntrue_avg\n#> [1] 50.32515\nsurvivor_avg\n#> [1] 53.3898\nlibrary(ggplot2)\n\ndf <- data.frame(earnings)\n\np <- ggplot(df, aes(x = earnings)) +\n  geom_histogram(\n    binwidth = 2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(aes(xintercept = true_avg, color = \"True Avg\"),\n             linetype = \"dashed\",\n             size = 1) +\n  geom_vline(\n    aes(xintercept = survivor_avg, color = \"Survivor Avg\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = c(\"True Avg\" = \"blue\", \"Survivor Avg\" = \"red\"),\n                     name = \"Average Type\") +\n  labs(title = \"Distribution of Company Earnings\",\n       x = \"Earnings\",\n       y = \"Number of Companies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"other-biases.html","id":"publication-bias","chapter":"34 Other Biases","heading":"34.4 Publication Bias","text":"Publication bias occurs results studies influence likelihood published. Typically, studies significant, positive, sensational results likely published non-significant negative results. can skew perceived effectiveness results researchers conduct meta-analyses literature reviews, leading draw inaccurate conclusions.Example: Imagine pharmaceutical research. 10 studies done new drug, 2 show positive effect 8 show effect, 2 positive studies get published, later review literature might erroneously conclude drug effective.Relation Biases:Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Selection Bias: Publication bias form selection bias, selection (publication case) isn’t random based results study.Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might find cite studies confirm beliefs, overlooking unpublished studies might contradict .Let’s simulate experiment new treatment. ’ll assume treatment effect, due random variation, studies show significant positive negative effects.Using histogram visualize distribution study results, highlighting “published” studies.plot might show “True Avg Effect” around zero, “Published Avg Effect” likely higher lower, depending studies happen significant results simulation.Remedies:Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Awareness: Understand accept publication bias exists, especially conducting literature reviews meta-analyses.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Study Registries: Encourage use study registries researchers register studies start. way, one can see initiated studies, just published ones.Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Publish Results: Journals researchers make effort publish negative null results. journals, known “null result journals”, specialize .Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Funnel Plots Egger’s Test: meta-analyses, methods visually statistically detect publication bias.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.Use Preprints: Promote use preprint servers researchers can upload studies ’re peer-reviewed, ensuring results available regardless eventual publication status.p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).p-curve analysis: addresses publication bias p-hacking analyzing distribution p-values 0.05 research studies. posits right-skewed distribution p-values indicates true effect, whereas left-skewed distribution suggests p-hacking true underlying effect. method includes “half-curve” test counteract extensive p-hacking Simonsohn, Simmons, Nelson (2015).","code":"\nset.seed(42)\n\n# Number of studies\nn <- 100\n\n# Assuming no real effect (effect size = 0)\ntrue_effect <- 0\n\n# Random variation in results\nresults <- rnorm(n, mean = true_effect, sd = 1)\n\n# Only \"significant\" results get published \n# (arbitrarily defining significant as abs(effect) > 1.5)\npublished_results <- results[abs(results) > 1.5]\n\n# Average effect for all studies vs. published studies\ntrue_avg_effect <- mean(results)\npublished_avg_effect <- mean(published_results)\n\ntrue_avg_effect\n#> [1] 0.03251482\npublished_avg_effect\n#> [1] -0.3819601\nlibrary(ggplot2)\n\ndf <- data.frame(results)\n\np <- ggplot(df, aes(x = results)) +\n  geom_histogram(\n    binwidth = 0.2,\n    fill = \"grey\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_vline(\n    aes(xintercept = true_avg_effect,\n        color = \"True Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  geom_vline(\n    aes(xintercept = published_avg_effect,\n        color = \"Published Avg Effect\"),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\n      \"True Avg Effect\" = \"blue\",\n      \"Published Avg Effect\" = \"red\"\n    ),\n    name = \"Effect Type\"\n  ) +\n  labs(title = \"Distribution of Study Results\",\n       x = \"Effect Size\",\n       y = \"Number of Studies\") +\n  causalverse::ama_theme()\n\nprint(p)"},{"path":"controls.html","id":"controls","chapter":"35 Controls","heading":"35 Controls","text":"section follows (Cinelli, Forney, Pearl 2022) codeTraditional literature usually considers adding additional control variables harmless analysis.specifically, problem prevalent review process. Reviewers ask authors add variables “control” variable, can asked limited rationale. Rarely ever see reviewer asks author remove variables see behavior variable interest (also related Coefficient stability).However, adding controls good limited cases.","code":"\nlibrary(dagitty)\nlibrary(ggdag)"},{"path":"controls.html","id":"bad-controls","chapter":"35 Controls","heading":"35.1 Bad Controls","text":"","code":""},{"path":"controls.html","id":"m-bias","chapter":"35 Controls","heading":"35.1.1 M-bias","text":"Traditional textbooks (G. W. Imbens Rubin 2015; J. D. Angrist Pischke 2009) consider \\(Z\\) good control ’s pre-treatment variable, correlates treatment outcome.prevalent Matching Methods, recommended include “pre-treatment” variables.However, bad control opens back-door path \\(Z \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\)Even though \\(Z\\) can correlate \\(X\\) \\(Y\\) well, ’s confounder.Controlling \\(Z\\) can bias \\(X \\Y\\) estimate, opens colliding path \\(X \\leftarrow U_1 \\rightarrow Z \\leftarrow U_2 \\leftarrow Y\\)Table 35.1:  Another worse variation isYou can’t much case.don’t control \\(Z\\), open back-door path \\(X \\leftarrow U_1 \\Z \\Y\\), unadjusted estimate biasedIf don’t control \\(Z\\), open back-door path \\(X \\leftarrow U_1 \\Z \\Y\\), unadjusted estimate biasedIf control \\(Z\\), open backdoor path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), adjusted estimate also biasedIf control \\(Z\\), open backdoor path \\(X \\leftarrow U_1 \\Z \\leftarrow U_2 \\Y\\), adjusted estimate also biasedHence, identify causal effect case.can sensitivity analyses examine (Cinelli et al. 2019; Cinelli Hazlett 2020)plausible bounds strength direct effect \\(Z \\Y\\)strength effects latent variables","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u1->x; u1->z; u2->z; u2->y}\")\n\n# set u as latent\nlatents(model) <- c(\"u1\", \"u2\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(x = c(\n    x = 1,\n    u1 = 1,\n    z = 2,\n    u2 = 3,\n    y = 3\n),\ny = c(\n    x = 1,\n    u1 = 2,\n    z = 1.5,\n    u2 = 2,\n    y = 1\n))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nu1 <- rnorm(n)\nu2 <- rnorm(n)\nz <- u1 + u2 + rnorm(n)\nx <- u1 + rnorm(n)\ncausal_coef <- 2\ny <- causal_coef * x - 4*u2 + rnorm(n)\n\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u1->x; u1->z; u2->z; u2->y; z->y}\")\n\n# set u as latent\nlatents(model) <- c(\"u1\", \"u2\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, u1=1, z=2, u2=3, y=3),\n  y = c(x=1, u1=2, z=1.5, u2=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()"},{"path":"controls.html","id":"bias-amplification","chapter":"35 Controls","heading":"35.1.2 Bias Amplification","text":"Controlling Z amplifies omitted variable biasTable 35.2:  ","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->x; u->y; z->x}\")\n\n# set u as latent\nlatents(model) <- c(\"u\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(z=1, x=2, u=3, y=4),\n  y = c(z=1, x=1, u=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nu <- rnorm(n)\nx <- 2*z + u + rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"overcontrol-bias","chapter":"35 Controls","heading":"35.1.3 Overcontrol bias","text":"Sometimes, similar controlling variables proxy dependent variable.X proxy Z (.e., mediator Z Y), controlling Z badTable 35.3:  Now see \\(Z\\) significant, technically true, interested causal coefficient \\(X\\) \\(Y\\).Another setting overcontrol bias isTable 35.4:  Another setting bias isTable 35.5:  total effect \\(X\\) \\(Y\\) biased (.e., \\(1.01 \\approx 1.48 - 0.47\\)).Controlling Z fail identify direct effect \\(X\\) \\(Y\\) opens biasing path \\(X \\rightarrow Z \\leftarrow U \\rightarrow Y\\)","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->z; z->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=1, z=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nz <- x + rnorm(n)\ny <- z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->m; m->z; m->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, m=2, z=2, y=3),\n  y = c(x=2, m=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nm <- x + rnorm(n)\nz <- m + rnorm(n)\ny <- m + rnorm(n)\n\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->z; z->y; u->z; u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, y=4),\n  y = c(x=1, z=1, u=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nset.seed(1)\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u + rnorm(n)\ny <- z + u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"selection-bias","chapter":"35 Controls","heading":"35.1.4 Selection Bias","text":"Also known “collider stratification bias”Adjusting \\(Z\\) opens colliding path \\(X \\Z \\leftarrow U \\Y\\)Table 35.6:  Another setting isControlling \\(Z\\) opens colliding path \\(X \\Z \\leftarrow Y\\)Table 35.7:  ","code":"\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; u->z;u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=2, y=3),\n  y = c(x=3, z=2, u=4, y=3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + u +  rnorm(n)\ny <- x + 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; y->z}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- x + y + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"case-control-bias","chapter":"35 Controls","heading":"35.1.5 Case-control Bias","text":"Controlling \\(Z\\) opens virtual collider (descendant collider).However, \\(X\\) truly causal effect \\(Y\\). , controlling \\(Z\\) valid testing whether effect \\(X\\) \\(Y\\) 0 X d-separated \\(Y\\) regardless adjusting \\(Z\\)Table 35.8:  ","code":"\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; y->z}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=3),\n  y = c(x=2, z=1, y=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\ny <- x + rnorm(n)\nz <- y + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"good-controls","chapter":"35 Controls","heading":"35.2 Good Controls","text":"","code":""},{"path":"controls.html","id":"omitted-variable-bias-correction","chapter":"35 Controls","heading":"35.2.1 Omitted Variable Bias Correction","text":"\\(Z\\) can block back-door paths.Unadjusted estimate biasedadjusting \\(Z\\) blocks backdoor pathTable 35.9:  Unadjusted estimate biasedadjusting \\(Z\\) blocks backdoor door path due \\(U\\)Table 35.10:  Even though \\(Z\\) significant, give causal interpretation.Table 17.1:  Even though \\(Z\\) significant, give causal interpretation.Summary","code":"\nrm(list = ls())\n\nmodel <- dagitty(\"dag{x->y; z->x; z->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, y=3, z=2),\n  y = c(x=1, y=1, z=2))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\ncausal_coef = 2\nbeta2 = 3\nx <- z + rnorm(n)\ny <- causal_coef * x + beta2 * z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Draw DAG\n\n# specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->x; u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nu <- rnorm(n)\nz <- u + rnorm(n)\ncausal_coef = 2\nx <- z + rnorm(n)\ny <- causal_coef * x + u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Draw DAG\n\n# specify edges\nmodel <- dagitty(\"dag{x->y; u->z; u->x; z->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=3, u=2, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- u + rnorm(n)\ncausal_coef <- 2\ny     <- causal_coef * x + z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# Model 1 \n\nmodel1 <- dagitty(\"dag{x->y; z->x; z->y}\")\n\n## coordinates for plotting\ncoordinates(model1) <-  list(\n  x = c(x=1, y=3, z=2),\n  y = c(x=1, y=1, z=2))\n\n\n\n# Model 2\n\n# specify edges\nmodel2 <- dagitty(\"dag{x->y; u->z; z->x; u->y}\")\n\n# set u as latent\nlatents(model2) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model2) <-  list(\n  x = c(x=1, z=2, u=3, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\n\n\n# Model 3\n\n# specify edges\nmodel3 <- dagitty(\"dag{x->y; u->z; u->x; z->y}\")\n\n# set u as latent\nlatents(model3) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model3) <-  list(\n  x = c(x=1, z=3, u=2, y = 4),\n  y = c(x=1, y=1, z=2, u = 3))\n\npar(mfrow=c(1,3))\n\n## ggplot\nggdag(model1) + theme_dag()\n\n## ggplot\nggdag(model2) + theme_dag()\n\n## ggplot\nggdag(model3) + theme_dag()"},{"path":"controls.html","id":"omitted-variable-bias-in-mediation-correction","chapter":"35 Controls","heading":"35.2.2 Omitted Variable Bias in Mediation Correction","text":"Common causes \\(X\\) mediator (\\(X\\) \\(Y\\)) confound effect \\(X\\) \\(Y\\)\\(Z\\) confounder mediator \\(M\\) \\(X\\)Table 35.11:  Table 17.2:  Table 35.12:  Summary","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->x; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, m=3, y=4),\n  y = c(x=1, z=2, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nz     <- rnorm(n)\nx     <- z + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + z + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->x; x->m; u->m; m->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, u=3, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- z + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + u + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; u->z; z->m; x->m; u->x; m->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=3, u=2, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn     <- 1e4\nu     <- rnorm(n)\nz     <- u + rnorm(n)\nx     <- u + rnorm(n)\ncausal_coef <- 2\nm     <- causal_coef * x + z + rnorm(n)\ny     <- m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# model 4\nmodel4 <- dagitty(\"dag{x->y; z->x; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model4) <-  list(\n  x = c(x=1, z=2, m=3, y=4),\n  y = c(x=1, z=2, m=1, y=1))\n\n\n# model 5\nmodel5 <- dagitty(\"dag{x->y; u->z; z->x; x->m; u->m; m->y}\")\n\n# set u as latent\nlatents(model5) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model5) <-  list(\n  x = c(x=1, z=2, u=3, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\n\n# model 6\n\nmodel6 <- dagitty(\"dag{x->y; u->z; z->m; x->m; u->x; m->y}\")\n\n# set u as latent\nlatents(model6) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model6) <-  list(\n  x = c(x=1, z=3, u=2, m=4, y=5),\n  y = c(x=1, z=2, u=3, m=1, y=1))\n\npar(mfrow=c(1,3))\n\n## ggplot\nggdag(model4) + theme_dag()\n\n## ggplot\nggdag(model5) + theme_dag()\n\n## ggplot\nggdag(model6) + theme_dag()"},{"path":"controls.html","id":"neutral-controls","chapter":"35 Controls","heading":"35.3 Neutral Controls","text":"","code":""},{"path":"controls.html","id":"good-predictive-controls","chapter":"35 Controls","heading":"35.3.1 Good Predictive Controls","text":"Good precisionControlling \\(Z\\) help hurt identification, can increase precision (.e., reducing SE)Table 35.13:  Similar coefficients, smaller SE controlling \\(Z\\)Another variation isTable 35.14:  Controlling \\(Z\\) can reduce SE","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->y}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\ny <- x + 2 * z + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->m; z->m; m->y}\")\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, m=2, y=3),\n  y = c(x=1, z=2, m=1, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- rnorm(n)\nm <- 2 * z + rnorm(n)\ny <- x + 2 * m + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"good-selection-bias","chapter":"35 Controls","heading":"35.3.2 Good Selection Bias","text":"Unadjusted estimate unbiasedControlling Z can increase SEControlling Z W can help identify XTable 35.15:  ","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z; z->w; u->w;u->y}\")\n\n# set u as latent\nlatents(model) <- \"u\"\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, w=3, u=3, y=5),\n  y = c(x=3, z=2, w=1, u=4, y=3))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nx <- rnorm(n)\nu <- rnorm(n)\nz <- x + rnorm(n)\nw <- z + u + rnorm(n)\ny <- x - 2*u + rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + w), lm(y ~ x + z + w))"},{"path":"controls.html","id":"bad-predictive-controls","chapter":"35 Controls","heading":"35.3.3 Bad Predictive Controls","text":"Table 35.16:  Similar coefficients, greater SE controlling \\(Z\\)Another variation isTable 35.17:  Worse SE controlling \\(Z\\) (\\(0.02 < 0.05\\))","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; z->x}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=1, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nn <- 1e4\nz <- rnorm(n)\nx <- 2 * z + rnorm(n)\ny <- x + 2 * rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=1, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()\nset.seed(1)\nn <- 1e4\nx <- rnorm(n)\nz <- 2 * x + rnorm(n)\ny <- x + 2 * rnorm(n)\n\njtools::export_summs(lm(y ~ x), lm(y ~ x + z))"},{"path":"controls.html","id":"bad-selection-bias","chapter":"35 Controls","heading":"35.3.4 Bad Selection Bias","text":"post-treatment variables bad.Controlling \\(Z\\) neutral, might hurt precision causal effect.","code":"\n# cleans workspace\nrm(list = ls())\n\n# DAG\n\n## specify edges\nmodel <- dagitty(\"dag{x->y; x->z}\")\n\n\n## coordinates for plotting\ncoordinates(model) <-  list(\n  x = c(x=1, z=2, y=2),\n  y = c(x=1, z=2, y=1))\n\n## ggplot\nggdag(model) + theme_dag()"},{"path":"controls.html","id":"choosing-controls","chapter":"35 Controls","heading":"35.4 Choosing Controls","text":"providing causal diagram, deciding appropriateness controls automated.FusionFusionDAGittyDAGittyGuide choose confounders: T. J. VanderWeele (2019)cases ’s hard determine plausibility controls, might need analysis.sensemakr provides tools.simple cases, can follow simple rules thumb provided Steinmetz Block (2022) (p. 614, Fig 2)","code":"\nlibrary(pcalg)\nlibrary(dagitty)\nlibrary(causaleffect)\nlibrary(sensemakr)"},{"path":"mediation.html","id":"mediation","chapter":"36 Mediation","heading":"36 Mediation","text":"","code":""},{"path":"mediation.html","id":"traditional-approach","chapter":"36 Mediation","heading":"36.1 Traditional Approach","text":"Baron Kenny (1986) outdated step 1, still see original idea.3 regressionsStep 1: \\(X \\Y\\)Step 1: \\(X \\Y\\)Step 2: \\(X \\M\\)Step 2: \\(X \\M\\)Step 3: \\(X + M \\Y\\)Step 3: \\(X + M \\Y\\)\\(X\\) = independent (causal) variable\\(X\\) = independent (causal) variable\\(Y\\) = dependent (outcome) variable\\(Y\\) = dependent (outcome) variable\\(M\\) = mediating variable\\(M\\) = mediating variableNote: Originally, first path \\(X \\Y\\) suggested (Baron Kenny 1986) needs significant. cases indirect \\(X\\) \\(Y\\) without significant direct effect \\(X\\) \\(Y\\) (e.g., effect absorbed M, two counteracting effects \\(M_1, M_2\\) cancel effect).\\(c\\) total effectwhere\\(c'\\) = direct effect (effect \\(X\\) \\(Y\\) accounting indirect path)\\(c'\\) = direct effect (effect \\(X\\) \\(Y\\) accounting indirect path)\\(ab\\) = indirect effect\\(ab\\) = indirect effectHence,\\[\n\\begin{aligned}\n\\text{total effect} &= \\text{direct effect} + \\text{indirect effect} \\\\\nc &= c' + ab\n\\end{aligned}\n\\]However, simple equation hold cases ofModels latent variablesLogistic models (approximately). Hence, can calculate \\(c\\) total effect \\(c' + ab\\)Multi-level models (Bauer, Preacher, Gil 2006)measure mediation (.e., indirect effect),\\(1 - \\frac{c'}{c}\\) highly unstable (D. P. MacKinnon, Warsi, Dwyer 1995), especially cases \\(c\\) small (re* recommended)Product method: \\(\\times b\\)Difference method: \\(c- c'\\)linear models, following assumptions:unmeasured confound \\(X-Y\\), \\(X-M\\) \\(M-Y\\) relationships.unmeasured confound \\(X-Y\\), \\(X-M\\) \\(M-Y\\) relationships.\\(X \\\\rightarrow C\\) \\(C\\) confounder \\(M-Y\\) relationship\\(X \\\\rightarrow C\\) \\(C\\) confounder \\(M-Y\\) relationshipReliability: errors measurement \\(M\\) (also known reliability assumption) (can consider errors--variables models)Reliability: errors measurement \\(M\\) (also known reliability assumption) (can consider errors--variables models)Mathematically,\\[\nY = b_0 + b_1 X + \\epsilon\n\\]\\(b_1\\) need significant.examine effect \\(X\\) \\(M\\). step requires significant effect \\(X\\) \\(M\\) continue analysisMathematically,\\[\nM = b_0 + b_2 X + \\epsilon\n\\]\\(b_2\\) needs significant.step, want effect \\(M\\) \\(Y\\) “absorbs” direct effect \\(X\\) \\(Y\\) (least makes effect smaller).Mathematically,\\[\nY = b_0 + b_4 X + b_3 M + \\epsilon\n\\]\\(b_4\\) needs either smaller insignificant.Examine mediation effect (.e., whether significant)Sobel Test (Sobel 1982)Sobel Test (Sobel 1982)Joint Significance TestJoint Significance TestBootstrapping Shrout Bolger (2002) (preferable)Bootstrapping Shrout Bolger (2002) (preferable)Notes:Proximal mediation (\\(> b\\)) can lead multicollinearity reduce statistical power, whereas distal mediation (\\(b > \\)) preferred maximizing test power.Proximal mediation (\\(> b\\)) can lead multicollinearity reduce statistical power, whereas distal mediation (\\(b > \\)) preferred maximizing test power.ideal balance maximizing power mediation analysis involves slightly distal mediators (.e., path \\(b\\) somewhat larger path \\(\\)) (Hoyle 1999).ideal balance maximizing power mediation analysis involves slightly distal mediators (.e., path \\(b\\) somewhat larger path \\(\\)) (Hoyle 1999).Tests direct effects (c c’) lower power compared indirect effect (ab), making possible ab significant c , even cases seems complete mediation statistical evidence direct cause-effect relationship X Y without considering M (Kenny Judd 2014).Tests direct effects (c c’) lower power compared indirect effect (ab), making possible ab significant c , even cases seems complete mediation statistical evidence direct cause-effect relationship X Y without considering M (Kenny Judd 2014).testing \\(ab\\) offers power advantage \\(c’\\) effectively combines two tests. However, claims complete mediation based solely non-significance \\(c’\\) approached caution, emphasizing need sufficient sample size power, especially assessing partial mediation. one never make complete mediation claim (Hayes Scharkow 2013)testing \\(ab\\) offers power advantage \\(c’\\) effectively combines two tests. However, claims complete mediation based solely non-significance \\(c’\\) approached caution, emphasizing need sufficient sample size power, especially assessing partial mediation. one never make complete mediation claim (Hayes Scharkow 2013)","code":""},{"path":"mediation.html","id":"assumptions-3","chapter":"36 Mediation","heading":"36.1.1 Assumptions","text":"","code":""},{"path":"mediation.html","id":"direction","chapter":"36 Mediation","heading":"36.1.1.1 Direction","text":"Quick fix convincing: Measure \\(X\\) \\(M\\) \\(Y\\) prevent \\(M\\) \\(Y\\) causing \\(X\\); measure \\(M\\) \\(Y\\) avoid \\(Y\\) causing \\(M\\).Quick fix convincing: Measure \\(X\\) \\(M\\) \\(Y\\) prevent \\(M\\) \\(Y\\) causing \\(X\\); measure \\(M\\) \\(Y\\) avoid \\(Y\\) causing \\(M\\).\\(Y\\) may cause \\(M\\) feedback model.\nAssuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.\nE. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .\n\\(Y\\) may cause \\(M\\) feedback model.Assuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.Assuming \\(c' =0\\) (full mediation) allows estimating models reciprocal causal effects \\(M\\) \\(Y\\) via IV estimation.E. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .E. R. Smith (1982) proposes treating \\(M\\) \\(Y\\) outcomes potential mediate , requiring distinct instrumental variables affect .","code":""},{"path":"mediation.html","id":"interaction","chapter":"36 Mediation","heading":"36.1.1.2 Interaction","text":"M interact X affect Y, M mediator mediator (Baron Kenny 1986).M interact X affect Y, M mediator mediator (Baron Kenny 1986).Interaction \\(XM\\) always estimated.Interaction \\(XM\\) always estimated.interpretation interaction, see (T. VanderWeele 2015)interpretation interaction, see (T. VanderWeele 2015)","code":""},{"path":"mediation.html","id":"reliability","chapter":"36 Mediation","heading":"36.1.1.3 Reliability","text":"mediator contains measurement errors, \\(b, c'\\) biased. Possible fix: mediator = latent variable (loss power) (Ledgerwood Shrout 2011)\n\\(b\\) attenuated (closer 0)\n\\(c'\\) \noverestimated \\(ab >0\\)\nunderestiamted \\(ab<0\\)\n\nmediator contains measurement errors, \\(b, c'\\) biased. Possible fix: mediator = latent variable (loss power) (Ledgerwood Shrout 2011)\\(b\\) attenuated (closer 0)\\(b\\) attenuated (closer 0)\\(c'\\) \noverestimated \\(ab >0\\)\nunderestiamted \\(ab<0\\)\n\\(c'\\) isoverestimated \\(ab >0\\)overestimated \\(ab >0\\)underestiamted \\(ab<0\\)underestiamted \\(ab<0\\)treatment contains measurement errors, \\(,b\\) biased\n\\(\\) attenuated\n\\(b\\) \noverestimated \\(ac'>0\\)\nunderestimated \\(ac' <0\\)\n\ntreatment contains measurement errors, \\(,b\\) biased\\(\\) attenuated\\(\\) attenuated\\(b\\) \noverestimated \\(ac'>0\\)\nunderestimated \\(ac' <0\\)\n\\(b\\) isoverestimated \\(ac'>0\\)overestimated \\(ac'>0\\)underestimated \\(ac' <0\\)underestimated \\(ac' <0\\)outcome contains measurement errors,\nunstandardized, bias\nstandardized, attenuation bias\noutcome contains measurement errors,unstandardized, biasIf unstandardized, biasIf standardized, attenuation biasIf standardized, attenuation bias","code":""},{"path":"mediation.html","id":"confounding","chapter":"36 Mediation","heading":"36.1.1.4 Confounding","text":"Omitted variable bias can happen pair relationshipsOmitted variable bias can happen pair relationshipsTo deal problem, one can either use\nDesign Strategies\nStatistical Strategies\ndeal problem, one can either useDesign StrategiesDesign StrategiesStatistical StrategiesStatistical Strategies","code":""},{"path":"mediation.html","id":"design-strategies","chapter":"36 Mediation","heading":"36.1.1.4.1 Design Strategies","text":"Randomization treatment variable. possible, also mediatorRandomization treatment variable. possible, also mediatorControl confounder (still measureable observables)Control confounder (still measureable observables)","code":""},{"path":"mediation.html","id":"statistical-strategies","chapter":"36 Mediation","heading":"36.1.1.4.2 Statistical Strategies","text":"Instrumental variable treatment\nSpecifically confounder affecting \\(M-Y\\) pair, front-door adjustment possible variable completely mediates effect mediator outcome unaffected confounder.\nInstrumental variable treatmentSpecifically confounder affecting \\(M-Y\\) pair, front-door adjustment possible variable completely mediates effect mediator outcome unaffected confounder.Weighting methods (e.g., inverse propensity) See Heiss R code\nNeed strong ignorability assumption (.e.., confounders included measured without error (Westfall Yarkoni 2016)). fixable, can examined robustness checks.\nWeighting methods (e.g., inverse propensity) See Heiss R codeNeed strong ignorability assumption (.e.., confounders included measured without error (Westfall Yarkoni 2016)). fixable, can examined robustness checks.","code":""},{"path":"mediation.html","id":"indirect-effect-tests","chapter":"36 Mediation","heading":"36.1.2 Indirect Effect Tests","text":"","code":""},{"path":"mediation.html","id":"sobel-test","chapter":"36 Mediation","heading":"36.1.2.1 Sobel Test","text":"developed Sobel (1982)developed Sobel (1982)also known delta methodalso known delta methodnot recommend assumes indirect effect \\(b\\) normal distribution ’s (D. P. MacKinnon, Warsi, Dwyer 1995).recommend assumes indirect effect \\(b\\) normal distribution ’s (D. P. MacKinnon, Warsi, Dwyer 1995).Mediation can occur even direct indirect effects oppose , termed “inconsistent mediation” (D. P. MacKinnon, Fairchild, Fritz 2007). mediator acts suppressor variable.Mediation can occur even direct indirect effects oppose , termed “inconsistent mediation” (D. P. MacKinnon, Fairchild, Fritz 2007). mediator acts suppressor variable.Standard Error\\[\n\\sqrt{\\hat{b}^2 s_{\\hat{}} + \\hat{}^2 s_{b}^2}\n\\]test indirect effect \\[\nz = \\frac{\\hat{ab}}{\\sqrt{\\hat{b}^2 s_{\\hat{}} + \\hat{}^2 s_{b}^2}}\n\\]DisadvantagesAssume \\(\\) \\(b\\) independent.Assume \\(\\) \\(b\\) independent.Assume \\(ab\\) normally distributed.Assume \\(ab\\) normally distributed.work well small sample sizes.work well small sample sizes.Power test low test conservative compared Bootstrapping.Power test low test conservative compared Bootstrapping.","code":""},{"path":"mediation.html","id":"joint-significance-test","chapter":"36 Mediation","heading":"36.1.2.2 Joint Significance Test","text":"Effective determining indirect effect nonzero (testing whether \\(\\) \\(b\\) statistically significant), assumes \\(\\perp b\\).Effective determining indirect effect nonzero (testing whether \\(\\) \\(b\\) statistically significant), assumes \\(\\perp b\\).’s recommended use tests similar performance Bootstrapping test (Hayes Scharkow 2013).’s recommended use tests similar performance Bootstrapping test (Hayes Scharkow 2013).test’s accuracy can affected heteroscedasticity (Fossum Montoya 2023) non-normality.test’s accuracy can affected heteroscedasticity (Fossum Montoya 2023) non-normality.Although helpful computing power test indirect effect, doesn’t provide confidence interval effect.Although helpful computing power test indirect effect, doesn’t provide confidence interval effect.","code":""},{"path":"mediation.html","id":"bootstrapping","chapter":"36 Mediation","heading":"36.1.2.3 Bootstrapping","text":"First used Bollen Stine (1990)First used Bollen Stine (1990)allows calculation confidence intervals, p-values, etc.allows calculation confidence intervals, p-values, etc.require \\(\\perp b\\) corrects bias bootstrapped distribution.require \\(\\perp b\\) corrects bias bootstrapped distribution.can handle non-normality (sampling distribution indirect effect), complex models, small samples.can handle non-normality (sampling distribution indirect effect), complex models, small samples.Concerns exist bias-corrected bootstrapping liberal (Fritz, Taylor, MacKinnon 2012). Hence, current recommendations favor percentile bootstrap without bias correction better Type error rates (Tibbe Montoya 2022).Concerns exist bias-corrected bootstrapping liberal (Fritz, Taylor, MacKinnon 2012). Hence, current recommendations favor percentile bootstrap without bias correction better Type error rates (Tibbe Montoya 2022).special case bootstrapping proposed don’t need access raw data generate resampling, need \\(, b, var(), var(b), cov(,b)\\) (can taken lots primary studies)special case bootstrapping proposed don’t need access raw data generate resampling, need \\(, b, var(), var(b), cov(,b)\\) (can taken lots primary studies)","code":"\nresult <-\n    causalverse::med_ind(\n        a = 0.5,\n        b = 0.7,\n        var_a = 0.04,\n        var_b = 0.05,\n        cov_ab = 0.01\n    )\nresult$plot"},{"path":"mediation.html","id":"with-instrument","chapter":"36 Mediation","heading":"36.1.2.3.1 With Instrument","text":"Alternatively, one can use robmed packagePower test use app","code":"\nlibrary(DiagrammeR)\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label = 'Treatment']\n    Y [label = 'Outcome']\n  edge [minlen = 2]\n    X->Y\n  { rank = same; X; Y }\n}\n\")\n\ngrViz(\"\ndigraph {\n  graph []\n  node [shape = plaintext]\n    X [label ='Treatment', shape = box]\n    Y [label ='Outcome', shape = box]\n    M [label ='Mediator', shape = box]\n    IV [label ='Instrument', shape = box]\n  edge [minlen = 2]\n    IV->X\n    X->M  \n    M->Y \n    X->Y \n  { rank = same; X; Y; M }\n}\n\")\nlibrary(mediation)\ndata(\"boundsdata\")\nlibrary(fixest)\n\n# Total Effect\nout1 <- feols(out ~ ttt, data = boundsdata)\n\n# Indirect Effect\nout2 <- feols(med ~ ttt, data = boundsdata)\n\n# Direct and Indirect Effect\nout3 <- feols(out ~ med + ttt, data = boundsdata)\n\n# Proportion Test\n# To what extent is the effect of the treatment mediated by the mediator?\ncoef(out2)['ttt'] * coef(out3)['med'] / coef(out1)['ttt'] * 100\n#>      ttt \n#> 68.63609\n\n\n# Sobel Test\nbda::mediation.test(boundsdata$med, boundsdata$ttt, boundsdata$out) |> \n    tibble::rownames_to_column() |> \n    causalverse::nice_tab(2)\n#>   rowname Sobel Aroian Goodman\n#> 1 z.value  4.05   4.03    4.07\n#> 2 p.value  0.00   0.00    0.00\n# Mediation Analysis using boot\nlibrary(boot)\nset.seed(1)\nmediation_fn <- function(data, i){\n    # sample the dataset\n    df <- data[i,]\n    \n    \n    a_path <- feols(med ~ ttt, data = df)\n    a <- coef(a_path)['ttt']\n    \n    b_path <-  feols(out ~ med + ttt, data = df)\n    b <- coef(b_path)['med']\n    \n    cp <- coef(b_path)['ttt']\n    \n    # indirect effect\n    ind_ef <- a*b\n    total_ef <- a*b + cp\n    return(c(ind_ef, total_ef))\n    \n}\n\nboot_med <- boot(boundsdata, mediation_fn, R = 100, parallel = \"multicore\", ncpus = 2)\nboot_med \n#> \n#> ORDINARY NONPARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> boot(data = boundsdata, statistic = mediation_fn, R = 100, parallel = \"multicore\", \n#>     ncpus = 2)\n#> \n#> \n#> Bootstrap Statistics :\n#>       original        bias    std. error\n#> t1* 0.04112035  0.0006346725 0.009539903\n#> t2* 0.05991068 -0.0004462572 0.029556611\n\nsummary(boot_med) |> \n    causalverse::nice_tab()\n#>     R original bootBias bootSE bootMed\n#> 1 100     0.04        0   0.01    0.04\n#> 2 100     0.06        0   0.03    0.06\n\n# confidence intervals (percentile is always recommended)\nboot.ci(boot_med, type = c(\"norm\", \"perc\"))\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#> Based on 100 bootstrap replicates\n#> \n#> CALL : \n#> boot.ci(boot.out = boot_med, type = c(\"norm\", \"perc\"))\n#> \n#> Intervals : \n#> Level      Normal             Percentile     \n#> 95%   ( 0.0218,  0.0592 )   ( 0.0249,  0.0623 )  \n#> Calculations and Intervals on Original Scale\n#> Some percentile intervals may be unstable\n\n# point estimates (Indirect, and Total Effects)\ncolMeans(boot_med$t)\n#> [1] 0.04175502 0.05946442\nlibrary(robmed)\nlibrary(pwr2ppl)\n\n# indirect path ab power\nmedjs(\n    # X on M (path a)\n    rx1m1 = .3,\n    # correlation between X and Y (path c')\n    rx1y  = .1,\n    # correlation between M and Y (path b)\n    rym1  = .3,\n    # sample size\n    n     = 100,\n    alpha = 0.05,\n    # number of mediators\n    mvars = 1,\n    # should use 10000\n    rep   = 1000\n)"},{"path":"mediation.html","id":"multiple-mediation","chapter":"36 Mediation","heading":"36.1.3 Multiple Mediation","text":"general package handle multiple cases manymomeSee vignette example","code":"\nlibrary(manymome)"},{"path":"mediation.html","id":"multiple-mediators","chapter":"36 Mediation","heading":"36.1.3.1 Multiple Mediators","text":"NotesNotesVignetteVignettePackagePackage","code":"\nlibrary(mma)"},{},{"path":"mediation.html","id":"multiple-treatments-1","chapter":"36 Mediation","heading":"36.1.3.2 Multiple Treatments","text":"(Hayes Preacher 2014)Code Process","code":""},{"path":"mediation.html","id":"causal-inference-approach","chapter":"36 Mediation","heading":"36.2 Causal Inference Approach","text":"","code":""},{"path":"mediation.html","id":"example-1-mediation-traditional","chapter":"36 Mediation","heading":"36.2.1 Example 1","text":"Virginia’s libraryTotal Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Total Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565Using mediation package suggested Imai, Keele, Yamamoto (2010). details package can found here2 types Inference package:Model-based inference:\nAssumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\n\nModel-based inference:Assumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\nAssumptions:Treatment randomized (use matching methods achieve ).Treatment randomized (use matching methods achieve ).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Design-based inferenceDesign-based inferenceNotations: stay consistent package instruction\\(M_i(t)\\) = mediator\\(M_i(t)\\) = mediator\\(T_i\\) = treatment status \\((0,1)\\)\\(T_i\\) = treatment status \\((0,1)\\)\\(Y_i(t,m)\\) = outcome \\(t\\) = treatment, \\(m\\) = mediating variables.\\(Y_i(t,m)\\) = outcome \\(t\\) = treatment, \\(m\\) = mediating variables.\\(X_i\\) = vector observed pre-treatment confounders\\(X_i\\) = vector observed pre-treatment confoundersTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effects\nCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)\nDirect effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)\nsumming treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)\nTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effectsCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Causal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)sequential ignorability\\[\n\\{ Y_i (t', m) , M_i (t) \\} \\perp T_i |X_i = x\n\\]\\[\nY_i(t',m) \\perp M_i(t) | T_i = t, X_i = x\n\\]\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)First condition standard strong ignorability condition treatment assignment random conditional pre-treatment confounders.Second condition stronger mediators also random given observed treatment pre-treatment confounders. condition satisfied unobserved pre-treatment confounders, post-treatment confounders, multiple mediators correlated.understanding moment write note, way test sequential ignorability assumption. Hence, researchers can sensitivity analysis argue result.","code":"\nmyData <-\n    read.csv('http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv')\n\n# Step 1 (no longer necessary)\nmodel.0 <- lm(Y ~ X, myData)\nsummary(model.0)\n#> \n#> Call:\n#> lm(formula = Y ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.0262 -1.2340 -0.3282  1.5583  5.1622 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.8572     0.6932   4.122 7.88e-05 ***\n#> X             0.3961     0.1112   3.564 0.000567 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.929 on 98 degrees of freedom\n#> Multiple R-squared:  0.1147, Adjusted R-squared:  0.1057 \n#> F-statistic:  12.7 on 1 and 98 DF,  p-value: 0.0005671\n\n# Step 2\nmodel.M <- lm(M ~ X, myData)\nsummary(model.M)\n#> \n#> Call:\n#> lm(formula = M ~ X, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3046 -0.8656  0.1344  1.1344  4.6954 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  1.49952    0.58920   2.545   0.0125 *  \n#> X            0.56102    0.09448   5.938 4.39e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.639 on 98 degrees of freedom\n#> Multiple R-squared:  0.2646, Adjusted R-squared:  0.2571 \n#> F-statistic: 35.26 on 1 and 98 DF,  p-value: 4.391e-08\n\n# Step 3\nmodel.Y <- lm(Y ~ X + M, myData)\nsummary(model.Y)\n#> \n#> Call:\n#> lm(formula = Y ~ X + M, data = myData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.7631 -1.2393  0.0308  1.0832  4.0055 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.9043     0.6055   3.145   0.0022 ** \n#> X             0.0396     0.1096   0.361   0.7187    \n#> M             0.6355     0.1005   6.321 7.92e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.631 on 97 degrees of freedom\n#> Multiple R-squared:  0.373,  Adjusted R-squared:  0.3601 \n#> F-statistic: 28.85 on 2 and 97 DF,  p-value: 1.471e-10\n\n# Step 4 (boostrapping)\nlibrary(mediation)\nresults <- mediate(\n    model.M,\n    model.Y,\n    treat = 'X',\n    mediator = 'M',\n    boot = TRUE,\n    sims = 500\n)\nsummary(results)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the Percentile Method\n#> \n#>                Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME             0.3565       0.2119         0.51  <2e-16 ***\n#> ADE              0.0396      -0.1750         0.28   0.760    \n#> Total Effect     0.3961       0.1743         0.64   0.004 ** \n#> Prop. Mediated   0.9000       0.5042         1.94   0.004 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 100 \n#> \n#> \n#> Simulations: 500"},{"path":"mediation.html","id":"model-based-causal-mediation-analysis","chapter":"36 Mediation","heading":"36.3 Model-based causal mediation analysis","text":"resources:hereFit 2 modelsmediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)mediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)mediation can accommodate almost types model mediator model outcome model except Censored mediator model.update estimation ACME rely product difference coefficients (see 36.2.1 ,requires strict assumption: (1) linear regression models mediator outcome, (2) \\(T_i\\) \\(M_i\\) effects additive interactionNonparametric bootstrap versionIf theoretically understanding suggests treatment mediator interactionmediation can used conjunction imputation packages.can also handle mediated moderation non-binary treatment variables, multi-level dataSensitivity Analysis sequential ignorabilitytest unobserved pre-treatment covariatestest unobserved pre-treatment covariates\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).ACME confidence intervals contains 0 \\(\\rho \\(0.3,0.4)\\)Alternatively, using \\(R^2\\) interpretation, need specify direction confounder affects mediator outcome variables plot using sign.prod = \"positive\" (.e., direction) sign.prod = \"negative\" (.e., opposite direction).","code":"\nlibrary(mediation)\nset.seed(2014)\ndata(\"framing\", package = \"mediation\")\n\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\n\n# Quasi-Bayesian Monte Carlo \nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100 # should be 10000 in practice\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0791       0.0351         0.15  <2e-16 ***\n#> ACME (treated)             0.0804       0.0367         0.16  <2e-16 ***\n#> ADE (control)              0.0206      -0.0976         0.12    0.70    \n#> ADE (treated)              0.0218      -0.1053         0.12    0.70    \n#> Total Effect               0.1009      -0.0497         0.23    0.14    \n#> Prop. Mediated (control)   0.6946      -6.3109         3.68    0.14    \n#> Prop. Mediated (treated)   0.7118      -5.7936         3.50    0.14    \n#> ACME (average)             0.0798       0.0359         0.15  <2e-16 ***\n#> ADE (average)              0.0212      -0.1014         0.12    0.70    \n#> Prop. Mediated (average)   0.7032      -6.0523         3.59    0.14    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        boot = TRUE,\n        treat = \"treat\",\n        mediator = \"emo\",\n        sims = 100, # should be 10000 in practice\n        boot.ci.type = \"bca\" # bias-corrected and accelerated intervals\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Nonparametric Bootstrap Confidence Intervals with the BCa Method\n#> \n#>                          Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.0848       0.0424         0.14  <2e-16 ***\n#> ACME (treated)             0.0858       0.0410         0.14  <2e-16 ***\n#> ADE (control)              0.0117      -0.0726         0.13    0.58    \n#> ADE (treated)              0.0127      -0.0784         0.14    0.58    \n#> Total Effect               0.0975       0.0122         0.25    0.06 .  \n#> Prop. Mediated (control)   0.8698       1.7460       151.20    0.06 .  \n#> Prop. Mediated (treated)   0.8804       1.6879       138.91    0.06 .  \n#> ACME (average)             0.0853       0.0434         0.14  <2e-16 ***\n#> ADE (average)              0.0122      -0.0756         0.14    0.58    \n#> Prop. Mediated (average)   0.8751       1.7170       145.05    0.06 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo * treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsummary(med.out)\n#> \n#> Causal Mediation Analysis \n#> \n#> Quasi-Bayesian Confidence Intervals\n#> \n#>                           Estimate 95% CI Lower 95% CI Upper p-value    \n#> ACME (control)             0.07417      0.02401         0.14  <2e-16 ***\n#> ACME (treated)             0.09496      0.02702         0.16  <2e-16 ***\n#> ADE (control)             -0.01353     -0.11855         0.11    0.76    \n#> ADE (treated)              0.00726     -0.11007         0.11    0.90    \n#> Total Effect               0.08143     -0.05646         0.19    0.26    \n#> Prop. Mediated (control)   0.64510    -14.31243         3.13    0.26    \n#> Prop. Mediated (treated)   0.98006    -17.83202         4.01    0.26    \n#> ACME (average)             0.08457      0.02738         0.15  <2e-16 ***\n#> ADE (average)             -0.00314     -0.11457         0.12    1.00    \n#> Prop. Mediated (average)   0.81258    -16.07223         3.55    0.26    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Sample Size Used: 265 \n#> \n#> \n#> Simulations: 100\n\ntest.TMint(med.out, conf.level = .95) # test treatment-mediator interaction effect \n#> \n#>  Test of ACME(1) - ACME(0) = 0\n#> \n#> data:  estimates from med.out\n#> ACME(1) - ACME(0) = 0.020796, p-value = 0.3\n#> alternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.01757310  0.07110837\nplot(med.out)\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsens.out <-\n    medsens(med.out,\n            rho.by = 0.1, # \\rho varies from -0.9 to 0.9 by 0.1\n            effect.type = \"indirect\", # sensitivity on ACME\n            # effect.type = \"direct\", # sensitivity on ADE\n            # effect.type = \"both\", # sensitivity on ACME and ADE\n            sims = 100)\nsummary(sens.out)\n#> \n#> Mediation Sensitivity Analysis: Average Mediation Effect\n#> \n#> Sensitivity Region: ACME for Control Group\n#> \n#>      Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n#> [1,] 0.3        0.0062      -0.0073       0.0188         0.09       0.0493\n#> [2,] 0.4       -0.0084      -0.0238       0.0017         0.16       0.0877\n#> \n#> Rho at which ACME for Control Group = 0: 0.3\n#> R^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\n#> R^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 \n#> \n#> \n#> Sensitivity Region: ACME for Treatment Group\n#> \n#>      Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n#> [1,] 0.3        0.0071      -0.0092       0.0213         0.09       0.0493\n#> [2,] 0.4       -0.0101      -0.0295       0.0023         0.16       0.0877\n#> \n#> Rho at which ACME for Treatment Group = 0: 0.3\n#> R^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09\n#> R^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493\nplot(sens.out, sens.par = \"rho\", main = \"Anxiety\", ylim = c(-0.2, 0.2))\nplot(sens.out, sens.par = \"R2\", r.type = \"total\", sign.prod = \"positive\")"},{"path":"directed-acyclic-graph.html","id":"directed-acyclic-graph","chapter":"37 Directed Acyclic Graph","heading":"37 Directed Acyclic Graph","text":"Native R:dagittydagittyggdagggdagdagRdagRr-causal: Center Causal Discovery. Also available Pythonr-causal: Center Causal Discovery. Also available PythonPublication-ready (R Latex): shinyDAGStandalone program: DAG program Sven Knuppel","code":""},{"path":"directed-acyclic-graph.html","id":"basic-notations","chapter":"37 Directed Acyclic Graph","heading":"37.1 Basic Notations","text":"Basic building blocks DAGMediators (chains): \\(X \\Z \\Y\\)\ncontrolling Z blocks (closes) causal impact \\(X \\Y\\)\nMediators (chains): \\(X \\Z \\Y\\)controlling Z blocks (closes) causal impact \\(X \\Y\\)Common causes (forks): \\(X \\leftarrow Z \\Y\\)\nZ (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).\nControlling \\(Z\\) close association.\n\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.\nCommon causes (forks): \\(X \\leftarrow Z \\Y\\)Z (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).Z (.e., confounder) common cause induces non-causal association \\(X\\) \\(Y\\).Controlling \\(Z\\) close association.Controlling \\(Z\\) close association.\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.\\(Z\\) d-separates \\(X\\) \\(Y\\) blocks (closes) paths \\(X\\) \\(Y\\) (.e., \\(X \\perp Y |Z\\)). applies common causes mediators.Common effects (colliders): \\(X \\Z \\leftarrow Y\\)\ncontrolling \\(Z\\) induce association \\(X\\) \\(Y\\)\nControlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)\nCommon effects (colliders): \\(X \\Z \\leftarrow Y\\)controlling \\(Z\\) induce association \\(X\\) \\(Y\\)controlling \\(Z\\) induce association \\(X\\) \\(Y\\)Controlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)Controlling \\(Z\\) induces non-causal association \\(X\\) \\(Y\\)Notes:descendant variable behavior similarly variable (e.g., descendant \\(Z\\) can behave like \\(Z\\) partially control \\(Z\\))descendant variable behavior similarly variable (e.g., descendant \\(Z\\) can behave like \\(Z\\) partially control \\(Z\\))Rule thumb multiple Controls: o Causal inference \\(X \\Y\\), must\nClose backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)\nclose causal path \\(X\\) \\(Y\\) (mediators).\nRule thumb multiple Controls: o Causal inference \\(X \\Y\\), mustClose backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)Close backdoor path \\(X\\) \\(Y\\) (eliminate spurious correlation)close causal path \\(X\\) \\(Y\\) (mediators).close causal path \\(X\\) \\(Y\\) (mediators).","code":""},{"path":"report.html","id":"report","chapter":"38 Report","heading":"38 Report","text":"StructureExploratory analysis\nplots\npreliminary results\ninteresting structure/features data\noutliers\nExploratory analysisplotspreliminary resultsinteresting structure/features dataoutliersModel\nAssumptions\nmodel/ model best one?\nConsideration: interactions, collinearity, dependence\nModelAssumptionsWhy model/ model best one?Consideration: interactions, collinearity, dependenceModel Fit\nwell fit?\nmodel assumptions met?\nResidual analysis\n\nModel FitHow well fit?well fit?model assumptions met?\nResidual analysis\nmodel assumptions met?Residual analysisInference/ Prediction\ndifferent way support inference?\nInference/ PredictionAre different way support inference?Conclusion\nRecommendation\nLimitation analysis\ncorrect future\nConclusionRecommendationRecommendationLimitation analysisLimitation analysisHow correct futureHow correct futureThis chapter based jtools package. information can found .","code":""},{"path":"report.html","id":"one-summary-table","chapter":"38 Report","heading":"38.1 One summary table","text":"Packages reporting:Summary Statistics Table:qwraps2vtablegtsummaryapaTablesstargazerRegression TablegtsummarysjPlot,sjmisc, sjlabelledstargazer: recommended (Example)modelsummaryModel Equation","code":"\nlibrary(jtools)\ndata(movies)\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\nsumm(fit)\nsumm(\n    fit,\n    scale = TRUE,\n    vifs = TRUE,\n    part.corr = TRUE,\n    confint = TRUE,\n    pvals = FALSE\n) # notice that scale here is TRUE\n\n#obtain clsuter-robust SE\ndata(\"PetersenCL\", package = \"sandwich\")\nfit2 <- lm(y ~ x, data = PetersenCL)\nsumm(fit2, robust = \"HC3\", cluster = \"firm\") \n# install.packages(\"equatiomatic\") # not available for R 4.2\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\n# show the theoretical model\nequatiomatic::extract_eq(fit)\n# display the actual coefficients\nequatiomatic::extract_eq(fit, use_coefs = TRUE)"},{"path":"report.html","id":"model-comparison","chapter":"38 Report","heading":"38.2 Model Comparison","text":"Table 35.1:  Another package modelsummaryAnother package stargazerCorrelation Table","code":"\nfit <- lm(metascore ~ log(budget), data = movies)\nfit_b <- lm(metascore ~ log(budget) + log(us_gross), data = movies)\nfit_c <- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies)\ncoef_names <- c(\"Budget\" = \"log(budget)\", \"US Gross\" = \"log(us_gross)\",\n                \"Runtime (Hours)\" = \"runtime\", \"Constant\" = \"(Intercept)\")\nexport_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nlibrary(modelsummary)\nlm_mod <- lm(mpg ~ wt + hp + cyl, mtcars)\nmsummary(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nmodelplot(lm_mod, vcov = c(\"iid\",\"robust\",\"HC4\"))\nlibrary(\"stargazer\")\nstargazer(attitude)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM\n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lccccc} \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#> Statistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Max} \\\\ \n#> \\hline \\\\[-1.8ex] \n#> rating & 30 & 64.633 & 12.173 & 40 & 85 \\\\ \n#> complaints & 30 & 66.600 & 13.315 & 37 & 90 \\\\ \n#> privileges & 30 & 53.133 & 12.235 & 30 & 83 \\\\ \n#> learning & 30 & 56.367 & 11.737 & 34 & 75 \\\\ \n#> raises & 30 & 64.633 & 10.397 & 43 & 88 \\\\ \n#> critical & 30 & 74.767 & 9.895 & 49 & 92 \\\\ \n#> advance & 30 & 42.933 & 10.289 & 25 & 72 \\\\ \n#> \\hline \\\\[-1.8ex] \n#> \\end{tabular} \n#> \\end{table}\n## 2 OLS models\nlinear.1 <-\n    lm(rating ~ complaints + privileges + learning + raises + critical,\n       data = attitude)\nlinear.2 <-\n    lm(rating ~ complaints + privileges + learning, data = attitude)\n## create an indicator dependent variable, and run a probit model\nattitude$high.rating <- (attitude$rating > 70)\n\nprobit.model <-\n    glm(\n        high.rating ~ learning + critical + advance,\n        data = attitude,\n        family = binomial(link = \"probit\")\n    )\nstargazer(linear.1,\n          linear.2,\n          probit.model,\n          title = \"Results\",\n          align = TRUE)\n#> \n#> % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n#> % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM\n#> % Requires LaTeX packages: dcolumn \n#> \\begin{table}[!htbp] \\centering \n#>   \\caption{Results} \n#>   \\label{} \n#> \\begin{tabular}{@{\\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } \n#> \\\\[-1.8ex]\\hline \n#> \\hline \\\\[-1.8ex] \n#>  & \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ \n#> \\cline{2-4} \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{rating} & \\multicolumn{1}{c}{high.rating} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{2}{c}{\\textit{OLS}} & \\multicolumn{1}{c}{\\textit{probit}} \\\\ \n#> \\\\[-1.8ex] & \\multicolumn{1}{c}{(1)} & \\multicolumn{1}{c}{(2)} & \\multicolumn{1}{c}{(3)}\\\\ \n#> \\hline \\\\[-1.8ex] \n#>  complaints & 0.692^{***} & 0.682^{***} &  \\\\ \n#>   & (0.149) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  privileges & -0.104 & -0.103 &  \\\\ \n#>   & (0.135) & (0.129) &  \\\\ \n#>   & & & \\\\ \n#>  learning & 0.249 & 0.238^{*} & 0.164^{***} \\\\ \n#>   & (0.160) & (0.139) & (0.053) \\\\ \n#>   & & & \\\\ \n#>  raises & -0.033 &  &  \\\\ \n#>   & (0.202) &  &  \\\\ \n#>   & & & \\\\ \n#>  critical & 0.015 &  & -0.001 \\\\ \n#>   & (0.147) &  & (0.044) \\\\ \n#>   & & & \\\\ \n#>  advance &  &  & -0.062 \\\\ \n#>   &  &  & (0.042) \\\\ \n#>   & & & \\\\ \n#>  Constant & 11.011 & 11.258 & -7.476^{**} \\\\ \n#>   & (11.704) & (7.318) & (3.570) \\\\ \n#>   & & & \\\\ \n#> \\hline \\\\[-1.8ex] \n#> Observations & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} \\\\ \n#> R$^{2}$ & \\multicolumn{1}{c}{0.715} & \\multicolumn{1}{c}{0.715} &  \\\\ \n#> Adjusted R$^{2}$ & \\multicolumn{1}{c}{0.656} & \\multicolumn{1}{c}{0.682} &  \\\\ \n#> Log Likelihood &  &  & \\multicolumn{1}{c}{-9.087} \\\\ \n#> Akaike Inf. Crit. &  &  & \\multicolumn{1}{c}{26.175} \\\\ \n#> Residual Std. Error & \\multicolumn{1}{c}{7.139 (df = 24)} & \\multicolumn{1}{c}{6.863 (df = 26)} &  \\\\ \n#> F Statistic & \\multicolumn{1}{c}{12.063$^{***}$ (df = 5; 24)} & \\multicolumn{1}{c}{21.743$^{***}$ (df = 3; 26)} &  \\\\ \n#> \\hline \n#> \\hline \\\\[-1.8ex] \n#> \\textit{Note:}  & \\multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n#> \\end{tabular} \n#> \\end{table}\n# Latex\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\n# ASCII text output\nstargazer(\n    linear.1,\n    linear.2,\n    type = \"text\",\n    title = \"Regression Results\",\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    ci = TRUE,\n    ci.level = 0.90,\n    single.row = TRUE\n)\n#> \n#> Regression Results\n#> ========================================================================\n#>                                        Dependent variable:              \n#>                          -----------------------------------------------\n#>                                          Overall Rating                 \n#>                                    (1)                     (2)          \n#> ------------------------------------------------------------------------\n#> Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)\n#> No Special Privileges    -0.104 (-0.325, 0.118)  -0.103 (-0.316, 0.109) \n#> Opportunity to Learn      0.249 (-0.013, 0.512)   0.238* (0.009, 0.467) \n#> Performance-Based Raises -0.033 (-0.366, 0.299)                         \n#> Too Critical              0.015 (-0.227, 0.258)                         \n#> Advancement              11.011 (-8.240, 30.262) 11.258 (-0.779, 23.296)\n#> ------------------------------------------------------------------------\n#> Observations                       30                      30           \n#> R2                                0.715                   0.715         \n#> Adjusted R2                       0.656                   0.682         \n#> ========================================================================\n#> Note:                                        *p<0.1; **p<0.05; ***p<0.01\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\ncorrelation.matrix <-\n    cor(attitude[, c(\"rating\", \"complaints\", \"privileges\")])\nstargazer(correlation.matrix, title = \"Correlation Matrix\")"},{"path":"report.html","id":"changes-in-an-estimate","chapter":"38 Report","heading":"38.3 Changes in an estimate","text":"","code":"\ncoef_names <- coef_names[1:3] # Dropping intercept for plots\nplot_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nplot_summs(\n    fit_c,\n    robust = \"HC3\",\n    coefs = coef_names,\n    plot.distributions = TRUE\n)"},{"path":"report.html","id":"standard-errors-3","chapter":"38 Report","heading":"38.4 Standard Errors","text":"sandwich vignetteHeterogeneityWhite’s estimatorAll heterogeneity SE methods derivatives .small sample bias adjustmentUses degrees freedom-based correctionWhen number clusters small, HC2 HC3 better (Cameron, Gelbach, Miller 2008)Better linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrixBetter linear model, still applicable Generalized Linear ModelsNeeds hat (weighted) matrix","code":"\ndata(cars)\nmodel <- lm(speed ~ dist, data = cars)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = speed ~ dist, data = cars)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -7.5293 -2.1550  0.3615  2.4377  6.4179 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  8.28391    0.87438   9.474 1.44e-12 ***\n#> dist         0.16557    0.01749   9.464 1.49e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.156 on 48 degrees of freedom\n#> Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 \n#> F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\nlmtest::coeftest(model, vcov. = sandwich::vcovHC(model, type = \"HC1\"))\n#> \n#> t test of coefficients:\n#> \n#>             Estimate Std. Error t value  Pr(>|t|)    \n#> (Intercept) 8.283906   0.891860  9.2883 2.682e-12 ***\n#> dist        0.165568   0.019402  8.5335 3.482e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"report.html","id":"coefficient-uncertainty-and-distribution","chapter":"38 Report","heading":"38.5 Coefficient Uncertainty and Distribution","text":"ggdist allows us visualize uncertainty frequentist Bayesian frameworks","code":"\nlibrary(ggdist)"},{"path":"report.html","id":"descriptive-tables","chapter":"38 Report","heading":"38.6 Descriptive Tables","text":"Export APA themeExport LatexHowever, codes play well notes. Hence, create custom code follows AMA guidelines","code":"\ndata(\"mtcars\")\n\nlibrary(flextable)\ntheme_apa(flextable(mtcars[1:5,1:5]))\nprint(xtable::xtable(mtcars, type = \"latex\"),\n      file = file.path(getwd(), \"output\", \"mtcars_xtable.tex\"))\n\n# American Economic Review style\nstargazer::stargazer(\n    mtcars,\n    title = \"Testing\",\n    style = \"aer\",\n    out = file.path(getwd(), \"output\", \"mtcars_stargazer.tex\")\n)\n\n# other styles include\n# Administrative Science Quarterly\n# Quarterly Journal of Economics\nama_tbl <- function(data, caption, label, note, output_path) {\n  library(tidyverse)\n  library(xtable)\n  # Function to determine column alignment\n  get_column_alignment <- function(data) {\n    # Start with the alignment for the header row\n    alignment <- c(\"l\", \"l\")\n    \n    # Check each column\n    for (col in seq_len(ncol(data))[-1]) {\n      if (is.numeric(data[[col]])) {\n        alignment <- c(alignment, \"r\")  # Right alignment for numbers\n      } else {\n        alignment <- c(alignment, \"c\")  # Center alignment for other data\n      }\n    }\n    \n    return(alignment)\n  }\n  \n  data %>%\n    # bold + left align first column \n    rename_with(~paste(\"\\\\multicolumn{1}{l}{\\\\textbf{\", ., \"}}\"), 1) %>% \n    # bold + center align all other columns\n    `colnames<-`(ifelse(colnames(.) != colnames(.)[1],\n                        paste(\"\\\\multicolumn{1}{c}{\\\\textbf{\", colnames(.), \"}}\"),\n                        colnames(.))) %>% \n    \n    xtable(caption = caption,\n           label = label,\n           align = get_column_alignment(data),\n           auto = TRUE) %>%\n    print(\n      include.rownames = FALSE,\n      caption.placement = \"top\",\n      \n      hline.after=c(-1, 0),\n      \n       # p{0.9\\linewidth} sets the width of the column to 90% of the line width, and the @{} removes any extra padding around the cell.\n      \n      add.to.row = list(pos = list(nrow(data)), # Add at the bottom of the table\n                        command = c(paste0(\"\\\\hline \\n \\\\multicolumn{\",ncol(data), \"}{l} {\", \"\\n \\\\begin{tabular}{@{}p{0.9\\\\linewidth}@{}} \\n\",\"Note: \", note, \"\\n \\\\end{tabular}  } \\n\"))), # Add your note here\n      \n      # make sure your heading is untouched (because you manually change it above)\n      sanitize.colnames.function = identity,\n      \n      # place a the top of the page\n      table.placement = \"h\",\n      \n      file = output_path\n    )\n}\nama_tbl(\n    mtcars,\n    caption     = \"This is caption\",\n    label       = \"tab:this_is_label\",\n    note        = \"this is note\",\n    output_path = file.path(getwd(), \"output\", \"mtcars_custom_ama.tex\")\n)"},{"path":"report.html","id":"visualizations-and-plots","chapter":"38 Report","heading":"38.7 Visualizations and Plots","text":"can customize plots based preferred journals. , creating custom setting American Marketing Association.American-Marketing-Association-ready theme plotsExampleOther pre-specified themes","code":"\nlibrary(ggplot2)\n\n# check available fonts\n# windowsFonts()\n\n# for Times New Roman\n# names(windowsFonts()[windowsFonts()==\"TT Times New Roman\"])\n# Making a theme\namatheme = theme_bw(base_size = 14, base_family = \"serif\") + # This is Time New Roman\n    \n    theme(\n        # remove major gridlines\n        panel.grid.major   = element_blank(),\n\n        # remove minor gridlines\n        panel.grid.minor   = element_blank(),\n\n        # remove panel border\n        panel.border       = element_blank(),\n\n        line               = element_line(),\n\n        # change font\n        text               = element_text(),\n\n        # if you want to remove legend title\n        # legend.title     = element_blank(),\n\n        legend.title       = element_text(size = rel(0.6), face = \"bold\"),\n\n        # change font size of legend\n        legend.text        = element_text(size = rel(0.6)),\n        \n        legend.background  = element_rect(color = \"black\"),\n        \n        # legend.margin    = margin(t = 5, l = 5, r = 5, b = 5),\n        # legend.key       = element_rect(color = NA, fill = NA),\n\n        # change font size of main title\n        plot.title         = element_text(\n            size           = rel(1.2),\n            face           = \"bold\",\n            hjust          = 0.5,\n            margin         = margin(b = 15)\n        ),\n        \n        plot.margin        = unit(c(1, 1, 1, 1), \"cm\"),\n\n        # add black line along axes\n        axis.line          = element_line(colour = \"black\", linewidth = .8),\n        \n        axis.ticks         = element_line(),\n        \n\n        # axis title\n        axis.title.x       = element_text(size = rel(1.2), face = \"bold\"),\n        axis.title.y       = element_text(size = rel(1.2), face = \"bold\"),\n\n        # axis text size\n        axis.text.y        = element_text(size = rel(1)),\n        axis.text.x        = element_text(size = rel(1))\n    )\nlibrary(tidyverse)\nlibrary(ggsci)\ndata(\"mtcars\")\nyourplot <- mtcars %>%\n    select(mpg, cyl, gear) %>%\n    ggplot(., aes(x = mpg, y = cyl, fill = gear)) + \n    geom_point() +\n    labs(title=\"Some Plot\") \n\nyourplot + \n    amatheme + \n    # choose different color theme\n    scale_color_npg() \n\nyourplot + \n    amatheme + \n    scale_color_continuous()\nlibrary(ggthemes)\n\n\n# Stata theme\nyourplot +\n    theme_stata()\n\n# The economist theme\nyourplot + \n    theme_economist()\n\nyourplot + \n    theme_economist_white()\n\n# Wall street journal theme\nyourplot + \n    theme_wsj()\n\n# APA theme\nyourplot +\n    jtools::theme_apa(\n        legend.font.size = 24,\n        x.font.size = 20,\n        y.font.size = 20\n    )"},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"39 Exploratory Data Analysis","heading":"39 Exploratory Data Analysis","text":"Data ReportFeature EngineeringMissing DataError IdentificationSummary statisticsNot code-y processQuick dirty way look dataCode generation wranglingShiny-app based Tableu styleCustomized daily/automatic report","code":"\n# load to get txhousing data\nlibrary(ggplot2)\n# install.packages(\"DataExplorer\")\nlibrary(DataExplorer)\n\n# creat a html file that contain all reports\ncreate_report(txhousing)\n\nintroduce() # see basic info\n\n\ndummify() # create binary columns from discrete variables\nsplit_columns() # split data into discrete and continuous parts\n\n\n\nplot_correlation() # heatmap for discrete var\nplot_intro() \n\nplot_missing() # plot missing value\nprofile_missing() # profile missing values\n\n\nplot_prcomp() # plot PCA\n# install.packages(\"dataReporter\")\nlibrary(dataReporter)\nmakeDataReport() # detailed report like DataExplorer\nlibrary(skimr)\nskim() # give only few quick summary stat, not as detailed as the other two packages\n# install.packages(\"rpivotTable\")\nlibrary(rpivotTable)\n# give set up just like Excel table \ndata %>% \n    rpivotTable::rpivotTable()\n# install.packages(\"esquisse\")\nlibrary(esquisse)\nesquisse::esquisser()\n# install.packages(\"chronicle\")\nlibrary(chronicle)\n# install.packages(\"dlookr\")\n# install.packages(\"descriptr\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"sensitivity-analysis-robustness-check","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40 Sensitivity Analysis/ Robustness Check","text":"","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"specification-curve","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1 Specification curve","text":"also known Specification robustness graph coefficient stability plotResourcesIn Stata speccurveIn Stata speccurve(Simonsohn, Simmons, Nelson 2020)(Simonsohn, Simmons, Nelson 2020)","code":""},{"path":"sensitivity-analysis-robustness-check.html","id":"starbility","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1.1 starbility","text":"RecommendInstallationExample package’s authorPlot different combinations controlsNote:\\(p < 0.01\\): red\\(p < 0.05\\): green\\(p < 0.1\\): blue\\(p > 0.1\\): blackMore Advanced StuffIn step 2, can modify use function (e.g., glm)getting specification (e.g., different CI)get customized plotTo get different model specification (e.g., probit vs. logit)","code":"\ndevtools::install_github('https://github.com/AakaashRao/starbility')\nlibrary(starbility)\nlibrary(tidyverse)\nlibrary(starbility)\nlibrary(lfe)\ndata(\"diamonds\")\nset.seed(43)\nindices = sample(1:nrow(diamonds),\n                 replace = F,\n                 size = round(nrow(diamonds) / 20))\ndiamonds = diamonds[indices, ]\n\n# If you want to make the diamond dimensions as base control\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z' # include all variables under 1 dimension\n)\n\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding fixed effects\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\n# Adding instrumental variables \ninstruments = 'x+y+z'\n\n# clustering and weights \ndiamonds$sample_weights = runif(n = nrow(diamonds))\n\n\n# robust standard errors \nstarb_felm_custom = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = lfe::felm(spec, data=data) %>% broom::tidy()\n\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n  \n  # 99% confidence interval\n  z = qnorm(0.995) \n  # one-tailed test\n  return(c(coef, p/2, coef+z*se, coef-z*se))\n}\n\nplots = stability_plot(\n    data = diamonds,\n    lhs = 'price',\n    rhs = 'carat',\n    error_geom = 'ribbon', # make the plot more aesthetics\n    # error_geom = 'none', # if you don't want ribbon (i.e., error bar)\n    model = starb_felm_custom,\n    cluster = 'cut',\n    weights = 'sample_weights',\n    # iv = instruments,\n    perm = perm_controls,\n    base = base_controls,\n    # perm_fe = perm_fe_controls,\n    \n    # if you want to include fixed effects sequentially (not all combinations) \n    # (e.g., you want to test country or state fixed effect, not both )\n    # nonperm_fe = nonperm_fe_controls, \n    # fe_always = F,  # if you want to have a model without any Fixed Effects\n    \n    # sort \"asc\", \"desc\", or by fixed effects: \"asc-by-fe\" or \"desc-by-fe\"\n    sort = \"asc-by-fe\", \n    \n    # if you have less variables and want more aesthetics \n    # control_geom = 'circle',\n    # point_size = 2,\n    # control_spacing = 0.3,\n    \n    \n    # error_alpha = 0.2, # change alpha of the error geom\n    # point_size = 1.5, # change the size of the coefficient points\n    # control_text_size = 10, # change the size of the control labels\n    # coef_ylim = c(-5000, 35000), # change the endpoints of the y-axis\n    # trip_top = 3, # change the spacing between the two panels\n    \n    rel_height = 0.6\n)\nplots\n\n# add comments\n# replacement_coef_panel = plots[[1]] +\n#   scale_y_reverse() +\n#   theme(panel.grid.minor = element_blank()) +\n#   geom_vline(xintercept = 41,\n#              linetype = 'dashed',\n#              alpha = 0.4) +\n#   annotate(\n#     geom = 'label',\n#     x = 52,\n#     y = 30000,\n#     label = 'What a great\\nspecification!',\n#     alpha = 0.75\n#   )\n# \n# combine_plots(replacement_coef_panel,\n#               plots[[2]],\n#               rel_height = 0.6)\n# Step 1: Control Grid\n\ndiamonds$high_clarity = diamonds$clarity %in% c('VS1','VVS2','VVS1','IF')\n\nbase_controls = c(\n  'Diamond dimensions' = 'x + y + z'\n)\n\nperm_controls = c(\n  'Depth' = 'depth',\n  'Table width' = 'table'\n)\n\nperm_fe_controls = c(\n  'Cut FE' = 'cut',\n  'Color FE' = 'color'\n)\nnonperm_fe_controls = c(\n  'Clarity FE (granular)' = 'clarity',\n  'Clarity FE (binary)' = 'high_clarity'\n)\n\ngrid1 = stability_plot(data = diamonds, \n                      lhs = 'price', \n                      rhs = 'carat', \n                      perm = perm_controls,\n                      base = base_controls, \n                      perm_fe = perm_fe_controls, \n                      nonperm_fe = nonperm_fe_controls, \n                      run_to=2)\n\nknitr::kable(grid1 %>% head(10))\n\n# Step 2: Get model expression\n\ngrid2 = stability_plot(grid = grid1,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=2,\n                      run_to=3)\n\n\nknitr::kable(grid2 %>% head(10))\n\n# Step 3: Estimate models\ngrid3 = stability_plot(grid = grid2,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=3,\n                      run_to=4)\n\nknitr::kable(grid3 %>% head(10))\n\n# Step 4: Get dataframe to draw\ndfs = stability_plot(grid = grid3,\n                      data=diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      perm=perm_controls, \n                      base=base_controls,\n                      run_from=4,\n                      run_to=5)\n\ncoef_grid = dfs[[1]]\ncontrol_grid = dfs[[2]]\n\nknitr::kable(coef_grid %>% head(10))\n\n# Step 5: plot the sensitivity graph \npanels = stability_plot(data = diamonds, \n                      lhs='price', \n                      rhs='carat', \n                      coef_grid = coef_grid,\n                      control_grid = control_grid,\n                      run_from=5,\n                      run_to=6)\n\nstability_plot(data = diamonds,\n               lhs='price', \n               rhs='carat', \n               coef_panel = panels[[1]],\n               control_panel = panels[[2]],\n               run_from = 6,\n               run_to = 7)\ndiamonds$above_med_price = as.numeric(diamonds$price > median(diamonds$price))\n\nbase_controls = c('Diamond dimensions' = 'x + y + z')\n\nperm_controls = c('Depth' = 'depth',\n                  'Table width' = 'table',\n                  'Clarity' = 'clarity')\nlhs_var = 'above_med_price'\nrhs_var = 'carat'\n\ngrid1 = stability_plot(\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_to = 2\n)\n\n# Create control part of formula\nbase_perm = c(base_controls, perm_controls)\ngrid1$expr = apply(grid1[, 1:length(base_perm)], 1,\n                   function(x)\n                     paste(base_perm[names(base_perm)[which(x == 1)]], \n                           collapse = '+'))\n\n# Complete formula with LHS and RHS variables\ngrid1$expr = paste(lhs_var, '~', rhs_var, '+', grid1$expr, sep = '')\n\nknitr::kable(grid1 %>% head(10))\n\n# customer function for the logit model\nstarb_logit = function(spec, data, rhs, ...) {\n  spec = as.formula(spec)\n  model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n    broom::tidy()\n  row = which(model$term==rhs)\n  coef = model[row, 'estimate'] %>% as.numeric()\n  se   = model[row, 'std.error'] %>% as.numeric()\n  p    = model[row, 'p.value'] %>% as.numeric()\n\n  return(c(coef, p, coef+1.96*se, coef-1.96*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from=3)\nlibrary(margins)\nstarb_logit_enhanced = function(spec, data, rhs, ...) {\n  # Unpack ...\n  l = list(...)\n  get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n  \n  spec = as.formula(spec)\n  if (get_mfx) {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      margins() %>%\n      summary\n    row = which(model$factor==rhs)\n    coef = model[row, 'AME'] %>% as.numeric()\n    se   = model[row, 'SE'] %>% as.numeric()\n    p    = model[row, 'p'] %>% as.numeric()\n  } else {\n    model = glm(spec, data=data, family='binomial', weights=data$weight) %>%\n      broom::tidy()\n    row = which(model$term==rhs)\n    coef = model[row, 'estimate'] %>% as.numeric()\n    se   = model[row, 'std.error'] %>% as.numeric()\n    p    = model[row, 'p.value'] %>% as.numeric()\n  }\n\n  z = qnorm(0.995)\n  return(c(coef, p, coef+z*se, coef-z*se))\n}\n\nstability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3)\ndfs = stability_plot(grid = grid1,\n               data = diamonds, \n               lhs = lhs_var, \n               rhs = rhs_var,\n               model = starb_logit_enhanced,\n               get_mfx = T,\n               perm = perm_controls,\n               base = base_controls,\n               fe_always = F,\n               run_from = 3,\n               run_to = 5)\n\ncoef_grid_logit = dfs[[1]]\ncontrol_grid_logit = dfs[[2]]\n\nmin_space = 0.5\n\ncoef_plot = ggplot2::ggplot(coef_grid_logit, aes(\n  x = model,\n  y = coef,\n  shape = p,\n  group = p\n)) +\n  geom_linerange(aes(ymin = error_low, ymax = error_high), alpha = 0.75) +\n  geom_point(size = 5, aes(col = p, fill = p), alpha = 1) +\n  viridis::scale_color_viridis(discrete = TRUE, option = \"D\") +\n  scale_shape_manual(values = c(15, 17, 18, 19)) +\n  theme_classic() +\n  geom_hline(yintercept = 0, linetype = 'dotted') +\n  ggtitle('A custom coefficient stability plot!') +\n  labs(subtitle = \"Error bars represent 99% confidence intervals\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks.x = element_blank()\n  ) +\n  coord_cartesian(xlim = c(1 - min_space, max(coef_grid_logit$model) + min_space),\n                  ylim = c(-0.1, 1.6)) +\n  guides(fill = F, shape = F, col = F)\n\n\ncontrol_plot = ggplot(control_grid_logit) +\n  geom_point(aes(x = model, y = y, fill=value), shape=23, size=4) +\n  scale_fill_manual(values=c('#FFFFFF', '#000000')) +\n  guides(fill=F) +\n  scale_y_continuous(breaks = unique(control_grid_logit$y), \n                     labels = unique(control_grid_logit$key),\n                     limits=c(min(control_grid_logit$y)-1, max(control_grid_logit$y)+1)) +\n  scale_x_continuous(breaks=c(1:max(control_grid_logit$model))) +\n  coord_cartesian(xlim=c(1-min_space, max(control_grid_logit$model)+min_space)) +\n  theme_classic() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title = element_blank(),\n        axis.text.y = element_text(size=10),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()) \n\ncowplot::plot_grid(coef_plot, control_plot, rel_heights=c(1,0.5), \n                   align='v', ncol=1, axis='b')\nstarb_probit = function(spec, data, rhs, ...) {\n    # Unpack ...\n    l = list(...)\n    get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F\n    \n    spec = as.formula(spec)\n    if (get_mfx) {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            margins() %>%\n            summary\n        row = which(model$factor == rhs)\n        coef = model[row, 'AME'] %>% as.numeric()\n        se   = model[row, 'SE'] %>% as.numeric()\n        p    = model[row, 'p'] %>% as.numeric()\n    } else {\n        model = glm(\n            spec,\n            data = data,\n            family = binomial(link = 'probit'),\n            weights = data$weight\n        ) %>%\n            broom::tidy()\n        row = which(model$term == rhs)\n        coef = model[row, 'estimate'] %>% as.numeric()\n        se   = model[row, 'std.error'] %>% as.numeric()\n        p    = model[row, 'p.value'] %>% as.numeric()\n    }\n    \n    z = qnorm(0.995)\n    return(c(coef, p, coef + z * se, coef - z * se))\n}\n\nprobit_dfs = stability_plot(\n    grid = grid1,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    model = starb_probit,\n    get_mfx = T,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 3,\n    run_to = 5\n)\n\n# We'll put the probit DFs on the left, \n #so we need to adjust the model numbers accordingly\n# so the probit and logit DFs don't plot on top of one another!\ncoef_grid_probit = probit_dfs[[1]] %>% \n    mutate(model = model + max(coef_grid_logit$model))\n\ncontrol_grid_probit = probit_dfs[[2]] %>% \n    mutate(model = model + max(control_grid_logit$model))\n\ncoef_grid    = bind_rows(coef_grid_logit, coef_grid_probit)\ncontrol_grid = bind_rows(control_grid_logit, control_grid_probit)\n\npanels = stability_plot(\n    coef_grid = coef_grid,\n    control_grid = control_grid,\n    data = diamonds,\n    lhs = lhs_var,\n    rhs = rhs_var,\n    perm = perm_controls,\n    base = base_controls,\n    fe_always = F,\n    run_from = 5,\n    run_to = 6\n)\n\ncoef_plot = panels[[1]] + geom_vline(xintercept = 8.5,\n                                     linetype = 'dashed',\n                                     alpha = 0.8) +\n    annotate(\n        geom = 'label',\n        x = 4.25,\n        y = 1.8,\n        label = 'Logit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    annotate(\n        geom = 'label',\n        x = 12.75,\n        y = 1.8,\n        label = 'Probit models',\n        size = 6,\n        fill = '#D3D3D3',\n        alpha = 0.7\n    ) +\n    coord_cartesian(ylim = c(-0.5, 1.9))\n\ncontrol_plot = panels[[2]] + geom_vline(xintercept = 8.5,\n                                        linetype = 'dashed',\n                                        alpha = 0.8)\n\ncowplot::plot_grid(\n    coef_plot,\n    control_plot,\n    rel_heights = c(1, 0.5),\n    align = 'v',\n    ncol = 1,\n    axis = 'b'\n)"},{"path":"sensitivity-analysis-robustness-check.html","id":"rdfanalysis","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.1.2 rdfanalysis","text":"recommendInstallationExample package’s authorShiny app readers explore","code":"\ndevtools::install_github(\"joachim-gassen/rdfanalysis\")\nlibrary(rdfanalysis)\nload(url(\"https://joachim-gassen.github.io/data/rdf_ests.RData\"))\nplot_rdf_spec_curve(ests, \"est\", \"lb\", \"ub\") \ndesign <- define_design(steps = c(\"read_data\",\n                                  \"select_idvs\",\n                                  \"treat_extreme_obs\",\n                                  \"specify_model\",\n                                  \"est_model\"),\n                        rel_dir = \"vignettes/case_study_code\")\n\nshiny_rdf_spec_curve(ests, list(\"est\", \"lb\", \"ub\"),\n                     design, \"vignettes/case_study_code\",\n                     \"https://joachim-gassen.github.io/data/wb_new.csv\")"},{"path":"sensitivity-analysis-robustness-check.html","id":"coefficient-stability","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.2 Coefficient stability","text":"(Oster 2019)Coefficient stability can evident omitted variable bias.Coefficient stability can evident omitted variable bias.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.coefficient stability alone can misleading, combing \\(R^2\\) movement, can become informative.Packagesmplot: graphical Model stability Variable Selectionmplot: graphical Model stability Variable Selectionrobomit: Robustness checks omitted variable bias (implementation ofrobomit: Robustness checks omitted variable bias (implementation ","code":"\nlibrary(robomit)\n\n# estimate beta \no_beta(\n  y     = \"mpg\",       # dependent variable\n  x     = \"wt\",        # independent treatment variable\n  con   = \"hp + qsec\", # related control variables\n  delta = 1,           # delta\n  R2max = 0.9,         # maximum R-square\n  type  = \"lm\",        # model type\n  data  = mtcars       # dataset\n) \n#> # A tibble: 10 × 2\n#>    Name                           Value\n#>    <chr>                          <dbl>\n#>  1 beta*                         -2.00 \n#>  2 (beta*-beta controlled)^2      5.56 \n#>  3 Alternative Solution 1        -7.01 \n#>  4 (beta[AS1]-beta controlled)^2  7.05 \n#>  5 Uncontrolled Coefficient      -5.34 \n#>  6 Controlled Coefficient        -4.36 \n#>  7 Uncontrolled R-square          0.753\n#>  8 Controlled R-square            0.835\n#>  9 Max R-square                   0.9  \n#> 10 delta                          1"},{"path":"sensitivity-analysis-robustness-check.html","id":"omitted-variable-bias-quantification","chapter":"40 Sensitivity Analysis/ Robustness Check","heading":"40.3 Omitted Variable Bias Quantification","text":"quantify bias needed change substantive conclusion causal inference study.","code":"\nlibrary(konfound)\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5\n)\n#> Robustness of Inference to Replacement (RIR):\n#> To invalidate an inference,  21.506 % of the estimate would have to be due to bias. \n#> This is based on a threshold of 3.925 for statistical significance (alpha = 0.05).\n#> \n#> To invalidate an inference,  215  observations would have to be replaced with cases\n#> for which the effect is 0 (RIR = 215).\n#> \n#> See Frank et al. (2013) for a description of the method.\n#> \n#> Citation: Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. (2013).\n#> What would it take to change an inference?\n#> Using Rubin's causal model to interpret the \n#>         robustness of causal inferences.\n#> Education, Evaluation and \n#>                        Policy Analysis, 35 437-460.\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"thresh_plot\"\n)\n\npkonfound(\n    est_eff = 5, \n    std_err = 2, \n    n_obs = 1000, \n    n_covariates = 5, \n    to_return = \"corr_plot\"\n)"},{"path":"replication-and-synthetic-data.html","id":"replication-and-synthetic-data","chapter":"41 Replication and Synthetic Data","heading":"41 Replication and Synthetic Data","text":"Access comprehensive data pivotal replication, especially realm social sciences. Yet, often data inaccessible, making replication challenge (G. King 1995). chapter dives nuances replication, exceptions norms, significance synthetic data.","code":""},{"path":"replication-and-synthetic-data.html","id":"the-replication-standard","chapter":"41 Replication and Synthetic Data","heading":"41.1 The Replication Standard","text":"Replicability research ensures:Credibility comprehension empirical studies.Continuity progression discipline.Enhanced readership academic citations.research replicable, “replication standard” vital: entails providing requisite information replication third parties. quantitative research can, extent, offer clear data, qualitative studies pose complexities due data depth.","code":""},{"path":"replication-and-synthetic-data.html","id":"solutions-for-empirical-replication","chapter":"41 Replication and Synthetic Data","heading":"41.1.1 Solutions for Empirical Replication","text":"Role Individual Authors:\nAuthors need vouch replication standard enhancing work’s credibility.\nArchives like Inter-University Consortium Political Social Research (ICPSR) serve depositories replication datasets.\nAuthors need vouch replication standard enhancing work’s credibility.Archives like Inter-University Consortium Political Social Research (ICPSR) serve depositories replication datasets.Creation Replication Data Set:\npublic data set, consisting original relevant complementary data, can serve replication purposes.\npublic data set, consisting original relevant complementary data, can serve replication purposes.Professional Data Archives:\nOrganizations like ICPSR provide solutions data storage accessibility problems.\nOrganizations like ICPSR provide solutions data storage accessibility problems.Educational Implications:\nReplication can excellent educational tool, many programs now emphasize importance.\nReplication can excellent educational tool, many programs now emphasize importance.","code":""},{"path":"replication-and-synthetic-data.html","id":"free-data-repositories","chapter":"41 Replication and Synthetic Data","heading":"41.1.2 Free Data Repositories","text":"Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.Zenodo: Hosted CERN, provides place researchers deposit datasets. ’s subject-specific, caters various disciplines.figshare: Allows researchers upload, share, cite datasets.figshare: Allows researchers upload, share, cite datasets.Dryad: Primarily datasets associated published articles biological medical sciences.Dryad: Primarily datasets associated published articles biological medical sciences.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.OpenICPSR: public-facing version Inter-University Consortium Political Social Research (ICPSR) researchers can deposit data without cost.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Harvard Dataverse: Hosted Harvard University, open-source repository software application dedicated archiving, sharing, citing research data.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Mendeley Data: multidisciplinary, free--use open access data repository researchers can upload share datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.Open Science Framework (OSF): Offers platform conducting research place deposit datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.PubMed Central: Specific life sciences, ’s open repository journal articles, preprints, datasets.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.Registry Research Data Repositories (re3data): repository , provides global registry research data repositories various academic disciplines.SocArXiv: open archive social sciences.SocArXiv: open archive social sciences.EarthArXiv: preprints archive earth science.EarthArXiv: preprints archive earth science.Protein Data Bank (PDB): 3D structures large biological molecules.Protein Data Bank (PDB): 3D structures large biological molecules.Gene Expression Omnibus (GEO): public functional genomics data repository.Gene Expression Omnibus (GEO): public functional genomics data repository.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.Language Archive (TLA): Dedicated data languages worldwide, especially endangered languages.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.B2SHARE: platform storing sharing research data sets various disciplines, especially European research projects.","code":""},{"path":"replication-and-synthetic-data.html","id":"exceptions-to-replication","chapter":"41 Replication and Synthetic Data","heading":"41.1.3 Exceptions to Replication","text":"exceptions replication standard :Confidentiality: Sometimes data, even fragmented, sensitive share.Proprietary Data: Data sets owned entities might restrict dissemination, usually, parts data can still shared.Rights First Publication: Embargos might set, essential data used study accessible.","code":""},{"path":"replication-and-synthetic-data.html","id":"synthetic-data-an-overview","chapter":"41 Replication and Synthetic Data","heading":"41.2 Synthetic Data: An Overview","text":"Synthetic data, modeling real data ensuring anonymity, becoming pivotal research. promising, complexities approached caution.","code":""},{"path":"replication-and-synthetic-data.html","id":"benefits","chapter":"41 Replication and Synthetic Data","heading":"41.2.1 Benefits","text":"Privacy preservation.Data fairness augmentation.Acceleration research.","code":""},{"path":"replication-and-synthetic-data.html","id":"concerns","chapter":"41 Replication and Synthetic Data","heading":"41.2.2 Concerns","text":"Misconceptions inherent privacy.Challenges data outliers.Models relying solely synthetic data can pose risks.","code":""},{"path":"replication-and-synthetic-data.html","id":"further-insights-on-synthetic-data","chapter":"41 Replication and Synthetic Data","heading":"41.2.3 Further Insights on Synthetic Data","text":"Synthetic data bridges model-centric data-centric perspectives, making essential tool modern research. Analogously, ’s like viewing Mona Lisa’s replica, real painting stored securely.Future projects, utilizing R’s diamonds dataset synthetic data generation, hold promise demonstrating vast potentials technology.deeper dive synthetic data applications, refer (Jordon et al. 2022).","code":""},{"path":"replication-and-synthetic-data.html","id":"application-16","chapter":"41 Replication and Synthetic Data","heading":"41.3 Application","text":"easiest way create synthetic data use synthpop package. Alternatively, can manuallyOpen data can assessed utility two distinct ways:General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.General Utility: refers broad resemblances within dataset, allowing preliminary data exploration.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.Specific Utility: focuses comparability models derived synthetic original datasets, emphasizing analytical reproducibility.General utilitySpecific utilityYou basically want lack--fit test non-significant.","code":"\nlibrary(synthpop)\nlibrary(tidyverse)\nlibrary(performance)\n\n# library(effectsize)\n# library(see)\n# library(patchwork)\n# library(knitr)\n# library(kableExtra)\n\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\n\nsynthpop::codebook.syn(iris)\n#> $tab\n#>       variable   class nmiss perctmiss ndistinct\n#> 1 Sepal.Length numeric     0         0        35\n#> 2  Sepal.Width numeric     0         0        23\n#> 3 Petal.Length numeric     0         0        43\n#> 4  Petal.Width numeric     0         0        22\n#> 5      Species  factor     0         0         3\n#>                             details\n#> 1                  Range: 4.3 - 7.9\n#> 2                    Range: 2 - 4.4\n#> 3                    Range: 1 - 6.9\n#> 4                  Range: 0.1 - 2.5\n#> 5 'setosa' 'versicolor' 'virginica'\n#> \n#> $labs\n#> NULL\n\nsyn_df <- syn(iris, seed = 3)\n#> \n#> Synthesis\n#> -----------\n#>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n\n# check for replciated uniques\nreplicated.uniques(syn_df, iris)\n#> $replications\n#>   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n#>  [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n#>  [25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n#>  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n#>  [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n#>  [97] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n#> [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#> [133] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [145] FALSE FALSE FALSE FALSE FALSE FALSE\n#> \n#> $no.uniques\n#> [1] 148\n#> \n#> $no.replications\n#> [1] 17\n#> \n#> $per.replications\n#> [1] 11.33333\n\n\n# remove replicated uniques and adds a FAKE_DATA label \n# (in case a person can see his or own data in \n# the replicated data by chance)\n\nsyn_df_sdc <- sdc(syn_df, iris, \n                  label = \"FAKE_DATA\",\n                  rm.replicated.uniques = T)\n#> no. of replicated uniques: 17\niris |> \n    GGally::ggpairs()\n\nsyn_df$syn |> \n    GGally::ggpairs()\nlm_ori <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = iris)\n# performance::check_model(lm_ori)\nsummary(lm_ori)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.96159 -0.23489  0.00077  0.21453  0.78557 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.24914    0.24797    9.07 7.04e-16 ***\n#> Sepal.Width   0.59552    0.06933    8.59 1.16e-14 ***\n#> Petal.Length  0.47192    0.01712   27.57  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3333 on 147 degrees of freedom\n#> Multiple R-squared:  0.8402, Adjusted R-squared:  0.838 \n#> F-statistic: 386.4 on 2 and 147 DF,  p-value: < 2.2e-16\n\nlm_syn <- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df$syn)\n# performance::check_model(lm_syn)\nsummary(lm_syn)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = syn_df$syn)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.79165 -0.22790 -0.01448  0.15893  1.13360 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.96449    0.24538  12.081  < 2e-16 ***\n#> Sepal.Width   0.39214    0.06816   5.754  4.9e-08 ***\n#> Petal.Length  0.45267    0.01743  25.974  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3658 on 147 degrees of freedom\n#> Multiple R-squared:  0.8246, Adjusted R-squared:  0.8222 \n#> F-statistic: 345.6 on 2 and 147 DF,  p-value: < 2.2e-16\ncompare(syn_df, iris)\n# just like regular lm, but for synthetic data\nlm_syn <- lm.synds(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df)\ncompare(lm_syn, iris)\n#> \n#> Call used to fit models to the data:\n#> lm.synds(formula = Sepal.Length ~ Sepal.Width + Petal.Length, \n#>     data = syn_df)\n#> \n#> Differences between results based on synthetic and observed data:\n#>              Synthetic  Observed        Diff Std. coef diff CI overlap\n#> (Intercept)  2.9644900 2.2491402  0.71534988       2.884829  0.2640608\n#> Sepal.Width  0.3921429 0.5955247 -0.20338187      -2.933611  0.2516161\n#> Petal.Length 0.4526695 0.4719200 -0.01925058      -1.124602  0.7131064\n#> \n#> Measures for one synthesis and 3 coefficients\n#> Mean confidence interval overlap:  0.4095944\n#> Mean absolute std. coef diff:  2.314347\n#> \n#> Mahalanobis distance ratio for lack-of-fit (target 1.0): 3.08\n#> Lack-of-fit test: 9.23442; p-value 0.0263 for test that synthesis model is\n#> compatible with a chi-squared test with 3 degrees of freedom.\n#> \n#> Confidence interval plot:\n\n# summary(lm_syn)"},{"path":"appendix.html","id":"appendix","chapter":"A Appendix","heading":"A Appendix","text":"","code":""},{"path":"appendix.html","id":"git","chapter":"A Appendix","heading":"A.1 Git","text":"Cheat SheetCheat Sheet different languagesLearn GitInteractive Cheat SheetUltimate Guide Git GitHub R userSetting Git: git config --global option configure user name, email, editor, etc.Setting Git: git config --global option configure user name, email, editor, etc.Creating repository: git init initialize repo. Git stores repo data .git directory.Creating repository: git init initialize repo. Git stores repo data .git directory.Tracking changes:\ngit status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\n\ngit add put files staging area\ngit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\n\nTracking changes:git status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\ngit status shows status repoFile stored project’s working directory (users see)File stored project’s working directory (users see)staging area (next commit built)staging area (next commit built)local repo commits permanently recordedlocal repo commits permanently recordedgit add put files staging areagit add put files staging areagit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\ngit commit saves staged content new commit local repo.git commit -m \"message\" give messages purpose commit.History\ngit diff shows differences commits\ngit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\n\nHistorygit diff shows differences commitsgit diff shows differences commitsgit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\ngit checkout recovers old version fieldsgit checkout HEAD go last commitgit checkout HEAD go last commitgit checkout <unique ID commit> go commitgit checkout <unique ID commit> go commitIgnoring\n.gitignore file tells Git files ignore\ncat . gitignore *.dat results/ ignore files ending “dat” folder “results”.\nIgnoring.gitignore file tells Git files ignore.gitignore file tells Git files ignorecat . gitignore *.dat results/ ignore files ending “dat” folder “results”.cat . gitignore *.dat results/ ignore files ending “dat” folder “results”.Remotes GitHub\nlocal git repo can connected one remote repos.\nUse HTTPS protocol connect remote repos\ngit push copies changes local repo remote repo\ngit pull copies changes remote repo local repo\nRemotes GitHubA local git repo can connected one remote repos.local git repo can connected one remote repos.Use HTTPS protocol connect remote reposUse HTTPS protocol connect remote reposgit push copies changes local repo remote repogit push copies changes local repo remote repogit pull copies changes remote repo local repogit pull copies changes remote repo local repoCollaborating\ngit clone copies remote repo create local repo remote called origin automatically set \nCollaboratinggit clone copies remote repo create local repo remote called origin automatically set upBranching\ngit check - b <new-branch-name\ngit checkout master switch master branch.\nBranchinggit check - b <new-branch-namegit check - b <new-branch-namegit checkout master switch master branch.git checkout master switch master branch.Conflicts\noccur 2 people change lines file\nversion control system allow overwrite ’s changes blindly, highlights conflicts can resolved.\nConflictsoccur 2 people change lines fileoccur 2 people change lines filethe version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.Licensing\nPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .\nCreative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.\nLicensingPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .People incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Citation:\nAdd CITATION file repo explain want others cite work.\nCitation:Add CITATION file repo explain want others cite work.Hosting\nRules regarding intellectual property storage sensitive info apply matter code data hosted.\nHostingRules regarding intellectual property storage sensitive info apply matter code data hosted.","code":""},{"path":"appendix.html","id":"short-cut","chapter":"A Appendix","heading":"A.2 Short-cut","text":"shortcuts probably remember working R. Even though might take bit time learn use second nature, save lot time.\nJust like learning another language, speak practice , comfortable speaking .Sometimes can’t stage folder ’s large. case, use Terminal pane Rstudio type git add -stage changes commit push like usual.","code":""},{"path":"appendix.html","id":"function-short-cut","chapter":"A Appendix","heading":"A.3 Function short-cut","text":"apply one function data create new variable: mutate(mod=map(data,function))\ninstead using 1:length(object): (seq_along(object))\napply multiple function: map_dbl\napply multiple function multiple variables:map2autoplot(data) plot times series datamod_tidy = linear(reg) %>% set_engine('lm') %>% fit(price ~ ., data=data) fit lm model. also fit models (stan, spark, glmnet, keras)Sometimes, data-masking able recognize whether ’re calling environment data variables. bypass , use .data$variable .env$variable. example data %>% mutate(x=.env$variable/.data$variableProblems data-masking:Unexpected masking data-var: Use .data .env disambiguate\nData-var cant get :\nTunnel data-var {{}} + Subset .data [[]]\nUnexpected masking data-var: Use .data .env disambiguateData-var cant get :Tunnel data-var {{}} + Subset .data [[]]Passing Data-variables argumentsTrouble selection:","code":"\nlibrary(\"dplyr\")\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{{var}}\":=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings\n}\n\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{var}\":=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions\n}\nlibrary(\"purrr\")\nname <- c(\"mass\",\"height\")\nstarwars %>% select(name) # Data-var. Here you are referring to variable named \"name\"\n\nstarwars %>% select(all_of((name))) # use all_of() to disambiguate when \n\naverages <- function(data,vars){ # take character vectors with all_of()\n    data %>%\n        select(all_of(vars)) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)\n\n\n# Another way\naverages <- function(data,vars){ # Tunnel selectiosn with {{}}\n    data %>%\n        select({{vars}}) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)"},{"path":"appendix.html","id":"citation","chapter":"A Appendix","heading":"A.4 Citation","text":"include citation [@Farjam_2015]cite packages used sessionpackage=ls(sessionInfo()$loadedOnly) (package){print(toBibtex(citation()))}","code":"\npackage=ls(sessionInfo()$loadedOnly) \nfor (i in package){\n    print(toBibtex(citation(i)))\n    }"},{"path":"appendix.html","id":"install-all-necessary-packageslibaries-on-your-local-machine","chapter":"A Appendix","heading":"A.5 Install all necessary packages/libaries on your local machine","text":"Get list packages need install book (local device)installed.csv file new local machine, can just install list packages","code":"\ninstalled <- as.data.frame(installed.packages())\n\nhead(installed)\n#>         Package                            LibPath Version Priority\n#> abind     abind C:/Program Files/R/R-4.2.3/library   1.4-5     <NA>\n#> ade4       ade4 C:/Program Files/R/R-4.2.3/library  1.7-22     <NA>\n#> admisc   admisc C:/Program Files/R/R-4.2.3/library    0.33     <NA>\n#> AER         AER C:/Program Files/R/R-4.2.3/library  1.2-10     <NA>\n#> afex       afex C:/Program Files/R/R-4.2.3/library   1.3-0     <NA>\n#> agridat agridat C:/Program Files/R/R-4.2.3/library    1.21     <NA>\n#>                                                                                        Depends\n#> abind                                                                             R (>= 1.5.0)\n#> ade4                                                                               R (>= 2.10)\n#> admisc                                                                            R (>= 3.5.0)\n#> AER     R (>= 3.0.0), car (>= 2.0-19), lmtest, sandwich (>= 2.4-0),\\nsurvival (>= 2.37-5), zoo\n#> afex                                                             R (>= 3.5.0), lme4 (>= 1.1-8)\n#> agridat                                                                                   <NA>\n#>                                                                                 Imports\n#> abind                                                                    methods, utils\n#> ade4                graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\\nRcpp\n#> admisc                                                                          methods\n#> AER                                                           stats, Formula (>= 0.2-0)\n#> afex    pbkrtest (>= 0.4-1), lmerTest (>= 3.0-0), car, reshape2,\\nstats, methods, utils\n#> agridat                                                                            <NA>\n#>                   LinkingTo\n#> abind                  <NA>\n#> ade4    Rcpp, RcppArmadillo\n#> admisc                 <NA>\n#> AER                    <NA>\n#> afex                   <NA>\n#> agridat                <NA>\n#>                                                                                                                                                                                                                                                                                                                                                                                                Suggests\n#> abind                                                                                                                                                                                                                                                                                                                                                                                              <NA>\n#> ade4                                                                                                                                                                                                                                                  ade4TkGUI, adegraphics, adephylo, ape, CircStats, deldir,\\nlattice, spdep, splancs, waveslim, progress, foreach, parallel,\\ndoParallel, iterators\n#> admisc                                                                                                                                                                                                                                                                                                                                                                                     QCA (>= 3.7)\n#> AER                                                                                                                                  boot, dynlm, effects, fGarch, forecast, foreign, ineq,\\nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\\nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\\nscatterplot3d, strucchange, systemfit (>= 1.1-20), truncreg,\\ntseries, urca, vars\n#> afex    emmeans (>= 1.4), coin, xtable, parallel, plyr, optimx,\\nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\\nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\\npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\\ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\\nstatmod, performance (>= 0.7.2), see (>= 0.6.4), ez,\\nggResidpanel, grid, vdiffr\n#> agridat                    AER, agricolae, betareg, broom, car, coin, corrgram, desplot,\\ndplyr, effects, equivalence, emmeans, FrF2, gam, gge, ggplot2,\\ngnm, gstat, HH, knitr, lattice, latticeExtra, lme4, lucid,\\nmapproj, maps, MASS, MCMCglmm, metafor, mgcv, NADA, nlme,\\nnullabor, ordinal, pbkrtest, pls, pscl, reshape2, rgdal,\\nrmarkdown, qicharts, qtl, sp, SpATS, survival, vcd, testthat\n#>         Enhances       License License_is_FOSS License_restricts_use OS_type\n#> abind       <NA>   LGPL (>= 2)            <NA>                  <NA>    <NA>\n#> ade4        <NA>    GPL (>= 2)            <NA>                  <NA>    <NA>\n#> admisc      <NA>    GPL (>= 3)            <NA>                  <NA>    <NA>\n#> AER         <NA> GPL-2 | GPL-3            <NA>                  <NA>    <NA>\n#> afex        <NA>    GPL (>= 2)            <NA>                  <NA>    <NA>\n#> agridat     <NA>  CC BY-SA 4.0            <NA>                  <NA>    <NA>\n#>         MD5sum NeedsCompilation Built\n#> abind     <NA>               no 4.2.0\n#> ade4      <NA>              yes 4.2.3\n#> admisc    <NA>              yes 4.2.3\n#> AER       <NA>               no 4.2.3\n#> afex      <NA>               no 4.2.3\n#> agridat   <NA>               no 4.2.3\n\nwrite.csv(installed, file.path(getwd(),'installed.csv'))\n# import the list of packages\ninstalled <- read.csv('installed.csv')\n\n# get the list of packages that you have on your device\nbaseR <- as.data.frame(installed.packages())\n\n# install only those that you don't have\ninstall.packages(setdiff(installed, baseR))"},{"path":"bookdown-cheat-sheet.html","id":"bookdown-cheat-sheet","chapter":"B Bookdown cheat sheet","heading":"B Bookdown cheat sheet","text":"","code":"\n# to see non-scientific notation a result\nformat(12e-17, scientific = FALSE)\n#> [1] \"0.00000000000000012\""},{"path":"bookdown-cheat-sheet.html","id":"operation","chapter":"B Bookdown cheat sheet","heading":"B.1 Operation","text":"R commands derivatives defined function Taking derivatives R involves using expression, D, eval functions. wrap function want take derivative expression(), use D, eval follows.simple exampleEvaluateThe first argument passed eval expression want evaluatethe second list containing values quantities defined elsewhere.","code":"\n#define a function\nf=expression(sqrt(x))\n\n#take the first derivative\ndf.dx=D(f,'x')\ndf.dx\n#> 0.5 * x^-0.5\n\n#take the second derivative\nd2f.dx2=D(D(f,'x'),'x')\nd2f.dx2\n#> 0.5 * (-0.5 * x^-1.5)\n#evaluate the function at a given x\neval(f,list(x=3))\n#> [1] 1.732051\n\n#evaluate the first derivative at a given x\neval(df.dx,list(x=3))\n#> [1] 0.2886751\n\n#evaluate the second derivative at a given x\neval(d2f.dx2,list(x=3))\n#> [1] -0.04811252"},{"path":"bookdown-cheat-sheet.html","id":"math-expression-syntax","chapter":"B Bookdown cheat sheet","heading":"B.2 Math Expression/ Syntax","text":"Full listAligning equations\\[\n\\begin{aligned}\n& = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\n\\]Cross-reference equationto refer sentence (B.1) (\\@ref(eq:test))Limit P(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\\[\nP(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\n\\]Matrices\\[\n\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n\\]\\[\n\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n\\]Aligning EquationsAligning Equations Comments\\[\n\\begin{aligned}\n    3+x &=4 & &\\text{(Solve } x \\text{.)} \\\\\n    x &=4-3 && \\text{(Subtract 3 sides.)} \\\\\n    x &=1   && \\text{(Yielding solution.)}\n\\end{aligned}\n\\]","code":"\\begin{aligned}\na & = b \\\\\nX &\\sim {Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\\begin{equation} \na = b\n(\\#eq:test)\n\\end{equation}$$\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n$$$$\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n$$\\begin{aligned}\n    3+x &=4 && \\text{(Solve for} x \\text{.)}\\\\\n    x &=4-3 && \\text{(Subtract 3 from both sides.)}\\\\\n    x &=1   && \\text{(Yielding the solution.)}\n\\end{aligned}"},{"path":"bookdown-cheat-sheet.html","id":"statistics-notation","chapter":"B Bookdown cheat sheet","heading":"B.2.1 Statistics Notation","text":"\\[\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n\\]\\[\n\\begin{cases}\n\\frac{1}{b-} & \\text{} x\\[,b]\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\]","code":"$$\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n$$\\begin{cases}\n\\frac{1}{b-a}&\\text{for $x\\in[a,b]$}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}"},{"path":"bookdown-cheat-sheet.html","id":"table","chapter":"B Bookdown cheat sheet","heading":"B.3 Table","text":"built-wrapperbright colorcures scurvytasty\\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\)","code":"+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| *Bananas*     | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - **tasty**        |\n+---------------+---------------+--------------------+(\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
