<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 21 Analysis of Variance (ANOVA) | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 21 Analysis of Variance (ANOVA) | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/analysis-of-variance-anova.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 21 Analysis of Variance (ANOVA) | A Guide on Data Analysis">
<meta name="twitter:description" content="ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="active" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">25</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">26</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">27</span> Event Studies</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">28</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">29</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">30</span> Endogeneity</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">31</span> Controls</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">32</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">33</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">34</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">35</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">36</span> Sensitivity Analysis/ Robustness Check</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="analysis-of-variance-anova" class="section level1" number="21">
<h1>
<span class="header-section-number">21</span> Analysis of Variance (ANOVA)<a class="anchor" aria-label="anchor" href="#analysis-of-variance-anova"><i class="fas fa-link"></i></a>
</h1>
<p>ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with <strong>qualitative variables</strong> and <strong>designed experiments</strong>.</p>
<p><br></p>
<p>Experimental Design</p>
<ul>
<li>
<strong>Factor</strong>: explanatory or predictor variable to be studied in an investigation</li>
<li>
<strong>Treatment</strong> (or Factor Level): “value” of a factor applied to the experimental unit</li>
<li>
<strong>Experimental Unit</strong>: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response</li>
<li>
<strong>Single Factor Experiment</strong>: one explanatory variable considered</li>
<li>
<strong>Multifactor Experiment</strong>: more than one explanatory variable</li>
<li>
<strong>Classification Factor</strong>: A factor that is not under the control of the experimenter (observational data)</li>
<li>
<strong>Experimental Factor</strong>: assigned by the experimenter</li>
</ul>
<p><br></p>
<p>Basics of experimental design:</p>
<ul>
<li>
<p>Choices that a statistician has to make:</p>
<ul>
<li>set of treatments</li>
<li>set of experimental units</li>
<li>treatment assignment (selection bias)</li>
<li>measurement (measurement bias, blind experiments)</li>
</ul>
</li>
<li>
<p>Advancements in experimental design:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Factorial Experiments</strong>:<br>
consider multiple factors at the same time (interaction)</p></li>
<li>
<p><strong>Replication</strong>: repetition of experiment</p>
<ul>
<li>assess mean squared error</li>
<li>control over precision of experiment (power)</li>
</ul>
</li>
<li>
<p><strong>Randomization</strong></p>
<ul>
<li>Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively</li>
<li>randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator</li>
</ul>
</li>
<li>
<p><strong>Local control</strong>: Blocking or Stratification</p>
<ul>
<li>Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units.</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>Randomization may also eliminate correlations due to time and space.</p>
<div id="completely-randomized-design-crd" class="section level2" number="21.1">
<h2>
<span class="header-section-number">21.1</span> Completely Randomized Design (CRD)<a class="anchor" aria-label="anchor" href="#completely-randomized-design-crd"><i class="fas fa-link"></i></a>
</h2>
<p>Treatment factor A with <span class="math inline">\(a\ge2\)</span> treatments levels. Experimental units are randomly assinged to each treatment. The number of experiemntal units in each group can be</p>
<ul>
<li>equal (balanced): n</li>
<li>unequal (unbalanced): <span class="math inline">\(n_i\)</span> for the i-th group (i = 1,…,a).</li>
</ul>
<p>The total sample size is <span class="math inline">\(N=\sum_{i=1}^{a}n_i\)</span></p>
<p>Possible assignments of units to treatments are <span class="math inline">\(k=\frac{N!}{n_1!n_2!...n_a!}\)</span></p>
<p>Each has probability 1/k of being selected. Each experimental unit is measured with a response <span class="math inline">\(Y_{ij}\)</span>, in which j denotes unit and i denotes treatment.</p>
<p>Treatment</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left"></th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">…</th>
<th align="center">a</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="center"><span class="math inline">\(Y_{11}\)</span></td>
<td align="center"><span class="math inline">\(Y_{21}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(Y_{a1}\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"><span class="math inline">\(Y_{12}\)</span></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="even">
<td align="left">Sample Mean</td>
<td align="center"><span class="math inline">\(\bar{Y_{1.}}\)</span></td>
<td align="center"><span class="math inline">\(\bar{Y_{2.}}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\bar{Y_{a.}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Sample SD</td>
<td align="center"><span class="math inline">\(s_1\)</span></td>
<td align="center"><span class="math inline">\(s_2\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(s_a\)</span></td>
</tr>
</tbody>
</table></div>
<p>where <span class="math inline">\(\bar{Y_{i.}}=\frac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}\)</span></p>
<p><span class="math inline">\(s_i^2=\frac{1}{n_i-1}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2\)</span></p>
<p>And the grand mean is <span class="math inline">\(\bar{Y_{..}}=\frac{1}{N}\sum_{i}\sum_{j}Y_{ij}\)</span></p>
<div id="single-factor-fixed-effects-model" class="section level3" number="21.1.1">
<h3>
<span class="header-section-number">21.1.1</span> Single Factor Fixed Effects Model<a class="anchor" aria-label="anchor" href="#single-factor-fixed-effects-model"><i class="fas fa-link"></i></a>
</h3>
<p>also known as Single Factor (One-Way) ANOVA or ANOVA Type I model.</p>
<p>Partitioning the Variance</p>
<p>The total variability of the <span class="math inline">\(Y_{ij}\)</span> observation can be measured as the deviation of <span class="math inline">\(Y_{ij}\)</span> around the overall mean <span class="math inline">\(\bar{Y_{..}}\)</span>: <span class="math inline">\(Y_{ij} - \bar{Y_{..}}\)</span></p>
<p>This can be rewritten as: <span class="math display">\[
\begin{split}
Y_{ij} - \bar{Y_{..}}&amp;=Y_{ij} - \bar{Y_{..}} + \bar{Y_{i.}} - \bar{Y_{i.}} \\
&amp;= (\bar{Y_{i.}}-\bar{Y_{..}})+(Y_{ij}-\bar{Y_{i.}})
\end{split}
\]</span> where</p>
<ul>
<li>the first term is the <em>between</em> treatment differences (i.e., the deviation of the treatment mean from the overall mean)<br>
</li>
<li>the second term is <em>within</em> treatment differences (i.e., the deviation of the observation around its treatment mean)</li>
</ul>
<p><br></p>
<p><span class="math display">\[
\begin{split}
\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})^2 &amp;=  \sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2+\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
SSTO &amp;= SSTR + SSE \\
total~SS &amp;= treatment~SS + error~SS \\
(N-1)~d.f. &amp;= (a-1)~d.f. + (N - a) ~ d.f.
\end{split}
\]</span></p>
<p>we lose a d.f. for the total corrected SSTO because of the estimation of the mean (<span class="math inline">\(\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})=0\)</span>)<br>
And, for the SSTR <span class="math inline">\(\sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})=0\)</span></p>
<p>Accordingly, <span class="math inline">\(MSTR= \frac{SST}{a-1}\)</span> and <span class="math inline">\(MSR=\frac{SSE}{N-a}\)</span></p>
<p><strong>ANOVA Table</strong></p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="29%">
<col width="48%">
<col width="6%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th align="center">SS</th>
<th align="center">df</th>
<th align="center">MS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Between Treatments</td>
<td align="center"><span class="math inline">\(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\)</span></td>
<td align="center">a-1</td>
<td align="center">SSTR/(a-1)</td>
</tr>
<tr class="even">
<td>Error (within treatments)</td>
<td align="center"><span class="math inline">\(\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2\)</span></td>
<td align="center">N-a</td>
<td align="center">SSE/(N-a)</td>
</tr>
<tr class="odd">
<td>Total (corrected)</td>
<td align="center"><span class="math inline">\(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\)</span></td>
<td align="center">N-1</td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
<p>Linear Model Explanation of ANOVA</p>
<div id="cell-means-model" class="section level4" number="21.1.1.1">
<h4>
<span class="header-section-number">21.1.1.1</span> Cell means model<a class="anchor" aria-label="anchor" href="#cell-means-model"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
Y_{ij}=\mu_i+\epsilon\_{ij}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(Y_{ij}\)</span> response variable in j-th subject for the i-th treatment<br></p></li>
<li><p><span class="math inline">\(\mu_i\)</span>: parameters (fixed) representing the unknown population mean for the i-th treatment<br></p></li>
<li><p><span class="math inline">\(\epsilon_{ij}\)</span> independent <span class="math inline">\(N(0,\sigma^2)\)</span> errors</p></li>
<li><p><span class="math inline">\(E(Y_{ij})=\mu_i\)</span> <span class="math inline">\(var(Y_{ij})=var(\epsilon_{ij})=\sigma^2\)</span><br></p></li>
<li><p>All observations have the same variance</p></li>
</ul>
<p>Example:</p>
<p>a = 3 (3 treatments) <span class="math inline">\(n_1=n_2=n_3=2\)</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{split}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{split}
\]</span></p>
<p><span class="math inline">\(X_{k,ij}=1\)</span> if the k-th treatment is used</p>
<p><span class="math inline">\(X_{k,ij}=0\)</span> Otherwise</p>
<p><br></p>
<p>Note: no intercept term.</p>
<span class="math display" id="eq:betaorigin">\[\begin{equation}
\begin{split}
\mathbf{b}= \left[\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right] &amp;=
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
&amp; =
\left[\begin{array}{ccc}
n_1 &amp; 0 &amp; 0\\
0 &amp; n_2 &amp; 0\\
0 &amp; 0 &amp; n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_1\\
Y_2\\
Y_3\\
\end{array}\right] \\
&amp; =
\left[\begin{array}{c}
\bar{Y_1}\\
\bar{Y_2}\\
\bar{Y_3}\\
\end{array}\right]
\end{split}
\tag{21.1}
\end{equation}\]</span>
<p>is the BLUE (best linear unbiased estimator) for <span class="math inline">\(\beta=[\mu_1 \mu_2\mu_3]'\)</span></p>
<p><br></p>
<p><span class="math display">\[
E(\mathbf{b})=\beta
\]</span></p>
<p><span class="math display">\[
var(\mathbf{b})=\sigma^2(\mathbf{X'X})^{-1}=\sigma^2
\left[\begin{array}{ccc}
1/n_1 &amp; 0 &amp; 0\\
0 &amp; 1/n_2 &amp; 0\\
0 &amp; 0 &amp; 1/n_3\\
\end{array}\right]
\]</span></p>
<p><span class="math inline">\(var(b_i)=var(\hat{\mu_i})=\sigma^2/n_i\)</span> where <span class="math inline">\(\mathbf{b} \sim N(\beta,\sigma^2(\mathbf{X'X})^{-1})\)</span></p>
<p><span class="math display">\[
\begin{split}
MSE &amp;= \frac{1}{N-a} \sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
    &amp;= \frac{1}{N-a} \sum_{i}[(n_i-1)\frac{\sum_{i}(Y_{ij}-\bar{Y_{i.}})^2}{n_i-1}] \\
    &amp;= \frac{1}{N-a} \sum_{i}(n_i-1)s_1^2
\end{split}
\]</span></p>
<p>We have <span class="math inline">\(E(s_i^2)=\sigma^2\)</span></p>
<p><span class="math inline">\(E(MSE)=\frac{1}{N-a}\sum_{i}(n_i-1)\sigma^2=\sigma^2\)</span></p>
<p>Hence, MSE is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, regardless of whether the treatment means are equal or not.</p>
<p><span class="math inline">\(E(MSTR)=\sigma^2+\frac{\sum_{i}n_i(\mu_i-\mu_.)^2}{a-1}\)</span><br>
where <span class="math inline">\(\mu_.=\frac{\sum_{i=1}^{a}n_i\mu_i}{\sum_{i=1}^{a}n_i}\)</span><br>
If all treatment means are equals (=<span class="math inline">\(\mu_.\)</span>), <span class="math inline">\(E(MSTR)=\sigma^2\)</span>.</p>
<p>Then we can use an F-test for the equality of all treatment means:</p>
<p><span class="math display">\[H_0:\mu_1=\mu_2=..=\mu_a\]</span></p>
<p><span class="math display">\[H_a: not~al l~ \mu_i ~ are ~ equal \]</span></p>
<p><span class="math inline">\(F=\frac{MSTR}{MSE}\)</span><br>
where large values of F support <span class="math inline">\(H_a\)</span> (since MSTR will tend to exceed MSE when <span class="math inline">\(H_a\)</span> holds)<br>
and F near 1 support <span class="math inline">\(H_0\)</span> (upper tail test)</p>
<p><strong>Equivalently</strong>, when <span class="math inline">\(H_0\)</span> is true, <span class="math inline">\(F \sim f_{(a-1,N-a)}\)</span></p>
<ul>
<li>If <span class="math inline">\(F \leq f_{(a-1,N-a;1-\alpha)}\)</span>, we cannot reject <span class="math inline">\(H_0\)</span><br>
</li>
<li>If <span class="math inline">\(F \geq f_{(a-1,N-a;1-\alpha)}\)</span>, we reject <span class="math inline">\(H_0\)</span>
</li>
</ul>
<p>Note: If a = 2 (2 treatments), F-test = two sample t-test</p>
<p><br></p>
</div>
<div id="treatment-effects-factor-effects" class="section level4" number="21.1.1.2">
<h4>
<span class="header-section-number">21.1.1.2</span> Treatment Effects (Factor Effects)<a class="anchor" aria-label="anchor" href="#treatment-effects-factor-effects"><i class="fas fa-link"></i></a>
</h4>
<p>Besides Cell means model, we have another way to formalize one-way ANOVA: <span class="math display">\[Y_{ij} = \mu + \tau_i + \epsilon_{ij}\]</span> where</p>
<ul>
<li>
<span class="math inline">\(Y_{ij}\)</span> is the j-th response for the i-th treatment<br>
</li>
<li>
<span class="math inline">\(\tau_i\)</span> i-th treatment effect<br>
</li>
<li>
<span class="math inline">\(\mu\)</span> constant component, common to all observations<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ij}\)</span> independent random errors ~ <span class="math inline">\(N(0,\sigma^2)\)</span>
</li>
</ul>
<p><br></p>
<p>For example, a = 3, <span class="math inline">\(n_1=n_2=n_3=2\)</span></p>
<p><br></p>
<span class="math display" id="eq:unsolvable">\[\begin{equation}
\begin{split}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{cccc}
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\tau_3\\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{split}
\tag{21.2}
\end{equation}\]</span>
<p><br></p>
<p>However,</p>
<p><span class="math display">\[
\mathbf{X'X} =
\left(
\begin{array}
{cccc}
\sum_{i}n_i &amp; n_1 &amp; n_2 &amp; n_3 \\
n_1 &amp; n_1 &amp; 0 &amp; 0 \\
n_2 &amp; 0 &amp; n_2 &amp; 0 \\
n_3 &amp; 0 &amp; 0 &amp; n_3 \\
\end{array}
\right)
\]</span></p>
<p>is <strong>singular</strong> thus does not exist, <span class="math inline">\(\mathbf{b}\)</span> is insolvable (infinite solutions)</p>
<p>Hence, we have to impose restrictions on the parameters to a model matrix <span class="math inline">\(\mathbf{X}\)</span> of full rank.</p>
<p>Whatever restriction we use, we still have:</p>
<p><span class="math inline">\(E(Y_{ij})=\mu + \tau_i = \mu_i = mean ~ response ~ for ~ i-th ~ treatment\)</span></p>
<div id="restriction-on-sum-of-tau" class="section level5" number="21.1.1.2.1">
<h5>
<span class="header-section-number">21.1.1.2.1</span> Restriction on sum of tau<a class="anchor" aria-label="anchor" href="#restriction-on-sum-of-tau"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(\sum_{i=1}^{a}\tau_i=0\)</span></p>
<p><br></p>
<p>implies</p>
<p><span class="math display">\[
\mu= \mu +\frac{1}{a}\sum_{i=1}^{a}(\mu+\tau_i)
\]</span></p>
<p><br></p>
<p>is the average of the treatment mean (grand mean) (overall mean)</p>
<p><span class="math display">\[
\begin{split}
\tau_i  &amp;=(\mu+\tau_i) -\mu = \mu_i-\mu \\
        &amp;= \text{treatment  mean} - \text{grand~mean} \\
        &amp;= \text{treatment  effect}
\end{split}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\tau_a=-\tau_1-\tau_2-...-\tau_{a-1}
\]</span></p>
<p>Hence, the mean for the a-th treatment is</p>
<p><br></p>
<p><span class="math display">\[
\mu_a=\mu+\tau_a=\mu-\tau_1-\tau_2-...-\tau_{a-1}
\]</span></p>
<p>Hence, the model need only “a” parameters:</p>
<p><br></p>
<p><span class="math display">\[
\mu,\tau_1,\tau_2,..,\tau_{a-1}
\]</span></p>
<p>Equation <a href="analysis-of-variance-anova.html#eq:unsolvable">(21.2)</a> becomes</p>
<p><br></p>
<span class="math display">\[\begin{equation}
\begin{split}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{ccc}
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; -1 &amp; -1 \\
1 &amp; -1 &amp; -1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{split}
\end{equation}\]</span>
<p>where <span class="math inline">\(\beta\equiv[\mu,\tau_1,\tau_2]'\)</span></p>
<p><br></p>
<p>Equation <a href="analysis-of-variance-anova.html#eq:betaorigin">(21.1)</a> with <span class="math inline">\(\sum_{i}\tau_i=0\)</span> becomes</p>
<p><br></p>
<p><span class="math display">\[
\begin{split}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_1} \\
\hat{\tau_2} \\
\end{array}\right] &amp;=
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
&amp; =
\left[\begin{array}{ccc}
\sum_{i}n_i &amp; n_1-n_3 &amp; n_2-n_3\\
n_1-n_3 &amp; n_1+n_3 &amp; n_3\\
n_2-n_3 &amp; n_3 &amp; n_2-n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{1.}-Y_{3.}\\
Y_{2.}-Y_{3.}\\
\end{array}\right] \\
&amp; =
\left[\begin{array}{c}
\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{1.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{2.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\end{array}\right]\\
&amp; =
\left[\begin{array}{c}
\hat{\mu}\\
\hat{\tau_1}\\
\hat{\tau_2}\\
\end{array}\right]
\end{split}
\]</span></p>
<p>and <span class="math inline">\(\hat{\tau_3}=-\hat{\tau_1}-\hat{\tau_2}=\bar{Y_3}-\frac{1}{3} \sum_{i}\bar{Y_{i.}}\)</span></p>
</div>
<div id="restriction-on-first-tau" class="section level5" number="21.1.1.2.2">
<h5>
<span class="header-section-number">21.1.1.2.2</span> Restriction on first tau<a class="anchor" aria-label="anchor" href="#restriction-on-first-tau"><i class="fas fa-link"></i></a>
</h5>
<p>In R, <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> uses the restriction <span class="math inline">\(\tau_1=0\)</span></p>
<p>For the previous example, for <span class="math inline">\(n_1=n_2=n_3=2\)</span>, and <span class="math inline">\(\tau_1=0\)</span>. Then the treatment means can be written as:</p>
<p><span class="math display">\[
\mu_1= \mu + \tau_1 = \mu + 0 = \mu  \\
\mu_2= \mu + \tau_2 \\
\mu_3 = \mu + \tau_3
\]</span></p>
<p>Hence, <span class="math inline">\(\mu\)</span> is the mean response for the first treatment</p>
<p>In the matrix form,</p>
<p><span class="math display">\[
\begin{split}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_2 \\
\tau_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{split}
\]</span></p>
<p><span class="math inline">\(\beta = [\mu,\tau_2,\tau_3]'\)</span></p>
<p><span class="math display">\[
\begin{split}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_2} \\
\hat{\tau_3} \\
\end{array}\right] &amp;=
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
&amp; =
\left[\begin{array}{ccc}
\sum_{i}n_i &amp; n_2 &amp; n_3\\
n_2 &amp; n_2 &amp; 0\\
n_3 &amp; 0 &amp; n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{2.}\\
Y_{3.}\\
\end{array}\right] \\
&amp; =
\left[
\begin{array}{c}
\bar{Y_{1.}} \\
\bar{Y_{2.}} - \bar{Y_{1.}} \\
\bar{Y_{3.}} - \bar{Y_{1.}}\\
\end{array}\right]
\end{split}
\]</span></p>
<p><span class="math display">\[
E(\mathbf{b})= \beta =
\left[\begin{array}{c}
{\mu}\\
{\tau_2}\\
{\tau_3}\\
\end{array}\right]
=
\left[\begin{array}{c}
\mu_1\\
\mu_2-\mu_1\\
\mu_3-\mu_1\\
\end{array}\right]
\]</span></p>
<p><span class="math display">\[
var(\mathbf{b}) = \sigma^2(\mathbf{X'X})^{-1} \\
var(\hat{\mu}) = var(\bar{Y_{1.}})=\sigma^2/n_1 \\
var(\hat{\tau_2}) = var(\bar{Y_{2.}}-\bar{Y_{1.}}) = \sigma^2/n_2 + \sigma^2/n_1 \\
var(\hat{\tau_3}) = var(\bar{Y_{3.}}-\bar{Y_{1.}}) = \sigma^2/n_3 + \sigma^2/n_1
\]</span></p>
<p><br></p>
<p><strong>Note</strong> For all three parameterization, the ANOVA table is the same</p>
<ul>
<li>
<a href="analysis-of-variance-anova.html#cell-means-model-1">Model 1</a>: <span class="math inline">\(Y_{ij} = \mu_i + \epsilon_{ij}\)</span>
</li>
<li>
<a href="analysis-of-variance-anova.html#restriction-on-sum-of-tau">Model 2</a>: <span class="math inline">\(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\)</span> where <span class="math inline">\(\sum_{i} \tau_i=0\)</span>
</li>
<li>
<a href="analysis-of-variance-anova.html#restriction-on-first-tau">Model 3</a>: <span class="math inline">\(Y_{ij}= \mu + \tau_i + \epsilon_{ij}\)</span> where <span class="math inline">\(\tau_1=0\)</span>
</li>
</ul>
<p>All models have the same calculation for <span class="math inline">\(\hat{Y}\)</span> as</p>
<p><span class="math display">\[
\mathbf{\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}
\]</span></p>
<p><strong>ANOVA Table</strong></p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="14%">
<col width="51%">
<col width="3%">
<col width="13%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th align="center">SS</th>
<th align="center">df</th>
<th align="center">MS</th>
<th align="center">F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Between Treatments</td>
<td align="center"><span class="math inline">\(\sum_{i} n _ i (\bar { Y_ {i .} } -\bar{Y_{..}})^2 = \mathbf{Y ' (P-P_1)Y}\)</span></td>
<td align="center">a-1</td>
<td align="center"><span class="math inline">\(\frac{SSTR}{a-1}\)</span></td>
<td align="center"><span class="math inline">\(\frac{MSTR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>
<p>Error</p>
<p>(within treatments)</p>
</td>
<td align="center"><span class="math inline">\(\sum_{i}\sum_{j}(Y_{ij} -\bar{Y_{i.}})^2=\mathbf{e'e}\)</span></td>
<td align="center">N-a</td>
<td align="center"><span class="math inline">\(\frac{SSE}{N-a}\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total (corrected)</td>
<td align="center"><span class="math inline">\(\sum_{i } n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2=\mathbf{Y'Y - Y'P_1Y}\)</span></td>
<td align="center">N-1</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
<p>where <span class="math inline">\(\mathbf{P_1} = \frac{1}{n}\mathbf{J}\)</span></p>
<p>The F-statistic here has (a-1,N-a) degrees of freedom, which gives the same value for all three parameterization, but the hypothesis test is written a bit different:</p>
<p><span class="math display">\[
H_0 : \mu_1 = \mu_2 = ... = \mu_a \\
H_0 : \mu + \tau_1 = \mu + tau_2 = ... = \mu + \tau_a \\
H_0 : \tau_1 = \tau_2 = ...= \tau_a
\]</span></p>
<p>The F-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects.</p>
</div>
</div>
<div id="testing-of-treatment-effects" class="section level4" number="21.1.1.3">
<h4>
<span class="header-section-number">21.1.1.3</span> Testing of Treatment Effects<a class="anchor" aria-label="anchor" href="#testing-of-treatment-effects"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>A <a href="analysis-of-variance-anova.html#single-treatment-mean">Single Treatment Mean</a> <span class="math inline">\(\mu_i\)</span>
</li>
<li>A <a href="analysis-of-variance-anova.html#differences-between-treatment-means">Differences Between Treatment Means</a>
</li>
<li>A <a href="analysis-of-variance-anova.html#contrast-among-treatment-means">Contrast Among Treatment Means</a>
</li>
<li>A <a href="analysis-of-variance-anova.html#linear-combination-of-treatment-means">Linear Combination of Treatment Means</a>
</li>
</ul>
<div id="single-treatment-mean" class="section level5" number="21.1.1.3.1">
<h5>
<span class="header-section-number">21.1.1.3.1</span> Single Treatment Mean<a class="anchor" aria-label="anchor" href="#single-treatment-mean"><i class="fas fa-link"></i></a>
</h5>
<p>We have <span class="math inline">\(\hat{\mu_i}=\bar{Y_{i.}}\)</span> where</p>
<ul>
<li><span class="math inline">\(E(\bar{Y_{i.}})=\mu_i\)</span></li>
<li>
<span class="math inline">\(var(\bar{Y_{i}})=\sigma^2/n_i\)</span> estimated by <span class="math inline">\(s^2(\bar{Y_{i.}})=MSE / n_i\)</span>
</li>
</ul>
<p>Since <span class="math inline">\(\frac{\bar{Y_{i.}}-\mu_i}{s(\bar{Y_{i.}})} \sim t_{N-a}\)</span> and the confidence interval for <span class="math inline">\(\mu_i\)</span> is <span class="math inline">\(\bar{Y_{i.}} \pm t_{1-\alpha/2;N-a}s(\bar{Y_{i.}})\)</span>,<br>
then we can do a t-test for the means difference with some constant c</p>
<p><span class="math display">\[
H_0: \mu_i = c \\
H_1: \mu_i \neq c
\]</span></p>
<p>where</p>
<p><span class="math display">\[
T =\frac{\bar{Y_{i.}}-c}{s(\bar{Y_{i.}})}
\]</span></p>
<p>follows <span class="math inline">\(t_{N-a}\)</span> when <span class="math inline">\(H_0\)</span> is true.<br>
If <span class="math inline">\(|T| &gt; t_{1-\alpha/2;N-a}\)</span>, we can reject <span class="math inline">\(H_0\)</span></p>
</div>
<div id="differences-between-treatment-means" class="section level5" number="21.1.1.3.2">
<h5>
<span class="header-section-number">21.1.1.3.2</span> Differences Between Treatment Means<a class="anchor" aria-label="anchor" href="#differences-between-treatment-means"><i class="fas fa-link"></i></a>
</h5>
<p>Let <span class="math inline">\(D=\mu_i - \mu_i'\)</span>, also known as <strong>pairwise comparison</strong><br><span class="math inline">\(D\)</span> can be estimated by <span class="math inline">\(\hat{D}=\bar{Y_{i}}-\bar{Y_{i}}'\)</span> is unbiased (<span class="math inline">\(E(\hat{D})=\mu_i-\mu_i'\)</span>)</p>
<p>Since <span class="math inline">\(\bar{Y_{i}}\)</span> and <span class="math inline">\(\bar{Y_{i}}'\)</span> are independent, then</p>
<p><span class="math display">\[
var(\hat{D})=var(\bar{Y_{i}}) + var(\bar{Y_{i'}}) = \sigma^2(1/n_i + 1/n_i')
\]</span></p>
<p>can be estimated with</p>
<p><span class="math display">\[
s^2(\hat{D}) = MSE(1/n_i + 1/n_i')
\]</span></p>
<p>With the single treatment inference,</p>
<p><span class="math display">\[
\frac{\hat{D}-D}{s(\hat{D})} \sim t_{N-a}
\]</span></p>
<p>hence,</p>
<p><span class="math display">\[
\hat{D} \pm t_{(1-\alpha/2;N-a)}s(\hat{D})
\]</span></p>
<p>Hypothesis tests:</p>
<p><span class="math display">\[
H_0: \mu_i = \mu_i' \\
H_a: \mu_i \neq \mu_i'
\]</span></p>
<p>can be tested by the following statistic</p>
<p><span class="math display">\[
T = \frac{\hat{D}}{s(\hat{D})} \sim t_{1-\alpha/2;N-a}
\]</span></p>
<p>reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|T| &gt; t_{1-\alpha/2;N-a}\)</span></p>
</div>
<div id="contrast-among-treatment-means" class="section level5" number="21.1.1.3.3">
<h5>
<span class="header-section-number">21.1.1.3.3</span> Contrast Among Treatment Means<a class="anchor" aria-label="anchor" href="#contrast-among-treatment-means"><i class="fas fa-link"></i></a>
</h5>
<p>generalize the comparison of two means, we have <strong>contrasts</strong></p>
<p>A contrast is a linear combination of treatment means:</p>
<p><span class="math display">\[
L = \sum_{i=1}^{a}c_i \mu_i
\]</span></p>
<p>where each <span class="math inline">\(c_i\)</span> is non-random constant and sum to 0:</p>
<p><span class="math display">\[
\sum_{i=1}^{a} c_i = 0
\]</span></p>
<p>An unbiased estimator of a contrast L is</p>
<p><span class="math display">\[
\hat{L} = \sum_{i=1}^{a}c_i \bar{Y}_{i.}
\]</span></p>
<p>and <span class="math inline">\(E(\hat{L}) = L\)</span>. Since the <span class="math inline">\(\bar{Y}_{i.}\)</span>, i = 1,…, a are independent.</p>
<p><span class="math display">\[
var(\hat{L}) = var(\sum_{i=1}^a c_i \bar{Y}_{i.}) = \sum_{i=1}^a var(c_i \bar{Y}_i)  \\
= \sum_{i=1}^a c_i^2 var(\bar{Y}_i) = \sum_{i=1}^a c_i^2 \sigma^2 /n_i \\
= \sigma^2 \sum_{i=1}^{a} c_i^2 /n_i
\]</span></p>
<p>Estimation of the variance:</p>
<p><span class="math display">\[
s^2(\hat{L}) = MSE \sum_{i=1}^{a} \frac{c_i^2}{n_i}
\]</span></p>
<p><span class="math inline">\(\hat{L}\)</span> is normally distributed (since it is a linear combination of independent normal random variables).</p>
<p>Then, since <span class="math inline">\(SSE/\sigma^2\)</span> is <span class="math inline">\(\chi_{N-a}^2\)</span></p>
<p><span class="math display">\[
\frac{\hat{L}-L}{s(\hat{L})} \sim t_{N-a}
\]</span></p>
<p>A <span class="math inline">\(1-\alpha\)</span> confidence limits are given by</p>
<p><span class="math display">\[
\hat{L} \pm t_{1-\alpha/2; N-a}s(\hat{L})
\]</span></p>
<p>Hypothesis testing</p>
<p><span class="math display">\[
H_0: L = 0 \\
H_a: L \neq 0
\]</span></p>
<p>with</p>
<p><span class="math display">\[
T = \frac{\hat{L}}{s(\hat{L})}
\]</span></p>
<p>reject H_0 if <span class="math inline">\(|T| &gt; t_{1-\alpha/2;N-a}\)</span></p>
</div>
<div id="linear-combination-of-treatment-means" class="section level5" number="21.1.1.3.4">
<h5>
<span class="header-section-number">21.1.1.3.4</span> Linear Combination of Treatment Means<a class="anchor" aria-label="anchor" href="#linear-combination-of-treatment-means"><i class="fas fa-link"></i></a>
</h5>
<p>just like contrast <span class="math inline">\(L = \sum_{i=1}^a c_i \mu_i\)</span> but no restrictions on the <span class="math inline">\(c_i\)</span> coefficients.</p>
<p>Tests og a single treatment mean, two treatment means, and contrasts can all be considered form the same perspective.</p>
<p><span class="math display">\[
H_0: \sum c_i \mu_i = c \\
H_a: \sum c_i \mu_i \neq c
\]</span></p>
<p>The test statistics (t-stat) can be considered equivalently as F-tests; <span class="math inline">\(F = (T)^2\)</span> where <span class="math inline">\(F \sim F_{1,N-a}\)</span>. Since the numerator degrees of freedom is always 1 in these cases, we refer to them as single-degree-of-freedom tests.</p>
<p><br></p>
<p><strong>Multiple Contrasts</strong></p>
<p>To test simultaneously <span class="math inline">\(k \ge 2\)</span> contrasts, let <span class="math inline">\(T_1,...,T_k\)</span> be the t-stat. The joint distribution of these random variables is a multivariate t-distribution (the tests are dependent since they re based on the same data).</p>
<p>Limitations for comparing multiple contrasts:</p>
<ol style="list-style-type: decimal">
<li><p>The confidence coefficient <span class="math inline">\(1-\alpha\)</span> only applies to a particular estimate, not a series of estimates; similarly, the Type I error rate, <span class="math inline">\(\alpha\)</span>, applies to a particular test, not a series of tests. Example: 3 t-tests at <span class="math inline">\(\alpha = 0.05\)</span>, if tests are independent (which they are not), <span class="math inline">\(0.95^3 = 0.857\)</span> (thus <span class="math inline">\(\alpha - 0.143\)</span> not 0.05)<br></p></li>
<li>
<p>The confidence coefficient <span class="math inline">\(1-\alpha\)</span> and significance level <span class="math inline">\(\alpha\)</span> are appropriate only if the test was not suggest by the data.</p>
<ul>
<li>often, the results of an experiment suggest important (ie..g, potential significant) relationships.<br>
</li>
<li>the process of studying effects suggests by the data is called <strong>data snooping</strong>
</li>
</ul>
</li>
</ol>
<p><br></p>
<p>Multiple Comparison Procedures:</p>
<ul>
<li><a href="analysis-of-variance-anova.html#tukey">Tukey</a></li>
<li><a href="analysis-of-variance-anova.html#scheffe">Scheffe</a></li>
<li><a href="analysis-of-variance-anova.html#bonferroni">Bonferroni</a></li>
</ul>
<div id="tukey" class="section level6" number="21.1.1.3.4.1">
<h6>
<span class="header-section-number">21.1.1.3.4.1</span> Tukey<a class="anchor" aria-label="anchor" href="#tukey"><i class="fas fa-link"></i></a>
</h6>
<p>All pairwise comparisons of factor level means. All pairs <span class="math inline">\(D = \mu_i - \mu_i'\)</span> or all tests of the form:</p>
<p><span class="math display">\[
H_0: \mu_i -\mu_i' = 0 \\
H_a: \mu_i - \mu_i' \neq 0
\]</span></p>
<ul>
<li>When all sample sizes are equal (<span class="math inline">\(n_1 = n_2 = ... = n_a\)</span>) then the Tukey method family confidence coefficient is exactly <span class="math inline">\(1-\alpha\)</span> and the significance level is exactly <span class="math inline">\(\alpha\)</span><br>
</li>
<li>When the sample sizes are not equal, the family confidence coefficient is greater than <span class="math inline">\(1-\alpha\)</span> (i.e., the significance level is less than <span class="math inline">\(\alpha\)</span>) so the test i<strong>conservative</strong><br>
</li>
<li>Tukey considers the <strong>studentized range distribution</strong>. If we have <span class="math inline">\(Y_1,..,Y_r\)</span>, observations from a normal distribution with mean <span class="math inline">\(\alpha\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Define: <span class="math display">\[
w = max(Y_i) - min(Y_i)
\]</span> as the range of the observations. Let <span class="math inline">\(s^2\)</span> be an estimate of <span class="math inline">\(\sigma^2\)</span> with v degrees of freedom. Then, <span class="math display">\[
q(r,v) = \frac{w}{s}
\]</span> is called the studentized range. The distribution of q uses a special table.</li>
</ul>
<p><br></p>
<p><strong>Notes</strong></p>
<ul>
<li>when we are not interested in testing all pairwise comparison,s the confidence coefficient for the family of comparisons under consideration will be greater than <span class="math inline">\(1-\alpha\)</span> (with the significance level less than <span class="math inline">\(\alpha\)</span>)<br>
</li>
<li>Tukey can be used for “data snooping” as long as the effects to be studied on the basis of preliminary data analysis are pairwise comparisons.</li>
</ul>
</div>
<div id="scheffe" class="section level6" number="21.1.1.3.4.2">
<h6>
<span class="header-section-number">21.1.1.3.4.2</span> Scheffe<a class="anchor" aria-label="anchor" href="#scheffe"><i class="fas fa-link"></i></a>
</h6>
<p>This method applies when the family of interest is the set of possible contrasts among the treatment means:</p>
<p><span class="math display">\[
L = \sum_{i=1}^a c_i \mu_i
\]</span></p>
<p>where <span class="math inline">\(\sum_{i=1}^a c_i =0\)</span></p>
<p>That is, the family of all possible contrasts L or</p>
<p><span class="math display">\[
H_0: L = 0 \\
H_a: L \neq 0
\]</span></p>
<p>The family confidence level for the Scheffe procedure is exactly <span class="math inline">\(1-\alpha\)</span> (i.e., significance level = <span class="math inline">\(\alpha\)</span>) whether the sample sizes are equal or not.</p>
<p>For simultaneous confidence intervals,</p>
<p><span class="math display">\[
\hat{L} \pm Ss(\hat{L})
\]</span></p>
<p>where <span class="math inline">\(\hat{L}=\sum c_i \bar{Y}_{i.},s^2(\hat{L}) = MSE \sum c_i^2/n_i\)</span> and <span class="math inline">\(S^2 = (a-1)f_{1-\alpha;a-1,N-a}\)</span></p>
<p>The Scheffe procedure considers</p>
<p><span class="math display">\[
F = \frac{\hat{L}^2}{(a-1)s^2(\hat{L})}
\]</span></p>
<p>where we reject <span class="math inline">\(H_0\)</span> at the family significance level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(F &gt; f_{(1-\alpha;a-1,N-a)}\)</span></p>
<p><strong>Note</strong></p>
<ul>
<li>Since applications of the Scheffe never involve all conceivable contrasts, the <strong>finite family</strong> confidence coefficient will be larger than <span class="math inline">\(1-\alpha\)</span>, so <span class="math inline">\(1-\alpha\)</span> is a lower bound. Thus, people often consider a larger <span class="math inline">\(\alpha\)</span> (e.g., 90% confidence interval)<br>
</li>
<li>Scheffe can be used for “data scooping” since the family of statements contains all possible contrasts.<br>
</li>
<li>If only pairwise comparisons are to be considered, The Tukey procedure gives narrower confidence limits.</li>
</ul>
</div>
<div id="bonferroni" class="section level6" number="21.1.1.3.4.3">
<h6>
<span class="header-section-number">21.1.1.3.4.3</span> Bonferroni<a class="anchor" aria-label="anchor" href="#bonferroni"><i class="fas fa-link"></i></a>
</h6>
<p>Applicable whether the sample sizes are equal or unequal.</p>
<p>For the confidence intervals,</p>
<p><span class="math display">\[
\hat{L} \pm B s(\hat{L})
\]</span></p>
<p>where <span class="math inline">\(B= t_{(1-\alpha/(2g);N-a)}\)</span> and g is the number of comparisons in the family.</p>
<p>Hypothesis testing</p>
<p><span class="math display">\[
H_0: L = 0 \\
H_a: L \neq 0
\]</span></p>
<p>Let <span class="math inline">\(T= \frac{\hat{L}}{s(\hat{L})}\)</span> and reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|T|&gt;t_{1-\alpha/(2g),N-a}\)</span></p>
<p><strong>Notes</strong></p>
<ul>
<li>If all pairwise comparisons are of interest, the Tukey procedure is superior (narrower confidence intervals). If not, Bonferroni may be better.</li>
<li>Bonferroni is better than Scheffe when the number of contrasts is about the same as the treatment levels (or less).</li>
<li>Recommendation: compute all threes and pick the smallest.</li>
<li>Bonferroni can’t be used for <strong>data snooping</strong>
</li>
</ul>
</div>
<div id="fishers-lsd" class="section level6" number="21.1.1.3.4.4">
<h6>
<span class="header-section-number">21.1.1.3.4.4</span> Fisher’s LSD<a class="anchor" aria-label="anchor" href="#fishers-lsd"><i class="fas fa-link"></i></a>
</h6>
<p>does not control for family error rate</p>
<p>use t-stat for testing</p>
<p><span class="math display">\[
H_0: \mu_i = \mu_j
\]</span></p>
<p>t-stat</p>
<p><span class="math display">\[
t = \frac{\bar{y}_i - \bar{y}_j}{\sqrt{MSE(\frac{1}{n_i}+ \frac{1}{n_j})}}
\]</span></p>
</div>
<div id="newman-keuls" class="section level6" number="21.1.1.3.4.5">
<h6>
<span class="header-section-number">21.1.1.3.4.5</span> Newman-Keuls<a class="anchor" aria-label="anchor" href="#newman-keuls"><i class="fas fa-link"></i></a>
</h6>
<p>Do not recommend using this test since it has less power than ANOVA.</p>
</div>
</div>
<div id="multiple-comparisons-with-a-control" class="section level5" number="21.1.1.3.5">
<h5>
<span class="header-section-number">21.1.1.3.5</span> Multiple comparisons with a control<a class="anchor" aria-label="anchor" href="#multiple-comparisons-with-a-control"><i class="fas fa-link"></i></a>
</h5>
<div id="dunnett" class="section level6" number="21.1.1.3.5.1">
<h6>
<span class="header-section-number">21.1.1.3.5.1</span> Dunnett<a class="anchor" aria-label="anchor" href="#dunnett"><i class="fas fa-link"></i></a>
</h6>
<p>We have <span class="math inline">\(a\)</span> groups where the last group is the control group, and the <span class="math inline">\(a-1\)</span> treatment groups.</p>
<p>Then, we compare treatment groups to the control group. Hence, we have <span class="math inline">\(a-1\)</span> contrasts (i.e., <span class="math inline">\(a-1\)</span> pairwise comparisons)</p>
</div>
</div>
<div id="summary-5" class="section level5" number="21.1.1.3.6">
<h5>
<span class="header-section-number">21.1.1.3.6</span> Summary<a class="anchor" aria-label="anchor" href="#summary-5"><i class="fas fa-link"></i></a>
</h5>
<p>When choosing a multiple contrast method:</p>
<ul>
<li>
<p>Pairwise</p>
<ul>
<li>Equal groups sizes: <a href="analysis-of-variance-anova.html#tukey">Tukey</a><br>
</li>
<li>Unequal groups sizes: <a href="analysis-of-variance-anova.html#tukey">Tukey</a>, <a href="analysis-of-variance-anova.html#scheffe">Scheffe</a><br>
</li>
</ul>
</li>
<li>
<p>Not pairwise</p>
<ul>
<li>with control: <a href="analysis-of-variance-anova.html#dunnett">Dunnett</a><br>
</li>
<li>general: <a href="analysis-of-variance-anova.html#bonferroni">Bonferroni</a>, <a href="analysis-of-variance-anova.html#scheffe">Scheffe</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div id="single-factor-random-effects-model" class="section level3" number="21.1.2">
<h3>
<span class="header-section-number">21.1.2</span> Single Factor Random Effects Model<a class="anchor" aria-label="anchor" href="#single-factor-random-effects-model"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as ANOVA Type II models.</p>
<p>Treatments are chosen at from from larger population. We extend inference to all treatments in the population and not restrict our inference to those treatments that happened to be selected for the study.</p>
<div id="random-cell-means" class="section level4" number="21.1.2.1">
<h4>
<span class="header-section-number">21.1.2.1</span> Random Cell Means<a class="anchor" aria-label="anchor" href="#random-cell-means"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
Y_{ij} = \mu_i + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_i \sim N(\mu, \sigma^2_{\mu})\)</span> and independent<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0,\sigma^2)\)</span> and independent</li>
</ul>
<p><span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(\epsilon_{ij}\)</span> are mutually independent for <span class="math inline">\(i =1,...,a; j = 1,...,n\)</span></p>
<p>With all treatment sample sizes are equal</p>
<p><span class="math display">\[
E(Y_{ij}) = E(\mu_i) = \mu \\
var(Y_{ij}) = var(\mu_i) + var(\epsilon_i) = \sigma^2_{\mu} + \sigma^2
\]</span></p>
<p>Since <span class="math inline">\(Y_{ij}\)</span> are not independent</p>
<p><span class="math display">\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &amp;= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\
&amp;= E(\mu_i^2 + \mu_i \epsilon_{ij'} + \mu_i \epsilon_{ij} + \epsilon_{ij}\epsilon_{ij'}) - \mu^2 \\
&amp;= \sigma^2_{\mu} + \mu^2 - \mu^2 &amp; \text{if} j \neq j' \\
&amp;= \sigma^2_{\mu} &amp; \text{if} j \neq j'
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
cov(Y_{ij},Y_{i'j'}) &amp;= E(\mu_i \mu_{i'} + \mu_i \epsilon_{i'j'}+ \mu_{i'}\epsilon_{ij}+ \epsilon_{ij}\epsilon_{i'j'}) - \mu^2 \\
&amp;= \mu^2 - \mu^2 &amp; \text{if } i \neq i' \\
&amp;= 0 \\
\end{aligned}
\]</span></p>
<p>Hence,</p>
<ul>
<li>all observations have the same variance<br>
</li>
<li>any two observations from the same treatment have covariance <span class="math inline">\(\sigma^2_{\mu}\)</span><br>
</li>
<li>The correlation between any two responses from the same treatment:<br><span class="math display">\[
\begin{aligned}
\rho(Y_{ij},Y_{ij'}) &amp;= \frac{\sigma^2_{\mu}}{\sigma^2_{\mu}+ \sigma^2} &amp;&amp; \text{$j \neq j'$}
\end{aligned}
\]</span>
</li>
</ul>
<p><strong>Inference</strong></p>
<p><strong>Intraclass Correlation Coefficient</strong></p>
<p><span class="math display">\[
\frac{\sigma^2_{\mu}}{\sigma^2 + \sigma^2_{\mu}}
\]</span></p>
<p>which measures the proportion of total variability of <span class="math inline">\(Y_{ij}\)</span> accounted for by the variance of <span class="math inline">\(\mu_i\)</span></p>
<p><span class="math display">\[
H_0: \sigma_{\mu}^2 = 0 \\
H_a: \sigma_{\mu}^2 \neq 0
\]</span></p>
<p><span class="math inline">\(H_0\)</span> implies <span class="math inline">\(\mu_i = \mu\)</span> for all i, which can be tested by the F-test in ANOVA.</p>
<p>The understandings of the <a href="analysis-of-variance-anova.html#single-factor-fixed-effects-model">Single Factor Fixed Effects Model</a> and the <a href="analysis-of-variance-anova.html#single-factor-random-effects-model">Single Factor Random Effects Model</a> are different, the ANOVA is same for the one factor model. The difference is in the expected mean squares</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>
<strong>Random Effects</strong> Model</th>
<th>
<strong>Fixed Effects</strong> Model</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(E(MSE) = \sigma^2\)</span></td>
<td><span class="math inline">\(E(MSE) = \sigma^2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(E(M STR) = \sigma^2 - n \sigma^2_\mu\)</span></td>
<td><span class="math inline">\(E(MSTR) = \sigma^2 + \frac{ \sum_i n_i (\mu_i - \mu)^2}{a-1}\)</span></td>
</tr>
</tbody>
</table></div>
<p>If <span class="math inline">\(\sigma^2_\mu\)</span>, then MSE and MSTR have the same expectation (<span class="math inline">\(\sigma^2\)</span>). Otherwise, <span class="math inline">\(E(MSTR) &gt;E(MSE)\)</span>. Large values of the statistic</p>
<p><span class="math display">\[
F = \frac{MSTR}{MSE}
\]</span></p>
<p>suggest we reject <span class="math inline">\(H_0\)</span>.</p>
<p>Since <span class="math inline">\(F \sim F_{(a-1,a(n-1))}\)</span> when <span class="math inline">\(H_0\)</span> holds. If <span class="math inline">\(F &gt; f_{(1-\alpha;a-1,a(n-1))}\)</span> we reject <span class="math inline">\(H_0\)</span>. If sample sizes are not equal, F-test can still be used, but the df are <span class="math inline">\(a-1\)</span> and <span class="math inline">\(N-a\)</span>.</p>
<div id="estimation-of-mu" class="section level5" number="21.1.2.1.1">
<h5>
<span class="header-section-number">21.1.2.1.1</span> Estimation of <span class="math inline">\(\mu\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-mu"><i class="fas fa-link"></i></a>
</h5>
<p>An unbiased estimator of <span class="math inline">\(E(Y_{ij})=\mu\)</span> is the grand mean: <span class="math inline">\(\hat{\mu} = \hat{Y}_{..}\)</span></p>
<p>The variance of this estimator is</p>
<p><span class="math display">\[
\begin{aligned}
var(\bar{Y}_{..}) &amp;= var(\sum_i \bar{Y}_{i.}/a) \\
&amp;= \frac{1}{a^2}\sum_ivar(\bar{Y}_{i.}) \\
&amp;= \frac{1}{a^2}\sum_i(\sigma^2_\mu+\sigma^2/n) \\
&amp;= \frac{1}{a^2}(\sigma^2_{\mu}+\sigma^2/n) \\
&amp;= \frac{n\sigma^2_{\mu}+ \sigma^2}{an}
\end{aligned}
\]</span></p>
<p>An unbiased estimator of this variance is <span class="math inline">\(s^2(\bar{Y})=\frac{MSTR}{an}\)</span>. Thus <span class="math inline">\(\frac{\bar{Y}_{..}-\mu}{s(\bar{Y}_{..})} \sim t_{a-1}\)</span></p>
<p>A <span class="math inline">\(1-\alpha\)</span> confidence interval is <span class="math inline">\(\bar{Y}_{..} \pm t_{(1-\alpha/2;a-1)}s(\bar{Y}_{..})\)</span></p>
</div>
<div id="estimation-of-sigma2_musigma2_musigma2" class="section level5" number="21.1.2.1.2">
<h5>
<span class="header-section-number">21.1.2.1.2</span> Estimation of <span class="math inline">\(\sigma^2_\mu/(\sigma^2_{\mu}+\sigma^2)\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2_musigma2_musigma2"><i class="fas fa-link"></i></a>
</h5>
<p>In the random and fixed effects model, MSTR and MSE are independent. When the sample sizes are equal (<span class="math inline">\(n_i = n\)</span> for all i),</p>
<p><span class="math display">\[
\frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \sim f_{(a-1,a(n-1))}
\]</span></p>
<p><span class="math display">\[
P(f_{(\alpha/2;a-1,a(n-1))}\le \frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \le f_{(1-\alpha/2;a-1,a(n-1))}) = 1-\alpha
\]</span></p>
<p><span class="math display">\[
L = \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(1-\alpha/2;a-1,a(n-1))}})-1) \\
U = \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(\alpha/2;a-1,a(n-1))}})-1)
\]</span></p>
<p>The lower and upper <span class="math inline">\((L^*,U^*)\)</span> confidence limits for <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2}\)</span></p>
<p><span class="math display">\[
L^* = \frac{L}{1+L} \\
U^* = \frac{U}{1+U}
\]</span></p>
<p>If the lower limit for <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2}\)</span> is negative, it is customary to set L = 0.</p>
</div>
<div id="estimation-of-sigma2" class="section level5" number="21.1.2.1.3">
<h5>
<span class="header-section-number">21.1.2.1.3</span> Estimation of <span class="math inline">\(\sigma^2\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(a(n-1)MSE/\sigma^2 \sim \chi^2_{a(n-1)}\)</span>, the <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
\frac{a(n-1)MSE}{\chi^2_{1-\alpha/2;a(n-1)}} \le \sigma^2 \le \frac{a(n-1)MSE}{\chi^2_{\alpha/2;a(n-1)}}
\]</span></p>
<p>can also be used in case sample sizes are not equal - then df is N-a.</p>
</div>
<div id="estimation-of-sigma2_mu" class="section level5" number="21.1.2.1.4">
<h5>
<span class="header-section-number">21.1.2.1.4</span> Estimation of <span class="math inline">\(\sigma^2_\mu\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2_mu"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(E(MSE) = \sigma^2\)</span> <span class="math inline">\(E(MSTR) = \sigma^2 + n\sigma^2_\mu\)</span>. Hence,</p>
<p><span class="math display">\[
\sigma^2_{\mu} = \frac{E(MSTR)- E(MSE)}{n}
\]</span></p>
<p>An unbiased estimator of <span class="math inline">\(\sigma^2_\mu\)</span> is given by</p>
<p><span class="math display">\[
s^2_\mu =\frac{MSTR-MSE}{n}
\]</span></p>
<p>if <span class="math inline">\(s^2_\mu &lt; 0\)</span>, set <span class="math inline">\(s^2_\mu = 0\)</span></p>
<p>If sample sizes are not equal,</p>
<p><span class="math display">\[
s^2_\mu = \frac{MSTR - MSE}{n'}
\]</span></p>
<p>where <span class="math inline">\(n' = \frac{1}{a-1}(\sum_i n_i- \frac{\sum_i n^2_i}{\sum_i n_i})\)</span></p>
<p>no exact confidence intervals for <span class="math inline">\(\sigma^2_\mu\)</span>, but we can approximate intervals.</p>
<p><strong>Satterthewaite Procedure</strong> can be used to construct approximate confidence intervals for linear combination of expected mean squares<br>
A linear combination:</p>
<p><span class="math display">\[
\sigma^2_\mu = \frac{1}{n} E(MSTR) + (-\frac{1}{n}) E(MSE)
\]</span></p>
<p><span class="math display">\[
S = d_1 E(MS_1) + ..+ d_h E(MS_h)
\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> are coefficients.</p>
<p>An unbiased estimator of S is</p>
<p><span class="math display">\[
\hat{S} = d_1 MS_1 + ...+ d_h  MS_h
\]</span></p>
<p>Let <span class="math inline">\(df_i\)</span> be the degrees of freedom associated with the mean square <span class="math inline">\(MS_i\)</span>. The <strong>Satterthwaite</strong> approximation:</p>
<p><span class="math display">\[
\frac{(df)\hat{S}}{S} \sim \chi^2_{df}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
df = \frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}
\]</span></p>
<p>An approximate <span class="math inline">\(1-\alpha\)</span> confidence interval for S:</p>
<p><span class="math display">\[
\frac{(df)\hat{S}}{\chi^2_{1-\alpha/2;df}} \le S \le \frac{(df)\hat{S}}{\chi^2_{\alpha/2;df}}
\]</span></p>
<p>For the single factor random effects model</p>
<p><span class="math display">\[
\frac{(df)s^2_\mu}{\chi^2_{1-\alpha/2;df}} \le \sigma^2_\mu \le \frac{(df)s^2_\mu}{\chi^2_{\alpha/2;df}}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
df = \frac{(sn^2_\mu)^2}{\frac{(MSTR)^2}{a-1}+ \frac{(MSE)^2}{a(n-1)}}
\]</span></p>
</div>
</div>
<div id="random-treatment-effects-model" class="section level4" number="21.1.2.2">
<h4>
<span class="header-section-number">21.1.2.2</span> Random Treatment Effects Model<a class="anchor" aria-label="anchor" href="#random-treatment-effects-model"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\tau_i = \mu_i - E(\mu_i) = \mu_i - \mu
\]</span></p>
<p>we have <span class="math inline">\(\mu_i = \mu + \tau_i\)</span> and</p>
<p><span class="math display">\[
Y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu\)</span> = constant, common to all observations<br>
</li>
<li>
<span class="math inline">\(\tau_i \sim N(0,\sigma^2_\tau)\)</span> independent (random variables)<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0,\sigma^2)\)</span> independent.<br>
</li>
<li>
<span class="math inline">\(\tau_{i}, \epsilon_{ij}\)</span> are independent (i=1,…,a; j =1,..,n)</li>
<li>our model is concerned with only balanced single factor ANOVA.</li>
</ul>
<p><br></p>
<p><strong>Diagnostics Measures</strong></p>
<ul>
<li>Non-constant error variance (plots, Levene test, Hartley test).<br>
</li>
<li>Non-independence of errors (plots, Durban-Watson test).<br>
</li>
<li>Outliers (plots, regression methods).<br>
</li>
<li>Non-normality of error terms (plots, Shapiro-Wilk, Anderson-Darling).<br>
</li>
<li>Omitted Variable Bias (plots)</li>
</ul>
<p><strong>Remedial</strong></p>
<ul>
<li>
<a href="linear-regression.html#weighted-least-squares">Weighted Least Squares</a><br>
</li>
<li><a href="linear-regression.html#transformations">Transformations</a></li>
<li>Non-parametric Procedures.</li>
</ul>
<p><strong>Note</strong></p>
<ul>
<li>
<p>Fixed effect ANOVA is relatively robust to</p>
<ul>
<li>non-normality<br>
</li>
<li>unequal variances when sample sizes are approximately equal; at least the F-test and multiple comparisons. However, single comparisons of treatment means are sensitive to unequal variances.<br>
</li>
</ul>
</li>
<li><p>Lack of independence can seriously affect both fixed and random effect ANVOA.</p></li>
</ul>
</div>
</div>
<div id="two-factor-fixed-effect-anova" class="section level3" number="21.1.3">
<h3>
<span class="header-section-number">21.1.3</span> Two Factor Fixed Effect ANOVA<a class="anchor" aria-label="anchor" href="#two-factor-fixed-effect-anova"><i class="fas fa-link"></i></a>
</h3>
<p>The multi-factor experiment is</p>
<ul>
<li>more efficient</li>
<li>provides more info<br>
</li>
<li>gives more validity to the findings.</li>
</ul>
<div id="balanced" class="section level4" number="21.1.3.1">
<h4>
<span class="header-section-number">21.1.3.1</span> Balanced<a class="anchor" aria-label="anchor" href="#balanced"><i class="fas fa-link"></i></a>
</h4>
<p>Assumption:</p>
<ul>
<li>All treatment sample sizes are equal<br>
</li>
<li>All treatment means are of equal importance</li>
</ul>
<p>Assume:</p>
<ul>
<li>Factor A has <code>a</code> levels and Factor B has <code>b</code> levels. All a x b factor levels are considered.<br>
</li>
<li>The number of treatments for each level is n. <span class="math inline">\(N = abn\)</span> observations in the study.</li>
</ul>
<div id="cell-means-model-1" class="section level5" number="21.1.3.1.1">
<h5>
<span class="header-section-number">21.1.3.1.1</span> Cell Means Model<a class="anchor" aria-label="anchor" href="#cell-means-model-1"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
Y_{ijk} = \mu_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{ij}\)</span> are fixed parameters (cell means)<br>
</li>
<li>
<span class="math inline">\(i = 1,...,a\)</span> = the levels of Factor A<br>
</li>
<li>
<span class="math inline">\(j = 1,...,b\)</span> = the levels of Factor B.<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\)</span> for <span class="math inline">\(i = 1,...,a\)</span>, <span class="math inline">\(j = 1,..,b\)</span> and <span class="math inline">\(k = 1,..,n\)</span>
</li>
</ul>
<p>And</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{ij} \\
var(Y_{ijk}) = var(\epsilon_{ijk}) = \sigma^2
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
Y_{ijk} \sim \text{indep } N(\mu_{ij},\sigma^2)
\]</span></p>
<p>And the model is<br></p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \beta + \epsilon
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
E(\mathbf{Y}) = \mathbf{X}\beta \\
var(\mathbf{Y}) = \sigma^2 \mathbf{I}
\]</span></p>
<p><strong>Interaction</strong></p>
<p><span class="math display">\[
(\alpha \beta)_{ij} = \mu_{ij} - (\mu_{..}+ \alpha_i + \beta_j)
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..} = \sum_i \sum_j \mu_{ij}/ab\)</span> is the grand mean<br>
</li>
<li>
<span class="math inline">\(\alpha_i = \mu_{i.}-\mu_{..}\)</span> is the main effect for factor A at the i-th level<br>
</li>
<li>
<span class="math inline">\(\beta_j = \mu_{.j} - \mu_{..}\)</span> is the main effect for factor B at the j-th level<br>
</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij}\)</span> is the interaction effect when factor A is at the i-th level and factor B is at the j-th level.<br>
</li>
<li><span class="math inline">\((\alpha \beta)_{ij} = \mu_{ij} - \mu_{i.}-\mu_{.j}+ \mu_{..}\)</span></li>
</ul>
<p>Examine interactions:</p>
<ul>
<li>Examine whether all <span class="math inline">\(\mu_{ij}\)</span> can be expressed as the sums <span class="math inline">\(\mu_{..} + \alpha_i + \beta_j\)</span><br>
</li>
<li>Examine whether the difference between the mean responses for any two levels of factor B is the same for all levels of factor A.<br>
</li>
<li>Examine whether the difference between the mean response for any two levels of factor A is the same for all levels of factor B<br>
</li>
<li>Examine whether the treatment mean curves for the different factor levels in a treatment plot are parallel.</li>
</ul>
<p>For <span class="math inline">\(j = 1,...,b\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\sum_i(\alpha \beta)_{ij} &amp;= \sum_i (\mu_{ij} - \mu_{..} - \alpha_i - \beta_j) \\
&amp;= \sum_i \mu_{ij} - a \mu_{..} - \sum_i \alpha_i - a \beta_j \\
&amp;= a \mu_{.j} - a \mu_{..}- \sum_i (\mu_{i.} - \mu_{..}) - a(\mu_{.j}-\mu_{..}) \\
&amp;= a \mu_{.j} - a \mu_{..} - a \mu_{..}+ a \mu_{..} - a (\mu_{.j} - \mu_{..}) \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>Similarly, <span class="math inline">\(\sum_j (\alpha \beta) = 0, i = 1,...,a\)</span> and <span class="math inline">\(\sum_i \sum_j (\alpha \beta)_{ij} =0\)</span>, <span class="math inline">\(\sum_i \alpha_i = 0\)</span>, <span class="math inline">\(\sum_j \beta_j = 0\)</span></p>
</div>
<div id="factor-effects-model" class="section level5" number="21.1.3.1.2">
<h5>
<span class="header-section-number">21.1.3.1.2</span> Factor Effects Model<a class="anchor" aria-label="anchor" href="#factor-effects-model"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\mu_{ij} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} \\
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span> is a constant<br>
</li>
<li>
<span class="math inline">\(\alpha_i\)</span> are constants subject to the restriction <span class="math inline">\(\sum_i \alpha_i=0\)</span><br>
</li>
<li>
<span class="math inline">\(\beta_j\)</span> are constants subject to the restriction <span class="math inline">\(\sum_j \beta_j = 0\)</span><br>
</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij}\)</span> are constants subject to the restriction <span class="math inline">\(\sum_i(\alpha \beta)_{ij} = 0\)</span> for <span class="math inline">\(j=1,...,b\)</span> and <span class="math inline">\(\sum_j(\alpha \beta)_{ij} = 0\)</span> for <span class="math inline">\(i = 1,...,a\)</span><br>
</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\)</span> for <span class="math inline">\(k = 1,..,n\)</span>
</li>
</ul>
<p>We have</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}\\
var(Y_{ijk}) = \sigma^2 \\
Y_{ijk} \sim N (\mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}, \sigma^2)
\]</span></p>
<p>We have <span class="math inline">\(1+a+b+ab\)</span> parameters. But there are <span class="math inline">\(ab\)</span> parameters in the <a href="analysis-of-variance-anova.html#cell-means-model-1">Cell Means Model</a>. In the <a href="analysis-of-variance-anova.html#factor-effects-model">Factor Effects Model</a>, the restrictions limit the number of parameters that can be estimated:</p>
<p><span class="math display">\[
1 \text{ for } \mu_{..} \\
(a-1) \text{ for } \alpha_i \\
(b-1) \text{ for } \beta_j \\
(a-1)(b-1) \text{ for } (\alpha \beta)_{ij}
\]</span></p>
<p>Hence, there are</p>
<p><span class="math display">\[
1 + a - 1 + b - 1 + ab - a- b + 1 = ab
\]</span></p>
<p>parameters in the model.</p>
<p>We can have several restrictions when considering the model in the form <span class="math inline">\(\mathbf{Y} = \mathbf{X} \beta + \epsilon\)</span></p>
<p>One way:</p>
<p><span class="math display">\[
\alpha_a  = \alpha_1 - \alpha_2 - ... - \alpha_{a-1} \\
\beta_b = -\beta_1 - \beta_2 - ... - \beta_{b-1} \\
(\alpha \beta)_{ib} = -(\alpha \beta)_{i1} -(\alpha \beta)_{i2} -...-(\alpha \beta)_{i,b-1} ; i = 1,..,a \\
(\alpha \beta)_{aj} = -(\alpha \beta)_{1j}-(\alpha \beta)_{2j} - ... -(\alpha \beta)_{a-1,j}; j = 1,..,b
\]</span></p>
<p>We can fit the model by least squares or maximum likelihood</p>
<p><strong>Cell Means Model</strong><br>
minimize<br></p>
<p><span class="math display">\[
Q = \sum_i \sum_j \sum_k (Y_{ijk}-\mu_{ij})^2
\]</span></p>
<p>estimators</p>
<p><span class="math display">\[
\hat{\mu}_{ij}= \bar{Y}_{ij} \\
\hat{Y}_{ijk} = \bar{Y}_{ij} \\
e_{ijk} = Y_{ijk} - \hat{Y}_{ijk} = Y_{ijk} - \bar{Y}_{ij}
\]</span></p>
<p><strong>Factor Effects Model</strong></p>
<p><span class="math display">\[
Q = \sum_i \sum_j \sum_k (Y_{ijk} - \mu_{..}-\alpha_i = \beta_j - (\alpha \beta)_{ij})^2
\]</span></p>
<p>subject to the restrictions</p>
<p><span class="math display">\[
\sum_i \alpha_i = 0 \\
\sum_j \beta_j = 0 \\
\sum_i (\alpha \beta)_{ij} = 0 \\
\sum_j (\alpha \beta)_{ij} = 0
\]</span></p>
<p>estimators</p>
<p><span class="math display">\[
\hat{\mu}_{..} = \bar{Y}_{...} \\
\hat{\alpha}_i = \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\beta}_j = \bar{Y}_{.j.}-\bar{Y}_{...} \\
(\hat{\alpha \beta})_{ij} = \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.}+ \bar{Y}_{...}
\]</span></p>
<p>The fitted values</p>
<p><span class="math display">\[
\hat{Y}_{ijk} = \bar{Y}_{...}+ (\bar{Y}_{i..}- \bar{Y}_{...})+ (\bar{Y}_{.j.}- \bar{Y}_{...}) + (\bar{Y}_{ij.} - \bar{Y}_{i..}-\bar{Y}_{.j.}+\bar{Y}_{...}) = \bar{Y}_{ij.}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
e_{ijk} = Y_{ijk} - \bar{Y}_{ij.} \\
e_{ijk} \sim \text{ indep } (0,\sigma^2)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
s^2_{\hat{\mu}..} = \frac{MSE}{nab} \\
s^2_{\hat{\alpha}_i} = MSE(\frac{1}{nb} - \frac{1}{nab}) \\
s^2_{\hat{\beta}_j} = MSE(\frac{1}{na} - \frac{1}{nab}) \\
s^2_{(\hat{\alpha\beta})_{ij}} = MSE (\frac{1}{n} - \frac{1}{na}- \frac{1}{nb} + \frac{1}{nab})
\]</span></p>
<div id="partitioning-the-total-sum-of-squares" class="section level6" number="21.1.3.1.2.1">
<h6>
<span class="header-section-number">21.1.3.1.2.1</span> Partitioning the Total Sum of Squares<a class="anchor" aria-label="anchor" href="#partitioning-the-total-sum-of-squares"><i class="fas fa-link"></i></a>
</h6>
<p><span class="math display">\[
Y_{ijk} - \bar{Y}_{...} = \bar{Y}_{ij.} - \bar{Y}_{...} + Y_{ijk} - \bar{Y}_{ij.}
\]</span></p>
<p><span class="math inline">\(Y_{ijk} - \bar{Y}_{...}\)</span>: Total deviation<br><span class="math inline">\(\bar{Y}_{ij.} - \bar{Y}_{...}\)</span>: Deviation of treatment mean from overall mean<br><span class="math inline">\(Y_{ijk} - \bar{Y}_{ij.}\)</span>: Deviation of observation around treatment mean (residual).</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{...})^2 &amp;= n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{...})^2+ \sum_i \sum_j sum_k (Y_{ijk} - \bar{ij.})^2 \\
SSTO &amp;= SSTR + SSE
\end{aligned}
\]</span></p>
<p>(cross product terms are 0)</p>
<p><span class="math display">\[
\bar{Y}_{ij.}- \bar{Y}_{...} = \bar{Y}_{i..}-\bar{Y}_{...} + \bar{Y}_{.j.}-\bar{Y}_{...} + \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...}
\]</span></p>
<p>squaring and summing:<br></p>
<p><span class="math display">\[
\begin{aligned}
n\sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{...})^2 &amp;= nb\sum_i (\bar{Y}_{i..}-\bar{Y}_{...})^2 + na \sum_j (\bar{Y}_{.j.}-\bar{Y}_{...})^2 + n \sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{i..}- \bar{Y}_{.j.}+ \bar{Y}_{...})^2 \\
SSTR &amp;= SSA + SSB + SSAB
\end{aligned}
\]</span></p>
<p>The interaction term from</p>
<p><span class="math display">\[
SSAB = SSTO - SSE - SSA - SSB \\
SSAB = SSTR - SSA - SSB
\]</span></p>
<p>where</p>
<ul>
<li>SSA is the factor A sum of squares (measures the variability of the estimated factor A level means <span class="math inline">\(\bar{Y}_{i..}\)</span>)- the more variable, the larger SSA<br>
</li>
<li>SSB is the factor B sum of squares<br>
</li>
<li>SSAB is the interaction sum of squares, measuring the variability of the estimated interactions.</li>
</ul>
</div>
<div id="partitioning-the-df" class="section level6" number="21.1.3.1.2.2">
<h6>
<span class="header-section-number">21.1.3.1.2.2</span> Partitioning the df<a class="anchor" aria-label="anchor" href="#partitioning-the-df"><i class="fas fa-link"></i></a>
</h6>
<p><span class="math inline">\(N = abn\)</span> cases and <span class="math inline">\(ab\)</span> treatments.</p>
<p>For one-way ANOVA and regression, the partition has df:</p>
<p><span class="math display">\[
SS: SSTO = SSTR + SSE \\
df: N-1 = (ab-1) + (N-ab)
\]</span></p>
<p>we must further partition the <span class="math inline">\(ab-1\)</span> df with SSTR</p>
<p><span class="math display">\[
SSTR = SSA + SSB + SSAB \\
ab-1 = (a-1) + (b-1) + (a-1)(b-1)
\]</span></p>
<ul>
<li>
<span class="math inline">\(df_{SSA} = a-1\)</span>: a treatment deviations but 1 df is lost due to the restriction <span class="math inline">\(\sum (\bar{Y}_{i..}- \bar{Y}_{...})=0\)</span><br>
</li>
<li>
<span class="math inline">\(df_{SSB} = b-1\)</span>: b treatment deviations but 1 df is lost due to the restriction <span class="math inline">\(\sum (\bar{Y}_{.j.}- \bar{Y}_{...})=0\)</span><br>
</li>
<li>
<span class="math inline">\(df_{SSAB} = (a-1)(b-1)= (ab-1)-(a-1)-(b-1)\)</span>: ab interactions, there are (a+b-1) restrictions, so df = ab-a-(b-1)= (a-1)(b-1)</li>
</ul>
</div>
<div id="mean-squares" class="section level6" number="21.1.3.1.2.3">
<h6>
<span class="header-section-number">21.1.3.1.2.3</span> Mean Squares<a class="anchor" aria-label="anchor" href="#mean-squares"><i class="fas fa-link"></i></a>
</h6>
<p><span class="math display">\[
MSA = \frac{SSA}{a-1}\\
MSB = \frac{SSB}{b-1}\\
MSAB = \frac{SSAB}{(a-1)(b-1)}
\]</span></p>
<p>The expected mean squares are</p>
<p><span class="math display">\[
E(MSE) = \sigma^2 \\
E(MSA) = \sigma^2 + nb \frac{\sum \alpha_i^2}{a-1} = \sigma^2 + nb \frac{\sum(\sum_{i.}-\mu_{..})^2}{a-1}  \\
E(MSB) = \sigma^2 + na \frac{\sum \beta_i^2}{b-1} = \sigma^2 + na \frac{\sum(\sum_{.j}-\mu_{..})^2}{b-1} \\
E(MSAB) = \sigma^2 + n \frac{\sum \sum (\alpha \beta)_{ij}^2}{(a-1)(b-1)} = \sigma^2 + n \frac{\sum (\mu_{ij}- \mu_{i.}- \mu_{.j}+ \mu_{..} )^2}{(a-1)(b-1)}
\]</span></p>
<p>If there are no factor A main effects (all <span class="math inline">\(\mu_{i.} = 0\)</span> or <span class="math inline">\(\alpha_i = 0\)</span>) the MSA and MSE have the same expectation; otherwise MSA &gt; MSE. Same for factor B, and interaction effects. which case we can examine F-statistics.</p>
<p><strong>Interaction</strong></p>
<p><span class="math display">\[
\begin{aligned}
H_0: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} = 0 &amp;&amp; \text{for all i,j} \\
H_a: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} \neq 0 &amp;&amp; \text{for some i,j}
\end{aligned}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
H_0: \text{All}(\alpha \beta)_{ij} = 0 \\
H_a: \text{Not all} (\alpha \beta) = 0
\]</span></p>
<p>Let <span class="math inline">\(F = \frac{MSAB}{MSE}\)</span>. When <span class="math inline">\(H_0\)</span> is true <span class="math inline">\(F \sim f_{((a-1)(b-1),ab(n-1))}\)</span>. So reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(F &gt; f_{((a-1)(b-1),ab(n-1))}\)</span></p>
<p>Factor A main effects:<br></p>
<p><span class="math display">\[
H_0: \mu_{1.} = \mu_{2.} = ... = \mu_{a.} \\
H_a: \text{Not all $\mu_{i.}$ are equal}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
H_0: \alpha_1 = ... = \alpha_a = 0 \\
H_a: \text{Not all $\alpha_i$ are equal to 0}
\]</span></p>
<p><span class="math inline">\(F= \frac{MSA}{MSE}\)</span> and reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F&gt;f_{(1-\alpha;a-1,ab(n-1))}\)</span></p>
</div>
<div id="two-way-anova" class="section level6" number="21.1.3.1.2.4">
<h6>
<span class="header-section-number">21.1.3.1.2.4</span> Two-way ANOVA<a class="anchor" aria-label="anchor" href="#two-way-anova"><i class="fas fa-link"></i></a>
</h6>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="31%">
<col width="8%">
<col width="17%">
<col width="28%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td>SSA</td>
<td>a-1</td>
<td>MSA = SSA/(a-1)</td>
<td>MSA/MSE</td>
</tr>
<tr class="even">
<td>Factor B</td>
<td>SSB</td>
<td>b-1</td>
<td>MSB = SSB/(b-1)</td>
<td>MSB/MSE</td>
</tr>
<tr class="odd">
<td>AB interactions</td>
<td>SSAB</td>
<td>(a-1)(b-1)</td>
<td>MSAB = SSAB /MSE</td>
<td></td>
</tr>
<tr class="even">
<td>Error</td>
<td>SSE</td>
<td>ab(n-1)</td>
<td>MSE = SSE/ab(n-1)</td>
<td></td>
</tr>
<tr class="odd">
<td>Total (corrected)</td>
<td>SSTO</td>
<td>abn - 1</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Doing 2-way ANOVA means you always check interaction first, because if there are significant interactions, checking the significance of the main effects becomes moot.</p>
<p>The main effects concern the mean responses for levels of one factor averaged over the levels of the other factor. WHen interaction is present, we can’t conclude that a given factor has no effect, even if these averages are the same. It means that the effect of the factor depends on the level of the other factor.</p>
<p>On the other hand, if you can establish that there is no interaction, then you can consider inference on the factor main effects, which are then said to be <strong>additive</strong>.<br>
And we can also compare factor means like the <a href="analysis-of-variance-anova.html#single-factor-fixed-effects-model">Single Factor Fixed Effects Model</a> using <a href="analysis-of-variance-anova.html#tukey">Tukey</a>, <a href="analysis-of-variance-anova.html#scheffe">Scheffe</a>, <a href="analysis-of-variance-anova.html#bonferroni">Bonferroni</a>.</p>
<p>We can also consider contrasts in the 2-way model</p>
<p><span class="math display">\[
L = \sum c_i \mu_i
\]</span></p>
<p>where <span class="math inline">\(\sum c_i =0\)</span><br>
which is estimated by</p>
<p><span class="math display">\[
\hat{L} = \sum c_i \bar{Y}_{i..}
\]</span></p>
<p>with variance</p>
<p><span class="math display">\[
\sigma^2(\hat{L}) = \frac{\sigma^2}{bn} \sum c_i^2
\]</span></p>
<p>and variance estimate</p>
<p><span class="math display">\[
\frac{MSE}{bn} \sum c_i^2
\]</span></p>
<p><strong>Orthogonal Contrasts</strong></p>
<p><span class="math display">\[
L_1 = \sum c_i \mu_i, \sum c_i = 0 \\
L_2 = \sum d_i \mu_i , \sum d_i = 0
\]</span></p>
<p>these contrasts are said to be <strong>orthogonal</strong> if</p>
<p><span class="math display">\[
\sum \frac{c_i d_i}{n_i} = 0
\]</span></p>
<p>in balanced case <span class="math inline">\(\sum c_i d_i =0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
cov(\hat{L}_1, \hat{L}_2) &amp;= cov(\sum_i c_i \bar{Y}_{i..}, \sum_l d_l \bar{Y}_{l..}) \\
&amp;= \sum_i \sum_l c_i d_l cov(\bar{Y}_{i..},\bar{Y}_{l..}) \\
&amp;= \sum_i c_i d_i \frac{\sigma^2}{bn} = 0
\end{aligned}
\]</span></p>
<p>Orthogonal contrasts can be used to further partition the model sum of squares. There are many sets of orthogonal contrasts and thus, many ways to partition the sum of squares.</p>
<p>A special set of orthogonal contrasts that are used when the levels of a factor can be assigned values on a metric scale are called <strong>orthogonal polynomials</strong></p>
<p>Coefficients can be found for the special case of</p>
<ul>
<li>equal spaced levels (e.g., (0 15 30 45 60))<br>
</li>
<li>equal sample sizes (<span class="math inline">\(n_1 = n_2 = ... = n_{ab}\)</span>)</li>
</ul>
<p>We can define the SS for a given contrast:</p>
<p><span class="math display">\[
SS_L = \frac{\hat{L}^2}{\sum_{i=1}^a (c^2_i/bn_i)}
\]</span></p>
<p><span class="math display">\[
T = \frac{\hat{L}}{\sqrt{MSE\sum_{i=1}^a(c_i^2/bn_i)}} \sim t
\]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[
t^2_{(1-\alpha/2;df)}=F_{(1-\alpha;1,df)}
\]</span></p>
<p>So,</p>
<p><span class="math display">\[
\frac{SS_L}{MSE} \sim F_{(1-\alpha;1,df_{MSE})}
\]</span></p>
<p>all contrasts have d.f = 1</p>
</div>
</div>
</div>
<div id="unbalanced" class="section level4" number="21.1.3.2">
<h4>
<span class="header-section-number">21.1.3.2</span> Unbalanced<a class="anchor" aria-label="anchor" href="#unbalanced"><i class="fas fa-link"></i></a>
</h4>
<p>We could have unequal numbers of replications for all treatment combinations:</p>
<ul>
<li>observational studies<br>
</li>
<li>dropouts in designed studies<br>
</li>
<li>larger sample sizes for inexpensive treatments<br>
</li>
<li>Sample sizes to match population makeup.</li>
</ul>
<p>Assume that each factor combination has at least 1 observation (no empty cells)</p>
<p>Consider the same model as:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where sample sizes are: <span class="math inline">\(n_{ij}\)</span>:</p>
<p><span class="math display">\[
n_{i.} = \sum_j n_{ij} \\
n_{.j} = \sum_i n_{ij} \\
n_T = \sum_i \sum_j n_{ij}
\]</span></p>
<p>Problem here is that</p>
<p><span class="math display">\[
SSTO \neq SSA + SSB + SSAB + SSE
\]</span></p>
<p>(the design is <strong>non-orthogonal</strong>)</p>
<ul>
<li>For <span class="math inline">\(i = 1,...,a-1,\)</span><br>
\begin{equation} u_i = \begin{cases} +1 &amp; \text{if the obs is from the i-th level of Factor 1} \\ -1 &amp; \text{if the obs is from the a-th level of Factor 1} \\ 0 &amp; \text{otherwise} \\ \end{cases} \end{equation}</li>
<li>For <span class="math inline">\(j=1,...,b-1\)</span><br>
\begin{equation} v_i = \begin{cases} +1 &amp; \text{if the obs is from the j-th level of Factor 1} \\ -1 &amp; \text{if the obs is from the b-th level of Factor 1} \\ 0 &amp; \text{otherwise} \\ \end{cases} \end{equation}</li>
</ul>
<p>We can use these indicator variables as predictor variables and <span class="math inline">\(\mu_{..}, \alpha_i ,\beta_j, (\alpha \beta)_{ij}\)</span> as unknown parameters.</p>
<p><span class="math display">\[
Y = \mu_{..} + \sum_{i=1}^{a-1} \alpha_i u_i + \sum_{j=1}^{b-1} \beta_j v_j + \sum_{i=1}^{a-1} \sum_{j=1}^{b-1}(\alpha \beta)_{ij} u_i v_j + \epsilon
\]</span></p>
<p>To test hypotheses, we use the extra sum of squares idea.</p>
<p>For interaction effects</p>
<p><span class="math display">\[
H_0: all (\alpha \beta)_{ij} = 0 \\
H_a: \text{not all }(\alpha \beta)_{ij} =0
\]</span></p>
<p>Or to test</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_2 = \beta_3 = 0 \\
H_a: \text{not all } \beta_j = 0
\]</span></p>
<p><strong>Analysis of Factor Means</strong></p>
<p>(e.g., contrasts) is analogous to the balanced case, with modifications in the formulas for means and standard errors to account for unequal sample sizes.</p>
<p>Or , we can fit the cell means model and consider it from a regression perspective</p>
<p>If you have empty cells (i.e., some factor combinations have no observation), then the equivalent regression approach can’t be used. But you can still do partial analyses</p>
</div>
</div>
<div id="two-way-random-effects-anova" class="section level3" number="21.1.4">
<h3>
<span class="header-section-number">21.1.4</span> Two-Way Random Effects ANOVA<a class="anchor" aria-label="anchor" href="#two-way-random-effects-anova"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span>: constant<br>
</li>
<li>
<span class="math inline">\(\alpha_i \sim N(0,\sigma^2_{\alpha}), i = 1,..,a\)</span> (independent)<br>
</li>
<li>
<span class="math inline">\(\beta_j \sim N(0,\sigma^2_{\beta}), j = 1,..,b\)</span> (independent)<br>
</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij} \sim N(0,\sigma^2_{\alpha \beta}),i=1,...,a,j=1,..,b\)</span> (independent)<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim N(0,\sigma^2)\)</span> (independent)</li>
</ul>
<p>All <span class="math inline">\(\alpha_i, \beta_j, (\alpha \beta)_{ij}\)</span> are pairwise independent</p>
<p>Theoretical means, variances, and covariances are</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} \\
var(Y_{ijk}) = \sigma^2_Y= \sigma^2_\alpha + \sigma^2_\beta +  \sigma^2_{\alpha \beta} + \sigma^2
\]</span></p>
<p>So</p>
<p><span class="math inline">\(Y_{ijk} \sim N(\mu_{..},\sigma^2_\alpha + \sigma^2_\beta + \sigma^2_{\alpha \beta} + \sigma^2)\)</span></p>
<p><span class="math display">\[
cov(Y_{ijk},Y_{ij'k'}) = \sigma^2_{\alpha}, j \neq j' \\
cov(Y_{ijk},Y_{i'jk'}) = \sigma^2_{\beta}, i \neq i'\\
cov(Y_{ijk},Y_{ijk'}) = \sigma^2_\alpha + \sigma^2_{\beta} + \sigma^2_{\alpha \beta}, k \neq k' \\
cov(Y_{ijk},Y_{i'j'k'}) = , i \neq i', j \neq j'
\]</span></p>
</div>
<div id="two-way-mixed-effects-anova" class="section level3" number="21.1.5">
<h3>
<span class="header-section-number">21.1.5</span> Two-Way Mixed Effects ANOVA<a class="anchor" aria-label="anchor" href="#two-way-mixed-effects-anova"><i class="fas fa-link"></i></a>
</h3>
<div id="balanced-1" class="section level4" number="21.1.5.1">
<h4>
<span class="header-section-number">21.1.5.1</span> Balanced<a class="anchor" aria-label="anchor" href="#balanced-1"><i class="fas fa-link"></i></a>
</h4>
<p>One fixed factor, while other is random treatment levels, we have a <strong>mixed effects model</strong> or a <strong>mixed model</strong></p>
<p><strong>Restricted mixed model</strong> for 2-way ANOVA:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span>: constant<br>
</li>
<li>
<span class="math inline">\(\alpha_i\)</span>: fixed effects with constraints subject to restriction <span class="math inline">\(\sum \alpha_i = 0\)</span><br>
</li>
<li>
<span class="math inline">\(\beta_j \sim indep N(0,\sigma^2_\beta)\)</span><br>
</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij} \sim N(0,\frac{a-1}{a}\sigma^2_{\alpha \beta})\)</span> subject to restriction <span class="math inline">\(\sum_i (\alpha \beta)_{ij} = 0\)</span> for all j, the variance here is written as the proportion for convenience; it makes the expected mean squares simpler (other assumed <span class="math inline">\(var((\alpha \beta)_{ij}= \sigma^2_{\alpha \beta}\)</span>)<br>
</li>
<li>
<span class="math inline">\(cov((\alpha \beta)_{ij},(\alpha \beta)_{i'j'}) = - \frac{1}{a} \sigma^2_{\alpha \beta}, i \neq i'\)</span><br>
</li>
<li>
<span class="math inline">\(\epsilon_{ijk}\sim indepN(0,\sigma^2)\)</span><br>
</li>
<li>
<span class="math inline">\(\beta_j, (\alpha \beta)_{ij}, \epsilon_{ijk}\)</span> are pairwise independent</li>
</ul>
<p>Two-way mixed models are written in an “unrestricted” form, with no restrictions on the interaction effects <span class="math inline">\((\alpha \beta)_{ij}\)</span>, they are pairwise independent. Let <span class="math inline">\(\beta^*, (\alpha \beta)^*_{ij}\)</span> be the unrestricted random effects, and <span class="math inline">\((\bar{\alpha \beta})_{ij}^*\)</span> the means averaged over the fixed factor for each level of random factor B.</p>
<p><span class="math display">\[
\beta_j = \beta_j^* + (\bar{\alpha \beta})_{ij}^* \\
(\alpha \beta)_{ij} = (\alpha \beta)_{ij}^* - (\bar{\alpha \beta})_{ij}^*
\]</span></p>
<p>Some consider the restricted model to be more general. but here we consider the restricted form.</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} + \alpha_i \\
var(Y_{ijk}) = \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} + \sigma^2
\]</span></p>
<p>Responses from the same random factor (B) level are correlated</p>
<p><span class="math display">\[
cov(Y_{ijk},Y_{ijk'}) = E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\
= \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} , k \neq k'
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
cov(Y_{ijk},Y_{i'jk'}) = \sigma^2_\beta - \frac{1}{a} \sigma^2_{\alpha\ \beta}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) = 0,  j \neq j'
\]</span></p>
<p>Hence, you can see that the only way you don’t have dependence in the Y is when they don’t share the same random effect.</p>
<p>An advantage of the <strong>restricted mixed model</strong> is that 2 observations from the same random factor b level can be positively or negatively correlated. In the <strong>unrestricted model</strong>, they can only be positively correlated.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="8%">
<col width="9%">
<col width="43%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th>Mean Square</th>
<th>
<p>Fixed ANOVA</p>
<p>(A, B Fixed)</p>
</th>
<th>
<p>Random ANOVA</p>
<p>(A,B random)</p>
</th>
<th>
<p>Mixed ANVOA</p>
<p>(A fixed, B random)</p>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSA</td>
<td>a - 1</td>
<td><span class="math inline">\(\sigma ^2+ n b \frac{\sum\alpha_i^2}{a-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + nb\sigma^ 2_ \alpha +n \sigma^ 2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td>MSB</td>
<td>b-1</td>
<td><span class="math inline">\(\sigma^2 + n a \frac{\sum\beta ^2_j}{b-1}\)</span></td>
<td><span class="math inline">\(\sigma^ 2 + na\sigma^2_ \beta +n \sigma^ 2_{\alpha \beta}\)</span></td>
</tr>
<tr class="odd">
<td>MSAB</td>
<td>( a-1)(b-1)</td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \sum(\alpha \beta )^2_ {ij}} { ( a-1)(b-1)}\)</span></td>
<td><span class="math inline">\(\sigma^2+n \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td>MSE</td>
<td>(n-1)ab</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>For fixed, random, and mixed models (balanced), the ANOVA table sums of squares calculations are identical. (also true for df and mean squares). The only difference is with the expected mean squares, thus the test statistics.</p>
<p>In Random ANOVA, we test</p>
<p><span class="math display">\[
H_0: \sigma^2 = 0 \\
H_a: \sigma^2 &gt; 0
\]</span></p>
<p>by considering <span class="math inline">\(F= \frac{MSA}{MSAB} \sim F_{a-1;(a-1)(b-1)}\)</span></p>
<p>The same test statistic is used for mixed models, but in that case we are testing null hypothesis that all of the <span class="math inline">\(\alpha_i = 0\)</span></p>
<p>The test statistic different for the same null hypothesis under the fixed effects model.</p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="25%">
<col width="23%">
<col width="23%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>Test for effects of</th>
<th>
<p>Fixed ANOVA</p>
<p>(A&amp;B fixed)</p>
</th>
<th>
<p>Random ANOVA</p>
<p>(A&amp;B random)</p>
</th>
<th>
<p>Mixed ANOVA</p>
<p>(A fixed, B random)</p>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td><span class="math inline">\(\frac{MSA}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSA}{MSAB}\)</span></td>
<td><span class="math inline">\(\frac{MSA}{MSAB}\)</span></td>
</tr>
<tr class="even">
<td>Factor B</td>
<td><span class="math inline">\(\frac{MSB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSB}{MSAB}\)</span></td>
<td><span class="math inline">\(\frac{MSB}{MSE}\)</span></td>
</tr>
<tr class="odd">
<td>AB interactions</td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Estimation Of Variance Components</strong></p>
<p>In random and mixed effects models, we are interested in estimating the <strong>variance components</strong><br>
Variance component <span class="math inline">\(\sigma^2_\beta\)</span> in the mixed ANOVA.</p>
<p><span class="math display">\[
E(\sigma^2_\beta) = \frac{E(MSB)-E(MSE)}{na} = \frac{\sigma^2 + na \sigma^2_\beta - \sigma^2}{na} = \sigma^2_\beta
\]</span></p>
<p>which can be estimated with</p>
<p><span class="math display">\[
\hat{\sigma}^2_\beta = \frac{MSB - MSE}{na}
\]</span></p>
<p>Confidence intervals for variance components can be constructed (approximately) by using the <strong>Satterthwaite</strong> procedure or the MLS procedure (like the 1-way random effects)</p>
<p><strong>Estimation of Fixed Effects in Mixed Models</strong></p>
<p><span class="math display">\[
\hat{\alpha}_i = \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\mu}_{i.} = \bar{Y}_{...} + (\bar{Y}_{i..}- \bar{Y}_{...}) = \bar{Y}_{i..}  \\
\sigma^2(\hat{\alpha}_i) = \frac{\sigma^2 + n \sigma^2_{\alpha \beta}}{bn} = \frac{E(MSAB)}{bn} \\
s^2(\hat{\alpha}_i) = \frac{MSAB}{bn}
\]</span></p>
<p>Contrasts on the <strong>Fixed Effects</strong></p>
<p><span class="math display">\[
L = \sum c_i \alpha_i \\
\sum c_i = 0 \\
\hat{L} = \sum c_i \hat{\alpha}_i \\
\sigma^2(\hat{L}) = \sum c^2_i \sigma^2 (\hat{\alpha}_i) \\
s^2(\hat{L}) = \frac{MSAB}{bn} \sum c^2_i
\]</span></p>
<p>Confidence intervals and tests can be constructed as usual</p>
<p><br></p>
</div>
<div id="unbalanced-1" class="section level4" number="21.1.5.2">
<h4>
<span class="header-section-number">21.1.5.2</span> Unbalanced<a class="anchor" aria-label="anchor" href="#unbalanced-1"><i class="fas fa-link"></i></a>
</h4>
<p>For a mixed model with a = 2, b = 4</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk} \\
var(\beta_j)= \sigma^2_\beta \\
var((\alpha \beta)_{ij})= \frac{2-1}{2}\sigma^2_{\alpha \beta} = \frac{\sigma^2_{\alpha \beta}}{2} \\
var(\epsilon_{ijk}) = \sigma^2 \\
E(Y_{ijk}) = \mu_{..} + \alpha_i \\
var(Y_{ijk}) = \sigma^2_{\beta} + \frac{\sigma^2_{\alpha \beta}}{2} + \sigma^2 \\
cov(Y_{ijk},Y_{ijk'}) = \sigma^2 + \frac{\sigma^2_{\alpha \beta}}{2}, k \neq k' \\
cov(Y_{ijk},Y_{i'jk'}) = \sigma^2_{\beta} - \frac{\sigma^2_{\alpha \beta}}{2}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) = 0, j \neq j'
\]</span></p>
<p>assume</p>
<p><span class="math display">\[
\mathbf{Y} \sim N(\mathbf{X}\beta, M)
\]</span></p>
<p>where <span class="math inline">\(M\)</span> is block diagonal</p>
<p>density function</p>
<p><span class="math display">\[
f(\mathbf{Y}) = \frac{1}{(2\pi)^{N/2}|M|^{1/2}}exp(-\frac{1}{2}\mathbf{(Y - X \beta)' M^{-1}(Y-X\beta)})
\]</span></p>
<p>if we knew the variance components, we could use GLS:</p>
<p><span class="math display">\[
\hat{\beta}_{GLS} = \mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}
\]</span></p>
<p>but we usually don’t know the variance components <span class="math inline">\(\sigma^2, \sigma^2_\beta, \sigma^2_{\alpha \beta}\)</span> that make up <span class="math inline">\(M\)</span><br>
Another way to get estimates is by <strong>Maximum likelihood estimation</strong></p>
<p>we try to maximize its log</p>
<p><span class="math display">\[
\ln L = - \frac{N}{2} \ln (2\pi) - \frac{1}{2}\ln|M| - \frac{1}{2} \mathbf{(Y-X \beta)'\Sigma^{-1}(Y-X\beta)}
\]</span></p>
</div>
</div>
</div>
<div id="nonparametric-anova" class="section level2" number="21.2">
<h2>
<span class="header-section-number">21.2</span> Nonparametric ANOVA<a class="anchor" aria-label="anchor" href="#nonparametric-anova"><i class="fas fa-link"></i></a>
</h2>
<div id="kruskal-wallis" class="section level3" number="21.2.1">
<h3>
<span class="header-section-number">21.2.1</span> Kruskal-Wallis<a class="anchor" aria-label="anchor" href="#kruskal-wallis"><i class="fas fa-link"></i></a>
</h3>
<p>Generalization of independent samples Wilcoxon Rank sum test for 2 independent samples (like F-test of one-way ANOVA is a generalization to several independent samples of the two sample t-test)</p>
<p>Consider the one-way case:</p>
<p>We have</p>
<ul>
<li>
<span class="math inline">\(a\ge2\)</span> treatments<br>
</li>
<li>
<span class="math inline">\(n_i\)</span> is the sample size for the ith treatment<br>
</li>
<li>
<span class="math inline">\(Y_{ij}\)</span> is the j-th observation from the ith treatment.<br>
</li>
<li>we make <strong>no</strong> assumption of normality<br>
</li>
<li>We only assume that observations on the ith treatment are a random sample from the continuous CDF <span class="math inline">\(F_i\)</span>, i = 1,..,n, and are mutually independent.</li>
</ul>
<p><span class="math display">\[
H_0: F_1 = F_2 = ... = F_a \\
H_a: F_i &lt; F_j \text{ for some } i \neq j
\]</span></p>
<p>or if distribution is from the location-scale family, <span class="math inline">\(H_0: \theta_1 = \theta_2 = ... = \theta_a\)</span>)</p>
<p><strong>Procedure</strong></p>
<ul>
<li>Rank all <span class="math inline">\(N = \sum_{i=1}^a n_i\)</span> observations in ascending order. Let <span class="math inline">\(r_{ij} = rank(Y_{ij})\)</span>, note <span class="math inline">\(\sum_i \sum_j r_{ij} = 1 + 2 .. + N = \frac{N(N+1)}{2}\)</span><br>
</li>
<li>Calculate the rank sums and averages:<br><span class="math display">\[
r_{i.} = \sum_{j=1}^{n_i} r_{ij}
\]</span> and <span class="math display">\[
\bar{r}_{i.} = \frac{r_{i.}}{n_i}, i = 1,..,a
\]</span>
</li>
<li>Calculate the test statistic on the ranks: <span class="math display">\[
\chi_{KW}^2 = \frac{SSTR}{\frac{SSTO}{N-1}}
\]</span> where <span class="math inline">\(SSTR = \sum n_i (\bar{r}_{i.}- \bar{r}_{..})^2\)</span> and <span class="math inline">\(SSTO = \sum \sum (\bar{r}_{ij}- \bar{r}_{..})^2\)</span>
</li>
<li>FOr large <span class="math inline">\(n_i\)</span> (<span class="math inline">\(\ge 5\)</span> observations) the Kruskal-Wallis statistic is approximated by a <span class="math inline">\(\chi^2_{a-1}\)</span> distribution when all the treatment means are equal. Hence, reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\chi^2_{KW} &gt; \chi^2_{(1-\alpha;a-1)}\)</span>.<br>
</li>
<li>If sample sizes are small, one can exhaustively work out all possible distinct ways of assigning N ranks to the observations from a treatments and calculate the value of the KW statistic in each case (<span class="math inline">\(\frac{N!}{n_1!..n_a!}\)</span> possible combinations). Under <span class="math inline">\(H_0\)</span> all of these assignments are equally likely.</li>
</ul>
</div>
<div id="friedman-test" class="section level3" number="21.2.2">
<h3>
<span class="header-section-number">21.2.2</span> Friedman Test<a class="anchor" aria-label="anchor" href="#friedman-test"><i class="fas fa-link"></i></a>
</h3>
<p>When the responses <span class="math inline">\(Y_{ij} = 1,..,n, j = 1,..,r\)</span> in a randomized complete block design are not normally distributed (or do not have constant variance), a nonparametric test is more helpful.</p>
<p>A distribution-free rank-based test for comparing the treatments in this setting is the Friedman test. Let <span class="math inline">\(F_{ij}\)</span> be the CDF of random <span class="math inline">\(Y_{ij}\)</span>, corresponding to the observed value <span class="math inline">\(y_{ij}\)</span></p>
<p>Under the null hypothesis, <span class="math inline">\(F_{ij}\)</span> are identical for all treatments j separately for each block i.</p>
<p><span class="math display">\[
H_0: F_{i1} = F_{i2} = ... = F_{ir}  \text{ for all i} \\
H_a: F_{ij} &lt; F_{ij'} \text{ for some } j \neq j' \text{ for all } i
\]</span></p>
<p>For location parameter distributions, treatment effects can be tested:</p>
<p><span class="math display">\[
H_0: \tau_1 = \tau_2 = ... = \tau_r \\
H_a: \tau_j &gt; \tau_{j'} \text{ for some } j \neq j'
\]</span></p>
<p><strong>Procedure</strong></p>
<ul>
<li>Rank observations from the r treatments separately within each block (in ascending order; if ties, each tied observation is given the mean of ranks involved). Let the ranks be called <span class="math inline">\(r_{ij}\)</span><br>
</li>
<li>Calculate the Friedman test statistic<br><span class="math display">\[
\chi^2_F = \frac{SSTR}{\frac{SSTR + SSE}{n(r-1)}}
\]</span> where <span class="math display">\[
SSTR = n \sum (\bar{r}_{.j}-\bar{r}_{..})^2 \\
SSE = \sum \sum (r_{ij} - \bar{r}_{.j})^2 \\
\bar{r}_{.j} = \frac{\sum_i r_{ij}}{n}\\
\bar{r}_{..} = \frac{r+1}{2}
\]</span>
</li>
</ul>
<p>If there is no ties, it can be rewritten as</p>
<p><span class="math display">\[
\chi^2_{F} = [\frac{12}{nr(n+1)}\sum_j r_{.j}^2] - 3n(r+1)
\]</span></p>
<p>with large number of blocks, <span class="math inline">\(\chi^2_F\)</span> is approximately <span class="math inline">\(\chi^2_{r-1}\)</span> under <span class="math inline">\(H_0\)</span>. Hence, we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\chi^2_F &gt; \chi^2_{(1-\alpha;r-1)}\)</span><br>
The exact null distribution for <span class="math inline">\(\chi^2_F\)</span> can be derived since there are r! possible ways of assigning ranks 1,2,…,r to the r observations within each block. There are n blocks and thus <span class="math inline">\((r!)^n\)</span> possible assignments to the ranks, which are equally likely when <span class="math inline">\(H_0\)</span> is true.</p>
</div>
</div>
<div id="sample-size-planning-for-anova" class="section level2" number="21.3">
<h2>
<span class="header-section-number">21.3</span> Sample Size Planning for ANOVA<a class="anchor" aria-label="anchor" href="#sample-size-planning-for-anova"><i class="fas fa-link"></i></a>
</h2>
<div id="balanced-designs" class="section level3" number="21.3.1">
<h3>
<span class="header-section-number">21.3.1</span> Balanced Designs<a class="anchor" aria-label="anchor" href="#balanced-designs"><i class="fas fa-link"></i></a>
</h3>
<div id="single-factor-studies" class="section level4" number="21.3.1.1">
<h4>
<span class="header-section-number">21.3.1.1</span> Single Factor Studies<a class="anchor" aria-label="anchor" href="#single-factor-studies"><i class="fas fa-link"></i></a>
</h4>
<div id="fixed-cell-means" class="section level5" number="21.3.1.1.1">
<h5>
<span class="header-section-number">21.3.1.1.1</span> Fixed cell means<a class="anchor" aria-label="anchor" href="#fixed-cell-means"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
P(F&gt;f_{(1-\alpha;a-1,N-a)}|\phi) = 1 - \beta
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the <strong>noncentrality parameter</strong> (measures how unequal the treatment means <span class="math inline">\(\mu_i\)</span> are)</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma}\sqrt{\frac{n}{a}\sum_i (\mu_i - \mu_.)^2} , (n_i \equiv n)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mu_. = \frac{\sum \mu_i}{a}
\]</span></p>
<p>To decide on the power probabilities we use the noncetral F distribution.</p>
<p>We could use the power table directly when effects are fixed and design is balanced by using <strong>minimum range</strong> of factor level means for your desired differences</p>
<p><span class="math display">\[
\Delta = \max(\mu_i) - \min(\mu_i)
\]</span></p>
<p>Hence, we need</p>
<ul>
<li>
<span class="math inline">\(\alpha\)</span> level</li>
<li><span class="math inline">\(\Delta\)</span></li>
<li><span class="math inline">\(\sigma\)</span></li>
<li><span class="math inline">\(\beta\)</span></li>
</ul>
<p>Notes:</p>
<ul>
<li>When <span class="math inline">\(\Delta/\sigma\)</span> is small greatly affects sample size, but if <span class="math inline">\(\Delta/\sigma\)</span> is large.</li>
<li>Reducing <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> increases the required sample sizes.</li>
<li>Error in estimating <span class="math inline">\(\sigma\)</span> can make a large difference.</li>
</ul>
</div>
</div>
<div id="multi-factor-studies" class="section level4" number="21.3.1.2">
<h4>
<span class="header-section-number">21.3.1.2</span> Multi-factor Studies<a class="anchor" aria-label="anchor" href="#multi-factor-studies"><i class="fas fa-link"></i></a>
</h4>
<p>The same noncentral F tables can be used here</p>
<p>For two-factor fixed effect model</p>
<p>Test for interactions:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\alpha \beta_{ij})^2}{(a-1)(b-1)+1}} = \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..})^2}{(a-1)(b-1)+1}} \\
\upsilon_1 = (a-1)(b-1) \\
\upsilon_2 = ab(n-1)
\]</span></p>
<p>Test for Factor A main effects:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{nb \sum \alpha_i^2}{a}} = \frac{1}{\sigma}\sqrt{\frac{nb \sum (\mu_{i.}- \mu_{..})^2}{a}} \\
\upsilon_1 = a-1 \\
\upsilon_2 = ab(n-1)
\]</span></p>
<p>Test for Factor B main effects:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{na \sum \beta_j^2}{b}} = \frac{1}{\sigma}\sqrt{\frac{na \sum (\mu_{.j}- \mu_{..})^2}{b}} \\
\upsilon_1 = b-1 \\
\upsilon_2 = ab(n-1)
\]</span></p>
<p>Procedure:</p>
<ol style="list-style-type: decimal">
<li>Specify the minimu range of Factor A means</li>
<li>Obtain sample sizes with r = a. The resulting sample size is bn, from which n can be obtained.</li>
<li>Repeat the first 2 steps for Factor B minimum range.</li>
<li>Choose the greater number of sample size between A and B.</li>
</ol>
</div>
</div>
<div id="randomized-block-experiments" class="section level3" number="21.3.2">
<h3>
<span class="header-section-number">21.3.2</span> Randomized Block Experiments<a class="anchor" aria-label="anchor" href="#randomized-block-experiments"><i class="fas fa-link"></i></a>
</h3>
<p>Analogous to completely randomized designs . The power of the F-test for treatment effects for randomized block design uses the same non-centrality parameter as completely randomized design:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n}{r} \sum (\mu_i - \mu_.)^2}
\]</span></p>
<p>However, the power level is different from the randomized block design because</p>
<ul>
<li>error variance <span class="math inline">\(\sigma^2\)</span> is different</li>
<li>df(MSE) is different.</li>
</ul>
</div>
</div>
<div id="randomized-block-designs" class="section level2" number="21.4">
<h2>
<span class="header-section-number">21.4</span> Randomized Block Designs<a class="anchor" aria-label="anchor" href="#randomized-block-designs"><i class="fas fa-link"></i></a>
</h2>
<p>To improve the precision of treatment comparisons, we can reduce variability among the experimental units. We can group experimental units into <strong>blocks</strong> so that each block contains relatively homogeneous units.</p>
<ul>
<li>Within each block, random assignment treatments to units (separate random assignment for each block)</li>
<li>The number of units per block is a multiple of the number of factor combinations.</li>
<li>Commonly, use each treatment once in each block.</li>
</ul>
<p>Benefits of <strong>Blocking</strong></p>
<ul>
<li>
<p>Reduction in variability of estimators for treatment means</p>
<ul>
<li>Improved power for t-tests and F-tests</li>
<li>Narrower confidence intervals</li>
<li>Smaller MSE</li>
</ul>
</li>
<li><p>Compare treatments under different conditions (related to different blocks).</p></li>
</ul>
<p>Loss from <strong>Blocking</strong> (little to lose)</p>
<ul>
<li>If you don’t do blocking well, you waste df on negligible block effects that could have been used to estimate <span class="math inline">\(\sigma^2\)</span>
</li>
<li>hence, the df for t-tests and denominator df for F-tests will be reduced without reducing MSE and small loss of power for both tests.</li>
</ul>
<p>Consider</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij} \\
i = 1,2,...,n \\
j = 1,2,..,r
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span>: overall mean response, averaging across all blocks and treatments</li>
<li>
<span class="math inline">\(\rho_i\)</span>: block effect, average difference in response for i-th block (<span class="math inline">\(\sum \rho_i =0\)</span>)</li>
<li>
<span class="math inline">\(\tau_j\)</span> treatment effect, average across blocks (<span class="math inline">\(\sum \tau_j = 0\)</span>)</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim iid N(0,\sigma^2)\)</span>: random experimental error.</li>
</ul>
<p>Here, we assume that the block and treatment effects are additive. The difference in average response for any pair of treatments i the same <strong>within</strong> each block</p>
<p><span class="math display">\[
(\mu_{..} +  \rho_i + \tau_j) - (\mu_{..} + \rho_i + \tau_j') = \tau_j - \tau_j'
\]</span></p>
<p>for all <span class="math inline">\(i=1,..,n\)</span> blocks</p>
<p><span class="math display">\[
\hat{\mu} = \bar{Y}_{..} \\
\hat{\rho}_i = \bar{Y}_{i.} - \bar{Y}_{..} \\
\hat{\tau}_j = \bar{Y}_{.j} - \bar{Y}_{..}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\hat{Y}_{ij} = \bar{Y}_{..} + (\bar{Y}_{i.} - \bar{Y}_{..}) + (\bar{Y}_{.j}- \bar{Y}_{..}) = \bar{Y}_{i.} + \bar{Y}_{.j} - \bar{Y}_{..} \\
e_{ij} = Y_{ij} - \hat{Y}_{ij} = Y_{ij}- \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..}
\]</span></p>
<p><strong>ANOVA table</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="10%">
<col width="39%">
<col width="6%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>SS</th>
<th>df</th>
<th>Fixed Treatments  E(MS)</th>
<th>Random Treatments  E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Blocks</td>
<td><span class="math inline">\(r \sum_i(\bar{Y}_{i.}-\bar{Y}_{..})^2\)</span></td>
<td>n - 1</td>
<td><span class="math inline">\(\sigma^2 +r \frac{\sum \rho^2_i}{n-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + r \frac{\sum \rho^2_i}{n-1}\)</span></td>
</tr>
<tr class="even">
<td>Treatments</td>
<td><span class="math inline">\(n\sum_ j (\bar{Y} _ {.j}-\bar{ Y}_{..})^2\)</span></td>
<td>r - 1</td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\tau\)</span></td>
</tr>
<tr class="odd">
<td>Error</td>
<td><span class="math inline">\(\sum_i \sum _j ( Y_{ ij } - \bar { Y}_{i.} - \bar{Y}_{.j} + \bar{ Y}_{..})^2\)</span></td>
<td>(n-1)(r-1)</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="even">
<td>Total</td>
<td>SSTO</td>
<td>nr-1</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p><strong>F-tests</strong></p>
<p><span class="math display">\[
\begin{aligned}
H_0: \tau_1 = \tau_2 = ... = \tau_r = 0 &amp;&amp; \text{Fixed Treatment Effects} \\
H_a: \text{not all } \tau_j = 0 \\
\\
H_0: \sigma^2_{\tau} = 0 &amp;&amp; \text{Random Treatment Effects} \\
H_a: \sigma^2_{\tau} \neq 0
\end{aligned}
\]</span></p>
<p>In both cases <span class="math inline">\(F = \frac{MSTR}{MSE}\)</span>, reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F &gt; f_{(1-\alpha; r-1,(n-1)(r-1))}\)</span></p>
<p>we don’t use F-test to compare blocks, because</p>
<ul>
<li>We have a priori that blocs are different<br>
</li>
<li>Randomization is done “within” block.</li>
</ul>
<p>To estimate the efficiency that was gained by blocking (relative to completely randomized design).</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\sigma}^2_{CR} &amp;= \frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\
\hat{\sigma}^2_{RB} &amp;= MSE \\
\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}} &amp;= \text{above 1} \\
\end{aligned}
\]</span></p>
<p>then a completely randomized experiment would</p>
<p><span class="math display">\[
(\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}}-1)\%%
\]</span></p>
<p>more observations than the randomized block design to get the same MSE</p>
<p>If batches are randomly selected then they are random effects. That is , if the experiment was repeated, a new sample of i batches would be selected,d yielding new values for <span class="math inline">\(\rho_1, \rho_2,...,\rho_i\)</span> then.</p>
<p><span class="math display">\[
\rho_1, \rho_2,...,\rho_j \sim N(0,\sigma^2_\rho)
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span> fixed<br>
</li>
<li>
<span class="math inline">\(\rho_i\)</span>: random iid <span class="math inline">\(N(0,\sigma^2_p)\)</span><br>
</li>
<li>
<span class="math inline">\(\tau_j\)</span> fixed (or random) <span class="math inline">\(\sum \tau_j = 0\)</span><br>
</li>
<li><span class="math inline">\(\epsilon_{ij} \sim iid N(0,\sigma^2)\)</span></li>
</ul>
<p>**Fixed Treatment&amp;&amp;</p>
<p><span class="math display">\[
E(Y_{ij}) = \mu_{..} + \tau_j \\
var(Y_{ij}) = \sigma^2_{\rho} + \sigma^2 \\
cov(Y_{ij},Y_{ij'}) = \sigma^2 , j \neq j' \text{ treatments within same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) = 0 , i \neq i' , j \neq j'
\]</span></p>
<p>Correlation between 2 observations in the same block</p>
<p><span class="math display">\[
\frac{\sigma^2_{\rho}}{\sigma^2 + \sigma^2_{\rho}}
\]</span></p>
<p>The expected MS for the additive fixed treatment effect, random block effect is</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Source</th>
<th>SS</th>
<th>E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Blocks</td>
<td>SSBL</td>
<td><span class="math inline">\(\sigma^2 + r \sigma^2_\rho\)</span></td>
</tr>
<tr class="even">
<td>Treatment</td>
<td>SSTR</td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\)</span></td>
</tr>
<tr class="odd">
<td>Error</td>
<td>SSE</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Interactions and Blocks</strong><br>
without replications within each block for each treatment, we can’t consider interaction between block and treatment when the block effect is fixed. Hence, only in the random block effect, we have</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + (\rho \tau)_{ij} + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span> constant<br>
</li>
<li>
<span class="math inline">\(\rho_i \sim idd N(0,\sigma^2_{\rho})\)</span> random<br>
</li>
<li>
<span class="math inline">\(\tau_j\)</span> fixed (<span class="math inline">\(\sum \tau_j = 0\)</span>)<br>
</li>
<li>
<span class="math inline">\((\rho \tau)_{ij} \sim N(0,\frac{r-1}{r}\sigma^2_{\rho \tau})\)</span> with <span class="math inline">\(\sum_j (\rho \tau)_{ij}=0\)</span> for all i<br>
</li>
<li>
<span class="math inline">\(cov((\rho \tau)_{ij},(\rho \tau)_{ij'})= -\frac{1}{r} \sigma^2_{\rho \tau}\)</span> for <span class="math inline">\(j \neq j'\)</span><br>
</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim iid N(0,\sigma^2)\)</span> random</li>
</ul>
<p>Note: a special case of mixed 2-factor model with 1 observation per “cell”</p>
<p><span class="math display">\[
E(Y_{ij}) = \mu_{..} + \tau_j \\
var(Y_{ij}) = \sigma^2_\rho + \frac{r-1}{r} \sigma^2_{\rho \tau} + \sigma^2 \\
cov(Y_{ij},Y_{ij'}) = \sigma^2_\rho - \frac{1}{r} \sigma^2_{\rho \tau}, j \neq j' \text{ obs from the same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) = 0, i \neq i', j \neq j' \text{ obs from different blocks are independent}
\]</span></p>
<p>The sum of squares and degrees of freedom for interaction model are the same as those for the additive model. The difference exists only with the expected mean squares</p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="11%">
<col width="6%">
<col width="12%">
<col width="66%">
</colgroup>
<thead><tr class="header">
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Blocks</td>
<td>SSBL</td>
<td>n-1</td>
<td><span class="math inline">\(\sigma^2 + r \sigma^2_\rho\)</span></td>
</tr>
<tr class="even">
<td>Treatment</td>
<td>SSTR</td>
<td>r -1</td>
<td><span class="math inline">\(\sigma^2 + \sigma ^2_{\rho \tau} + n \frac{\sum \tau_j^2}{r-1}\)</span></td>
</tr>
<tr class="odd">
<td>Error</td>
<td>SSE</td>
<td>(n-1)(r-1)</td>
<td><span class="math inline">\(\sigma^2 + \sigma ^2_{\rho \tau}\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>No exact test is possible for block effects when interaction is present (Not important if blocks are used primarily to reduce experimental error variability)<br>
</li>
<li>
<span class="math inline">\(E(MSE) = \sigma^2 + \sigma^2_{\rho \tau}\)</span> the error term variance and interaction variance <span class="math inline">\(\sigma^2_{\rho \tau}\)</span>. We can’t estimate these components separately with this model. The two are <strong>confounded</strong>.<br>
</li>
<li>If more than 1 observation per treatment block combination, one can consider interaction with fixed block effects, which is called <strong>generalized randomized block designs</strong> (multifactor analysis).</li>
</ul>
<div id="tukey-test-of-additivity" class="section level3" number="21.4.1">
<h3>
<span class="header-section-number">21.4.1</span> Tukey Test of Additivity<a class="anchor" aria-label="anchor" href="#tukey-test-of-additivity"><i class="fas fa-link"></i></a>
</h3>
<p>(Tukey’s 1 df test for additivity)</p>
<p>formal test of interaction effects between blocks and treatments for a randomized block design. can also considered for testing additivity in 2-way analyses when there is only one observation per cell.</p>
<p>we consider a less restricted interaction term</p>
<p><span class="math display">\[
(\rho \tau)_{ij} = D\rho_i \tau_j \text{(D: Constant)}
\]</span></p>
<p>So,</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + D\rho_i \tau_j + \epsilon_{ij}
\]</span></p>
<p>the least square estimate or MLE for D</p>
<p><span class="math display">\[
\hat{D} = \frac{\sum_i \sum_j \rho_i \tau_j Y_{ij}}{\sum_i \rho_i^2 \sum_j \tau^2_j}
\]</span></p>
<p>replacing the parameters by their estimates</p>
<p><span class="math display">\[
\hat{D} = \frac{\sum_i \sum_j (\bar{Y}_{i.}- \bar{Y}_{..})(\bar{Y}_{.j}- \bar{Y}_{..})Y_{ij}}{\sum_i (\bar{Y}_{i.}- \bar{Y}_{..})^2 \sum_j(\bar{Y}_{.j}- \bar{Y}_{..})^2}
\]</span></p>
<p>Thus, the interaction sum of squares</p>
<p><span class="math display">\[
SSint = \sum_i \sum_j \hat{D}^2(\bar{Y}_{i.}- \bar{Y}_{..})^2(\bar{Y}_{.j}- \bar{Y}_{..})^2
\]</span></p>
<p>The ANOVA decomposition</p>
<p><span class="math display">\[
SSTO = SSBL + SSTR + SSint + SSRem
\]</span></p>
<p>where <span class="math inline">\(SSRem\)</span>: remainder sum of squares</p>
<p><span class="math display">\[
SSRem = SSTO - SSBL - SSTR - SSint
\]</span></p>
<p>if D = 0 (i.e., no interactions of the type <span class="math inline">\(D \rho_i \tau_j\)</span>). SSint and SSRem are independent <span class="math inline">\(\chi^2_{1,rn-r-n}\)</span>.</p>
<p>If D = 0,</p>
<p><span class="math display">\[
F = \frac{SSint/1}{SSRem/(rn-r-n)} \sim f_{(1-\alpha;rn-r-n)}
\]</span></p>
<p>if</p>
<p><span class="math display">\[
H_0: D = 0 \text{ no interaction present} \\
H_a: D \neq 0 \text{ interaction of form $D \rho_i \tau_j$ present}
\]</span></p>
<p>we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F &gt; f_{(1-\alpha;1,nr-r-n)}\)</span></p>
</div>
</div>
<div id="nested-designs" class="section level2" number="21.5">
<h2>
<span class="header-section-number">21.5</span> Nested Designs<a class="anchor" aria-label="anchor" href="#nested-designs"><i class="fas fa-link"></i></a>
</h2>
<p>Let <span class="math inline">\(\mu_{ij}\)</span> be the mean response when factor A is at the i-th level and factor B is at the j-th level.<br>
If the factors are crossed, the jth level of B is the same for all levels of A.<br>
If factor B is nested within A, the j-th level of B when A is at level 1 has nothing in common with the j-th level of B when A is at level 2.</p>
<p>Factors that can’t be manipulated are designated as <strong>classification factors</strong>, as opposed to <strong>experimental factors</strong> (i.e., you assign to the experimental units).</p>
<div id="two-factor-nested-designs" class="section level3" number="21.5.1">
<h3>
<span class="header-section-number">21.5.1</span> Two-Factor Nested Designs<a class="anchor" aria-label="anchor" href="#two-factor-nested-designs"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Consider B is nested within A.</li>
<li>both factors are fixed</li>
<li>All treatment means are equally important.</li>
</ul>
<p><strong>Mean responses</strong></p>
<p><span class="math display">\[
\mu_{i.} = \sum_j \mu_{ij}/b
\]</span></p>
<p>Main effect factor A</p>
<p><span class="math display">\[
\alpha_i = \mu_{i.} - \mu_{..}
\]</span></p>
<p>where <span class="math inline">\(\mu_{..} = \frac{\mu_{ij}}{ab} = \frac{\sum_i \mu_{i.}}{a}\)</span> and <span class="math inline">\(\sum_i \alpha_i = 0\)</span></p>
<p>Individual effects of B is denoted as <span class="math inline">\(\beta_{j(i)}\)</span> where <span class="math inline">\(j(i)\)</span> indicates the j-th level of factor B is nested within the it-h level of factor A</p>
<p><span class="math display">\[
\beta_{j(i)} = \mu_{ij} - \mu_{i.} \\
= \mu_{ij} - \alpha_i - \mu_{..} \\
\sum_j \beta_{j(i)}=0 , i = 1,...,a
\]</span></p>
<p><span class="math inline">\(\beta_{j(i)}\)</span> is the <strong>specific effect</strong> of the jth level of factor B nested within the ith level of factor A. Hence,</p>
<p><span class="math display">\[
\mu_{ij} \equiv \mu_{..} + \alpha_i + \beta_{j(i)} \equiv \mu_{..} + (\mu_{i.} - \mu_{..}) + (\mu_{ij} - \mu_{i.})
\]</span></p>
<p><strong>Model</strong></p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(Y_{ijk}\)</span> response for the kth treatment when factor A is at the i-th level and factor B is at hte jth level (i = 1,..,a; j = 1,..,b; k = 1,..n)<br>
</li>
<li>
<span class="math inline">\(\mu_{..}\)</span> constant<br>
</li>
<li>
<span class="math inline">\(\alpha_i\)</span> constants subject to restriction <span class="math inline">\(\sum_i \alpha_i = 0\)</span><br>
</li>
<li>
<span class="math inline">\(\beta_{j(i)}\)</span> constants subject to restriction <span class="math inline">\(\sum_j \beta_{j(i)} = 0\)</span> for all i<br>
</li>
<li><span class="math inline">\(\epsilon_{ijk} \sim iid N(0,\sigma^2)\)</span></li>
</ul>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} + \alpha_i + \beta_{j(i)} \\
var(Y_{ijk}) = \sigma^2
\]</span></p>
<p>there is no interaction term in a nested model</p>
<p><strong>ANOVA for Two-Factor Nested Designs</strong></p>
<p>Least Squares and MLE estimates</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Parameter</th>
<th>Estimator</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu_{..}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{...}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_i\)</span></td>
<td><span class="math inline">\(\bar{Y}_{i..} - \bar{Y}_{...}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_{j(i)}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{ij.} - \bar{Y}_{i..}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y}_{ijk}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{ij.}\)</span></td>
</tr>
</tbody>
</table></div>
<p>residual <span class="math inline">\(e_{ijk} = Y_{ijk} - \bar{Y}_{ijk}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
SSTO &amp;= SSA + SSB(A) + SSE \\
\sum_i \sum_j \sum_k (Y_{ijk}- \bar{Y}_{...})^2 &amp;= bn \sum_i (\bar{Y}_{i..}- \bar{Y}_{...})^2 + n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{i..})^2 + \sum_i \sum_j \sum_k (Y_{ijk} -\bar{Y}_{ij.})^2
\end{aligned}
\]</span></p>
<p>ANOVA Table</p>
<div class="inline-table"><table style="width:97%;" class="table table-sm">
<colgroup>
<col width="18%">
<col width="7%">
<col width="8%">
<col width="7%">
<col width="54%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td>SSA</td>
<td>a-1</td>
<td>MSA</td>
<td><span class="math inline">\(\sigma^2 + bn \frac{\sum \alpha_i^2}{a-1}\)</span></td>
</tr>
<tr class="even">
<td>Factor B</td>
<td>SSB(A)</td>
<td>a(b-1)</td>
<td>MSB(A)</td>
<td><span class="math inline">\(\sigma^2 + n \frac{\  | | | | um \sum e ta_{i)}^ 2}{a(b-1)}\)</span></td>
</tr>
<tr class="odd">
<td>Error</td>
<td>SSE</td>
<td>ab(n-1)</td>
<td>MSE</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="even">
<td>Total</td>
<td>SSTO</td>
<td>abn -1</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p><strong>Tests For Factor Effects</strong></p>
<p><span class="math display">\[
H_0: \text{ All } \alpha_i =0 \\
H_a: \text{ not all } \alpha_i = 0
\]</span></p>
<p><span class="math inline">\(F = \frac{MSA}{MSE} \sim f_{(1-\alpha;a-1,(n-1)ab)}\)</span> reject if <span class="math inline">\(F &gt; f\)</span></p>
<p><span class="math display">\[
H_0: \text{ All } \beta_{j(i)} =0 \\
H_a: \text{ not all } \beta_{j(i)} = 0
\]</span></p>
<p><span class="math inline">\(F = \frac{MSB(A)}{MSE} \sim f_{(1-\alpha;a(b-1),(n-1)ab)}\)</span> reject <span class="math inline">\(F&gt;f\)</span></p>
<p><strong>Testing Factor Effect Contrasts</strong></p>
<p><span class="math inline">\(L = \sum c_i \mu_i\)</span> where <span class="math inline">\(\sum c_i =0\)</span></p>
<p><span class="math display">\[
\hat{L} = \sum c_i \bar{Y}_{i..} \\
\hat{L} \pm t_{(1-\alpha/2;df)}s(\hat{L})
\]</span></p>
<p>where <span class="math inline">\(s^2(\hat{L}) = \sum c_i^2 s^2(\bar{Y}_{i..})\)</span>, where <span class="math inline">\(s^2(\bar{Y}_{i..}) = \frac{MSE}{bn}, df = ab(n-1)\)</span></p>
<p><strong>Testing Treatment Means</strong></p>
<p><span class="math inline">\(L = \sum c_i \mu_{.j}\)</span> estimated by <span class="math inline">\(\hat{L} = \sum c_i \bar{Y}_{ij}\)</span> with confidence limits:</p>
<p><span class="math display">\[
\hat{L} \pm t_{(1-\alpha/2;(n-1)ab)}s(\hat{L})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
s^2(\hat{L}) = \frac{MSE}{n}\sum c^2_i
\]</span></p>
<p><strong>Unbalanced Nested Two-Factor Designs</strong></p>
<p>If there are different number of levels of factor B for different levels of factor A, then the design is called <strong>unbalanced</strong></p>
<p>The model</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk} \\
i = 1,2;j =1,..,b_i;k=1,..,n_{ij} \\
b_1 = 3, b_2= 2, n_{11} =  n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\
\sum_{i=1}^2 \alpha_i =0 \\
\sum_{j=1}^3 \beta_{j(1)} = 0 \\
\sum_{j=1}^2 \beta_{j(2)}=0
\]</span></p>
<p>where <span class="math inline">\(\alpha_1,\beta_{1(1)}, \beta_{2(1)}, \beta_{1(2)}\)</span> are parameters. And constraints: <span class="math inline">\(\alpha_2 = - \alpha_1, \beta_{3(1)}= - \beta_{1(1)}-\beta_{2(1)}, \beta_{2(2)}=-\beta_{1(2)}\)</span></p>
<p>4 indicator variables</p>
<p><span class="math display">\[\begin{equation}
X_1 =
\begin{cases}
1&amp;\text{if obs from school 1}\\
-1&amp;\text{if obs from school 2}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X_2 =
\begin{cases}
1&amp;\text{if obs from instructor 1 in school 1}\\
-1&amp;\text{if obs from instructor 3 in school 1}\\
0&amp;\text{otherwise}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X_3 =
\begin{cases}
1&amp;\text{if obs from instructor 2 in school 1}\\
-1&amp;\text{if obs from instructor 3 in school 1}\\
0&amp;\text{otherwise}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X_4 =
\begin{cases}
1&amp;\text{if obs from instructor 1 in school 1}\\
-1&amp;\text{if obs from instructor 2 in school 1}\\
0&amp;\text{otherwise}\\
\end{cases}
\end{equation}\]</span></p>
<p>Regression Full Model</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_1 X_{ijk1} + \beta_{1(1)}X_{ijk2} + \beta_{2(1)}X_{ijk3} + \beta_{1(2)}X_{ijk4} + \epsilon_{ijk}
\]</span></p>
<p><strong>Random Factor Effects</strong></p>
<p>If</p>
<p><span class="math display">\[
\alpha_1 \sim iid N(0,\sigma^2_\alpha) \\
\beta_{j(i)} \sim iid N(0,\sigma^2_\beta)
\]</span></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="10%">
<col width="48%">
<col width="40%">
</colgroup>
<thead><tr class="header">
<th>Mean Square</th>
<th>Expected Mean Squares A fixed, B random</th>
<th>Expected Mean Squares A random, B random</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSA</td>
<td><span class="math inline">\(\sigma^ 2 + n \sigma^2_\beta + bn \frac{\sum \alpha_i^2}{a-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + bn \sigma^2_{\alpha} + n \sigma^2_\beta\)</span></td>
</tr>
<tr class="even">
<td>MSB(A)</td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\beta\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\beta\)</span></td>
</tr>
<tr class="odd">
<td>MSE</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>Test Statstics</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Factor A</th>
<th><span class="math inline">\(\frac{MSA}{MSB(A)}\)</span></th>
<th><span class="math inline">\(\frac{MSA}{MSB(A)}\)</span></th>
</tr></thead>
<tbody><tr class="odd">
<td>Factor B(A)</td>
<td><span class="math inline">\(\frac{MSB(A)}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSB(A)}{MSE}\)</span></td>
</tr></tbody>
</table></div>
<p>Another way to increase the precision of treatment comparisons by reducing variability is to use regression models to adjust for differences among experimental units (also known as <strong>analysis of covariance</strong>).</p>
</div>
</div>
<div id="single-factor-covariance-model" class="section level2" number="21.6">
<h2>
<span class="header-section-number">21.6</span> Single Factor Covariance Model<a class="anchor" aria-label="anchor" href="#single-factor-covariance-model"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[
Y_{ij} = \mu_{.} + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) + \epsilon_{ij}
\]</span></p>
<p>for <span class="math inline">\(i = 1,...,r;j=1,..,n_i\)</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mu_.\)</span> overall mean<br>
</li>
<li>
<span class="math inline">\(\tau_i\)</span>: fixed treatment effects (<span class="math inline">\(\sum \tau_i =0\)</span>)<br>
</li>
<li>
<span class="math inline">\(\gamma\)</span>: fixed regression coefficient effect between X and Y<br>
</li>
<li>
<span class="math inline">\(X_{ij}\)</span> covariate (not random)<br>
</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim iid N(0,\sigma^2)\)</span>: random errors</li>
</ul>
<p>If we just use <span class="math inline">\(\gamma X_{ij}\)</span> as the regression term (rather than <span class="math inline">\(\gamma(X_{ij}-\bar{X}_{..})\)</span>), then <span class="math inline">\(\mu_.\)</span> is no longer the overall mean; thus we need to centered mean.</p>
<p><span class="math display">\[
E(Y_{ij}) = \mu_. + \tau_i + \gamma(X_{ij}-\bar{X}_{..}) \\
var(Y_{ij}) = \sigma^2
\]</span></p>
<p><span class="math inline">\(Y_{ij} \sim N(\mu_{ij},\sigma^2)\)</span>, where</p>
<p><span class="math display">\[
\mu_{ij} = \mu_. + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) \\
\sum \tau_i =0
\]</span></p>
<p>Thus, the mean response (<span class="math inline">\(\mu_{ij}\)</span>) is a regression line with intercept <span class="math inline">\(\mu_. + \tau_i\)</span> and slope <span class="math inline">\(\gamma\)</span> for each treatment i.</p>
<p><strong>Assumption</strong>:</p>
<ul>
<li>All treatment regression lines have the same slope<br>
</li>
<li>when treatment interact with covariate X (non-parallel slopes), covariance analysis is <strong>not</strong> appropriate. in which case we should use separate regression lines.</li>
</ul>
<p>More complicated regression features (e.g., quadratic, cubic) or additional covariates e.g.,</p>
<p><span class="math display">\[
Y_{ij} = \mu_. + \tau_i + \gamma_1(X_{ij1}-\bar{X}_{..2}) + \gamma_2(X_{ij2}-\bar{X}_{..2}) + \epsilon_{ij}
\]</span></p>
<p><strong>Regression Formulation</strong></p>
<p>We can use indicator variables for treatments</p>
<p><span class="math display">\[
l_1 =
\begin{cases}
1 &amp; \text{if case is from treatment 1}\\
-1 &amp; \text{if case is from treatment r}\\
0 &amp;\text{otherwise}\\
\end{cases}
. \\
. \\
. \\
l_{r-1} =
\begin{cases}
1 &amp; \text{if case is from treatment r-1}\\
-1 &amp; \text{if case is from treatment r}\\
0 &amp;\text{otherwise}\\
\end{cases}
\]</span></p>
<p>Let <span class="math inline">\(x_{ij} = X_{ij}- \bar{X}_{..}\)</span>. the regression model is</p>
<p><span class="math display">\[
Y_{ij} = \mu_. + \tau_1l_{ij,1} + .. + \tau_{r-1}l_{ij,r-1} + \gamma x_{ij}+\epsilon_{ij}
\]</span></p>
<p>where <span class="math inline">\(I_{ij,1}\)</span> is the indicator variable <span class="math inline">\(l_1\)</span> for the j-th case from treatment i. The treatment effect <span class="math inline">\(\tau_1,..\tau_{r-1}\)</span> are just regression coefficients for the indicator variables.</p>
<p>We could use the same diagnostic tools for this case.</p>
<p><strong>Inference</strong></p>
<p>Treatment effects</p>
<p><span class="math display">\[
H_0: \tau_1 = \tau_2 = ...= 0 \\
H_a: \text{not all } \tau_i =0
\]</span></p>
<p><span class="math display">\[
\text{Full Model}: Y_{ij} = \mu_. + \tau_i + \gamma X_{ij} +\epsilon_{ij}  \\
\text{Reduced Model}: Y_{ij} = \mu_. + \gamma X_{ij} + \epsilon_{ij}
\]</span></p>
<p><span class="math display">\[
F = \frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \frac{SSE(F)}{N-(r+1)} \sim F_{(r-1,N-(r+1))}
\]</span></p>
<p>If we are interested in comparisons of treatment effects.<br>
For example, r - 3. We estimate <span class="math inline">\(\tau_1,\tau_2, \tau_3 = -\tau_1 - \tau_2\)</span></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="25%">
<col width="58%">
</colgroup>
<thead><tr class="header">
<th>Comparison</th>
<th>Estimate</th>
<th>Variance of Estimator</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\tau_1 - \tau_2\)</span></td>
<td><span class="math inline">\(\hat{\tau}_1 - \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(var(\hat {\tau}_1) + var(\hat{\tau}_2) - 2cov(\hat{ \tau}_1\hat{\tau}_2)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\tau_1 - \tau_3\)</span></td>
<td><span class="math inline">\(2 \hat{\tau}_1 + \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(4var(\hat {\tau}_1) + var(\hat{\tau}_2) - 4cov(\hat{ \tau}_1\hat{\tau}_2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\tau_2 - \tau_3\)</span></td>
<td><span class="math inline">\(\hat{\tau}_1 + 2 \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(var(\hat{\tau}_1) + 4var(\hat{\tau}_2) - 4cov(\hat{\tau}_1\hat{\tau}_2)\)</span></td>
</tr>
</tbody>
</table></div>
<p>Testing for Parallel Slopes</p>
<p>Example:</p>
<p>r = 3</p>
<p><span class="math display">\[
Y_{ij} = \mu_{.} + \tau_1 I_{ij,1} + \tau_2 I_{ij,2} + \gamma X_{ij} + \beta_1 I_{ij,1}X_{ij} + \beta_2 I_{ij,2}X_{ij} + \epsilon_{ij}
\]</span></p>
<p>where <span class="math inline">\(\beta_1,\beta_2\)</span>: interaction coefficients.</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_2 = 0 \\
H_a: \text{at least one} \beta \neq 0
\]</span></p>
<p>If we can’t reject <span class="math inline">\(H_0\)</span> using F-test then we have evidence that the slopes are parallel</p>
<p><strong>Adjusted Means</strong></p>
<p>The means in response after adjusting for the covariate effect</p>
<p><span class="math display">\[
Y_{i.}(adj) = \bar{Y}_{i.} - \hat{\gamma}(\bar{X}_{i.} - \bar{X}_{..})
\]</span></p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sampling.html"><span class="header-section-number">20</span> Sampling</a></div>
<div class="next"><a href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#analysis-of-variance-anova"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li>
<a class="nav-link" href="#completely-randomized-design-crd"><span class="header-section-number">21.1</span> Completely Randomized Design (CRD)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#single-factor-fixed-effects-model"><span class="header-section-number">21.1.1</span> Single Factor Fixed Effects Model</a></li>
<li><a class="nav-link" href="#single-factor-random-effects-model"><span class="header-section-number">21.1.2</span> Single Factor Random Effects Model</a></li>
<li><a class="nav-link" href="#two-factor-fixed-effect-anova"><span class="header-section-number">21.1.3</span> Two Factor Fixed Effect ANOVA</a></li>
<li><a class="nav-link" href="#two-way-random-effects-anova"><span class="header-section-number">21.1.4</span> Two-Way Random Effects ANOVA</a></li>
<li><a class="nav-link" href="#two-way-mixed-effects-anova"><span class="header-section-number">21.1.5</span> Two-Way Mixed Effects ANOVA</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#nonparametric-anova"><span class="header-section-number">21.2</span> Nonparametric ANOVA</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#kruskal-wallis"><span class="header-section-number">21.2.1</span> Kruskal-Wallis</a></li>
<li><a class="nav-link" href="#friedman-test"><span class="header-section-number">21.2.2</span> Friedman Test</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sample-size-planning-for-anova"><span class="header-section-number">21.3</span> Sample Size Planning for ANOVA</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#balanced-designs"><span class="header-section-number">21.3.1</span> Balanced Designs</a></li>
<li><a class="nav-link" href="#randomized-block-experiments"><span class="header-section-number">21.3.2</span> Randomized Block Experiments</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#randomized-block-designs"><span class="header-section-number">21.4</span> Randomized Block Designs</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#tukey-test-of-additivity"><span class="header-section-number">21.4.1</span> Tukey Test of Additivity</a></li></ul>
</li>
<li>
<a class="nav-link" href="#nested-designs"><span class="header-section-number">21.5</span> Nested Designs</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#two-factor-nested-designs"><span class="header-section-number">21.5.1</span> Two-Factor Nested Designs</a></li></ul>
</li>
<li><a class="nav-link" href="#single-factor-covariance-model"><span class="header-section-number">21.6</span> Single Factor Covariance Model</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/19-ANOVA.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/19-ANOVA.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2023-06-18.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
