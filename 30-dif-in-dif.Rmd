# Difference-in-Differences {#sec-difference-in-differences}

[Difference-in-Differences](#sec-difference-in-differences) (DID) is a widely used causal inference method for estimating the effect of **policy interventions** or **exogenous shocks** when randomized experiments are not feasible. The key idea behind DID is to compare changes in outcomes over time between **treated** and **control** groups, under the assumption that---absent treatment---both groups would have followed parallel trends.

<!-- [List of packages](https://github.com/lnsongxf/DiD-1) -->

DID analysis can go beyond simple treatment effects by exploring causal mechanisms using mediation and moderation analyses:

-   [Mediation Under DiD]: Examines how intermediate variables (e.g., consumer sentiment, brand perception) mediate the treatment effect [@habel2021variable].
-   [Moderation] Analysis: Studies how treatment effects vary across different groups (e.g., high vs. low brand loyalty) [@goldfarb2011online].

## Empirical Studies

### Applications of DID in Marketing

DID has been extensively applied in marketing and business research to measure the impact of policy changes, advertising campaigns, and competitive actions. Below are several notable examples:

-   **TV Advertising & Online Shopping** [@liaukonyte2015television]: Examines how TV ads influence consumer behavior in online shopping.
-   **Political Advertising & Voting Behavior** [@wang2018border]: Uses geographic discontinuities at state borders to analyze how ad sources and tone affect voter turnout.
-   **Music Streaming & Consumption** [@datta2018changing]: Investigates how adopting a music streaming service affects total music consumption.
-   **Data Breaches & Customer Spending** [@janakiraman2018effect]: Analyzes how customer spending changes after a firm announces a data breach.
-   **Price Monitoring & Policy Enforcement** [@israeli2018online]: Studies the effect of digital monitoring on minimum advertised price policy enforcement.
-   **Foreign Direct Investment & Firm Responses** [@ramani2019effects]: Examines how firms in India responded to FDI liberalization reforms in 1991.
-   **Paywalls & Readership** [@pattabhiramaiah2019paywalls]: Investigates how implementing paywalls affects online news consumption.
-   **Aggregators & Airline Business** [@akca2020value]: Evaluates how online aggregators impact airline ticket sales.
-   **Nutritional Labels & Competitive Response** [@lim2020competitive]: Analyzes whether nutrition labels affect the nutritional quality of competing brands.
-   **Payment Disclosure & Physician Behavior** [@guo2020let]: Studies how payment disclosure laws impact prescription behavior.
-   **Fake Reviews & Sales** [@he2022market]: Uses an Amazon policy change to measure the effect of fake reviews on sales and ratings.
-   **Data Protection Regulations & Website Usage** [@peukert2022regulatory]: Assesses the impact of GDPR regulations on website usage and online business models.

### Applications of DID in Economics

DID has also been extensively applied in **economics**, particularly in policy evaluation, labor economics, and macroeconomics:

-   **Natural Experiments in Development Economics** [@rosenzweig2000natural]
-   **Instrumental Variables & Natural Experiments** [@angrist2001instrumental]
-   **DID in Macroeconomic Policy Analysis** [@fuchs2016natural]

------------------------------------------------------------------------

## Visualization {#sec-visualization-did}

```{r}
library(panelView)
library(fixest)
library(tidyverse)
base_stagg <- fixest::base_stagg |>
    # treatment status
    dplyr::mutate(treat_stat = dplyr::if_else(time_to_treatment < 0, 0, 1)) |> 
    select(id, year, treat_stat, y)

head(base_stagg)

panelView::panelview(
    y ~ treat_stat,
    data = base_stagg,
    index = c("id", "year"),
    xlab = "Year",
    ylab = "Unit",
    display.all = F,
    gridOff = T,
    by.timing = T
)

# alternatively specification
panelView::panelview(
    Y = "y",
    D = "treat_stat",
    data = base_stagg,
    index = c("id", "year"),
    xlab = "Year",
    ylab = "Unit",
    display.all = F,
    gridOff = T,
    by.timing = T
)

# Average outcomes for each cohort
panelView::panelview(
    data = base_stagg, 
    Y = "y",
    D = "treat_stat",
    index = c("id", "year"),
    by.timing = T,
    display.all = F,
    type = "outcome", 
    by.cohort = T
)
```

## Simple Difference-in-Differences {#sec-simple-difference-in-differences}

Difference-in-Differences originated as a tool to analyze [natural experiments](#sec-natural-experiments), but its applications extend far beyond that. DID is built on the [Fixed Effects Estimator], making it a fundamental approach for policy evaluation and causal inference in observational studies.

DID leverages inter-temporal variation between groups:

-   **Cross-sectional comparison**: Helps avoid omitted variable bias due to common trends.
-   **Time-series comparison**: Helps mitigate omitted variable bias due to cross-sectional heterogeneity.

------------------------------------------------------------------------

### Basic Setup of DID

Consider a simple setting with:

-   **Treatment Group** ($D_i = 1$)
-   **Control Group** ($D_i = 0$)
-   **Pre-Treatment Period** ($T = 0$)
-   **Post-Treatment Period** ($T = 1$)

+---------------------+-----------------------------------+------------------------------------+
|                     | **After Treatment (**$T = 1$**)** | **Before Treatment (**$T = 0$**)** |
+=====================+===================================+====================================+
| Treated ($D_i = 1$) | $E[Y_{1i}(1)|D_i = 1]$            | $E[Y_{0i}(0)|D_i = 1]$             |
+---------------------+-----------------------------------+------------------------------------+
| Control ($D_i = 0$) | $E[Y_{0i}(1)|D_i = 0]$            | $E[Y_{0i}(0)|D_i = 0]$             |
+---------------------+-----------------------------------+------------------------------------+

The **fundamental challenge**: We cannot observe $E[Y_{0i}(1)|D_i = 1]$---i.e., the **counterfactual outcome** for the treated group had they not received treatment.

------------------------------------------------------------------------

DID estimates the [Average Treatment Effect on the Treated] using the following formula:

$$
\begin{aligned}
E[Y_1(1) - Y_0(1) | D = 1] &= \{E[Y(1)|D = 1] - E[Y(1)|D = 0] \} \\
&- \{E[Y(0)|D = 1] - E[Y(0)|D = 0] \}
\end{aligned}
$$

This formulation differences out time-invariant unobserved factors, assuming the parallel trends assumption holds.

-   For the treated group, we isolate the difference between being treated and not being treated.
-   If the control group would have experienced a different trajectory, the DID estimate may be biased.
-   Since we cannot observe treatment variation in the control group, we cannot infer the treatment effect for this group.

```{r}
# Load required libraries
library(dplyr)
library(ggplot2)
set.seed(1)

# Simulated dataset for illustration
data <- data.frame(
  time = rep(c(0, 1), each = 50),  # Pre (0) and Post (1)
  treated = rep(c(0, 1), times = 50), # Control (0) and Treated (1)
  error = rnorm(100)
)

# Generate outcome variable
data$outcome <-
    5 + 3 * data$treated + 2 * data$time + 
    4 * data$treated * data$time + data$error

# Compute averages for 2x2 table
table_means <- data %>%
  group_by(treated, time) %>%
  summarize(mean_outcome = mean(outcome), .groups = "drop") %>%
  mutate(
    group = paste0(ifelse(treated == 1, "Treated", "Control"), ", ", 
                   ifelse(time == 1, "Post", "Pre"))
  )

# Display the 2x2 table
table_2x2 <- table_means %>%
  select(group, mean_outcome) %>%
  tidyr::spread(key = group, value = mean_outcome)

print("2x2 Table of Mean Outcomes:")
print(table_2x2)

# Calculate Diff-in-Diff manually

# Treated, Post
Y11 <- table_means$mean_outcome[table_means$group == "Treated, Post"]  

# Treated, Pre
Y10 <- table_means$mean_outcome[table_means$group == "Treated, Pre"]   

# Control, Post
Y01 <- table_means$mean_outcome[table_means$group == "Control, Post"]  

# Control, Pre
Y00 <- table_means$mean_outcome[table_means$group == "Control, Pre"]   

diff_in_diff_formula <- (Y11 - Y10) - (Y01 - Y00)

# Estimate DID using OLS
model <- lm(outcome ~ treated * time, data = data)
ols_estimate <- coef(model)["treated:time"]

# Print results
results <- data.frame(
  Method = c("Diff-in-Diff Formula", "OLS Estimate"),
  Estimate = c(diff_in_diff_formula, ols_estimate)
)

print("Comparison of DID Estimates:")
print(results)

# Visualization
ggplot(data,
       aes(
           x = as.factor(time),
           y = outcome,
           color = as.factor(treated),
           group = treated
       )) +
    stat_summary(fun = mean, geom = "point", size = 3) +
    stat_summary(fun = mean,
                 geom = "line",
                 linetype = "dashed") +
    labs(
        title = "Difference-in-Differences Visualization",
        x = "Time (0 = Pre, 1 = Post)",
        y = "Outcome",
        color = "Group"
    ) +
    scale_color_manual(labels = c("Control", "Treated"),
                       values = c("blue", "red")) +
    causalverse::ama_theme()
```

|              | Control (0)        | Treated (1)         |
|--------------|--------------------|---------------------|
| **Pre (0)**  | $\bar{Y}_{00} = 5$ | $\bar{Y}_{10} = 8$  |
| **Post (1)** | $\bar{Y}_{01} = 7$ | $\bar{Y}_{11} = 14$ |

The table organizes the mean outcomes into four cells:

1.  Control Group, Pre-period ($\bar{Y}_{00}$): Mean outcome for the control group before the intervention.

2.  Control Group, Post-period ($\bar{Y}_{01}$): Mean outcome for the control group after the intervention.

3.  Treated Group, Pre-period ($\bar{Y}_{10}$): Mean outcome for the treated group before the intervention.

4.  Treated Group, Post-period ($\bar{Y}_{11}$): Mean outcome for the treated group after the intervention.

The DID treatment effect calculated from the simple formula of averages is identical to the estimate from an OLS regression with an interaction term.

The treatment effect is calculated as:

$\text{DID} = (\bar{Y}_{11} - \bar{Y}_{10}) - (\bar{Y}_{01} - \bar{Y}_{00})$

Compute manually:

$(\bar{Y}_{11} - \bar{Y}_{10}) - (\bar{Y}_{01} - \bar{Y}_{00})$

Use OLS regression:

$Y_{it} = \beta_0 + \beta_1 \text{treated}_i + \beta_2 \text{time}_t + \beta_3 (\text{treated}_i \cdot \text{time}_t) + \epsilon_{it}$

Using the simulated table:

$\text{DID} = (14 - 8) - (7 - 5) = 6 - 2 = 4$

This matches the **interaction term coefficient** ($\beta_3 = 4$) from the OLS regression.

Both methods give the same result!

------------------------------------------------------------------------

### Extensions of DID

#### DID with More Than Two Groups or Time Periods

DID can be extended to **multiple treatments, multiple controls**, and more than two periods:

$$
Y_{igt} = \alpha_g + \gamma_t + \beta I_{gt} + \delta X_{igt} + \epsilon_{igt}
$$

where:

-   $\alpha_g$ = Group-Specific Fixed Effects (e.g., firm, region).

-   $\gamma_t$ = Time-Specific Fixed Effects (e.g., year, quarter).

-   $\beta$ = DID Effect.

-   $I_{gt}$ = Interaction Terms (Treatment × Post-Treatment).

-   $\delta X_{igt}$ = Additional Covariates.

This is known as the [Two-Way Fixed Effects DID](#sec-two-way-fixed-effects) model. However, TWFE performs poorly under staggered treatment adoption, where different groups receive treatment at different times.

------------------------------------------------------------------------

#### Examining Long-Term Effects (Dynamic DID)

To examine the dynamic treatment effects (that are not under rollout/staggered design), we can create a centered time variable.

+------------------------+---------------------------------------------------------+
| Centered Time Variable | Interpretation                                          |
+========================+=========================================================+
| $t = -2$               | Two periods before treatment                            |
+------------------------+---------------------------------------------------------+
| $t = -1$               | One period before treatment                             |
+------------------------+---------------------------------------------------------+
| $t = 0$                | Last pre-treatment period right before treatment period |
|                        |                                                         |
|                        | (Baseline/Reference Group)                              |
+------------------------+---------------------------------------------------------+
| $t = 1$                | Treatment period                                        |
+------------------------+---------------------------------------------------------+
| $t = 2$                | One period after treatment                              |
+------------------------+---------------------------------------------------------+

**Dynamic Treatment Model Specification**

By interacting this factor variable, we can examine the dynamic effect of treatment (i.e., whether it's fading or intensifying):

$$
\begin{aligned}
Y &= \alpha_0 + \alpha_1 Group + \alpha_2 Time  \\
&+ \beta_{-T_1} Treatment + \beta_{-(T_1 -1)} Treatment + \dots + \beta_{-1} Treatment \\
&+ \beta_1 + \dots + \beta_{T_2} Treatment
\end{aligned}
$$

where:

-   $\beta_0$ (Baseline Period) is the reference group (i.e., drop from the model).

-   $T_1$ = Pre-Treatment Period.

-   $T_2$ = Post-Treatment Period.

-   Treatment coefficients ($\beta_t$) measure the effect over time.

**Key Observations**:

-   Pre-treatment coefficients should be close to zero ($\beta_{-T_1}, \dots, \beta_{-1} \approx 0$), ensuring no pre-trend bias.

-   Post-treatment coefficients should be significantly different from zero ($\beta_1, \dots, \beta_{T_2} \neq 0$), measuring the treatment effect over time.

-   Higher standard errors with more interactions: Including too many lags can reduce precision.

------------------------------------------------------------------------

#### DID on Relationships, Not Just Levels

DID can also be applied to relationships between variables rather than just outcome levels.

For example, DID can be used to estimate treatment effects on regression coefficients by comparing relationships before and after a policy change.

------------------------------------------------------------------------

### Goals of DID

1.  **Pre-Treatment Coefficients Should Be Insignificant**
    -   Ensure that $\beta_{-T_1}, \dots, \beta_{-1} = 0$ (similar to a [Placebo Test](Ensure%20no%20pre-treatment%20effects.)).
2.  **Post-Treatment Coefficients Should Be Significant**
    -   Verify that $\beta_1, \dots, \beta_{T_2} \neq 0$.
    -   Examine whether the trend in post-treatment coefficients is increasing or decreasing over time.

------------------------------------------------------------------------

```{r}
library(tidyverse)
library(fixest)

od <- causaldata::organ_donations %>%
    
    # Treatment variable
    dplyr::mutate(California = State == 'California') %>%
    # centered time variable
    dplyr::mutate(center_time = as.factor(Quarter_Num - 3))  
# where 3 is the reference period precedes the treatment period

class(od$California)
class(od$State)

cali <- feols(Rate ~ i(center_time, California, ref = 0) |
                  State + center_time,
              data = od)

etable(cali)

iplot(cali, pt.join = T)
coefplot(cali)
```

## Empirical Research Walkthrough

### Example: The Unintended Consequences of "Ban the Box" Policies

@doleac2020unintended examine the unintended effects of "Ban the Box" (BTB) policies, which prevent employers from asking about criminal records during the hiring process. The intended goal of BTB was to increase job access for individuals with criminal records. However, the study found that employers, unable to observe criminal history, resorted to statistical discrimination based on race, leading to unintended negative consequences.

Three Types of "Ban the Box" Policies:

1.  Public employers only
2.  Private employers with government contracts
3.  All employers

Identification Strategy

-   If any county within a Metropolitan Statistical Area (MSA) adopts BTB, the entire MSA is considered treated.
-   If a state passes a law banning BTB, then all counties in that state are treated.

------------------------------------------------------------------------

The [basic DiD model](#sec-simple-difference-in-differences) is:

$$
Y_{it} = \beta_0 + \beta_1 \text{Post}_t + \beta_2 \text{Treat}_i + \beta_3 (\text{Post}_t \times \text{Treat}_i) + \epsilon_{it}
$$

where:

-   $Y_{it}$ = employment outcome for individual $i$ at time $t$
-   $\text{Post}_t$ = indicator for post-treatment period
-   $\text{Treat}_i$ = indicator for treated MSAs
-   $\beta_3$ = the DiD coefficient, capturing the effect of BTB on employment
-   $\epsilon_{it}$ = error term

**Limitations**: If different locations adopt BTB at different times, this model is not valid due to staggered treatment timing.

------------------------------------------------------------------------

For settings where different MSAs adopt BTB at different times, we use a **staggered DiD** approach:

$$
\begin{aligned} 
E_{imrt} &= \alpha + \beta_1 BTB_{imt} W_{imt} + \beta_2 BTB_{mt} + \beta_3 BTB_{mt} H_{imt} \\
&+ \delta_m + D_{imt} \beta_5 + \lambda_{rt} + \delta_m \times f(t) \beta_7 + e_{imrt} 
\end{aligned}
$$

where:

-   $i$ = individual, $m$ = MSA, $r$ = region (e.g., Midwest, South), $t$ = year
-   $W$ = White; $B$ = Black; $H$ = Hispanic
-   $BTB_{imt}$ = Ban the Box policy indicator
-   $\delta_m$ = MSA fixed effect
-   $D_{imt}$ = individual-level controls
-   $\lambda_{rt}$ = region-by-time fixed effect
-   $\delta_m \times f(t)$ = linear time trend within MSA

**Fixed Effects Considerations**:

-   Including $\lambda_r$ and $\lambda_t$ separately gives broader fixed effects.

-   Using $\lambda_{rt}$ provides more granular controls for regional time trends.

------------------------------------------------------------------------

To estimate the effects for Black men specifically, the model simplifies to:

$$
E_{imrt} = \alpha + BTB_{mt} \beta_1 + \delta_m + D_{imt} \beta_5 + \lambda_{rt} + (\delta_m \times f(t)) \beta_7 + e_{imrt}
$$

------------------------------------------------------------------------

To check for pre-trends and dynamic effects, we estimate:

$$
\begin{aligned} 
E_{imrt} &= \alpha + BTB_{m (t - 3)} \theta_1 + BTB_{m (t - 2)} \theta_2 + BTB_{m (t - 1)} \theta_3 \\
&+ BTB_{mt} \theta_4 + BTB_{m (t + 1)} \theta_5 + BTB_{m (t + 2)} \theta_6 + BTB_{m (t + 3)} \theta_7 \\
&+ \delta_m + D_{imt} \beta_5 + \lambda_{r} + (\delta_m \times f(t)) \beta_7 + e_{imrt}
\end{aligned}
$$

Key points:

-   Leave out $BTB_{m (t - 1)} \theta_3$ as the reference category (to avoid perfect collinearity).
-   If $\theta_2$ is significantly different from $\theta_3$, it suggests pre-trend issues, which could indicate anticipatory effects before BTB implementation.

> Substantively, @shoag2021ban show that Ban-the-box policies increased employment in high-crime neighborhoods by up to 4%, especially in the public sector and low-wage jobs. This is the first nationwide evidence that such laws improve job access for areas with many ex-offenders.

------------------------------------------------------------------------

### Example: Minimum Wage and Employment

@card1993minimum famously studied the effect of an increase in the minimum wage on employment, challenging the traditional economic view that higher wages reduce employment.

-   [Philipp Leppert](https://rpubs.com/phle/r_tutorial_difference_in_differences) provides an R-based replication.
-   Original datasets are available at [David Card's website](https://davidcard.berkeley.edu/data_sets.html).

Setting

-   **Treatment group**: New Jersey (NJ), which increased its minimum wage.
-   **Control group**: Pennsylvania (PA), which did not change its minimum wage.
-   **Outcome variable**: Employment levels in fast-food restaurants.

The study used a Difference-in-Differences approach to estimate the impact:

|           | State | After (Post) | Before (Pre) | Difference        |
|-----------|-------|--------------|--------------|-------------------|
| Treatment | NJ    | A            | B            | A - B             |
| Control   | PA    | C            | D            | C - D             |
|           |       | A - C        | B - D        | (A - B) - (C - D) |

where:

-   $A - B$ captures the treatment effect plus general time trends.
-   $C - D$ captures only the general time trends.
-   $(A - B) - (C - D)$ isolates the causal effect of the minimum wage increase.

For the DiD estimator to be valid, the following conditions must hold:

1.  **Parallel Trends Assumption**
    -   The employment trends in NJ and PA would have been the same in the absence of the policy change.
    -   Pre-treatment employment trends should be similar between the two states.
2.  **No "Switchers"**
    -   The policy must not induce restaurants to switch locations between NJ and PA (e.g., a restaurant relocating across the border).
3.  **PA as a Valid Counterfactual**
    -   PA represents what NJ would have looked like had it not changed the minimum wage.
    -   The study focuses on bordering counties to increase comparability.

------------------------------------------------------------------------

The main regression specification is:

$$
Y_{jt} = \beta_0 + NJ_j \beta_1 + POST_t \beta_2 + (NJ_j \times POST_t)\beta_3+ X_{jt}\beta_4 + \epsilon_{jt}
$$

where:

-   $Y_{jt}$ = Employment in restaurant $j$ at time $t$
-   $NJ_j$ = 1 if restaurant is in NJ, 0 if in PA
-   $POST_t$ = 1 if post-policy period, 0 if pre-policy
-   $(NJ_j \times POST_t)$ = **DiD interaction term**, capturing the causal effect of NJ's minimum wage increase
-   $X_{jt}$ = Additional controls (optional)
-   $\epsilon_{jt}$ = Error term

Notes on Model Specification

-   $\beta_3$ (DiD coefficient) is the key parameter of interest, representing the causal impact of the policy.

-   $\beta_4$ (controls $X_{jt}$) is not necessary for unbiasedness but improves efficiency.

-   If we difference out the pre-period ($\Delta Y_{jt} = Y_{j,Post} - Y_{j,Pre}$), we can simplify the model:

    $$
    \Delta Y_{jt} = \alpha + NJ_j \beta_1 + \epsilon_{jt}
    $$

    Here, we no longer need $\beta_2$ for the post-treatment period.

------------------------------------------------------------------------

An alternative specification uses high-wage NJ restaurants as a control group, arguing that they were not affected by the minimum wage increase. However:

-   This approach eliminates cross-state differences, but
-   It may be harder to interpret causality, as the control group is not entirely untreated.

------------------------------------------------------------------------

A common misconception in DiD is that treatment and control groups must have the same baseline levels of the dependent variable (e.g., employment levels). However:

-   DiD only requires parallel trends, meaning the slopes of employment changes should be the same pre-treatment.
-   If pre-treatment trends diverge, this threatens validity.
-   If post-treatment trends converge, it may suggest policy effects rather than pre-trend violations.

Is Parallel Trends a Necessary or Sufficient Condition?

-   Not sufficient: Even if pre-trends are parallel, other confounders could affect results.
-   Not necessary: Parallel trends may emerge only after treatment, depending on behavioral responses.

Thus, we cannot prove DiD is valid---we can only present evidence that supports the assumptions.

------------------------------------------------------------------------

### Example: The Effects of Grade Policies on Major Choice

@butcher2014effects investigate how grading policies influence students' major choices. The central theory is that grading standards vary by discipline, which affects students' decisions.

Why do the highest-achieving students often major in hard sciences?

1.  **Grading Practices Differ Across Majors**
    -   In STEM fields, grading is often stricter, meaning professors are less likely to give students the benefit of the doubt.
    -   In contrast, softer disciplines (e.g., humanities) may have more lenient grading, making students' experiences more pleasant.
2.  **Labor Market Incentives**
    -   Degrees with lower market value (e.g., humanities) might compensate by offering a more pleasant academic experience.
    -   STEM degrees tend to be more rigorous but provide higher job market returns.

------------------------------------------------------------------------

To examine how grades influence major selection, the study first estimates an OLS model:

$$
E_{ij} = \beta_0 + X_i \beta_1 + G_j \beta_2 + \epsilon_{ij}
$$

where:

-   $E_{ij}$ = Indicator for whether student $i$ chooses major $j$.
-   $X_i$ = Student-level attributes (e.g., SAT scores, demographics).
-   $G_j$ = Average grade in major $j$.
-   $\beta_2$ = Key coefficient, capturing how grading standards influence major choice.

Potential Biases in $\hat{\beta}_2$:

-   **Negative Bias**:
    -   Departments with lower enrollment rates may offer higher grades to attract students.
    -   This endogenous response leads to a downward bias in the OLS estimate.
-   **Positive Bias**:
    -   STEM majors attract the best students, so their grades would naturally be higher if ability were controlled.
    -   If ability is not fully accounted for, $\hat{\beta}_2$ may be upward biased.

------------------------------------------------------------------------

To address potential endogeneity in OLS, the study uses a difference-in-differences approach:

$$
Y_{idt} = \beta_0 + POST_t \beta_1 + Treat_d \beta_2 + (POST_t \times Treat_d)\beta_3 + X_{idt} + \epsilon_{idt}
$$

where:

-   $Y_{idt}$ = Average grade in department $d$ at time $t$ for student $i$.
-   $POST_t$ = 1 if post-policy period, 0 otherwise.
-   $Treat_d$ = 1 if department is treated (i.e., grade policy change), 0 otherwise.
-   $(POST_t \times Treat_d)$ = **DiD interaction term**, capturing the causal effect of grade policy changes on major choice.
-   $X_{idt}$ = Additional student controls.

------------------------------------------------------------------------

+-------------------+-----------------------+-----------------------+------------------+-------------------------+
| Group             | Intercept ($\beta_0$) | Treatment ($\beta_2$) | Post ($\beta_1$) | Interaction ($\beta_3$) |
+===================+=======================+=======================+==================+=========================+
| **Treated, Pre**  | 1                     | 1                     | 0                | 0                       |
+-------------------+-----------------------+-----------------------+------------------+-------------------------+
| **Treated, Post** | 1                     | 1                     | 1                | 1                       |
+-------------------+-----------------------+-----------------------+------------------+-------------------------+
| **Control, Pre**  | 1                     | 0                     | 0                | 0                       |
+-------------------+-----------------------+-----------------------+------------------+-------------------------+
| **Control, Post** | 1                     | 0                     | 1                | 0                       |
+-------------------+-----------------------+-----------------------+------------------+-------------------------+

: Difference-in-Differences Table

-   The average pre-period outcome for the control group is given by $\beta_0$.
-   The key coefficient of interest is $\beta_3$, which captures the difference in the post-treatment effect between treated and control groups.

------------------------------------------------------------------------

A more flexible specification includes fixed effects:

$$
Y_{idt} = \alpha_0 + (POST_t \times Treat_d) \alpha_1 + \theta_d + \delta_t + X_{idt} + u_{idt}
$$

where:

-   $\theta_d$ = Department fixed effects (absorbing $Treat_d$).
-   $\delta_t$ = Time fixed effects (absorbing $POST_t$).
-   $\alpha_1$ = Effect of policy change (equivalent to $\beta_3$ in the simpler model).

Why Use Fixed Effects?

-   **More flexible specification**:
    -   Instead of assuming a uniform treatment effect across groups, this model allows for department-specific differences ($\theta_d$) and time-specific shocks ($\delta_t$).
-   **Higher degrees of freedom**:
    -   Fixed effects absorb variation that would otherwise be attributed to $POST_t$ and $Treat_d$, making the estimation more efficient.

Interpretation of Results

-   If $\alpha_1 > 0$, then the policy **increased** grades in treated departments.
-   If $\alpha_1 < 0$, then the policy **decreased** grades in treated departments.

------------------------------------------------------------------------

## One Difference

The regression formula is as follows @liaukonyte2023frontiers:

$$
y_{ut} = \beta \text{Post}_t + \gamma_u + \gamma_w(t) + \gamma_l + \gamma_g(u)p(t) + \epsilon_{ut}
$$

where

-   $y_{ut}$: Outcome of interest for unit u in time t.
-   $\text{Post}_t$: Dummy variable representing a specific post-event period.
-   $\beta$: Coefficient measuring the average change in the outcome after the event relative to the pre-period.
-   $\gamma_u$: Fixed effects for each unit.
-   $\gamma_w(t)$: Time-specific fixed effects to account for periodic variations.
-   $\gamma_l$: Dummy variable for a specific significant period (e.g., a major event change).
-   $\gamma_g(u)p(t)$: Group x period fixed effects for flexible trends that may vary across different categories (e.g., geographical regions) and periods.
-   $\epsilon_{ut}$: Error term.

This model can be used to analyze the impact of an event on the outcome of interest while controlling for various fixed effects and time-specific variations, but using units themselves pre-treatment as controls.

------------------------------------------------------------------------

## Two-Way Fixed Effects {#sec-two-way-fixed-effects}

A generalization of the [Difference-in-Differences](#sec-difference-in-differences) model is the two-way fixed effects (TWFE) model, which accounts for **multiple groups** and **multiple time periods** by including both unit and time fixed effects. In practice, TWFE is frequently used to estimate causal effects in panel data settings. However, it is **not** a design-based, non-parametric causal estimator [@imai2021use], and it can suffer from severe biases if the treatment effect is heterogeneous across units or time.

When applying TWFE to datasets with **multiple treatment groups** and **staggered treatment timing**, the estimated causal coefficient is a **weighted average** of all possible two-group, two-period DiD comparisons. Crucially, some of these weights can be **negative** [@goodman2021difference], which leads to potential biases. The weighting scheme depends on:

-   **Group sizes**
-   **Variation in treatment timing**
-   **Placement in the middle of the panel** (units in the middle tend to get the highest weight)

------------------------------------------------------------------------

### Canonical TWFE Model

The canonical TWFE model is typically written as:

$$
Y_{it} = \alpha_i + \lambda_t + \tau W_{it} + \beta X_{it} + \epsilon_{it},
$$

where:

-   $Y_{it}$ = Outcome for unit $i$ at time $t$

-   $\alpha_i$ = Unit fixed effect

-   $\lambda_t$ = Time fixed effect

-   $\tau$ = Causal effect of treatment

-   $W_{it}$ = Treatment indicator ($1$ if treated, $0$ otherwise)

-   $X_{it}$ = Covariates

-   $\epsilon_{it}$ = Error term

An illustrative TWFE event-study model [@stevenson2006bargaining]:

$$ \begin{aligned} Y_{it} &= \sum_{k} \beta_{k} \cdot Treatment_{it}^{k} \;+\; \eta_{i} \;+\; \lambda_{t} \;+\; Controls_{it} \;+\; \epsilon_{it}, \end{aligned} $$

where:

-   $Treatment_{it}^k$: Indicator for whether unit $i$ is in its $k$-th year relative to treatment at time $t$.

-   $\eta_i$: Unit fixed effects, controlling for time-invariant unobserved heterogeneity.

-   $\lambda_t$: Time fixed effects, capturing overall macro shocks.

-   Standard Errors: Typically clustered at the group or cohort level.

Usually, researchers drop the period **immediately before treatment** ($k=-1$) to avoid collinearity. However, dropping this or another period inappropriately can shift or bias the estimates.

When there are only two time periods $(T=2)$, TWFE simplifies to the [traditional DiD model](#sec-simple-difference-in-differences). Under **homogeneous treatment effects** and if the [parallel trends assumption](#prior-parallel-trends-test) holds, $\hat{\tau}_{OLS}$ is unbiased. Specifically, the model assumes [@imai2021use]:

1.  **Homogeneous treatment effects** across units and time periods, meaning:
    -   No dynamic treatment effects (i.e., treatment effects do not evolve over time).
    -   The treatment effect is constant across units [@goodman2021difference; @de2020two; @sun2021estimating; @borusyak2021revisiting].
2.  [Parallel trends assumption](#prior-parallel-trends-test)
3.  **Linear additive effects** are valid [@imai2021use].

However, in practice, **treatment effects are often heterogeneous**. If effects vary by cohort or over time, then standard TWFE estimates can be biased---particularly when there is staggered adoption or dynamic treatment effects [@goodman2021difference; @de2020two; @sun2021estimating; @borusyak2021revisiting]. Hence, to use the TWFE, we actually have to argue why the effects are homogeneous to justify TWFE use:

-   **Assess treatment heterogeneity**: If heterogeneity exists, TWFE may produce biased estimates. Researchers should:
    -   Plot treatment timing across units.
    -   Decompose the treatment effect using the [Goodman-Bacon decomposition](#sec-goodman-bacon-decomposition) to identify negative weights.
    -   Check the proportion of never-treated observations: When 80% or more of the sample is never treated, TWFE bias is negligible.
    -   Beware of bias worsening with long-run effects.
-   **Dropping relative time periods**:
    -   If all units eventually receive treatment, two relative time periods must be dropped to avoid multicollinearity.
    -   Some software packages drop periods randomly; if a post-treatment period is dropped, bias may result.
    -   The standard approach is to drop periods -1 and -2.
-   **Sources of treatment heterogeneity**:
    -   Delayed treatment effects: The impact of treatment may take time to manifest.
    -   Evolving effects: Treatment effects can increase or change over time (e.g., phase-in effects).

------------------------------------------------------------------------

TWFE compares different types of treatment/control groups:

-   **Valid comparisons**:
    -   Newly treated units vs. control units
    -   Newly treated units vs. not-yet treated units
-   **Problematic comparisons**:
    -   Newly treated units vs. already treated units (since already treated units do not represent the correct counterfactual).
    -   **Strict exogeneity violations**:
        -   Presence of time-varying confounders
        -   Feedback from past outcomes to treatment [@imai2019should]
    -   **Functional form restrictions**:
        -   Assumes treatment effect homogeneity.
        -   No carryover effects or anticipation effects [@imai2019should].

------------------------------------------------------------------------

### Limitations of TWFE

TWFE DiD is **valid only** under strong assumptions that the treatment effect does not vary across units or over time. In reality, we almost always see some form of **treatment heterogeneity**:

-   **No dynamic treatment effects**: The model requires that the treatment effect not evolve over time.
-   **No unit-level differences**: The treatment effect must be constant across all units.
-   **Linear additive effects**: TWFE assumes that the underlying data-generating process is captured by additive fixed effects plus a constant treatment effect [@imai2021use].

If any of these assumptions are violated, TWFE can produce biased estimates. Specifically:

-   **Negative Weights & Biased Estimates**: With multiple groups and staggered timing, the TWFE estimate becomes a complicated average of "two-group, two-period" DiD comparisons, some of which can receive **negative weights** [@goodman2021difference].
-   **Potential Bias from Dropping Relative Time Periods**: If all units eventually get treated, software often drops a reference period (or periods) to avoid multicollinearity. If the dropped period is post-treatment, the bias can worsen. Researchers often drop relative time $-1$ or $-2$.
-   **Delayed or Evolving Treatment Effects**: If the effect of treatment takes time to manifest or changes over time, TWFE's single coefficient $\tau$ can be misleading.

When **two time periods only** exist, TWFE collapses back to the [traditional DiD model](#sec-simple-difference-in-differences), making these problems far less severe. But as soon as one moves beyond a single treatment period or has variation in treatment timing, these issues become critical.

Several authors [@sun2021estimating; @callaway2021difference; @goodman2021difference] have raised concerns that TWFE DiD regressions under staggered adoption:

-   **Mixes Cohorts**: May unintentionally compare newly treated units to already treated units, conflating post-treatment behavior of early adopters with the pre-treatment trends of later adopters.
-   **Negative Weights**: Some group comparisons receive negative weights, which can reverse the sign of the overall estimate.
-   **Pre-Treatment Leads**: Leads may appear non-zero if earlier-treated groups remain in the sample while later adopters are still untreated.
-   **Long-Run Effects**: Heterogeneity in lagged (long-run) effects can exacerbate bias.

In fields like finance and accounting, [newer estimators](#sec-modern-estimators-for-staggered-adoption) often reveal **null or much smaller** effects than standard TWFE once bias is properly accounted for [@baker2022much].

------------------------------------------------------------------------

### Diagnosing and Addressing Bias in TWFE

Researchers can identify and mitigate the biases arising from heterogeneous treatment effects through diagnostic checks and alternative estimators:

1.  [Goodman-Bacon Decomposition](#sec-goodman-bacon-decomposition)

-   **Purpose**: Decomposes the TWFE DiD estimate into the sum of all two-group, two-period comparisons.
-   **Insight**: Reveals which comparisons have negative weights and how much each comparison contributes to the overall estimate [@goodman2021difference].
-   **Implementation**: Identify subgroups by treatment timing, then examine each group--time pair to see how it contributes to the aggregate TWFE coefficient.

2.  [Plotting Treatment Timing](#sec-visualization-did)

-   **Visual Inspection**: Always plot the distribution of treatment timing across units.
-   **High Risk of Bias**: If treatment is staggered and many units differ in their adoption times, standard TWFE will often be biased.

3.  **Assessing Treatment Heterogeneity Directly**

-   **Check for Variation in Effects**: If there is a theoretical or empirical reason to believe that treatment effects differ by subgroup or over time, TWFE might not be appropriate.
-   **Size of Never-Treated Sample**: When 80% or more of the sample is never treated, the potential for bias in TWFE is smaller. However, large shares of treated units with varied adoption times raise red flags.
-   **Long-Run Effects**: Bias can worsen if the treatment effect accumulates or changes over time.

------------------------------------------------------------------------

#### Goodman-Bacon Decomposition {#sec-goodman-bacon-decomposition}

The Goodman-Bacon decomposition [@goodman2021difference] is a powerful diagnostic tool for understanding the [TWFE](#sec-two-way-fixed-effects) estimator in settings with [staggered treatment adoption](#sec-staggered-difference-in-differences). This approach clarifies how the TWFE DiD estimate is a weighted average of many **2×2 difference-in-differences comparisons** between groups treated at different times (or never treated).

<!-- For an excellent set of explanatory slides by the author, [see here](https://www.stata.com/meeting/chicago19/slides/chicago19_Goodman-Bacon.pdf). -->

Key Takeaways

-   A pairwise DiD estimate ($\tau$) receives **more weight** when:
    -   The treatment happens **closer to the midpoint** of the observation window.
    -   The comparison involves **more observations** (e.g., more units or more years).
-   Comparisons between early-treated and later-treated groups can produce **negative weights**, potentially biasing the aggregate TWFE estimate.

We illustrate the decomposition using the `castle` dataset from the `bacondecomp` package:

```{r}
library(bacondecomp)
library(tidyverse)

# Load and inspect the castle dataset
castle <- bacondecomp::castle %>% 
  dplyr::select(l_homicide, post, state, year)
head(castle)
```

Running the Goodman-Bacon Decomposition

```{r}
# Apply Goodman-Bacon decomposition
df_bacon <- bacon(
  formula = l_homicide ~ post,
  data = castle,
  id_var = "state",
  time_var = "year"
)

# Display weighted average of the decomposition
weighted_avg <- sum(df_bacon$estimate * df_bacon$weight)
weighted_avg
```

Comparing with the TWFE Estimate

```{r}
library(broom)

# Fit a TWFE model
fit_tw <- lm(l_homicide ~ post + factor(state) + factor(year), data = castle)
tidy(fit_tw)
```

> **Interpretation**: The TWFE estimate (approx. 0.08) equals the weighted average of the Bacon decomposition estimates, confirming the decomposition's validity.

------------------------------------------------------------------------

Visualizing the Decomposition

```{r}
library(ggplot2)

ggplot(df_bacon) +
  aes(
    x = weight,
    y = estimate,
    color = type
  ) +
  geom_point() +
  labs(
    x = "Weight",
    y = "Estimate",
    color = "Comparison Type"
  ) +
  causalverse::ama_theme()
```

> **Insight**: This plot shows the contribution of each 2×2 DiD comparison, highlighting how estimates with large weights dominate the overall TWFE coefficient.

------------------------------------------------------------------------

Interpretation and Practical Implications

-   **Purpose**: Decomposes the TWFE DiD estimate into the sum of all two-group, two-period comparisons.
-   **Insight**: Reveals how much each comparison contributes to the overall estimate and whether any have negative or misleading effects.
-   **Implementation**:
    -   Identify subgroups by treatment timing.
    -   Compute DiD for each 2×2 comparison (early vs. late, late vs. never, etc.).
    -   Evaluate how these contribute to the final TWFE estimate.

When time-varying covariates are included that allow for identification within treatment timing groups, certain problematic comparisons (like "early vs. late") may no longer influence the TWFE estimator directly. These scenarios may collapse into simpler within-group estimates, improving identification.

Summary Table: Goodman-Bacon Comparison Types

+---------------------+------------------------------------------------+-----------------------+
| Comparison Type     | Description                                    | Common Issue          |
+=====================+================================================+=======================+
| Treated vs. Never   | Clean comparisons if never-treated units exist | Often reliable        |
+---------------------+------------------------------------------------+-----------------------+
| Early vs. Late      | Later group is control in earlier period       | May introduce bias    |
+---------------------+------------------------------------------------+-----------------------+
| Late vs. Early      | Early group is control in later period         | May reverse causality |
+---------------------+------------------------------------------------+-----------------------+
| Treated vs. Treated | Within-treatment variation by timing           | Sensitive to dynamics |
+---------------------+------------------------------------------------+-----------------------+

------------------------------------------------------------------------

### Remedies for TWFE's Shortcomings

This section outlines **alternative estimators** and design-based approaches that explicitly handle **heterogeneous treatment effects**, **staggered adoption** [@baker2022much], and **dynamic treatment effects** better than standard TWFE (e.g., [Modern Estimators for Staggered Adoption](#sec-modern-estimators-for-staggered-adoption)).

1.  [Group-Time Average Treatment Effects](#sec-group-time-average-treatment-effects-callaway2021difference)

@callaway2021difference propose a two-step approach:

1.  **Group-time treatment effects**: In each time period, estimate the effect for the cohort that first received treatment in that period (compared to a never-treated group).
2.  **Aggregate**: Use a bootstrap procedure to account for autocorrelation and clustering, then aggregate across groups.

-   **Advantages**: Allows for **heterogeneous treatment effects** across groups and over time; compares treated groups only with never-treated units (or well-chosen controls).
-   **Implementation**: `did` package in R.

2.  [Event-Study Design with Cohort-Specific Estimates](#sec-cohort-average-treatment-effects-sun2021estimating)

@sun2021estimating build on @callaway2021difference to handle event-study settings:

-   **Lags and Leads**: Capture dynamic treatment effects by including time lags and leads relative to the event (treatment).
-   **Cohort-Specific Estimates**: Estimate separate paths of outcomes for each cohort, controlling for other cohorts carefully.
-   **Interaction-Weighted Estimator**: Adjusts for differences in when treatment began.
-   **Implementation**: `fixest` package in R.

3.  [Panel Match DiD Estimator with In-and-Out Treatment Conditions](#sec-panel-match-did-estimator-with-in-and-out-treatment-conditions)

@imai2021use develop methods allowing units to **switch in and out** of treatment:

-   **Matching** to create a weighted version of TWFE, addressing some of the bias from heterogeneous effects.
-   **Implementation**: `wfe` and `PanelMatch` R packages.

4.  Two-Stage Difference-in-Differences (DiD2S)

@gardner2022two propose **two-stage DiD**:

-   **Idea**: Partial out fixed effects first, then perform a second-stage regression that focuses on within-group/time variation.
-   **Strength**: Handles heterogeneous treatment effects well, especially when never-treated units are present.
-   **Implementation**: `did2s` R package.

5.  [Switching DiD Estimator](#sec-switching-difference-in-differences-estimator-de2020two)

-   If a study has **never-treated units**, @de2020two suggest an [switching DiD estimator](#sec-switching-difference-in-differences-estimator-de2020two) to recover the [average treatment effect](#sec-average-treatment-effect).
-   **Caveat**: This approach still fails to detect heterogeneity if treatment effects vary with **exposure length** [@sun2022linear].

6.  [Matrix Completion Estimator](#sec-matrix-completion-estimator)
7.  [Reshaped Inverse Probability Weighting--TWFE Estimator](#sec-reshaped-inverse-probability-weighting-twfe-estimator)

-   **Design-Based Approaches**: @arkhangelsky2024design offer further refinements that incorporate inverse probability weighting.
-   **Goal**: Improve balance and reduce bias from non-random treatment timing.

7.  [Stacked DID](#sec-stacked-difference-in-differences) (Simpler but Biased)
    -   Build stacked datasets for each treatment cohort, running separate regressions for each "event window."
    -   This approach is simpler but can still carry biases if the underlying assumptions are violated [@gormley2011growing; @cengiz2019effect; @deshpande2019screened].
8.  [Doubly Robust Difference-in-Differences Estimators] (DR-DID) [@sant2020doubly]
    -   DR-DID estimators combine outcome regression and propensity score weighting to identify treatment effects, remaining consistent if either model is correctly specified.
    -   They achieve local efficiency under joint correctness and can be applied to both panel and repeated cross-section data.
9.  [Nonlinear Difference-in-Differences]

------------------------------------------------------------------------

### Best Practices and Recommendations

Below are practical guidelines for deciding when to use TWFE and how to diagnose or address potential bias.

1.  **When is TWFE Appropriate?**
    -   **Single Treatment Period**: TWFE DiD works well if there is **only one** treatment period for all treated units (no variation in timing).
    -   **Homogeneous Effects**: If strong theoretical or empirical reasons suggest **constant treatment effects** across cohorts and over time, TWFE remains a reasonable choice.
2.  **Diagnosing and Addressing Bias with Staggered Adoption**
    -   **Plot Treatment Timing**: Examine the distribution of treatment timing across units. If treatment adoption is highly staggered, TWFE is likely to produce biased estimates.
    -   **Decomposition Methods**: Use the [Goodman-Bacon Decomposition](#sec-goodman-bacon-decomposition) [@goodman2021difference] to see how TWFE pools comparisons (and whether negative weights emerge). If decomposition is infeasible (e.g., unbalanced panels), the share of never-treated units can indicate potential bias severity.
        -   Decomposes the TWFE DiD estimate into two-group, two-period comparisons.
        -   Identifies which comparisons receive negative weights, which can lead to biased estimates.
        -   Helps determine the influence of specific groups on the overall estimate.
    -   **Discuss Heterogeneity**: Explicitly state the likelihood of treatment effect heterogeneity; incorporate it into the research design.
3.  **Event-Study Specifications within TWFE**
    -   **Avoid Arbitrary Binning**: Do not collapse multiple time periods into a single bin unless you can justify **homogeneous** effects within that bin.
    -   **Full Relative-Time Indicators**: Include flexible event-time indicators, carefully choosing a reference period (commonly $-1$, the year before treatment). Specifically, Include fully flexible relative time indicators, and justify the reference period (usually $l = -1$ or the period just prior to treatment).
    -   **Beware of Multicollinearity**: Including leads and lags can cause multicollinearity and artificially produce significant "pre-trends."
    -   **Drop the Right Periods**: If all units eventually get treated, dropping post-treatment periods accidentally can bias results.
4.  [**Consider Alternative Estimators**](#sec-modern-estimators-for-staggered-adoption)

------------------------------------------------------------------------

## Multiple Periods and Variation in Treatment Timing {#sec-multiple-periods-and-variation-in-treatment-timing}

TWFE has been extended beyond the simple DiD setup to **multiple periods** and **staggered adoption** (where treatment occurs at different times for different units). Such designs are common in applied economics, public policy, and longitudinal research. However, standard TWFE regressions **can** be biased in these contexts when treatment effects are heterogeneous across groups or over time.

### Staggered Difference-in-Differences {#sec-staggered-difference-in-differences}

In **staggered treatment adoption** (also called event-study DiD or dynamic DiD):

-   Different units adopt the treatment at **different time periods**.
-   Standard TWFE often produces biased estimates because it "pools" all treated units (regardless of when they started treatment) together, implicitly comparing newly treated units to already treated ones.
-   Treatments that occurred earlier may contaminate the counterfactual for later adopters if the model does not properly handle dynamic or heterogeneous effects [@wing2024designing; @baker2022much].
-   For applied guidance, see [@wing2024designing] and recommendations in [@baker2022much].

Researchers should be aware that standard TWFE can **mix treatment effects** of early adopters (long-exposed) with later adopters (newly exposed), potentially assigning **negative weights** to particular group comparisons [@goodman2021difference].

When using staggered adoption, the following assumptions are critical:

1.  **Rollout Exogeneity**\
    Treatment assignment and timing should be uncorrelated with potential outcomes.

    -   Evidence: Regress adoption on pre-treatment variables. And if you find evidence of correlation, include linear trends interacted with pre-treatment variables [@hoynes2009consumption]
    -   Evidence: [@deshpande2019screened, p. 223]
        -   Treatment is random: Regress treatment status at the unit level to all pre-treatment observables. If you have some that are predictive of treatment status, you might have to argue why it's not a worry. At best, you want this.
        -   Treatment timing is random: Conditional on treatment, regress timing of the treatment on pre-treatment observables. At least, you want this.

2.  **No Confounding Events**\
    Ensure no other policies or shocks coincide with the staggered treatment rollout.

3.  **Exclusion Restrictions**

    -   **No Anticipation**: Treatment timing should not affect outcomes prior to treatment.
    -   **Invariance to History**: Treatment duration shouldn't matter; only the treated status matters (often violated).

4.  **Standard DID Assumptions**

    -   **Parallel Trends** (Conditional or Unconditional)
    -   **Random Sampling**
    -   **Overlap** (Common Support)
    -   **Effect Additivity**

------------------------------------------------------------------------

